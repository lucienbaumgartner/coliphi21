[{"date.published":"2010-07-21","date.changed":"2017-05-29","url":"https://plato.stanford.edu/entries/computation-physicalsystems/","author1":"Gualtiero Piccinini","author1.info":"http://www.umsl.edu/~piccininig/","entry":"computation-physicalsystems","body.text":"\n\n\n\nIn our ordinary discourse, we distinguish between physical systems\nthat perform computations, such as computers and calculators, and\nphysical systems that don't, such as rocks. Among computing\ndevices, we distinguish between more and less powerful ones. These\ndistinctions affect our behavior: if a device is computationally more\npowerful than another, we pay more money for it. What grounds these\ndistinctions? What is the principled difference, if there is one,\nbetween a rock and a calculator, or between a calculator and a\ncomputer? Answering these questions is more difficult than it may\nseem.\n\n\n\nIn addition to our ordinary discourse, computation is central to\nmany sciences. Computer scientists design, build, and program\ncomputers. But again, what counts as a computer? If a salesperson sold\nyou an ordinary rock as a computer, you should probably get your money\nback. Again, what does the rock lack that a genuine computer has?\n\n\n\nHow powerful a computer can you build? Can you build a machine that\ncomputes anything you wish? Although it is often said that modern\ncomputers can compute anything (i.e., any function of natural numbers,\nor equivalently, any function of strings of letters from a finite\nalphabet), this is incorrect. Ordinary computers can compute only a\ntiny subset of all functions. Is it physically possible to do better?\nWhich functions are physically computable? These questions are bound\nup with the foundations of physics.\n\n\n\nComputation is also central to psychology and neuroscience, and\nperhaps other areas of biology. According to the computational theory\nof cognition, cognition is a kind of computation: the behavior of\ncognitive systems is causally explained by the computations they\nperform. In order to test a computational theory of something, we need\nto know what counts as a computation in a physical system. Once again,\nthe nature of computation lies at the foundation of empirical\nscience.\n\n\n\nComputation may be studied mathematically by formally defining\ncomputational objects, such as algorithms and Turing machines, and\nproving theorems about their properties. The mathematical theory of\ncomputation is a well-established branch of mathematics. It deals with\ncomputation in the abstract, without worrying much about physical\nimplementation. \n\nBy contrast, most uses of computation in science and ordinary\npractice deal with concrete computation: computation in concrete\nphysical systems such as computers and brains. Concrete computation is\nclosely related to abstract computation: we speak of physical systems\nas running an algorithm or as implementing a Turing machine, for\nexample. But the relationship between concrete computation and abstract\ncomputation is not part of the mathematical theory of computation per\nse and requires further investigation. Questions about concrete\ncomputation are the main subject of this entry. Nevertheless, it is\nimportant to bear in mind some basic mathematical results. \n\nThe most important notion of computation is that of digital\ncomputation, which Alan Turing, Kurt Gödel, Alonzo\nChurch, Emil Post, and Stephen Kleene formalized in the 1930s. Their\nwork investigated the foundations of mathematics. One crucial question\nwas whether first order logic is decidable — whether there\nis an algorithm that determines whether any given first order logical\nformula is a theorem. \n\nTuring (1936–7) and Church (1936) proved that the answer is\nnegative: there is no such algorithm. To show this, they offered\nprecise characterizations of the informal notion of algorithmically\ncomputable function. Turing did so in terms of so-called Turing\nmachines — devices that manipulate discrete symbols written on a\ntape in accordance with finitely many instructions. Other logicians did\nthe same thing — they formalized the notion of algorithmically\ncomputable function — in terms of other notions, such as\nλ-definable functions and general recursive functions. \n\nTo their surprise, all such notions turned out to be extensionally\nequivalent, that is, any function computable within any of these\nformalisms is computable within any of the others. They took this as\nevidence that their quest for a precise definition of\n“algorithm” or “algorithmically computable\nfunction” had been successful. The resulting view — that\nTuring machines and other equivalent formalisms capture the informal\nnotion of algorithm — is now known as the Church-Turing thesis\n(more on this in Section 4). The study of computable functions, made\npossible by the work of Turing et al., is part of the mathematical\ntheory of computation. \n\nThe theoretical significance of Turing et al.'s notion of\ncomputation can hardly be overstated. As Gödel pointed out (in a\nlecture following one by Tarski): \n\nTuring also showed that there are universal Turing\nmachines — machines that can compute any function computable by any\nother Turing machine. Universal machines do this by executing\ninstructions that encode the behavior of the machine they simulate.\nAssuming the Church-Turing thesis, universal Turing machines can\ncompute any function computable by algorithm. This result is\nsignificant for computer science: you don't need to build\ndifferent computers for different functions; one universal computer\nwill suffice to compute any computable function. Modern digital\ncomputers approximate universal machines in Turing's sense:\ndigital computers can compute any function computable by algorithm for\nas long as they have time and memory. (Strictly speaking, a universal\nmachine has an unbounded memory, whereas digital computer memories can\nbe extended but not indefinitely, so they are not unbounded.) \n\nThe above result should not be confused with the common claim that\ncomputers can compute anything. This claim is false: another\nimportant result of computability theory is that most functions\nare not computable by Turing machines (and hence, by digital\ncomputers). Turing machines compute functions defined over denumerable\ndomains, such as strings of letters from a finite alphabet. There are\nuncountably many such functions. But there are only countably many\nTuring machines; you can enumerate Turing machines by enumerating all\nlists of Turing machine instructions. Since an uncountable infinity is\nmuch larger than a countable one, it follows that Turing machines (and\nhence digital computers) can compute only a tiny portion of all\nfunctions (over denumerable domains, such as natural numbers or\nstrings of letters). \n\nTuring machines and most modern computers are known as (classical)\ndigital computers, that is, computers that manipulate strings\nof discrete, unambiguously distinguishable states. Digital computers\nare sometimes contrasted with analog computers, that\nis, machines that manipulate continuous variables. Continuous variables\nare variables that can change their value continuously over time while\ntaking any value within a certain interval. Analog computers are used\nprimarily to solve certain systems of differential equations (Pour-El\n1974, Rubel 1993). \n\nClassical digital computers may also be contrasted with\nquantum computers. Quantum computers manipulate\nquantum states called qubits. Unlike the computational states of\ndigital computers, qubits are not unambiguously distinguishable from\none another. This entry will focus primarily on classical digital\ncomputation. For more on quantum computation, see the entry on quantum\ncomputing. \n\nThe same objects studied in the mathematical theory of\ncomputation — Turing machines, algorithms, and so on — are\ntypically said to be implemented by concrete physical systems. This\nposes a problem: how can a concrete, physical system perform a\ncomputation when computation is defined by an abstract mathematical\nformalism? This may be called the problem of computational\nimplementation. \n\nThe problem of computational implementation may be formulated in a\ncouple of different ways. Some people interpret the formalisms of\ncomputability theory as defining abstract objects. According\nto this interpretation, Turing machines, algorithms, and the like are\nabstract objects. But how can a concrete physical system implement an\nabstract object? Other people treat the formalisms of computability\ntheory simply as abstract computational descriptions. But how\ncan a concrete physical system satisfy an abstract computational\ndescription? Regardless of how the problem of computational\nimplementation is formulated, solving it requires an account of\nconcrete computation — an account of what it takes for a physical\nsystem to perform a given computation. \n\nA closely related problem is that of distinguishing between physical\nsystems such as digital computers, which appear to compute, and\nphysical systems such as rocks, which appear not to compute. Unlike\ncomputers, ordinary rocks are not sold in computer stores and are\nusually not considered computers. Why? What do computers have that\nrocks lack, such that computers compute and rocks don't? (If\nindeed they don't?) In other words, what does it take for a\ncomputation to be implemented in a concrete physical system? Different\nanswers to these questions give rise to different accounts of concrete\ncomputation. \n\nQuestions on the nature of concrete computation should not be\nconfused with questions about computational modeling. The dynamical\nevolution of many physical systems may be described by computational\nmodels. Computational models describe the dynamics of a system that are\nwritten into, and run by, a computer. The behavior of rocks — as\nwell as rivers, ecosystems, and planetary systems, among many\nothers — may well be modeled computationally. From this, it\ndoesn't follow that the modeled systems are computing\ndevices — that they themselves perform computations. Prima facie,\nonly relatively few and quite special systems compute. Explaining what\nmakes them special — or explaining away our feeling that they are\nspecial — is the job of an account of concrete computation. \n\nOne of the earliest and most influential accounts of computation is\ndue to Hilary Putnam. To a first approximation, the account says that\nanything that is accurately described by a computational description\nC is a computing system implementing C. \n\nMore precisely, Putnam sketches his earliest account in terms of\nTuring machines only, appealing to the “machine tables”\nthat are a standard way of defining specific Turing machines. A machine\ntable consists of one column for each of the (finitely many) internal\nstates of the Turing machine and one row for each of the\nmachine's symbol types. Each entry in the machine table specifies\nwhat the machine does given the pertinent symbol and internal state.\nHere is how Putnam explains what it takes for a physical system to be a\nTuring machine: \n\nThis account relies on several unexplained notions, such as square\n(of tape), symbol, scanning, and carrying out an instruction.\nFurthermore, the account is specified in terms of Turing machine\ntables, but there are other kinds of computational description. A\ngeneral account of concrete computation should cover other\ncomputational descriptions besides Turing machine tables. Perhaps for\nthese reasons, Putnam — soon followed by many\nothers — abandoned reference to squares, symbols, etc.; he\nsubstituted them with an appeal to a physical description of the\nsystem. The result of that substitution is what Godfrey-Smith (2009)\ndubs the “simple mapping account” of computation. \n\nAccording to the simple mapping account, a physical system\nS performs computation C just in case (i) there is a\nmapping from the states ascribed to S by a physical\ndescription to the states defined by computational description\nC, such that (ii) the state transitions between the physical\nstates mirror the state transitions between the computational states.\nClause (ii) requires that for any computational state transition of the\nform s1 → s2 (specified by\nthe computational description C), if the system is in the\nphysical state that maps onto s1, it then goes into\nthe physical state that maps onto s2. \n\nOne difficulty with the formulation above is that ordinary physical\ndescriptions, such as systems of differential equations, generally\nascribe uncountably many states to physical systems, whereas ordinary\ncomputational descriptions, such as Turing machine tables, ascribe at\nmost countably many states. Thus, there are not enough computational\nstates for the physical states to map onto. One solution to this\nproblem is to reverse the direction of the mapping, requiring a mapping\nof the computational states onto (a subset of) the physical states.\nAnother, more common solution to this problem — often left\nimplicit — is to select either a subset of the physical states or\nequivalence classes of the physical states and map those onto the\ncomputational states. When this is done, clause (i) is replaced by the\nfollowing: (i′) there is a mapping from a subset of (or\nequivalence classes of) the states ascribed to S by a\nphysical description to the states defined by computational description\nC. \n\nThe simple mapping account turns out to be very liberal: it\nattributes many computations to many systems. In the absence of\nrestrictions on which mappings are acceptable, such mappings are\nrelatively easy to come by. As a consequence, some have argued that\nevery physical system implements every computation (Putnam 1988, Searle\n1992). This thesis, which trivializes the claim that something is a\ncomputing system, will be discussed in Section 3.1. Meanwhile, the\ndesire to avoid this trivialization result is one motivation behind\nother accounts of concrete computation. \n\nOne way to construct accounts of computation that are more\nrestrictive than the simple mapping account is to impose a constraint\non acceptable mappings. Specifically, clause (ii) may be modified so as\nto require that the conditional that specifies the relevant physical\nstate transitions be logically stronger than a material\nconditional. \n\nAs the simple mapping account has it, clause (ii) requires that for\nany computational state transition of the form s1\n→ s2 (specified by a computational\ndescription), if the system is in the physical state that maps onto\ns1, it then goes into the physical state that maps\nonto s2. The second part of (ii) is a material\nconditional. It may be strengthened by turning it into a logically\nstronger conditional — specifically, a conditional expressing a\nrelation that supports counterfactuals. \n\nIn a pure counterfactual account, clause (ii) is strengthened simply\nby requiring that the physical state transitions support certain\ncounterfactuals (Block 1978, Maudlin 1989, Copeland 1996, Rescorla\n2014). In other words, the pure counterfactual account requires the\nmapping between computational and physical descriptions to be such\nthat the counterfactual relations between the physical states are\nisomorphic to the counterfactual relations between the computational\nstates. \n\nDifferent authors formulate the relevant counterfactuals in slightly\ndifferent ways: (a) if the system had been in a physical state that\nmaps onto an arbitrary computational state (specified by the relevant\ncomputational description), it would then have gone into a physical\nstate that maps onto the relevant subsequent computational state (as\nspecified by the computational description) (Maudlin 1989, 415), (b) if\nthe system had been in a physical state that maps onto\ns1, it would have gone into a physical state that\nmaps onto s2 (Copeland 1996, 341), (c) if the\nsystem were in a physical state that maps onto s1,\nit would go into a physical state that maps onto s2\n(Chalmers 1996, 312). Regardless of the exact formulation, none of\nthese counterfactuals are satisfied by the material conditional of\nclause (ii) as it appears in the simple mapping account of computation.\nThus, counterfactual accounts are stronger than the simple mapping\naccount. \n\nAn account of concrete computation in which the physical state\ntransitions support counterfactuals may also be generated by appealing\nto causal or dispositional relations, assuming (as most people do) that\ncausal or dispositional relations support counterfactuals. Appealing to\ncausation or dispositions may also have advantages over pure\ncounterfactual accounts in blocking unwanted computational\nimplementations (Klein 2008, 145, makes the case for dispositional\nversus counterfactual accounts). \n\nIn a causal account, clause (ii) is strengthened by requiring a\ncausal relation between the physical states: for any computational\nstate transition of the form s1 →\ns2 (specified by a computational description), if\nthe system is in the physical state that maps onto\ns1, its physical state causes it to go\ninto the physical state that maps onto s2 (Chrisley\n1995, Chalmers 1995, 1996, 2011, Scheutz 1999, 2001). \n\nTo this causal constraint on acceptable mappings, David Chalmers\n(1995, 1996, 2011) adds a further restriction (in order to avoid\nunlimited pancomputationalism, which is discussed in Section 3): a genuine\nphysical implementation of a computational system must divide into\nseparate physical components, each of which maps onto the components\nspecified by the computational formalism. As Godfrey-Smith (2009, 293)\nnotes, this combination of a causal and a localizational\nconstraint goes in the direction of mechanistic explanation (Machamer,\nDarden, and Craver 2000). An account of computation that is explicitly\nbased on mechanistic explanation will be discussed in Section 2.5. For\nnow, the causal account simpliciter requires only that the mappings\nbetween computational and physical descriptions be such that the causal\nrelations between the physical states are isomorphic to the relations\nbetween state transitions specified by the computational description.\nThus, according to the causal account, concrete computation is the\ncausal structure of a physical process. \n\nIn a dispositional account, clause (ii) is strengthened by requiring\na dispositional relation between the physical states: for any\ncomputational state transition of the form s1\n→ s2 (specified by a computational\ndescription), if the system is in the physical state that maps onto\ns1, the system manifests a disposition\nwhose manifestation is the transition from the physical state that maps\nonto s1 to the physical state that maps onto\ns2 (Klein 2008). In other words, the dispositional\naccount requires the mapping between computational and physical\ndescriptions to be such that the dispositional relations between the\nphysical states are isomorphic to the relations between state\ntransitions specified by the computational description. Thus, according\nto the dispositional account, concrete computation is the dispositional\nstructure of a physical process. \n\nThe difference between the simple mapping account on the one hand\nand counterfactual, causal, and dispositional accounts on the other may\nbe seen by examining a simple example. \n\nConsider a rock under the sun, early in the morning. During any time\ninterval, the rock's temperature rises. The rock goes from\ntemperature T to temperature T+1, to T+2, to\nT+3. Now consider a NOT gate that feeds its output back to\nitself. At first, suppose the NOT gate receives ‘0’ as an\ninput; it then returns a ‘1’. After the ‘1’ is\nfed back to the NOT gate, the gate returns a ‘0’ again, and\nso on. The NOT gate goes back and forth between outputting a\n‘0’ and outputting a ‘1’. Now map physical\nstates T and T+2 onto ‘0’; then map\nT+1 and T+3 onto ‘1’. \n\nAccording to the simple mapping account, the rock implements a NOT gate\nundergoing the computation represented by ‘0101’. \n\nBy contast, according to the counterfactual account, the rock's putative\ncomputational implementation is spurious, because the physical state\ntransitions do not support counterfactuals. If the rock were put in\nstate T, it may or may not transition into T+1\ndepending on whether it is morning or evening and other extraneous\nfactors. Since the rock's physical state transitions that map\nonto the NOT gate's computational state transitions do not\nsupport counterfactuals, the rock does not implement the NOT gate\naccording to the counterfactual account. \n\nAccording to the causal and dispositional accounts too, this\nputative computational implementation is spurious, because the physical\nstate transitions are not due to causal or dispositional properties of\nthe rock and its states. T does not cause T+1, nor\ndoes the rock have a disposition to go into T+1 when it is in\nT. Rather, the rock changes its state due to the action of the\nsun. Since the rock's physical state transitions that map onto\nthe NOT gate's computational state transitions are not grounded\nin either the causal or dispositional properties of the rock and its\nstates, the rock does not implement the NOT gate according to the\ncausal and dispositional accounts. \n\nIt is important to note that under the present family of accounts,\nthere are mappings between any physical system and at least some\ncomputational descriptions. Thus, according to the present accounts,\neverything performs at least some computations (cf. Section 3.2). This\nstill strikes some as overly inclusive. In computer science and\ncognitive science, there seems to be a distinction between systems\nthat compute and systems that do not. To account for this distinction,\none option is to retain the current account of computational\nimplementation while restricting the class of descriptions that count\nas computational descriptions.  Another option is to move beyond this\naccount of implementation. \n\nIn our everyday life, we usually employ computations to process\nmeaningful symbols, in order to extract information from them. The\nsemantic account of computation turns this practice into a metaphysical\ndoctrine: computation is the processing of\nrepresentations — or at least, the processing of appropriate\nrepresentations in appropriate ways. Opinions as to which\nrepresentational manipulations constitute computations vary a great\ndeal (Fodor 1975, Cummins 1983, Pylyshyn 1984, Churchland and Sejnowski\n1992, Shagrir 2006). What all versions of the semantic account have in\ncommon is that they take seriously the reference to symbols in\nPutnam's original account of computation: there is “no\ncomputation without representation” (Fodor 1981, 180). \n\nThe semantic account may be seen as imposing a further restriction\non acceptable mappings. In addition to the causal restriction imposed\nby the causal account (mutatis mutandis for the counterfactual and\ndispositional accounts), the semantic account imposes a semantic\nrestriction. Only physical states that qualify as representations may\nbe mapped onto computational descriptions, thereby qualifying as\ncomputational states. If a state is not representational, it is not\ncomputational either. \n\nThe semantic account is probably the most popular in the philosophy\nof mind, because it appears to fit its specific needs better than other\naccounts. Since minds and digital computers are generally assumed to\nmanipulate (the right kind of) representations, they turn out to\ncompute. Since most other systems are generally assumed not to\nmanipulate (the relevant kind of) representations, they do not compute.\nThus, the semantic account appears to accommodate some common\nintuitions about what does and does not count as a computing system. It\nkeeps minds and computers in while leaving most everything else out,\nthereby vindicating the computational theory of cognition as a strong\nand nontrivial theory. \n\nThe semantic account raises three important questions: how\nrepresentations are to be individuated, what counts as a representation\nof the relevant kind, and what gives representations their semantic\ncontent. \n\nOn the individuation of computational states, the main debate\ndivides internalists from externalists. According to externalists,\ncomputational vehicles are symbols individuated by their wide\ncognitive contents — paradigmatically, the things that the\nsymbols stand for (Burge 1986, Shapiro 1997, Shagrir 2001). By\ncontrast, most internalists maintain that computational vehicles are\nsymbols individuated by narrow cognitive contents (Segal\n1991). Narrow contents are, roughly speaking, semantic contents defined\nin terms of intrinsic properties of the system. Cognitive\ncontents, in turn, are contents ascribed to a system by a cognitive\npsychological theory. For instance, the cognitive contents of the\nvisual system are visual contents, whereas the cognitive contents of\nthe auditory system are auditory contents. \n\nTo illustrate the dispute, consider two physically identical\ncognitive systems A and B. Among the symbols\nprocessed by A is symbol S. A produces\ninstances of S whenever A is in front of bodies of\nwater, when A is thinking of water, and when A is\nforming plans to interact with water. In short, symbol S\nappears to stand for water. Every time A processes S,\nsystem B processes symbol S′, which is\nphysically identical to S. But system B lives in an\nenvironment different from A's environment. Whenever\nA is surrounded by water, B is surrounded by\ntwater. Twater is a substance superficially indistinguishable\nfrom water but in fact physically different from it. Thus, symbol\nS′ appears to stand for twater (cf. Putnam 1975b). So,\nwe are assuming that A and B live in relevantly\ndifferent environments, such that S appears to stand for water\nwhile S′ appears to stand for twater. We are also\nassuming that A is processing S in the same way that\nB is processing S′. There is no intrinsic\nphysical difference between A and B. \n\nAccording to externalists, when A is processing S\nand B is processing S′ they are in\ncomputational states of different types. According to\ninternalists, A and B are in computational states of\nthe same type. In other words, externalists maintain that\ncomputational states are individuated in part by their reference, which\nis determined at least in part independently of the intrinsic physical\nproperties of cognitive systems. By contrast, internalists maintain\nthat computational states are individuated in a way that supervenes\nsolely on the intrinsic physical properties of cognitive systems. \n\nSo far, externalists and internalists agree on one thing:\ncomputational states are individuated by cognitive contents.\nThis assumption can be resisted without abandoning the semantic account\nof computation. According to Egan (1999), computational vehicles are\nnot individuated by cognitive contents of any kind, whether wide or\nnarrow. Rather, they are individuated by their mathematical\ncontents — that is, mathematical functions and objects ascribed as\nsemantic contents to the computational vehicles by a computational\ntheory of the system. Since mathematical contents are the same across\nphysical duplicates, Egan maintains that her mathematical contents are\na kind of narrow content — she is a kind of internalist. \n\nLet us now turn to what counts as a representation. This debate is\nless clearly delineated. According to some authors, only structures\nthat have a language-like combinatorial syntax, which supports a\ncompositional semantics, count as computational vehicles, and only\nmanipulations that respect the semantic properties of such structures\ncount as computations (Fodor 1975, Pylyshyn 1984). This suggestion\nflies in the face of computability theory, which imposes no such\nrequirement on what counts as a computational vehicle. Other authors\nare more inclusive on what representational manipulations count as\ncomputations, but they have not been especially successful in drawing\nthe line between computational and non-computational processes. Few\npeople would include all manipulations of\nrepresentations — including, say, painting a picture and recording\na speech — as computations, but there is no consensus on where to\ndraw the boundary between representational manipulations that count as\ncomputations and representational manipulations that do not. \n\nA third question is what gives representations their semantic\ncontent. There are three families of views. Instrumentalists believe\nthat ascribing semantic content to things is just heuristically useful\nfor prediction and explanation; semantic properties are not real\nproperties of computational states (e.g., Dennett 1987, Egan\n2010). Realists who are not naturalists believe semantic\nproperties are real properties of computational states, but they are\nirreducible to non-semantic properties. Finally, realists who are also\nnaturalists believe semantic properties are both real and reducible to\nnon-semantic properties, though they disagree on exactly how to reduce\nthem (e.g., Fodor 2008, Harman 1987). \n\nThe semantic account of computation is closely related to the common\nview that computation is information processing. This idea is less\nclear than it may seem, because there are several notions of\ninformation. The connection between information processing and\ncomputation is different depending on which notion of information is at\nstake. What follows is a brief disambiguation of the view that\ncomputation is information processing based on four important notions\nof information (cf. Piccinini and Scarantino 2011). Information in the sense of thermodynamics is closely related to\nthermodynamic entropy. Entropy is a property of every physical\nsystem. Thermodynamic entropy is, roughly, a measure of an\nobserver's uncertainty about the microscopic state of a system\nafter she considers the observable macroscopic properties of the\nsystem. The study of the thermodynamics of computation is a lively\nfield with many implications in the foundations of physics (Leff and\nRex 2003). In this thermodynamic sense of “information,”\nany difference between two distinguishable states of a system may be\nsaid to carry information. Computation may well be said to be\ninformation processing in this sense, but this has little to do with\nsemantics properly so called. However, the connections between\nthermodynamics, computation, and information theory are one possible\nsource of inspiration for the view that every physical system is a\ncomputing system (see Section 3.4). Information in the sense of communication theory is a measure of\nthe average likelihood that a given message is transmitted between a\nsource and a receiver (Shannon and Weaver 1949). This has little to do\nwith semantics, too. Information in one semantic sense is approximately the same as\n“natural meaning” (Grice 1957). A signal carries\ninformation in this sense just in case it reliably correlates with a\nsource (Dretske 1981). The view that computation is information\nprocessing in this sense is prima facie implausible, because many\ncomputations — such as arithmetical calculations carried out on\ndigital computers — do not seem to carry any natural meaning.\nNevertheless, this notion of semantic information is relevant here\nbecause it has been used by some theorists to ground an account of\nrepresentation (Dretske 1981, Fodor 2008). Information in another semantic sense is just ordinary semantic\ncontent or “non-natural meaning” (Grice 1957). This is the\nkind of semantic content that most philosophers discuss. The view that\ncomputation is information processing in this sense is similar to a\ngeneric semantic account of computation. \n\nAlthough the semantic account of computation appears to fit the needs\nof philosophers of mind, it appears less suited to make sense of other\nsciences. Most pertinently, representation does not seem to be\npresupposed by the notion of computation employed in at least some\nareas of cognitive science as well as computability theory and\ncomputer science — the very sciences that gave rise to the\nnotion of computation at the origin of the computational theory of\ncognition (Fresco 2010). If this is correct, the\nsemantic account may not even be adequate to the needs of philosophers\nof mind — at least those philosophers of mind who wish to make\nsense of the analogy between minds and the systems designed and\nstudied by computer scientists and computability theorists. Another\ncriticism of the semantic account is that specifying the kind of\nrepresentation and representational manipulation that is relevant to\ncomputation may require a non-semantic way of individuating\ncomputations (Piccinini 2004). These concerns motivate efforts to\naccount for computation in non-semantic terms. \n\nAs we saw, the semantic account needs to specify which\nrepresentations are relevant to computation. One view is that the\nrelevant representations are language-like, that is, they have the kind\nof syntactic structure exhibited by sentences in a language.\nComputation, then, is the manipulation of language-like representations\nin a way that is sensitive to their syntactic structure and preserves\ntheir semantic properties (Fodor 1975). \n\nAs discussed in the previous section, however, using the notion of\nrepresentation in an account of computation involves some difficulties.\nIf computation could be accounted for without appealing to\nrepresentation, those difficulties would be avoided. One way to do so\nis to maintain that computation simply is the manipulation of\nlanguage-like structures in accordance with their syntactic properties,\nleaving semantics by the wayside. The structures being manipulated are\nassumed to be language-like only in that they have syntactic\nproperties — they need not have any semantics. In this syntactic\naccount of computation, the notion of representation is not used at\nall. \n\nThe syntactic account may be seen as adding a restriction on\nacceptable mappings that replaces the semantic restriction proposed by\nthe semantic account. Instead of a semantic restriction, the syntactic\naccount imposes a syntactic restriction: only physical states that\nqualify as syntactic may be mapped onto computational descriptions,\nthereby qualifying as computational states. If a state lacks syntactic\nstructure, it is not computational. \n\nWhat remains to be seen is what counts as a syntactic state. An\nimportant account of syntax in the physical world is due to Stephen\nStich (1983, 150–157). Although Stich does not use the term\n“computation,” his account of syntax is aimed at grounding\na syntactic account of mental states and processes. Stich's\nsyntactic theory of mind is, in turn, his interpretation of the\ncomputational theories proposed by cognitive scientists — in\ncompetition with Fodor's semantic interpretation. Since\nStich's account of syntax is ultimately aimed at grounding\ncomputational theories of cognition, Stich's account of syntax\nalso provides an (implicit) syntactic account of computation. \n\nAccording to Stich, roughly speaking, a physical system contains\nsyntactically structured objects when two conditions are satisfied.\nFirst, there is a mapping between the behaviorally relevant physical\nstates of the system and a class of syntactic types, which are\nspecified by a grammar that defines how complex syntactic types can be\nformed out of (finitely many) primitive syntactic types. Second, the\nbehavior of the system is explained by a theory whose generalizations\nare formulated in terms of formal relations between the syntactic types\nthat map onto the physical states of the system. \n\nThe syntactic account of computation is not very popular. A common\nobjection is that it seems difficult to give an account of primitive\nsyntactic types that does not presuppose a prior semantic individuation\nof the types (Crane 1990, Jacquette 1991, Bontly 1998). In fact, it is\ncommon to make sense of syntax by construing it as a way to combine\nsymbols, that is, semantically interpreted constituents. If syntax is\nconstrued in this way, it presupposes semantics. And if so, the\nsyntactic account of computation collapses into the semantic\naccount. \n\nAnother objection is that language-like syntactic structure is not\nnecessary for computation as it is understood in computer science and\ncomputability theory. Although computing systems surely can manipulate\nlinguistic structures, they don't have to. They can also\nmanipulate simple sequences of letters, without losing their identity\nas computers. (Computability theorists call any set of words from a\nfinite alphabet a language, but that broad notion of language should\nnot be confused with the narrower notion — inspired by grammars in\nlogic and linguistics — that Stich employs in his syntactic account\nof computation.)  \nThe mechanistic account (Piccinini 2007, 2015; cf. Kaplan 2011,\nMilkowski 2013, Fresco 2014) avoids appealing to both syntax and\nsemantics.  Instead, it accounts for concrete computation in terms of\nthe mechanistic properties of a system. According to the mechanistic\naccount, concrete computing systems are functional mechanisms of a\nspecial kind — mechanisms that perform concrete\ncomputations. \n\nA functional mechanism is a system of organized components, each of\nwhich has functions to perform (cf. Craver 2012, Garson 2013). When\nappropriate components and their functions are appropriately organized\nand functioning properly, their combined activities constitute the\ncapacities of the mechanism. Conversely, when we look for an\nexplanation of the capacities of a mechanism, we decompose the\nmechanism into its components and look for their functions and\norganization. The result is a mechanistic explanation of the\nmechanism's capacities. \n\nThis notion of mechanism is familiar to biologists and engineers.\nFor example, biologists explain physiological capacities (digestion,\nrespiration, etc.) in terms of the functions performed by systems of\norganized components (the digestive system, the respiratory system,\netc.). \n\nAccording to the mechanistic account, a computation in the generic\nsense is the processing of vehicles according to rules that are\nsensitive to certain vehicle properties, and specifically, to\ndifferences between different portions of the vehicles. The processing\nis performed by a functional mechanism, that is, a mechanism whose\ncomponents are functionally organized to perform the computation. Thus,\nif the mechanism malfunctions, a miscomputation occurs. \n\nDigital computation, analog computation, etc. turn out to be species\nof generic computation. They are differentiated by more specific\nproperties of the vehicles being processed. If a computing system\nprocesses strings of discrete states, then it performs a digital\ncomputation. If a computing system processes continuous variables, then\nit performs an analog computation. If a computing system processes\nqubits, then it performs a quantum computation. \n\nWhen we define concrete computations and the vehicles that they\nmanipulate, we need not consider all of their specific physical\nproperties. We may consider only the properties that are relevant to\nthe computation, according to the rules that define the computation. A\nphysical system can be described more or less abstractly. According to\nthe mechanistic account, an abstract description of a physical system\nis not a description of an abstract object but rather a description of\na concrete system that omits certain details. Descriptions of concrete\ncomputations and their vehicles are sufficiently abstract as to be\ndefined independently of the physical media that implement them in\nparticular cases. Because of this, concrete computations and their vehicles\nare sometimes said to be ‘medium-independent’. \n\nIn other words, a vehicle is medium-independent just in case the\nrules (i.e., the input-output maps) that define a computation are\nsensitive only to differences between portions of the vehicles along\nspecific dimensions of variation — they are insensitive to any more\nconcrete physical properties of the vehicles. Put yet another way, the\nrules are functions of state variables associated with a set of\nfunctionally relevant degrees of freedom, which can be implemented\ndifferently in different physical media. Thus, a given computation can\nbe implemented in multiple physical media (e.g., mechanical,\nelectro-mechanical, electronic, magnetic, etc.), provided that the\nmedia possess a sufficient number of dimensions of variation (or\ndegrees of freedom) that can be appropriately accessed and manipulated\nand that the components of the mechanism are functionally organized in\nthe appropriate way. \n\nNotice that the mechanistic account avoids pancomputationalism.\nFirst, physical systems that are not functional mechanisms are ruled\nout. Functional mechanisms are complex systems of components that are\norganized to perform functions. Any system whose components are not\norganized to perform functions is not a computing system because it is\nnot a functional mechanism. Second, mechanisms that lack the function\nof manipulating medium-independent vehicles are ruled out. Finally,\nmedium-independent vehicle manipulators whose manipulations fail to\naccord with appropriate rules are ruled out. The second and third\nconstraints appeal to special functional properties — manipulating\nmedium-independent vehicles, doing so in accordance with rules defined\nover the vehicles — that are possessed only by relatively few\nphysical systems. According to the mechanistic account, those few\nsystems are the genuine computing systems. \n\nAnother feature of the mechanistic account is that it accounts for the\npossibility of miscomputation — a possibility difficult to make\nsense of under other accounts. To illustrate the point, consider an\nordinary computer programmed to compute function f on input\ni. Suppose that the computer malfunctions and produces an\noutput different from f(i). According to the causal\n(semantic) account, the computer just underwent a causal process (a\nmanipulation of representations), which may be given a computational\ndescription and hence counts as computing some function\ng(i), where g≠f. By contrast,\naccording to the mechanistic account, the computer simply failed to\ncompute, or at least it failed to complete its computation correctly. Given the\nimportance of avoiding miscomputations in the design and use of\ncomputers, the ability of the mechanistic account to make sense of\nmiscomputation may be an advantage over rival accounts. \n\nA final feature of the mechanistic account is that it distinguishes\nand characterizes precisely many different kinds of computing systems\nbased on the specific vehicles they manipulate and their specific\nmechanistic properties. The mechanistic account has been used to\nexplicate digital computation, analog computation, computation by\nneural networks, and other important distinctions such as hardwired\nvs. programmable and serial vs. parallel computation (Piccinini\n2015). \n\nWhich physical systems perform computations? According to\npancomputationalism, they all do. Even rocks, hurricanes, and planetary\nsystems — contrary to appearances — are computing systems.\nPancomputationalism is quite popular among some philosophers and\nphysicists. \n\nVarieties of pancomputationalism vary with respect to how\nmany computations — all, many, a few, or just one — they\nattribute to each system. \n\nThe strongest version of pancomputationalism is that every physical\nsystem performs every computation — or at least, every\nsufficiently complex system implements a large number of non-equivalent\ncomputations (Putnam 1988, Searle 1992). This may be called\nunlimited pancomputationalism. \n\nThe weakest version of pancomputationalism is that every physical\nsystem performs some (as opposed to every)\ncomputation. A slightly stronger version maintains that everything\nperforms a few computations, some of which encode the others\nin some relatively unproblematic way (Scheutz 2001). These versions may\nbe called limited pancomputationalism. \n\nVarieties of pancomputationalism also vary with respect to\nwhy everything performs computations — the source of\npancomputationalism. \n\nOne alleged source of pancomputationalism is that which computation\na system performs is a matter of relatively free interpretation. If\nwhether a system performs a given computation depends solely or\nprimarily on how the system is perceived, as opposed to objective fact,\nthen it seems that everything computes because everything may be seen\nas computing (Searle 1992). This may be called interpretivist\npancomputationalism. \n\nAnother alleged source of pancomputationalism is that everything has\ncausal structure. According to the causal account, computation is the\ncausal structure of physical processes (Chrisley 1995, Chalmers 1995,\n1996, Scheutz 1999, 2001). Assuming that everything has causal\nstructure, it follows that everything performs the computation\nconstituted by its causal structure. This may be called causal\npancomputationalism. \n\nNot everyone will agree that everything has causal structure. Some\nprocesses may be non-causal, or causation may be just a façon de\nparler that does not capture anything fundamental about the world\n(e.g., Norton 2003). But those who have qualms about causation can\nrecover a view similar to causal pancomputationalism by reformulating\nthe causal account of computation and consequent version of\npancomputationalism in terms they like — e.g., in terms of the\ndynamical properties of physical systems. \n\nA third alleged source of pancomputationalism is that every physical\nstate carries information, in combination with an information-based\nsemantics plus a liberal version of the semantic view of computation.\nAccording to the semantic view of computation, computation is the\nmanipulation of representations. According to information-based\nsemantics, a representation is anything that carries information.\nAssuming that every physical state carries information, it follows that\nevery physical system performs the computations constituted by the\nmanipulation of its information-carrying states (cf. Shagrir 2006).\nBoth information-based semantics and the assumption that every physical\nstate carries information (in the relevant sense) remain\ncontroversial. \n\nYet another alleged source of pancomputationalism is that\ncomputation is the nature of the physical universe. According to some\nphysicists, the physical world is computational at its most fundamental\nlevel. This view, which is a special version of limited\npancomputationalism, will be discussed in Section 3.4. \n\nArguments for unlimited pancomputationalism go back to\nHinckfuss's pail, a putative counterexample to computational\nfunctionalism — the view that the mind is the software of the\nbrain. Hinckfuss's pail is named after its proponent, Ian\nHinckfuss, but was first discussed in print by William Lycan. A pail of\nwater contains a huge number of microscopic processes: \n\nHinckfuss's implied answer to this question is that yes, a\npail of water might implement a human program, and therefore any\narbitrary computation, at least for a short time. \n\nOther authors developed more detailed arguments along the lines of\nHinckfuss's pail. John Searle (1992) explicitly argues that\nwhether a physical system implements a computation depends on how an\nobserver interprets the system; therefore, for any sufficiently complex\nobject and for any computation, the object can be described as\nimplementing the computation. The first rigorous argument for unlimited\npancomputationalism is due to Hilary Putnam (1988), who argues that\nevery ordinary open system implements every abstract finite automaton\n(without inputs and outputs). \n\nPutnam assumes that electromagnetic and gravitational fields are\ncontinuous and that physical systems are in different maximal states at\ndifferent times. He considers an arbitrarily chosen finite automaton\nwhose table calls for the sequence of states ABABABA. He then\nconsiders an arbitrary physical system S over the arbitrarily\nchosen time interval from 12:00 to 12:07 and argues that S\nimplements the sequence ABABABA. Since both the automaton and\nthe physical system are arbitrary, the argument generalizes to any\nautomaton and any physical state. Here is the core of Putnam's\nargument: Let the beginnings of the intervals during which S is to be\nin one of its stages A or B be\nt1, t2, …\ntn (in the example given, n = 7,\nand the times in question are t1 = 12:00,\nt2 = 12:01, t3 = 12:02,\nt4 = 12:03, t5 = 12:04,\nt6 = 12:05, t7 = 12:06). The\nend of the real-time interval during which we wish S to\n“obey” this table we call\ntn+1 (= t8 = 12:07,\nin our example). For each of the intervals ti to\nti+1, i = 1, 2, …,\nn, define a (nonmaximal) interval state\nsi which is the “region” in phase space\nconsisting of all the maximal states … with\nti ≤ t < t+1. (I.e.,\nS is in si just in case S is in\none of the maximal states in this “region.”) Note that the\nsystem S is in s1 from\nt1 to t2, in\ns2 from t2 to\nt3, …, in sn from\ntn\nto tn+1. (Left endpoint\nincluded in all cases but not the right — this is a convention\nto ensure the “machine” is in exactly one of the\nsi at a given time.) … \n\nDefine A = s1 ∨\ns3 ∨ s5 ∨\ns7; B = s2 ∨\ns4 ∨ s6. \n\nThen, as is easily checked, S is in state A from\nt1 to t2, from\nt3 to t4, and from\nt5 to t6, and from\nt7 to t8, and in state\nB at all other times between t1 and\nt8. So S “has” the table we\nspecified, with the states A,B we just defined as the\n“realizations” of the states A,B\ndescribed by the table. (Putnam 1988, 122–3, emphasis original) \n\nIn summary, Putnam picks an arbitrary physical system with\ncontinuous dynamics, slices up its dynamics into discrete time\nintervals, and then aggregates the slices so that they correspond to an\narbitrary sequence of computational states. He concludes that every\nphysical system implements every finite automaton. \n\nPutnam points out that his argument does not apply directly to\ncomputational theories of cognition, because cognitive systems receive\nspecific physical inputs through their sensory organs and yield\nspecific physical outputs through their motor organs. To determine\nwhich computations are implemented by a system with physical inputs and\noutputs, the inputs and outputs must be taken into account: \nImagine … that an object S which takes strings of\n“1”s as inputs and prints such strings as outputs behaves\nfrom 12:00 to 12:07 exactly as if it had a certain\n[computational] description D. That is, S receives a\ncertain string, say “111111,” at 12:00 and prints a certain\nstring, say “11,” at 12:07, and there “exists”\n(mathematically speaking) a machine with description D which\ndoes this (by being in the appropriate state at each of the specified\nintervals, say 12:00 to 12:01, 12:01 to 12:02, …, and printing\nor erasing what it is supposed to print or erase when it is in a given\nstate and scanning a given symbol). In this case, S too can be\ninterpreted as being in these same logical states\nA,B,C, … at the very same times and\nfollowing the very same transition rules; that is to say, we can find\nphysical states A,B,C …\nwhich S possesses at the appropriate times and which stand in\nthe appropriate causal relations to one another and to the inputs and\nthe outputs. The method of proof is exactly the same… Thus we\nobtain that the assumption that something is a\n“realization” of a given automaton description … is\nequivalent to the statement that it behaves as if it had that\ndescription (Putnam 1988, 124, emphasis original).\n \n\nIn summary, Putnam picks an arbitrary physical system with\nphysically specified inputs and outputs and then matches it to an\narbitrary finite automaton whose abstractly specified inputs and\noutputs map onto the physically specified inputs and outputs. He then\nslices up the physical system's internal dynamics as before, and\nthen aggregates the slices so that they correspond to the sequence of\ncomputational states of the finite automaton. It follows that given any\nphysical system and any finite automaton with isomorphic inputs and\noutputs, the physical system implements the computational system. \n\nAlthough this result is weaker than the result for systems without\ninputs and outputs, it is still striking because for any abstract\ninput-output pair <i, o>, there are infinitely\nmany automata that yield output o given input i.\nGiven Putnam's conclusion, any physical system with inputs and\noutputs isomorphic to i and o implements all of the\ninfinitely many automata with input i and output\no. \n\nIf unlimited pancomputationalism is correct, then the claim that a\nsystem S performs a certain computation becomes trivially\ntrue and vacuous or nearly so; it fails to distinguish S from\nanything else (or perhaps from anything else with the same inputs and\noutputs). Thus, unlimited pancomputationalism threatens the\ncomputational theory of cognition. If cognition is computation simply\nbecause cognitive systems, like everything else, may be seen as\nperforming more or less arbitrary computations over the relevant\ninputs, then it appears that the computational theory of cognition is\nboth trivial and vacuous. By the same token, unlimited\npancomputationalism threatens the foundations of computer science,\nwhere the objective computational power of different systems is\nparamount. The threat of trivialization is a major motivation behind\nresponses to the arguments for unlimited pancomputationalism. \n\nThe first thing to notice is that arguments for unlimited\npancomputationalism rely either implicitly or explicitly on the simple\nmapping account of computation. They assume that an arbitrary mapping\nfrom a computational description C to a physical description\nof a system is sufficient to conclude that the system implements\nC. In fact, avoiding unlimited pancomputationalism is a major\nmotivation for rejecting the simple mapping account of computation. By\nimposing restrictions on which mappings are legitimate, other accounts\nof computation aim to avoid unlimited pancomputationalism. \n\nIn one response to unlimited pancomputationalism, Jack Copeland\n(1996) argues that the mappings it relies on are illegitimate because\nthey are constructed ex post facto — after the computation is\nalready given. In the case of kosher computational\ndescriptions — the kind normally used in scientific\nmodeling — the work of generating successive descriptions of a\nsystem's physical dynamics is done by a computer running an\nappropriate program (e.g., a weather forecasting program), not by the\nmapping relation. In the sort of descriptions employed in arguments for\nunlimited pancomputationalism, instead, the descriptive work is done by\nthe mapping relation. \n\nAn arbitrarily chosen computational description, such as those\nemployed in arguments for unlimited pancomputationalism, does not\ngenerate successive descriptions of the state of an arbitrary system.\nIf someone wants a genuine computational description of a physical\nsystem, she must first identify physical states and state transitions\nof the system, then represent them by a computational description\n(thereby fixing the mapping relation between the computational\ndescription and the system), and finally use a computer to generate\nsubsequent representations of the state of the system, while the\nmapping relation stays fixed. By contrast, the arguments for unlimited\npancomputationalism pick a computation first, then slice and aggregate\nthe physical system to fit the computational description, and finally\ngenerate the mapping between the two. The work of describing the\nphysical system is not done by the computational description but by\nwhoever constructs the mapping. Copeland concludes that such ex post\nfacto mappings are illegitimate. \n\nIn addition, both Chalmers (1995, 1996) and Copeland (1996) argue\nthat the mappings invoked by unlimited pancomputationalism violate the\ncounterfactual relations between the computational states. Consider\nagain Putnam's slice-and-aggregate strategy for generating\nmappings. The mappings are constructed based on an arbitrary dynamical\nevolution of an arbitrary physical system. No attempt is made to\nestablish what would happen to the physical system had conditions been\ndifferent. Chalmers and Copeland argue that this is illegitimate, as a\ngenuine implementation must exhibit the same counterfactual relations\nthat obtain between the computational states. This response leads to\nthe counterfactual account of computation, according to which the\ncounterfactual relations between the physical states must be isomorphic\nto the counterfactual relations between the computational states. \n\nAnother possible response to unlimited pancomputationalism is that\nits mappings fail to construct an isomorphism between the causal\nstructure of the physical system and the state transitions specified by\nthe computational description. Consider Putnam's argument again.\nThe mapping from the computational description to the physical\ndescription is chosen with no regard to the causal relations that\nobtain between the physical states of the system. Thus, after a\ncomputational description is mapped onto a physical description in that\nway, the computational description does not describe the causal\nstructure of the physical system. According to several authors,\nnon-causal mappings are illegitimate (Chrisley 1995, Chalmers 1995,\n1996, 2011, Scheutz 1999, 2001). Naturally, these authors defend the causal\naccount of computation, according to which acceptable mappings must\nrespect the causal structure of a system. \n\nYet another response to unlimited pancomputationalism is implicitly\ngiven by Godfrey-Smith (2009). Although Godfrey-Smith is primarily\nconcerned with functionalism as opposed to computation per se, his\nargument is still relevant here. Godfrey-Smith argues that for a\nmapping to constitute a genuine implementation, the microscopic\nphysical states that are clustered together (to correspond to a given\ncomputational state) must be physically similar to one\nanother — there cannot be arbitrary groupings of arbitrarily\ndifferent physical states, as in the arguments for unlimited\npancomputationalism. Godfrey-Smith suggests that his similarity\nrestriction on legitimate mappings may be complemented by the kind of\ncausal and localizational restrictions proposed by Chalmers (1996). \n\nThe remaining accounts of computation — the semantic, syntactic,\nand mechanistic accounts — are even more restrictive than the\ncausal and counterfactual accounts; they impose further constraints on\nacceptable mappings. Therefore, like the causal and counterfactual\naccounts, they have resources for avoiding unlimited\npancomputationalism. \n\nSuch resources are not always straightforward to deploy. For\nexample, consider the semantic account, according to which computation\nrequires representation. If being a representation of something is an\nobjective property possessed by relatively few things, then unlimited\npancomputationalism is ruled out on the grounds that only the few items\nthat constitute representations are genuine computational states. If,\nhowever, everything is representational in the relevant way, then\neverything is computational (cf. Churchland and Sejnowski 1992, Shagrir\n2006). If, in addition, whether something represents something else is\njust a matter of free interpretation, then the semantic account of\ncomputation gives rise to unlimited pancomputationalism all over again.\nSimilar considerations apply to the syntactic and mechanistic accounts.\nFor such accounts to truly avoid unlimited pancomputationalism, they\nmust not rely on free interpretation. \n\nLimited pancomputationalism is much weaker than its unlimited\ncousin. It holds that every physical system performs one (or relatively\nfew) computations. Which computations are performed by which system is\ndeemed to be a matter of fact, depending on objective properties of the\nsystem. In fact, several authors who have mounted detailed responses to\nunlimited pancomputationalism explicitly endorse limited\npancomputationalism (Chalmers 1996, 331, Scheutz 1999, 191). \n\nUnlike unlimited pancomputationalism, limited pancomputationalism\ndoes not turn the claim that something is computational into a vacuous\nclaim. Different systems generally have different objective properties;\nthus, according to limited pancomputationalism, different systems\ngenerally perform different computations. Nevertheless, it may seem\nthat limited pancomputationalism still trivializes the claim that a\nsystem is computational. For according to limited pancomputationalism,\ndigital computers perform computations in the same sense in which\nrocks, hurricanes, and planetary systems do. This may seem to do an\ninjustice to computer science — in computer science, only\nrelatively few systems count as performing computations and it takes a\nlot of difficult technical work to design and build systems that\nperform computations reliably. Or consider the claim that cognition is\ncomputation. This computational theory of cognition was introduced to\nshed new and explanatory light on cognition. But if every physical\nprocess is a computation, the computational theory of cognition seems\nto lose much of its explanatory force. \n\nAnother objection to limited pancomputationalism begins with the\nobservation that any moderately complex system satisfies indefinitely\nmany objective computational descriptions (Piccinini 2010). This may be\nseen by considering computational modeling. A computational model of a\nsystem may be pitched at different levels of granularity. For example,\nconsider cellular automata models of the dynamics of a galaxy or a\nbrain. The dynamics of a galaxy or a brain may be described using an\nindefinite number of cellular automata — using different state\ntransition rules, different time steps, or cells that represent spatial\nregions of different sizes. Furthermore, an indefinite number of\nformalisms different from cellular automata, such as Turing machines,\ncan be used to compute the same functions computed by cellular\nautomata. It appears that limited pancomputationalists are committed to\nthe galaxy or the brain performing all these computations at once. But\nthat does not appear to be the sense in which computers (or brains)\nperform computations. \n\nIn the face of these objections, limited pancomputationalists are\nlikely to maintain that the explanatory force of computational\nexplanations does not come from the claim that a system is\ncomputational simpliciter. Rather, explanatory force comes from the\nspecific computations that a system is said to perform. Thus, a rock\nand a digital computer perform computations in the same sense. But they\nperform radically different computations, and it is the difference\nbetween their computations that explains the difference between them.\nAs to the objection that there are still too many computations\nperformed by each system, limited pancomputationalists have two main\noptions: either to bite the bullet and accept that every system\nimplements indefinitely many computations, or to find a way to single\nout, among the many computational descriptions satisfied by each\nsystem, the one that is ontologically privileged — the one that\ncaptures the computation performed by the system. One way to\ndo this is to postulate a fundamental physical level, whose most\naccurate computational description identifies the (most fundamental)\ncomputation performed by the system. This response is built into the\nview that the physical world is fundamentally computational (next\nsection). \n\nAs to those who remain unsatisfied with limited pancomputationalism,\ntheir desire to avoid limited pancomputationalism motivates the shift\nto more restrictive accounts of computation, analogously to how the\ndesire to avoid unlimited pancomputationalism motivates the shift from\nthe simple mapping account to more restrictive accounts of computation,\nsuch as the causal account. The semantic account may be able to\nrestrict genuine computational descriptions to fewer systems than the\ncausal account, provided that representations — which are needed\nfor computation according to the semantic account — are hard to\ncome by. Mutatis mutandis, the same is true of the syntactic and\nmechanistic accounts. \n\nSome authors argue that the physical universe is fundamentally\ncomputational. The universe itself is a computing system, and\neverything in it is a computing system too (or part thereof). Unlike\nthe previous versions of pancomputationalism, which originate in\nphilosophy, this ontic pancomputationalism originates in\nphysics. It includes both an empirical claim and a metaphysical one.\nAlthough the two claims are logically independent, supporters of ontic\npancomputationalism tend to make them both. \n\nThe empirical claim is that all fundamental physical magnitudes and\ntheir state transitions are such as to be exactly described by an\nappropriate computational formalism — without resorting to the\napproximations that are a staple of standard computational modeling.\nThis claim takes different forms depending on which computational\nformalism is taken to describe the universe exactly. The two main\noptions are cellular automata, which are a classical computational\nformalism, and quantum computing, which is non-classical. \n\nThe earliest and best known version of ontic pancomputationalism is\ndue to Konrad Zuse (1970, 1982) and Edward Fredkin, whose unpublished\nideas on the subject influenced a number of American physicists (e.g.,\nFeynman 1982, Toffoli 1982, Wolfram 2002; see also Wheeler 1982,\nFredkin 1990). According to some of these physicists, the universe is a\ngiant cellular automaton. A cellular automaton is a lattice of cells;\neach cell can take one out of finitely many states and updates its\nstate in discrete steps depending on the state of its neighboring\ncells. For the universe to be a cellular automaton, all fundamental\nphysical magnitudes must be discrete, i.e., they must take at most\nfinitely many values. In addition, time and space must be fundamentally\ndiscrete or must emerge from the discrete processing of the cellular\nautomaton. At a fundamental level, continuity is not a real feature of\nthe world — there are no truly real-valued physical quantities.\nThis flies in the face of most mainstream physics, but it is not an\nobviously false hypothesis. The hypothesis is that at a sufficiently\nsmall scale, which is currently beyond our observational and\nexperimental reach, (apparent) continuity gives way to discreteness.\nThus, all values of all fundamental variables, and all state\ntransitions, can be fully and exactly captured by the states and state\ntransitions of a cellular automaton. \n\nAlthough cellular automata have been shown to describe many aspects\nof fundamental physics, it is difficult to see how to simulate the\nquantum mechanical features of the universe using a classical formalism\nsuch as cellular automata (Feynman 1982). This concern motivated the\ndevelopment of quantum computing formalisms (Deutsch 1985, Nielsen and\nChuang 2000). Instead of relying on digits — most commonly, binary\ndigits or bits — quantum computation relies on qudits — most\ncommonly, binary qudits or qubits. The main difference between a digit\nand a qudit is that whereas a digit can take only one out of finitely\nmany states, such as 0 and 1 (in the case of a bit), a qudit can also\ntake an uncountable number of states that are a superposition of the\nbasis states in varying degrees, such as superpositions of 0 and 1 (in\nthe case of a qubit). Furthermore, unlike a collection of digits, a\ncollection of qudits can exhibit quantum entanglement. According to the\nquantum version of ontic pancomputationalism, the universe is not a\nclassical computer but a quantum computer, that is, not a computer that\nmanipulates digits but a computer that manipulates qubits (Lloyd\n2006) — or, more generally, qudits. \n\nThe quantum version of ontic pancomputationalism is less radical\nthan the classical version. The classical version eliminates continuity\nfrom the universe, primarily on the grounds that eliminating continuity\nallows classical computers to describe the universe exactly rather than\napproximately. Thus, the classical version appears to be motivated not\nby empirical evidence but by epistemological concerns. Although there\nis no direct evidence for classical ontic pancomputationalism, in\nprinciple it is a testable hypothesis (Fredkin 1990). By contrast,\nquantum ontic pancomputationalism may be seen as a reformulation of\nquantum mechanics in the language of quantum computation and quantum\ninformation theory (qubits), without changes in the empirical content\nof the theory (e.g., Fuchs 2004, Bub 2005). \n\nBut ontic pancomputationalists do not limit themselves to making\nempirical claims. They often make an additional metaphysical claim.\nThey claim that computation (or information, in the physical sense\ndescribed in Section 2.3) is what makes up the physical universe. This\npoint is sometimes made by saying that at the most fundamental physical\nlevel, there are brute differences between states — nothing more\nneed or can be said about the nature of the states. This view reverses\nthe traditional conception of the relation between computation and the\nphysical world. \n\nAccording to the traditional conception, which is presupposed by all\naccounts of computation discussed above, physical computation requires\na physical substratum that implements it. Computation is an aspect of\nthe organization and behavior of a physical system; there is no\nsoftware without hardware. Thus, according to the traditional\nconception, if the universe is a cellular automaton, the ultimate\nconstituents of the universe are the physical cells of the cellular\nautomaton. It is legitimate to ask what kind of physical entity such\ncells are and how they interact with one another so as to satisfy their\ncellular automata rules. \n\nBy contrast, according to the metaphysical claim of ontic\npancomputationalism, a physical system is just a system of\ncomputational states. Computation is ontologically prior to physical\nprocesses, as it were. “‘Hardware’ [is] made of\n‘software’” (Kantor 1982, 526, 534). According to\nthis non-traditional conception, if the universe is a cellular\nautomaton, the cells of the automaton are not concrete, physical\nstructures that causally interact with one another. Rather, they are\nsoftware — purely “computational” entities. \n\nSuch a metaphysical claim requires an account of what computation,\nor software, or physical information, is. If computations are not\nconfigurations of physical entities, the most obvious alternative is\nthat computations are abstract, mathematical entities, like numbers and\nsets. As Wheeler (1982, 570) puts it, “the building element [of\nthe universe] is the elementary ‘yes, no’ quantum\nphenomenon. It is an abstract entity. It is not localized in space and\ntime”. Under this account of computation, the ontological claim\nof ontic pancomputationalism is a version of Pythagoreanism. All is\ncomputation in the same sense in which more traditional versions of\nPythagoreanism maintain that all is number or that all is sets (Quine\n1976). \n\nOntic pancomputationalism may be attacked on both the empirical and\nthe ontological fronts. On the empirical front, there is little\npositive evidence to support ontic pancomputationalism. Supporters\nappear to be motivated by the desire for exact computational models of\nthe world rather than empirical evidence that the models are correct.\nEven someone who shares this desire may well question why we should\nexpect nature to fulfill it. On the metaphysical front, Pythagoreanism\nfaces the objection that the abstract entities it puts at the\nfundamental physical level lack the causal and qualitative properties\nthat we observe in the physical world — or at least, it is\ndifficult to understand how abstract entities could give rise to\nphysical qualities and their causal powers (e.g., Martin 1997). \n\nAccording to the Church-Turing thesis (CTT), any function that is\nintuitively computable is computable by some Turing machine (i.e.,\nTuring-computable). Alternatively, CTT may be formulated as follows:\nany function that is “naturally regarded as computable”\n(Turing 1936–7, 135) is Turing-computable. The phrases\n“intuitively computable” and “naturally regarded as\ncomputable” are somewhat ambiguous. When they are disambiguated,\nCTT takes different forms. \n\nIn one sense, “intuitively computable” means computable\nby following an algorithm or effective procedure. An effective\nprocedure is a finite list of clear instructions for generating new\nsymbolic structures out of old symbolic structures. When CTT is\ninterpreted in terms of effective procedures, it may be called\nMathematical CTT, because the relevant evidence is more\nlogical or mathematical than physical. Mathematical CTT says that any\nfunction computable by an effective procedure is\nTuring-computable. \n\nThere is compelling evidence that Mathematical CTT is true (Kleene\n1952, §62, §67; cf. also Sieg 2006): \n\nIn another sense, “intuitively computable” means\ncomputable by physical means. When CTT is so interpreted, it may be\ncalled Physical CTT (following Pitowsky 1990), because the\nrelevant evidence is more physical than logical or mathematical. \n\nPhysical CTT is often formulated in very strong forms. To a first\napproximation, Bold Physical CTT holds that any physical\nprocess — anything doable by a physical system — is\ncomputable by some Turing machine. \n\nBold Physical CTT can be made more precise in a number of ways. Here\nis a representative sample, followed by references to where they are\ndiscussed: \n\nThesis (A) is ambiguous between two notions of simulation. In one\nsense, simulation is the process by which a digital computing system\n(such as a Turing machine) computes the same function as another\ndigital computing system. This is the sense in which universal Turing\nmachines can simulate any other Turing machine. If (A) is interpreted\nusing this first notion of simulation, it entails that everything in\nthe universe is a digital computing system. This is (a variant of)\nontic pancomputationalism (Section 3.4). \n\nIn another sense, simulation is the process by which the output of a\ndigital computing system represents an approximate description of the\ndynamical evolution of another system. This is the sense in which\ncomputational models of the weather simulate the weather. If (A) is\ninterpreted using this second notion of simulation, then (A) is true\nonly if we do not care how close our computational approximations are.\nIf we want close computational approximations — as we usually\ndo — then (A) turns into the claim that any physical process can be\ncomputationally approximated to the degree of accuracy that is desired\nin any given case. Whether that is true varies from case to case\ndepending on the dynamical properties of the system, how much is known\nabout them, what idealizations and simplifications are adopted in the\nmodel, what numerical methods are used in the computation, and how many\ncomputational resources (such as time, processing speed, and memory)\nare available (Piccinini 2015, Chap 4). \n\nThesis (B) is straightforwardly and radically false. Blum et\nal.  (1989) set up a mathematical theory of computation over\nreal-valued quantities, which they see as a fruitful extension of\nordinary computability theory. Within such a theory, Blum et\nal. define idealized “computing” machines that\nperform addition, subtraction, multiplication, division, and equality\ntesting as primitive operations on arbitrary real-valued\nquantities. They easily prove that such machines can compute all sets\ndefined over denumerable domains by encoding their characteristic\nfunction as a real-valued constant (ibid., 405). Although they do not\ndiscuss this result as a refutation of Physical CTT, their work is\noften cited in discussions of physical computability and Physical\nCTT. \n\nTheses (C) and (D) have interesting counterexamples that are\nconsistent with some physical theories (cf. below and Pour-El 1999).\nThese theoretical counterexamples may or may not occur in our concrete\nphysical universe. \n\nEach of (A)–(D) raises important questions pertaining to the\nfoundations of computer science, physics, and mathematics. It is not\nclear, however, that any of these theses bears an interesting analogy\nto Mathematical CTT. Below are two reasons why. \n\nFirst, (A)–(D) are falsified by processes that cannot be built\nand used as computing devices. The most obvious example is\n(B). Blum et al.'s result is equivalent to demonstrating that\nall functions over denumerable domains — including the\nuncountably many functions that are not Turing-computable — are\ncomputable by Blum et al.'s “computing” systems,\nwhich are allowed to manipulate the exact values of arbitrary real\nnumbers. Hence, (B) is radically false. But at the same time, this\nresult has no direct practical applications, as there is no evidence\nthat concrete physical systems can manipulate arbitrary real-valued\nquantities in the way that Blum et al.'s systems do. \n\nMore generally, formulations (A)–(D) would be falsified by a\nsequence generated by a random (i.e., nondeterministic) physical\nprocess. According to quantum mechanics, some physical\nprocesses — such as atom decay — contain an objectively random\nelement. Hidden variable interpretations dispute this, but the\npossibility of genuine randomness is sufficiently plausible that it\nshould be taken into account. \n\nConsider a random process that produces discrete outputs over an\ninfinite period of time, e.g., the decay of atoms from a radioactive\nsample. Its output at regular time intervals is a string of\ndigits — ‘0’ if no atoms decay during a time interval,\n‘1’ if one or more atoms decay during a time interval. A\nsimple consideration shows that, with probability one, the sequence\nproduced by our random process is not Turing-computable. There are\nuncountably many infinite strings of digits (even more strongly, there\nare uncountably many infinite strings of digits with any given limiting\nfrequency of ‘0's and ‘1's). But there are only\ncountably many Turing-computable infinite strings. Therefore, assuming\nthat each infinite string (or each infinite string with a certain\nlimiting frequency) has the same probability of occurring as a result\nof a random process, the probability that a random process would\ngenerate a Turing-computable string of digits is zero, whereas the\nprobability that the string of digits is not Turing-computable is one\n(for the stronger conclusion that no such string of digits is\nTuring-computable, see Calude & Svozil 2008). Thus, simply by using\na random process to generate a string, there is a sense in which we\nwould have physical means that go beyond what is Turing-computable. As\nAlan Turing (1950, 438–439) pointed out, a machine with a “random\nelement” can do “more” than a Turing machine. But\ndoing more is not the same as computing more.\nContrary to what is sometimes suggested (e.g., Calude 2005, 10), the\ncombination of a Turing machine and a random process does not threaten\nPhysical CTT. \n\nRandom processes should not count as computations: unlike\ncomputations properly so called, random processes cannot be used to\ngenerate the desired values of a function or solve the desired\ninstances of a general problem. Random processes can be\nexploited by a computation, of course — there are\nimportant computational techniques that rely on random or pseudo-random\nchoices at some stages of a computation. If some quantum random\nsequences were random in the sense of algorithmic information theory,\nthey may even raise the probability of obtaining correct solutions from\ncomputational techniques that rely on random choices (Calude 2005, 10).\nBut no computational technique can amount to a mere sequence of random\nchoices. So any thesis, such as Bold Physical CTT, that would be\nfalsified by a sequence generated by a random process is too strong to\ncapture the notion of physical computability — the\nphysical analogue of algorithmic computability. Thus, contrary to what\nsome authors seem to assume, Bold Physical CTT is too strong to be a\nphysical analogue of Mathematical CTT. \n\nAnother feature that theses (A)–(D) have in common is that they\nappeal quite freely to arbitrary real-valued quantities. This is\nexplicit in (B). To see why they all appeal to arbitrary real-valued\nquantities, consider that most physical theories assume that many\nphysical magnitudes, including time, can take arbitrary real numbers as\nvalues. Hence, systems of physical equations, whose simulations,\nsolutions, or observables are appealed to, respectively, by (A), (C),\nand (D), involve arbitrary real numbers. Therefore, all formulations of\nBold Physical CTT involve, explicitly or implicitly, arbitrary real\nnumbers. \n\nThe expressive power of real numbers may be used to generate a\nsimple recipe to obtain the values of a Turing-uncomputable function.\nConsider that the digital expansion of a real number contains countably\nmany digits. Hence, for any characteristic function (i.e., a function\nwhose values are ‘0’ or ‘1’) defined over a\ncountable domain, including all Turing-uncomputable such functions,\nthere is a real number whose binary expansion encodes its values. This\nis because for all values of a characteristic function, the\nnth value of the function may be defined to be the\nnth digit of the binary expansion of a real\nnumber. \n\nSuppose you wish to know the value of a specific Turing-uncomputable\ncharacteristic function, such as the halting function for Turing\nmachines, for its nth argument. Take the real\nnumber r whose digital expansion encodes the values of the\nhalting function. All you need to do is obtain the value of the\nnth digit in the binary expansion of r and\nyou have the result you desire. If you can do this, you may obtain any\nvalue of any characteristic function defined over strings, including\nall Turing-uncomputable such functions. \n\nThe above recipe, if feasible, is a trivial consequence of the\nexpressive power of real numbers. Yet it is discussed in the literature\nas an example of perhaps possible physical computation beyond the power\nof Turing machines (e.g., Copeland 2000) — something that would\nfalsify Physical CTT. But there is no reason to believe such a recipe\ncan be implemented, because it requires either the measurement or the\npreparation of a Turing-uncomputable real-valued quantity with\nunbounded precision. There is no evidence that either is doable. \n\nBy relying quite freely on arbitrary real-valued quantities, many\nversions of Bold Physical CTT make themselves vulnerable to\nfalsification by relatively trivial (though presumably unfeasible)\nrecipes such as the one just mentioned. This casts doubt on the\nassumption — made by some who seek ways to falsify Bold Physical\nCTT — that Bold Physical CTT is actually an interesting physical\nanalogue of Mathematical CTT. \n\n(The point just made does not impugn analog computation in the\nstandard sense of Pour-El (1974). Analog computation does not\nmanipulate exact values of arbitrary real-valued quantities but rather\ncontinuous variables. Although a continuous variable may be\nassumed to take any real value within a relevant interval, a successful\nconcrete analog computation requires only the measurement of real\nvariables with some degree of approximation. No exact values\nof arbitrary real-valued quantities are exploited by an analog\ncomputation, so analog computation does not falsify Bold Physical CTT\n(cf. Rubel 1989).) \n\nSimilar doubts about the alleged analogy between Mathematical CTT\nand Bold Physical CTT may be generated by a related observation. Many\ncurrent physical theories assume that nature contains real-valued\nquantities that vary along a continuum. These may include the velocity\nof objects, the coordinates that define the position of their center of\ngravity in the spacetime manifold, and more. If these physical theories\nare correct, then many properties of many entities take arbitrary real\nnumbers as their values. And most real numbers in any continuous\ninterval are Turing-uncomputable. In fact, there are only countably\nmany computable numbers, but any continuous interval contains\nuncountably many real numbers. Thus, the probability that a randomly\npicked real-valued quantity is computable is zero. Hence, if our\nphysical theories are correct, most transformations of the relevant\nphysical properties are transformations of Turing-uncomputable\nquantities into one another. \n\nFor instance, an object's change of speed, or even its simple\nchange of spatial location may be transformations of one\nTuring-uncomputable real-valued quantity into another. A transformation\nof one Turing-uncomputable value into another Turing-uncomputable value\nis certainly a Turing-uncomputable operation. Hence, it would seem that\ngiven many of our physical theories, the physical world is chock-full\nof operations that outstrip the power of Turing machines. If this is\ncorrect, it falsifies Bold Physical CTT. \n\nBut, as before, there is no reason to believe that we can use the\nTuring-uncomputable operations just mentioned to compute in\nthe epistemological sense that motivates CTT in the first\nplace — to solve problems, to generate the values of desired\nfunctions for desired arguments. In other words, it is implausible that\nthe operations in question should count as computations. Bold Physical\nCTT, which is threatened by such operations, is not interestingly\nanalogous to Mathematical CTT. \n\nTo conclude our discussion of Bold Physical CTT, it may be helpful\nto distinguish the issue of physical computability proper — the\nissue that pertains to the physical analogue of Mathematical\nCTT — from other issues that connect computability and physics.\nMany questions about the relationship between physical processes and\ncomputability deserve to be asked. What are the physically computable\nfunctions? This is the question that should motivate Physical CTT. What\ncan be computationally approximated to what degree under what\ncircumstances? This is presumably what (A) is after. As important and\ninteresting as this question is, it is different from the question of\nwhat can be physically computed. Yet other questions motivate theses\n(B)-(D) above as well as other versions of Bold Physical CTT to be\nfound in the literature. Many of these questions are interesting and\ndeserve to be investigated. Nevertheless, they do not properly belong\nin discussions of CTT, because they are different from the\ncomputability question that motivates CTT in the first place. \n\nIn the literature on computation in physical systems, there is\ngrowing concern that a physical analogue of Mathematical CTT should\ninclude only usable physical processes (e.g., Németi\nand Dávid 2006; Ord 2006, Smith 2006a, Beggs and Tucker 2007).\nGiven this concern, an adequate version of Physical CTT ought to be\nmore modest than Bold Physical CTT. \n\nModest Physical CTT ought to be formulated in terms of what can be\nphysically computed. Prototypical examples of physical\ncomputation are the processes of ordinary digital computers and their\ncomponents, including digital circuits. Such processes can be\nexhaustively described by effective procedures, which are already\ncovered by Mathematical CTT. Mathematical CTT says that any function\ncomputable by an effective procedure is computable by a Turing machine.\nAs Turing machines can be physically implemented (or replaced by a\ncomputing human), any process that follows an effective procedure is\nphysically computable. \n\nBut physical computation in the present sense is a broader notion\nthan computation by effective procedure. A process may count as a\nphysical computation even if there is no effective procedure for\ndescribing the process, perhaps because there are no finite\ninstantaneous descriptions of the internal states that constitute the\nprocess or no way to finitely and exactly specify the transition from\none instantaneous description to the next. Thus, Modest Physical CTT is\nstronger than Mathematical CTT. In addition to physical processes that\nfollow effective procedures, Modest Physical CTT may cover continuous\ndynamical processes (as in certain kinds of connectionist computing),\nprocesses that span large portions of spacetime, and quantum processes\n(as in quantum computing). But physical computation does not include\nall physical processes. \n\nIn accordance with this proposal, Modest Physical CTT holds\nthat any function that is physically computable is\nTuring-computable. \n\nFor a process to count as a physical computation, and hence for it\nto be relevant to Modest Physical CTT, it must be usable by an observer\nto generate the desired values of an independently specified function.\nThis requirement may be spelled out in terms of a number of constraints\n(Piccinini 2015, Chaps 15 and 16). This list is not intended to be\nexhaustive: \nConstraint 1: Readable inputs and outputs. The process must take\ninputs and yield outputs that observers can read without error, so that\nobservers can use the outputs as solutions to problems or values of\nfunctions defined over the inputs. For that to be possible, presumably\nthe inputs and outputs need to be strings of discrete states, like the\ninputs and outputs of ordinary digital computers. \n\nConstraint 2: Process-independent rule. There must be a fixed rule\nor map — specifiable independently of the physical\nprocess — that links the outputs to the inputs. By defining the\nproblem to be solved by the process, this rule tells the user what she\nis going to learn by running the process. Since the rule defines a\nphysical computation in general, the rule need not be recursive. For\ninstance, it may be the rule that defines the halting problem for\nTuring machines. But like all recursive rules, the rule must be the\nsame for all inputs that belong in the same problem; it cannot change\nfrom one input to the next. \n\nConstraint 3: Repeatability. The process must be repeatable, so as\nto allow users to obtain the same results multiple times and to check a\ncomputation by repeating it. \n\nConstraint 4: Settability. The system undergoing the process must be\nsettable, so that a user can choose which argument of a function the\nsystem computes and set the system to compute the relevant value of\nthe function. \n\nConstraint 5: Physical constructibility. The system must be\nphysically constructible. \n\nConstraint 6: Reliability. The system must not break down before the\nprocess is completed. \n\nIn summary, Modest Physical CTT asserts that every function that can\nbe physically computed, i.e., every usable transformation of input\nstrings into output strings in accordance with a process-independent\nrule defined over the strings, is Turing-computable. \n\nSince Modest Physical CTT is restricted by epistemologically\nrelevant criteria, it doesn't raise the worries associated with\nBold Physical CTT — namely, that it's too easy to falsify and\nirrelevant to the notion of computability that motivates CTT. And there\nare good reasons to believe Modest Physical CTT. All computing\nmechanisms that have been physically built or are in the process of\nbeing built compute only functions that are Turing-computable. \n\nIt is important to understand the exact scope of Modest Physical\nCTT. Modest Physical CTT does not entail that every physical process is\na computation, or that every physical system is a computing system.\nIt only says that if something physical is a computing system, then\nthe functions it computes are Turing-computable. \n\nTo fully assess Modest Physical CTT, we should consider whether it\nis possible to build a machine that, like an ordinary digital computer,\ncan be used by human observers, but, unlike an ordinary digital\ncomputer, generates the values of functions that are\nTuring-uncomputable. In recent years, several designs for\nhypercomputation have been proposed. Hypercomputation is the\ncomputation of Turing-uncomputable functions. If hypercomputation turns\nout to be physically possible, it will refute Modest Physical CTT. \n\nTo a first approximation, a hypercomputer is a system that yields\nthe values of a function that is not Turing-computable. If what counts\nas yielding the values of a function is left unspecified, any of the\nsystems discussed in Section 4.1, such as genuinely random processes\nand systems that manipulate arbitrary real-valued quantities, would\ncount as hypercomputers. But in discussing Bold Physical CTT, we saw\nthat yielding the values of a function that is Turing-uncomputable,\nwithout further constraints, is not enough for genuine physical\ncomputation. \n\nBy analogy with the distinction between Bold Physical CTT and Modest\nPhysical CTT, let us distinguish between a weak and a strong notion of\nhypercomputation by distinguishing usable hypercomputers from unusable\nhypercomputational processes. \n\nAn unusable hypercomputational process is a physical\nprocess that fails to satisfy at least one of the first four\nconstraints on physical computation. Examples include processes whose\ninputs or outputs are arbitrary real-valued quantities (which are not\nreadable without error) and genuine random processes (which have no\nrule characterizing the inputs and outputs independently of the\nprocess, and are neither repeatable nor settable). These processes\nare not usable because an observer cannot obtain from them arbitrary\nvalues of an independently defined function on a chosen input, as\nordinary computing systems can (given enough time and space). Since\nthey are not usable, unusable hypercomputational processes are\nirrelevant to Modest Physical CTT. \n\nA usable hypercomputer is a physical system that satisfies\nat least the first four constraints on physical computation. It has\nreadable inputs and outputs, there is a rule characterizing its\ninput-output properties that may be defined independently of the\nprocess itself, and its processes are repeatable and settable. If a\nsystem does not satisfy one of these conditions, it does not count as\ncomputing in the sense relevant to Modest Physical CTT. \n\nA system that satisfies these conditions — a usable\nhypercomputer — may be purely notional. For instance, infinitely\naccelerating Turing machines (Copeland 2002) are Turing machines that\nperform each computational operation in half the time as their previous\noperation. As a consequence, infinitely accelerating Turing machines\ncomplete an infinite number of operations (a supertask) within\ntwice the time it takes them to perform their first operation. This\nallows infinitely accelerating Turing machines to compute functions,\nsuch as the halting function, that are Turing-uncomputable. But\ninfinitely accelerating Turing machines are usually discussed as\nnotional entities, without evidence that they can be constructed.\nPurely notional systems, of course, do not falsify Modest Physical CTT.\nTo do that, a system must satisfy at least the fifth and sixth\nconstraints on physical computation: it must be physically\nconstructible and it must operate reliably. \n\nOne way to construct something like an infinitely accelerating\nTuring machine would be to make a computing machine that, after\nperforming some computational operations, builds a smaller and faster\ncopy of itself (Davies 2001). The smaller and faster copy will also\nperform some operations and then build a faster and smaller copy, and\nso on. Given appropriate assumptions, the resulting series of\ninfinitely shrinking machines will be able to complete an infinite\nnumber of computational operations within a finite time, thereby\nsurpassing the power of Turing machines. While infinitely shrinking\nmachines appear to be consistent with Newtonian mechanics, Davies\n(2001, 672) points out that the atomic and quantum mechanical nature of\nmatter in our universe makes infinitely shrinking machines physically\nimpossible. \n\nNeural networks have sometimes been discussed as computing systems\nthat may go beyond Turing-computability (e.g., Smolensky 1988, 3). If\nwe restrict our attention to classes of neural networks that contain\nall systems with current or foreseeable practical applications, this\nopinion is unwarranted. There is now a considerable literature on the\ncomputational and complexity properties of large classes of neural\nnetworks. The most relevant systems have digital inputs and outputs (so\nas to satisfy our first constraint on physical computation) but may\nhave, and typically do have, non-digital internal processes (i.e.,\ntheir internal processes are not discrete step-by-step manipulations of\nstrings of digits). The main results are the following: (i) feedforward\nnetworks with finitely many processing units are computationally\nequivalent to Boolean circuits with finitely many gates; (ii) recurrent\nnetworks with finitely many units are equivalent to finite state\nautomata; (iii) networks with unbounded tapes or an unbounded number of\nunits are equivalent to Turing machines (Šíma &\nOrponen 2003). \n\nNeural networks more powerful than Turing machines may be defined,\nhowever, by exploiting once again the expressive power of real numbers.\nThe best known networks of this kind are Analog Recurrent Neural\nNetworks (ARNNs) (Siegelmann 1999). ARNNs should not be confused with\nanalog computers in the traditional sense (Pour-El 1974, Rubel 1993,\nMills 2008). Whereas analog computers manipulate real variables without\nrelying on the exact value of arbitrary real-valued quantities, ARNNs\nmanipulate strings of digits by (possibly) relying on the exact value\nof arbitrary real-valued quantities. Specifically, the weights\nconnecting individual processing units within ARNNs can take exact\nvalues of arbitrary real numbers, including values that are\nTuring-uncomputable. When their weights are Turing-uncomputable, ARNNs\ncan go beyond the power of Turing-machines: they compute any function\nover binary strings. ARNNs more powerful than Turing machines cannot be\nbuilt and operated reliably, however, for at least two reasons: first,\nthey require unboundedly precise weights, and second, such weights are\nnot Turing computable (Davis 2004, Schonbein 2005, Siegelmann 1999,\n148). \n\nPerhaps the best known proposal for a hypercomputer is due to Mark\nHogarth (1994, 2004), who developed an idea of Itamar Pitowsky (1990).\nRelativistic hypercomputers exploit the properties of a special kind of\nspacetime called Malament-Hogarth spacetime, which is physically\npossible in the sense of constituting a solution to Einstein's\nfield equations for General Relativity. Malament-Hogarth spacetimes\ncontain regions with an infinite time-like trajectory λ that can\nbe circumvented by a finite time-like trajectory γ. In other\nwords, λ and γ have a common origin, and there is a\nspacetime point p on γ such that λ, even though\nit is infinite, lies entirely in p's chronological past.\nIf an observer launches a Turing machine along λ and then\ntravels along γ she may, within finite time, find herself in the\nfuture of an infinitely long computation performed by the Turing\nmachine. If the Turing machine is able to send signals to the observer,\nthe observer would be able to know the outcome of a potentially\ninfinitely long computation, thereby having computational means more\npowerful than (ordinary) Turing machines. For instance, an observer may\nbe able to obtain the results of an arbitrary instance of the halting\nfunction for Turing machines. \n\nConstructing and using a relativistic hypercomputer is a nontrivial\naffair. The first question is whether our spacetime is\nMalament-Hogarth. The answer is currently unknown. Even if our\nspacetime is not Malament-Hogarth globally, it might still contain\nregions that have the Malament-Hogarth property locally. An example of\nsuch a region is a huge, rotating black hole; there is evidence that\nour universe contains such black holes (Etesi and Németi 2002).\nBut even if there are Malament-Hogarth regions in our universe, there\nremain considerable obstacles. For starters, (i) a machine that\nrequires an unbounded amount of matter running for an infinite amount\nof time will malfunction with probability 1, rendering it useless\n(Button, 2009, 779), and (ii) the huge, rotating black hole that is\nclosest to us is probably out of our reach as well as our\ndescendants' reach (see also Németi and Dávid 2006,\nAndréka, Németi, and Németi 2009). \n\nQuantum computing has also been invoked as a possible source of\nhypercomputation. Quantum computing is the manipulation of qubits (and\nmore generally, qudits) in accordance with the laws of quantum\nmechanics. Qubits are variables that, like bits, can be prepared or\nmeasured in one or two states, 0 and 1. Unlike bits, qubits can (i)\ntake states that are a superposition of 0 and 1 and (ii) become\nentangled with each other during a computation. A surprising feature of\nquantum computing is that it allows computing certain functions much\nfaster than any known classical computation (Shor 1994). But while\nmainstream quantum computing may be more efficient than classical\ncomputing, it does not allow computing any functions beyond those\ncomputable by Turing machines. \n\nSome authors have questioned whether the mainstream quantum\ncomputing paradigm is general enough and, if not, whether some aspects\nof quantum mechanics may be exploited to design a quantum hypercomputer\n(Nielsen 1997, Calude and Pavlov 2002). The most prominent proposal for\na quantum hypercomputer is due to Tien Kieu (2002, 2003, 2004, 2005).\nHe argues that an appropriately constructed quantum system can decide\nwhether an arbitrary Diophantine equation has an integral\nsolution — a problem which is known to be unsolvable by Turing\nmachines. Kieu's method involves encoding a specific instance of\nthe problem in an appropriate Hamiltonian, which represents the total\nenergy of a quantum system. Kieu shows that such a system can\ndynamically evolve over time into an energy ground state that encodes\nthe desired solution. Unfortunately, Kieu's scheme does not\nappear to be workable. For one thing, it requires infinite precision in\nsetting up and maintaining the system (Hodges 2005). For another thing,\nKieu does not provide a successful criterion for knowing when\nthe system has evolved into the solution state, and the problem of\ndetermining when the solution state is reached is unsolvable by Turing\nmachines (Smith 2006b, Hagar and Korolev 2007). Thus, operating\nKieu's proposed hypercomputer would require already possessing\nhypercomputational powers. \n\nIn conclusion, the candidate hypercomputers proposed so far have not\nbeen shown to be physically constructible and reliable. For the time\nbeing, Modest Physical CTT remains plausible. It may well be that for\nall practical purposes, any function that is physically computable is\nTuring-computable.","contact.mail":"piccininig@umsl.edu","contact.domain":"umsl.edu"}]
