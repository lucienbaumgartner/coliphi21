[{"date.published":"2018-07-17","url":"https://plato.stanford.edu/entries/prediction-accommodation/","author1":"Eric Christian Barnes","entry":"prediction-accommodation","body.text":"\n\n\nIn early philosophical literature, a ‘prediction’ was\nconsidered to be an empirical consequence of a theory that had not yet\nbeen verified at the time the theory was constructed—an\n‘accommodation’ was one that had.  The view that\npredictions are superior to accommodations in the assessment of\nscientific theories is known as ‘predictivism’. Commonly,\nhowever, predictivism is understood more precisely as entailing that\nevidence confirms theory more strongly when predicted than when\naccommodated.  Much ink has been spilled modifying the concept of\n‘prediction’ and explaining why predictivism is or is not\ntrue, and whether the history of science reveals that scientists are\npredictivist in their assessment of theories. The debate over\npredictivism also figures importantly in the debate about scientific\nrealism. \n\nThere was in the eighteenth and nineteenth centuries a passionate\ndebate about scientific method—at stake was the ‘method of\nhypothesis’ which postulated hypotheses about unobservable\nentities which ‘saved the phenomena’ and thus were\narguably true (see Laudan 1981a). Critics of this method pointed out\nthat hypotheses could always be adjusted artificially to accommodate\nany amount of data. But it was noted that some such theories had the\nfurther virtue of generating specific predictions of heretofore\nunobserved phenomena—thus scientists like John Herschel and\nWilliam Whewell argued that hypotheses that saved phenomena could be\njustified when they were confirmed by such ‘novel’\nphenomena. Whewell maintained that predictions carry special weight\nbecause a theory that correctly predicts a surprising result cannot\nhave done so by chance, and thus must be true (Whewell 1849 [1968:\n294]). It thus appeared that predicted evidence confirmed theory more\nstrongly than accommodated evidence. But John Stuart Mill (in his\ndebate with Whewell) categorically denied this claim, affirming that\n \n(s)uch predictions and their fulfilment are, indeed, well calculated\nto impress the ignorant vulgar, whose faith in science rests solely\nupon similar coincidences between its prophecies and what comes to\npass.  But it is strange that any considerable stress should be laid\nupon such a coincidence by scientific thinkers. (1843, Vol. 2, 23) \nJohn Maynard Keynes provides a simple account of why predictivism has\na misleading appearance of truth in a brief passage in his book A\nTreatise on Probability: \nThe peculiar virtue of prediction or predesignation is altogether\nimaginary… The plausibility of the argument [for predictivism]\nis derived from a different source. If a hypothesis is proposed a\npriori, this commonly means that there is some ground for it,\narising out of our previous knowledge, apart from the purely inductive\nground, and if such is the case the hypothesis is clearly stronger\nthan one which reposes on inductive grounds only. But if it is merely\na guess, the lucky fact of its preceding some or all of the cases\nwhich verify it adds nothing whatever to its value. It is the union of\nprior knowledge, with the inductive grounds which arise out of the\nimmediate instances, that lends weight to any hypothesis, and not the\noccasion on which the hypothesis is first proposed. (1921:\n 305–306)[1] \nBy ‘the inductive ground’ for a hypothesis Keynes clearly\nmeans the data that the hypothesis fits. Keynes means that when some\ntheorist who undertakes to test a hypothesis first proposes it,\ntypically some other (presumably theoretical) form of support prompted\nthe proposal. Thus hypotheses which are proposed without being built\nto fit the empirical data (which they are subsequently shown to\nentail) are typically better supported than hypotheses which are\nproposed merely to fit the data—for the latter lack the\nindependent support possessed by the former. The appearance of\nplausibility to predictivism arises because the role of the\npreliminary hypothesis-inducing evidence is being suppressed.  \nKarl Popper is probably the most famous proponent of prediction in the\nhistory of philosophy. In his lecture “Science: Conjectures and\nRefutations” Popper recounts his boyhood attempt to grapple with\nthe question “When should a theory be ranked as\nscientific?” (Popper 1963: 33–65). Popper had become\nconvinced that certain popular theories of his day, including\nMarx’s theory of history and Freudian psychoanalysis, were\npseudosciences. Popper deemed the problem of distinguishing scientific\nfrom pseudoscientific theories ‘the demarcation problem’.\nHis solution to the demarcation problem, as is well known, was to\nidentify the quality of falsifiability (or ‘testability’)\nas the mark of the scientific theory.  \nThe pseudosciences were marked, Popper claimed, by their vast\nexplanatory power. They could explain not only all the relevant actual\nphenomena the world presented, they could explain any conceivable\nphenomena that might fall within their domain. This was because the\nexplanations offered by the pseudosciences were sufficiently malleable\nthat they could always be adjusted ex post facto to explain anything.\nThus the pseudosciences never ran the risk of being inconsistent with\nthe data. By contrast, a genuinely scientific theory made specific\npredictions about what should be observed and thus ran the risk of\nfalsification. Popper emphasized that what established the scientific\ncharacter of relativity theory was that it ‘stuck its neck\nout’ in a way that pseudosciences never did.  \nLike Whewell and Herschel, Popper appeals to the predictions a theory\nmakes as a way of separating the illegitimate uses of the method of\nhypothesis from its legitimate uses. But while Whewell and Herschel\npointed to predictive success as a necessary condition for the\nacceptability of a theory that had been generated by the method of\nhypothesis, Popper focuses in his solution to the demarcation problem\nnot on the success of a prediction but on the fact that the theory\nmade the prediction at all. Of course, there was for Popper an\nimportant difference between scientific theories whose predictions\nwere confirmed and those whose prediction were falsified. Falsified\ntheories were to be rejected, whereas theories that survived testing\nwere to be ‘tentatively accepted’ until falsified. Popper\ndid not hold, with Whewell and Hershel, that successful predictions\ncould constitute legitimate proof of a theory—in fact Popper\nheld that it was impossible to show that a theory was even probable\nbased on the evidence, for he embraced Hume’s critique of\ninductive logic that made evidential support for the truth of theories\nimpossible. Thus, one should ascribe to Popper a commitment to\npredictivism only in the broad sense that he held predictions to be\nsuperior to accommodations—he did not hold that predictions\nconfirmed theory more strongly than accommodations. It would\nultimately prove impossible for Popper to reconcile his claim that a\ntheory which enjoyed predictive success ought to be ‘tentatively\naccepted’ with his anti-inductivism (see, e.g., Salmon 1981).\n \nImre Lakatos (1970, 1971) proposed an account of scientific method in\nthe form of his ‘methodology of scientific research\nprogrammes’ which was a development of Popper’s approach.\nA scientific research program was constituted by a ‘hard\ncore’ of propositions which were retained throughout the life of\nthat programme together with a ‘protective belt’ which was\nconstituted by auxiliary hypotheses that were adjusted so as to\nreconcile the hard core with the empirical data. The attempt on the\npart of the proponents of the research programme to reconcile the\nprogramme to empirical data produced a series of theories \\(T_1\\),\n\\(T_2\\),… \\(T_n\\) where, at least in some cases, \\(T_{i+1}\\)\nserves to explain some data that is anomalous for \\(T_i\\). Lakatos\nheld that a research programme was ‘theoretically\nprogressive’ insofar as each new theory predicts some novel\nhitherto unexpected fact. A research programme is ‘empirically\nprogressive’ to the extent that its novel empirical content was\ncorroborated, that is, if each new theory leads to the discovery of\n“some new fact” (Lakatos 1970: 118). Lakatos thus offered\na new solution to the demarcation problem: a research programme was\npseudoscientific to the extent that it was not theoretically\nprogressive. Theory evaluation is construed in terms of competing\nresearch programmes: a research programme defeats a rival programme by\nproving more empirically progressive over the long run. \nAccording to Merriam-Webster’s Collegiate\n Dictionary,[2]\n something is ‘ad hoc’ if it is ‘formed or used for\nspecific or immediate problems or needs’. An ad hoc hypothesis\nthen is one formed to address a specific problem—such as the\nproblem of immunizing a particular theory from falsification by\nanomalous data (and thereby accommodating that data). Consequently\nwhat makes a hypothesis ad hoc, in the ordinary English sense of the\nterm, has nothing to do with the content of the hypothesis but simply\nwith the motivation of the scientist who proposes it—and it is\nunclear why there would be anything suspicious about such a\nmotivation. Nonetheless, ad hoc hypotheses have long been suspect in\ndiscussions of scientific method, a suspicion that resonates with the\npredictivist’s skepticism about accommodation.  \nFor Popper, a conjecture is ad hoc “if it is\nintroduced…to explain a particular difficulty,\nbut…cannot be tested independently” (Popper 1974: 986).\nThus Popper’s conception of ad hocness added to the ordinary\nEnglish meaning a further requirement—in the case of an ad hoc\nhypothesis that was simply introduced to explain a single phenomenon,\nthe ad hoc hypothesis has no testable consequences other than that\nphenomenon. In the case of an ad hoc theory modification introduced to\nresolve an anomaly for a theory, the modified theory had no testable\nconsequences other than those of the original theory.  \nPopper offered two explications of why ad hoc hypotheses were suspect.\nOne was that if we offer T as an explanation of f, but\nthen cite f as the only reason we have to believe T,\nPopper claims that we have engaged in reasoning that is suspicious for\nreasons of circularity (Popper 1972: 192–3). This was arguably\nfallacious on Popper’s part—a circular proof would offer\none proposition, p, in support of a second proposition\nq, when q has already been offered in support of\np. But in the above example, while f is offered as\nevidence for T, T is offered as an explanation of (not\nas evidence for) f—and thus there is no circular\nreasoning (Bamford 1993: 338).  \nPopper’s other explanation of why ad hoc hypotheses were\nregarded with suspicion was that they ran counter to the aim of\nscience, which for Popper included the proposal of theories with\nincreasing empirical content, viz., increasing falsifiability. Ad hoc\nhypotheses, for Popper, suffer from a lack of independent testability\nand thus reduce (or at least fail to increase) the testability of the\ntheories they modify (cf. above). However, Popper’s claim that\nthe process of modifying a theory ad hoc tends to lead to insufficient\nfalsifiability and is ‘unscientific practice’ has been\nchallenged (e.g., Bamford 1993: 350).  \nSubsequent authors argued that a hypothesis proposed for the sake of\nimmunizing a theory from falsification could be\n‘suspicious’ for various reasons, and thus could be\n‘ad hoc’ in various ways. Zahar (1973) argued that a\nhypothesis was ad hoc1 if it had no novel consequences as\ncompared with its predecessor (i.e. was not independently testable),\nad hoc2 if none of its novel predictions have actually been\nverified (either because it has not yet been tested or has been\nfalsified), and ad hoc3 \nif it is obtained from its predecessor through a modification of the\nauxiliary hypotheses which does not accord with the spirit of the\nheuristic of the programme. (1973: 101)  \nBeyond Popper’s criterion of a lack of independent testability\nthen, a hypothesis introduced to accommodate some datum could be ad\nhoc because it was simply unconfirmed (ad hoc2) or because\nit failed to cohere with the basic commitments of the research\nprogramme in which it is proposed (ad hoc3). \nAnother approach proposes that a hypothesis H introduced into a\ntheory T in response to an experimental result E is ad\nhoc if it is generally unsupported and appears to be a superficial\nattempt to paper over deep problems with a theory that is actually in\nneed of substantive revision. Thus to level the charge of ad hocness\nagainst a hypothesis was actually to direct serious skepticism toward\nthe theory the hypothesis was meant to rescue. This concept of\nad hocness arguably makes sense of Einstein’s critique of the\nLorentz-Fitzgerald contraction hypothesis as ‘ad hoc’ as a\nsupplementary hypothesis to the aether theory, and Pauli’s\npostulation of the neutrino as an ad hoc rescue of classical quantum\nmechanics (Leplin 1975, 1982; for further discussion see Grünbaum\n1976). \nIt seems clearly true that the scientific community’s judgment\nabout whether a hypothesis is ad hoc can change. Given this\nrevisability, and the aesthetic dimension of theory evaluation (which\nleaves assessment to some degree ‘in the eye of the\nbeholder’) there may be no particular point to embracing a\ntheory of ad hocness, if by the term ‘ad hoc’ we mean\n‘illegitimately proposed’ (Hunt 2012). \nPopper wrote that  \nConfirmations should count only if they are the result of risky\npredictions; that is to say, if, unenlightened by the theory in\nquestion, we should have expected an event which was incompatible with\nthe theory in question, we should have expected an event which was\nincompatible with the theory—an event which would have refuted\nthe theory. (1963: 36)  \nPopper (and subsequently Lakatos) thereby endorsed a temporal\ncondition of novelty—a prediction counts as novel is if it is\nnot known to be true (or is expected to prove false) at the time the\ntheory is constructed. But it was fairly obvious that this made\nimportant questions of confirmation turn implausibly on the time at\nwhich certain facts were known. \nThus Zahar proposed that a fact is novel “if it did not belong\nto the problem-situation which governed the construction of the\nhypothesis” (1973: 103). This form of novelty has been deemed\n‘problem-novelty’ (Gardner 1982: 2). But in the same paper\nZahar purports to exemplify this concept of novelty by referring to\nthe case in which Einstein did not use the known behavior of\nMercury’s perihelion in constructing his theory of\n relativity.[3]\n Gardner notes that this latter conception of novelty, which he deemed\n‘use-novelty’, is distinct from problem-novelty (Gardner\n1982: 3). Evidence is use-novel for T if T was not built\nto fit that evidence (whether or not it was part of the relevant\n‘problem-situation’ the theory was intended to address).\nIn subsequent literature, the so-called heuristic conception of\nnovelty has been identified with use-novelty—it was further\narticulated in Worrall 1978 and\n 1985.[4] \nAnother approach argues that a novel consequence of a theory is one\nthat was not known to the theorist at the time she formulated the\ntheory—this seems like a version of the temporal conception, but\nthis point appeals implicitly to the heuristic conception: if a\ntheorist knew of a result prior to constructing a theory which\nexplains it, it may be difficult to determine whether that theorist\nsomehow tailored the theory to fit the fact (e.g., she may have done\nso unconsciously). A knowledge-based conception is thus the best that\nwe can do to handle this difficulty (Gardner\n 1982).[5] \nThe heuristic conception is, however, deeply\ncontroversial—because it makes the epistemic assessment of\ntheories curiously dependent on the mental life of their constructors,\nspecifically on the knowledge and intentions of the theorist to build\na theory that accommodated certain data rather than others.\nLeplin’s comment is typical:  \nThe theorist’s hopes, expectations, knowledge, intentions, or\nwhatever, do not seem to relate to the epistemic standing of his\ntheory in a way that can sustain a pivotal role for them….\n(1997: 54)  \n(For similar comments see Gardner 1982: 6; Thomason 1992: 195;\nSchlesinger 1987: 33; Achinstein 2001: 210–230; and Collins\n1994.)  \nAnother approach notes that scientists operate with competing theories\nand that the role of novel confirmations is to decide between them.\nThus, a consequence of a theory T is a ‘novel\nprediction’ if it is not a consequence of the best available\ntheory actually present in the field other than T (e.g., the\nprediction of the Mercury perihelion by Einstein’s relativity\ntheory constituted a novel prediction because it was not a\n(straightforward) consequence of Newtonian mechanics; Musgrave 1974:\n18). Operating in a Lakatosian framework, Frankel claims a consequence\nwas novel with respect to a theory and its research programme if it is\nnot similar to a fact which already has been used by members of the\nsame research program to support a theory designed to solve the same\nproblems as the theory in question (1979: 25). Also in a Lakatosian\nframework, Nunan claims that a consequence is novel if it has not\nalready been used to support, or cannot readily be explained in terms\nof, a theory entertained in some rival research program (1984:\n 279).[6] \nThere are clearly multiple forms of novelty and it is generally\nrecognized that a fact could be ‘novel’ in multiple\nsenses—as we will see, some carry more epistemic weight than\nothers (Murphy 1989). \nGlobal predictivism holds that predictions are always superior to\naccommodations, while local predictivism holds that this only holds in\ncertain cases. Strong predictivism asserts that prediction is\nintrinsically superior to accommodation, whereas weak predictivism\nholds that predictive success is epistemically relevant because it is\nsymptomatic of other features that have epistemic import. The\ndistinction between strong and weak predictivism cross classifies with\nthe distinctions between different types of novelty. For example, one\ncould maintain that temporal predictions are intrinsically superior to\ntemporal accommodations (strong temporal predictivism) or that\ntemporal predictions were symptomatic of some other good-making\nfeature of theories (weak temporal predictivism; Hitchcock and Sober\n2004: 3–5). These distinctions will be further illustrated\nbelow. \nA version of global strong heuristic predictivism is the null support\nthesis that holds that theories never receive confirmation from\nevidence they were built to fit—precisely because of how they\nwere built. This thesis has been attributed to Bacon and Descartes\n(Howson 1990: 225). Popper and Lakatos also subscribe to this thesis,\nthough it is important to remember that they do not recognize any form\nof confirmational support—even from successful predictions. But\nothers who maintained that successful predictions do confirm theories\nnonetheless endorsed the null support hypothesis. Giere provides the\nfollowing argument: \nIf the known facts were used in constructing the model and were thus\nbuilt into the resulting hypothesis…then the fit between these\nfacts and the hypothesis provides no evidence that the hypothesis is\ntrue [since] these facts had no chance of refuting the hypothesis.\n(1984: 161; Glymour 1980: 114 and Zahar 1983: 245 offer similar\narguments) \nThe idea is that the way the theory was built provided an illegitimate\nprotection against falsification by the facts—hence the facts\ncannot support the theory. Others however find this argument specious,\nnoting that since the content of the hypothesis is fixed, it makes no\nsense to think of any facts as having a ‘chance’ to\nfalsify the theory. The theory says what it says, and any particular\nfact refutes it or it doesn’t.  \nGiere has confused what is in effect a random variable (the\nexperimental setup or data source E together with its set of\ndistinct possible outcomes) with one of its values (the outcome\ne)…Moreover, it makes perfectly good sense to say that\nE might well have produced an outcome other than the one,\ne, it did as a matter of fact produce. (Howson 1990: 229; see\nalso Collins 1994: 220)  \nThus Giere’s argument collapses. \nHowson argued in a series of papers (1984, 1988, 1990) that the null\nsupport thesis is falsified using simple examples, such as the\nfollowing: \nAn urn contains an unknown number of black and white tickets, where\nthe proportion p of black tickets is also unknown. The data\nconsists simply in a report of the relative frequency \\(r/k\\) of black\ntickets in a large number k of draws with replacement from the\nurn. In the light of the data we propose the hypothesis that \\(p =\n(r/k)+\\epsilon\\) for some suitable \\(\\epsilon\\) depending on k.\nThis hypothesis is, according to standard statistical lore, very well\nsupported by the data from which it is clearly constructed. (1990:\n231)  \nIn this case there is, Howson notes, a background theory that supplies\na model of the experiment (it is a sequence of Bernoulli trials, viz.,\na sequence of trials with two outcomes in which the probability of\ngetting either outcome is the same on each trial; it leaves only a\nsingle parameter to be evaluated). As long as we have good reason to\nbelieve that this model applies, our inference to the high probability\nof the hypothesis is a matter of standard statistical methodology, and\nthe null support thesis is refuted. \nIt has been argued that one of the limitations of Bayesianism is that\nit is fatally committed to the (clearly false) null support thesis\n(Glymour 1980). The standard Bayesian condition by which evidence\ne supports h is given by the inequality \\(p(h\\mid e) \\gt\np(h)\\). But where e is known (and thus \\(p(e) = 1\\)), we have\n\\(p(h\\mid e) = p(h)\\). This came to be known as the ‘Bayesian\nproblem of old evidence’. Howson (1984) noted that this problem\ncould be overcome by selecting a probability function \\(p^*\\) based on\nthe assumption that e was not known—thus even if \\(p(h\\mid e)\n= p(h)\\), it could still hold that \\({p^*}(h\\mid e) \\gt {p^*}(h)\\). Thus\nfollowed an extensive literature on the old evidence problem which\nwill not be summarized here (see, e.g., Christiansen 1999; Eells &\nFitelson 2000; Barnes 1999, 2008: Ch. 7; and Hartmann & Fitelson\n2015). \nPatrick Maher (1988, 1990, 1993) presented a seminal thought\nexperiment and a Bayesian analysis of its predictivist implications.\n \nThe thought experiment contained two scenarios: in the first scenario,\na subject (the accommodator) is presented with E, a sequence of\n99 coin flips. E forms an apparently random sequence of heads\nand tails. The accommodator is then instructed to tell us the outcome\nof the first 100 flips—he responds by reciting E and then\nadding the prediction that the 100th toss will be\nheads—the conjunction of E and this last toss is\nT. In the other scenario, another subject (the predictor) is\nasked to predict the first 100 flip outcomes without witnessing any\noutcomes—the predictor endorses theory T. Thereafter the\ncoin is flipped 99 times, E is established, and the\npredictor’s first 99 predictions are confirmed. The question is\nin which of these two scenarios is T better confirmed. It is\nstrongly intuitive that T is better confirmed in the\npredictor’s scenario than in the accommodator’s scenario,\nsuggesting that predictivism holds true in this case. If we allow\n‘O’ to assert that evidence E was input into\nthe construction of T, predictivism asserts: \nMaher argues that the successful prediction of the initial 99 flips\nconstitutes persuasive evidence that the predictor ‘has a\nreliable method’ for making predictions of coin flip outcomes.\nT’s consistency with E in the case of the\naccommodator provides no particular evidence that the\naccommodator’s method of prediction is reliable—thus we\nhave no particular reason to endorse his prediction about the\n100th flip. Allowing R to assert that the method in\nquestion is reliable, and \\(M_T\\) that method M generated\nhypothesis T, this amounts to: \nMaher’s (1988) provides a rigorous proof of (2), which is shown\nto entail (1) on various assumptions.  \nMaher’s (1988) makes the simplifying assumption that any method\nof prediction used by a predictor is either completely reliable (this\nis the claim abbreviated by ‘R’) or is no better\nthan a random method (\\(\\neg R\\)). (Maher [1990] shows that this\nassumption can be surrendered and a continuum of degrees of\nreliability of scientific methods assumed; the predictivist result is\nstill generated.) In qualitative terms, where M generates\nT (and thus predicts E) without input of evidence\nE, we should infer that it is much more likely that the method\nthat generated E is reliable than that E just happened\nto turn out true though R was no better than a random method.\nIn other words, we judge that we are much more likely to stumble on a\nsubject using a reliable method M of coin flip prediction than\nwe are to stumble on a sequence of 99 true flip predictions that were\nmerely lucky guesses—because \n\nMaher has articulated a weak heuristic predictivism because he claims\nthat predictive success is symptomatic of the use of a reliable\ndiscovery\n method.[7] \nFor critical discussion of Maher’s theory of predictivism see\nHowson and Franklin 1991 (and Maher’s 1993 reply); Barnes\n1996a,b; Lange 2001; Harker 2006; and Worrall \n 2014.[8] \nIt was noted above that ad hoc hypotheses stand under suspicion for\nvarious reasons, one of which was that a hypothesis that was proposed\nto resolve a particular difficulty may not cohere well with the theory\nit purports to save or relevant background\n beliefs.[9]\n This could result from the fact that there is no obvious way to\nresolve the difficulty in a way that is wholly ‘natural’\nfrom the standpoint of the theory itself or operative criteria of\ntheory choice. For example, the phlogiston theory claimed that\nsubstances emitted phlogiston while burning. However, it was\nestablished that some substances actually gained weight while burning.\nTo accommodate the latter phenomenon it was proposed that phlogiston\nhad negative weight—but the latter hypothesis was clearly ad hoc\nin the sense of failing to cohere with the background belief that\nsubstances simply do not have negative weight, and with the knowledge\nthat many objects lost weight when burned (Partington & McKie\n1938a: 33–38). \nThus the ‘fudging explanation’ defends predictivism by\npointing out that the process of accommodation lends itself to the\nproposal of hypotheses that do not cohere naturally with operative\nconstraints on theory choice, while successful predictions are immune\nfrom this worry (Lipton 1990, 1991: Ch. 8). Of course, it is an\nimportant question whether scientists actually rely on the fact that\nevidence was predicted (or accommodated) in their assessment of\ntheories—if a theory was fudged to accommodate some datum,\ncouldn’t the scientist simply note that the fudged theory\nsuffers a defect of coherence and pay no attention to whether the data\nwas accommodated or predicted? Some argue, however that scientists are\nimperfect judges of such coherence—a scientist who accommodates\nsome datum may think his accommodation is fully coherent, while his\npeers may have a more accurate and objective view that it is not. The\nscientist’s ‘assessed support’ of his proposed\naccommodation may thus fail to coincide with its ‘objective\nsupport’, and the scientist might rely on the fact that his\nevidence was accommodated as evidence that it was fudged (or\nconversely, that his evidence was predicted as evidence that it was\nnot fudged; Lipton 1991: 150f). \nLange (2001) offers an alternate interpretation of the coin flip\nexample that claims that the process of accommodation (unlike\nprediction) tends to generate theories that are not strongly supported\nby confirming data. He imagines a ‘tweaked’ version of the\ncoin flip example in which the initial 99 outcomes form a strict\nalternating sequence ‘tails heads tails heads…’\n(instead of forming the ‘apparently random sequence’ of\noutcomes provided in the original case). Again we imagine a predictor\nwho correctly predicts 99 outcomes in advance and an accommodator who\nwitnesses them. Both the predictor and the accommodator predict that\nthe 100th outcome will be tails. Now there is little or no\ndifference in our assessed probability that the subject will correctly\npredicted the 100th outcome.  \nThis suggests that the intuitive difference between Maher’s\noriginal pair of examples does not reflect a difference between\nprediction and accommodation per se. (Lange 2001: 580) \nLange’s analysis appeals to what Goodman called an\n‘arbitrary conjunction’—the mark of which is that\n \nestablishment of one component endows the whole statement with no\ncredibility that is transmitted to other component statements. (1983:\n68–9)  \nAn example of an arbitrary conjunction is “The sun is made of\nhelium and August 3rd 2017 falls on a Thursday and 17 is a\nprime number”. In the original coin flip case, we judge that\nH is weakly supported in the accommodator’s scenario\nbecause we judge that the apparently random sequence of outcomes is\nprobably an arbitrary conjunction—thus the fact that the initial\n99 conjuncts are confirmed implies almost nothing about what the\n100th outcome will be. But the success of the predictor in\npredicting the initial 99 outcomes strongly implies that the sequence\nis not an arbitrary conjunction after all:  \n(w)e now believe it more likely that the agent was led to posit this\nparticular sequence by way of something we have not noticed that ties\nthe sequence together—that would keep it from being a\ncoincidence that the hypothesis is accurate to the 100th\ntoss…. (Lange 2001: 581)  \nHaving judged it not to be an arbitrary conjunction, we are now\nprepared to recognize the first 99 outcomes as strongly confirming the\nprediction in the 100th case. What accounts for the\ndifference between the two scenarios, in other words, is not primarily\nwhether E was predicted or accommodated, but whether we judge\nH to be an arbitrary conjunction, and thus whether E\nprovides support for the remaining portion of H.  \nThus in Lange’s tweaked case, the non-existence of the\npredictivist effect is due to the fact that it is clear from the\ninitial 99 flips that the sequence is not an arbitrary\nconjunction—thus E confirms H equally strongly in\nboth scenarios.  \nLange goes on to suggest that in actual science the practice of\nconstructing a hypothesis by way of accommodating known evidence has a\ntendency to generate arbitrary conjunctions. Thus Lorentz’s\ncontraction hypothesis, when appended to his electrodynamics to\naccommodate the failure to detect optically any motion with respect to\nthe aether, resulted in an arbitrary conjunction (since evidence that\nsupported the contraction hypothesis did not support the\nelectrodynamics, or vice versa)—essentially for this reason,\nLange argues, it was rejected by Einstein as ad hoc. When evidence is\npredicted by a theory, by contrast, this is typically because the\ntheory is not an arbitrary conjunction. The evidential significance of\nprediction and accommodation for Lange is that they tend to be\ncorrelated (negatively and positively) with the construction of\ntheories that are arbitrary conjunctions. Lange’s view might\nthus be classed as a weak heuristic predictivism, though Lange never\ntakes a stand on whether scientists actually rely on such correlations\nin assessing theories.  \nFor critical discussion of Lange’s theory see Worrall 2014:\n59–61 and Harker 2006: 317f. \nDeborah Mayo has argued (particularly in Mayo 1991, 1996, and 2014)\nthat the intuition that predictivism is true derives from a premium on\nsevere tests of hypotheses. A test of a hypothesis H is severe\nto the extent that H is unlikely to pass that test if H\nis false. Intuitively, if a novel consequence N is shown to\nfollow from H, and the probability of N on the\nassumption \\({\\sim}H\\) is very low (for the reason of its being\nnovel), then testing for N would seem to count as a severe test\nof H, and a positive outcome should strongly support H.\nHere novelty and severity appear to coincide—but Mayo observes\nthat there are cases in which they come apart. For example, it has\nseemed to many that if H is built to fit some body of evidence\nE then the fact that H fits E does not support\nH because this fit does not constitute H’s having\nsurvived a severe test (or a test at all). One of Mayo’s central\nobjectives is to expose the fallacies that this latter reasoning\ninvolves. \nGiere (1984: 161, 163) affirms that evidence H was built to fit\ncannot support H because, given how H was built, it was\ndestined to fit that evidence. Mayo summarizes his reasoning as\nfollows: \nBut Mayo notes that ‘no matter what’ can be interpreted in\ntwo ways: (a) no matter what the data are, and (b) no matter whether\nH is true or false. (1) is true when interpreted as (a), but in\norder to establish that accommodated evidence fails to support\nH (as Giere intends) (1) must be interpreted as (b). However,\n(1) is false when so interpreted. Mayo (1996: 271) illustrates this\nwith a simple example: let the evidence e be a list of SAT\nscores from students in a particular class. Use this evidence to\ncompute the average score x, and set h = the mean SAT\nscore for these students is x. Now of course h has been\nuse-constructed from e. It is true that whatever mean score was\ncomputed would fit the data no matter what the data are—but\nhardly true that h would have fit the evidence no matter\nwhether h was true or false. If h were false it would\nnot fit the data, because the data will inevitably fit only a true\nhypothesis. Thus h has passed a maximally severe test: it is\nvirtually impossible for h to fit the data if h is\nfalse—despite the fact that h is built to fit e.\n \nMayo gives an additional example of how a use-constructed hypothesis\ncan count as having survived a severe test that pertains to the famous\n1919 Eddington eclipse experiment of Einstein’s General Theory\nof Relativity. GTR predicted that starlight that passed by the sun\nwould be bent to a specific degree (specifically 1.75 arcseconds). There\nwere actually two expeditions carried out during the eclipse—one\nto Sobral in Northern Brazil and the other to the island of Principe\nin the Gulf of Guinea. Each expedition generated a result that\nsupported GTR, but there was a third result generated by the Sobral\nexpedition that appeared to refute GTR. This result was however\ndisqualified because it was determined that a mirror used to acquire\nthe images of the stars’ position had been damaged by the heat\nof the sun. While one might worry that such dismissing of anomalous\nevidence was the kind of ad hoc adjustment that Popper warned against,\nMayo notes that this is instead a perfectly legitimate case of using\nevidence to support a hypothesis (that the third result was\nunreliable) that amounted to that hypothesis having passed a severe\ntest. Mayo concludes that a general prohibition on use-constructed\nhypothesis “fails to distinguish between problematic and\nunproblematic use-constructions (or double countings)” (1996:\n285). However, Hudson (2003) argues that there is historical evidence\nthat suggests there was legitimate reason to question the hypothesis\nthat the third result was unreliable (he uses this point to support\nhis own contention that the fact that a hypothesis was use-constructed\nis prima facie evidence that the hypothesis is suspect). Mayo (2003)\nreplies that insofar as the third result was nonetheless suspect the\nphysicists involved were right to discard it.  \nMayo (1996: Ch. 9) defends a predictivist-like position attributed to\nNeyman-Pearson statistical methods—the prohibition on\nafter-trial constructions of hypotheses. To illustrate: Kish (1959)\ndescribes a study that investigated the statistical relationship\nbetween a large number of infant training experiences (nursing, toilet\ntraining, weaning, etc.) and subsequent personality and behavioral\ntraits (e.g., school adjustment, nail biting, etc.) The study found a\nnumber of high correlations between certain training experience and\nlater traits. The problem was that the study investigated so many\ntraining experiences that it was quite likely that some correlations\nwould appear in the data simply by chance—even if there would\nultimately prove to be no such correlation. An investigator who\nstudied many possible correlations thus could survey that data and\nsimply look for statistically significant differences and proclaim\nevidence for correlations despite such evidence being\nmisleading—thus engaging in the dubious practice of the\n‘after-trial construction of\n hypothesis’.[10]\n Mayo notes that such hypotheses should not count as having passed a\nsevere test, thus she endorses the Neyman-Pearson prohibition on such\nconstruction. Hitchcock and Sober (2004) note that Mayo’s\ndefinition of severity as applied in this case differs from the one\nshe employs in dealing with cases like her SAT example; Mayo (2008)\nreplies at length to their criticism and argues that while she does\nemploy two versions of the severity definition they nonetheless\nreflect a unified conception of severity. \nFor critical discussion of Mayo’s account see Iseda 1999 and\nWorrall 2006: 56–60, 2010: 145–153—see also\nMayo’s (1996: 265f, 2010) replies to Worrall.  \nJohn Worrall has been an important contributor to the predictivism\nliterature from the 1970s until the present time. He was, along with\nElie Zahar, one of the early proponents of the significance of\nheuristic novelty (e.g., Worrall 1978, 1985). In his more recent work\n(cf. his 1989, 2002, 2005, 2006, 2010, 2014; also Scerri & Worrall\n2001) Worrall has laid out a detailed theory of predictivism that,\nwhile sometimes presented in heuristic terms, is “at root a\nlogical theory of confirmation” (2005: 819)—it is thus a\nweak heuristic account that takes use-novelty of evidence to be\nsymptomatic of underlying logical features that establish strong\nconfirmation of theory.  \nWorrall’s mature account is based on a view of scientific\ntheories that he credits to Duhem—which claims that a scientific\ntheory is naturally thought of as consisting of a core claim together\nwith some set of more specific auxiliary claims. It is commonly the\ncase that the core theory will leave undetermined certain ‘free\nparameters’ and the auxiliary claims fix values for such\nparameters. To cite an example Worrall often uses, the wave theory of\nlight consists of the core theory that light is a periodic disturbance\ntransmitted though some sort of elastic medium. This core claim by\nitself leaves open various free parameters concerning the wavelengths\nof particular types of monochromatic light. Worrall proposes to\nunderstand the diminished status of evidential support associated with\naccommodation as follows: when evidence e is ‘used’\nin the construction of a theory, it is typically used to establish the\nvalue of a free parameter in some core theory T. The fixed\nversion will be a specific version \\(T'\\) of T.\ne serves to confirm \\(T'\\), then, only on the\ncondition that there is independent support for T—thus\naccommodation provides only ‘conditional confirmation’.\nImportantly, evidence e that is used in this way will by itself\ntypically provide no evidence for core theory T. Worrall (2002:\n201) offers as an illustration the support offered to the wave theory\nof light (W) by the two slit experiment using light from a\nsodium arc—the data will consist of various alternating light\nand dark ‘fringes’. The fringe data can be used to compute\nthe wavelength of sodium light—and thus used to generate a more\nspecific version of the wave theory of light \\(W'\\)—one which\nconjoins W with a claim about the wavelength of this particular\nsort of light. But the data offer merely conditional support to\n\\(W'\\)—that is the data support \\(W'\\) only on the condition\nthat there is independent evidence for W.  \nPredicted evidence for Worrall is thus evidence that is not used to\nfix free parameters. Worrall cites two forms that predictions can\ntake: one is when a particular evidential consequence falls\n‘immediately out of the core’, i.e., is a consequence of\nthe core, together with ‘natural auxiliaries’, and the\nother is when it is a consequence of a specific version of a theory\nwhose free parameters have been fixed using other data. To illustrate\nthe first: retrograde\n motion[11]\n was a natural consequence of the Copernican core (the claim that the\nearth and planets orbit the sun) because observation of the planets\nwas carried out on a moving observatory that periodically passed other\nplanets—however it could only be accommodated by Ptolemaic\nastronomy by proposing and adjusting auxiliary hypotheses that\nsupposed the planet to move on an epicycle (retrograde motion did not\nfollow naturally from the Ptolemaic core idea that the Sun, stars and\nplanets orbit the earth). Thus retrograde motion was predicted by the\nCopernican theory and thus offered unconditional support to that\ntheory, while it offered only conditional confirmation to the\nPtolemaic theory. The second form of prediction is one which follows\nfrom a specific version of a theory but was not used to fix a\nparameter—imagine \\(W'\\) in the preceding paragraph makes a new\nprediction p (say for another experiment, such as the one slit\nexperiment)—p offers unconditional confirmation of \\(W'\\)\n(and W; Worrall 2002: 203). \nHowever it is important to understand that Worrall’s repeated\nexpression of his position in terms of the heuristic conception of\nnovelty (particularly after his 1985) does not amount to an\nendorsement of strong heuristic predictivism. Worrall clarifies this\nin his 1989 article that focuses on the evidential significance of the\n‘white spot’ confirmation of Fresnel’s version of\nthe wave theory of light. The reason the white spot datum carried such\nimportant weight is not ultimately that it was not used by Fresnel in\nthe construction of the theory but because this datum followed\nnaturally from the core theory that light is a wave. The reason the\nfringe data that was used to compute the wavelength of sodium light\n(cf. above) did not carry such weight is that it is not a consequence\nof this core idea (nor has the wavelength of sodium light been fixed\nby some other data). Thus d is novel for T when\n“there is a heuristic path to [T] that does not\npresuppose [d’s] existence” (Scerri & Worrall 2001:\n418). As Worrall sometimes puts it, whether d carries\nunconditional confirmation for T does not depend on whether\nd was actually used in constructing T, but whether it\nwas ‘needed’ to construct T (e.g., 1989:\n149–151). Thus Worrall is actually a proponent of\n‘essential use-novelty’ (Alai 2014: 304). For Worrall,\nfacts about heuristic prediction and accommodation serve to track\nunderlying facts about the logical relationship between theory and\nevidence. Thus Worrall is ultimately a proponent of weak (not strong)\nheuristic predictivism. Worrall categorically rejects temporal\npredictivism, arguing that the fact that the white spot was a\ntemporally novel consequence in itself was of no epistemic importance.\n \nFor further discussion of Worrall’s theory of predictivism see\nMayo 2010: 155f; Schurz 2014; Votsis 2014; and Douglas & Magnus\n2013: 587–8.  \nScerri and Worrall 2001 contains a detailed rendering of the\nhistorical episode of the scientific community’s assessment of\nMendeleev’s theory of the periodic law—it is argued that\nthis story ultimately vindicates Worrall’s theory of\npredictivism.  \nFor discussion of Scerri and Worrall see Akeroyd 2003; Barnes 2005b\n(and replies from Worrall 2005 and Scerri 2005); Schindler 2008, 2014;\nand Brush 2007.  \nA common argument for predictivism is that we should avoid inferring\nthat a theory T is true on the basis of evidence E that\nit is built to fit because we can explain why T entails\nE by simply noting how T was built—but if T\nwas not built to fit E then only the truth of T can\nexplain the fact that T fits E. Various philosophers\nhave noted that this reasoning is fallacious. As noted above it makes\nno sense to offer an explanation (for example, in terms of how the\ntheory was built) for the fact that T entails\nE—for this latter fact is a logical fact for which no\ncausal explanation can be given. Insofar as there is an\nexplanandum in need of an explanans here it is\nrather the fact that the theorist managed to construct or\n‘choose’ a theory (which turned out to be T) that\ncorrectly entailed E (Collins 1994; Barnes 2002)—that\nexplanandum could be explained by noting that the theorist\nbuilt a theory (which turned out to be T) to fit E, or\nendorsed it because it fit E.  \nWhite (2003) offers a theory of predictivism that begins with this\nsame insight—the relevant explanandum is: \nThis explanandum could be explained in one of two ways: \nWhite explains that (RA) means “roughly that the mechanisms\nwhich led to her selection of a theory gave her a good chance of\narriving at the truth” (2003: 664). (Thus White analogizes the\ntheorist to an ‘archer’ who is more or less reliable in\n‘aiming’ at the truth in selecting a theory.) Then White\noffers a simple argument for predictivism: assuming ~DS, ES provides\nevidence for RA. But assuming DS, ES provides no evidence for RA.\nThus, heuristic predictivism is true. \nInterestingly, White bills his account as a strong heuristic account.\nIn making this claim he is claiming that the epistemic advantage of\nprediction would not be entirely erased for an observer who was\ncompletely aware of all relevant evidence and background knowledge\npossessed by the scientific community at the relevant point in time.\nThis is because the degree to which theorizing is reliable depends\nupon principles of evidence assessment and causal relations (including\nthe reliability of our perceptual faculties, accuracy of measuring\ninstruments, etc.) that are not entirely “transparent” to\n us.[12]\n Insofar as fully informed scientists may not be fully convinced of\njust how reliable these principles and relations are, evidence that\nthey lead to the endorsement of theories which are predictively\nsuccessful continues to redound to their assessed reliability. Thus,\nWhite concludes, strong heuristic predictivism is vindicated (2003:\n671–4).  \nHitchcock and Sober (2004) provide an original theory of weak\nheuristic predictivism that is based on a particular worry about\naccommodation. On the assumption that data are noisy (i.e. imbued with\nobservational error), a good theory will almost never fit the data\nperfectly. To construct a theory that fits the data better than a good\ntheory should, given noisy data, is to be guilty of\n“overfitting”—if we know a theorist built her theory\nto accommodate data, we may well worry that she has overfit the data\nand thus constructed a flawed theory. If we know however that a\ntheorist built her theory without access to such data, or without\nusing it in the process of theory construction, we need not worry that\noverfitting that data has occurred. When such a theory goes on to make\nsuccessful predictions, Hitchcock and Sober moreover argue, this\nprovides us with evidence that the data on which the theory was\ninitially based were not overfit in the process of constructing the\ntheory.  \nHitchcock and Sober’s approach derives from a particular\nsolution to the curve-fitting problem presented in Forster and Sober\n1994. The curve fitting problem is how to select an optimally\nsupported curve on the basis of a given body of data (e.g., a set of\n\\([X,Y]\\) points plotted on a coordinate graph). A well-supported\ncurve will feature both ‘goodness of fit’ with the data\nand simplicity (intuitively, avoiding highly bumpy or irregular\npatterns). Solving the curve-fitting problem requires some precise way\nof characterizing a curve’s simplicity, a way of characterizing\ngoodness of fit, and a method of balancing simplicity against goodness\nof fit to identify an optimal curve.  \nForster and Sober cite Akaike’s (1973) result that an unbiased\nestimate of the predictive accuracy of a model can be computed by\nassessing both its goodness of fit and its simplicity as measured by\nthe number of adjustable parameters it contains. A model is a\nstatement (a polynomial, in the case of a proposed curve) that\ncontains at least one adjustable parameter. For any particular model\nM, a given data set, and identifying \\(L(M)\\) as the likeliest\n(i.e. best data fitting) curve from M, Akaike showed: \nAn unbiased estimate of the predictive accuracy of model \nThis estimate is deemed a model’s ‘Akaike Information\nCriterion’ (AIC) score—it measures goodness of fit in\nterms of the log likelihood of the data on the assumption of \\(L(M)\\).\nThe simplicity of the model is inversely proportion to k, the\nnumber of adjustable parameters in the model. The intuitive idea is\nthat models with a high k value will provide a large variety of\ncurves that will tend to fit data more closely than models with a\nlower k value—and thus large k values are more\nprone to overfitting than small k values. So the AIC score\nassesses a model’s likely predictive accuracy in a way that\nbalances both goodness of fit and simplicity, and the curve-fitting\nproblem is arguably solved.  \nHitchcock and Sober (2004) consider a hypothetical example involving\ntwo scientists, Penny Predictor and Annie Accommodator. Working\nindependently, they acquire the same set of data D—Penny\nproposes theory Tp while Annie proposes Ta. The critical\ndifference however was that Penny proposed Tp on the basis of\nan initial segment of the data D1—thereafter she\npredicted the remaining data D2 to a high degree of accuracy\n\\((D = D1 \\cup D2)\\). Annie however was in possession of all the data\nin D prior to proposing Ta and in proposing this theory\naccommodated D. Hitchcock and Sober ask whether there might be\nreason to suspect that Penny’s theory will be more predictively\naccurate in the future, and in this precise sense be better confirmed.\n \nHitchcock and Sober argue that there is no one answer to this\nquestion—and then present a series of several cases. Insofar as\npredictivism holds in some and not others, their account of\npredictivism is clearly a local (rather than global) account. In cases\nin which Penny and Annie propose the same theory, or propose theories\nwhose AIC scores can be computed and directly compared, there is no\nreason to regard facts about how they built the theory to carry\nfurther significance. But if we do not know which theories were\nproposed, or by what method they were constructed, the fact that Penny\npredicted data that Annie accommodated can argue for Penny’s\ntheory having a higher AIC score than Annie’s, and thus carry an\nepistemic advantage. \nInsofar as predictivism holds in some cases but not the others, the\nquestion whether predictivism holds in actual episodes of science\ndepends on which cases such actual episodes tend to resemble, but\nHitchcock and Sober “take no stand on how often the various\ncases arise” (2004: 21).  \nAlthough their account of predictivism is tailored initially to the\ncurve-fitting problem, it is by no means limited to such cases. They\nnote that it is natural to think of a model as analogous to the\nontological framework of a scientific theory where the various\nontological commitments can function as ‘adjustable\nparameters’—for example, the Ptolemaic and Copernican\nworld pictures both begin with a claim that a certain entity (the sun\nor the earth) is at the center, and these models are articulated by\nproducing models with adjustable parameters. \nFor critical discussion of Sober and Hitchcock’s account, see\nLee 2012, 2013 and Douglas & Magnus 2013: 582–584.  \nBarnes (2005a, 2008) maintains that predictivism is frequently a\nmanifestation of a phenomenon he calls ‘epistemic\npluralism’. A ‘T-evaluator’ (a scientist who\nassigns some probability to theory T) is an epistemic pluralist\ninsofar as she regards one form of evidence to be the probabilities\nposted (i.e. publicly presented) by other scientists for and against\nT and other relevant claims (she is an epistemic individualist\nif she does not do this but considers only the scientific evidence\n‘on her own’). One form of pluralistic evidence is the\nevent in which a reputable scientist endorses a theory—this\ntakes place when a scientist posts a probability for T that is\n(1) no lower than the evaluator’s probability and (2) high\nenough that subsequent predictive confirmation of T would\nredound to the scientist’s credibility (2008: 2.2).  \nBarnes rejects the heuristic conception of novelty on the grounds that\nit is a mistake to think that what matters epistemically is the\nprocess by which the theory was constructed—what matters is on\nwhat basis the theory was endorsed (2008: 33f) . In the example above,\nconfirmation of N (a consequence of T) could carry\nspecial weight for an evaluator who learned that the theorist endorsed\nthe theory without appeal to observational evidence for N\n(irrespective of how the theory was constructed). He proposes to\nreplace the heuristic conception with his endorsement conception of\nnovelty: N (a known consequence of T) counts as a novel\nconfirmation of T relative to agent X insofar as\nX posts an endorsement-level probability for T that is\nbased on a body of evidence that does not include observation-based\nevidence for N.  \nBarnes claims that the notion of endorsement novelty has several\nadvantages over the heuristic conception—one is that endorsement\nnovelty can account for the fact that prediction is a matter of\ndegree: the more strongly the theorist endorses T, the more\nstrongly its consequence N is predicted (and thus the more\nevidence for T for pluralist evaluators who trust the\nendorser). Another is that the orthodox distinction between the\ncontext of discovery and the context of justification is preserved.\nAccording to the latter distinction, it does not matter for purposes\nof theory evaluation how a theory was discovered. But this turns out\nnot to be true on the heuristic conception given the central\nimportance it accords to how a theory was built (cf. Leplin 1987).\nEndorsement novelty respects the irrelevance of the process by which\ntheories are discovered (Barnes 2008: 37–8).  \nOne claim central to this account is that confirmation is a three-way\nrelation between theory, evidence, and background belief (cf. Good\n1967). Barnes distinguishes between two types of theory endorser: (1)\nvirtuous endorsers post probabilities for theories that cohere with\ntheir evidence and background beliefs and (2) unvirtuous endorsers who\npost probabilities that do not so cohere. A common way of explaining\nthe predictivist intuition is to note that accommodators tend to be\nviewed with a certain suspicion—their endorsement of T\nbased on accommodated evidence may reflect a kind of social pressure\nto endorse T whatever its merits (cf. the ‘fudging\nexplanation’ above). Such an endorser may post a probability\nfor T that is too high given her total evidence and background\nbelief—predictivism thus becomes a strategy by which pluralist\nendorsers protect themselves from unvirtuous accommodators (Barnes\n2008: 61–69).  \nBarnes then presents a theory of predictivism that is designed to\napply to virtuous endorsers. Virtuous predictivism has two roots: (1)\nthe prediction per se, which is constituted by an endorser’s\nposting an endorsement level probability for T that entails\nempirical consequence N on a basis that does not include\nobservation-based evidence for T, and (2) predictive success,\nconstituted by the empirical demonstration that N is true. The\nprediction per se carries epistemic significance for a pluralist\nendorser because it implies that the predictor possesses reason\nR (consisting of background beliefs) that supports T. If\nthe endorser views the predictor as credible, this simple act of\nprediction carries epistemic weight. Predictive success then confirms\nthe truth of R, which thereby counts as evidence for T.\nNovel confirmation thus has the special virtue of confirming the\nbackground beliefs of the predictor—accommodative confirmation\nlacks this virtue.  \nBarnes presents two Bayesian thought experiments that purport to\nestablish virtuous predictivism. In each experiment an evaluator Eva\nfaces two scenarios—one in which she confronts Peter who posts\nan endorsement probability for T without appeal to\nN-supporting observations (thus Peter predicts N) and\nanother in which she confronts Alex who posts an endorsement\nprobability for T on a basis that includes observations that\nestablish N (thus Alex accommodates N). The idea behind\nboth thought experiments is to make the scenarios otherwise as similar\nas possible—Barnes makes a number of ceteris paribus\nassumption that render the probability functions of Peter and Alex\nmaximally similar. However it turns out that there is more than one\nway to keep the scenarios maximally similar: in the first experiment,\nPeter and Alex have the same likelihood ratio but have different\nposteriors for T. In the second scenario they have the same\nposteriors but different likelihood ratios. Barnes demonstrates that\nEva’s posterior probability is higher in the predictor scenario\nin both experiments—thus vindicating virtuous predictivism\n(2008: 69–80). \nAlthough his defense of virtuous predictivism is the centerpiece of\nhis account, Barnes claims that predictivism can hold true of actual\ntheory evaluation in a variety of ways. He maintains that the position\ndeemed ‘weak predictivism’ is actually ambiguous—it\ncould refer to the claim that scientists actually rely on knowledge\nthat evidence was (or was not) predicted because prediction is\nsymptomatic of a some other feature(s) of theories that is\nepistemically important (‘tempered\n predictivism’[13])\n or simply to the fact that there is a correlation between prediction\nand this other feature(s) (‘thin predictivism’). The\ndistinction between tempered and thin predictivism cross classifies\nwith the distinction between virtuous and unvirtuous predictivism to\nproduce four varieties of weak predictivism. Barnes then turns to the\ncase of Mendeleev’s periodic law and argues that all four\nvarieties can be distinguished in the scientific community’s\nreaction to Mendeleev’s theory of the elements (2008:\n82–122). In particular, he argues that it was specifically\nMendeleev’s predicted evidence, not his accommodated evidence,\nthat had the power to confirm his scientific and methodological\nbackground beliefs from the standpoint of the scientific community.\n \nCritical responses to Barnes’s account are presented in Glymour\n2008; Leplin 2009; and Harker 2011. Barnes 2014 responds to these. See\nalso Magnus 2011 and Alai 2016. \nIt was noted in\n Section 1\n that John Maynard Keynes rejected predictivism—he argued that\nwhen a theory T is first constructed it is usually the case\nthat there are reasons R that favor T. If T goes\non to generate successful novel predictions E then those\nreasons combine with R to support T—but if some\n\\(T'\\) is constructed ‘merely because it fit\nE’ then \\(T'\\) will be less supported than\nT. This has been deemed the “Keynesian dissolution of the\nparadox of predictivism” (Barnes 2008: 15–18)  \nColin Howson cites with approval the Keynesian dissolution (1988: 382)\nand provides the following illustration: consider h and\n\\(h'\\) which are rival explanatory frameworks.\n\\(h'\\) independently predicts e; h does not\nentail e but has a free parameter which is fixed on the basis\nof e to produce \\(h(a_{0})\\)—this latter hypothesis thus\nentails e. So \\(h'\\) predicts e while\n\\(h(a_{0})\\) merely accommodates e. Let us assume that the\nprior probabilities of h and \\(h'\\) are equal (i.e.,\n\\(p(h) = p(h')\\)). Now it stands to reason that \\(p(h(a_0)) \\lt\np(h)\\) since \\(h(a_{0})\\) entails h but not vice\nversa—thus Howson shows it follows that the effect of\ne’s confirmation will be to leave \\(h'\\) no less\nprobable—and quite possibly more probable—than\n\\(h(a_{0})\\) (1990: 236–7). Thus predictivism appears true but\nthe operating factor is the role of unequal prior\n probabilities.[14] \nThe argument from Keynes and Howson against predictivism holds that\nthe evidence which appears to support predictivism is\nillusory—they are clearly asserting that strong predictivism is\nfalse, presumably in its temporal and heuristic forms. \nHowever, it is important to note that the arguments of Keynes and\nHowson cited above predate the injection of the concept of ‘weak\npredictivism’ into the\n literature.[15]\n It is thus unclear what stand Keynes or Howson would take on weak\npredictivism. Likewise, Collins’ 1994 paper “Against the\nEpistemic Value of Prediction” strongly rejects predictivism,\nbut what he is clearly denying is what has since been deemed strong\nheuristic predictivism. He might endorse weak heuristic predictivism\nas he concedes that  \nall sides to the debate agree that knowing that a theory predicted,\ninstead of accommodated, a set of data can give us an additional\nreason for believing it is true by telling us something about the\nstructural/relational features of a theory. (1994: 213)  \nSimilarly Harker argues that “it is time to leave predictivism\nbehind” but also concedes that “some weak predictivist\ntheses may be correct” (2008: 451); Harker worries that\nproclaiming weak predictivism may mislead some into thinking that\npredictive success is somehow more important than other epistemic\nindicators (such as endorsement by reliable scientists). White goes so\nfar as to claim that weak predictivism “is not\ncontroversial” (2003: 656). \nStephen Brush is the author of a body of historical work much of which\npurports to show that temporal predictivism does not hold in various\nepisodes of the history of\n science.[16]\n These include the case of starlight bending in the assessment of the\nGeneral Theory of Relativity (Brush 1989), Alfvén’s\ntheories of space plasma phenomena (Brush 1990), and the revival of\nbig bang cosmology (Brush 1993). However, Brush (1996) argues that\ntemporal novelty did play a role in the acceptance of\nMendeleev’s Periodic Table based on Mendeleev’s\npredictions. Scerri and Worrall (2001) presents considerable\nhistorical detail about the assessment of Mendeleev’s theory and\ndispute Brush’s claim that temporal novelty played an important\nrole in the acceptance of the theory (2001: 428–436).  \nScientific realism holds that there is sufficient evidence to believe\nthat the theories of the ‘mature sciences’ are at least\napproximately true. Appeals to novelty have been important in\nformulating two arguments for realism—these are the ‘no\nmiracle argument’ and the realist reply to the so-called\n‘pessimistic\n induction’.[17] \nThe no-miracle argument for scientific realism holds that realism is\nthe only account that does not make the success of science a miracle\n(Putnam 1975: 73). ‘The success of science’ here refers to\nthe myriad verified empirical consequences of the theories of the\nmature sciences—but as we have seen there is a long standing\ntendency to regard with suspicion those verified empirical\nconsequences the theory was built to fit. Thus the ‘ultimate\nargument for scientific realism’ refers to a version of the no\nmiracle argument that focuses just on the verified novel consequences\nof theories—it would be a miracle, this argument proclaims, if a\ntheory managed to have a sustained record of successful novel\npredictions if the theory were not at least approximately true. Thus,\nassuming there are no competing theories with comparable records of\nnovel success, we ought to infer that such theories are at least\napproximately true (Musgrave\n 1988).[18] \nInsofar as the ultimate argument for realism clearly emphasizes a\nspecial role for novel successes, the nature of novelty has been an\nimportant focus in the realist account. Leplin 1997 is a book length\narticulation of the ultimate argument for realism; Leplin proposes a\nsufficient condition for novelty consisting of two conditions: \nAn observational result O is novel for T if:  \nLeplin clarifies that a ‘minimally adequate\nreconstruction’ of such reasoning will be a valid deduction\nD of the ‘basic identifying hypotheses’ of T\nfrom independently warranted background assumptions—the premises\nof D cannot be weakened or simplified while preserving\nD’s validity. Thus for Leplin what establishes whether\nO is a novel consequence of T is not whether O\nwas actually used in the construction of T, but rather whether\nit was ‘needed’ for T’s construction. As with\nWorrall’s mature ‘essential use’ conception of\nnovelty, what matters is whether there is a heuristic path to T\nthat does not appeal to O, whether or not O was used in\nconstructing T. The Uniqueness Condition helps bolster the\nargument for the truth of theories with true novel consequences, for\nif there were another theory \\(T'\\) (incompatible with\nT) that also provides a viable explanation of O, the\nimputation of truth could not explain the novel success of both\nT and \\(T'\\). The success of at least one would have\nto be due to chance, but if chance could explain one such success it\ncould explain the other as well.  \nBoth of these conditions for novelty have been questioned. Given the\nIndependence Condition, it is unclear that any observational result\nO will count as novel for any theory, for it may always be true\nthat the logically weakest set of premises that entail T (which\nwill be cited in a minimally adequate reconstruction of the reasoning\nthat led to T) will include O as a disjunct of one of\nthe premises (Healey 2001: 779). The Uniqueness Condition insists that\nthere be no available alternative explanation of O at the time\nT first explains O—but clearly, theories that\nexplain O could be subsequently proposed and would threaten the\nimputation of truth to T no less. This condition seems\narbitrarily to privilege theories depending on when they were proposed\n(Sarkar 1998: 206–8; Ladyman 1999: 184).  \nAnother conception of novelty whose purpose is to bolster the ultimate\nargument for realism is ‘functional novelty’ (Alai 2014).\nA datum d is ‘functionally novel’ for theory\nT if (1) d was not used essentially in constructing\nT (viz., there is a heuristic path to T and related\nauxiliary hypotheses that does not cite d), (2) d is\na priori improbable, and (3) d is heterogeneous with\nrespect to data that is used in constructing T and related\nauxiliary hypotheses (i.e. d is qualitatively different from\nsuch data). Functional novelty is a ‘gradual’ concept\ninsofar as a priori improbability and data heterogeneity come\nin degrees. If there is more than one theory for which d is\nfunctionally novel then the dispute between these theories cannot be\nsettled by the ultimate argument (Alai 2014: 306).  \nAnti-realists have argued that insofar as we adopt a naturalistic\nphilosophy of science, the same standards should be used for assessing\nphilosophical theories as scientific theories. Consequently, if novel\nconfirmations are necessary for inferring a theory’s truth then\nscientific realism should not be accepted as true, as the latter\nthesis has no novel confirmations to its credit (Frost-Arnold 2010,\nMizrahi 2012).  \nAnother component of the realist/anti-realist debate in which appeals\nto novel success figure importantly is the debate over the\n‘pessimistic induction’ (or ‘pessimistic\nmeta-induction’). According to this argument, the history of\nscience is almost entirely a history of theories that were judged\nempirically successful in their day only to be shown subsequently to\nbe entirely false. There is no reason to think that currently accepted\ntheories are any different in this regard (Laudan 1981b).  \nIn response some realists have defended ‘selective\nrealism’ which concedes that while the majority of theories from\nthe history of science have proven false, some of them have components\nthat were retained in subsequent theories—these tend to be the\ncomponents that were responsible for novel successes. Putative\nexamples of this phenomenon are the caloric theory of heat and\nnineteenth century optical theories (Psillos 1999: Ch. 6), both of\nwhich were ultimately rejected as false but which had components that\nwere retained in subsequent theories; these were the portions that\nwere responsible for their novel\n confirmations.[19]\n So in line with the ultimate argument the claim is made that novel\nsuccesses constitute a serious argument for the truth of the theory\ncomponent which generates them. However, antirealists have responded\nby citing cases of theoretical claims that were subsequently\ndetermined to be entirely false but which managed nonetheless to\ngenerate impressive records of novel predictions. These include\ncertain key claims made by Johannes Kepler in his Mysterium\nCosmographicum (1596), assumptions used by Adams and Leverrier in\nthe prediction of the planet Neptune’s existence and location\n(Lyons 2006), and Ptolemaic astronomy (Carman & Díez\n2015).","contact.mail":"ebarnes@smu.edu","contact.domain":"smu.edu"}]
