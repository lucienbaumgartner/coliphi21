[{"date.published":"2018-08-07","url":"https://plato.stanford.edu/entries/causal-models/","author1":"Christopher Hitchcock","author1.info":"http://hss.divisions.caltech.edu/people/christopher-r-hitchcock","entry":"causal-models","body.text":"\n\n\nCausal models are mathematical models representing causal\nrelationships within an individual system or population. They\nfacilitate inferences about causal relationships from statistical\ndata. They can teach us a good deal about the epistemology of\ncausation, and about the relationship between causation and\nprobability. They have also been applied to topics of interest to\nphilosophers, such as the logic of counterfactuals, decision theory,\nand the analysis of actual causation.\n\nCausal modeling is an interdisciplinary field that has its origin in\nthe statistical revolution of the 1920s, especially in the work of the\nAmerican biologist and statistician Sewall Wright (1921). Important\ncontributions have come from computer science, econometrics,\nepidemiology, philosophy, statistics, and other disciplines. Given the\nimportance of causation to many areas of philosophy, there has been\ngrowing philosophical interest in the use of mathematical causal\nmodels. Two major works—Spirtes, Glymour, and Scheines 2000\n(abbreviated SGS), and Pearl 2009—have been particularly\ninfluential. \nA causal model makes predictions about the behavior of a system. In\nparticular, a causal model entails the truth value, or the\nprobability, of counterfactual claims about the system; it predicts\nthe effects of interventions; and it entails the probabilistic\ndependence or independence of variables included in the model. Causal\nmodels also facilitate the inverse of these inferences: if we have\nobserved probabilistic correlations among variables, or the outcomes\nof experimental interventions, we can determine which causal models\nare consistent with these observations. The discussion will focus\non what it is possible to do in “in principle”. For\nexample, we will consider the extent to which we can infer the correct\ncausal structure of a system, given perfect information about the\nprobability distribution over the variables in the system. This\nignores the very real problem of inferring the true probabilities from\nfinite sample data. In addition, the entry will discuss the application of\ncausal models to the logic of counterfactuals, the analysis of\ncausation, and decision theory. \nThis section introduces some of the basic formal tools used in causal\nmodeling, as well as terminology and notational conventions. \nVariables are the basic building blocks of causal models.\nThey will be represented by italicized upper case letters. A variable\nis a function that can take a variety of values. The values of a\nvariable can represent the occurrence or non-occurrence of an event, a\nrange of incompatible events, a property of an individual or of a\npopulation of individuals, or a quantitative value. For instance, we\nmight want to model a situation in which Suzy throws a stone and a\nwindow breaks, and have variables S and W such that: \nIf we are modeling the influence of education on income in the United\nStates, we might use variables E and I such that: \nThe set of possible values of a variable is the range of that\nvariable. We will usually assume that variables have finitely many\npossible values, as this will keep the mathematics and the exposition\nsimpler. However, causal models can also feature continuous variables,\nand in some cases this makes an important difference. \nA world is a complete specification of a causal model; the\ndetails will depend upon the type of model. For now, we note that a\nworld will include, inter alia, an assignment of values to\nall of the variables in the model. If the variables represent the\nproperties of individuals in a population, a world will include an\nassignment of values to every variable, for every individual in the\npopulation. A variable can then be understood as a function whose\ndomain is a set of worlds, or a set of worlds and individuals. \nIf X is a variable in a causal model, and x is a\nparticular value in the range of X, then \\(X = x\\) is an\natomic proposition. The logical operations of negation\n(“not”), conjunction (“and”), disjunction\n(“or”), the material conditional\n(“if…then…”), and the biconditional\n(“if and only if”) are represented by\n“\\({\\sim}\\)”, “&”, “\\(\\lor\\)”,\n“\\(\\supset\\)”, and “\\(\\equiv\\)”\nrespectively. Any proposition built out of atomic propositions and\nthese logical operators will be called a Boolean\nproposition. Note that when the variables are defined over individuals\nin a population, reference to an individual is not included\nin a proposition; rather, the proposition as a whole is true or false\nof the various individuals in the population. \nWe will use basic notation from set theory. Sets will appear in\nboldface. \nIf \\(\\bS = \\{x_1 , \\ldots ,x_n\\}\\) is a set of values in the range of\nX, then \\(X \\in \\bS\\) is used as shorthand for the disjunction\nof propositions of the form \\(X = x_i\\), for \\(i = 1,\\ldots\\), n.\nBoldface represents ordered sets or vectors. If\n\\(\\bX = \\{X_1 , \\ldots ,X_n\\}\\) is a vector of variables, and \\(\\bx =\n\\{x_1 , \\ldots ,x_n\\}\\) is a vector of values, with each value \\(x_i\\)\nin the range of the corresponding variable \\(X_i\\), then \\(\\bX = \\bx\\)\nis the conjunction of propositions of the form \\(X_i = x_i\\). \nIn\n section 4,\n we will consider causal models that include probability. Probability\nis a function, P, that assigns values between zero and one, inclusive.\nThe domain of a probability function is a set of propositions that\nwill include all of the Boolean propositions described above, but\nperhaps others as well. \nSome standard properties of probability are the following: \nSome further definitions: \nThe conditional probability of A given B, written \\(\\Pr(A\n\\mid B)\\) is standardly defined as follows: \nWe will ignore problems that might arise when \\(\\Pr(B) = 0\\). \nAs a convenient shorthand, a probabilistic statement that contains\nonly a variable or set of variables, but no values, will be understood\nas having a universal quantification over all possible values of the\nvariable(s). Thus if \\(\\bX = \\{X_1 ,\\ldots ,X_m\\}\\) and \\(\\bY = \\{Y_1\n,\\ldots ,Y_n\\}\\), we may write \nas shorthand for \nwhere the domain of quantification for each variable will be the range\nof the relevant variable. \nWe will not presuppose any particular interpretation of probability\n(see the entry on\n interpretations of probability),\n but we will assume that frequencies in appropriately chosen samples\nprovide evidence about the underlying probabilities. For instance,\nsuppose there is a causal model including the variables E\nand I described above, with \\(\\Pr(E = 3) = .25\\). Then we expect\nthat if we survey a large, randomly chosen sample of American adults, we\nwill find that approximately a quarter of them have a Bachelor’s\ndegree, but no higher degree. If the survey produces a sample frequency\nthat substantially differs from this, we have evidence that the model\nis inaccurate. \nIf \\(\\bV\\) is the set of variables included in a causal model, one way\nto represent the causal relationships among the variables in \\(\\bV\\)\nis by a graph. Although we will introduce and use graphs in\n section 3,\n they will play a more prominent role in\n section 4.\n We will discuss two types of graphs. The first is the directed\nacyclic graph (DAG). A directed graph \\(\\bG\\) on\nvariable set \\(\\bV\\) is a set of ordered pairs of variables in\n\\(\\bV\\). We represent this visually by drawing an arrow from X\nto Y just in case \\(\\langle X, Y\\rangle\\) is in \\(\\bG\\).\n Figure 1\n shows a directed graph on variable set \\(\\bV = \\{S, T, W, X, Y,\nZ\\}\\). \nFigure 1 \nA path in a directed graph is a non-repeating sequence of\narrows that have endpoints in common. For example, in\n Figure 1\n there is a path from X to Z, which we can write as \\(X\n\\leftarrow T \\rightarrow Y \\rightarrow Z\\). A directed path\nis a path in which all the arrows point in the same direction; for\nexample, there is a directed path \\(S \\rightarrow T \\rightarrow Y\n\\rightarrow Z\\). A directed graph is acyclic, and hence a\nDAG, if there is no directed path from a variable to itself. Such a\ndirected path is called a cycle. The graph in Figure 1\ncontains no cycles, and hence is a DAG. \nThe relationships in the graph are often described using the language\nof genealogy. The variable X is a parent of Y\njust in case there is an arrow from X to Y. \\(\\bPA(Y)\\)\nwill denote the set of all parents of Y. In\n Figure 1,\n \\(\\bPA(Y) = \\{T, W\\}\\). X is an ancestor of Y\n(and Y is a descendant of \\(X)\\) just in case there is\na directed path from X to Y. However, it will be\nconvenient to deviate slightly from the genealogical analogy and\ndefine “descendant” so that every variable is a descendant\nof itself. \\(\\bDE(X)\\) denotes the set of all descendants of X.\nIn Figure 1, \\(\\bDE(T) = \\{T,X, Y, Z\\}\\). \nAn arrow from Y to Z in a DAG represents that Y\nis a direct cause of \\(Z.\\) Roughly, this means that the\nvalue of Y makes some causal difference for the value of\nZ, and that Y influences Z through some process\nthat is not mediated by any other variable in \\(\\bV\\). Directness is\nrelative to a variable set: Y may be a direct cause of Z\nrelative to variable set \\(\\bV\\), but not relative to variable set\n\\(\\bV'\\) that includes some additional variable(s) that mediate the\ninfluence of Y on \\(Z.\\) As we develop our account of graphical\ncausal models in more detail, we will be able to say more precisely\nwhat it means for one variable to be a direct cause of another. While\nwe will not define “cause”, causal models presuppose a\nbroadly difference-making notion of causation, rather than a\ncausal process notion (Salmon 1984, Dowe 2000) or a mechanistic notion\n(Machamer, Darden, & Craver 2000; Glennan 2017). We will call the\nsystem of direct causal relations represented in a DAG such as\n Figure 1\n the causal structure on the variable set \\(\\bV\\). \nA second type of graph that we will consider is an acyclic\ndirected mixed graph (ADMG). An ADMG will contain double-headed\narrows, as well as single-headed arrows. A double-headed arrow\nrepresents a latent common cause. A latent common cause of\nvariables X and Y is a common cause that is not included\nin the variable set \\(\\bV\\). For example, suppose that X and\nY share a common cause L\n (Figure 2(a)).\n An ADMG on the variable set \\(\\bV = \\{X, Y\\}\\) will look like\n Figure 2(b). \n(a) \n(b) \nFigure 2 \nWe can be a bit more precise. We only need to represent missing common\ncauses in this way when they are closest common causes. That\nis, a graph on \\(\\bV\\) should contain a double-headed arrow between\nX and Y when there is a variable L that is\nomitted from \\(\\bV\\), such that if L were added to \\(\\bV\\) it\nwould be a direct cause of X and Y. \nIn an ADMG, we expand the definition of a path to include\ndouble-headed arrows. Thus, \\(X \\leftrightarrow Y\\) is a path in the\nADMG shown in\n Figure 2(b).\n Directed path retains the same meaning, and a directed path\ncannot contain double-headed arrows. \nWe will adopt the convention that both DAGs and ADMGs represent the\npresence and absence of both direct causal relationships and\nlatent common causes. For example the DAG in\n Figure 1\n represents that X is a direct cause of Y, that W\nis not a direct cause of Y, and that there are no latent\ncommon causes. The absence of double-headed arrows from Figure 1\ndoes not show merely that we have chosen not to include latent common\ncauses in our representation; it shows that there are no latent common\ncauses. \nIn this section, we introduce deterministic structural equation\nmodels (SEMs), postponing discussion of probability until\n Section 4.\n We will consider two applications of deterministic SEMs: the logic of\ncounterfactuals, and the analysis of actual causation. \nA SEM characterizes a causal system with a set of variables, and a set\nof equations describing how each variable depends upon its immediate\ncausal predecessors. Consider a gas grill, used to cook meat. We can\ndescribe the operations of the grill using the following\nvariables: \nThus, for example, Gas knob = 1 means that the gas knob is\nset to low; Igniter = 1 means that the igniter is pressed,\nand so on. Then the equations might be: \nThe last equation, for example, tells us that if the meat is not put\non the grill, it will remain raw (Meat cooked = 0). If the\nmeat is put on the grill, then it will get cooked according to the\nlevel of the flame: if the flame is low (Flame = 1), the meat\nwill be rare (Meat cooked = 1), and so on. \nBy convention each equation has one effect variable on the left hand\nside, and one or more cause variables on the right hand side. We also\nexclude from each equation any variable that makes no difference to\nthe value of the effect variable. For example, the equation for\nGas level could be written as Gas level = (Gas\nconnected \\(\\times\\) Gas knob) \\(+\\) (0 \\(\\times\\) Meat\ncooked); but since the value of Meat cooked makes no\ndifference to the value of Gas level in this equation, we\nomit the variable Meat cooked. A SEM is acyclic if\nthe variables can be ordered so that variables never appear on the\nleft hand side of an equation after they have appeared on the right.\nOur example is acyclic, as shown by the ordering of variables given\nabove. In what follows, we will assume that SEMs are acyclic, unless\nstated otherwise. \nWe can represent this system of equations as a DAG\n (Figure 3): \nFigure 3 \nAn arrow is drawn from variable X to variable Y\njust in case X figures as an argument in the equation for\nY. The graph contains strictly less information than the set of\nequations; in particular, the DAG gives us qualitative information\nabout which variables depend upon which others, but it does not tell\nus anything about the functional form of the dependence. \nThe variables in a model will typically depend upon further variables\nthat are not explicitly included in the model. For instance, the level\nof the flame will also depend upon the presence of oxygen. Variables that\nare not explicitly represented in the model are assumed to be fixed at\nvalues that make the equations appropriate. For example, in our model\nof the gas grill, oxygen is assumed to be present in sufficient\nquantity to sustain a flame ranging in intensity from low to high. \nIn our example, the variables Gas level, Flame, and\nMeat cooked are endogenous, meaning that their\nvalues are determined by other variables in the model. Gas\nconnected, Gas knob, Igniter, and Meat on are\nexogenous, meaning that their values are determined outside\nof the system. In all of the models that we will consider in\n section 3,\n the values of the exogenous variables are given or otherwise\nknown. \nFollowing Halpern (2016), we will call an assignment of values to the\nexogenous variables a context. In an acyclic SEM, a context\nuniquely determines the values of all the variables in the model. An\nacyclic SEM together with a context is a world (what Halpern\n2016 calls a “causal setting”). For instance, if we add\nthe setting \nto our three equations above, we get a world in which Gas\nlevel = 3, Flame = 3, and Meat cooked = 3. \nThe distinctively causal or “structural” content of a SEM\nderives from the way in which interventions are represented.\nTo intervene on a variable is to set the value of that variable by a\nprocess that overrides the usual causal structure, without interfering\nwith the causal processes governing the other variables. More\nprecisely, an intervention on a variable X overrides the normal\nequation for X, while leaving the other equations unchanged.\nFor example, to intervene on the variable Flame in our\nexample would be to set the flame to a specified level regardless of\nwhether the igniter is pressed or what the gas level is. (Perhaps, for\nexample, one could pour kerosene into the grill and light it with a\nmatch.) Woodward (2003) proposes that we think of an intervention as a\ncausal process that operates independently of the other variables in\nthe model. Randomized controlled trials aim to intervene in this\nsense. For example, a randomized controlled trial to test the efficacy\nof a drug for hypertension aims to determine whether each subject\ntakes the drug (rather than a placebo) by a random process such as a\ncoin flip. Factors such as education and health insurance that\nnormally influence whether someone takes the drug no longer play this\nrole for subjects in the trial population. Alternately, we could\nfollow the approach of Lewis (1979) and think of an intervention\nsetting the value of a variable by a minor “miracle”. \nTo represent an intervention on a variable, we replace the\nequation for that variable with a new equation stating the value to\nwhich the variable is set. For example, if we intervene to set the\nlevel of flame at low, we would represent this by replacing\nthe equation Flame = Gas level \\(\\times\\)\nIgniter with Flame = 1. This creates a new causal\nstructure in which Flame is an exogenous variable;\ngraphically, we can think of the intervention as “breaking the\narrows” pointing into Flame. The new system of\nequations can then be solved to discover what values the other\nvariables would take as a result of the intervention. In the world\ndescribed above, our intervention would produce the following set of\nequations: \nWe have struck through the original equation for Flame to\nshow that it is no longer operative. The result is a new world with a\nmodified causal structure, with Gas level = 3, Flame\n= 1, and Meat cooked = 1. Since the equation connecting\nFlame to its causes is removed, any changes introduced by\nsetting Flame to 1 will only propagate forward through the\nmodel to the descendants of Flame. The intervention changes\nthe values of Flame and Meat cooked, but it does not\naffect the values of the other variables. We can represent\ninterventions on multiple variables in the same way, replacing the\nequations for all of the variables intervened on. \nInterventions help to give content to the arrows in the corresponding\nDAG. If variable \\(X_i\\) is a parent of \\(X_j\\), this means that there\nexists some setting for all of the other variables in the model, such\nthat when we set those variables to those values by means of an\nintervention, intervening on \\(X_i\\) can still make a difference for\nthe value of \\(X_j\\). For example, in our original model, Gas\nlevel is a parent of Flame. If we set the value of\nIgniter to 1 by means of an intervention, and set Gas\nknob, Gas connected, Meat on, and Meat cooked to any\nvalues at all, then intervening on the value of Gas level\nmakes a difference for the value of Flame. Setting the value\nof Gas level to 1 would yield a value of 1 for\nFlame; setting Gas level to 2 yields a\nFlame of 2; and so on. \nA counterfactual is a proposition in the form of a subjunctive\nconditional. The antecedent posits some circumstance, typically one\nthat is contrary to fact. For example, in our gas grill world, the\nflame was high, and the meat was well done. We might reason: “if\nthe flame had been set to low, the meat would have been rare”.\nThe antecedent posits a hypothetical state of affairs, and the\nconsequent describes what would have happened in that hypothetical\nsituation. \nDeterministic SEMs naturally give rise to a logic of counterfactuals.\nThese counterfactuals are called structural counterfactuals\nor interventionist counterfactuals. Structural\ncounterfactuals are similar in some ways to what Lewis (1979) calls\nnon-backtracking counterfactuals. In a non-backtracking\ncounterfactual, one does not reason backwards from a counterfactual\nsupposition to draw conclusions about the causes of the hypothetical\nsituation. For instance, one would not reason “If the meat had\nbeen cooked rare, then the flame would have been set to low”.\nLewis (1979) proposes that we think of the antecedent of a\ncounterfactual as coming about through a minor “miracle”.\nThe formalism for representing interventions described in the previous\nsection prevents backtracking from effects to causes. \nThe logic of structural counterfactuals has been developed by Galles\nand Pearl (1998), Halpern (2000), Briggs (2012), and Zhang\n(2013a). This section will focus on Briggs’ formulation; it has\nthe richest language, but unlike the other approaches it can not be\napplied to causal models with cycles. Despite a shared concern with\nnon-backtracking counterfactuals, Briggs’ logic differs in a\nnumber of ways from the more familiar logic of counterfactuals\ndeveloped by Stalnaker (1968) and Lewis (1973b). \nWe interpret the counterfactual conditional \\(A \\boxright B\\) as saying\nthat B would be true, if A were made true by an\nintervention. The language of structural counterfactuals does not\nallow the connective ‘\\(A \\boxright B\\)’ to appear in the antecedents of\ncounterfactuals. More precisely, we define well-formed formulas\n(wffs) for the language inductively: \nThis means, for example, that \\(A \\boxright (B\\boxright (C\\boxright D))\\)\nis a wff, but \\(A\\boxright ((B\\boxright C)\\boxright D)\\) is not,\nsince the embedded counterfactual in the consequent does not have a\nBoolean proposition as an antecedent. \nConsider the world of the gas grill, described in the previous\nsection: \nTo evaluate the counterfactual \\({\\textit{Flame} = 1} \\boxright\n{\\textit{Meat cooked} = 1}\\) (if the flame had been set to\nlow, the meat would have been cooked rare), we replace the\nequation for Flame with the assignment Flame = 1. We\ncan then compute that Meat cooked = 1; the counterfactual is\ntrue. If the antecedent is a conjunction of atomic propositions, such\nas Flame = 1 and Igniter = 0, we replace all of the\nrelevant equations. A special case occurs when the antecedent conjoins\natomic propositions that assign different values to the same variable,\nsuch as Flame = 1 and Flame = 2. In this case, the\nantecedent is a contradiction, and the counterfactual is considered\ntrivially true. \nIf the antecedent is a disjunction of atomic propositions, or a\ndisjunction of conjunctions of atomic propositions, then the\nconsequent must be true when every possible intervention or\nset of interventions described by the antecedent is performed.\nConsider, for instance,  \nIf we perform the first intervention, we compute that Meat\ncooked = 1, so the consequent is true. However, if we perform the\nsecond intervention, we compute that Meat cooked = 0. Hence\nthe counterfactual comes out false. Some negations are treated as\ndisjunctions for this purpose. For example, \\({\\sim}(\\textit{Flame} =\n1)\\) would be treated in the same way as the disjunction \nIf the consequent contains a counterfactual, we iterate the procedure.\nConsider the counterfactual:  \nTo evaluate this counterfactual, we first change the equation for\nFlame to Flame = 1. Then we change the equation for\nGas level to Gas level = 0. Then we change the\nequation for Flame again, from Flame = 1, to\nFlame = 2. Finally, we compute that Meat cooked = 2,\nso the counterfactual comes out false. Unlike the case where\nFlame = 1 and Flame = 2 are conjoined in the\nantecedent, the two different assignments for Flame do not\ngenerate an impossible antecedent. In this case, the interventions are\nperformed in a specified order: Flame is first set to 1, and\nthen set to 2. \nThe differences between structural counterfactuals and Stalnaker-Lewis\ncounterfactuals stem from the following two features of structural\ncounterfactuals: \nThe truth values of counterfactuals are determined solely by the\ncausal structures of worlds, together with the interventions specified\nin the their antecedents. No further considerations of\nsimilarity play a role. For example, the counterfactual  \nwould be false in our gas grill world (and indeed in all possible\nworlds). We do not reason that a world in which Flame = 2 is\ncloser to our world (in which Flame = 3) than a\nworld in which Flame = 1. \nThese features of structural counterfactuals lead to some unusual\nproperties in the full language developed by Briggs (2012): \nTo handle the second kind of case, Briggs (2012) defines a relation of\nexact equivalence among Boolean propositions using\nthe state space semantics of Fine (2012). Within a world, the state\nthat makes a proposition true is the collection of values of variables\nthat contribute to the truth of the proposition. In our example world,\nthe state that makes Gas level = 3 true is the valuation\nGas level = 3. By contrast, the state that makes  \ntrue includes both Gas level = 3 and Flame = 3.\nPropositions are exactly equivalent if they are made true by the same\nstates in all possible worlds. The truth value of a counterfactual is\npreserved when exactly equivalent propositions are substituted into\nthe antecedent. \nBriggs (2012) provides a sound and complete axiomatization for\nstructural counterfactuals in acyclic SEMs. The axioms and inference\nrules of this system are presented in\n Supplement on Briggs’ Axiomatization. \nMany philosophers and legal theorists have been interested in the\nrelation of actual causation. This concerns the assignment of\ncausal responsibility for some event that occurs, based on how events\nactually play out. For example, suppose that Billy and Suzy are both\nholding rocks. Suzy throws her rock at a window, but Billy does not.\nSuzy’s rock hits the window, which breaks. Then Suzy’s\nthrow was the actual cause of the window breaking. \nWe can represent this story easily enough with a structural equation\nmodel. For variables, we choose: \nOur context and equation will then be: \nThe equation for W tells us that the window would shatter if\neither Billy or Suzy throws their rock. The corresponding DAG is shown\nin\n Figure 4\n  \nFigure 4 \nBut we cannot simply read off the the relation of actual causation\nfrom the graph or from the equations. For example, the arrow from\nB to W in\n Figure 4\n cannot be interpreted as saying that Billy’s (in)action is an\nactual cause of the window breaking. Note that while it is common to\ndistinguish between singular or token causation, and general or\ntype-level causation (see, e.g., Eells 1991, Introduction), that is\nnot what is at issue here. Our causal model does not represent any\nkind of causal generalization: it represents the actual and possible\nactions of Billy and Suzy at one particular place and time. Actual\ncausation is not just causal structure of the single case. A further\ncriterion for actual causation, defined in terms of the causal\nstructure together with the actual values of the variables, is\nneeded. \nFollowing Lewis (1973a), it is natural to try to analyze the relation\nof actual causation in terms of counterfactual dependence. In\nour model, the following propositions are all true: \nIn words: Suzy threw her rock, the window shattered, and if Suzy\nhadn’t thrown her rock, the window wouldn’t have\nshattered. In general, we might attempt to analyze actual causation as\nfollows: \n\\(X = x\\) is an actual cause of \\(Y = y\\) in world w just in\ncase: \nUnfortunately, this simple analysis will not work, for familiar\nreasons involving preemption and overdetermination.\nHere is an illustration of each: \nPreemption: Billy decides that he will give Suzy the\nopportunity to throw first. If Suzy throws her rock, he will not\nthrow, but if she doesn’t throw, he will throw and his rock will\nshatter the window. In fact, Suzy throws her rock, which shatters the\nwindow. Billy does not throw. \nOverdetermination: Billy and Suzy throw their rocks\nsimultaneously. Their rocks hit the window at the same time,\nshattering it. Either rock by itself would have been sufficient to\nshatter the window. \nIn both of these cases, Suzy’s throw is an actual cause\nof the window’s shattering, but the shattering does not\ncounterfactually depend upon her throw: if Suzy hadn’t thrown\nher rock, Billy’s rock would have shattered the window. Much of\nthe work on counterfactual theories of causation since 1973 has been\ndevoted to addressing these problems. \nA number of authors have used SEMs to try to formulate adequate\nanalyses of actual causation in terms of counterfactuals, including\nBeckers & Vennekens (2018), Glymour & Wimberly (2007), Halpern\n(2016), Halpern & Pearl (2001, 2005), Hitchcock (2001), Pearl\n(2009: Chapter 10), Weslake (forthcoming), and Woodward (2003: Chapter\n2). As an illustration, consider one analysis based closely on\na proposal presented in Halpern (2016): \n(AC) \\(X = x\\) is an actual cause of \\(Y = y\\) in\nworld w just in case: \nThat is, X belongs to a minimal set of variables \\(\\bX\\), such\nthat when we intervene to hold the variables in \\(\\bZ\\) fixed at the\nvalues they actually take in w, Y counterfactually\ndepends upon the values of the variables in \\(\\bX.\\) We will\nillustrate this account with our examples of preemption and\noverdetermination. \nIn Preemption, let the variables B, S, and\nW be defined as above. Our context and equations are: \nThat is: Suzy throws her rock; Billy will throw his rock if Suzy\ndoesn’t; and the window will shatter if either throws their\nrock. The DAG is shown in\n Figure 5. \nFigure 5 \nWe want to show that \\(S = 1\\) is an actual cause of \\(W = 1\\).\nConditions AC(1) and AC(2) are clearly satisfied. For condition AC(3),\nwe choose \\(\\bX = \\{S\\}\\) and \\(\\bZ = \\{B\\}\\). Since \\(B = 0\\) in\nPreemption, we want to fix \\(B = 0\\) while varying S.\nWe can see easily that \\({S = 0} \\amp {B = 0} \\boxright {W = 0}\\):\nreplacing the two equations for B and S with \\(B = 0\\)\nand \\(S = 0\\), the solution yields \\(W = 0\\). In words, this\ncounterfactual says that if neither Billy nor Suzy had thrown their\nrock, the window would not have shattered. Thus condition AC(3a) is\nsatisfied. AC(3b) is satisfied trivially, since \\(\\bX = \\{S\\}\\) is a\nsingleton set. \nHere is how AC works in this example. S influences W\nalong two different paths: the direct path \\(S \\rightarrow W\\) and the\nindirect path \\(S \\rightarrow B \\rightarrow W\\). These two paths\ninteract in such a way that they cancel each other out, and the value\nof S makes no net difference to the value of W. However,\nby holding B fixed at its actual value of 0, we eliminate the\ninfluence of S on W along that path. The result is that\nwe isolate the contribution that S made to W along the\ndirect path. AC defines actual causation as a particular kind of\npath-specific effect. \nTo treat Overdetermination, let B, S, and\nW keep the same meanings. Our setting and equation will be: \nThe graph is the same as that shown in\n Figure 4\n above. Again, we want to show that \\(S = 1\\) is an actual cause of\n\\(W = 1\\). Conditions AC(1) and AC(2) are obviously satisfied. For\nAC(3), we choose \\(\\bX = \\{B, S\\}\\) and \\(\\bZ = \\varnothing\\). For\ncondition AC(3a), we choose for our alternate setting \\(\\bX = \\bx'\\) \\(B =\n0\\) and \\(S = 0\\). Once again, the counterfactual \\({S = 0} \\amp {B =\n0} \\boxright {W = 0}\\) is true. Now, for AC(3b) we must show that \\(\\bX\n= \\{B, S\\}\\) is minimal. It is easy to check that \\(\\{B\\}\\) alone\nwon’t satisfy AC(3a). Whether we take \\(\\bZ = \\varnothing\\) or\n\\(\\bZ = \\{S\\}\\), changing B to 0 (perhaps while also setting\nS to 1) will not change the value of W. A parallel\nargument shows that \\(\\{S\\}\\) alone won’t satisfy AC(3a) either.\nThe key idea here is that S is a member of a minimal set of\nvariables that need to be changed in order to change the value of\nW. \nDespite these successes, none of the analyses of actual causation\ndeveloped so far perfectly captures our pre-theoretic intuitions in\nevery case. One strategy that has been pursued by a number of authors\nis to incorporate some distinction between default and\ndeviant values of variables, or between normal and\nabnormal conditions. See, e.g., Hall (2007), Halpern (2008;\n2016: Chapter 3), Halpern & Hitchcock (2015), Hitchcock (2007),\nand Menzies (2004). Blanchard & Schaffer (2017) present arguments\nagainst this approach. Glymour et al. (2010) raise a number of\nproblems for the project of trying to analyze actual causation. \nIn this section, we will discuss causal models that incorporate\nprobability in some way. Probability may be used to represent our\nuncertainty about the value of unobserved variables in a particular\ncase, or the distribution of variable values in a population. Often we\nare interested in when some feature of the causal structure of a\nsystem can be identified from the probability distribution\nover values of variables, perhaps in conjunction with background\nassumptions and other observations. For example, we may know the\nprobability distribution over a set of variables \\(\\bV\\), and want to\nknow which causal structures over the variables in \\(\\bV\\) are\ncompatible with the distribution. In realistic scientific cases, we\nnever directly observe the true probability distribution P over a set\nof variables. Rather, we observe finite data that approximate the true\nprobability when sample sizes are large enough and observation\nprotocols are well-designed. We will not address these important\npractical concerns here. Rather, our focus will be on what it is\npossible to infer from probabilities, in principle if not in practice.\nWe will also consider the application of probabilistic causal models\nto decision theory and counterfactuals. \nWe can introduce probability into a SEM by means of a probability\ndistribution over the exogenous variables. \nLet \\(\\bV = \\{X_1, X_2 ,\\ldots ,X_n\\}\\) be a set of endogenous\nvariables, and \\(\\bU = \\{U_1, U_2 ,\\ldots ,U_n\\}\\) a corresponding set\nof exogenous variables. Suppose that each endogenous variable \\(X_i\\)\nis a function of its parents in \\(\\bV\\) together with \\(U_i\\), that\nis: \nAs a general rule, our graphical representation of this SEM will\ninclude only the endogenous variables \\(\\bV\\), and we use\n\\(\\bPA(X_i)\\) to denote the set of endogenous parents of\n\\(X_i . U_i\\) is sometimes called an error variable for\n\\(X_i\\): it is responsible for any difference between the actual value\nof \\(X_i\\) and the value predicted on the basis of \\(\\bPA(X_i)\\)\nalone. We may think of \\(U_i\\) as encapsulating all of the causes of\n\\(X_i\\) that are not included in \\(\\bV\\). The assumption that each\nendogenous variable has exactly one error variable is innocuous. If\nnecessary, \\(U_i\\) can be a vector of variables. For example, if\n\\(Y_1\\), \\(Y_2\\), and \\(Y_3\\) are all causes of \\(X_i\\) that are not\nincluded in \\(\\bV\\), we can let \\(U_i = \\langle Y_1, Y_2,\nY_3\\rangle\\). Moreover, the error variables need not be distinct or\nindependent from one another. \nAssuming that the system of equations is acyclic, an assignment of\nvalues to the exogenous variables \\(U_1\\), \\(U_2\\),… ,\\(U_n\\)\nuniquely determines the values of all the variables in the model.\nThen, if we have a probability distribution \\(\\Pr'\\) over the values\nof variables in \\(\\bU\\), this will induce a unique probability\ndistribution P on \\(\\bV\\). \nSuppose we have a SEM with endogenous variables \\(\\bV\\), exogenous\nvariables \\(\\bU\\), probability distribution P on \\(\\bU\\) and \\(\\bV\\)\nas described in the previous section, and DAG \\(\\bG\\) representing the\ncausal structure on \\(\\bV\\). Pearl and Verma (1991) prove that if the\nerror variables \\(U_i\\) are probabilistically independent in P, then\nthe probability distribution on \\(\\bV\\) will satisfy the Markov\nCondition (MC) with respect to \\(\\bG\\). The Markov Condition has\nseveral formulations, which are equivalent when \\(\\bG\\) is a a DAG\n(Pearl 1988): \nLet us take some time to explain each of these formulations. \nMCScreening_off says that the parents of variable X\nscreen X off from all other variables, except for the\ndescendants of X. Given the values of the variables that are\nparents of X, the values of the variables in \\(\\bY\\) (which\nincludes no descendants of \\(X)\\), make no further difference to the\nprobability that X will take on any given value. \nMCFactorization tells us that once we know the conditional\nprobability distribution of each variable given its parents, \\(\\Pr(X_i\n\\mid \\bPA(X_i))\\), we can compute the complete joint distribution over\nall of the variables. It is relatively easy to see that MCFactorization follows from MCScreening_off. Since \\(\\bG\\)\nis acyclic, we may re-label the subscripts on the variables so that\nthey are ordered from ‘earlier’ to ‘later’,\nwith only earlier variables being ancestors of later ones. It follows\nfrom the probability calculus that \n\n\\[\\Pr(X_1, X_2 , \\ldots ,X_n)  = \\Pr(X_1) \\times \\Pr(X_2 \\mid X_1) \\times \\ldots \\times \\Pr(X_n \\mid X_1, X_2 , \\ldots ,X_{n-1})\\] \n\n (this is a version of\nthe theorem of total probability). For each term \\(\\Pr(X_i \\mid X_1,\nX_2 , \\ldots ,X_{i-1})\\), our ordering ensures that all of the parents\nof \\(X_i\\) will be included on the right hand side, and none of its\ndescendants will. MCScreening_off then tells us that we can\neliminate all of the terms from the right hand side except for the\nparents of \\(X_i\\). \nMCd-separation introduces the graphical notion of\nd-separation. As noted above, a path from X to Y\nis a sequence of variables \\(\\langle X = X_1 , \\ldots ,X_k =\nY\\rangle\\) such that for each \\(X_i\\), \\(X_{i+1}\\), there is either an\narrow from \\(X_i\\) to \\(X_{i+1}\\)or an arrow from \\(X_{i+1}\\) to\n\\(X_i\\) in \\(\\bG\\). A variable \\(X_i , 1 \\lt i \\lt k\\) is a\ncollider on the path just in case there is an arrow from\n\\(X_{i-1}\\) to \\(X_i\\) and from \\(X_{i+1}\\) to \\(X_i\\). In other\nwords, \\(X_i\\) is a collider just in case the arrows converge on\n\\(X_i\\) in the path. Let \\(\\bX, \\bY\\), and \\(\\bZ\\) be disjoint subsets\nof \\(\\bV\\). \\(\\bZ\\) d-separates \\(\\bX\\) and \\(\\bY\\) just in case every\npath \\(\\langle X_1 , \\ldots ,X_k\\rangle\\) from a variable in \\(\\bX\\)\nto a variable in \\(\\bY\\) contains at least one variable \\(X_i\\) such\nthat either: (i) \\(X_i\\) is a collider, and no descendant of \\(X_i\\)\n(including \\(X_i\\) itself) is in \\(\\bZ\\); or (ii) \\(X_i\\) is not a\ncollider, and \\(X_i\\) is in \\(\\bZ\\). Any path that meets this\ncondition is said to be blocked by \\(\\bZ\\). If \\(\\bZ\\) does\nnot d-separate \\(\\bX\\) and \\(\\bY\\), then \\(\\bX\\) and \\(\\bY\\)\nare d-connected by \\(\\bZ\\). \nNote that MC provides sufficient conditions for variables to be\nprobabilistically independent, conditional on others, but no necessary\ncondition. \nHere are some illustrations: \nFigure 6 \nIn\n Figure 6,\n MC implies that X screens Y off from all of the other\nvariables, and W screens Z off from all of the other\nvariables. This is most easily seen from MCScreening_off.\nW also screens T off from all of the other variables,\nwhich is most easily seen from MCd-separation.\nT does not necessarily screen Y off from Z (or\nindeed anything from anything). \nFigure 7 \nIn\n Figure 7,\n MC entails that X and Z will be unconditionally\nindependent, but not that they will be independent conditional on\nY. This is most easily seen from MCd-separation. \nLet \\(V_i\\) and \\(V_j\\) be two distinct variables in \\(\\bV\\), with\ncorresponding exogenous error variables \\(U_i\\) and \\(U_j\\),\nrepresenting causes of \\(V_i\\) and \\(V_j\\) that are excluded from the\n\\(\\bV\\). Suppose \\(V_i\\) and \\(V_j\\) share at least one common cause\nthat is excluded from \\(\\bV\\). In this case, we would not expect\n\\(U_i\\) and \\(U_j\\) to be probabilistically independent, and the\ntheorem of Pearl and Verma (1991) would not apply. In this case, the\ncausal relationship among the variables in \\(\\bV\\) would not be\nappropriately represented by a DAG, but would require an acyclic\ndirected mixed graph (ADMG) with a double-headed arrow connecting\n\\(V_i\\) and \\(V_j\\). We will discuss this kind of case in more detail\nin\n Section 4.6\n below. \nMC is not expected to hold for arbitrary sets of variables \\(\\bV\\),\neven when the DAG \\(\\bG\\) accurately represents the causal relations\namong those variables. For example, (MC) will typically fail in the\nfollowing kinds of case: \nBoth SGS (2000) and Pearl (2009) contain statements of a principle\ncalled the Causal Markov Condition (CMC). The statements are\nin fact quite different from one another. In Pearl’s\nformulation, (CMC) is just a statement of the mathematical theorem\ndescribed above: If each variable in \\(\\bV\\) is a deterministic\nproduct of its parents in \\(\\bV\\), together with an error term; and\nthe errors are probabilistically independent of each other; then the\nprobability distribution on \\(\\bV\\) will satisfy (MC) with respect to\nthe DAG \\(\\bG\\) representing the functional dependence relations among\nthe variables in \\(\\bV\\). Pearl interprets this result in the\nfollowing way: Macroscopic systems, he believes, are deterministic. In\npractice, however, we never have access to all of the causally\nrelevant variables affecting a macroscopic system. But if we include\nenough variables in our model so that the excluded variables are\nprobabilistically independent of one another, then our model will\nsatisfy the MC, and we will have a powerful set of analytic tools for\nstudying the system. Thus MC characterizes a point at which we have\nconstructed a useful approximation of the complete system. \nIn SGS (2000), the (CMC) has more the status of an empirical posit. If\n\\(\\bV\\) is set of macroscopic variables that are well-chosen, meaning\nthat they are free from the sorts of defects described above; \\(\\bG\\)\nis a DAG representing the causal structure on \\(\\bV\\); and P is the\nempirical probability distribution resulting from this causal\nstructure; then P can be expected to satisfy MC relative to \\(\\bG\\).\nThey defend this assumption in (at least) two ways: \nCartwright (1993, 2007: chapter 8) has argued that MC need not hold\nfor genuinely indeterministic systems. Hausman and Woodward (1999,\n2004) attempt to defend MC for indeterministic systems. \nA causal model that comprises a DAG and a probability distribution\nthat satisfies MC is called a causal Bayes net. \nThe MC states a sufficient condition but not a necessary condition for\nconditional probabilistic independence. As such, the MC by itself can\nnever entail that two variables are conditionally or unconditionally\ndependent. The Minimality and Faithfulness Conditions are two\nconditions that give necessary conditions for probabilistic\nindependence. (This is employing the terminology of Spirtes et\nal. (SGS 2000). Pearl (2009) contains a “Minimality\nCondition” that is slightly different from the one described\nhere.) \n(i) The Minimality Condition. Suppose that the DAG \\(\\bG\\) on\nvariable set \\(\\bV\\) satisfies MC with respect to the probability\ndistribution P. The Minimality Condition asserts that no sub-graph of\n\\(\\bG\\) over \\(\\bV\\) also satisfies the Markov Condition with respect\nto P. As an illustration, consider the variable set \\(\\{X, Y\\}\\), let\nthere be an arrow from X to Y, and suppose that X\nand Y are probabilistically independent of each other. This\ngraph would satisfy the MC with respect to P: none of the independence\nrelations mandated by the MC are absent (in fact, the MC mandates no\nindependence relations). But this graph would violate the Minimality\nCondition with respect to P, since the subgraph that omits the arrow\nfrom X to Y would also satisfy the MC. The Minimality\nCondition implies that if there is an arrow from X to Y,\nthen X makes a probabilistic difference for Y,\nconditional on the other parents of Y. In other words, if \\(\\bZ\n= \\bPA(Y) \\setminus \\{X\\}\\), there exist \\(\\bz\\), y, x,\n\\(x'\\) such that \\(\\Pr(Y = y \\mid X = x' \\amp \\bZ = \\bz) \\ne \\Pr(Y = y\n\\mid X = x' \\amp \\bZ = \\bz)\\). \n(ii) The Faithfulness Condition. The Faithfulness Condition\n(FC) is the converse of the Markov Condition: it says that all of the\n(conditional and unconditional) probabilistic independencies that\nexist among the variables in \\(\\bV\\) are required by the MC.\nFor example, suppose that \\(\\bV = \\{X, Y, Z\\}\\). Suppose also that\nX and Z are unconditionally independent of one another,\nbut dependent, conditional upon Y. (The other two variable\npairs are dependent, both conditionally and unconditionally.) The\ngraph shown in\n Figure 8\n does not satisfy FC with respect to this distribution (colloquially,\nthe graph is not faithful to the distribution). MC, when applied to\nthe graph of Figure 8, does not imply the independence of X and\nZ. This can be seen by noting that X and Z are\nd-connected (by the empty set): neither the path \\(X\n\\rightarrow Z\\) nor \\(X \\rightarrow Y\\rightarrow Z\\) is blocked (by\nthe empty set). By contrast, the graph shown in\n Figure 7\n above is faithful to the described distribution. Note that Figure 8\ndoes satisfy the Minimality Condition with respect to the\ndistribution; no subgraph satisfies MC with respect to the described\ndistribution. In fact, FC is strictly stronger than the Minimality\nCondition. \nFigure 8 \nHere are some other examples: In\n Figure 6\n above, there is a path \\(W\\rightarrow X\\rightarrow Y\\); FC implies\nthat W and Y should be probabilistically dependent. In\n Figure 7,\n FC implies that X and Z should be dependent,\nconditional on Y. \nFC can fail if the probabilistic parameters in a causal model are just\nso. In\n Figure 8,\n for example, X influences Z along two different\ndirected paths. If the effect of one path is to exactly undo the\ninfluence along the other path, then X and Z will be\nprobabilistically independent. If the underlying SEM is linear,\nSpirtes et al. (SGS 2000: Theorem 3.2) prove that the set of\nparameters for which Faithfulness is violated has Lebesgue measure 0.\nNonetheless, parameter values leading to violations of FC are\npossible, so FC does not seem plausible as a metaphysical or\nconceptual constraint upon the connection between causation and\nprobabilities. It is, rather, a methodological principle:\nGiven a distribution on \\(\\{X, Y, Z\\}\\) in which X and Z\nare independent, we should prefer the causal structure depicted in\n Figure 7\n to the one in Figure 8. This is not because Figure 8 is conclusively\nruled out by the distribution, but rather because it is preferable to\npostulate a causal structure that implies the independence of\nX and Z rather than one that is merely\nconsistent with independence. See Zhang and Spirtes 2016 for\ncomprehensive discussion of the role of FC. \nViolations of FC are often detectable in principle. For example,\nsuppose that the true causal structure is that shown in\n Figure 7,\n and that the probability distribution over X, Y, and\nZ exhibits all of the conditional independence relations\nrequired by MC. Suppose, moreover, that X and Z are\nindependent, conditional upon Y. This conditional independence\nrelation is not entailed by MC, so it constitutes a violation of FC.\nIt turns out that there is no DAG that is faithful to this probability\ndistribution. This tips us off that there is a violation of FC. While\nwe will not be able to infer the correct causal structure, we will at\nleast avoid inferring an incorrect one in this case. For details, see\nSteel 2006, Zhang & Spirtes 2008, and Zhang 2013b. \nResearchers have explored the consequences of adopting a variety of\nassumptions that are weaker than FC; see for example Ramsey et al.\n2006, Spirtes & Zhang 2014, and Zhalama et al. 2016. \nIf we have a set of variables \\(\\bV\\) and know the probability\ndistribution P on \\(\\bV\\), what can we infer about the causal\nstructure on \\(\\bV\\)? This epistemological question is closely related\nto the metaphysical question of whether it is possible to\nreduce causation to probability (as, e.g., Reichenbach 1956\nand Suppes 1970 proposed). \nPearl (1988: Chapter 3) proves the following theorem: \n(Identifiability with time-order)\nIf \nthen it will be possible to uniquely identify \\(\\bG\\) on the basis of\nP. \nIt is relatively easy to see why this holds. For each variable\n\\(X_i\\), its parents must come from among the variables with lower\ntime indices, call them \\(X_1 ,\\ldots ,X_{i-1}\\). Any variables in\nthis group that are not parents of \\(X_i\\) will be nondescendants of\n\\(X_i\\); hence they will be screened off from \\(X_i\\) by its parents\n(from MCScreening_off). Thus we can start with the\ndistributions \\(\\Pr(X_i\\mid X_1 ,\\ldots ,X_{i-1})\\), and then weed out\nany variables from the right hand side that make no difference to the\nprobability distribution over \\(X_i\\). By the Minimality Condition, we\nknow that the variables so weeded are not parents of \\(X_i\\). Those\nvariables that remain are the parents of \\(X_i\\) in \\(\\bG\\). \nIf we don’t have information about time ordering, or other\nsubstantive assumptions restricting the possible causal structures\namong the variables in \\(\\bV\\), then it will not always be possible to\nidentify the causal structure from probability alone. In general,\ngiven a probability distribution P on \\(\\bV\\), it is only possible to\nidentify a Markov equivalence class of causal structures.\nThis will be the set of all DAGs on \\(\\bV\\) that (together with MC)\nimply all and only the conditional independence relations contained in\nP. In other words, it will be the set of all DAGs \\(\\bG\\) such that P\nsatisfies MC and FC with respect to \\(\\bG\\). The PC algorithm\ndescribed by SGS (2000: 84–85) is one algorithm that generates\nthe Markov equivalence class for any probability distribution with a\nnon-empty Markov equivalence class. \nConsider two simple examples involving three variables \\(\\{X, Y,\nZ\\}\\). Suppose our probability distribution has the following\nproperties: \nThen the Markov equivalence class is: \nWe cannot determine from the probability distribution, together with\nMC and FC, which of these structures is correct. \nOn the other hand, suppose the probability distribution is as\nfollows: \nThen the Markov equivalence class is: \nThis is the only DAG relative to which the given probability\ndistribution satisfies MC and FC. \nSuppose we have a SEM with endogenous variables \\(\\bV\\) and exogenous\nvariables \\(\\bU\\), where each variable in \\(\\bV\\) is determined by an\nequation of the form: \nSuppose, moreover, that we have a probability distribution \\(\\Pr'\\) on\n\\(\\bU\\) in which all of the \\(U_i\\)s are independent. This will induce\na probability distribution P on \\(\\bV\\) that satisfies MC relative to\nthe correct causal DAG on \\(\\bV\\). In other words, our probabilistic\nSEM will generate a unique causal Bayes net. The methods described in\nthe previous section attempt to infer the underlying graph \\(\\bG\\)\nfrom relations of probabilistic dependence and independence. These\nmethods can do no better than identifying the Markov equivalence\nclass. Can we do better by making use of additional information about\nthe probability distribution P, beyond relations of dependence and\nindependence? \nThere is good news and there is bad news. First the bad news. If the\nvariables in \\(\\bV\\) are discrete, and we make no assumptions about\nthe form of the functions \\(f_i\\), then we can infer no more about the\nSEM than the Markov equivalence to which the graph belongs (Meek\n1995). \nMore bad news: If the variables in \\(\\bV\\) are continuous, the\nsimplest assumption, and the one that has been studied in most detail,\nis that the equations are linear with Gaussian\n(normal, or bell-shaped) errors. That is: \nIt turns out that with these assumptions, we can do no better than\ninferring the Markov equivalence class of the causal graph on \\(\\bV\\)\nfrom probabilistic dependence and independence (Geiger & Pearl\n1988). \nNow for the good news. There are fairly general assumptions that allow\nus to infer a good deal more. Here are some fairly simple cases: \n(LiNGaM) (Shimizu et al. 2006)\nIf: \nthen the correct DAG on \\(\\bV\\) can be uniquely determined by the\ninduced probability distribution P on \\(\\bV\\). \n(Non-linear additive) (Hoyer et al. 2009)\nAlmost all functions of the following form allow the correct DAG on\n\\(\\bV\\) to be uniquely determined by the induced probability\ndistribution P on \\(\\bV\\).: \nIn fact, this case can be generalized considerably: \nSee also Peters et al. (2017) for discussion. \nWhile there are specific assumptions behind these results, they are\nnonetheless remarkable. They entail, for example, that (given the\nassumptions of the theorems) knowing only the probability distribution\non two variables X and Y, we can infer whether X\ncauses Y or Y causes X. \nThe discussion so far has focused on the case where there are no\nlatent common causes of the variables in \\(\\bV\\), and the error\nvariables \\(U_i\\) can be expected to be probabilistically independent.\nAs we noted in\n Section 2.3\n above, we represent a latent common cause with a double-headed arrow.\nFor example, the acyclic directed mixed graph in\n Figure 9\n represents a latent common cause of X and Z. More\ngenerally, we can use an ADMG like Figure 9 to represent that the\nerror variables for X and Z are not probabilistically\nindependent. \nFigure 9 \nIf there are latent common causes, we expect MCScreening_off and MCFactorization to fail if we apply them in a\nnaïve way. In\n Figure 9,\n Y is the only parent of Z shown in the graph, and if we\ntry to apply MCScreening_off, it tells us that Y should\nscreen X off from Z. However, we would expect X\nand Z to be correlated, even when we condition on Y, due\nto the latent common cause. The problem is that the graph is missing a\nrelevant parent of Z, namely the omitted common cause. However,\nsuppose that the probability distribution on \\(\\{L, X, Y, Z\\}\\)\nsatisfies MC with respect to the DAG that includes L as a\ncommon cause of X and Z. Then it turns out that the\nprobability distribution will still satisfy MCd-separation with respect to the ADMG of Figure 9. A causal\nmodel incorporating an ADMG and probability distribution satisfying\nMCd-separation is called a semi-Markov\ncausal model (SMCM). \nIf we allow that the correct causal graph may be an ADMG, we can still\napply MCd-separation, and ask which graphs imply the\nsame sets of conditional independence relations. The Markov\nequivalence class will be larger than it was when we did not allow for\nlatent variables. For instance, suppose that the probability\ndistribution on \\(\\{X, Y, Z\\}\\) has the following features: \nWe saw in\n Section 4.4\n that the only DAG that implies just these (in)dependencies is: \nBut if we allow for the possibility of latent common causes, there\nwill be additional ADMGs that also imply just these (in)dependencies.\nFor example, the structure \nis also in the Markov equivalence class, as are several others. \nLatent variables present a further complication. Unlike the case where\nthe error variables \\(U_i\\) are probabilistically independent, a SEM\nwith correlated error terms may imply probabilistic constraints in\naddition to conditional (in)dependence relations, even in the absence\nof further assumptions about functional form. This means that we may\nbe able to rule out some of the ADMGs in the Markov equivalence class\nusing different kinds of probabilistic constraints. \nA conditional probability such as \\(\\Pr(Y = y \\mid X = x)\\) gives us\nthe probability that Y will take the value y, given that\nX has been observed to take the value x. Often,\nhowever, we are interested in predicting the value of Y that\nwill result if we intervene to set the value of X\nequal to some particular value x. Pearl (2009) writes \\(\\Pr(Y =\ny \\mid \\ido(X = x))\\) to characterize this probability. The notation\nis misleading, since \\(\\ido(X = x)\\) is not an event in the original\nprobability space. It might be more accurate to write \\(\\Pr_{\\ido(X =\nx)} (Y = y)\\), but we will use Pearl’s notation here. What is the\ndifference between observation and intervention? When we merely\nobserve the value that a variable takes, we are learning about the\nvalue of the variable when it is caused in the normal way, as\nrepresented in our causal model. Information about the value of the\nvariable will also provide us with information about its causes, and\nabout other effects of those causes. However, when we intervene, we\noverride the normal causal structure, forcing a variable to take a\nvalue it might not have taken if the system were left alone.\nGraphically, we can represent the effect of this intervention by\neliminating the arrows directed into the variable intervened upon.\nSuch an intervention is sometimes described as “breaking”\nthose arrows. As we saw in Section\n 3.1, in the\ncontext of a SEM, we represent an intervention that sets X to\nx by replacing the equation for X with a new one\nspecifying that \\(X = x\\). \nAs we saw in\n Section 3.2,\n there is a close connection between interventions and\ncounterfactuals; in particular, the antecedents of structural\ncounterfactuals are thought of as being realized by interventions.\nNonetheless, Pearl (2009) distinguishes claims about interventions\nrepresented by the do operator from counterfactuals. The former\nare understood in the indicative mood; they concern interventions that\nare actually performed. Counterfactuals are in the subjunctive mood,\nand concern hypothetical interventions. This leads to an important\nepistemological difference between ordinary interventions and\ncounterfactuals: they behave differently in the way that they interact\nwith observations of the values of variables. In the case of\ninterventions, we are concerned with evaluating probabilities such\nas \nWe assume that the intervention \\(\\ido(\\bZ = \\bz)\\) is being performed\nin the actual world, and hence that we are observing the values that\nother variables take \\((\\bX =\\bx)\\) in the same world where the\nintervention takes place. In the case of counterfactuals, we observe\nthe value of various variables in the actual world, in which there is\nno intervention. We then ask what would have happened if an\nintervention had been performed. The variables whose values\nwe observed may well take on different values in the\nhypothetical world where the intervention takes place. Here is a\nsimple illustration of the difference. Suppose that we have a causal\nmodel in which treatment with a drug causes recovery from a disease.\nThere may be other variables and causal relations among them as\nwell. \nIntervention: \nCounterfactual:  \nWe will discuss interventions in the present section, and\ncounterfactuals in\n Section 4.10\n below. \nSuppose that we have an acyclic structural equation model with\nexogenous variables \\(\\bU\\) and endogenous variables \\(\\bV\\). We have\nequations of the form \nand a probability distribution \\(\\Pr'\\) on the exogenous variables\n\\(\\bU\\). \\(\\Pr'\\) then induces a probability distribution P on\n\\(\\bV\\). To represent an intervention that sets \\(X_k\\) to \\(x_k\\), we\nreplace the equation for \\(X_k\\) with \\(X_k = x_k\\). Now \\(\\Pr'\\)\ninduces a new probability distribution P* on \\(\\bV\\) (since settings\nof the exogenous variables \\(\\bU\\) give rise to different values of\nthe variables in \\(\\bV\\) after the intervention). P* is the new\nprobability distribution that Pearl writes as \\(\\Pr(• \\mid \\ido(X_k\n= x_k))\\). \nBut even if we do not have a complete SEM, we can often compute the\neffect of interventions. Suppose we have a causal model in which the\nprobability distribution P satisfies MC on the causal DAG \\(\\bG\\) over\nthe variable set \\(\\bV = \\{X_1, X_2 ,\\ldots ,X_n\\}\\). The most useful\nversion of MC for thinking about interventions is MCFactorization (see\n Section 4.2),\n which tells us: \nNow suppose that we intervene by setting the value of \\(X_k\\) to\n\\(x_k\\). The post-intervention probability P* is the result of\naltering the factorization as follows: \nwhere \\(\\Pr'(X_k = x_k) = 1\\). The conditional probabilities of the\nform \\(\\Pr(X_i \\mid \\bPA(X_i))\\) for \\(i \\ne k\\) remain unchanged by\nthe intervention. This gives the same result as computing the result\nof an intervention using a SEM, when the latter is available. This\nresult can be generalized to the case where the intervention imposes a\nprobability distribution \\(\\Pr^{\\dagger}\\) on some subset of the\nvariables in \\(\\bV\\). For simplicity, let’s re-label the\nvariables so that \\(\\{X_1, X_2 ,\\ldots ,X_k\\}\\) is the set of\nvariables that we intervene on. Then, the post-intervention\nprobability distribution is: \nThe Manipulation Theorem of SGS (2000: theorem 3.6)\ngeneralizes this formula to cover a much broader class of\ninterventions, including ones that don’t break all the arrows\ninto the variables that are intervened on. \nPearl (2009: Chapter 3) develops an axiomatic system he calls the\ndo-calculus for computing post-intervention probabilities\nthat can be applied to systems with latent variables, where the causal\nstructure on \\(\\bV\\) is represented by an ADMG (including\ndouble-headed arrows) instead of a DAG. The axioms of this system are\npresented in\n Supplement on the do-calculus.\n One useful special case is given by the \nBack-Door Criterion. Let X and Y be variables\nin \\(\\bV\\), and \\(\\bZ \\subseteq \\bV \\setminus \\{X, Y\\}\\) such\nthat: \nthen \\(\\Pr(Y \\mid \\ido(X), \\bZ) = \\Pr(Y \\mid X, \\bZ)\\). \nThat is, if we can find an appropriate conditioning set \\(\\bZ\\), the\nprobability resulting from an intervention on X will be the\nsame as the conditional probability corresponding to an observation of\nX. \nEvidential Decision Theory of the sort developed by Jeffrey (1983),\nruns into well-known problems in variants of Newcomb’s\nproblem (Nozick 1969). For example, suppose Cheryl believes the\nfollowing: She periodically suffers from a potassium deficiency. This\nstate produces two effects with high probability: It causes her to eat\nbananas, which she enjoys; and it causes her to suffer debilitating\nmigraines. On days when she suffers from the potassium deficiency, she\nhas no introspective access to this state. In particular, she is not\naware of any banana cravings. Perhaps she rushes to work every\nmorning, grabbing whatever is at hand to eat on her commute.\nCheryl’s causal model is represented by the DAG in\n Figure 10. \n \nFigure 10 \n\\(K = 1\\) represents potassium deficiency, \\(B = 1\\) eating a banana,\nand \\(M = 1\\) migraine. Her probabilities are as follows: \nHer utility for the state of the world \\(w \\equiv \\{K = k, B = b, M =\nm\\}\\) is \\(\\Ur(w) = b - 20m\\). That is, she gains one unit of utility\nfor eating a banana, but loses 20 units for suffering a migraine. She\nassigns no intrinsic value to the potassium deficiency. \nCheryl is about to leave for work. Should she eat a banana? According\nto Evidential Decision Theory (EDT), Cheryl should maximize\nEvidential Expected Utility, where \nFrom the probabilities given, we can compute that: \nEating a banana is strongly correlated with migraine, due to the\ncommon cause. Thus \nSo EDT, at least in its simplest form, recommends abstaining from\nbananas. Although Cheryl enjoys them, they provide strong evidence\nthat she will suffer from a migraine. \nMany think that this is bad advice. Eating a banana does not\ncause Cheryl to get a migraine; it is a harmless pleasure. A\nnumber of authors have formulated versions of Causal Decision\nTheory (CDT) that aim to incorporate explicitly causal\nconsiderations (e.g., Gibbard & Harper 1978; Joyce 1999; Lewis\n1981; Skyrms 1980). Causal models provide a natural setting for CDT,\nan idea proposed by Meek and Glymour (1994) and developed by Hitchcock\n(2016), Pearl (2009: Chapter 4) and Stern (2017). The central idea is\nthat the agent should treat her action as an intervention.\nThis means that Cheryl should maximize her Causal Expected\nUtility: \nNow we can compute \nSo that now \nThis yields the plausible result that eating a banana gives Cheryl a\nfree unit of utility. By intervening, Cheryl breaks the arrow from\nK to B and destroys the correlation between eating a\nbanana and suffering a migraine. \nMore generally, one can use the methods for calculating the effects of\ninterventions described in the previous section to compute the\nprobabilities needed to calculate Causal Expected Utility. Stern\n(2017) expands this approach to allow for agents who distribute their\ncredence over multiple causal models. Hitchcock (2016) shows how the\ndistinction between interventions and counterfactuals, discussed in\nmore detail in\n Section 4.10\n below, can be used to deflect a number of alleged counterexamples to\nCDT. \nThere is much more that can be said about the debate between EDT and\nCDT. For instance, if Cheryl knows that she is intervening, then she\nwill not believe herself to be accurately described by the causal\nstructure in\n Figure 10.\n Instead, she will believe herself to instantiate a causal structure\nin which the arrow from K to B is removed. In this\ncausal structure, if P satisfies MC, we will have \\(\\Pr(w \\mid B = b)\n= \\Pr(w \\mid \\ido(B = b))\\), and the difference between EDT and CDT\ncollapses. If there is a principled reason why a deliberating agent\nwill always believe herself to be intervening, then EDT will yield the\nsame normative recommendations as CDT, and will avoid counterexamples\nlike the one described above. Price’s defense of EDT (Price\n1986) might be plausibly reconstructed along these lines. So the moral\nis not necessarily that CDT is normatively correct, but rather that\ncausal models may be fruitfully employed to clarify issues in decision\ntheory connected with causation. \nIn the previous section, we discussed how to use knowledge (or\nassumptions) about the structure of a causal graph \\(\\bG\\) to make\ninferences about the results of interventions. In this section, we\nexplore the converse problem. If we can intervene on variables and\nobserve the post-intervention probability distribution, what can we\ninfer about the underlying causal structure? This topic has been\nexplored extensively in the work of Eberhardt and his collaborators.\n(See, for example, Eberhardt & Scheines 2007 and Hyttinen et al.\n2013a.) Unsurprisingly, we can learn more about causal structure if we\ncan perform interventions than if we can only make passive\nobservations. However, just how much we can infer depends upon what\nkinds of interventions we can perform, and on what background\nassumptions we make. \nIf there are no latent common causes, so that the true causal\nstructure on \\(\\bV\\)\nis represented by a DAG \\(\\bG\\), then it will always be possible to discover\nthe complete causal structure using interventions. If we can only\nintervene on one variable at a time, we may need to separately\nintervene on all but one of the variables before the causal structure\nis uniquely identified. If we can intervene on multiple variables at\nthe same time, we can discover the true causal structure more\nquickly. \nIf there are latent common causes, so that the true causal structure\non \\(\\bV\\) is represented by an ADMG, then it may not be possible to\ndiscover the true causal structure using only single-variable\ninterventions.  (Although we can do this in the special case where the\nfunctions in the underlying structural equation model are all linear.)\nHowever, if we can intervene on multiple variables at the same time,\nthen it is possible to discover the true causal graph. \nEberhardt and collaborators have also explored causal discovery using\nsoft interventions. A soft intervention influences the value\nof a variable without breaking the arrows into that variable. For\ninstance, suppose we want to know whether increasing the income of\nparolees will lead to decreased recidivism. We randomly divide\nsubjects into treatment and control conditions, and give regular cash\npayments to those in the treatment condition. This is not an\nintervention on income per se, since income will still be\ninfluenced by usual factors: savings and investments, job training,\nhelp from family members, and so on. Soft interventions facilitate\ncausal inference because they create colliders, and as we have seen,\ncolliders have a distinct probabilistic signature. Counterintuitively,\nthis means that if we want to determine whether X causes\nY it is desirable to perform a soft intervention on Y\n(rather than X), to see if we can create a collider\n\\(I\\rightarrow Y\\leftarrow X\\) (where I is the intervention).\nSoft interventions are closely related to instrumental\nvariables. If there are no latent common causes, we can infer the\ntrue causal structure using soft interventions. Indeed, if we can\nintervene on every variable at once, we can determine the correct\ncausal structure from this one intervention. However, if there are\nlatent common causes, it is not in general possible to discover the\ncomplete causal structure using soft interventions. (Although this can\nbe done if we assume linearity.) \n Section 3.3\n above discussed counterfactuals in the context of deterministic\ncausal models. The introduction of probability adds a number of\ncomplications. In particular, we can now talk meaningfully about the\nprobability of a counterfactual being true. Counterfactuals play a\ncentral role in the potential outcome framework for causal\nmodels pioneered by Neyman (1923), and developed by Rubin (1974) and\nRobins (1986), among others. \nCounterfactuals in the potential outcome framework interact with\nprobability differently than counterfactuals in Lewis’s (1973b)\nframework. Suppose that Ted was exposed to asbestos and developed lung\ncancer. We are interested in the counterfactual: “If Ted had not\nbeen exposed to asbestos, he would not have developed lung\ncancer”. Suppose that the processes by which cancer develops are\ngenuinely indeterministic. Then, it seems wrong to say that if Ted had\nnot been exposed to asbestos, he definitely would have developed lung\ncancer; and it seems equally wrong to say that he definitely would not\nhave developed lung cancer. In this case, Lewis would say that the\ncounterfactual “If Ted had not been exposed to asbestos, he\nwould not have developed lung cancer” is determinately\nfalse. As a result, the objective probability of this\ncounterfactual being true is zero. On the other hand, a counterfactual\nwith objective probability in the consequent may be true:\n“If Ted had not been exposed to asbestos, his objective chance\nof developing lung cancer would have been .06”. By contrast, in\nthe potential outcome framework, probability may be pulled out of the\nconsequent and applied to the counterfactual as a whole: The\nprobability of the counterfactual “If Ted had not been exposed\nto asbestos, he would have developed lung cancer” can be\n.06. \nIf we have a complete structural equation model, we can assign\nprobabilities to counterfactuals, in light of observations. Let \\(\\bV\n= \\{X_1, X_2 ,\\ldots ,X_n\\}\\) be a set of endogenous variables, and\n\\(\\bU = \\{U_1, U_2 ,\\ldots ,U_n\\}\\) a set of exogenous variables. Our\nstructural equations have the form: \nWe have a probability distribution \\(\\Pr'\\) on \\(\\bU\\), which induces\na probability distribution P on \\(\\bU \\cup \\bV\\). Suppose that we\nobserve the value of some of the variables: \\(X_j = x_j\\) for all \\(j\n\\in \\bS \\subseteq \\{1,\\ldots ,n\\}\\). We now want to assess the\ncounterfactual “if \\(X_k\\) had been \\(x_k\\), then \\(X_l\\) would\nhave been \\(x_l\\)”, where k and l may be in\n\\(\\bS\\) but need not be. We can evaluate the probability of this\ncounterfactual using this three-step process: \nThis procedure differs from the procedure for interventions (discussed\nin\n Section 4.7)\n in that steps 1 and 2 have been reversed. We first update the\nprobability distribution, then perform the intervention. This reflects\nthe fact that the observations tell us about the actual world, in\nwhich the intervention did not (necessarily) occur. \nIf we do not have a complete SEM, it is not generally possible to\nidentify the probability of a counterfactual, but only to set upper\nand lower bounds. For example, suppose that we believe that asbestos\nexposure causes lung cancer, so that we posit a simple DAG: \nSuppose also that we have data for people similar to Ted which yields\nthe following probabilities: \n(We are oversimplifying, and treating asbestos and lung cancer as\nbinary variables.) We observe that Ted was in fact exposed to asbestos\nand did in fact develop lung cancer. What is the probability of the\ncounterfactual: “If Ted had not been exposed to asbestos, he\nwould not have developed lung cancer”? Pearl (2009) calls a\nprobability of this form a probability of necessity. It is\noften called the probability of causation, although this\nterminology is misleading for reasons discussed by Greenland and\nRobins (1988). This quantity is often of interest in tort law. Suppose\nthat Ted sues his employer for damages related to his lung cancer. He\nwould have to persuade a jury that his exposure to asbestos caused his\nlung cancer. American civil law requires a “more probable than\nnot” standard of proof, and it employs a “but for”\nor counterfactual definition of causation. Hence Ted must convince the\njury that it is more probable than not that he would not have\ndeveloped lung cancer if he had not been exposed. \nWe may divide the members of the population into four categories,\ndepending upon which counterfactuals are true of them: \nIt is easiest to think of the population as being divided into four\ncategories, with each person being one of these four types. However,\nwe do not need to assume that the process is deterministic; it may be\nthe case that each person only has a certain probability of falling\ninto one of these categories. \nMathematically, this is equivalent to the following. Let \\(U_L\\) be\nthe error variable for \\(L. U_L\\) takes values of the form \\((u_1,\nu_2)\\) with each \\(u_i\\) being 0 or 1. \\((1, 1)\\) corresponds to\ndoomed, \\((0, 0)\\) to immune, \\((1, 0)\\) to\nsensitive, and \\((0, 1)\\) to reverse. That is, the\nfirst element tells us what value L will take if an individual\nis exposed to asbestos, and the second element what value L\nwill take if an individual is not exposed. The equation for L\nwill be \\(L = (A \\times u_1) + ((1 - A) \\times u_2)\\). \nLet us assume that the distribution of the error variable \\(U_L\\) is\nindependent of asbestos exposure A. The observed probability of\nlung cancer is compatible with both of the following probability\ndistributions over our four counterfactual categories: \nMore generally, the observed probability is compatible with any\nprobability \\(\\Pr'\\) satisfying: \n\\(\\Pr_1\\) and \\(\\Pr_2\\) are just the most extreme cases. From the fact\nthat Ted was exposed to asbestos and developed lung cancer, we know\nthat he is either sensitive or doomed. The\ncounterfactual of interest will be true just in case he is\nsensitive. Hence the probability of the counterfactual, given\nthe available evidence, is P(sensitive | sensitive\nor doomed). However, using \\(\\Pr_1\\) yields a conditional\nprobability of .45 (5/11), while \\(\\Pr_2\\) yields a conditional\nprobability of 1. Given the information available to us, all we can\nconclude is that the probability of necessity is between .45 and 1. To\ndetermine the probability more precisely, we would need to know the\nprobability distribution of the error variable. \nA closely related counterfactual quantity is what Pearl (2009) calls\nthe probability of sufficiency. Suppose that Teresa, unlike\nTed, was not exposed to asbestos, and did not develop lung cancer. The\nprobability of sufficiency is the probability that she would\nhave suffered lung cancer if she had been exposed. That is,\nthe probability of sufficiency is the probability that if the cause\nwere added to a situation in which it and the effect was absent, it\nwould have resulted in the effect occurring. The probability of\nsufficiency is closely related to the quantity that Sheps (1958)\ncalled the relative difference, and that Cheng (1997) calls\nthe causal power. Cheng’s terminology reflects the idea\nthat the probability of sufficiency of C for E is the\npower of C to bring about E in cases where E is\nabsent. As in the case of the probability of necessity, if one does\nnot have a complete structural equation model, but only a Causal Bayes\nNet or Semi-Markov Causal Model, it is usually only possible to put\nupper and lower bounds on the probability of sufficiency. Using the\nprobabilities from the previous example, the probability of\nsufficiency of asbestos for lung cancer would be between .05 (5/94)\nand .12 (11/94). \nDetermining the probabilities of counterfactuals, even just upper and\nlower bounds, is computationally demanding. Balke and Pearl’s\ntwin network method (Balke and Pearl (1994a), (1994b); Pearl (2009,\npp. 213 - 215)) and Richardson and Robins’ split-node method\n(Richardson and Robins (2016)) are two methods that have been proposed\nfor solving this kind of problem. \nThe most important works surveyed in this entry are Pearl 2009 and\nSpirtes, Glymour, & Scheines 2000. Pearl 2010, Pearl et al. 2016,\nand Pearl & Mackenzie 2018 are three overviews of Pearl’s\nprogram. Pearl 2010 is the shortest, but the most technical. Pearl\n& Mackenzie 2018 is the least technical. Scheines 1997 and the\n“Introduction” of Glymour & Cooper 1999 are accessible\nintroductions to the SGS program. Eberhardt 2009, Hausman 1999,\nGlymour 2009, and Hitchcock 2009 are short overviews that cover some\nof the topics raised in this entry. \nThe entry on\n causation and manipulability\n contains extensive discussion of interventions, and some discussion\nof causal models. \nHalpern (2016) engages with many of the topics in\n Section 3.\n See also the entry for\n counterfactual theories of causation.\n\n \nThe entry on\n probabilistic causation\n is written by the same author as this entry, and contains some\noverlap. Some of the material from\n Section 4\n of this entry is also presented in Section 3 of that entry. That\nentry contains in addition some discussion of the connection between\nprobabilistic causal models and earlier probabilistic theories of\ncausation. \nEberhardt 2017 is a short survey that provides a clear introduction to\nmany of the topics covered in\n Sections 4.2\n through 4.6, as well as Section\n 4.9. Spirtes and\nZhang 2016 is a longer and more technical overview that covers much of\nthe same ground. It has particularly good coverage on the issues\nraised in\n Section 4.5. \nThe entries on\n decision theory\n and\n causal decision theory\n present more detailed background information about some of the issues\nraised in\n Section 4.8. \nThis entry has focused on topics that are likely to be of most\ninterest to philosophers. There are a number of important technical\nissues that have been largely ignored. Many of these address problems\nthat arise when various simplifying assumptions made here (such as\nacyclicity, and knowledge of the true probabilities) are\nrejected. Some of these issues are briefly surveyed along with\nreferences in\n Supplement on Further Topics in Causal Inference.","contact.mail":"cricky@caltech.edu","contact.domain":"caltech.edu"}]
