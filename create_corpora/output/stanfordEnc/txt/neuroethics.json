[{"date.published":"2016-02-10","date.changed":"2021-03-03","url":"https://plato.stanford.edu/entries/neuroethics/","author1":"Adina Roskies","author1.info":"http://www.dartmouth.edu/~adinar/Adinas_homepage/Homepage.html","entry":"neuroethics","body.text":"\n\n\nNeuroethics is an interdisciplinary field focusing on ethical\nissues raised by our increased and constantly improving understanding\nof the brain and our ability to monitor and influence it.\n\nNeuroethics focuses on ethical issues raised by our continually\nimproving understanding of the brain, and by consequent improvements\nin our ability to monitor and influence brain function. Significant\nattention to neuroethics can be traced to 2002, when the Dana\nFoundation organized a meeting of neuroscientists, ethicists, and\nother thinkers, entitled Neuroethics: Mapping the Field. A\nparticipant at that meeting, columnist and wordsmith William Safire,\nis often credited with introducing and establishing the meaning of the\nterm “neuroethics”, defining it as ‘the examination\nof what is right and wrong, good and bad about the treatment of,\nperfection of, or unwelcome invasion of and worrisome manipulation of\nthe human brain’ (Marcus, 2002, p.5). Others contend that the\nword “neuroethics” was in use prior to this (Illes, 2003;\nRacine, 2010), although all agree that these earlier uses did not\nemploy it in a disciplinary sense, or to refer to the entirety of the\nethical issues raised by neuroscience. \nAnother attendee at that initial meeting, Adina Roskies, in response\nto a perceived lack of recognition of the potential novelty of\nneuroethics, penned “Neuroethics for the new millennium”\n(Roskies, 2002), an article in which she proposed a bipartite division\nof neuroethics into the “ethics of neuroscience”, which\nencompasses the kinds of ethical issues raised by Safire, and\n“the neuroscience of ethics”, thus suggesting an extension\nof the scope of neuroethics to encompass our burgeoning understanding\nof the biological basis of ethical thought and behavior and the ways\nin which this could itself influence and inform our ethical thinking.\nThis broadening of the scope of neuroethics highlights the obvious and\nnot-so-obvious ways that understanding our own moral thinking might\naffect our moral views; it is one aspect of neuroethics that\ndistinguishes it from traditional bioethics. Another way of\ncharacterizing the field is as a study of ethical issues arising from\nwhat we can do to the brain (e.g. with neurotechnologies) and from\nwhat we know about it (including, for example, understanding the basis\nof ethical behavior). \nAlthough Roskies’ definition remains influential, it has been\nchallenged in various ways. Some have argued that neuroethics should\nnot be limited to the neuroscience of ethics, but rather be broadened\nto the cognitive science of ethics (Levy, personal communication),\nsince so much work that enables us to understand the brain takes place\nin disciplines outside of neuroscience, strictly defined. This is in\nfact in the spirit of the original proposal, since it has been widely\nrecognized that the brain sciences encompass a wide array of\ndisciplines, methods, and questions. However, the most persistent\ncriticisms have been from those who have questioned whether the\nneuroscience of ethics should be considered a part of neuroethics at\nall: they argue that understanding our ethical faculties is a\nscientific and not an ethical issue, and thus should not be part of\nneuroethics. This argument is usually followed by a denial that\nneuroethics is sufficiently distinct from traditional bioethics to\nwarrant being called a discipline in its own right. \nThe response to these critics is different: Whether or not these\nvarious branches of inquiry form a natural kind or are themselves a\nfocus of ethical analysis is quite beside the point. Neuroethics is\nporous. One cannot successfully engage with many of the ethical issues\nwithout also understanding the science. In addition, academic or\nintellectual disciplines are at least in part (if not entirely) social\nconstructs. And in this case the horse is out of the barn: It is clear\nthat interesting and significant work is being pursued regarding the\nbrain bases of ethical thought and behavior, and that this theoretical\nunderstanding has influenced, and has the potential to influence, our\nown thinking about ethics and our ethical practices. That neuroethics\nexists is undeniable: Neuroethical lines of research have borne\ninteresting fruit over the last 10–15 years; neuroethics is now\nrecognized as an area of study both nationally and internationally;\nneuroethics courses are taught at many universities; and training\nprograms, professional societies, and research centers for neuroethics\nhave already been established. The NIH BRAIN Initiative has devoted\nconsiderable resources to encouraging neuroscientific projects that\nincorporate neuroethical projects and analyses. Neuroethics is a\ndiscipline in its own right in part because we already structure our\npractices in ways that recognize it as such. What is most significant\nabout neuroethics is not whether both the ethics of neuroscience and\nthe neuroscience of ethics are given the same overarching disciplinary\nname, but that there are people working on both endeavors and that\nthey are in dialogue (and sometimes, the very same people do\nboth). \nOf course, to the extent that neuroethicists asks questions about\ndisease, treatment, and so on, the questions will look familiar, and\nfor answers they can and should look to extant work in traditional\nbioethics so as not to reinvent the wheel. But, ultimately, Farah is\ncorrect in saying that “New ethical issues are arising as\nneuroscience gives us unprecedented ways to understand the human mind\nand to predict, influence, and even control it. These issues lead us\nbeyond the boundaries of bioethics into the philosophy of mind,\npsychology, theology, law and neuroscience itself. It is this larger\nset of issues that has…earned it a name of its own”\n(Farah 2010, p. 2). \nNeuroethics is driven by neurotechnologies: it is concerned with the\nethical questions that attend the development and effects of novel\nneurotechnologies, as well as other ethical and philosophical issues\nthat arise from our growing understanding of how brains give rise to\nthe people that we are and the social structures that we inhabit and\ncreate. These questions are intimately bound up with scientific\nquestions about what kinds of knowledge can be acquired with\nparticular techniques: what are the scope and limits of what a\ntechnique can tell us? With many new techniques, answers to these\nquestions are obscure not only to the lay public, but often to the\nscientists themselves. The uncertainty about the reach of these\ntechnologies adds to the challenge of grappling with the ethical\nissues raised. \nMany new neurotechnologies enable us to monitor brain processes and\nincreasingly, to understand how the brain gives rise to certain\nbehaviors; others enable us to intervene in these processes, to change\nand perhaps to control behaviors, traits, or abilities. Although it\nwill be impossible to fully canvass the range of questions neuroethics\nhas thus far contemplated, discussion of the issues raised by a few\nneurotechnologies will allow me to illustrate the range of questions\nneuroethics entertains. The following is a not-exhaustive list of\ntopics that fall under the general rubric of neuroethics. \nWhile medicine’s traditional goal of treating illness is pursued\nby the development of drugs and other treatments that counteract the\ndetrimental effects of disease or insult, the same kinds of compounds\nand methods that are being developed to treat disease may also enhance\nnormal cognitive functioning. We already possess the ability to\nimprove some aspects of cognition above baseline, and will certainly\ndevelop other ways of doing so. Thus, a prominent topic in neuroethics\nis the ethics of neuroenhancement: What are the arguments for and\nagainst the use of neurotechnologies to enhance one’s\nbrain’s capacities and functioning? \nProponents of enhancement are sometimes called\n“transhumanists,” and opponents are identified as\n“bioconservatives”. These value-laden appellations may\nunnecessarily polarize a debate that need not pit extreme viewpoints\nagainst each other, and that offers many nuanced intermediate\npositions that recognize shared values (Parens, 2005) and make room\nfor embracing the benefits of enhancement while recognizing the need\nfor some type of regulation (e.g. Lin and Alhoff, 2008). The relevance\nof this debate itself depends to some extent upon a philosophical\nissue familiar to traditional bioethicists: the notorious difficulty\nof identifying the line between disease and normal function, and the\ncorresponding difference between treatment and enhancement. However,\ndespite the difficulty attending the principled drawing of this line,\nthere are already clear instances in which a technology such as a drug\nis used with the aim of improving a capacity or behavior that is by no\nmeans clinically dysfunctional, or with the goal of improving a\ncapacity beyond the range of normal functioning. One common example is\nthe use, now widespread on college campuses and beyond, of\nmethylphenidate, a stimulant typically prescribed for the treatment of\nADHD. Known by the brand name Ritalin, methylphenidate has been shown\nto improve performance on working memory, episodic memory and\ninhibitory control tasks. Many students use it as a study aid, and the\nethical standing of such off-label use is a focus of debate among\nneuroethicists (Sahakian, 2007; Greely et al., 2008). \nAs in the example above, the enhancements neuroethicists most often\ndiscuss are cognitive enhancements: technologies that allow\nnormal people to function cognitively at a higher level than they\nmight without use of the technology (Knafo and Venero, 2015). One\nstanding theoretical issue for neuroethics is a careful and precise\narticulation of whether, how and why cognitive enhancement has a\nphilosophical status different than any other kind of enhancement,\nsuch as enhancement of physical capacities by the use of steroids\n(Dresler, 2019). \nOften overlooked are other interesting potential neuroenhancements.\nThese are less frequently discussed than cognitive enhancements, but\njust as worthy of consideration. They include social/moral\nenhancements, such as the use of oxytocin to enhance pro-social\nbehavior, and other noncognitive but biological enhancements, such as\npotential physical performance enhancers controlled by brain-computer\ninterfaces (BCIs) (see, e.g. Savulescu and Persson, 2012; Douglas,\n2008; Dubljevíc and Racine, 2017; Annals of NYAC, 2004). In\nmany ways, discussions regarding these kinds of enhancement\neffectively recapitulate the cognitive enhancement debate, but in some\nrespects they raise different concerns and prompt different\narguments. \nNaturalness: Although the aim of cognitive enhancement may at first\nseem ethically questionable at best, it is plausible that humans\nnaturally engage in many forms of enhancement, including cognitive\nenhancement. Indeed, we typically applaud and value these efforts.\nAfter all, the aim of education is to cognitively enhance students\n(which, we now understand, occurs by changing their brains), and we\nlook askance at those who devalue this particular enhancement, rather\nthan at those who embrace it. So some kinds of cognitive enhancement\nare routine and unremarkable. Proponents of neuroenhancement will\nargue that there is no principled difference between the enhancements\nwe routinely engage in, and enhancement by use of drugs or other\nneurotechnologies. Many in fact argue that we are a species whose\nnature it is to develop and use technology for augmenting our\ncapacities, and that continual pursuit of enhancement is a mark of the\nhuman. \nCognitive liberty: Those who believe that “cognitive\nliberty” (see\n section\n 2.2 below) is a fundamental right argue that an important element of\nthe autonomy at stake in cognitive liberty is the liberty to determine\nfor ourselves what to do with our minds and to them, including\ncognitive enhancement, if we so choose. Although many who champion\n“cognitive liberty” do so in the context of a strident\npolitical libertarianism (e.g. Boire, 2001), one can recognize the\nvalue of cognitive liberty without swallowing an entire political\nagenda. So, for example, even if we think that there is a prima\nfacie right to determine our own cognitive states, there may be\njustifiable limits to that right. More work needs to be done to\nestablish the boundaries of the cognitive liberty we ought to\nsafeguard. \nUtilitarian arguments: Many proponents of cognitive enhancement point\nto the positive effects of enhancement and argue that the benefits\noutweigh the costs. In these utilitarian arguments it is important to\nconsider the positive and negative effects not only for individuals,\nbut also for society more broadly (see, e.g. Selgelid, 2007). \nDeontological arguments: Sometimes enhancements are argued to be an\navenue for leveling the playing field, in pursuit of fairness and\nequity. Such arguments are bolstered by the finding that at least for\nsome interventions, enhancement effects are greater for those who have\nlower baseline functioning than those starting with a higher baseline\n(President’s Commission on Bioethics, 2015). \nPractical arguments: These often point to the difficulty in enforcing\nregulations of extant technology, or the detrimental effects of trying\nto do so. They tend to be not really arguments in favor of\nenhancement, but rather reasons not to oppose its use. \nThere are a variety of arguments against enhancement. Most fall into\nthe following types: \nHarms: The simplest and most powerful argument against enhancement is\nthe claim that brain interventions carry with them the risk of harm,\nrisks that make the use of these interventions unacceptable. The low\nbar for acceptable risk is an effect of the context of enhancement:\nrisks deemed reasonable to incur when treating a deficiency or disease\nwith the potential benefit of restoring normal function may be deemed\nunreasonable when the payoff is simply augmenting performance above a\nnormal baseline. Some suggest that no risk is justified for\nenhancement purposes. In evaluating the strength of a harm-based\nargument against enhancement, several points should be considered: 1)\nWhat are the actual and potential harms and benefits (medical and\nsocial) of a given enhancement? 2) Who should make the judgments about\nappropriate tradeoffs? Different individuals may judge differently at\nwhat point the risk/benefit threshold occurs, and their judgments may\ndepend upon the precise natures of the risks and benefits. Notice,\ntoo, the harm argument is toothless against enhancements that\ndon’t pose any risks. \nUnnaturalness: A number of thinkers argue, in one form or another,\nthat use of drugs or technologies to enhance our capacities is\nunnatural, and the implication is that unnatural implies\nimmoral. Of course, to be a good argument, more reason has to\nbe given both for why it is unnatural (see an argument for\nnaturalness, above), and for why naturalness and morality align. Some\narguments suggest that manipulating our cognitive machinery amounts to\ntinkering with “God-given” capacities, and usurping the\nrole of God as creator can be easily understood as transgressive in a\nreligious-moral framework. Despite its appeal to religious\nconservatives, a neuroethicist may want to offer a more ecumenical or\nnaturalistic argument to support the link between unnatural\nand immoral, and will have to counter the claim, above,\nthat it is natural for humans to enhance themselves. \nDiminishing human agency: Another argument suggests that the effect of\nenhancement will be to diminish human agency by undermining the need\nfor real effort, and allowing for success with morally meaningless\nshortcuts. Human life will lose the value achieved by the process of\nstriving for a goal and will be belittled as a result (see, e.g.\nSchermer, 2008; Kass, 2003). Although this is a promising form of\nargument, more needs to be done to undergird the claims that effort is\nintrinsically valuable. Recent work suggests no general argument to\nthis effect is forthcoming (Douglas, 2019). After all, few find\ncompelling the argument that we ought to abandon transportation by car\nfor horses, walking, or bicycling, because these require more effort\nand thus have more moral value. \nThe hubris objection: This interesting argument holds that the type of\nattitude that seems to underlie pursuit of such interventions is\nmorally defective in some way, or is indicative of a morally defective\ncharacter trait. So, for example, Michael Sandel suggests that the\nattitude underlying the attempt to enhance ourselves is a\n“Promethean” attitude of mastery that overlooks or\nunderappreciates the “giftedness of human life.” It is the\nexpression and indulgence of a problematic attitude of dominion toward\nlife to which Sandel primarily objects: “The moral problem with\nenhancement lies less in the perfection it seeks than in the human\ndisposition it expresses and promotes” (Sandel, 2002). Others\nhave pushed back against this tack, arguing that the hubris objection\nagainst enhancement is at base a religious one, or that it\nfundamentally misunderstands the concepts it relies upon (Kahane,\n2011). \nEquality and Distributive Justice: One question that routinely arises\nwith new technological advances is “who gets to benefit from\nthem?” As with other technologies, neuroenhancements are not\nfree. However, worries about access are compounded in the case of\nneuroenhancements (as they may also be with other learning\ntechnologies). As enhancements increase capacities of those who use\nthem, they are likely to further widen the already unconscionable gap\nbetween the haves and have-nots: We can foresee that those already\nwell-off enough to afford enhancements will use them to increase their\ncompetitive advantage against others, leaving further behind those who\ncannot afford them. Not all arguments in this vein militate against\nenhancement. For example, the finding mentioned above -- that at least\nwith some cognitive enhancement technologies, those who have lower\nbaseline functioning experience greater improvements than those\nstarting at a higher level -- could ground pro-enhancement fairness\nand equity arguments for leveling the playing field (President’s\nCommission on Bioethics, 2015). As public consciousness about racial\nand economic disparities increases, we should expect more neuroethical\nwork on this topic. Although one can imagine policy solutions to\ndistributive justice concerns, such as having enhancements covered by\nhealth insurance, having the state distribute them to those who cannot\nafford them, etc., widespread availability of neuroenhancements will\ninevitably raise questions about coercion. \nCoercion: The prospect of coercion is raised in several ways.\nObviously, if the state decides to mandate an enhancement, treating\nits beneficial effects as a public health issue, this is effectively\ncoercion. We see this currently in the backlash against vaccinations:\nthey are mandated with the aim of promoting public health, but in some\nminds the mandate raises concerns about individual liberty. I would\nsubmit that the vaccination case demonstrates that at least on some\noccasions coercion is justified. The question is whether coercion\ncould be justifiable for enhancement, rather than for harm prevention.\nAlthough some coercive ideas, such as the suggestion that we put\nProzac or other enhancers in the water supply, are unlikely to be\ntaken seriously as a policy issue (however, see Appel 2010 [2011]),\nless blatant forms of coercion are more realistic. For example, if\npeople immersed in tomorrow’s competitive environment are in the\ncompany of others who are reaping the benefits from cognitive\nenhancement, they may feel compelled to make use of the same\ntechniques just to remain competitive, even though they would rather\nnot use enhancements. The danger is that respecting the autonomy of\nsome may put pressure on the autonomy of others. \nThere is unlikely to be any categorical resolution of the ethics of\nenhancement debate. The details of a technology will be relevant to\ndetermining whether a technology ought to be made available for\nenhancement purposes: we ought to treat a highly enhancing technology\nthat causes no harm differently from one that provides some benefit at\nnoticeable cost. Moreover, the magnitude of some of the\nequality-related issues will depend upon empirical facts about the\ntechnologies. Are neurotechnologies equally effective for everyone? As\nmentioned, there is evidence that some known enhancers such as the\npsychostimulants are more effective for those with deficiencies than\nfor the unimpaired: studies suggest the beneficial effects of these\ndrugs are proportional to the degree to which a capacity is impaired\n(Hussain et al., 2011). Other reports claim that normal\nsubjects’ capacities are not actually enhanced by these drugs,\nand some aspects of functioning may actually be impaired (Mattay, et\nal., 2000; Ileva et al., 2013). If this is a widespread pattern, it\nmay alleviate some worries about distributive justice and\ncontributions to social and economic stratification, since people with\na deficit will benefit proportionately more than those using the drug\nfor enhancement purposes. Bear in mind, however, that biology is\nrarely that equitable, and it would be surprising if this pattern\nturned out to be the norm. Since the technologies that could provide\nenhancements are extremely diverse, ranging from drugs to implants to\ngenetic manipulations, assessment of the risks and benefits and the\nway in which these technologies bear upon our conception of humanity\nwill have to be empirically grounded. \nFreedom is a cornerstone value in liberal democracies like our own,\nand one of the most cherished kinds of freedom is freedom of thought.\nThe main elements of freedom of thought, or “cognitive\nliberty” as it is sometimes called (Sententia, 2013), include\nprivacy and autonomy. Both of these can be challenged by the new\ndevelopments in neuroscience. The value of, potential threat to, and\nways to protect these aspects of freedom are a concern for\nneuroethics. Several recent papers have posited novel rights in this\nrealm, such as rights to cognitive liberty, to mental privacy, to\nmental integrity, and to psychological continuity (Ienca and Andorno,\n2017), or to psychological integrity and mental self-determination\n(Bublitz, 2020). \nAs the framers of our constitution were well aware, freedom is\nintimately linked with privacy: even being monitored is considered\npotentially “chilling” to the kinds of freedoms our\nsociety aims to protect. One type of freedom that has been championed\nin American jurisprudence is “the right to be let alone”\n(Warren and Brandeis, 1890), to be free from government or other\nintrusion in our private lives. \nIn the past, mental privacy could be taken for granted: the\nfirst-person accessibility of the contents of consciousness ensured\nthat the contents of one’s mind remained hidden to the outside\nworld, until and unless they were voluntarily disclosed. Instead, the\nbattles for freedom of thought were waged at the borders where thought\nmeets the outside world -- in expression -- and were won with the\nFirst Amendment’s protections for those freedoms (note, however,\nthat these protections are only against government infringement). Over\nthe last half century, technological advances have eroded or impinged\nupon many traditional realms of worldly privacy. Most of the avenues\nfor expression can be (and increasingly are) monitored by third\nparties. It is tempting to think that the inner sanctum of the mind\nremains the last bastion of real privacy. \nThis may still be largely true, but even the privacy of the mind can\nno longer to be taken for granted. Our neuroscientific achievements\nhave already made significant headway in allowing others to discern\nsome aspects of our mental content through neurotechnologies.\nNoninvasive methods of brain imaging have revolutionized the study of\nhuman cognition and have dramatically altered the kinds of knowledge\nwe can acquire about people and their minds. Niether is the threat to\nmental privacy as simple as the naive claim that neuroimaging can read\nour thoughts, nor are the capabilities of imaging so innocuous and\nblunt that we needn’t worry about that possibility. A focus of\nneuroethics is to determine the real nature of the threat to mental\nprivacy, and to evaluate its ethical implications, many of which are\nrelevant to legal, medical, and other social issues (Shen, 2013). For\nexample, in a world in which the bastion of the mind may be lowering\nits drawbridges, do we need extra protections? Doing so effectively\nwill require both a solid understanding of the neuroscientific\ntechnologies and the neural bases of thought, as well as a sensitivity\nto the ethical problems raised by our growing knowledge and\never-more-powerful neurotechnologies. These dual necessities\nillustrate why neuroethicists must be trained both in neuroscience and\nin ethics. In what follows I briefly discuss the most relevant\nneurotechnology and its limitations and then canvas a few ways in\nwhich privacy may be infringed by it. \nOne of the most prominent neurotechnologies poised to pose a threat to\nprivacy is Magnetic Resonance Imaging, or MRI. MRI can provide both\nstructural and functional information about a person’s brain\nwith minimal risk and inconvenience. In general, MRI is a tool that\nallows researchers noninvasively to examine or monitor brain structure\nand activity, and to correlate that structure or function with\nbehavior. Structural or anatomical MRI provides high-resolution\nstructural images of the brain. While structural imaging in the\nbiosciences is not new, MRI provides much higher resolution and better\nability to differentiate tissues than prior techniques such as x-rays\nor CT scans. \nHowever, it is not structural but functional MRI (fMRI) that has\nrevolutionized the study of human cognition. fMRI provides information\nabout correlates of neuronal activity, from which neural activity can\nbe inferred. Recent advances in analysis methods for neuroimaging data\nsuch as multi-voxel pattern analysis and related techniques now allow\nrelatively fine-grained “decoding” of brain activity.\nDecoding involves probabilistic matching, using machine learning, of\nan observed pattern of brain activation with experimentally\nestablished correlations between activity patterns and some kind of\nfunctional variable, such as task, behavior, or content. The kind of\ninformation provided by functional imaging promises to provide\nimportant evidence useful for three goals: Decoding mental content,\ndiagnosis, and prediction. Neuroethical questions arise in all these\nareas. \nBefore discussing these issues, it is important to remember that\nneuroimaging is a technology that is subject to a number of\nsignificant limitations, and these technical issues limit how precise\nthe inferences can be. For example: \nWithout appreciating these technical issues and the resulting limits\nto what can legitimately be inferred from fMRI, one is likely to\noverestimate or mischaracterize the potential threat that it poses. In\nfact, much of the fear of mindreading expressed in non-scientific\npublications stems from a lack of understanding of the science\n(Racine, 2015). For example, there is no scientific basis to the worry\nthat imaging would enable the reading of mental content without our\nknowing it. Thus, fears that the government is able to remotely or\ncovertly monitor the thoughts of citizens are unfounded. \nNoninvasive ways of inferring neural activity have led many to worry\nthat mindreading is possible, not just in theory, but even now. Using\ndecoding techniques fMRI can be used, for example, to reconstruct a\nvisual stimulus from activity of the visual cortex while a subject is\nlooking at a scene or to determine whether a subject is looking at a\nfamiliar face, or hearing a particular sound. If mental content\nsupervenes on the physical structure and function of our brains, as\nmost philosophers and neuroscientists think it does, then in principle\nit should be possible to read minds by reading brains. Because of the\npotential to identify mental content, decoding raises issues about\nmental privacy. \nDespite the remarkable advances in brain imaging technology, however,\nwhen it comes to mental content, our current abilities to\n“mind-read” are relatively limited, but continually\nimproving (Roskies, 2015, 2020). Although some aspects of content can\nbe decoded from neural data, these tend to be quite general and\nnonpropositional in character. The ability to infer semantic meaning\nfrom ideation or visual stimulation tends to work best when the realm\nof possible contents are quite constrained. Our current abilities\nallow us to infer some semantic atoms, such as representations\ndenoting one of a prespecified set of concrete objects, but not\nunconstrained content, or entire propositions. Of course, future\nadvances might make worries about mindreading more pressing. For\nexample, if we develop robust means for decoding compositional\nmeaning, we may one day come to be able to decode propositional\nthought. \nStill, some worries are warranted. Even if neuroimaging is not at the\nstage where mindreading is possible, it can nonetheless threaten\naspects of privacy in ways that should give us pause. It is possible\nto identify individuals on the basis of their brain scans (Valizadeh\net al., 2018). In addition, neuroimaging can provide some insights\ninto attributes of people that they may not want known or disclosed.\nIn some cases, subjects may not even know that these attributes are\nbeing probed, thinking they are being scanned for other purposes. A\nwilling subject may not want certain things to be monitored. In what\nfollows, I consider a few of these more realistic worries. \nImplicit bias: Although explicitly acknowledged racial biases are\ndeclining, this may be due to a reporting bias attributable to the\nincreased negative social valuation of racial prejudice. Much\ncontemporary research now focuses on examining implicit racial biases,\nwhich are automatic or unconscious reflections of racial bias. With\nfMRI and EEG, it is possible to interrogate implicit biases, sometimes\nwithout the subject’s awareness that that is what is being\nmeasured (Checkroud, 2014)[3]. While there is disagreement about how\nbest to interpret implicit bias results (e.g., as a measure of\nperceived threat, as in-group/out-group distinctions, etc.), and what\nrelevance they have for behavior, the possibility that implicit biases\ncan be measured, either covertly or overtly, raises scientific and\nethical questions (Molenberghs and Louis, 2018). When ought this\ninformation be collected? What procedures must be followed for\nsubjects legitimately to consent to implicit measures? What\nsignificance should be attributed to evidence of biases? What kind of\nresponsibility should be attributed to people who hold them? What\npredictive power might they hold? Should they be used for practical\npurposes? One can imagine obvious but controversial potential uses for\nimplicit bias measures in legal situations, in employment contexts, in\neducation, and in policing, all areas in which concerns of social\njustice are significant. \nLie detection: Several neurotechnologies are being used to detect\ndeception or neural correlates of lying or concealing information in\nexperimental situations. For example, both fMRI measures and EEG\nanalysis techniques relying on the P300 signal have been used in the\nlaboratory to detect deception with varying levels of success. These\nmethods are subject to a variety of criticisms (Farah et al., 2014).\nFor example, almost all experimental studies fail to study\nreal lying or deception, but instead investigate some version\nof instructed misdirection. The context, tasks, and motivations differ\ngreatly between actual instances of lying and these experimental\nanalogs, calling into question the ecological validity of these\nexperimental techniques. Moreover, accuracy, though significantly\nhigher than chance, is far from perfect, and because of the inability\nto determine base rates of lying, error rates cannot be effectively\nassessed. Thus, we cannot establish their reliability for real-world\nuses. Finally, both physical and mental countermeasures decrease the\naccuracy of these methods (Hsu et al., 2019). Despite these\nlimitations, several companies have marketed neurotechnologies for\nthis purpose. \nCharacter traits: Neurotechnologies have shown some promise in\nidentifying or predicting aspects of personality or character. In an\ninteresting study aimed at determining how well neuroimaging could\ndetect lies, Greene and colleagues gave subjects in the fMRI scanner a\nprediction task in a game of chance that they could easily cheat on.\nBy using statistical analysis the researchers could identify a group\nof subjects who clearly cheated and others who did not (Greene and\nPaxton, 2009). Although they could not determine with neuroimaging on\nwhich trials subjects cheated, there were overall differences in brain\nactivation patterns between cheaters and those who played fair and\nwere at chance in their predictions. Moreover, Greene and colleagues\nrepeated this study at several months remove, and found that the\ncharacter trait of honesty or dishonesty was stable over time:\ncheaters the first time were likely to cheat (indeed, cheated even\nmore the second time), and honest players remained honest the second\ntime around. Also interesting was the fact that the brain patterns\nsuggested that cheaters had to activate their executive control\nsystems more than noncheaters, not only when they cheated, but also\nwhen deciding not to cheat. While the differential activations cannot\nbe linked specifically to the propensity to cheat rather than to the\nact of cheating, the work suggests that these task-related activation\npatterns may reflect correlates of trustworthiness. \nThe prospect of using methods for detecting these sorts of traits or\nbehaviors in real-world situations raises a host of thorny issues.\nWhat level of reliability should be required for their employment? In\nwhat circumstances should they be admissible as evidence in the\ncourtroom? For other purposes? Using lie detection or decoding\ntechniques from neuroscience in legal contexts may raise\nconstitutional concerns: Is brain imaging a search or seizure as\nprotected by the 4th Amendment? Would its forcible use be\nprecluded by 5th Amendment rights? These questions, though\ntroubling, might not be immediately pressing: in a landmark case\n(US v. Semrau, 2012) the court ruled that fMRI lie detection\nis inadmissible, given its current state of development. However, the\nopinion left open the possibility that it may be admissible in the\nfuture, if methods improve. Finally, to the extent that relevant\nactivation patterns may be found to correlate significantly with\nactivation patterns on other tasks, or with a task-free measure such\nas default-network activity, it raises the possibility that\ninformation about character could be inferred merely by scanning them\ndoing something innocuous, without their knowledge of the kind of\ninformation being sought. Thus, there are multiple dimensions to the\nthreat to privacy posed by imaging techniques. \nIncreasingly, neuroimaging information can bear upon diagnoses for\ndiseases, and in some instances may provide predictive information\nprior to the onset of symptoms. Work on the default network is\npromising for improving diagnosis in certain diseases without\nrequiring that subjects perform specific tasks in the scanner (Buckner\net al., 2008). For some diseases, such as in Alzheimer’s\ndisease, MRI promises to provide diagnostic information that\npreviously could only be established at autopsy (Liu et al., 2018).\nfMRI signatures have also been linked to a variety of psychiatric\ndiseases, although not yet with the reliability required for clinical\ndiagnosis (Aydin et al., 2019). Neuroethical issues also arise\nregarding ways to handle incidental findings, that is, evidence of\nunsymptomatic tumors or potentially benign abnormalities that appear\nin the course of scanning research subjects for non-medical purposes\n(Illes et al. 2006; Illes and Sahakian, 2011). \nThe ability to predict future functional deficits raises a host of\nissues, many of which have been previously addressed by genethics (the\nethics of genetics), since both provide information about future\ndisease risk. What may be different is that the diseases for which\nneurotechnologies are diagnostically useful are those that affect the\nbrain, and thus potentially mental competence, mood, personality, or\nsense of self. As such they may raise peculiarly neuroethical\nquestions (see below). \nAs discussed, decoding methods allow one to associate observed brain\nactivity with previously observed brain/behavior correlations. In\naddition, such methods can also be used to predict future behaviors,\ninsofar as these are correlated with observations of brain activity\npatterns. Some studies have already reported predictive power over\nupcoming decisions (Soon et al., 2008). Increasingly, we will see\nneuroscience or neuroimaging data that will give us some predictive\npower over longer-range future behaviors. For example, brain imaging\nmay allow us to predict the onset of psychiatric symptoms such as\npsychotic or depressive episodes. In cases in which this behavior is\nindicative of mental dysfunction it raises questions about stigma, but\nalso may allow more effective interventions. \nOne confusion regarding neuro prediction should be clarified\nimmediately: When neuroimages are said to “predict” future\nactivity, it means they provide some statistical information\nregarding likelihood. Prediction in this sense does not imply that the\npredicted behavior necessarily will come to pass; it does not mean a\nperson’s future is fated or determined. Although scientists\noccasionally make this mistake when discussing their results, the fact\nthat brain function or structure may give us some information about\nfuture behaviors should not be interpreted as a strong challenge to\nfree will. The prevalence of this mistake among both philosophers and\nscientists again illustrates the importance for neuroethicists of\nsophistication in both neuroscience and philosophy. \nPerhaps the most consequential and most ethically difficult potential\nuse of predictive information is in the criminal justice system. For\nexample, there is evidence that structural brain differences are\npredictive of scores on the PCL-R, a tool developed to diagnose\npsychopathy. It is also well-established that psychopaths have high\nrates of recidivism for violent offenses. Thus, in principle\nneuroimaging could be used to provide information about an\nindividual’s likelihood of recidivism. Indeed, brain information\nappears to offer some predictive value when combined with other\nfactors (Poldrack et al., 2019; Delfin et al., 2019). One cautionary\ntale comes from a recent exchange in the literature: A report\nsuggested that brain activity on a cognitive task predicts recidivism\n(Aharoni et al., 2013), but a critical reanalysis of the data suggests\nthat methodological concerns led to an overestimate of the predictive\nvalue of the neural data (Poldrack et al., 2019; Aharoni et al.,\n2014), highlighting the importance of technical expertise in assessing\nthe findings and for translating the results of scientific experiments\nfor practical purposes and ethical analysis. \nNeuroethical analysis here is essential. Should neural data be\nadmissible for determining sentences or parole decisions? Would that\nbe equivalent to punishing someone for crimes they have not committed?\nOr is it just a neutral extension of current uses of actuarial\ninformation, such as age, gender, and income level? At an extreme, one\ncould imagine using predictive information to detain people who have\nnot yet committed a crime, arresting them before they do. This\ndystopian scenario, portrayed in the film Minority Report (Speilberg,\n2002), also illustrates how our abilities to predict can raise\ndifficult ethical and policy questions when they collide with\nintuitions about and the value of free will and autonomy. More\ngenerally, work in neuroethics could be of significant practical use\nfor the law, and indeed is often called by another moniker,\n“neurolaw” (see section 2.7). \nIn sum, neuroimaging techniques raise a number of neuroethical issues.\nThe ones discussed above pertain to the use of fMRI, currently an\nexpensive and cumbersome technique. But other imaging methods exist\nthat could be far more widespread. If car companies install imaging\nmethods, for example using NIRS (near infrared spectroscopy), which is\nan imaging method that could be used at a distance and without the\nsubject’s knowledge, or some other form of brain monitoring\n(https://www.jaguarlandrover.com/news/2015/06/jaguar-land-rover-road-safety-research-includes-brain-wave-monitoring-improve-driver)\nto monitor levels of attention in order to alert drivers who begin to\ndoze off, could that data be used in a court of law in the event of an\naccident? Even though the kind of information these methods provide is\nvery crude and generally unsuitable for decoding mental content, there\nare conceivable everyday situations on the horizon in which issues of\nmental privacy and neurotechnology might arise. \nA second way in which cognitive liberty could be impacted is by\nlimiting a person’s autonomy. Autonomy is the freedom to be the\nperson one wants to be, to pursue one’s own goals without\nunjustifiable hindrances or interference, to be self-governing.\nAlthough definitions of autonomy differ, it is widely appreciated as a\nvaluable aspect of personhood. Autonomy of the mental can be impacted\nin a number of ways. Here are several: \nDirect interventions: The ability to directly manipulate our brains to\ncontrol our thoughts or behavior is an obvious threat to our autonomy\n(Gilbert, 2015; Walker and Mackenzie, 2020). Some of our\nneurotechnologies offer that potential, although these sorts of\nneurotechnologies are invasive and used only in cases where they are\nmedically justified. Other types of interventions, such as the\nadministration of drugs to calm a psychotic person, may also impact\nautonomy. \nWe know that stimulating certain brain areas in animals will lead to\nrepetitive and often stereotyped behaviors. Scientists have implanted\nrats with electrodes and have been able to control their foraging\nbehaviors by stimulating their cortex. In theory we could control a\nperson’s behavior by implanting electrodes in the relevant\nregions of cortex. In practice, we have a few methods that can do\nthis, but only in a limited way. For example, Transcranial Magnetic\nStimulation (TMS) applied to motor cortex can elicit involuntary\nmovements in the part of the body controlled by the cortical area\naffected, or when repetitively administered it can inhibit activity\nfor a period of time, acting as a temporary lesion. Effects will vary\ndepending on what area of the brain is stimulated; higher cognitive\nfunctions can be impacted as well. Relatively invasive methods,\nsuch as Deep Brain Stimulation (DBS, discussed below) and\nelectrocorticography (ECOG), both techniques requiring brain surgery,\ndemonstrate that direct interventions can affect cognition, action,\nand emotion, often in very particular and predictable ways. \nHowever much of a threat to autonomy these methods pose in theory,\nthey are rarely used with the aim of compromising autonomy. On the\ncontrary, direct brain interventions, when used, are largely aimed at\naugmenting or restoring rather than bypassing or diminishing autonomy\n(Roskies, 2015; Brown, 2015). For example, one rapidly advancing field\nin neuroscience is the area of neural prostheses and brain computer\ninterfaces (Jebari, 2013; Klein et al. 2015). Neural prostheses are\nartificial systems that replace defective neural ones, usually of\nsensory systems. Some of the more advanced and widely-known are\nartificial cochleas. Other systems have been developed that allow\nvision-like information to feed to touch-specific receptors, enabling\nblind people to navigate the visual world. Brain computer interfaces,\non the other hand, are systems that read brain activity and use it to\nguide robotic prostheses for limbs, or to move a cursor on a video\nscreen. Prosthetic limbs that are guided by neural signals have\nrestored motor agency to paraplegics and quadriplegics, and other BCIs\nhave been used to communicate with people who are “locked\nin” and cannot move their bodies (Abbott and Peck, 2017).\nAdvisory and predictive implants use neural information to warn\npatients about the risk of, for example, an upcoming seizure, allowing\nthem to prophylactically self-medicate (Brown, 2015; Lazaro-Munoz et\nal., 2017). Thus, although in principle brain interventions could be\nused to control people and diminish their autonomy, in general, direct\ninterventions are being developed to restore and enhance it (Lavazza,\n2018). \nNeuroeconomics and neuromarketing: There are more subtle ways to\nimpact autonomy than direct brain manipulations, and these are well\nwithin our grasp: Our thoughts can be manipulated indirectly: old\nworries prompted by propaganda and subliminal advertising have taken\non a renewed currency with the advent of neuroeconomics and\nneuromarketing (Spence, 2020). By better understanding how we process\nreward, how we make decisions more generally, and how we can bias or\ninfluence that process, we open the door to more effective external\nindirect manipulations. Indeed, social psychology has been showing how\nsubtle alterations to our external environment can affect beliefs,\nmoods, and behaviors. The precise threats posed by understanding the\nneural mechanisms of decision making have yet to be fully articulated\n(Stanton et al., 2017). Is neuromarketing being used merely to design\nproducts that satisfy our desires more fully or is it being used to\nmanipulate us? Depending on how you see it, it could be construed as a\ngood or an evil. Does understanding the neural substrates of choice\nand reward provide advertisers more effective tools than they had\nmerely by using behavioral data, or just more costly ones? Do\nconsumers consequently have less autonomy? How can we compensate for\nor counteract these measures? These questions have yet to be\nadequately addressed. \nRegulation: Yet another way that autonomy can be impacted is by\nrestricting the things that a person can do with and to her own mind.\nFor instance, banning mind-altering drugs is an externally imposed\nrestraint on people’s ability to choose their states of\nconsciousness (Biore, 2001). The degree to which a person should be\nprevented from doing what he wishes to his or her self, body or mind,\nis an ethical issue on which people have differing opinions. Some\nclaim this kind of regulation is a problematic infringement of\nautonomy, but certain regulations of this type are already largely\naccepted in our society. Regulation of drugs does impact our autonomy,\nbut it arguably averts potentially great harms. Allowing cognitive\nenhancing technologies only for treatment uses but not for enhancement\npurposes is another restriction of mental autonomy. Whether it is one\nwe want to sanction is still up for debate. Regardless, as the\ncoronavirus pandemic has made abundantly clear, complete autonomy is\nnot practically possible in a world in which one person’s\nactions affect the well-being of others. \nBelief in free will: Advances in neuroscience have been frequently\nclaimed to have bearing upon the question of whether we have free will\nand on whether we can be truly morally responsible for our actions.\nAlthough the philosophical problem of free will is generally\nconsidered to be a metaphysical problem, demonstrable lack of freedom\nwould have significant ethical consequences. A number of\nneuroscientists and psychologists have intimated or asserted that\nneuroscience can show or has shown that free will is an illusion\n(Brembs, 2010; Libet, 1983; Soon, 2010; Harris, 2012). Others have\ncountered with arguments to the effect that such a demonstration is in\nprinciple impossible (Roskies 2006). Regardless of what science\nactually shows about the nature of free will, the fact that people\nbelieve neuroscience evidence supports or undermines free will has\nbeen shown to have practical consequences. For example, evidence\nmerely supporting the premise that our minds are a function of our\nbrains, as most of neuroscience does, is perceived by some people to\nbe a challenge to free will. And in several studies, manipulating\nbelief in free will affects the likelihood of cheating (e.g. Vohs and\nSchooler 2008). The debate within neuroscience about the nature and\nexistence of free will will remain relevant to neuroethics in part\nbecause of its impact on our moral, legal and interpersonal practices\nof blaming and punishing people for their harmful actions. \nOne of the aspects of neuroethics that makes it distinctive and\nimportantly different from traditional bioethics is that we recognize\nthat, in some yet-to-be-articulated sense, the brain is the seat of\nwho we are. For example, we now have techniques that alter\nmemories by blunting them, strengthening them, or selectively editing\nthem. We have drugs that affect sexuality, and others that affect\nmood. Here, neuroethics rubs up against some of the most challenging\nand contentious questions in philosophy: What is the self? What sorts\nof changes can we undergo and still remain ourselves? What is it that\nmakes us the same person over time? Of what value is this temporal\npersistence? What costs would changing personhood incur? \nBecause neuroscience intervention techniques can affect memory,\ndesires, personality, mood, impulsivity and other things we might\nthink of as constitutive of the person or the self, the changes they\ncan cause (and combat) have a unique potential to affect both the\nmeaning and quality of the most intimate aspects of our lives.\nAlthough neuroethics is quite different from traditional bioethics in\nthis regard, it is not so different from genethics. For a long time,\nit was argued that “you are your genes”, and so the\nability to interrogate our genomes, to change them, or to select among\nthem was seen as both a promising and potentially problematic one,\nenabling us to understand and manipulate human nature to an extent far\nbeyond any we had previously enjoyed. But as we have discovered, we\nare not (just) our genes. Our ability to sequence the human genome has\nnot laid bare the causes of cancer, the genetic basis for\nintelligence, or of psychiatric illness, as many had anticipated. One\nreason is that our genome is a distal cause of the people we come to\nbe: many complex and intervening factors matter along the way. Our\nbrains, on the other hand, are a far more proximal cause of who we are\nand what we do. Our moment-to-moment behavior and our long-range plans\nare directly controlled by our brains, in a way they are not directly\ncontrolled by our genomes. If “You are your genes” seemed\na plausible maxim, “You are your brain” is far more\nso. \nDespite its plausibility, it is notoriously difficult to articulate\nthe way in which we are our brains: What aspects of our brains makes\nus the people that we are? What aspects of brain function shape our\nmemories, our personality, our dispositions? What aspects are\nirrelevant or inessential to who we are? What makes possible a\ncoherent sense of self? The lack of answers we have to these deep\nneurophilosophical questions does little to alleviate the pragmatic\nworries raised by neuroscience, since our ability to intervene in\nbrains outstrips our understanding of what we are doing, and can\naffect all these aspects of our being. \nIn philosophy, work focusing on persons may address a variety of\ndistinct issues using different constructs. Philosophers might be\ninterested in the nature of personhood, in the nature of the self, in\nthe kinds of traits and psychological states or processes that give an\nexperienced life coherence or authenticity, or in the ingredients for\na flourishing life. Each calls for its own analysis. Outside of\nphilosophy, many of these issues are run together, and confusion often\nresults. Neuroethics, while in a unique position to leverage these\nissues and apply them in a fruitful way, often fails to make the most\nof the conceptual work philosophers have done in this area. For\nexample, papers in neuroethics often conflate a number of these\ndistinct concepts, referring them under the rubric of “personal\nidentity”. This conflation further muddies already difficult\nwaters, and diminishes the potential value of neuroethical work. Below\nI try to give a brief roadmap of the separate strands that\nneuroethicists have been concerned with. \nThe philosopher’s conception of personal identity refers to the\nissue of what makes a person at one time numerically identical to a\nperson at another time. This metaphysical question has been addressed\nby a variety of philosophical theories. For example, some theorists\nargue that what it is to be the numerically identical over time is to\nbe the same human organism (Olson, 1999), and that being the same\norganism is determined by sameness of life. If having the same life is\nthe relevant criterion, one could argue that life-sustaining areas of\nthe brainstem are essential to personal identity (Olson, 1999). For\nthose who believe instead that bodily integrity is what is essential,\nthe ability of neuroscience to alter the brain will arguably have\nlittle effect on personal identity. Many other philosophers have\nidentified the same person as being grounded in psychological\ncontinuity of some sort (e.g., Locke). If this criterion is the\ncorrect one, then the stringency of that criterion may be crucial:\nradical brain manipulation may cause an abrupt enough shift in\nmemories and other psychological states that a person after brain\nintervention is no longer the same person he or she was prior. The\nmore stringent the criterion, the greater is the potential threat of\nneurotherapies to personal identity (Jecker and Ko, 2017; Pascalev et\nal. 2016). On the other hand, if the standards for psychological\ncontinuity or connectedness are high enough, changes in personal\nidentity may in fact be commonplace even without neurotherapies.\nRecognizing this may prompt us to question the criterion and/or the\nimportance or value of personal identity. Parfit, for example, argues\nthat what makes us one and the same person over time, and what we\nvalue (psychological continuity and connectedness) come apart (Parfit,\n1984). \nFor some, the question of personhood comes apart from the question of\nidentity. Even if personal (i.e. numerical) identity is unchallenged\nby neurotechnologies and by brain dysfunction, important neuroethical\nquestions may still be raised. Philosophers less concerned with\nmetaphysical questions about numerical identity have focused more on\nthe self, and on notions of authenticity and self-identification,\nemphasizing the importance of the psychological perspective of the\nperson in question in creating a coherent self (Mackenzie and Walker,\n2015; Erler, 2011; Pugh et al., 2017). In this vein, Schectman has\nsuggested that what is important is the ability to create a coherent\nnarrative, or “narrative self” (Schectman, 2014). There is\nevidence that the ability to create and sustain a coherent narrative\nin which we are the protagonist and with which we identify, is a\nmeasure of psychological health (Waters et al., 2014). On the other\nhand, some philosophers deny that they have a narrative self and\nlocate selfhood in a synchronic property (Strawson, 2004). To further\ncomplicate matters, it has been suggested that there is a distinction\nbetween the narrative person and the narrative self, these being\ndifferentiable via degrees of appropriation. Concerns about the nature\nand coherence of the narrative self, and about authenticity and\nautonomy, tend to be the ones most relevant to neuroethics, since\nthese constructs clearly can be affected by even modest brain changes.\nFor example, how do we evaluate the costs and ethical issues attending\na dramatic change in personality, or a modification of key memories?\nWhat are the criteria governing whether one is authentic or\ninauthentic, and what is the value of authenticity? If\nneurointerventions promise to result in dramatic shifts in a\nperson’s values and commitments, whose interests should take\npriority if one person must be favored - the original or the resulting\nperson? The relevance of personhood, self, agency, identity and\nidentification needs further elaboration for neuroethics. In what\nfollows we discuss how one neurotechnology can bear upon some of these\nquestions. \nDeep Brain Stimulation (DBS) involves the stimulation of chronically\nimplanted electrodes deep in the brain, and it is FDA approved for\ntreating Parkinson’s Disease, a neurodegenerative disease\naffecting the dopamine neurons in the striatum. Neuromodulation with\nDBS often restores motor function in these patients, permitting many\nto live much improved lives. It is also being explored as treatments\nfor treatment-resistant depression, OCD, addiction, and other\nneurological and psychiatric issues. Although DBS is clearly a boon to\nmany people suffering from neurological diseases, there are a number\nof puzzling issues that arise from its adoption. First, it is a highly\ninvasive treatment, requiring brain surgery and permanent implantation\nof a stimulator, thus posing a real possibility of harm and raising\nquestions of cost/benefit tradeoffs. This is coupled with the fact\nthat scientists have little mechanistic understanding of how the\ntreatment works when it does, and treatment regimes and electrode\nplacement tend to be symptomatic. Occasionally DBS causes unusual side\neffects, such as mood changes, hypomania or mania, addictive\nbehaviors, or hypersexual behavior. In one case a patient with\nwide-ranging musical tastes developed a fixation for Johnny\nCash’s music, which persisted until stimulation was ceased\n(Mantione et al., 2014). Other reported cases involve changes in\npersonality. The ethical questions in this area revolve around the\nethics of intervening in ways that alter mood and/or personality,\nwhich is often discussed in terms of personal identity or\n“changing who the person is”, and around questions of\nautonomy and alienation (Klaming et al, 2013; Kraemer, 2013a). \nOne poignant example from the literature tells of a patient who,\nwithout intervention, was bedridden and had to be hospitalized due to\nsevere motor dysfunction caused by Parkinson’s Disease\n(Leentjens et al., 2003). DBS resulted in a marked improvement in his\nmotor symptoms but also caused him to be untreatably manic, which\nrequired institutionalization. Thus, this unfortunate man had to\nchoose between being bedridden and catatonic, or manic and\ninstitutionalized. He made the choice (in his unstimulated state) to\nremain on stimulation (the literature does not mention whether his\nstimulated self concurred, as he was not deemed mentally competent in\nthat state) (Kraemer, 2013b). While it did not happen in this case,\none could imagine a situation in which the patient will choose, while\nunstimulated, to undergo chronic stimulation, but, while under\nstimulation, would choose otherwise (or vice versa). The\npossibility for dilemmas or paradoxes will arise when, for example, we\ntry to determine the value of two potential outcomes that are\ndifferently valued by the people who might exist. To which person (or\nto the person in which state) should we give priority? Or, even more\nperplexing: if the “identity” (narrative or numerical) of\nthe person is indeed shifted by the treatment, should we give one\nperson the authority to consent to a procedure or choose an outcome\nthat in practice affects a different person? DBS cases like this will\nprovide fodder for neuroethicists for years to come (Skorburg and\nSinnott-Armstrong, 2020). \nMany other neurotechnologies that have been developed for treating\nbrain dysfunction have primary or side effects that affect some aspect\nof what we may think of as related to human agency (Zuk et al., 2018).\nThe ethical issues that arise with these neurotechnologies involve\ndetermining 1) in which way they do impact our selves or our agency;\n2) what value, positive or negative we should put on this impact (or\nability to so affect agency); and 3) how to weigh the positive gains\nagainst the negatives. One issue that has been raised is whether we\npossess a clear enough conception of the elements of agency in order\nto effectively perform this sort of analysis (Roskies, 2015).\nMoreover, given the likelihood that no objective criteria exist for\nhow to evaluate tradeoffs in these elements, and the fact that\ndifferent people may value different aspects of themselves\ndifferently, the weighing process will likely have to be subjectively\nrelativized. \nFinally, DBS as well as neural prostheses and BCIs raise another\nneuroethical issue: our conception of humanity and our relations to\nmachines. Some contend that these technologies effectively turn a\nperson into a cyborg, making him or her something other than human.\nWhile some find this an ethically unproblematic natural extension of\nour species’ characteristic drive to invent and improve our\nselves with technology (Clark, 2004), others fear that creating a\nbio-cybernetic organism raises troubling questions about the nature or\nvalue of humanity, about the bounds of self, or about Promethean\nimpulses. These questions too fall squarely in the domain of\nneuroethics. \nThe Hard Problem of consciousness (Chalmers, 1995) has yielded little\nto the probings of neuroscience, and it is not clear whether it ever\nwill. However, in the last decade impressive advances have been made\nin other realms of consciousness research. Most impressive have been\nthe improvements in detecting altered levels of consciousness with\nbrain imaging. Diagnosing behaviorally unresponsive patients has long\nbeen a problem for neurology, although as long as 20 years ago,\nneurologists had recognized systematic differences between and\nprognoses for a persistent vegetative state (PVS), a minimally\nconscious state (MCS), and locked-in syndrome, a syndrome in which the\npatient has normal levels of awareness but cannot move. Functional\nbrain imaging has fundamentally changed the problems faced by those\ncaring for these patients. Owen and colleagues have shown that it is\npossible to identify some patients mischaracterized as being in PVS by\ndemonstrating that they are able to understand commands and follow\ndirections (Owen, 2006). In these studies, both normal subjects and\nbrain injured patients were instructed to visualize doing two\ndifferent activities while in the fMRI scanner. In normal subjects\nthese two tasks activated different parts of cortex. Owen showed that\none patient diagnosed as in PVS showed this normal pattern, unlike\nother PVS patients, who showed no differential activation when given\nthese instructions. This data suggests that some PVS diagnosed\nsubjects can in fact process and understand the instructions, and that\nthey have the capacity for sustained attention and voluntary mental\naction. These results were later replicated in other such patients,\nand based on small cohorts, it is estimated that approximately 20% of\nPVS patients have been misdiagnosed. In a later study the same group\nused these imagination techniques to elicit from some patients with\nsevere brain injury answers to Yes/No questions (Monti et al., 2010).\nMore recent work aims to adapt these methods for EEG, a cheaper and\nmore portable neurotechnology (Bai et al., 2019). Neuroimaging\nprovides new tools for evaluating and diagnosing patients with\ndisorders of consciousness (Owen, 2013; Campbell, 2020). \nThese studies have the potential to revolutionize the way in which\npatients with altered states of consciousness are diagnosed and cared\nfor, may have bearing on when life support is terminated, and raise\nthe possibility of allowing patients to have some control over\nquestions regarding their care and end of life decisions (Peterson et\nal., 2020; Braddock, 2017). This last possibility, while in some ways\nalleviating some worries about how to treat severely brain-damaged\nindividuals, raises other thorny ethical problems. One of the most\npressing is how to deal with questions of competence and informed\nconsent: These are people with severe brain damage, and even when they\ndo appear capable on occasion of understanding and answering\nquestions, there are still questions about whether their abilities are\nstable, how sophisticated they are, and whether they can competently\nmake decisions about such weighty issues, as well as whether it is\nreally in their interest to remain on life support (Kahane and\nSavalescu, 2009; Fischer and Truog, 2017). Nonetheless, these methods\nopen up new possibilities for diagnosis and treatment, and for\nrestoring a measure of autonomy and self-determination to people with\nsevere brain damage. \nMedical practice and neuroscientific research raise a number of\nneuroethical issues, many of which are common to bioethics. For\nexample, issues of consent, of incidental findings, of competence, and\nof privacy of information arise here. In addition, practicing\nneurologists, psychologists and psychiatrists may routinely encounter\ncertain brain diseases, disabilities, or psychological dysfunctions\nthat raise neuroethical issues that they must address in their\npractices. (For a more detailed discussion of these more applied\nissues approached from a pragmatic point of view, see for example\nRacine, 2010; Martineau and Racine, 2020). \nThe advances of neuroscience have become a common topic in the popular\nmedia, with colorful brain images becoming a pervasive illustrative\ntrope in news stories about neuroscience. While no one doubts that\npopularizing neuroscience is a positive good, neuroethicists have been\nlegitimately worried about the possibilities for misinformation. These\ninclude worries about “the seductive allure” of\nneuroscience, and of misleading and oversimplified media coverage of\ncomplex scientific questions. \nThere is a documented tendency for the layperson to think that\ninformation that makes reference to the brain, or to neuroscience or\nneurology, is more privileged, more objective, or more trustworthy\nthan information that makes reference to the mind or psychology. For\nexample, Weisberg and colleagues report that subjects with little or\nno neuroscience training rated bad explanations as better when they\nmade reference to the brain or incorporated neuroscientific\nterminology (Weisberg et al., 2008). This “seductive allure of\nneuroscience” is akin to an unwarranted epistemic deference to\nauthority. This differential appraisal extends into real-world\nsettings, with testimony from a neuroscientist or neurologist judged\nto be more credible than that of a psychologist. The tendency is to\nview neuroscience as a hard science, in contrast to “soft”\nmethods of inquiry that focus on function or behavior. With\nneuroimaging methods, this belies a deep misunderstanding of the\ngenesis and significance of the neuroscientific information. What\npeople fail to realize is that neuroimaging information is classified\nand interpreted by its ties to function, so (barring unusual\ncircumstances) it cannot be more reliable or “harder” than\nthe psychology it relies upon. \nBrain images in particular have prompted worries that the colorful\nimages of brains with “hotspots” that accompany media\ncoverage could themselves be misleading. If people intuitively\nappreciate brain images as if they were akin to a photograph of the\nbrain in action, that this could mislead them into thinking of these\nimages as objective representations of reality, prompting them to\noverlook the many inferential steps and nondemonstrative decisions\nthat underlie creation of the image they see (Roskies, 2007). The\nworry is that the powerful pull of the brain image will lend a study\nmore epistemic weight than is justified, and discourage people from\nasking the many complicated questions that one must ask in order to\nunderstand what the image signifies, and what can be inferred from the\ndata. Further work, however, has suggested that once one takes into\naccount the privilege accorded to neuroscience over psychology, the\nimages themselves do not further mislead (Schweitzer et al.,\n2011). \nIn this era of indubitably exciting progress in brain research, there\nis a “brain-mania” that is partially warranted but holds\nits own dangers. The culture of science is such that it is not\nuncommon for scientists to describe their work in the most dramatic\nterms possible in order to secure funding and/or fame. Although the\nhyperbole can be discounted by knowledgeable readers, those less\nsophisticated about the science may take it at face value. Studies\nhave shown that the media is rarely critical of the scientific\nfindings they report, and they tend not to present alternative\ninterpretations (Racine, 2006, 2015). The result is that the popular\nmedia conveys sometimes wildly inaccurate pictures of legitimate\nscientific discoveries, which can fuel both overly optimistic\nenthusiasm as well as fear. One of the clear pragmatic goals of\nneuroethics, whether it regards basic research or clinical treatments,\nis to exhort and educate scientists and the media to better convey\nboth the promise and complexities of scientific research. It is the\njob of both these groups to teach people enough about science in\ngeneral, and brain science in particular, that they see it as worthy\nof respect, and also of the same critical assessment that to which\nscientists themselves subject their own work. \nIt is admittedly difficult to accurately translate complicated\nscientific findings for the lay public, but it is essential.\nOverstatement of the significance of results can instill unwarranted\nhope in some cases, fear in others, and jadedness and suspicion going\nforward. Providing fodder for scientific naysayers has policy\nimplications that go far beyond the reach of neuroscience. Mistrust of\nscience is its own epidemic that needs to be inoculated against by\ncareful, early, and continuing education of the public. This is\nessential for the future status and funding of the basic sciences,\nand, as we have seen, for the health of democracy and our planet more\ngenerally. \nSocial justice is a concern of ethics, and of neuroethics. Many of the\nethical questions are not new, but some have novel aspects. Bioethics\nalso has traditionally been concerned with issues of respect for\npatient’s autonomy and right to self-determination. As mentioned\nabove, these questions take on added weight when the organ at issue is\nthe patient’s brain, and questions about competence arise. \nEthical issues also attend doing neuroscientific research on\nnonhumans. Like traditional bioethics, neuroethics must address\nquestions about the ethical use of animals for experimental purposes\nin neuroscience. In addition, however, it ought to consider questions\nregarding the use of animals as model systems for understanding the\nhuman brain and human cognition (Johnson et al., 2020). Animal studies\nhave given us the bulk of our understanding of neural physiology and\nanatomy and have provided significant insight into function of\nconserved biological capacities. However, the further we push into\nunknown territory about higher cognitive functions, the more we will\nhave to attend to the specifics of similarities and differences\nbetween humans and other species and evaluating the model system may\ninvolve considerable philosophical work. In some cases, the\ndissimilarities may not warrant animal experiments. \nOther issues to which neuroethics also must be attentive to involve\nsocial justice. As neuroscience promises to offer treatments and\nenhancements, it must attend to issues of distributive justice, and\nplay a role in ensuring that the fruits of neuroscientific research do\nnot go only to those who enjoy the best our society has to offer.\nMoreover, a growing understanding that poverty and socioeconomic\nstatus more generally have long-lasting cognitive effects raises moral\nquestions about the social policy and the structure of our society,\nand the growing gap between rich and poor (Farah, 2017). It seems that\nthe social and neuroscientific realities may reveal the American Dream\nto be largely hollow, and these findings may undercut some popular\npolitical ideologies. There are also global issues to consider (Stein\nand Singh, 2020). Justice may demand more involvement of\nneuroethicists in policy decisions. \nFinally, neuroethics stretches seamlessly into the law (see, e.g.\nVincent, 2013; Morse and Roskies, 2013; Jones et al., 2014).\nNeuroethical issues arise in criminal law, in particular with the\nissue of criminal responsibility (see, e.g. Birks & Douglas,\n2018). For example, the recognition that a large percentage of prison\ninmates have some history of head trauma or other abnormality raises\nthe question of where to draw the line between the bad and the mad.\nNeuroethics has bearing on issues of addiction and juvenile\nresponsibility, as well as on some other areas of law, such as in tort\nlaw, employment law, and health care law. \nNeuroscience, or more broadly the cognitive and neural sciences, have\nmade significant inroads into understanding the neural basis of\nethical thought and social behavior. In the last decades, these fields\nhave begun to flesh out the neural machinery underlying human\ncapacities for moral judgment, altruistic action, and the moral\nemotions (Liao, 2016). The field of social neuroscience, nonexistent\ntwo decades ago, is thriving, and our understanding of the circuitry,\nthe neurochemistry, and the modulatory influences underlying some of\nour most complex and nuanced interpersonal behaviors is growing\nrapidly. Neuroethics recognizes that the heightened understanding of\nthe biological bases of social and moral behaviors can itself have\neffects on how we conceptualize ourselves as social and moral agents,\nand foresees the importance of the interplay between our scientific\nconception of ourselves and our ethical views and theories (Roskies,\n2002). The interplay and its effects provide reason to view the\nneuroscience of ethics (or more broadly, of sociality) as part of the\ndomain of neuroethics. \nPerhaps the most well-known and controversial example of such an\ninterplay marks the beginning of this kind of exploration. In 2001,\nJoshua Greene scanned people while they made a series of moral and\nnonmoral decisions in different scenarios, including dilemmas modeled\non the philosophical “Trolley Problem” (Thomson, 1985). He\nnoted systematic differences in the engagement of brain regions\nassociated with moral processing in “personal” as opposed\nto “impersonal” moral dilemmas and hypothesized that\nemotional interference was behind the differential reaction times in\njudgments of permissibility in the footbridge case. In later work, he\nproposed a dual-process model of moral judgment, where relatively\nautomatic emotion-based reactions and high-level cognitive control\njointly determined responses to moral dilemmas, and he related his\nfindings to philosophical moral theories (Greene, 2004, 2008). Most\ncontroversially, he suggested that there are reasons to be suspicious\nof our deontological judgments, and interpreted his work as lending\ncredence to utilitarian theories (Greene 2013). Greene’s work is\nthus a clear example of how neuroscience might affect our ethical\ntheorizing. Claims regarding the import of neuroscience studies for\nphilosophical questions have sparked a heated debate in philosophy and\nbeyond, and prompted critiques and replies from scholars both within\nand outside of philosophy (see, e.g. Berker, 2009; Kahane, 2011;\nChristensen, 2014). One effect of these exchanges is to highlight a\nproblematic tendency for scientists and some philosophers to think\nthey can draw normative conclusions from purely descriptive data;\nanother is to illuminate the ways in which descriptive data might\nitself masquerade as normative (Roskies, forthcoming). \nGreene’s early studies demonstrated that neuroscience can be\nused in the service of examining extremely high-level behaviors and\ncapacities, and have served as an inspiration for numerous other\nexperiments investigating the neural basis of social and moral\nbehavior and competences (May et al., forthcoming). Neuroethics has\nalready turned its attention to phenomena such as altruism, empathy,\nwell-being, and theory of mind, as well as to disorders such as autism\nand psychopathy. The relevant works range from imaging studies using a\nvariety of imaging techniques, to manipulation of hormones and\nneurochemicals, to purely behavioral studies, and the use of virtual\nreality. In addition, interest in moral and social neuroscience has\ncollided synergistically with the growth of neuroeconomics, which has\nflourished in large part independently. A recent bibliography has\ncollected almost 400 references to works in the neuroscience of ethics\nsince 2002 (Darragh et al., 2015). We can safely assume that many more\nadvances will be made in the years to come, and that neuroethicists\nwill be called upon to advance, evaluate, expound upon, or deflate\nclaims for the purported ethical implications of our new\nknowledge. \nThe examples discussed above included pharmaceuticals that are already\napproved for use, existing brain imaging techniques and invasive\nneurotherapies. But practical neuroethical concerns, and some\ntheoretical concerns, are highly dependent upon the details of\ntechnologies. For example, adaptive DBS or aDBS, which “closes\nthe loop” by concurrently stimulating and recording from neural\ntissue, and automatically adjusting stimulation based on the state of\nthe brain, raises more pressing and somewhat different concerns about\nagency and autonomy than does regular DBS (see, e.g. Goering et al.,\n2017). Several technologies are already on the horizon that are bound\nto raise some new neuroethical questions, or old questions in new\nguises. One of the most powerful new tools in the research\nneuroscientist’s arsenal is “optogenetics”, a method\nof transfecting brain cells with genetically engineered proteins that\nmake the cell responsive to light of specific wavelengths (Diesseroth,\n2011). The cells can then be activated or silenced by shining light\nupon them, allowing for cell-specific external control. Optogenetics\nhas been successfully used in many model organisms, including rats,\nand work is underway to use optogenetics in monkeys. One may presume\nit is only a matter of time before it will be developed for use in\nhumans. The method promises to provide precise control of specific\nneural populations and relatively noninvasive targeted treatments for\ndiseases. It promises to raise the kind of neuroethical issues raised\nby many mechanisms that intervene on brain function: Questions of\nharm, of authenticity, and the prospect of brain cells being\ncontrolled by someone other than the agent himself (Gilbert, 2014;\nAdamczyk & Zawadzki, 2020). A second technique, CRISPR, allows\npowerful targeted gene editing. Although not strictly a\nneuroscientific technique, it can be used on neural cells to effect\nbrain changes at the genetic level (Canli, 2015). Genetic engineering\nmight make possible neural gene therapies and designer babies, making\nreal consequences of the genetic revolution thus far only\nimagined. \nThese and other technologies were not even imagined a few decades ago,\nand is likely that other future technologies will emerge which we\ncannot currently conceive of. If many neuroethical issues are closely\ntied to the capabilities of neurotechnologies, as I have argued, then\nwe will likely be unlikely to anticipate future technologies in enough\ndetail to predict the constellation of neuroethical issues that they\nmay give rise to. Neuroethics will have to grow as neuroscience does,\nadapting to novel ethical and technological challenges.","contact.mail":"adina.roskies@dartmouth.edu","contact.domain":"dartmouth.edu"}]
