[{"date.published":"2010-08-18","date.changed":"2018-02-08","url":"https://plato.stanford.edu/entries/chance-randomness/","author1":"Antony Eagle","author1.info":"http://antonyeagle.org/","entry":"chance-randomness","body.text":"\n\n\n\nRandomness, as we ordinarily think of it, exists when some outcomes occur haphazardly, unpredictably, or by chance. These latter three notions are all distinct, but all have some kind of close connection to probability. Notoriously, there are many kinds of probability: subjective probabilities (‘degrees of belief’), evidential probabilities, and objective chances, to name a few (Hájek 2012), and we might enquire into the connections between randomness and any of these species of probability.\n\nIn this entry, we focus on the potential connections between randomness and chance, or physical probability. The ordinary way that the word ‘random’ gets used is more or less interchangeable with ‘chancy’, which suggests this Commonplace Thesis—a useful claim to target in our discussion:\n\n(CT)\n\nSomething is random iff it happens by chance.\n\n\nThe Commonplace Thesis, and the close connection between randomness and chance it proposes, appears also to be endorsed in the scientific\nliterature, as in this example from a popular textbook on evolution (which also throws in the notion of unpredictability for good measure):\n\n\n\nscientists use chance, or randomness, to mean that when physical\ncauses can result in any of several outcomes, we cannot predict what\nthe outcome will be in any particular case. (Futuyma 2005:\n225)\n\n\n\nSome philosophers are, no doubt, equally subject to this unthinking\nelision, but others connect chance and randomness deliberately. Suppes\napprovingly introduces\n\n\n\nthe view that the universe is essentially probabilistic in character,\nor, to put it in more colloquial language, that the world is full of\nrandom happenings. (Suppes 1984: 27)\n\n\n\nHowever a number of technical and philosophical advances in our\nunderstanding of both chance and randomness open up the possibility\nthat the easy slide between chance and randomness in ordinary and\nscientific usage—a slide that would be vindicated by the truth of\nthe Commonplace Thesis—is quite misleading. This entry will\nattempt to spell out these developments and clarify the differences\nbetween chance and randomness, as well as the areas in which they\noverlap in application. It will also aim to clarify the relationship of\nchance and randomness to other important notions in the vicinity,\nparticularly determinism and predictability (themselves often subject\nto confusion).\n\n\n\nThere will be philosophically significant consequences if the\nCommonplace Thesis is incorrect, and if ordinary usage is misleading.\nFor example, it is intuitively plausible that if an event is truly\nrandom it cannot be explained (if it happens for a reason, it isn’t truly random). It might seem then that the possibility\nof probabilistic explanation is undermined when the probabilities\ninvolved are genuine chances. Yet this pessimistic conclusion only\nfollows under the assumption, derived from the Commonplace Thesis, that\nall chancy outcomes are random. Another interesting case is the role of\nrandom sampling in statistical inference. If randomness requires\nchance, then no statistical inferences on the basis of\n‘randomly’ sampling a large population will be valid unless\nthe experimental design involves genuine chance in the selection of\nsubjects. But the rationale for random sampling may not require chance sampling—as long as our sample is representative, those\nstatistical inferences may be reliable. But in\nthat case, we’d be in a curious situation where random sampling\nwouldn’t have much to do with randomness, and whatever justification\nfor beliefs based on random sampling that randomness is currently\nthought to provide would need to be replaced by something else.\n\nA final case of considerable philosophical interest is the frequentist approach to objective probability, which claims (roughly) that the chance of an outcome is its frequency in an appropriate series of outcomes (Hájek 2012 §3.4). To avoid classifying perfectly regular recurring outcomes as chancy, frequentists like von Mises (1957) proposed to require that the series of outcomes should be random, without pattern or order. Frequentism may fall with the Commonplace Thesis: if there can be chancy outcomes without randomness, both will fail.\n\n\n\nThe Commonplace Thesis is central to all three examples. As it is widely accepted that probabilistic explanation is legimitate, that random sampling doesn’t need genuine chance (though it can help), and that frequentism is in serious trouble (Hájek 1997), there is already some some pressure on the Commonplace\nThesis. But we must subject it to closer examination to clarify\nwhether these arguments do succeed, and what exactly it means to say of\nsome event or process that it is random or chancy. Though developing\nfurther consequences of this kind is not the primary aim of this entry,\nit is hoped that what is said here may help to untangle these and other\nvexed issues surrounding chance and randomness.\n\n\n\nTo get clear on the connections and differences between chance and\nrandomness, it would be good first to have some idea of what chance and\nrandomness amount to. Interestingly, philosophical attention has\nfocussed far more on chance than randomness. This may well be a\nconsequence of the Commonplace Thesis. Whatever its source, we can\nappeal to a substantial consensus in the philosophical literature as to\nwhat kind of thing chance must be. \n\nCarnap (1945) distinguished between two conceptions of probability,\narguing that both were scientifically important. His\n‘probability\\(_1\\)’ corresponds to an epistemic\nnotion, nowadays glossed either (following Carnap himself) as\nevidential probability, or as credence or degree of\nbelief. This is contrasted with Carnap’s\n‘probability\\(_2\\)’, which is the concept of a\nnon-epistemic objective kind of probability, better known as\nchance. \n\nThere are many philosophical accounts of what actually grounds\nchance, as part of the minor philosophical industry of producing\n‘interpretations’—really, reductive analyses or\nexplications—of probability. In this vein we have at least the\nfrequency theory of Reichenbach (1949) and von Mises (1957)\n(and Carnap’s own explication of probability\\(_2\\) was in terms\nof frequencies), the propensity theory of Popper (1959) and\nGiere (1973), as well as many more recent accounts, notably Lewis’\n(1994) ‘Best System’ account of chance (see also Loewer\n2004). There is no agreement over which, if any, of these accounts are\nright; certainly both the accounts mentioned face difficulties in\ngiving an adequate account of chance. The consensus mentioned earlier\nis not over what actually plays the role of chance, but rather on the\nconstraints that determine what that role is. \n\nThere can be such a consensus because ‘chance’ is not a\ntechnical term, but is rather an ordinary concept deployed in fairly\nfamiliar situations (games of chance, complicated and unpredictable\nscenarios, large ensembles of similar events, etc.). There is\nwidespread agreement amongst native speakers of English over when\n‘chance’ applies to a particular case, and this agreement\nat least indicates that there is a considerable body of ordinary belief\nabout chance. One needn’t take the deliverances of folk intuition as\nsacrosanct to recognise that this ordinary belief provides the starting\npoint for philosophical accounts of chance. It may turn out that\nnothing fits the role picked out by these ordinary beliefs and\ntheir philosophical precisifications, yet even in that case we’d be\ninclined to conclude that chance doesn’t exist, not that our ordinary\nbeliefs about what chance must be are incorrect. \n\nBelow, some of the theoretical principles that philosophers have\nextracted from commonplace beliefs about chance will be outlined. (In\nso doing, we freely use probabilistic notation and concepts; see the\n entry on interpretations of probability,\n Hájek 2012:\n §1,\n for background\nprobability theory required to understand these expressions.) Two such\nconstraints have been widely accepted since the early days of the\nphilosophy of probability. Firstly, it is required that the mathematics\nof chance should conform to some standard mathematical theory of\nprobability such as Kolmogorov’s 1933 axiomatisation of the probability\ncalculus (or some recognisable variant thereof, like Popper’s\naxiomatisation of conditional probability). Secondly, chance should be\nobjective: mind-independent and not epistemic or evidential. But a\nnumber of other constraints have been articulated and defended in the\nliterature. (Schaffer 2007: §4 contains a useful discussion of\nthese and other constraints on the chance role.) While these principles\nhave been termed ‘things we know about chance’, this\nshouldn’t be taken to preclude our discovering that there is no such\nthing as chance—rather, the philosophical consensus is that if\nthere is any such thing as chance, it will (more or less) fit these\nconstraints. \n\nChance should regulate (that is, it is involved with norms\ngoverning) rational belief in line with Lewis’ Principal Principle\n(Lewis, 1980), or his New Principle (Lewis, 1994; Hall, 2004). Where\n\\(C\\) is a reasonable initial credence function, and \\(E\\) is\nthe evidence, the Principal Principle (omitting some complications) is this: \n(PP)\n\\(C(p \\mid \\ulcorner\\Ch(p) = x\\urcorner \\wedge E) = x\\); This principle says that rational initial credence should treat\nchance as an expert, deferring to it with respect to opinions about\nthe outcome \\(p\\), by adopting the corresponding chances as your\nown conditional degree of belief. The New Principle—adopted to\ndeal with some problematic interactions between Lewis’ metaphysics and\nthe PP—advocates deferring to chance in a somewhat different\nway. This New Principle NP suggests (more or less) that rational initial credence\nshould treat chance as an expert with respect to the\noutcome \\(p\\), by adopting the conditional chances, on the\nadmissible evidence, as your conditional degrees of belief on that\nsame evidence, as in this principle (this claim corresponds to\nequation 3.9 in Hall 2004; see his discussion at pp. 102–5 for\nsome important qualifications, and the connection to the formulation\nof the NP in Lewis 1994):  Non-reductionist views about chance, which take chances to be\nindependent fundamental features of reality, can follow PP.\nReductionists, who take the values of chances to be fixed entirely by\nother features of reality (normally frequencies and symmetries, but\nchance is typically constrained by them in a quite indirect manner),\n are for technical reasons (to do with undermining, see supplement\n A.1)\n ordinarily forced to adopt NP, and Chance-Analyst, as\nnorms on credence, though in many ordinary cases, NP and PP give\nvery similar recommendations. In either case, both formal principles\ngive content to the intuitively plausible idea that chances should\n guide reasonable\n credence.[1] \n\nChance should connect with possibility. Leibniz claimed that\nprobability was a kind of ‘graded possibility’, and more\nrecent authors have largely agreed. In particular, it seems clear that\nif an outcome has some chance of occurring, then it is possible that\nthe outcome occurs. This intuition has been made precise in the Basic\n Chance Principle (BCP) (See supplement\n A.2\n for further details on this\nprinciple): \n\nBut one needn’t accept precisely this version of the BCP to endorse\nthe general thesis that chance and possibility must be linked—for\nother versions of this kind of claim, see Mellor (2000); Eagle\n(2011) and the ‘realization principle’ of Schaffer\n(2007: 124). \n\nChance should connect with actual frequencies, at least to the\nextent of permitting frequencies to be good evidence for the values of\nchances. This may be through some direct connection between chance and\nfrequency, or indirectly through the influence of observed outcome\nfrequencies on credences about chances via the Principal Principle\n(Lewis, 1980: 104–6). But chance should not be identified with\nfrequency—since a fair coin can produce any sequence of outcomes,\nthere is no possibility of identifying chance with observed frequency.\n(Though of course a fair coin is overwhelmingly likely to produce\nroughly even numbers of heads and tails when tossed often enough).\nMoreover, there can be a chance for a kind of outcome even when there\nare very few instances of the relevant process that leads to that\noutcome, resulting in the actual frequencies being misleading or\ntrivial (for example, if there is only a single actual outcome:\nHájek 1997). \n\nWhen considering the connection between frequency and chance, not\njust any frequency will do. What is wanted is the frequency in\nrelevantly similar trials, with the same kind of experimental setup.\nThe relevance of frequencies in such trial is derived from the\nassumption that in such similar trials, the same chances exist:\nintra-world duplicate trials should have the same chances. This is\nclosely related to the ‘stable trial principle’ (Schaffer,\n2003: 37ff). Chances attach to the outcomes of trials, but the physical\ngrounds of the chance lie in the physical properties of the trial\ndevice or chance setup. \n\nMore details on all of these principles can be found in this\nsupplementary document: \n\nChance, it is commonly said, is ‘single-case objective\nprobability’. Philosophers haven’t been very clear on what is\nmeant by ‘single-case’, and the terminology is slightly\nmisleading, as it falsely suggests that perhaps multiple cases have\nless claim to their chances. The most minimal version of the claim is\nthat, at least sometimes, an outcome can have a chance to be a result\nof an instance of a given kind of process, or trial, even\nthough no other trials of that process occur. This is what we will\nmean by ‘single case’ chance. (A stronger claim is that\nthe chance of an outcome resulting from a given process is an\nintrinsic property of a single trial. The stronger claim is\ninconsistent with standard versions of the frequency theory, and\nindeed it can be difficult to see how chance and frequency might be\nconnected if that stronger claim were true.) Some have claimed that\nsingle-case chance is no part of objective probability; for example,\nvon Mises (1957: p. 11) remarks that the ‘concept of probability\n… applies only to problems in which either the same event\nrepeats itself again and again, or a great number of uniform elements\nare involved at the same time.’. However, this is a theoretical\njudgment on von Mises’ part, based on difficulties he perceived in\ngiving an account of single-case chance; it is not a judgement derived\nfrom internal constraints on the chance role. And how could it be?\nFor \n\nA number of the constraints discussed above require the legitimacy\nof single-case chance. The objection to frequentist accounts of chance\nthat the frequency might misrepresent the chance if the number of\nactual outcomes is too low apparently requires that there be\nnon-trivial chances even for events resulting from a type of trial that\noccurs only very few times, and perhaps even can only occur very few\ntimes (Hájek 2009: 227–8). The strong connections between\npossibility and chance mooted by the BCP and variants thereof also\nrequire that there are single-case chances. For the BCP requires that,\nfor every event with some chance, it is possible that the event has\nthat same chance and occurs. As noted in Supplement\n A.2,\n this renders the chances\nrelatively independent of the occurrent frequencies, which in turn\nrequires single-case chance. For some single outcomes—for\nexample, the next toss of a coin that is biased ⅔ towards\nheads—only very few assignments of credence are reasonable; in\nthat case, we should, if we are rational, have credence of ⅔ in the\ncoin landing heads. The rationality of this unequal assignment cannot\nbe explained by anything like symmetry or indifference. Its rationality\ncan be explained by the PP only if the single-case chance of heads on\nthe next toss is ⅔. Moreover, the existence of this constraint on\nrational credence should have an explanation. Therefore the PP, if it\nis to play this necessary explanatory role, requires single-case\nchance. \n\nIt is the stable trial principle that has the closest connection\nwith single-case chance, however. For in requiring that duplicate\ntrials should receive the same chances, it is natural to take the\nchance to be grounded in the properties of that trial, plus the laws of\nnature. It is quite conceivable that the same laws could obtain even if\nthat kind of trial has only one instance, and the very same chances\nshould be assigned in that situation. But then there are well-defined\nchances even though that type of event occurs only once. \n\nThe upshot of this discussion is that chance is a process\nnotion, rather than being entirely determined by features of the\noutcome to which the surface grammar of chance ascriptions assigns the\nchance. For if there can be a single-case chance of \\(\\frac{1}{2}\\) for a coin to\nland heads on a toss even if there is only one actual toss, and it\nlands tails, then surely the chance cannot be fixed by properties of\nthe outcome ‘lands heads’, as that outcome does not\n exist.[2]\n The chance must rather be grounded in features\nof the process that can produce the outcome: the coin-tossing trial,\nincluding the mass distribution of the coin and the details of how it\nis tossed, in this case, plus the background conditions and laws that\ngovern the trial. Whether or not an event happens by chance is a\nfeature of the process that produced it, not the event itself. The fact\nthat a coin lands heads does not fix that the coin landed heads by\nchance, because if it was simply placed heads up, as opposed to tossed\nin a normal fashion, we have the same outcome not by chance. Sometimes\nfeatures of the outcome event cannot be readily separated from the\nfeatures of its causes that characterise the process by means of which\nit was produced. But the upshot of the present discussion is that even\nin those cases, whether an outcome happens by chance is fixed by the\nproperties of the process leading up to it, the causal situation in\nwhich it occurs, and not simply by the fact that an event of a given\ncharacter was the product of that\n process.[3] \n\nDo chances exist? The best examples of probability functions that\nmeet the principles about chance are those provided by our best\nphysical theories. In particular, the probability functions that\nfeature in radioactive decay and quantum mechanics have some claim to\nbeing chance functions. In orthodox approaches to quantum mechanics,\nsome measurements of a system in a given state will not yield a result\nthat represents a definite feature of that prior state (Albert 1992).\nSo, for example, an \\(x\\)-spin measurement on a system in a\ndeterminate \\(y\\)-spin state will not yield a determinate result\nreflecting some prior state of \\(x\\)-spin, but rather has a 0.5\nprobability of resulting in \\(x\\)-spin \\(= +1\\), and a 0.5 probability\nof resulting in \\(x\\)-spin \\(= -1\\). That these measurement\nresults cannot reflect any prior condition of the system is a\nconsequence of various no-hidden variables theorems, the most\nfamous of which is Bell’s theorem (Bell 1964; see the entry on Bell’s\n theorem,\n Shimony 2009).\n Bell’s theorem\nshows that the probabilities predicted by quantum mechanics, and\nexperimentally confirmed, for spin measurements on a two-particle\nentangled but spatially separated system cannot be equal to the joint\nprobabilities of two independent one-particle systems. The upshot is\nthat the entangled system cannot be represented as the product of two\nindependent localised systems with determinate prior \\(x\\)-spin\nstates. Therefore, there can be no orthodox local account of these\nprobabilities of measurement outcomes as reflecting our ignorance of a\nhidden quality found in half of the systems, so that the probabilities\nare in fact basic features of the quantum mechanical systems\n themselves.[4] \n\nThe standard way of understanding this is that something—the\nprocess of measurement, on the Copenhagen interpretation, or\nspontaneous collapse on the GRW theory—induces a\nnon-deterministic state transition, called collapse, into a\nstate in which the system really is in a determinate state with respect\nto a given quality (though it was not previously). These transition\nprobabilities are dictated entirely by the state and the process of\ncollapse, which allows these probabilities to meet the stable trial\nprinciple. The models of standard quantum mechanics explicitly permit\ntwo systems prepared in identical states to evolve via collapse into\nany state which has a non-zero prior probability in the original state,\nwhich permits these probabilities to meet the BCP. And the no-hidden\nvariables theorems strongly suggest that there is no better information\nabout the system to guide credence in future states than the chances,\nwhich makes these probabilities play the right role in the PP. These\nbasic quantum probabilities governing state transitions seem to be\nstrong candidates to be called chances. \n\nThe foregoing argument makes essential use of collapse. The\nexistence of collapse as an alternative rule governing the evolution of\nthe quantum state controversial, and it is a scandal of quantum\nmechanics that we have no satisfactory understanding of why collapse\n(or measurement) should give rise to basic probabilities. But that\nsaid, the existence of well-confirmed probabilistic theories which\ncannot be plausibly reduced to any non-probabilistic theory is some\nevidence that there are chances. (Though the Everettian\n(‘many-worlds’) program of generating quantum probabilities\nfrom subjective uncertainty, without basic chance in the present sense,\nhas recently been gaining adherents—see Barrett 1999; Wallace\n2007.) Indeed, it looks like the strongest kind of evidence that there\nare chances. For if our best physical theories did not feature\nprobabilities, we should have little reason to postulate them, and\nlittle reason to take chances to exist. This will become important\nbelow\n (§5),\n when we discuss classical physics.\nThe conventional view of classical physics, including statistical\nmechanics, is that it does not involve basic probability (because the\nstate transition dynamics is deterministic), and is not accordingly a\ntheory that posits chances (Loewer\n 2001).[5]\n Below, we will examine\nthis view, as well as some of the recent challenges to this\nconventional view. But there is at least enough evidence from\nfundamental physics for the existence of chances for us to adopt it\nalready at this point as a defensible assumption. \n\nAs mentioned in the introduction, some philosophers deliberately use\n‘random’ to mean ‘chancy’. A random process, in\ntheir view, is one governed by chance in the sense of the previous\nsection. This generates stipulative definitions like this one: \n\nThis process conception of randomness is perfectly legitimate, if\nsomewhat redundant. But it is not adequate for our purposes. It makes the Commonplace Thesis a triviality, and thereby neither\ninteresting in itself nor apt to support the interesting conclusions\nsome have drawn from it concerning explanation or experimental\ndesign. Moreover,  \n\nThe invocation of a notion of process randomness is inadequate in another way, as it does not cover all cases of randomness. Take a clear case of process randomness, such as one thousand consecutive tosses of a fair coin. We would expect, with very high confidence, to toss at least one head. But as that outcome has some chance of not coming to pass, it counts as process random even when it does. This is at variance with what we would ordinarily say about such an outcome, which is not at all unexpected, haphazard, or unpredictable. We could search for some refinement of the notion of process randomness that would reserve the word ‘random’ for more irregular looking outcomes. But a better approach, and the one we pursue in this entry, is to distinguish between randomness of the process generating an outcome (which we stipulate to amount to its being a chance process), and randomness of the product of that random process. In the case just envisaged, we have a random process, while the outomce ‘at least one head in 1000 tosses’ is not a random product. \n\n The introduction of product randomness helps us make sense of some\nfamiliar uses of ‘random’ to characterise an entire\ncollection of outcomes of a given repeated process. This is the sense\nin which a random sample is random: it is an unbiased\nrepresentation of the population from which it is drawn—and that\nis a property of the entire sample, not each individual member. If a random sample is to do its job, it should be irregular and haphazard with respect to the population variables of interest. We should not be able to predict the membership of the sample to any degree of reliability by making use of some other feature of individuals in the population. (So we should not be able to guess at the likely membership of a random sample by using some feature like ‘is over 180cm tall’.) A random sample is one that is representative in the sense of being typical of the underlying population from which it is drawn, which means in turn that—in the ideal case—it will exhibit no order or pattern that is not exemplified in that underlying population. While\nmany random samples will be drawn using a random process, they need not\nbe. For example, if we are antecedently convinced that the final digit\nof someone’s minute of birth is not correlated with their family\nincome, we may draw a random sample of people’s incomes by choosing\nthose whose birth minute ends in ‘7’, and that process of\nchoice is not at all random. To be sure that our sample is random, we\nmay wish to use random numbers to decide whether to include a given\nindividual in the sample; to that end, large tables of random digits\nhave been produced, displaying no order or pattern (RAND Corporation\n1955). This other conception of randomness, as attaching primarily to\ncollections of outcomes, has been termed product\nrandomness. \n\nProduct randomness also plays an important role in scientific\ninference. Suppose we encounter a novel phenomenon, and attempt to give\na theory of it. All we have to begin with is the data concerning what\nhappened. If that data is highly regular and patterned, we may attempt\nto give a deterministic theory of the phenomenon. But if the data is\nirregular and disorderly—random—we may offer only a\nstochastic theory. As we cannot rely on knowing whether the phenomenon\nis chancy in advance of developing a theory of it, it is extremely\nimportant to be able to characterise whether the data is random or not\ndirectly, without detouring through prior knowledge of the process\nbehind it. We might think that we could simply do this by examination\nof the data—surely the lack of pattern will be apparent to the\nobserver? (We may assume that patternlessness is good evidence for\nrandomness, even if not entailed by it.) Yet psychological research has\nrepeatedly shown that humans are poor at discerning patterns, seeing\nthem in completely random data, and (for precisely the same reason, in\nfact) failing to see them in non-random data (Gilovich et al.,\n1985; Kahneman and Tversky, 1972; Bar-Hillel and Wagenaar, 1991; Hahn\nand Warren, 2009). So the need for an objective account of randomness\nof a sequence of outcomes is necessary for reliable scientific\ninference. \n\nIt might seem initially that giving a rigorous characterisation of\ndisorder and patternlessness is a hopeless task, made even more difficult by the fact that we need to characterise it without using the notion of chance. (Otherwise we make CT trivial.) Yet a series of\nmathematical developments in the theory of algorithmic\nrandomness, culminating in the early 1970s, showed that a\nsatisfactory characterisation of randomness of a sequence of outcomes\nwas possible. This notion has shown its theoretical fruitfulness not\nonly in the foundations of statistics and scientific inference, but\nalso in connection with the development of information theory and\ncomplexity theory. The task of this section is to introduce the\nmathematical approach to the definition of random sequences, just as\nwe introduced the philosophical consensus on chance in the previous\nsection. We will then be in a position to evaluate the Commonplace\nThesis, when made precise using theoretically fruitful notions of chance and randomness. \n\nThe fascinating mathematics of algorithmic randomness are largely\nunknown to philosophers. For this reason, I will give a fairly detailed\nexposition in this entry. Various points of more technical interest\nhave been relegated to this supplementary document: \n\nMost proofs will be skipped, or relegated to this supplementary\ndocument: \n\nFuller discussions can be found in the cited references. \n\nThroughout the focus will be on a simple binary process, which has\nonly two types of outcome \\(O = \\{0,1\\}\\). (The theory of\nrandomness for the outcome sequences of such a simple process can be\nextended to more complicated sets of outcomes, but there is much of interest even in the question which binary sequences are product random?) A sequence of\noutcomes is an ordered collection of events, finite or infinite,\nsuch that each event is of a type in \\(O\\). So a sequence\n\\(x = x_1 x_2 \\ldots x_k\\ldots\\),\nwhere each \\(x_i \\in O\\). The set of all\ninfinite binary sequences of outcomes is known as the\nCantor space. One familiar example of a process the outcomes\nof which form a Cantor space is an infinite sequence of independent\nflips of a fair coin, where 1 denotes heads and 0 tails. Notions from\nmeasure theory and computability theory are used in the discussion\nbelow; an elementary presentation of the mathematics needed can be\nfound in supplement\n B.2. \n\n Perhaps counterintuitively, we begin with the case of infinite binary sequences. Which of these should count as random products of our binary process?\n\nEach individual infinite sequence, whether orderly or not, has\nmeasure zero under the standard (Lebesgue) measure over the Cantor space.\nWe cannot determine whether an individual sequence is random from\nconsidering what fraction it constitutes of the set of all such\nsequences. But, intuitively, almost all such infinite sequences should\nbe random and disorderly, and only few will be orderly (an observation\nfirst due to Ville 1939). A typical infinite sequence is one\nwithout pattern; only exceptional cases have order to them. If the\nactual process that generate the sequences are perfectly deterministic,\nit may be that a typical product of that process is not random. But we\nare rather concerned to characterise which of all the possible\nsequences produced by any process whatsoever are random, and it seems\nclear that most of the ways an infinite sequence might be produced, and\nhence most of the sequences so produced, will be random. This fits with\nintuitive considerations:  This fertile remark underscores both that random sequences should be unruly, and that they should be common. In the present framework: the set of non-random sequences should\nhave measure zero, proportional to the set of all such\nsequences—correspondingly, the set of random sequences should\nhave measure one (Dasgupta, 2011: §3; Gaifman and Snir,\n1982: 534; Williams, 2008: 407–11). \n\nThis helps, but not much. For there are many measure one subsets of\nthe Cantor space, and we need some non-arbitrary way of selecting a\nprivileged such subset. (The natural option, to take the intersection\nof all measure one subsets, fails, because the complement of the\nsingleton of any specific sequence is measure one, so for each sequence\nthere is a measure one set which excludes it; therefore the\nintersection of all measure one sets excludes every sequence, so is the\nempty set.) The usual response is to take the random sequences to be\nthe intersection of all measure one subsets of the space which have\n‘nice’ properties, and to give some principled delimitation\nof which properties are to count as ‘nice’ and why. \n\nFor example, if a sequence is genuinely random, we should expect that\nin the long run it would tend to have features we associate with the\noutputs of (independent, identically distributed trials of) a chancy\nprocess. The sequence should look as disorderly as if it were the expected product of genuine chance. This approach is known accordingly as the typicality approach to randomness. Typicality is normally defined with respect to a prior probability function, since what is a typical series of fair coin toss outcomes might not be a typical series of unfair coin toss outcomes (Eagle 2016: 447). In the present case, we use the Lebesgue measure as it is the natural measure definable from the symmetries of the outcome space of the binary process itself.\n\n \nA typical sequence should satisfy all of the various ‘properties\nof stochasticity’ (Martin-Löf 1966: 604).  What are these\nproperties? They include the property of large numbers, the claim that\nthe limit frequency of a digit in a random sequence should not be\nbiased to any particular digit. The (strong) law of large\nnumbers is the claim that, with probability 1, an infinite sequence of\nindependent, identically distributed Bernoulli trials will have the\nproperty of large numbers. If we concentrate on the sequence of\noutcomes as independently given mathematical entities, rather than as\nthe products of a large number of independent Bernoulli trials, we can\nfollow Borel’s (1909) characterisation of the strong law. Let\n\\(S_n (x)\\) be the number of 1s occurring in the first \\(n\\) places of\nsequence \\(x\\) (this is just \\(\\sum^{n}_{k=1}x_{k}\\)), and let \\(B\\)\nbe the set of infinite sequences \\(x\\) such that the limit of \\(S_n\n(x)/n\\) as \\(n\\) tends to infinity is \\(\\frac{1}{2}\\). Borel’s\ntheorem is that \\(B\\) has measure one; almost all infinite sequences\nare, in the limit, unbiased with respect to digit frequency. \n\nClearly, the property of large numbers is a necessary condition for\nrandomness of a sequence. It is not sufficient, however. Consider the\nsequence 10101010…. This sequence is not biased. But it is\nclearly not random either, as it develops in a completely regular and\npredictable fashion. So we need to impose additional constraints. Each\nof these constraints will be another property of stochasticity we\nshould expect of a random sequence, including all other such limit\nproperties of ‘unbiasedness’. \n\nOne such further property is Borel normality, also defined\nin that paper by Borel. A sequence is Borel normal iff each finite\nstring of digits of equal length has equal frequency in the\n sequence.[6]\n Borel proved that a measure one set of\nsequences in the Cantor space are Borel normal. Borel normality is a\nuseful condition to impose for random sequences, as it has the\nconsequence that there will be no predictable pattern to the sequence:\nfor any string \\(\\sigma\\) appearing multiple times in a sequence, it will as\ncommonly be followed by a 1 as by a 0. This lack of predictability\nbased on the previous elements of the sequence is necessary for genuine\nrandomness. But again Borel normality is not sufficient for randomness.\nThe Champernowne sequence (Champernowne 1933) is the sequence\nof digits in the binary representations of each successive non-negative\ninteger: \n\nThis is Borel normal, but perfectly predictable, because there is a\ngeneral law which states what the value of the sequence at each index\nwill be—not because it can be predicted from prior elements of the sequence, but because it can be predicted from the index. \n\nWe must impose another condition to rule out the Champernowne\nsequence. We could proceed, piecemeal, in response to various problem\ncases, to successively introduce further stochastic properties, each of\nwhich is a necessary condition for randomness, eventually hoping to\ngive a characterisation of the random sequences by aggregating enough\nof them together. Given the complex structure of the Cantor space, the\nprospects for success of such a cumulative approach seem dim. A more\npromising bolder route is to offer one stochastic property that is by\nitself necessary and sufficient for randomness, the possession of which\nwill entail the possession of the other properties we’ve mentioned (the\nproperty of large numbers, Borel normality, etc.). \n\nThe first detailed and sophisticated attempt at a bolder approach to\ndefining randomness for a sequence with a single stochastic property\nwas by von Mises (von Mises, 1957; von Mises, 1941). Suppose you were\npresented with any subsequence \\(x_1 , \\ldots ,x_{n-1}\\) of (not necessarily consecutive\nmembers of) a sequence, and asked to predict the value of\n\\(x_n\\). If the sequence were really random,\nthen this information—the values of any previous members of the\nsequence, and the place of the desired outcome in the\nsequence—should be of no use to you in this task. To suppose\notherwise is to suppose that there is an exploitable regularity in the\nrandom sequence; a gambler could, for example, bet reliably on their\npreferred outcome and be assured of a positive expected gain if they\nwere in possession of this information. A gambling system selects\npoints in a sequence of outcomes to bet on; a successful gambling\nsystem would be one where the points selected have a higher frequency\nof ‘successes’ than in the sequence as a whole, so that by\nemploying the system one can expect to do better than chance. But the\nfailure of gambling systems to make headway in games of chance suggests\nthat genuinely random sequences of outcomes aren’t so exploitable. Von\nMises, observing the empirical non-existence of successful gambling\nsystems, makes it a condition of randomness for infinite sequences that\nthey could not be exploited by a gambling system (his ‘Prinzip\nvom ausgeschlossenen Spielsystem’). The idea is that without what\neffectively amounts to a crystal ball, there is no way of selecting a\nbiased selection of members of a random sequence. \n\nShorn of the inessential presentational device of selecting an\noutcome to bet on based on past outcomes, von Mises contends that it is\na property of stochasticity that a random sequence should not be such\nthat information about any initial subsequence\n\\(x_1 x_2 \\ldots x_{k-1}\\)\nprovides information about the contents of outcome\n\\(x_k\\). He formally implements this idea by defining a\nplace selection as ‘the selection of a partial sequence\nin such a way that we decide whether an element should or should not be\nincluded without making use of the [value] of the element’ (von\nMises 1957: 25). He then defines a random sequence as one such that\nevery infinite subsequence selected by an admissible place selection\nretains the same relative digit frequencies as in the original sequence\n(so one cannot select a biased subsequence, indicating that this is a\ngenuine property of stochasticity). In our case this will mean that\nevery admissibly selected subsequence will meet the property of large\nnumbers with equal frequency of 1s and\n 0s.[7]\n One way to characterise\nthe resulting set of von Mises-random (vM-random) sequences is that it\nis the largest set which contains only infinite sequences with the\nright limit frequency and is closed under all admissible place\nselections. If the limit frequency of a digit is 1, say in the sequence\n\\(111\\ldots\\), it is true that every admissible place selection\ndetermines a subsequence with the same limit frequency. Von Mises\nintends this result, for this is what a random sequence of outcomes of\ntrials with probability 1 of obtaining the outcome 1 looks like. This\nsequence does not meet the property of large numbers, however. So we\nmodify von Mises’ own condition, defining the vM-random sequences as\nthe largest set of infinite sequences which have the limit frequency\n\\(\\frac{1}{2}\\), and which is closed under all admissible place selections. \n\nVon Mises’ original proposals were deliberately imprecise about what\nkinds of procedures count as admissible place selections. This\nimprecision did not concern him, as he was disposed to regard the\n‘right’ set of place selections for any particular random\ncollective as being fixed by context and not invariantly specifiable.\nBut his explicit characterisation is subject to counterexamples. Since\n‘any increasing sequence of natural numbers\n\\(n_1 \\lt n_2 \\lt \\ldots\\)\ndefines a corresponding selection rule, … given an arbitrary\nsequence of 0s and 1s … there is among the selection rules the\none which selects the 1s of the given sequence, so the limit frequency\nis changed’ (Martin-Löf 1969b: 27). This clearly violates\nvon Mises’ intentions, as he presumably intended that the place\nselections should be constructively specified, yet the notion of\nvM-randomness remains tantalisingly vague without some more concrete\nspecification. \n\nSuch a specification arrived in the work of Church (1940), drawing\non the then newly clarified notion of an effective procedure. Church\nobserved that \n\nChurch therefore imposes the condition that the admissible place\nselections should be, not arbitrary functions, but effectively\ncomputable functions of the preceding outcomes in a sequence.\nFormally, we consider (following Wald 1938) a place selection as a\nfunction \\(f\\) from an initial segment\n\\(x_1 x_2 \\ldots x_{i-1}\\)\nof a sequence \\(\\sigma\\) into \\(\\{0,1\\}\\), such that the selected\nsubsequence \\(\\sigma' = \\{x_i : f(x_1 \\ldots x_{i - 1}) = 1\\}\\), Church’s proposal is that we admit only those\nplace selections which are computable (total recursive)\n functions.[8]\n Church’s proposal applies equally to the\nsequences von Mises is concerned with, namely those with arbitrary\nnon-zero outcome frequencies for each type of outcome; to get the\nrandom sequences, we again make the restriction to those normal binary\nsequences where the limit relative frequency of each outcome is \\(\\frac{1}{2}\\)\n(these are sometimes called the Church stochastic\nsequences). \n\nAs Church points out, if we adopt the Church-Turing computable\nfunctions as the admissible place selections, it follows quickly that\nthe set of admissible place selections is countably infinite. We may\nthen show: \n\n Theorem 1 (Doob-Wald). The set of\n random sequences forms a measure one subset of the Cantor space. \n\n [Proof] \n\nThus von Mises’ conception of randomness was made mathematically\nrobust (Martin-Löf 1969b). We can see that various properties of\nstochasticity follow from this characterisation. For example, we can\nshow: \n\n Corollary 1. Every von\n Mises-random sequence is Borel normal. \n\n [Proof] \n\nThese successes for the approach to randomness based on the\nimpossibility of gambling systems were, however, undermined by a\ntheorem of Ville (1939): \n\n Theorem 2 (Ville). For any\n countable set of place selections \\(\\{\\phi_n\\}\\)\n(including the identity), there exists an infinite binary\nsequence \\(x\\) such that: \n\nThat is, for any specifiable set of place selections, including the\ntotal recursive place selections proposed by Church as the invariantly\nappropriate set, there exist sequences which have the right limit\nrelative frequencies to satisfy the strong law of large numbers (and\nindeed Borel normality), as do all their acceptable subsequences, but\n which are biased in all initial\n segments.[9] \n\nWhy should this be a problem for random sequences? The property of\nlarge numbers shows that almost all infinite binary sequences have\nlimit digit frequency \\(\\frac{1}{2}\\), but says nothing about how quickly\nthis convergence happens or about the statistical properties of the\ninitial segments. There certainly exist sequences that converge to\n\\(\\frac{1}{2}\\) but\nwhere \\(S_n (x)/n \\gt \\frac{1}{2}\\) for all \\(n\\) (the sequence has the right mean but\nconverges ‘from above’). In the ‘random walk’\nmodel of our random sequences, where each element in the sequence is\ninterpreted as a step left (if 0) or right (if 1) along the integer\nline, this sequence would consist of a walk that (in the limit) ends\nup back at the origin but always (or even eventually) stays to the\nright. Intuitively, such a sequence is not random. \n\nSuch sequences do indeed violate at least one property of\nstochasticity, as it turns out that in a measure one set of sequences,\n\\(S_n (x)/n\\) will be above\nthe mean infinitely many times, and below the mean infinitely many\ntimes. So stated, this is the law of symmetric oscillation\n(Dasgupta, 2011:\n 13).[10]\n Since the law of symmetric oscillations\nholds for a measure one set of sequences, it is a plausible property of\nrandomness (it is naturally of a family with other properties of\nstochasticity). Ville’s result shows that von Mises’ definition in\nterms of place selections cannot characterise random sequences exactly\nbecause it includes sequences that violate this law (so don’t\ncorrespond to a truly random random walk). Indeed, such sequences don’t\neven correspond to von Mises’ avowed aims. As Li and Vitányi say\n(2008: 54), ‘if you bet 1 all the time against such a sequence of\noutcomes, your accumulated gain is always positive’. As such,\nVille-style sequences seem to permit successful gambling, despite the\nfact that they do not permit a system to be formulated in terms of\nplace selections. \n\nVon Mises and Church identified a class of sequences, those with\nlimit frequencies invariant under recursive place selections, that\nsatisfied a number of the measure one stochastic properties of\nsequences that are thought characteristic of randomness. But the class\nthey identified was too inclusive. The next insight in this area was\ndue to (Martin-Löf 1966), who realised that rather than looking\nfor another single property of sequences that would entail\nthat the sequence met all the further conditions on randomness, it was\nsimpler to adopt as a definition that a sequence is random iff the\nsequence has all the measure one properties of randomness that can be\nspecified. Here again recursion theory plays a role, for the idea of\na measure one property of randomness that can be specified is\nequivalent to the requirement that there be an effective procedure for\ntesting whether a sequence violates the property. This prompts the\nfollowing very bold approach to the definition of random sequences: \n\nRecalling the definition of effective measure zero from\n supplement\n B.2,\n Martin-Löf suggests that a\nrandom sequence is any that does not belong to any effective measure\nzero set of sequences, and thus belongs to every effective measure one\nset of sequences. An effective measure zero set of sequences will\ncontain sequences which can be effectively determined to have a\n‘special hallmark’ (for example, having a ‘1’\nat every seventh place, or never having the string ‘000000’\noccurring as a subsequence). It is part of von Mises’ insight that no\nrandom sequence will possess any of these effectively determinable\nspecial hallmarks: such hallmarks would permit exploitation as part of\na gambling system. But Martin-Löf notices that all of the commonly\nused measure one properties of stochasticity are effective measure one.\nAny sequence which violates the property of large numbers, or the law\nof symmetric oscillations, etc., will do so on increasingly long\ninitial subsequences. So the violation of any such property will also\nbe a special hallmark of a non-random sequence, an indicator that the\nsequence which possesses it is an unusual one. Since the unusual\nproperties of non-stochasticity in question are effective measure zero,\nwe can therefore say that the random sequences are those which are not\nspecial in any effectively determinable way. To formalise this,\nMartin-Löf appeals to the language of significance testing. His\nmain result is sometimes put as the claim that random sequences as\nthose which pass all recursive significance tests for sequences\n(Schnorr 1971: §1)—they are never atypical enough to prompt us to reject the hypothesis that they are random. See supplement\n B.1.1\n for more detail on this\npoint. \n\nNote that the restriction to effective properties of sequences is\ncrucial here. If we allowed, for example, the property being\nidentical to my favourite random sequence x, that would define a\ntest which the sequence \\(x\\) would fail, even though it is\nrandom. But it follows from our observations about von Mises randomness\n(which is still a necessary condition on randomness) that no\neffectively computable sequence is random (if it were, there would be a\nplace selection definable from the algorithm that selected all the 1s\nin the sequence). So there is no effective test that checks whether a\ngiven sequence is identical to some random sequence. \n\nThe central result of Martin-Löf (1966) is the following: \n\n Theorem 3 (Universal Tests and the\n Existence of Random Sequences). There is a universal test\nfor ML-randomness; moreover, only a measure zero set of infinite\nbinary sequences fails this test. So almost all such sequences are\nML-random. \n\n [Proof] \n\nThe universal test does define an effective measure one property,\nbut (unlike normality, or having no biased admissible subsequences), it\nis far from a naturally graspable property. Nevertheless,\nMartin-Löf’s result does establish that there are random sequences\nthat satisfy all the properties of stochasticity, and that in fact\nalmost all binary sequences are random in that sense. Returning to\n Ville’s theorem\n 2,\n it can be shown that all\nML-random sequences satisfy the law of symmetric oscillations (van\nLambalgen 1987a: §3.3). Hence the kind of construction Ville uses\nyields vM-random sequences which are not ML-random. All the ML-random\nsequences have the right limit relative frequencies, since they satisfy\nthe effective measure one property of large numbers. So Martin-Löf\nrandom sequences satisfy all the intuitive properties we would expect\nof a sequence produced by tosses of a fair coin, but are characterised\nentirely by reference to the effectively specifiable measure one sets\nof infinite sequences. We have therefore characterised random sequences\nentirely in terms of the explicit features of the product, and not of\nthe process that may or may not lie behind the production of these\nsequences. \n\nThere are other accounts that develop and extend Martin-Löf’s\naccount of randomness, in the same kind of framework, such as that of\nSchnorr (1971); for some further details, see supplement\n B.1.2. \n\nFor infinite binary sequences, the Martin-Löf definition in\nterms of effective tests is a robust and mathematically attractive\nnotion. However, it seems to have the major flaw that it applies only\nto infinite binary sequences. (Since finiteness of a sequence is\neffectively positively decidable, and the set of all finite sequences\nis measure zero, every finite sequence violates an effective measure\none randomness property.) Yet ordinarily we are happy to characterise\neven quite small finite sequences of outcomes as random. As mentioned\n above\n (§2),\n there is room for doubt at our\nability to do so correctly, as we seem to be prone to mischaracterise\nsequences we are presented with, and perform poorly when asked to\nproduce our own random sequences. However, there is nothing in this\nliterature to suggest that we are fundamentally mistaken in applying the notion of randomness to finite sequences at all. So one might think\nthis shows that the Martin-Löf approach is too\nrestrictive. \n\nYet there is something in the idea of ML-randomness that we might\napply profitably to the case of finite sequences. Since being generated\nby an effective procedure is a measure zero property of infinite\nsequences, given that there are only countably many effective\nprocedures, it follows immediately that no ML-random sequence can be\neffectively produced. This fits well with the intuitive idea that\nrandom sequences don’t have the kind of regular patterns that any\nfinite algorithm, no matter how complex, must exploit in order to\nproduce an infinite sequence. This contrast between random sequences\nwhich lack patterns that enable them to be algorithmic generated, and\nnon-random sequences which do exhibit such patterns, does not apply\nstraightforwardly to the finite case, because clearly there is an\neffective procedure which enables us to produce any particular finite\nsequence of outcomes—simply to list those outcomes in the\nspecification of the algorithm. But a related contrast does\nexist—between those algorithms which are simply crude lists of\noutcomes, and those which produce outcomes which involve patterns and\nregularities in the outcome sequence. This leads us to the idea that\nfinite random sequences, like their infinite cousins, are not able to\nbe generated by an algorithm which exploits patterns in the outcome\nsequence. The outcomes in random sequences are thus patternless, or\ndisorderly, in a way that is intuitively characteristic of\nrandom sequences. \n\nDisorderly sequences, in the above sense, are highly\nincompressible. The best effective description we can give of\nsuch a sequence—one that would enable someone else, or a\ncomputer, to reliably reproduce it—would be to simply list the\nsequence itself. This feature allows us to characterise the random\nsequences as those which cannot be produced by a compact algorithm\n(compact with respect to the length of the target sequence, that is).\nGiven that algorithms can be specified by a list of Turing machine\ninstructions, we have some basic idea on how to characterise the length\nof an algorithm. We can then say that a random sequence is one such\nthat the shortest algorithm which produces it is approximately (to be\nexplained below) the same length as the sequence itself—no\ngreater compression in the algorithm can be attained. This proposal,\nsuggested by the work of Kolmogorov, Chaitin and Solomonov (KCS),\ncharacterises randomness as the algorithmic or informational\ncomplexity of a sequence. Comprehensive surveys of complexity\nand the complexity-based approach to randomness are Li and\nVitányi 2008 and Downey and Hirschfeldt 2010: Part I. (See also\nChaitin 1975, Dasgupta 2011: §7, Downey et al. 2006:\n§§1–3, Earman 1986: 141–7, Kolmogorov 1963,\nKolmogorov and Uspensky 1988, Smith (1998: ch. 9) and van Lambalgen\n1995.) \n\nIf \\(f\\) is effectively computable—a recursive\nfunction—let us say that \\(\\delta\\) is an \\(f\\)-description of a\nfinite string \\(\\sigma\\) iff \\(f\\) yields \\(\\sigma\\) on input\n\\(\\delta\\). We may define the \\(f\\)-complexity of a string\n\\(\\sigma , C_f (\\sigma)\\), as the length of the shortest string\n\\(\\delta\\) that \\(f\\)-describes \\(\\sigma\\). If there is no such\n\\(\\delta\\), let the \\(f\\)-complexity of \\(\\sigma\\) be infinite. \\(f\\)\nis thus a decompression algorithm, taking the compressed\ndescription \\(\\delta\\) back to the original string \\(\\sigma\\). There\nare obviously many different kinds of decompression algorithm. One\nboring case is the identity function (the empty program), which takes\neach string to itself. The existence of this function shows that there\nare decompression algorithms \\(f\\) which have a finite\n\\(f\\)-complexity for any finite string. Any useful decompression\nalgorithm will, however, yield an output string significantly longer\nthan the input description, for at least some input descriptions. \n\nOne example is this algorithm: on input of a binary string \\(\\delta\\)\nof length \\(4n\\), the algorithm breaks the input down into \\(n\\)\nblocks of 4, which it turns into an output sequence \\(\\sigma\\), as\nfollows. Given a block \\(b_1 , \\ldots ,b_4\\), it produces a block of\nthe symbol contained in \\(b_1\\), the length of which is governed by\nthe binary numeral \\(b_2 b_3 b_4\\). So the block 1101 produces a\nstring of five 1s. The output sequence is obtained by concatenating\nthe output of successive blocks in order.  Every string \\(\\sigma\\) can\nbe represented by this algorithm, since the string \\(\\sigma '\\) which\ninvolves replacing every 1 in \\(\\sigma\\) by 1001, and every 0 by 0001,\nwill yield \\(\\sigma\\) when given as input to this algorithm. So this\nalgorithm has finite complexity for any string. But this algorithm can\ndo considerably better; if the original string, for example, is a\nstring of sixteen 1s, it can be obtained by input of this description:\n11111111, which is half the length. Indeed, as reflection on this\nalgorithm shows, this algorithm can reconstruct an original string\nfrom a shorter description, for many strings, particularly if they\ncontain reasonably long substrings of consecutive 1s or 0s. \n\nHowever, there is a limit on how well any algorithm can compress a\nstring. If \\(\\lvert\\sigma\\rvert\\) is the length of \\(\\sigma\\), say\nthat a string \\(\\sigma\\) is compressed by \\(f\\) if there is\nan \\(f\\)-description \\(\\delta\\) such that \\(\\sigma = f(\\delta)\\) and\n\\(\\lvert\\delta\\rvert \\lt \\lvert\\sigma\\rvert\\). If a useful decompression\nalgorithm is such that for some fixed \\(k\\), \\(\\lvert f(\\delta)\\rvert \\le \\lvert\\delta\\rvert +\nk\\), so that \\(f\\)-descriptions are at least \\(k\\) shorter than the\nsequence to be compressed, then it follows that very few strings\nusefully compress. For there are \\(2^l\\) strings \\(\\sigma\\) such that\n\\(\\lvert\\sigma\\rvert = l\\); so there are at most \\(2^{l - k} f\\)-descriptions;\nsince \\(f\\) is a function, there are at most \\(2^{l-k}\\) compressible\nstrings. As a proportion of all strings of length \\(l\\), then, there\nare at most \\(2^{l-k}/2^{l} = \\frac{1}{2}^{k}\\) compressible strings\n. This means that as the amount of compression required increases, the\nnumber of sequences so compressible decreases exponentially. Even in\nthe most pitiful amount of compression, \\(k = 1\\), we see that at most\nhalf the strings of a given length can be compressed by any algorithm\n\\(f\\). \n\nSo our interest must be in those decompression functions which do best\noverall. We might hope to say: \\(f\\) is better than \\(g\\) iff for all\n\\(\\sigma , C_f (\\sigma) \\le C_g (\\sigma)\\). Unfortunately, no function\nis best in this sense, since for any given string \\(\\sigma\\) with\n\\(f\\)-complexity \\(\\lvert\\sigma\\rvert - k\\), we can design a function\n\\(g\\) as follows: on input 1, output \\(\\sigma\\); on\ninput n\\(\\delta\\) (for any \\(n)\\), output\n\\(f(\\delta)\\). (Generalising, we can add arbitrarily long prefixes of\nlength \\(m\\) onto the inputs to \\(g\\) and have better-than\\(-f\\)\ncompression for \\(2^m\\) sequences.) But we can define a notion of\ncomplexity near-superiority of \\(f\\) to \\(g\\) iff\nthere is some constant \\(k\\) such that for any string,\n\\(C_f (\\sigma) \\le C_g (\\sigma) + k.\nf\\) is least as good as \\(g\\), subject to some constant\nwhich is independent of the functions in question. If \\(f\\) and\n\\(g\\) are both complexity near-superior to each other, for the\nsame \\(k\\), we say they are complexity equivalent. \n\nKolmogorov (1965) showed that there is an optimal decompression\nalgorithm: \n\n Theorem 4 (Kolmogorov). There\n exists a decompression algorithm which\nis near-superior to any other program. Moreover, any such optimal\nalgorithm is complexity equivalent to any other optimal algorithm (see\nalso Chaitin 1966 and Martin-Löf 1969a). \n\n [Proof] \n\nSuch a universal function Kolmogorov called asymptotically\noptimal (for as \\(\\lvert\\sigma\\rvert\\) increases, the constant\n\\(k\\) becomes asymptotically negligible). \n\nChoose some such asymptotically optimal function \\(u\\), and\ndefine the complexity (simpliciter) \\(C(\\sigma) = C_u (\\sigma)\\). Since \\(u\\) is optimal,\nit is near-superior to the identity function; it follows that there\nexists a \\(k\\) such that \\(C(\\sigma) \\le \\lvert\\sigma\\rvert\n+ k\\). On the other hand, we also know that the number of\nstrings for which \\(C(\\sigma) \\le \\lvert\\sigma\\rvert\n- k\\) is at most \\(\\frac{1}{2} ^k\\). We know\ntherefore that all except \\(1 - 2^k\\) sequences\nof length \\(n\\) have a complexity within \\(k\\)\nof \\(n\\). As \\(n\\) increases, for fixed large \\(k\\),\ntherefore, we see that almost all sequences have complexity of\napproximately their length. All this can be used to make precise the\ndefinition of randomness sketched above. \n\nIt follows from what we have just said that there exist random\nsequences for any chosen length \\(n\\), and that as \\(n\\)\nincreases with respect to \\(k\\), random sequences come to be the\noverwhelming majority of sequences of that length. Theorem 5. A random sequence\n of a given length cannot be effectively produced. \n\n [Proof] \n\nAn immediate corollary is that the complexity function \\(C\\) is\nnot a recursive function. If it were, for any \\(n\\), we could\neffectively compute \\(C(\\sigma)\\) for any \\(\\sigma\\) of\nlength \\(n\\). By simply listing all such sequences, we could halt\nafter finding the first \\(\\sigma\\) for which \\(C(\\sigma)\n\\ge n\\). But then we could effectively produce a random\nsequence, contrary to theorem\n 5. \n\nThe notion of Kolmogorov randomness fits well with the intuitions\nabout the disorderliness of random sequences we derived from the\nMartin-Löf account. It also fits well with other intuitions about\nrandomness—random sequences don’t have a short description, so\nthere is no sense in which they are produced in accordance with a\nplan. As such, Kolmogorov randomness also supports von Mises’\nintuitions about randomness being linked to the impossibility of\ngambling systems, as there will be no way of effectively producing a\ngiven random sequence of outcomes using a set of initially given data\nany smaller than the sequence itself. There is no way of predicting a\ngenuinely random sequence in advance because no random sequence can be\neffectively produced, yet every predictable sequence of outcomes can\n(intuitively) be generated by specifying the way in which future\noutcomes can be predicted on the basis of prior outcomes. Moreover,\nbecause for increasing \\(k\\) the number of strings of length\n\\(n\\) which are random increases, and because for increasing\n\\(n\\) we can choose larger and larger \\(k\\), there is some\nsense in which the great majority of sequences are random; this matches\nwell the desiderata in the infinite case that almost all sequences\nshould be random. Finally, it can be shown that the Kolmogorov\nrandomness of a sequence is equivalent to that sequence passing a\nbattery of statistical tests, in the Martin-Löf\nsense—indeed, that the Kolmogorov random sequences are just those\nthat pass a certain universal test of non-randomness (Martin-Löf\n1969a:\n §2).[11] \n\nThe plain Kolmogorov complexity measure is intuitively appealing.\nYet the bewildering variety of permitted \\(f\\)-descriptions\nincludes many unmanageable encodings. In particular, for a given\ndecompression algorithm \\(f\\), there are \\(f\\)-descriptions\n\\(\\gamma\\) and \\(\\delta\\) such that \\(\\delta = \\gamma\\unicode{x2040}\\tau\\), for some string\n\\(\\tau\\). This is an inefficient encoding, because if\n\\(\\gamma\\) can occur both as a code itself, and as the initial\npart of another code, then an algorithm cannot decode its input string\n‘on the fly’ as soon as it detects a comprehensible input,\nbut must wait until it has scanned and processed the entire input\nbefore beginning to decode it. An efficient coding, such that no\nacceptable input is an initial substring of another acceptable input,\nis called prefix-free (because no member is a prefix of any\nother member). A good example of an encoding like this is the encoding\nof telephone numbers: the telephone exchange can, on input of a string\nof digits that it recognises, immediately connect you; once an\nacceptable code from a prefix-free set has been input, no other\nacceptable code can follow it. \n\nPrefix-free encodings are useful for a number of practical purposes,\nand they turn out to be useful in defining randomness also. (As we will\nsee in\n §2.3,\n they are of special importance in\navoiding a problem in the definition of infinite Kolmogorov random\nsequences.) The change is the natural one: we appeal, not to the plain\ncomplexity of a sequence in defining its randomness, but the so-called\nprefix-free complexity (Downey and Hirschfeldt 2010:\n§2.5ff; Dasgupta 2011: §8). \n\nTo fix ideas, it is useful to have an example of a prefix-free\nencoding in mind. Suppose we have a string\n\\(\\sigma =x_1 \\ldots x_k\\)\nof length \\(k\\). This is the initial part of the string\n\\(\\sigma 1\\), so if any string was an acceptable input, we would\nnot have a prefix-free encoding. But if the code contained information\nabout the length of the string encoded, we would know that the\nlength of \\(\\sigma , k\\), is less than the length of\n\\(\\sigma 1\\). We can make this idea precise as follows (using a\ncode similar to that used to a different end in the\n proof\n of Theorem\n 4).\nLet the code of \\(\\sigma\\) be the string\n\\(1^{[\\lvert\\sigma\\rvert]}0\\sigma\\)—that is, the\ncode of a string consists of a representation of the length of the\nstring, followed by a 0, followed by the string. This is clearly a\nprefix-free\n encoding.[12]\n This coding is not particularly efficient,\nbut more compact prefix-free encodings do exist. \n\nThe notion of prefix-free complexity is defined in exactly the same\nway as plain complexity, with the additional restriction that the\nallowable \\(f\\)-descriptions of a string, given a decompression\nfunction \\(f\\), must form a prefix-free set. With an appropriate\nchoice of coding, we can get a set of \\(f\\)-descriptions which is\nmonotonic with increasing length, i.e., if \\(\\lvert\\gamma\\rvert \\lt \\lvert\\delta\\rvert\\) \nthen \\(\\lvert f(\\gamma)\\rvert \\lt \\lvert f(\\delta)\\rvert\\). Our definition \ngoes much as before: The prefix free Kolmogorov complexity, given a\ndecompression function\n\\(f\\) with a prefix-free domain, of a string \\(\\sigma\\),\ndenoted \\(K_f (\\sigma)\\), is the\nlength of the shortest \\(f\\)-description of \\(\\sigma\\) (and\ninfinite otherwise). Since\n\\(1^{[\\lvert\\sigma\\rvert]}0\\sigma\\) is a finite\nprefix-free code for \\(\\sigma\\), we know there are at least some\nprefix-free decompression algorithms with finite\n\\(K_f\\) for every string. As before, we can\nshow there exist better decompression algorithms than this one, and\nindeed, that there exists a universal prefix-free decompression\nalgorithm \\(u\\), such that for every other algorithm \\(m\\)\nthere is a \\(k\\) such that\n\\(K_u (\\sigma) \\le K_m (\\sigma) + k\\), for all\n\\(\\sigma\\) (Downey and Hirschfeldt 2010: §2.5). We define\n\\(K(\\sigma) = K_u (\\sigma)\\). \n\nSince the set of prefix-free codes is a subset of the set of all\npossible codes, we should expect generally that\n\\(C(\\sigma) \\le K(\\sigma)\\). On the\nother hand, we can construct a universal prefix-free algorithm\n\\(u\\) as follows. A universal Turing machine \\(u'\\)\ntakes as input the Gödel number of the Turing machine we wish to\nsimulate, and the input we wish to give to that machine. Let us\nconcatenate these two inputs into a longer input string that is itself\nuniquely readable; and we then encode that longer string into our\nprefix-free encoding. The encoding is effectively computable, clearly,\nso we can chain a decoding machine together with our universal machine\n\\(u'\\); on input an acceptable prefix-free string, the\ndecoder will decompose it into the input string, we then decompose the\ninput string into the Gödel number and the input, and run the\nmachine \\(u'\\) on that pair of inputs. Depending on the\nparticular encoding we choose, we may establish various bounds on\n\\(K\\); one obvious bound we have already established is that\n\\(C(\\sigma) \\le K(\\sigma) \\lt C(\\sigma) + 2\\lvert\\sigma\\rvert\\). By using a more\nefficient prefix-free coding of a \\(u'\\)-description, we can\nestablish better\n bounds.[13]\n (Some more results on the connection\nbetween \\(K\\) and \\(C\\) are in Downey and Hirschfeldt 2010:\n§§3.1–3.2.) \n\nWith prefix-free complexity in hand, we may define: \n\nAgain, there do exist prefix-free random sequences, since we know\nthat there are plain random sequences, and given the greater length of\na prefix-free encoding, we know that the prefix-free code of ordinary\nrandom sequence will be generally longer than an arbitrary code of it,\nand thus random too. Indeed, there will be more prefix-free random\nsequences because strings compress less effectively under \\(K\\)\nthan \\(C\\). Yet \\(K\\) and \\(C\\) behave similarly enough\nthat the success of plain Kolmogorov complexity at capturing our\nintuitions about randomness carry over to prefix-free Kolmogorov\nrandomness, and the label ‘Kolmogorov random’ has come to\nbe used generally to refer to prefix-free Kolmogorov random\nsequences. \n\nBoth plain and prefix-free Kolmogorov randomness provide\nsatisfactory accounts of the randomness of finite sequences. One\ndifficulty arises, as I suggested earlier, when we attempt to extend\nplain Kolmogorov randomness to the case of infinite sequences in the\nmost obvious way, that is, by defining an infinite sequence as\nKolmogorov random iff all finite initial segments are Kolmogorov\nrandom. It would then turn out that no infinite sequence is random.\nWhy? Because of the following theorem, which shows that there is no\nsequence such that all of its initial segments are random: \n\n Theorem 6 (Martin-Löf\n 1966). For any sufficiently long string, there will always \nbe some fairly compressible initial segments. (See also Li and Vitányi 2008:\n§2.5.1 and Downey and\nHirschfeldt 2010: §2.1.) \n\n [Proof] \n\nThis dip in complexity of an initial subsequence will occur infinitely\noften in even a random infinite sequence, a phenomenon known\nas complexity oscillation (Li and Vitányi 2008:\n§2.5.1). This phenomenon means that ‘it is difficult to\nexpress a universal sequential test precisely in terms\nof \\(C\\)-complexity’ (Li and Vitányi 2008: 151), and\nthe best that can be precisely done is to find upper and lower bounds\nexpressible in terms of ordinary Kolmogorov complexity between which\nthe set of ML-random sequences falls (Li and Vitányi 2008:\n§ 2.5.3).  \n\nHowever, the phenomenon of complexity oscillation does not pose as\nsignificant a problem for prefix-free Kolmogorov\ncomplexity. Complexity oscillation does arise, but in fact the\ninefficiency of prefix-free encodings is a benefit here:\n‘\\(K\\) exceeds \\(C\\) by so much that the complexity of\nthe prefix does not drop below the length of the prefix itself (for\nrandom infinite \\(\\omega)\\)’ (Li and Vitányi 2008:\n221). That is, while the complexity of some initial segments dips\ndown, it always remains greater than the length of the prefix. So it\ncan be that, uniformly, when \\(x\\) is an infinite sequence, for\nany of its initial subsequences \\(\\sigma , K(\\sigma) \\ge \n\\lvert\\sigma\\rvert\\). This suggests that we can extend prefix-free Kolmogorov\ncomplexity to the infinite case in the straightforward way: an\ninfinite sequence \\(x\\) is prefix-free Kolmogorov random iff\nevery finite initial subsequence is prefix-free Kolmogorov random. \n\nWith this definition in hand we obtain a very striking result. The\nclass of infinite prefix-free Kolmogorov random sequences is certainly\nnon-empty. Indeed: it is just the class of ML-random sequences! \n\n Theorem 7 (Schnorr). A sequence \n is ML-random iff it is prefix-free Kolmogorov random.  \n\n [Proof] \n\nSchnorr’s theorem is evidence that we really have captured the\nintuitive notion of randomness. Different intuitive starting points\nhave generated the same set of random sequences. This has been taken\nto be evidence that ML-randomness or equivalently (prefix-free)\nKolmogorov randomness is really the intuitive notion of randomness, in\nmuch the same way as the coincidence of Turing machines, Post\nmachines, and recursive functions was taken to be evidence\nfor Church’s Thesis, the claim that any one of these notions\ncaptures the intuitive notion of effective computability. Accordingly,\nDelahaye (1993) has proposed the Martin-Löf-Chaitin\nThesis, that either of these definitions captures the intuitive\nnotion of randomness. If this thesis is true, this undermines at least\nsome sceptical contentions about randomness, such as the claim of\nHowson and Urbach (1993: 324) that ‘it seems highly doubtful\nthat there is anything like a unique notion of randomness there to be\nexplicated’. \nThere are some reasons to be suspicious of the Martin-Löf-Chaitin\nThesis, despite the mathematically elegant convergence between these\ntwo mathematical notions. For one, there is quite a bit of intuitive\nsupport for accounts of randomness which do not make it primarily a\nproperty of sequences, and those other accounts are no less able to be\nmade mathematically rigorous (see especially the\n‘epistemic’ theories of randomness discussed in\n§6.2, as well as theories of randomness as\nindeterminism discussed in §7.2). The\nexistence of other intuitive notions makes the case of randomness\nrather unlike the supposedly analogous case of Church’s Thesis, where\nno robust alternative characterisation of effective computability is\navailable.   Even if we accept that randomness, like disorder, is at root a\nproduct notion, there are a number of candidates in the vicinity of\nthe set identified by Schnorr’s thesis that might also deserve to be\ncalled the set of random sequences. Most obviously, there is Schnorr’s\nown conception of randomness (§2.1.2;\nsupplement B.1.2). Schnorr\n(1971) suggests that, for technical and conceptual reasons, Schnorr\nrandomness is to be preferred to Martin-Löf randomness as an\naccount of the intuitive notion. While results that parallel the\nconvergence of ML-randomness and Kolmogorov randomness have been given\n(Downey and Griffiths 2004), the relevant compressibility notion of\nrandomness for Schnorr randomness was not known until quite recently,\nand is certainly less intuitively clear than Kolmogorov\nrandomness. Moreover, since the set of ML-random sequences is a strict\nsubset of the set of Schnorr random sequences, any problematic members\nof the former are equally problematic members of the latter; and of\ncourse there will be Schnorr random sequences which fail some\nMartin-Löf statistical test, which might lead some to reject the\nviability of Schnorr’s notion from the start. Schnorr’s result showing the convergence between prefix-free\nKolmogorov complexity and Martin-Löf randomness is very\nsuggestive. As has become clear, the existence of other notions of\nrandomness—incluing Schnorr randomness, as well as a number of\nother proposals (Li and Vitányi 2008: §2.5; Porter 2016: 464–6)—shows that\nwe should be somewhat cautious in yielding to its suggestion.  \nThis is especially true in light of a recent argument by Porter (2016: 469–70). He considers a certain schematic characterisation of computable functions, something like this: for every computable function \\(f\\) with property \\(P_i, f\\) is differentiable at \\(x\\) iff \\(x\\) corresponds to a random\\(_i\\) sequence. It turns out that for each sense of randomness (ML randomness, Schnorr randomness, computable randomness, etc.), there is some corresponding property of computable functions. Most importantly, none of these properties look overwhelmingly more natural or canonical than the others. For example, computable functions of bounded variation are differentiable at the Martin-Löf random points, while nondecreasing computable functions are differentiable at the computably random points. The difficulty for the Martin-Löf-Chaitin Thesis is this: these results give us the notion of a typical sequence, with respect to the differentiability of various kinds of function. Unfortunately, these notions of typical sequence diverge from one another. Unlike Church’s thesis, where all the notions of effective computability line up, here we have a case where various notions of a typical sequence do not line up with each other (though there is significant overlap). Porter concludes that ‘no single definition of randomness can do the work of capturing every mathematically significant collection of typical points’ (Porter 2016: 471).  \n\nThat conclusion may well be justified. But we can largely sidestep the dispute over whether there is a single precise notion of\nrandomness that answers perfectly to our intuitive conception of\nrandom sequence. Kolmogorov-Martin-Löf randomness is a reasonable and\nrepresentative exemplar of the algorithmic approach to randomness, and it overlaps almost everywhere with any other plausible definition of randomness. It is adopted here as a useful working account of randomness for\nsequences. None of the difficulties and problems I raise below for the\nconnection between random sequences and chance turns in any material\nway on the details of which particular set of sequences gets counted\nas random (most are to do with the mismatch between the process notion\nof chance and any algorithmic conception of randomness, with\ndifferences amongst the latter being relatively unimportant). So while\nthe observations below are intended to generalise to Schnorr\nrandomness and other proposed definitions of random sequences, I will\nexplicitly treat only KML randomness in what follows. \n\nThe notions of chance and randomness discussed and clarified in the\nprevious two sections are those that have proved scientifically and\nphilosophically most fruitful. Whatever misfit there may be between\nordinary language uses of these terms and these scientific\nprecisifications, is made up for by the usefulness of these concepts.\nThis is particularly so with the notion of randomness, chance being in\nphilosopher’s mouths much closer to what we ordinarily take ourselves\nto know about chance. On these conceptions, randomness is\nfundamentally a product notion, applying in the first instance to\nsequences of outcomes, while chance is a process notion, applying in\nthe single case to the process or chance setup which produces a token\noutcome. Of course the terminology in common usage is somewhat\nslippery; it’s not clear, for example, whether to count random\nsampling as a product notion, because of the connection with\nrandomness, or as a process notion, because sampling is a process. The\northodox view of the process, in fact, is that it should be governed\nby a random sequence; we enumerate the population, and sample an\nindividual \\(n\\) just in case the \\(n\\)-th outcome in a\npre-chosen random sequence is 1. (Of course the sample thus selected\nmay not be random in some intuitive sense; nevertheless, it will not\nbe biased because of any defect in the selection procedure, but rather\nonly due to bad luck.) \n\nWith these precise notions in mind, we can return to the Commonplace\nThesis CT connecting chance and randomness. Two readings make\nthemselves available, depending on whether we take single outcomes or\nsequences of outcomes to be primary: \n\n\\(\\mathbf{CTa}\\):\n\nA sequence of outcomes happens by chance iff that sequence is\nrandom. \n\n\\(\\mathbf{CTb}\\):\n\nAn outcome happens by chance iff there is a random sequence of outcomes\nincluding it. \n\nGiven the standard probability calculus, any sequence of outcomes is\nitself an outcome (in the domain of the chance function defined over a\n\\(\\sigma\\)-algebra of outcomes, as in standard mathematical probability);\nso we may without loss of generality consider only (CTb). But a\nproblem presents itself, if we consider chancy outcomes in possible\nsituations in which only very few events ever occur. It may be that\nthe events which do occur, by chance, are all of the same type, in\nwhich case the sequence of outcomes won’t be random. This problem is\nanalogous to the ‘problem of the single case’ for\n frequency views of chance\n (Hájek 2012:\n §3.3),\n because\nrandomness is, like frequency, a property of an outcome sequence. The\nproblem arises because the outcomes may be too few or too orderly to\nproperly represent the randomness of the entire sequence of which they\nare part (all infinite random sequences have at least some non-random\ninitial subsequences). The most common solution in the case of\nfrequentism was to opt for a hypothetical outcome sequence—a\nsequence of outcomes produced under the same conditions with a stable\nlimit frequency (von Mises 1957: 14–5). Likewise, we may refine\nthe commonplace thesis as follows: \n\nHere the idea is that chancy outcomes would, if repeated often\nenough, produce an appropriately homogenous sequence of outcomes which\nis random. If the trial is actually repeated often enough, this\nsequence should be the actual sequence of outcomes; the whole point of\nKolmogorov randomness was to permit finite sequences to be random. \n\nRCT is intuitively appealing, even once we distinguish process and\nproduct randomness in the way sketched above. It receives significant\nsupport from the fact that fair coins, tossed often enough, do in our\nexperience invariably give rise to random sequences, and that the\nexistence of a random sequence of outcomes is compelling evidence for\nchance. The truth of RCT explains this useful constraint on the\nepistemology of chance, since if we saw an actual finite random\nsequence, we could infer that the outcomes constituting that sequence\nhappened by chance. However, in the next two sections, we will see that\nthere are apparent counterexamples even to RCT, posing grave\n difficulties for the Commonplace Thesis. In\n §4\n we\nwill see a number of cases where there are apparently chancy outcomes\n without randomness, while in\n §5\n we will see cases\nof apparent randomness where there is no chance involved. \n\nA fundamental problem with RCT seems to emerge when we consider the\nfate of hypothetical frequentism as a theory of chance. For there\nseems to be no fact of the matter, in a case of genuine chance, as to\nwhat sequence of outcomes would result: as Jeffrey (1977: 193) puts\nit, ‘there is no telling whether the coin would have landed head\nup on a toss that never takes place. That’s what probability is all\nabout.’ Many philosophers (e.g., Hájek 2009) have\nfollowed Jeffrey in scepticism about the existence and tractability of\nthe hypothetical sequences apparently required by the right hand side\nof RCT. However, there is some reason to think that RCT will fare\nbetter than hypothetical frequentism in this respect. In particular,\nRCT does not propose to analyse chance in terms of these\nhypothetical sequences, so we can rely on the law of large numbers to\nguide our expectation that chance processes produce certain outcome\nfrequencies, in the limit, with probability 1; this at least may\nprovide some reason for thinking that the outcome sequences will\nbehave as needed for RCT to turn out true. Even so, one might suspect\nthat many difficulties for hypothetical frequentism will recur for\nRCT. However, these difficulties stem from general issues with merely\npossible evidential manifestations of chance processes, and have\nnothing specifically to do with randomness. The objections canvassed\nbelow are, by contrast, specifically concerned with the interaction\nbetween chance and randomness. So these more general potential worries\nwill be set aside, though they should not be forgotten—they may\neven be, in the end, the most significant problem for RCT (if the\nright kind of merely possible sequence doesn’t exist, we must retreat\nto CTa or CTb and their problems). \n\nIt is possible for a fair coin—i.e., such that the chances of\nheads and tails are equal—to be tossed infinitely many times, and\nto land heads on every toss. An infinite sequence of heads has, on the\nstandard probability calculus, zero chance of occurring. (Indeed, it\nhas zero chance even on most non-standard views of probability:\nWilliamson 2007.) Nevertheless if such a sequence of outcomes did\noccur, it would have happened by chance—assuming, plausibly, that\nif each individual outcome happens by chance, the complex event\ncomposed by all of them also happens by chance. But in that case we\nwould have an outcome that happened by chance and yet the obvious\nsuitable sequence of outcomes is not KML-random. This kind of example\nexploits the fact that while random sequences are a measure one set of\npossible outcome sequences of any process, chancy or otherwise, measure\none does not mean every. \n\nThis counterexample can be resisted. For while it is possible that a\nfair coin lands only heads when tossed infinitely many times, it may\nnot be that this all heads outcome sequence is a suitable sequence. For\nif we consider the counterfactual involved in RCT—what would\nhappen, if a fair coin were tossed infinitely many times—we would\nsay: it would land heads about half the time. That is (on the standard,\nthough not uncontroversial, Lewis-Stalnaker semantics for\ncounterfactuals: Lewis 1973), though the all-heads outcome sequence is\npossible, it does not occur at any of the nearest possibilities in\nwhich a fair coin is tossed infinitely many times. \n\nIf we adopt a non-reductionist account of chance, this line of\nresistance is quite implausible. For there is nothing inconsistent on\nsuch views about a situation where the statistical properties of the\noccurrent sequence of outcomes and the chance diverge arbitrarily far,\nand it seems that such possibilities are just as close in relevant\nrespects as those where the occurrent outcome statistics reflect the\nchances. In particular, as the all-heads sequence has some chance of\ncoming to pass, there is (by BCP) a physical possibility sharing\nhistory and laws with our world in which all-heads occurs. This looks\nlike a legitimately close possibility to our own. \n\nProspects look rather better on a reductionist view of chance\n (Supplement\n A.3).\n On such a view, we\ncan say that worlds where an infinite sequence of heads does occur at\nsome close possibility will look very different from ours; they differ\nin law or history to ours. In such worlds, the chance of heads is much\ncloser to 1 (reflecting the fact that if a coin were tossed infinitely\nmany times, it might well land heads on each toss)—the coin is\nnot after all fair. The response, then, is that in any situation where\nthe reductionist chance of heads really is 0.5, suitable outcome\nsequences in that situation or its nearest neighbours are in\nfact all unbiased with respect to outcome frequencies. That is\nto say, they at least satisfy the property of large numbers; and\narguably they can be expected to meet other randomness properties also.\nSo, on this view, there is no counterexample to RCT from the mere\npossibility of these kinds of extreme outcome sequences. This response\ndepends on the success of the reductionist similarity metrics for\nchancy counterfactuals developed by Lewis (1979a) and Williams (2008);\nthe latter construction, in particular, invokes a close connection\nbetween similarity and randomness. (Lewis’ original construction is\ncriticised in Hawthorne 2005.) \n\nHowever, we needn’t use such an extreme example to make the point.\nFor the same phenomenon exists with almost any unrepresentative outcome\nsequence. A fair coin, tossed 1000 times, has a positive chance of\nlanding heads more than 700 times. But any outcome sequence of 1000\ntosses which contains more than 700 heads will be compressible (long\nruns of heads are common enough to be exploited by an efficient coding\nalgorithm, and 1000 outcomes is long enough to swamp the constants\ninvolved in defining the universal prefix-free Kolmogorov complexity).\nSo any such outcome sequence will not be random, even though it quite\neasily could come about by chance. The only way to resist this\ncounterexample is to refuse to acknowledge that such a sequence of\noutcomes can be an appropriate sequence in RCT. This is implausible,\nfor such sequences can be actual, and can be sufficiently long to avoid\nthe analogue of the problem of the single case, certainly long enough\nfor the Kolmogorov definition of randomness to apply. The only reason\nto reject such sequences as suitable is to save RCT, but that is\nclearly question begging in this context. In this case of an\nunrepresentative finite sequence, even reductionism about chance\nneedn’t help, because it might be that other considerations suffice to\nfix the chance, and so we can have a genuine fair chance but a biased\nand non-random outcome sequence. \n\nAny given token event is an instance of many different types of\ntrial: \n\nSuppose Lizzie tosses a coin on Tuesday; this particular coin toss\nmay be considered as a coin toss; a coin toss on a Tuesday; a coin toss\nby Lizzie; an event caused by Lizzie; etc. Each of these ways of typing\nthe outcome give rise to different outcome sequences, some of which may\nbe random, while others are not. Each of these outcome sequences is\nunified by a homogenous kind of trial; as such, they may all be\nsuitable sequences to play a role in RCT. \n\nThis is not a problem if chance too is relative to a type of trial,\nfor we may simply make the dependence on choice of reference class\nexplicit in both sides of RCT. If chances were relative frequencies, it\nwould be easy enough to see why chance is relative to a type of trial.\nBut chances aren’t frequencies, and single-case chance is almost\nuniversally taken to be not only well-defined for a specific event, but\nunique for that event. We naturally speak of the chance that\nthis coin will lands heads on its next toss, with the chance taken to\nbe a property of the possible outcome directly, and not mediated by\nsome particular description of that outcome as an instance of this or\nthat kind of trial. Moreover, for chance to play its role in the\n Principal Principle\n (§1\n and Supplement\n A.1),\n there must be a unique chance for a\ngiven event that is to guide our rational credence, as we have only one\ncredence in a particular proposition stating the occurrence of that\nevent. (Indeed, the inability of frequentists to single out a unique\nreference class, the frequency in which is the chance, was taken to be\na decisive objection to frequentism.) On the standard understanding of\nchance, then, there is a mismatch between the left and right sides of\nthe RCT. And this gives rise to a counterexample to RCT, if we take an\nevent with a unique non-trivial single-case chance, but such that at\nleast one way of classifying the trial which produced it is such that\nthe sequence of outcomes of all trials of that kind is not random. The\ntrivial case might be this: a coin is tossed and lands heads. That\nevent happened by chance, yet it is of the type ‘coin toss which\nlands heads’, and the sequence consisting of all outcomes of that\ntype is not random. \n\nThe natural response—and the response most frequentists\n offered, with the possible exception of von\n Mises[14]—was\n to narrow\n the available reference classes. (As noted in Supplement\n A.3,\n many frequentists were explicit that\nchances were frequencies in repetitions of natural kinds of processes.)\nSalmon (1977) appeals to objectively homogenous reference classes\n(those which cannot be partitioned by any relevant property into\nsubclasses which differ in attribute frequency to the original\nreference class). Salmon’s proposal, in effect, is that homogeneous\nreference classes are random sequences, the evident circularity of\nwhich will hardly constitute a reply to the present objection.\nReichenbach (1949: 374) proposed to ‘proceed by considering the\nnarrowest class for which reliable statistics can be compiled’,\nwhich isn’t circular, but which fails to respond to the objection since\nit provides no guarantee that there will be only one such class. There\nmay well be multiple classes which are equally ‘narrow’ and\nfor which reliable statistics can be collected (Gillies, 2000: 816). In\nthe present context, this will amount to a number of sequences long\nenough to make for reliable judgements of their randomness or lack\nthereof. \n\nThis objection requires the chance of an event to be insensitive to\nreference class. Recently, Hájek (2007) has argued that\nno adequate conception of probability is immune to a reference\nclass problem, so that this requirement cannot be met. (For a related\nview of relativised chance, though motivated by quite different\nconsiderations, see Glynn 2010.) However, as Hájek notes, this\nconclusion makes it difficult to see how chance could guide credence,\nand it remains an open question whether a relativised theory of chance\nthat meets the platitudes concerning chance can be developed. \n\nThe two previous problems notwithstanding, many have found the most\ncompelling cases of chance without randomness to be situations in which\nthere is a biased chance process. A sequence of unfair coin tosses will\nhave an unbalanced number of heads and tails, and such a sequence\ncannot be random. But such a sequence, and any particular outcome in\nthat sequence, happens by chance. \n\nThat such sequences aren’t random can be seen by using both\nMartin-Löf- and Kolmogorov-style considerations. In the latter\ncase, as we have already seen, a biased sequence will be more\ncompressible than an unbiased sequence, if the sequence is long enough,\nbecause an efficient coding will exploit the fact that biased sequences\nwill typically have longer subsequences of consecutive digits, and\nhence will not be random. In the former case, a biased sequence will\nviolate at least one measure one property, on the standard Lebesgue\nmeasure on infinite binary sequences—in particular, a measure one\n subset of the Cantor space will be Borel normal\n (§2.1),\n but no biased sequence is Borel normal. So on the\nstandard account of randomness, no sequences of outcomes of a biased\nchance process are random, but of course these outcomes happened by\nchance. \n\nOne response to this problem is to try and come up with a\ncharacterisation of randomness which will permit the outcomes of\nbiased chances to be random. It is notable that von Mises’ initial\ncharacterisation of randomness was expressly constructed with this in\nmind—for him, a random sequence is one for which there is no\nadmissible subsequence having a frequency differing from the frequency\nin the original sequence. This account is able to handle any value for\nthe frequency, not only the case where each of two outcomes are\nequifrequent. Given that the Martin-Löf approach is a\ngeneralisation of von Mises’, it is not surprising that it too can be\nadapted to permit biased sequences to be random. Consider a binary\nprocess with outcome probabilities \\((p, 1 - p)\\). The law of large numbers in a general form tells\nus that a measure one set of sequences of independent trials of such a\nprocess will have limit frequencies of outcomes equal to \\((p, 1 - p)\\). This measure is not the standard Lebesgue\nmeasure, but rather a measure defined by the chance function in\nquestion. We can similarly re-interpret the other effective\nstatistical tests of randomness. Drawing as we did above\n(§2.1.2) on the language of statistical\ntesting, we can characterise the random sequences as those which are\nnot significant with respect to the hypothesis that the outcome\nchances are \\((p, 1 - p)\\)—those which, as\nit were, conform to our prior expectations based on the underlying\nchances. \n\nTo approach the topic of randomness of biased sequences through\nKolmogorov complexity, suppose we are given—somehow—any\ncomputable probability measure \\(\\lambda\\) over the set of infinite\nbinary sequences (that is, the probability of a given sequence can be\napproximated arbitrarily well by a recursive function). A sequence\n\\(\\sigma\\) is \\(\\lambda\\)-incompressible iff for each \\(n\\), the\nKolmogorov complexity of the length \\(n\\) initial subsequence of\n\\(\\sigma\\) (denoted \\(\\sigma_n )\\) is greater than or\nequal to \\({-}\\log_2 (\\lambda(\\sigma_n ))\\). Where \\(\\lambda\\) is the\nLebesgue measure\n(supplement B.2), it\nfollows that so we get back the original\ndefinition of Kolmogorov complexity in that special case. With this\ngeneralised definition of Kolmogorov randomness in hand, it turns out\nthat we can show a generalisation of Schnorr’s theorem\n(§2.3): a sequence \\(\\sigma\\) is\n\\(\\lambda\\)-incompressible iff \\(\\sigma\\) is ML-random with respect to\n\\(\\lambda\\).[15] In\nthe framework of\nsupplement B.1.1, a\nsequence is ML-random in this generalised sense iff the measure, under\nour arbitrary computable measure \\(\\lambda\\), of the \\(n\\)th\nsequential significance test is no greater than\n\\(1/2^n\\). (There are some potential pitfalls,\nsuggesting that perhaps the generalisation to an arbitrary computable\nmeasure is an overgeneralisation: see\nsupplement B.1.3 for\nsome details.) \n\nWhile the above approach, with the modifications suggestion in the\nsupplement taken on board, does permit biased random sequences, it\ncomes at a cost. While the Lebesgue measure is a natural one that is\ndefinable on the Cantor space of sequences directly, the\ngeneralisation of ML-randomness requires an independent computable\nprobability measure on the space of sequences to be provided. While\nthis may be done in cases where we antecedently know the chances, it\nis of no use in circumstances where the existence of chance is to be\ninferred from the existence of a random sequence of outcomes, in line\nwith RCT—for every sequence there is some chance measure\naccording to which it is random, which threatens to trivialise the\ninference from randomness to chance. As Earman (1986: 145) also\nemphasises, this approach to randomness seems to require essentially\nthat the chanciness of the process producing the random sequence is\nconceptually prior to the sequence being random. This kind of process\napproach to randomness has some intuitive support, and we will return\nto it below (§6.2), but it risks turning RCT\ninto an uninformative triviality. By contrast, the Lebesgue measure\nhas the advantage of being intrinsically definable from the\nsymmetries of the Cantor space, a feature other computable measures\nlack. \n\nThe main difficulty with the suggested generalisation to biased\nsequences lies in the simple fact that biased sequences, while they\nmight reflect the probabilities in the process which produces them,\nsimply don’t seem to be random in the sense of disorderly and\nincompressible. The generalisation above shows that we can define a\nnotion of disorderliness that is relative to the probabilities\nunderlying the sequence, but that is not intrinsic to the sequence\nitself independently of whatever measure we are considering. As Earman\nputs it (in slightly misleading terminology):  As we might put it: Kolmogorov randomness is conceptually linked to\ndisorderliness, and while we can gerry-rig a notion of ‘biased\ndisorder’, that doesn’t really answer to what we already know\nabout the incompressibility of disorderly sequences. While we might\nwell regard a sequence featuring even numbers of heads and tails, but\nproduced by successive tosses of an unfair coin, to be biased\nin some sense with respect to the underlying measure of process behind\nit, it is still plausible that this unrepresentativeness of\nthe sequence isn’t conceptually connected with disorder in any\ninteresting sense. It is very intuitive, as this remark from Dasgupta\nsuggests, to take that biasedness—the increased orderliness of\nthe sequence—to contrast with randomness: \n\nAs the bias in a chance process approaches extremal values, it is very\nnatural to reject the idea that the observed outcomes are random. (As\nan example, we may consider human behaviour—while people aren’t\nperfectly predictable, and apparently our behaviour doesn’t obey\nnon-probabilistic psychological laws, nevertheless to say that people\nact randomly is incorrect.) Moreover, there is a relatively\nmeasure-independent notion of disorder or incompressibility of\nsequences, such that biased sequences really are less disorderly. We\ncan define a measure-dependent notion of disorder for biased sequences\nonly by ignoring the availability of better compression techniques\nthat really do compress biased sequences more than unbiased ones. To\ngeneralise the notion of randomness, as proposed above, permits highly\nnon-random sequences to be called random as long as they reflect the\nchances of highly biased processes. So there is at least some\nintuitive pull towards the idea that if randomness does bifurcate as\nEarman suggests, the best deserver of the name is Kolmogorov\nrandomness in its original sense. But this will be a sense that\ncontrasts with the natural generalisation of ML-randomness to deal\nwith arbitrary computable probability measures, and similarly\ncontrasts with the original sense of randomness that von Mises must\nhave been invoking in his earliest discussions of randomness in the\nfoundations of probability. In light of the above discussion, while there has been progress on\ndefining randomness for biased sequences in a general and\ntheoretically robust way, there remain difficulties in using that\nnotion in defence of any non-trivial version of RCT, and difficulties\nin general with the idea that biased sequences can be genuinely\ndisorderly. But the generalisation invoked here does give some succour\nto von Mises, for a robust notion of randomness for biased sequences\nis a key ingredient of his form of frequentism.\n \n\nA further counterexample to RCT, related to the immediately previous\none, is that randomness is indifferent to history, while chance is not.\nChance is history-dependent. The simplest way in which chance is\nhistory-dependent is when the conditions that may produce a certain\nevent change over time: \n\nBut there are more complicated types of history-dependence. In Lewis’\nexample, the property which changes to alter the chances is how close\nthe agent is to the centre. But there are cases where the property\nwhich changes is a previous outcome of the very same process. Indeed,\nany process in which successive outcomes of repeated trials are\nnot probabilistically independent will have this feature. \n\nOne example of chance without randomness involves an unbiased urn\nwhere balls are drawn without replacement. Each draw (with the\nexception of the last) is an event which happens by chance, but the\nsequence of outcomes will not be random (because the first half of the\nsequence will carry significant information about the composition of\nthe second half, which may aid compressibility). But a more compelling\nexample is found in stochastic processes in which the chances of future\noutcomes depend on past outcomes. One well-known class of such process\nare Markov chains, which produce a discrete sequence of outcomes with\nthe property that the value of an outcome is dependent on the value of\nthe immediately prior outcome (but that immediately prior outcome\nscreens off the rest of the history). A binary Markov chain might be\nthe weather (Gates and Tong 1976): if the two possible outcomes are\n‘sunny’ and ‘rainy’, it is plausible to suppose\nthat whether tomorrow is rainy depends on whether today was rainy (a\nrainy day is more likely to be preceded by another rainy day); but\nknowing that today was rainy arguably makes yesterday’s weather\nirrelevant. \n\nIf a Markov chain is the correct model of a process, then even when\nthe individual trial outcomes happen by chance, we should expect the\nentire sequence of repeated trials to be non-random. In the weather\ncase just discussed, we should expect a sunny day to be followed by a\nsunny day, and a rainy day by a rainy one. In our notation 11 and 00\nshould be more frequent than 10 or 01. But the condition of Borel\nnormality, which all random sequences obey, entails that every finite\nsequence of outcomes of equal length should have equal frequency in the\nsequence. So no Borel normal sequence, and hence no random sequence,\ncan model the sequence of outcomes of a Markov chain, even though each\noutcome happens by chance. \n\nAt least some non-random sequences satisfy many of the measure one\nproperties required of random sequences. For example, the Champernowne\nsequence, consisting of all the binary numerals for every non-negative\ninteger listed consecutively (i.e., 011011100101110111…), is\nBorel normal. This sequence isn’t random, as initial subsequences of\nreasonable length are highly compressible. But it looks like it\nsatisfies at least some desiderata for random sequences. This sequence\nis an attempt at producing a pseudorandom sequence—one\nthat passes at least some statistical tests for randomness, yet can be\neasily produced. (The main impetus behind the development of\npseudorandom number generators has been the need to efficiently produce\nnumbers which are random for all practical purposes, for use in\ncryptography or statistical sampling.) Much better examples exist than\nthe Champernowne sequence, which meet more stringent randomness\n properties.[16]\n One simple technique for generating\npseudorandom sequences is a symbol shift algorithm (Smith\n1998: 53). Given an initial ‘seed’ numeral\n\\(s_1, s_2 , \\ldots ,s_n\\), the algorithm simply spits out the\ndigits in order. Obviously this is useless if the seed is known, or can\nin some way be expected to be correlated with the events to which one\nis applying these pseudorandom numbers. But in practical applications,\nthe seed is often chosen in a way that we do expect it to carry no\ninformation about the application (in simple computer pseudorandom\nnumber generators, the seed may be derived in some way from the time at\nwhich the seed is called for). With a finite seed, this sequence will\nobviously repeat after a some period. A symbol shift is the simplest\npossible function from seed to outcome sequence; better algorithms use\na more complicated but still efficiently computable function of the\nseed to generate outcome sequences with a longer period, much longer\nthan the length of the seed (e.g., the ‘Mersenne twister’\nof Matsumoto and Nishimura 1998 has a period of \\(2^{19937} - 1)\\). \n\nIf the seed is not fixed, but is chosen by chance, we can have\nchance without randomness. For example, suppose the computer has a\nclock representing the external time; the time at which the algorithm\nis started may be used as a seed. But if it is a matter of chance when\nthe algorithm is started, as it well may be in many cases, then the\nparticular sequence that is produced by the efficient pseudorandom\nsequence generator algorithm will be have come about by chance, but not\nbe random (since there is a program which runs the same algorithm on an\nexplicitly given seed; since the seed is finite, there will be such a\nprogram; and since the algorithm is efficient, the length before the\nsequence produced repeats will be longer than the code of the program\nplus the length of the seed, making the produced sequence\ncompressible). Whether the seed is produced by chance or explicitly\nrepresented in the algorithm, the sequence of outcomes will be the\nsame—one more way in which it seems that the chanciness of a\nsequence can vary while whether or not it is random remains constant.\n(Symbol shift dynamics also permit counterexamples to the other\ndirection of RCT—see\n §5.2.) \n\nMuch the same point could have been made, of course, with reference\nto any algorithm which may be fed an input chosen by chance, and so may\nproduce an outcome by chance, but where the output is highly\ncompressible. (One way in which pseudorandom sequence generators are\nnice in this respect is that they are designed to produce highly\ncompressible sequences, though non-obviously highly compressible ones).\nThe other interesting thing about those algorithms which produce\npseudorandom sequences is that they provide another kind of\ncounterexample to the epistemic connection between chance and\nrandomness. For our justification in thinking that a given sequence is\nrandom will be based on its passing only finitely many tests; we can be\njustified in believing pseudorandom sequence to be random (in some\nrespectable sense of justification, as long as justification is weaker\nthan truth), and justified in making the inference to chance via RCT.\nBut then we might think that this poses a problem for RCT to play the\nright role epistemically, even if it were true. Suppose one sees a\ngenuinely random sequence and forms the justified belief that it is\nrandom. The existence of pseudorandom sequences entails that things\nmight seem justificatorily exactly as they are and yet the sequence not\nbe random. But such a scenario, arguably, defeats my knowing that the\nsequence is random, and thus defeats my knowing the sequence to have\nbeen produced by chance (and presumably undermines the goodness of the\ninference from randomness to chance). \n\nThe counterexamples to RCT offered in\n §§4.3–4.4\n suggest strongly that\nthe appeal of RCT depends on our curious tendency to take independent\nidentically distributed trials, like the Bernoulli process of fair coin\ntossing, to be paradigms of chance processes. Yet when we widen our\ngaze to encompass a fuller range of chance processes, the appeal of the\nright-to-left direction of RCT is quite diminished. It is now time to\nexamine potential counterexamples to the other direction of RCT. There\nare a number of plausible cases where a random sequence potentially\nexists without chance. Many of these cases involve interesting features\nof classical physics, which is apparently not chancy, and yet which\ngives rise to a range of apparently random phenomena. Unfortunately\nsome engagement with the details of the physics is unavoidable in the\nfollowing. \n\nOne obvious potential counterexample involves coin tossing. Some\nhave maintained that coin tossing is a deterministic process, and as\nsuch entirely without chances, and yet which produces outcome sequences\nwe have been taking as paradigm of random sequences. This will be set\naside until\n §7,\n where the claim that determinism\nprecludes chance will be examined. \n\nFor many short sequences, even the most efficient prefix-free code\nwill be no shorter than the original sequence (as prefix-free codes\ncontain information about the length of the sequence as well as its\ncontent, if the sequence is very short the most efficient code may well\nbe the sequence itself prefixed with its length, which will be longer\nthan the sequence). So all short sequences will be Kolmogorov random.\nThis might seem counterintuitive, but if randomness indicates a lack of\npattern or repetition, then sequences which are too short to display\npattern or repetition must be random. Of course it will not usually be\nuseful to say that such sequences are random, mostly because in very\nshort sequences we are unlikely to talk of the sequence at all, as\nopposed to talking directly about its constituent outcomes. \n\nBecause of the modal aspect of RCT, for most processes there will\npossibly be a long enough sequence of outcomes to overcome any\n‘accidental’ randomness due to actual brevity of the\noutcome sequence. But for events which are unrepeatable or seldom\nrepeatable, even the merely possible suitable reference classes will be\nsmall. And such unrepeatable events do exist—consider the Big\nBang which began our universe, or your birth (your birth, not\nthe birth of qualitatively indistinguishable counterpart), or the death\nof Ned Kelly. These events are all part of outcome sequences that are\nnecessarily short, and hence these events are part of Kolmogorov random\nsequences. But it is implausible to say that all of these events\nhappened by chance; no probabilistic theory need be invoked to predict\nor explain any of them. In the case of Kelly’s death, for example,\nwhile it may have been a matter of chance when he happened to die, it\nwas not chance that he died, as it is (physically) necessary that he\ndid. So there are random sequences—those which are essentially\nshort—in which each outcome did not happen by chance. \n\nThe natural response is to reject the idea that short sequences are\napt to be random. The right hand side of RCT makes room for this, for\nwe may simply insist that unrepeatable events cannot be repeated often\nenough to give rise to an adequate sequence (whether or not the\ninadequate sequence they do in fact give rise to is random). The\nproblem here is that we can now have chance without randomness, if\nthere is a single-case unrepeatable chance event. Difficulties in fact\nseem unavoidable. If we consider the outcomes alone, either all short\nsequences are random or none of them are; there is no way to\ndifferentiate on the basis of any product-based notion between\ndifferent short sequences. But as some single unrepeatable events are\nchancy, and some are not, whichever way we opt to go with respect to\nrandomness of the singleton sequences of such events we will discover\ncounterexamples to one direction or another of RCT. \n\nThe simple symbol shift dynamics in\n §4.5\n had\na finite seed, which allowed for chance without randomness. Yet there\nseem to be physical situations in which a symbol shift dynamics is an\naccurate way of representing the physical processes at work. One simple\nexample might be a stretch and fold dynamics, of the kind common in\nchaos theory (Smith 1998: §4.2). The classic example is the\nbaker’s transformation (Earman 1986: 167–8; Ekeland\n1988: 50–9). We take a system the state of which at any one time\nis characterised by a point \\((p, q)\\) in the real unit\nsquare. We specify the evolution of this system over time as follows,\nletting \\(\\phi\\) be the function governing the discrete evolution\nof the system over time (i.e., \\(s_{t + 1} = \\phi(s_t))\\): \n\nThis corresponds to transforming the unit square to a rectangle\ntwice as wide and half the height, chopping off the right half, and\nstacking it back on top to fill the unit square again. (That this\ntransformation reminded mathematicians of baking says something about\ntheir unfamiliarity with the kitchen—similar transformations,\nwhere the right half is ‘folded’ back on top, are more\nrealistic.) If we represent the coordinates \\(p\\) and \\(q\\)\nin binary, the transformation is this: \nSo this is a slight variant on a simple symbol shift, as the\n\\(p\\)-coordinate is a symbol shift to the right, while the\n\\(q\\) coordinate is, in effect, a symbol shift to the\n left.[17] \n\nOne important feature of this dynamics is that it is measure\npreserving, so that if \\(X\\) is a subset of the unit square,\n\\(\\mu(X) = \\mu(\\phi(X))\\).\n(This is easily seen, as the symbol shift dynamics on the basic sets of\ninfinite binary sequences is measure-preserving, and each coordinate\ncan be represented as an infinite binary sequence.) Define the set\n\\(L = \\{(p, q) : 0 \\le p \\lt \\frac{1}{2} \\}\\).\nWe see that \\((p, q) \\in L\\) iff\n\\(p_1 = 0\\). Since \\(p\\) can be represented by an\ninfinite binary sequence, and a measure one set of finite binary\nsequences is Borel normal, we see that almost all states of this system\nare such that, over time, \\(\\mu(\\phi(s)\n\\in L \\mid s \\in L) = \\mu(\\phi(s) \\in L)\\)—that\nis, whether the system is in \\(L\\) at \\(t\\) is\nprobabilistically independent of its past history. Furthermore,\n\\(\\mu(L) = \\mu(\\overline{L})\\). The behaviour of\nthis system over time, with respect to the partition \n\\(\\{L, \\overline{L}\\}\\), is therefore a\nBernoulli process, exactly like a sequence of fair coin tosses—a\nseries of independent and identically distributed repetitions of a\nchance process. If the RCT is true, then a system which behaves exactly\nlike a chance process should have as its product a random sequence. So\nthe sequence of outcomes for the baker’s transformation (under this\npartition) is a random sequence. \n\nBut unlike a sequence of fair coin tosses, supposing them to involve\ngenuine chance, the baker’s transformation is entirely\ndeterministic. Given a particular point \\((p,\nq)\\) as an initial condition, the future evolution of states of\nthe system at each moment is time \\(t\\) is determined to be\n\\(\\phi^t (p, q)\\). So while\nthe product produced is random, as random as a genuine chance process,\nthese outcomes do not happen by chance; given the prior state of the\nsystem, the future evolution is not at all a matter of chance. So we\nhave a random sequence without chance outputs. (Indeed, given the\nsymbol shift dynamics, the evolution of the system over time in\n\\(\\{L, \\overline{L}\\}\\)\nsimply recapitulates successive digits of the starting point.) To be\nperfectly precise, the trial in this case is sampling the system at a\ngiven time point, and seeing which cell of the coarse grained partition\nit is in at each time. This is a sequence of arbitrarily repeated\ntrials which produces a random sequence; yet none of these outcomes\nhappens by\n chance.[18]\n To put the point slightly differently:\nwhile the sequence of outcomes is random, there is a perfectly adequate\ntheory of the system in question in which probability plays no role.\nAnd if probability plays no role, it is very difficult to see how\nchance could play a role, since there is no probability function which\nserves as norm for credences, governs possibility, or is non-trivial\nand shared between intrinsic duplicate trials. In short, no probability\nfunction that has the features required of chance plays a role in the\ndynamics of this system, and that seems strong reason for thinking\nthere is no chance in this system. \n\nThe baker’s transformation provides a simple model of deterministic\nmacro-randomness—a system which has a\n\\(\\mu\\)-measure-preserving temporal evolution, and produces a\nsequence of coarse grained states that have the Bernoulli property. It\nis a question of considerable interest whether there are physically\nmore realistic systems which exhibit the same features. We may conceive\nof an \\(n\\)-particle classical (Newtonian) system as having, at\neach moment, a state that is characterised by a single point in a\n\\(6n\\)-dimensional state space (each point a \\(6n\\)-tuple\ncharacterising the position and momentum of each particle). The\nevolution of the system over time is characterised by its\nHamiltonian, a representation of the energetic and other\nproperties of the system. The evolution under the Hamiltonian is\n\\(\\mu\\)-measure-preserving (by Liouville’s theorem), so it might\nbe hoped that at least some systems could be shown to be Bernoulli too.\nYet for closed systems, in which energy is conserved over time, this is\nnot generally possible. Indeed, for closed systems it is not generally\npossible to satisfy even a very weak randomness property,\nergodicity. A system is ergodic just in case, in the limit,\nwith probability one, the amount of time a system spends in a given\nstate is equal to the (standard) measure of state space that\ncorresponds to that state (Earman 1986: 159–61; Sklar 1993:\n60–3; Albert 2000: 59–60). While a Bernoulli system is\nergodic, the converse entailment does not hold; if the system moves\nonly slowly from state to state, it may be ergodic while the state at\none time is strongly dependent on past history (Sklar 1993:\n166–7). While ergodicity has been shown to hold of at least one\nphysically interesting system (Yakov Sinai has shown that the motion of\nhard spheres in a box is ergodic, a result of great significance for\nthe statistical mechanics of ideal gases), a great many physically\ninteresting systems cannot be ergodic. This is the upshot of the\nso-called KAM theorem, which says that for almost all closed systems in\nwhich there are interactions between the constituent particles, there\nwill be stable subregions of the state space—regions of positive\nmeasure such that if a system is started in such a region, it will\nalways stay in such a region (Sklar 1993: 169–94). Such systems\nobviously cannot be ergodic. \n\nThe upshot of this for our discussion may be stated: ‘there are\nno physically realistic classical systems which exhibit even\nergodicity, and so no physically realistic classical system can\nexhibit randomness. The baker’s transformation is a mathematical\ncuriosity, but is not a genuine case of randomness without chance,\nsince systems like it are not physically possible.’ This\nresponse is premature. There are physically interesting systems to\nwhich the KAM theorem does not apply. Open or dissipative systems,\nthose which are not confined to a state space region of constant\nenergy, are one much studied class, because such systems are paradigms\nof chaotic systems. The hallmarks of a chaotic dissipative system are\ntwo (Smith 1998: §1.5): \n\nThere are physically realistic classical systems which exhibit both\nof these features, of which the best known is perhaps Lorenz’s model of\natmospheric convection (Smith 1998: §1.4; Earman 1986: 165). The\ncombination of these two features permits very interesting\nbehaviour—while the existence of an attractor means that over\ntime the states of the system will converge to the region of the\nattractor, the sensitive dependence on initial conditions means that\nclose states, at any time, will end up arbitrarily far apart. For this\nto happen the attractor must have a very complex shape (it will be a\nregion of small measure, but most of the state space will be in the\nneighbourhood of the attractor). More importantly for our purposes, a\nsystem with these characteristics, supposing that the divergence under\nevolution of close states happens quickly enough, will yield behaviour\nclose to Bernoulli—it will yield rapid mixing (Luzzatto\net al.\n 2005).[20]\n Roughly, a system is mixing iff the\npresence of the system in coarse state at one time is probabilistically\nindependent of its presence in another coarse state at another time,\nprovided there is a sufficient amount of time between the two\ntimes. This is weaker than Bernoulli (since the states of a\nBernoulli system are probabilistically independent if there is any time\nbetween them), but still strong enough to plausibly yield a random\nsequence of outcomes from a coarse grained partition of the state\nspace, sampled infrequently enough. So we do seem to have physically\nrealistic systems that yield random behaviour without chance. (See also\nthe systems discussed in Frigg 2004.) \n\nIndeed, the behaviour of a chaotic system will be intuitively random\nin other ways too. The sensitive dependence on initial conditions means\nthat, no matter how accurate our finite discrimination of the initial\nstate of a given chaotic system is, there will exist states\nindiscriminable from the initial state (and so consistent with our\nknowledge of the initial state), but which would diverge arbitrarily\nfar from the actual evolution of the system. No matter, then, how well\nwe know the initial condition (as long as we do not have infinite\npowers of discrimination), there is another state the system could be\nin for all we know that will evolve to a discriminably different future\ncondition. Since this divergence happens relatively quickly, the system\nis unable to be predicted. (Anecdotally, at least, Lorenz’ model of the\nweather seems borne out by our inability to reliably predict future\nweather even a few days from now.) Insofar as randomness and lack of\nreliable prediction go hand in hand, we have another reason for\nthinking there is product randomness here\n (§6.2). \n\nJust as before, the classical physical theory underlying the\ndynamics of these chaotic systems is one in which probability does not\nfeature. So we are able to give an adequate characterisation of the\nphysical situation without appeal to any probabilities fit to play the\nchance role. Given well-enough behaved boundary conditions, this system\nis also deterministic (though see\n §5.3),\n and\nthat may also be thought to preclude a role for non-trivial chances. So\nagain we have randomness in the performance, though none of the\noutcomes happened by chance. \n\nTwo avenues of resistance to the line of thought in this section\nsuggest themselves. The first is to maintain that, despite the truth of\ndeterminism for the baker’s transformation and classical physics\n(modulo worries to be addressed in the following section), there are\nstill, or at there least may be, non-trivial chances in these theories.\nThe proposal is that the possibility remains of deterministic\nchance, so that from the fact of determinism alone it does not\nfollow that we have a counterexample to RCT. The outcomes may be\ndetermined to happen by the prior conditions, but (so the suggestion\ngoes) they may still happen by chance. This radical proposal is\ndiscussed below, in\n §7.\n It should be noted,\nhowever, that even if it is true that deterministic chance is possible,\nthat observation is far from establishing that the physical theories\nunder discussion here are ones in which deterministic chance features.\nThat some deterministic theories may have chances is no argument that\nall will, and particularly in very simple cases like the baker’s\ntransformation, there doesn’t seem much point in an invocation of\ndeterministic chance: chances will be trivial or redundant if classical\nphysics is true. The second avenue of resistance is to claim that there\nreally is chance here—it is chance of the initial\nconditions which is recapitulated in the random sequence of\noutcomes. While the Lebesgue measure over the set of initial conditions\nin our models is formally like a probability function, to assume it\nyields genuine chances is a fairly controversial thesis (for a contrary\nopinion, see Clark 1987). Other initial conditions could have obtained;\nstill it seems wrong to think that (somehow) there was a chance process\nwhich terminated in the selection of the initial conditions which\nhappened in fact to obtain at our world. Rather, that the initial\nconditions are what they are seems to be a brute fact. If there are to\nbe chances, then, they cannot be dynamical chances, the kind that is\nfamiliar from physics, and from our discussion in\n §1.2.\n Some recent arguments in favour of the possibility\nof chancy initial conditions are discussed in the following\nsupplementary document: \n\nBut whether or not the idea of chancy initial conditions can be made\nto work, the fact remains that at most one outcome in the random\nsequence—the first one—happens by chance. The subsequent\nstates do not, yet RCT is committed to there being (dynamical,\ntransition) chances in those state transitions. \n\nDespite the neat picture of classical determinism drawn in the\nprevious section, it is well known that classical physics is not in\nfact deterministic. These cases of indeterminism do not undermine the\napplications of classical mechanics in the previous section. But\nclassical indeterminism may provide problems of its own for RCT. Useful\nfurther material on the topic of this section can be found in the entry\non causal determinism\n (Hoefer 2010:\n §4.1). \n\nFor present purposes, indeterminism occurs when the state of the\nsystem at one time does not uniquely fix the state the system will be\nin at some future time (see\n §7).\n To show\nindeterminism in the classical case, it suffices to give a state of\nsome system at a given time and to specify two future states that are\nincompatible with each other and yet both states are consistent with\nNewton’s laws of motion and the initial state. \n\nTo help us in this task, it is useful to note one fact about\nNewtonian mechanics: the laws are time-reversal invariant\n(Albert 2000: ch. 1). That is, for every lawful trajectory through the\nstate space, there is another lawful trajectory which results from the\nfirst by mapping every instantaneous state in the first trajectory to\nits image state in which the particles have the same positions but the\nsigns on the components of momentum are reversed, and running the\ntrajectory in reverse order. These image states are those where the\nparticles are in the same positions but moving in exactly the opposite\ndirection. So for every lawful process, that process running backwards\nis also lawful. (If these trajectories are so lawful, why don’t we see\nthem?—that is the deep question of thermodynamic asymmetry,\ndiscussed briefly in supplement\n D.) \n\nTwo examples serve to illustrate the possibility of classical\nindeterminism. A very elegant recent example is provided by\n‘Norton’s dome’ (Norton 2003; 2008). A point mass is at\nrest at the apex of a dome (of certain shape) at\n\\(t^*\\). One obvious solution to Newton’s equations of\nmotion is that it continues to do so for all moments \\(t \\gt t^*\\). But, Norton points out, there is another\nsolution: that while at \\(t = t^*\\) the mass is\nat rest, at every moment \\(t \\gt t^*\\), the\nmass is moving in some direction. But this means the mass spontaneously\nmoves off in some arbitrary direction at an arbitrary\n time.[21]\nDeterminism is clearly violated: for some given time\n\\(t^*\\), there is a state at \\(t'\\) where the\nparticle remains at the apex of the dome; and there are many\nincompatible states where at \\(t'\\) the particle is\nsomewhere else on the surface of the dome. Nothing in the conditions at\n\\(t^*\\) fixes which of these many future states will be\nthe one to eventuate. An easy way to understand the dome example is to\nconsider its time reversal: a ball is given some initial velocity along\nthe surface of the dome toward the apex. Too little, and it falls\nshort; too great, and it overshoots. Just right, however, and the ball\ncomes to rest precisely at the apex, and remains at rest. The time\nreversal of this system is the original dome example. \n\nA more exotic example involves ‘space invaders’ (Earman\n1986: ch. III). These are particles that are at no spatial location at\ntime \\(t\\), and therefore form no part of the state at time\n\\(t\\), but which travel in ‘from’ spatial infinity and\nby time \\(t'\\) have a location. We can see the example more\nclearly if we invoke time-reversal invariance. Consider two point\nparticles, \\(a\\) and \\(b\\), at rest and in the vicinity of\none another at \\(t^*\\). From \\(t^*\\)\nonwards, force is applied to \\(a\\) in such a way that the velocity\nof \\(a\\) away from \\(b\\) increases without bound. This is\npossible, because there is no upper bound on velocity in classical\nphysics. Indeed, let the velocity of \\(a\\) increase fast enough\nthat by some finite time \\(t', a\\) has no finite\nvelocity, and is thus ‘at’ spatial infinity. The\ntime-reversal of this system has the particle \\(a\\) having no\nlocation at \\(t'\\), but having a location and continuously\ndecreasing velocity at every moment \\(t \\gt t'\\),\nuntil it comes to rest at \\(t^*\\). This system violates\ndeterminism in the sense given above. The state at \\(t'\\)\nconsists of a single particle \\(b\\) at rest. That state could be\nfollowed at \\(t^*\\) by the space-invaded state just\ndescribed; or it could be followed at \\(t^*\\) by the\nincompatible state of \\(b\\) simply being at rest some more.\nNothing in the laws rules out either of these transitions. Of course,\nthe model isn’t particularly physically realistic—where does the\nforce acting on \\(a\\) come from? But physically more realistic\nsystems exhibiting the same general structure have been given; Earman\nmentions one due to Mather and McGehee (1975) involving four point\nparticles moving in such a way that the forces they exert on each other\nin collisions end up unboundedly far from one another in a finite time\n(see also Saari and Xia 1995). \n\nWhile classical mechanics is thus indeterministic, it is importantly\nnot chancy. There is no reason to think that we either need to or can\nassign a probability distribution over the possible future states in\nour indeterministic systems. Norton says this about his dome: \n\nOne might think that … we can assign probabilities to the\nvarious possible outcomes. Nothing in the Newtonian physics requires us\nto assign the probabilities, but we might choose to try to add them for\nour own conceptual comfort. It can be done as far as the direction of\nthe spontaneous motion is concerned. The symmetry of the surface about\nthe apex makes it quite natural for us to add a probability\ndistribution that assigns equal probability to all directions. The\ncomplication is that there is no comparable way for us to assign\nprobabilities for the time of the spontaneous excitation that respect\nthe physical symmetries of solutions. Those solutions treat all\ncandidate excitation times \\(T\\) equally. A probability\ndistribution that tries to make each candidate time equally likely\ncannot be proper—that is, it cannot assign unit probability to\nthe union of all disjoint\n outcomes.[22]\n Or one that is proper\ncan only be defined by inventing extra physical properties, not given\nby the physical description of the dome and mass, Newton’s laws and the\nlaws of gravitation, and grafting them unnaturally onto the physical\nsystem. (Norton 2003: 9–10) \n\nThe point Norton makes about the impossibility of a uniform\ndistribution over a countable set of time intervals holds also for the\ntime at which we might expect the space invader to occur in the second\ntype of case. Thus, it seems, we have indeterminism without chance. \n\nWe can use these constructions to come up with counterexamples to\nRCT. Let a dome system be prepared, and left for a period of 5 seconds.\nIf the ball remains at rest on the apex, call the outcome\n‘0’. If the ball moves, and so is at the end of 5 seconds\nsomewhere on the dome other than the apex, call the outcome\n‘1’. Both outcomes are physically possible final states of\nthe system after 5 seconds. If the system is repeatedly prepared in\nthis state, it is physically possible to get a random sequence of\noutcomes of these repeated trials. Certainly the outcomes cannot be\nproduced by some finite algorithm, since the indeterministic dynamics\npermits as physically possible every sequence of outcomes, including\nthose that differ at some place from every algorithmically compressible\nsequence. In the infinite future case, it is physically possible for\nthe system to produce every infinite binary sequence, but at most\ncountably many of these are non-random. So it is physically possible\nfor these setups to produce a random sequence of outcomes in the KML\nsense. But we then have randomness without a chance distribution over\nthe outcomes. Randomness only requires two distinguishable possible\noutcomes and the possibility of production of arbitrary sequences of\nsuch outcomes. Chance requires two distinguishable outcomes, each of\nwhich has some chance. These cases show that chance and possibility\ncome apart—there are cases where there are two possible outcomes\nof a process, neither of which has any chance at all (not even a chance\nof zero). \n\n The last objection draws on a remark made in\n §4.4.\n Consider a large finite urn full of \nblack and white balls. The number of balls is sufficiently large that\nthe sequence of outcomes of draws is long enough to be random. So\nsuppose we make a random selection from this urn, drawing balls\nwithout replacement until the urn is empty. The resulting sequence of\noutcomes is, or at least could be, random—it had better be,\nsince this sequence meets all the conditions to be a simple random\nsample from the population. (We could attach a number to each member\nof an experimental population, and give the \\(n\\)-th member a\npill of an active substance (respectively, a placebo) if\nthe \\(n\\)-th draw is black (white).) But in this process, the\noutcomes become less and less chancy as the number of balls\ndiminishes. Since there are only finitely many balls, there will a\ndraw such that the chance of it being black is either 1 or 0, and so\nwhatever outcome eventuates did not occur by chance. But then we have\na random sequence that includes an outcome (drawing a black ball,\nsay) which did not happen by chance, contrary to RCT. \n\nOne response is to say that this last outcome did happen by chance,\nsince at the start of the drawing there was a positive chance that a\nwhite ball would be drawn last, and a positive chance that a black ball\nwould be. This response neglects the time-dependence of chances. If\nthere were \\(n\\) balls initially, and we let ‘Lastie’\nname the ball that was in actual fact drawn last, then we can say: the\nchance that Lastie is drawn last was \\(1/n\\) initially, and after\nthe \\(m\\)-th draw it was \\(1/(n - m)\\), until\nit reached 1 and remained there. At that last stage it was no longer a\nmatter of chance whether Lastie would be drawn last; it is the only\nball left in the urn. RCT maintains that a given outcome happens by\nchance iff it is part of a random sequence. At every time, the event of\nLastie being drawn last is part of a random sequence. But then there is\nat least one time at which Lastie being drawn last is part of random\nsequence but, at that time, it did not happen by chance.\n(Alternatively, of course, it could be maintained that even events with\ntrivial chances happen by chance. But this would permit again the\nproblem of biased sequences; a string of all heads tossed by a\ntwo-headed coin could then be random, which it is not.) \n\nThe discussion in\n §§4–5\n leaves RCT in a doubtful position. But it may be that\nthe problems for RCT are due more to some defect in the theories of\nchance and randomness sketched in\n §§1–2.\n As noted earlier, there are\nalternative conceptions of chance and randomness that have some appeal\nand might perhaps save RCT. They won’t have much to say about the modal\nproblems for merely possible random sequences mentioned when RCT was\nfirst introduced. But perhaps the other objections can be avoided. The\nproblems for RCT arise fundamentally because of the split between\nproduct randomness and process chance. Closing this gap promises to aid\nRCT. This following two subsections will consider product conceptions\nof chance and process based conceptions of randomness. \n\nThe frequency theory is a product conception of chance. An\noutcome-type has a chance, according to von Mises (1957), just in case\nit is part of a random sequence of outcomes in which that outcome type\nhas a robust limit relative frequency. So chance can’t be separated\nfrom randomness; it in fact requires randomness. Moreover, since having\na limit relative frequency of \\(\\frac{1}{2}\\) is a measure one property of infinite\nbinary sequences, all random sequences define collectives. (Those\ninfinite binary sequences which do not converge on a limit frequency\nare non-random.) Yet the problems with frequentism as a theory of\nchance are well known (Hájek, 1997; 2009; Jeffrey,\n1977)—we have come across some of them above—and to save\nRCT at the price of accepting frequentism has not attracted many. \n\nBut the prospects are more promising for Humean views of chance\n(Lewis 1994; Loewer 2004), like the reductionist views discussed in\nsupplements\n A.1,\n A.3,\n and\n D.\n These are a species of product\nconception of chance, for a possible world will not feature chances\nunless the best (simplest, best fitting, and most informative)\ndescription of what events occur in that world involves a probability\nfunction. Two worlds couldn’t differ in their chances without differing\nalso somewhere in the events which occur. Chance thus supervenes on the\nactual outcomes in a given world, but not necessarily in a direct\nway—in the service of simplicity, some deviation from the actual\nfrequency values may yield a description that is better overall. There\nis considerable debate over whether a Humean best systems account can\nexplain all of the things we know about chance. Lewis thought that\nchance was the ‘big bad bug’ for his broadly Humean world\nview (though he thought that the NP, discussed in supplement\n A.1,\n debugged Humean Supervenience:\nLewis 1994), and there has been considerable debate over whether or not\nthe PP or the BCP can be accounted for by the Humean, as the references\nin the\n supplement A\n attest. Moreover, there\nare problems about whether the notion of a probability distribution\n‘fitting’ a set of outcomes makes sense (Elga 2004). But\nsuppose a best systems account can be made to work. \n\nThe role of simplicity in Humean views is important for randomness.\nFor if a world contains a genuinely random sequence of outcomes, there\nwill be no short description of that sequence. Those descriptions which\ndo not attempt to describe what happens in all its particularity, but\ninstead involve a probability distribution that makes the sequence a\ntypical one with respect to that probability function, will be less\ninformative but much shorter, and still fit just as well. So it seems\nthat if a world contains a random sequence of outcomes, the best theory\nwill be one which involves probabilities, and in virtue of being part\nof the best theory, those probabilities will be chances. That makes the\nright to left direction of RCT plausible. The other direction would\nhold if this route through simplicity was the only way in which chances\ncould enter into the best system. Could there be a world with chances\nin which there was no random sequence? Here things are rather murkier\nfor the Humean. For the hallmark of the best systems approach to chance\nis that, by involving simplicity, it avoids some of the problems for\npure frequentism. In particular, an unrepresentative outcome sequence\n(a short one, or highly biased one) need not force the chances to take\nthe value of the actual frequencies. Suppose a world contains two\nintrinsic duplicate coins, one of which is tossed many times, landing\nheads about half the time; the other is tossed few times and lands\ntails every time. The second coin’s outcome sequence has a heads\nfrequency of zero. It is a strength of the best systems account that we\nmay still say the chance of heads was \\(\\frac{1}{2}\\), because the coin is embedded\nin a world where another, similar, coin did have the appropriate\noutcome frequencies, and it is overall a simpler theory to say that\nboth of these coins obeyed the same probabilistic laws than that the\nsecond one was an oasis of determinism. But this case provides a\nproblem for RCT—it looks like the second coin toss is not part of\nany random sequence of outcomes (since a few all tails tosses is not\nrandom), but it has a chance. \n\nWe should not let even the partial success of best systems analysis\nin preserving RCT sway us. The Humean account of chance is perfectly\ncompatible with the existence of bias and with non-independent\nsequences of trials; RCT is not. The problem just mentioned arises even\nin the best circumstances for RCT, where there is at least one actual\nunbiased fair coin sequence. The existence of such a problem could have\nbeen predicted. The best systems account deviates from pure frequentism\nprecisely in trying to accommodate the single-case process intuitions\nthat, as we saw in\n §1,\n characterise chance. Every\nsuccess in this direction brings this broadly product conception of\nchance closer to a process conception, and will therefore be a\npotential opportunity for counterexamples to RCT to emerge. \n\nAs mentioned at the beginning of\n §2,\n sometimes\n‘random’ is used in a process sense. There have been some\nphilosophical approaches to randomness which attempt to take this\nseriously, but which do not take it to be merely equivalent to\n‘chancy’ and thus trivialise RCT. The most popular such\napproach is to connect randomness with indeterminism, and to\ndefend RCT by arguing directly that indeterminism yields both chance\nand randomness. Prospects for that approach will be discussed\n in\n §7. \n\nThe next most discussed view of process randomness is an epistemic\none. Random processes on this view are those whose outcomes we cannot\nknow in advance; that is, random processes are unpredictable\n(Eagle 2005; Kyburg 1974: ch.\n 9).[23]\n Here is one clear\nexpression of the view: \n\nFor this kind of view to yield the right results, we cannot count as\n‘being able to predict a process’ if we merely guess\nrightly about its outcomes. For that reason, prediction must involve\nsome notion of reasonableness; it must be rational for the\nagent to make the predictions they do. For example, Eagle (2005: 770)\ninsists that it is the physical theory accepted by the predicting agent\nthat makes certain posterior credences reasonable; simply guessing will\nnot be reasonable, even if it’s correct. \n\nThis definition overlaps considerably with those definitions of\nrandomness canvassed in\n §2.\n In particular, if a\nprocess is predictable, that will make available a winning betting\nstrategy on the sequence of outcomes of that process, which cannot\ntherefore be a KML-random sequence of outcomes. So unpredictability of\nthe generating process is a necessary condition on KML-randomness of\nany concrete outcome sequence. \n\nBut unpredictability is not sufficient, for it may be that we cannot\nknow everything true of the future outcomes of the process, and those\ntruths might preclude a KML-random sequence. One way to see this draws\non our discussion of chaotic dynamics. Let’s say that a system exhibits\napparent dependence on initial conditions if states\nindiscriminable to us may end up arbitrarily far apart after some\nrelatively short period of time. (Another definition, if knowledge\nobeys a margin for error principle, would be: a system exhibits\napparent dependence on initial conditions if, for all we know, it is\nsensitively dependent on initial conditions.) Apparent dependence on\ninitial conditions can obtain even if sensitive dependence on initial\nconditions does not. For there may well be a size \\(v\\) such that,\nunder some partition of the set of states into regions of measure\nsmaller than \\(v\\), any two points starting in the same cell of\nthe partition evolve to future states which are close to one\nanother—as long as \\(v\\) is small with respect to our\nabilities to discriminate. A system which is apparently but not\nsensitively dependent on initial conditions will be unpredictable to\nus, but there will exist an algorithm that, given some finite\nspecification of the initial conditions, generates the future evolution\nof the system perfectly. (The key is that the finite specification must\ninvolve more precise data than we could know to characterise the\nsystem.) This sequence, if long enough, is not KML-random, but it is\n unpredictable.[24] \n\nBecause of the considerable overlap between the unpredictably\ngenerated sequences and the KML-random sequences (Frigg 2004: 431),\nmany roles the latter can play will be played by the former too. Eagle\n(2005) further argues that the unpredictably generated sequences are a\nbetter fit to the theoretical role of randomness, and claims on that\nbasis that randomness is unpredictability. One nice thing\nabout this thesis is that, being a process notion, it directly connects\nwith chance processes. We can thus directly evaluate the original\nthesis connecting chance and randomness, CT, in the form: \n\nThe left-to-right direction of CTU looks relatively secure when we\nattend just to independent and identically distributed trials. But when\nthe trials are not independent, like the examples in\n §4.4,\n future outcomes can happen by chance, even if\nknowing the past states of the system does put one in a position to\nbetter predict the future outcomes. The right-to-left direction of CTU\nis in even worse straits. For if KML-randomness of a sequence entails\nits unpredictable generation, then every counterexample to RCT which\ninvolves KML-randomness without chance will also involve\nunpredictability without chance, and also constitute a counterexample\nto CTU. There is no succour to be found for defenders of RCT in this\nconception of randomness. \n\nOne last hope for the thesis that chancy outcomes are random comes\nfrom the connection of both notions with indeterminism. Consider this\nargument: \n\n\\(\\mathbf{P2}\\):\n\nA possible sequence of outcomes is random iff a repeated\nindeterministic process could produce all of the outcomes. \n\n\\(\\mathbf{RCT}\\):\n\nTherefore, an outcome happens by chance iff there is a possible random\nsequence of outcomes, produced by the same process, of which it is a\nmember. \n\nThis argument is valid. If the premises are true, we have a direct\nargument for RCT. We have no direct response to the objections raised\nin\n §§4–5,\n but\nsomehow, if this argument succeeds, those objections must miss the\npoint. The premises have some initial plausibility (though P2 is\ndubious: surely a properly indeterministic process could produce any\nsequence of outcomes whatsoever, including many non-random sequences?\nWe discuss this further in\n §7.2).\n The thesis\nthat indeterminism is necessary and sufficient for chance has long been\na popular claim. And randomness and indeterminism also seem to have a\nclose connection. But to evaluate them, we will need to be more precise\nthan we have been about determinism. \n\nDeterminism so stated is a supervenience thesis: as Schaffer (2007:\n115) puts it, ‘A world \\(w\\) is deterministic iff: for all\ntimes \\(t\\) in \\(w\\), the total occurrent history of\n\\(w\\) supervenes on the occurrent state of \\(w\\) at\n\\(t\\) together with the laws of \\(w\\).’ \n\nWith this in mind, we now evaluate the premises of this argument.\nThere seem to be significant doubts about both directions of both\npremises. The upshot is that indeterminism offers little comfort for\nthe defender of RCT. \n\nIt is very nearly a piece of philosophical orthodoxy that\nnon-trivial objective chances require indeterminism. The view is seldom\ndefended; even those who trouble to state the view explicitly (Lewis,\n1980: 120) don’t go to the further effort of justifying it, perhaps\nbecause it seems obvious. After all, under determinism, someone who\nknew the past and the laws would be in a position to know with\ncertainty every future outcome. So our use of probability under\ndeterminism must be purely subjective, a side-effect of our (perhaps\nunavoidable) ignorance of the past or the laws. If this orthodoxy is\ncorrect, at least the left-to-right direction of P1 would be true. \n\nHowever, there has recently been a considerable amount of work by\nphilosophers defending the thesis that chance and determinism are\nconsistent. Many of the topics dealt with above feature in their\narguments. Loewer (2001) draws on the best systems analysis of chance\nto argue that, in worlds like ours (with entropy-increasing processes,\nand apparently fair coin tosses, etc.), the best description involves\nsome probabilistic component which deserves to be called chance. Clark\n(1987) draws on how we use the phase space measure \\(\\mu\\) to\ngovern our expectations about the behaviour of Bernoulli and mixing\nsystems in classical statistical mechanics, and argues (in effect) that\nthis is an objective probability function despite the deterministic\nunderlying physics. Various other proposals for deterministic chance\nhave been developed (Eagle 2011; Glynn, 2010; Hoefer, 2007;\nIsmael, 2009; Sober, 2010). The general technique is to argue that\nthere are probability distributions over outcomes that can play the\nchance role, even in impeccably deterministic theories. Many of these\nphilosophers are sympathetic to reductionism about chance, which\npermits the compatibility of chance and determinism. For the fact that\nthe entire history of a world supervenes on any moment of its history,\nas determinism states, apparently entails nothing one way or another\nabout whether the best description of that entire history involves\nchances (this is related to the ‘no-connection’ argument of\nSchaffer 2007: 115). If there is such a thing as deterministic chance,\nhowever, P1 is false. \n\nAnti-reductionists about chance have generally found these arguments\nless persuasive (Popper 1992; Black 1998; Weiner and Belnap 2006). In\nparticular, it is in many ways hard to reconcile the BCP (and RP) with\ndeterministic chance. Doing so will require that there is a physically\npossible world (sharing the same laws) which matches ours in occurrent\nfacts up until \\(t\\) but diverges thereafter; but if determinism\nis true, such a divergence is not possible. If that world matches ours\nat any time, it matches it at all times. So, it seems, ‘only an\nincompatibilist function can fit the RP’ (Schaffer 2007: 130). In\nany case, the debate over whether deterministic ‘chance’,\nand reductionism about chance more generally is ongoing (see further\nthe discussion in supplement\n A);\n the status\nof the left-to-right direction of P1 is at best unsettled. \n\nThe same cannot be said for the right-to-left direction. For the\ndiscussion in\n §5.3\n showed that there are\nindeterministic theories without chances, those where indeterminism is\nsecured by the existence of alternative future possibilities, but where\nthose possibilities collectively do not permit or require a probability\ndistribution over them. A more controversial class of cases of\nindeterminism without chances comes from those who reject\nuniversalism about chance: the thesis ‘that chance of\ntruth applies to any proposition whatever’ (Lewis, 1980: 91). If\nuniversalism is false, there may be indeterministic situations where\nthe alternative outcomes are those to which chance does not apply. Von\nMises rejected universalism, because he thought that chance applied\nproperly only to ‘mass phenomena’; in an indeterministic\nworld where the indeterministic process occurs only once, for example,\nthe theory of probability does not apply. Hoefer (2007) also holds a\nview something like this, rejecting chances for those processes which\ndon’t have stable enough outcome\n patterns.[25] \n\nWe could give an argument for P2 if it could be shown that, from the\nabove definition of determinism, we could conclude (i) that only\nrandom sequences could occur under indeterminism, and (ii) that random\nsequences could only occur under indeterminism. However, both parts of\nthis claim are problematic. \n\nThe proof of this theorem relies on the fact that a theory can be\nnon-trivially deterministic without being computable. There is an\narithmetically definable function which governs the evolution of the\nsystem over time (in Humphrey’s construction, the instantaneous state\nof the system is an arithmetically definable function of the time,\nwhich ensures that any two models agreeing at one time will agree at\nall times, guaranteeing determinism). But the function is not\neffectively computable, so no algorithm can produce the sequence of\n states that this system goes\n through.[26]\n The physical\nsignificance of such uncomputable functions is not clear (though see\nPour-El and Richards 1983), but the possibility of a deterministic\nphysics which features such equations of motion is enough to undermine\na close connection between randomness and indeterminism. This shows\nthat claim (ii) above is false. \n\n Moreover, since we’ve already seen\n (§4.1)\n that it is possible for a chancy and indeterministic process to produce\na non-random sequence of outcomes, and such a sequence would not be\nrandom, we also have a counterexample to claim (i). Claim (i) could be\nsaved if we made a more robust claim about what could happen under\nindeterminism. There is a sense in which, while it is possible that a\nfair coin could land heads an infinite number of times, it would\nnot. That is, the counterfactual ‘If I tossed the coin an\ninfinite number of times, it wouldn’t land all heads’ is\napparently true. There is some question whether the counterfactual\nreally is true; Lewis (1979a) and Williams (2008) argue that it is,\nwhile Hawthorne (2005)  argues that it is not. But if it is, the way lies open to\ndefend a modified counterfactual version of P2: \n\nBut this is highly controversial; and the problem for claim (ii)\nwould still stand. \n\nIf we are to accept this argument, then, we shall have to take P2 as\nan independent truth about randomness. Analyses of randomness as\nindeterminism, which take P2 to be analytic, have been given: to their\ndetriment, if the foregoing observations are correct. Hellman (1978:\n83) suggests that randomness is ‘roughly interchangeable with\n“indeterministic”’, while Ekeland (1988: 49) says\n‘the main feature of randomness is some degree of independence\nfrom the initial conditions. … Better still, if one performs\nthe same experiment twice with the same initial conditions, one may\nget two different outcomes’. \n\nHowever, it has been argued that this view of randomness as\nindeterminism makes it difficult to understand many of the uses of\nrandomness in science (Eagle 2005: §3.2). This view entails that\nrandom sampling, and random outcomes in chaotic dynamics, and random\nmating in population genetics, etc., are not in fact random, despite\nthe plausibility of their being so. It does not apparently require\nfundamental indeterminism to have a randomised trial, and our\nconfidence in the deliverances of such trials does not depend on our\nconfidence that the trial design involved radioactive decay or some\nother fundamentally indeterministic process. Indeed, if Bohmians or\nEverettians are right (an open epistemic possibility), and quantum\nmechanics is deterministic, this view entails that nothing is actually\nrandom, not even the most intuitively compelling cases. This kind of\nview attributes to scientists a kind of error theory about many of\ntheir uses of the term ‘random’, but as yet the\nphilosophical evidence adduced to convict scientists of this pervasive\nerror is not compelling. \n\nOne reason for the continuing attractiveness of the thesis that\nrandomness is indeterminism may be the fact that, until quite\nrecently, there has been a tendency for philosophers and other to\nconfuse unpredictability and indeterminism. Laplace’s original\nconception of determinism was an epistemic one: \n\nThis kind of account still resonates with us, despite the fact that\nwith the Montague-Earman definition we now have a non-epistemic\ncharacterisation of determinism. Since random sequences will, almost\nalways, be unpredictable, it is clear why we might then taken them to\nbe indeterministic. But once we keep clear the distinction between\npredictability and determinism, we should be able to avoid this\nconfusion (Bishop, 2003; Schurz, 1995; Werndl, 2009). \n\nFrom what we have seen, the commonplace thesis cannot be sustained.\nIt would in many ways have been nice if chance of a process and\nrandomness of its product had gone hand in hand—the epistemology\nof chance would be much aided if it invariably showed itself in random\noutputs, and we could have had a tight constraint on what outcomes to\nexpect of a repeated chance process, to say nothing of the further\ninteresting consequences the thesis may have for random sampling or\nprobabilistic explanation, mentioned in the introduction. But the\ncounterexamples to the thesis in\n §§4–5\n show that it is false, even in\nits most plausible form. Various attempts to salvage the thesis, by\nappeal to non-standard accounts of chance or randomness, fail to give\nus a version of the thesis of much interest or bearing on the issues we\nhad hoped it would illuminate. A final attempt to argue directly for\nthe thesis from the connections between chance, randomness, and\ndeterminism also failed, though it does shed light on all three\nnotions. It is safest, therefore, to conclude that chance and\nrandomness, while they overlap in many cases, are separate\nconcepts. \n\nThis is not to say that there is no link between KML-randomness and\nphysical chance. The observation of a random sequence of outcomes is a\ndefeasible incentive to inquire into the physical basis of the outcome\nsequence, and it provides at least a prima facie reason to think that a\nprocess is chancy (though recall\n §4.5).\nMoreover, if we knew that a process is chancy, we should expect\n(eventually, with high and increasing probability) a random sequence of\noutcomes. Conversely, a sequence of outcomes that appears predictable, compressible, and rule-governed will be strong disconfirming evidence for any hypothesis to the effect that the sequence was produced by chance alone. \nHellman concludes \nTaking ‘mathematical randomness’ to be product randomness, and ‘physical randomness’ to mean process randomness (chanciness), this conclusion seems unavoidable.\n A parallel with the relationship between frequencies and chances is tempting and unsurprising. Relative frequencies are good but not infallible indicators of the chances, and the existence of outcome frequencies strictly between 0 and 1 is evidence that chance processes are involved in the production of those outcomes. But frequentism is implausible as a reductive account of chance. And it is no more plausible to think that chance is present iff random sequences of outcomes are.[27] An evidential and epistemic connection between chance\nand randomness falls well short of the conceptual connection proposed\nby the Commonplace Thesis with which we started.","contact.mail":"antony.eagle@adelaide.edu.au","contact.domain":"adelaide.edu.au"}]
