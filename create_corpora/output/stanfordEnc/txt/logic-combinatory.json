[{"date.published":"2008-11-14","date.changed":"2020-11-16","url":"https://plato.stanford.edu/entries/logic-combinatory/","author1":"Katalin Bimbó","author1.info":"http://www.ualberta.ca/~bimbo/","entry":"logic-combinatory","body.text":"\n\n\nCombinatory logic (henceforth: CL) is an elegant and\npowerful logical theory that is connected to many areas of\nlogic, and has found applications in other disciplines, especially, in\ncomputer science and mathematics.\n\n\nCL was originally invented as a continuation of the reduction of the\nset of logical constants to a singleton set in classical first-order\nlogic (FOL). CL untangles the problem of substitution, because\nformulas can be prepared for the elimination of bound variables\nby inserting combinators. Philosophically speaking, an expression that\nhas no bound variables represents the logical form of the original\nformula. Sometimes, bound variables are thought to signify\n“ontological commitments.”\nAnother philosophical rôle of CL is to show the variability of\nthe ontological assumptions a theory has.\n\n\nSubstitution is a crucial operation not only in first-order logics,\nbut also in higher-order logics,\nas well as in other formal systems that contain a variable binding\noperator, such as the \\(\\lambda\\)-calculi and the \\(\\varepsilon\\)-calculus.\nIndeed, carrying out substitution correctly is particularly pressing in\n\\(\\lambda\\)-calculi\nand in the closely related functional programming languages. CL can\nemulate \\(\\lambda\\)-abstraction despite the fact that CL has no variable\nbinding operators. This makes CL a suitable target language for\nfunctional programming languages to be compiled into.\n\n\nThe connection to \\(\\lambda\\)-calculi might suggest—correctly—that\nCL is sufficiently expressive to formalize\nrecursive functions\n(i.e., computable functions) and arithmetic. Consequently, CL is\nsusceptible to \nGödel-type incompleteness theorems. \n \n\nCL is an archetypical term rewriting system (TRS). These\nsystems comprise a wide range of formal calculi from syntactic\nspecifications of programming languages and context-free grammars to\nMarkov algorithms; even some number theoretic problems may be viewed\nas special instances of questions about TRSs. Several notions and\nproof techniques that were originally invented for CL, later turned\nout to be useful in applications to less well-understood TRSs.\n\n\nCL is connected to nonclassical logics via typing. First, a\ncorrespondence between formulas that are provable in the implicational\nfragment of intuitionistic logic and the typable combinatory terms was\ndiscovered. Then the isomorphism was generalized to other combinatory\nbases and implicational logics (such as the logic of relevant\nimplication, exponential-free linear logic, affine logic, etc.).\n\n\nSelf-reference factors into some paradoxes, such as the widely known\nliar paradox\nand\nRussell’s paradox.\nThe set theoretical understanding of functions also discourages the\nidea of self-application. Thus it is remarkable that pure untyped CL\ndoes not exclude the self-application of functions. Moreover, its\nmathematical models showed that a theory in which functions can become\ntheir own arguments is completely sensible, in addition to being\nconsistent (what was established earlier using proof theoretic\nmethods).\n\nClassical first-order logic includes quantifiers that are\ndenoted by \\(\\forall\\) (“for all”) and \\(\\exists\\) (“there\nis a”). A simple sentence such as “All birds are\nanimals” may be formalized as \\(\\forall x(Bx\\supset Ax)\\), where \\(x\\)\nis a variable, \\(B\\) and \\(A\\) are one-place predicates, and \\(\\supset\\) is a\nsymbol for (material) implication. The occurrences of the variables in the\nclosed formula \\(\\forall x(Bx\\supset Ax)\\) are bound, whereas those in\nthe open formula \\(Bx\\supset Ax\\) are free. If we assume that\n\\(t\\) (for “Tweety”) is a name constant, then an instance of the\nabove sentence is \\(Bt\\supset At\\), that may be read as “Tweety is an\nanimal, provided Tweety is a bird.” This illustrates that the\ninstantiation of a (universal) quantifier involves substitution. \nDue to the simplicity of the example, the substitution of\n\\(t\\) for \\(x\\) in \\(Bx\\) and in \\(Ax\\) seems to be easy to understand and to\nperform. However, a definition of substitution for FOL (and in general, for an\nabstract syntax, that is, for a language with a variable binding operator) has\nto guarantee that no free occurrence of a variable in the substituted\nexpression becomes bound in the resulting expression. \nTo see what can go wrong, let us consider the (open) formula\n\\(\\forall x(Rxy\\land Rxr)\\), where \\(R\\) is a two-place predicate, \\(r\\) \nis a name constant abbreviating “Russell” and \\(\\land\\) is\nconjunction.  \\(\\forall x(Rxy\\land Rxr)\\) contains a free occurrence\nof \\(y\\) (that is, \\(y\\) is a free variable of the formula), however, \\(y\\)\nis not free for a substitution of a term that contains a free occurrence\nof \\(x\\), for instance, \\(x\\) itself. More formally, the occurrence\nof \\(y\\) in the second argument place of \\(R\\) in \n\\(\\forall x(Rxy\\land Rxr)\\) is not bound by a quantifier (the only quantifier)\nof the formula, whereas \\(\\forall x(Rxx\\land Rxr)\\) is a closed formula, that\nis, it contains no free occurrences of variables.  Informally, the following\nnatural language sentences could be thought of as interpretations of the\nprevious formulas. “Everybody reads him and Russell,” (where\n‘him’ is deictic, or perhaps, anaphoric) and “Everybody\nreads himself and Russell.” Obviously, the meanings of the two sentences\nare vastly different, even if we assume that everybody pens something. As\na contrast, \\(\\forall x(Rxw\\land Rxr)\\) exhibits an unproblematic substitution\nof the name constant \\(w\\) for the free occurrence of \\(y\\). (The latter\nformula, perhaps, formalizes the sentence “Everybody reads Ludwig\nWittgenstein and Russell.”) These examples are meant to demonstrate the\nmore complex part of the problem Moses Schönfinkel set out to solve,\nand for what he invented\n CL.[1] \nA well-known result about classical sentential logic (SL) is\nthat all truth-functions can be expressed in terms of \\(\\lnot\\) and \\(\\land\\)\n(or of \\(\\lnot\\) and \\(\\lor\\), etc.). A minimal sufficient set of connectives,\nhowever, can contain just one connective such as \\(\\mid\\) (“nand,”\nwhich is often called, Sheffer’s stroke), or \\(\\downarrow\\)\n(“nor,” which is Peirce’s joint denial). “Nand” is\n“not-and,” in other words, \n\\(A\\mid B\\) is defined as \\(\\lnot(A\\land B)\\), where \\(A\\), \\(B\\) range over\nformulas and \\(\\lnot\\) is negation. Going into the other \ndirection, if \\(\\mid\\) is a primitive, then \\(\\lnot A\\) is definable\nas \\(A\\mid A\\), and \\(A\\land B\\) is \\((A\\mid B)\\mid(A\\mid B)\\).  Although\nformulas with numerous vertical lines may quickly become \nvisually confusing and hard to parse, it is straightforward to prove\nthat \\(\\mid\\) alone is sufficiently expressive to define all the\ntruth-functions. \nSchönfinkel’s aim was to minimize the number of logical constants\nthat are required for a formalization of FOL, just as\nHenry M. Sheffer (indeed, already\nCharles S. Peirce) did for classical propositional logic.\nOne of the two quantifiers mentioned above suffices and the other may\nbe assumed to be defined. Let us say, \\(\\exists x A\\) is an\nabbreviation for \\(\\lnot\\forall x\\lnot A\\). Even if \\(\\lnot\\)\nand the rest of the connectives are traded in for \\(\\mid\\),\ntwo logical constants remain: \\(\\forall\\) and \\(\\mid\\).  A\nfurther pressing issue is that quantifiers may be nested (i.e., the\nscope of a quantifier may fully contain the scope of another\nquantifier), and the variable bindings (that could be visualized by\ndrawing lines between quantifiers and the variables they bind) may get\nquite intertwined. Keeping for a moment the familiar logical\nconstants, we may look at the following formula that hints at the\nemerging difficulties—when the question to be tackled is\nconsidered in its full\n generality.[2] \n\\(\\forall x\\) binds all occurrences of \\(x\\); the variables in\nthe second argument place of the two \\(B\\)s are bound by one of the\ntwo \\(\\exists y\\)s, the latter of which interacts with \\(\\forall z\\)\nvia \\(Ozy\\). \nPredicates have a fixed finite arity in FOL, and nothing precludes\nbinding at once a variable in the first argument of one predicate and\nin the second argument of another predicate. (Indeed, FOL would lose\nsome of its expressiveness, if bindings of this sort would be excluded\nwithout some means to compensate for them.) These difficulties persist\nwhen a formula is transformed into a(n equivalent) formula in\nprenex normal form. As long as the variable bindings can\ninterweave and braid into arbitrarily complex patterns, there seems to\nbe no way to eliminate bound variables. (Note that free variables in\nopen formulas—in a sense—behave like local name constants,\nand their elimination is neither intended, nor achieved in the\nprocedures described here.) \nSchönfinkel’s ingenuity was that he introduced combinators to\nuntangle variable bindings. The combinators \\(\\textsf{S}\\),\n\\(\\textsf{K}\\), \\(\\textsf{I}\\), \\(\\textsf{B}\\) and \\(\\textsf{C}\\)\n(in contemporary notation) are his, and he established that \\(\\textsf{S}\\) and\n\\(\\textsf{K}\\) suffice to define all the other combinators. In\neffect, he also defined an algorithm to carry out the elimination of\nbound variables, which is essentially one of the algorithms used\nnowadays to define bracket abstraction in\n CL.[3] \nSchönfinkel introduced a new logical constant \\(U\\), that expresses\nthe disjointness of two classes. For instance, \\(UPQ\\) may be written\nin usual FOL notation as \\(\\lnot\\exists x(Px\\land Qx)\\), when \\(P\\) and \\(Q\\) \nare one-place predicates. (The formula may be thought to formalize, for\ninstance, the natural language sentence “No parrots are quiet.”)\nIn the process of the elimination of the bound variables, \\(UXY\\) is obtained\nfrom an expression that contains ‘\\(Xx\\)’ and\n‘\\(Yx\\)’, where \\(x\\) does not occur in \\(X\\) or \\(Y\\). For\nexample, if \\(X\\) and \\(Y\\) happen to be \\(n\\)-ary predicates with\n\\(n\\ge2\\), then \\(x\\) occurs only in their last argument places. “Nobody\nreads Aristotle and Plato” can be formalized as \\(\\lnot\\exists x(Rxa\\land\nRxp)\\), where \\(a\\) and \\(p\\) are name constants that stand for\n“Aristotle” and “Plato,” respectively. This\nformula cannot be written as \\(U(Ra)(Rp)\\). On the other hand,\n“There is nobody whom both Russell and Wittgenstein read,” that\nis, \\(\\lnot\\exists x(Rrx\\land Rwx)\\) turns into \\(U(Rr)(Rw)\\), where the\nparentheses delineate the arguments of \\(U\\). Often, the expressions \\(X\\)\nand \\(Y\\) (in \\(UXY\\)) consist of predicates (and name constants) together \nwith combinators and other \\(U\\)s. \nIt is useful to have a notation for “nextand” (i.e.,\n“not-exists-and”) without assuming either that \\(x\\)\nhas a free occurrence in the expressions joined, or that if it has\none, then it is the last component of the expressions. Following\nSchönfinkel, we use \\(\\mid^x\\) for the\n“nextand” operator that binds \\(x\\). (The \nnotation \\(\\mid^-\\), where \\(^-\\) is the place for a variable, closely\nresembles the Sheffer stroke.) Schönfinkel achieved his goal of\nthe reduction of the set of logical constants for FOL to a\nsingleton set \\(\\{\\mid^-\\}\\), because every formula of FOL is\nequivalent to a formula that contains only “nextand.” \nA formula \\(\\forall x A\\) is usually defined to be well-formed in\nFOL even if \\(A\\) has no free occurrences of \\(x\\). Then, of\ncourse, \\(\\forall x A\\) is equivalent to \\(A\\) as well as to \\(\\exists x A\\),\nand such quantifiers are called vacuous. In order to show that any\nformula can be rewritten into an equivalent formula that contains only\n“nextand,” it is sufficient to inspect the following definitions\nfor \\(\\lnot\\), \\(\\lor\\) and \\(\\forall\\) (with suitable variables)—that\nare due to Schönfinkel. \nThe definition for \\(\\lnot\\), for instance, may be justified by the following\nequivalences. \\(A\\Leftrightarrow A\\land A\\), \\(A\\land A\\Leftrightarrow \\exists\nx(A\\land A)\\) (assuming that \\(x\\) is not free in \\(A\\)), hence by\nreplacement, \\(\\lnot A\\Leftrightarrow \\lnot\\exists x(A\\land A)\\). \nNow we give a concrete example to illustrate how to turn a formula of\nFOL into one that contains only \\(\\mid^-\\), and then how to eliminate the\nbound variables using \\(U\\) and combinators. To put some excitement into the\nprocess, we start with the sentence in (#1). \nA straightforward formalization of this sentence—on the\nforeground of the domain of numbers—is the formula in (#2),\n(where ‘\\(Nx\\)’ stands for “\\(x\\) is a natural\nnumber,” ‘\\(Px\\)’ stands for “\\(x\\) is a\nprime” and ‘\\(Gxy\\)’ is to be read as “\\(x\\) is\ngreater that \\(y\\)”). \nThis formula is equivalent to \\(\\forall y(Ny\\supset\\exists x(Px\\land Gxy))\\)\nand further to \\(\\lnot\\exists y\\lnot(Ny\\supset\\exists x(Px\\land Gxy))\\).  In\none or two more steps, we get \\(Ny\\mid^y(Px\\mid^xGxy)\\).  (Expressions are\nconsidered to be grouped to the left unless parentheses indicate\notherwise. E.g., \\(Gxy\\) is \\(((Gx)y)\\) not \\(G(xy)\\) as could have been,\nperhaps, expected based on the most common way of arranging parentheses in FOL\nformulas.) Unfortunately, neither \\(\\mid^x\\) nor \\(\\mid^y\\) can be replaced\nby \\(U\\) in the last expression. However, if the arguments of \\(G\\) were\npermuted then the former reduction could be carried out. One of the\ncombinators, \\(\\textsf{C}\\) does exactly what is needed: \\(Gxy\\) can be\nchanged to \\(\\textsf{C}Gyx\\) (see the definition of combinators in section\n2.1). That is, we have \\(Ny\\mid^y(Px\\mid^x\\textsf{C}Gyx)\\), and then \\(Ny\\mid^yUP\n(\\textsf{C}Gy)\\).[4]  The\nexpression may give the impression that \\(y\\) is the last component\nof \\(UP(\\textsf{C}Gy)\\), which is the second argument of \\(\\mid^y\\), but it \nis not so. The grouping within expressions cannot be disregarded, and another\ncombinator, \\(\\textsf{B}\\) is needed to turn \\(UP(\\textsf{C}Gy)\\) into the\ndesired form \\(\\textsf{B}(UP)(\\textsf{C}G)y\\).  From\n\\(Ny\\mid^y\\textsf{B}(UP)(\\textsf{C}G)y\\), we get \\(UN(\\textsf{B}(UP)\n(\\textsf{C}G))\\) in one more step. This expression is completely free of\nvariables, and it also makes the renaming of bound variables in FOL\neasily comprehensible: given two sequences of (distinct) variables that are\ndifferent in their first two elements, the reversal of the above\nprocess yields formulas that are (logically equivalent) alphabetic\nvariants of the formula in (#2). \nThe expression \\(UN(\\textsf{B}(UP)(\\textsf{C}G))\\) may look\n“unfamiliar” when compared to formulas of FOL, but\nnotation—to a large extent—is a matter of convention. It may be\ninteresting to note that the first \\(U\\) is simply followed by its two\narguments, however, the second \\(U\\) is not. \\(\\textsf{B}(UP)\\) is a\nsubexpression, but \\(UP(\\textsf{C}G)\\) is not a subexpressions of \\(UN\n(\\textsf{B}(UP)(\\textsf{C}G))\\).  Furthermore, the whole\nexpression can be transformed into \\(XNPG\\) using combinators,\nwhere \\(X\\) is composed of \\(U\\)s and combinators only. Such\nan \\(X\\) concisely encodes the logical form or logical\ncontent of the formula with the predicates being\narguments.[5] \nThe expressions obtained via the transformations outlined above\nquickly become lengthy—as trying to rewrite a simple FOL\nsentence such as \\(\\exists x(Px\\land Qx)\\)\ncan show.[6]  However,\nthis does not diminish the importance of Schönfinkel’s theoretical\nresults. A slight increase (if any) in the length of the expressions is not\neven an inconvenience, let alone an impediment in the era of computers with\npetabytes (or even exa- and zettabytes) of memory. \nIt seems unfortunate that Schönfinkel’s reduction procedure for\nFOL is not widely known. As a measure of how widely Sheffer’s and\nSchönfinkel’s reductions are known, we appeal to the fact that\nthe first is part of standard intro courses in logic, whereas the\nsecond is not. Undoubtedly, one of the reasons for this is that\nSchönfinkel’s process to eliminate bound variables is\nconceptually more opulent than defining a few truth functions from\n\\(\\mid\\) (or \\(\\downarrow\\)). Another reason may be that\nSchönfinkel, perhaps, did not place a sufficiently strong\nemphasis on the intermediate step that allows the elimination of all\nother logical connectives and quantifiers via “nextand.”\nThe importance of this step was also overlooked in the introduction to\nthe English translation of Schönfinkel’s paper, which was written\nmore than 30 years after the original publication. We may also note\nthat although “nextand” is an operator in the standard\nlogical sense, it is binary—unlike \\(\\forall\\) and \\(\\exists\\), which\nare unary. \nIf \\(A\\mid B \\Leftrightarrow_\\textrm{df}\n\\lnot(A\\land B)\\) is added as a definition to SL, then the\nresult is a conservative extension, and it becomes provable\nthat for any formula \\(A(p_0,\\ldots,p_n)\\) (i.e., for a formula containing the\ndisplayed propositional variables and some connectives) there is a formula\n\\(B(p_0,\\ldots,p_n)\\) containing only the connective \\(\\mid\\), and\n\\(B(p_0,\\ldots,p_n)\\Leftrightarrow A(p_0,\\ldots,p_n)\\) itself is\nprovable. \\(\\mid\\) is, of course, interpreted as the “nand” truth\nfunction.  “Nand” as a binary connective or as a binary truth\nfunction is of the same sort of object as conjunction, disjunction,\netc. \nThe first stage in Schönfinkel’s extension of FOL is analogous.\nThe addition of \\(\\mid^-\\) is (also) a conservative extension of FOL, and\nevery occurrence of \\(\\mid^-\\) can be eliminated. (We noted that \\(\\mid^-\\) is\na binary operator, and so it may be thought to combine a quantifier\n(\\(\\exists\\)) with connectives (\\(\\lnot\\), \\(\\land\\)), but \\(\\mid^-\\)\nof course, does not introduce any objects that are not definable in FOL.) \nThe second stage in Schönfinkel’s extension of FOL is slightly\ndifferent. \\(UXY\\) is definable in FOL only for one-place\npredicates \\(P\\) and \\(Q\\) (or for predicates of higher\narity when the variable in their last argument is bound). Thus, in general,\nneither \\(U\\) nor the combinators are definable in FOL. \nThe elimination of bound variables goes beyond the resources of FOL.\nThe combinators are not only undefinable, but they are new kinds of\nobjects—which are absent from FOL itself. Also, the intermediate\nsteps of the bound variable elimination procedure presuppose that\nfunctions of several arguments can be viewed as functions in one\nvariable, and the other way\naround.[7]\nEnriching a presentation of FOL with predicate letters that have\nsufficiently many arguments in the right order would be more or less\nunproblematic, and it would add objects to the language that would\nhave the same sort of interpretation as other predicates. A potential\nproblem though is that for each predicate, infinitely many\n(\\(\\aleph_0\\) many) new predicates would be needed—together with axioms\nstipulating the intended equivalences between the \nmeanings of the variants of the predicates. Notationally, these steps\namount to padding predicate symbols with extra arguments, omitting\nsome arguments, as well as permuting and regrouping the arguments.\nAlthough some of these additions may look superfluous or too fussy,\nfor the understanding of Schönfinkel’s procedure to eliminate\nbound variables, it is crucial to note that formulas are viewed as\nstructured strings of\nsymbols.[8] \nIn conclusion to this section, it is important to emphasize that there\nare no questions of consistency with respect to the above\nreduction process, because it can be viewed—or described in\ncontemporary terms—as a well-defined algorithm. It is a\ncompletely different issue that if we consider the language of FOL\nexpanded with combinators, then the resulting system is inconsistent,\nbecause CL is powerful enough to define the fixed point of any\nfunction. The effect of having fixed points for all\nfunctions—including truth functions—may be thought to\namount to adding certain biconditionals (which may or may not be\nvalid) as axioms. (For instance, Russell’s paradox emerges from the\nfixed point of the negation connective.) Notably, both FOL and (pure)\nCL are consistent. \nIn this section we briefly outline two ideas that are related to\nSchönfinkel’s work or are motivated by his use of combinators in\nthe elimination of bound variables. \nFitch’s metalogic\n\nFrom the late 1930s, Frederic Fitch worked on a logic that he called\nbasic logic. The label is motivated by his aim to provide a\nframework in which any logic could be formalized. Fitch’s approach is\nutterly syntactic (much like Schönfinkel’s), and\n“formalization” is to be understood as encoding a\nformally described system in another—not unlike the\narithmetization of the syntax in Gödel’s incompleteness\ntheorem. \nIn 1942, Fitch introduced a logic that he labeled \\(K\\). The\nexpressions in \\(K\\) are formed like combinatory terms by a binary\napplication operation, which is not assumed to be associative. (See\nthe definition of combinatory terms in the next section.) However, the\nconstants of \\(K\\) do not coincide with the constants of pure CL.\nFitch uses 10 constants: \\(\\varepsilon\\), \\(o\\), \\(\\acute{\\varepsilon}\\), \n\\(\\acute{o}\\), \\(W\\), \\(=\\), \\(\\land\\), \\(\\lor\\), \\(E\\) \nand \\(*\\). The first five constants are combinators, though the notation may\nsuggest a different (informal) meaning. ‘\\(=\\)’ is the syntactical\nidentity of expressions. ‘\\(\\land\\)’ and ‘\\(\\lor\\)’\nare intended to stand for “and” and “or.”\n‘\\(E\\)’ is the analogue of Schönfinkel’s \n\\(U\\), but it corresponds to a non-vacuous existential\nquantifier. Finally, ‘\\(*\\)’ is similar to the transitive closure\noperator for binary relations or the Kleene star. Notably, there is no\nnegation or universal quantifier in the system. The uses of the constants are\ncharacterized as follows—somewhat like axioms characterize\ncombinators. \nIn CL, the axioms are followed up with notions such as one-step and\nweak reduction, the latter of which can be viewed as a computation or\ninference step. (See the next section for some of these notions.)\nSimilarly, an axiomatic calculus for FOL, for instance, would contain\nrules of inference in addition to the axioms. One of the obstacles to\npenetrate the various presentations of basic logic is the lack of a\nsimilar formulation. \nDuring the next two decades or so after his first paper on basic\nlogic, Fitch published a series of papers on basic logic devoted to\n(1) the representation of recursive functions (i.e., a\ndemonstration of the possibility of the arithmetization of syntax),\n(2) \\(K^\\prime\\), an extension of \\(K\\) with negation,\nuniversal quantifier and # (the dual of the \\(*\\) operator), (3)\nthe consistency of \\(K\\) and \\(K^\\prime\\),\n(4) \\(L\\), an extension of \\(K^\\prime\\)\nwith implication and necessity operators, (5)\nthe definability of some of the constants such as \\(*\\) and #, as well\nas \\(E\\). \nThe combinators that are included in \\(K\\) (hence, in all its\nextensions) are \\(\\textsf{T}\\), \\(\\textsf{B}\\) and \\(\\textsf{W}\\). \\(\\acute\n\\varepsilon\\) and \\(\\acute o\\) are the ternary version of \\(\\textsf{T}\\)\nand the quaternary version of \\(\\textsf{B}\\), respectively. Russell’s paradox\ninvolves negation, but (either variant of) Curry’s paradox is positive, in the\nsense that it relies on one or two theorems of the positive implicational\nlogic of David Hilbert. This means that if the various systems of basic logic,\nespecially \\(K^\\prime\\) and \\(L\\) are consistent, then they\neither cannot contain full abstraction, or the notions of implication,\nentailment and identity should differ from their usual counterparts. Indeed,\n\\(K\\), \\(K^\\prime\\) and \\(L\\) are not\nextensional systems. That is, even if two expressions applied to the same\nexpression are always equal, the equality of the applied expressions\ndoes not follow. Turning basic logic into an extensional system proved\nless than straightforward. Fitch’s system \\(JE^\\prime\\) was shown\nto be inconsistent by Myhill, which led to a more complicated\nformulation of the conditions for extensional identity. \nBasic logic has not (yet) become a widely used general framework for\nthe description of formal systems; however, renewed interest in this\napproach is signaled by Updike (2010), which attempts to situate basic\nlogic in the broader context of foundational work at the middle of the\n20th century. \nQuine’s elimination strategy\n\nFrom the late 1930s, W. V. O. Quine worked on an\nalternative way to eliminate bound variables from first-order logic.\nIt is plausible to assume that Schönfinkel’s goal was to find a\nsingle operator in classical logic and then to eliminate the bound\nvariables—as he claims in Schönfinkel (1924)—rather\nthan defining an overarching symbolic system to describe all\nmathematics. Nonetheless, CL was soon fused with classical logic in a\nmore free-wheeling fashion, which resulted in an inconsistent\nsystem. \nQuine saw the way out of a situation where inconsistency may arise via\nimplicit typing of constants that are to some extent similar to\ncombinators. He called such constants predicate functors, and\nintroduced several groups of them, the last one in Quine (1981). \nThe most common presentations of FOL stipulate that an \\(n\\)-place\npredicate followed by a sequence of \\(n\\) terms (possibly,\npunctuated by commas and surrounded by parentheses) is a formula.\n(This is in contrast with Schönfinkel’s view of formulas and in\naccordance with the informal and formal interpretations of predicates\nas \\(n\\)-ary relations. In other words, FOL does not permit\n“currying” of predicates or of their interpretations.)\nQuine subscribes to the view that sequences of terms follow\npredicates. \nPredicate functors are not applicable to each other—unlike the\ncombinators are. This is a point that Quine repeatedly emphasizes.\nAtomic predicates are the predicates of a first-order language,\nwhereas complex predicates are obtained by applying a predicate\nfunctor (of appropriate arity) to predicates (which may be atomic or\ncomplex). \nThe prohibition of self-application together with the use of\n“flat” sequences of arguments means that infinitely\nmany predicate functors are needed to ensure the elimination of\nbound variables from all formulas of FOL. To explain the problem\nquickly: a permutation of a pair of elements that are arbitrarily far\napart cannot be ensured otherwise. Just as combinators may be divided\ninto groups based on their effect, Quine was able to select predicate\nfunctors that can be grouped together naturally based on their\neffects. Indeed, the groups of predicate functors are similar to\nclasses of combinators, though Quine’s labels are often sublime. In\norder to give a concrete example of this alternative approach, we\noutline a slightly modified version of a set of predicate functors\nfrom Quine (1981). \nA first-order language with \\(\\mid^-\\) as the only operator is\nassumed. (\\(F\\) and \\(G\\) are metavariables for predicates\nin the predicate functor language.) \\(\\wr^n\\)\n\\(\\textit{Inv}^n\\), \\(\\textit{inv}^n\\), \\(\\textit{Pad}^{n+1}\\) \nand \\(\\textit{Ref}^n\\) are predicate functors, for every \n\\(n\\in\\omega\\). A formula of FOL is rewritten into a formula in\na predicate functor language by applications of the following\nclauses. \nThere is an obvious similarity between \\(\\textit{Ref}\\) and \n\\(\\textsf{W}\\), \\(\\textit{Pad}\\) and \\(\\textsf{K}\\), as\nwell as \\(\\textit{Inv}\\) and \\(\\textit{inv}\\) and various\ncombinators with permutative effects (e.g., \\(\\textsf{C}\\) and\n\\(\\textsf{T}\\)). If \\(\\mid^-\\) is the only operator in the first-order\nlanguage, then all formulas, which are not atomic, are almost of the form of\nthe left-hand side expression in 2. What has to be assured is that the side\ncondition is satisfied, and that is where clauses 3–6 enter. Although\nthe various \\(n\\)-ary versions of \\(\\wr\\), \\(\\textit{inv}\\), \\(\\textit{Pad}\\)\nand \\(\\textit{Ref}\\) could be conflated (by ignoring unaffected arguments),\n\\(\\textit{Inv}\\) clearly stands for infinitely many predicate functors,\nbecause \\(x_1,\\ldots,x_n\\) cannot be ignored or omitted. \nIt may be interesting to note that there is a difference between \\(\\wr\\)\nand Schönfinkel’s \\(U\\). Not only the place of the bound\nvariable is different, but \\(\\wr\\) builds in contraction for \\(n-1\\) \nvariables (which are separated by \\(\\mid^-\\) and other symbols\nin the left-hand expression). \nQuine intended the predicate functor language to lead to a novel\nalgebraization of first-order logic. While bound variables can be\neliminated using predicate functors, Quine never defined an algebra in\nthe usual sense—something similar, for instance, to cylindric\nalgebras. Predicate functors, by design, have a very limited\napplicability, which has the unfortunate side effect that they seem to\nbe of little interest and not much of use outside their intended\ncontext. \nThe paradoxes that were discovered by Georg Cantor and Bertrand\nRussell in the late 19th–early 20th century both involve\nself-membership of a set. The ramified theory of types due to Alfred\nN. Whitehead and Bertrand Russell, and\n ZF\n(the formalization of set theory named after Ernst Zermelo and\nAbraham A. Fraenkel) exclude self-membership. However, there\nseems to have been always a desire to create a theory that allows\nself-membership or self-application. Indeed, one of Curry’s\nmotivations for the development of CL was the goal to construct a\nformal language that includes a wide range of well-formed expressions,\nsome of which—under certain interpretations—may turn out\nto be meaningless. (This idea may be compared to the\n von Neumann–Bernays–Gödel\nformalization of set theory, in which—without the axiom of\nfoundation—the Russell class can be proved not to be a set,\nhence, to be a proper class.) \nA few natural language examples provide a convenient illustration to\nclarify the difference between (1), that is a well-formed (but\nmeaningless) expression and (2), which is a meaningful (but\nill-formed) sentence. (The meaningfulness of (2), of course, should be\ntaken with a grain of salt. In reality,\n Kurt Gödel\nproved the system of PM to be incomplete in 1930. Thus (2) may be\nguessed—using syntactic and semantics clues—to be a\ndistorted version of (2′) Peano arithmetic was proved to be\nincomplete by Gödel in 1930.) \nAfter these informal motivations, we turn to CL proper and introduce\nsome of its notions a bit more formally. \nThe objects in CL are called\nterms.[9]\nTerms may be thought to be interpreted as functions (as further\nexplained in section 4.1). Primitive terms comprise\nvariables and constants, whereas compound terms\nare formed by combining terms. Usually, a denumerable set (i.e., a set\nwith cardinality \\(\\aleph_0\\)) of variables is included, and\nthe constants include some (undefined) combinators. (We use\n\\(x,y,z,v,w,u,x_0,\\ldots\\) as variables in the\nobject language, and \\(M,N,P,Q,\\ldots\\) \nas metavariables that range over terms.) \nTerms are inductively defined as follows. \nIn the above definition, (t3) conceals the binary operation that\nconjoins the two terms \\(M\\) and \\(N\\). This operation is\ncalled application, and it is often denoted by juxtaposition, that is,\nby placing its two arguments next to each other as in (\\(MN\\)). \nApplication is not assumed to possess additional properties (such as\ncommutativity), because its intended interpretation is function\napplication. For instance, \\(((vw)u)\\) and\n\\((v(wu))\\) are distinct terms—just as the derivative \nof \\(\\lambda x.\\,x^2+4x-6\\) applied to 8 (that is,\n(\\(\\lambda x.\\,2x+4)8=20\\)) is different from the derivative of 90 (that is,\n\\((8^2+32-6)'=0\\)). Using \\(\\lambda\\) notation, the two terms in the example\nmay be expressed as vs \nIf terms are viewed as structured strings (where parentheses show grouping),\nthen the number of distinct terms associated to a string of length \n\\(n\\) is the Catalan number \\(C_{n-1}\\).  For a\nnon-negative integer \\(n\\) (i.e., for \\(n\\in\\mathbb{N}\\)), \nThe first seven Catalan numbers are \\(C_0=1\\), \\(C_1=1\\), \\(C_2=2\\), \n\\(C_3=5\\), \\(C_4=14\\), \\(C_5=42\\) and \\(C_6=132\\).  As an\nillustration, we may take—for simplicity—strings consisting\nof \\(x\\)s, because the terms are to differ only in their grouping.\nClearly, if the term is \\(x\\) or \\(xx\\), that is of length 1\nor 2, then there is only one way to form a term, that is, there exists\njust one possible term in each case. If we start with three \\(x\\)s,\nthen we may form \\((xx)x\\) or \\(x(xx)\\). If the length of the term is 4, then\nthe five terms are: \\(xxxx\\), \\(x(xx)x\\), \\(xx(xx)\\), \\(x(xxx)\\) and \\(x(x(xx\n))\\). (It is a useful exercise to try to list the 14 distinct terms that\ncan be formed from 5 \\(x\\)s.) \nThe usual notational convention in CL is to drop parentheses\nfrom left-associated terms together with the outmost pair. For\ninstance, \\(xyz\\) would be fully written as \\(((xy)z)\\), whereas \\(xy(xz)\\)\nand \\((xy)(xz)\\) are both “shorthand versions” of\nthe term \\(((xy)(xz))\\) (unlike \\(xyxz\\)). Grouping\nin terms delineates subterms. For instance, \\(xy\\) is a subterm of\neach of the terms mentioned in this paragraph, whereas \\(yz\\)\nand \\(yx\\) are subterms of none of those terms. \nSubterms of a term are recursively defined as follows. \nIncidentally, the notion of free variables is straightforwardly\ndefinable now: \\(x\\) is a free variable of \\(M\\) iff\n\\(x\\) is a subterm of \\(M\\). The set of free variables\nof \\(M\\) is sometimes denoted by \\(\\textrm{fv}(M)\\). \nAll terms are interpreted as functions, and combinators are functions\ntoo. Similarly, to some numerical and geometrical functions, that can\nbe described and grasped easily, the combinators that are frequently\nencountered can be characterized as perspicuous transformations on\nterms. (Sans serif letters denote combinators and > denotes\none-step reduction.) \nThese axioms tacitly specify the arity of a combinator as well\nas their reduction (or contraction) pattern. Perhaps,\nthe simplest combinaetor is the identity combinator \\(\\textsf{I}\\),\nthat applied to an argument \\(x\\) returns\nthe same \\(x\\). \\(\\textsf{K}\\) applied to \\(x\\) is\na constant function, because when it is further applied to \\(y\\),\nit yields \\(x\\) as a result, that is, \\(\\textsf{K}\\)\nis a cancellator with respect to its second argument. \\(\\textsf{W}\\)\nand \\(\\textsf{M}\\) are duplicators, because in the result of their\napplication one of the arguments (always) appears\ntwice.[10]\n\\(\\textsf{C}\\), \\(\\textsf{T}\\) and \\(\\textsf{V}\\) are permutators,\nbecause they change the order of some of their arguments. \\(\\textsf{B}\\) is an\nassociator, because \\(\\textsf{B}xyz\\) turns\ninto a term in which \\(y\\) is applied to \\(z\\)\nbefore \\(x\\) is applied to the result. \\(\\textsf{Y}\\) is the fixed\npoint combinator, because for any function \\(x\\),\n\\(\\textsf{Y}x\\)  is the fixed point of that function (see\nsection 2.3). The combinator \\(\\textsf{B}^\\prime\\) is an\nassociator and a permutator, whereas \\(\\textsf{S}\\) and\n\\(\\textsf{J}\\) are also duplicators. \\(\\textsf{S}\\)\nis very special and it is called the strong composition combinator,\nbecause when applied to two functions, let us say, \\(g\\) and \n\\(f\\) (in that order), as well as \\(x\\), then the resulting \nterm \\(gx(fx)\\) expresses the composition of \\(g\\)\nand \\(f\\) both applied to the same argument \\(x\\). \nThese informal explications did not mention any restrictions on the\nsort of functions \\(x,y,z,f,g,\\ldots\\) may be. However, the axioms above limit\nthe applicability of the combinators to variables. Intuitively, we would like\nto say that given any terms, that is, any functions \\(M\\) and \\(N\\),\n\\(\\textsf{W}MN\\) one-step reduces to \\(MNN\\) (possibly, as a subterm of\nanother term). For example, \\(M\\) may be \\(\\textsf{K}\\) and \\(N\\) may be\n\\(yy\\), and then \\(\\textsf{WK}(yy)\\triangleright_1\\textsf{K}(yy)\n(yy)\\).  The latter term suggests a further one-step reduction, and\nindeed we might be interested in successive one-step reductions—such as\nthose leading from \\(\\textsf{WK}(yy)\\) to \\(yy\\). A way to\nachieve these goals is to formalize (a theory of) CL starting with the\nstandard inequational logic but to omit the anti-symmetry rule and to\nadd certain other axioms and rules. \nThe use of metavariables encompasses substitution (that we illustrated\nabove on the term \\(\\textsf{W}MN)\\).  The identity axiom and the rule of\ntransitivity imply that \\(\\triangleright\\) is a transitive and reflexive\nrelation. The last two rules characterize application as an operation that\nis monotone in both of its argument places. \\(\\text{CL}_\\triangleright\\)\nincludes only \\(\\textsf{S}\\) and \\(\\textsf{K}\\), because the other combinators\nare definable from them—as we already mentioned in section 1.2, and\nas we explain more precisely toward the end of this section. \nThe set of combinators \\(\\{\\textsf{S},\\textsf{K}\\}\\) is called\na combinatory base, that is, these two combinators are the undefined\nconstants of \\(\\text{CL}_\\triangleright\\). To give an idea of a proof in this\ncalculus, the following steps may be pieced together to prove \\(\\textsf{SKK}\n(\\textsf{KSK})\\triangleright \\textsf{S}\\).  \\(\\textsf{KSK}\\triangleright\n\\textsf{S}\\) is an instance of an axiom. Then \\(\\textsf{SKK}(\\textsf{KSK})\n\\triangleright\\textsf{SKKS}\\) is obtained by right monotonicity, and further,\n\\(\\textsf{SKK}(\\textsf{KSK})\\triangleright\\textsf{S}\\) results by instances of\nthe \\(\\textsf{S}\\) and \\(\\textsf{K}\\) axioms together with applications of the\ntransitivity rule. \nThe relation \\(\\triangleright\\) is called weak reduction, and it may be \ndefined alternatively as follows. (‘Weak reduction’ is a technical\nterm used in CL to distinguish this relation on the set of terms from some\nother relations, one of which is called ‘strong reduction’.) A\nterm that is either of the form \\(\\textsf{S}MNP\\) or of the form \n\\(\\textsf{K}MN\\) is a redex, and the leading combinators\n(\\(\\textsf{S}\\) and \\(\\textsf{K}\\), respectively) are the heads of the\nredexes. If a term \\(Q\\) contains a subterm of the form \\(\\textsf{S}MNP\\),\nthen \\(Q^\\prime\\); which is obtained by replacing that subterm by \\(MP(NP)\\)\nis a one-step reduct of \\(Q\\). (Similarly, for the redex \\(\\textsf{K}\nMN\\) and \\(M\\).) That is, \\(Q\\triangleright Q^\\prime\\) in both\ncases.  Reduction then may be defined as the reflexive transitive\nclosure of one-step reduction. This notion is completely captured by\n\\(\\text{CL}_\\triangleright\\). The calculus \\(\\text{CL}_\\triangleright\\) is complete\nin the sense that if \\(M\\triangleright N\\) in the sense we have just described,\nthen \\(\\text{CL}_\\triangleright\\) proves \\(M\\triangleright N\\).  (It is easy to see\nthat the converse implication is true too.) \nThe notion of reduction is a weaker relation than one-step reduction, and so\nit is useful to distinguish a subclass of terms using the stronger relation. A\nterm is in normal form (nf) when it contains no redexes. Note that\none-step reduction does not need to decrease the total number of redexes that\na term contains, hence, it does not follow that every term can be turned into\na term in nf via finitely many one-step reductions. Indeed, some terms do not\nreduce to a term in nf. \nReduction is arguably an important relation between terms that denote\nfunctions. The typical steps in a program execution and in other concrete\ncalculations are function applications rather than moves in the other\ndirection, what is called expansion. However, the notion of the\nequality of functions is familiar to everybody from mathematics, and the\nanalogous notion has been introduced in CL too. The transitive, reflexive,\nsymmetric closure of the one-step reduction relation is called\n(weak) equality. A formalization of equational CL may be obtained by\nextending the standard equational logic with combinatory axioms and rules\ncharacterizing the combinatory constants and the application operation. \nThe first axiom and the first two rules constitute equational logic. The\nconstants are again the combinators \\(\\textsf{S}\\) and \\(\\textsf{K}\\). Note\nthat \\(\\text{CL}_=\\) could have been defined as an extension of \\(\\text{CL}_\\triangleright\\)\nby adding the rule of symmetry, that would have paralleled the description of\nthe definition of equality from reduction as its transitive, symmetric\nclosure. We chose instead to repeat the inequational axioms and rules with the\nnew notation (and add the rule of symmetry) to make the two definitions\nself-contained and easy to grasp. The two characterizations of \\(=\\)\ncoincide—as those of \\(\\triangleright\\) did. \n\\(\\text{CL}_\\triangleright\\) and \\(\\text{CL}_=\\) share a feature that may or may not be\ndesirable—depending on what sort of understanding of functions is to be\ncaptured. To illustrate the issue, let us consider the one-place combinators\n\\(\\textsf{SKK}\\) and \\(\\textsf{SK}(\\textsf{KK})\\). It is easy to verify that\n\\(\\textsf{SKK}M\\triangleright M\\) and \\(\\textsf{SK}(\\textsf{KK})M\n\\triangleright M\\). However, neither \\(\\textsf{SKK}\\triangleright\n\\textsf{SK}(\\textsf{KK})\\) nor \\(\\textsf{SK}(\\textsf{KK})\\triangleright\n\\textsf{SKK}\\) is provable in \\(\\text{CL}_\\triangleright\\); a fortiori, the equality\nof the two terms in not provable in \\(\\text{CL}_=\\). This means that \n\\(\\text{CL}_\\triangleright\\) and \\(\\text{CL}_=\\) formalize intensional notions of\nfunctions, where “intensionality” implies that functions that give\nthe same output on the same input may remain distinguishable. \nThe archetypical intensional functions that one is likely to encounter\nare algorithms. As examples, we might think of various specifications\nto calculate the decimal expansion of \\(\\pi\\), or various computer programs\nthat behave in the same way. For instance, compilers (for one and the same\nlanguage) may differ from each other by using or not using some optimizations,\nand thereby, producing programs from a given piece of code that have identical\ninput–output behavior but different run times. \nIf functions that are indistinguishable from the point of view of their\ninput–output behavior are to be identified, that is,\nan extensional understanding of functions is sought, then\n\\(\\text{CL}_\\triangleright\\) and \\(\\text{CL}_=\\) have to be extended by the following rule,\n(where the symbol \\(\\ddagger\\) is to be replaced by \\(\\triangleright\\) or\n\\(=\\), respectively). \nThe calculi \\(\\text{CL}_\\triangleright\\) and \\(\\text{CL}_=\\) of the previous section\nformalize reduction and equality. However, \\(\\triangleright\\) and \\(=\\) \nhave some further properties that are important when the terms are thought to\nstand for functions.  The next theorem is one of the earliest and best-known\nresults about CL. \nChurch–Rosser theorem (I). If \\(M\\) reduces to \\(N\\) and \\(P\\),\nthen there is a term \\(Q\\) to which both \\(N\\) and \\(P\\) reduce.\n Figure 1. Illustration for the Church–Rosser theorem (I) \nIf we think that reduction is like computing the value of a function, then the\nChurch–Rosser theorem—in a first approximation—can be\nthought to state that the final result of a series of calculations with a term\nis unique—independently of the order of the steps. This is a slight\noverstatement though, because uniqueness implies that each series of\ncalculations ends (or “loops” on a term). That is, if there is a\nunique final term, then only finitely many distinct consecutive calculation\nsteps are possible. \nA coarse analogy with elementary arithmetic operations, perhaps, can shed some\nlight on the situation. The addition and multiplication of natural numbers\nalways yield a natural number. However, if division is included then it is no\nlonger true that all numerical expressions evaluate to a natural number, since\n\\(7/5\\) is a rational number that is not a natural one, and \\(n/0\\) is\nundefined (even if \\(n\\) were real). That is, some numerical expressions do\nnot evaluate to a (natural) number. Although the analogy with combinatory\nterms is not very tight, it is useful. For instance, \\(n/0\\) (assuming that\nthe codomain of the function \\(\\lambda n.\\,n/0\\) is extended to permit \\(r\\)\nto be rational) could be implemented on a computer by a loop (that would never\nterminate when executed if \\(n\\ne0\\)) which would go through an enumeration of\nthe rational numbers trying to find an \\(r\\) such that \\(r\\cdot0=n\\). \nThe combinatory terms \\(\\textsf{WWW}\\) and \\(\\textsf{WI}(\\textsf{WI})\\) are,\nperhaps, the simplest examples of terms that do not have a normal form. Both\nterms induce an infinite reduction sequence, that is, an infinite chain\nof successive one-step reductions. To make the example more transparent, let\nus assume for a moment that \\(\\textsf{W}\\), \\(\\textsf{I}\\), \\(\\textsf{C}\\),\netc. are not defined from \\(\\textsf{K}\\) and \\(\\textsf{S}\\), but are primitive\nconstants. The contraction of the only redex in \\(\\textsf{WWW}\\) returns the\nsame term, which shows that uniqueness does not imply that the term is in\nnf. The contraction of the only redex in \\(\\textsf{WI}(\\textsf{WI})\\) gives\n\\(\\textsf{I}(\\textsf{WI})(\\textsf{WI})\\) that further reduces to the term we\nstarted with. A slightly more complicated example of a term that has only\ninfinite reduction sequences is \\(\\textsf{Y}(\\textsf{CKI})\\).  This term has a \nreduction sequence (in which each contracted redex is headed by \n\\(\\textsf{Y}\\)) that contains infinitely many distinct terms. It is also\npossible to create infinite reduction sequences that start with \\(\\textsf{Y}\n(\\textsf{CKI})\\) and have various loops too. To sum up, the\nChurch–Rosser theorem, in general, does not guarantee the uniqueness of\nthe term \\(Q\\). However, if \\(M\\) has a normal form then that is\nunique. \nThe Church–Rosser theorem is often stated as follows. \nChurch–Rosser theorem (II). If \\(N\\) and \\(P\\) are equal, then\nthere is a term \\(Q\\) to which both \\(N\\) and \\(P\\) reduces.\n Figure 2. Illustration for the Church–Rosser theorem (II) \nThe second form of the Church–Rosser theorem differs from the first in\nits assumption. From the definition of equality as a superset of\nreduction, it is obvious that the first form of the theorem is implied by the\nsecond. However, despite the weaker assumption in the second formulation of\nthe Church–Rosser theorem, the two theorems are equivalent. \nEquality is the transitive, symmetric closure of reduction, which means that\nif two terms are equal then there is a finite path comprising reduction and\nexpansion steps (which decompose into one-step reductions and one-step\nexpansions, respectively). Then by finitely many applications of the first\nChurch–Rosser theorem (i.e., by induction on the length of the path\nconnecting \\(N\\) and \\(P\\)), the first Church–Rosser theorem implies the\nsecond formulation. \nModern proofs of the Church–Rosser theorem for CL proceed\nindirectly because one-step reduction fails to have the diamond property. A\nbinary relation \\(R\\) (e.g., reduction) is said to have the diamond\nproperty when \\(xRy\\) and \\(xRz\\) imply that \\(yRv\\) and \\(zRv\\) for some\n\\(v\\). If a binary relation \\(R\\) has the diamond property, so does its \ntransitive closure. To exploit this insight in the proof of the \nChurch–Rosser theorem, a suitable subrelation of reduction has to\nbe found. The sought after subrelation should possess the diamond property,\nand its reflexive transitive closure should coincide with reduction. \nThe following counterexample illustrates that one-step reductions of a term\nmay yield terms that further do not reduce to a common term in one\nstep. \\(\\textsf{SKK}(\\textsf{KKS})\\triangleright_1\\textsf{SKKK}\\)\nand \\(\\textsf{SKK}(\\textsf{KKS})\\triangleright_1\\textsf{K}(\\textsf{KKS})\n(\\textsf{K}(\\textsf{KKS}))\\), and then the potential reduction sequences are\nas follows. \nThe failure of the diamond property is obvious once we note that\n\\(\\textsf{SKKK}\\triangleright_1\\textsf{KK}(\\textsf{KK})\\)\n(only), but \\(\\textsf{K}(\\textsf{KKS})(\\textsf{K}(\\textsf{KKS}))\\) does not\nreduce in one step to \\(\\textsf{KK}(\\textsf{KK})\\). \nAn appropriate subrelation of reduction is the simultaneous reduction of a\nset of nonoverlapping redexes, which is denoted by \\(\\triangleright\n_\\textrm{sr}\\). ‘Nonoverlapping’ means that there are no shared\nsubterm occurrences between two redexes. \\(\\triangleright_\\textrm{sr}\\) \nincludes \\(\\triangleright_1\\) because a one-step reduction of a redex may be \nviewed instead as \\(\\triangleright_\\textrm{sr}\\) of a singleton set of\nredexes. \\(\\triangleright_\\textrm{sr}\\) is, obviously, included in \n\\(\\triangleright\\) (i.e., in reduction). These two facts imply that the \nreflexive transitive closure of \\(\\triangleright_\\textrm{sr}\\) is \nreduction—when the tonicity of the reflexive transitive closure\noperation (denoted by \\(^*\\)) is taken into account. \n(1)–(3) summarize the key inclusions between the relations\nmentioned. \nThe central property of \\(\\triangleright_\\textrm{sr}\\) that we need is the\ncontent of the following theorem. \nTheorem. (Diamond property for \\(\\triangleright_\\textrm{sr}\\)) If \n\\(M\\triangleright_\\textrm{sr}N\\) and \\(M\\triangleright_\\textrm{sr}P\\) then\nthere is a term \\(Q\\) such that both \\(N\\triangleright_\\textrm{sr}Q\\) and \n\\(P\\triangleright_\\textrm{sr}Q\\).\n \nThe proof of this theorem is an easy induction on the term \\(M\\). The\nproperties of \\(\\triangleright_\\textrm{sr}\\) guarantee that one or more\none-step reductions can be performed at once, but the reductions cannot\ninterfere (or overlap) with each other. \nThe consistency of CL follows from the Church–Rosser\ntheorem together with the existence of (at least two) distinct normal\nforms. \nTheorem. (Consistency) CL is consistent, that is, there are terms that\ndo not reduce to each other, hence they are not equal. \n \nNot all terms have an nf, however, many do. Examples, first of all, include\n\\(\\textsf{S}\\) and \\(\\textsf{K}\\). (The variables, if included, of which there\nare \\(\\aleph_0\\) many, are all in nf.) None of these terms contains a redex,\nhence each reduces only to itself. By the Church–Rosser theorem, it is\nexcluded that some term \\(M\\) could reduce to both \\(x\\) and \\(\\textsf{S}\\)\n(making \\(\\textsf{S}\\) equal to \\(x\\)). \nThe interaction between infinite reduction sequences and nfs deserves a more\ncareful inspection though. The terms \\(\\textsf{WWW}\\), \\(\\textsf{Y}\n(\\textsf{CKI})\\) and \\(\\textsf{WI}(\\textsf{WI})\\) have only infinite\nreduction sequences. However, the existence of an infinite reduction sequence\nfor a term does not imply that the term has no normal form (when the\ncombinatory base is complete or contains a cancellator). \\(\\textsf{Y}\n(\\textsf{KI})\\) reduces to \\(\\textsf{KI}(\\textsf{Y}(\\textsf{KI}))\\),\n\\(\\textsf{KI}(\\textsf{KI}(\\textsf{Y}(\\textsf{KI})))\\), \\(\\textsf{KI}\n(\\textsf{KI}(\\textsf{KI}(\\textsf{Y}(\\textsf{KI})))),\\ldots\\) as well as to\n\\(\\textsf{I}\\). \nA term weakly normalizes when it has an nf, whereas a term strongly\nnormalizes when all its reduction sequences lead to an nf (hence,\nto the nf) of the term. A computational analogue of a strongly\nnormalizing term is a (nondeterministic) program that terminates on every\nbranch of computation, whereas termination on at least one branch is akin to\nweak normalization. \nThe importance of normalization led to a whole range of questions (and an\nextensive literature of answers). How does the order of the reduction steps\n(i.e., a reduction strategy) affect finding the nf (if there is one)? Are\nthere combinatory bases that guarantee the existence of normal forms for every\ncombinator over that base? To quickly illustrate possible answers to our\nsample questions, we start with noting that if there is no combinator with a\nduplicative effect in a base, then all combinators over that base strongly\nnormalize. This is a very easy answer, and as a concrete base, we could have,\nfor example, \\(\\{\\textsf{B},\\textsf{C},\\textsf{K}\\}\\) or \\(\\{\\textsf{B},\n\\textsf{C},\\textsf{I}\\}\\), which have some independent interest in view of\ntheir connection to simply typed calculi. However, these bases are far from\nbeing combinatorially complete and even a fixed point combinator is\nundefinable in them. \nWe could ask a slightly different question: If we start with the base\n\\(\\{\\textsf{S},\\textsf{K}\\}\\) and we omit \\(\\textsf{S}\\), then we get the base\n\\(\\{\\textsf{K}\\}\\) and all the combinators strongly normalize, but what if we\nomit \\(\\textsf{K}\\)? Do the combinators over \\(\\{\\textsf{S}\\}\\) strongly\nnormalize or at least normalize? The answer is “no.” A term\n(discovered by Henk Barendregt in the early 1970s) that shows the lack of\nstrong normalization is \\(\\textsf{SSS}(\\textsf{SSS})(\\textsf{SSS})\\). The\nfirst \\(\\textsf{S}\\) is the head of a (indeed, the only) redex, and the\nhead reduction sequence of this term is infinite. Since \\(\\{\\textsf{S}\\}\\)\ndoes not contain any combinator with a cancellative effect, the existence of\nan infinite reduction sequence for a term means that the term has no nf. There\nare shorter combinators over the base \\(\\{\\textsf{S}\\}\\) without an nf, for\nexample, \\(\\textsf{S}(\\textsf{SS})\\textsf{SSSSS}\\) comprises only eight\noccurrences of \\(\\textsf{S}\\). \nThe sorts of questions we illustrated here (or rather, the answers to them)\ncan become a bit technical, because they often involve concepts and techniques\nfrom graph theory, automata theory and the theory of term-rewriting. \nSchönfinkel proved that \\(\\textsf{S}\\) and \\(\\textsf{K}\\) suffice to\ndefine the other combinators he introduced, and we mentioned in the definition\nof \\(\\text{CL}_\\triangleright\\) that the set of constants is limited to\n\\(\\textsf{S}\\) and \\(\\textsf{K}\\), because other combinators could be defined\nfrom those. \nTo demonstrate the sense in which definability is understood here we consider\nthe example of \\(\\textsf{B}\\). The axiom for \\(\\textsf{B}\\) is \\(\\textsf{B}\nxyz\\triangleright_1x(yz)\\), and if we take \\(\\textsf{S}(\\textsf{KS})\n\\textsf{K}\\) instead of \\(\\textsf{B}\\), then the following reduction sequence\nresults. \nThe term \\(\\textsf{S}(\\textsf{KS})\\textsf{K}\\) is in nf, however, to be in nf\nis not a requirement for definability. It is more convenient to work with\ndefining terms that are in nf, because an application of a combinator that is\nnot in nf could be started with reducing the combinator to its normal\nform. (Also, there are always infinitely many combinators that reduce to a\ncombinator.) However, note that the preference for choosing combinators in nf\nis not meant to imply that a combinator cannot be defined by two or more terms\nin nf; below we give two definitions (involving only \\(\\textsf{S}\\) and\n\\(\\textsf{K}\\)) for \\(\\textsf{I}\\). \nIf the constants are \\(\\textsf{S}\\) and \\(\\textsf{K}\\), then\nthe combinators are all those terms that are formed from \\(\\textsf{S}\\)\nand \\(\\textsf{K}\\) (without variables). Once we have defined \\(\\textsf{B}\\)\nas \\(\\textsf{S}(\\textsf{KS})\\textsf{K}\\), we may use \\(\\textsf{B}\\) in further\ndefinitions as an abbreviation, and we do that primarily to reduce the size of\nthe resulting terms as well as to preserve the transparency of the\ndefinitions. \nThe following list gives definitions for the other well-known combinators that\nwe mentioned earlier. (Here ‘\\(=\\)’ is placed between a definiendum\nand a definiens.) \nThe definitions are easily seen to imply that all these combinators depend on\nboth \\(\\textsf{S}\\) and \\(\\textsf{K}\\), but it is not obvious from the\ndefinitions that the defined combinators are mutually independent, that is,\nthat none of the listed combinators is definable from another one. (Clearly,\nsome subsets suffice to define some of the combinators.) We do not intend to\ngive an exhaustive list of interdefinability between various subsets of these\ncombinators, but to hint at the multiplicity and intricacy of such\ndefinitions, we list a handful of them. We also introduce two further\ncombinators \\(\\textsf{S}^\\prime\\) and \\(\\textsf{R}\\). \nIf the fixed point combinator \\(\\textsf{Y}\\) is not taken to be a primitive,\nthen there are various ways to define it—so far, we have listed\nthree. \nFixed point theorem. For any function \\(M\\), there is a term \\(N\\) such\nthat \\(MN = N\\). \n \nThe proof of this theorem is easy using a fixed point combinator, because a\nterm that can play the rôle of \\(N\\) is \\(\\textsf{Y}M\\). \nSome of the definitions of \\(\\textsf{Y}\\) have slightly different properties\nwith respect to reduction. But the importance of the fixed point combinator is\nthat it ensures that all functions have a fixed point and all recursive\nequations can be solved. \nBoth Haskell B. Curry and Alan Turing defined fixed point combinators (in\nCL or in the \\(\\lambda\\)-calculus). If we consider the definitions \n(where the subscripts are added to distinguish the two definitions),\nthen we can see that \\(\\textsf{Y}_1 M=M(\\textsf{Y}_1M)\\), but for\n\\(\\textsf{Y}_2\\), \\(\\textsf{Y}_2M\\triangleright M(\\textsf{Y}_2M)\\) holds too.\nIn this respect, \\(\\textsf{Y}_1\\) is similar to Curry’s fixed point combinator\n(and really, to any fixed point combinator), whereas \\(\\textsf{Y}_2\\) is like\nTuring’s fixed point combinator. \nThe fixed point theorem demonstrates—to some extent—the\nexpressive power of CL. However, fixed point combinators may be defined from\nbases without a cancellator (as \\(\\textsf{Y}_1\\) and \\(\\textsf{Y}_2\\)\nshow). The full power of CL (with the base \\(\\{\\textsf{S},\\textsf{K}\\}\\)) is\nenunciated by the following theorem. \nTheorem. (Combinatorial completeness) If  \\(f(x_1,\\ldots,x_n)=\nM\\) (where \\(M\\) is a term containing no other variables than those explicitly\nlisted), then there is a combinator \\(\\textsf{X}\\) such that \\(\\textsf{X}\nx_1\\ldots x_n\\) reduces to \\(M\\). \n \nThe theorem’s assumption may be strengthened to exclude the\npossibility that some occurrences of \\(x\\) do not occur in \\(M\\). Then\nthe consequent may be strengthened by adding the qualification that\n\\(\\textsf{X}\\) is a relevant combinator, more specifically,\n\\(\\textsf{X}\\) is a combinator over \\(\\{\\textsf{B},\\textsf{W},\n\\textsf{C},\\textsf{I}\\}\\) (a base that does not contain a combinator\nwith cancellative effect), or equivalently, \\(\\textsf{X}\\) is a\ncombinator over \\(\\{\\textsf{I},\\textsf{J}\\}\\).  (These bases\ncorrespond to Church’s preferred\n\\(\\lambda\\textsf{I}\\)-calculus.) \nCombinatorial completeness is usually proved via defining a\n“pseudo” \\(\\lambda\\)-abstraction (or bracket\nabstraction) in CL. There are various algorithms to define a bracket\nabstraction operator in CL, that behaves as the \\(\\lambda\\) operator does in a\n\\(\\lambda\\)-calculus. This operator is usually denoted by \\([\\,]\\) or by\n\\(\\lambda^*\\). The algorithms differ from each other in various aspects: (i)\nthe set of combinators they presuppose, (ii) the length of the resulting\nterms, (iii) whether they compose into (syntactic) identity with the algorithm\nthat translates a combinatory term into a \\(\\lambda\\)-term, and (iv) whether\nthey commute with either of the reductions or equalities. \nThe first algorithm, the elements of which may already be found in\nSchönfinkel (1924), consists of the following clauses that are applied in\nthe order of their listing. \nFor example, if this algorithm is applied to the term \\(\\lambda xyz.x(yz)\\)\n(that is, to the \\(\\lambda\\)-translation of \\(\\textsf{B}\\)), then the\nresulting term is \\(\\textsf{B}\\).  However, if \\(\\eta\\) is omitted then a much\nlonger term results, namely, \\(\\textsf{C}(\\textsf{BB}(\\textsf{BBI}))(\\textsf{C}\n(\\textsf{BBI})\\textsf{I})\\).  Another algorithm, for example, consists of\nclauses (i), (k) and (s). \nCombinatory terms are thought of as functions, and functions are thought to\nhave a domain (a set of possible inputs) and a codomain (a set\nof possible outputs). For example, if a unary function is considered as a set\nof ordered pairs, then the domain and codomain are given by the first and\nsecond projections, respectively. If partial and non-onto functions are\npermitted, then supersets of the sets resulting from the first and\nsecond projections can also be domains and codomains. \n Category theory,\nwhere functions are components of categories (without a set theoretic\nreduction assumed), retains the notions of a domain and a codomain; moreover,\nevery function has a unique domain and codomain. \nFunctions that have the same domain and codomain may be quite different,\nhowever, by abstraction, they are of the same sort or type. As a simple\nillustration, let \\(f_1\\) and \\(f_2\\) be two functions defined as \\(f_1=\n\\lambda x.\\,8\\cdot x\\) and \\(f_2=\\lambda x.\\,x/3\\). If \\(x\\) is a variable \nranging over reals, then \\(f_1\\) and \\(f_2\\) have the same domain and codomain\n(i.e., they have the same type \\(\\mathbb{R}\\rightarrow\\mathbb{R}\\)), although\n\\(f_1\\ne f_2\\), because \\(f_1(x)\\ne f_2(x)\\) whenever \\(x\\ne0\\). The usual\nnotation to indicate that a function \\(f\\) has \\(A\\) as its domain and \\(B\\)\nas its codomain is \\(f\\colon A\\rightarrow B\\). It is a happy coincidence that\nnowadays ‘\\(\\rightarrow\\)’ is often used in logics as a symbol for\nentailment or (nonclassical) implication. \nGiven a set of basic types (that we denote by \\(P\\)), types are defined\nas follows. \nTo distinguish these types from other types—some of which are introduced\nin the next section—they are called simple types. \nThe connection between combinators and types may be explained on the example\nof the identity combinator. Compound combinatory terms are formed by the\napplication operation. Premises of modus ponens can be joined by fusion\n(denoted by \\(\\circ\\)), which is like the application operation in the\nstrongest relevance logic \\(B\\). \\(\\textsf{I}x \\triangleright x\\) and so if\n\\(x\\)'s type is \\(A\\), then \\(\\textsf{I}x\\)'s type should imply \\(A\\). \nFurthermore, \\(\\textsf{I}x\\)'s type should be of the form \\(X\\circ A\\), for\nsome type \\(X\\); then \\(\\textsf{I}\\) can be of type \\(A\\rightarrow A\\). In\nthe example, we fixed \\(x\\)'s type, however, \\(\\textsf{I}\\) can be applied to\nany term, hence, it is more accurate to say that \\(A\\rightarrow A\\) is the\ntype schema of \\(\\textsf{I}\\), or that \\(\\textsf{I}\\)'s type can be any\nformula of the form of self-implication. \nThe type-assignment system TA\\(_\\textrm{CL}\\) is formally defined as the\nfollowing deduction system. (When implicational formulas are considered as\ntypes, the usual convention is to omit parentheses by association to the\nright.) \nExpressions of the form \\(M\\colon A\\) above are called type\nassignments. A characteristic feature of type-assignment systems is that\nif \\(M\\colon A\\) is provable then \\(A\\) is considered to be one of the types\nthat can be assigned to \\(M\\). However, a provable assignment does not\npreclude other types from becoming associated to the same term \\(M\\), that is\na type assignment does not fix the type of a term rigidly. \\(\\Delta\\) and\n\\(\\Theta\\) on the left-hand side of \\(\\vdash\\)  are sets of type assignments\nto variables, and they are assumed to be consistent—meaning that no\nvariable may be assigned two or more types. \nType assignment systems are often called Curry-style typing systems. \nAnother way to type terms is by fixing a type for each term, in which case\neach term has exactly one type. Such calculi are called Church-style typing\nsystems. Then, for example, the identity combinator \\(\\textsf{I}\\) of type  is not the same as the identity combinator \\(\\textsf{I}\\) of type  The two styles of typing have quite a lot in\ncommon, but there are certain differences between them. In particular,\nno self-application is typable in a Church-style typing system,\nwhereas some of those terms can be assigned a type in a Curry-style\ntyping system. Curry-style typing systems proved very useful in\nestablishing various properties of CL and \\(\\lambda\\)-calculi. The\nChurch-style typing, on the other hand, emulates more closely the\ntyping in certain functional programming languages (without\nobjects). \nThere is no one-one correspondence between types and combinators in either\nstyle of typing: not all combinators can be assigned a type, and some\nimplicational formulas cannot be assigned to any combinatory term. A\ncombinator that can be assigned a type is said to be typable, and a\ntype that can be assigned to a combinator is said to be inhabited. For\ninstance, \\(\\textsf{M}\\) has no (simple) type, because an implicational\nformula is never identical to its own antecedent. On the other hand, Peirce’s\nlaw, \\(((A\\rightarrow B)\\rightarrow A) \\rightarrow A\\) is not the type of any\ncombinator in the type assignment system TA\\(_\\textrm{CL}\\).  Despite (or,\nindeed, due to) the discrepancy between implicational formulas and combinatory\nterms, classes of implicational formulas that can be assigned to certain sets\nof combinatory terms coincide with sets of theorems of some important\nlogics. \nTheorem. \\(A\\rightarrow B\\) is a theorem of the intuitionistic\nimplicational logic, denoted by \\(IPC_\\rightarrow\\) or\n\\(J_\\rightarrow\\), iff for some \\(M\\), \\(M\\colon A\\rightarrow B\\) is a\nprovable type assignment in TA\\(_\\textrm{CL}\\), where the term \\(M\\)\nis built from \\(\\textsf{S}\\) and \\(\\textsf{K}\\), that is, \\(M\\) is a\ncombinator over the base \\(\\{\\textsf{S},\\textsf{K}\\}\\).\n \nA combinator that inhabits an implicational theorem encodes a proof of\nthat theorem in the deduction system TA\\(_\\textrm{CL}\\). There is an algorithm\nto recover the formulas that constitute a proof of the type of the combinator,\nmoreover, the algorithm produces a proof that is minimal and well-structured. \nThe correspondence between implicational theorems of intuitionistic logic (and\ntheir proofs) and typable closed \\(\\lambda\\)-terms (or combinators) is called\nthe Curry–Howard isomorphism. The usual notion of a proof in a\nHilbert-style axiomatic system is quite lax, but it can be tidied up to obtain\nthe notion of traversing proofs. In a traversing proof there is a\none-one correspondence between subterms of a combinator and the formulas in\nthe traversing proof as well as between applications and detachments therein\n(cf. Bimbó 2007). \nThe above correspondence can be modified for other implicational logics and\ncombinatory bases. The next theorem lists correspondences that obtain between\nthe implicational fragments of the relevance logics \\(R\\) and \\(T\\) and\nsome combinatory bases that are of interest in themselves. \nThe calculus \\(\\textrm{TA}_\\textrm{CL}\\) may be amended by adding\naxiom schemas for the combinators in the two bases. (The axiom schemas\nof the combinators that are not in these bases may be omitted from the\ncalculus or simply may be neglected in proofs.) The new axioms\nare as follows. \nThe combinatory base \\(\\{\\textsf{B},\\textsf{C},\\textsf{W},\\textsf{I}\\}\\) is\nespecially interesting, because these combinators suffice for a definition of\na bracket abstraction that is equivalent to the \\(\\lambda\\)-abstraction of the\n\\(\\lambda\\textsf{I}\\)-calculus.  To put it differently, all functions that\ndepend on all of their arguments can be defined by this base. The other base\nallows the definition of functions that can be described by terms in the class\nof the so-called hereditary right maximal terms (cf. Bimbó\n2005). Informally, the idea behind these terms is that functions can be\nenumerated, and then their successive applications should form a sequence in\nwhich the indexes are “globally increasing.” \nA type assignment has two parts: a term and a formula. The\nquestions whether some term can be assigned a type and whether some type can\nbe assigned to a term are the problems of typability and\nof inhabitation, respectively. Although these questions may be posed\nabout one and the same set of type assignments, the computational properties\nof these problems may differ widely.  \nTheorem. It is decidable if a term \\(M\\) can be assigned a type, that\nis, if \\(M\\) is typable. \n \nThe theorem is stated in a rather general way without specifying exactly which\ncombinatory base or which modification of TA\\(_\\textrm{CL}\\) is assumed,\nbecause the theorem holds for any combinatory base. Indeed, there is an\nalgorithm that given a combinator decides if the combinator is typable, and\nfor a typable combinator produces a type too. Of course, in the\ncombinatorially complete base \\(\\{\\textsf{S},\\textsf{K}\\}\\) all the\ncombinators are expressible as terms consisting of these two combinators\nonly. However, this assumption is not needed for a solution of typability,\nthough it might provide an explanation for the existence of a general\nalgorithm. \nThe problem of inhabitation does not have a similar general solution,\nbecause the problem of the equality of combinatory terms is undecidable. Given\na set of axiom schemas that are types of combinators with detachment as the\nrule of inference, the problem of the decidability of a logic can be\nviewed as the problem of inhabitation. Indeed, if \\(A\\) is an implicational\nformula, then to decide whether \\(A\\) is a theorem amounts to determining if\nthere is a term (over the base that corresponds to the axiom schemas) that\ncan be assigned \\(A\\) as its type. (Of course, a more sophisticated algorithm\nmay actually produce such a term, in which case it is easy to verify the\ncorrectness of the claim by reconstructing the proof of the theorem.) \nTo see from where complications can emerge in the case of decidability, we\ncompare the rule of the formation of terms and the rule\nof detachment. Given a combinatory base and a denumerable set of\nvariables, it is decidable by inspection whether a term is or is\nnot in the set of the generated terms. That is, all the inputs of the rule\nare retained in the output as subterms of the resulting term. In contrast, an\napplication of detachment results in a formula that is a proper subformula of\nthe major premise (and in the exceptional case when the major premise is an\ninstance of self-identity it is identical to the minor premise). The lack of\nthe retention of all subformulas of premises through applications of modus\nponens is the culprit behind the difficulty of some of the decision problems\nof implicational logics. It is then somewhat unsurprising that for many\ndecidable logics there is a decision procedure utilizing sequent calculi in\nwhich the cut theorem and the subformula property hold. A solution to the\nproblem of inhabitation may run into difficulties similar to those that arise\nin decidability problems in general. \nFor example, the combinator \\(\\textsf{K}\\) can be assigned the following\ntype. \n\\(\\textsf{SKK}\\) can be assigned the type \\(p\\rightarrow p\\). There is a\nproof in TA\\(_\\textrm{CL}\\) ending in \\(\\textsf{SKK}\\colon p\\rightarrow p\\) \nthat does not contain the long formula above. However, there is a proof of\n\\(\\textsf{SKK}\\colon p\\rightarrow p\\) that contains the above formula the\nsecond antecedent of which is not a subformula of \\(p\\rightarrow p\\), indeed,\nthe sets of the subformulas of the two formulas are disjoint. (We picked two\ndifferent propositional variables, \\(p\\) and \\(q\\) to emphasize this point.)\nSome important cases of the problem of inhabitation, however, are\ndecidable. \nTheorem. It is decidable if a type has an inhabitant over the base\n\\(\\{\\textsf{S},\\textsf{K}\\}\\). \n \nThis theorem amounts to the typed version of the decidability of the\nimplicational fragment of \n intuitionistic logic\nthat is part of\n Gentzen’s decidability result\n(dating from 1935). \nTheorem. It is decidable if a type has an inhabitant over the base \n\\(\\{\\textsf{I},\\textsf{C},\\textsf{B}^\\prime,\\textsf{W}\\}\\).\n \nThe theorem is the typed equivalent of the decidability of the implicational\nfragment of the logic of relevant implication. The decidability\nof \\(R_{\\rightarrow}\\) was proved by Saul A. Kripke in 1959 together\nwith the decidability of the closely related \\(E_{\\rightarrow}\\) (the\nimplicational fragment of the logic of entailment). \nTheorem. It is decidable if a type has an inhabitant over the base \n\\(\\{\\textsf{B},\\textsf{B}^\\prime,\\textsf{I},\\textsf{W}\\}\\).\n \nthe theorem is the typed version of the decidability of the\nimplicational fragment of the logic of ticket entailment\n\\(T_\\rightarrow\\), that was proved—together with the\ndecidability of \\(R_\\rightarrow\\) (\\(R_\\rightarrow\\) with the truth\nconstant \\(t\\)) and \\(T_\\rightarrow^\\textbf{t}\\) (\\(T_\\rightarrow\\)\nwith the truth constant \\(t\\))—in Bimbó and Dunn\n(2012) and Bimbó and Dunn (2013). An independent result\n(for \\(T_\\rightarrow\\) only) is in Padovani (2013), which\nextends Broda et al. (2004). \nThe decision procedures for \\(T_\\rightarrow^\\textbf{t}\\) and\n\\(R_\\rightarrow^\\textbf{t}\\) do not use \\(\\textrm{TA}_\\textrm{CL}\\) or\naxiomatic calculi, instead, they build upon consecution calculi\n(i.e., sequent calculi in which the structural connective is not\nassumed to be associative). The idea that there is an affinity between\nstructural rules and combinators goes back at least to Curry\n(1963). To tighten the connection, Dunn and Meyer (1997)\nintroduced structurally free logics in which introduction rules\nfor combinators replace structural rules—hence the label for\nthese logics. Bimbó and Dunn (2014) introduced a technique to\ngenerate a combinatory inhabitant for theorems of \\(T_\\rightarrow\\)\nfrom their standard proofs in the sequent calculus, which is used in\nthe decision procedure for \\(T_\\rightarrow^\\textbf{t}\\).  Sequent\ncalculi provide better control over proofs than natural deduction or\naxiomatic systems do. The combinatory extraction procedure of\nBimbó and Dunn (2014) yields an effective link between\ncombinators and types grounded in sequent calculus proofs, which\nobviates the apparent advantage of \\(\\textrm{TA}_\\textrm{CL}\\) and\naxiomatic systems. \nThe rule of substitution is built-in into the formulation of\n\\(\\textrm{TA}_\\textrm{CL}\\) via the rule schema called detachment and the\naxiom schemas for the basic combinators. It is obvious that there are\nformulas of least complexity that are types of \\(\\textsf{S}\\) and\n\\(\\textsf{K}\\), such that all the other types of \\(\\textsf{S}\\) and\n\\(\\textsf{K}\\) are their substitution instances. A formula that has this\nproperty is called a principal type of a combinator. Obviously, a\ncombinator that has a principal type, has denumerably many principal types,\nwhich are all substitution instances of each other; hence, it is justified to\ntalk about the principal type schema of a combinator. The existence of\nprincipal types for complex combinators is not obvious, nevertheless,\nobtains. \nTheorem. If the term \\(M\\) is typable, then \\(M\\) has a principal type\nand a principal type schema. \n \nPrincipal types and principal type schemas may seem now to be\ninterchangeable everywhere. Thus we could take a slightly different\napproach and define \\(\\textrm{TA}_\\textrm{CL}\\) to include axioms and\nthe rule schema of detachment together with the rule of\nsubstitution. This version of \\(\\textrm{TA}_\\textrm{CL}\\) would\nassume the following form. \nwhere \\(P\\) ranges over propositional variables. (The substitution notation is\nextended—in the obvious way—to sets of type assignments.) Clearly,\nthe two deduction systems are equivalent. \nIf substitution were dropped altogether, then the applicability of detachment\nwould become extremely limited, for instance, \\(\\textsf{SK}\\) no longer would\nbe typable. A compromise between having substitution everywhere and having no\nsubstitution at all is to modify the detachment rule so that that includes as\nmuch substitution as necessary to ensure the applicability of the detachment\nrule. Such a rule (without combinatory terms or type assignments) was invented\nin the 1950s by Carew A. Meredith, and it is usually\ncalled condensed detachment. The key to the applicability of detachment\nis to find a common substitution instance of the minor premise and of the\nantecedent of the major premise. This step is called unification.  (A\nbit more formally, let \\(s(A)\\) denote the application of the substitution\n\\(s\\) to \\(A\\).  Then, the result of the condensed detachment of \\(A\\) from\n\\(B\\rightarrow C\\) is \\(s(C)\\), when there is an \\(s\\) such that \\(s(A)=\ns(B)\\), and for any \\(s_1\\) with this property, there is an \\(s_2\\) such that\n\\(s_1\\) is the composition of \\(s\\) and \\(s_2\\).) \nNotice that it is always possible to choose substitution instances of a pair\nof formulas so that the sets of their propositional variables are disjoint,\nbecause formulas are finite objects. The most general common instance\nof two formulas \\(A\\) and \\(B\\) (that do not share a propositional variable)\nis \\(C\\), where \\(C\\) is a substitution instance of both \\(A\\) and \\(B\\), and \npropositional variables are identified by the substitutions only if the\nidentification is necessary to obtain a formula that is a substitution\ninstance of both \\(A\\) and \\(B\\). The unification theorem (specialized\nto simple types) implies that if two formulas \\(A\\) and \\(B\\) have a common\ninstance then there is a formula \\(C\\) such that all the common instances of\n\\(A\\) and \\(B\\) are substitution instances of \\(C\\). Obviously, a pair of\nformulas either has no common instance at all, or it has \\(\\aleph_0\\) many\nmost general common instances. \nA famous example of a pair of formulas that have no common instance is\n\\(A\\rightarrow A\\) and \\(A\\rightarrow A\\rightarrow B\\). The instances \\(p\n\\rightarrow p\\) and \\(q\\rightarrow q\\rightarrow r\\) share no propositional\nvariables, however, neither \\(q\\rightarrow q\\) nor \\((q\\rightarrow r)\\rightarrow\nq\\rightarrow r\\) matches the shape of the second formula. To put the problem\ndifferently, \\(q\\) and \\(q\\rightarrow r\\) would have to be unified, but they\ncannot be unified no matter what formula is substituted for \\(q\\). An\nimmediate consequence of this is that \\(\\textsf{WI}\\) is not typable. \nOn the other hand,  and   are substitution instances of \\(p\\rightarrow\np\\) and of \\(q\\rightarrow q\\). Furthermore, all simple types are substitution\ninstances of a propositional variable, hence \\(\\textsf{II}\\) can be assigned\nboth the type \\(r\\rightarrow r\\) and the type (\\(s\\rightarrow s)\\rightarrow\ns\\rightarrow s\\)—and, of course, the latter happens to be an instance of \nthe former because \\(A\\rightarrow A\\) is the principal type schema of\n\\(\\textsf{II}\\). If we apply condensed detachment to \\(p\\rightarrow p\\) and\n\\(q\\rightarrow q\\), then we get \\(q\\rightarrow q\\) (via the substitutions\n\\([p/q\\rightarrow q]\\) and \\([q/q])\\), and so condensed detachment yields the\nprincipal type of \\(\\textsf{II}\\). Incidentally, \\(\\textsf{II}\\) and \n\\(\\textsf{I}\\) provide an excellent example to illustrate that distinct\nterms may have the same principal type schema. \nCondensed detachment has been used extensively to refine\naxiomatizations of various implicational logics, especially, in search\nfor shorter and fewer axioms. Some logics may be formulated using\naxioms (rather than axiom schemas) together with the rule of condensed\ndetachment—without loss of theorems. All the logics that we\nmentioned so far (\\(J_{\\rightarrow}\\), \\(R_{\\rightarrow}\\),\n\\(T_{\\rightarrow}\\) and \\(E_{\\rightarrow}\\)) are\n\\(\\mathbf{D}\\)-complete, that is, they all may be axiomatized\nby axioms and the rule of condensed detachment. That is, the\nimplicational fragments of classical and intuitionistic logics, and\nthe implicational fragments of the relevance logics \\(R\\), \\(E\\)\nand \\(T\\) are all \\(\\mathbf{D}\\)-complete.  (See Bimbó (2007) for\nsome further technical details.) \nSimply typed systems have been extended in various directions. Logics often\ncontain connectives beyond implication. It is a natural modification of a type\nassignment system to expand the set of types via including further type\nconstructors. Conjunction and fusion are the easiest to explain or\nmotivate as type constructors, however, disjunction and backward implication\nhave been introduced into types too. Types are useful, because they allow us\nto get a grip on classes of terms from the point of view of their behavior\nwith respect to reduction. \nTait’s theorem. If a combinatory term \\(M\\) is typable (with simple\ntypes) then \\(M\\) strongly normalizes, that is, all reduction sequences of\n\\(M\\) are finite (i.e., terminate). \n \nThe converse of this claim is, obviously, not true. For example,\n\\(\\textsf{WI}\\) strongly normalizes but untypable, because the antecedent of\ncontraction cannot be unified with any instance of self-implication. The aim\nto extend the set of typable terms led to the introduction of \\(\\land\\) into\ntypes. \nA different way to look at the problem of typing \\(\\textsf{WI}\\) is to say\nthat \\(\\textsf{W}\\) should have a type similar to the formula \\((A\\rightarrow\n(A\\rightarrow B))\\rightarrow A\\rightarrow B\\), but in which the formulas in\nplace of the two formulas \\(A\\) and \\(A\\rightarrow B\\) in the antecedent can\nbe unified. This is the motivation for the inclusion of conjunction\n(\\(\\land\\)) and top (\\(\\top\\)) as new type constructors. \nAn extended type system, that is often called the intersection type\ndiscipline, is due to Mario Coppo and Mariangiola Dezani-Ciancaglini. The\nset of intersection types (denoted by wff) is defined as follows. \nOf course, if TA\\(_\\textrm{CL}\\) is augmented with an expanded set of types,\nthen new instances of the previously assigned types become available. However,\nthe gist of having types with the new type constructors \\(\\land\\) and \\(\\top\\)\nis that the set of types has a richer structure than the relationships between\ntypes determined by the rules of substitution and modus ponens. \nThe structure of intersection types is described by the\nconjunction–implication fragment of \\(B\\), the basic relevance\nlogic. In the following presentation of this logic, \\(\\le\\) is the main\nconnective (an implication) of a formula and \\(\\Rightarrow\\) separates the\npremises and the conclusion of an inference rule. \nThe axiom schemas for the combinators \\(\\textsf{S}\\),\\(\\textsf{K}\\) and\n\\(\\textsf{I}\\) are as follows. Note that the axiom for \\(\\textsf{S}\\) is not\nsimply a substitution instance (with new connectives included) of the previous\naxiom for \\(\\textsf{S}\\). \nThere are four new rules added, and there is an axiom for \\(\\top\\). \nThis type assignment system is equivalent to the intersection type assignment\nsystem for the \\(\\lambda\\)-calculus, and it allows a more precise\ncharacterization of classes of terms with respect to the termination of\nreduction sequences. \nTheorem.\n(1) A term \\(M\\) normalizes whenever \\(M\\) is typable.\n\n(2) A term \\(M\\) strongly normalizes whenever \\(M\\) is typable and the\nproof does not contain \\(\\top\\). \nCL has various kinds of models, three of which are exemplified in some detail\nin this section. Algebraic models (often called “term\nmodels”) may be constructed without difficulty for both the inequational\nand the equational systems of CL that were introduced in section 2.1. The set\nof terms forms an algebra, and given a suitable equivalence relation (that is\nalso a congruence), the application operation can be lifted to the equivalence\nclasses of terms in the standard way. The quasi-inequational characterization\nof the so obtained algebra provides the basis for an algebraic semantics for\nthese logics. Isolating the Lindenbaum algebra and verifying that it is not a\ntrivial algebra constitutes a consistency proof for \\(\\text{CL}_\\triangleright\\) and\n\\(\\text{CL}_=\\). \nDana Scott defined \\(P\\omega\\) and \\(D_\\infty\\) for the \\(\\lambda\\)-calculus. \nWe first outline \\(P\\omega\\)—the so-called graph model, which is\neasier to understand. \nThe natural numbers have a very rich structure. \\(P\\omega\\) is the power set\nof the set of natural numbers and it is at the core of the model bearing the\nsame label. Every natural number has a unique representation in base\n\\(2\\). (E.g., \\(7_{10}\\) is \\(111\\), that is, \\(7=1\\cdot2^2+1\\cdot2^1+1\\cdot\n2^0=4+2+1\\).) Each binary representation is of the form where each \\(b\\) is \\(0\\) or\n\\(1\\). Then each binary number may be viewed as the characteristic function of\na finite subset of natural numbers. (On the left, there are infinitely\nmany zeros, as in \\(\\ldots000111\\), which are omitted.) For a natural number\n\\(n\\), \\(e_n\\) denotes the corresponding finite set of natural numbers. (E.g., \n\\(e_7=\\{0,1,2\\}\\).) \nThe positive topology on \\(P\\omega\\) comprises finitely generated open\nsets. Let \\(E\\) denote the finite subsets of \\(\\omega\\). \\(X\\subseteq\nP\\omega\\) is open iff \\(X\\) is a cone (with respect to \\(\\subseteq\\))\ngenerated by a subset of \\(E\\). Given the positive topology, a function\n\\(f\\colon P\\omega\\rightarrow P\\omega\\) turns out to be continuous (in\nthe usual topological sense) iff \\(f(x)=\\cup\\{f(e_n)\\colon e_n\\subseteq x\\}\\), \nwhere \\(e_n\\in E\\). This means that \\(m\\in f(x)\\) iff \\(\\exists e_n\\subseteq\nx.\\, m\\in f(e_n)\\), which leads to a characterization of a continuous function \n\\(f\\) by the pairs of natural numbers \\((n,m)\\). \nA one-one correspondence between (ordered) pairs of natural numbers and\nnatural numbers is defined by \nThe set of pairs that constitute a (unary) function is sometimes called\nthe graph of the function. The graph of a continuous function \\(f\\colon\nP\\omega\\rightarrow P\\omega\\) is defined by \\(\\textrm{graph}(f)=\\{(n,m)\\colon \nm\\in f(e_n)\\}\\). In order to be able to model type-free\napplication—including self-application—subsets of \\(\\omega\\)\nshould be viewed as functions too. For \\(x, y\\subseteq\\omega\\), the function\ndetermined by \\(y\\) is defined as \\(\\textrm{fun}(y)(x)=\\{m\\colon \\exists\ne_n\\subseteq x.\\,(n,m)\\in y\\}\\). For a continuous function \\(f\\), \n\\(\\textrm{fun}(\\textrm{graph}(f))=f\\) holds. \nThe graph model of CL maps terms into subsets of \\(\\omega\\). To start with,\nthe combinators have concrete sets as their interpretations. As a simple\nexample, \\(\\textsf{I}= \\{(n,m)\\colon m\\in e_n\\}\\). Of course, each pair\ncorresponds to an element of \\(\\omega\\), hence, we get a set of natural\nnumbers. In particular, some of the elements are \\(\\{1,6,7,11,15,23,29,30,\n\\ldots\\}\\). \nIf the combinators (as well as the variables) are interpreted as subsets of\n\\(\\omega\\), then function application should take the first set (viewed as a\nfunction of type \\(P\\omega\\rightarrow P\\omega)\\) and apply that to the second\nset. \\(\\textrm{fun}(y)\\) is a suitable function that is determined by \\(y\n\\subseteq\\omega\\). In general, if \\(M\\) and \\(N\\) are CL-terms, and \\(I\\) is\nthe interpretation of atomic terms into \\(P\\omega\\), then \\(I\\) is extended to \ncompound terms by \\(I(MN)=\\textrm{fun}(I(M))(I(N))\\). (E.g., let \\(I(x)\\) be\n\\(e_9=\\{0,3\\}\\). \\(I(\\textsf{I}x)=\\textrm{fun}(I(\\textsf{I}))(I(x))=\\{m\\colon \n\\exists e_n\\subseteq I(x).\\,(n, m)\\in I(\\textsf{I})\\}\\). We know what \\(I\n(\\textsf{I})\\) and \\(I(x)\\) are; hence, we get further that \\(I(\\textsf{I}x)=\n\\{m\\colon\\exists e_n\\subseteq e_9.\\,m\\in e_n\\}\\). Of course, \\(\\{0,3\\}\\subseteq\n\\{0,3\\}\\), and so we have that \\(I(\\textsf{I}x)=\\{0,3\\}\\).) This model\nsupports an intensional notion of functions. \nThe earliest model for a typefree applicative system as a function\nspace was also given by Scott, a couple of years before the graph model,\nin the late 60s. The following is an outline of some of the key elements of\nthe construction. \nIn pure typefree CL, an expression of the form \\(MM\\) is a well-formed term. \nMoreover, terms of this form can enter into provable equations and inequations\nin multiple ways. For example, \\(xx=xx\\) is an axiom, and by one of the rules,\n\\(y(xx)=y(xx)\\) is provable too. A more interesting occurrence of a term of\nthe form \\(MM\\) can be seen in the provable inequation \\(\\textsf{S}\n(\\textsf{SKK})(\\textsf{SKK})x\\triangleright xx\\). \nThe set-theoretic reduction of a function yields a set of pairs (in general, a\nset of tuples). In set theory (assuming well-foundedness, of course) a pair\n(e.g., \\(\\{\\{ a\\},\\{ a,b\\}\\}\\)) is\nnever identical to either of its two elements. Therefore, the main question\nconcerning a mathematical model of CL is how to deal\nwith self-application. \nScott’s original model is built starting with a complete lattice \\((D,\n\\le)\\). That is, \\((D,\\le)\\) is a partially ordered set in which greatest\nlower bounds (infima) and least upper bounds (suprema) exist for arbitrary\nsets of elements. A function \\(f\\) from \\((D,\\le)\\) into a complete lattice\n\\((E,\\le)\\) is said to be continuous when it preserves the supremum of\neach ideal on \\(D\\), where an ideal is an upward directed downward closed\nsubset. \nA topology may be defined on \\(D\\) by selecting certain increasing sets\nas the opens. More precisely, if \\(I\\) is an ideal and \\(C\\) is a cone, then\n\\(C\\) is open iff \\(C\\cap I \\ne\\emptyset\\) provided that \\(\\bigvee I\\in C\\),\nthat is, provided that the supremum of \\(I\\) is an element of \\(C\\).  (For\nexample, complements of principal ideals are open.) \\(f\\) turns out to be\ncontinuous in the usual topological sense, that is, the inverse image of an\nopen set is open, when \\(D\\) and \\(E\\) are taken together with their\ntopologies. This motivates the earlier labeling of these functions as\ncontinuous. Notably, all continuous functions are monotone. \nFor the purposes of modeling functions in CL, the interesting functions are\nthose that are continuous on \\(D\\). However, these functions by\nthemselves are not sufficient to obtain a modeling of self-application,\nbecause none of them has as its domain a set of functions—as \\(D\\) is\nnot assumed to be a function space. The solution starts with defining\na hierarchy of function spaces \\(D_0\\), \\(D_1\\), \\(D_2,\\ldots\\) so that\neach function space \\(D_n\\), is a complete lattice on which continuous\nfunctions may be defined (creating the function space \\(D_{n+1}\\)). The\nimportance of selecting continuous functions is that the emerging function\nspace has the same cardinality as the underlying set, which allows us\nto define embeddings between function spaces adjacent within the\nhierarchy. \nThe hierarchy of all the function spaces \\(D_n\\) may be accumulated\ntogether. A standard construction in model theory is to form the disjoint\nunion of structures. (Disjointness can always be guaranteed by indexing the\ncarrier sets of the structures.) Scott defined \\(D_\\infty\\) to be\nthe disjoint union of the function spaces \\(D_n\\), for all \\(n\\),\nexcept that the extremal elements are “glued together.” (More\nformally, the top elements and the bottom elements of the function spaces,\nrespectively, are identified with each other.) \\(D_\\infty\\) is a complete\nlattice, and by Tarski’s fixed point theorem, a continuous function that maps \n\\(D_\\infty\\) into \\(D_\\infty\\) has a fixed point, which implies that \\(D\n_\\infty\\) is isomorphic to \\(D_\\infty\\rightarrow D_\\infty\\). \nThe above construction may also be conceptualized in terms of strings\nand Cartesian products. The back-and-forth moves between functions of\none and more than one variable—the “uncurrying” and\n“currying” of functions—algebraically corresponds to the two\ndirections of residuation. For example, a function \\(f\\colon A\\times\nB\\rightarrow C\\) may be represented by a function \\(f^\\prime\\colon A\n\\rightarrow B\\rightarrow C\\), and vice versa. Thus, without loss of\ngenerality, it is sufficient to consider unary functions. If \\(a\\) is a fixed\nelement of the function space \\(D_\\infty\\), then \\(x = (a,x)\\) holds when\n\\(x\\) is the fixed point of \\(a\\). In terms of tuples, the solution may be\nviewed as the infinite tuple \\((a,(a,(a,\\ldots\\). \nA further model that we briefly outline falls into the set-theoretical\nsemantics paradigm of nonclassical logic and it is due to\nJ. Michael Dunn and Robert K. Meyer (see Dunn and\nMeyer (1997)). CL and \\(\\lambda\\)-calculi are inherently connected to\nintuitionistic, relevance and other nonclassical logics.  In\nparticular, the \\(\\text{CL}_\\triangleright\\) and \\(\\text{CL}_=\\)\ncalculi are nonclassical logics themselves. Set theoretical semantics\nin which the intensional connectives are modeled from relations on a\ncollection of situations have been the preferred interpretation of\nnonclassical logics since the early 1960s. These sorts of semantics\nare sometimes called “Kripke semantics” (because Kripke\nintroduced possible-world semantics for some normal modal logics in\n1959) or “gaggle semantics” (after the pronunciation of\nthe abbreviation ‘ggl’ that stands for “generalized\nGalois logics” introduced by Dunn (1991)). \nA model for \\(\\text{CL}_\\triangleright\\) may be defined as follows. Let (\\(W,\\le,\nR,S,K,v)\\) comprise a (nonempty) partially ordered set \\((W,\\le)\\) with a\nthree-place relation \\(R\\) on \\(W\\), and let \\(S\\), \\(K\\in W\\).  Furthermore,\nfor any \\(\\alpha,\\beta,\\gamma,\\delta\\in W\\), the conditions (s) and (k) are\ntrue. \nThe ternary relation is stipulated to be antitone in its first two argument\nplaces and monotone in the third. These components define a frame for\n\\(\\text{CL}_\\triangleright\\). The valuation function \\(v\\) maps variables \\(x,y,z,\n\\ldots\\) into (nonempty) cones on \\(W\\), and it maps the two primitive\ncombinators \\(\\textsf{S}\\) and \\(\\textsf{K}\\) into the cones generated by\n\\(S\\) and \\(K\\), respectively. Recall, that the standard notation in CL hides\napplication, a binary operation that allows forming compound terms. The next\nclause extends \\(v\\) to compound terms and makes this operation explicit\nagain. \nAn inequation \\(M\\triangleright N\\) is valid if \\(v(M)\\subseteq v(N)\\) under\nall valuations on frames for \\(\\text{CL}_\\triangleright\\). (That is, the relationship \nbetween the interpretations of the two terms is invariant whenever \\(v\\) is\nvaried on the set of variables.) \nInformally, the underlying set \\(W\\) is a set of situations, and \\(R\\)\nis an accessibility relation connecting situations. All the terms are\ninterpreted as sets of situations, and function application is\nthe existential image operation derived from \\(R\\). A difference from\nthe previous models is that the result of an application of a term to a term\nis not determined by the objects themselves that interpret the two\nterms—rather the application operation is defined from \\(R\\). \nThis semantics generalizes the possible worlds semantics for normal modal\nlogics. Therefore, it is important to note that the situations are not\nmaximally consistent theories, rather they are theories possessing the\nproperty that for any pair of formulas they contain a formula that implies\nboth of them. Equivalently, the situations may be taken to be dual ideals on\nthe Lindenbaum algebra of \\(\\text{CL}_\\triangleright\\). These situations are\ntypically consistent in the sense that they do not contain all the terms in\nall but one case. (The notion of negation consistency, of course, cannot be\ndefined for \\(\\text{CL}_\\triangleright\\) or for \\(\\text{CL}_=\\).) \nRelational semantics can be defined for \\(\\text{CL}_=\\) along similar lines. Then\nsoundness and completeness—that is, the following\ntheorem—obtains. \nTheorem.\n(1) An inequation \\(M\\triangleright N\\) is provable in\n\\(\\text{CL}_\\triangleright\\) if and only if \\(v(M)\\subseteq v(N)\\) in any\nmodel for \\(\\text{CL}_\\triangleright.\\)\n\n(2) An equation \\(M = N\\) is provable in \\(\\text{CL}_=\\) if and only if\n\\(v(M)=v(N)\\) in any model for \\(\\text{CL}_=.\\) \nRelational and operational semantics for systems of CL that include dual and\nsymmetric combinators can be found in Bimbó (2004). \nA remarkable feature of CL is that despite its seeming simplicity it is\na powerful formalism. Of course, the strength of CL cannot be\nappreciated without discovering certain relationships between combinatory\nterms or without an illustration that computable functions are definable. \nAn important beginning step in the formalization of mathematics is the\n formalization of arithmetic,\nthat was first achieved by the Dedekind–Peano\naxiomatization. There are various ways to formalize arithmetic in CL; two\nrepresentations of numbers are described in this section together with some\nfunctions on them. \nNumbers may be thought to be objects (or abstract objects) of some\nsort. (Here by numbers we mean natural numbers, that is, \\(0\\) and the\npositive integers.) Numbers could be characterized, for example, by\nthe structure they possess as a set. This structure supports properties\nsuch as \\(0\\ne1\\), and that the sum of \\(n\\) and \\(m\\) is the same number as\nthe sum of \\(m\\) and \\(n\\).  Another well-known property of the natural\nnumbers is, for example, the existence of infinitely many prime numbers. \nNumbers can be represented in CL by terms, and one way is to choose the terms\n\\(\\textsf{KI}\\), \\(\\textsf{I}\\), \\(\\textsf{SBI}\\), \\(\\textsf{SB}\n(\\textsf{SBI}),\\ldots\\) for \\(0\\), \\(1\\), \\(2\\), \\(3\\), etc. The terms that\nrepresent the arithmetic operations vary, depending on which terms stand for\nthe numbers. Note that unlike the Dedekind–Peano formalization of\narithmetic, CL makes no syntactic distinction that would parallel the\ndifference between individual constants and function symbols—in CL the\nonly objects are terms. The above list of terms already shows the successor\nfunction, which is \\(\\textsf{SB}\\). (\\(\\textsf{SB}(\\textsf{KI})\\) strongly\nequals to \\(\\textsf{I}\\), that is, \\(1\\) is the successor of \\(0\\).) \nAddition is the term \\(\\textsf{BS}(\\textsf{BB})\\),\nand multiplication is the term \\(\\textsf{B}\\). The usual recursive\ndefinition of multiplication based on addition may suggest that addition\nshould be a simpler operation than multiplication. However, in CL the numbers \nthemselves are functions, and so they have properties that allows \n\\(\\textsf{B}\\)—a simpler looking term—to be chosen for the\nfunction that is often perceived to be more complex than addition. (The\naddition operation could be defined using primitive recursion, which would\nproduce a more complex term.) As a classical example, we may consider the term\n\\(\\textsf{BII}\\), that is strongly equal to \\(\\textsf{I}\\), that is, \n\\(1\\times1=1\\)—as expected. We do not pursue here this numerical\nrepresentation further. We only note that the shape of these numbers is\nclosely related to Church’s numerals in the \\(\\lambda\\)-calculus, each of\nwhich is a binary function (whereas here, each number is a unary\nfunction). \nAnother way to represent numbers in CL is to start with a different choice of\nterms for the numbers. Previously, \\(\\textsf{I}\\) stood for \\(1\\), now we take\n\\(\\textsf{I}\\) to be \\(0\\). The successor of a number \\(n\\) is \\(\\textsf{V}\n(\\textsf{KI})n\\), where the second occurrence of \\(n\\) indicates a numeral,\nthat is, the combinator that represents \\(n\\). (The numeral for \\(n\\) is often\ndenoted—more precisely—by an overlined or otherwise decorated\n\\(n\\). However, the double usage of \\(n\\) in a limited context should not\ncause any confusion.) In other words, the successor function is \n\\(\\textsf{V}(\\textsf{KI})\\). Notice that the numbers in the present\nrepresentation are terms over a more restricted combinatory base than in the\nformer case. For example, no combinator with duplicative effect is definable\nfrom \\(\\{\\textsf{I},\\textsf{K},\\textsf{V}\\}\\). \nSome simple recursive functions may be defined as follows. The \npredecessor function \\(P\\) on numbers is “\\(-1\\)” (i.e.,\nsubtracting one) for all numbers greater than or equal to \\(1\\), and the\npredecessor of \\(0\\) is set to be \\(0\\). The next term defines the predecessor\nfunction which is abbreviated by \\(P\\). \nIf \\(n\\) is a numeral, then \\(Pn\\) reduces to \\(n\\textsf{KI}(n(\n\\textsf{KI}))\\), which suggests that for positive numbers, \\(P\\) could have\nbeen defined to be the term \\(\\textsf{T}(\\textsf{KI})\\), because \n\\(\\textsf{T}(\\textsf{KI})n\\) reduces to \\(n-1\\) whenever \\(n\\) is a term of\nthe form \\(\\textsf{V}(\\textsf{KI})(n-1)\\). \nSome models of computation (such as register machines) and certain programming\nlanguages include a test for zero as a primitive construct. It is\nuseful to find a CL-term for a function \\(Z\\) such that \\(Znxy\\) reduces to \\(x\\)\nif \\(n\\) is zero, whereas \\(Znxy\\) reduces to \\(y\\) when \\(n\\) is positive. \n\\(Znxy\\) may be thought of as the conditional instruction “If \\(n=0\\) \nthen \\(x\\) else \\(y\\),” where \\(x\\) and \\(y\\) are themselves functions. \n(Of course, in the pseudo-code one should have assumed that \\(n\\) is of\ninteger type and cannot take a negative value, that could be guaranteed by a\ndeclaration of variables and an additional conditional statement.) The\nfollowing definition works for branching on zero. \n\\(\\textsf{TK}nxy=n\\textsf{K}xy\\), and if \\(n\\) is zero, that is, \\(n= \n\\textsf{I}\\), then by another step \\(\\textsf{K}xy\\) and then \\(x\\) results;\nwhereas if \\(n\\) is positive, then after a few more reductions, one gets\n\\(\\textsf{KI}xy\\), that is, \\(y\\). The two terms, \\(\\textsf{K}xy\\) and\n\\(\\textsf{KI}xy\\), hint toward an interpretation of \\(\\textsf{K}\\) and\n\\(\\textsf{KI}\\) as truth and falsity, or they can be viewed as\nterms that can select, respectively, the first or the second argument. These\nideas may be further developed into definitions of truth functions and a\nrepresentation of tuples. \nAddition may be defined by the recursive equation \\(+mn=Zmn\n(\\textsf{V}(\\textsf{KI})(+(Pm)n))\\), where \\(m\\) and \\(n\\) are numerals, and\n\\(P\\) and \\(Z\\) are the already defined functions. (The abbreviations are used\nto enhance the readability of the terms—they can be replaced everywhere\nby the defining combinators.) To put into words, if \\(m\\) is \\(0\\) then the\nsum is \\(n\\), otherwise \\(m+n\\) is the successor of \\((m-1)+n\\). Of course,\nthis definition closely simulates the definition of addition from recursion\ntheory, where addition is often defined by the two equations \\(+(0,n)=n\\) and\n\\(+(s(m),n)=s(+(m,n))\\) (with \\(s\\) denoting the successor function). The fact\nthat CL can express addition in this form shows—once again—the\nversatility of CL. \nCombinatorial completeness guarantees that the term on the right hand side of\nthe defining equation for \\(+\\) (i.e., the term \\(Zmn(\\textsf{V}(\\textsf{KI})\n(+(Pm)n)))\\) can be transformed into a term in which \\(+\\) is the first, \\(m\\)\nand \\(n\\) are the second and third arguments, respectively. Then \\(+\\) can be\ndefined explicitly as the fixed point of the combinator \nOf course, we can abbreviate the so obtained term as \\(+\\) for the sake of\ntransparency, just as we used earlier \\(P\\) and \\(Z\\) as shorthands for longer\ncombinatory terms. \nMultiplication is often denoted by \\(\\,\\cdot\\,\\). The recursive\nequation \\(\\,\\cdot\\, mn = Zm\\textsf{I}(+n(\\,\\cdot\\,(Pm)n))\\) defines\nmultiplication and it can be deciphered as “if \\(m\\) is \\(0\\)\nthen the result is \\(0\\), else \\(n\\) is added to the result of\n\\((m-1)\\cdot n\\).” The next step in the definition brings the\nright-hand side term to the form \\(\\textsf{X}\\cdot mn\\), where\n\\(\\textsf{X}\\) does not contain occurrences of \\(\\,\\cdot\\,\\), \\(m\\)\nor \\(n\\). Then taking the fixed point of \\(\\textsf{X}\\), and setting\n\\(\\,\\cdot\\,\\) to be \\(\\textsf{YX}\\) concludes the definition of the\nmultiplication function. For instance, the abstraction can yield the\ncombinator \nThe factorial function is definable from the predecessor\nfunction plus from multiplication, and it is useful e.g., in\ncombinatorics. The factorial function (denoted by \\(\\,!\\,\\)) is\nrecursively definable by the equation \\(\\,!\\,\nm=Zm(\\textsf{V}(\\textsf{KI})\\textsf{I})(\\cdot m(\\,!  \\,(Pm)))\\), that\nmay be read as “if \\(m\\) is \\(0\\), then \\(\\,!\\, m=1\\), otherwise\n\\(\\,!\\, m\\) equals to \\(m\\) multiplied by the factorial of\n\\(m-1\\).” \nOf course, it is not necessary to define various numerical functions by\nsimulating their recursive definitions. As we saw above in the case of the\nfirst representation of numbers, we might just happen to have the right terms\nsuch as \\(\\textsf{BS}(\\textsf{BB})\\) and \\(\\textsf{B}\\), that behave as the\ntarget functions do on numbers. That is, an equally good way to define\narithmetic functions is to simply list some terms and then show that they\nbehave as expected. However, once it has been shown that the basic\nprimitive recursive functions together with recursion\nand minimization can be emulated in CL, we have got not only a nice\ncollection of arithmetic functions in the form of combinators to work with,\nbut also a proof that combinatory logic is sufficiently expressive to\nformalize all partial recursive functions. Indeed, the remaining steps\nof such a proof can be carried out in CL, though the details are beyond the\nscope of this entry. \nThe abbreviations and the interspersed explanations in the sketch above may\nobscure that arithmetic has been formalized in a language that consists of\nfive symbols (when juxtaposition is not counted): \\(\\textsf{S}\\), \n\\(\\textsf{K}\\), \\(=\\) plus two delimiters, \\(\\,(\\,\\) and \n\\(\\,)\\,\\). The finite (and perhaps, surprisingly small) number of\nsymbols and the availability of recursive functions conjure the thought that\nan arithmetization of the syntax of CL could be attempted. \n Gödel\nachieved an encoding of a formal language by assigning numbers to symbols,\nformulas and sequences of formulas, which later became known as\n“Gödel numbers.” Concretely, Gödel assigned odd numbers\nto symbols and products of powers of primes (with the number corresponding to \na symbol in the exponent) to sequences of symbols. However, it is possible to \narithmetize the language of CL without placing a strong emphasis on the \nexistence and the properties of prime numbers. (See for example, \nRaymond M. Smullyan’s books: Smullyan (1985) and Smullyan (1994).) \nThe five symbols get as their Gödel numbers the first five positive \nintegers. A string is assigned the number in base \\(10\\) that results from \nthe concatenation of the corresponding numbers for the symbols. \nThe following outline gives the flavor of an analogue of Gödel’s\nincompleteness theorem adapted to CL. It is possible to define a combinator\nsuch that if this combinator is applied to a numeral \\(n\\), then the whole\nterm reduces to the numeral \\(m\\) that is the numeral denoting\nthe Gödel number of the numeral \\(n\\). Slightly more formally,\nthere is a combinator \\(\\delta\\) such that \\(\\delta n = G(n)\\) (where \\(G\n(n)\\) denotes the Gödel number of the expression \\(n\\)). Furthermore,\nthere is a combinatory term, which returns the numeral itself followed by\n\\(G(n)\\), when applied to a numeral \\(n\\). For any term \\(A\\) there is a term\n\\(B\\) such that the equation \\(A(\\delta B)=B\\) is true. This statement (or its\nconcrete variant for a particular formal system) is usually called\nthe second fixed point theorem. Computable characteristic functions of\nrecursive sets of numbers can be represented by combinators with the choice of\n\\(\\textsf{K}\\) for truth and \\(\\textsf{KI}\\) for falsity. The complements of\nsuch functions are computable too. Finally, it can be proved that there is no\ncombinator that represents the set of all true equations. To put it\ndifferently, any combinator either represents a set of equations that fails to\ninclude some true equations, or represents a set of equations that includes\nall true but also some false equations. \n Alonzo Church\nproved the undecidability of classical first-order logic relying on \nGödel’s incompleteness theorem. Dana Scott proved that if \\(A\\) is a \nnonempty proper subset of \\(\\lambda\\)-terms that is closed under equality \nthen \\(A\\) is not recursive. The analogous claim for CL, that follows from \nthe existence of a Gödelian sentence for CL, is that it is \nundecidable if two CL-terms are equal.","contact.mail":"bimbo@ualberta.ca","contact.domain":"ualberta.ca"}]
