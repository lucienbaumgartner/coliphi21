[{"date.published":"2017-08-22","url":"https://plato.stanford.edu/entries/fine-tuning/","author1":"Simon Friederich","author1.info":"http://simonfriederich.eu","entry":"fine-tuning","body.text":"\n\n\nThe term “fine-tuning” is used to characterize\nsensitive dependences of facts or properties on the values of certain\nparameters. Technological devices are paradigmatic examples of\nfine-tuning. Whether they function as intended depends sensitively on\nparameters that describe the shape, arrangement, and material\nproperties of their constituents, e.g., the constituents’\nconductivity, elasticity and thermal expansion coefficient.\nTechnological devices are the products of actual\n“fine-tuners”—engineers and manufacturers who\ndesigned and built them—but for fine-tuning in the broad sense\nof this article to obtain, sensitivity with respect to the values of\ncertain parameters is sufficient.\n\n\nPhilosophical debates in which “fine-tuning” appears are\noften about the universe’s fine-tuning for life:\naccording to many physicists, the fact that the universe is able to\nsupport life depends delicately on various of its fundamental\ncharacteristics, notably on the form of the laws of nature, on the\nvalues of some constants of nature, and on aspects of the\nuniverse’s conditions in its very early stages. Various\nreactions to the universe’s fine-tuning for life have been\nproposed: that it is a lucky coincidence which we have to accept as a\nprimitive given; that it will be avoided by future best theories of\nfundamental physics; that the universe was created by some divine\ndesigner who established life-friendly conditions; and that\nfine-tuning for life indicates the existence of multiple other\nuniverses with conditions very different from those in our own\nuniverse. Sections 1–4 of the present article review the case\nfor this fine-tuning for life, the reactions to it, and major\ncriticisms of these reactions. Section 5 turns from fine-tuning for\nlife to the criterion of naturalness—a condition of no\nfine-tuning in a rather different sense which applies to theories in\nquantum field theory and plays a large role in contemporary particle\nphysics and cosmology.\n\n\n\n\nOur best current theories of fundamental physics are the Standard\nModel of elementary particle physics and the theory of general\nrelativity. The Standard Model accounts for three of the known four\nfundamental forces of nature—the strong, the weak, and the\nelectromagnetic force—while general relativity accounts for the\nfourth—gravity. Arguments according to which our universe is\nfine-tuned for life are aimed at showing that life could not have\nexisted for the vast majority of other forms of the laws of nature,\nother values of the constants of nature, and other conditions in the\nvery early universe. \nThe following is an—incomplete—list of suggested instances\nof fine-tuning for life. (For popular overviews see Leslie 1989: ch.\n2, Rees 2000, Davies 2006, and Lewis & Barnes 2016; for technical\nones see Hogan 2000, Uzan 2011, and Barnes 2012.) \nIt has been claimed that the laws of physics are fine-tuned for life\nnot only with respect to the constants that appear in them but also\nwith respect to their form itself. Three of the four known fundamental\nforces—gravity, the strong force, and\nelectromagnetism—play key roles in the organisation of complex\nmaterial systems. A universe in which one of these forces is\nabsent—and the others are present as in our own\nuniverse—would most likely not give rise to life, at least not\nin any form that resembles life as we know it. The fundamental force\nwhose existence is least clearly needed for life is the weak force\n(Harnik et al. 2006). Further general features of the actual laws of\nnature that have been claimed to be necessary for the existence of\nlife are the quantization principle and the Pauli exclusion principle\nin quantum theory (Collins 2009: 213f.). \nConsiderations according to which the laws of nature, values of the\nconstants, and boundary conditions of the universe are fine-tuned for\nlife refer to life in general, not merely human life. According to\nthem, a universe with different laws, constants, and boundary\nconditions would almost certainly not give rise to any form\nof life. A common worry about such considerations is that they are\nill-founded due to lack of a widely accepted definition of\n“life”. Another worry is that we may seriously\nunderestimate life’s propensity to appear under different laws,\nconstants, and boundary conditions because we are biased to assume\nthat all possible kinds of life will resemble life as we know it. A\njoint response to both worries is that, according to the fine-tuning\nconsiderations, universes with different laws, constants, and boundary\nconditions would typically give rise to much less structure and\ncomplexity, which would seem to make them life-hostile, irrespective\nof how exactly one defines “life” (Lewis & Barnes\n2016: 255–274). \nVictor Stenger (2011) is extremely critical of considerations\naccording to which the laws, constants, and boundary conditions of our\nuniverse are fine-tuned. According to Stenger, the form of the laws of\nnature is fixed by the reasonable—very weak—requirement\nthat they be “point-of-view-invariant” in that, as he\nclaims, the laws “will be the same in any universe where no\nspecial point of view is present” (p. 91). Luke Barnes\ncriticizes this claim (2012: sect. 4.1), arguing that it relies on\nconfusingly identifying point-of-view-invariance with non-trivial\nsymmetry properties that the laws in our universe happen to exhibit.\nNotably, as Barnes emphasizes, neither general relativity nor the\nStandard Model of elementary particle physics are without conceptually\nviable, though perhaps empirically disfavoured, alternatives. \nA further criticism by Stenger is that considerations according to\nwhich the conditions in our universe are fine-tuned for life routinely\nfail to consider the consequences of varying more than one parameter\nat a time. In response to this criticism, Barnes (2012: sect. 4.2)\ngives an overview of various studies such as Barr and Khan 2007 and\nTegmark et al. 2006 that explore the complete parameter space of\n(segments of) the Standard Model and arrives at the conclusion that\nthe life-permitting range in multidimensional parameter space is\nlikely very small. \nBiological organisms are fine-tuned for life in the sense that their\nability to solve problems of survival and reproduction depends\ncrucially and sensitively on specific details of their behaviour and\nphysiology. For example, many animals rely on their visual apparatus\nto spot prey, predators, or potential mates. The proper functioning of\ntheir visual apparatus, in turn, depends sensitively on physiological\ndetails of their eyes and brain. \nBiological fine-tuning has a long tradition of being regarded as\nevidence for divine design (Paley 1802), but modern biology regards it\nas the product of Darwinian evolution, notably as driven by natural\nand sexual selection. Relatively recently, some researchers have\nclaimed that some specific “fine-tuned” features of\norganisms cannot possibly be the outcomes of Darwinian evolutionary\ndevelopment alone and that interventions by some designer must be\ninvoked to account for them. For example, Michael Behe (1996) claims\nthat the so-called flagellum, a bacterial organ that enables\nmotion, is irreducibly complex in the sense that it cannot be\nthe outcome of consecutive small-scale individual evolutionary steps,\nas they are allowed by standard, Darwinian, evolutionary theory. In a\nsimilar vein, William Dembski (1998) argues that some evolutionary\nsteps hypothesized by Darwinian are so improbable that one would\nnot rationally expect them to occur even once in a volume the size of\nthe visible universe. Behe and Dembski conclude that an intelligent\ndesigner likely intervened in the evolutionary course of events. \nThe overwhelming consensus in modern biology is that the challenges to\nDarwinian evolutionary theory brought forward by Behe, Dembski and\nothers can be met. According to Kenneth Miller (1999), Behe’s\narguments fail to establish that there are no plausible small-step\nevolutionary paths which have Behe’s allegedly\n“irreducibly complex” features as outcomes. For example,\nas Miller argues, there is in fact strong evidence for a Darwinian\nevolutionary history of the flagellum and its constituents (Miller\n1999: 147–148). \nMany researchers believe that the fine-tuning of the universe’s\nlaws, constants, and boundary conditions for life calls for inferring\nthe existence of a divine designer (see\n Section 3)\n or a multiverse—a vast collection of universes with differing\nlaws, constants, and boundary conditions (see\n Section 4).\n The inference to a divine designer or a multiverse typically rests on\nthe idea that, in view of the required fine-tuning, life-friendly\nconditions are in some sense highly improbable if there is\nonly one, un-designed, universe. It is controversial, however, whether\nthis idea can coherently be fleshed out in terms of any philosophical\naccount of probability. \nConsiderations as reviewed in\n Section 1.1\n according to which the laws, constants and boundary conditions in our\nuniverse are fine-tuned for life are based on investigations of\nphysical theories and their parameter spaces. It may therefore seem\nnatural to expect that the relevant probabilities in the light of\nwhich fine-tuning for life is improbable will be physical\nprobabilities. On closer inspection, however, it is difficult to see\nhow this could be the case: according to the standard view of physical\npossibility, alternative physical laws and constants are physically\nimpossible by the definition of physical possibility (Colyvan et al.\n2005: 329). Accordingly, alternative laws and constants trivially have\nphysical probability zero, whereas the actual laws and constants have\nphysical probability one. If the laws and constants that physics has\nso far determined turned out to be merely effective laws and constants\nfixed by some random process in the early universe which might be\ngoverned by more fundamental physical laws, it would start to make\nsense to apply the concept of physical probability to those effective\nlaws and constants (Juhl 2006: 270). However, the fine-tuning\nconsiderations as outlined in\n Section 1.1\n do not seem to be based on speculations about any such process, so\nthey do not seem to implicitly rely on the notion of physical\nprobability in that sense. \nAttempts to apply the notion of logical probability to\nfine-tuning for life are beset with difficulties as well. Critics\nargue that, from a logical point of view, arbitrary real numbers are\npossible values of the constants (McGrew et al. 2001; Colyvan et al.\n2005). According to them, any probability measure over the real\nnumbers as values of the constants that differs from the uniform\nmeasure would be arbitrary and unmotivated. The uniform measure\nitself, however, assigns zero probability to any finite interval. By\nthis standard, the life-permitting range, if finite, trivially has\nprobability zero, which would mean that life-friendly constants are\nhighly improbable whether or not fine-tuning in the sense of\n Section 1.1\n is required for life. This conclusion seems counterintuitive, but\nKoperski (2005) argues that it is not as unacceptable for proponents\nof the view that life-friendly conditions are improbable and require a\nresponse as it may initially seem. \nMotivated by the difficulties that arise in attempts to apply the\nphysical and logical notions of probability to fine-tuning for life,\ncontemporary accounts often appeal to an essentially epistemic notion\nof probability (e.g., Monton 2006; Collins 2009). According to these\napproaches, life-friendly conditions are improbable in that we would\nnot rationally expect them. An obvious problem for this view is that\nlife-friendly conditions are not literally unexpected for us: as a\nmatter of fact, we have long been aware that the conditions are right\nfor life in our universe, so the epistemic probability of\nlife-friendly conditions appears to be trivially \\(1\\). As Monton\n(2006) highlights, to make sense of the idea that life-friendly\nconditions are improbable in an epistemic sense, we must find a way of\nstrategically abstracting from some of our background knowledge,\nnotably from our knowledge that life exists, and assess the\nprobability of life’s existence from that perspective. (See\n Section 3.3\n for further discussion.) \nViews according to which life-friendly conditions are epistemically\nimprobable face the challenge to provide reasons as to why we\nshould not expect life-friendly conditions from an epistemic\nperspective which ignores that life exists. One response to this\nchallenge is to point out that there is no clear systematic pattern in\nthe actual, life-permitting, combination of values of the constants\n(Donoghue 2007: sect. 8), which suggests that this combination is\ndisfavoured in terms of elegance and simplicity. Another response is\nto appeal to the criterion of naturalness (see\n Section 5),\n which would lead one to expect values for at least two constants of\nnature—the cosmological constant and the mass of the Higgs\nparticle—which differ radically from the actual ones. Neither\nelegance and simplicity nor naturalness dictate any specific\nprobability distribution over the values of the constants, however,\nlet alone over the form of the laws itself. But proponents of the view\nthat fine-tuning for life is epistemically improbable can appeal to\nthese criteria to argue that life-friendly conditions will be ascribed\nvery low probability by any probability distribution that respects\nthese criteria. \nEven if fine-tuned conditions are improbable in some substantive\nsense, it might be wisest to regard them as primitive coincidences\nwhich we have to accept without resorting to such speculative\nresponses as divine design or a multiverse. It is indeed\nuncontroversial that being improbable does not by itself automatically\namount to requiring a theoretical response. For example, any specific\nsequence of outcomes in a long series of coin tosses has low initial\nprobability (namely, \\(2^{-N}\\) if the coin is fair, which approaches\nzero as the number \\(N\\) of tosses increases), but one would not\nreasonably regard any specific sequence of outcomes as calling for\nsome theoretical response, e.g., a re-assessment of our initial\nprobability assignment. The same attitude is advocated by Gould (1983)\nand Carlson and Olsson (1998) with respect to fine-tuning for life.\nLeslie concedes that improbable events do not in general call for an\nexplanation, but he argues that the availability of reasonable\ncandidate explanations of fine-tuning for life—namely, the\ndesign hypothesis and the multiverse hypothesis—suggests that we\nshould not “dismiss it as how things just happen to be”\n(Leslie 1989: 10). Views similar to Leslie’s are defended by van\nInwagen (1993), Bostrom (2002: 23–41), and Manson and Thrush\n(2003: 78–82). \nCory Juhl (2006) argues along independent lines that we should not\nregard fine-tuning for life as calling for a response. According to\nJuhl, forms of life are plausibly “causally ramified” in\nthat they “causally depend, for [their] existence, on a large\nand diverse collection of logically independent facts” (2006:\n271). He argues that one would expect “causally ramified”\nphenomena to depend sensitively on the values of potentially relevant\nparameters such as, in the case of life, the values of the constants\nand boundary conditions. According to him, fine-tuning for life\ntherefore does not require “exotic explanations involving\nsuper-Beings or super-universes” (2006: 273). \nThe sense in which fine-tuning for life fails to be\nsurprising according to Juhl differs from the sense in which it\nis surprising according to authors such as Leslie, van\nInwagen, Bostrom, Manson and Thrush: while the latter hold that\nlife-friendly conditions are rationally unexpected from an epistemic\npoint of view which sets aside our knowledge that life exists, Juhl\nholds that—given our knowledge that life exists and is\ncausally ramified—it is unsurprising that life depends\nsensitively, for its existence, on the constants and boundary\nconditions. \nBiological fine-tuning for survival and reproduction, as marvellous as\nit often appears, is regarded as unmysterious by biologists because\nevolution as driven by natural and sexual selection can generate it\n(see\n Section 1.3).\n One may hope that, similarly, future developments in fundamental\nphysics will reveal principles or mechanisms which explain the\nlife-friendly conditions in our universe. \nThere are two different types of scenarios of how future developments\nin physics could realize this hope: first, physicists may hit upon a\nso-called theory of everything according to which, as\nenvisaged by Albert Einstein,  \nnature is so constituted that it is possible logically to lay down\nsuch strongly determined laws that within these laws only rationally\ncompletely determined constants occur (not constants, therefore, whose\nnumerical values could be changed without destroying the theory).\n(Einstein 1949: 63)  \nEinstein’s idea is that, ultimately, the laws and constants of\nphysics will turn out to be dictated completely by fundamental general\nprinciples. This would make considerations about alternative laws and\nconstants obsolete, and thereby undermine any perspective according to\nwhich these are fine-tuned for life. \nUnfortunately, developments in the last few decades have not been kind\nto hopes of the sort expressed by Einstein. In the eyes of many\nphysicists, string theory is still the most promising\ncandidate “theory of everything” in that it potentially\noffers a unified account of all known forces of nature, including\ngravity. (See Susskind 2005 for a popular introduction, Rickles 2014\nfor a philosopher’s historical account and Dawid 2013 for a\nrecent, favourable, methodological appraisal.) But according to our\npresent understanding of string theory, the theory has an enormous\nnumber of lowest energy states, or vacua, which would\nmanifest themselves at the empirical level in terms of radically\ndifferent effective physical laws and different values of the\nconstants. These would be the laws and constants that we have\nempirical access to, and so string theory would not come close to\nuniquely determining the laws and constants in the manner envisaged by\nEinstein. \nA second type of scenario according to which future developments in\nphysics may eliminate at least some fine-tuning for life would be a\ndynamical account of the generation of life-friendly\nconditions, in analogy to the Darwinian “dynamical”\nevolutionary account of biological fine-tuning for survival and\nreproduction. Inflationary cosmology (Guth 1981, 2000) is a paradigm\ncandidate example of such an account in that it dynamically explains\nwhy the total cosmic energy density \\(\\Omega\\) in the early universe\nis extremely close to the so-called critical value \\(\\Omega_c\\) (see\n Section 1.1)—or,\n equivalently, why the overall spatial curvature of the universe is\nclose to zero. According to inflationary cosmology, the very early\nuniverse undergoes a period of exponential or near-exponential\nexpansion (“inflation”) which effectively flattens out\nspace and results in near-zero post-inflation curvature, leading to a\ntotal energy density \\(\\Omega\\) extremely close to the critical\ndensity \\(\\Omega_c\\). Further claimed achievements of inflationary\ncosmology include its ability to account for the observed near-perfect\nisotropy of the universe and the absence of magnetic monopoles. The\nstrongest empirical support for inflationary cosmology, however, is\nnow widely believed to come from of its apparently correct predictions\nof the shape of the cosmic microwave background fluctuations (PLANCK\ncollaboration 2014). \nInflationary cosmology’s attractions notwithstanding, its\nsuggested achievements are not universally recognized. (See Steinhardt\n& Turok [2008] for harsh criticism by two eminent cosmologists and\nEarman & J. Mosterín [1999] and McCoy [2015] for critical\nappraisals by philosophers.) However, even if its dynamical accounts of\nthe flatness, isotropy, and absence of magnetic monopoles in the early\nuniverse are correct, there is little reason to accept that similar\naccounts will be forthcoming for many other constants, boundary\nconditions, or even laws of nature that seem fine-tuned for life:\nwhereas, notably, the critical energy density \\(\\Omega_c\\) has\nindependently specifiable dynamical properties that characterize it as\na systematically distinguished value of the energy density \\(\\Omega\\),\nthe actual values of most other constants and parameters that\ncharacterize boundary conditions are not similarly distinguished and\ndo not form any clear systematic pattern (Donoghue 2007: sect. 8).\nThis makes it difficult to imagine that future physical theories will\nindeed reveal dynamical mechanisms which inevitably lead to these\nvalues (Lewis & Barnes 2016: 181f.). \nA classic response to the observation that the conditions in our\nuniverse seem fine-tuned for life is to infer the existence of a\ncosmic designer who created life-friendly conditions. If one\nidentifies this designer with some supernatural agent or God, the\ninference from fine-tuning for life to the existence of a designer\nbecomes a version of the teleological argument. Indeed, many regard\nthe argument from fine-tuning for a designer as the strongest version\nof the teleological argument that contemporary science affords. \nExpositions of the argument from fine-tuning for design are typically\ncouched in terms of probabilities (e.g., Holder 2002; Craig 2003;\nSwinburne 2004; Collins 2009), see also the review Manson 2009. An\nelementary Bayesian formulation considers the rational impact of the\nobservation \\(R\\)—that the constants (and laws and boundary\nconditions) are right for life—on our degree of belief\nconcerning the design hypothesis \\(D\\)—that there is a cosmic\ndesigner. According to standard Bayesian conditioning, our posterior\ndegree of belief \\(P^+(D)\\) after taking into account \\(R\\)\nis given by our prior conditional degree of belief \\(P(D\\mid R)\\).\nAnalogously, our posterior \\(P^+(\\neg D)\\) that there is no cosmic\ndesigner is given by our prior conditional degree of belief \\(P(\\neg\nD\\mid R)\\). By Bayes’ theorem, the ratio between the two posteriors\nis \n\n\\[\\begin{equation}\n\\label{simpledes}\n\\frac{P^+(D)}{P^+(\\neg D)} = \\frac{P(D\\mid R)}{P(\\neg D\\mid R)} = \\frac{P(R\\mid D)}{P(R\\mid \\neg D)} \\frac{P(D)}{P(\\neg D)}\\,. \n\\end{equation}\n\\]\n\n Proponents of the argument from fine-tuning for design\nargue that, in view of the required fine-tuning, life-friendly\nconditions are highly improbable if there is no divine designer. Thus,\nthe conditional probability \\(P(R\\mid \\neg D)\\) should be set close to\nzero. In contrast, it is highly likely according to them that the\nconstants are right for life if there is indeed a designer. Thus the\nconditional probability \\(P(R\\mid \\neg D)\\) should be given a value not\nfar from \\(1\\). If a sufficiently powerful divine being\nexists—the idea goes—it is only to be expected that she/he\nwill be interested in creating, or at least enabling, intelligent\nlife, which means that we can expect the constants to be right for\nlife on that assumption. This motivates the likelihood inequality  \nwhich expresses that life-friendly conditions confirm the designer\nhypothesis and which likelihoodists such as Sober (2003) regard as at\nthe core of the argument from fine-tuning for design. \nBayesians focus not only on likelihoods but also on priors and\nposteriors, and in their eyes the crucial significance of the\ninequality \\(\\eqref{likelihoods}\\) is that it leads to a ratio\n\\(P^+(D)/P^+(\\neg D)\\) of posteriors that is much larger than the\nratio \\(P(D)/P(\\neg D)\\) of the priors. Whether belief in a designer\nis rational depends ultimately on the priors as well, but unless those\nvalues dramatically favour \\(\\neg D\\) over \\(D\\) in that \\(P(\\neg\nD)\\gg P(D)\\), the posteriors will favour design in that \\(P^+(D)>\nP^+(\\neg D)\\). Bayesian proponents of the argument from fine-tuning\nfor design conclude that our degree of belief in the existence of some\ndivine designer should be greater than \\(1/2\\) in view of the fact\nthat there is life, given the required fine-tuning. \nWe could not possibly have existed in conditions that are incompatible\nwith the existence of observers. The famous weak anthropic\nprinciple (WAP) (Carter 1974) suggests that this apparently\ntrivial point may have important consequences: \n[W]e must be prepared to take account of the fact that our location in\nthe universe is necessarily privileged to the extent of being\ncompatible with our existence as observers. (Carter 1974: 293,\nemphasis due to Carter) \nOur methods of empirical observation are unavoidably biased\ntowards detecting conditions which are compatible with the existence\nof observers. For example, even if life-hostile places vastly\noutnumber life-friendly places in our universe, we should not be\nsurprised to find ourselves in one of the relatively few places that\nare life-friendly and seek an explanation for this finding, simply\nbecause—in virtue of being living organisms—we could not\npossibly have found ourselves in a life-hostile place. Biases that\nresult from the fact that what we observe must be compatible with the\nexistence of observers are referred to as observation selection\neffects. The observation selection effects emphasized by the weak\nanthropic principle with respect to location in the universe are\nemphasized by what Carter dubs the strong anthropic principle\n(SAP) with respect to the universe as a whole: \n[T]he Universe (and hence the fundamental parameters on which it\ndepends) must be such as to admit within it the creation of observers\nwithin it at some stage. (Carter 1974: 294) \nCarter’s formulation of the SAP has led some authors, most\ninfluentially Barrow and Tipler (1986), to misinterpret it along\nteleological lines and as thereby categorically different from the\nWAP. But, as Carter himself highlights (1983: 352), see also Leslie\n(1989: 135–145), the SAP is meant to highlight exactly the same\ntype of bias as the WAP and is literally stronger than the WAP only\nwhen conjoined with a version of the multiverse hypothesis. \nThe so-called anthropic objection against the argument from\nfine-tuning for design argues that that argument breaks down once our\nbiasedness due to the observation selection effects emphasized by the\nweak and strong anthropic principles is taken into account. Elliott\nSober (2003, 2009) advocates this objection. According to him, the\nargument from fine-tuning for design requires not the likelihood\ninequality \\(\\eqref{likelihoods}\\) but the much more problematic  \nwhere “OSE” stands for “observation\nselection effect”. Sober himself spells out OSE as\n“We exist, and if we exist, the constants must be right”\n(2003: 44). According to this interpretation, \\(\\eqref{ose}\\) is\npatently false: our existence as living organisms entails that the\nconstants are right for life, which means that the terms on both sides\nof \\(\\eqref{ose}\\) are trivially \\(1\\) and hence equal, so\n\\(\\eqref{ose}\\) does not hold on Sober’s analysis. \nCritics of the anthropic objection argue that Sober’s reasoning\ndelivers highly implausible results when transferred to examples where\nrational inferences are less controversial. Most famous is\nLeslie’s firing squad (Leslie 1989: 13f.), in which a prisoner\nexpects to be executed by a firing squad but, to his own surprise,\nfinds himself alive after all the marksmen have fired and wonders\nwhether they intended to miss. The firing squad scenario involves an\nobservation selection effect because the prisoner cannot contemplate\nhis post-execution situation unless he somehow survives the execution.\nHis observations, in other words, are “biased” towards\nfinding himself alive (see Juhl [2007] and Kotzen [2012] for further\nuseful examples). Sober’s analysis, applied to the firing squad\nscenario, suggests that it would not be rational for the prisoner to\nsuspect that the marksmen intended to miss (unless independent\nevidence suggests so) because that would mean overlooking the\nobservation selection effect that he faces. But, as Leslie, Weisberg\n(2005) and Kotzen (2012) argue, this recommendation seems very\nimplausible. \nAccording to Weisberg, Sober’s analysis fails due to its\nincorrect identification of the observation selection effect\nOSE with “We exist, and if we exist, the constants must\nbe right”. Weisberg argues that the weaker, purely conditional,\nstatement “If we exist, the constants must be right”\n(Weisberg 2005: 819, Weisberg’s wording differs) suffices to\ncapture the observation selection effect. But if we interpret\n“OSE” as this statement, there is no reason to\nsuppose that the inequality \\(\\eqref{ose}\\) fails and the argument\nfrom fine-tuning for design appears vindicated inasmuch as the\nanthropic objection is concerned. (See Sober 2009 for Sober’s\nresponse.) \nTo resolve the difficulty of accommodating observation selection\neffects in likelihood arguments, Kotzen (2012) suggests that bias due\nto such effects be taken into account as evidence rather than\nbackground information. Notably, instead of \\(\\eqref{ose}\\) Kotzen\nproposes to consider  \nwhere \\(I\\) contains information about the observation process,\nincluding observation selection effects (Kotzen 2012: 835). According\nto this analysis, the argument from fine-tuning for design can be\nsaved from the anthropic objection for a variety of ways to spell out\nthe information \\(I\\) about the observation process and anthropic\nbias. \nViews according to which life-friendly conditions are improbable in an\nepistemic sense due to the required fine-tuning are challenged to come\nto terms with the fact that, as a matter of fact, we have long known\nthat our universe is life-friendly, which means that life-friendly\nconditions are not literally unexpected for us. As a\nconsequence of this fact, the Bayesian version of the argument from\nfine-tuning for a designer as outlined in\n Section 3.1\n must adopt some solution to Bayesianism’s notorious problem\nof old evidence (Glymour 1980) because \\(R\\)—that the\nconstants are right for life—is inevitably old evidence\nfor us. \nAn obvious choice, endorsed by Monton (2006), who is critical of the\nargument from fine-tuning for design, and Collins (2009), who supports\nit, is the so-called counterfactual or ur-probability\nsolution to the problem of old evidence, as defended by Howson (1991).\nThe main advantage of this solution as applied to the argument from\nfine-tuning for design is that it allows to essentially preserve the\nargument, including \\(\\eqref{simpledes}\\) and \\(\\eqref{likelihoods}\\),\nwith the sole refinement that one must consistently construe all prior\nprobabilities \\(P(\\cdot)\\), conditional and unconditional, as\n“ur-probabilities”, i.e., rational credences of some\ncounterfactual epistemic agent who is unaware that the constants are\nright for life. Somewhat bizarrely, as Monton points out (2006: 416),\nsuch an agent would have to be at least temporarily unaware of her/his\nexistence (or at least her/his existence as a form of life) because\notherwise she/he could not possibly be unaware that the conditions are\nright for life. Tentative suggestions concerning the background\nknowledge that can reasonably be ascribed to such an agent are\ndeveloped by Monton (2006: sect. 4) and Collins (2009: sect. 4.3). \nAn advantage of approaching the argument from fine-tuning for design\nusing the ur-probability solution is that it offers proponents of the\nargument a clear-cut rejection of the anthropic objection: as in\nKotzen’s (2012) approach, the fact that we exist is treated not\nas background knowledge but as evidence taken into account by Bayesian\nconditioning. The appropriate comparison between likelihoods to\nconsider is thus not Sober’s \\(\\eqref{ose}\\)—at least not\nunder Sober’s own interpretation of “OSE”\nas including “We exist”—but rather\n\\(\\eqref{likelihoods}\\) or \\(\\eqref{ose_kotzen}\\), both of which evade\nthe anthropic objection. \nThe likelihood inequality \\(\\eqref{likelihoods}\\) on which the\nfine-tuning argument for design rests is based on the assumption that,\nreasonably, \\(P(R\\mid \\neg D)\\) is very small because life-friendly\nconditions are improbable if there is no designer. This assumption can\nbe challenged, as already discussed in\n 2.1.\n But the likelihood inequality \\(\\eqref{likelihoods}\\) also rests on\nthe assumption that \\(P(R\\mid D)\\) is comparatively large, i.e., on the\nview that, if there is indeed a designer, life-friendly conditions are\nmore to be expected than if there is no designer. This assumption can\nbe challenged as well. \nReasonable assignments of \\(P(R\\mid D)\\) depend on how exactly the\ndesigner hypothesis \\(D\\) is spelled out. According to Swinburne, the\nmost promising candidate designer is “the God of traditional\ntheism” whom he characterizes as “a being essentially\neternal, omnipotent (in the sense that He can do anything logically\npossible), omniscient, perfectly free, and perfectly good”\n(2003: 107). Swinburne argues that we can be at least moderately\nconfident that the God of traditional theism, if he exists,\n“will bring about an orderly, spatially extended, world in which\nhumans have a location” (2003: 113; note that Swinburne operates\nwith a generalized, non-biological concept of “humans”).\nNotably, life-friendly conditions, conditional on the existence of the\nGod of traditional theism, do not have very low probability according\nto Swinburne, i.e., \\(P(R\\mid D)\\) is not many orders of magnitude smaller\nthan \\(1\\). This allows Swinburne to argue that\n\\(\\eqref{likelihoods}\\) is indeed fulfilled for reasonable probability\nassignments. \nCriticisms of the view that life-friendly constants are to be expected\nif there is a designer have a long tradition and go back to John Venn\n(1866) and John Maynard Keynes (1921). More recently, Sober has voiced general reservations about our abilities to competently judge what a divine designer, if real, would do: \nOur judgements about what counts as a sign of intelligent design must\nbe based on empirical information about what designers often do and\nwhat they rarely do. As of now, these judgements are based on our\nknowledge of human intelligence. The more our hypotheses of\nintelligent designers depart from the human case, the more in the dark\nwe are as to what the ground rules are for inferring intelligent\ndesign. (Sober 2003: 38) \nIn a similar spirit, Narveson complains that we are in no position to\npredict how a cosmic designer would behave because “[b]odiless\nminded super-creators are a category that is way, way out of\ncontrol” (Narveson 2003: 99). According to Sober and Narveson it\nis particularly problematic for theists to confidently assume that\nGod, if he exists, would create life-friendly conditions and, at the\nsame time, react to the problem of evil by highlighting our inability\nto understand “the mysterious ways of the Deity” (Narveson\n2003: 99). \nOne can construct versions of the designer hypothesis \\(D\\) that are\ntailored to fulfil the likelihood inequality \\(\\eqref{likelihoods}\\)\nby defining the designer as a being with both the intention\nand ability to create life-friendly conditions. However, one may\nquestion whether such tailored versions of the designer hypothesis\nhave sufficient independent motivation and plausibility to deserve\nserious consideration in the first place. To use Bayesian terms, one\nmay hesitate to ascribe them non-negligible prior probabilities\n\\(P(D)\\). \nMotivating a non-negligible prior \\(P(D)\\) for design is especially\nchallenging in the framework of the ur-probability solution to the\nproblem of old evidence because it constrains the background evidence\nto facts that do not entail the existence of life. Collins argues that\nif we focus only on a limited class of constants \\(C\\), the background\nevidence that we can use to motivate the prior \\(P(D)\\) is allowed to\n“includ[e] the initial conditions of the universe, the laws of\nphysics, and the values of all the other constants except C”.\nBut appeals to the sacred texts of religions cannot be used to\nmotivate the ascription of a non-negligible ur-prior \\(P(D)\\) because\nthey presuppose, and thus entail, the existence of life. Notably, as\npointed out by Monton, “[i]n formulating an urprobability for\nthe existence of God, one cannot take into account Biblical accounts\nabout Jesus” (2006: 418). According to Monton (2006: 419),\nproponents of the argument from fine-tuning for design may, however,\ntry to motivate a non-negligible ur-prior \\(P(D)\\) by resorting to\narguments for the existence of God that are either a priori, e.g.,\nontological argument, or appeal only to very general empirical facts\nthat do not entail that the conditions are right for life, e.g., the\ncosmological argument. According to Swinburne (2004: ch. 5), the\nhypothesis of traditional theism is a simple one and, as such,\nwarrants the ascription of a non-negligible prior. \nThe argument from fine-tuning for design as reviewed in\n Section 3.1\n treats the fact that life requires fine-tuned conditions as\nbackground knowledge and assesses the evidential significance of the\nobservation that life-friendly conditions obtain against that\nbackground. An alternative argument from fine-tuning for design,\nexplored by John Roberts (2012) and independently investigated and\nendorsed by Roger White (2011) in a reply to Weisberg (2010), treats\nour knowledge that the conditions are right for life as background\ninformation and assesses the rational impact of physicists’\ninsight that life requires fine-tuned conditions against this\nbackground. An advantage of this alternative is that it fits better\nwith our actual epistemic situation: that the conditions are right for\nlife is something we have known for a long time; our actual new\nevidence is that the laws of physics—as White (2011) and\nWeisberg (2012) put it—are stringent rather than\nlax in the constraints that they impose on the constants and\nboundary conditions if there is to be life. \nThe central likelihood inequality around which White’s version\nof the argument revolves is  \nwhere “\\(D\\)” is, again, the designer hypothesis,\n“\\(S\\)” is the proposition that the laws are stringent\n(i.e., that life requires delicate fine-tuning of the constants) and\n“\\(O\\)” is our background knowledge that life exists\n(White 2011: 678). (See Roberts [2012: 296] for an assumption that\nplays an analogous role as \\(\\eqref{alternative}\\).) The inequality\n\\(\\eqref{alternative}\\) expresses the statement that stringent laws\nconfirm the designer hypothesis, given our background knowledge that\nlife exists. Does it plausibly hold for reasonable probability\nassignments? White argues that it does and supports this claim by\ngiving a rigorous derivation of \\(\\eqref{alternative}\\) from\nassumptions that he regards as plausible. Crucial among them is the\ninequality  \nwhich White motivates by arguing that “the fact that the laws\nput stringent conditions on life does not by itself provide any\nevidence against design” (White 2011: 678). Put\ndifferently, according to White, absent information that life exists,\ninformation that the laws are stringent does at least not speak\nagainst the existence of a designer. \nWeisberg (2012) criticizes \\(\\eqref{alternative1}\\)—and takes\nhis criticism to undermine \\(\\eqref{alternative}\\)—arguing that\nit is implausible by the design theorist’s own standards. The\ndesign theorist holds a combination of views according to which, on\nthe one hand, life is more probable if there is a designer than if\nthere is no designer and life is less probable if the laws are\nstringent rather than lax. If one adds to this combination of views\nthe assumption that none of the possible life-friendly conditions has\nhigher probability than the others, both if there is a designer and if\nthere is no designer, it dictates that—bracketing knowledge that\nlife exists—stringent laws speak against the existence\nof a designer, i.e., it dictates \\(P(D\\mid S) < P(D\\mid \\neg S)\\), contrary\nto \\(\\eqref{alternative1}\\). Absent any evidence that life exists,\nevidence that the laws are stringent speaks against the existence of\nlife in that stringent laws make life unexpected. \nA possible response for the design theorist, anticipated by Weisberg\n(2012: 713), would be to support \\(\\eqref{alternative1}\\) by arguing\nthat the designer would plausibly first choose either\nstringent or lax laws, sidestepping her intention to enable the\nexistence of life at that stage or actively preferring stringent laws,\nand only then choosing life-friendly constants. A problem\nwith this response, similar to the difficulties discussed in\n Section 3.4,\n is that we have little experience with cosmic designers and,\ntherefore, difficulties to predict the hypothesized designer’s\npreferences and likely actions. \nAccording to the multiverse hypothesis, there are multiple universes,\nsome of them radically different from our own. Many of those who\nbelieve that fine-tuning for life requires some theoretical response\nregard it as the main alternative beside the designer hypothesis. The\nidea that underlies it is that, if there is a sufficiently diverse\nmultiverse in which the conditions differ between universes, it is\nonly to be expected that there is at least one where they are right\nfor life. As the strong anthropic principle highlights (see\n Section 3.2),\n the universe in which we, as observers, find ourselves must be one\nwhere the conditions are compatible with the existence of observers.\nThis suggests that, on the assumption that there is a sufficiently\ndiverse multiverse, it is neither surprising that there is at least\none universe that is hospitable to life nor—since we could not\nhave found ourselves in a life-hostile universe—that we find\nourselves in a life-friendly one. Many physicists (e.g., Susskind\n[2005], Greene [2011], Tegmark [2014]) and philosophers (e.g., Leslie\n[1989], Smart [1989], Parfit [1998], Bradley [2009]) regard this line\nof thought as suggesting the inference to a multiverse as a rational\nresponse to the finding that the conditions are right for life in our\nuniverse despite the required fine-tuning. \nThe argument from fine-tuning for the multiverse as just sketched is\nsometimes characterized as an inference to the multiverse as the best\nexplanation of fine-tuning for life—an explanation which, in\nview of its appeal to anthropic reasoning, is sometimes characterized\nas “anthropic” (e.g., Leslie 1986, 1989: ch. 6; McMullin\n1993: 376f., sect. 7; Bostrom 2002). It is controversial, however,\nwhether this characterization is adequate. A paradigmatic anthropic\n“explanation”, characterized as such by Carter in the\nseminal paper (1974) that introduces the anthropic principles, is\nastrophysicist Robert Dicke’s (1961) account of coincidences\nbetween large numbers in cosmology. A prominent example of such a\ncoincidence is that the relative strength of electromagnetism and\ngravity as acting on an electron/proton pair is of roughly the same\norder of magnitude (namely, \\(10^{40}\\)) as the age of the universe,\nmeasured in natural units of atomic physics. Impressed by this and\nother coincidences, Dirac (1938) stipulated that they might hold\nuniversally and as a matter of physical principle. He conjectured that\nthe strength of gravity may decrease as the age of the universe\nincreases, which would indeed make it possible for the coincidence to\nhold across all cosmic times. \nDicke (1961), criticizing Dirac, argues that standard cosmology with\ntime-independent gravity suffices to account for the coincidence,\nprovided that we take into account the fact that our existence is tied\nto the presence of mainline stars like then sun and of various\nchemical elements produced in supernovae. As Dicke shows, this\nrequirement dictates that we could only have found ourselves in that\ncosmic period in which the coincidence holds. Accordingly, there is no\nneed to assume, as suggested by Dirac, that gravity varies with time\nto make the coincidence unsurprising. Carter (1974) and Leslie (1986,\n1989: ch. 6) describe Dicke’s account as an “anthropic\nexplanation” of the coincidence that impressed Dirac, and Leslie\ndiscusses it continuously with the argument from fine-tuning for the\nmultiverse. (Earman [1987: 309], however, disputes that Dicke’s\naccount is adequately characterized as an “explanation”.)\nBut whereas Dicke’s account of the coincidence uses life’s\nexistence as background knowledge to show that standard cosmology\nsuffices to make the coincidence expectable, the argument from\nfine-tuning for the multiverse treats life’s existence as\nrequiring a theoretical response (rather than as background knowledge)\nand advocates the multiverse hypothesis as the best such response. \nMore often than as an inference to the best explanation the argument\nfrom fine-tuning for the multiverse is formulated using probabilities,\nin analogy to the argument from fine-tuning for design (see\n Section 3.1).\n In a simple version of the argument, reasonable probability\nassignments are compared for a single-universe hypothesis \\(U\\) (where\nthe universe has uniform laws and constants) and a rival multiverse\nhypothesis \\(M\\) according to which there are many universes with\nconditions that differ between universes. (For the purposes of the\ndiscussion about fine-tuning for life, hypotheses according to which\nthere is only a single universe with constants that vary across\nspace-time qualify as versions of the multiverse hypothesis. They seem\nto be disfavoured by the available evidence, however, see Uzan 2003\nfor a review.) \nAs in Bradley 2009, we consider as the fine-tuning evidence the\nproposition \\(R\\) that there is (at least) one universe with the right\nconstants for life. Using Bayesian conditioning and Bayes’\ntheorem one obtains for the ratio of the posteriors  \nIf the multiverse according to \\(M\\) is sufficiently vast and varied,\nlife unavoidably appears somewhere in it, so the conditional prior\n\\(P(R\\mid M)\\) must be \\(1\\) (or very close to \\(1\\)). If we assume that,\non the assumption that there is only a single universe, it is\nimprobable that it has the right conditions for life (see\n Section 2.1\n for discussion), the conditional prior \\(P(R\\mid U)\\) must be much\nsmaller than \\(1\\). This gives \\(P(R\\mid M) \\gg P(R\\mid U)\\), which entails\n\\(P(R\\mid M)/P(R\\mid U)\\gg1\\), which in turn entails a ratio of posteriors\nthat is much larger than the ratio of the priors:\n\\(\\frac{P(M\\mid R)}{P(U\\mid R)}\\gg\\frac{P(M)}{P(U)}\\). Unless we have prior\nreasons to dramatically prefer a single universe over the multiverse,\ni.e., unless \\(P(U)\\gg P(M)\\), the ratio of the posteriors\n\\(\\frac{P^+(M)}{P^+(U)}\\) will be larger than \\(1\\). \nJust as the argument from fine-tuning for design, the argument from\nfine-tuning for the multiverse must come to terms with the problem\nthat the existence of life is old evidence for us. If one applies\nHowson’s ur-probability solution to it, one must consistently\ninterpret all the probabilities in equation \\(\\eqref{simplemult}\\) as\nassigned from the perspective of a counterfactual epistemic agent who\nis unaware of her/his own existence. At least prima facie, it is\nunclear what background knowledge can be assumed for an agent in that\ncurious condition (see\n Section 3.3\n for considerations). Juhl (2007) speculates that motivating a\nnon-negligible prior \\(P(M)\\) is impossible without implicitly relying\non evidence which entails that the conditions are right for life. If\nthis is correct, it means that running the fine-tuning argument for\nthe multiverse as in equation \\(\\eqref{simplemult}\\) based on an\nempirically well motivated non-negligible prior \\(P(M)\\) would\ninevitably involve fallacious double-counting\n(“double-dipping”, as Juhl calls it (2007: 554)) of the\nfine-tuning evidence \\(R\\). \nThe inverse gambler’s fallacy, identified by Ian Hacking (1987),\nconsists in inferring from an event with a remarkable outcome that\nthere have likely been many more events of the same type in the past,\nmost with less remarkable outcomes. For example, the inverse\ngambler’s fallacy is committed by someone who enters a casino\nand, upon witnessing a remarkable outcome at the nearest\ntable—say, a five-fold six in a quintuple die\ntoss—concludes that the toss is most likely part of a large\nsequence of tosses. Critics of the argument from fine-tuning for the\nmultiverse accuse it of committing the inverse gambler’s\nfallacy. According to them, the argument commits this fallacy by, as\nWhite puts it,  \nsupposing that the existence of many other universes makes it more\nlikely that this one—the only one that we have\nobserved—will be life-permitting. (White 2000: 263)  \nVersions of this criticism are endorsed by Draper et al. (2007) and\nLandsman (2016). Hacking (1987) regards only those versions of the\nargument from fine-tuning for the multiverse as guilty of the inverse\ngambler’s fallacy that infer the existence of multiple universes\nin a temporal sequence. \nAdherents of the inverse gambler’s fallacy charge against the\nargument from fine-tuning for the multiverse object against focusing\non the impact of the proposition \\(R\\)—that the conditions are\nright for life in some universe. According to them, we should\ninstead consider the impact of the more specific proposition \\(H\\):\nthat the conditions are right for life here, in this\nuniverse. If we replace \\(R\\) by \\(H\\), they argue, it becomes clear\nthat the argument breaks down because the existence of other universe\ndoes not raise the probability that this universe here is\nlife-friendly. \nMany philosophers defend the argument from fine-tuning for the\nmultiverse against this objection (McGrath 1988; Leslie 1988; Bostrom\n2002; Manson & Thrush 2003; Juhl 2005; Bradley 2009). In an early\nresponse to Hacking, McGrath (1988) argues that the analogy between\nthe argument from fine-tuning for the multiverse and a person who\nrandomly enters a casino and witnesses a remarkable outcome is\nmisleading: while the person entering a casino could have found any\narbitrary outcome, we could not have found ourselves in a universe\nwith conditions that are not right for life. The appropriate analogy\nto consider, according to McGrath, involves someone who is allowed to\nenter the casino only if and when some specific remarkable outcome\noccurs and who, upon being called in and finding that this outcome has\noccurred, infers the existence of other trials in the past. In\nthat scenario, the inference to multiple trials (in the past)\nis indeed rational, and so, according to McGrath, is the inference\nfrom fine-tuned conditions to multiple universes. \nThe adequacy of McGrath’s casino analogy is contested as well.\nWhereas in McGrath’s analogy the epistemic agent waits outside\nthe casino until the remarkable outcome occurs and she/he is called\nin, “it is not as though we were disembodied spirits waiting for\na big bang to produce some universe which could accommodate us”,\nas White puts it (2000: 268). Bradley (2009) endorses McGrath’s\nanalysis, offering further casino analogies that suggest rejecting the\ninverse gambler’s fallacy charge, but White’s diagnosis\ncontinues to find support, e.g., by Landsman (2016). \nAs just outlined, it is controversial whether it is rational to infer\nthe existence of multiple universes from our universe’s\nfine-tuning for life. However, if we had strong independent\nevidence for other universes with life-hostile conditions, attempts to\naccount for why our own universe is life-friendly would most likely\nseem futile. Thus independent evidence for some multiverse scenario\ncould have a strong impact on what we regard as a rational response to\nfine-tuning for life. Proponents of the argument from fine-tuning for\nthe multiverse could moreover welcome such evidence as potentially\nhelping to motivate a non-negligible prior \\(P(M)\\) for the\nmultiverse. \nMany physicists nowadays believe that a specific version of the\nmultiverse hypothesis is indeed suggested by contemporary developments\nin fundamental physics, notably by the combination of inflationary\ncosmology and string theory, both of which have been introduced in\n Section 2.3.\n According to many advocates of inflationary cosmology, the process of\ninflation results in causally isolated space-time regions, so-called\n“island universes”. This process is in general\n“eternal” in that the formation of island universes never\nends. As a result, it leads to the production of a vast (and,\naccording to most models, infinite) “multiverse” of island\nuniverses (Guth 2000). \nAs remarked in\n Section 2.3,\n string theory has an enormous number of lowest energy states (vacua),\nwhich would manifest themselves at the level of observations and\nexperiments in terms of different higher level physical laws and\nvalues of the constants. When combined with the idea of island\nuniverses as suggested by inflationary cosmology one obtains a\ncosmological picture in which there are infinitely many island\nuniverses where all the different string theory\nvacua—corresponding to different higher-level physical laws and\nconstants in these laws—are actually realized in the different\nisland universes. This so-called landscape multiverse\nqualifies as a concrete multiverse scenario in the sense of the\nargument from fine-tuning for the multiverse. A necessary condition is\nof course that the collection of island universes that are part of the\nlandscape multiverse includes, as is widely believed to be the case,\nat least one universe with the same effective (higher-level) laws and\nconstants as our own. \nUnfortunately, concrete multiverse scenarios such as the landscape\nmultiverse are extremely difficult to test, precisely because they\nentail that different universes exhibit very different conditions. The\nbroad consensus in the literature on multiverse cosmology is that, in\norder for a multiverse scenario to qualify as empirically confirmed,\nit must entail that those conditions that we find in our own universe\nare typical among those found by observers across the\nmultiverse. Widely used formulations of typicality are\nVilenkin’s principle of mediocrity (Vilenkin 1995) and\nBostrom’s self-sampling assumption (Bostrom 2002).\nTypicality principles can be regarded as refinements of the anthropic\nprinciples (Bostrom 2002) in the form of indifference principles\nof self-locating belief (Elga 2004): inasmuch as we are ignorant\nabout who and where among observers we are, they recommend to reason\nas if we were equally likely to be any of the observers who we\nmight possibly be, given our empirical evidence. \nTypicality principles have the benefit of making multiverse theories\nat least in principle testable (Aguirre 2007; Barnes 2017). They are\ncontroversial, however, because it is contested whether typicality is\nalways a reasonable assumption (Hartle & Srednicki 2007; Smolin\n2007) and because it is difficult to specify with respect to which\nreference class of observers typicality should be assumed (Friederich 2017). These\ndifficulties are exacerbated in cosmological scenarios such as the\nlandscape multiverse in which reference classes of observers that one\nmight reasonably choose are all infinite. The problem of regularizing\nthose infinities corresponds to the so-called measure problem\nof cosmology, according to some cosmologists “the greatest\ncrisis in physics today” (Tegmark 2014: 314). (See Schellekens\n[2013: sect. VI.B] for an introduction to the measure problem aimed at\nphysicists, Smeenk [2014] for a philosopher’s sceptical\nassessment of its solvability, and Dorr & Arntzenius [2017] for a\nmore optimistic perspective.) \nThe persisting difficulties with testing multiverse theories are a\nprime reason of why the multiverse idea itself continues to be viewed\nvery critically by many leading physicists (e.g., Ellis 2011). \nAccording to many contemporary physicists, the most deeply problematic\ninstances of fine-tuning do not concern fine-tuning for life but\nviolations of naturalness—a principle of theory choice\nin particle physics and cosmology that can be characterized as a\nno fine-tuning criterion. \nThe idea that underlies naturalness is that the phenomena\ndescribed by some physical theory should not depend sensitively on\nspecific details of a more fundamental (currently unknown) theory to\nwhich it is an effective low-energy approximation. In what follows,\nthe motivation, significance, and implementation of this idea in the\nframework of quantum field theory are explained. For a more detailed\nintroduction aimed at physicists see Giudice (2008), for one aimed at\nphilosophers of physics see Williams (2015). \nModern physics regards our currently best theories of particle physics\ncollected in the Standard Model as effective field theories.\nEffective field theories are low-energy effective approximations to\nhypothesized more fundamental physical theories whose details are\ncurrently unknown. An effective field theory has an in-built limit to\nits range of applicability, determined by some energy scale\n\\(\\Lambda\\), beyond which phenomena become relevant that are covered\nonly by the more fundamental theory to which the effective field\ntheory supposedly is a low-energy approximation. For the theories\ncollected in the Standard Model, it is known that they cannot possibly\nbe empirically adequate beyond energies around the Planck scale\n\\(\\Lambda_{\\textrm{Planck}}\\approx 10^{19} \\,\\textrm{GeV}\\),\nwhere—presently unknown—quantum gravitational effects\nbecome relevant. However, the Standard Model may well be empirically\ninadequate already at energy scales significantly below the Planck\nscale. For example, if there is some presently unknown particle with\nmass \\(M\\) smaller than the Planck scale \\(\\Lambda_{\\textrm{Planck}}\\)\nbut beyond the range of current accelerator technology which interacts\nwith particles described by the Standard Model, the cut-off scale\n\\(\\Lambda\\) where the Standard Model becomes inadequate may well be\n\\(M\\) rather than \\(\\Lambda_{\\textrm{Planck}}\\). \nIn an effective field theory, any physical quantity\n\\(g_{\\Fphys}\\) can be represented as the sum of a so-called\nbare quantity \\(g_0\\) and a contribution \\(\\Delta g\\) from vacuum\nfluctuations corresponding to energies up to the cut-off \\(\\Lambda\\):\n \nThe bare quantity \\(g_0\\) can be regarded as a black box that sums up\neffects associated with energies beyond the cut-off scale \\(\\Lambda\\)\nwhere unknown effects must be taken into account. Viewing a theory as\nan effective field theory means viewing it as a self-contained\ndescription of phenomena up to the cut-off scale \\(\\Lambda\\). This\nperspective suggests that one may only consider an effective theory as\nnatural if the physical quantity \\(g_{\\Fphys}\\) can be\nof its actual order of magnitude without any need for a delicate\ncancellation between \\(g_0\\) and \\(\\Delta g\\) to many orders of\nmagnitude. Since the bare quantity \\(g_0\\) sums up information about\nphysics beyond the cut-off scale \\(\\Lambda\\), such a delicate\ncancellation between \\(g_0\\) and \\(\\Delta g\\) would mean that the\norder of magnitude of the physical quantity \\(g_{\\Fphys}\\)\nwould be different if phenomena associated with energies beyond the\ncut-off scale \\(\\Lambda\\) were slightly different. \nOne can characterize violations of naturalness as instances of\nfine-tuning in that, where naturalness is violated,\nlow-energy phenomena depend sensitively on the details of some unknown\nfundamental theory concerning phenomena at very high energies.\nPhysicists have developed ways of quantifying fine-tuning in this\nsense (Barbieri & Guidice 1988), critically discussed by Grinbaum (2012). Note that the sense in which violations of naturalness qualify as instances of\nfine-tuning is very different from fine-tuning for life, as discussed\nin the previous sections. \nAn alternative criterion of naturalness—sometimes dubbed\nabsolute naturalness (see Wells [2015] for an empirical\nmotivation)—is that a theory is natural if and only if it can be\nformulated using dimensionless numbers that are all of order \\(1\\).\nMore permissive is ’t Hooft’s technical\nnaturalness criterion (’t Hooft 1980), according to which a\ntheory is natural if it can be formulated in terms of numbers that are\neither of order \\(1\\) or very small but such that, if they were\nexactly zero, the theory would have an additional symmetry. The\nmotivation for this prima facie arbitrary criterion is that it\nelegantly reproduces verdicts based on the above formulation of\nnaturalness according to which low-energy phenomena should not depend\nsensitively on the details of some more fundamental theory with\nrespect to high energies. \nA prime example of a violation of naturalness occurs in quantum field\ntheories with a spin \\(0\\) scalar particle such as the Higgs particle.\nIn this case, the dependence of the squared physical mass on the\ncut-off \\(\\Lambda\\) is quadratic:  \nThe physical mass of the Higgs particle is empirically known to be\n\\(m_{\\FH,\\Fphys}\\approx125\\,\\textrm{GeV}\\). The dominant\ncontribution to \\(\\Delta m^2\\), specified as \\(h_t\\Lambda^2\\) in\nequation \\(\\eqref{Higgs}\\), is due to the interaction between the\nHiggs particle and the heaviest fermion, the top quark, where \\(h_t\\)\nis some parameter that measures the strength of that interaction.\nGiven the empirically known properties of the top quark, the factor\n\\(\\frac{h_t}{16\\pi^2}\\) is of order \\(10^{-2}\\). Due to its quadratic\ndependence on the cut-off scale \\(\\Lambda\\) the term\n\\(\\frac{h_t}{16\\pi^2}\\Lambda^2\\) is very large if the cut-off scale is\nlarge. If the Standard Model is valid up to the Planck scale\n\\(\\Lambda_{\\textrm{Planck}}\\approx10^{19}\\,\\textrm{GeV}\\), the squared\nbare mass \\(m_{\\FH,0}^2\\) and the effect of the vacuum fluctuations\nwould have to cancel each other out to about 34 orders of magnitude in\norder to result in a physical Higgs mass of \\(125\\,\\textrm{GeV}\\).\nThere is no known physical reason why the effects collected in the\nbare mass \\(m_{\\FH}\\) should be in such a delicately balance with the\neffects from the vacuum fluctuations collected in \\(\\Delta m^2\\). The\nfact that two fundamental scales—the Planck scale and the Higgs\nmass—are so widely separated from each other is referred to as\nthe hierarchy problem. As a consequence of this problem, the\nviolation of naturalness due to the Higgs mass is so severe. \nVarious solutions to the naturalness problem for the Higgs mass have\nbeen proposed in the form of theoretical alternatives to the Standard\nModel. In supersymmetry (see Martin [1998] for an introduction),\ncontributions to \\(\\Delta m_{\\FH}^{2}\\) from supersymmetric partner\nparticles can compensate the contribution from heavy fermions such as\nthe top quark and thereby eliminate the fine-tuning problem. However,\nsupersymmetric theories with this feature appear to be disfavoured by\nmore recent experimental results, notably from the Large Hadron\nCollider (Draper et al. 2012). Other suggested solutions to the\nnaturalness problem for the Higgs particle include so-called\nTechnicolour models (Hill & Simmons 2003), in which the\nHiggs particle is replaced by additional fermionic particles, models\nwith large extra dimensions, where the hierarchy between the Higgs\nmass and the Planck scale is drastically diminished (Arkani-Hamed et\nal. 1998), and models with so-called warped extra dimensions\n(Randall & Sundrum 1999). \nAn even more severe violation of naturalness is created by the\ncosmological constant \\(\\rho_V\\), which specifies the overall vacuum\nenergy density. Here the contribution due to vacuum fluctuations is\nproportional to the fourth power of the cut-off scale \\(\\Lambda\\):\n \nThe physical value \\(\\rho_V\\) of the cosmological constant is\nempirically found to be of order \\(\\rho_V\\sim10^{-3}\\,\\textrm{eV}\\).\nThe constant \\(c\\), which depends on parameters that characterize the\ntop quark and the Higgs particle, is empirically known to be roughly\nof order \\(1\\). If we take the cut-off to be of the order of the\nPlanck scale \\(\\Lambda\\sim10^{19}\\,\\textrm{GeV}\\), the bare term\n\\(\\rho_0\\), must cancel the contribution \\(c\\Lambda^4\\) to more than\n120 orders of magnitude. Even if we assume a cut-off as low as\n\\(\\Lambda\\sim1\\,\\textrm{TeV}\\), i.e., already within reach of current\naccelerator technology, we find that a cancellation between \\(\\rho_0\\)\nand \\(c\\Lambda^4\\) to about 50 digits remains necessary. Contrary to\nthe case of the Higgs mass, there are few ideas of how future physical\ntheories might be able to avoid this problem. \nAs explained in\n Section 5.1,\n violations of naturalness can be seen as instances of fine-tuning,\nbut not in the sense of fine-tuning for life. A connection between\nnaturalness and fine-tuning for life can be constructed, however,\nalong the following lines: \nOne can interpret equations \\(\\eqref{Higgs}\\) and\n\\(\\eqref{cosmo_constant}\\) as suggesting that the actual physical\nvalues of the Higgs mass and the cosmological constant are much\nsmaller than the values that one would expect for them in the\nframework of the Standard Model. Notably, if the Higgs mass were of\norder of the cut-off \\(\\Lambda\\), e.g., the Planck scale, and if the\ncosmological constant were of order \\(\\Lambda^4\\), the bare parameters\nwould not need to be fixed to many digits in order for the physical\nparameters to have their respective orders of magnitude, which means\nthat the physical values would be natural. Thus, assuming naturalness\nand the validity of our currently best physical theories up to the\nPlanck scale, one would expect values for the Higgs mass and the\ncosmological constants of the same order of magnitude as their vacuum\ncontributions, i.e., values much larger than the actual ones. \nWith respect to the problem of specifying probability distributions\nover possible values of physical parameters discussed in\n Section 2.1\n naturalness may be taken to suggest that all reasonable such\ndistributions have most of their probabilistic weight close to the\nnatural values. As explained, for the Higgs mass and the\ncosmological constant the natural values are much larger than the\nobserved ones. Advocates of the view that fine-tuning for life\nrequires a response because life-friendly constants are improbable\ntherefore put particular emphasis on those instances of fine-tuning\nfor life that are associated with violations of naturalness, notably\nthe cosmological constant (e.g., Susskind 2005: ch. 2; Donoghue 2007;\nCollins 2009: sect. 2.3.3; Tegmark 2014: 140f.).","contact.mail":"email@simonfriederich.eu","contact.domain":"simonfriederich.eu"}]
