[{"date.published":"2017-09-26","url":"https://plato.stanford.edu/entries/cosmology/","author1":"Christopher Smeenk","author1.info":"http://www.math.uct.ac.za/mam/staff/ellis","entry":"cosmology","body.text":"\n\n\nCosmology (the study of the physical universe) is a science that, due\nto both theoretical and observational developments, has made enormous\nstrides in the past 100 years. It began as a branch of theoretical\nphysics through Einstein’s 1917 static model of the universe\n(Einstein 1917) and was developed in its early days particularly\nthrough the work of Lemaître\n (1927).[1]\n As recently as 1960, cosmology was widely regarded as a branch of\nphilosophy. It has transitioned to an extremely active area of\nmainstream physics and astronomy, particularly due to the application\nto the early universe of atomic and nuclear physics, on the one hand,\nand to a flood of data coming in from telescopes operating across the\nentire electromagnetic spectrum on the other. However, there are two\nmain issues that make the philosophy of cosmology unlike that of any\nother science. The first is,\n\n\nThe uniqueness of the Universe: there exists only one universe, so\nthere is nothing else similar to compare it with, and the idea of\n“Laws of the universe” hardly makes sense.\n\n\nThis means it is the historical science par excellence: it\ndeals with only one unique object that is the only member of its class\nthat exists physically; indeed there is no non-trivial class of such\nobjects (except in theoreticians’ minds) precisely for this\nreason. This issue will recur throughout this discussion. The second\nis\n\n\nCosmology deals with the physical situation that is the context in the\nlarge for human existence: the universe has such a nature that our\nlife is possible.\n\n\nThis means that although it is a physical science, it is of particular\nimportance in terms of its implications for human life. This leads to\nimportant issues about the explanatory scope of cosmology, which we\nreturn to at the end.\n\nPhysical cosmology has achieved a consensus Standard Model (SM), based\non extending the local physics governing gravity and the other forces\nto describe the overall structure of the universe and its evolution.\nAccording to the SM, the universe has evolved from an extremely high\ntemperature early state, by expanding, cooling, and developing\nstructures at various scales, such as galaxies and stars. This model\nis based on bold extrapolations of existing theories—applying\ngeneral relativity, for example, at length scales 14 orders of\nmagnitude larger than the those at which it has been tested—and\nrequires several novel ingredients, such as dark matter and dark\nenergy. The last few decades have been a golden age of physical\ncosmology, as the SM has been developed in rich detail and\nsubstantiated by compatibility with a growing body of observations.\nHere we will briefly introduce some of the central concepts of the SM\nto provide the minimal background needed for the ensuing\n discussion.[2] \nGravity is the dominant interaction at large length scales. General\nrelativity introduced a new way of representing gravity: rather than\ndescribing gravity as a force deflecting bodies from inertial motion,\nbodies free from non-gravitational forces move along the analog of\nstraight lines, called geodesics, through a curved spacetime\n geometry.[3]\n The spacetime curvature is related to the distribution of energy and\nmatter through GR’s fundamental equations (Einstein’s\nfield equations, EFE). The dynamics of the theory are non-linear:\nmatter curves spacetime, and the curvature of spacetime determines how\nmatter moves; and gravitational waves interact with each other\ngravitationally, and act as gravitational sources. The theory also\nreplaces the single gravitational potential, and associated field\nequation, of Newton’s theory, with a set of 10 coupled,\nnon-linear equations for ten independent\n potentials.[4]\n This complexity is an obstacle to understanding the general features\nof solutions to EFE, and to finding exact solutions to describe\nspecific physical situations. Most exact solutions have been found\nbased on strong idealizations, introduced to simplify the\nmathematics. \nRemarkably, much of cosmology is based on an extremely simple set of\nsolutions found within a decade of Einstein’s discovery of GR.\nThese Friedman-Lemaître-Robertson-Walker (FLRW) solutions have,\nin a precise sense, the most symmetry possible. The spacetime geometry\nis constrained to be uniform, so that there are no preferred locations\nor\n directions.[5]\n They have a simple geometric structure, consisting of a\n“stack” of three-dimensional spatial surfaces\n\\(\\Sigma(t)\\) labeled by values of the cosmic time \\(t\\)\n(topologically, \\(\\Sigma \\times \\mathbb{R}\\)). The surfaces\n\\(\\Sigma(t)\\) are three-dimensional spaces (Riemannian manifolds) of\nconstant curvature, with three possibilities: (1) spherical space, for\nthe case of positive curvature; (2) Euclidean space, for zero\ncurvature; and (3) hyperbolic space, for negative\n curvature.[6] \nThese models describe an expanding universe, characterized fully by\nthe behavior of the scale factor \\(R(t)\\). The worldlines of\n“fundamental observers”, defined as at rest with respect\nto matter, are orthogonal to these surfaces, and the cosmic time\ncorresponds to the proper time measured by the fundamental observers.\nThe scale factor \\(R(t)\\) represents the spatial distance in\n\\(\\Sigma\\) between nearby fundamental observers as a function of\ncosmic time. The evolution of these models is described by a simple\nset of equations governing \\(R(t)\\), implied by Einstein’s field\nequations (EFE): the Friedmann\n equation,[7] \nand the isotropic form of the Raychaudhuri equation:  \nThe curvature of surfaces \\(\\Sigma(t)\\) of constant cosmic time is\ngiven by \\(\\frac{k}{R^2(t)}\\), where \\(k = \\{-1,0,1\\}\\) for negative,\nflat, and positive curvature (respectively). The assumed symmetries\nforce the matter to be described as a perfect\n fluid[8]\n with energy density \\(\\rho\\) and pressure \\(p\\), which obey the\nenergy conservation equation  \nThe unrelenting symmetry of the FLRW models makes them quite simple\ngeometrically and dynamically. Rather than a set of coupled partial\ndifferential equations, which generically follow from EFE, in the FLRW\nmodels one only has to deal with 2 ordinary differential equations\n(only two of \\((\\ref{eq:Fried})\\)–\\((\\ref{eq:cons})\\) are\nindependent) which are determinate once an equation of state \\(p =\np(\\rho)\\) is given. \nThese equations reveal three basic features of these models. First,\nthese are dynamical models: it is hard to arrange an unchanging\nuniverse, with \\(\\dot{R}(t) =0\\). “Ordinary” matter has\npositive total stress-energy density, in the sense that\n\\(\\rho_{\\textit{grav}}\\coloneqq \\rho + 3p > 0\\). From\n(\\(\\ref{eq:Ray}\\)), the effect of such ordinary matter is to\ndecelerate cosmic expansion, \\(\\ddot{R} < 0\\)—gravity is a\nforce of attraction. This is only so for ordinary matter: a positive\ncosmological constant, or matter with negative gravitational-energy\ndensity \\(\\rho_{\\textit{grav}}\\) leads, conversely, to accelerating\nexpansion, \\(\\ddot{R} > 0\\). Einstein was only able to construct a\nstatic model by delicately balancing the attraction of ordinary matter\nwith a precisely chosen value of \\(\\Lambda\\); he unfortunately failed\nto notice that the solution was unstable, and overlooked the dynamical\nimplications of his own theory. \nSecond, the expansion rate varies as different types of matter come to\ndominate the dynamics. As shown by (\\(\\ref{eq:cons}\\)), the energy\ndensity for different types of matter and radiation dilutes at\ndifferent rates: for example, pressureless dust (\\(p=0\\)) dilutes as\n\\(\\propto R^{-3}\\), radiation (\\(p=\\rho/3\\)) as \\(\\propto R^{-4}\\),\nand the cosmological constant (\\(p=-\\rho\\)) remains (as the name\nsuggests) constant. The SM describes the early universe as having a\nmuch higher energy density in radiation than matter. This\nradiation-dominated phase eventually transitions to a matter-dominated\nphase as radiation dilutes more rapidly, followed eventually, if\n\\(\\Lambda > 0\\), by a transition to a \\(\\Lambda\\)-dominated phase;\nif \\(k \\neq 0\\) there may also be a curvature dominated phase. \nThird, FLRW models with ordinary matter have a singularity at a finite\ntime in the past. Extrapolating back in time, given that the universe\nis currently expanding, eqn. (\\(\\ref{eq:Ray}\\)) implies that the\nexpansion began at some finite time in the past. The current rate of\nexpansion is given by the Hubble parameter, \\(H_0 =\n(\\frac{\\dot{R}}{R})_0\\). Simply extrapolating this expansion rate\nbackward, from eqn. (\\(\\ref{eq:Ray}\\)) the expansion rate must\nincrease at earlier times, so \\(R(t) \\rightarrow 0\\) at a time less\nthan the Hubble time Hubble time \\(H_0^{-1}\\) before now, if\n\\(\\rho_{\\textit{grav}}\\geq 0\\). As this “big bang” is\napproached, the energy density and curvature increase without bound\nprovided \\(\\rho_{\\textit{inert}}\\coloneqq (\\rho+p)>0\\) (which\ncondition guarantees that \\(\\rho \\rightarrow\\infty\\) as\n\\(R\\rightarrow0\\)). This reflects gravitational instability: as\n\\(R(t)\\) decreases, the energy density and pressure both increase, and\nthey both appear with the same sign on the right hand side of eqn.\n(\\(\\ref{eq:Ray}\\)), hence pressure \\(p>0\\) does not help avoid\nthe singularity. Work in the 1960s, discussed below in\n §4.1,\n established that the existence of a singularity holds in more\nrealistic models, and is not an artifact of the symmetries of the FLRW\nmodels. \nThe SM adds small departures from strict uniformity in order to\naccount for the formation and evolution of structure. Due to\ngravitational instability, such perturbations are enhanced\ndynamically—the density contrast of an initial region that\ndiffers from the average density grows with time. Sufficiently small\nfluctuations can be treated as linear perturbations to a background\ncosmological model, governed by an evolution equation that follows\nfrom EFE. Yet as the fluctuations grow larger, linearized perturbation\ntheory no longer applies. According to the SM, structure grows\nhierarchically with smaller length scales going non-linear first, and\nlarger structures forming via later mergers. Models of evolution of\nstructures at smaller length scales (e.g., the length scales of\ngalaxies) include physics other than gravity, such as gas dynamics, to\ndescribe the collapsing clumps of matter. Cold dark matter (CDM) also\nplays a crucial role in the SM’s account of structure formation:\nit clumps first, providing scaffolding for clumping of baryonic\nmatter. \nA full account of structure formation requires integrating physics\nover an enormous range of dynamical scales and including a\ncosmological constant as well as baryonic matter, radiation, and dark\nmatter. This is an active area of research, primarily pursued using\nsophisticated \\(N\\)-body computer simulations to study features of the\ngalaxy distribution produced by the SM, given various\n assumptions.[9] \nThere are two main ways in which cosmological observations support\nperturbed FLRW models. First, cosmologists use matter and radiation in\nthe universe to probe the background spacetime geometry and its\nevolution. The universe appears to be isotropic at sufficiently large\nscales, as indicated by background radiation (most notably the cosmic\nmicrowave background radiation (CMB), discussed below) and discrete\nsources (e.g., galaxies). Isotropy observed along a single worldline\nis, however, not sufficient to establish the universe is well\ndescribed by an FLRW geometry. A further assumption that our worldline\nis not the only vantage point from which the universe appears\nisotropic, often called the Copernican principle, is needed. Granting\nthis principle, there are theorems establishing that observations of\nalmost isotropic background radiation implies that the spacetime\ngeometry is almost\n FLRW.[10]\n The principle itself cannot be established directly via observations\n(see\n §2).\n Given that we live in an almost FLRW models, we need to determine its\nparameters such as the Hubble constant \\(H_0\\) and the deceleration\nparameter \\(q_0 \\coloneqq {-}\\ddot{R}/(RH_0^2)\\), which measures how\nthe rate of expansion is changing, and the normalized density\nparameters \\(\\Omega_m\\coloneqq \\rho_m/(3H_0^2)\\) for each matter or\nenergy density component \\(m\\). There are a variety of ways to\ndetermine the accuracy of the background evolution described by the\nFLRW models, which depends on these parameters. For this purpose,\ncosmologists seek effective standard candles and standard\nrulers—objects with a known intrinsic luminosity and length,\nrespectively, which can then be used to measure the expansion history\nof the universe. \nThe second main avenue of testing focuses on the SM’s account of\nstructure formation, which describes the evolution of small\nperturbations away from the background FLRW geometry in terms of a\nsmall number of parameters such as the tilt \\(n_s\\) and the scalar to\ntensor ratio \\(r\\). Observations from different epochs, such as\ntemperature anisotropies in the CMB and the matter power spectrum\nbased on galaxy surveys, can be used as independent constraints on\nthese parameters as well as on the background parameters (indeed such\nobservations turn out to give the best constraints on the background\nmodel parameters). These two routes to testing almost FLRW spacetime\ngeometry are closely linked because the background model provides the\ncontext for the evolution of perturbations under the dynamics\ndescribed by general relativity. \nThe remarkable success of perturbed FLRW models in describing the\nobserved universe has led many cosmologists to focus almost\nexclusively on them, yet there are drawbacks to such a myopic\napproach. For example, the observations at best establish that the\nobserved universe can be well-approximated by an almost FLRW model\nwithin some (large) domain. But they are not the only models that fit\nthe data: there are other cosmological models that mimic FLRW models\nin the relevant domain, yet differ dramatically elsewhere (and\nelsewhen). Specifically, on the one hand there are a class of\nspatially homogeneous and anisotropic models (Bianchi models) that\nexhibit “intermediate isotropization”: namely, they have\nphysical properties that are arbitrarily close to (isotropic) FLRW\nmodels over some time scale\n \\(T\\).[11]\n Agreement over the time interval \\(T\\) does not imply global\nagreement, however, as these models have large anisotropies at other\ntimes. Relying on the FLRW models in making extrapolations to the\nearly or late universe requires some justification for ignoring\nmodels, such as these Bianchi models, that mimic their behavior for a\nfinite time interval. On the other hand there are inhomogeneous\nspherically symmetric models that can reproduce exactly the background\nmodel observations (number counts versus redshifts and angular\ndiameter distance versus redshift, for example) with or without a\ncosmological constant (Mustapha et al. 1997). These can be excluded by\ndirect observations with good enough standard candles (Clarkson et al.\n2008) or by observations of structure formation features in such\nuniverses (Clarkson & Maartens 2010); but that exclusion cannot\ntake place unless one indeed examines such models and their\nobservational consequences. \nLack of knowledge of the full space of solutions to EFE makes it\ndifficult to assess the fragility of various inferences cosmologists\nmake based on perturbed FLRW models. A fragile inference depends on\nthe properties of the model holding exactly, contrasted with robust\ninferences that hold even if the models are good approximations (up to\nsome tolerable error) that will hold even if the model is perturbed.\nThe singularity theorems (Hawking & Ellis 1973), for example,\nestablish that the existence of an initial singularity is robust:\nrather than being features specific to the FLRW models, or other\nhighly symmetric models, singularities are generic in models\nsatisfying physically plausible assumptions. The status of various\nother inferences cosmologists make is less clear. For example, how\nsensitively does the observational case in favor of dark energy, which\ncontributes roughly 70% of the total energy density of the universe in\nthe SM, depend upon treating the universe as having almost-FLRW\nspacetime geometry? As mentioned above, recent work has pursued the\npossibility of accounting for the same observations based upon\nlarge-scale inhomogeneities or local back-reaction, without recourse\nto dark\n energy.[12]\n Studies along these lines are needed to evaluate the possibility that\nsubtle dynamical effects, absent in the FLRW models, provide\nalternative explanations of observed phenomena. The deduction also\ndepends on the assumption that the EFE hold at cosmological scales -\nwhich may not be true: maybe for example some form of scalar-tensor\ntheory should be used. More generally, an assessment of the\nreliability of a variety of cosmological inferences requires detailed\nstudy of a larger space of cosmological models. \nThe SM’s account of the evolution of the matter and radiation in\nthe universe reflects the dynamical effect of expansion. Consider a\ncube of spacetime in the early universe, filled with matter and\nradiation. The dynamical effects of the universe’s expansion are\nlocally the same as slowly stretching the cube. For some stages of\nevolution the contents of the cube interact sufficiently quickly that\nthey reach and stay in local thermal equilibrium as the cube changes\nvolume. (Because of isotropy, equal amounts of matter and radiation\nenter and leave the cube from neighboring cubes.) But when the\ninteractions are too slow compared to the rate of expansion, the cube\nchanges volume too rapidly for equilibrium to be maintained. As a\nresult, particle species “freeze out” and decouple, and\nentropy increases. Without a series of departures from equilibrium,\ncosmology would be boring—the system would remain in equilibrium\nwith a state determined solely by the temperature, without a trace of\nthings past. The rate of expansion of the cube varies with cosmic\ntime. Because radiation, matter, and a cosmological constant term (or\ndark energy) dilute with expansion at different rates, an expanding\nuniverse naturally falls into separate epochs, characterized by\ndifferent expansion rates. \nThere are several distinctive epochs in the history of the universe,\naccording to the SM, including the following: \nThe development of a precise cosmological model compatible with the\nrich set of cosmological data currently available is an impressive\nachievement. Cosmology clearly relies very heavily on theory; the\ncosmological parameters that have been the target of observational\ncampaigns are only defined given a background model. The strongest\ncase for accepting the SM rests on the evidence in favor of the\nunderlying physics, in concert with the overdetermination of\ncosmological parameters. The SM includes several free parameters, such\nas the density parameters characterizing the abundance of different\ntypes of matter, each of which can be measured several\n ways.[14]\n These methods have distinctive theoretical assumptions and sources of\nerror. For example, the abundance of deuterium produced during big\nbang nucleosynthesis depends sensitively on the baryon density.\nNucleosynthesis is described using well-tested nuclear physics, and\nthe light element abundances are frozen in within the “first\nthree minutes”. The amplitudes of the acoustic peaks in the CMB\nangular power spectrum depend on the baryon density at the time of\ndecoupling. Current measurements fix the baryon density to an accuracy\nof one percent, and the values determined by these two methods agree\nwithin observational error. This agreement is one of many consistency\nchecks for the\n SM.[15]\n There are important discrepancies, such as that between local versus\nglobal measurements of the Hubble parameter \\(H_0\\) (Luković et\nal. 2016; Bernal et al. 2016). The significance and further\nimplications of these discrepancies is not clear. \nThe SM from nucleosynthesis on can be regarded as well supported by\nmany lines of evidence. The independence and diversity of the\nmeasurements provides some assurance that the SM will not be\nundermined by isolated theoretical mistakes or undetected sources of\nsystematic error. But the SM is far from complete, and there are three\ndifferent types of significant open issues. \nFirst, we do not understand three crucial components of the SM that\nrequire new physics. We do not have a full account of the nature, or\nunderlying dynamics, of dark matter (Bertone et al. 2005), dark energy\n(Peebles & Ratra 2003), or the inflaton field (Lyth & Riotto\n1999; Martin et al. 2014). These are well-recognized problems that\nhave inspired active theoretical and observational work, although as\nwe note below in\n §2.4\n they will be challenging to resolve due to inaccessibility of physics\nat the appropriate scale. \nThe second set of open questions regards structure formation. While\nthe account of structure formation matches several significant\nobserved features, such as the correlations among galaxies in large\nscale surveys, there are a number of open questions about how galaxies\nform (Silk 2017). Many of these, such as the cusp-core problem\n(Weinberg et al. 2015), and the dark halos problem (a great many more\nsmall dark halos are predicted around galaxies than observed) regard\nfeatures of galaxies on relatively small scales, which require\ndetailed modeling of a variety of astrophysical processes over an\nenormous dynamical range. This is also a very active area of research,\ndriven in particular by a variety of new lines of observational\nresearch and large-scale numerical simulations. \nThe third and final set of open issues regards possible observations\nthat would show that the SM is substantially wrong. Any scientific\ntheory should be incompatible with at least some observations, and\nthat is the case for the SM. In the early days of relativistic\ncosmology, the universe was judged to be younger than some stars or\nglobular clusters. This conflict arose due to a mistaken value of the\nHubble constant. There is currently no such age problem for the SM,\nbut obviously discovering an object older than 13.7 Gyr would force a\nmajor re-evaluation of current cosmological models. Another example\nwould be if there was not a dipole in matter number counts that agrees\nwith the CMB dipole (Ellis & Baldwin 1984). \nAlthough cosmology is generally seen as fitting into the general\nphysics paradigm of everything being determined in a bottom up manner,\nas in the discussion above, there is another tradition that sees the\neffect of the global on the local in cosmology. \nThe traditional issues of this kind (Bondi 1960; Ellis & Sciama\n1972; Ellis 2002) are \nIn each case, global boundary conditions have an important effect on\nlocal physics. More recent ones relate to \nRelevant to all this is the idea of an “effective\nhorizon”: the domain that has direct impact on structures\nexisting on the Earth, roughly 1 Mpc co-moving sphere, see Ellis &\nStoeger 2009. This is the part of the universe that actually has a\nsignificant effect on our history. \nMany philosophers hold that evidence is not sufficient to determine\nwhich scientific theory we should choose. Scientific theories make\nclaims about the natural world that extend far beyond what can be\ndirectly established through observations or experiments. Rival\ntheories may fare equally well with regard to some body of data, yet\ngive quite different accounts of the world. Philosophers often treat\nthe existence of such rivals as inevitable: for a given theory, it is\nalways possible to construct rival theories that have “equally\ngood fit” with available data. Duhem (1914 [1954]) gave an\ninfluential characterization of the difficulty in establishing\nphysical theories conclusively, followed a half century later by\nQuine’s arguments for a strikingly general version of\nunderdetermination (e.g., Quine 1970). The nature of this proposed\nunderdetermination of theory by evidence, and appropriate responses to\nit, have been central topics in philosophy of science (Stanford 2009\n[2016]). Although philosophers have identified a variety of distinct\nsenses of underdetermination, they have generally agreed that\nunderdetermination poses a challenge to justifying scientific\ntheories. \nThere is a striking contrast with discussions of underdetermination\namong scientists, who often emphasize instead the enormous difficulty\nin constructing compelling rival\n theories.[16]\n This contrast reflects a disagreement regarding how to characterize\nthe empirical content of theories. Suppose that the empirical content\nof theory consists of a set of observational claims implied by the\ntheory. Philosophers then take the existence of rival theories to be\nstraightforward. Van Fraassen (1980), for example, defines a theory as\n“empirically adequate” if what it says about observable\nphenomena is true, and argues that for any successful theory there are\nrival theories that disagree about theoretical claims. If we demand\nmore of theories than empirical adequacy in this sense, it is possible\nto draw distinctions among theories that philosophers would regard as\nunderdetermined. Furthermore, even when scientists do face a choice\namong competing theories, they are almost never rivals in the\nphilosopher’s sense. Instead, they differ in various ways:\nintended domain of applicability, explanatory scope, importance\nattributed to particular problems, and so on. \nThe scientists’ relatively dismissive attitude towards alleged\nunderdetermination threats may be based on a more demanding conception\nof empirical\n success.[17]\n Scientists demand much more of their theories than mere compatibility\nwith some set of observational claims: they must fit into a larger\nexplanatory scheme, and be compatible with other successful theories.\nGiven a more stringent account of empirical success it is much more\nchallenging to find rival theories. (We return to this issue in\n §5\n below.) \nOne aspect of underdetermination (emphasized by Stanford 2006) is of\nmore direct relevance to scientific debates: current theories may be\nindistinguishable, within a restricted domain, from a successor\ntheory, even though the successor theory makes different predictions\nfor other domains. This raises the question of how far we can rely on\nextrapolating a theory to a new domain. For example, despite its\nsuccess in describing objects moving with low relative velocities in a\nweak gravitational field, where it is nearly indistinguishable from\ngeneral relativity, Newtonian gravity does not apply to other regimes.\nHow far, then, can we rely on a theory to extend our reach? The\nobstacles to making such reliable inferences reflect the specific\ndetails of particular domains of inquiry. Below we will focus on the\nobstacles to answering theoretical questions in cosmology due to the\nstructure of the universe and our limited access to phenomena. \nGiven the grand scope of cosmology, one might expect that many\nquestions must remain unresolved. Basic features of the SM impose two\nfundamental limits to the ambitions of cosmological theorizing. First,\nthe finitude of the speed of light ensures that we have a limited\nobservational window on the universe due to existence of the visual\nhorizon, representing the most distant matter from which we can\nreceive and information by electromagnetic radiation, and the particle\nhorizon, representing the most distant matter with which we can have\nhad any causal interaction (matter up to that distance can influence\nwhat we see at the visual horizon). Recent work has precisely\ncharacterized what can be established via idealized astronomical\nobservations, regarding spacetime geometry within, or outside, our\npast light cone (the observationally accessible region). Second, in\naddition to enormous extrapolations of well-tested physics in the SM,\ncosmologists have explored speculative ideas in physics that can only\nbe tested through their implications for cosmology; the energies\ninvolved are too high to be tested by any accelerator on Earth. Ellis\n(2007) has characterized these speculative aspects of cosmology as\nfalling on the far side of a “physics horizon”. We will\nbriefly discuss how this second type of horizon poses limits for\ncosmological theorizing. In both cases, the type of underdetermination\nthat arises differs from that discussed in the philosophical\nliterature. \nTo what extent can observations determine the spacetime geometry of\nthe universe directly? The question can be posed more precisely in\nterms of the region that is, in principle, accessible to an observer\nat a location in spacetime \\(p\\)—the causal past,\n\\(J^-(p)\\), of that point. This set includes all regions of spacetime\nfrom which signals traveling at or below the speed of light can reach\n\\(p\\). What can observations confined to \\(J^-(p)\\), assuming that GR\nis valid, reveal about the spacetime geometry of \\(J^-(p)\\) itself,\nand the rest of spacetime? \nThe observational cosmology program (Kristian & Sachs 1966; Ellis\net al. 1985) clarifies the extent to which a set of ideal observations\ncan determine the spacetime geometry directly with minimal\ncosmological assumptions. (By contrast, the standard approach starts\nby assuming a background cosmological model and then finding an\noptimal parameter fit.) Roughly put, the ideal data set consists of a\nset of astrophysical objects that can be used as standard candles and\nstandard rulers. If the intrinsic properties and evolution of a\nvariety of sources are given, observations can directly determine the\narea (or luminosity) distance of the sources, and the distortion of\ndistant images determines lensing effects. These observations thus\ndirectly constrain the spacetime geometry of the past light cone\n\\(C^-(p)\\). Number counts of discrete sources (such as galaxies or\nclusters) can be used to infer the total amount of baryonic matter,\nagain granting various assumptions. Ellis et al. (1985) proved the\nremarkable result that an appropriate idealized data set of this kind\nis sufficient, if we grant that EFE hold, to fully fix the spacetime\ngeometry and distribution of matter on the past light cone \\(C^-(p)\\),\nand from that, in the causal past \\(J^-(p)\\) of the observation point\n\\(p\\).[18]\nObservers do not have access to anything like the ideal data set,\nobviously, and in practice cosmologists face challenges in\nunderstanding the nature of sources and their evolution with\nsufficient clarity that they can be used to determine spacetime\ngeometry, so this is the ideal situation. \nWhat does \\(J^-(p)\\) reveal about the rest of spacetime? In classical\nGR, we would not expect the physical state on \\(J^-(p)\\) to determine\nthat of other regions of spacetime—even the causal past of a\npoint just to the future of \n\\(p\\).[19] \nThere are some models in which \\(J^-(p)\\)\ndoes reveal more: “small universe” models are closed\nmodels with a finite maximum length in all directions that is smaller\nthan the visual horizon (Ellis & Schreiber 1986). Observers in\nsuch a model would be able to “see around the universe” in\nall directions, and establish some global properties via direct\nobservation because they would be able to see all matter that\nexists.[20] \nUnless this is the case, the causal past for a single observer, and\neven a collection of causal pasts, place very weak constraints on the\nglobal properties of spacetime. The global properties of a spacetime\ncharacterize its causal structure, such as the presence or absence of\n singularities.[21]\n General relativity tolerates a wide variety of global properties,\nsince EFE impose only a local constraint on the spacetime geometry.\nOne way to make this question precise is to consider whether there are\nany global properties shared by spacetimes that are constructed as\nfollows. For a given spacetime, construct an indistinguishable\ncounterpart that includes the collection of causal pasts\n\\(\\{J^-(p)\\}\\) for all points in the original spacetime. The\nconstructed spacetime is indistinguishable from the first, because for\nany observer in the first spacetime there is a “copy” of\ntheir causal past in the counterpart. It is possible, however, to\nconstruct counterparts that do not have the same global properties as\nthe original spacetime. The property of having a Cauchy surface, for\nexample, need not be shared by an indistinguishable\n counterpart.[22]\n More generally, the only properties that are guaranteed to hold for\nan indistinguishable counterpart are those that can be established\nbased on the causal past of a single point. This line of work\nestablishes that (some) global properties cannot be established\nobservationally, and raises the question of whether there are\nalternative justifications. \nThe case of global spacetime geometry is not a typical instance of\nunderdetermination of theory by evidence, as discussed by\nphilosophers, for two reasons (see Manchak 2009, Norton 2011,\nButterfield 2014). First, this whole discussion assumes that classical\nGR holds; the question regards discriminating among models of a given\ntheory, rather than a choice among competing theories. Second, these\nresults establish that all observations available to us that are\ncompatible with a given spacetime, with some appealing global\nproperty, are equally compatible with its indistinguishable\ncounterparts. But as is familiar from more prosaic examples of the\nproblem of induction, evidence of past events is compatible, in a\nsimilar sense, with many possible futures. Standard accounts of\ninductive inference aim to justify some expectations about the future\nas more reasonable, e.g., those based on extending past uniformities.\nThe challenge in this case is to articulate an account of inductive\ninferences that justifies accepting one spacetime over its\nindistinguishable counterparts. \nAs a specific instance of this challenge, consider the status of the\ncosmological principle, the global symmetry assumed in the derivation\nof the FLRW models. The results above show that all evidence available\nto us is equally compatible with models in which the cosmological\nprinciple does or does not hold. One might take the principle as\nholding a priori, or as a pre-condition for cosmological\ntheorizing (Beisbart 2009). A recent line of work aims to justify the\nFLRW models by appealing to a weaker general principle in conjunction\nwith theorems relating homogeneity and isotropy. Global isotropy\naround every point implies global homogeneity, and it is natural to\nseek a similar theorem with a weaker antecedent formulated in terms of\nobservable quantities. The Ehlers-Geren-Sachs theorem (Ehlers et al.\n1968) shows that if all geodesic fundamental observers in an expanding\nmodel find that freely propagating background radiation is exactly\nisotropic, then their spacetime is an FLRW model. If our causal past\nis “typical”, observations along our worldline will\nconstrain what other observers should see. This is often called the\nCopernican principle—namely, no point \\(p\\) is distinguished\nfrom other points \\(q\\) by any spacetime symmetries or lack thereof\n(there are no “special locations”). There are indirect\nways of testing this principle empirically: the\nSunyaev-Zel’dovich effect can be used to indirectly measure the\nisotropy of the CBR as observed from distant points. Other tests are\ndirect tests with a good enough set of standard candles, and an\nindirect test based on the time drift of cosmological redshift. This\nline of work provides an empirical argument that the observed universe\nis well-approximated by an FLRW model, thus changing that assumption\nfrom a philosophically based starting point to an observationally\ntested foundation. \nThe Standard Model of particle physics and classical GR provide the\nstructure and framework for the SM. But cosmologists have pursued a\nvariety of questions that extend beyond these core theories. In these\ndomains, cosmologists face a form of underdetermination: should a\nphenomena be accounted for by extending the core theories, or by\nchanging physical or astrophysical assumptions? \nThe Soviet physicist Yakov Zel’dovich memorably called the early\nuniverse the “poor man’s accelerator”, because\nrelatively inexpensive observations of the early universe may reveal\nfeatures of high-energy physics well beyond the reach of even the most\nlavishly funded earth-bound accelerators. For many aspects of\nfundamental physics, including quantum gravity in particular,\ncosmology provides the only feasible way to assess competing ideas.\nThis ambitious conception of cosmology as the sole testing ground for\nnew physics extends beyond the standard model of particle physics\n(which is generally thought to be incomplete, even though there are no\nobservations that contradict it). Big bang nucleosynthesis, for\nexample, is an application of well-tested nuclear physics to the early\nuniverse, with scattering cross-sections and other relevant features\nof the physics fixed by terrestrial experiments. While working out how\nnuclear physics applied in detail required substantial effort, there\nwas little uncertainty regarding the underlying physics. By contrast,\nin some domains cosmologists now aim to explain the universe’s\nhistory while at the same time evaluating new physics used in\nconstructing it. \nThis contrast can be clarified in terms of the “physics\nhorizon” (Ellis 2007), which delimits the physical regime\naccessible to terrestrial experiments and observations, roughly in\nterms of energy scales associated with different interactions. The\nhorizon can be characterized more precisely for a chosen theory, by\nspecifying the regions of parameter space that can be directly tested\nby experiments and\n observations.[23]\n Aspects of cosmological theories that extend past the physics horizon\ncannot be independently tested through non-cosmological experiments or\nobservations; the only empirical route to evaluating these ideas is\nthrough their implications for cosmology. (This is not to deny that\nthere may be strong theoretical grounds to favor particular proposals,\nas extensions of the core theories.) \nCosmological physics extending beyond the physics horizon faces an\nunderdetermination threat due to the lack of independent lines of\nrelevant evidence. The case of dark matter illustrates the value of\nsuch independent evidence. Dark matter was first proposed to account\nfor the dynamical behavior of galaxy clusters and galaxies, which\ncould not be explained using Newtonian gravitational theory with only\nthe luminous matter observed. Dark matter also plays a crucial role in\naccounts of structure formation, as it provides the scaffolding\nnecessary for baryonic matter to clump, without conflicting with the\nuniformity of the\n CMB.[24]\n Both inferences to the existence of dark matter rely on gravitational\nphysics, raising the question of whether we should take these\nphenomena as evidence that our gravitational theory fails, rather than\nas evidence for a new type of matter. There is an active research\nprogram (MOND, for Modified Newtonian\nDynamics) devoted to accounting for the relevant phenomena by\nmodifying gravity. Regardless of one’s stance on the relative\nmerits of MOND vs. dark matter (obviously MOND needs to be extended to\na relativistic theory), direct evidence of existence of dark matter,\nor indirect evidence via decay products, would certainly reshape the\ndebate. Efforts have been underway for some time to find dark matter\nparticles through direct interactions with a detector, mediated by the\nweak force. A positive outcome of these experiments would provide\nevidence of the existence of dark matter that does not depend upon\ngravitational\n theory.[25] \nSuch independent evidence is not available for two prominent examples\nof new physics motivated by discoveries in cosmology. “Dark\nenergy” was introduced in studies of structure formation, which\nemployed a non-zero cosmological constant to fit observational\nconstraints (the \\(\\Lambda\\)CDM models). Subsequent observations of\nthe redshift-distance relation, using supernovae (type Ia) as a\nstandard candle, led to the discovery that the expansion of the\nuniverse is\n accelerating.[26]\n (For \\(\\ddot{R}>0\\) in an FLRW model, there must be a contribution\nthat appears in eqn. (\\(\\ref{eq:Ray}\\)) like a positive \\(\\Lambda\\)\nterm.) Rather than treating these observations as simply determining\nthe value of a parameter in the SM, many cosmologists have developed\nphenomenological models of “dark energy” that leads to an\neffective \\(\\Lambda\\). Unlike dark matter, however, the properties of\ndark energy insure that any attempt at non-cosmological detection\nwould be futile: the energy density is so small, and uniform, that any\nlocal experimental study of its properties is practically impossible.\nFurthermore these models are not based in well-motivated physics: they\nhave the nature of ‘saving the phenomena’ in that they are\ntailored to fitting the cosmological observations by curve\n fitting.[27] \nInflationary cosmology originally promised a powerful unification of\nparticle physics and cosmology. The earliest inflationary models\nexplored the consequences of specific scalar fields introduced in\nparticle physics (the then supposed Higgs field for the strong\ninteractions). Yet theory soon shifted to treating the scalar field\nresponsible for inflation as the “inflaton” field, leaving\nits relationship to particle physics unresolved, and the promise of\nunification unfulfilled. If the properties of the inflaton field are\nunconstrained, inflationary cosmology is extremely flexible; it is\npossible to construct an inflationary model that matches any chosen\nevolutionary history of the early\n universe.[28]\n Specific models of inflation, insofar as they specify the features of\nthe field or fields driving inflation and its initial state, do have\npredictive content. In principle, cosmological observations could\ndetermine some of the properties of the inflaton field and so select\namong them (Martin et al. 2014). This could in principle then have\nimplications for a variety of other experiments or observations; yet\nin practice the features of the inflaton field in most viable models\nof inflation guarantee that it cannot be detected in other regimes.\nThe one exception to this is if the inflaton were the electroweak\nHiggs particle detected at the LHC (Ellis & Uzan 2014). This\nremains a viable inflaton candidate, so testing if it is indeed the\ninflaton is an important task (Bezrukov & Gorbunov 2012). \nThe physics horizon poses a challenge because one particularly\npowerful type of evidence—direct experimental detection or\nobservation, with no dependence on cosmological assumptions—is\nunavailable for the physics relevant at earliest times (before\ninflation, and indeed even for baryosynthesis after inflation). Yet\nthis does not imply that competing theories, such as dark matter vs.\nmodified gravity, should be given equal credence. The case in favor of\ndark matter draws on diverse phenomena, and it has been difficult to\nproduce a compelling modified theory of gravity, consistent with GR,\nthat captures the full range of phenomena as an alternative to dark\nmatter. Cosmology typically demands a more intricate assessment of\nbackground assumptions, and the degree of independence of different\ntests, in evaluating proposed extensions of the core theories. Yet\nthis evidence may still be sufficiently strong, in the sense discussed\nmore fully in\n §5\n below, to justify new physics. \nThere is a distinctive form of underdetermination regarding the use of\nstatistics in cosmology, due to the uniqueness of the universe. To\ncompare the universe with the statistical predictions of the SM, we\nconceptualize it as one realization of a family of possible universes,\nand compare what we actually measure with what is predicted to occur\nin the ensemble of hypothetical models. When they are significantly\ndifferent, the key issue is: Are these just statistical fluctuations\nwe can ignore? Or are they serious anomalies that need an\nexplanation? \nThis question arises in several concrete cases: \nHow do we decide? This will depend on the particular measurement (see\ne.g., Kamionkowski & Loeb 1997; Marra et al. 2013), but in general\nbecause of the uniqueness of the universe, we don’t know if\nthese potential anomalies are real, pointing to serious problems with\nthe models, or not real—just statistical flukes in the way the\nfamily of models differs from the one instance that we have at hand,\nthe unique universe that actually exists. In all the physical\nsciences, this is a unique problem of\n cosmology.[29] \nCosmology confronts a distinctive challenge in accounting for the\norigin of the universe. In most other branches of physics the initial\nor boundary conditions of a system do not call out for theoretical\nexplanation. They may reflect, for example, the impact of the\nenvironment, or an arbitrary choice regarding when to cut off the\ndescription of a subsystem of interest. But in cosmology there are\nheated debates regarding what form a “theory of the initial\nstate” should take, and what it should contribute to our\nunderstanding of the universe. This basic question regarding the\nnature of aims of a theory of origins has significant ramifications\nfor various lines of research in cosmology. \nContemporary cosmology at least has a clear target for a theory of\norigins: the SM describes the universe as having expanded and evolved\nover 13.7 billion years from an initial state where many physical\nquantities diverged. In the FLRW models, the cosmic time \\(t\\) can be\nmeasured by the total proper time elapsed along the worldline of a\nfundamental observer, from the “origin” of the universe\nuntil the present epoch. Extrapolating backwards from the present,\nvarious quantities diverge as the cosmic time \\(t \\rightarrow\n0\\)—for example, \\(R(t) \\rightarrow 0\\) and the matter density\ngoes to\n infinity.[30]\n The worldlines of observers cannot be extended arbitrarily far into\nthe past. Although there is no “first moment” of time,\nbecause the very concept of time breaks down as \\(t\\rightarrow 0\\),\nthe age of the universe is the maximum length of these worldlines. \nThe singularity theorems proved in the 60s (see, in particular,\nHawking & Ellis 1973) show that the universe is finite to the past\nin a broad class of cosmological models. Past singularities, signaled\nby the existence of inextendible geodesics with bounded length, must\nbe present in models with a number of plausible features. (Geodesics\nare the curves of extreme length through curved spacetime, and freely\nfalling bodies follow timelike geodesics.) Intuitively, extrapolating\nbackwards from the present, an inextendible geodesic reaches, within\nfinite distance, an “edge” beyond which it cannot be\nextended. There is not a uniquely defined “cosmic time”,\nin general, but the maximum length of these curves reflects the finite\nage of the universe. The singularity theorems plausibly apply to the\nobserved universe, within the domain of applicability of general\nrelativity. There are various related theorems differing in detail,\nbut one common ingredient is an assumption that there is sufficient\nmatter and energy present to guarantee that our past light cone\n refocuses.[31]\n The energy density of the CMB alone is sufficient to justify this\nassumption. The theorems also require an energy condition: a\nrestriction on the types of matter present in the model, guaranteeing\nthat gravity leads to focusing of nearby geodesics. (In eqn.\n(\\(\\ref{eq:Ray}\\)) above, this is the case if \\(\\rho_{\\textit{grav}}\n> 0\\) and \\(\\Lambda=0\\); it is possible to avoid a singularity with\na non-zero cosmological constant, for example, since it appears with\nthe opposite sign as ordinary matter, counteracting this focusing\neffect.) \nThe prediction of singularities is usually taken to be a deep flaw of\n GR.[32]\n One potential problem with singularities is that they may lead to\nfailures of determinism, because the laws “break down” in\nsome sense. This concern only applies to some kinds of singularities,\nhowever. Relativistic spacetimes that are globally hyperbolic have\nCauchy surfaces, and appropriate initial data posed on such surfaces\nfix a unique solution throughout the spacetime. Global hyperbolicity\ndoes not rule out the existence of singularities, and in particular\nthe FLRW models are globally hyperbolic in spite of the existence of\nan initial singularity. The threat to determinism is thus more\nqualified: the laws do not apply “at the singularity\nitself” even though the subsequent evolution is fully\ndeterministic, and there are some types of singularities that pose\nmore serious threats to determinism. \nAnother common claim is that the presence of singularities establish\nthat GR is incomplete, since it fails to describe physics “at\nthe\n singularity”.[33]\n This is difficult to spell out fully without a local analysis of\nsingularities, which would give precise meaning to talk of\n“approaching” or being “near” the singularity.\nIn any case, it is clear that the presence of a singularity in a\ncosmological model indicates that spacetime, as described by GR, comes\nto an end: there is no way of extending the spacetime through the\nsingularity, without violating mathematical conditions needed to\ninsure that the field equations are well-defined. Any description of\nphysical conditions “before the big bang” must be based on\na theory that supersedes GR, and allows for an extension through the\nsingularity. \nThere are two limitations regarding what we can learn about the\norigins of the universe based on the singularity theorems. First,\nalthough these results establish the existence of an initial\nsingularity, they do not provide much guidance regarding its\nstructure. The spacetime structure near a “generic”\ninitial singularity has not yet been fully characterized. Partial\nresults have been established for restricted classes of solutions; for\nexample, numerical simulations and a number of theorems support the\nBKL conjecture, which holds that isotropic, inhomogeneous models\nexhibit a complicated form of chaotic, oscillatory behavior. The\nresulting picture of the approach to the initial singularity contrasts\nsharply with that in FLRW\n models.[34]\n It is also possible to have non-scalar singularities (Ellis &\nKing 1974). \nSecond, classical general relativity does not include quantum effects,\nwhich are expected to be relevant as the singularity is approached.\nCrucial assumptions of the singularity theorems may not hold once\nquantum effects are taken into account. The standard energy conditions\ndo not hold for quantum fields, which can have negative energy\ndensities. This opens up the possibility that a model including\nquantum fields may exhibit a “bounce” rather than collapse\nto a singularity. More fundamentally, GR’s classical spacetime\ndescription may fail to approximate the description provided by a full\ntheory of quantum gravity. According to recent work applying loop\nquantum gravity to cosmology, spacetime collapses to a minimum finite\nsize rather than reaching a true singularity (Ashtekar & Singh\n2011; Bojowald 2011). On this account, GR fails to provide a good\napproximation in the region of the bounce, and the apparent\nsingularity is an artifact. Classical spacetime “emerges”\nfrom a state to which familiar spacetime concepts do not apply. There\nare several accounts of the early universe, motivated by string theory\nand other approaches, that similarly avoid the initial singularity due\nto quantum gravity effects. \nIn practice, cosmologists often take the physical state at the\nexpected boundary of the domain of applicability of GR as the\n“initial state”. (For example, this might be taken as the\nstate specified on a spatial hypersurface at a very early cosmic time.\nHowever, the domain of applicability of GR is not well understood,\ngiven uncertainty about quantum gravity.) Projecting observed features\nof the universe backwards leads to an initial state with three\npuzzling\n features:[35] \nOn a more phenomenological approach, the gravitational degrees of\nfreedom of the initial state could simply be chosen to fit with later\nobservations, but many proposed “theories of initial\nconditions” aim to account for these features based on new\nphysical principles. The theory of inflation discussed below aims to\nexplain these issues. \nThere are three main approaches to theories of the initial state, all\nof which have been pursued by cosmologists since the late 60s in\ndifferent forms. Expectations for what a theory of initial conditions\nshould achieve have been shaped, in particular, by inflationary\ncosmology. Inflation provided a natural account of the three otherwise\npuzzling features of the initial state emphasized in the previous\nsection. Prior to inflation, these features were regarded as\n“enigmas” (Dicke & Peebles 1979), but after inflation,\naccounting for these features has served as an eligibility requirement\nfor any proposed theory of the early universe. \nThe first approach aims to reduce dependence on special initial\nconditions by introducing a phase of attractor dynamics. This phase of\ndynamical evolution “washes away” the traces of earlier\nstates, in the sense that a probability distribution assigned over\ninitial states converges towards an equilibrium distribution. Misner\n(1968) introduced a version of this approach (his “chaotic\ncosmology program”), proposing that free-streaming neutrinos\ncould isotropize an initially anisotropic state. Inflationary\ncosmology was initially motivated by a similar idea: a\n“generic” or “random” initial state at the\nPlanck time would be expected to be “chaotic”, far from a\nflat FLRW model. During an inflationary stage, arbitrary initial\nstates are claimed to converge towards a state with the three features\ndescribed above. \nThe second approach regards the initial state as extremely special\nrather than generic. Penrose, in particular, has argued that the\ninitial state must be very special to explain time’s arrow; the\nusual approaches fail to take seriously the fact that gravitational\ndegrees of freedom are not excited in the early universe like the\nothers (Penrose 2016). Penrose (1979) treats the second law as arising\nfrom a law-like constraint on the initial state of the universe,\nrequiring that it has low entropy. Rather than introducing a\nsubsequent stage of dynamical evolution that erases the imprint of the\ninitial state, we should aim to formulate a “theory of initial\nconditions” that accounts for its special features.\nPenrose’s conjecture is that the Weyl curvature tensor\napproaches zero as the initial singularity is approached; his\nhypothesis is explicitly time asymmetric, and implies that the early\nuniverse approaches an FLRW solution. (It does not account for the\nobserved perturbations, however.) Later he proposed the idea of\nConformal Cyclic Cosmology, where such a special initial state at the\nstart of one expansion epoch is the result of expansion in a previous\nepoch that wiped out almost all earlier traces of matter and radiation\n(Penrose 2016). \nA third approach rejects the framework accepted by the other two\nproposals, and regards the “initial state” as a misnomer:\nit should instead by regarded as a “branch point” where\nour pocket universe separated off from a larger multiverse. (There are\nstill, of course, questions regarding the initial state of the\nmultiverse ensemble, if one exists.) We will return to this approach\nin\n §5\n below. \nA dynamical approach, even if it is successful in describing a phase\nof the universe’s evolution, arguably does not offer a complete\nsolution to the problem of initial conditions: it collapses into one\nof the other two approaches. For example, an inflationary stage can\nonly begin in a region of spacetime if the inflaton field and the\ngeometry are uniform over a sufficiently large region, such that the\nstress-energy tensor is dominated by the potential term (implying that\nthe derivative terms are small) and the gravitational entropy is\nsmall. There are other model-dependent constraints on the initial\nstate of the inflaton field. One way to respond is to adopt\nPenrose’s point of view, namely that this reflects the need to\nchoose a special initial state, or to derive one from a previous\nexpansion phase. The majority of those working in inflationary\ncosmology instead appeal to the third approach: rather than treating\ninflation as an addition to standard big-bang evolution in a single\nuniverse, we should treat the observed universe as part of a\nmultiverse, discussed below. But even this must have a theory of\ninitial conditions. \nCosmology provokes questions about the limits of scientific\nexplanation because it lacks many of the features that are present in\nother areas of physics. Physical laws are usually regarded as\ncapturing the features of a type of system that remain invariant under\nsome changes, and explanations often work by placing a particular\nevent in larger context. Theories of the initial state cannot appeal\nto either idea: we have access to only one universe, and there is no\nlarger context to appeal to in explaining its properties. This\ncontrast between the types of explanation available in cosmology and\nother areas of physics has often led to dissatisfaction (see, e.g.,\nUnger & Smolin 2014). At the very least, cosmology forces us to\nreconsider basic questions about modalities, and what constitutes\nscientific explanation. \nOne challenge to establishing theories of the initial state is\nentirely epistemic. As emphasized in\n §2.4,\n we lack independent experimental probes of physics at the relevant\nscales, so the extensions of core theories described above are only\ntested indirectly through their implications for cosmology. This\nlimitation reflects contingent facts about the universe, namely the\ncontrast between the energy scales of the early universe and those\naccessible to us, and does not follow from the uniqueness of the\nuniverse per se. Yet this limitation does not imply that it would be\nimpossible to establish laws. There are cases in the history of\nphysics, such as celestial mechanics, where confidence in a\ntheory’s laws is based primarily on successful application under\ncontinually improving standards of precision. \nA further conceptual challenge regards whether it even makes sense to\nseek “laws” in cosmology (Munitz 1962; Ellis 2007). Laws\nare usually taken to cover multiple instances of some type of\nphenomena, or family of objects. What can we mean by\n“laws” for a unique object (the universe as a whole) or a\nunique event (its origin)? \nCompeting philosophical analyses of laws of nature render different\nverdicts on the possibility of cosmological laws. Cosmological laws,\nif possible, differ from local physical laws in a variety of\nways—they do not apply to subsystems of the universe, they lack\nmultiple instances, and etc. Philosophical accounts of laws take\ndifferent features to be essential to law-hood. For example, the\ninfluential Mill-Ramsey-Lewis account takes the laws to be axioms of\nthe deductive system capturing some body of physical knowledge that\noptimally balances strength (the scope of derived claims) and\nsimplicity (the number of axioms) (see, e.g., Loewer 1996). It is\nquite plausible that a constraint on the initial state, such as\nPenrose’s Weyl curvature hypothesis, would count as a law on\nthis account. By contrast, accounts that take other features, such as\ngoverning evolution, as essential, reach the opposite verdict. \nFinally, there are a number of conceptual pitfalls regarding what\nwould count as an adequate “explanation” of the origins of\nthe universe. What is the target of such explanations, and what can be\nused in providing an explanation? The target might be the state\ndefined at the earliest time when extrapolations based on the SM can\nbe trusted. The challenge is that this state then needs to be\nexplained in terms of a physical theory, quantum gravity, whose basic\nconcepts are still obscure to us. This is a familiar challenge in\nphysics, where substantial work is often required to clarify how\ncentral concepts (such as space and time) are modified by a new\ntheory. An explanation of origins in this first sense would explain\nhow it is that classical spacetime emerges from a quantum gravity\nregime. While any such proposals remain quite speculative, the form of\nthe explanation is similar to other cases in physics: what is\nexplained is the applicability of an older, less fundamental theory\nwithin some domain. Such an explanation does not address ultimate\nquestions regarding why the universe exists—instead, such\nquestions are pushed back one step, into the quantum gravity\nregime. \nMany discussions of origins pursue a more ambitious target: they aim\nto explain the creation of the universe “from\n nothing”.[39]\n The target is the true initial state, not just the boundary of\napplicability of the SM. The origins are supposedly then explained\nwithout positing an earlier phase of evolution; supposedly this can be\nachieved, for example, by treating the origin of the universe as a\nfluctuation away from a vacuum state. Yet obviously a vacuum state is\nnot nothing: it exists in a spacetime, and has a variety of\nnon-trivial properties. It is a mistake to take this explanation as\nsomehow directly addressing the metaphysical question of why there is\nsomething rather than\n nothing.[40] \nThe physical conditions necessary for our existence impose a selection\neffect on what we observe. The significance of this point for\ncosmological theorizing is exemplified by Dicke’s criticism of\nDirac’s speculative “large number hypothesis”. Dirac\n(1937) noted the age of the universe expressed in terms of fundamental\nconstants in atomic physics is an extremely large number (roughly\n\\(10^{39}\\)), which coincides with other large, dimensionless numbers\ndefined in terms of fundamental constants. Inspired by this\ncoincidence, he proposed that the large numbers vary to maintain this\norder of magnitude agreement, implying (for example) that the\ngravitational “constant” \\(G\\) is a function of cosmic\ntime. Dicke (1961) noted\nthat creatures like us, made of carbon produced in an earlier\ngeneration of red giants and sustained by the light and heat of a main\nsequence star, can only exist within a restricted interval of cosmic\ntimes, and that Dirac’s coincidence holds for observations made\nwithin this interval. Establishing that the coincidence holds at a\nrandomly chosen \\(t\\) would support Dirac’s hypothesis, however\nslightly, but Dicke’s argument shows that our evidence\ndoes not do so. \nDicke’s reasoning illustrates how taking selection effects into\naccount can mitigate surprise, and undermine the apparent implications\nof facts like those noted by Dirac (see Roush 2003). These facts\nreflect biases in the evidence available to us, rather than supporting\nhis hypothesis. It is also clear that Dicke’s argument is\n“anthropic” in only a very limited sense: his argument\ndoes not depend on a detailed characterization of human observers. All\nthat matters is that we can exist at a cosmic time constrained by the\ntime scales of stellar evolution. \nHow to account for selection effects, within a particular approach to\nconfirmation theory, is one central issue in discussions of anthropic\nreasoning. This question is intertwined with other issues that are\nmore muddled and contentious. Debates among cosmologists regarding\n“anthropic principles” ignited in the 70s, prompted by the\nsuggestion that finely-tuned features of the universe—such as\nthe universe’s isotropy (Collins & Hawking 1973)—can\nbe explained as necessary conditions for the existence of\nobservers.[41]\nMore recently, a number of cosmologists have argued that cosmological\ntheories should be evaluated based on predictions for what a\n“typical” observer should expect to see. These ideas have\ndovetailed with work in formal epistemology. A number of philosophers\nhave developed extensions of Bayesianism to account for\n“self-locating” evidence, for\nexample.[42]\nThis kind of evidence includes indexical information characterizing an\nagent’s beliefs about their identity and location. At present\nwork in this area has not reached a consensus, and we will present a\nbrief overview of some of the considerations that have motivated\ndifferent positions in these debates. \nIn cosmology the most famous example of an “anthropic\nprediction” is Weinberg (1987)’s prediction for\n \\(\\Lambda\\).[43]\n One part of Weinberg’s argument is similar to Dicke’s: he\nargued that there are anthropic bounds on \\(\\Lambda\\), due to its\nimpact on structure formation. The existence of large, gravitationally\nbound structures such as galaxies is only possible if \\(\\Lambda\\)\nfalls within certain bounds. Weinberg went a step further than Dicke,\nand considered what value of \\(\\Lambda\\) a “typical\nobserver” should see. He assumed that observers occupy different\nlocations within a multiverse, and that the value of \\(\\Lambda\\)\nvaries across different regions. Weinberg further argues that the\nprior probability assigned to different values of \\(\\Lambda\\) should\nbe uniform within the anthropic bounds. Typical observers should\nexpect to see a value close to the mean of the anthropic bounds,\nleading to Weinberg’s prediction for \\(\\Lambda\\). \nEssential to Weinberg’s argument is an appeal to the principle\nof indifference, applied to a class of\n observers.[44]\n We should calculate what we expect to observe, that is, as if we are\na “random choice” among all possible\n observers.[45]\n Bostrom (2002) argues that indifference-style reasoning is necessary\nto respond to the problem of “freak observers”. As Bostrom\nformulates it, the problem is that in an infinite universe,\nany observation \\(O\\) is true for some observer\n(even if only for an observer who has fluctuated into existence from\nthe vacuum). His response is that we should evaluate theories based\nnot on the claim that some observer sees \\(O\\), but on an\nindexical claim: that is, we make the observation \\(O\\). He\nassumes that we are a “random” choice among the class of\npossible observers. (How to justify such a strong claim is a major\nchallenge for this line of thought.) If we grant the assumption, then\nwe can assign low probability to the observations of the\n“freak” observers, and recover the evidential value of\n\\(O\\). \nThere are three immediate questions regarding this proposal. The first\nis called the “reference class” problem. The assignments\nof probabilities to events requires specifying how they are grouped\n together.[46]\n Obviously, what is typical with respect to one reference class will\nnot be typical with respect to another (compare, for example,\n“conscious observers” with “carbon-based\nlife”). Second, the principle of indifference has been\nthoroughly criticized as a justification for probability in other\ncontexts; what justifies the use of indifference in this case? Why\nshould we take ourselves as “randomly chosen” among an\nappropriate reference class? The third problem reflects the intended\napplication of these ideas: Bostrom and other authors in this line of\nwork are particularly concerned with observes that may occupy an\ninfinite universe. There is no proof that the universe is in fact\ninfinite. These are all pressing problems for those who hold that the\nprinciple of indifference is essential to making cosmological\npredictions. \nFurthermore, one way of implementing this approach leads to absurd\nconsequences. The Doomsday Argument, for example, claims to reach a\nstriking conclusion about the future of the human species without any\nempirical input (see, e.g., Leslie 1992; Gott 1993; Bostrom 2002).\nSuppose that we are “typical” humans, in the sense of\nhaving a birth rank that is randomly selected among the collection of\nall humans that have ever lived. We should then expect that there are\nnearly as many humans before and after us in overall birth rank. For\nthis to be true, given current rates of population growth, there must\nbe a catastrophic drop in the human population\n(“Doomsday”) in the near future. The challenge to\nadvocates of indifference applied to observers is to articulate\nprinciples that avoid such consequences, while still solving (alleged)\nproblems such as that of freak observers. \nIn sum, one approach to anthropic reasoning aims to clarify the rules\nof reasoning applicable to predictions made by observers in a large or\ninfinite universe. This line of work is motivated by the idea that\nwithout such principles we face a severe skeptical predicament, as\nobservations would not have any bearing on the theory. Yet there is\nstill not general agreement on the new principles required to handle\nthese cases, which are of course not scientifically testable\nprinciples: they are philosophically based proposals. According to an\nalternative approach, selection effects can and should be treated\nwithin the context of a Bayesian approach to inductive inference (see\nNeal 2006; Trotta 2008). On this line of thought,\n“predictions” like those that Bostrom and others hope to\nanalyze play no direct role in the evaluation of cosmological\ntheories, so further principles governing anthropic reasoning are\nsimply not necessary. There is much further work to be done in\nclarifying and assessing these (and other) approaches to anthropic\n reasoning.[47] \nFine-tuning arguments start from a conflict between two different\nperspectives on certain features of cosmology (or other physical\ntheories). On the first perspective, the existence of creatures like\nus seems to be sensitive to a wide variety of aspects of cosmology and\nphysics. To be more specific, the prospects for life depend\nsensitively on the values of the various fundamental constants that\nappear in these theories. The SM includes about 10 constants, and the\nparticle physics standard model includes about 20\n more.[48]\n Tweaking the SM, or the standard model of particle physics, by\nchanging the values of these constants seems to lead to a barren\n cosmos.[49]\n Focusing on the existence of “life” runs the risk of\nbeing too provincial; we don’t have a good general account of\nwhat physical systems can support intelligent life. Yet it does seem\nplausible that intelligence requires an organism with complex\nstructural features, living in a sufficiently stable environment. \nAt a bare minimum, the existence of life seems to require the\nexistence of complex structures at a variety of scales, ranging from\ngalaxies to planetary systems to macro-molecules. Such complexity is\nextremely sensitive to the values of the fundamental constants of\nnature. From this perspective, the existence of life in the universe\nis fragile in the sense that it depends sensitively on these aspects\nof the underlying theory. \nThis view contrasts sharply with the status of the constants from the\nperspective of fundamental physics. Particle physicists typically\nregard their theories as effective field theories, which suffice for\ndescribing interactions at some specified energy scale. These theories\ninclude various constants, characterizing the relative strength of the\ninteractions they describe, that cannot be further explained by the\neffective field theory. The constants can be fixed by experimental\nresults, but are not derivable from fundamental physical principles.\n(If the effective field theory can be derived from a more fundamental\ntheory, the value of the constants can in principle be determined by\nintegrating out higher-energy degrees of freedom. But this merely\npushes the question back one step: the constants appearing in the more\nfundamental theory are determined experimentally.) Similarly, the\nconstants appearing in the SM are treated as contingent features of\nthe universe. There is no underlying physical principle that sets, for\nexample, the cosmological densities of different kinds of matter, or\nthe value of the Hubble constant. \nSo features of our theories that appear entirely contingent, from the\npoint of view of physics, are necessary to account for the complexity\nof the observed universe and the very possibility of life. The\nfine-tuning argument starts from a sense of unease about this\nsituation: shouldn’t something as fundamental as the complexity\nof the universe be explained by the laws or basic\nprinciples of the theory, and not left to brute facts regarding\nthe values of various constants? The unease develops into serious\ndiscomfort if the specific values of the constants are taken to be\nextremely unlikely: how could the values of all these constants be\njust right, by sheer coincidence? \nIn many familiar cases, our past experience is a good guide to when an\napparent coincidence calls for further explanation. As Hume\nemphasized, however, intuitive assessments from everyday life of\nwhether a given event is likely, or requires a further explanation, do\nnot extend to cosmology. Recent formulations of fine-tuning arguments\noften introduce probabilistic considerations. The constants are\n“fine-tuned”, meaning that the observed values are\n“improbable” in some sense. Introducing a well-defined\nprobability over the constants would provide a response to Hume:\nrather than extrapolating our intuitions, we would be drawing on the\nformal machinery of our physical theories to identify fine-tuning.\nPromising though this line of argument may be, there is not an obvious\nway to define physical probabilities over the values of different\nconstants, or over other features of the laws. There is nothing like\nthe structure used to justify physical probabilities in other\ncontexts, such as equilibrium statistical\n mechanics.[50] \nThere are four main responses to fine-tuning: \nIn the next section we discuss the last response in more detail; see\n §3\n for further discussion of the third response. \nThe multiverse response replaces a single, apparently finely-tuned\nuniverse within an ensemble of universes, combined with an appeal to\nanthropic selection. Suppose that all possible values of the\nfundamental constants are realized in individual elements of the\nensemble. Many of these universes will be inhospitable to life. In\ncalculating the probabilities that we observe specific values of the\nfundamental constants, we need only consider the subset of universe\ncompatible with the existence of complexity (or some more specific\nfeature associated with life). If we have some way of assigning\nprobabilities over the ensemble, we could then calculate the\nprobability associated with our measured values. These calculations\nwill resolve the fine-tuning puzzles if they show that we observe\ntypical values for a complex (or life-permitting) universe. \nMany cosmologists have argued in favor of a specific version of the\nmultiverse called eternal inflation\n (EI).[51]\n On this view, the rapid expansion hypothesized by inflationary\ncosmology continues until arbitrarily late times in some regions, and\ncomes to an end (with a transition to slower expansion) in others.\nThis leads to a global structure of “pocket” universes\nembedded within a larger multiverse. \nOn this line of thought, the multiverse should be accepted for the\nsame reason we accept many claims about what we cannot directly\nobserve—namely, as an inevitable consequence of an established\nphysical theory. It is not clear, however, that EI is inevitable, as\nnot all inflationary models, arguably including those favored by CMB\nobservations, have the kind of potential that leads to\n EI.[52]\n Accounts of how inflation leads to EI rely on speculative\n physics.[53]\n Furthermore, if inflation does lead to EI, that threatens to\nundermine the original reasons for accepting inflation (Smeenk 2014):\nrather than the predictions regarding the state produced at the end of\ninflation taken to provide evidence for inflation, EI seems to imply\nthat, as Guth (2007) put it, in EI “anything that can happen\nwill happen; in fact, it will happen an infinite number of\ntimes”. \nThere have been two distinct approaches to recovering some empirical\ncontent in this\n situation.[54]\n First, there may be traces of the early formation of the pocket\nuniverses, the remnants of collisions between neighboring\n“bubbles”, left on the CMB sky (Aguirre & Johnson\n2011). Detection of a distinctive signature that cannot be explained\nby other means would provide evidence for the multiverse. However,\nthere is no expectation that a multiverse theory would generically\npredict such traces; for example, if the collision occurs too early\nthe imprint is erased by subsequent inflationary expansion. \nThe other approach regards predictions for the fundamental constants,\nsuch as Weinberg’s prediction of \\(\\Lambda\\) discussed above.\nThe process of forming the pocket universes is assumed to yield\nvariation in the local, low-energy physics in each pocket. Predictions\nfor the values of the fundamental constants follow from two things:\n(1) a specification of the probabilities for different values of the\nconstant over the ensemble, and (2) a treatment of the selection\neffect imposed by restricting consideration to pocket universes with\nobservers and then choosing a “typical” observer. \nThe aim is to obtain probabilistic predictions for what a typical\nobserver should see in the EI multiverse. Yet there are several\nchallenges to overcome, alongside those mentioned above related to\nanthropics. The assumption that the formation of pocket universes\nleads to variation in constants is just an assumption, which is not\nyet justified by a plausible, well-tested dynamical theory. The most\nwidely discussed challenge in the physics literature is the\n“measure problem”: roughly, how to assign\n“size” to different regions of the multiverse, as a first\nstep towards assigning probabilities. It is difficult to define a\nmeasure because the EI multiverse is usually taken to be an infinite\nensemble, lacking in the kinds of structure used in constructing a\nmeasure. On our view, these unmet challenges undercut the hope that\nthe EI multiverse yields probabilistic predictions. And without such\nan account, the multiverse proposal does not have any testable\nconsequences. If everything happens somewhere in the ensemble, then\nany potential observation is compatible with the theory. \nSupposing that we grant a successful resolution of all these\nchallenges, the merits of a multiverse solution of fine-tuning\nproblems could then be evaluated by comparison with competing ideas.\nThe most widely cited evidence in favor of a multiverse is\nWeinberg’s prediction for the value of \\(\\Lambda\\), discussed\nabove. There are other proposals to explain the observed value of\n\\(\\Lambda\\); Wang, Zhu, and Unruh (2017), for example, treat the\nquantum vacuum as extremely inhomogeneous, and argue that resonance\namong the vacuum fluctuations leads to a small \\(\\Lambda\\). \nThe unease many have about multiverse proposals are only reinforced by\nthe liberal appeals to “infinities” in discussion of the\n idea.[55]\n Many have argued, for example, that we must formulate an account of\nanthropic reasoning that applies to a truly infinite, rather than\nmerely very large, universe. Claims that we occupy one of infinitely\nmany possible pocket universes, filled with an infinity of other\nobservers, rest on an enormous and speculative extrapolation. Such\nclaims fail to take seriously the concept of infinity, which is not\nmerely a large number. Hilbert (1925 [1983]) emphasized that while\ninfinity is required to complete mathematics, it does not occur\nanywhere in the accessible physical universe. One response is\nto require that infinities in cosmology should have a restricted use.\nIt may be useful to introduce infinity as part of an explanatory\naccount of some aspect of cosmology, as is common practice in\nmathematical models that introduce various idealizations. Yet this\ninfinity should be eliminable, such that the explanation of the\nphenomena remains valid when the idealization is\n removed.[56]\n Even for those who regard this demand as too stringent, there\ncertainly needs to be more care in clarifying and justifying claims\nregarding infinities. \nIn sum, interest in the multiverse stems primarily from speculations\nabout the consequences of inflation for the global structure of the\nuniverse. The main points of debate regard whether EI is a disaster\nfor inflation, undermining the possibility of testing inflation at\nall, and how much predictions such as that for \\(\\Lambda\\) lend\ncredence to these\n speculations.[57]\n Resolution of these questions is needed to decide whether the\nmultiverse can be tested in a stronger sense, going beyond the special\ncases (such as bubble collisions) that may provide more direct\nevidence. \nAs mentioned at the start, the uniqueness of the universe raises\nspecific problems as regards cosmology as a science. First we consider\nissues to do with verification of cosmological models, and then make a\ncomment as regards interpreting the human implications of\ncosmology \nThe basic challenge in cosmology regards how to test and evaluate\ncosmological models, given our limited access to the unique universe.\nAs discussed above, current cosmological models rely in part on\nextrapolations of well-tested local physics along with novel\nproposals, such as the inflaton field. The challenge is particularly\npressing in evaluating novel claims that only have cosmological\nimplications, due to the physics horizon\n (§2.4).\n Distinctions that are routinely employed in other areas of physics,\nsuch as that between laws and initial conditions, or chance and\nnecessity, are not directly applicable, due to the uniqueness of the\nuniverse. \nRecent debates regarding the legitimacy of different lines of research\nin cosmology reflect different responses to this challenge. One\nresponse is to retreat to hypothetico-deductivism (HD): a hypothesis\nreceives an incremental boost in confidence when one of its\nconsequences is verified (and a decrease if it is\n falsified).[58]\n Proponents of inflation argue, for example, that inflation should be\naccepted based on its successful prediction of a flat universe with a\nspecific spectrum of density perturbations. Some advocates of the\nmultiverse take its successful prediction of the value of \\(\\Lambda\\)\nas the most compelling evidence in its favor. \nDespite its appeal, there are well-known problems with taking HD as a\nsufficient account of how evidence supports theories (this is often\ncalled “naïve HD”). In particular, the naïve\nview lacks the resources to draw distinctions among underdetermined\nrival theories that make the same predictions (see Crupi 2013 [2016]).\nWe take it as given that scientists do draw distinctions among\ntheories that naïve HD would treat as on par, as is reflected in\njudgments regarding how much a given body of evidence supports a\nparticular theory. Scientists routinely distinguish among, for\nexample, theories that may merely “fit the data” as\nopposed to those that accurately capture laws governing a particular\ndomain, and evaluate some successful predictions as being far more\nrevealing than others. \nA second response is that the challenge requires a more sophisticated\nmethodology. This may take the form of acknowledging explicitly the\ncriteria that scientists use to assess desirability of scientific\ntheories (Ellis 2007), which include considerations of explanatory\npower, consistency with other theories, and other factors, in addition\nto compatibility with the evidence. These come into conflict in\nunexpected ways in cosmology, and these different factors should be\nclearly articulated and weighed against one another. Alternatively,\none might try to show that some of these desirable features, such as\nthe ability to unify diverse phenomena, should be taken as part of\nwhat constitutes empirical\n success.[59]\n This leads to a more demanding conception of empirical success,\nexemplified by historical cases such as Perrin’s argument in\nfavor of the atomic constitution of matter. \nFinally, a key issue is what scope do we expect our theories to have.\nEllis (2017) makes a distinction between Cosmology, which is\nthe physically based subject dealt with in the textbooks listed in\nthis article, dealing with the expansion of the universe, galaxies,\nnumber counts, background radiation, and so on, and\nCosmologia, where one takes all that as given but adds in\nconsideration about the meaning this all has for life. Clearly the\nanthropic discussions mentioned above are a middle\n ground.[60]\n However a number of popular science books by major scientists are\nappearing that make major claims about Cosmologia, based purely in\narguments from fundamental physics together with astronomical\nobservations. We will make just one remark about this here. If one is\ngoing to consider Cosmologia seriously, it is incumbent on one to take\nseriously the full range of data appropriate to that enterprise. That\nis, the data needed for the attempted scope of such a theory must\ninclude data to do with the meaning of life as well as data derived\nfrom telescopes, laboratory experiments, and particle colliders. It\nmust thus include data about good and evil, life and death, fear and\nhope, love and pain, writings from the great philosophers and writers\nand artists who have lived in human history and pondered the meaning\nof life on the basis of their life experiences. This is all of great\nmeaning to those who live on Earth (and hence in the Universe). To\nproduce books saying that science proves there is no purpose in the\nuniverse is pure myopia. It just means that one has shut ones eyes to\nall the data that relates to purpose and meaning; and that one\nsupposes that the only science is physics (for psychology and biology\nare full of purpose).","contact.mail":"csmeenk2@uwo.ca","contact.domain":"uwo.ca"},{"date.published":"2017-09-26","url":"https://plato.stanford.edu/entries/cosmology/","author1":"Christopher Smeenk","author1.info":"http://www.math.uct.ac.za/mam/staff/ellis","entry":"cosmology","body.text":"\n\n\nCosmology (the study of the physical universe) is a science that, due\nto both theoretical and observational developments, has made enormous\nstrides in the past 100 years. It began as a branch of theoretical\nphysics through Einstein’s 1917 static model of the universe\n(Einstein 1917) and was developed in its early days particularly\nthrough the work of Lemaître\n (1927).[1]\n As recently as 1960, cosmology was widely regarded as a branch of\nphilosophy. It has transitioned to an extremely active area of\nmainstream physics and astronomy, particularly due to the application\nto the early universe of atomic and nuclear physics, on the one hand,\nand to a flood of data coming in from telescopes operating across the\nentire electromagnetic spectrum on the other. However, there are two\nmain issues that make the philosophy of cosmology unlike that of any\nother science. The first is,\n\n\nThe uniqueness of the Universe: there exists only one universe, so\nthere is nothing else similar to compare it with, and the idea of\n“Laws of the universe” hardly makes sense.\n\n\nThis means it is the historical science par excellence: it\ndeals with only one unique object that is the only member of its class\nthat exists physically; indeed there is no non-trivial class of such\nobjects (except in theoreticians’ minds) precisely for this\nreason. This issue will recur throughout this discussion. The second\nis\n\n\nCosmology deals with the physical situation that is the context in the\nlarge for human existence: the universe has such a nature that our\nlife is possible.\n\n\nThis means that although it is a physical science, it is of particular\nimportance in terms of its implications for human life. This leads to\nimportant issues about the explanatory scope of cosmology, which we\nreturn to at the end.\n\nPhysical cosmology has achieved a consensus Standard Model (SM), based\non extending the local physics governing gravity and the other forces\nto describe the overall structure of the universe and its evolution.\nAccording to the SM, the universe has evolved from an extremely high\ntemperature early state, by expanding, cooling, and developing\nstructures at various scales, such as galaxies and stars. This model\nis based on bold extrapolations of existing theories—applying\ngeneral relativity, for example, at length scales 14 orders of\nmagnitude larger than the those at which it has been tested—and\nrequires several novel ingredients, such as dark matter and dark\nenergy. The last few decades have been a golden age of physical\ncosmology, as the SM has been developed in rich detail and\nsubstantiated by compatibility with a growing body of observations.\nHere we will briefly introduce some of the central concepts of the SM\nto provide the minimal background needed for the ensuing\n discussion.[2] \nGravity is the dominant interaction at large length scales. General\nrelativity introduced a new way of representing gravity: rather than\ndescribing gravity as a force deflecting bodies from inertial motion,\nbodies free from non-gravitational forces move along the analog of\nstraight lines, called geodesics, through a curved spacetime\n geometry.[3]\n The spacetime curvature is related to the distribution of energy and\nmatter through GR’s fundamental equations (Einstein’s\nfield equations, EFE). The dynamics of the theory are non-linear:\nmatter curves spacetime, and the curvature of spacetime determines how\nmatter moves; and gravitational waves interact with each other\ngravitationally, and act as gravitational sources. The theory also\nreplaces the single gravitational potential, and associated field\nequation, of Newton’s theory, with a set of 10 coupled,\nnon-linear equations for ten independent\n potentials.[4]\n This complexity is an obstacle to understanding the general features\nof solutions to EFE, and to finding exact solutions to describe\nspecific physical situations. Most exact solutions have been found\nbased on strong idealizations, introduced to simplify the\nmathematics. \nRemarkably, much of cosmology is based on an extremely simple set of\nsolutions found within a decade of Einstein’s discovery of GR.\nThese Friedman-Lemaître-Robertson-Walker (FLRW) solutions have,\nin a precise sense, the most symmetry possible. The spacetime geometry\nis constrained to be uniform, so that there are no preferred locations\nor\n directions.[5]\n They have a simple geometric structure, consisting of a\n“stack” of three-dimensional spatial surfaces\n\\(\\Sigma(t)\\) labeled by values of the cosmic time \\(t\\)\n(topologically, \\(\\Sigma \\times \\mathbb{R}\\)). The surfaces\n\\(\\Sigma(t)\\) are three-dimensional spaces (Riemannian manifolds) of\nconstant curvature, with three possibilities: (1) spherical space, for\nthe case of positive curvature; (2) Euclidean space, for zero\ncurvature; and (3) hyperbolic space, for negative\n curvature.[6] \nThese models describe an expanding universe, characterized fully by\nthe behavior of the scale factor \\(R(t)\\). The worldlines of\n“fundamental observers”, defined as at rest with respect\nto matter, are orthogonal to these surfaces, and the cosmic time\ncorresponds to the proper time measured by the fundamental observers.\nThe scale factor \\(R(t)\\) represents the spatial distance in\n\\(\\Sigma\\) between nearby fundamental observers as a function of\ncosmic time. The evolution of these models is described by a simple\nset of equations governing \\(R(t)\\), implied by Einstein’s field\nequations (EFE): the Friedmann\n equation,[7] \nand the isotropic form of the Raychaudhuri equation:  \nThe curvature of surfaces \\(\\Sigma(t)\\) of constant cosmic time is\ngiven by \\(\\frac{k}{R^2(t)}\\), where \\(k = \\{-1,0,1\\}\\) for negative,\nflat, and positive curvature (respectively). The assumed symmetries\nforce the matter to be described as a perfect\n fluid[8]\n with energy density \\(\\rho\\) and pressure \\(p\\), which obey the\nenergy conservation equation  \nThe unrelenting symmetry of the FLRW models makes them quite simple\ngeometrically and dynamically. Rather than a set of coupled partial\ndifferential equations, which generically follow from EFE, in the FLRW\nmodels one only has to deal with 2 ordinary differential equations\n(only two of \\((\\ref{eq:Fried})\\)–\\((\\ref{eq:cons})\\) are\nindependent) which are determinate once an equation of state \\(p =\np(\\rho)\\) is given. \nThese equations reveal three basic features of these models. First,\nthese are dynamical models: it is hard to arrange an unchanging\nuniverse, with \\(\\dot{R}(t) =0\\). “Ordinary” matter has\npositive total stress-energy density, in the sense that\n\\(\\rho_{\\textit{grav}}\\coloneqq \\rho + 3p > 0\\). From\n(\\(\\ref{eq:Ray}\\)), the effect of such ordinary matter is to\ndecelerate cosmic expansion, \\(\\ddot{R} < 0\\)—gravity is a\nforce of attraction. This is only so for ordinary matter: a positive\ncosmological constant, or matter with negative gravitational-energy\ndensity \\(\\rho_{\\textit{grav}}\\) leads, conversely, to accelerating\nexpansion, \\(\\ddot{R} > 0\\). Einstein was only able to construct a\nstatic model by delicately balancing the attraction of ordinary matter\nwith a precisely chosen value of \\(\\Lambda\\); he unfortunately failed\nto notice that the solution was unstable, and overlooked the dynamical\nimplications of his own theory. \nSecond, the expansion rate varies as different types of matter come to\ndominate the dynamics. As shown by (\\(\\ref{eq:cons}\\)), the energy\ndensity for different types of matter and radiation dilutes at\ndifferent rates: for example, pressureless dust (\\(p=0\\)) dilutes as\n\\(\\propto R^{-3}\\), radiation (\\(p=\\rho/3\\)) as \\(\\propto R^{-4}\\),\nand the cosmological constant (\\(p=-\\rho\\)) remains (as the name\nsuggests) constant. The SM describes the early universe as having a\nmuch higher energy density in radiation than matter. This\nradiation-dominated phase eventually transitions to a matter-dominated\nphase as radiation dilutes more rapidly, followed eventually, if\n\\(\\Lambda > 0\\), by a transition to a \\(\\Lambda\\)-dominated phase;\nif \\(k \\neq 0\\) there may also be a curvature dominated phase. \nThird, FLRW models with ordinary matter have a singularity at a finite\ntime in the past. Extrapolating back in time, given that the universe\nis currently expanding, eqn. (\\(\\ref{eq:Ray}\\)) implies that the\nexpansion began at some finite time in the past. The current rate of\nexpansion is given by the Hubble parameter, \\(H_0 =\n(\\frac{\\dot{R}}{R})_0\\). Simply extrapolating this expansion rate\nbackward, from eqn. (\\(\\ref{eq:Ray}\\)) the expansion rate must\nincrease at earlier times, so \\(R(t) \\rightarrow 0\\) at a time less\nthan the Hubble time Hubble time \\(H_0^{-1}\\) before now, if\n\\(\\rho_{\\textit{grav}}\\geq 0\\). As this “big bang” is\napproached, the energy density and curvature increase without bound\nprovided \\(\\rho_{\\textit{inert}}\\coloneqq (\\rho+p)>0\\) (which\ncondition guarantees that \\(\\rho \\rightarrow\\infty\\) as\n\\(R\\rightarrow0\\)). This reflects gravitational instability: as\n\\(R(t)\\) decreases, the energy density and pressure both increase, and\nthey both appear with the same sign on the right hand side of eqn.\n(\\(\\ref{eq:Ray}\\)), hence pressure \\(p>0\\) does not help avoid\nthe singularity. Work in the 1960s, discussed below in\n §4.1,\n established that the existence of a singularity holds in more\nrealistic models, and is not an artifact of the symmetries of the FLRW\nmodels. \nThe SM adds small departures from strict uniformity in order to\naccount for the formation and evolution of structure. Due to\ngravitational instability, such perturbations are enhanced\ndynamically—the density contrast of an initial region that\ndiffers from the average density grows with time. Sufficiently small\nfluctuations can be treated as linear perturbations to a background\ncosmological model, governed by an evolution equation that follows\nfrom EFE. Yet as the fluctuations grow larger, linearized perturbation\ntheory no longer applies. According to the SM, structure grows\nhierarchically with smaller length scales going non-linear first, and\nlarger structures forming via later mergers. Models of evolution of\nstructures at smaller length scales (e.g., the length scales of\ngalaxies) include physics other than gravity, such as gas dynamics, to\ndescribe the collapsing clumps of matter. Cold dark matter (CDM) also\nplays a crucial role in the SM’s account of structure formation:\nit clumps first, providing scaffolding for clumping of baryonic\nmatter. \nA full account of structure formation requires integrating physics\nover an enormous range of dynamical scales and including a\ncosmological constant as well as baryonic matter, radiation, and dark\nmatter. This is an active area of research, primarily pursued using\nsophisticated \\(N\\)-body computer simulations to study features of the\ngalaxy distribution produced by the SM, given various\n assumptions.[9] \nThere are two main ways in which cosmological observations support\nperturbed FLRW models. First, cosmologists use matter and radiation in\nthe universe to probe the background spacetime geometry and its\nevolution. The universe appears to be isotropic at sufficiently large\nscales, as indicated by background radiation (most notably the cosmic\nmicrowave background radiation (CMB), discussed below) and discrete\nsources (e.g., galaxies). Isotropy observed along a single worldline\nis, however, not sufficient to establish the universe is well\ndescribed by an FLRW geometry. A further assumption that our worldline\nis not the only vantage point from which the universe appears\nisotropic, often called the Copernican principle, is needed. Granting\nthis principle, there are theorems establishing that observations of\nalmost isotropic background radiation implies that the spacetime\ngeometry is almost\n FLRW.[10]\n The principle itself cannot be established directly via observations\n(see\n §2).\n Given that we live in an almost FLRW models, we need to determine its\nparameters such as the Hubble constant \\(H_0\\) and the deceleration\nparameter \\(q_0 \\coloneqq {-}\\ddot{R}/(RH_0^2)\\), which measures how\nthe rate of expansion is changing, and the normalized density\nparameters \\(\\Omega_m\\coloneqq \\rho_m/(3H_0^2)\\) for each matter or\nenergy density component \\(m\\). There are a variety of ways to\ndetermine the accuracy of the background evolution described by the\nFLRW models, which depends on these parameters. For this purpose,\ncosmologists seek effective standard candles and standard\nrulers—objects with a known intrinsic luminosity and length,\nrespectively, which can then be used to measure the expansion history\nof the universe. \nThe second main avenue of testing focuses on the SM’s account of\nstructure formation, which describes the evolution of small\nperturbations away from the background FLRW geometry in terms of a\nsmall number of parameters such as the tilt \\(n_s\\) and the scalar to\ntensor ratio \\(r\\). Observations from different epochs, such as\ntemperature anisotropies in the CMB and the matter power spectrum\nbased on galaxy surveys, can be used as independent constraints on\nthese parameters as well as on the background parameters (indeed such\nobservations turn out to give the best constraints on the background\nmodel parameters). These two routes to testing almost FLRW spacetime\ngeometry are closely linked because the background model provides the\ncontext for the evolution of perturbations under the dynamics\ndescribed by general relativity. \nThe remarkable success of perturbed FLRW models in describing the\nobserved universe has led many cosmologists to focus almost\nexclusively on them, yet there are drawbacks to such a myopic\napproach. For example, the observations at best establish that the\nobserved universe can be well-approximated by an almost FLRW model\nwithin some (large) domain. But they are not the only models that fit\nthe data: there are other cosmological models that mimic FLRW models\nin the relevant domain, yet differ dramatically elsewhere (and\nelsewhen). Specifically, on the one hand there are a class of\nspatially homogeneous and anisotropic models (Bianchi models) that\nexhibit “intermediate isotropization”: namely, they have\nphysical properties that are arbitrarily close to (isotropic) FLRW\nmodels over some time scale\n \\(T\\).[11]\n Agreement over the time interval \\(T\\) does not imply global\nagreement, however, as these models have large anisotropies at other\ntimes. Relying on the FLRW models in making extrapolations to the\nearly or late universe requires some justification for ignoring\nmodels, such as these Bianchi models, that mimic their behavior for a\nfinite time interval. On the other hand there are inhomogeneous\nspherically symmetric models that can reproduce exactly the background\nmodel observations (number counts versus redshifts and angular\ndiameter distance versus redshift, for example) with or without a\ncosmological constant (Mustapha et al. 1997). These can be excluded by\ndirect observations with good enough standard candles (Clarkson et al.\n2008) or by observations of structure formation features in such\nuniverses (Clarkson & Maartens 2010); but that exclusion cannot\ntake place unless one indeed examines such models and their\nobservational consequences. \nLack of knowledge of the full space of solutions to EFE makes it\ndifficult to assess the fragility of various inferences cosmologists\nmake based on perturbed FLRW models. A fragile inference depends on\nthe properties of the model holding exactly, contrasted with robust\ninferences that hold even if the models are good approximations (up to\nsome tolerable error) that will hold even if the model is perturbed.\nThe singularity theorems (Hawking & Ellis 1973), for example,\nestablish that the existence of an initial singularity is robust:\nrather than being features specific to the FLRW models, or other\nhighly symmetric models, singularities are generic in models\nsatisfying physically plausible assumptions. The status of various\nother inferences cosmologists make is less clear. For example, how\nsensitively does the observational case in favor of dark energy, which\ncontributes roughly 70% of the total energy density of the universe in\nthe SM, depend upon treating the universe as having almost-FLRW\nspacetime geometry? As mentioned above, recent work has pursued the\npossibility of accounting for the same observations based upon\nlarge-scale inhomogeneities or local back-reaction, without recourse\nto dark\n energy.[12]\n Studies along these lines are needed to evaluate the possibility that\nsubtle dynamical effects, absent in the FLRW models, provide\nalternative explanations of observed phenomena. The deduction also\ndepends on the assumption that the EFE hold at cosmological scales -\nwhich may not be true: maybe for example some form of scalar-tensor\ntheory should be used. More generally, an assessment of the\nreliability of a variety of cosmological inferences requires detailed\nstudy of a larger space of cosmological models. \nThe SM’s account of the evolution of the matter and radiation in\nthe universe reflects the dynamical effect of expansion. Consider a\ncube of spacetime in the early universe, filled with matter and\nradiation. The dynamical effects of the universe’s expansion are\nlocally the same as slowly stretching the cube. For some stages of\nevolution the contents of the cube interact sufficiently quickly that\nthey reach and stay in local thermal equilibrium as the cube changes\nvolume. (Because of isotropy, equal amounts of matter and radiation\nenter and leave the cube from neighboring cubes.) But when the\ninteractions are too slow compared to the rate of expansion, the cube\nchanges volume too rapidly for equilibrium to be maintained. As a\nresult, particle species “freeze out” and decouple, and\nentropy increases. Without a series of departures from equilibrium,\ncosmology would be boring—the system would remain in equilibrium\nwith a state determined solely by the temperature, without a trace of\nthings past. The rate of expansion of the cube varies with cosmic\ntime. Because radiation, matter, and a cosmological constant term (or\ndark energy) dilute with expansion at different rates, an expanding\nuniverse naturally falls into separate epochs, characterized by\ndifferent expansion rates. \nThere are several distinctive epochs in the history of the universe,\naccording to the SM, including the following: \nThe development of a precise cosmological model compatible with the\nrich set of cosmological data currently available is an impressive\nachievement. Cosmology clearly relies very heavily on theory; the\ncosmological parameters that have been the target of observational\ncampaigns are only defined given a background model. The strongest\ncase for accepting the SM rests on the evidence in favor of the\nunderlying physics, in concert with the overdetermination of\ncosmological parameters. The SM includes several free parameters, such\nas the density parameters characterizing the abundance of different\ntypes of matter, each of which can be measured several\n ways.[14]\n These methods have distinctive theoretical assumptions and sources of\nerror. For example, the abundance of deuterium produced during big\nbang nucleosynthesis depends sensitively on the baryon density.\nNucleosynthesis is described using well-tested nuclear physics, and\nthe light element abundances are frozen in within the “first\nthree minutes”. The amplitudes of the acoustic peaks in the CMB\nangular power spectrum depend on the baryon density at the time of\ndecoupling. Current measurements fix the baryon density to an accuracy\nof one percent, and the values determined by these two methods agree\nwithin observational error. This agreement is one of many consistency\nchecks for the\n SM.[15]\n There are important discrepancies, such as that between local versus\nglobal measurements of the Hubble parameter \\(H_0\\) (Luković et\nal. 2016; Bernal et al. 2016). The significance and further\nimplications of these discrepancies is not clear. \nThe SM from nucleosynthesis on can be regarded as well supported by\nmany lines of evidence. The independence and diversity of the\nmeasurements provides some assurance that the SM will not be\nundermined by isolated theoretical mistakes or undetected sources of\nsystematic error. But the SM is far from complete, and there are three\ndifferent types of significant open issues. \nFirst, we do not understand three crucial components of the SM that\nrequire new physics. We do not have a full account of the nature, or\nunderlying dynamics, of dark matter (Bertone et al. 2005), dark energy\n(Peebles & Ratra 2003), or the inflaton field (Lyth & Riotto\n1999; Martin et al. 2014). These are well-recognized problems that\nhave inspired active theoretical and observational work, although as\nwe note below in\n §2.4\n they will be challenging to resolve due to inaccessibility of physics\nat the appropriate scale. \nThe second set of open questions regards structure formation. While\nthe account of structure formation matches several significant\nobserved features, such as the correlations among galaxies in large\nscale surveys, there are a number of open questions about how galaxies\nform (Silk 2017). Many of these, such as the cusp-core problem\n(Weinberg et al. 2015), and the dark halos problem (a great many more\nsmall dark halos are predicted around galaxies than observed) regard\nfeatures of galaxies on relatively small scales, which require\ndetailed modeling of a variety of astrophysical processes over an\nenormous dynamical range. This is also a very active area of research,\ndriven in particular by a variety of new lines of observational\nresearch and large-scale numerical simulations. \nThe third and final set of open issues regards possible observations\nthat would show that the SM is substantially wrong. Any scientific\ntheory should be incompatible with at least some observations, and\nthat is the case for the SM. In the early days of relativistic\ncosmology, the universe was judged to be younger than some stars or\nglobular clusters. This conflict arose due to a mistaken value of the\nHubble constant. There is currently no such age problem for the SM,\nbut obviously discovering an object older than 13.7 Gyr would force a\nmajor re-evaluation of current cosmological models. Another example\nwould be if there was not a dipole in matter number counts that agrees\nwith the CMB dipole (Ellis & Baldwin 1984). \nAlthough cosmology is generally seen as fitting into the general\nphysics paradigm of everything being determined in a bottom up manner,\nas in the discussion above, there is another tradition that sees the\neffect of the global on the local in cosmology. \nThe traditional issues of this kind (Bondi 1960; Ellis & Sciama\n1972; Ellis 2002) are \nIn each case, global boundary conditions have an important effect on\nlocal physics. More recent ones relate to \nRelevant to all this is the idea of an “effective\nhorizon”: the domain that has direct impact on structures\nexisting on the Earth, roughly 1 Mpc co-moving sphere, see Ellis &\nStoeger 2009. This is the part of the universe that actually has a\nsignificant effect on our history. \nMany philosophers hold that evidence is not sufficient to determine\nwhich scientific theory we should choose. Scientific theories make\nclaims about the natural world that extend far beyond what can be\ndirectly established through observations or experiments. Rival\ntheories may fare equally well with regard to some body of data, yet\ngive quite different accounts of the world. Philosophers often treat\nthe existence of such rivals as inevitable: for a given theory, it is\nalways possible to construct rival theories that have “equally\ngood fit” with available data. Duhem (1914 [1954]) gave an\ninfluential characterization of the difficulty in establishing\nphysical theories conclusively, followed a half century later by\nQuine’s arguments for a strikingly general version of\nunderdetermination (e.g., Quine 1970). The nature of this proposed\nunderdetermination of theory by evidence, and appropriate responses to\nit, have been central topics in philosophy of science (Stanford 2009\n[2016]). Although philosophers have identified a variety of distinct\nsenses of underdetermination, they have generally agreed that\nunderdetermination poses a challenge to justifying scientific\ntheories. \nThere is a striking contrast with discussions of underdetermination\namong scientists, who often emphasize instead the enormous difficulty\nin constructing compelling rival\n theories.[16]\n This contrast reflects a disagreement regarding how to characterize\nthe empirical content of theories. Suppose that the empirical content\nof theory consists of a set of observational claims implied by the\ntheory. Philosophers then take the existence of rival theories to be\nstraightforward. Van Fraassen (1980), for example, defines a theory as\n“empirically adequate” if what it says about observable\nphenomena is true, and argues that for any successful theory there are\nrival theories that disagree about theoretical claims. If we demand\nmore of theories than empirical adequacy in this sense, it is possible\nto draw distinctions among theories that philosophers would regard as\nunderdetermined. Furthermore, even when scientists do face a choice\namong competing theories, they are almost never rivals in the\nphilosopher’s sense. Instead, they differ in various ways:\nintended domain of applicability, explanatory scope, importance\nattributed to particular problems, and so on. \nThe scientists’ relatively dismissive attitude towards alleged\nunderdetermination threats may be based on a more demanding conception\nof empirical\n success.[17]\n Scientists demand much more of their theories than mere compatibility\nwith some set of observational claims: they must fit into a larger\nexplanatory scheme, and be compatible with other successful theories.\nGiven a more stringent account of empirical success it is much more\nchallenging to find rival theories. (We return to this issue in\n §5\n below.) \nOne aspect of underdetermination (emphasized by Stanford 2006) is of\nmore direct relevance to scientific debates: current theories may be\nindistinguishable, within a restricted domain, from a successor\ntheory, even though the successor theory makes different predictions\nfor other domains. This raises the question of how far we can rely on\nextrapolating a theory to a new domain. For example, despite its\nsuccess in describing objects moving with low relative velocities in a\nweak gravitational field, where it is nearly indistinguishable from\ngeneral relativity, Newtonian gravity does not apply to other regimes.\nHow far, then, can we rely on a theory to extend our reach? The\nobstacles to making such reliable inferences reflect the specific\ndetails of particular domains of inquiry. Below we will focus on the\nobstacles to answering theoretical questions in cosmology due to the\nstructure of the universe and our limited access to phenomena. \nGiven the grand scope of cosmology, one might expect that many\nquestions must remain unresolved. Basic features of the SM impose two\nfundamental limits to the ambitions of cosmological theorizing. First,\nthe finitude of the speed of light ensures that we have a limited\nobservational window on the universe due to existence of the visual\nhorizon, representing the most distant matter from which we can\nreceive and information by electromagnetic radiation, and the particle\nhorizon, representing the most distant matter with which we can have\nhad any causal interaction (matter up to that distance can influence\nwhat we see at the visual horizon). Recent work has precisely\ncharacterized what can be established via idealized astronomical\nobservations, regarding spacetime geometry within, or outside, our\npast light cone (the observationally accessible region). Second, in\naddition to enormous extrapolations of well-tested physics in the SM,\ncosmologists have explored speculative ideas in physics that can only\nbe tested through their implications for cosmology; the energies\ninvolved are too high to be tested by any accelerator on Earth. Ellis\n(2007) has characterized these speculative aspects of cosmology as\nfalling on the far side of a “physics horizon”. We will\nbriefly discuss how this second type of horizon poses limits for\ncosmological theorizing. In both cases, the type of underdetermination\nthat arises differs from that discussed in the philosophical\nliterature. \nTo what extent can observations determine the spacetime geometry of\nthe universe directly? The question can be posed more precisely in\nterms of the region that is, in principle, accessible to an observer\nat a location in spacetime \\(p\\)—the causal past,\n\\(J^-(p)\\), of that point. This set includes all regions of spacetime\nfrom which signals traveling at or below the speed of light can reach\n\\(p\\). What can observations confined to \\(J^-(p)\\), assuming that GR\nis valid, reveal about the spacetime geometry of \\(J^-(p)\\) itself,\nand the rest of spacetime? \nThe observational cosmology program (Kristian & Sachs 1966; Ellis\net al. 1985) clarifies the extent to which a set of ideal observations\ncan determine the spacetime geometry directly with minimal\ncosmological assumptions. (By contrast, the standard approach starts\nby assuming a background cosmological model and then finding an\noptimal parameter fit.) Roughly put, the ideal data set consists of a\nset of astrophysical objects that can be used as standard candles and\nstandard rulers. If the intrinsic properties and evolution of a\nvariety of sources are given, observations can directly determine the\narea (or luminosity) distance of the sources, and the distortion of\ndistant images determines lensing effects. These observations thus\ndirectly constrain the spacetime geometry of the past light cone\n\\(C^-(p)\\). Number counts of discrete sources (such as galaxies or\nclusters) can be used to infer the total amount of baryonic matter,\nagain granting various assumptions. Ellis et al. (1985) proved the\nremarkable result that an appropriate idealized data set of this kind\nis sufficient, if we grant that EFE hold, to fully fix the spacetime\ngeometry and distribution of matter on the past light cone \\(C^-(p)\\),\nand from that, in the causal past \\(J^-(p)\\) of the observation point\n\\(p\\).[18]\nObservers do not have access to anything like the ideal data set,\nobviously, and in practice cosmologists face challenges in\nunderstanding the nature of sources and their evolution with\nsufficient clarity that they can be used to determine spacetime\ngeometry, so this is the ideal situation. \nWhat does \\(J^-(p)\\) reveal about the rest of spacetime? In classical\nGR, we would not expect the physical state on \\(J^-(p)\\) to determine\nthat of other regions of spacetime—even the causal past of a\npoint just to the future of \n\\(p\\).[19] \nThere are some models in which \\(J^-(p)\\)\ndoes reveal more: “small universe” models are closed\nmodels with a finite maximum length in all directions that is smaller\nthan the visual horizon (Ellis & Schreiber 1986). Observers in\nsuch a model would be able to “see around the universe” in\nall directions, and establish some global properties via direct\nobservation because they would be able to see all matter that\nexists.[20] \nUnless this is the case, the causal past for a single observer, and\neven a collection of causal pasts, place very weak constraints on the\nglobal properties of spacetime. The global properties of a spacetime\ncharacterize its causal structure, such as the presence or absence of\n singularities.[21]\n General relativity tolerates a wide variety of global properties,\nsince EFE impose only a local constraint on the spacetime geometry.\nOne way to make this question precise is to consider whether there are\nany global properties shared by spacetimes that are constructed as\nfollows. For a given spacetime, construct an indistinguishable\ncounterpart that includes the collection of causal pasts\n\\(\\{J^-(p)\\}\\) for all points in the original spacetime. The\nconstructed spacetime is indistinguishable from the first, because for\nany observer in the first spacetime there is a “copy” of\ntheir causal past in the counterpart. It is possible, however, to\nconstruct counterparts that do not have the same global properties as\nthe original spacetime. The property of having a Cauchy surface, for\nexample, need not be shared by an indistinguishable\n counterpart.[22]\n More generally, the only properties that are guaranteed to hold for\nan indistinguishable counterpart are those that can be established\nbased on the causal past of a single point. This line of work\nestablishes that (some) global properties cannot be established\nobservationally, and raises the question of whether there are\nalternative justifications. \nThe case of global spacetime geometry is not a typical instance of\nunderdetermination of theory by evidence, as discussed by\nphilosophers, for two reasons (see Manchak 2009, Norton 2011,\nButterfield 2014). First, this whole discussion assumes that classical\nGR holds; the question regards discriminating among models of a given\ntheory, rather than a choice among competing theories. Second, these\nresults establish that all observations available to us that are\ncompatible with a given spacetime, with some appealing global\nproperty, are equally compatible with its indistinguishable\ncounterparts. But as is familiar from more prosaic examples of the\nproblem of induction, evidence of past events is compatible, in a\nsimilar sense, with many possible futures. Standard accounts of\ninductive inference aim to justify some expectations about the future\nas more reasonable, e.g., those based on extending past uniformities.\nThe challenge in this case is to articulate an account of inductive\ninferences that justifies accepting one spacetime over its\nindistinguishable counterparts. \nAs a specific instance of this challenge, consider the status of the\ncosmological principle, the global symmetry assumed in the derivation\nof the FLRW models. The results above show that all evidence available\nto us is equally compatible with models in which the cosmological\nprinciple does or does not hold. One might take the principle as\nholding a priori, or as a pre-condition for cosmological\ntheorizing (Beisbart 2009). A recent line of work aims to justify the\nFLRW models by appealing to a weaker general principle in conjunction\nwith theorems relating homogeneity and isotropy. Global isotropy\naround every point implies global homogeneity, and it is natural to\nseek a similar theorem with a weaker antecedent formulated in terms of\nobservable quantities. The Ehlers-Geren-Sachs theorem (Ehlers et al.\n1968) shows that if all geodesic fundamental observers in an expanding\nmodel find that freely propagating background radiation is exactly\nisotropic, then their spacetime is an FLRW model. If our causal past\nis “typical”, observations along our worldline will\nconstrain what other observers should see. This is often called the\nCopernican principle—namely, no point \\(p\\) is distinguished\nfrom other points \\(q\\) by any spacetime symmetries or lack thereof\n(there are no “special locations”). There are indirect\nways of testing this principle empirically: the\nSunyaev-Zel’dovich effect can be used to indirectly measure the\nisotropy of the CBR as observed from distant points. Other tests are\ndirect tests with a good enough set of standard candles, and an\nindirect test based on the time drift of cosmological redshift. This\nline of work provides an empirical argument that the observed universe\nis well-approximated by an FLRW model, thus changing that assumption\nfrom a philosophically based starting point to an observationally\ntested foundation. \nThe Standard Model of particle physics and classical GR provide the\nstructure and framework for the SM. But cosmologists have pursued a\nvariety of questions that extend beyond these core theories. In these\ndomains, cosmologists face a form of underdetermination: should a\nphenomena be accounted for by extending the core theories, or by\nchanging physical or astrophysical assumptions? \nThe Soviet physicist Yakov Zel’dovich memorably called the early\nuniverse the “poor man’s accelerator”, because\nrelatively inexpensive observations of the early universe may reveal\nfeatures of high-energy physics well beyond the reach of even the most\nlavishly funded earth-bound accelerators. For many aspects of\nfundamental physics, including quantum gravity in particular,\ncosmology provides the only feasible way to assess competing ideas.\nThis ambitious conception of cosmology as the sole testing ground for\nnew physics extends beyond the standard model of particle physics\n(which is generally thought to be incomplete, even though there are no\nobservations that contradict it). Big bang nucleosynthesis, for\nexample, is an application of well-tested nuclear physics to the early\nuniverse, with scattering cross-sections and other relevant features\nof the physics fixed by terrestrial experiments. While working out how\nnuclear physics applied in detail required substantial effort, there\nwas little uncertainty regarding the underlying physics. By contrast,\nin some domains cosmologists now aim to explain the universe’s\nhistory while at the same time evaluating new physics used in\nconstructing it. \nThis contrast can be clarified in terms of the “physics\nhorizon” (Ellis 2007), which delimits the physical regime\naccessible to terrestrial experiments and observations, roughly in\nterms of energy scales associated with different interactions. The\nhorizon can be characterized more precisely for a chosen theory, by\nspecifying the regions of parameter space that can be directly tested\nby experiments and\n observations.[23]\n Aspects of cosmological theories that extend past the physics horizon\ncannot be independently tested through non-cosmological experiments or\nobservations; the only empirical route to evaluating these ideas is\nthrough their implications for cosmology. (This is not to deny that\nthere may be strong theoretical grounds to favor particular proposals,\nas extensions of the core theories.) \nCosmological physics extending beyond the physics horizon faces an\nunderdetermination threat due to the lack of independent lines of\nrelevant evidence. The case of dark matter illustrates the value of\nsuch independent evidence. Dark matter was first proposed to account\nfor the dynamical behavior of galaxy clusters and galaxies, which\ncould not be explained using Newtonian gravitational theory with only\nthe luminous matter observed. Dark matter also plays a crucial role in\naccounts of structure formation, as it provides the scaffolding\nnecessary for baryonic matter to clump, without conflicting with the\nuniformity of the\n CMB.[24]\n Both inferences to the existence of dark matter rely on gravitational\nphysics, raising the question of whether we should take these\nphenomena as evidence that our gravitational theory fails, rather than\nas evidence for a new type of matter. There is an active research\nprogram (MOND, for Modified Newtonian\nDynamics) devoted to accounting for the relevant phenomena by\nmodifying gravity. Regardless of one’s stance on the relative\nmerits of MOND vs. dark matter (obviously MOND needs to be extended to\na relativistic theory), direct evidence of existence of dark matter,\nor indirect evidence via decay products, would certainly reshape the\ndebate. Efforts have been underway for some time to find dark matter\nparticles through direct interactions with a detector, mediated by the\nweak force. A positive outcome of these experiments would provide\nevidence of the existence of dark matter that does not depend upon\ngravitational\n theory.[25] \nSuch independent evidence is not available for two prominent examples\nof new physics motivated by discoveries in cosmology. “Dark\nenergy” was introduced in studies of structure formation, which\nemployed a non-zero cosmological constant to fit observational\nconstraints (the \\(\\Lambda\\)CDM models). Subsequent observations of\nthe redshift-distance relation, using supernovae (type Ia) as a\nstandard candle, led to the discovery that the expansion of the\nuniverse is\n accelerating.[26]\n (For \\(\\ddot{R}>0\\) in an FLRW model, there must be a contribution\nthat appears in eqn. (\\(\\ref{eq:Ray}\\)) like a positive \\(\\Lambda\\)\nterm.) Rather than treating these observations as simply determining\nthe value of a parameter in the SM, many cosmologists have developed\nphenomenological models of “dark energy” that leads to an\neffective \\(\\Lambda\\). Unlike dark matter, however, the properties of\ndark energy insure that any attempt at non-cosmological detection\nwould be futile: the energy density is so small, and uniform, that any\nlocal experimental study of its properties is practically impossible.\nFurthermore these models are not based in well-motivated physics: they\nhave the nature of ‘saving the phenomena’ in that they are\ntailored to fitting the cosmological observations by curve\n fitting.[27] \nInflationary cosmology originally promised a powerful unification of\nparticle physics and cosmology. The earliest inflationary models\nexplored the consequences of specific scalar fields introduced in\nparticle physics (the then supposed Higgs field for the strong\ninteractions). Yet theory soon shifted to treating the scalar field\nresponsible for inflation as the “inflaton” field, leaving\nits relationship to particle physics unresolved, and the promise of\nunification unfulfilled. If the properties of the inflaton field are\nunconstrained, inflationary cosmology is extremely flexible; it is\npossible to construct an inflationary model that matches any chosen\nevolutionary history of the early\n universe.[28]\n Specific models of inflation, insofar as they specify the features of\nthe field or fields driving inflation and its initial state, do have\npredictive content. In principle, cosmological observations could\ndetermine some of the properties of the inflaton field and so select\namong them (Martin et al. 2014). This could in principle then have\nimplications for a variety of other experiments or observations; yet\nin practice the features of the inflaton field in most viable models\nof inflation guarantee that it cannot be detected in other regimes.\nThe one exception to this is if the inflaton were the electroweak\nHiggs particle detected at the LHC (Ellis & Uzan 2014). This\nremains a viable inflaton candidate, so testing if it is indeed the\ninflaton is an important task (Bezrukov & Gorbunov 2012). \nThe physics horizon poses a challenge because one particularly\npowerful type of evidence—direct experimental detection or\nobservation, with no dependence on cosmological assumptions—is\nunavailable for the physics relevant at earliest times (before\ninflation, and indeed even for baryosynthesis after inflation). Yet\nthis does not imply that competing theories, such as dark matter vs.\nmodified gravity, should be given equal credence. The case in favor of\ndark matter draws on diverse phenomena, and it has been difficult to\nproduce a compelling modified theory of gravity, consistent with GR,\nthat captures the full range of phenomena as an alternative to dark\nmatter. Cosmology typically demands a more intricate assessment of\nbackground assumptions, and the degree of independence of different\ntests, in evaluating proposed extensions of the core theories. Yet\nthis evidence may still be sufficiently strong, in the sense discussed\nmore fully in\n §5\n below, to justify new physics. \nThere is a distinctive form of underdetermination regarding the use of\nstatistics in cosmology, due to the uniqueness of the universe. To\ncompare the universe with the statistical predictions of the SM, we\nconceptualize it as one realization of a family of possible universes,\nand compare what we actually measure with what is predicted to occur\nin the ensemble of hypothetical models. When they are significantly\ndifferent, the key issue is: Are these just statistical fluctuations\nwe can ignore? Or are they serious anomalies that need an\nexplanation? \nThis question arises in several concrete cases: \nHow do we decide? This will depend on the particular measurement (see\ne.g., Kamionkowski & Loeb 1997; Marra et al. 2013), but in general\nbecause of the uniqueness of the universe, we don’t know if\nthese potential anomalies are real, pointing to serious problems with\nthe models, or not real—just statistical flukes in the way the\nfamily of models differs from the one instance that we have at hand,\nthe unique universe that actually exists. In all the physical\nsciences, this is a unique problem of\n cosmology.[29] \nCosmology confronts a distinctive challenge in accounting for the\norigin of the universe. In most other branches of physics the initial\nor boundary conditions of a system do not call out for theoretical\nexplanation. They may reflect, for example, the impact of the\nenvironment, or an arbitrary choice regarding when to cut off the\ndescription of a subsystem of interest. But in cosmology there are\nheated debates regarding what form a “theory of the initial\nstate” should take, and what it should contribute to our\nunderstanding of the universe. This basic question regarding the\nnature of aims of a theory of origins has significant ramifications\nfor various lines of research in cosmology. \nContemporary cosmology at least has a clear target for a theory of\norigins: the SM describes the universe as having expanded and evolved\nover 13.7 billion years from an initial state where many physical\nquantities diverged. In the FLRW models, the cosmic time \\(t\\) can be\nmeasured by the total proper time elapsed along the worldline of a\nfundamental observer, from the “origin” of the universe\nuntil the present epoch. Extrapolating backwards from the present,\nvarious quantities diverge as the cosmic time \\(t \\rightarrow\n0\\)—for example, \\(R(t) \\rightarrow 0\\) and the matter density\ngoes to\n infinity.[30]\n The worldlines of observers cannot be extended arbitrarily far into\nthe past. Although there is no “first moment” of time,\nbecause the very concept of time breaks down as \\(t\\rightarrow 0\\),\nthe age of the universe is the maximum length of these worldlines. \nThe singularity theorems proved in the 60s (see, in particular,\nHawking & Ellis 1973) show that the universe is finite to the past\nin a broad class of cosmological models. Past singularities, signaled\nby the existence of inextendible geodesics with bounded length, must\nbe present in models with a number of plausible features. (Geodesics\nare the curves of extreme length through curved spacetime, and freely\nfalling bodies follow timelike geodesics.) Intuitively, extrapolating\nbackwards from the present, an inextendible geodesic reaches, within\nfinite distance, an “edge” beyond which it cannot be\nextended. There is not a uniquely defined “cosmic time”,\nin general, but the maximum length of these curves reflects the finite\nage of the universe. The singularity theorems plausibly apply to the\nobserved universe, within the domain of applicability of general\nrelativity. There are various related theorems differing in detail,\nbut one common ingredient is an assumption that there is sufficient\nmatter and energy present to guarantee that our past light cone\n refocuses.[31]\n The energy density of the CMB alone is sufficient to justify this\nassumption. The theorems also require an energy condition: a\nrestriction on the types of matter present in the model, guaranteeing\nthat gravity leads to focusing of nearby geodesics. (In eqn.\n(\\(\\ref{eq:Ray}\\)) above, this is the case if \\(\\rho_{\\textit{grav}}\n> 0\\) and \\(\\Lambda=0\\); it is possible to avoid a singularity with\na non-zero cosmological constant, for example, since it appears with\nthe opposite sign as ordinary matter, counteracting this focusing\neffect.) \nThe prediction of singularities is usually taken to be a deep flaw of\n GR.[32]\n One potential problem with singularities is that they may lead to\nfailures of determinism, because the laws “break down” in\nsome sense. This concern only applies to some kinds of singularities,\nhowever. Relativistic spacetimes that are globally hyperbolic have\nCauchy surfaces, and appropriate initial data posed on such surfaces\nfix a unique solution throughout the spacetime. Global hyperbolicity\ndoes not rule out the existence of singularities, and in particular\nthe FLRW models are globally hyperbolic in spite of the existence of\nan initial singularity. The threat to determinism is thus more\nqualified: the laws do not apply “at the singularity\nitself” even though the subsequent evolution is fully\ndeterministic, and there are some types of singularities that pose\nmore serious threats to determinism. \nAnother common claim is that the presence of singularities establish\nthat GR is incomplete, since it fails to describe physics “at\nthe\n singularity”.[33]\n This is difficult to spell out fully without a local analysis of\nsingularities, which would give precise meaning to talk of\n“approaching” or being “near” the singularity.\nIn any case, it is clear that the presence of a singularity in a\ncosmological model indicates that spacetime, as described by GR, comes\nto an end: there is no way of extending the spacetime through the\nsingularity, without violating mathematical conditions needed to\ninsure that the field equations are well-defined. Any description of\nphysical conditions “before the big bang” must be based on\na theory that supersedes GR, and allows for an extension through the\nsingularity. \nThere are two limitations regarding what we can learn about the\norigins of the universe based on the singularity theorems. First,\nalthough these results establish the existence of an initial\nsingularity, they do not provide much guidance regarding its\nstructure. The spacetime structure near a “generic”\ninitial singularity has not yet been fully characterized. Partial\nresults have been established for restricted classes of solutions; for\nexample, numerical simulations and a number of theorems support the\nBKL conjecture, which holds that isotropic, inhomogeneous models\nexhibit a complicated form of chaotic, oscillatory behavior. The\nresulting picture of the approach to the initial singularity contrasts\nsharply with that in FLRW\n models.[34]\n It is also possible to have non-scalar singularities (Ellis &\nKing 1974). \nSecond, classical general relativity does not include quantum effects,\nwhich are expected to be relevant as the singularity is approached.\nCrucial assumptions of the singularity theorems may not hold once\nquantum effects are taken into account. The standard energy conditions\ndo not hold for quantum fields, which can have negative energy\ndensities. This opens up the possibility that a model including\nquantum fields may exhibit a “bounce” rather than collapse\nto a singularity. More fundamentally, GR’s classical spacetime\ndescription may fail to approximate the description provided by a full\ntheory of quantum gravity. According to recent work applying loop\nquantum gravity to cosmology, spacetime collapses to a minimum finite\nsize rather than reaching a true singularity (Ashtekar & Singh\n2011; Bojowald 2011). On this account, GR fails to provide a good\napproximation in the region of the bounce, and the apparent\nsingularity is an artifact. Classical spacetime “emerges”\nfrom a state to which familiar spacetime concepts do not apply. There\nare several accounts of the early universe, motivated by string theory\nand other approaches, that similarly avoid the initial singularity due\nto quantum gravity effects. \nIn practice, cosmologists often take the physical state at the\nexpected boundary of the domain of applicability of GR as the\n“initial state”. (For example, this might be taken as the\nstate specified on a spatial hypersurface at a very early cosmic time.\nHowever, the domain of applicability of GR is not well understood,\ngiven uncertainty about quantum gravity.) Projecting observed features\nof the universe backwards leads to an initial state with three\npuzzling\n features:[35] \nOn a more phenomenological approach, the gravitational degrees of\nfreedom of the initial state could simply be chosen to fit with later\nobservations, but many proposed “theories of initial\nconditions” aim to account for these features based on new\nphysical principles. The theory of inflation discussed below aims to\nexplain these issues. \nThere are three main approaches to theories of the initial state, all\nof which have been pursued by cosmologists since the late 60s in\ndifferent forms. Expectations for what a theory of initial conditions\nshould achieve have been shaped, in particular, by inflationary\ncosmology. Inflation provided a natural account of the three otherwise\npuzzling features of the initial state emphasized in the previous\nsection. Prior to inflation, these features were regarded as\n“enigmas” (Dicke & Peebles 1979), but after inflation,\naccounting for these features has served as an eligibility requirement\nfor any proposed theory of the early universe. \nThe first approach aims to reduce dependence on special initial\nconditions by introducing a phase of attractor dynamics. This phase of\ndynamical evolution “washes away” the traces of earlier\nstates, in the sense that a probability distribution assigned over\ninitial states converges towards an equilibrium distribution. Misner\n(1968) introduced a version of this approach (his “chaotic\ncosmology program”), proposing that free-streaming neutrinos\ncould isotropize an initially anisotropic state. Inflationary\ncosmology was initially motivated by a similar idea: a\n“generic” or “random” initial state at the\nPlanck time would be expected to be “chaotic”, far from a\nflat FLRW model. During an inflationary stage, arbitrary initial\nstates are claimed to converge towards a state with the three features\ndescribed above. \nThe second approach regards the initial state as extremely special\nrather than generic. Penrose, in particular, has argued that the\ninitial state must be very special to explain time’s arrow; the\nusual approaches fail to take seriously the fact that gravitational\ndegrees of freedom are not excited in the early universe like the\nothers (Penrose 2016). Penrose (1979) treats the second law as arising\nfrom a law-like constraint on the initial state of the universe,\nrequiring that it has low entropy. Rather than introducing a\nsubsequent stage of dynamical evolution that erases the imprint of the\ninitial state, we should aim to formulate a “theory of initial\nconditions” that accounts for its special features.\nPenrose’s conjecture is that the Weyl curvature tensor\napproaches zero as the initial singularity is approached; his\nhypothesis is explicitly time asymmetric, and implies that the early\nuniverse approaches an FLRW solution. (It does not account for the\nobserved perturbations, however.) Later he proposed the idea of\nConformal Cyclic Cosmology, where such a special initial state at the\nstart of one expansion epoch is the result of expansion in a previous\nepoch that wiped out almost all earlier traces of matter and radiation\n(Penrose 2016). \nA third approach rejects the framework accepted by the other two\nproposals, and regards the “initial state” as a misnomer:\nit should instead by regarded as a “branch point” where\nour pocket universe separated off from a larger multiverse. (There are\nstill, of course, questions regarding the initial state of the\nmultiverse ensemble, if one exists.) We will return to this approach\nin\n §5\n below. \nA dynamical approach, even if it is successful in describing a phase\nof the universe’s evolution, arguably does not offer a complete\nsolution to the problem of initial conditions: it collapses into one\nof the other two approaches. For example, an inflationary stage can\nonly begin in a region of spacetime if the inflaton field and the\ngeometry are uniform over a sufficiently large region, such that the\nstress-energy tensor is dominated by the potential term (implying that\nthe derivative terms are small) and the gravitational entropy is\nsmall. There are other model-dependent constraints on the initial\nstate of the inflaton field. One way to respond is to adopt\nPenrose’s point of view, namely that this reflects the need to\nchoose a special initial state, or to derive one from a previous\nexpansion phase. The majority of those working in inflationary\ncosmology instead appeal to the third approach: rather than treating\ninflation as an addition to standard big-bang evolution in a single\nuniverse, we should treat the observed universe as part of a\nmultiverse, discussed below. But even this must have a theory of\ninitial conditions. \nCosmology provokes questions about the limits of scientific\nexplanation because it lacks many of the features that are present in\nother areas of physics. Physical laws are usually regarded as\ncapturing the features of a type of system that remain invariant under\nsome changes, and explanations often work by placing a particular\nevent in larger context. Theories of the initial state cannot appeal\nto either idea: we have access to only one universe, and there is no\nlarger context to appeal to in explaining its properties. This\ncontrast between the types of explanation available in cosmology and\nother areas of physics has often led to dissatisfaction (see, e.g.,\nUnger & Smolin 2014). At the very least, cosmology forces us to\nreconsider basic questions about modalities, and what constitutes\nscientific explanation. \nOne challenge to establishing theories of the initial state is\nentirely epistemic. As emphasized in\n §2.4,\n we lack independent experimental probes of physics at the relevant\nscales, so the extensions of core theories described above are only\ntested indirectly through their implications for cosmology. This\nlimitation reflects contingent facts about the universe, namely the\ncontrast between the energy scales of the early universe and those\naccessible to us, and does not follow from the uniqueness of the\nuniverse per se. Yet this limitation does not imply that it would be\nimpossible to establish laws. There are cases in the history of\nphysics, such as celestial mechanics, where confidence in a\ntheory’s laws is based primarily on successful application under\ncontinually improving standards of precision. \nA further conceptual challenge regards whether it even makes sense to\nseek “laws” in cosmology (Munitz 1962; Ellis 2007). Laws\nare usually taken to cover multiple instances of some type of\nphenomena, or family of objects. What can we mean by\n“laws” for a unique object (the universe as a whole) or a\nunique event (its origin)? \nCompeting philosophical analyses of laws of nature render different\nverdicts on the possibility of cosmological laws. Cosmological laws,\nif possible, differ from local physical laws in a variety of\nways—they do not apply to subsystems of the universe, they lack\nmultiple instances, and etc. Philosophical accounts of laws take\ndifferent features to be essential to law-hood. For example, the\ninfluential Mill-Ramsey-Lewis account takes the laws to be axioms of\nthe deductive system capturing some body of physical knowledge that\noptimally balances strength (the scope of derived claims) and\nsimplicity (the number of axioms) (see, e.g., Loewer 1996). It is\nquite plausible that a constraint on the initial state, such as\nPenrose’s Weyl curvature hypothesis, would count as a law on\nthis account. By contrast, accounts that take other features, such as\ngoverning evolution, as essential, reach the opposite verdict. \nFinally, there are a number of conceptual pitfalls regarding what\nwould count as an adequate “explanation” of the origins of\nthe universe. What is the target of such explanations, and what can be\nused in providing an explanation? The target might be the state\ndefined at the earliest time when extrapolations based on the SM can\nbe trusted. The challenge is that this state then needs to be\nexplained in terms of a physical theory, quantum gravity, whose basic\nconcepts are still obscure to us. This is a familiar challenge in\nphysics, where substantial work is often required to clarify how\ncentral concepts (such as space and time) are modified by a new\ntheory. An explanation of origins in this first sense would explain\nhow it is that classical spacetime emerges from a quantum gravity\nregime. While any such proposals remain quite speculative, the form of\nthe explanation is similar to other cases in physics: what is\nexplained is the applicability of an older, less fundamental theory\nwithin some domain. Such an explanation does not address ultimate\nquestions regarding why the universe exists—instead, such\nquestions are pushed back one step, into the quantum gravity\nregime. \nMany discussions of origins pursue a more ambitious target: they aim\nto explain the creation of the universe “from\n nothing”.[39]\n The target is the true initial state, not just the boundary of\napplicability of the SM. The origins are supposedly then explained\nwithout positing an earlier phase of evolution; supposedly this can be\nachieved, for example, by treating the origin of the universe as a\nfluctuation away from a vacuum state. Yet obviously a vacuum state is\nnot nothing: it exists in a spacetime, and has a variety of\nnon-trivial properties. It is a mistake to take this explanation as\nsomehow directly addressing the metaphysical question of why there is\nsomething rather than\n nothing.[40] \nThe physical conditions necessary for our existence impose a selection\neffect on what we observe. The significance of this point for\ncosmological theorizing is exemplified by Dicke’s criticism of\nDirac’s speculative “large number hypothesis”. Dirac\n(1937) noted the age of the universe expressed in terms of fundamental\nconstants in atomic physics is an extremely large number (roughly\n\\(10^{39}\\)), which coincides with other large, dimensionless numbers\ndefined in terms of fundamental constants. Inspired by this\ncoincidence, he proposed that the large numbers vary to maintain this\norder of magnitude agreement, implying (for example) that the\ngravitational “constant” \\(G\\) is a function of cosmic\ntime. Dicke (1961) noted\nthat creatures like us, made of carbon produced in an earlier\ngeneration of red giants and sustained by the light and heat of a main\nsequence star, can only exist within a restricted interval of cosmic\ntimes, and that Dirac’s coincidence holds for observations made\nwithin this interval. Establishing that the coincidence holds at a\nrandomly chosen \\(t\\) would support Dirac’s hypothesis, however\nslightly, but Dicke’s argument shows that our evidence\ndoes not do so. \nDicke’s reasoning illustrates how taking selection effects into\naccount can mitigate surprise, and undermine the apparent implications\nof facts like those noted by Dirac (see Roush 2003). These facts\nreflect biases in the evidence available to us, rather than supporting\nhis hypothesis. It is also clear that Dicke’s argument is\n“anthropic” in only a very limited sense: his argument\ndoes not depend on a detailed characterization of human observers. All\nthat matters is that we can exist at a cosmic time constrained by the\ntime scales of stellar evolution. \nHow to account for selection effects, within a particular approach to\nconfirmation theory, is one central issue in discussions of anthropic\nreasoning. This question is intertwined with other issues that are\nmore muddled and contentious. Debates among cosmologists regarding\n“anthropic principles” ignited in the 70s, prompted by the\nsuggestion that finely-tuned features of the universe—such as\nthe universe’s isotropy (Collins & Hawking 1973)—can\nbe explained as necessary conditions for the existence of\nobservers.[41]\nMore recently, a number of cosmologists have argued that cosmological\ntheories should be evaluated based on predictions for what a\n“typical” observer should expect to see. These ideas have\ndovetailed with work in formal epistemology. A number of philosophers\nhave developed extensions of Bayesianism to account for\n“self-locating” evidence, for\nexample.[42]\nThis kind of evidence includes indexical information characterizing an\nagent’s beliefs about their identity and location. At present\nwork in this area has not reached a consensus, and we will present a\nbrief overview of some of the considerations that have motivated\ndifferent positions in these debates. \nIn cosmology the most famous example of an “anthropic\nprediction” is Weinberg (1987)’s prediction for\n \\(\\Lambda\\).[43]\n One part of Weinberg’s argument is similar to Dicke’s: he\nargued that there are anthropic bounds on \\(\\Lambda\\), due to its\nimpact on structure formation. The existence of large, gravitationally\nbound structures such as galaxies is only possible if \\(\\Lambda\\)\nfalls within certain bounds. Weinberg went a step further than Dicke,\nand considered what value of \\(\\Lambda\\) a “typical\nobserver” should see. He assumed that observers occupy different\nlocations within a multiverse, and that the value of \\(\\Lambda\\)\nvaries across different regions. Weinberg further argues that the\nprior probability assigned to different values of \\(\\Lambda\\) should\nbe uniform within the anthropic bounds. Typical observers should\nexpect to see a value close to the mean of the anthropic bounds,\nleading to Weinberg’s prediction for \\(\\Lambda\\). \nEssential to Weinberg’s argument is an appeal to the principle\nof indifference, applied to a class of\n observers.[44]\n We should calculate what we expect to observe, that is, as if we are\na “random choice” among all possible\n observers.[45]\n Bostrom (2002) argues that indifference-style reasoning is necessary\nto respond to the problem of “freak observers”. As Bostrom\nformulates it, the problem is that in an infinite universe,\nany observation \\(O\\) is true for some observer\n(even if only for an observer who has fluctuated into existence from\nthe vacuum). His response is that we should evaluate theories based\nnot on the claim that some observer sees \\(O\\), but on an\nindexical claim: that is, we make the observation \\(O\\). He\nassumes that we are a “random” choice among the class of\npossible observers. (How to justify such a strong claim is a major\nchallenge for this line of thought.) If we grant the assumption, then\nwe can assign low probability to the observations of the\n“freak” observers, and recover the evidential value of\n\\(O\\). \nThere are three immediate questions regarding this proposal. The first\nis called the “reference class” problem. The assignments\nof probabilities to events requires specifying how they are grouped\n together.[46]\n Obviously, what is typical with respect to one reference class will\nnot be typical with respect to another (compare, for example,\n“conscious observers” with “carbon-based\nlife”). Second, the principle of indifference has been\nthoroughly criticized as a justification for probability in other\ncontexts; what justifies the use of indifference in this case? Why\nshould we take ourselves as “randomly chosen” among an\nappropriate reference class? The third problem reflects the intended\napplication of these ideas: Bostrom and other authors in this line of\nwork are particularly concerned with observes that may occupy an\ninfinite universe. There is no proof that the universe is in fact\ninfinite. These are all pressing problems for those who hold that the\nprinciple of indifference is essential to making cosmological\npredictions. \nFurthermore, one way of implementing this approach leads to absurd\nconsequences. The Doomsday Argument, for example, claims to reach a\nstriking conclusion about the future of the human species without any\nempirical input (see, e.g., Leslie 1992; Gott 1993; Bostrom 2002).\nSuppose that we are “typical” humans, in the sense of\nhaving a birth rank that is randomly selected among the collection of\nall humans that have ever lived. We should then expect that there are\nnearly as many humans before and after us in overall birth rank. For\nthis to be true, given current rates of population growth, there must\nbe a catastrophic drop in the human population\n(“Doomsday”) in the near future. The challenge to\nadvocates of indifference applied to observers is to articulate\nprinciples that avoid such consequences, while still solving (alleged)\nproblems such as that of freak observers. \nIn sum, one approach to anthropic reasoning aims to clarify the rules\nof reasoning applicable to predictions made by observers in a large or\ninfinite universe. This line of work is motivated by the idea that\nwithout such principles we face a severe skeptical predicament, as\nobservations would not have any bearing on the theory. Yet there is\nstill not general agreement on the new principles required to handle\nthese cases, which are of course not scientifically testable\nprinciples: they are philosophically based proposals. According to an\nalternative approach, selection effects can and should be treated\nwithin the context of a Bayesian approach to inductive inference (see\nNeal 2006; Trotta 2008). On this line of thought,\n“predictions” like those that Bostrom and others hope to\nanalyze play no direct role in the evaluation of cosmological\ntheories, so further principles governing anthropic reasoning are\nsimply not necessary. There is much further work to be done in\nclarifying and assessing these (and other) approaches to anthropic\n reasoning.[47] \nFine-tuning arguments start from a conflict between two different\nperspectives on certain features of cosmology (or other physical\ntheories). On the first perspective, the existence of creatures like\nus seems to be sensitive to a wide variety of aspects of cosmology and\nphysics. To be more specific, the prospects for life depend\nsensitively on the values of the various fundamental constants that\nappear in these theories. The SM includes about 10 constants, and the\nparticle physics standard model includes about 20\n more.[48]\n Tweaking the SM, or the standard model of particle physics, by\nchanging the values of these constants seems to lead to a barren\n cosmos.[49]\n Focusing on the existence of “life” runs the risk of\nbeing too provincial; we don’t have a good general account of\nwhat physical systems can support intelligent life. Yet it does seem\nplausible that intelligence requires an organism with complex\nstructural features, living in a sufficiently stable environment. \nAt a bare minimum, the existence of life seems to require the\nexistence of complex structures at a variety of scales, ranging from\ngalaxies to planetary systems to macro-molecules. Such complexity is\nextremely sensitive to the values of the fundamental constants of\nnature. From this perspective, the existence of life in the universe\nis fragile in the sense that it depends sensitively on these aspects\nof the underlying theory. \nThis view contrasts sharply with the status of the constants from the\nperspective of fundamental physics. Particle physicists typically\nregard their theories as effective field theories, which suffice for\ndescribing interactions at some specified energy scale. These theories\ninclude various constants, characterizing the relative strength of the\ninteractions they describe, that cannot be further explained by the\neffective field theory. The constants can be fixed by experimental\nresults, but are not derivable from fundamental physical principles.\n(If the effective field theory can be derived from a more fundamental\ntheory, the value of the constants can in principle be determined by\nintegrating out higher-energy degrees of freedom. But this merely\npushes the question back one step: the constants appearing in the more\nfundamental theory are determined experimentally.) Similarly, the\nconstants appearing in the SM are treated as contingent features of\nthe universe. There is no underlying physical principle that sets, for\nexample, the cosmological densities of different kinds of matter, or\nthe value of the Hubble constant. \nSo features of our theories that appear entirely contingent, from the\npoint of view of physics, are necessary to account for the complexity\nof the observed universe and the very possibility of life. The\nfine-tuning argument starts from a sense of unease about this\nsituation: shouldn’t something as fundamental as the complexity\nof the universe be explained by the laws or basic\nprinciples of the theory, and not left to brute facts regarding\nthe values of various constants? The unease develops into serious\ndiscomfort if the specific values of the constants are taken to be\nextremely unlikely: how could the values of all these constants be\njust right, by sheer coincidence? \nIn many familiar cases, our past experience is a good guide to when an\napparent coincidence calls for further explanation. As Hume\nemphasized, however, intuitive assessments from everyday life of\nwhether a given event is likely, or requires a further explanation, do\nnot extend to cosmology. Recent formulations of fine-tuning arguments\noften introduce probabilistic considerations. The constants are\n“fine-tuned”, meaning that the observed values are\n“improbable” in some sense. Introducing a well-defined\nprobability over the constants would provide a response to Hume:\nrather than extrapolating our intuitions, we would be drawing on the\nformal machinery of our physical theories to identify fine-tuning.\nPromising though this line of argument may be, there is not an obvious\nway to define physical probabilities over the values of different\nconstants, or over other features of the laws. There is nothing like\nthe structure used to justify physical probabilities in other\ncontexts, such as equilibrium statistical\n mechanics.[50] \nThere are four main responses to fine-tuning: \nIn the next section we discuss the last response in more detail; see\n §3\n for further discussion of the third response. \nThe multiverse response replaces a single, apparently finely-tuned\nuniverse within an ensemble of universes, combined with an appeal to\nanthropic selection. Suppose that all possible values of the\nfundamental constants are realized in individual elements of the\nensemble. Many of these universes will be inhospitable to life. In\ncalculating the probabilities that we observe specific values of the\nfundamental constants, we need only consider the subset of universe\ncompatible with the existence of complexity (or some more specific\nfeature associated with life). If we have some way of assigning\nprobabilities over the ensemble, we could then calculate the\nprobability associated with our measured values. These calculations\nwill resolve the fine-tuning puzzles if they show that we observe\ntypical values for a complex (or life-permitting) universe. \nMany cosmologists have argued in favor of a specific version of the\nmultiverse called eternal inflation\n (EI).[51]\n On this view, the rapid expansion hypothesized by inflationary\ncosmology continues until arbitrarily late times in some regions, and\ncomes to an end (with a transition to slower expansion) in others.\nThis leads to a global structure of “pocket” universes\nembedded within a larger multiverse. \nOn this line of thought, the multiverse should be accepted for the\nsame reason we accept many claims about what we cannot directly\nobserve—namely, as an inevitable consequence of an established\nphysical theory. It is not clear, however, that EI is inevitable, as\nnot all inflationary models, arguably including those favored by CMB\nobservations, have the kind of potential that leads to\n EI.[52]\n Accounts of how inflation leads to EI rely on speculative\n physics.[53]\n Furthermore, if inflation does lead to EI, that threatens to\nundermine the original reasons for accepting inflation (Smeenk 2014):\nrather than the predictions regarding the state produced at the end of\ninflation taken to provide evidence for inflation, EI seems to imply\nthat, as Guth (2007) put it, in EI “anything that can happen\nwill happen; in fact, it will happen an infinite number of\ntimes”. \nThere have been two distinct approaches to recovering some empirical\ncontent in this\n situation.[54]\n First, there may be traces of the early formation of the pocket\nuniverses, the remnants of collisions between neighboring\n“bubbles”, left on the CMB sky (Aguirre & Johnson\n2011). Detection of a distinctive signature that cannot be explained\nby other means would provide evidence for the multiverse. However,\nthere is no expectation that a multiverse theory would generically\npredict such traces; for example, if the collision occurs too early\nthe imprint is erased by subsequent inflationary expansion. \nThe other approach regards predictions for the fundamental constants,\nsuch as Weinberg’s prediction of \\(\\Lambda\\) discussed above.\nThe process of forming the pocket universes is assumed to yield\nvariation in the local, low-energy physics in each pocket. Predictions\nfor the values of the fundamental constants follow from two things:\n(1) a specification of the probabilities for different values of the\nconstant over the ensemble, and (2) a treatment of the selection\neffect imposed by restricting consideration to pocket universes with\nobservers and then choosing a “typical” observer. \nThe aim is to obtain probabilistic predictions for what a typical\nobserver should see in the EI multiverse. Yet there are several\nchallenges to overcome, alongside those mentioned above related to\nanthropics. The assumption that the formation of pocket universes\nleads to variation in constants is just an assumption, which is not\nyet justified by a plausible, well-tested dynamical theory. The most\nwidely discussed challenge in the physics literature is the\n“measure problem”: roughly, how to assign\n“size” to different regions of the multiverse, as a first\nstep towards assigning probabilities. It is difficult to define a\nmeasure because the EI multiverse is usually taken to be an infinite\nensemble, lacking in the kinds of structure used in constructing a\nmeasure. On our view, these unmet challenges undercut the hope that\nthe EI multiverse yields probabilistic predictions. And without such\nan account, the multiverse proposal does not have any testable\nconsequences. If everything happens somewhere in the ensemble, then\nany potential observation is compatible with the theory. \nSupposing that we grant a successful resolution of all these\nchallenges, the merits of a multiverse solution of fine-tuning\nproblems could then be evaluated by comparison with competing ideas.\nThe most widely cited evidence in favor of a multiverse is\nWeinberg’s prediction for the value of \\(\\Lambda\\), discussed\nabove. There are other proposals to explain the observed value of\n\\(\\Lambda\\); Wang, Zhu, and Unruh (2017), for example, treat the\nquantum vacuum as extremely inhomogeneous, and argue that resonance\namong the vacuum fluctuations leads to a small \\(\\Lambda\\). \nThe unease many have about multiverse proposals are only reinforced by\nthe liberal appeals to “infinities” in discussion of the\n idea.[55]\n Many have argued, for example, that we must formulate an account of\nanthropic reasoning that applies to a truly infinite, rather than\nmerely very large, universe. Claims that we occupy one of infinitely\nmany possible pocket universes, filled with an infinity of other\nobservers, rest on an enormous and speculative extrapolation. Such\nclaims fail to take seriously the concept of infinity, which is not\nmerely a large number. Hilbert (1925 [1983]) emphasized that while\ninfinity is required to complete mathematics, it does not occur\nanywhere in the accessible physical universe. One response is\nto require that infinities in cosmology should have a restricted use.\nIt may be useful to introduce infinity as part of an explanatory\naccount of some aspect of cosmology, as is common practice in\nmathematical models that introduce various idealizations. Yet this\ninfinity should be eliminable, such that the explanation of the\nphenomena remains valid when the idealization is\n removed.[56]\n Even for those who regard this demand as too stringent, there\ncertainly needs to be more care in clarifying and justifying claims\nregarding infinities. \nIn sum, interest in the multiverse stems primarily from speculations\nabout the consequences of inflation for the global structure of the\nuniverse. The main points of debate regard whether EI is a disaster\nfor inflation, undermining the possibility of testing inflation at\nall, and how much predictions such as that for \\(\\Lambda\\) lend\ncredence to these\n speculations.[57]\n Resolution of these questions is needed to decide whether the\nmultiverse can be tested in a stronger sense, going beyond the special\ncases (such as bubble collisions) that may provide more direct\nevidence. \nAs mentioned at the start, the uniqueness of the universe raises\nspecific problems as regards cosmology as a science. First we consider\nissues to do with verification of cosmological models, and then make a\ncomment as regards interpreting the human implications of\ncosmology \nThe basic challenge in cosmology regards how to test and evaluate\ncosmological models, given our limited access to the unique universe.\nAs discussed above, current cosmological models rely in part on\nextrapolations of well-tested local physics along with novel\nproposals, such as the inflaton field. The challenge is particularly\npressing in evaluating novel claims that only have cosmological\nimplications, due to the physics horizon\n (§2.4).\n Distinctions that are routinely employed in other areas of physics,\nsuch as that between laws and initial conditions, or chance and\nnecessity, are not directly applicable, due to the uniqueness of the\nuniverse. \nRecent debates regarding the legitimacy of different lines of research\nin cosmology reflect different responses to this challenge. One\nresponse is to retreat to hypothetico-deductivism (HD): a hypothesis\nreceives an incremental boost in confidence when one of its\nconsequences is verified (and a decrease if it is\n falsified).[58]\n Proponents of inflation argue, for example, that inflation should be\naccepted based on its successful prediction of a flat universe with a\nspecific spectrum of density perturbations. Some advocates of the\nmultiverse take its successful prediction of the value of \\(\\Lambda\\)\nas the most compelling evidence in its favor. \nDespite its appeal, there are well-known problems with taking HD as a\nsufficient account of how evidence supports theories (this is often\ncalled “naïve HD”). In particular, the naïve\nview lacks the resources to draw distinctions among underdetermined\nrival theories that make the same predictions (see Crupi 2013 [2016]).\nWe take it as given that scientists do draw distinctions among\ntheories that naïve HD would treat as on par, as is reflected in\njudgments regarding how much a given body of evidence supports a\nparticular theory. Scientists routinely distinguish among, for\nexample, theories that may merely “fit the data” as\nopposed to those that accurately capture laws governing a particular\ndomain, and evaluate some successful predictions as being far more\nrevealing than others. \nA second response is that the challenge requires a more sophisticated\nmethodology. This may take the form of acknowledging explicitly the\ncriteria that scientists use to assess desirability of scientific\ntheories (Ellis 2007), which include considerations of explanatory\npower, consistency with other theories, and other factors, in addition\nto compatibility with the evidence. These come into conflict in\nunexpected ways in cosmology, and these different factors should be\nclearly articulated and weighed against one another. Alternatively,\none might try to show that some of these desirable features, such as\nthe ability to unify diverse phenomena, should be taken as part of\nwhat constitutes empirical\n success.[59]\n This leads to a more demanding conception of empirical success,\nexemplified by historical cases such as Perrin’s argument in\nfavor of the atomic constitution of matter. \nFinally, a key issue is what scope do we expect our theories to have.\nEllis (2017) makes a distinction between Cosmology, which is\nthe physically based subject dealt with in the textbooks listed in\nthis article, dealing with the expansion of the universe, galaxies,\nnumber counts, background radiation, and so on, and\nCosmologia, where one takes all that as given but adds in\nconsideration about the meaning this all has for life. Clearly the\nanthropic discussions mentioned above are a middle\n ground.[60]\n However a number of popular science books by major scientists are\nappearing that make major claims about Cosmologia, based purely in\narguments from fundamental physics together with astronomical\nobservations. We will make just one remark about this here. If one is\ngoing to consider Cosmologia seriously, it is incumbent on one to take\nseriously the full range of data appropriate to that enterprise. That\nis, the data needed for the attempted scope of such a theory must\ninclude data to do with the meaning of life as well as data derived\nfrom telescopes, laboratory experiments, and particle colliders. It\nmust thus include data about good and evil, life and death, fear and\nhope, love and pain, writings from the great philosophers and writers\nand artists who have lived in human history and pondered the meaning\nof life on the basis of their life experiences. This is all of great\nmeaning to those who live on Earth (and hence in the Universe). To\nproduce books saying that science proves there is no purpose in the\nuniverse is pure myopia. It just means that one has shut ones eyes to\nall the data that relates to purpose and meaning; and that one\nsupposes that the only science is physics (for psychology and biology\nare full of purpose).","contact.mail":"george.ellis@uct.ac.za","contact.domain":"uct.ac.za"}]
