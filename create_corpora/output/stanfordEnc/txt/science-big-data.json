[{"date.published":"2020-05-29","url":"https://plato.stanford.edu/entries/science-big-data/","author1":"Sabina Leonelli","entry":"science-big-data","body.text":"\n\n\nBig Data promises to revolutionise the production of knowledge within\nand beyond science, by enabling novel, highly efficient ways to plan,\nconduct, disseminate and assess research. The last few decades have\nwitnessed the creation of novel ways to produce, store, and analyse\ndata, culminating in the emergence of the field of data\nscience, which brings together computational, algorithmic,\nstatistical and mathematical techniques towards extrapolating\nknowledge from big data. At the same time, the Open Data\nmovement—emerging from policy trends such as the push for Open\nGovernment and Open Science—has encouraged the sharing and\ninterlinking of heterogeneous research data via large digital\ninfrastructures. The availability of vast amounts of data in\nmachine-readable formats provides an incentive to create efficient\nprocedures to collect, organise, visualise and model these data. These\ninfrastructures, in turn, serve as platforms for the development of\nartificial intelligence, with an eye to increasing the reliability,\nspeed and transparency of processes of knowledge creation. Researchers\nacross all disciplines see the newfound ability to link and\ncross-reference data from diverse sources as improving the accuracy\nand predictive power of scientific findings and helping to identify\nfuture directions of inquiry, thus ultimately providing a novel\nstarting point for empirical investigation. As exemplified by the rise\nof dedicated funding, training programmes and publication venues, big\ndata are widely viewed as ushering in a new way of performing research\nand challenging existing understandings of what counts as scientific\nknowledge.\n\n\nThis entry explores these claims in relation to the use of big data\nwithin scientific research, and with an emphasis on the philosophical\nissues emerging from such use. To this aim, the entry discusses how\nthe emergence of big data—and related technologies, institutions\nand norms—informs the analysis of the following themes: \n\n how statistics, formal and computational models help to\nextrapolate patterns from data, and with which consequences; \n\n the role of critical scrutiny (human intelligence) in machine\nlearning, and its relation to the intelligibility of research\nprocesses; \n\n the nature of data as research components; \n\n the relation between data and evidence, and the role of data as\nsource of empirical insight; \n\n the view of knowledge as theory-centric; \n\n understandings of the relation between prediction and causality;\n\n\n the separation of fact and value; and \n\n the risks and ethics of data science. \n\nThese are areas where attention to research practices revolving around\nbig data can benefit philosophy, and particularly work in the\nepistemology and methodology of science. This entry doesn’t cover the\nvast scholarship in the history and social studies of science that has\nemerged in recent years on this topic, though references to some of\nthat literature can be found when conceptually relevant. Complementing\nhistorical and social scientific work in data studies, the\nphilosophical analysis of data practices can also elicit significant\nchallenges to the hype surrounding data science and foster a critical\nunderstanding of the role of data-fuelled artificial intelligence in\nresearch.\n\nWe are witnessing a progressive “datafication” of social\nlife. Human activities and interactions with the environment are being\nmonitored and recorded with increasing effectiveness, generating an\nenormous digital footprint. The resulting “big data” are a\ntreasure trove for research, with ever more sophisticated\ncomputational tools being developed to extract knowledge from such\ndata. One example is the use of various different types of data\nacquired from cancer patients, including genomic sequences,\nphysiological measurements and individual responses to treatment, to\nimprove diagnosis and treatment. Another example is the integration of\ndata on traffic flow, environmental and geographical conditions, and\nhuman behaviour to produce safety measures for driverless vehicles, so\nthat when confronted with unforeseen events (such as a child suddenly\ndarting into the street on a very cold day), the data can be promptly\nanalysed to identify and generate an appropriate response (the car\nswerving enough to avoid the child while also minimising the risk of\nskidding on ice and damaging to other vehicles). Yet another instance\nis the understanding of the nutritional status and needs of a\nparticular population that can be extracted from combining data on\nfood consumption generated by commercial services (e.g., supermarkets,\nsocial media and restaurants) with data coming from public health and\nsocial services, such as blood test results and hospital intakes\nlinked to malnutrition. In each of these cases, the availability of\ndata and related analytic tools is creating novel opportunities for\nresearch and for the development of new forms of inquiry, which are\nwidely perceived as having a transformative effect on science as a\nwhole. \nA useful starting point in reflecting on the significance of such\ncases for a philosophical understanding of research is to consider\nwhat the term “big data” actually refers to within\ncontemporary scientific discourse. There are multiple ways to define\nbig data (Kitchin 2014, Kitchin & McArdle 2016). Perhaps the most\nstraightforward characterisation is as large datasets that\nare produced in a digital form and can be analysed through\ncomputational tools. Hence the two features most commonly\nassociated with Big Data are volume and velocity. Volume\nrefers to the size of the files used to archive and spread data.\nVelocity refers to the pressing speed with which data is\ngenerated and processed. The body of digital data created by research\nis growing at breakneck pace and in ways that are arguably impossible\nfor the human cognitive system to grasp and thus require some form of\nautomated analysis. \nVolume and velocity are also, however, the most disputed features of\nbig data. What may be perceived as “large volume” or\n“high velocity” depends on rapidly evolving technologies\nto generate, store, disseminate and visualise the data. This is\nexemplified by the high-throughput production, storage and\ndissemination of genomic sequencing and gene expression data, where\nboth data volume and velocity have dramatically increased within the\nlast two decades. Similarly, current understandings of big data as\n“anything that cannot be easily captured in an Excel\nspreadsheet” are bound to shift rapidly as new analytic software\nbecomes established, and the very idea of using spreadsheets to\ncapture data becomes a thing of the past. Moreover, data size and\nspeed do not take account of the diversity of data types used by\nresearchers, which may include data that are not generated in digital\nformats or whose format is not computationally tractable, and which\nunderscores the importance of data provenance (that is, the conditions\nunder which data were generated and disseminated) to processes of\ninference and interpretation. And as discussed below, the emphasis on\nphysical features of data obscures the continuing dependence of data\ninterpretation on circumstances of data use, including specific\nqueries, values, skills and research situations. \nAn alternative is to define big data not by reference to their\nphysical attributes, but rather by virtue of what can and cannot be\ndone with them. In this view, big data is a heterogeneous ensemble of\ndata collected from a variety of different sources, typically (but not\nalways) in digital formats suitable for algorithmic processing, in\norder to generate new knowledge. For example boyd and Crawford (2012:\n663) identify big data with “the capacity to search, aggregate\nand cross-reference large datasets”, while O’Malley and\nSoyer (2012) focus on the ability to interrogate and interrelate\ndiverse types of data, with the aim to be able to consult them as a\nsingle body of evidence. The examples of transformative “big\ndata research” given above are all easily fitted into this view:\nit is not the mere fact that lots of data are available that makes a\ndifferent in those cases, but rather the fact that lots of data can be\nmobilised from a wide variety of sources (medical records,\nenvironmental surveys, weather measurements, consumer behaviour). This\naccount makes sense of other characteristic “v-words” that\nhave been associated with big data, including: \nThis list of features, though not exhaustive, highlights how big data\nis not simply “a lot of data”. The epistemic power of big\ndata lies in their capacity to bridge between different research\ncommunities, methodological approaches and theoretical frameworks that\nare difficult to link due to conceptual fragmentation, social barriers\nand technical difficulties (Leonelli 2019a). And indeed, appeals to\nbig data often emerge from situations of inquiry that are at once\ntechnically, conceptually and socially challenging, and where existing\nmethods and resources have proved insufficient or inadequate (Sterner\n& Franz 2017; Sterner, Franz, & Witteveen 2020). \nThis understanding of big data is rooted in a long history of\nresearchers grappling with large and complex datasets, as exemplified\nby fields like astronomy, meteorology, taxonomy and demography (see\nthe collections assembled by Daston 2017; Anorova et al. 2017; Porter\n& Chaderavian 2018; as well as Anorova et al. 2010, Sepkoski 2013,\nStevens 2016, Strasser 2019 among others). Similarly, biomedical\nresearch—and particularly subfields such as epidemiology,\npharmacology and public health—has an extensive tradition of\ntackling data of high volume, velocity, variety and volatility, and\nwhose validity, veracity and value are regularly negotiated and\ncontested by patients, governments, funders, pharmaceutical companies,\ninsurances and public institutions (Bauer 2008). Throughout the\ntwentieth century, these efforts spurred the development of\ntechniques, institutions and instruments to collect, order, visualise\nand analyse data, such as: standard classification systems and\nformats; guidelines, tools and legislation for the management and\nsecurity of sensitive data; and infrastructures to integrate and\nsustain data collections over long periods of time (Daston 2017). \nThis work culminated in the application of computational technologies,\nmodelling tools and statistical methods to big data (Porter 1995;\nHumphreys 2004; Edwards 2010), increasingly pushing the boundaries of\ndata analytics thanks to supervised learning, model fitting, deep\nneural networks, search and optimisation methods, complex data\nvisualisations and various other tools now associated with artificial\nintelligence. Many of these tools are based on algorithms whose\nfunctioning and results are tested against specific data samples (a\nprocess called “training”). These algorithms are\nprogrammed to “learn” from each interaction with novel\ndata: in other words, they have the capacity to change themselves in\nresponse to new information being inputted into the system, thus\nbecoming more attuned to the phenomena they are analysing and\nimproving their ability to predict future behaviour. The scope and\nextent of such changes is shaped by the assumptions used to build the\nalgorithms and the capability of related software and hardware to\nidentify, access and process information of relevance to the learning\nin question. There is however a degree of unpredictability and opacity\nto these systems, which can evolve to the point of defying human\nunderstanding (more on this below). \nNew institutions, communication platforms and regulatory frameworks\nalso emerged to assemble, prepare and maintain data for such uses\n(Kitchin 2014), such as various forms of digital data infrastructures,\norganisations aiming to coordinate and improve the global data\nlandscape (e.g., the Research Data Alliance), and novel measures for\ndata protection, like the General Data Protection Regulation launched\nin 2017 by the European Union. Together, these techniques and\ninstitutions afford the opportunity to assemble and interpret data at\na much broader scale, while also promising to deliver finer levels of\ngranularity in data\n analysis.[1]\n They increase the scope of any investigation by making it possible\nfor researchers to link their own findings to those of countless\nothers across the world, both within and beyond the academic sphere.\nBy enhancing the mobility of data, they facilitate their repurposing\nfor a variety of goals that may have been unforeseeable when the data\nwere originally generated. And by transforming the role of data within\nresearch, they heighten their status as valuable research outputs in\nand of themselves. These technological and methodological developments\nhave significant implications for philosophical conceptualisations of\ndata, inferential processes and scientific knowledge, as well as for\nhow research is conducted, organised, governed and assessed. It is to\nthese philosophical concerns that I now turn. \nBig data are often associated to the idea of data-driven\nresearch, where learning happens through the accumulation of data and\nthe application of methods to extract meaningful patterns from those\ndata. Within data-driven inquiry, researchers are expected to use data\nas their starting point for inductive inference, without relying on\ntheoretical preconceptions—a situation described by advocates as\n“the end of theory”, in contrast to theory-driven\napproaches where research consists of testing a hypothesis (Anderson\n2008, Hey et al. 2009). In principle at least, big data constitute the\nlargest pool of data ever assembled and thus a strong starting point\nto search for correlations (Mayer-Schönberger & Cukier 2013).\nCrucial to the credibility of the data-driven approach is the efficacy\nof the methods used to extrapolate patterns from data and evaluate\nwhether or not such patterns are meaningful, and what\n“meaning” may involve in the first place. Hence, some\nphilosophers and data scholars have argued that  \nthe most important and distinctive characteristic of Big Data [is] its\nuse of statistical methods and computational means of analysis,\n(Symons & Alvarado 2016: 4)  \nsuch as for instance machine learning tools, deep neural networks and\nother “intelligent” practices of data handling.  \nThe emphasis on statistics as key adjudicator of validity and\nreliability of patterns extracted from data is not novel. Exponents of\nlogical empiricism looked for logically watertight methods to secure\nand justify inference from data, and their efforts to develop a theory\nof probability proceeded in parallel with the entrenchment of\nstatistical reasoning in the sciences in the first half of the\ntwentieth century (Romeijn 2017). In the early 1960s, Patrick Suppes\noffered a seminal link between statistical methods and the philosophy\nof science through his work on the production and interpretation of\ndata models. As a philosopher deeply embedded in experimental\npractice, Suppes was interested in the means and motivations of key\nstatistical procedures for data analysis such as data reduction and\ncurve fitting. He argued that once data are adequately\nprepared for statistical modelling, all the concerns and\nchoices that motivated data processing become irrelevant to their\nanalysis and interpretation. This inspired him to differentiate\nbetween models of theory, models of experiment and models of data,\nnoting that such different components of inquiry are governed by\ndifferent logics and cannot be compared in a straightforward way. For\ninstance,  \nthe precise definition of models of the data for any given experiment\nrequires that there be a theory of the data in the sense of the\nexperimental procedure, as well as in the ordinary sense of the\nempirical theory of the phenomena being studied. (Suppes 1962: 253)\n \nSuppes viewed data models as necessarily statistical: that is, as\nobjects  \ndesigned to incorporate all the information about the experiment which\ncan be used in statistical tests of the adequacy of the theory.\n(Suppes 1962: 258)  \nHis formal definition of data models reflects this decision, with\nstatistical requirements such as homogeneity, stationarity and order\nidentified as the ultimate criteria to identify a data model Z and\nevaluate its adequacy:  \nZ is an N-fold model of the data for experiment Y if and\nonly if there is a set Y and a probability measure P on\nsubsets of Y such that \\(Y = \\langle Y, P\\rangle\\) is a model\nof the theory of the experiment, Z is an N-tuple of elements of\nY, and Z satisfies the statistical tests of homogeneity,\nstationarity and order. (1962: 259)  \nThis analysis of data models portrayed statistical methods as key\nconduits between data and theory, and hence as crucial components of\ninferential reasoning. \nThe focus on statistics as entry point to discussions of inference\nfrom data was widely promoted in subsequent philosophical work.\nProminent examples include Deborah Mayo, who in her book Error and\nthe Growth of Experimental Knowledge asked:  \nWhat should be included in data models? The overriding constraint is\nthe need for data models that permit the statistical assessment of fit\n(between prediction and actual data); (Mayo 1996: 136)  \nand Bas van Fraassen, who also embraced the idea of data models as\n“summarizing relative frequencies found in data” (Van\nFraassen 2008: 167). Closely related is the emphasis on statistics as\nmeans to detect error within datasets in relation to specific\nhypotheses, most prominently endorsed by the error-statistical\napproach to inference championed by Mayo and Aris Spanos (Mayo &\nSpanos 2009a). This\napproach aligns with the emphasis on computational methods for data\nanalysis within big data research, and supports the idea that the\nbetter the inferential tools and methods, the better the chance to\nextract reliable knowledge from data. \nWhen it comes to addressing methodological challenges arising from the\ncomputational analysis of big data, however, statistical expertise\nneeds to be complemented by computational savvy in the training and\napplication of algorithms associated to artificial intelligence,\nincluding machine learning but also other mathematical procedures for\noperating upon data (Bringsjord & Govindarajulu 2018). Consider\nfor instance the problem of overfitting, i.e., the mistaken\nidentification of patterns in a dataset, which can be greatly\namplified by the training techniques employed by machine learning\nalgorithms. There is no guarantee that an algorithm trained to\nsuccessfully extrapolate patterns from a given dataset will be as\nsuccessful when applied to other data. Common approaches to this\nproblem involve the re-ordering and partitioning of both data and\ntraining methods, so that it is possible to compare the application of\nthe same algorithms to different subsets of the data\n(“cross-validation”), combine predictions arising from\ndifferently trained algorithms (“ensembling”) or use\nhyperparameters (parameters whose value is set prior to data training)\nto prepare the data for analysis. \nHandling these issues, in turn, requires  \nfamiliarity with the mathematical operations in question, their\nimplementations in code, and the hardware architectures underlying\nsuch implementations. (Lowrie 2017: 3)  \nFor instance, machine learning  \naims to build programs that develop their own analytic or descriptive\napproaches to a body of data, rather than employing ready-made\nsolutions such as rule-based deduction or the regressions of more\ntraditional statistics. (Lowrie 2017: 4)  \nIn other words, statistics and mathematics need to be complemented by\nexpertise in programming and computer engineering. The ensemble of\nskills thus construed results in a specific epistemological approach\nto research, which is broadly characterised by an emphasis on the\nmeans of inquiry as the most significant driver of research goals and\noutputs. This approach, which Sabina Leonelli characterised as\ndata-centric, involves “focusing more on the processes\nthrough which research is carried out than on its ultimate\noutcomes” (Leonelli 2016: 170). In this view, procedures,\ntechniques, methods, software and hardware are the prime motors of\ninquiry and the chief influence on its outcomes. Focusing more\nspecifically on computational systems, John Symons and Jack Horner\nargued that much of big data research consists of\nsoftware-intensive science rather than data-driven research:\nthat is, science that depends on software for its design, development,\ndeployment and use, and thus encompasses procedures, types of\nreasoning and errors that are unique to software, such as for example\nthe problems generated by attempts to map real-world quantities to\ndiscrete-state machines, or approximating numerical operations (Symons\n& Horner 2014: 473). Software-intensive science is arguably\nsupported by an algorithmic rationality focused on the\nfeasibility, practicality and efficiency of algorithms, which is\ntypically assessed by reference to concrete situations of inquiry\n(Lowrie 2017).  \nAlgorithms are enormously varied in their mathematical structures and\nunderpinning conceptual commitments, and more philosophical work needs\nto be carried out on the specifics of computational tools and software\nused in data science and related applications—with emerging work\nin philosophy of computer science providing an excellent way forward\n(Turner & Angius 2019). Nevertheless, it is clear that whether or\nnot a given algorithm successfully applies to the data at hand depends\non factors that cannot be controlled through statistical or even\ncomputational methods: for instance, the size, structure and format of\nthe data, the nature of the classifiers used to partition the data,\nthe complexity of decision boundaries and the very goals of the\ninvestigation. \nIn a forceful critique informed by the philosophy of mathematics,\nChristian Calude and Giuseppe Longo argued that there is a fundamental\nproblem with the assumption that more data will necessarily yield more\ninformation:  \nvery large databases have to contain arbitrary correlations. These\ncorrelations appear only due to the size, not the nature, of data.\n(Calude & Longo 2017: 595)  \nThey conclude that big data analysis is by definition unable to\ndistinguish spurious from meaningful correlations and is therefore a\nthreat to scientific research. A related worry, sometimes dubbed\n“the curse of dimensionality” by data scientists, concerns\nthe extent to which the analysis of a given dataset can be scaled up\nin complexity and in the number of variables being considered. It is\nwell known that the more dimensions one considers in classifying\nsamples, for example, the larger the dataset on which such dimensions\ncan be accurately generalised. This demonstrates the continuing, tight\ndependence between the volume and quality of data on the one hand, and\nthe type and breadth of research questions for which data need to\nserve as evidence on the other hand. \nDetermining the fit between inferential methods and data requires high\nlevels of expertise and contextual judgement (a situation known within\nmachine learning as the “no free lunch theorem”). Indeed,\noverreliance on software for inference and data modelling can yield\nhighly problematic results. Symons and Horner note that the use of\ncomplex software in big data analysis makes margins of error\nunknowable, because there is no clear way to test them statistically\n(Symons & Horner 2014: 473). The path complexity of programs with\nhigh conditionality imposes limits on standard error correction\ntechniques. As a consequence, there is no effective method for\ncharacterising the error distribution in the software except by\ntesting all paths in the code, which is unrealistic and intractable in\nthe vast majority of cases due to the complexity of the code. \nRather than acting as a substitute, the effective and responsible use\nof artificial intelligence tools in big data analysis requires the\nstrategic exercise of human intelligence—but for this to happen,\nAI systems applied to big data need to be accessible to scrutiny and\nmodification. Whether or not this is the case, and who is best\nqualified to exercise such scrutiny, is under dispute. Thomas Nickles\nargued that the increasingly complex and distributed algorithms used\nfor data analysis follow in the footsteps of long-standing scientific\nattempts to transcend the limits of human cognition. The resulting\nepistemic systems may no longer be intelligible to humans: an\n“alien intelligence” within which “human abilities\nare no longer the ultimate criteria of epistemic success”\n(Nickles forthcoming). Such unbound cognition holds the promise of\nenabling powerful inferential reasoning from previously unimaginable\nvolumes of data. The difficulties in contextualising and scrutinising\nsuch reasoning, however, sheds doubt on the reliability of the\nresults. It is not only machine learning algorithms that are becoming\nincreasingly inaccessible to evaluation: beyond the complexities of\nprogramming code, computational data analysis requires a whole\necosystem of classifications, models, networks and inference tools\nwhich typically have different histories and purposes, and whose\nrelation to each other—and effects when they are used\ntogether—are far from understood and may well be\nuntraceable. \nThis raises the question of whether the knowledge produced by such\ndata analytic systems is at all intelligible to humans, and if so,\nwhat forms of intelligibility it yields. It is certainly the case that\nderiving knowledge from big data may not involve an increase in human\nunderstanding, especially if understanding is understood as an\nepistemic skill (de Regt 2017). This may not be a problem to those who\nawait the rise of a new species of intelligent machines, who may\nmaster new cognitive tools in a way that humans cannot. But as\nNickles, Nicholas Rescher (1984), Werner Callebaut (2012) and others\npointed out, even in that case “we would not have arrived at\nperspective-free science” (Nickles forthcoming). While the human\nhistories and assumptions interwoven into these systems may be hard to\ndisentangle, they still affect their outcomes; and whether or not\nthese processes of inquiry are open to critical scrutiny, their telos,\nimplications and significance for life on the planet arguably should\nbe. As argued by Dan McQuillan (2018), the increasing automation of\nbig data analytics may foster acceptance of a Neoplatonist\nmachinic metaphysics, within which mathematical structures\n“uncovered” by AI would trump any appeal to human\nexperience. Luciano Floridi echoes this intuition in his analysis of\nwhat he calls the infosphere:  \nThe great opportunities offered by Information and Communication\nTechnologies come with a huge intellectual responsibility to\nunderstand them and take advantage of them in the right way.\n(2014: vii) \nThese considerations parallel Paul Humphreys’s long-standing\ncritique of computer simulations as epistemically opaque\n(Humphreys 2004, 2009)—and particularly his definition of what\nhe calls essential epistemic opacity:  \nA process is essentially epistemically opaque to X if and only\nif it is impossible, given the nature of X, for\nX to know all of the epistemically relevant elements of the\nprocess. (Humphreys 2009: 618)  \nDifferent facets of the general problem of epistemic opacity are\nstressed within the vast philosophical scholarship on the role of\nmodelling, computing and simulations in the sciences: the implications\nof lacking experimental access to the concrete parts of the world\nbeing modelled, for instance (Morgan 2005; Parker 2009; Radder 2009);\nthe difficulties in testing the reliability of computational methods\nused within simulations (Winsberg 2010; Morrison 2015); the relation\nbetween opacity and justification (Durán & Formanek 2018);\nthe forms of black-boxing associated to mechanistic reasoning\nimplemented in computational analysis (Craver and Darden 2013; Bechtel\n2016); and the debate over the intrinsic limits of computational\napproaches and related expertise (Collins 1990; Dreyfus 1992). Roman\nFrigg and Julian Reiss argued that such issues do not constitute\nfundamental challenges to the nature of inquiry and modelling, and in\nfact exist in a continuum with traditional methodological issues\nwell-known within the sciences (Frigg & Reiss 2009). Whether or\nnot one agrees with this position (Humphreys 2009; Beisbart 2012), big\ndata analysis is clearly pushing computational and statistical methods\nto their limit, thus highlighting the boundaries to what even\ntechnologically augmented human beings are capable of knowing and\nunderstanding. \nResearch on big data analysis thus sheds light on elements of the\nresearch process that cannot be fully controlled, rationalised or even\nconsidered through recourse to formal tools.  One such element is the work required to present empirical data in\na machine-readable format that is compatible with the software and\nanalytic tools at hand. Data need to be selected, cleaned and prepared\nto be subjected to statistical and computational analysis. The\nprocesses involved in separating data from noise, clustering data so\nthat it is tractable, and integrating data of different formats turn\nout to be highly sophisticated and theoretically structured, as\ndemonstrated for instance by James McAllister’s (1997, 2007,\n2011) and Uljana Feest’s (2011) work on data patterns, Marcel\nBoumans’s and Leonelli’s comparison of clustering\nprinciples across fields (forthcoming), and James Griesemer’s\n(forthcoming) and Mary Morgan’s (forthcoming) analyses of the\npeculiarities of datasets. Suppes was so concerned by what he called\nthe “bewildering complexity” of data production and\nprocessing activities, that he worried that philosophers would not\nappreciate the ways in which statistics can and does help scientists\nto abstract data away from such complexity. He described the large\ngroup of research components and activities used to prepare data for\nmodelling as “pragmatic aspects” encompassing “every\nintuitive consideration of experimental design that involved no formal\nstatistics” (Suppes 1962: 258), and positioned them as the\nlowest step of his hierarchy of models—at the opposite end of\nits pinnacle, which are models of theory. Despite recent efforts to\nrehabilitate the methodology of inductive-statistical modelling and\ninference (Mayo & Spanos 2009b), this approach has been shared by\nmany philosophers who regard processes of data production and\nprocessing as so chaotic as to defy systematic analysis. This explains\nwhy data have received so little consideration in philosophy of\nscience when compared to models and theory. \nThe question of how data are defined and identified, however, is\ncrucial for understanding the role of big data in scientific research.\nLet us now consider two philosophical views—the\nrepresentational view and the relational\nview—that are both compatible with the emergence of big\ndata, and yet place emphasis on different aspects of that phenomenon,\nwith significant implications for understanding the role of data\nwithin inferential reasoning and, as we shall see in the next section,\nas evidence. The representational view construes data as\nreliable representations of reality which are produced via the\ninteraction between humans and the world. The interactions that\ngenerate data can take place in any social setting regardless of\nresearch purposes. Examples range from a biologist measuring the\ncircumference of a cell in the lab and noting the result in an Excel\nfile, to a teacher counting the number of students in her class and\ntranscribing it in the class register. What counts as data in these\ninteractions are the objects created in the process of description\nand/or measurement of the world. These objects can be digital (the\nExcel file) or physical (the class register) and form a footprint of a\nspecific interaction with the natural world. This\nfootprint—“trace” or “mark”, in the\nwords of Ian Hacking (1992) and Hans-Jörg Rheinberger (2011),\nrespectively—constitutes a crucial reference point for analytic\nstudy and for the extraction of new insights. This is the reason why\ndata forms a legitimate foundation to empirical knowledge: the\nproduction of data is equivalent to “capturing” features\nof the world that can be used for systematic study. According to the\nrepresentative approach, data are objects with fixed and unchangeable\ncontent, whose meaning, in virtue of being representations of reality,\nneeds to be investigated and revealed step-by-step through adequate\ninferential methods. The data documenting cell shape can be modelled\nto test the relevance of shape to the elasticity, permeability and\nresilience of cells, producing an evidence base to understand\ncell-to-cell signalling and development. The data produced counting\nstudents in class can be aggregated with similar data collected in\nother schools, producing an evidence base to evaluate the density of\nstudents in the area and their school attendance frequency. \nThis reflects the intuition that data, especially when they come in\nthe form of numerical measurements or images such as photographs,\nsomehow mirror the phenomena that they are created to document, thus\nproviding a snapshot of those phenomena that is amenable to study\nunder the controlled conditions of research. It also reflects the idea\nof data as “raw” products of research, which are as close\nas it gets to unmediated knowledge of reality. This makes sense of the\ntruth-value sometimes assigned to data as irrefutable sources of\nevidence—the Popperian idea that if data are found to support a\ngiven claim, then that claim is corroborated as true at least as long\nas no other data are found to disprove it. Data in this view represent\nan objective foundation for the acquisition of knowledge and this very\nobjectivity—the ability to derive knowledge from human\nexperience while transcending it—is what makes knowledge\nempirical. This position is well-aligned with the idea that big data\nis valuable to science because it facilitates the (broadly understood)\ninductive accumulation of knowledge: gathering data collected via\nreliable methods produces a mountain of facts ready to be analysed\nand, the more facts are produced and connected with each other, the\nmore knowledge can be extracted. \nPhilosophers have long acknowledged that data do not speak for\nthemselves and different types of data require different tools for\nanalysis and preparation to be interpreted (Bogen 2009 [2013]).\nAccording to the representative view, there are correct and incorrect\nways of interpreting data, which those responsible for data analysis\nneed to uncover. But what is a “correct” interpretation in\nthe realm of big data, where data are consistently treated as mobile\nentities that can, at least in principle, be reused in countless\ndifferent ways and towards different objectives? Perhaps more than at\nany other time in the history of science, the current mobilisation and\nre-use of big data highlights the degree to which data\ninterpretation—and with it, whatever data is taken to\nrepresent—may differ depending on the conceptual, material and\nsocial conditions of inquiry. The analysis of how big data travels\nacross contexts shows that the expectations and abilities of those\ninvolved determine not only the way data are interpreted, but also\nwhat is regarded as “data” in the first place (Leonelli\n& Tempini forthcoming). The representative view of data as objects\nwith fixed and contextually independent meaning is at odds with these\nobservations. \nAn alternative approach is to embrace these findings and abandon the\nidea of data as fixed representations of reality altogether. Within\nthe relational view, data are objects that are treated as\npotential or actual evidence for scientific claims in ways that can,\nat least in principle, be scrutinised and accounted for (Leonelli\n2016). The meaning\nassigned to data depends on their provenance, their physical features\nand what these features are taken to represent, and the motivations\nand instruments used to visualise them and to defend specific\ninterpretations. The reliability of data thus depends on the\ncredibility and strictness of the processes used to produce and\nanalyse them. The presentation of data; the way they are identified,\nselected, and included (or excluded) in databases; and the information\nprovided to users to re-contextualise them are fundamental to\nproducing knowledge and significantly influence its content. For\ninstance, changes in data format—as most obviously involved in\ndigitisation, data compression or archival procedures— can have\na significant impact on where, when, and who uses the data as source\nof knowledge. \nThis framework acknowledges that any object can be used as a datum, or\nstop being used as such, depending on the circumstances—a\nconsideration familiar to big data analysts used to pick and mix data\ncoming from a vast variety of sources. The relational view also\nexplains how, depending on the research perspective interpreting it,\nthe same dataset may be used to represent different aspects of the\nworld (“phenomena” as famously characterised by James\nBogen and James Woodward, 1988). When considering the full cycle of\nscientific inquiry from the viewpoint of data production and analysis,\nit is at the stage of data modelling that a specific\nrepresentational value is attributed to data (Leonelli 2019b). \nThe relational view of data encourages attention to the history of\ndata, highlighting their continual evolution and sometimes radical\nalteration, and the impact of this feature on the power of data to\nconfirm or refute hypotheses. It explains the critical importance of\ndocumenting data management and transformation processes, especially\nwith big data that transit far and wide over digital channels and are\ngrouped and interpreted in different ways and formats. It also\nexplains the increasing recognition of the expertise of those who\nproduce, curate, and analyse data as indispensable to the effective\ninterpretation of big data within and beyond the sciences; and the\ninextricable link between social and ethical concerns around the\npotential impact of data sharing and scientific concerns around the\nquality, validity, and security of data (boyd & Crawford 2012;\nTempini & Leonelli, 2018). \nDepending on which view on data one takes, expectations around what\nbig data can do for science will vary dramatically. The\nrepresentational view accommodates the idea of big data as providing\nthe most comprehensive, reliable and generative knowledge base ever\nwitnessed in the history of science, by virtue of its sheer size and\nheterogeneity. The relational view makes no such commitment, focusing\ninstead on what inferences are being drawn from such data at any given\npoint, how and why. \nOne thing that the representational and relational views agree on is\nthe key epistemic role of data as empirical evidence for knowledge\nclaims or interventions. While there is a large philosophical\nliterature on the nature of evidence (e.g., Achinstein 2001; Reiss\n2015; Kelly 2016), however, the relation between data and evidence has\nreceived less attention. This is arguably due to an implicit\nacceptance, by many philosophers, of the representational view of\ndata. Within the representational view, the identification of what\ncounts as data is prior to the study of what those data can be\nevidence for: in other words, data are “givens”, as the\netymology of the word indicates, and inferential methods are\nresponsible for determining whether and how the data available to\ninvestigators can be used as evidence, and for what. The focus of\nphilosophical attention is thus on formal methods to single out errors\nand misleading interpretations, and the probabilistic and/or\nexplanatory relation between what is unproblematically taken to be a\nbody of evidence and a given hypothesis. Hence much of the expansive\nphilosophical work on evidence avoids the term “data”\naltogether. Peter Achinstein’s seminal work is a case in point:\nit discusses observed facts and experimental results, and whether and\nunder which conditions scientists would have reasons to believe such\nfacts, but it makes no mention of data and related processing\npractices (Achinstein 2001). \nBy contrast, within the relational view an object can only be\nidentified as datum when it is viewed as having value as evidence.\nEvidence becomes a category of data identification, rather than a\ncategory of data use as in the representational view (Canali 2019). Evidence is thus constitutive of the very notion of data\nand cannot be disentangled from it. This involves accepting that the\nconditions under which a given object can serve as evidence—and\nthus be viewed as datum - may change; and that should this evidential\nrole stop altogether, the object would revert back into an ordinary,\nnon-datum item. For example, the photograph of a plant taken by a\ntourist in a remote region may become relevant as evidence for an\ninquiry into the morphology of plants from that particular locality;\nyet most photographs of plants are never considered as evidence for an\ninquiry into the features and functioning of the world, and of those\nwho are, many may subsequently be discarded as uninteresting or no\nlonger pertinent to the questions being asked. \nThis view accounts for the mobility and repurposing that characterises\nbig data use, and for the possibility that objects that were not\noriginally generated in order to serve as evidence may be subsequently\nadopted as such. Consider Mayo and Spanos’s “minimal\nscientific principle for evidence”, which they define as\nfollows:  \nData x0 provide poor evidence for H if they\nresult from a method or procedure that has little or no ability of\nfinding flaws in\nH, even if H is false. (Mayo & Spanos 2009b)  \nThis principle is compatible with the relational view of data since it\nincorporates cases where the methods used to generate and process data\nmay not have been geared towards the testing of a hypothesis H: all it\nasks is that such methods can be made relevant to the testing of H, at\nthe point in which data are used as evidence for H (I shall come back\nto the role of hypotheses in the handling of evidence in the next\nsection). \nThe relational view also highlights the relevance of practices of data\nformatting and manipulation to the treatment of data as evidence, thus\ntaking attention away from the characteristics of the data objects\nalone and focusing instead on the agency attached to and enabled by\nthose characteristics. Nora Boyd has provided a way to conceptualise\ndata processing as an integral part of inferential processes, and thus\nof how we should understand evidence. To this aim she introduced the\nnotion of “line of evidence”, which she defines as:  \na sequence of empirical results including the records of data\ncollection and all subsequent products of data processing generated on\nthe way to some final empirical constraint. (Boyd 2018:406)  \nShe thus proposes a conception of evidence that embraces both data and\nthe way in which data are handled, and indeed emphasises the\nimportance of auxiliary information used when assessing data for\ninterpretation, which includes  \nthe metadata regarding the provenance of the data records and the\nprocessing workflow that transforms them. (2018: 407)  \nAs she concludes,  \ntogether, a line of evidence and its associated metadata compose what\nI am calling an “enriched line of evidence”. The\nevidential corpus is then to be made up of many such enriched lines of\nevidence. (2018: 407) \nThe relational view thus fosters a functional and contextualist\napproach to evidence as the manner through which one or more objects\nare used as warrant for particular knowledge items (which can be\npropositional claims, but also actions such as specific decisions or\nmodes of conduct/ways of operating). This chimes with the contextual\nview of evidence defended by Reiss (2015), John Norton’s work on\nthe multiple, tangled lines of inferential reasoning underpinning\nappeals to induction (2003), and Hasok Chang’s emphasis on the\nepistemic activities required to ground evidential claims (2012).\nBuilding on these ideas and on Stephen Toulmin’s seminal work on\nresearch schemas (1958),\nAlison Wylie has gone one step further in evaluating the inferential\nscaffolding that researchers (and particularly archaeologists, who so\noften are called to re-evaluate the same data as evidence for new\nclaims; Wylie 2017) need to make sense of their data, interpret them\nin ways that are robust to potential challenges, and modify\ninterpretations in the face of new findings. This analysis enabled\nWylie to formulate a set of conditions for robust evidential\nreasoning, which include epistemic security in the chain of evidence,\ncausal anchoring and causal independence of the data used as evidence,\nas well as the explicit articulation of the grounds for calibration of\nthe instruments and methods involved (Chapman & Wylie 2016; Wylie\nforthcoming). A similar conclusion is reached by Jessey Wright’s\nevaluation of the diverse data analysis techniques that\nneuroscientists use to make sense of functional magnetic resonance\nimaging of the brain (fMRI scans):  \ndifferent data analysis techniques reveal different patterns in the\ndata. Through the use of multiple data analysis techniques,\nresearchers can produce results that are locally robust. (Wright 2017:\n1179)  \nWylie’s and Wright’s analyses exemplify how a relational\napproach to data fosters a normative understanding of “good\nevidence” which is anchored in situated judgement—the\narguably human prerogative to contextualise and assess the\nsignificance of evidential claims. The advantages of this view of\nevidence are eloquently expressed by Nancy Cartwright’s critique\nof both philosophical theories and policy approaches that do not\nrecognise the local and contextual nature of evidential reasoning. As\nshe notes,  \nwe need a concept that can give guidance about what is relevant to\nconsider in deciding on the probability of the hypothesis, not one\nthat requires that we already know significant facts about the\nprobability of the hypothesis on various pieces of evidence.\n(Cartwright 2013: 6)  \nThus she argues for a notion of evidence that is not too restrictive,\ntakes account of the difficulties in combining and selecting evidence,\nand allows for contextual judgement on what types of evidence are best\nsuited to the inquiry at hand (Cartwright 2013, 2019). Reiss’s\nproposal of a pragmatic theory of evidence similarly aims to  \ntakes scientific practice [..] seriously, both in terms of its greater\nuse of knowledge about the conditions under which science is practised\nand in terms of its goal to develop insights that are relevant to\npractising scientists. (Reiss 2015: 361) \nA better characterisation of the relation between data and evidence,\npredicated on the study of how data are processed and aggregated, may\ngo a long way towards addressing these demands. As aptly argued by\nJames Woodward, the evidential relationship between data and claims is\nnot a “a purely formal, logical, or a priori matter”\n(Woodward 2000: S172–173). This again sits uneasily with the expectation that\nbig data analysis may automate scientific discovery and make human\njudgement redundant. \nLet us now return to the idea of data-driven inquiry, often suggested\nas a counterpoint to hypothesis-driven science (e.g., Hey et al.\n2009). Kevin Elliot and colleagues have offered a brief history of\nhypothesis-driven inquiry (Elliott et al. 2016), emphasising how\nscientific institutions (including funding programmes and publication\nvenues) have pushed researchers towards a Popperian conceptualisation\nof inquiry as the formulation and testing of a strong hypothesis. Big\ndata analysis clearly points to a different and arguably Baconian\nunderstanding of the role of hypothesis in science. Theoretical\nexpectations are no longer seen as driving the process of inquiry and\nempirical input is recognised as primary in determining the direction\nof research and the phenomena—and related\nhypotheses—considered by researchers. \nThe emphasis on data as a central component of research poses a\nsignificant challenge to one of the best-established philosophical\nviews on scientific knowledge. According to this view, which I shall\nlabel the theory-centric view of science, scientific\nknowledge consists of justified true beliefs about the world. These\nbeliefs are obtained through empirical methods aiming to test the\nvalidity and reliability of statements that describe or explain\naspects of reality. Hence scientific knowledge is conceptualised as\ninherently propositional: what counts as an output are claims\npublished in books and journals, which are also typically presented as\nsolutions to hypothesis-driven inquiry. This view acknowledges the\nsignificance of methods, data, models, instruments and materials\nwithin scientific investigations, but ultimately regards them as means\ntowards one end: the achievement of true claims about the world.\nReichenbach’s seminal distinction between contexts of discovery\nand justification exemplifies this position (Reichenbach 1938).\nTheory-centrism recognises research components such as data and\nrelated practical skills as essential to discovery, and more\nspecifically to the messy, irrational part of scientific work that\ninvolves value judgements, trial-and-error, intuition and exploration\nand within which the very phenomena to be investigated may not have\nbeen stabilised. The justification of claims, by contrast, involves\nthe rational reconstruction of the research that has been performed,\nso that it conforms to established norms of inferential reasoning.\nImportantly, within the context of justification, only data that\nsupport the claims of interest are explicitly reported and discussed:\neverything else—including the vast majority of data produced in\nthe course of inquiry—is lost to the chaotic context of\n discovery.[2] \nMuch recent philosophy of science, and particularly modelling and\nexperimentation, has challenged theory-centrism by highlighting the\nrole of models, methods and modes of intervention as research outputs\nrather than simple tools, and stressing the importance of expanding\nphilosophical understandings of scientific knowledge to include these\nelements alongside propositional claims. The rise of big data offers\nanother opportunity to reframe understandings of scientific knowledge\nas not necessarily centred on theories and to include\nnon-propositional components—thus, in Cartwright’s\nparaphrase of Gilbert Ryle’s famous distinction, refocusing on\nknowing-how over knowing-that (Cartwright 2019). One way to construe\ndata-centric methods is indeed to embrace a conception of knowledge as\nability, such as promoted by early pragmatists like John Dewey and\nmore recently reprised by Chang, who specifically highlighted it as\nthe broader category within which the understanding of\nknowledge-as-information needs to be placed (Chang 2017). \nAnother way to interpret the rise of big data is as a vindication of\ninductivism in the face of the barrage of philosophical criticism\nlevelled against theory-free reasoning over the centuries. For\ninstance, Jon Williamson (2004: 88) has argued that advances in\nautomation, combined with the emergence of big data, lend plausibility\nto inductivist philosophy of science. Wolfgang Pietsch agrees with\nthis view and provided a sophisticated framework to understand just\nwhat kind of inductive reasoning is instigated by big data and related\nmachine learning methods such as decision trees (Pietsch\n2015). Following John Stuart Mill, he calls this approach\nvariational induction and presents it as common to both big\ndata approaches and exploratory experimentation, though the former can\nhandle a much larger number of variables (Pietsch 2015: 913). Pietsch\nconcludes that the problem of theory-ladenness in machine learning can\nbe addressed by determining under which theoretical assumptions\nvariational induction works (2015: 910ff). \nOthers are less inclined to see theory-ladenness as a problem that can\nbe mitigated by data-intensive methods, and rather see it as a\nconstitutive part of the process of empirical inquiry. Arching back to\nthe extensive literature on perspectivism and experimentation (Gooding\n1990; Giere 2006; Radder 2006; Massimi 2012), Werner Callebaut has\nforcefully argued that the most sophisticated and standardised\nmeasurements embody a specific theoretical perspective, and this is no\nless true of big data (Callebaut 2012). Elliott and colleagues\nemphasise that conceptualising big data analysis as atheoretical risks\nencouraging unsophisticated attitudes to empirical investigation as a\n \n“fishing expedition”, having a high probability of leading\nto nonsense results or spurious correlations, being reliant on\nscientists who do not have adequate expertise in data analysis, and\nyielding data biased by the mode of collection. (Elliott et al. 2016:\n880) \nTo address related worries in genetic analysis, Ken Waters has\nprovided the useful characterisation of “theory-informed”\ninquiry (Waters 2007),\nwhich can be invoked to stress how theory informs the methods used to\nextract meaningful patterns from big data, and yet does not\nnecessarily determine either the starting point or the outcomes of\ndata-intensive science. This does not resolve the question of what\nrole theory actually plays. Rob Kitchin (2014) has proposed to see big data\nas linked to a new mode of hypothesis generation within a\nhypothetical-deductive framework. Leonelli is more sceptical of\nattempts to match big data approaches, which are many and diverse,\nwith a specific type of inferential logic. She rather focused on the\nextent to which the theoretical apparatus at work within big data\nanalysis rests on conceptual decisions about how to order and classify\ndata—and proposed that such decisions can give rise to a\nparticular form of theorization, which she calls classificatory theory\n(Leonelli 2016). \nThese disagreements point to big data as eliciting diverse\nunderstandings of the nature of knowledge and inquiry, and the complex\niterations through which different inferential methods build on each\nother. Again, in the words of Elliot and colleagues,  \nattempting to draw a sharp distinction between hypothesis-driven and\ndata-intensive science is misleading; these modes of research are not\nin fact orthogonal and often intertwine in actual scientific practice.\n(Elliott et al. 2016: 881, see also O’Malley et al. 2009,\nElliott 2012) \nAnother epistemological debate strongly linked to reflection on big\ndata concerns the specific kinds of knowledge emerging from\ndata-centric forms of inquiry, and particularly the relation between\npredictive and causal knowledge.  \nBig data science is widely seen as revolutionary in the scale and\npower of predictions that it can support. Unsurprisingly perhaps, a\nphilosophically sophisticated defence of this position comes from the\nphilosophy of mathematics, where Marco Panza, Domenico Napoletani and\nDaniele Struppa argued for big data science as occasioning a momentous\nshift in the predictive knowledge that mathematical analysis can\nyield, and thus its role within broader processes of knowledge\nproduction. The whole point of big data analysis, they posit, is its\ndisregard for causal knowledge:  \nanswers are found through a process of automatic fitting of the data\nto models that do not carry any structural understanding beyond the\nactual solution of the problem itself. (Napoletani, Panza, &\nStruppa 2014: 486)  \nThis view differs from simplistic popular discourse on “the\ndeath of theory” (Anderson 2008) and the “power of\ncorrelations” (Mayer-Schoenberg and Cukier 2013) insofar as it\ndoes not side-step the constraints associated with knowledge and\ngeneralisations that can be extracted from big data analysis.\nNapoletani, Panza and Struppa recognise that there are inescapable\ntensions around the ability of mathematical reasoning to overdetermine\nempirical input, to the point of providing a justification for any and\nevery possible interpretation of the data. In their words,  \nthe problem arises of how we can gain meaningful understanding of\nhistorical phenomena, given the tremendous potential variability of\ntheir developmental processes. (Napoletani et al. 2014: 487)  \nTheir solution is to clarify that understanding phenomena is not the\ngoal of predictive reasoning, which is rather a form of agnostic\nscience: “the possibility of forecasting and analysing\nwithout a structured and general understanding” (Napoletani et al.\n2011: 12). The\nopacity of algorithmic rationality thus becomes its key virtue and the\nreason for the extraordinary epistemic success of forecasting grounded\non big data. While “the phenomenon may forever re-main hidden to\nour understanding”(ibid.: 5), the application of mathematical models and algorithms\nto big data can still provide meaningful and reliable answers to\nwell-specified problems—similarly to what has been argued in the\ncase of false models (Wimsatt 2007). Examples include the use of\n“forcing” methods such as regularisation or diffusion\ngeometry to facilitate the extraction of useful insights from messy\ndatasets.  \nThis view is at odds with accounts that posit scientific understanding\nas a key aim of science (de Regt 2017), and the intuition that what\nresearchers are ultimately interested in is  \nwhether the opaque data-model generated by machine-learning\ntechnologies count as explanations for the relationships found between\ninput and output. (Boon 2020: 44)  \nWithin the philosophy of biology, for example, it is well recognised\nthat big data facilitates effective extraction of patterns and trends,\nand that being able to model and predict how an organism or ecosystem\nmay behave in the future is of great importance, particularly within\nmore applied fields such as biomedicine or conservation science. At\nthe same time, researchers are interested in understanding the reasons\nfor observed correlations, and typically use predictive patterns as\nheuristics to explore, develop and verify causal claims about the\nstructure and functioning of entities and processes. Emanuele Ratti\n(2015) has argued that big data mining within genome-wide association\nstudies often used in cancer genomics can actually underpin\nmechanistic reasoning, for instance by supporting eliminative\ninference to develop mechanistic hypotheses and by helping to explore\nand evaluate generalisations used to analyse the data. In a similar\nvein, Pietsch (2016)\nproposed to use variational induction as a method to establish what\ncounts as causal relationships among big data patterns, by focusing on\nwhich analytic strategies allow for reliable prediction and effective\nmanipulation of a phenomenon. \nThrough the study of data sourcing and processing in epidemiology,\nStefano Canali has instead highlighted the difficulties of deriving\nmechanistic claims from big data analysis, particularly where data are\nvaried and embodying incompatible perspectives and methodological\napproaches (Canali 2016, 2019). Relatedly, the semantic and logistical\nchallenges of organising big data give reason to doubt the reliability\nof causal claims extracted from such data. In terms of logistics,\nhaving a lot of data is not the same as having all of them, and\ncultivating illusions of comprehensiveness is a risky and potentially\nmisleading strategy, particularly given the challenges encountered in\ndeveloping and applying curatorial standards for data other than the\nhigh-throughput results of “omics” approaches (see also\nthe next section). The constant worry about the partiality and\nreliability of data is reflected in the care put by database curators\nin enabling database users to assess such properties; and in the\nimportance given by researchers themselves, particularly in the\nbiological and environmental sciences, to evaluating the quality of\ndata found on the internet (Leonelli 2014, Fleming et al. 2017). In\nterms of semantics, we are back to the role of data classifications as\ntheoretical scaffolding for big data analysis that we discussed in the\nprevious section. Taxonomic efforts to order and visualise data inform\ncausal reasoning extracted from such data (Sterner\n& Franz 2017), and can themselves constitute a bottom-up\nmethod—grounded in comparative reasoning—for assigning\nmeaning to data models, particularly in situation where a full-blown\ntheory or explanation for the phenomenon under investigation is not\navailable (Sterner 2014). \nIt is no coincidence that much philosophical work on the relation\nbetween causal and predictive knowledge extracted from big data comes\nfrom the philosophy of the life sciences, where the absence of\naxiomatized theories has elicited sophisticated views on the diversity\nof forms and functions of theory within inferential reasoning.\nMoreover, biological data are heterogeneous both in their content and\nin their format; are curated and re-purposed to address the needs of\nhighly disparate and fragmented epistemic communities; and present\ncurators with specific challenges to do with tracking complex, diverse\nand evolving organismal structures and behaviours, whose relation to\nan ever-changing environment is hard to pinpoint with any stability\n(e.g., Shavit & Griesemer 2009). Hence in this domain, some of the\ncore methods and epistemic concerns of experimental\nresearch—including exploratory experimentation, sampling and the\nsearch for causal mechanisms—remain crucial parts of\ndata-centric inquiry. \nAt the start of this entry I listed “value” as a major\ncharacteristic of big data and pointed to the crucial role of valuing\nprocedures in identifying, processing, modelling and interpreting data\nas evidence. Identifying and negotiating different forms of data value\nis an unavoidable part of big data analysis, since these valuation\npractices determine which data is made available to whom, under which\nconditions and for which purposes. What researchers choose to consider\nas reliable data (and data sources) is closely intertwined not only\nwith their research goals and interpretive methods, but also with\ntheir approach to data production, packaging, storage and sharing.\nThus, researchers need to consider what value their data may have for\nfuture research by themselves and others, and how to enhance that\nvalue—such as through decisions around which data to make\npublic, how, when and in which format; or, whenever dealing with data\nalready in the public domain (such as personal data on social media),\ndecisions around whether the data should be shared and used at all,\nand how. \nNo matter how one conceptualises value practices, it is clear that\ntheir key role in data management and analysis prevents facile\ndistinctions between values and “facts” (understood as\npropositional claims for which data provide evidential warrant). For\nexample, consider a researcher who values both\nopenness—and related practices of widespread data\nsharing—and scientific rigour—which requires a\nstrict monitoring of the credibility and validity of conditions under\nwhich data are interpreted. The scale and manner of big data\nmobilisation and analysis create tensions between these two values.\nWhile the commitment to openness may prompt interest in data sharing,\nthe commitment to rigour may hamper it, since once data are freely\ncirculated online it becomes very difficult to retain control over how\nthey are interpreted, by whom and with which knowledge, skills and\ntools. How a researcher responds to this conflict affects which data\nare made available for big data analysis, and under which conditions.\nSimilarly, the extent to which diverse datasets may be triangulated\nand compared depends on the intellectual property regimes under which\nthe data—and related analytic tools—have been produced.\nPrivately owned data are often unavailable to publicly funded\nresearchers; and many algorithms, cloud systems and computing\nfacilities used in big data analytics are only accessible to those\nwith enough resources to buy relevant access and training. Whatever\nclaims result from big data analysis are, therefore, strongly\ndependent on social, financial and cultural constraints that condition\nthe data pool and its analysis. \nThis prominent role of values in shaping data-related epistemic\npractices is not surprising given existing philosophical critiques of\nthe fact/value distinction (e.g., Douglas 2009), and the existing\nliterature on values in science—such as Helen Longino’s\nseminal distinction between constitutive and contextual values, as\npresented in her 1990 book Science as Social\nKnowledge—may well apply in this case too. Similarly, it is\nwell-established that the technological and social conditions of\nresearch strongly condition its design and outcomes. What is\nparticularly worrying in the case of big data is the temptation,\nprompted by hyped expectations around the power of data analytics, to\nhide or side-line the valuing choices that underpin the methods,\ninfrastructures and algorithms used for big data extraction. \nConsider the use of high-throughput data production tools, which\nenable researchers to easily generate a large volume of data in\nformats already geared to computational analysis. Just as in the case\nof other technologies, researchers have a strong incentive to adopt\nsuch tools for data generation; and may do so even in cases where such\ntools are not good or even appropriate means to pursue the\ninvestigation. Ulrich Krohs uses the term convenience\nexperimentation to refer to experimental designs that are adopted\nnot because they are the most appropriate ways of pursuing a given\ninvestigation, but because they are easily and widely available and\nusable, and thus “convenient” means for researchers to\npursue their goals (Krohs 2012). \nAppeals to convenience can extend to other aspects of data-intensive\nanalysis. Not all data are equally easy to digitally collect,\ndisseminate and link through existing algorithms, which makes some\ndata types and formats more convenient than others for computational\nanalysis. For example, research databases often display the outputs of\nwell-resourced labs within research traditions which deal with\n“tractable” data formats (such as “omics”).\nAnd indeed, the existing distribution of resources, infrastructure and\nskills determines high levels of inequality in the\nproduction, dissemination and use of big data for research. Big\nplayers with large financial and technical resources are leading the\ndevelopment and uptake of data analytics tools, leaving much publicly\nfunded research around the world at the receiving end of innovation in\nthis area. Contrary to popular depictions of the data revolution as\nharbinger of transparency, democracy and social equality, the\ndigital divide between those who can access and use data\ntechnologies, and those who cannot, continues to widen. A result of\nsuch divides is the scarcity of data relating to certain subgroups and\ngeographical locations, which again limits the comprehensiveness of\navailable data resources. \nIn the vast ecosystem of big data infrastructures, it is difficult to\nkeep track of such distortions and assess their significance for data\ninterpretation, especially in situations where heterogeneous data\nsources structured through appeal to different values are mashed\ntogether. Thus, the systematic aggregation of convenient datasets and\nanalytic tools over others often results in a big data pool where the\nrelevant sources and forms of bias are impossible to locate and\naccount for (Pasquale 2015; O’Neill 2016; Zuboff 2017; Leonelli\n2019a). In such a landscape, arguments for a separation between fact\nand value—and even a clear distinction between the role of\nepistemic and non-epistemic values in knowledge\nproduction—become very difficult to maintain without\ndiscrediting the whole edifice of big data science. Given the extent\nto which this approach has penetrated research in all domains, it is\narguably impossible, however, to critique the value-laden structure of\nbig data science without calling into question the legitimacy of\nscience itself. A more constructive approach is to embrace the extent\nto which big data science is anchored in human choices, interests and\nvalues, and ascertain how this affects philosophical views on\nknowledge, truth and method. \nIn closing, it is important to consider at least some of the risks and\nrelated ethical questions raised by research with big data. As already\nmentioned in the previous section, reliance on big data collected by\npowerful institutions or corporations risks raises significant social\nconcerns. Contrary to the view that sees big and open data as\nharbingers of democratic social participation in research, the way\nthat scientific research is governed and financed is not challenged by\nbig data. Rather, the increasing commodification and large value\nattributed to certain kinds of data (e.g., personal data) is\nassociated to an increase in inequality of power and visibility\nbetween different nations, segments of the population and scientific\ncommunities (O’Neill 2016; Zuboff 2017; D’Ignazio and\nKlein 2020). The digital gap between those who not only can access\ndata, but can also use it, is widening, leading from a state of\ndigital divide to a condition of “data divide”\n(Bezuidenout et al. 2017). \nMoreover, the privatisation of data has serious implications for the\nworld of research and the knowledge it produces. Firstly, it affects\nwhich data are disseminated, and with which expectations. Corporations\nusually only release data that they regard as having lesser commercial\nvalue and that they need public sector assistance to interpret. This\nintroduces another distortion on the sources and types of data that\nare accessible online while more expensive and complex data are kept\nsecret. Even many of the ways in which citizens -researchers included\n- are encouraged to interact with databases and data interpretation\nsites tend to encourage participation that generates further\ncommercial value. Sociologists have recently described this type of\nsocial participation as a form of exploitation (Prainsack & Buyx\n2017; Srnicek 2017). In turn, these ways of\nexploiting data strengthen their economic value over their scientific\nvalue. When it comes to the commerce of personal data between\ncompanies working in analysis, the value of the data as commercial\nproducts -which includes the evaluation of the speed and efficiency\nwith which access to certain data can help develop new products -\noften has priority over scientific issues such as for example,\nrepresentativity and reliability of the data and the ways they were\nanalysed. This can result in decisions that pose a problem\nscientifically or that simply are not interested in investigating the\nconsequences of the assumptions made and the processes used. This lack\nof interest easily translates into ignorance of discrimination,\ninequality and potential errors in the data considered. This type of\nignorance is highly strategic and economically productive since it\nenables the use of data without concerns over social and scientific\nimplications. In this scenario the evaluation on the quality of data\nshrinks to an evaluation of their usefulness towards short-term\nanalyses or forecasting required by the client. There are no\nincentives in this system to encourage evaluation of the long-term\nimplications of data analysis. The risk here is that the commerce of\ndata is accompanied by an increasing divergence between data and their\ncontext. The interest in the history of the transit of data, the\nplurality of their emotional or scientific value and the re-evaluation\nof their origins tend to disappear over time, to be substituted by the\nincreasing hold of the financial value of data. \nThe multiplicity of data sources and tools for aggregation also\ncreates risks. The complexity of the data landscape is making it\nharder to identify which parts of the infrastructure require updating\nor have been put in doubt by new scientific developments. The\nsituation worsens when considering the number of databases that\npopulate every area of scientific research, each containing\nassumptions that influence the circulation and interoperability of\ndata and that often are not updated in a reliable and regular way.\nJust to provide an idea of the numbers involved, the prestigious\nscientific publication Nucleic Acids Research publishes a\nspecial issue on new databases that are relevant to molecular biology\nevery year and included: 56 new infrastructures in 2015, 62 in 2016,\n54 in 2017 and 82 in 2018. These are just a small proportion of the\nhundreds of databases that are developed each year in the life\nsciences sector alone. The fact that these databases rely on short\nterm funding means that a growing percentage of resources remain\navailable to consult online although they are long dead. This is a\ncondition that is not always visible to users of the database who\ntrust them without checking whether they are actively maintained or\nnot. At what point do these infrastructures become obsolete? What are\nthe risks involved in weaving an ever more extensive tapestry of\ninfrastructures that depend on each other, given the disparity in the\nways they are managed and the challenges in identifying and comparing\ntheir prerequisite conditions, the theories and scaffolding used to\nbuild them? One of these risks is rampant conservativism: the\ninsistence on recycling old data whose features and management\nelements become increasingly murky as time goes by, instead of\nencouraging the production of new data with features that specifically\nrespond to the requirements and the circumstances of their users. In\ndisciplines such as biology and medicine that study living beings and\ntherefore are by definition continually evolving and developing, such\ntrust in old data is particularly alarming. It is not the case, for\nexample, that data collected on fungi ten, twenty or even a hundred\nyears ago is reliable to explain the behaviour of the same species of\nfungi now or in the future (Leonelli 2018). \nResearchers of what Luciano Floridi calls the\ninfosphere—the way in which the introduction of digital\ntechnologies is changing the world - are becoming aware of the\ndestructive potential of big data and the urgent need to focus efforts\nfor management and use of data in active and thoughtful ways towards\nthe improvement of the human condition. In Floridi’s own words:\n \nICT yields great opportunity which, however, entails the enormous\nintellectual responsibility of understanding this technology to use it\nin the most appropriate way. (Floridi 2014: vii; see also\nBritish Academy & Royal Society 2017)  \nIn light of these findings, it is essential that ethical and social\nissues are seen as a core part of the technical and scientific\nrequirements associated with data management and analysis. The ethical\nmanagement of data is not obtained exclusively by regulating the\ncommerce of research and management of personal data nor with the\nintroduction of monitoring of research financing, even though these\nare important strategies. To guarantee that big data are used in the\nmost scientifically and socially forward-thinking way it is necessary\nto transcend the concept of ethics as something external and alien to\nresearch. An analysis of the ethical implications of data science\nshould become a basic component of the background and activity of\nthose who take care of data and the methods used to view and analyse\nit. Ethical evaluations and choices are hidden\nin every aspect of data management, including those choices that may\nseem purely technical. \nThis entry stressed how the emerging emphasis on big data signals the\nrise of a data-centric approach to research, in which efforts to\nmobilise, integrate, disseminate and visualise data are viewed as\ncentral contributions to discovery. The emergence of data-centrism\nhighlights the challenges involved in gathering, classifying and\ninterpreting data, and the concepts, technologies and institutions\nthat surround these processes. Tools such as high-throughput\nmeasurement instruments and apps for smartphones are fast generating\nlarge volumes of data in digital formats. In principle, these data are\nimmediately available for dissemination through internet platforms,\nwhich can make them accessible to anybody with a broadband connection\nin a matter of seconds. In practice, however, access to data is\nfraught with conceptual, technical, legal and ethical implications;\nand even when access can be granted, it does not guarantee that the\ndata can be fruitfully used to spur further research. Furthermore, the\nmathematical and computational tools developed to analyse big data are\noften opaque in their functioning and assumptions, leading to results\nwhose scientific meaning and credibility may be difficult to assess.\nThis increases the worry that big data science may be grounded upon,\nand ultimately supporting, the process of making human ingenuity\nhostage to an alien, artificial and ultimately unintelligible\nintelligence. \nPerhaps the most confronting aspect of big data science as discussed\nin this entry is the extent to which it deviates from understandings\nof rationality grounded on individual agency and cognitive abilities\n(on which much of contemporary philosophy of science is predicated).\nThe power of any one dataset to yield knowledge lies in the extent to\nwhich it can be linked with others: this is what lends high epistemic\nvalue to digital objects such as GPS locations or sequencing data, and\nwhat makes extensive data aggregation from a variety of sources into a\nhighly effective surveillance tool. Data production and dissemination\nchannels such as social media, governmental databases and research\nrepositories operate in a globalised, interlinked and distributed\nnetwork, whose functioning requires a wide variety of skills and\nexpertise. The distributed nature of decision-making involved in\ndeveloping big data infrastructures and analytics makes it impossible\nfor any one individual to retain oversight over the quality,\nscientific significance and potential social impact of the knowledge\nbeing produced. \nBig data analysis may therefore constitute the ultimate instance of a\ndistributed cognitive system. Where does this leave accountability\nquestions? Many individuals, groups and institutions end up sharing\nresponsibility for the conceptual interpretation and social outcomes\nof specific data uses. A key challenge for big data governance is to\nfind mechanisms for allocating responsibilities across this complex\nnetwork, so that erroneous and unwarranted decisions—as well as\noutright fraudulent, unethical, abusive, discriminatory or misguided\nactions—can be singled out, corrected and appropriately\nsanctioned. Thinking about the complex history, processing and use of\ndata can encourage philosophers to avoid ahistorical, uncontextualized\napproaches to questions of evidence, and instead consider the methods,\nskills, technologies and practices involved in handling data—and\nparticularly big data—as crucial to understanding empirical\nknowledge-making.","contact.mail":"s.leonelli@exeter.ac.uk","contact.domain":"exeter.ac.uk"}]
