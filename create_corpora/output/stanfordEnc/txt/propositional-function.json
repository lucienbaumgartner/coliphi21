[{"date.published":"2011-07-20","url":"https://plato.stanford.edu/entries/propositional-function/","author1":"Edwin Mares","author1.info":"http://www.vuw.ac.nz/phil/ed.html","entry":"propositional-function","body.text":"\n\n\nAs the name suggests, propositional functions are functions that have\npropositions as their values. Propositional functions have played an\nimportant role in modern logic, from their beginnings in Frege's\ntheory of concepts and their analyses in Russell's works, to their\nappearance in very general guise in contemporary type theory and\ncategorial grammar.\n\nIn this article, I give an historical overview of the use of\npropositional functions in logical theory and of views about their\nnature and ontological status.\n\nBefore we begin our discussion of propositional functions, it will\nbe helpful to note what came before their introduction. In\ntraditional logic, the role of propositional functions is\napproximately held by terms. In traditional logic, statements such as\n‘dogs are mammals’ are treated as postulating a relation between the\nterms ‘dogs’ and ‘mammals’. A term is treated either extensionally as a class of objects or\nintensionally as a set of properties. The ‘intent’ of the\nterm ‘dog’ includes all the properties that are included\nin the intent of ‘mammal’. The intensional treatment of\n‘dogs are mammals’ interprets this sentence as true\nbecause the semantic interpretation of the subject is a\nsuperset of the interpretation of the predicate. On the\nextensional treatment of the sentence, however, the sentence is true\nbecause the interpretation of the subject (the class of dogs) is a\nsubset of the interpretation of the predicate (the set of\nmammals). These two treatments of the predicate are typical of the two\ntraditions in traditional logic—the intensional and the\nextensional traditions. Logicians who can be counted among the\nintensional logicians are Gottfried Leibniz, Johann Lambert, William\nHamilton, Stanley Jevons, and Hugh MacColl. Among the extensional\nlogicians are George Boole, Augustus De Morgan, Charles Peirce, and\nJohn Venn. The treatment of terms in the intensional logic tradition property\nof certain sentences might seem strange to modern readers. The\nintension of a predicate, in 20th Century philosophy, includes only\nthose properties that any competent speaker of a language would\nassociate with that predicate. These properties are not enough to\nmake true ordinary statements like ‘every dog in my house is\nasleep’. But we can make sense of the intensional view of terms by\nconsidering its origins. One of the founders of the intensional logic\ntradition is Leibniz, who thinks that all truths are grounded in the\nnature of individuals. The complete concept of an individual contains\neverything that is true of it. Building on this, we can see that the\ncomplete concept of a term will include enough to ground any truth\nabout it as well. In both the intensional and extensional logic traditions, we see\ntheories of complex terms. In the extensional tradition, disjunctive\nand conjunctive terms are interpreted by taking the union and\nintersection of classes. The conjunctive term\nAB is interpreted as the intersection of the class A\nand the class\nB and the extension of the disjunctive term A+B is\nunderstood as the union of the extensions of\nA and B.  In the intensional tradition, the reverse holds. The\nterm AB is interpreted as the union of the properties in the\nintent of\nA and the intent of B and A+B is\ninterpreted as the intersection of the properties in\nA and B. This reversal makes sense, since more\nthings fit a smaller number of properties and fewer things fit a\nlarger number of properties. Although some of the logicians working in term logic have very\ncomplicated treatments of negation, we can see the origin of the\nmodern conception in the extensional tradition as well. In Boole and\nmost of his followers, the negation of a term is understood as the\nset theoretic complement of the class represented by that term. For\nthis reason, the negation of classical propositional logic is often\ncalled ‘Boolean negation’. In Charles Peirce's ‘Logic of Relatives’ (1883), we see a\nmove towards an understanding of terms as functions. One problem with\ntraditional term logic is that it lacks the ability to deal with\nrelations. Peirce's logic of relatives is meant to remedy that.  He\nadds terms to Boolean algebra that represent relations, and gives an\nextensional interpretation of them. They are not propositional\nfunctions in the full sense. Peirce's relatives are ‘common\nnames’ that represent classes of pairs of objects (1883, 328).\nThus, the logic of relatives represents a generalization of\ntraditional logic rather than a departure from it. Peirce extends the algebra of terms to deal with particular\nfeatures of relations. Like other terms, we can have conjunctive,\ndisjunctive, and negative terms. Where\nf and g are relatives, then fg represents\nthe class of pairs (I,J) such that I bears both\nf and g to J. Similarly, the disjunctive\nrelative, f+g is such that it represent\n(I,J) if I bears either\nf or g to J and f′—the negation of the\nterm f—represents the class of pairs (I,J) such that\nf does not hold between them. Peirce also has a composition\noperator, ; , such that\nf;g names (I,J) if there is some\nentity K such that f names (I,K)\nand\ng names (K,J). In ‘The Critic of Arguments’ (1892), Peirce adopts a\nnotion that is even closer to that of a propositional function. There\nhe develops the concept of the ‘rhema’. He says the rhema\nis like a relative term, but it is not a term. It contains a copula,\nthat is, when joined to the correct number of arguments it produces\nan assertion. For example, ‘__ is bought by __ from __ for __’ is a\nfour-place rhema. Applying it to four objects\na, b, c, and d produces the assertion that \na is bought by b from c for d (ibid. 420).  One especially interesting point about Peirce's rhema is that he\nuses the same chemical analogy as Frege does when they discuss the\nrelation between relations and their arguments. They both compare\nrelations (and properties) to ‘atoms or radicals with\nunsaturated bonds’. What exactly this analogy says of relations\nor properties, either in Frege or Peirce is somewhat unclear.\n See the entry on Peirce's logic, for\na more complete exposition of his work.  In the work of Giuseppe Peano (1858–1932), we find another\nimportant step towards the modern notion of a propositional\nfunction. Although his work is not as sophisticated as Frege's (see\nbelow), it is important because it is influential particularly on\nBertrand Russell.  In his ‘Principles of Arithmetic Presented by a New\nMethod’ (1889), Peano introduces propositional connectives in the\nmodern sense (an implication, negation, conjunction, disjunction, and\na biconditional) and propositional constants (a verum and a falsum).\n More important for us is his treatment of quantification. Peano\nallows propositions to contain variables, that is to say, he utilizes\nopen formulas. He does not give an interpretation of open\nformulas. He does not tell us what they represent. But they are used\nin his theory of quantification. Peano only has a universal\nquantifier. He does not define an existential quantifier in the\n‘Principles’. The quantifier is always attached to a\nconditional or biconditional. Quantified propositions are always of\nthe form A ⊃x,y,…B or A =x,y,…B Peano reads ‘A ⊃x,y,…B’ as saying ‘whatever\nx,y,…  may be, from the proposition A one\ndeduces B’ and ‘=’ is Peano's biconditional, that he\ndefines in the usual way from the conditional and conjunction. But he\nprovides us with no more interpretation than that. He refers to\nvariables as ‘indeterminate objects’, but does not discuss\nwhat this or what a proposition (or propositional function) that\ncontains propositional objects might be. In Frege we have a fairly general interpretation of sentences as\nexpressing functions applying to arguments. The view that I explore\nhere is one that he develops in the 1890s. Consider the sentence My dog is asleep on the floor. This sentence, like all linguistic expressions, has\nboth a sense and a referent. Its sense is an abstract object—a\nthought. Its referent is its truth value (which at the moment is the\nTrue). We will discuss Frege's analysis of the thought soon, but right\nnow let us look at the referents of the expressions that make up this\nsentence. The expression ‘my dog’, according to Frege, is a\nsingular term. It picks out an object (my dog, Zermela). The\nexpression ‘is asleep on the floor’ refers to a\nconcept. Concepts are functions. In this case, the concept is\na function from objects to truth values (which are also objects). So,\nwe can treat the above sentence as representing the concept __\nis asleep on the floor as applying to the object my\ndog.  Frege's concepts are very nearly propositional\nfunctions in the modern sense. Frege explicitly recognizes them as\nfunctions. Like Peirce's rhema, a concept is\nunsaturated. They are in some sense incomplete. Although\nFrege never gets beyond the metaphorical in his description of the\nincompleteness of concepts and other functions, one thing is clear:\nthe distinction between objects and functions is the main division in\nhis metaphysics. There is\nsomething special about functions that makes them very\ndifferent from objects. Let us consider ‘my dog is asleep on the floor’\nagain. Frege thinks that this sentence can be analyzed in various\ndifferent ways. Instead of treating it as expressing the application\nof __ is asleep on the floor to my dog, we can think\nof it as expressing the application of the concept my dog is asleep on __ to the object the floor (see Frege 1919). Frege recognizes what is now a\ncommonplace in the logical analysis of natural language.\nWe can attribute more than one logical form to a single\nsentence. Let us call this the\nprinciple of multiple analyses. Frege does not claim\nthat the principle always holds, but as we shall see, modern type\ntheory does claim this. With regard to the sense of sentences, they are also the result of\napplying functions to objects. The sense of ‘my dog’ is an\nabstract object. The sense of ‘is asleep on the floor’ is\na function from individual senses, like that of ‘my dog’,\nto thoughts (see Frege 1891). The sense of ‘is asleep on the\nfloor’ is a conceptual sense. It would seem that the\nprinciple of multiple analyses holds as much for senses as it does for\nreferents. Frege, however, sometimes talks as if the senses of the\nconstituent expressions of a sentence are actually contained somehow\nin the thought. It is difficult to understand how all such senses\ncould be in the thought if there are different ways in which the\nsentence can be analyzed into constituent expressions.\n In addition to concepts and conceptual senses, Frege\nholds that there are extensions of concepts. Frege calls an extension\nof a concept a ‘course of values’. A course of values is\ndetermined by the value that the concept has for each of its\narguments. Thus, the course of values for the concept __ is a\ndog records that its value for the argument Zermela is the True\nand for Socrates is the False, and so on. If two concepts have the\nsame values for every argument, then their courses of values are the\nsame. Thus, courses of values are extensional. For more about Frege's theory of concepts and its relation to his logic, see\nthe entry on Frege's theorem and foundations for arithmetic. The term ‘propositional function’ appears in print for\nthe first time in Bertrand Russell's\nPrinciples of Mathematics (1903). Russell introduces the\nnotion through a discussion of kinds of propositions. Consider\npropositions of the type that says of something that it is a dog. This\nis the kind ‘x is a dog’. This kind is a\npropositional function that takes any object o to the\nproposition that o is a dog. In this period, Russell holds that propositions are entities that\nhave individuals and properties and relations as constituents. The\nproposition that Socrates is a man has Socrates and the property of\nbeing a man as constituents. In complex propositions the relation\nbetween propositional function and the proposition is less\nclear. Like Frege, Russell allows the abstraction of a propositional\nfunction from any omission of an entity from a proposition. Thus, we\ncan view the proposition\n if Socrates drinks hemlock he will die  as representing the application of the function x drinks hemlock ⊃ x will die  to Socrates, or the function Socrates will drink x ⊃ Socrates will die to hemlock, and so on. In other words, Russell accepts the\nprinciple of multiple analyses. In the Principles, the quantifier ‘all’ is analyzed as a\npart of referring phrases that pick out classes (1903, 72). This, we\ncan see, is a hold-over from the 19th Century extensional logicians\n(see Section 1). But in slightly later works, such as ‘On\nDenoting’ (1905), propositional functions are said to be\nconstituents of universal propositions. According to this analysis the\nproposition expressed by sentences such as ‘All dogs bark’ is made up\nof the propositional function x is a dog ⊃ x\nbarks and a function (of propositional functions) that is\nrepresented by the quantifier phrase ‘all’. Quantified propositions\nare interesting for us because they contain propositional functions as\nconstituents. It is unclear whether Russell holds that propositional functions\nalso occur as constituents in singular propositions like\nif Socrates drinks hemlock he will die. These propositions do\ncontain properties, like\ndies, and relations, like drinks, but it is\ncontroversial as to whether Russell thinks that these are\npropositional functions (see Linsky 1999 and Landini 1998). While writing the Principles of Mathematics, Russell\ndiscovered the paradox that now bears his name. Before we get to\nRussell's paradox, let us discuss some the method\nof diagonalization by which this and many other paradoxes are\ngenerated. The power set of a set S, ℘S contains all\nthe subsets of S. Georg Cantor (1845–1918) used the method of\ndiagonalization to show that for any set S,\n℘S is larger than S. Here is Cantor's proof. Suppose that ℘S\nand S are the same size. Then, by the set-theoretic\ndefinition of “same size” (more correctly, ‘same\ncardinality’) there is a one-to-one surjection\nbetween S and ℘S. This means that there is a\nfunction that matches up each member of S with a unique\nmember of ℘S so that there are no members of\n℘S left over. Let us call this\nfunction, f. Then, if x is a member\nof S, f(x) is in ℘S. Now,\nsince ℘S is the power set of S, it may be\nthat x is in f(x) or it may not be\nin f(x).  Let us now define a set C: C = {x ∈ S: \nx ∉ f(x)} Clearly, C is a subset of S, so it is in\n℘S. By hypothesis, f is onto—for\nevery member y of ℘S, there is\nan x ∈ S such\nthat f(x) = y.  Thus there must be\nsome c ∈ S such that f(c) = C\n Now, either c ∈ C\n or c ∉ C.\n Suppose that c is in C. Then, by the definition\nof C, c is not in\nf(c). That is to say, c\n∉ C. But, if c is not in C,\nthen c ∉ f(c). So, by the definition\nof C, c is in C.  Thus, c is in C if and only if c is not\nin C. Therefore, the assumption that a set is the same size\nas its power set leads to a paradox, and so this assumption must be\nfalse. Cantor's theorem has important consequences for the theory of\npropositional functions. Consider a model for a (first-order) logical\nlanguage that has a domain D. The variables of the language\nrange over members of D. Now let us add predicate variables\nto the language. These stand for propositional functions. How are we\nto interpret them in the model? The standard way of doing so—that\nis inherited from the extensional logic tradition—is to have\npredicate variables range over subsets of the domain. A model in which\npredicate variables range over all subsets of the domain is called a\n‘standard model’ for second-order logic. Cantor's theorem\ntells us that the domain for predicate variables in the standard model\nis larger than the domain for individual variables.  If we have\npredicates of predicates, then the domain for third order predicates\nis even larger. And so on. Russell's paradox is very closely related to Cantor's\ntheorem. There are two versions of the paradox: (1) the class\nversion; (2) the propositional function version. I only discuss the\npropositional function version of the paradox. In his early writings, Russell wants logic to be a\nuniversal science. It should allow us to talk about properties\nof everything. By this he means that the variables in logic should be\ntaken to range over all entities. But propositional functions,\nat least in the Principles, are entities. So variables\nshould range over them. Now consider the predicate\nR such that, \n(∀x)(Rx =\n¬xx)\n (Russell's predicate R is very similar to\nCantor's set C.) If we instantiate\nand substitute R for x, we obtain RR ≡ ¬RR It seems, then, that the treatment of variables as\ncompletely general together with the liberty to define propositional\nfunctions by means of any well-formed formula enables us to derive a\ncontradiction. Russell blocks the contradiction in\nthe Principles by the introduction of a theory of types. This\nis a simple theory of types, that only distinguishes between the types\nof various propositional functions (or, in its class-form, of\nclasses).  Let us depart from Russell's own exposition of the theory\nof types in order to give a more rigorous and more modern version of\nthe theory. This will make my presentations of the ramified theory of\ntypes and more modern versions of type theory easier. We'll use one basic type, i (the type of individuals) and\ndefine the types as follows: The type\n<t1,…, tn>\nis the type of a relation among entities of\ntypes t1,…, tn.\nBut, for simplicity, we will interpret this as the type of a function\nthat takes these entities to a proposition. (Note that\nwhen n = 0, then the empty type, < >, is\nthe type for propositions.) This definition incorporates the idea of a\nwell-founded structure. There are no cycles here. We cannot have a\nfunction that takes as an argument a function of the same or higher\ntype. Thus, simple type theory bans the sort of self-application that\ngives rise to Russell's paradox. The type hierarchy corresponds neatly to the hierarchy of domains\nthat we saw in our discussion of Cantor's theorem.  A unary predicate\nhas the type <i>; its domain is D—the set\nof individuals. A unary predicate of predicates has the type\n<<i>>, and this corresponds to the domain of\nsubsets of D. And so on. For more, \nsee the entry on\n  Russell's paradox. After the Principles, however, Russell comes to believe\nthat the simple theory of types is insufficient. The reason for\nit has to do with the\n  liar paradox. Suppose that ‘L’ is a name\nfor the proposition: L is false. This statement is false if and only if it is true.\nThe problem here has something to do with self-reference, but it\ncannot be avoided by the simple theory of types alone. For simple\ntypes only give us a hierarchy of types of propositional functions.\nIn simple type theory, all propositions have the same type.  The idea behind ramified type theory is to introduce a\nhierarchy of propositions as well. On this view, propositions and\npropositional functions have an\norder. If a propositional function is applied to a\nproposition of a particular order, then it yields a proposition of a\nhigher order. And every function must have a higher order than its\narguments. Thus, we avoid the liar paradox by banning a proposition\nfrom occurring within itself. If a proposition p occurs\nwithin another proposition, as the argument of a function such as\nx is false, then the resulting proposition is of a higher\norder than p. Unfortunately, Russell never gives a precise formulation of\nramified type theory. Perhaps the best formulation is due to Alonzo\nChurch (1976).[1] Almost at the same time as he adopts the ramified theory of types,\nRussell abandons propositions. From about 1908 until 1918, although\nRussell retains the idea that there are true propositions, he denies\nthat there are false ones. When we think about something that is\nfalse, say, Zermela is a cat, we are not thinking about a\nfalse proposition, but rather the objects of our thought are just\nZermela and the property of being a cat. It might seem odd to have a\nhierarchy especially designed to stratify the propositions and then\nclaim that there are no propositions. Some interpreters, however,\nhave claimed that Russell's denial of the existence of propositions\nshould not be taken seriously and that there are very good reasons to\nread Principia as being largely a theory of propositions\n(see Church 1984). One reason to take the ramified theory of types seriously (even\nwithout accepting propositions) is that it can be usefully\nincorporated into a substitutional theory of quantification. On the\nsubstitutional interpretation of the quantifiers, a universally\nquantified formula such as (∀x)Fx is true if\nand only if all of its instances\nFa1, Fa2, Fa3,…\nare true. Similarly, (∀x)Fa is true if and\nonly if at least one of its instances is true.\n Consider a substitutional interpretation of quantifiers with\nvariables ranging over predicates, as in the formula,\n(∀P)Pa. This formula is true if and only if all\nof its instances are true. On a simple theory of types, the type of\nthe variable\nP is <i>, since its arguments are all\nindividuals (or singular terms). But the simple type of the function,\n(∀P)Px is also <i>. So an instance of\n(∀P)Pa is (∀P)Pa\nitself. A substitutional interpretation of the quantifiers requires\nthat instances be simpler than the formulas of which they are\ninstances. In this case, all we find out is that a particular formula\nis true only if it is true. This is uninformative and it seems\nviciously circular. To block this sort of circularity, we can turn to the ramified\ntheory of types. On the ramified theory, the propositional function\n(∀P)Px is of order 2, because of the presence\nof the quantifier binding a variable of order 1. In this way, the\nramified theory forces formulas to be simpler (at least in terms of\norder) than the formulas of which they are instances (see Hazen and\nDavoren 2000). After 1905, we see in Russell a parsimonious inclination. He wants\nto eliminate entities from his ontology. Some time between 1908 and\n1910 he begins to deny the existence of propositions and this denial\ncontinues until he develops a theory of propositions as structures of\nimages or words in (1918). What, then, is the fate of propositional\nfunctions? It might seem difficult to understand what a propositional\nfunction is without the existence of propositions, but Russell's view\nis, not that complicated.  Russell only rejects false propositions. He\nretains facts in his ontology. Propositional functions, in\nPrincipia, are what we now call ‘partial\nfunctions’. That is to say, they do not always have values. For\nexample, the propositional function\n__ is a dog does not have a value for the Sydney Opera House\ntaken as an argument, but it does have a value when my dog is taken as\nits argument. So, the rejection of false propositions does not cause a\nserious problem for the theory of propositional functions in\nRussell. Having dealt with that problem, let us go on to see what Whitehead\nand Russell think the nature of propositional functions is. In\nPrincipia, they say: By a ‘propositional function’ we mean something which\ncontains a variable x, and expresses a proposition as soon\nas a value is assigned to x. That is to say, it differs from a\nproposition solely by the fact that it is ambiguous: it contains a\nvariable of which the value is unassigned. (1910, 38). In this passage, it seems as though they are saying that a\npropositional function is an ambiguous proposition. In light of the\nrejection of propositions, this view is especially hard to\nunderstand. Urquhart (2003) says that for Whitehead and Russell, a\npropositional function is something rather like a formula. This seems\nright, since propositional functions contain variables. But what exactly are propositional functions in Principia?\nThis is a matter of heated debate among Russell scholars. Perhaps the\nmost influential interpretation is the constructive interpretation,\ndue to Kurt Gödel (1944). On this interpretation, propositional\nfunctions are human constructs of some sort. They depend on our\nability to think about them or refer to them. A version of the\nconstructive interpretation can also be found in Linsky (1999). There\nis also a more nominalist interpretation in Landini (1998). On the\nrealist side, are the interpretations given by Alonzo Church (1984)\nand Warren Goldfarb (1989). Goldfarb thinks that the logical theory of\nPrincipia is motivated by Russell's attempt to find the real\nnature of propositional functions and that this nature is independent\nof our thinking about it. Goldfarb has a good point, since Russell's\nlogic is supposed to be a perspicuous representation of the way things\nare. But Russell often seems to deny that propositional functions are\nreal entities. Jumping ahead some decades, adding possible worlds together with\nset theory to the logicians' toolbox has provided them with a very\npowerful and flexible framework for doing semantics. First, let us recall the modern notion of a function. A function is\na set of ordered pairs. If <a,b> is in a\nfunction\nf, this means that the value of f for the\nargument a is\nb or, more concisely, f(a) = b. By\nthe mathematical definition of a function, for each argument of a\nfunction there is one and only one value. So, if the ordered pair\n<a,b> is in a function\nf and so is <a,c>, then b is the\nsame thing as c. The construction of propositional functions begins with possible\nworlds and the assumption that there are sets. Let us call the set of\npossible worlds\nW. A proposition is a set of possible worlds. The proposition\nthat Zermela barks, for example, is all the sets of worlds in which\nZermela barks. We also need to assume that there is a set\nI of possible individuals (i.e., the individuals that exist in\nat least one possible world). We now have all the materials to\nconstruct a simple type-theoretic hierarchy of functions.\n The usual treatment of the meaning of predicates differs slightly\nfrom the manner I have described here. Usually, the intension of a\npredicate is taken to be a function from possible worlds to sets of\nindividuals (or sets of ordered pairs of individuals for binary\nrelations, ordered triples for three place relations, and so\non). Strictly speaking, these functions are not propositional\nfunctions because they do not take propositions as values. But for\neach such function, we can construct an ‘equivalent’\npropositional functions by using a process called\n‘Currying’ after the logician Haskell Curry. Let's start\nwith a function\nf from worlds to sets of individuals. Then we can construct\nthe corresponding propositional function\ng as follows. For each world w and\nindividual i, we construct\ng so that w is in g(i) if and only\nif i is in f(w). So, the more standard treatment of the meanings of\npredicates is really equivalent to the use of propositional\nfunctions. Now that we have a whole hierarchy of propositional functions, we\nshould find some work for them to do. One theory in which\npropositional functions do good work is Montague semantics, developed\nin the late 1960s by Richard Montague. In order to understand Montague's method we need to\nunderstand lambda abstraction. For the formula\nA(x) we read the expression\nλx[A(x)] as a predicate expression. It\nextension (in a given possible world) is the set of things that\nsatisfy the formula\nA(x). Lambda abstractors are governed by two rules,\nknown as α-conversion and β-reduction: (α-con) A(a) (a formula with a free\nfor x) can be replaced by\nλx[A(x)]a. (β-red)\nλx[A(x)]a can be replaced by\nA(a) (where x is free for a\nin A(x)). Because of the equivalence between a formula A(x) and\nλx[A(x)]a, one might wonder why add\nlambda abstractors to our language. In Montague semantics, the answer\nhas to do with the very direct way that he translates expressions of\nnatural languages into his logical language. We will discuss that\nsoon, but first let us learn a bit about Montague's\nintensional logic. Montague adds two other pieces of notation to his\nlanguage: ∧ and  ∨. The\nexpression ∧λx[Fx] represents\na function from worlds to sets of individuals. Given a possible world\nw, ∧λx[Fx] represents a function that takes\nw to the extension of λx[Fx]. The\noperator ∨ takes expressions of the form\n∧λx[Fx] ‘down’ to\ntheir extensions at the world in which the expression is being\nevaluated. For example, the extension of\n∨∧λx[Fx]\nat w is just the same as the extension of\nλx[Fx] at w. What is so special about Montague semantics is that it can be used\nin a very direct way as a semantics for large fragments of natural\nlanguages. Consider the following sentence: Zermela barks. The meaning of this sentence is understood in Montague\nsemantics as a structure of the meanings of its constituent\nexpressions. Montague represents the meanings of expressions using\ntranslation rules. Here we use the following translation rules: Zermela translates into\nλP[(∨P)z] barks translates\ninto ∧B Now we can construct a formula that gives the meaning\nof ‘Zermela barks’: λP[(∨P)z]∧B Notice that in constructing the sentence we place the\nexpressions in the same order in which they occur in English. The use\nof lambda abstracts allows us to reverse the order of two expressions\nfrom the way in which they would appear in ordinary statements of a\nformal logical language (that does not have lambdas). Now we can use\nβ-reduction to obtain: (∨∧B)z And now we apply Montague's rule to\neliminate ∨∧: Bz In this process we start with an expression that has the same order\nof expressions as the original English sentence and then reduce it to\na very standard formula of logic. This tells us that the truth\ncondition of the sentence ‘Zermela barks’ is the set of\nworlds that is the proposition expressed by Bz. Of course we\nknew that independently of Montague's work, but the point is that the\nMontague reduction shows us how we can connect the surface grammar of\nEnglish sentences to the formula of our logical language. The formula\nof standard logic, moreover, displays its truth-conditions in a very\nperspicuous way. So, the Montague reduction shows us the connection\nbetween sentences of natural languages to their truth conditions. Categorial grammars were first constructed in the 1930s by Kazamir\nAjdukiewicz (1890–1963), and developed by Yehoshua Bar Hillel\n(1915–1975) and Joachim Lambek (1922–) in the 1950s an\n1960s. Categorial grammars are logical tools for representing the\nsyntax of languages. In categorial grammar, the syntax of languages is represented using\na different sort of generalization of the functional notation than in\nMontague semantics. In Montague Semantics, the lambda abstractor is\nused to move the meaning of an expression to the location that the\nexpression occupies in a sentence. In categorial grammar, predicates\nand many other sorts of expressions are taken to be functions of\nsorts. But there is a distinction in categorial grammar between two\nsorts of application of a function to its arguments.  Let's see how this works. Let's start with the primitive types CN\n(common noun) and NP (noun phrase). The indefinite article ‘a’ takes\na common noun (on its right) and returns a NP. So it has the type\nNP/CN. The common noun ‘dog’, of course, has the type CN. We write\n‘A has the type T’ as\n‘A⊢T’. So we have, a ⊢ NP/CN and dog ⊢ CN In order to put these two sequents together, we can\nuse a form of the rule modus ponens which says that from a sequent X\n⊢ A/B and a sequent Y ⊢ B, we can derive the sequent X.Y\n⊢ A. We can use this rule to derive: a.dog ⊢ NP Moreover, an intransitive verb has the type NP\\S,\nwhere S is the type of sentences. The backslash in NP\\S means that the\nexpression takes an argument of type NP\non the left side and returns an expression of type S. The\nverb ‘barks’ is intransitive, that is, barks ⊢ NP\\S The version of modus ponens that we use with the backslash is\nslightly different. It tells us that from X ⊢ A\\B and Y ⊢\nA we can derive Y.X ⊢ B. So we now can obtain, (a.dog).barks ⊢ S This says that ‘a dog barks’ is a sentence. The logics used to describe grammars in this way\nare substructural logics. What is of interest to us here is that in categorial grammars\ndeterminers such as ‘a’ and verbs are thought of as functions, but\nthey can differ from one another in terms of whether they take\narguments on their right or on their left. In the set theoretic\nconcept of function as a set of ordered pairs, functions are thought\nof just in terms of their correlating arguments with values. A\nfunction, as it is understood in categorial grammar has more\nstructure than this. This is an interesting generalization of the\nnotion of a function as it is used in logic. We can see that it also\nhas important links to the concept of a propositional function,\nespecially as it is used in Montague semantics. In categorial grammar we can attribute more than one type to a\nsingle expression in a language. Let us call this the\nprinciple of multiple types. Here is an example due to Mark\nSteadman. Consider the sentence I dislike, and Mary enjoys musicals. The transitive verbs ‘dislike’ and ‘enjoys’ have the\ntype (NP\\S)/NP, that is, they take a noun phrase on their right and\nreturn a verb phrase. But in the case of ‘I dislike, and Mary enjoys\nmusicals’ the verbs are separated from their object and joined to\ntheir objects. Steadman deals with this by raising the type of the\nsubjects ‘I’ and ‘Mary’. Usually, we treat these words as having the\ntype NP, but here they have the type S/(NP\\S). This is the type of an\nexpression that takes a verb phrase on its right and returns a\nsentence. Steadman then uses a rule that makes the backslash\ntransitive and derives that ‘I.dislike’ has the type S/NP, which takes\na noun phrase (such as ‘musicals’) on its right an returns a\nsentence. We can see that the principle of multiple types also holds if\nanalyze sentences other type theories, such as the simple theory of\ntypes. For consider the sentence Mary eats a hamburger. In interpreting this sentence we can take ‘Mary’ to be\nof type i, but we can also take it to be of type\n<<i>>, that is, the type of a propositional\nfunction on propositional functions of individuals. We can also raise\nthe type of ‘eats a hamburger’ to\n<<<i>>>, a propositional function on\npropositional functions on propositional functions on individuals. And\nso on. The principle of multiple types and the principle of multiple\nanalyses together show that a single expression or sentence can be\ninterpreted as having a very large number of logical forms. This brief history of propositional functions shows that they are\nuseful entities and that they have played a central role in logic as\nit is used in philosophy and linguistics. I have omitted the more\nmathematical uses of propositional functions, for example, in\nRussell's and Ramsey's constructions of classes, and in treatments of\ngeneral models for higher-order logic. But the topic of propositional\nfunctions is a big one and we can't cover it all in a single\nencyclopedia article.","contact.mail":"Edwin.Mares@vuw.ac.nz","contact.domain":"vuw.ac.nz"}]
