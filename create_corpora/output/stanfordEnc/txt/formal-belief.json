[{"date.published":"2008-10-22","date.changed":"2020-11-13","url":"https://plato.stanford.edu/entries/formal-belief/","author1":"Konstantin Genin","author1.info":"https://kgenin.github.io/","author2.info":"https://huber.blogs.chass.utoronto.ca/","entry":"formal-belief","body.text":"\n\n\nEpistemologists are interested in the norms governing the structure\nand dynamics of systems of belief: how an individual's beliefs must\ncohere in order to be considered rational; how they must be reflected\nin decision making; and how they ought to accommodate new evidence.\nFormal epistemologists pursue these questions by constructing\nmathematical models, or “formal representations,” of\nbelief systems that are, in some sense, epistemically exemplary.\nBroadly speaking, these models capture something important about how\nan ideally rational agent would manage her epistemic life. This entry\ngives an overview of the formal representations that have been\nproposed for this purpose.\n\n\nBelief comes in a qualitative (full) form, as when Sophia believes\nthat Vienna is the capital of Austria, and a quantitative (partial)\nform, as when Sophia's belief that Vienna is the capital of Austria is\nstronger, in some sense, than her belief that Vienna is more populous\nthan Budapest. The question of how full and partial belief are related\nhas received considerable attention in formal epistemology, giving\nrise to several subtle, elegant and, unfortunately, incompatible\nsolutions. The debate between these alternatives is a particular focus\nof this entry, covered in Section 4. \n\nWhy construct formal representations of belief? To a large\nextent, the rigours and rewards of formalism in epistemology are the\nsame as they are in any other discipline. It requires that we state\nbasic principles precisely, and in return it permits us to\ndemonstrate, whether by deductive proof or\n computer simulation,\n their logical consequences and connections. Often, these consequences\nand connections are unexpected—sometimes so unexpected\nthat it is hard to imagine how they could have been discovered without\nthe aid of the formalism. If an attractive-looking principle has an\nunattractive consequence, it is easy to imagine how one might have\nbeen tempted to evade or ignore it, had it not been so clearly\ndemonstrated. Perhaps the most common tragedy is to find that several\nattractive principles are mutually inconsistent and that some\nunpleasant sacrifice must therefore be made. After a few such\nexperiences, the epistemologist comes to feel that formal\nrepresentations forestall equivocation and self-deception and promote\nprogress and understanding.  \nOf course, formal representations are not without their drawbacks.\nOnce a formal framework is adopted, certain questions and projects\nbecome salient and others recede into the background. Some\nantecedently interesting questions may become impossible to express in\nthe new formalism; insiders may be tempted to disparage these\nquestions as senseless or uninteresting. In the worst cases, this can\nrender formal work shallow, or evasive of important issues. These are\nthe pitfalls of parochialism. To avoid them, it is sometimes helpful\nto recall what motivated the formalism in the first place. To this\nend, a framework's history is sometimes illuminating. It also helps to\ntransition out of, and between, formalisms: this change of perspective\ncombats parochialism in epistemology in the same way that immersion in\na new language or culture tends to combat it more broadly.  \nThe preceding could be said for many subjects apart from epistemology.\nIndeed, mathematical method may facilitate progress on questions\nspecific to epistemology, much as it has on questions specific to\nphysics or economics. And excessive allegiance to a particular\nformalism has pitfalls in physics and economics, just as it does in\nepistemology. If we left the matter there, however, we would not do\njustice to the unique ambitions that have been framed for mathematical\nmethod in epistemology in particular. \nLeibniz dreamed of a characteristica universalis—an exact\nidea-calculus in which it would be possible to arbitrate scientific\ndisputes by calculation and which would serve as a\n“lodestar” to those “who navigate the sea of\nexperiments” (Leibniz 1679/1989). In the last century, the Unity\nof Science movement hoped that a new logic would serve as a universal\nlanguage in which to commensurate all of the sciences and—so\nunited—apply their full power to the problems of the day. But,\nas the clash between the Copernican and Ptolemaic systems illustrates,\nstating two theories precisely and mathematically does not suffice to\nadjudicate disputes between them. To that end, a significant effort\nwas devoted to developing a calculus in which to compute the precise\nbearing of evidence, and thereby arbitrate between theories. This\ncalculus was conceived not as a boon to philosophy alone, but as a\nkind of common law to govern and coordinate the federated republics of\nscience. Many of the formalisms covered in this article can be traced\nback to a program whose ambition may stagger our contemporary\nimaginations. \nPascal (ca. 1658/2004) applied the new probability calculus to\nintensely personal questions of faith. Indeed, many epistemologists\nare not particularly interested in norms for scientific\ninquirers, but in the norms governing the entire belief system of an\nindividual: how their beliefs must cohere in order to be rational; how\nthey must be reflected in decision making; and how they ought to\naccommodate new evidence. Subjective probability theory and its\naccompanying theory of rational decision is the most comprehensive\ncontemporary expression of this project; for better or worse, it is by\nfar the dominant theory of practical rationality of our day. We\nintroduce it in Section 3.1.  \nWe invoke our illustrious forebears here mainly to illustrate the\ngreat scope of their ambitions, with the hope that their meliorist\nspirit will breathe life into the material covered in this article. In\nthe midst of mathematical detail, it is easy to forget that formal\nepistemology is an expression of the hope that reason can be\nproductively turned upon itself. If this project is miscarried, we\nrisk obstructing or artificially circumscribing the scope and power of\nhuman reason. But what if it succeeds?  \nIn the following we will see several proposed models for the structure\nof belief. Most of these proposals take the objects of belief to be\neither  propositions, or  sentences in a formalized\nlanguage. This section reviews the basic notions required to work with\npropositions and sentences in a formal language. If the reader feels\noverwhelmed with the technicalities in this section, they should feel\nfree to postpone them, and refer back to it on-the-fly. Readers who\nare accustomed to working with these objects may freely skip this\nsection. \nThe received view is that the objects of belief are proposition and\npropositions are sets of possible worlds. But what are these\nsupposed to be? This is a rather difficult question (see the entry on\n possible worlds).\n On one picturesque view, a possible world is a complete description\nof an alternative reality. To pick out a possible world is to\nspecify–in a way careful to avoid contradiction–every fact\nthat holds in some possible reality that is not necessarily our own.\nOn this view, the set of  all  possible worlds \\(W\\) is like\na giant library that contains the complete history of every possible\nreality. The actual world picks out the volume that\ncorresponds to our own reality. \nIt is not necessary–and perhaps unhelpful–to think of\npossible worlds as total metaphysical possibilities. At this extremely\nfine level of granularity, each possibility specifies an infinity of\nobscure and uninteresting details. But context usually determines\nwhich features of the world we can take for granted; which we are\nuncertain about but would prefer not to be; and which are of no\ninterest. For example, Sophia may be interested in the identity of the\nnext mayor of Vienna, but whether they are left or right-handed is of\nno importance. For our purposes, a possible world is a complete\nspecification of all and only those features of the world that are\nrelevant given the context. The set \\(W\\), therefore, is the set of\nall contextually relevant epistemic possibilities. Narrowing down the\nset of possibilities to an individual \\(w\\in W\\) would completely\nsettle some interesting question under discussion. A proposition\n\\(P\\subseteq W\\) is a  set of possible worlds, i.e. it is a\npartial specification of the way the world is. To be certain that\n\\(P\\) is true is to be certain that the actual world is among the set\nof worlds \\(\\{ w : w \\in P \\}\\) since \\(P\\) is true in a possible\nworld \\(w\\) iff \\(w\\in P\\). \nPropositions enjoy a set-theoretic structure. The relative complement\nof \\(P\\), \\(\\neg P = W\\setminus P\\), is the set of all worlds in which\n\\(P\\) is false. If \\(P,Q\\) are arbitrary propositions, then their\nintersection \\(P\\cap Q\\) is the set of all worlds in which \\(P\\) and\n\\(Q\\) are both true. The disjunction \\(P\\cup Q\\) is the set of worlds\nin which at least one of \\(P,Q\\) is true. The material conditional\n\\(P\\rightarrow Q\\) is the set of worlds \\(\\neg P \\cup Q,\\) in which\neither \\(P\\) is false or \\(Q\\) is true. If \\(P\\subseteq Q\\) we say\nthat \\(P\\)  entails \\(Q\\) and also that \\(P\\) is \nlogically stronger than \\(Q\\). If \\(P\\subseteq Q\\) and\n\\(Q\\subseteq P\\) we write \\(P\\equiv Q\\) and say that \\(P\\) and \\(Q\\)\nare  logically equivalent. The tautological proposition \\(W\\)\nis true in all worlds and the contradictory proposition, the empty set\n\\(\\varnothing\\), is not true in any world. A set of propositions\n\\(\\mathbf{A}\\) is  consistent iff there is a world in which\nall the elements of \\(\\mathbf{A}\\) are true, i.e. if \\(\\cap \\mathbf{A}\n\\neq \\varnothing.\\) Otherwise, we say that \\(\\mathbf{A}\\) is \ninconsistent. A set of propositions \\(\\mathbf{A}\\) is \nmutually exclusive iff the truth of any one element implies the\nfalsehood of all other elements. The set of logical consequences of\n\\(\\mathbf{A}\\), written \\(\\Cn(\\mathbf{A}),\\) is the set \\( \\{ B\n\\subseteq W : \\cap \\mathbf{A} \\text{ entails } B \\}\\). Note that if\n\\(\\mathbf{A}\\) is inconsistent, then \\(\\Cn(\\mathbf{A})\\) is\n\\(\\mathcal{P}(W)\\), the set of all propositions over \\(W\\). \nA set of propositions \\(\\mathbf{F}\\) is a  field (sometimes\n algebra) iff \\(\\mathbf{F}\\) contains \\(W\\) and it is closed\nunder intersection, union and complementation. That is to say that if\n\\(A,B\\) are both elements of \\(\\mathbf{F}\\) then \\(W,A\\cup B,A\\cap B\\)\nand \\(\\neg A\\) are also elements of \\(\\mathbf{F}.\\) A set of\npropositions \\(\\mathbf{F}\\) is a  \\(\\sigma\\)-field (sometimes\n \\(\\sigma\\)-algebra) iff it is a field that is closed under\n countable intersections, i.e. if \\(\\mathbf{S}\\subseteq\n\\mathbf{F}\\) is a countable collection of propositions, then the\nintersection of all its elements \\(\\cap \\mathbf{S}\\) is also an\nelement of \\(\\mathbf{F}.\\) That definition implies that a\n\\(\\sigma\\)-field is also closed under countable unions. It is not\ndifficult to prove that the intersection of \\(\\sigma\\)-fields is also\na \\(\\sigma\\)-field. That implies that every collection of propositions\n\\(\\mathbf{F}\\) generates \\(\\sigma(\\mathbf{F})\\), the least\n\\(\\sigma\\)-field containing \\(\\mathbf{F}\\), by intersecting the set of\nall \\(\\sigma\\)-fields containing \\(\\mathbf{F}\\). \nPropositions, although usually expressed by sentences in a language,\nare not themselves sentences. That distinction is commonly drawn by\nsaying that propositions are  semantic objects, whereas\nsentences are  syntactic objects. Semantic objects (like\npropositions) are meaningful, since they represent meaningful\npossibilities, whereas bits of syntax must be\n“interpreted” before they become meaningful. In a slogan:\nsentences are potentially meaningful, whereas propositions already\nare. \nFor our purposes, a  language \\(\\mathbf{L}\\) is identified\nwith the set of all grammatical sentences it contains. Sentences will\nbe denoted by lowercase letters Greek \\(\\alpha, \\beta, \\ldots\\). The\nlanguage \\(\\mathbf{L}\\) is assumed to contain a set of \natomic sentences \\(\\alpha,\\beta, \\ldots\\) which are not built out\nof any other sentences, as well as all the sentences generated by\ncombining the atomic sentences with truth-functional connectives from\npropositional logic. In other words: if \\(\\alpha,\\beta\\) are sentences\nin \\(\\mathbf{L}\\) then \\(\\neg \\alpha\\), \\(\\alpha\\vee \\beta\\),\n\\(\\alpha\\wedge \\beta\\), \\(\\alpha\\rightarrow \\beta\\), and \\(\\alpha\n\\leftrightarrow \\beta\\) are also sentences in \\(\\mathbf{L}\\). These\nare meant to be read respectively as “not \\(\\alpha\\)”,\n“\\(\\alpha\\) or \\(\\beta\\)”, “\\(\\alpha\\) and\n\\(\\beta\\)”, “if \\(\\alpha\\), then \\(\\beta\\)” and\n“\\(\\alpha\\) if and only if \\(\\beta\\)”. The symbol \\(\\bot\\)\n(pronounced “falsum”) denotes an arbitrarily chosen\ncontradiction (e.g. \\(\\alpha\\wedge \\neg \\alpha)\\) and the symbol\n\\(\\top\\) (pronounced “top”) denotes an arbitrary\ntautology. \nSome of the sentences in \\(\\mathbf{L}\\) follow “logically”\nfrom others. For example, under the intended interpretation of the\ntruth-functional connectives, \\(\\alpha\\) follows from the sentence\n\\(\\alpha\\wedge \\beta\\) and also from the set of sentences \\(\\{\\beta,\n\\beta\\rightarrow \\alpha \\}\\). To capture the essentials of deductive\nconsequence, we introduce a  consequence relation,\n\\(\\vdash\\), which holds between any two sentences \\(\\alpha \\vdash\n\\beta\\), whenever \\(\\beta\\) is a deductive consequnce of \\(\\alpha\\) .\nThe consequence operator is assumed to satisfy the following\nproperties, which abstract the characteristic features of deductive\nlogic: \nReflexivity merely expresses the triviality that any sentence\n\\(\\alpha\\) is a deductive consequence of itself. Monotonicity\nexpresses the fact that adding more premises to a deductive argument\nallows you to derive all the same conclusions as you could with fewer.\nCut says roughly that deductive conclusions are on an equal epistemic\nfooting with their premises: there is no loss of confidence as\nderivations get longer. Together, these principles imply that\n“consequences of consequences are consequences” i.e. that\n\\(\\alpha \\vdash \\beta\\) and \\(\\beta \\vdash \\gamma\\) implies that\n\\(\\alpha \\vdash \\gamma\\).  \nWe will write \\(\\Cn(\\alpha)\\) for the set of all \\(\\beta\\) such that\n\\(\\alpha\\vdash \\beta\\). If \\(\\Delta\\) is a finite set of sentences, we\nwrite \\(\\Cn(\\Delta)\\) for \\(\\Cn(\\wedge_{\\alpha\\in\\Delta}\\alpha)\\) the\nset of all deductive consequences of the conjunction of all sentences\nin \\(\\Delta\\). If \\(\\Delta\\) is an infinite set of sentences, it is a\nbit more complicated to define \\(\\Cn(\\Delta)\\) since the infinite\nconjunction of all sentences in \\(\\Delta\\) is not a sentence of the\nformal language. To avoid infinite conjunctions, let \\(\\alpha_1,\n\\alpha_2, \\ldots\\) be an enumeration of the sentences in \\(\\Delta\\)\nand let \\(\\beta_i = \\bigwedge_{j\\leq i} \\alpha_j\\), the conjunction of\nthe first \\(i\\) sentences in the enumeration. Finally, let\n\\(\\Cn(\\Delta) = \\cup_{i=1}^\\infty \\Cn(\\beta_i)\\). It is sometimes\nconvenient to state principles in terms of the consequence operator\n\\(\\Cn(\\cdot)\\). For example, we assume that the deductive consequence\nrelation satisfies the following additional property. \nThe deduction theorem expresses the fact that you can prove the\nconditional sentence \\(\\alpha\\rightarrow \\beta\\) by assuming\n\\(\\alpha\\) and then deriving \\(\\beta\\). Unsurprisingly, it is possible\nto prove that this property holds for most deductive logics one would\nencounter, including both propositional and first-order logic. \nEvery formal language \\(\\mathbf{L}\\) gives rise to a set of possible\nworlds in a canonical way. A model of \\(\\mathbf{L}\\) assigns\na truth value to every sentence in \\(\\mathbf{L}\\) by first, assigning\na truth value to ever atomic sentence and then, assigning truth values\nto all other sentences by respecting the intended meaning of the\nconnectives. We write \\( Mod_{\\mathbf{L}} \\) for the set of all models\nof \\(\\mathbf{L}\\). Thus, each language \\(\\mathbf{L}\\) induces a\nfinitary field \\(\\mathbf{A}\\) over the set of all models for\n\\(\\mathbf{L}\\). \\(\\mathbf{A}\\) is the set of propositions over\n\\(Mod_{\\mathbf{L}}\\) that are expressed by the sentences in\n\\(\\mathbf{L}\\). \\(\\mathbf{A}\\) in turn induces a unique smallest\n\\(\\sigma\\)-field \\(\\sigma(\\mathbf{A})\\) that contains\n\\(\\mathbf{A}\\). \nOnce we have both a set of possibilities \\(W\\) and a formal language\n\\(\\mathbf{L}\\) in context, there is a standard systematic way of\nconnecting them. A  valuation function \\(V\\) maps every\natomic sentence \\(\\alpha\\) in \\(\\mathbf{L}\\) to a proposition\n\\(V(\\alpha)\\subseteq W\\), the set of worlds in which \\(\\alpha\\) is\ntrue under that interpretation of the atoms. For example, if\n\\(W=Mod_\\mathbf{L}\\), then atoms are mapped exactly to the models in\nwhich they are true. The valuation function also interprets the\nnon-atomic sentences in a way that respects the intended meanings of\nthe logical connectives, i.e. so that \\(V(\\top)=W\\), \\(V(\\neg\n\\alpha)=W\\setminus V(\\alpha)\\) and \\(V(\\alpha\\wedge \\beta)=\nV(\\alpha)\\cap V(\\beta).\\) In this fashion, each sentence in\n\\(\\mathbf{L}\\) is mapped to a set of possible worlds. \nWe write \\(\\alpha \\vDash \\beta \\) if for all valuations \\(V\\),\n\\(V(\\alpha) \\subseteq V(\\beta).\\) Then, \\( \\alpha \\vDash \\beta\\)\nexpresses the fact that no matter how the non-logical vocabulary of\n\\(\\mathbf{L}\\) are interpreted, \\(\\beta\\) is true in all the worlds in\nwhich all sentences in \\(\\alpha\\) are true. We say that \\(\\alpha\\) is\n valid iff \\(\\{ \\top \\} \\vDash \\alpha\\), i.e if \\(W\\subseteq\nV(\\alpha)\\) for all valuation functions. Then, \\(\\alpha\\) is valid iff\n\\(\\alpha\\) is true in all possible worlds, no matter how the\nnon-logical vocabulary are interpreted. For example, the sentence\n\\(\\alpha\\vee \\neg \\alpha\\) is valid. \nWe assume the following property of our deductive consequence\nrelation. \nSoundness says that if the sentence \\(\\beta\\) is a derivable\nconsequence of \\(\\alpha\\), then no matter how the non-logical\nvocabulary of \\(\\mathbf{L}\\) are interpreted, \\(\\beta\\) is true in all\nthe worlds in which \\(\\alpha\\) true. That is to say that from true\npremises, our consequence relation always derives true conclusions.\nSoundness also implies that every theorem is valid. Soundness is a\nbasic requirement of any  deductive consequence relation, and\nillustrates the intended connection between deductive proof and\nsemantic entailment. \nSentences are, in a sense, capable of expressing distinctions that\npropositions cannot. For example, the two sentences \\(p\\) and \\(\\neg\n\\neg p\\) are obviously distinct. But if \\(p\\) and \\(q\\) are provably\nequivalent, i.e. if \\(\\vdash p \\leftrightarrow q,\\) then \\(\\{p\\}\\vdash\nq\\) and \\(\\{q\\} \\vdash p.\\) By Soundness, \\(\\{p\\} \\vDash q\\) and\n\\(\\{q\\}\\vDash p.\\) Therefore, for any valuation function,\n\\(V(p)=V(q).\\) So \\(p\\) and \\(q\\) must express the same proposition.\nOf course, an agent who is unaware of the equivalence might believe\n\\(p\\) without believing \\(q\\). What's worse, every sentence \\(p\\) such\nthat \\(\\vdash p\\) must express the tautological proposition \\(W\\). Of\ncourse, ordinary agents do not always recognize theorems of\npropositional logic. For this reason, some argue that it is sentences,\nrather than propositions, that are the appropriate objects of belief.\nHowever, most of the proposed models we will study require that\nrational agents adopt the same belief attitude toward logically\nequivalent sentences. That is a very strict requirement, amounting to\nan assumption that every rational agents is logically\nomniscient, i.e. she finds all logical entailments to be\ncompletely transparent. So long as that is the case, there is no\nsignificant difference between taking the objects of belief to be\nsentences or propositions. See, however, Hacking (1967), Garber (1983)\nand Pettigrew (forthcoming) for ideas on how to relax the requirement of\nlogical omniscience. Still others are not satisfied with either\nsentences, or propositions. Perry (1979), Lewis (1979) and Stalnaker\n(1981) argue that in order to capture  essentially indexical\nbeliefs–beliefs that essentially involve indexicals such as \nI, here, or  now–the objects of belief must be\n centered propositions. We will not take up this helpful\nsuggestion here, but see Ninan (2019) or Liao (2012) for more on\ncentered propositions. \nA prominent tradition in epistemology takes belief to be an\nall-or-nothing matter. According to this view, there are three\ndoxastic attitudes an agent can take toward a sentence or proposition:\neither she believes \\(\\alpha\\) but not \\(\\neg \\alpha\\); she believes\n\\(\\neg \\alpha\\) but not \\(\\alpha\\); or she believes neither \\(\\alpha\\)\nnor \\(\\neg \\alpha\\). In the first case we say simply that she fully\nbelieves \\(\\alpha\\); in the second case we say she fully\ndisbelieves \\(\\alpha\\) and in the third we say she\nsuspends judgement with respect to \\(\\alpha\\) and \\(\\neg\n\\alpha\\). Perhaps it is also psychologically possible for an agent to\nfully believe both \\(\\alpha\\) and \\(\\neg \\alpha\\). Since most\ntheorists agree that she ought not, we introduce no special\nterminology for this case. Any belief representation that allows for\nfiner gradations of belief attitude we will call a graded\nrepresentation of belief.  \nThe frameworks we introduce in this section deal chiefly with belief\nattitudes of the all-or-nothing variety. Most of these represent an\nagent’s belief state at any particular time by the set of all\nsentences that she fully believes. All of the frameworks will require\nthat, as a matter of rationality, belief states must be deductively\nclosed and not entail any contradictions. If that were the end of the\nstory, these frameworks would not be very interesting. The real\ninsight of these frameworks is that an agent’s belief state is\nnot adequately represented by a complete list of her current beliefs,\nbut only by her dispositions to update those beliefs upon\nacquiring new information. Thus, the focus of these frameworks is\ndeveloping normative principles governing the dynamics of belief\nstates as new information is assimilated. As we shall see, although\nthese frameworks largely agree on the static principles of\nrationality, they conflict over the dynamical principles of belief\nupdate. \nIt is often informative to see how qualitative update dynamics emerge\nfrom the finer structure of degrees of belief. We will see in this\nsection several versions of the following kind of\n“representation” result: every agent satisfying a certain\nset of qualitative dynamical update principles can be thought of as an\nagent updating graded beliefs of a certain structure and, conversely,\nevery agent with graded beliefs of that structure will satisfy the\nsame set of qualitative dynamical principles. These results suggest\nbridge principles connecting full and partial representations of\nbelief. \nIn\n Section 1.2\n we introduced the notion of a deductive consequence relation. One of\nthe characteristic features of a deductive consequence relation is\nthat adding more premises to a deductive argument allows you to derive\nall the same consequences as you could with fewer. In other words, for\nany sentences \\(\\alpha , \\beta , \\gamma\\) in \\(\\mathbf{L}\\): if\n\\(\\alpha \\vdash \\gamma,\\) then \\( \\alpha\\wedge \\beta \\vdash\n\\gamma\\). \nOf course, all sorts of seemingly rational everyday reasoning violates\nMonotonicity. If Sophia is told that her thermometer reads 85°\nFahrenheit, she would be justified in concluding that it is not too\ncold to have dinner in the garden. If she then learns that her\nthermometer was moved above the oven where she is boiling her pasta,\nshe might retract her conclusion. That does not mean that her original\ninference was unreasonable or irrational. Non-monotonicity is simply\nunavoidable in ordinary human contexts. Inductive inference is\nfamously non-monotonic. Ethical and legal reasoning is simlarly shot\nthrough with non-monotonicities (Ross, 1930 and Ullman-Margalit,\n1983). \nNon-monotonic logic studies a defeasible consequence relation\n\\(\\dproves\\) between premises, on the left of the wavy turnstile, and\nconclusions on the right. One may think of the premise \\(\\alpha\\) on\nthe left as a sentence expressing all the “hard evidence”\nthat an agent may possesses, and the conclusion on the right to be the\ndefeasible conclusions that are justified on the basis of \\(\\alpha\\).\nThe expression \\(\\alpha \\dproves \\beta \\) may be read as: if my total\nevidence were \\(\\alpha\\), I would be justified in concluding\n\\(\\beta\\). Thus a particular relation of defeasible consequence\nrepresents an agent's dispositions to update her beliefs in light of\nnew information. \nRecall from\n Section 1.2\n that a deductive consequence relation satisfies Soundness. That is to\nsay that \\( \\alpha \\vdash \\beta\\) only if \\( \\beta \\) is true in all\nthe worlds in which \\(\\alpha\\) is true. It is clear from the preceding\nexamples that defeasible reasoning cannot satisfy Soundness. If\n\\(\\alpha \\dproves \\beta\\) then perhaps \\(\\beta\\) is true in\n“typical” worlds in which \\(\\alpha\\) is true. We call a\nconsequence relation ampliative if \\(\\alpha \\dproves \\beta,\\)\nbut there are worlds in which \\( \\alpha \\) is true, but \\(\\beta\\) is\nfalse. The terminology derives from the Latin ampliare,\n“to enlarge”, because defeasible reasoning\n“extends” or “goes beyond” the premises.\nAmpliativity and non-monotonicity go hand in hand in all but the most\ncontrived circumstances.  \nOver the past forty years, researchers in artificial intelligence have\ncreated many different logics for defeasible inference, often\ndeveloped to model a specific kind of defeasible reasoning. See the\nentry on\n non-monotonic logic\n for an excellent overview. In view of this profusion of specialized\nlogics, non-monotonic logic investigates which properties a logic of\ndefeasible consequence must have in order to count as a \nlogic at all. (See Gabbay (1985) for the origins of this abstract\npoint of view.) Non-monotonic logic provides a crucial  lingua\nfranca for comparing different logics of defeasible inference. It\nis also extremely apt for the purposes of this article because it\nallows us to compare different normative theories of how beliefs ought\nto be updated in light of new evidence, as well as theories of how\nfull and partial beliefs ought to relate to each other.  \nBefore we proceed to the technical development, it will be helpful to\nintroduce an important early critique of nonmonotonic logic due to the\nphilosopher John Pollock. Pollock (1987) identifies two sources of\nnonmonotonicity in defeasible reasoning. An agent may believe\n\\(\\beta\\) because she believes \\(\\alpha\\) and takes \\(\\alpha\\) to be a\ndefeasible  reason for \\(\\beta\\). Pollock distinguishes two\nkinds of  defeaters for this inference: a  rebutting\ndefeater is a defeasible reason to believe \\(\\neg \\beta,\\) whereas an\n undercutting  defeater is a reason to believe \\(\\neg\n\\alpha\\). Either kind of defeater may induce an agent to retract her\nbelief in \\(\\beta\\). Pollock's point is that since nonmonotonic logics\ntypically do not represent the structure of an agent's reasons, they\noften fail to elegantly handle cases of undercutting defeat. We shall\nsoon see several examples.  \nKraus, Lehmann and Magidor (1990) articulate a set of principles that\nany “reasonable” nonmonotonic language must satisfy. It is\nnow common to refer to this set of principles as System P. To this\nday, these principles remain the uncontroversial core of nonmonotonic\nlogic. \nReflexivity merely expresses the truism that one is entitled to infer\n\\(\\alpha\\) from itself. The next two principles govern how a\nnon-monotonic consequence relation should interact with deductive\nconsequence. Left Logical Equivalence says that if \\(\\alpha\\) and\n\\(\\beta\\) are classically equivalent, then they license the exact same\ndefeasible inferences. Right Weakening says that if \\(\\alpha\\)\ndefeasibly licenses \\(\\beta\\), then it also licenses all the deductive\nconsequences of \\(\\beta\\). Together, these principles say that\ndefeasible reasoning encompasses all of deductive reasoning. This\nsounds unreasonable if we think of \\(\\dproves\\) as modeling the\ndefeasible reasoning of a bounded agent. It begins to sound better if\nwe think of \\(\\dproves\\) as modeling the ampliative conclusions that\nare justified on the basis of some “hard” evidence. \nThe remaining principles are the heart of System P. Cut says that\nadding conclusions defeasibly inferred from \\(\\alpha \\) to the\npremises does not increase inferential power. Cautious\nMonotonicity says that it does not decrease inferential\npower. Let \\(C(\\alpha)\\) be \\( \\{ \\beta : \\alpha \\dproves \\beta \\} \\),\nthe set of conclusions licensed by \\(\\alpha\\). If we think of the\npremise on the left of \\(\\dproves\\) as expressing my total\n“hard” evidence, and the set \\(C(\\alpha)\\) as a theory\ninductively inferred on the basis of \\(\\alpha\\), then Cautious\nMonotonicity is an expression of hypothetico-deductivism: if I learn a\nconsequence of my theory \\(C(\\alpha)\\), I should not retract any of my\nprevious conclusions. Moreover, Cut says that I should not add any new\nconclusions. Taken together, the two principles say that if a\nconsequence of your theory is added to your total evidence, your\ntheory should not change, i.e. if \\(\\alpha\\dproves \\beta\\), then\n\\(C(\\alpha)=C(\\alpha\\wedge \\beta)\\).  \nThe final principle is best approached by way of one of its instances.\nSubstituting \\(\\neg \\alpha\\) for \\(\\beta\\) yields the following\nprinciple: \nAny genuine consequence relation ought to enable reasoning by cases.\nIf I would infer \\(\\gamma\\) irrespective of what I learned about\n\\(\\alpha\\), I should be able to infer \\(\\gamma\\) before the matter of\n\\(\\alpha\\) has been decided. In full generality, Or says that if\n\\(\\gamma\\) follows defeasibly from both \\(\\alpha\\) and \\(\\beta\\), it\nought to follow from their disjunction. Any consequence relation that\nsatisfies Reflexivity, Right Weakening and Or must also satisfy the\nfollowing principle: \nConditionalization says that upon learning new evidence, you never\n“jump to conclusions” that are not entailed by the\ndeductive closure of your old beliefs with the new evidence. That is\nnot an obviously appealing principle. An agent that starts out with\nonly the trivial evidence \\(\\top\\) will either fail to validate\nConditionalization or never make any ampliative inferences at all.\nSuppose that after observing 100 black ravens an agent validating\nConditionalization comes to believe that all ravens are black. Then,\nat the outset of inquiry, she must have believed that either all\nravens are black, or she will see the first non-black raven among the\nfirst hundred. Such an agent seems strangely opinionated about when\nthe first counterexample to the inductive generalization must\nappear. \nFor a more realistic example, consider the 1887 Michelson-Morely\nexperiment. After a null result failing to detect any significant\ndifference between the speed of light in the prevailing direction of\nthe presumed aether wind, and the speed at right angles to the wind,\nphysicists turned against the aether theory. If the physicists\nvalidated Conditionalization then, before the experiments, they must\nhave believed that either there is no luminiferous aether, or the\naether wind blows quickly enough to be detected by their equipment.\nBut why should they have been so confident that the aether wind is not\ntoo slow to be detectable? \nEven if there is nothing objectionable about an agent who validates\nConditionalization, there is something very anti-inductivist about the\nthesis that all justified defeasible inferences on the basis of new\nevidence can be reconstructed as deductive inferences from prior\nconclusions plus the new evidence. Under Conditionalization,\ndispositions to form inductive generalizations must be\n“programmed in” with material conditionals at the outset\nof inquiry. (See Schurz (2011) for a similar critique, in a slightly\ndifferent context.) Anyone sympathetic to this critique must reject\neither Or, Reflexivity, or Right Weakening. Finding such surprising\nconsequences of seemingly unproblematic principles is one of the boons\nof studying non-monotonic logic. \nWe finish this section by introducing one more prominent and\ncontroversial principles of non-monotonic logic. Adding this principle\nto System P results in what is commonly referred to as System R. The\nposition one takes on this principle will determine how one feels\nabout many of the theories which we turn to in the following. Kraus et\nal. (1990) claim that any rational reasoner should validate the\nfollowing strengthening of Cautious Monotonicity: \nRational Monotonicity says that so long as new evidence \\(\\gamma\\) is\nlogically compatible with your prior beliefs \\(C(\\alpha)\\), you should\nnot retract any beliefs from \\(C(\\alpha)\\). Accepting both Rational\nMonotonicity and Conditionalization amounts to saying that when\nconfronted with new evidence that is logically consistent with her\nbeliefs, a rational agent responds by simply forming the deductive\nclosure of her existing beliefs with the new evidence. On that view,\ndeductive logic is the only necessary guide to reasoning, so long as\nyou do not run into contradiction. Stalnaker (1994) gives the\nfollowing well-known purported counterexample to Rational\nMonotonicity.  \nSuppose that Sophia believes that Verdi is Italian and that Bizet and\nSatie are both French. Let \\(\\alpha\\) be the sentence that Verdi and\nBizet are compatriots. Let \\(\\beta\\) be the belief that Satie is\nFrench. Let \\(\\gamma\\) be the sentence that Bizet and Satie are\ncompatriots. Suppose that Sophia receives the evidence \\(\\alpha\\). As\na result, she concludes that either Verdi and Bizet are both French,\nor they are both Italian, but she cannot say which. She retains her\nbelief that Satie is French. So \\(\\alpha \\dproves \\beta\\) and \\(\\alpha\n\\not\\mkern-7mu\\dproves \\neg \\gamma\\). Now suppose that she receives\nthe evidence \\(\\gamma\\). Since \\(\\gamma\\) is compatible with her\nprevious conclusions, Rational Monotonicity requires her to retain\nbelief in \\(\\beta\\) and conclude that all three composers are French.\nHowever, it seems perfectly rational to retract \\(\\beta\\) and conclude\nthat the three are either all Italian, or all French.  \nKelly and Lin (forthcoming) use the following example to argue against\nRational Monotonicity. There are just two people in Sophia's office,\nnamed Alice and Bob. She is interested in whether one of them owns a\ncertain Ford. Let \\(\\beta\\) be the sentence that Alice owns the Ford.\nLet \\(\\gamma\\) be the sentence that Bob has the Ford. Sophia has\ninconclusive evidence that Alice owns the Ford–she saw her\ndriving one just like it. She has weaker evidence that Bob owns the\nFord–his brother owns a Ford dealership. Based on her total\nevidence \\(\\alpha\\) she concludes \\(\\beta \\vee \\gamma\\), i.e. that\nsomeone in the office owns the Ford, but does not go so far as\ninferring \\(\\beta\\), or \\(\\gamma\\). Then, Alice says that the Ford she\nwas driving was rented and that she doesn't own a car. That defeats\nSophia's main reason for \\(\\beta\\vee\\gamma\\), and therefore she\nretracts her belief that someone in the office has a Ford. But since\n\\( \\alpha \\not\\mkern-7mu\\dproves \\neg \\beta \\), Rational Monotonicity\nrequires her to retain belief in \\(\\beta\\vee\\gamma\\) and conclude that\nBob owns the Ford. However, there does not seem to be anything\nirrational about she has reasoned. This seems to be an illustration of\nPollock’s (1987) point: the logic is going wrong because it is\nignoring the structure of Sophia’s reasons. \nDefenders of Rational Monotonicity will argue that, if we find these\ncounterexamples plausible, it is because we are under-representing\neither the structure of the agents prior beliefs, or of the\ninformation that triggers the update. If we were to describe these in\nrealistic detail, they argue, we would not be convinced by these\ncounterexamples. For an elaboration of this line of defense, see\nSection 5 in Lin (2019). Other criticisms of Rational Monotonicity do\nnot rest essentially on the plausibility of counterexamples, but on\nthe incompatibility of Rational Monotonicity with competing norms of\ninquiry. One such criticism, due to Lin and Kelly (2012), will be\ndiscussed in Section 4.2.4. For another example, see Genin and Kelly\n(2018).  \nSo far we have considered a non-monotonic consequence relation merely\nas a relation between syntactic objects. We can rephrase properties of\nnon-monotonic logic “semantically,” i.e. in terms of the\npossible worlds in which the sentences are true or false. In some\ncases, this allows us to give a very perspicuous view on defeasible\nlogic. \nRecall from\n Section 1.2\n that a deductive consequence relation satisfies Soundness, i.e. that\n\\(\\alpha\\vdash \\beta\\) only if \\(\\beta\\) is true in all the worlds in\nwhich \\(\\alpha\\) is true. As we have discussed, non-monotonic logics\nare ampliative, and therefore must violate Soundness. Shoham (1987)\ninaugurated a semantics for non-monotonic logics in which\n\\(\\alpha\\dproves \\beta\\) only if \\(\\beta\\) is true in a\n“preferred” set of worlds in which \\(\\alpha\\) is true. On\na typical interpretation, these are the most typical, or most normal\nworlds in which \\(\\alpha\\) is true. \nKraus et al. (1990) first proved most of the results of this section.\nHowever, the original results are stated at a level of generality that\ncreates too much technical fuss for our purposes here. We state a\nsimplified, but more perspicuous, version of their results. See\nMakinson (1994) and the entry on\n defeasible reasoning\n for a presentation truer to the original. \nA preferential model is a triple \\(\\langle W,V,\\prec \\rangle\n\\) where \\(W\\) is a set of possible worlds, \\(V\\) is a valuation\nfunction and \\(\\prec\\) is an arbitrary relation on the elements of\n\\(W\\). Recall from Section\n Section 1.2\n that \\(V(\\alpha)\\) is the set of worlds in which \\(\\alpha\\) is true.\nThe relation \\(\\prec\\) is transitive iff \\(x\\prec z\\)\nwhenever \\(x\\prec y\\) and \\(y \\prec z \\). The relation \\(\\prec \\) is\nirreflexive iff for all \\(w\\in W\\) it is not the case that\n\\(w\\prec w\\). A transitive, irreflexive relation is called a\nstrict order. We write \\(w\\preceq v\\) iff \\(w\\prec v\\) or\n\\(w=v\\). The strict order \\(\\prec\\) is  total iff for\n\\(w,v\\in W\\) either \\(w\\prec v\\) or \\(v\\prec w\\). \nWe say that \\(w\\) is an \\(\\alpha\\)-minimal world iff \\(w\\in\nV(\\alpha)\\) and there is no \\(v\\in V(\\alpha)\\) such that \\(v\\prec w\\).\nEvery preferential model gives rise to a consequence relation by\nsetting \\(\\alpha \\dproves \\beta\\) whenever \\(\\beta\\) is true in all\nthe \\(\\alpha\\)-minimal worlds. Kraus et al. (1990) prove the\nfollowing: \nTheorem. Suppose that \\(\\prec\\) is a strict order\nover \\(W\\). Define the consequence relation \\(\\dproves\\) by setting\n\\(\\alpha \\dproves \\beta\\) iff \\(\\beta\\) is true in all the\n\\(\\alpha\\)-minimal worlds. Then, \\(\\dproves\\) satisfies KLM1-6. If\n\\(\\prec\\) is total, KLM7 is also satisfied. Conversely, suppose that\nthe consequence relation \\(\\dproves\\) satisfies KLM1–6. Then, there is\na strict order \\(\\prec\\), such that \\(\\alpha \\dproves \\beta\\) iff\n\\(\\beta\\) is true in all the \\(\\alpha\\)-minimal worlds. If KLM7 is\nalso satisfied, then the order can be chosen to be total. \nThat means that we can interpret any consequence relation \\(\\dproves\\)\nsatisfyings the basic KLM postulates in the following way:\n\\(\\dproves\\) represents the belief dispositions of an agent who (1)\nhas a strict plausibility ordering over the set of possibilities in\n\\(W\\) and (2) believes whatever is entailed by the “most\nplausible” possibilities compatible with her hard evidence. If\nRational Monotonicity is not satisfied, then some possibilities will\nbe incommensurable in plausibility. Otherwise, all possibilities will\nbe commensurable. \nLike nonmonotonic logic, the theory of belief revision is concerned\nwith how to update one’s beliefs in light of new evidence. When\nthe new evidence is consistent with all prior beliefs, the counsel of\nthe theory is simple, if exacting: simply add the new evidence to your\nold stock of beliefs and close under deductive consequence. Things are\nmore complicated when the new evidence is inconsistent with your prior\nbeliefs. If you want to incorporate the new information and remain\nlogically consistent, you will have to retract some of your original\nbeliefs. The central problem of belief revision is that deductive\nlogic alone cannot tell you which of your beliefs to give\nup–this has to be decided by some other means. \nConsidering a similar problem, Quine and Ullian (1970) enunciated the\nprinciple of “conservatism,” conseling that our new\nbeliefs “may have to conflict with some of our previous beliefs;\nbut the fewer the better.” In his (1990), Quine dubs this the\n“maxim of minimal mutilation.” Inspired by these\nsuggestive principles, Alchourrón, Gärdenfors, and\nMakinson (1985) developed a highly influential theory of belief\nrevision, known thereafter as AGM theory, after its three originators.\n \nIn AGM theory, an agent’s beliefs are represented by a set of\nsentences \\(\\bB\\) from a formal language \\(\\mathbf{L}\\). The set\n\\(\\bB\\) is called the belief state of the agent. The belief\nstate is required to be consistent and deductively closed (Hintikka\n1961 and the entry on\n epistemic logic).\n Of course, this is a somewhat unrealistic idealization. Levi (1991)\ndefends this idealization by changing the interpretation of the set\n\\(\\bB\\)–these are the sentences that the agent is\ncommitted to believe, not those that she actually believes.\nAlthough we may never live up to our commitments, Levi argues that we\nare committed to the logical consequences of our beliefs. That may\nrescue the principle, but only by changing the interpretation of the\ntheory. \nAGM theory studies three different types of belief change.\nContraction occurs when the belief state \\(\\bB\\) is replaced\nby \\(\\bB\\div \\alpha\\), a logically closed subset of \\( \\bB\\) no longer\ncontaining \\(\\alpha\\). Expansion occurs when the belief state\n\\( \\bB\\) is replaced with \\(\\bB+\\alpha = \\Cn(\\bB\\cup \\{\\alpha \\})\\),\nthe result of simply adding \\(\\alpha\\) to \\(\\bB\\) and closing under\nlogical consequence. Revision occurs when the belief state\n\\(\\bB\\) is replaced by the belief state \\(\\bB*\\alpha\\), the result of\n“minimally mutilating” \\(\\bB\\) in order to accomodate\n\\(\\alpha\\). \nContraction is the fundamental form of belief change studied by AGM.\nThere is no mystery in how to define expansion, and revision is\nusually defined derivatively via the  Levi identity (1977):\n\\(\\bB*p = (\\bB \\div \\neg p) + p.\\) Setting the mold for future work\n(Gärdenfors 1988, Gärdenfors and Rott 1995),\nAlchourrón, Gärdenfors and Makinson (1985) proceed\naxiomatically: they postulate several principles that every rational\ncontraction operation must satisfy. Fundamental to AGM theory are\nseveral representation theorems showing that certain intuitive\nconstructions give rise to contraction operations satisfying the basic\npostulates and conversely, that every operation satisfying the basic\npostulates can be seen as the outcome of such a construction. See the\nentry on\n belief revision,\n Huber (2013a), or Lin (2019) for an excellent introduction to these\nresults.  \nAGM theory is unique in focusing on belief contraction. For someone\nconcerned with maintaining a database, contraction is a fairly natural\noperation. Medical researchers might want to publish a data set, but\nmake sure that it cannot be used to identify their patients. Privacy\nregulations may force data collectors to “forget” certain\nfacts about you and, naturally, they would want to do this as\nconservatively as possible. However, a plausible argument holds that\nall forms of rational belief change occurring “in the\nwild” involve learning new information, rather than\nconservatively removing an old belief. All the other formalisms\ncovered in the article focus on this form of belief change. For this\nreason, we focus on the AGM theory of revision and neglect\ncontraction. \nBefore delving into some of the technical development, we mention some\nimportant objections and alternatives to the AGM framework. As we have\nmentioned, the belief state of an agent is represented by the\n(deductively closed) set \\(\\bB\\) of sentences the agent is committed\nto believe. The structure of the agent’s  reasons is\nnot explicitly represented: you cannot tell of any two\n\\(\\alpha,\\beta\\in \\bB\\) whether one is a reason for the other.\nGärdenfors (1992) distinguishes between  foundations\ntheories, that keep track of which beliefs justify which others, and\n coherence theories, which ignore the structure of\njustification and focus instead on whether beliefs are consistent with\none another. Arguing for the coherence approach, he draws a stark\ndistinction between the two: \nAccording to the foundations theory, belief revision should consist,\nfirst, in giving up all beliefs that no longer have a \nsatisfactory justification and, second, in adding new beliefs\nthat have become justified. On the other hand, according to the\ncoherence theory, the objectives are, first, to maintain \nconsistency in the revised epistemic state, and, second, to make\n minimal changes of the old state that guarantee overall\ncoherence.  \nImplicit in this passage is the idea that foundations theory are out\nof sympathy with the principle of minimal mutilation. Elsewhere\n(1988), Gärdenfors is more conciliatory, suggesting that some\nhybrid theory is possible and perhaps even preferable: \nI admit that the postulates for contractions and revisions that have\nbeen introduced here are quite simpleminded, but they seem to capture\nwhat can be formulated for the meager structure of belief sets. In\nricher models of epistemic states, admitting, for example, reasons to\nbe formulated, the corresponding conservativity postulates must be\nformulated much more cautiously (p. 67). \nPreviously, we have seen Pollock (1987) advocating for\nfoundationalism. In artificial intelligence, Doyle’s (1979) \nreason maintenance system is taken to exemplify the foundations\napproach. Horty (2012) argues that\n default logic\n aptly represents the structure of reasons. For a defense of\nfoundationalism, as well as a useful comparison of the two approaches,\nsee Doyle (1992). \nAnother foundationalist view advocates for  belief bases\ninstead of belief states. A belief base is a set of sentences that is\ntypically not closed under logical consequence. Its elements represent\n“basic” beliefs that are not derived from other beliefs.\nThis allows us to distinguish between sentences that are explicit\nbeliefs, like “Shakespeare wrote Hamlet” and\nnever-thought-of deductive consequences like “Either Shakespeare\nwrote Hamlet or Alan Turing was born on a Monday.” Revision and\ncontraction are then redefined to operate on belief bases, rather than\nbelief sets. That allows for finer distinctions to be articulated,\nsince belief bases which have the same logical closure are not treated\ninterchangeably. For an introduction to belief bases see the\nrelevant section of the entry on the  \n logic of belief revision.\n For a book-length treatment, see Hansson (1999b). \nFinally, one of the most common criticisms of AGM theory is that it\ndoes not illuminate  iterated belief change. In the\nfollowing, we shall see that the canonical revision operation takes as\ninput an entrenchment ordering on a belief state, but outputs a belief\nstate without an entrenchment order. That severely underdetermines the\nresult of a subsequent revision. For more on the problem of iterated\nbelief revision, see Huber (2013a). \nGärdenfors (1988) proposes the following postulates for rational\nbelief revision. For every set of sentences \\(\\bB \\subseteq\n\\mathbf{L}\\) and any sentences \\(\\alpha , \\beta \\in \\mathbf{L}\\): \nClosure, Success, Consistency and Extensionality all impose\nsynchronic constraints on \\(\\bB*\\alpha\\). They say nothing\nabout how it must be related to the prior belief state \\(\\bB\\).\nClosure requires the new belief set to be deductively closed. Dropping\nthis requirement puts you in the camp of belief bases, rather\nthan belief sets (Hansson 1999a). Success requires that the new\ninformation is included in the new belief state.\nNon-prioritized belief revision relaxes this requirement\n(Hansson 1999a). Extensionality requires that the agent is sensitive\nonly to the logical content of the evidence, and not to its mode of\npresentation. Consistency requires that the new belief state is\nlogically consistent, at least when the evidence is\nnon-contradictory. \nPreservation and Inclusion are the only norms that are really about\nrevision–they capture the diachronic spirit of AGM\nrevision. Inclusion says that revision by \\(\\alpha\\) should yield \nno more new beliefs than expansion by \\(\\alpha\\). In other words,\nany sentence \\(\\beta\\) that you come to believe after revising by\n\\(\\alpha\\) is a deductive consequence of \\(\\alpha\\) and your prior\nbeliefs. Consider the following principle: \nIn\n Section 2.1.1,\n we considered an analogue of conditionalization for nonmonotonic\nlogic. All the same objections apply equally well in the context of\nbelief revision. Recall from\n Section 1.2\n that a deductive consequence relation admits a deduction theorem iff\n\\(\\Delta \\cup \\{ \\alpha\\} \\vdash \\beta\\) implies that \\(\\Delta \\vdash\n\\alpha\\rightarrow \\beta.\\) So long as a deduction theorem is provable\nfor \\(\\Cn(\\cdot),\\) Inclusion and Conditionalization are equivalent.\nIf you found any of the arguments against Conditionalization\nconvincing, you ought to be skeptical of Inclusion. \nPreservation says that, so long as the new information \\(\\alpha\\) is\nlogically consistent with your prior beliefs, all of your prior\nbeliefs survive revision by \\(\\alpha\\). In the setting of\nnon-monotonic logic, we called this principle Rational Monotony. All\nobjections and counterexamples to Rational Monotony from\n Section 2.1.1\n apply equally well in belief revision. As we have seen, Preservation\nrules out any kind of  undercutting defeat of previously\nsuccessful defeasible inferences. Accepting both Preservation\n(rational monotonicity) and Inclusion (conditionalization) amounts to\nsaying that when confronted with new evidence that is logically\nconsistent with her beliefs, a rational agent responds by simply\nforming the deductive closure of her existing beliefs with the new\nevidence. On that view, deductive logic is the only necessary guide to\nreasoning, so long as you do not run into contradiction. \nGärdenfors (1988) also proposes the following two additional\nrevision postulates, closely related to Inclusion and\nPreservation. \nIt is possible to make the connection between belief revision and\nnonmonotonic logic precise. Given a belief set \\(\\bB\\) and a revision\noperation \\(*,\\) we can define a defeasible consequence relation by\nsetting \\(\\alpha \\dproves \\beta \\text{ iff } \\beta \\in \\bB*\\alpha.\\)\nSimilarly, given a defeasible consequence relation \\(\\dproves\\) we can\ndefine \\(\\bB = \\{ \\alpha : \\top \\dproves \\alpha \\} \\text{ and } \\bB *\n\\alpha = \\{ \\beta : \\alpha\\dproves \\beta \\}.\\) Then it is possible to\nprove the following correspondences between AGM belief revision and\nthe set of principles we called System R in\n Section 2.1.1.\n It follows that AGM revision can be represented in terms of a total\npreferential model over possible worlds. \nTheorem. Suppose that \\(*\\) is a revision operation\nfor \\(\\bB\\) satisfying all eight revision postulates. Then, the\nnon-nonmonotonic consequence relation given by \\(\\alpha \\dproves \\beta\n\\text{ iff } \\beta \\in \\bB * \\alpha\\) satisfies all the principles of\nSystem R. Conversely, suppose that \\(\\dproves\\) is a consequence\nrelation that satisfies all the principles of System R. Then, the\nrevision operation \\(*\\) defined letting \\(\\bB = \\{ \\alpha : \\top\n\\dproves \\alpha \\}\\) and \\(\\bB*\\alpha = \\{ \\beta : \\alpha \\dproves\n\\beta \\}\\) satisfies all eight revision postulates. \nGärdenfors (1988) introduces the notion of an entrenchment\nrelation in the following way: \nEven if all sentences in a … set are accepted or considered as\nfacts …, this does not mean that all sentences are of equal\nvalue for planning or problem-solving purposes. … We will say\nthat some sentences … have a higher degree of epistemic\nentrenchment than others. The degree of entrenchment will,\nintuitively, have a bearing on what is abandoned …, and what is\nretained, when a contraction or revision is carried out.  \nTo model the degree of entrenchment, a relation \\(\\preceq\\) is\nintroduced holding between sentences of the language \\(\\mathbf{L}\\).\nThe notation \\(\\alpha \\preceq \\beta\\) is pronounced ‘\\(\\alpha\\)\nis at most as entrenched as \\(\\beta\\).’ Gärdenfors (1988)\nrequires that the relation satisfies the following postulates. For all\n\\(\\alpha , \\beta , \\gamma\\) in \\(\\mathbf{L}\\):  \nGiven a fixed set of background beliefs \\(\\bB\\) and an entrenchment\nordering \\(\\preccurlyeq\\) on \\(\\mathbf{L}\\) and letting \\(\\alpha\n\\preccurlyneq \\beta\\) hold just in case \\(\\alpha \\preccurlyeq \\beta\\)\nand \\(\\beta \\not\\preccurlyeq \\alpha\\), we can define a revision\noperator \\(*\\) as follows: \nThe idea behind this equation is that the agent revises by \\(\\alpha\\)\nby first clearing from her belief set anything less entrenched than\n\\(\\neg \\alpha,\\) (by dominance, this includes everything entailing\n\\(\\neg \\alpha\\)), adding \\(\\alpha\\), and then closing under logical\nconsequence. Gärdenfors and Makinson (1988) prove that this\ndefinition gives rise to a revision operator satisfying AGM*1–8\nand, conversely, that every revision operator satisfying AGM*1–8\ncan be seen as arising from some entrenchment order in this\nway. These results illustrate why AGM theory is not a theory of\niterated revision: the revision operation takes as input an\nentrenchment order and belief state, but outputs only a belief state.\nThat severely underdetermines the results of subsequent revisions.\n \nGrove (1988) proves an analogous representation theorem for a systems\nof spheres semantics that generalizes Lewis’ (1973) semantics\nfor counterfactuals. See the relevant section of the entry on the\n logic of belief revision\n for an introduction to\nGrove's possible world semantics. Segerberg (1995) formulates the AGM\napproach in the framework of dynamic doxastic logic. Lindström\n& Rabinowicz (1999) extend this to iterated belief revision. Of\nnecessity, our presentation of belief revision theory has been rather\ncompressed. For an excellent article-length introduction, see the\nentry on\n logic of belief revision,\n Huber (2013a), or Lin (2019). For a book-length treatment see\nGärdenfors (1988), Hansson (1999), or Rott (2001).  \nThe formal frameworks we have seen so far foreground dynamical\nprinciples of belief revision—they are essentially concerned\nwith how to accomodate new information. That theoretical emphasis\nrelegates other issues to the background. In particular, these\nframeworks cannot easily express principles of higher-order belief\n(regulating beliefs about beliefs), or about the relationship between\nbelief and knowledge. Therefore, these frameworks are relatively\nsilent about classical epistemological issues such as whether\nknowledge entails belief (see the entry on the\n analysis of knowledge),\n or why it is absurd to believe both \\(p\\) and that you don't believe\nthat \\(p\\) (see the entry on \n epistemic paradoxes).  \nEpistemic logic, which in its modern formulation is typically traced\nto von Wright (1951) and Hintikka (1962), is a formal framework\ndesigned to foreground these matters. For an influential introductory\ntext, see Fagin et al. (1995). The formal language of epistemic logic\nextends that of propositional logic with epistemic operators \\({\\bf\nB}_a \\phi\\) and \\({\\bf K}_a \\phi\\), pronounced respectively as\n‘agent \\(a\\) believes that \\(\\phi\\),’ and ‘agent\n\\(a\\) knows that \\(\\phi.\\)’ When only one agent is under\ndiscussion, the subscripts are omitted. (Readers familiar\nwith modal logic will recognize these as\nanalogues of other “box” operators, such as necessity and\nobligation.)  Given this extension of the language, it is possible to\nexpress epistemic principles such as \\[{\\bf B}(p\\rightarrow\nq)\\rightarrow ({\\bf B}p\\rightarrow {\\bf B}q),\\] which says that your\nbeliefs are closed under modus ponens, or \\[ {\\bf B}p \\rightarrow {\\bf\nB}{\\bf B}p,\\] which says that if you believe \\(p\\), then you also\nbelieve that you believe it. Note that although the first principle\nhas natural analogues in AGM (closure) and non-monotonic logic (right\nweakening), the second principle cannot be straightforwardly expressed\nin either formalism. For example, in the AGM framework we might\nunderstand \\(p \\in {\\mathbf B}\\) as saying the same thing as \\( {\\bf\nB}p,\\) but it is not at all clear how to express \\( {\\bf BB} p\\). (See\nhowever Moore (1985) and the auto-epistemology section of the entry on\n non-monotonic logic.) Furthermore,\nepistemic logic has no difficulty expressing principles governing the\ninteraction of knowledge and belief. For example \\( {\\bf K}p\n\\rightarrow {\\bf B}p\\) expresses the thesis that everything that is\nknown is also believed. AGM and non-monotonic logic cannot easily say\nanything about these matters.  \nDifferent choices of axioms governing belief, knowledge and their\ninteraction give rise to different epistemic logics. A significant\namount of work in epistemic logic characterizes these systems and\nproves various surprising consequences of the axioms. Kripke\nmodels are the basis of the most widely used semantics for\nepistemic logic. (See Goldblatt (2006) for a discussion of whether or\nnot these models are properly attributed to Kripke.) A Kripke model\nequips the set of possible worlds \\(W\\) with a\nbinary indistinguishibility relation over \\(W\\). The basic\nidea is that two worlds \\(w,w^{\\prime}\\) stand in the relation \\(w R_a\nw'\\) if, were \\(w\\) the actual world, agent \\(a\\) could not rule out\nthat the actual world is \\(w'\\). Then, the model is connected with the\nformal language via the following definition: \\[ w \\vDash {\\bf\nB}_a\\phi \\text{ iff } w'\\vDash \\phi \\text{ for all } w' \\text{ such\nthat } w R_a w^\\prime. \\] In other words: it is true in \\(w\\) that\n\\(a\\) believes \\(\\phi\\) iff \\(\\phi\\) is true in all the worlds \\(w'\\)\nwhich \\(a \\) cannot distinguish from \\(w.\\) Equipped with this\nsemantics, it is possible to prove various elegant correspondences\nbetween epistemic principles and structural properties of the\nindistinguishibility relation. For example, the principle \\( {\\bf\nB}\\phi \\rightarrow {\\bf BB} \\phi \\) is validated whenever the\nindistinguishibility relation is transitive. The suspicious principle\n\\({\\bf B}\\phi \\rightarrow \\phi\\) is validated whenever the\nindistinguishibility relation is reflexive. A significant amount of\ntheoretical activity is devoted to the interplay between intuitive\nepistemic principles expressed in the language of modal logic and\nelegant structural conditions on indistinguishibility relations. For\nan excellent introduction to this subject see the entry\non epistemic logic, or the more\ngeneral entry on modal logic. For more on\nthe relationship between the logic of knowledge and belief, see the\nentry on philosophical aspects of\nmulti-modal logic. More recently, alternative topological\nsemantics for epistemic logic have seen significant theoretical\nactivity (for example: Bjorndahl and Özgün,\nforthcoming). \n As originally formulated in von Wright (1951)\nand Hintikka (1962), epistemic logic says nothing about how agents\nshould react to new information. Segerberg introduced dynamic doxastic\nlogic (1995, 1999) to bridge the gap between belief revision and\nepistemic logic. Moreover, dynamic epistemic logic was developed for\nthe same purpose, but with an emphasis on multiagent contexts (see\nPlaza 1989, Baltag et al. 1998, van Ditmarsch et al. 2007). This\nframework benefits from theoretical contributions from philosophers,\nlogicians, economists and computer scientists. A proper discussion of\nthese developments would take us too far afield. The interested reader\nshould consult the entry on\n dynamic epistemic logic.\n For Bayesian approaches to auto-epistemology see van Fraassen (1985,\n1995). For a ranking-theoretic approach, see chapter 9 in Spohn (2012)\nas well as Hild (1998) and Spohn (2017b). For a very different\napproach to combining qualitative notions from traditional\nepistemology with probabilistic notions see Moss (2013, 2018), who\ndefends the thesis that knowledge involves probabilistic contents. \nIt is commonly held that belief is not just an all-or-nothing matter,\nbut admits of degrees. Sophia may believe that Vienna is the capital\nof Austria to a greater degree than she believes that Vienna will be\nsunny tomorrow. If her degrees of belief have numerical–and not\njust ordinal–structure, she might be twice as sure of the former\nthan the latter. Philosophers sufficiently impressed by examples of\nthis sort orient their activity around the structure of “partial\nbelief” rather than the all-or-nothing variety. \nAlthough it is easy to generate plausible examples of partial beliefs,\nit is harder to say exactly what is meant by a degree of belief. An\nagent's degree of belief in \\(P\\) may reflect their level of\nconfidence in the truth of \\(P\\), their willingness to assent to \\(P\\)\nin conversation, or perhaps how much evidence is required to convince\nthem to abandon their belief in \\(P.\\) A venerable tradition,\nreceiving classical expression in Ramsey (1926) and de Finetti (1937),\nholds that degrees of belief are most directly reflected in which bets\nregarding \\(P\\) an agent is willing to accept. At least since Pascal\n(ca. 1658/2004), mainstream philosophical opinion has held that\ndegrees of belief are well-modeled by probabilities (see Hacking\n(1975) for a readable history). To this day, subjective, or\n‘epistemic’, probability remains one of the dominant\ninterpretations of the probability calculus.  \nA parallel tradition, though never as dominant, holds that degrees of\nbelief are neither so precise, nor as definitely comparable as\nsuggested by Pascal's probabilistic analysis. Keynes (1921) famously\nproposed that degrees of belief may enjoy only an ordinal\nstructure, which admits of qualitative, but not quantitative,\ncomparison. Keynes even suggests that the strength of some pairs of\npartial beliefs cannot be compared at all. \nCohen (1980) traces another minority tradition to Francis Bacon's\nNovum Organum (1620/2000). On the usual probability scale a\ndegree of belief of zero in some proposition implies maximal\nconviction in its negation. On the Baconian scale, a degree of belief\nof zero implies no conviction in either the proposition or its\nnegation. Thus, the usual scale runs from “disproof to\nproof” whereas the Baconian runs from “no evidence, or\nnon-proof to proof.” In the past few decades, Baconian\nprobability has received increasing attention, resulting in theories\napproaching the maturity and sophistication of those in the Pascalian\ntradition (Spohn, 2012, Huber, 2019). \nIn this section we introduce several frameworks for representing\npartial belief, starting with what is by far the most prominent\nframework: subjective probability theory.  \nSubjective probability theory, often going under the sobriquet\n‘Bayesianism,’ is by far the dominant paradigm for\nmodeling partial belief. The literature on the subject is by now very\nlarge. The summary provided here will, of necessity, be rather brief.\nFor an article-length introduction see the entry on\n Bayesian epistemology,\n Easwaran (2011a, 2011b) or Weisberg (2011). For a book-length\nintroduction see Earman (1992), Skyrms (2000), Hacking (2001), Howson\nand Urbach (2006) or Huber (2018). For an article-length introduction\nto Bayesian models of rational choice, see the entry on\n decision theory.\n For an approachable book-length introduction to the theory of\nrational choice see Resnik (1987). \nThe first two principles are the synchronic requirements of Bayesian\ntheory; the third principle concerns diachronic updating behavior.\nMost Bayesians would also agree to some version of the following\nprinciples, which link subjective probabilities with deliberation and\naction: \nWhat makes Bayesianism so formidable is that, in addition to providing\nan account of rational belief and its updating, it also provides an\naccount of rational action and deliberation. No other theory can claim\na developed, fine-grained account of all three of these aspects of\nbelief. In the following we will briefly spell out some of the\ntechnical details of the Bayesian picture. \nDegrees of belief in the [0,1] interval are introduced to quantify the\nstrength of belief attitudes. For our purposes, we will take\npropositions to be the objects of partial belief. It is also possible\nto assign degrees of belief to sentences in a formal language, but for\nthe most part nothing hinges on the approach we choose (see Weisberg,\n2011). Accordingly, let \\(\\mathbf{A}\\) be a field of propositions over\na set \\(W\\) of possibilities. A function Pr: \\(\\mathbf{A} \\rightarrow\n\\Re\\) from \\(\\mathbf{A}\\) into the set of real numbers, \\(\\Re\\), is a\n(finitely additive and non-conditional) probability measure\non \\(\\mathbf{A}\\) if and only if for all propositions \\(A, B\\) in\n\\(\\mathbf{A}\\): \nA triple \\(\\langle W, \\mathbf{A}, \\Pr\\rangle\\) satisfying these three\nprinciples is called a (finitely additive) probability space.\nFrom these principles it is possible to derive many illuminating\ntheorems. For example, the degree of belief assigned to the\ncontradictory proposition must equal zero. Furthermore, if \\(E\\)\nentails \\(F,\\) then \\(\\Pr(E)\\leq \\Pr(F)\\). Finally, for any\nproposition \\(E\\in\\mathbf{A}\\) we have that \\(0\\leq \\Pr(E)\\leq\n1.\\) \nSuppose that \\(\\mathbf{A}\\) is also closed under countable\nintersections (and is thus a \\(\\sigma\\)-field). Suppose Pr\nadditionally satisfies, for all propositions \\(A_{1}, \\ldots A_{n},\n\\ldots\\) in \\(\\mathbf{A}\\), \nThen, Pr is a \\(\\sigma\\)- or countably additive probability\nmeasure on \\(\\mathbf{A}\\) (Kolmogorov 1956, ch. 2, actually gives a\ndifferent but equivalent definition; see e.g. Huber 2007a, sct. 4.1).\nIn this case \\(\\langle W, \\mathbf{A}, \\Pr\\rangle\\) is a \\(\\sigma\\)- or\ncountably additive probability space. \nCountable additivity is not as innocent as it looks: it rules out the\npossibility that any agent is indifferent over a countably infinite\nset of mutully exclusive possibilities. De Finetti (1970, 1972)\nfamously argued that we ought to reject countable additivity since it\nis conceivable that God could pick out a natural number “at\nrandom” and with equal (zero) probability. For another example,\nsuppose you assign 50% credence to the proposition \\(\\neg B\\) that not\nall ravens that will ever be observed are black. Let \\(\\neg B_i\\) be\nthe proposition that the \\(i^{th}\\) observed raven is the first\nnon-black raven to appear. Then \\(\\neg B = \\cup_{i=1}^\\infty \\neg\nB_i.\\) Countable additivity entails that for all \\(\\epsilon>0\\) there\nis a finite \\(n\\) such that \\(p(\\cup_{i=1}^n \\neg B_i) = 1/2\n-\\epsilon.\\) So you must be nearly certain that if not all ravens are\nblack, the first non-black raven will appear among the first \\(n\\)\nravens. But is it really a requirement of rationality that\nyou must be opinionated about when the first non-black raven must\nappear? The only way to assign equal probability to all \\(\\neg B_i\\)\nis to violate countable additivity by setting \\(p(\\neg B_i)=0\\) for\nall \\(i\\). This solution has its own drawbacks. On all standard models\nof Bayesian update it will be impossible to become convinced that the\n\\(i^{th}\\) raven is indeed non-black, even if you are looking at a\nwhite one. For more on countable additivity, see Chapter 13 in Kelly\n(1996). \nA probability measure Pr on \\(\\mathbf{A}\\) is regular just in\ncase \\(\\Pr(A) \\gt 0\\) for every non-empty or consistent proposition\n\\(A\\) in \\(\\mathbf{A}\\). Let \\(\\mathbf{A}^{\\Pr}\\) be the set of all\npropositions \\(A\\) in \\(\\mathbf{A}\\) with \\(\\Pr(A) \\gt 0\\). The\nconditional probability measure \\(\\Pr(\\cdot\\mid -):\n\\mathbf{A}\\times \\mathbf{A}^{\\Pr} \\rightarrow \\Re\\) on \\(\\mathbf{A}\\)\n(based on the non-conditional probability measure Pr on\n\\(\\mathbf{A})\\) is defined for all pairs of propositions \\(A\\) in\n\\(\\mathbf{A}\\) and \\(B\\) in \\(\\mathbf{A}^{\\Pr}\\) by the ratio \n(Kolmogorov 1956, ch. 1, §4). Conditionalization by \\(B\\)\nrestricts all possibilities to those compatible with \\(B\\) and\nrenormalizes by the probability of \\(B\\) to ensure that unitarity\nholds. The domain of the second argument place of \\(\\Pr(\\cdot \\mid\n-)\\) is restricted to \\(\\mathbf{A}^{\\Pr}\\), since the ratio\n\\(\\Pr(A\\cap B)/\\Pr(B)\\) is not defined if \\(\\Pr(B) = 0\\). Note that\n\\(\\Pr(\\cdot\\mid B)\\) is a probability measure on \\(\\mathbf{A}\\), for\nevery proposition \\(B\\) in \\(\\mathbf{A}^{\\Pr}\\). Some authors take\nconditional probability measures \\(\\Pr(\\cdot, \\text{given } -):\n\\mathbf{A}\\times(\\mathbf{A}\\setminus \\{\\varnothing \\}) \\rightarrow\n\\Re\\) as primitive and define (non-conditional) probability measures\nin terms of them as \\(\\Pr(A) = \\Pr(A\\), given \\(W)\\) for all\npropositions \\(A\\) in \\(\\mathbf{A}\\) (see Hájek 2003).\nConditional probabilities are usually assumed to be\nPopper-Rényi measures (Popper 1955, Rényi 1955,\nRényi 1970, Stalnaker 1970, Spohn 1986). Spohn (2012, 202ff)\ncritizices Popper-Rényi measures for their lack of a complete\ndynamics, a feature already pointed out by Harper (1976), and for\ntheir lack of a reasonable notion of independence. Relative\nprobabilities (Heinemann 1997, Other Internet Resources) are claimed\nnot to suffer from these two shortcomings. \nWhat does it mean to say that Sophia’s subjective probability\nfor the proposition that tomorrow it will be sunny in Vienna equals\n.55? This is a difficult question. Let us first answer a different\none. How do we measure Sophia’s subjective probabilities? On one\ntraditional account, Sophia’s subjective probability for \\(A\\)\nis measured by her betting ratio for \\(A\\), i.e., the highest\nprice she is willing to pay for a bet that returns $1 if \\(A\\), and $0\notherwise. On a slightly different account Sophia’s subjective\nprobability for \\(A\\) is measured by her fair betting ratio\nfor \\(A\\), i.e., that number \\(r = b/(a + b)\\) such that she considers\nthe following bet to be fair: $\\(a\\) if \\(A\\), and $\\(-b\\) otherwise\n\\((a, b \\ge 0\\) with inequality for at least one). \nIt need not be irrational for Sophia to be willing to bet you $5.5 to\n$4.5 that tomorrow it will be sunny in Vienna, but not be willing to\nbet you $550 to $450 that this proposition is true. She may even\nrefuse a bet of $200 to $999. That is because there are other factors\naffecting her betting quotients than her degrees of belief. Sophia may\nbe risk-averse, e.g. if she cannot risk blowing a $200 dollar hole in\nher monthly budget. Others may be risk-prone. For example, gamblers in\nthe casino are risk prone: they pay more for playing roulette than the\nfair monetary value according to reasonable subjective probabilities.\nThat may be perfectly rational if the thrill of gambling is itself a\ncompensation. Note that it does not help to say that Sophia’s\nfair betting ratio for \\(A\\) is that number \\(r = b/(a + b)\\) such\nthat she considers the following bet to be fair: $\\(1 - r = a/(a +\nb)\\) if \\(A\\), and \\(\\$ -r = -b/(a + b)\\) otherwise \\((a, b \\ge 0\\)\nwith inequality for at least one). Just as stakes of $200 may be too\nhigh for the measurement to work, stakes of $1 may be too low. \nAnother complication arises when the proposition itself is a matter of\npersonal importance to the agent. Suppose Sophia would be very unhappy\nif the Freedom Party wins a majority in the next election, but she\nthinks it is very unlikely. Nevertheless, she may be willing to pay\n$20 for a bet that pays $100 if they do, and $0 otherwise. Imagine\nthat she is buying a kind of insurance policy against bitter\ndisappointment. In this case, her betting ratio does not obviously\nreflect her degree of belief that the FPÖ will win. \nRamsey (1926) avoids the first difficulty by pricing bets in utility\ninstead of money. He avoids the second difficulty by presupposing the\nexistence of at least one “ethically neutral” proposition\n(a proposition whose truth or falsity is a matter of indifference)\nwhich the agent takes to be just as likely to be true as she takes it\nto be false. See Section 3.5 of the entry on\n interpretations of probability.\n Nevertheless, the bearing of the preceding examples is that fair\nbetting ratios and subjective probabilities can easily come apart.\nSubjective probabilities are measured by, but not identical to, (fair)\nbetting ratios. The latter are operationally defined and observable.\nThe former are unobservable, theoretical entities that, following\nEriksson & Hájek (2007), we take as primitive. \nThe theory of subjective probability does not accurately describe the\nbehavior of actual human beings (Kahneman et. al., 1982). It is a\nnormative theory intended to tell us how we  ought  to govern\nour epistemic lives.  Probabilism  is the thesis that a\nperson’s degrees of belief ought to satisfy the axioms\nof probability. But why should they?  \nThe traditional answer is that an agent that violates the axioms of\nprobability opens herself up, in some sense, to a system of bets that\nguarantee a sure loss. Answers of this flavor are called Dutch\nBook arguments. The pragmatic version of the argument\nposits a tight connection between degrees of belief and betting\nbehavior. The argument concludes by proving a theorem to the effect\nthat an agent would enter into a system of bets guaranteeing a sure\nloss iff her degrees of belief violate the probability calculus. But,\nas we have seen, there are reasons to doubt that the connection\nbetween degrees of belief and betting behavior is really so tight as\nrequired by the pragmatic Dutch book argument. That renders the\nargument less convincing. The depragmatized version of the\nargument posits a connection between degress of belief and\ndispositions to consider systems of bets fair, without necessarily\nentering into them (Armendt 1993 Christensen 1996, Ramsey 1926, Skyrms\n1984). It concludes by proving an essentially identical theorem to the\neffect that an agent would consider fair a system of bets guaranteeing\na sure loss iff her degrees of belief violate the probability\ncalculus. The depragmatized Dutch Book Argument is a more promising\njustification for probabilism. See, however, Hájek (2005;\n2008). For a much more extensive discussion see the entry on\n Dutch book arguments.\n  \nSome epistemologists find Dutch book arguments to be unconvincing\neither because they disavow any suitable connection between degrees of\nbelief and betting quotients, or they deny that any facts about\nsomething so pragmatic as betting could have normative epistemic\nforce. Joyce (1998) attempts to vindicate probabilism by considering\nthe accuracy of degrees of belief. The basic idea here is\nthat a degree of belief function is defective if there exists an\nalternative degree of belief function that is at least as accurate in\neach, and strictly more accurate in some, possible world. The accuracy\nof a degree of belief \\(b(A)\\) in a proposition \\(A\\) in a world \\(w\\)\nis identified with the distance between \\(b(A)\\) and the truth value\nof \\(A\\) in \\(w\\), where 1 represents truth and 0 represents falsity.\nFor instance, a degree of belief up to 1 in a true proposition is more\naccurate, the higher it is — and perfectly accurate if it equals\n1. The overall accuracy of a degree of belief function \\(b\\) in a\nworld \\(w\\) is then determined by the accuracy of the individual\ndegrees of belief \\(b(A)\\). Joyce is able to prove that, given some\nconditions on how to measure distance or inaccuracy, a degree of\nbelief function obeys the probability calculus if and only if there\nexists no alternative degree of belief function that is at least as\naccurate in each, and strictly more accurate in some, possible world\n(the only-if-part is not explicitly mentioned in Joyce 1998, but\npresent in Joyce 2009). Therefore, degrees of belief should obey the\nprobability calculus. \nBronfman (2006, Other Internet Resources) observes that Joyce’s\nconditions on measures of inaccuracy do not determine a single\nmeasure, but rather a whole family of inaccuracy measures. All of\nJoyce’s measures agree that an agent whose degree of belief\nfunction violates the probability axioms should adopt a probabilistic\ndegree of belief function which is at least as accurate in each, and\nmore accurate in some, possible world. However, these measures may\ndiffer in their recommendation as to which particular probability\nmeasure the agent should adopt. In fact, for each possible world,\nfollowing the recommendation of one measure will leave the agent less\naccurate according to some other measure. Why, then, Bronfman objects,\nshould the ideal doxastic agent move from her non-probabilistic degree\nof belief function to a probability measure in the first place? Other\nobjections are articulated in Maher (2002) and, more recently, in\nEaswaran and Fitelson (2012). These are addressed by Joyce (in 2009\nand in 2013 (Other Internet Resources)) and Pettigrew (2013,\n2016). Leitgeb and Pettigrew (2010a; 2010b) present conditions that\nnarrow down the set of measures of inaccuracy to the so-called\nquadratic scoring rules. This enables them to escape Bronfman’s\nobjection. For a detailed treatment, see the entry on\n epistemic utility arguments for probabilism.\n  \nFor alternative justifications of probabilism see Cox (1946) and the\nrepresentation theorem of measurement theory (Krantz et al., 1971).\nFor criticism of the latter see Meacham & Weisberg (2011). For a\nrecent justification of probabilism from partition invariance see\nLeitgeb (forthcoming). \nProbabilism imposes synchronic conditions on degrees of belief. But\nhow should subjective probabilities be updated when new information is\nreceived? Update rules are diachronic conditions that tell us\nhow to revise our subjective probabilities when we receive new\ninformation. There are two standard update rules. Strict\nconditionalization applies when the new information receives the\nmaximal degree of belief. Jeffrey conditionalization allows for the\nsituation in which no proposition is upgraded to full certainty when\nnew information is acquired. In the first case, probabilism is\nextended by \nStrict Conditionalization\n\nIf \\(\\Pr(\\cdot)\\) is your subjective probability at time \\(t\\), and if\nbetween \\(t\\) and \\(t^\\prime\\) you become certain of \\(A \\in\n\\mathbf{A}^{\\Pr}\\) and no logically stronger proposition, then your\nsubjective probability at time \\(t^\\prime\\) should be \\(\\Pr(\\cdot \\mid\nA)\\).  \nStrict conditionalization says that, so long as \\(A\\) has positive\nprior probability, the agent’s new subjective probability for a\nproposition \\(B\\) after becoming certain of \\(A\\) should equal her old\nsubjective probability for \\(B\\) conditional on \\(A\\). It is by far\nthe most standard model of partial belief update. \nThe discerning reader may object that there is a suppressed\nceteris paribus clause in our statement of strict\nconditionalization. On the intended interpretation, the only\nexternal change to your subjective belief state between \\(t\\)\nand \\(t^\\prime\\) is that you become certain of \\(A\\). Moreover, you do\nnot forget anything, fall into doubts about prior beliefs, or acquire\nany new concepts. Finally, there is no time \\(t^{\\prime\\prime}\\)\nbetween \\(t\\) and \\(t^{\\prime}\\) by which your degree of belief in\n\\(A\\) is promoted to 1, but before your other subjective probabilities\nhave been able to “catch up”. If there were, you would be\nprobabilistically incoherent between \\(t^{\\prime\\prime}\\) and\n\\(t^{\\prime}.\\) On the intended interpretation, although you acquire\nthe information between \\(t\\) and \\(t^{\\prime}\\), your subjective\nprobabilities remain unchanged until \\(t^{\\prime}\\), at which point\nthe new information is assimilated holus-bolus. These sorts of\nconsiderations apply equally to the other forms of conditioning we\ndiscuss in this article, including those in Section 3.4. For a good\ndiscussion of the difficulties in interpreting conditionalization, see\nSpohn (2012, p. 186–8). \nWhat if new information does not render any proposition certain, but\nmerely changes the subjective probability of some propositions?\nJeffrey (1983a) gives the most widely accepted answer to this\nquestion. Roughly, Jeffrey conditionalization says that the ideal\ndoxastic agent should keep fixed her “inferential\nbeliefs,” that is, the probabilities of all hypotheses\nconditional on any evidential proposition. \nJeffrey Conditionalization\n\nSuppose that \\(\\Pr(\\cdot)\\) is your subjective probability at time\n\\(t\\) and that between \\(t\\) and \\(t^\\prime\\) your subjective\nprobabilities in the partition \\(\\{A_i : 1\\leq i \\leq n \\}\\subseteq\n\\mathbf{A}^{\\Pr}\\) (and no finer partition) change to \\(p_{i} \\in\n[0,1]\\) with \\(\\sum_{i} p_{i} = 1\\). Then, your subjective probability\nat time \\(t^\\prime\\) should be \\(\\Pr^\\prime(\\cdot) = \\sum_{i}\n\\Pr(\\cdot \\mid A_{i}) p_{i}\\).  \nJeffrey conditionalization says that the agent’s new subjective\nprobability for \\(B\\), after her subjective probabilities for the\nelements of the partition have changed to \\(p_{i}\\), should equal the\nweighted sum of her old subjective probabilities for \\(B\\) conditional\non the \\(A_{i}\\), where the weights are the new subjective\nprobabilities \\(p_{i}\\) for the elements of the partition. \nWhy should we update our subjective probabilities according to strict\nor Jeffrey conditionalization? Dutch-book style arguments for strict\nconditionalization are given in Teller (1973) and Lewis (199) and\nextended to Jeffrey conditionalization in Armendt (1980). For more,\nsee Skyrms (1987, 2006). Leitgeb and Pettigrew (2010b) present an\naccuracy argument for strict conditionalization (see also Greaves and\nWallace, 2006) as well as an argument for an alternative to Jeffrey\nconditionalization. For an overview, see the entry on\n epistemic utility arguments for probabilism. \nOther philosophers have provided arguments against strict (and\nJeffrey) conditionalization: van Fraassen (1989) holds that\nrationality does not require the adoption of a particular update rule\n(but see Hájek, 1998 and Kvanvig, 1994). Arntzenius (2003)\nuses, among others, the “shifting” nature of self-locating\nbeliefs to argue against strict conditionalization, as well as against\nvan Fraassen’s reflection principle (see van Fraassen\n1995; for an illuminating discussion of the reflection principle and\nDutch Book arguments see Briggs 2009a). The second feature used by\nArntzenius (2003), called “spreading”, is not special to\nself-locating beliefs. Weisberg (2009) argues that Jeffrey\nconditionalization cannot handle a phenomenon he terms perceptual\nundermining. See Huber (2014) for a defense of Jeffrey\nconditionalization. \nFor our purposes it is important to point out that conditional\nprobability is always a lower bound for the probability of the\nmaterial conditional. In other words, \\[p(H|E)\\leq p(E\\rightarrow\nH),\\] whenever \\(p(E)>0\\). We can see this as a quantitative version\nof the qualitative principle of Conditionalization we discussed\nin Section 2.1.1. However confident a\nBayesian agent becomes in \\(H\\) after updating on \\(E,\\) she must have\nbeen at least as confident that \\(H\\) is a material consequence of\n\\(E\\). Popper and Miller (1983) took this observation to be\n“completely devastating to the inductive interpration of the\ncalculus of probability.” For the history of the Popper-Miller\ndebate see Chapter 4 in Earman (1992). A similar property can be\ndemonstrated for Jeffrey conditioning (Genin 2017, Other Internet\nResources). \nSubjective probability theory models ignorance with respect to a\nproposition \\(A\\) by assigning probability of .5 to \\(A\\) and its\ncomplement \\(\\neg A\\). More generally, an agent with subjective\nprobability Pr is said to be ignorant with respect to the\npartition \\(\\{A_{1},\\ldots,A_{n}\\}\\) if and only if \\(\\Pr(A_{i}) =\n1/n\\). The Principle of Indifference requires a doxastic\nagent to distribute her subjective probabilities in this fashion\nwhenever, roughly, the agent lacks evidence of the relevant kind.\nLeitgeb and Pettigrew (2010b) give an accuracy argument for the\nprinciple of indifference. However, the principle leads to\ncontradictory results if the partition in question is not held fixed.\nFor a simple example, suppose that Sophia is ignorant about the color\nof a certain marble. Then, she must be nearly certain that it is not\nBlue. In this case, ignorance about one thing entails that she is very\nopinionated about another. But presumably she is also ignorant about\nwhether or not the color is Blue. For more on this, see the discussion\nof Bertrand’s paradox in Kneale (1949) and Section 3.1\nof the entry on\n interpretations of probability.\n A more cautious version of the principle of indifference, also\napplicable if the partition contains a countable infinity of elements,\nis the principle of maximum entropy. It requires the agent to\nadopt one of those probability measures Pr as her degree of belief\nfunction over (the \\(\\sigma\\)-field generated by) the countable\npartition \\(\\{A_{i}\\}\\) that maximize the quantity \\(-\\sum_{i}\n\\Pr(A_{i}) \\log \\Pr(A_{i})\\). The latter is known as the\nentropy of Pr with respect to the partition \\(\\{A_{i}\\}\\).\nSee Paris (1994). \nSuppose Sophia does not know much about wine. By the principle of\nindifference, her degree of belief that an Austrian Schilcher is a\nwhite wine and her degree of belief that it is a red wine should both\nbe .5. Contrast this with the following case. Sophia is certain that a\nparticular coin is fair, i.e. that the objective chance of the coin\nlanding heads and the objective chance of the coin landing tails are\nboth exactly .5. The principal principle (Lewis, 1980)\nrequires roughly that, conditional on the objective chances,\none’s subjective probability should equal the objective\nprobability (see Briggs, 2009b). By the principal principle, her\ndegree of belief that the coin will land heads on the next spin should\nalso be .5. Although Sophia’s subjective probabilities are alike\nin these two scenarios, there is an important difference. In the first\ncase a subjective probability of .5 represents complete ignorance. In\nthe second case it represents certainty about objective chances. \nExamples like these suggest that subjective probability theory is not\nan adequate account of partial belief because it cannot distinguish\nbetween total ignorance and knowledge, or at least certainty, about\nchances. We discuss potential responses to this objection in Section\n3.2 \nOne of the signal advantages of the Bayesian model of partial belief\nis that it is ready-made to plug into a prominent model of practical\ndeliberation. Decision theory, or rational choice theory, is too large\nand sprawling a subject to be effectively covered here, although it\nwill be presented in cursory outline. For an excellent introduction,\nsee Thoma (2019) and the entry on\n decision theory.\n For our purposes, it is enough to note that a well-developed theory\nexists and that no comparable theory exists for alternative models of\nbelief. However, recent work such as Lin (2013) and Spohn (2017a,\n2019) may remedy that inadequacy in the case of qualitative\nbelief. \nSuppose you would like to make a six egg omelet. You’ve broken 5\nfresh eggs into a mixing bowl. Rooting around your fridge, you find a\nloose egg of uncertain provenance. If you are feeling lucky you can\nbreak the suspect egg directly into the mixing bowl; if you are wary\nof the egg, you might break it into a saucer first and incur more\ndishwashing. \nThere are four essential ingredients to this sort of\ndecision-theoretic situation. There are  outcomes, over which\nwe have defined  utilities measuring the desirability of the\noutcome. In the case of the omelet the outcomes are a ruined omelet or\na 5–6 egg omelet, with or without extra washing. There are \nstates–usually unknown to and out of the control of the\nactor–which influence the outcome of the decision. In our case\nthe states are exhausted by the possible states of the suspect egg:\neither good or rotten. Finally, there are acts which are\nunder the control of the decision maker. In our case the acts include\nbreaking the egg into the bowl or the saucer. Of course, there are\nother conceivable acts: you might throw the suspect egg away and make\ndo with a 5-egg omelet; you might even flip a coin to decide what to\ndo. We omit these for the sake of simplicity. \nTo fit this into the framework of partial belief we assume that the\nset of acts \\(A_1, A_2, \\ldots, A_n\\) partition \\(W\\). We also assume\nthe set of states \\(S_1, S_2, \\ldots, S_m\\) partition \\(W\\). We assume\nthat the subjective probability function assigns a probability to\nevery state given every act. We assume that acts and states are\nlogically independent, so that no state rules out the performance of\nany act. Finally, we assume that given a state of the world \\(S_j\\)\nand an act \\(A_i\\) there is exactly one outcome \\(O_{ij}\\) which is\nassigned a utility \\(U(O_{ij})\\). The ultimate counsel of rational\nchoice theory is that agents ought to perform only those acts that\nmaximize  expected utility. The expected utility of an act is\ndefined as: \nwhere \\(\\Pr_{A_i}(S_j)\\) is roughly how likely the agent considers\n\\(S_j\\) given that she has performed act \\(A_i\\). Difficulties about\nhow this quantity should be defined give rise to the schism between\nevidential and causal decision theory (see Section 3.3 in Thoma,\n2019). However, in many situations, including the dilemma of the\nomelet, the act chosen does not affect the probabilities with which\nstates obtain. This is called “act-state independence” in\nthe jargon of rational choice theory. In cases of act-state\nindependence there is broad consensus that \\(\\Pr_{A_i}(S_j)\\) should\nbe equal to the unconditional degree of belief \\(\\Pr(S_j)\\). \nCentral to the literature on decision theory are a number of\nrepresentation theorems showing that every agent with qualitative\npreferences satisfying a set of rationality postulates can be\nrepresented as an expected utility maximizer (von Neumann and\nMorgenstern, 1944 and Savage, 1972). These axioms are controversial,\nand are subject to intuitive counterexamples. Allais (1953) and\nEllsberg (1961) give examples in which seemingly rational agents\nviolate the rationality postulates and therefore cannot, even in\nprinciple, be represented as expected utility maximizers. For more on\nthe challenge of Ellsberg and Allais, see the entry on\n descriptive decision theory\n as well as Buchak (2013). \nConsider the following modification of the two examples from section\n3.1.5. In the first case, Sophia is presented with a weathered and\nirregularly shaped coin–it was just discovered in an\narcheological dig of an ancient city. By probabilism, Sophia must have\na precise real-valued credence that it will land heads on the\nnext spin. By the principle of indifference, her credence in\nHeads must be precisely .5. In the second case, she is\npresented with a Euro coin that she is certain is fair–this has\nbeen confirmed by extensive experimentation. Probabilism and the\nprincipal principle require her credence in Heads to be\nprecisely .5 in this case as well. As we have already seen, there is\nsomething unsatisfactory about treating ignorance (in the first case)\nin the same way as certainty about chances (in the second case). \nThere are several different ways of spelling out what is wrong with\nthis situation. In the case of the ancient coin, Sophia has a precise\ncredence based only on some vague and imprecise information. In the\ncase of the Euro, she has a precise attitude based on precise\ninformation. The basic intuition here is that it is bizzarre to\nrequire, as a matter of rationality, that Sophia have a\nprecise attitude when her evidence is so imprecise. Sturgeon (2008)\nsuggests that evidence and attitude must “match in\ncharacter,” i.e. sharp evidence warrants a sharp attitude and\nimprecise evidence warrants only an imprecise attitude. According to\nthe “character matching” thesis, rationality requires that\nSophia have an imprecise attitude toward the ancient\ncoin. \nJoyce (2005) articulates the difficulty in a different way. He\nsuggests that there is an important difference between the\nweight and the balance of the evidence. In the case\nof the ancient coin, the evidence is balanced (by symmetry) but so\nscanty that it has no weight. In the case of the Euro, the evidence is\nweighty (since there is so much of it) and balanced because it favors\nboth heads and tails equally. Joyce criticizes precise probabilism for\nits inability to represent the distinction between weight and balance\nof evidence. Skyrms (2011) and Leitgeb (2014) argue that this\ndistinction is represented: beliefs that reflect weighty\nevidence are more resilient (Skyrms) or stable\n(Leitgeb) under update. A few trials with the ancient coin might\nchange Sophia’ credences dramatically, but not so for the Euro.\n \nThe question takes on a different character when we consider its\nconsequence for decision making. According to the standard theory, a\nutility-maximizing Bayesian must find at least one side of any bet\nattractive. It is straightforward to check that if a bet that costs\n\\($\\ell\\) and pays out \\($w\\) has negative expected utility, then the\nother side of the bet, that costs \\($w\\) and pays out \\($\\ell\\), has\npositive expected utility. Since her credences in both cases are the\nsame, whatever bet Sophia accepts on the next spin of the Euro, she\nmust also accept on the ancient coin. So even though her beliefs about\nthe Euro are more stable, this difference is not reflected in her\nbetting behavior. Yet intuitively, it seems reasonable to refuse to\nbet on the ancient coin at all. A bet on the Euro is risky:\nthe outcome of the spin is uncertain, but there is no uncertainty\nabout the chance with which each outcome occurs. A bet on the ancient\ncoin is ambiguous: the outcome of the spin is uncertain and\nthere is significant uncertainty about the chance with which each\noutcome occurs. Many seemingly rational people take account of this\ndistinction in their decision making (see the discussion of \nEllsberg decisions in the entry on \n imprecise probabilities).\n However, this distinction cannot be represented in the standard\ntheory (see Buchak (2013)). \nImprecise probabilists (van Fraasen, 1990, Levi 1974) respond to these\ndifficulties by denying that an agent’s credences are adequately\ndescribed by a single probability function. Instead, they propose that\na belief state is better represented by a set of probability\nfunctions. This set represents a kind of “credal\ncommittee,” where each member represents a way of precisifying\nthe probability of every proposition. When new information arrives,\neach member updates by conditioning in the usual way. Levi requires\nthat the credal set be closed under convex combinations. In other\nwords, if \\(p,q\\) are members of your credal set then \\(\\lambda p +\n(1-\\lambda) q\\) must also be a member of your credal set, for all\n\\(\\lambda \\in [0,1]\\). According to Levi, convex combinations of\n\\(p,q\\) are “potential resolutions of the conflict”\nbetween them and “one should not preclude potential resolutions\nwhen suspending judgement between rival systems” (Levi, 1980).\nHowever, resolving conflicts in this way leads to some unintuitive\nconsequence. For example, if \\(p,q\\) agree that two events are\nprobabilistically independent, the same is not true of their convex\ncombinations. Furthermore, if all members of the credal committee\nagree that some coin is biased (because it is bent) but not on the\ndirection of the bias, it is unintuitive to require that some\ncommittee member believe that the coin is fair. For more on how to\naggregate the opinions of agents with probabilistic credences, see\nDietrich and List (2016) and section 10.4 in Pettigrew (2019). \nWhether or not we accept Levi’s convexity requirement, sets of\nprobability functions give us the resources to distinguish ignorance\nfrom certainty about chances. If an agent is certain of an objective\nchance, every element of her credal committee will assign the same\nprobability. However, if she is ignorant with respect to some\nproposition, her credal set will admit values in some interval \\([a,\nb] \\subseteq\\) [0,1]. In her total ignorance about the ancient coin,\nSophia might admit any value in the interval [0,1] for the proposition\nthat it will land heads on the next spin. But since she knows that the\nEuro is fair, every member of her credal committee assigns .5 to the\ncorresponding proposition. If we similarly redefine expected utility,\nthen a bet on the Euro that costs $1 and pays out $2 if it lands heads\nhas positive expected utility. But a similar bet on the ancient coin\nhas expected utility ranging from -$1 to $2. This distinction allows\nfor different attitudes towards the two bets. \nFor a detailed exposition of imprecise probability theory, see Levi\n(1980), van Fraassen (1990), Walley (1991) and Kyburg and Teng (2001).\nFor excellent introductions see Mahtani (2019), as well as the entry\non\n imprecise probability\n and its\n technical\n and\n historical\n appendices. See Weichselberger (2000) for an approach that avoids\nsets of probability functions by assigning credal intervals\n\\([a,b]\\subseteq[0,1]\\) directly to propositions. For a sophisticated\nrecent view on which the contents of beliefs are sets of probability\nfunctions, see Moss (2018).  \nDempster-Shafer (DS) belief functions (Dempster 1968, Shafer 1976) can\nalso be understood as an attempt to formally distinguish risk from\nambiguity. Like probability functions, DS belief functions are\nreal-valued functions \\( \\Bel: \\mathbf{A} \\rightarrow \\Re\\) satisfying\nPositivity and Unitarity. But whereas probability functions are\nadditive, DS belief functions are only Super-additive, i.e.,\nfor all propositions \\(A, B\\) in \\(\\mathbf{A}\\): \nAlthough \\(0\\leq\\Bel(A)\\leq 1\\) for all \\(A\\in\\mathbf{A}\\), the\nagent’s degree of belief in \\(A\\) and her degree of belief in\n\\(\\neg A\\) need not sum to 1. \nAccording to one interpretation (Haenni & Lehmann 2003), the\nnumber \\(\\Bel(A)\\) represents the strength with which \\(A\\) is\nsupported by the agent’s knowledge or belief base. It may well\nbe that this base supports neither \\(A\\) nor its complement \\(\\neg\nA\\). Since Sophia knows little about the ancient coin, her belief base\nwill support neither the proposition \\(H_a\\), that it will land heads,\nnor the proposition \\(T_a\\), that it will land tails. However, Sophia\nmay well be certain that it will not land on its edge. Hence\nSophia’s DS belief function \\(\\Bel\\) will be such that\n\\(\\Bel(H_a) = \\Bel(T_a) = 0\\) while \\(\\Bel(H_a \\cup T_a) = 1\\). On the\nother hand, Sophia is certain that the Euro coin is fair. Therefore,\nif \\(H_e,T_e\\) are the propositions that the Euro will land heads and\ntails respectively, her \\(\\Bel\\) will be such that \\(\\Bel(H_e) =\n\\Bel(T_e) = .5\\) and \\(\\Bel(H_e\\cup T_e) = 1\\). In this way, the\ntheory of DS belief functions can distinguish between uncertainty and\nignorance about chances. Indeed, \ncan be seen as a measure of the agent’s ignorance with respect\nto the countable partition \\(\\{A_{i}\\}\\). On this definition, Sophia\nis maximally ignorant about the outcome of the next spin of the\nancient coin and minimally ignorant about the Euro.  \nEvery proposition \\(A\\) can be seen as dividing the agent’s\nknowledge base into three mutually exclusive and jointly exhaustive\nparts: a part that speaks in favor of \\(A\\), a part that speaks\nagainst \\(A\\) (i.e., in favor of \\(\\neg A \\)), and a part that speaks\nneither in favor of nor against \\(A\\). \\(\\Bel(A)\\) quantifies the part\nthat supports \\(A, \\Bel( \\neg A)\\) quantifies the part that supports\n\\(\\neg A\\), and I\\((\\{A, \\neg A\\}) = 1 - \\Bel(A) - \\Bel(\\neg A)\\)\nquantifies the part that supports neither \\(A\\) nor \\(\\neg A\\). \nWe can understand the relation to subjective probability theory in the\nfollowing way. Subjective probabilities require the ideal doxastic\nagent to divide her knowledge base into two mutually exclusive and\njointly exhaustive parts: one that speaks in favor of \\(A\\), and one\nthat speaks against \\(A\\). That is, the neutral part has to be\ndistributed among the positive and negative parts. Subjective\nprobabilities can thus be seen as DS belief functions without\nignorance. (See Pryor (2007, Other Internet Resources) for a\nmodel of doxastic states that includes probability theory and\nDempster-Shafer theory as special cases.) \nA DS belief function induces a plausibility function \\(\\rP:\n\\mathbf{A} \\rightarrow \\Re\\), by setting  \nfor all \\(A\\) in \\(\\mathbf{A}\\). Degrees of plausibility quantify that\npart of the agent’s knowledge or belief base which is compatible\nwith \\(A\\), i.e., the part that supports \\(A\\) and the part that\nsupports neither \\(A\\) nor \\( \\neg A\\). Dempster and Shafer call the\nplausibility of \\(A\\) its upper probability. Indeed, one can\ninterpret the interval \\( [\\Bel(A), P(A)]\\) as the interval of\nprobabilities that the agent assigns to the proposition \\(A\\). In our\nrunning example, Sophia assigns the interval \\([0,1]\\) to the\nproposition that the ancient coin will land heads and \\([.5,.5]\\) to\nthe proposition that the Euro will land heads.  \nDempster-Shafer theory is more general than the theory of subjective\nprobability in the sense that the latter requires additivity, whereas\nthe former requires only super-additivity. However, most authors agree\nthat DS theory is not as general as imprecise probability theory. The\nreason is that DS belief functions can be represented as convex sets\nof probabilities. More precisely, for every DS belief function\n\\(\\Bel\\) there is a convex set of probabilities \\(\\mathcal{P}\\) such\nthat \\(\\Bel(A)=\\min \\{ p(A) : p \\in \\mathcal{P}\\}\\) and \\(P(A) = \\max\n\\{ p(A) : p\\in\\mathcal{P}\\} \\) (Walley 1991). As not every convex set\nof probabilities can be represented as a DS belief function, sets of\nprobabilities arguably provide the most general framework we have come\nacross so far. \nHow are DS belief functions reflected in decision making? One\ninterpretation of DS theory, called the tranferable belief\nmodel (Smets and Kennes, 1994), distinguishes between two mental\nlevels: the credal level, where one entertains and quantifies various\nbeliefs, and the pignistic level, where one uses those beliefs for\ndecision making. Its twofold thesis is that (fair) betting ratios\nshould indeed obey the probability calculus, but that degrees of\nbelief, being different from (fair) betting ratios, need not. It\nsuffices that they satisfy the weaker DS principles. The idea is that\nwhenever one is forced to bet on the pignistic level, the degrees of\nbelief from the credal level are used to calculate (fair) betting\nratios that satisfy the probability axioms. These in turn are then\nused to calculate the agent’s expected utility for various\nacts. \nOne of the chief novelties of Dempster-Shafer theory is its modeling\nof belief update, which we do not cover here. For a good introduction\nto this and other aspects of Dempster-Shafer theory, see Section 5.4\nin Kyburg and Teng (2001). For a Dutch Book-style argument for\nDempster-Shafer theory, see Paris (2001). For an accuracy-style\nargument see Williams (2012). \nLet us summarize the accounts we have dealt with so far. Subjective\nprobability theory requires degrees of belief to be additive. For any\ndisjoint \\(A,B\\) in \\(\\mathbf{A}\\), a subjective probability function\nPr: \\(\\mathbf{A} \\rightarrow \\Re\\) must satisfy: \nDempster-Shafer theory only requires degrees of belief to be\nsuper-additive. For any disjoint \\(A,B\\) in \\(\\mathbf{A}\\), a DS\nbelief function Bel: \\(\\mathbf{A} \\rightarrow \\Re\\) must satisfy: \nBut there are many other ways that we could have weakened the\nadditivity axiom. Possibility theory (Dubois and Prade, 1988) requires\ndegrees of belief to be maxitive and hence sub-additive. For\nany \\(A,B \\in \\mathbf{A}\\), a possibility measure \\(\\Pi : \\mathbf{A}\n\\rightarrow \\Re\\) must satisfy: \nwhich entails that \nThe idea is that a proposition is at least as possible as each of the\npossibilities it comprises, and no more possible than the “most\npossible” possibility. The dual notion of a necessity\nmeasure \\(\\Nu : \\mathbf{A} \\rightarrow \\Re\\) is defined for all\n\\(A\\) in \\(\\mathbf{A}\\) by \nwhich implies that \nAlthough the agent’s doxastic state in possibility theory is\ncompletely specified by either \\(\\Pi\\) or \\(\\Nu\\), the agent’s\nepistemic attitude towards a particular proposition \\(A\\) is only\njointly specified by \\(\\Pi(A)\\) and \\(\\Nu(A)\\). The reason is that, in\ncontrast to probability theory, \\(\\Pi(W \\setminus A)\\) is not\ndetermined by \\(\\Pi(A)\\). \nPossiblity theory is inspired by fuzzy set theory (Zadeh, 1978). The\nlatter is designed to accomodate linguistic phenomena of\nvagueness (see Égré & Barberousse 2014, Raffman 2014,\nWilliamson 1994, Field 2016, as well as the entry on\n vagueness).\n Vagueness arises for predicates like “tall” for which\nthere are extreme, paradigmatic and borderline cases. We might\nrepresent this phenomenon formally by a membership function \\(\\mu_{T}:\nH \\rightarrow [0,1]\\), where \\(\\mu_{T}(h)\\) is the degree to which\nperson of height \\(h \\in H\\) belongs to the set of tall people \\(T\\).\nThen, \\(\\mu^{-1}(1)\\) is the set of all tall heights; \\(\\mu^{-1}(0)\\)\nis the set of all short heights; and \\(\\cup_{r\\in(0,1)}\\mu^{-1}(r)\\)\nis the set of all borderline heights. Since many distinct heights are\ntall, it is clear why such membership functions should not satisfy\nAdditivity. \nFuzzy set theory interprets \\(\\mu_{T}(178cm)\\) as the degree to which\nthe vague statement “someone measuring 178cm is tall” is\ntrue. Degrees of truth belong to the philosophy of language.\nThey do not (yet) have anything to do with degrees of belief, which\nbelong to epistemology. The epistemological thesis of possibility\ntheory is that your subjective degrees of possibility should reflect\nsemantic facts about degree of membership. Suppose you learn that\nSophia is tall. Then, your degree of possibility for the statement\n“Sophia is 178cm” should equal \\(\\mu_{T}(178\\text{cm})\\).\nThat may handle borderline heights elegantly, but it has somewhat\nunintuitive consequences at the extremes. Since, presumably,\n\\(\\mu_T(190\\text{cm})=\\mu_T(210\\text{cm})=1\\), you must find it\nmaximally possible that Sophia is 190 and 210cm tall.  \nIn “chancy” situations, possibility theory can only make\nvery course-grained distinctions. Since Sophia is certain that the\nEuro coin will not land on its edge \\(\\Pi(H\\cup T)=\\Pi(W)\\), she must\nhave \\(\\Pi(H)=1\\) or \\(\\Pi(T)=1.\\) The most natural way to model her\nattitude is to set \\(\\Pi(H)=\\Pi(T)=1\\). There is no way of expressing\na partial attitude towards both heads and tails, since these are both\nmaximally possible. \nAn even more general framework than possibility or Dempster-Shafer\ntheory is provided by Halpern’s plausibility measures\n(Halpern 2003). These are functions Pl:\\(\\mathbf{A} \\rightarrow \\Re\\)\nsuch that for all \\(A, B\\) in \\(\\mathbf{A}\\): \nWith the exception of sets of probabilities, every model of partial\nbelief that we have seen in this section is a special case of\nplausibility measures. While it is fairly uncontroversial that partial\nbelief functions should obey Halpern’s plausibility calculus, it\nis questionable whether his minimal principles capture anything of\nepistemic interest. The resulting epistemology is, in any case, very\nthin. It should be noted, though, that Halpern does not intend\nplausibility measures to provide a complete epistemology, but rather a\ngeneral framework to study more specific accounts. \nRanking theory (Spohn 1988, 1990 and especially 2012) assigns\nnumerical degrees of disbelief directly to possibilities in\n\\(W\\). A pointwise ranking function \\(\\kappa : W \\rightarrow\n\\bN\\cup \\{\\infty \\}\\) assigns a natural number (or \\(\\infty\\)) to each\npossible world in \\(W\\). These numbers represent the degree of\ndisbelief you assign to each possibility. Equipped with a pointwise\nranking function, we can generate a numbered partition of\n\\(W\\): \nThe cell \\(\\kappa^{-1}(\\infty)\\) is the set of possibilities which are\nmaximally disbelieved. The cell \\(\\kappa^{-1}(n)\\) is the set of\npossibilities which are disbelieved to degree \\(n\\). Finally,\n\\(\\kappa^{-1}(0)\\) contains the possibilities which are not\ndisbelieved (although this does not mean that they are believed).\nExcept for \\(\\kappa^{-1}(0)\\), the cells \\(\\kappa^{-1}(n)\\) may be\nempty. Since one cannot consistently disbelieve everything, the first\ncell must not be empty.  \nA pointwise ranking function \\(\\kappa\\) induces a ranking\nfunction \\(\\varrho : \\mathbf{A} \\rightarrow \\bN\\cup \\{\\infty \\}\\)\non the field \\(\\mathbf{A}\\) by setting, \nfor each \\(A \\in \\mathbf{A}\\). In other words: a set of worlds is only\nas disbelieved as its most plausible member. By convention, we set\n\\(\\varrho(\\varnothing)=\\infty\\). This entails that ranking functions\nare (finitely) minimitive and hence super-additive. In other\nwords, for all \\(A, B\\) in \\(\\mathbf{A}\\), \nAlternatively, we can characterize ranking functions as those\nfunctions \\(\\varrho: \\mathbf{A}\\rightarrow \\mathbf{N}\\cup\\{\\infty\\}\\)\nsatisfying  \nfor all \\(A,B\\in \\mathbf{A}.\\) The first axiom says that no agent\nshould disbelieve the tautological proposition \\(W\\). The second says\nthat every agent should maximally disbelieve the contradictory\nprosition. Intuitively, the final axiom says that a disjunction \\(A\n\\cup B\\) should be disbelieved just in case both disjuncts are\ndisbelieved. However, the meaning of the final axiom is not exhausted\nby this gloss (see Section 4.1 in Huber, 2020). \nAs stated above, the third axiom is called finite\nminimitivity. Just as in probability theory, it can be\nstrengthened to countable unions, resulting in countably\nminimitive ranking functions. Unlike probability theory, finite\nminimitivity can also be strenghtened to arbitrary unions, resulting\nin completely minimitive ranking functions. See Huber (2006)\nfor conditions under which a ranking function defined on a field of\npropositions induces a pointwise ranking functions on the underlying\nset of possibilities. \nThe number \\(\\varrho(A)\\) represents the agent’s degree of\ndisbelief in the proposition \\(A\\). If \\(\\varrho(A) \\gt 0\\), the agent\ndisbelieves \\(A\\) to a positive degree. Therefore, on pain of\ninconsistency, she cannot also disbelieve \\(\\neg A\\) to a positive\ndegree. In other words, for every proposition \\(A\\) in \\(\\mathbf{A}\\),\nat least one of \\(A, \\neg A\\) has to be assigned rank 0. If\n\\(\\varrho(A) = 0\\), the agent does not disbelieve \\(A\\) to a positive\ndegree. However, this does not mean that she believes \\(A\\) to a\npositive degree \\(-\\) the agent may suspend judgment and assign rank 0\nto both \\(A\\) and \\(\\neg A\\). So belief in a proposition is\ncharacterized by disbelief in its negation. \nFor each ranking function \\(\\varrho\\) we can define a corresponding\nbelief function \\(\\beta : \\mathbf{A} \\rightarrow\n\\mathbf{Z}\\cup \\{\\pm \\infty \\}\\) by setting \\( \\beta(A) = \\varrho(\\neg\nA) - \\varrho(A) \\) for all \\(A\\) in \\(\\mathbf{A}\\). The belief\nfunction assigns positive numbers to those propositions that are\nbelieved, negative numbers to those propositions that are disbelieved,\nand 0 to those propositions with respect to which the agent suspends\njudgment. Each ranking function \\(\\varrho\\) induces a belief\nset \n\\(\\bB\\) is the set of all propositions the agent believes to some\npositive degree, or equivalently, whose complements she disbelieves to\na positive degree. The belief set \\(\\bB\\) induced by a\n(finitely/countable/completely minimitive) ranking function\n\\(\\varrho\\) is consistent and deductively closed (in the\nfinite/countably/complete sense). Since any proposition with\n\\(\\beta(A)>0\\) is believed, ranking theory can be seen as satisfying\nthe Lockean thesis (see\n Section 4.2.2)\n without sacrificing deductive closure. Note, however, that we could\nhave defined \\(\\bB\\) to include all those propositions \\(A\\) with\n\\(\\beta(A) \\gt t\\) for some threshold \\(t\\) greater than zero. See\nRaidl (2019) for an exploration of these possibilities.  \nSo far we have discussed the synchronic structure ranking theory\nimposes on belief. How should ranks be updated when new information is\nreceived? The conditional ranking function\n\\(\\varrho(\\cdot\\mid \\cdot):\\mathbf{A}\\times\\mathbf{A} \\rightarrow\n\\bN\\cup \\{\\infty \\} \\) based on the non-conditional ranking function\n\\(\\varrho\\) is defined by setting  \nfor all \\(A, B\\) in \\(\\mathbf{A}\\) where \\(A \\neq \\varnothing\\). We\nadopt the convention that \\(\\infty - \\infty = 0\\) and \\(\\infty - n =\n\\infty\\) for all finite \\(n\\). Note that this entails that\n\\(\\varrho(\\neg A\\mid A) = \\infty\\). Requiring that\n\\(\\varrho(\\varnothing \\mid B) = \\infty\\) for all \\(B\\) in\n\\(\\mathbf{A}\\) guarantees that \\(\\varrho(\\cdot\\mid B)\\) is a\n(non-conditional) ranking function. Conditional ranking functions give\nrise to a ranking-theoretic counterpart of strict\nconditionalization: \nPlain Conditionalization.\n\nSuppose that \\(\\varrho(\\cdot)\\) is your ranking function at time \\(t\\)\nand that \\(\\varrho(A),\\varrho(\\neg A) \\lt \\infty\\). Furthermore,\nsuppose that between \\(t\\) and \\(t^\\prime\\) you become certain of\n\\(A\\) and no logically stronger proposition. Then, your ranking\nfunction at time \\(t^\\prime\\) should be \\(\\varrho(\\cdot\\mid A)\\).  \nNote that all the considerations relevant to the interpretation of\nBayesian conditionalization (discussed in\n Section 3.1.4)\n apply equally to ranking-theoretic conditioning. It is clear from the\ndefinition of conditioning that, as in the Bayesian case, the rank of\nthe material conditional is a lower bound for the conditional rank:\n\\(\\beta(A\\rightarrow B)\\leq \\beta(B|A)\\). That ensures that\nranking-theoretic update satisfies Conditionalization ( see\n Section 2.1.1).\n It also satisfies a version of Rational Monotony: if \\(\\beta(\\neg\nA)=0\\) and \\(\\beta(B)>0,\\) then \\(\\beta(B|A)>0.\\) Therefore,\nranking-theoretic update satisfies the “spirit” of AGM\nupdate. Note, however, that ranking theory has no trouble with\niterated belief revision: a revision takes as input a ranking function\nand an evidential proposition and outputs a new ranking function.\nMoreveover, the situation changes if the threshold for belief is\nraised to some positive number greater than zero: in that case,\nRational Monotony may no longer be satisfied (see Raidl, 2019).  \nPlain conditionalization only covers the case when new evidence\nacquires the maximum degree of certainty. Spohn (1988) also defines a\nranking-theoretic analogue to Jeffrey conditionalization. In this\ncase, new evidence does not acquire maximum certainty, but merely\nchanges your ranks for various propositions. For more on\nranking-theoretic notions of belief update see Huber (2014, 2019). For\na comparison of probability theory and ranking theory see Spohn (2009,\nSection 3)  \nWhy should grades of disbelief obey the ranking calculus? And why\nshould ranking functions be updated according to ranking-theoretic\nconditionalization? Recall that in the case of subjective probability\ntheory, analogous questions were answered by appeal either to the\nDutch book arguments or considerations of epistemic accuracy. Is it\npossible to make similar arguments in favor of ranking theory? \nThe short answer is yes: Huber (2007 and 2020, Chapter 5) proves that\nobeying the normative constraints of the ranking calculus is a\nnecessary and sufficient condition for maintaining consistency and\ndeductive closure, both synchronically and in the face of new\nevidence. The latter end is, in turn, a necessary, but insufficient\nmeans to attaining the end of always having only true beliefs, and as\nmany thereof as possible. Unlike the Dutch book arguments, this result\nrequires no appeal to pragmatic considerations about betting behavior.\nBrössel, Eder & Huber (2013) discuss the importance of this\nresult as well as its Bayesian role-model, Joyce’s (1998, 2009)\n“non-pragmatic” vindication of probabilism. \nIt is fairly straightforward to see that any agent that obeys the\nsynchronic requirements of the ranking calculus (and updates according\nto one of the sactioned update rules) will maintain beliefs that are\nmutually consistent and deductively closed, no matter what new\nevidence arrives. What is slightly more puzzling is why any agent who\ndoes not validate the ranking calculus must either have\ninconsistent beliefs, or fail to believe a logical consequence of some\nof her beliefs. In other words: it is clear that the ranking calculus\nis sufficient for deductive cogency, but why is it\nnecessary? \nTo sketch the necessity argument, we must introduce a bit of\nterminology. An agent’s degree of entrenchment for a\nproposition \\(A\\) is the number of “independent and minimally\npositively reliable” (mp-reliable) information sources saying\n\\(A\\) that it would take for the agent to give up her disbelief that\n\\(A\\). If the agent does not disbelieve \\(A\\) to begin with, her\ndegree of entrenchment for \\(A\\) is 0. If no finite number of\ninformation sources is able to make the agent give up her disbelief\nthat \\(A\\), her degree of entrenchment for \\(A\\) is \\(\\infty\\). Of\ncourse, information sources we typically encounter are rarely\nindependent or mp-reliable. However, this is not crucial. Independent,\nmp-reliable sources are a theoretical construct – the connection\nbetween entrenchment and update is stated as a counterfactual because\nthe agent need never actually encounter such information sources. \nThe necessity argument proceeds by stipulating that entrenchment is\nreflected in degrees of disbelief. This connects an agent’s\nupdating behavior with her ranks for various propositions.\nRecall that Bayesians stipulate a similar connection between degrees\nof belief and acceptable betting ratios. Equipped with this\nconnection, Huber (2007, 2020) proves that if an agent fails to\nvalidate the ranking calculus then, were she to encounter independent,\nmp-reliable information sources of various kinds, her beliefs would\nfail to be deductively cogent. That completes the necessity\nargument. \nWith the possible exception of decision making (see however Giang and\nShenoy, 2000 as well as Spohn 2017a, 2020), it seems that we can do\neverything with ranking functions that we can do with probability\nmeasures. Ranking theory also naturally gives rise to a notion of\nqualitative belief that incurs no Lottery-style paradoxes (see\n Section 4.2.2).\n This may be vital if we want to stay in tune with traditional\nepistemology. The treatment of ranking theory in this article has\nbeen, of necessity, rather compressed. For an excellent article-length\nintroduction see Huber (2019). For an accessible book, see Huber\n(2020). For an extensive book-length treatment, with applications to\nmany subjects in epistemology and philosophy of science, see Spohn\n(2012). \nThere are those who deny that there are any interesting principles\nbridging full and partial belief. Theorists of this persuasion often\nwant either to eliminate one of these attitudes or reduce it to a\nspecial case of the other. Jeffrey (1970) suggests that talk of full\nbelief is vestigial and will be entirely superseded by talk of partial\nbelief and utility: \nTheorists such as Kaplan (1996) also suggests that talk of full belief\nis superfluous once the mechanisms of Bayesian decision theory are in\nplace. After all, only partial beliefs and utilities play any role in\nthe Bayesian framework of rational deliberation, whereas full belief\nneed not be mentioned at all. Those committed to full beliefs have the\nburden of showing how rationality will be the poorer without recourse\nto full belief. Kaplan calls this the Bayesian Challenge.\nStalnaker (1984) is much more sympathetic to a qualitative notion of\nbelief, but acknowledges the force of the Bayesian challenge.  \nIt is true that there is no canonical qualitative analogue to the\nBayesian theory of practical deliberation. However, the fact that it\nis the theorist of full belief that feels the challenge, and not \nvice versa, may be an accident of history: if a qualitative\ntheory of practical deliberation had been developed first, the shoe\nwould now be on the other foot. The situation would be even more\nsevere if qualitative decision making, which we seem to implement as a\nmatter of course, were less cognitively demanding than its Bayesian\ncounterpart. Of course, this anticipates a robust theory of rational\nqualitative deliberation that is not immediately forthcoming. However,\nrecent work such as Lin (2013) and Spohn (2017a, 2019) may remedy that\ninadequacy. For example, Lin (2013) proves a Savage-style\nrepresentation theorem characterizing the relationship between full\nbeliefs, desires over possible outcomes and preferences over acts. By\ndeveloping a theory of rational action in terms of qualitative belief,\nLin demonstrates how one might answer the Bayesian challenge.  \nOn the other hand there are partisans of full belief that are deeply\nskeptical about partial beliefs. See Harman (1986), Pollock (2006),\nMoon (2017), Horgan (2017) and the “bad cop” in\nHájek and Lin (2017). Many of these object that partial beliefs\nhave no psychological reality and would be too difficult to reason\nwith if they did. Horgan (2017) goes so far as to say that typically\n“there is no such psychological state as the agent’s\ncredence in \\(p\\)” and that Bayesian epistemology is “like\nalchemy and phlogiston theory: it is not about any real phenomena, and\nthus it also is not about any genuine norms that govern real\nphenomena.” Harman (1986) argues that we have very few explicit\npartial beliefs. A theory of reasoning, according to Harman, can\nconcern only explicit attitudes, since these are the only ones that\ncan figure in a reasoning process. Therefore, Bayesian epistemology,\nwhile perhaps an account of dispositions to act, is not a guide to\nreasoning. Nevertheless, partial beliefs may be implicit in our system\nof full beliefs in that they can be reconstructed from our\ndispositions to revise them: \nOn this picture, almost all of our explicit beliefs are qualitative.\nPartial beliefs are not graded  belief attitudes toward\npropositions, but rather dispositions to revise our  full\nbeliefs. The correct theory of partial belief, according to Harman,\nhas more to do with entrechment orders (see Section 2.2.2) or\nranking-theoretic degrees of belief (see Section 3.4) than with\nprobabilities. Other apparently partial belief attitudes are explained\nas full beliefs  about objective probabilities. So, in the\ncase of a fair lottery with ten thousand tickets, the agent does not\nbelieve to a  high degree that the \\(n^{th}\\) ticket will not\nwin, but rather fully believes that it is objectively improbable that\nit will win. \nFrankish (2009) objects that Harman’s view requires that an\nagent have a full belief in any proposition that we have a degree of\nbelief in: “And this is surely wrong. I have some degree of\nconfidence (less than 50%) in the proposition that it will rain\ntomorrow, but I do not believe flat-out that it will rain – not,\nat least, by the everyday standards for flat-out belief.” Harman\nmight reply that Frankish merely has a full belief in the objective\nprobability of rain tomorrow. Frankish claims that this escape route\nis closed to Harman because single events “do not have objective\nprobabilities,” but this matter is hardly settled. \nStaffel (2013) gives an example in which a proposition with a higher\ndegree of belief is apparently less entrenched than one with a lower\ndegree of belief. Suppose that you will draw a sequence of two million\nmarbles from a big jar full of red and black marbles. You do not know\nwhat proportion of the marbles are red. Consider the following\ncases: \nStaffel argues that your degree of belief in the first case is higher\nthan in the second, but much more entrenched in the second than in the\nfirst. Therefore, degree of belief cannot be reduced to degree of\nentrenchment. Nevertheless, the same gambit is open to Harman in the\ncase of the marbles–he can claim that in both scenarios you\nmerely have a full belief in a proposition about objective chance. See\nStaffel (2013) for a much more extensive engagement with Harman\n(1986).  \nAnyone who allows for the existence of both full and partial belief\ninherits a thorny problem: how are full beliefs related to partial\nbeliefs? That seemingly innocent question leads to a treacherous\nsearch for  bridge principles connecting a rational\nagent’s partial beliefs with her full beliefs. Theorists engaged\nin the search for bridge principles usually take for granted some set\nof rationality principles governing full belief and its revision, e.g.\nAGM theory, or a rival system of non-monotonic reasoning. Theorists\nusually also take for granted that partial belief ought to be\nrepresentable by probability functions obeying some flavor of Bayesian\nrationality. The challenge is to propose additional rationality\npostulates governing how a rational agent’s partial beliefs\ncohere with her full beliefs. In this section, we will for the most\npart accept received wisdom and assume that orthodox Bayesianism is\nthe correct model of partial belief and its updating. We will be more\nopen-minded about the modeling of full belief and its rational\nrevision.  \nIn this section, we will once again take propositions over the set\n\\(W\\) to be the objects of belief. As before, the reader is invited to\nthink of \\(W\\) as a set of coarse-grained, mutually exclusive,\npossible ways the actual world might be. We write \\(\\Bel\\) to denote\nthe set of  propositions that the agent believes and use\n\\(\\Bel(A)\\) as shorthand for \\(A\\in \\Bel\\). We will also require some\nnotation for qualitative propositional belief change. For all\n\\(E\\subseteq W\\), write \\(\\Bel_E\\) for the set of propositions the\nagent would believe upon learning \\(E\\) and no stronger proposition.\nWe will also write \\(\\Bel(A|E)\\) as shorthand for \\(A\\in \\Bel_E.\\) By\nconvention, \\(\\Bel= \\Bel_W.\\) If \\({\\bf F}\\) is a set of propositions,\nwe let \\(\\Bel_{\\bf F}\\) be the set \\(\\{ \\Bel_E : E \\in {\\bf F} \\}.\\)\nThe set \\(\\Bel_{\\bf F}\\) represents an agent’s  dispositions\n to update her qualitative beliefs given information from \\({\\bf\nF}\\).  \nThe following normative constraint on the set of full beliefs \\(\\Bel\\)\nplays a large role in what follows. \nDeductive Cogency. The belief set \\(\\Bel\\) is\nconsistent and \\(B\\in\\Bel\\) if and only if \\(\\cap \\Bel \\subseteq B.\\)\n \nIn other words, deductive cogency means that there is a single,\nnon-empty proposition, \\(\\cap \\Bel\\), which is the logically strongest\nproposition that the agent believes, entailing all her other\nbeliefs. \nAll of the rationality norms that we have seen for updating\nqualitative beliefs have propositional analogues. The following are\npropositional analogues for the AGM principles from Section 2.2.1. \nSupposing that for all \\(E\\subseteq W\\), \\(\\Bel_E\\) satisfies\ndeductive cogency, the first six postulates reduce to the following\nthree.  Together, Inclusion and Preservation say that whenever information \\(E\\) is consistent with  current belief \\(\\cap \\Bel,\\) \\[\\cap \\Bel_E = \\cap \\Bel \\cap E.\\] If \\({\\bf F}\\) is a collection of propositions and for all \\(E\\in {\\bf F},\\) the belief sets \\(\\Bel,\\Bel_E\\) satisfy the AGM principles, we say that \\(\\Bel_{\\bf F},\\) the agent’s disposition to update her qualitative beliefs given information from \\({\\bf F}\\), satsifies the basic AGM principles. \nWe will use \\(\\Pr(\\cdot)\\) to denote the probability function\nrepresenting the agent’s partial beliefs. Of course,\n\\(\\Pr(\\cdot)\\) is defined on a \\(\\sigma\\)-algebra of subsets of \\(W\\).\nIn the usual case, when \\(W\\) is finite, we can take the powerset\n\\(\\mathcal{P}(W)\\) to be the relevant \\(\\sigma\\)-algebra. To update\npartial belief, we adopt the standard probabilistic modeling. For\n\\(E\\subseteq W\\) such that \\(\\Pr(E)>0,\\) \\(\\Pr(\\cdot|E)\\) is the\npartial belief function resulting from learning \\(E.\\) We will\nsometimes use \\(\\Pr_E\\) as a shorthand for \\(\\Pr(\\cdot|E)\\). Almost\nalways, partial belief is updated via standard conditioning. As\nbefore, \\({\\bf A}^{\\Pr}\\) is the set of propositions with positive\nprobability according to \\(\\Pr\\). \nThe first bridge principle that suggests itself is that full belief is\njust the maximum degree of partial belief. Expressed\nprobabilistically, it says that at all times a rational agent’s\nbeliefs and partial beliefs can be represented by a pair \\(\\langle\n\\Bel,p \\rangle\\) satisfying: \nExtremal Probability. \\( A\\in\\Bel\\) if and only if\n\\(\\Pr(A)=1.\\)  \nRoorda (1995) calls this the received view of how full and\npartial belief ought to interact. Gärdenfors (1986) is a\nrepresentative of this view, as are van Fraasen (1995) and\nArló-Costa (1999), although the latter two accept a slightly\nnon-standard probabilistic modeling for partial belief. For fans of\ndeductive cogency, the following observations ought to count in favor\nof the received view. \nTheorem. If \\(\\langle \\Bel, p \\rangle\\) satisfy\nextremal probability, then \\(\\Bel\\) is deductively cogent.  \nTheorem. Suppose that\\(\\langle \\Bel_E, \\Pr_E\n\\rangle\\) satisfy extremal probability for all \\(E\\in {\\bf A}^{\\Pr}.\\)\nThen \\(\\Bel_{{\\bf A}^{\\Pr}}\\) satisfies the AGM postulates.  \nIn other words: if an agent’s partial beliefs validate the\nprobability axioms, she updates by Bayesian conditioning and fully\nbelieves all and only those propositions with extremal probability,\nher qualitative update behavior will satisfy all the AGM postulates\n(at least whenever Bayesian conditioning is defined). Readers who take\nthe AGM revision postulates to be a sine qua non of rational\nbelief update will take this to be good news for the received view.\n \nRoorda (1995) makes three criticisms of the received view. Consider\nthe following three propositions. \nOf course, I am not as confident that Fillmore was the 13th president\nas I am in the truth of the tautology expressed in (3). Yet there does\nnot seem to be anything wrong with saying that I fully believe each of\n(1), (2) and (3). However, if extremal probability is right, it is\nirrational to fully believe each of (1), (2) and (3) and not assign\nthem all the same degree of belief. \nRoorda’s second objection appeals to the standard connection\nbetween degrees of belief and practical decision making. Suppose I\nfully believe (1). According to the standard interpretation of degrees\nof belief in terms of betting quotients, I ought to accept a bet that\npays out a dollar if (1) is true, and costs me a million dollars if\n(1) is false. In fact, if I truly assign unit probability to (1), I\nought to accept nearly any stakes whatsoever that guarantee some\npositive payout if (1) is true. Yet it seems perfectly rational to\nfully believe (1) and refrain from accepting such a bet. If we accept\nBayesian decision theory, extremal probability seems to commit me to\nall sorts of weird and seemingly irrational betting behavior. \nRoorda’s final challenge to extremal probability appeals to\ncorrigibility, according to which it is reasonable to believe\nthat at least some of my beliefs may need to be abandoned in light of\nnew information. However, if partial beliefs are updated via Bayesian\nconditioning, I can never cease to believe any of my full beliefs\nsince if \\(\\Pr(A)=1\\) it follows that \\(\\Pr(A|E)=1\\) for all \\(E\\)\nsuch that \\(\\Pr(E)>0\\). If we believe in Bayesian conditioning,\nextremal probability seems to entail that I cannot revise any of my\nfull beliefs in light of new information. \nMost discussions of the Lockean thesis have in mind the strong thesis.\nMore recent work, especially Leitgeb (2017), adopts the weaker thesis.\nThe strong thesis leaves the correct threshold unspecified. Of course,\nfor every \\(s\\in(\\frac{1}{2},1)\\) we can formulate a specific thesis\n\\(\\text{SLT}^s\\) in virtue of which the strong thesis is true. For\nexample, \\(\\text{SLT}^{.51}\\) is a very permissive version of the\nthesis, whereas \\(\\text{SLT}^{.95}\\) and \\(\\text{SLT}^{.99}\\) are more\nstringent. It is also possible to further specify the weak thesis. For\nexample, Leitgeb (2017) believes that the contextually-determined\nthreshold should be equal to the degree of belief assigned to the\nstrongest proposition that is fully believed. In light of deductive\ncogency, that corresponds to the orthographically ungainly\n\\(\\text{WLT}^{\\Pr(\\cap \\Bel)}\\). \nThe strong Lockean thesis gives rise to the well-known  Lottery\nparadox, due originally to Kyburg (1961, 1997). The lesson of the\nLottery is that the strong thesis is in tension with deductive\ncogency. Suppose that \\(s\\) is the universally correct Lockean\nthreshold. Now think of a fair lottery with \\(N\\) tickets, where \\(N\\)\nis chosen large enough that \\(1-(1/N) \\geq s.\\) Since the lottery is\nfair, it seems permissible to fully believe that  some ticket\nis the winner. It also seems reasonable to assign degree of belief\n\\(1/N\\) to each proposition of the form ‘The \\(i^{\\text{th}}\\)\nticket is the winner.’ According to the Lockean thesis, such an\nagent ought to fully believe that the first ticket is a loser, the\nsecond ticket is a loser, the third is a loser, etc. Since cogency\nrequires belief to be closed under conjunction, she ought to believe\nthat all the tickets are losers. But now she violates cogency by\nbelieving both that every ticket is a loser and that some ticket is a\nwinner. Since \\(s\\) was arbitrary, we have shown that no matter how\nhigh we set the threshold, there is some Lottery for which an agent\nmust either violate the Lockean thesis or violate deductive cogency.\nAccording to Kyburg, what the paradox teaches is that we should give\nup on deductive cogency: full belief should not necessarily be closed\nunder conjunction. Many others take the lesson of the Lottery to be\nthat the strong Lockean thesis is untenable. \nSeveral authors (Pollock (1995), Ryan (1996), Douven (2002)) attempt\nto revise the strong Lockean thesis by placing restrictions on when a\nhigh degree of belief warrants full belief. Broadly speaking, they\npropose that a high degree of belief is sufficient to warrant full\nbelief unless some defeating condition holds. For example, Douven\n(2002) says that it is sufficient except when the proposition is a\nmember of a  probabilistically self-undermining set. A set\n\\({\\bf S}\\) is probabilistically self-undermining iff for all \\(A \\in\n{\\bf S}, \\Pr(A)>s\\) and \\(\\Pr(A | B)\\leq s,\\) where \\(B=\\cap( {\\bf S}\n\\setminus \\{ A \\}).\\) It is clear that this proposal would prohibit\nfull belief that a particular lottery ticket will lose. \nAll proposals of this kind are vitiated by the following sort of\nexample due to Korb (1992). Let \\(A\\) be any proposition with a degree\nof belief above threshold but short of certainty. Let \\(L_i\\) be the\nproposition that the \\(i^{th}\\) lottery ticket (of a large lottery\nwith \\(N\\) tickets) will lose. Consider the set \\({\\bf S} = \\{ \\neg A\n\\cup L_i | 1 \\leq i \\leq N \\}.\\) Each member of \\({\\bf S}\\) is above\nthreshold, since \\(L_i\\) is above threshold. Furthermore, the set\n\\({\\bf S} \\cup \\{ A\\}\\) meets Douven’s (as well as\nPollock’s and Ryan’s) defeating conditions. Therefore,\nthese proposals prohibit full belief in any proposition with degree of\nbelief short of certainty. Douven and Williamson (2006) generalize\nthis sort of example to trivialize an entire class of similar formal\nproposals. \nBuchak (2014) argues that what partial beliefs count as full beliefs\ncannot merely be a matter of the degree of partial belief, but must\nalso depend on the type of evidence it is based on. According to\nBuchak, this means there can be no merely formal answer to the\nquestion: what conditions on partial belief are necessary and\nsufficient for full belief? The following example, of a type going\nback to Thomson (1986), illustrates the point. Your parked car was hit\nby a bus in the middle of the night. The bus could belong either to\nthe blue bus company or the red bus company. Consider the following\ntwo scenarios. \nBuchak (2014) argues that it is rational to have full belief that a\nblue bus is to blame in the first scenario, but not in the second. You\nhave only statistical evidence in the first scenario, whereas in the\nsecond, a causal chain of events connects your belief to the accident\n(see also Thomson (1986), Nelkin (2000) and Schauer (2003)). These\nintuitions, Buchak observes, are reflected in our legal practice:\npurely statistical evidence is not sufficient to convict. If you find\nBuchak’s point convincing, you will be unsatisfied with most of\nthe proposed accounts for how full and partial belief ought to\ncorrespond (see Staffel (2016)).  \nDespite difficulties with buses and lotteries, the dynamics of\nqualitative belief under the strong thesis are independently\ninteresting to investigate. For example, van Eijk and Renne (2014, Other Internet Resources)\naxiomatize the logic of belief for a Lockean with threshold\n\\(\\frac{1}{2}\\). Makinson and Hawthorne (2015) investigate which\nprinciples of non-monotonic logic are validated by Lockean agents.\nBefore turning to proposed solutions to the Lottery paradox, we make\nsome observations about qualitative Lockean revision, inspired largely\nby Shear and Fitelson (2018). \nIt is a theorem of the probability calculus that \\(\\Pr(H|E) \\leq\n\\Pr(E\\rightarrow H)\\). So if \\(H\\) is assigned a high degree of belief\ngiven \\(E\\), the material conditional \\(E\\rightarrow H\\) must have\nbeen assigned a degree of belief at least as high ex ante. It\nis easy to see that as a probabilistic analogue of the principle of\nConditionalization from non-monotonic logic or, equivalently, the AGM\nInclusion principle. That observation has the following consequence:\nany belief that the Lockean comes to have after conditioning, she\ncould have arrived at by adding the evidence to her prior beliefs and\nclosing under logical consequence. Therefore Lockean updating\nsatisfies the AGM principle of Inclusion. Furthermore, it follows\nimmediately from definitions that Lockean update satisfies Success and\nExtensionality. \nTheorem. Suppose that \\(s\\in (\\frac{1}{2}, 1)\\). For\nall \\(E\\in{\\bf A}^{\\Pr}\\), let \\(\\Bel_E = \\{ A : \\Pr(A|E) \\geq s\\}\\).\nThen, \\(\\Bel_{{\\bf A}^{\\Pr}}\\) satisfies Inclusion, Success and\nExtensionality.  \nIn Section 2.2, we argued that Inclusion and Preservation capture the\nspirit of AGM revision. If Lockean revision also satsified\nPreservation, we would have a clean sweep of the AGM principles, with\nthe exception of deductive cogency. However, that cannot hold in\ngeneral. It is possible to construct examples where \\(\\Pr(\\neg E) \\lt\ns,\\) \\(\\Pr(H)\\geq s\\) and yet \\(\\Pr(H|E) \\lt s\\). For Lockean agents\nthis means that it is possible to lose a belief, even when revising on\na proposition that is not disbelieved.  \nRecall the example of Alice, Bob and the Ford from Section\n2.1.1. Let \\(W=\\{a, b, c\\}\\) corresponding to the worlds in which\nAlice owns the Ford, Bob own the Ford and no one in the office owns\nthe Ford.  Suppose the probability function \\[\\Pr(a) =\\frac{6}{10},\n\\Pr(b)=\\frac{3}{10} \\text{ and } \\Pr(c)=\\frac{1}{10}\\] captures my\npartial beliefs. For Lockean thresholds in the interval \\((.75,.9]\\),\nmy full beliefs are exhausted by \\(\\Bel=\\{ \\{a,b\\}, W\\}.\\) Now suppose\nI were to learn that Alice does not own the Ford. That is consistent\nwith all beliefs in \\(\\Bel\\), but since \\(\\Pr(\\{a,b\\} | \\{b,c\\}) =\n\\frac{3}{4}\\) it follows by the Lockean thesis that \\(\\{a,b\\} \\notin\n\\Bel_{\\{b,c\\}}\\). So Lockeanism does not in general validate\nPreservation. The good news, at least for those sympathetic to\nPollock’s critique of non-monotonic logic, is that the Lockean\nthesis allows for undercutting defeat of previous beliefs. \nHowever, Shear and Fitelson (2018) also have some good news for fans\nof AGM and the Lockean thesis. Two quantities are in the golden\nratio \\(\\phi\\) if their ratio is the same as the ratio of their\nsum to the larger of the two quantities, i.e. for \\(a>b>0\\), if\n\\(\\frac{a+b}{a} = \\frac{a}{b}\\) then \\(\\frac{a}{b} = \\phi\\). The\ngolden ratio is an irrational number approximately equal to \\(1.618.\\)\nIts inverse \\(\\phi^{-1}\\) is approximately \\(.618.\\) Shear and\nFitelson prove the following intriguing result.  \nTheorem. Suppose that \\(s\\in (\\frac{1}{2},\n\\phi^{-1}]\\). For all \\(E\\in{\\bf A}^{\\Pr}\\), let \\(\\Bel_E = \\{ A :\n\\Pr(A|E) \\geq s\\}.\\) Let \\({\\bf D}=\\{E\\in{\\bf A}^{\\Pr} : \\Bel_E \\text{\nis deductively cogent} \\}.\\) Then \\(\\Bel_{{\\bf D}}\\) satisfies the six\nbasic AGM postulates.  \nThat shows that for relatively low thresholds, Lockean updating\nsatsifies all the AGM postulates–at least when we restrict to\ndeductively cogent belief sets. For an explanation of why the golden\nratio arises in this context see Section 6.2 in Genin (2019). \nFor many, sacrificing deductive cogency is simply too high a price to\npay for a bridge principle, even one so simple and intuitive as the\nstrong Lockean thesis. That occassions a search for bridge principles\nthat can be reconciled with deductive cogency. One proposal, due to\nLeitgeb (2013, 2014, 2017) and Arló-Costa (2012), holds that\nrational full belief corresponds to a stably high degree of belief,\ni.e. a degree of belief that remains high even after conditioning on\nnew information. Leitgeb calls this view the  Humean thesis,\ndue to Hume’s conception of belief as an idea of superior\nvivacity, but also of superior steadiness. See Loeb (2002, 2010) for a\ndetailed development of the stability theme in Hume’s conception\nof belief. Leitgeb (2017) formalizes Hume’s definition,\narticulating the following version of the thesis: \nIn other words: every full belief must have stably high conditional\ndegree of belief, at least when conditioning on propositions which are\nnot currently disbelieved. Since full belief occurs on both sides of\nthe biconditional, it is evident that this is not a proposed \nreduction of full belief to partial belief, but rather a\nconstraint that every rational agent must satisfy. The Humean thesis\nleaves the precise threshold \\(s\\) unspecified. Of course, for every\n\\(\\frac{1}{2} \\lt s \\lt 1,\\) we can formulate a specific thesis\n\\(\\text{HT}^s\\) in virtue of which the thesis is true. For example,\n\\(\\text{HT}^{.5}\\) requires that every fully believed proposition\nremains more likely than its negation when conditioning on\npropositions not currently disbelieved. \nSome form of stability is widely considered to be a necessary\ncondition for  knowledge. Socrates propounds such a view in\nthe  Meno. Paxson Jr. and Lehrer (1969) champion such a view\nin the epistemology literature post-Gettier (1963). However, stability\nis not usually mooted as a condition of  belief. Raidl and\nSkovgaard-Olsen (2017) claim that Leitgeb’s stability condition\nis more appropriate in an analysis of knowledge and too stringent a\ncondition on belief. A defender of the Humean thesis might say that\nevery  rational belief is possibly an instance of knowledge.\nSince knowledge is necessarily stable, unstable beliefs are  ipso\nfacto not known. \nLeitgeb demonstrates the following relationships between the Humean\nthesis, deductive cogency and the weak Lockean thesis. \nTheorem. Suppose that \\(\\langle \\Bel, \\Pr \\rangle\\)\nsatsify \\(\\text{HT}\\) and \\(\\varnothing \\notin \\Bel.\\) Then, \\(\\Bel\\)\nis deductively cogent and \\(\\langle \\Bel, \\Pr\\rangle\\) satisfy\n\\(\\text{WLT}^{\\Pr(\\cap \\Bel)}.\\)  \nSo if an agent satisfies the Humean thesis and does not\n“fully” believe the contradictory proposition, her\nqualitative beliefs are deductively cogent and furthermore, she\nsatisfies the weak Lockean thesis, where the threshold is set by the\ndegree of belief assigned to \\(\\cap\\Bel,\\) the logically strongest\nproposition she believes. Leitgeb also proves the following partial\nconverse. \nTheorem. Suppose that \\(\\Bel\\) is deductively cogent\nand \\(\\langle \\Bel, \\Pr \\rangle\\) satisfy \\(\\text{WLT}^{\\Pr(\\cap\n\\Bel)}\\). Then, \\(\\langle \\Bel, \\Pr \\rangle\\) satsify\n\\(\\text{HT}^{\\frac{1}{2}}\\) and \\(\\varnothing \\notin \\Bel\\). \nTogether, these two theorems say that the Humean thesis (with\nthreshold 1/2) is equivalent to deductive cogency and the weak Lockean\nthesis (with threshold \\(\\Pr(\\cap\\Bel)\\)). Since it is always possible\nto satisfy HT\\(^\\frac{1}{2}\\), Leitgeb gives us an ingenious way to\nreconcile deductive cogency with a version of the Lockean thesis. \nRecall the example of the lottery. Let \\(W=\\{w_1, w_2, \\ldots, w_N\n\\},\\) where \\(w_i\\) is the world in which the \\(i^{\\text{th}}\\) ticket\nis the winner. No matter how many tickets are in the lottery, a Humean\nagent cannot believe any ticket will lose. Suppose for a contradiction\nthat she believes \\(W\\setminus \\{w_1\\}\\), the proposition that the\nfirst ticket will lose. Now suppose she learns \\(\\{w_1, w_2\\}\\), that\nall but the first and second ticket will lose. This is compatible with\nher initial belief, but her updated degree of belief that the first\nticket will lose must be \\(1/2\\). That contradicts the Humean thesis.\nSo she cannot believe that any ticket will lose. In this Lottery\nsituation the agent cannot fully believe any non-trivial proposition.\nThis example also shows how sensitive the Humean proposal is to the\nfine-graining of possibilities. If we coarsen \\(W\\) into the set of\npossibilities \\(W=\\{w_1, w_2\\}\\), where \\(w_1\\) is the world in which\nthe first ticket is the winner and \\(w_2\\) is “the” world\nin which some other ticket is the winner, the agent can believe that\nthe first ticket will lose without running afoul of the Humean thesis.\n \nIf Buchak (2014) is right, no agent should have beliefs in lottery\npropositions–these beliefs would necessarily be formed on the\nbasis of purely statistical evidence. Kelly and Lin (2019) give\nanother scenario in which Humean agents seem radically skeptical, but\nin situations which are evidentially unproblematic. Suppose the\nluckless Job goes in for a physical. On the basis of a thorough\nexamination, the doctor forms the following dire opinion of his\nhealth: her degree of belief that Job will survive exactly \\(n\\)\nmonths is \\(\\frac{1}{2^n}.\\) Therefore, her degree of belief that Job\nwill not survive the year is \\(\\frac{1}{2} + \\frac{1}{4} + \\cdots +\n\\frac{1}{2^{12}} > .999.\\) Shockingly, the Humean thesis prevents the\ndoctor from forming  any non-trivial beliefs. Let \\(\\leq n\\)\nbe the proposition that Job survives at most \\(n\\) months and let\n\\(\\geq n\\) be the proposition that he survives at least \\(n\\) months.\nLet \\(B\\) be the strongest proposition that the doctor believes.\nSuppose for a contradiction that \\(B\\) entails some least upper bound\nfor the number of Job’s remaining months, i.e for some \\(n\\),\n\\(B\\) entails \\(\\leq n\\) and does not entail \\(\\leq n^\\prime \\) for\nany \\(n^\\prime \\lt n\\). By construction, \\(\\Pr(B| \\geq n) =\n\\frac{\\Pr(n)}{\\Pr(\\geq n)} = \\frac{1}{2}\\) for all \\(n\\). But since\n\\(\\geq n\\) is compatible with \\(B\\), the Humean thesis requires that\n\\(\\Pr(B| \\geq n) > \\frac{1}{2}.\\) Contradiction.  \nThe example of the doctor suggests that the price of Humeanism is a\nrather extreme form of skepticism: in many situations a Humean agent\nwill have no non-trivial full beliefs at all. That criticism is\ndeveloped extensively in Rott (2017) and Douven and Rott (2018). The\ndoctor also illustrates how the Humean proposal allows arbitrarily\nsmall perturbations of partial beliefs to be reflected as huge\ndifferences in full beliefs. Suppose the doctor is slightly more\nconfident that Job will not survive a month, i.e. her survival\nprobabilities decrease as \\(\\frac{1}{2} + \\epsilon, \\frac{1}{4},\n\\frac{1}{8} - \\epsilon, \\frac{1}{16}, \\frac{1}{32}, \\ldots.\\) Now the\ndoctor can believe that Job will be dead in two months without running\nafoul of the Humean thesis. \nSo far we have inquired only into the synchronic content of the Humean\nproposal. What sort principles of qualitative belief update does it\nunderwrite? Leitgeb demonstrates an intimate relationship between the\nAGM revision principles and the Humean thesis: every agent that\nsatisfies the AGM principles, as well as a weak version of the Lockean\nthesis, must also satisfy the Humean thesis. So if you think that AGM\ntheory is the correct theory of rational qualitative belief update\n(and you believe that a high degree of partial belief is a \nnecessary condition of full belief) you must also accept the\nHumean thesis. More precisely, Leitgeb proves the following: \nTheorem. Suppose that \\(\\Bel_{{\\bf A}^{\\Pr}}\\)\nsatisfies all AGM postulates and for all \\(E\\in{\\bf A}^{\\Pr},\\) \\(A\\in\n\\Bel_E\\) only if \\(\\Pr(A|E)>r.\\) Then, for all \\(E\\in{\\bf A}^{\\Pr},\\)\n\\(\\langle \\Bel_E, \\Pr_E \\rangle\\) satisfy \\(\\text{HT}^r\\).  \nSo any agent that violates the Humean thesis must either fail to\nsatisfy the AGM postulates, or the high-probability requirement. Note\nthat the converse is not true: it is not the case that that if all\npairs \\(\\langle \\Bel_E, \\Pr_E \\rangle\\) satisfy the Humean thesis,\nthen \\(\\Bel_{{\\bf A}^{\\Pr}}\\) must satisfy the AGM postulates. To\nprove this, suppose that \\(\\langle \\Bel, \\Pr \\rangle\\) satisfy the\nHumean thesis and \\(\\cap \\Bel \\subset E\\) for some \\(E\\in{\\bf\nA}^{\\Pr}.\\) If we let \\(\\Bel_E = \\{E\\}\\), then \\(\\langle \\Bel_E, \\Pr_E\n\\rangle\\) satisfy the Humean thesis. However, such an agent patently\nviolates rational and even Cautious Monotony. For a somewhat more\ndetailed treatment of the relationship between AGM theory and the\nHumean thesis, see Section 6.3 in Genin (2019). For a full treatment,\nsee Leitgeb (2017). \nLin and Kelly (2012) propose that qualitative belief update ought to\n track partial belief update. On their picture, partial and\nfull beliefs are maintained and updated by parallel cognitive systems.\nThe first system, governed by the probabilistic norms of Bayesian\ncoherence and conditioning, is precise, slow and cognitively\nexpensive. That system is engaged for important deliberations\nrequiring a lot of precision and occuring without much time pressure,\ne.g. retirement planning. The second, which in some way maintains and\nupdates full beliefs, is quicker and less cognitively burdensome. That\nsystem is engaged in ordinary planning: grocery shopping, or selecting\na restaurant for a department event. (For an objection to the two\nsystems view, see Staffel (2018).) What keeps these two parallel\nsystems in sync with each other?  Lin and Kelly (2012) study  acceptance rules that specify a mechanism for transitioning gracefully into the qualitative and out of the probabilistic system. An acceptance rule \\(\\alpha\\) maps every partial belief state \\(\\Pr\\) to a unique qualitative belief state \\(\\alpha(\\Pr)\\) with which it coheres. For example, the strong Lockean thesis determines an acceptance rule once we specify a threshold. The Humean thesis, on the other hand, underdetermines an acceptance rule, merely imposing constraints on acceptable pairs \\(\\langle \\Bel, \\Pr \\rangle.\\) An agent’s qualitative updates  track her probabilistic updates iff \\[\\alpha(\\Pr)_E=\\alpha(\\Pr_E),\\] whenever \\(\\Pr(E)>0\\). In other words: acceptance followed by qualitative revision yields the same belief state as probabilistic revision followed by acceptance. \nHere is a way to understand the tracking requirement. Suppose that,\nalthough an agent maintains a latent probabilistic belief state, most\nof her cognitive life is spent reasoning with and updating qualitative\nbeliefs. A typical day will go by without having to engage the\nprobabilistic system at all. Suppose Monday is a typical day. Let\n\\(\\langle \\alpha(\\Pr), \\Pr \\rangle\\) be the belief state she woke up\nwith on Monday: her full and partial beliefs are in harmony. Let \\(E\\)\nbe the total information she acquired since waking up. Since\nqualitative beliefs are updated on the fly, she goes to sleep with the\nqualitative belief state \\(\\alpha(\\Pr)_E\\). Overnight, her\nprobabilistic system does the difficult work of Bayesian conditioning\nand computes the partial belief state \\(\\Pr_E\\), just in case she runs\ninto any sophisticated decision problems on Tuesday. Before waking,\nshe transitions out of her probabilistic system \\(\\Pr_E\\) and into the\nqualitative belief state \\(\\alpha(\\Pr_E)\\). If she fails the tracking\nrequirement, she may wake up on Tuesday morning with a qualitative\nbelief state that is drastically different from the one she had went\nto sleep with on Monday night. If she tracks, then she will notice no\ndifference at all. For such an agent, no mechanism (other than memory)\nis required to bring her full and partial beliefs back into harmony on\nTuesday morning. Supposing that we  enter the probabilistic\nsystem by conditioning our previous partial belief state \\(\\Pr\\) on\nall new information \\(E\\), and  exit by accepting\n\\(\\alpha(\\Pr_E),\\) tracking ensures that transitioning in and out of\nthe probabilistic system does not induce any drastic changes in\nqualitative beliefs. An agent that tracks will notice no difference at\nall. An agent that does not track may find her full and partial\nbeliefs perpetually falling out of sync, requiring many expensive\nacceptance operations to bring them back into harmony.  \nTracking may be a desirable property, but are there any architectures\nthat exhibit it? Lin and Kelly (2012) answer this question\naffirmitively. Since Bayesian conditioning is taken for granted, Lin\nand Kelly must specify two things: a qualitative revision operation\nand an acceptance rule that jointly track conditioning. We turn now to\nthe details of their proposal. As usual, let \\(W\\) be a set of worlds.\nA  question  \\({\\bf Q}\\) is a partition of \\(W\\) into a\ncountable collection of mutually exhaustive propositions \\(H_1, H_2,\n\\ldots,\\) which are the complete  answers to \\({\\bf Q}.\\) The\npartial belief function \\(\\Pr\\) is defined over the algebra of\npropositions \\({\\bf A}\\) generated by \\({\\bf Q}.\\) Let \\(\\prec\\) be a\nwell-founded, strict partial order over the answers to \\({\\bf Q}\\). (A\nstrict partial order is  well-founded iff every subset of the\norder has a least element.) This is interpreted as a  plausibility\nordering, where \\(H_i \\prec H_j\\) means that \\(H_i\\) is strictly\n more  plausible than \\(H_j\\). Every plausibility order\n\\(\\prec\\) gives rise to a deductively cogent belief state\n\\(\\Bel_\\prec\\) by letting \\(\\neg H_i\\in \\Bel_\\prec\\) iff there is some\n\\(H_j\\) strictly more plausible than \\(H_i\\) and closing under logical\nconsequence. In other words, \\(\\cap \\Bel_\\prec\\) is the disjunction of\nthe minimal elements in the plausibility order.   First we  specify an acceptance rule. Lin and Kelly propose the  odds threshold rule. The  degree of belief function \\(\\Pr\\) is used to determine a plausibility order by setting \\[H_i \\prec_p H_j \\text{ if and only if } \\frac{\\Pr(H_i)}{\\Pr(H_j)} > t,\\] where \\(t\\) is a constant greater than \\(1\\) and \\(\\Pr(H_i),\\Pr(H_j)>0\\). This determines an acceptance rule by setting \\(\\alpha(\\Pr)= \\Bel_{\\prec_p}.\\) Since the odds threshold rule determines a plausibility order \\(\\prec_p\\) and any plausibility order \\(\\prec\\)  gives rise to a deductively cogent belief state \\(\\Bel_\\prec,\\) the Lottery paradox is avoided. In other words: the bridge principle that any rational \\(\\langle \\Bel,p \\rangle\\)  are related by \\(\\Bel=\\alpha(\\Pr)\\) enures that \\(\\Bel\\) is deductively cogent. Furthermore, the odds threshold rule allows non-trivial qualitative beliefs in situations where the stability theory precludes them. Recall the case of the doctor. Consider the odds threshold \\(2^{10} -1\\). Given this threshold, the hypothesis that Job will survive exactly 1 month is strictly more plausible than the proposition that he will survive at least \\(n\\) months for any \\(n\\geq 10\\). This threshold yields the full belief that Job will survive at most 10 months. However, in the case of the Lottery, the odds threshold rule precludes any non-trivial beliefs. (The content-dependent threshold rule proposed by Kelly and Lin (forthcoming) may allow non-trivial beliefs in the Lottery situation.) See Rott (2017) and  Douven and Rott (2018) for an extensive comparison of the relative likelihood of forming non-trivial qualitative beliefs on the odds-threshold and stability proposals.  \nIt remains to specify the qualitative revision operation. Lin and\nKelly adopt an operation proposed by Shoham (1987). The plausibility\norder \\(\\prec\\) is updated on evidence \\(E\\) by setting every answer\nincompatible with \\(E\\) to be strictly less plausible than every\nanswer compatible with \\(E,\\) and otherwise leaving the order\nunchanged. Let \\(\\prec_E\\) denote the result of this update operation.\nWe use the updated plausibility order to define a belief revision rule\nby setting \\(\\Bel_E = \\Bel_{\\prec_E}.\\) Then, for all \\(E,F\\subseteq\nW\\), \\(B_E\\) is deductively cogent and satisfies: \nHowever, it does not necessarily satisfy Preservation. To see this\nsuppose that \\({\\bf Q}=\\{H_1, H_2, H_3\\}\\) and \\(H_1 \\prec H_2\\) but\n\\(H_3\\) is not ordered with \\(H_1\\) or \\(H_2\\). Then \\(\\cap \\Bel =\nH_1\\cup H_3.\\) However \\(\\cap\\Bel_{\\neg H_1} = H_2 \\cup H_3\\nsubseteq\n\\cap \\Bel\\) even though \\(\\cap \\Bel \\cap \\neg H_1 \\neq\n\\varnothing.\\) \nLin and Kelly prove that Shoham revision and odds-threshold based\nacceptance jointly track conditioning: \nTheorem. Let \\(\\prec\\) equal \\(\\prec_p\\) and let\n\\(\\Bel_E = \\Bel_{\\prec_E}\\). Then \\(\\Bel_{\\mathcal{P}(W)}\\) satisfies\ndeductive cogency, Success, Cautious Monotony and Inclusion.\nFurthermore, \\(\\Bel_E = \\alpha(\\Pr)_E = \\alpha(\\Pr_E)\\) for all\n\\(E\\in{\\bf A}^{\\Pr}\\).  \nIn other words: odds-threshold acceptance followed by Shoham revision\nyields the same belief state as Bayesian conditioning followed by\nodds-threshold acceptance. (Kelly and Lin (forthcoming) recommend a\nmodification of the odds-threshold rule proposed in Lin and Kelly\n(2012).) Although the original plausibility ordering \\(\\prec_p\\) is\nbuilt from the probability function \\(\\Pr,\\) subsequent qualitative\nupdate proceeds without consulting the (conditioned) probabilities.\nThat shows that there are at least some architectures that\neffortlessly keep the probabilistic and qualitative reasoning systems\nin harmony. \nFans of AGM will regret that Shoham revision does not satisfy AGM\nPreservation (Rational Monotony). Lin and Kelly (2012) prove that no\n“sensible” acceptance rule that tracks conditioning can\nsatisfy Inclusion and Preservation. We omit the technical definition\nof sensible rules here. For a summary, see Section 6.4 in Genin\n(2019). \nAll of the bridge principles we have seen so far have the following in\ncommon: whether an agent’s full and partial beliefs cohere is a\nmatter of the full and partial beliefs  alone. It is not\nnecessary to mention preferences or utilities in order to evaluate a\nbelief state. There is another tradition, originating in Hempel (1962)\nand receiving classical expression in Levi (1967a), that assimilates\nthe problem of “deciding” what to believe to a Bayesian\ndecision-theoretic model. Crucially, these authors are not committed\nto a picture on which agents literally decide what to\nbelieve–rather they claim that an agent’s beliefs are\nsubject to the same kind of normative evaluation as their practical\ndecision-making. Contemporary contributions to this tradition include\nEaswaran (2015), Pettigrew (2016) and Dorst (2017). Presented here is\na somewhat simplified version of Levi’s (1967a) account taking\npropositions, rather than sentences, as the objects of belief. \nAs usual, let \\(W\\) be a set of possible worlds. The agent is taken to\nbe interested in answering a  question  \\({\\bf Q},\\) which is\na partition of \\(W\\) into a finite collection of mutually exhaustive\nanswers \\(\\{ H_1, H_2, \\ldots H_n\\}.\\) Levi calls situations of this\nsort “efforts to replace agnosticism by true belief,”\nechoing themes in Peirce (1877):  The agent’s partial beliefs are represented by a probability function \\(\\Pr\\) that is defined, at a minimum, over the algebra \\({\\bf A}\\) generated by the question. Levi recommends the following procedure to determine which propositions are fully believed: disjoin all those elements of \\({\\bf Q}\\) that have  maximal  expected epistemic utility and then close under deductive consequence. The  expected epistemic utility of a hypothesis \\(H\\in{\\bf A}\\) is defined as: \\[E(H) := \\Pr(H)\\cdot U(H) + \\Pr(\\neg H)\\cdot u(H),\\] where \\(U(H)\\) is the epistemic utility of accepting \\(H\\) when it is true, and \\(u(H)\\) is the utility of accepting \\(H\\) when it is false. How are \\(u(H), U(H)\\) to be determined? Levi is guided by the following principles. \nIt is easy to object to these principles. The first principle\nestablishes a lexicographic preference for true beliefs. It is\nconceivable that, contra this principle, an informative false belief\nthat is approximately true should have greater epistemic utility than\nan uninformative true belief. The first principle precludes trading\ncontent against truthlikeness. It is also conceivable that, contra the\nthird principle, one would prefer to be wrong, but not too\nopinionated, than wrong and opinionated. The only unexceptionable\nprinciple seems to be the second.  \nTo measure the degree of relief from agnosticism, a probability\nfunction \\(m(\\cdot)\\) is defined over the elements of \\({\\bf A}\\).\nCrucially, \\(m(\\cdot)\\) does not measure a degree of belief, but\ndegree of  uninformativeness. The degree of relief from\nagnosticism afforded by \\(H\\in {\\bf A},\\) also referred to as the \namount of content  in \\(H\\), is defined to be the complement of\nuninformativeness: \\(cont(H_i) = m(\\neg H_i).\\) Levi argues that all\nthe elements of \\({\\bf Q}\\) ought to be assigned the same amount of\ncontent, i.e. \\(m(H_i)=\\frac{1}{n}\\) and therefore \\(cont(H_i)=\n\\frac{n-1}{n}\\) for each \\(H_i\\in{\\bf Q}\\). The set of epistemic\nutility functions that Levi recommends satisfy the following\nconditions:   where \\(0 \\lt q \\lt 1.\\) All such utility functions are guaranteed to satisfy Levi’s three principles. The parameter \\(q\\) is interepreted as a “degree of caution,” representing the premium  placed on truth as opposed to relief from agnosticism. When \\(q=1\\) the epistemic utility of suspending judgement, \\(U(W)\\), is equal to zero. This is the situation in which the premium placed on relief from doubt is the maximum. Levi proves that expected epistemic utility \\(E(H)\\) is maximal iff \\(\\Pr(H)> q \\cdot cont(\\neg H).\\) Therefore, Levi’s ultimate recommendation is that the agent believe all deductive consequences of \\[\\cap \\{ \\neg H_i \\in {\\bf Q} : \\Pr(\\neg H_i)> 1 - q \\cdot cont(\\neg H_i) \\}.\\] From this formulation  it is possible to see Levi’s proposal as a question-dependent version of the Lockean thesis where the appropriate threshold is a function of content. However, Levi takes pains to make sure that the result of this operation is deductively cogent and therefore  avoids Lottery-type paradoxes. \nContemporary contributions to the decision-theoretic tradition proceed\ndifferently from Levi. Most recent work does not take epistemic\nutility to be primarily a function of content. Most of these proposals\ndo not refer to a question in context. Many proposals, such as\nEaswaran (2015), Dorst (2017), are equivalent to a version of the\nLockean thesis, where the threshold is determined by the utility the\nagent assigns to true and false beliefs. Since these are essentially\nLockean proposals, they are subject to Lottery-style paradoxes. ","contact.mail":"konstantin.genin@gmail.com","contact.domain":"gmail.com"}]
