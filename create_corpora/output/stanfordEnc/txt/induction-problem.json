[{"date.published":"2018-03-21","url":"https://plato.stanford.edu/entries/induction-problem/","author1":"Leah Henderson","author1.info":"http://lhenderson.org/","entry":"induction-problem","body.text":"\n\n\nWe generally think that the observations we make are able to justify\nsome expectations or predictions about observations we have not yet\nmade, as well as general claims that go beyond the observed. For\nexample, the observation that bread of a certain appearance has thus\nfar been nourishing seems to justify the expectation that the next\nsimilar piece of bread I eat will also be nourishing, as well as the\nclaim that bread of this sort is generally nourishing. Such inferences\nfrom the observed to the unobserved, or to general laws, are known as\n“inductive inferences”.\n\n\nThe original source of what has become known as the “problem of\ninduction” is in Book 1, part iii, section 6 of A Treatise\nof Human Nature by David Hume, published in 1739. In 1748, Hume\ngave a shorter version of the argument in Section iv of An enquiry\nconcerning human understanding. Throughout this article we will\ngive references to the Treatise as “T”, and the\nEnquiry as “E”.\n\n\nHume asks on what grounds we come to our beliefs about the unobserved\non the basis of inductive inferences. He presents an argument in the\nform of a dilemma which appears to rule out the possibility of any\nreasoning from the premises to the conclusion of an inductive\ninference. There are, he says, two possible types of arguments,\n“demonstrative” and “probable”, but neither\nwill serve. A demonstrative argument produces the wrong kind of\nconclusion, and a probable argument would be circular. Therefore, for\nHume, the problem remains of how to explain why we form any\nconclusions that go beyond the past instances of which we have had\nexperience (T. 1.3.6.10). Hume stresses that he is not disputing that\nwe do draw such inferences. The challenge, as he sees it, is to\nunderstand the “foundation” of the inference—the\n“logic” or “process of argument” that it is\nbased upon (E. 4.2.21). The problem of meeting this challenge, while\nevading Hume’s argument against the possibility of doing so, has\nbecome known as “the problem of induction”.\n\n\nHume’s argument is one of the most famous in philosophy. A\nnumber of philosophers have attempted solutions to the problem, but a\nsignificant number have embraced his conclusion that it is insoluble.\nThere is also a wide spectrum of opinion on the significance of the\nproblem. Some have argued that Hume’s argument does not\nestablish any far-reaching skeptical conclusion, either because it was\nnever intended to, or because the argument is in some way\nmisformulated. Yet many have regarded it as one of the most profound\nphilosophical challenges imaginable since it seems to call into\nquestion the justification of one of the most fundamental ways in\nwhich we form knowledge. Bertrand Russell, for example, expressed the\nview that if Hume’s problem cannot be solved, “there is no\nintellectual difference between sanity and insanity” (Russell\n1946: 699).\n\n\nIn this article, we will first examine Hume’s own argument,\nprovide a reconstruction of it, and then survey different responses to\nthe problem which it poses.\n\n\n\n\n\n\nHume introduces the problem of induction as part of an analysis of the\nnotions of cause and effect. Hume worked with a picture, widespread in\nthe early modern period, in which the mind was populated with mental\nentities called “ideas”. Hume thought that ultimately all\nour ideas could be traced back to the “impressions” of\nsense experience. In the simplest case, an idea enters the mind by\nbeing “copied” from the corresponding impression (T.\n1.1.1.7/4). More complex ideas are then created by the combination of\nsimple ideas (E. 2.5/19). Hume took there to be a number of relations\nbetween ideas, including the relation of causation (E. 3.2; for more\non Hume’s philosophy in general, see Morris & Brown\n2014). \nFor Hume, the relation of causation is the only relation by means of\nwhich “we can go beyond the evidence of our memory and\nsenses” (E. 4.1.4, T. 1.3.2.3/74). Suppose we have an object\npresent to our senses: say gunpowder. We may then infer to an effect\nof that object: say, the explosion. The causal relation links our past\nand present experience to our expectations about the future (E.\n4.1.4/26). \nHume argues that we cannot make a causal inference by purely a\npriori means (E. 4.1.7). Rather, he claims, it is based on\nexperience, and specifically experience of constant conjunction. We\ninfer that the gunpowder will explode on the basis of past experience\nof an association between gunpowder and explosions. \nHume wants to know more about the basis for this kind of inference. If\nsuch an inference is made by a “chain of reasoning” (E.\n4.2.16), he says, he would like to know what that reasoning is. In\ngeneral, he claims that the inferences depend on a transition of the\nform: \nI have found that such an object has always been attended with\nsuch an effect, and I foresee, that other objects, which are, in\nappearance, similar, will be attended with similar effects. (E.\n4.2.16) \nIn the Treatise, Hume says that \nif Reason determin’d us, it would proceed upon that principle\nthat instances, of which we have had no experience, must resemble\nthose, of which we have had experience, and that the course of nature\ncontinues always uniformly the same. (T. 1.3.6.4) \nFor convenience, we will refer to this claim of similarity or\nresemblance between observed and unobserved regularities as the\n“Uniformity Principle (UP)”. Sometimes it is also called\nthe “Resemblance Principle”, or the “Principle of\nUniformity of Nature”. \nHume then presents his famous argument to the conclusion that there\ncan be no reasoning behind this principle. The argument takes the form\nof a dilemma. Hume makes a distinction between relations of ideas and\nmatters of fact. Relations of ideas include geometric, algebraic and\narithmetic propositions, “and, in short, every affirmation,\nwhich is either intuitively or demonstratively certain”.\n“Matters of fact”, on the other hand are empirical\npropositions which can readily be conceived to be other than they are.\nHume says that \nAll reasonings may be divided into two kinds, namely, demonstrative\nreasoning, or that concerning relations of ideas, and moral reasoning,\nor that concerning matter of fact and existence. (E. 4.2.18) \nHume considers the possibility of each of these types of reasoning in\nturn, and in each case argues that it is impossible for it to supply\nan argument for the Uniformity Principle. \nFirst, Hume argues that the reasoning cannot be demonstrative, because\ndemonstrative reasoning only establishes conclusions which cannot be\nconceived to be false. And, he says,  \nit implies no contradiction that the course of nature may change, and\nthat an object seemingly like those which we have experienced, may be\nattended with different or contrary effects. (E. 4.2.18)  \nIt is possible, he says, to clearly and distinctly conceive of a\nsituation where the unobserved case does not follow the regularity so\nfar observed (E. 4.2.18, T. 1.3.6.5/89). \nSecond, Hume argues that the reasoning also cannot be “such as\nregard matter of fact and real existence”. He also calls this\n“probable” reasoning. All such reasoning, he claims,\n“proceed upon the supposition, that the future will be\nconformable to the past”, in other words on the Uniformity\nPrinciple (E. 4.2.19). \nTherefore, if the chain of reasoning is based on an argument of this\nkind it will again be relying on this supposition, “and taking\nthat for granted, which is the very point in question”. (E.\n4.2.19, see also T. 1.3.6.7/90). The second type of reasoning then\nfails to provide a chain of reasoning which is not circular. \nIn the Treatise version, Hume concludes \nThus, not only our reason fails us in the discovery of the\nultimate connexion of causes and effects, but even after\nexperience has inform’d us of their constant\nconjunction, ’tis impossible for us to satisfy ourselves by\nour reason, why we shou’d extend that experience beyond those\nparticular instances, which have fallen under our observation. (T.\n1.3.6.11/91–2) \nThe conclusion then is that our tendency to project past regularities\ninto the future is not underpinned by reason. The problem of induction\nis to find a way to avoid this conclusion, despite Hume’s\nargument. \nAfter presenting the problem, Hume does present his own\n“solution” to the doubts he has raised (E. 5, T.\n1.3.7–16). This consists of an explanation of what the inductive\ninferences are driven by, if not reason. In the Treatise Hume\nraises the problem of induction in an explicitly contrastive way. He\nasks whether the transition involved in the inference is produced \nby means of the understanding or imagination; whether we are\ndetermin’d by reason to make the transition, or by a certain\nassociation and relation of perceptions? (T. 1.3.6.4) \nAnd he goes on to summarize the conclusion by saying \nWhen the mind, therefore, passes from the idea or impression of one\nobject to the idea or belief of another, it is not determin’d by\nreason, but by certain principles, which associate together the ideas\nof these objects, and unite them in the imagination. (T. 1.3.6.12) \nThus, it is the imagination which is taken to be responsible for\nunderpinning the inductive inference, rather than reason. \nIn the Enquiry, Hume suggests that the step taken by the\nmind,  \nwhich is not supported by any argument, or process of the\nunderstanding … must be induced by some other principle of\nequal weight and authority. (E. 5.1.2)  \nThat principle is “custom” or “habit”. The\nidea is that if one has seen similar objects or events constantly\nconjoined, then the mind is inclined to expect a similar regularity to\nhold in the future. The tendency or “propensity” to draw\nsuch inferences, is the effect of custom: \n… having found, in many instances, that any two kinds of\nobjects, flame and heat, snow and cold, have always been conjoined\ntogether; if flame or snow be presented anew to the senses, the mind\nis carried by custom to expect heat or cold, and to believe,\nthat such a quality does exist and will discover itself upon a nearer\napproach. This belief is the necessary result of of placing the mind\nin such circumstances. It is an operation of the soul, when we are so\nsituated, as unavoidable as to feel the passion of love, when we\nreceive benefits; or hatred, when we meet with injuries. All these\noperations are a species of natural instincts, which no reasoning or\nprocess of the thought and understanding is able, either to produce,\nor to prevent. (E. 5.1.8) \nHume argues that the fact that these inferences do follow the course\nof nature is a kind of “pre-established harmony” (E.\n5.2.21). It is a kind of natural instinct, which may in fact be more\neffective in making us successful in the world, than if we relied on\nreason to make these inferences. \nHume’s argument has been presented and formulated in many\ndifferent versions. There is also an ongoing lively discussion over\nthe historical interpretation of what Hume himself intended by the\nargument. It is therefore difficult to provide an unequivocal and\nuncontroversial reconstruction of Hume’s argument. Nonetheless,\nfor the purposes of organizing the different responses to Hume’s\nproblem that will be discussed in this article, the following\nreconstruction will serve as a useful starting point. \nHume’s argument concerns specific inductive inferences such\nas: \nAll observed instances of A have been B. \nThe next instance of A will be B. \nLet us call this “inference I”. Inferences which\nfall under this type of schema are now often referred to as cases of\n“simple enumerative induction”. \nHume’s own example is: \nAll observed instances of bread (of a particular appearance) have been\nnourishing. \nThe next instance of bread (of that appearance) will be\nnourishing. \nHume’s argument then proceeds as follows (premises are labeled\nas P, and subconclusions and conclusions as C): \n1st horn: \n2nd horn: \nConsequences: \nThere have been different interpretations of what Hume means by\n“demonstrative” and “probable” arguments.\nSometimes “demonstrative” is equated with\n“deductive”, and probable with “inductive”\n(e.g., Salmon 1966). Then the first horn of Hume’s dilemma would\neliminate the possibility of a deductive argument, and the second\nwould eliminate the possibility of an inductive argument. However,\nunder this interpretation,\n premise P3\n would not hold, because it is possible for the conclusion of a\ndeductive argument to be a non-necessary proposition. Premise P3 could\nbe modified to say that a demonstrative (deductive) argument\nestablishes a conclusion that cannot be false if the premises are\ntrue. But then it becomes possible that the supposition that the\nfuture resembles the past, which is not a necessary proposition, could\nbe established by a deductive argument from some premises, though not\nfrom a priori premises (in contradiction to conclusion\n C1). \nAnother common reading is to equate “demonstrative” with\n“deductively valid with a priori premises”, and\n“probable” with “having an empirical premise”\n(e.g., Okasha 2001). This may be closer to the mark, if one thinks, as\nHume seems to have done, that premises which can be known a\npriori cannot be false, and hence are necessary. If the inference\nis deductively valid, then the conclusion of the inference from a\npriori premises must also be necessary. What the first horn of\nthe dilemma then rules out is the possibility of a deductively valid\nargument with a priori premises, and the second horn rules\nout any argument (deductive or non-deductive), which relies on an\nempirical premise. \nHowever, recent commentators have argued that in the historical\ncontext that Hume was situated in, the distinction he draws between\ndemonstrative and probable arguments has little to do with whether or\nnot the argument has a deductive form (Owen 1999; Garrett 2002). In\naddition, the class of inferences that establish conclusions whose\nnegation is a contradiction may include not just deductively valid\ninferences from a priori premises, but any inferences that\ncan be drawn using a priori reasoning (that is, reasoning\nwhere the transition from premises to the conclusion makes no appeal\nto what we learn from observations). It looks as though Hume does\nintend the argument of the first horn to rule out any a\npriori reasoning, since he says that a change in the course of\nnature cannot be ruled out “by any demonstrative argument or\nabstract reasoning a priori” (E. 5.2.18). On this\nunderstanding, a priori arguments would be ruled out by the\nfirst horn of Hume’s dilemma, and empirical arguments by the\nsecond horn. This is the interpretation that I will adopt for the\npurposes of this article. \nIn Hume’s argument, the UP plays a central role. As we will see\nin\n section 4.2,\n various authors have been doubtful about this principle. Versions of\nHume’s argument have also been formulated which do not make\nreference to the UP. Rather they directly address the question of what\narguments can be given in support of the transition from the premises\nto the conclusion of the specific inductive inference I. What\narguments could lead us, for example, to infer that the next piece of\nbread will nourish from the observations of nourishing bread made so\nfar? For the first horn of the argument, Hume’s argument can be\ndirectly applied. A demonstrative argument establishes a conclusion\nwhose negation is a contradiction. The negation of the conclusion of\nthe inductive inference is not a contradiction. It is not a\ncontradiction that the next piece of bread is not nourishing.\nTherefore, there is no demonstrative argument for the conclusion of\nthe inductive inference. In the second horn of the argument, the\nproblem Hume raises is a circularity. Even if Hume is wrong that all\ninductive inferences depend on the UP, there may still be a\ncircularity problem, but as we shall see in\n section 4.1,\n the exact nature of the circularity needs to be carefully considered.\nBut the main point at present is that the Humean argument is often\nformulated without invoking the UP. \nSince Hume’s argument is a dilemma, there are two main ways to\nresist it. The first is to tackle the first horn and to argue that\nthere is after all a demonstrative argument –here taken to mean\nan argument based on a priori reasoning—that can\njustify the inductive inference. The second is to tackle the second\nhorn and to argue that there is after all a probable (or empirical)\nargument that can justify the inductive inference. We discuss the\ndifferent variants of these two approaches in sections\n 3\n and\n 4. \nThere are also those who dispute the consequences of the dilemma. For\nexample, some recent commentators on Hume interpret him as drawing\nonly conclusion\n C4,\n and not the normative conclusion\n C5\n (we discuss these interpretations in\n section 5.1).\n There are also approaches which take issue with\n premise P8\n and argue that providing a chain of reasoning from the premises to\nthe conclusion is not a necessary condition for justification of an\ninductive inference (sections\n 5.2\n and\n 5.3).\n Finally, there are some philosophers who do accept the skeptical\nconclusion\n C5\n and attempt to accommodate it. For example, there have been attempts\nto argue that inductive inference is not as central to scientific\ninquiry as is often thought\n (section 6).\n It is also possible to argue that even though Hume’s argument\ndoes establish that inductive inferences are not justified in the\nsense that we have reasons to think their conclusions true,\nnonetheless a weaker kind of justification is possible. This is based\non the idea that we can establish that following inductive procedures\nis a means to certain epistemic ends. We examine the tradition\nassociated with this approach in\n section 7. \nThe first horn of Hume’s argument, as formulated above, is aimed\nat establishing that there is no demonstrative argument for the UP. A\nnumber of philosophers have thought that this does not definitively\nrule out the possibility of a justification of inductive inferences\nbased on a demonstrative argument. There are two main potential escape\nroutes from the first horn of Hume’s dilemma. The first is to\ndeny\n premise P3,\n which amounts to admitting the possibility of synthetic a\npriori propositions. The second is to accept the conclusion\n C1,\n that there is no demonstrative argument for the UP, but to argue that\nsuch an argument is not necessary for justification. Indeed, one could\nsay that it is not even necessary to have a demonstrative argument for\nthe conclusion of the inductive inference. Rather, the thought is, it\nwill be sufficient for justification to have an argument to the\nproposition that the conclusion of the inductive inference is\nprobable. We address each of these approaches in the next two\nsections. \nAs we have seen in\n section 1,\n Hume takes demonstrative arguments to have conclusions which are\n“relations of ideas”, whereas “probable” or\n“moral” arguments have conclusions which are\n“matters of fact”. Hume’s distinction between\n“relations of ideas” and “matters of fact”\nanticipates the distinction drawn by Kant between\n“analytic” and “synthetic” propositions (Kant\n1781). A classic example of an analytic proposition is\n“Bachelors are unmarried men”, and a synthetic proposition\nis “My bike tyre is flat”. For Hume, demonstrative\narguments, which are based on a priori reasoning, can\nestablish only relations of ideas, or analytic propositions. The\nassociation between a prioricity and analyticity underpins\n premise P3,\n which states that a demonstrative argument establishes a conclusion\nwhose negation is a contradiction. \nOne possible response to Hume’s problem is to deny\n premise P3,\n by allowing the possibility that a priori reasoning could\ngive rise to synthetic propositions. Kant famously argued in response\nto Hume that such synthetic a priori knowledge is possible\n(Kant 1781, 1783). He does this by a kind of reversal of the\nempiricist programme espoused by Hume. Whereas Hume tried to\nunderstand how the concept of a causal or necessary connection could\nbe based on experience, Kant argued instead that experience only comes\nabout through the concepts or “categories” of the\nunderstanding. On his view, one can gain a priori knowledge\nof these concepts, including the concept of causation, by a\ntranscendental argument concerning the necessary preconditions of\nexperience. A more detailed account of Kant’s response to Hume\ncan be found in de Pierris and Friedman 2013. \nThe first horn of Hume’s dilemma implies that there cannot be a\ndemonstrative argument to the conclusion of an inductive inference\nbecause it is possible to conceive of the negation of the conclusion.\nFor instance, it is quite possible to imagine that the next piece of\nbread I eat will poison me rather than nourish me. However, this does\nnot rule out the possibility of a demonstrative argument that\nestablishes only that the bread is highly likely to nourish, not that\nit definitely will. There are several approaches that attempt to\nproduce a demonstrative argument that the conclusion of an inductive\ninference is probable, though not certain. If this succeeds, a chain\nof reasoning based on demonstrative arguments from the premises of\ninference I to the proposition that the conclusion is probable\nis not ruled out by Hume’s argument. One might then challenge\n premise P8,\n by saying that it is not necessary for justification of an inductive\ninference to have a chain of reasoning from its premises to its\nconclusion. Rather it would suffice if we had an argument from the\npremises to the claim that the conclusion is probable or likely. Then\nan a priori justification of the inductive inference would\nhave been provided. \nThe first of these approaches is the\n“Nomological-explanatory” solution, which has been put\nforward by Armstrong, BonJour and Foster (Armstrong 1983; BonJour\n1998; Foster 2004). This solution appeals to Inference to the Best\nExplanation (IBE), which says that we should infer that the hypothesis\nwhich provides the best explanation of the evidence is probably true.\nProponents of this approach take Inference to the Best Explanation to\nbe a mode of inference which is distinct from the type of\n“extrapolative” inductive inference that Hume was trying\nto justify. They also regard it as a type of inference which although\nnon-deductive, is justified a priori. For example, Armstrong\nsays “To infer to the best explanation is part of what it is to\nbe rational. If that is not rational, what is?” (Armstrong 1983:\n59). \nThe a priori justification is taken to proceed in two steps.\nFirst, it is argued that we should recognize that certain observed\nregularities require an explanation in terms of some underlying law.\nFor example, if a coin persistently lands heads on repeated tosses,\nthen it becomes increasingly implausible that this occurred just\nbecause of “chance”. Rather, we should infer to the better\nexplanation that the coin has a certain bias. Saying that the coin\nlands heads not only for the observed cases, but also for the\nunobserved cases, does not provide an explanation of the observed\nregularity. Thus, mere Humean constant conjunction is not sufficient.\nWhat is needed for an explanation is a “non-Humean,\nmetaphysically robust conception of objective regularity”\n(BonJour 1998), which is thought of as involving actual natural\nnecessity (Armstrong 1983; Foster 2004). \nOnce it has been established that there must be some metaphysically\nrobust explanation of the observed regularity, the second step is to\nargue that out of all possible metaphysically robust explanations, the\n“straight” inductive explanation is the best one, where\nthe straight explanation extrapolates the observed frequency to the\nwider population. For example, given that a coin has some objective\nchance of landing heads, the best explanation of the fact that \\(m/n\\)\nheads have been so far observed, is that the objective chance of the\ncoin landing heads is \\(m/n\\). And this objective chance determines\nwhat happens not only in observed cases but also in unobserved\ncases. \nThe Nomological-Explanatory solution relies on taking IBE as a\nrational, a priori form of inference which is distinct from\ninductive inferences like inference I. However, one might\nalternatively view inductive inferences as a special case of IBE\n(Harman 1968), or take IBE to be merely an alternative way of\ncharacterizing inductive inference (Henderson 2014). If either of\nthese views is right, IBE does not have the necessary independence\nfrom inductive inference to provide a non-circular justification of\nit. \nOne may also object to the Nomological-Explanatory approach on the\ngrounds that regularities do not necessarily require an explanation in\nterms of necessary connections or robust metaphysical laws. The\nviability of the approach also depends on the tenability of a\nnon-Humean conception of laws. There have been several serious\nattempts to develop such an account (Armstrong 1983; Tooley 1977;\nDretske 1977), but also much criticism (see J. Carroll 2016). \nAnother critical objection is that the Nomological-Explanatory\nsolution simply begs the question, even if it is taken to be\nlegitimate to make use of IBE in the justification of induction. In\nthe first step of the argument we infer to a law or regularity which\nextends beyond the spatio-temporal region in which observations have\nbeen thus far made, in order to predict what will happen in the\nfuture. But why could a law that only applies to the observed\nspatio-temporal region not be an equally good explanation? The main\nreply seems to be that we can see a priori that laws with\ntemporal or spatial restrictions would be less good explanations.\nFoster argues that the reason is that this would introduce more\nmysteries:  \nFor it seems to me that a law whose scope is restricted to some\nparticular period is more mysterious, inherently more puzzling, than\none which is temporally universal. (Foster 2004) \nAnother way in which one can try to construct an a priori\nargument that the premises of an inductive inference make its\nconclusion probable, is to make use of the formalism of probability\ntheory itself. At the time Hume wrote, probabilities were used to\nanalyze games of chance. And in general, they were used to address the\nproblem of what we would expect to see, given that a certain cause was\nknown to be operative. This is the so-called problem of “direct\ninference”. However, the problem of induction concerns the\n“inverse” problem of determining the cause or general\nhypothesis, given particular observations. \nOne of the first and most important methods for tackling the\n“inverse” problem using probabilities was developed by\nThomas Bayes. Bayes’s essay containing the main results was\npublished after his death in 1764 (Bayes 1764). However, it is\npossible that the work was done significantly earlier and was in fact\nwritten in direct response to the publication of Hume’s Enquiry\nin 1748 (see Zabell 1989: 290–93, for discussion of what is\nknown about the history). \nWe will illustrate the Bayesian method using the problem of drawing\nballs from an urn. Suppose that we have an urn which contains white\nand black balls in an unknown proportion. We draw a sample of balls\nfrom the urn by removing a ball, noting its color, and then putting it\nback before drawing again. \nConsider first the problem of direct inference. Given the proportion\nof white balls in the urn, what is the probability of various outcomes\nfor a sample of observations of a given size? Suppose the proportion\nof white balls in the urn is \\(\\theta = 0.6\\). The probability of\ndrawing one white ball in a sample of one is then \\(p(W; \\theta = 0.6)\n= 0.6\\). We can also compute the probability for other outcomes, such\nas drawing two white balls in a sample of two, using the rules of the\nprobability calculus (see section 1 of Hájek 2011). Generally,\nthe probability that \\(n_w\\) white balls are drawn in a sample of size\nN, is given by the binomial distribution: \nThis is a specific example of a “sampling distribution”,\n\\(p(E\\mid H)\\), which gives the probability of certain evidence\nE in a sample, on the assumption that a certain hypothesis\nH is true. Calculation of the sampling distribution can in\ngeneral be done a priori, given the rules of the probability\ncalculus. \nHowever, the problem of induction is the inverse problem. We want to\ninfer not what the sample will be like, with a known hypothesis,\nrather we want to infer a hypothesis about the general situation or\npopulation, based on the observation of a limited sample. The\nprobabilities of the candidate hypotheses can then be used to inform\npredictions about further observations. In the case of the urn, for\nexample, we want to know what the observation of a particular sample\nfrequency of white balls, \\(\\frac{n_w}{N}\\), tells us about\n\\(\\theta\\), the proportion of white balls in the urn. \nThe idea of the Bayesian approach is to assign probabilities not only\nto the events which constitute evidence, but also to hypotheses. One\nstarts with a “prior probability” distribution over the\nrelevant hypotheses \\(p(H)\\). On learning some evidence E, the\nBayesian updates the prior \\(p(H)\\) to the conditional probability\n\\(p(H\\mid E)\\). This update rule is called the “rule of\nconditionalisation”. The conditional probability \\(p(H\\mid E)\\)\nis known as the “posterior probability”, and is calculated\nusing Bayes’ rule: \nHere the sampling distribution can be taken to be a conditional\nprobability \\(p(E\\mid H)\\), which is known as the\n“likelihood” of the hypothesis H on evidence\nE. \nOne can then go on to compute the predictive distribution for as yet\nunobserved data \\(E'\\), given observations E. The predictive\ndistribution in a Bayesian approach is given by \nwhere the sum becomes an integral in cases where H is a\ncontinuous variable. \nFor the urn example, we can compute the posterior probability\n\\(p(\\theta\\mid n_w)\\) using Bayes’ rule, and the likelihood\ngiven by the binomial distribution above. In order to do so, we also\nneed to assign a prior probability distribution to the parameter\n\\(\\theta\\). One natural choice, which was made early on by Bayes\nhimself and by Laplace, is to put a uniform prior over the parameter\n\\(\\theta\\). Bayes’ own rationale for this choice was that then\nif you work out the probability of each value for the number of whites\nin the sample based only on the prior, before any data is observed,\nall those probabilities are equal. Laplace had a different\njustification, based on the Principle of Indifference. This principle\nstates that if you don’t have any reason to favor one hypothesis\nover another, you should assign them all equal probabilities. \nWith the choice of uniform prior, the posterior probability and\npredictive distribution can be calculated. It turns out that the\nprobability that the next ball will be white, given that \\(n_w\\) of\nN draws were white, is given by \nThis is Laplace’s famous “rule of succession”\n(1814). Suppose on the basis of observing 90 white balls out of 100,\nwe calculate by the rule of succession that the probability of the\nnext ball being white is \\(91/102=0.89\\). It is quite conceivable that\nthe next ball might be black. Even in the case, where all 100 balls\nhave been white, so that the probability of the next ball being white\nis 0.99, there is still a small probability that the next ball is not\nwhite. What the probabilistic reasoning supplies then is not an\nargument to the conclusion that the next ball will be a certain color,\nbut an argument to the conclusion that certain future observations are\nvery likely given what has been observed in the past. \nOverall, the Bayes-Laplace argument in the urn case provides an\nexample of how probabilistic reasoning can take us from evidence about\nobservations in the past to a prediction for how likely certain future\nobservations are. The question is what kind of solution, if any, this\ntype of calculation provides to the problem of induction. At first\nsight, since it is just a mathematical calculation, it looks as though\nit does indeed provide an a priori argument from the premises\nof an inductive inference to the proposition that a certain conclusion\nis probable. \nHowever, in order to establish this definitively, one needs to argue\nthat all the components and assumptions of the argument are a\npriori and this requires further examination of at least three\nimportant issues. \nFirst, the Bayes-Laplace argument relies on the rules of the\nprobability calculus. What is the status of these rules? Does\nfollowing them amount to a priori reasoning? The answer to\nthis depends in part on how probability itself is interpreted. Broadly\nspeaking, there are prominent interpretations of probability according\nto which the rules plausibly have a priori status and could\nform the basis of a demonstrative argument. These include the\nclassical interpretation originally developed by Laplace (1814), the\nlogical interpretation which had its heyday in the work of Keynes\n(1921), Johnson (1921), Jeffreys (1939), and Carnap (1950), and the\nsubjectivist interpretation of Ramsey (1926), Savage (1954), and de\nFinetti (1964). Attempts to argue for a probabilistic a\npriori solution to the problem of induction have been primarily\nassociated with these interpretations. \nSecondly, in the case of the urn, the Bayes-Laplace argument is based\non a particular probabilistic model—the binomial model. This\ninvolves the assumption that there is a parameter describing an\nunknown proportion \\(\\theta\\) of balls in the urn, and that the data\namounts to independent draws from a distribution over that parameter.\nWhat is the basis of these assumptions? Do they generalize to other\ncases beyond the actual urn case—i.e., can we see observations\nin general as analogous to draws from an “Urn of Nature”?\nThere has been a persistent worry that these types of assumptions,\nwhile reasonable when applied to the case of drawing balls from an\nurn, will not hold for other cases of inductive inference. Thus, the\nprobabilistic solution to the problem of induction might be of\nrelatively limited scope. At the least, there are some assumptions\ngoing into the choice of model here that need to be made explicit. \nThirdly, the Bayes-Laplace argument relies on a particular choice of\nprior probability distribution. What is the status of this assignment,\nand can it be based on a priori principles? Historically, the\nBayes-Laplace choice of a uniform prior, as well as the whole concept\nof classical probability, relied on the Principle of Indifference.\nThis principle has been regarded by many as an a priori\nprinciple. However, it has also been subjected to much criticism on\nthe grounds that it can give rise to inconsistent probability\nassignments (Bertrand 1888; Borel 1909; Keynes 1921). Such\ninconsistencies are produced by there being more than one way to carve\nup the space of alternatives, and different choices give rise to\nconflicting probability assignments. One attempt to rescue the\nPrinciple of Indifference has been to appeal to explanationism, and\nargue that the principle should be applied only to the carving of the\nspace at “the most explanatorily basic level”, where this\nlevel is identified according to an a priori notion of\nexplanatory priority (Huemer 2009). \nThe quest for an a priori argument for the assignment of the\nprior has been largely abandoned. For many, the subjectivist\nfoundations developed by Ramsey, de Finetti and Savage provide a more\nsatisfactory basis for understanding probability. From this point of\nview, it is a mistake to try to introduce any further a\npriori constraints on the probabilities beyond those dictated by\nthe probability rules themselves. Rather the assignment of priors may\nreflect personal opinions or background knowledge, and no prior is\na priori an unreasonable choice. \nSo far, we have considered probabilistic arguments which place\nprobabilities over hypotheses in a hypothesis space as well as\nobservations. There is also a tradition of attempts to determine what\nprobability distributions we should have, given certain observations,\nfrom the starting point of a joint probability distribution over all\nthe observable variables. One may then postulate axioms directly on\nthis distribution over observables, and examine the consequences for\nthe predictive distribution. Much of the development of inductive\nlogic, including the influential programme by Carnap, proceeded in\nthis manner (Carnap 1950, 1952). \nThis approach helps to clarify the role of the assumptions behind\nprobabilistic models. One fundamental assumption that one can make\nabout the observations is that they are “exchangeable”.\nThis means that the joint distribution of the random variables is\ninvariant under permutations. Informally, this means that the order of\nthe observations does not affect the probability. For instance, in the\nurn case, this would mean that drawing first a white ball and then a\nblack ball is just as probable as first drawing a black and then a\nwhite. De Finetti proved a general representation theorem that if the\njoint probability distribution of an infinite sequence of random\nvariables is assumed to be exchangeable, then it can be written as a\nmixture of distribution functions from each of which the data behave\nas if they are independent random draws (de Finetti 1964). In the case\nof the urn example, the theorem shows that it is as if the\ndata are independent random draws from a binomial distribution over a\nparameter \\(\\theta\\), which itself has a prior probability\ndistribution. \nThe assumption of exchangeability may be seen as a natural\nformalization of Hume’s assumption that the past resembles the\nfuture. This is intuitive because assuming exchangeability means\nthinking that the order of observations, both past and future, does\nnot matter to the probability assignments. \nHowever, the development of the programme of inductive logic revealed\nthat many generalizations are possible. For example, Johnson proposed\nto assume an axiom he called the “sufficientness\npostulate”. This states that outcomes can be of a number of\ndifferent types, and that the conditional probability that the next\noutcome is of type i depends only on the number of previous\ntrials and the number of previous outcomes of type i (Johnson\n1932). Assuming the sufficientness postulate for three or more types\ngives rise to a general predictive distribution corresponding to\nCarnap’s “continuum of inductive methods” (Carnap\n1952). This predictive distribution takes the form: \nfor some positive number k. This reduces to Laplace’s\nrule of succession when \\(t=2\\) and \\(k=1\\). \nGeneralizations of the notion of exchangeability, such as\n“partial exchangeability” and “Markov\nexchangeability”, have been explored, and these may be thought\nof as forms of symmetry assumption (Zabell 1988; Skyrms 2012). As less\nrestrictive axioms on the probabilities for observables are assumed,\nthe result is that there is no longer a unique result for the\nprobability of a prediction, but rather a whole class of possible\nprobabilities, mapped out by a generalized rule of succession such as\nthe above. Therefore, in this tradition as in the Bayes-Laplace\napproach, we have moved away from producing an argument which produces\na unique a priori probabilistic answer to Hume’s problem. \nOne might think then that the assignment of the prior, or the relevant\ncorresponding postulates on the observable probability distribution,\nis precisely where empirical assumptions enter into inductive\ninferences. The probabilistic calculations are empirical arguments,\nrather than a priori ones. If this is correct, then the\nprobabilistic framework has not in the end provided an a\npriori solution to the problem of induction, but it has rather\nallowed us to clarify what could be meant by Hume’s claim that\ninductive inferences rely on the Uniformity Principle. \nSome think that although the problem of induction is not solved, there\nis in some sense a partial solution, which has been called a\n“logical solution”. Howson, for example, argues that\n“Inductive reasoning is justified to the extent that it is\nsound, given appropriate premises” (Howson 2000: 239, his\nemphasis). According to this view, there is no getting away from an\nempirical premise for inductive inferences, but we might still think\nof Bayesian conditioning as functioning like a kind of logic or\n“consistency constraint” which “generates\npredictions from the assumptions and observations together”\n(Romeijn 2004: 360). Once we have an empirical assumption,\ninstantiated in the prior probability, and the observations, Bayesian\nconditioning tells us what the resulting predictive probability\ndistribution should be. \nAn alternative attempt to use probabilistic reasoning to produce an\na priori justification for inductive inferences is the\nso-called “combinatorial” solution. This was first put\nforward by Donald C. Williams (1947) and later developed by David\nStove (1986). \nLike the Bayes-Laplace argument, the solution relies heavily on the\nidea that straightforward a priori calculations can be done\nin a “direct inference” from population to sample. As we\nhave seen, given a certain population frequency, the probability of\ngetting different frequencies in a sample can be calculated\nstraightforwardly based on the rules of the probability calculus. The\nBayes-Laplace argument relied on inverting the probability\ndistribution using Bayes’ rule to get from the sampling\ndistribution to the posterior distribution. Williams instead proposes\nthat the inverse inference may be based on a certain logical\nsyllogism: the proportional (or statistical) syllogism. \nThe proportional, or statistical syllogism, is the following:  \nTherefore, a is P, with probability \\(m/n\\). \nFor example, if 90% of rabbits in a population are white and we\nobserve a rabbit a, then the proportional syllogism says that\nwe infer that a is white with a probability of 90%. Williams\nargues that the proportional syllogism is a non-deductive logical\nsyllogism, which effectively interpolates between the syllogism for\nentailment \nTherefore, a is P. \nAnd the syllogism for contradiction \nTherefore, a is not P. \nThis syllogism can be combined with an observation about the behavior\nof increasingly large samples. From calculations of the sampling\ndistribution, it can be shown that as the sample size increases, the\nprobability that the sample frequency is in a range which closely\napproximates the population frequency also increases. In fact,\nBernoulli’s law of large numbers states that the probability\nthat the sample frequency approximates the population frequency tends\nto one as the sample size goes to infinity. Williams argues that such\nresults support a “general over-all premise, common to all\ninductions, that samples ‘match’ their populations”\n(Williams 1947: 78). \nWe can then apply the proportional syllogism to samples from a\npopulation, to get the following argument: \nTherefore, S matches its population, with high probability. \nThis is an instance of the proportional syllogism, and it uses the\ngeneral result about samples matching populations as the first major\npremise. \nThe next step is to argue that if we observe that the sample contains\na proportion of \\(m/n\\) Fs, then we can conclude that since\nthis sample with high probability matches its population, the\npopulation, with high probability, has a population frequency that\napproximates the sample frequency \\(m/n\\). Both Williams and Stove\nclaim that this amounts to a logical a priori solution to the\nproblem of induction. \nA number of authors have expressed the view that the Williams-Stove\nargument is only valid if the sample S is drawn randomly from\nthe population of possible samples—i.e., that any sample is as\nlikely to be drawn as any other (Brown 1987; Will 1948; Giaquinto\n1987). Sometimes this is presented as an objection to the application\nof the proportional syllogism. The claim is that the proportional\nsyllogism is only valid if a is drawn randomly from the\npopulation of Ms. However, the response has been that there is\nno need to know that the sample is randomly drawn in order to apply\nthe syllogism (Maher 1996; Campbell 2001; Campbell & Franklin\n2004). Certainly if you have reason to think that your sampling\nprocedure is more likely to draw certain individuals than\nothers—for example, if you know that you are in a certain\nlocation where there are more of a certain type—then you should\nnot apply the proportional syllogism. But if you have no such reasons,\nthe defenders claim, it is quite rational to apply it. Certainly it is\nalways possible that you draw an unrepresentative sample—meaning\none of the few samples in which the sample frequency does not match\nthe population frequency—but this is why the conclusion is only\nprobable and not certain. \nThe more problematic step in the argument is the final step, which\ntakes us from the claim that samples match their populations with high\nprobability to the claim that having seen a particular sample\nfrequency, the population from which the sample is drawn has frequency\nclose to the sample frequency with high probability. The problem here\nis a subtle shift in what is meant by “high probability”,\nwhich has formed the basis of a common misreading of\nBernouilli’s theorem. Hacking (1975: 156–59) puts the\npoint in the following terms. Bernouilli’s theorem licenses the\nclaim that much more often than not, a small interval around the\nsample frequency will include the true population frequency. In other\nwords, it is highly probable in the sense of “usually\nright” to say that the sample matches its population. But this\ndoes not imply that the proposition that a small interval around the\nsample will contain the true population frequency is highly probable\nin the sense of “credible on each occasion of use”. This\nwould mean that for any given sample, it is highly credible that the\nsample matches its population. It is quite compatible with the claim\nthat it is “usually right” that the sample matches its\npopulation to say that there are some samples which do not match their\npopulations at all. Thus one cannot conclude from Bernouilli’s\ntheorem that for any given sample frequency, we should assign high\nprobability to the proposition that a small interval around the sample\nfrequency will contain the true population frequency. But this is\nexactly the slide that Williams makes in the final step of his\nargument. Maher (1996) argues in a similar fashion that the last step\nof the Williams-Stove argument is fallacious. In fact, if one wants to\ndraw conclusions about the probability of the population frequency\ngiven the sample frequency, the proper way to do so is by using the\nBayesian method described in the previous section. But, as we there\nsaw, this requires the assignment of prior probabilities, and this\nexplains why many people have thought that the combinatorial solution\nsomehow illicitly presupposed an assumption like the principle of\nindifference. The Williams-Stove argument does not in fact give us an\nalternative way of inverting the probabilities which somehow bypasses\nall the issues that Bayesians have faced. \nSo far we have considered ways in which the first horn of Hume’s\ndilemma might be tackled. But it is of course also possible to take on\nthe second horn instead. \nOne may argue that a probable argument would not, despite what Hume\nsays, be circular in a problematic way (we consider responses of this\nkind in\n section 4.1).\n Or, one might attempt to argue that probable arguments are not\ncircular at all\n (section 4.2). \nOne way to tackle the second horn of Hume’s dilemma is to reject\n premise P6,\n which rules out circular arguments. Some have argued that certain\nkinds of circular arguments would provide an acceptable justification\nfor the inductive inference. Since the justification would then itself\nbe an inductive one, this approach is often referred to as an\n“inductive justification of induction”. \nFirst we should examine how exactly the Humean circularity supposedly\narises. Take the simple case of enumerative inductive inference that\nfollows the following pattern (X): \nMost observed Fs have been Gs \nTherefore: Most Fs are Gs. \nHume claims that such arguments presuppose the Uniformity Principle\n(UP). According to premises\n P7\n and\n P8,\n this supposition also needs to be supported by an argument in order\nthat the inductive inference be justified. A natural idea is that we\ncan argue for the Uniformity Principle on the grounds that “it\nworks”. We know that it works, because past instances of\narguments which relied upon it were found to be successful. This alone\nhowever is not sufficient unless we have reason to think that such\narguments will also be successful in the future. That claim must\nitself be supported by an inductive argument (S): \nMost arguments of form X that rely on UP have succeeded in the\npast. \nTherefore, most arguments of form X that rely on UP\nsucceed. \nBut this argument itself depends on the UP, which is the very\nsupposition which we were trying to justify. \nAs we have seen in\n section 2,\n some reject Hume’s claim that all inductive inferences\npresuppose the UP. However, the argument that basing the justification\nof the inductive inference on a probable argument would result in\ncircularity need not rely on this claim. The circularity concern can\nbe framed more generally. If argument S relies on\nsomething which is already presupposed in inference X,\nthen argument S cannot be used to justify inference X.\nThe question though is what precisely the something is. \nSome authors have argued that in fact S does not rely on any\npremise or even presupposition that would require us to already know\nthe conclusion of X. S is then not a “premise\ncircular” argument. Rather, they claim, it is\n“rule-circular”—it relies on a rule of inference in\norder to reach the conclusion that that very rule is reliable. Suppose\nwe adopt the rule R which says that when it is observed that\nmost Fs are Gs, we should infer that most Fs are\nGs. Then inference X relies on rule R. We want to\nshow that rule R is reliable. We could appeal to the fact that\nR worked in the past, and so, by an inductive argument, it will\nalso work in the future. Call this argument S*: \nMost inferences following rule R have been successful \nTherefore, most inferences following R are successful. \nSince this argument itself uses rule R, using it to establish\nthat R is reliable is rule-circular. \nSome authors have then argued that although premise-circularity is\nvicious, rule-circularity is not (Cleve 1984; Papineau 1992). One\nreason for thinking rule-circularity is not vicious would be if it is\nnot necessary to know or even justifiably believe that rule R\nis reliable in order to move to a justified conclusion using the rule.\nThis is a claim made by externalists about justification (Cleve 1984).\nThey say that as long as R is in fact reliable, one\ncan form a justified belief in the conclusion of an argument relying\non R, as long as one has justified belief in the premises. \nIf one is not persuaded by the externalist claim, one might attempt to\nargue that rule circularity is benign in a different fashion. For\nexample, the requirement that a rule be shown to be reliable without\nany rule-circularity might appear unreasonable when the rule is of a\nvery fundamental nature. As Lange puts it: \nIt might be suggested that although a circular argument is ordinarily\nunable to justify its conclusion, a circular argument is acceptable in\nthe case of justifying a fundamental form of reasoning. After all,\nthere is nowhere more basic to turn, so all that we can reasonably\ndemand of a fundamental form of reasoning is that it endorse itself.\n(Lange 2011: 56) \nProponents of this point of view point out that even deductive\ninference cannot be justified deductively. Consider Lewis\nCarroll’s dialogue between Achilles and the Tortoise (Carroll\n1895). Achilles is arguing with a Tortoise who refuses to perform\nmodus ponens. The Tortoise accepts the premise that p,\nand the premise that p implies q but he will not accept\nq. How can Achilles convince him? He manages to persuade him to\naccept another premise, namely “if p and p implies\nq, then q”. But the Tortoise is still not prepared\nto infer to q. Achilles goes on adding more premises of the\nsame kind, but to no avail. It appears then that modus ponens\ncannot be justified to someone who is not already prepared to use that\nrule. \nIt might seem odd if premise circularity were vicious, and rule\ncircularity were not, given that there appears to be an easy\ninterchange between rules and premises. After all, a rule can always,\nas in the Lewis Carroll story, be added as a premise to the argument.\nBut what the Carroll story also appears to indicate is that there is\nindeed a fundamental difference between being prepared to accept a\npremise stating a rule (the Tortoise is happy to do this), and being\nprepared to use that rule (this is what the Tortoise refuses to\ndo). \nSuppose that we grant that an inductive argument such as S (or\nS*) can support an inductive inference X without vicious\ncircularity. Still, a possible objection is that the argument simply\ndoes not provide a full justification of X. After all, less\nsane inference rules such as counterinduction can support themselves\nin a similar fashion. The counterinductive rule is CI: \nMost observed As are Bs. \nTherefore, it is not the case that most As are Bs. \nConsider then the following argument CI*: \nMost CI arguments have been unsuccessful \nTherefore, it is not the case that most CI arguments are unsuccessful,\ni.e., many CI arguments are successful. \nThis argument therefore establishes the reliability of CI in a\nrule-circular fashion (see Salmon 1963). \nArgument S can be used to support inference X, but only\nfor someone who is already prepared to infer inductively by using\nS. It cannot convince a skeptic who is not prepared to rely\nupon that rule in the first place. One might think then that the\nargument is simply not achieving very much. \nThe response to these concerns is that, as Papineau puts it, the\nargument is “not supposed to do very much”\n(Papineau 1992: 18). The fact that a counterinductivist counterpart of\nthe argument exists is true, but irrelevant. It is conceded that the\nargument cannot persuade either a counterinductivist, or a skeptic.\nNonetheless, proponents of the inductive justification maintain that\nthere is still some added value in showing that inductive inferences\nare reliable, even when we already accept that there is nothing\nproblematic about them. The inductive justification of induction\nprovides a kind of important consistency check on our existing\nbeliefs. \nIt is possible to go even further in an attempt to dismantle the\nHumean circularity. Maybe inductive inferences do not even have a rule\nin common. What if every inductive inference is essentially unique?\nOkasha, for example, argues that Hume’s circularity problem can\nbe evaded if there are “no rules” behind induction (Okasha\n2005a,b). Norton puts forward the similar idea that all inductive\ninferences are material, and have nothing formal in common (Norton\n2003). \nProponents of such views have attacked Hume’s claim that there\nis a UP on which all inductive inferences are based. There have long\nbeen complaints about the vagueness of the Uniformity Principle\n(Salmon 1953). The future only resembles the past in some respects,\nbut not others. Suppose that on all my birthdays so far, I have been\nunder 40 years old. This does not give me a reason to expect that I\nwill be under 40 years old on my next birthday. There seems then to be\na major lacuna in Hume’s account. He might have explained or\ndescribed how we draw an inductive inference, on the assumption that\nit is one we can draw. But he leaves untouched the question\nof how we distinguish between cases where we extrapolate a regularity\nlegitimately, regarding it as a law, and cases where we do not. \nNelson Goodman is often seen as having made this point in a\nparticularly vivid form with his “new riddle of induction”\n(Goodman 1955: 59-83). Suppose we define a predicate “grue” in\nthe following way. An object is “grue” when it is green if\nobserved before time t and blue otherwise. Goodman considers a\nthought experiment in which we observe a bunch of green emeralds\nbefore time t. We could describe our results by saying all the\nobserved emeralds are green. Using a simple enumerative inductive\nschema, we could infer from the result that all observed emeralds are\ngreen, that all emeralds are green. But equally, we could describe the\nsame results by saying that all observed emeralds are grue. Then using\nthe same schema, we could infer from the result that all observed\nemeralds are grue, that all emeralds are grue. In the first case, we\nexpect an emerald observed after time t to be green, whereas in\nthe second, we expect it to be blue. Thus the two predictions are\nincompatible. Goodman claims that what Hume omitted to do was to give\nany explanation for why we project predicates like\n“green”, but not predicates like “grue”. This\nis the “new riddle”, which is often taken to be a further\nproblem of induction that Hume did not address. \nOne moral that could be taken from Goodman is that there is not one\ngeneral Uniformity Principle that all probable arguments rely upon\n(Sober 1988; Norton 2003; Okasha 2001, 2005a,b). Rather each inductive\ninference presupposes some more specific empirical presupposition. A\nparticular inductive inference depends on some specific way in which\nthe future resembles the past. It can then be justified by another\ninductive inference which depends on some quite different empirical\nclaim. This will in turn need to be justified—by yet another\ninductive inference. The nature of Hume’s problem in the second\nhorn is thus transformed. There is no circularity. Rather there is a\nregress of inductive justifications, each relying on their own\nempirical presuppositions (Sober 1988; Norton 2003; Okasha 2001,\n2005a,b). \nOne way to put this point is to say that Hume’s argument rests\non a quantifier shift fallacy (Sober 1988; Okasha 2005a). Hume says\nthat there exists a general presupposition for all inductive\ninferences, whereas he should have said that for each inductive\ninference, there is some presupposition. Different inductive\ninferences then rest on different empirical presuppositions, and the\nproblem of circularity is evaded. \nWhat will then be the consequence of supposing that Hume’s\nproblem should indeed have been a regress, rather than a circularity?\nHere different opinions are possible. On the one hand, one might think\nthat a regress still leads to a skeptical conclusion. So although the\nexact form in which Hume stated his problem was not correct, the\nconclusion is not substantially different (Sober 1988). Another\npossibility is that the transformation mitigates or even removes the\nskeptical problem. For example, Norton argues that the upshot is a\ndissolution of the problem of induction, since the regress of\njustifications benignly terminates (Norton 2003). And Okasha more\nmildly suggests that even if the regress is infinite, “Perhaps\ninfinite regresses are less bad than vicious circles after all”\n(Okasha 2005b: 253). \nAny dissolution of Hume’s circularity does not depend only on\narguing that the UP should be replaced by empirical presuppositions\nwhich are specific to each inductive inference. It is also necessary\nto establish that inductive inferences share no common\nrules—otherwise there will still be at least some\nrule-circularity. Okasha suggests that the Bayesian model of\nbelief-updating is an illustration how induction can be characterized\nin a rule-free way, but this is problematic, since in this model all\ninductive inferences still share the common rule of Bayesian\nconditionalisation. Norton’s material theory of induction more\ngenuinely promises a rule-free characterization of induction, but it\nis not clear whether it really can avoid any role for general rules\n(Achinstein 2010; Worrall 2010). \nHume is usually read as delivering a negative verdict on the\npossibility of justifying inference I, via a premise such as\n P8.\n There are however some who question whether Hume is best interpreted\nas drawing a conclusion about justification of inference I at\nall (we will discuss these interpretations in\n section 5.1).\n There are also those who question in different ways whether\n premise P8\n really does give a valid necessary condition for justification of\ninference I (sections\n 5.2\n and\n 5.3). \nSome scholars have denied that Hume should be read as invoking a\npremise such\n premise P8\n at all. The reason, they claim, is that he was not aiming for an\nexplicitly normative conclusion about justification such as\n C5.\n Hume certainly is seeking a “chain of reasoning” from the\npremises of the inductive inference to the conclusion, and he thinks\nthat an argument for the UP is necessary to complete the chain.\nHowever, one could think that there is no further premise regarding\njustification, and so the conclusion of his argument is simply\n C4:\n there is no chain of reasoning from the premises to the conclusion of\nan inductive inference. Hume could then be, as Don Garrett and David\nOwen have argued, advancing a “thesis in cognitive\npsychology”, rather than making a normative claim about\njustification (Owen 1999; Garrett 2002). The thesis is about the\nnature of the cognitive process underlying the inference. According to\nGarrett, the main upshot of Hume’s argument is that there can be\nno reasoning process that establishes the UP. For Owen, the message is\nthat the inference is not drawn through a chain of ideas connected by\nmediating links, as would be characteristic of the faculty of\nreason. \nThere are also interpreters who have argued that Hume is merely trying\nto exclude a specific kind of justification of induction, based on a\nconception of reason predominant among rationalists of his time,\nrather than a justification in general (Beauchamp & Rosenberg\n1981; Baier 2009). In particular, it has been claimed that it is\n“an attempt to refute the rationalist belief that at least some\ninductive arguments are demonstrative” (Beauchamp &\nRosenberg 1981: xviii). Under this interpretation,\n premise P8\n should be modified to read something like: \nSuch interpretations do however struggle with the fact that\nHume’s argument is explicitly a two-pronged attack, which\nconcerns not just demonstrative arguments, but also probable\narguments. \nThe question of how expansive a normative conclusion to attribute to\nHume is a complex one. It depends in part on the interpretation of\nHume’s own solution to his problem. As we saw in\n section 1,\n Hume attributes the basis of inductive inference to principles of the\nimagination in the Treatise, and in the Enquiry to\n“custom”, “habit”, conceived as a kind of\nnatural instinct. The question is then whether this alternative\nprovides any kind of justification for the inference, even if not one\nbased on reason. On the face of it, it looks as though Hume is\nsuggesting that inductive inferences proceed on an entirely arational\nbasis. He clearly does not think that they do not succeed in producing good\noutcomes. In fact, Hume even suggests that this operation of the mind\nmay even be less “liable to error and mistake” than if it\nwere entrusted to “the fallacious deductions of our reason,\nwhich is slow in its operations” (E. 5.2.22). It is also not\nclear that he sees the workings of the imagination as completely\ndevoid of rationality. For one thing, Hume talks about the imagination\nas governed by principles. Later in the Treatise, he\neven gives “rules” and “logic” for\ncharacterizing what should count as a good causal inference (T.\n1.3.15). He also clearly sees it as possible to distinguish between\nbetter forms of such “reasoning”, as he continues to call\nit. Thus, there may be grounds to argue that Hume was not trying to\nargue that inductive inferences have no rational foundation\nwhatsoever, but merely that they do not have the specific type of\nrational foundation which is rooted in the faculty of Reason. \nAll this indicates that there is room for debate over the intended\nscope of Hume’s own conclusion. And thus there is also room for\ndebate over exactly what form a premise (such as\n premise P8)\n that connects the rest of his argument to a normative conclusion\nshould take. No matter who is right about this however, the fact\nremains that Hume has throughout history been predominantly read as\npresenting an argument for inductive skepticism. \nEven if one does attribute a normative conclusion to Hume, one may\nquestion his argument by asking whether\n premise P8\n is true. This can prompt general reflection on what is needed for\njustification of an inference in the first place, and what Hume is\neven asking for. \nFor example, Wittgenstein raised doubts over whether it is even\nmeaningful to ask for the grounds for inductive inferences. \nIf anyone said that information about the past could not convince him\nthat something would happen in the future, I should not understand\nhim. One might ask him: what do you expect to be told, then? What sort\nof information do you call a ground for such a belief? … If\nthese are not grounds, then what are grounds?—If you say these\nare not grounds, then you must surely be able to state what must be\nthe case for us to have the right to say that there are grounds for\nour assumption…. (Wittgenstein 1953: 481) \nOne might not, for instance, think that there even needs to be a chain\nof reasoning in which each step or presupposition is supported by an\nargument. Wittgenstein took it that there are some principles so\nfundamental that they do not require support from any further\nargument. They are the “hinges” on which enquiry\nturns. \nOut of Wittgenstein’s ideas has developed a general notion of\n“entitlement”, which is a kind of rational warrant to hold\ncertain propositions which does not come with the same requirements as\n“justification”. Entitlement provides epistemic rights to\nhold a proposition, without responsibilities to base the belief in it\non an argument. Crispin Wright (2004) has argued that there are\ncertain principles, including the Uniformity Principle, that we are\nentitled in this sense to hold. \nSome philosophers have set themselves the task of determining a set or\nsets of postulates which form a plausible basis for inductive\ninferences. Bertrand Russell, for example, argued that five postulates\nlay at the root of inductive reasoning (Russell 1948). Arthur Burks,\non the other hand, proposed that the set of postulates is not unique,\nbut there may be multiple sets of postulates corresponding to\ndifferent inductive methods (Burks 1953, 1955). \nThe main objection to all these views is that they do not really solve\nthe problem of induction in a way that adequately secures the pillars\non which inductive inference stands. As Salmon puts it,\n“admission of unjustified and unjustifiable postulates to deal\nwith the problem is tantamount to making scientific method a matter of\nfaith” (Salmon 1966: 48). \nRather than allowing undefended empirical postulates to give normative\nsupport to an inductive inference, one could instead argue for a\ncompletely different conception of what is involved in justification.\nLike Wittgenstein, later ordinary language philosophers, notably P.F.\nStrawson, also questioned what exactly it means to ask for a\njustification of inductive inferences (Strawson 1952). This has become\nknown as the “Ordinary language dissolution” of the\nproblem of induction. \nStrawson points out that it could be meaningful to ask for a deductive\njustification of inductive inferences. But it is not clear that this\nis helpful since this is effectively “a demand that induction\nshall be shown to be really a kind of deduction” (Strawson 1952:\n230). Rather, Strawson says, when we ask about whether a particular\ninductive inference is justified, we are typically judging whether it\nconforms to our usual inductive standards. Suppose, he says, someone\nhas formed the belief by inductive inference that All f’s\nare g. Strawson says that if that person is asked for their\ngrounds or reasons for holding that belief, \nI think it would be felt to be a satisfactory answer if he replied:\n“Well, in all my wide and varied experience I’ve come\nacross innumerable cases of f and never a case of f\nwhich wasn’t a case of g”. In saying this, he is\nclearly claiming to have inductive support,\ninductive evidence, of a certain kind, for his belief.\n(Strawson 1952) \nThat is just because inductive support, as it is usually understood,\nsimply consists of having observed many positive instances in a wide\nvariety of conditions. \nIn effect, this approach denies that producing a chain of reasoning is\na necessary condition for justification. Rather, an inductive\ninference is justified if it conforms to the usual standards of\ninductive justification. But, is there more to it? Might we not ask\nwhat reason we have to rely on those inductive standards? \nIt surely makes sense to ask whether a particular inductive inference\nis justified. But the answer to that is fairly straightforward.\nSometimes people have enough evidence for their conclusions and\nsometimes they do not. Does it also make sense to ask about whether\ninductive procedures generally are justified? Strawson draws the\nanalogy between asking whether a particular act is legal. We may\nanswer such a question, he says, by referring to the law of the land.\n \nBut it makes no sense to inquire in general whether the law of the\nland, the legal system as a whole, is or is not legal. For to what\nlegal standards are we appealing? (Strawson 1952: 257) \nAccording to Strawson, \nIt is an analytic proposition that it is reasonable to have a degree\nof belief in a statement which is proportional to the strength of the\nevidence in its favour; and it is an analytic proposition, though not\na proposition of mathematics, that, other things being equal, the\nevidence for a generalisation is strong in proportion as the number of\nfavourable instances, and the variety of circumstances in which they\nhave been found, is great. So to ask whether it is reasonable to place\nreliance on inductive procedures is like asking whether it is\nreasonable to proportion the degree of one’s convictions to the\nstrength of the evidence. Doing this is what “being\nreasonable” means in such a context. (Strawson 1952:\n256–57) \nThus, according to this point of view, there is no further question to\nask about whether it is reasonable to rely on inductive\ninferences. \nThe ordinary language philosophers do not explicitly argue against\nHume’s\n premise P8.\n But effectively what they are doing is offering a whole different\nstory about what it would mean to be justified in believing the\nconclusion of inductive inferences. What is needed is just conformity\nto inductive standards, and there is no real meaning to asking for any\nfurther justification for those. \nThe main objection to this view is that conformity to the usual\nstandards is insufficient to provide the needed justification. What we\nneed to know is whether belief in the conclusion of an inductive\ninference is “epistemically reasonable or justified in the sense\nthat …there is reason to think that it is likely to be\ntrue” (BonJour 1998: 198). The problem Hume has raised is\nwhether, despite the fact that inductive inferences have tended to\nproduce true conclusions in the past, we have reason to think the\nconclusion of an inductive inference we now make is likely to be true.\nArguably, establishing that an inductive inference is rational in the\nsense that it follows inductive standards is not sufficient to\nestablish that its conclusion is likely to be true. In fact Strawson\nallows that there is a question about whether “induction will\ncontinue to be successful”, which is distinct from the question\nof whether induction is rational. This question he does take to hinge\non a “contingent, factual matter” (Strawson 1952: 262).\nBut if it is this question that concerned Hume, it is no answer to\nestablish that induction is rational, unless that claim is understood\nto involve or imply that an inductive inference carried out according\nto rational standards is likely to have a true conclusion. \nSo far we have considered the various ways in which we might attempt\nto solve the problem of induction by resisting one or other premise of\nHume’s argument. Some philosophers have however seen his\nargument as unassailable, and have thus accepted that it does lead to\ninductive skepticism, the conclusion that inductive inferences cannot\nbe justified. The challenge then is to find a way of living with such\na radical-seeming conclusion. We appear to rely on inductive inference\nubiquitously in daily life, and it is also generally thought that it\nis at the very foundation of the scientific method. Can we go on with\nall this, whilst still seriously thinking none of it is justified by\nany rational argument? \nOne option here is to argue, as does Nicholas Maxwell, that the\nproblem of induction is posed in an overly restrictive\ncontext. Maxwell argues that the problem does not arise if we adopt a\ndifferent conception of science than the ‘standard\nempiricist’ one, which he denotes ‘aim-oriented\nempiricism’ (Maxwell 2017). \nAnother option here is to think that the significance of the problem of\ninduction is somehow restricted to a skeptical context. Hume himself\nseems to have thought along these lines. For instance he says: \nNature will always maintain her rights, and prevail in the end over\nany abstract reasoning whatsoever. Though we should conclude, for\ninstance, as in the foregoing section, that, in all reasonings from\nexperience, there is a step taken by the mind, which is not supported\nby any argument or process of the understanding; there is no danger,\nthat these reasonings, on which almost all knowledge depends, will\never be affected by such a discovery. (E. 5.1.2) \nHume’s purpose is clearly not to argue that we should not make\ninductive inferences in everyday life, and indeed his whole method and\nsystem of describing the mind in naturalistic terms depends on\ninductive inferences through and through. The problem of induction\nthen must be seen as a problem that arises only at the level of\nphilosophical reflection. \nAnother way to mitigate the force of inductive skepticism is to\nrestrict its scope. Karl Popper, for instance, regarded the problem of\ninduction as insurmountable, but he argued that science is not in fact\nbased on inductive inferences at all (Popper 1935 [1959]). Rather he\npresented a deductivist view of science, according to which it\nproceeds by making bold conjectures, and then attempting to falsify\nthose conjectures. In the simplest version of this account, when a\nhypothesis makes a prediction which is found to be false in an\nexperiment, the hypothesis is rejected as falsified. The logic of this\nprocedure is fully deductive. The hypothesis entails the prediction,\nand the falsity of the prediction refutes the hypothesis by modus\ntollens. Thus, Popper claimed that science was not based on the\nextrapolative inferences considered by Hume. The consequence then is\nthat it is not so important, at least for science, if those inferences\nwould lack a rational foundation. \nPopper’s account appears to be incomplete in an important way.\nThere are always many hypotheses which have not yet been refuted by\nthe evidence, and these may contradict one another. According to the\nstrictly deductive framework, since none are yet falsified, they are\nall on an equal footing. Yet, scientists will typically want to say\nthat one is better supported by the evidence than the others. We seem\nto need more than just deductive reasoning to support practical\ndecision-making (Salmon 1981). Popper did indeed appeal to a notion of\none hypothesis being better or worse “corroborated” by the\nevidence. But arguably, this took him away from a strictly deductive\nview of science. It appears doubtful then that pure deductivism can\ngive an adequate account of scientific method. \nHume’s argument might be taken as having definitively ruled out\nthe kind of justification for inductive inferences that he was looking\nfor. That is, it may preclude a justification which gives reason to\nbelieve the conclusion of a particular inductive inference is correct,\nor even likely to be correct. However, it is also possible to move\naway from the focus on justifying particular inductive inferences, and\nto consider inductive methods more generally. In simple cases of\nenumerative induction, the “inductive method”, or\n“inductive principle”, as it is sometimes called, is a\nrule for how to extrapolate from the observed instances. For example,\nit might be the rule that one should infer to a universal\ngeneralization, after a certain number of positive instances and\nreject the universal generalization after observation of a\ncounter-instances. Or it might be formulated as the so-called\n“straight rule”, which says that one should project the\nobserved frequency of an attribute to the population as a whole,\nincluding future instances. Might it be the case that the general\nproperties of an inductive method give grounds for employing that\nmethod, even when we have no reason to think that the method will\nresult in a correct answer in any particular application? Given a\nparticular inductive problem, we can look for an optimal method, or\nmeans, for providing a solution. Such a means-ends argument may then\nform the basis for following the method, even in the absence of\nreasons to believe in its success in particular instances. \nOne of the main early attempts in this direction was the\n“pragmatic” approach of Reichenbach (1938 [2006]).\nReichenbach did think Hume’s argument unassailable, but\nnonetheless he attempted to provide a weaker kind of justification for\ninduction. In order to emphasize the difference from the kind of\njustification Hume sought, some have given it a different term and\nrefer to Reichenbach’s solution as a “vindication”,\nrather than a justification of induction (Feigl 1950; Salmon\n1963). \nAccording to this approach, we have a certain aim in making inductive\ninferences. Even if we cannot be sure we can achieve the aim, we can\nstill argue that if the aim can be met, it will be by following the\nusual principles of inductive inference. This provides a reason for\nmaking those usual inductive inferences. Reichenbach makes a\ncomparison to the situation where a man is suffering from a disease,\nand the physician says “I do not know whether an operation will\nsave the man, but if there is any remedy, it is an operation”\n(Reichenbach 1938 [2006: 349]). This provides some kind of justification for\noperating on the man, even if one does not know that the operation\nwill succeed. \nReichenbach applied the strategy to a general form of\n“statistical induction” in which we observe the relative\nfrequency \\(f_n\\) of a particular event in n observations and\nthen form expectations about the frequency that will arise when more\nobservations are made. The “inductive principle” then\nstates that if after a certain number of instances, an observed\nfrequency of \\(m/n\\) is observed, for any prolongation of the series\nof observations, the frequency will continue to fall within a small\ninterval of \\(m/n\\). Cases such as Hume considered are a special case\nof this principle, where the observed frequency is 1. For example, in\nHume’s bread case, suppose bread was observed to nourish\nn times out of n (i.e. an observed frequency of 100%),\nthen according to the principle of induction, we expect that as we\nobserve more instances, the frequency of nourishing ones will continue\nto be within a very small interval of 100%. Following this inductive\nprinciple is also sometimes referred to as following the\n“straight rule”. The problem then is to justify the use of\nthis rule. \nReichenbach argued that even if Hume is right to think that we cannot\nbe justified in thinking for any particular application of the rule\nthat the conclusion is likely to be true, for the purposes of\npractical action we do not need to establish this. We can instead\nregard the inductive rule as resulting in a “posit”, or\nstatement that we deal with as if it is true. We posit a certain\nfrequency f on the basis of our evidence, and this is like\nmaking a wager or bet that the frequency is in fact f. \nThe aim of inductive inference, according to Reichenbach, is\n“to find series of events whose frequency of occurrence\nconverges towards a limit” (1938 [2006: 350]). It is\npossible that the world is so disorderly that we cannot construct\nseries with such limits. But if there is a limit, there is some\nelement of a series of observations, beyond which the principle of\ninduction will lead to the true value of the limit. Although the\ninductive rule may give quite wrong results early in the sequence, as\nit follows chance fluctuations in the sample frequency, it is\nguaranteed to eventually approximate the limiting frequency, if such a\nlimit exists. Therefore, the rule of induction is justified as an\ninstrument of positing because it is a method of which we know that if\nit is possible to make statements about the future we shall find them\nby means of this method (Reichenbach 1949: 475). This justification is\ntaken to be a pragmatic one, since though it does not supply knowledge\nof a future event, it supplies a sufficient reason for action\n(Reichenbach 1949: 481). \nThere are several problems with this pragmatic approach. One concern\nis that the kind of justification it offers is too much tied to the\nlong run, while allowing essentially no constraint on what can be\nposited in the short-run. Yet it is in the short run that inductive\npractice actually occurs and where it really needs justification\n(BonJour 1998: 194; Salmon 1966: 53). \nRelated to this is the worry that the justification is weak in the\nsense that it applies to many other rules of inference as well as the\nso-called “straight rule” (Salmon 1966: 53). It applies,\nin fact, to any method which converges asymptotically to the straight\nrule. An easily specified class of such rules are those which add to\nthe inductive rule a function \\(c_n\\) in which the \\(c_n\\) converge to\nzero with increasing n. \nReichenbach makes two suggestions aimed at avoiding this problem. On\nthe one hand, he claims, since we have no real way to pick between\nmethods, we might as well just use the inductive rule since it is\n“easier to handle, owing to its descriptive simplicity”.\nHe also claims that the method which embodies the “smallest\nrisk” is following the inductive rule (Reichenbach 1938 [2006:\n355–356]). \nAnother problem is whether Reichenbach has really established that\nthere could not be a better rule than the straight rule. For instance,\nfor all that has been said, there might be a soothsayer or psychic who\nis able to predict future events reliably. Here Reichenbach argues\nthat by using induction we could recognize the reliability of the\nalternative method, by examining its track record. This thought was\nlater picked up and developed into the suggestion that a\n“meta-inductivist” who applies induction not only at the\n“object” level to observations, but also to the success of\nothers’ methods, might by those means be able to do as well\npredictively as the alternative method (Schurz 2008; see\n section 7.3\n for more discussion of meta-induction). \nOne might also question whether a pragmatic argument can really\ndeliver an all-purpose, general justification for following the\ninductive rule. Surely a pragmatic solution should be sensitive to\ndifferences in pay-offs that depend on the circumstances. For example,\nReichenbach offers the following analogue to his pragmatic\njustification: \nWe may compare our situation to that of a man who wants to fish in an\nunexplored part of the sea. There is no one to tell him whether or not\nthere are fish in this place. Shall he cast his net? Well, if he wants\nto fish in that place, I should advise him to cast the net, to take\nthe chance at least. It is preferable to try even in uncertainty than\nnot to try and be certain of getting nothing. (Reichenbach 1938 [2006:\n362–363]) \nAs Lange points out, the argument here “presumes that there is\nno cost to trying”. In such a situation, “the fisherman\nhas everything to gain and nothing to lose by casting his net”\n(Lange 2011: 77). But if\nthere is some significant cost to making the attempt, it may not be so\nclear that the most rational course of action is to cast the net.\nSimilarly, whether or not it would make sense to adopt the policy of\nmaking no predictions, rather than the policy of following the\ninductive rule, may depend on what the practical penalties are for\nbeing wrong. A pragmatic solution may not be capable of offering\nrationale for following the inductive rule which is applicable in all\ncircumstances. \nAs we saw above, one of the problems for Reichenbach was that there\nare too many rules which converge in the limit to the true frequency.\nWhich one should we then choose in the short-run? It is possible to\nbroaden Reichenbach’s general strategy by considering what\nhappens if we have other epistemic goals besides long-run convergence.\nMight other goals place constraints on which methods should be used in\nthe short-run? The field of formal learning theory has developed\nanswers to these questions (Kelly 1996; Schulte 1999; also see Schulte\n2017). \nIn particular, formal learning theorists have considered the goal of\ngetting to the truth as efficiently, or quickly, as possible, as well\nas the goal of minimizing the number of mind-changes, or retractions\nalong the way. It has then been shown that the usual inductive method,\nwhich is characterized by a preference for simpler hypotheses\n(Occam’s razor), can be justified since it is the unique method\nwhich meets the standards for getting to the truth in the long run as\nefficiently as possible, with a minimum number of retractions (Schulte\n1999). \nFormal learning theory can be regarded as a kind of extension of the\nReichenbachian programme. It does not offer justifications for\ninductive inferences, in the sense of giving reasons why they should\nbe taken as likely to produce a true conclusion. Rather it offers\nreasons for following particular methods based on their optimality in\nachieving certain desirable epistemic ends, even if there is no\nguarantee that at any given stage of inquiry the results they produce\nare at all close to the truth. Recently, however, Steel (2010) has\nsuggested that formal learning theory offers more, and does provide a\nsolution to the problem of induction. This claim is based on a rather\nrestrictive interpretation of “Hume’s problem” as\nthe problem: “What is the justification for making inductive\ngeneralizations at all?” (2010: 182), rather than as the problem\nof giving the grounds for a given inductive inference. Steel’s\nclaims have been disputed by Colin Howson (2011). \nAnother approach to pursuing a broadly Reichenbachian programme is to\nmove to the level of meta-induction. We can draw a distinction between\napplying inductive methods at the level of events—so-called\n“object-level” induction, and applying inductive methods\nat the level of competing prediction methods—so-called\n“meta-induction”. Whereas object-level inductive methods\nmake predictions based on the events which have been observed to\noccur, meta-inductive methods make predictions based on aggregating\nthe predictions of different available prediction methods according to\ntheir success rates. Here, the success rate of a method is defined\naccording to some precise way of scoring success in making\npredictions. \nThe question is then whether there can be a meta-inductive method\nwhich is “predictively optimal” in the sense that\nfollowing that method succeeds best in predictions among all competing\nmethods, no matter what data is received. Gerhard Schurz has\nhighlighted results from the regret-based learning framework of\nCesa-Bianchi that there is a meta-inductive strategy that is\npredictively optimal among all predictive methods that are accessible\nto an epistemic agent (Cesa-Bianchi & Lugosi 2006; Schurz 2008,\nforthcoming). This meta-inductive strategy, which Schurz calls\n“wMI”, predicts a weighted average of the predictions of\nthe accessible methods, where the weights are\n“attractivities”, which measure the difference between the\nmethod’s own success rate and the success rate of wMI. \nThe main result is that the wMI strategy is long-run optimal in the\nsense that it converges to the maximum success rate of the accessible\nprediction methods. Worst-case bounds for short-run performance can\nalso be derived. The optimality result forms the basis for an a\npriori means-ends justification for the use of wMI. Namely, the\nthought is, it is reasonable to use wMI, since it achieves the best\nsuccess rate possible in the long run out of the given methods. \nSchurz also claims that this a priori justification of wMI,\ntogether with the contingent fact that inductive methods have so far\nbeen much more successful than non-inductive methods, gives rise to an\na posteriori justification of induction. Since wMI will\nachieve in the long run the maximal success rate of the available\nprediction methods, it is reasonable to use it. But as a matter of\nfact, the maximal success rate is achieved by inductive methods.\nTherefore, since it is a priori justified to use wMI, it is\nalso a priori justified to use the maximally successful\nmethod at the object level. Since it turns out that that the maximally\nsuccessful method is induction, then it is reasonable to use\ninduction. \nSchurz’s theorems on the optimality of wMI apply to the case\nwhere there are finitely many predictive methods. One point of\ndiscussion is whether this amounts to an important limitation on its\nclaims to provide a full solution of the problem of induction\n(Eckhardt 2010).","contact.mail":"l.henderson@rug.nl","contact.domain":"rug.nl"}]
