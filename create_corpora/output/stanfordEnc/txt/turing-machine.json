[{"date.published":"2018-09-24","url":"https://plato.stanford.edu/entries/turing-machine/","author1":"Liesbeth De Mol","author1.info":"https://pro.univ-lille.fr/liesbeth-de-mol/","entry":"turing-machine","body.text":"\n\n\nTuring machines, first described by Alan Turing\nin Turing 1936–7, are simple abstract computational devices\nintended to help investigate the extent and limitations of what can be\ncomputed. Turing’s ‘automatic machines’, as he\ntermed them in 1936, were specifically devised for the computing of\nreal numbers. They were first named ‘Turing machines’ by\nAlonzo Church in a review of Turing’s paper (Church 1937).\nToday, they are considered to be one of the foundational models of\ncomputability and (theoretical) computer \nscience.[1]\n\n\nTuring introduced Turing machines in the context of research into the\nfoundations of mathematics. More particularly, he used these abstract\ndevices to prove that there is no effective general method or\nprocedure to solve, calculate or compute every instance of the\nfollowing problem: \nEntscheidungsproblem The problem to decide\nfor every statement in first-order logic (the so-called restricted\nfunctional calculus, see the entry on\n classical logic\n for an introduction) whether or not it is derivable in that\nlogic. \nNote that in its original form (Hilbert & Ackermann 1928), the\nproblem was stated in terms of validity rather than derivability.\nGiven Gödel’s completeness theorem (Gödel 1929)\nproving that there is an effective procedure (or not) for derivability\nis also a solution to the problem in its validity form. In order to\ntackle this problem, one needs a formalized notion of “effective\nprocedure” and Turing’s machines were intended to do\nexactly that. \nA Turing machine then, or a computing machine as Turing\ncalled it, in Turing’s original definition is a machine capable\nof a finite set of configurations \\(q_{1},\\ldots,q_{n}\\) (the\nstates of the machine, called m-configurations by Turing). It\nis supplied with a one-way infinite and one-dimensional tape divided\ninto squares each capable of carrying exactly one symbol. At any\nmoment, the machine is scanning the content of one square\nr which is either blank (symbolized by \\(S_0\\)) or contains a\nsymbol \\(S_{1},\\ldots ,S_{m}\\) with \\(S_1 = 0\\) and \\(S_2 =\n1\\). \nThe machine is an automatic machine (a-machine) which means\nthat at any given moment, the behavior of the machine is completely\ndetermined by the current state and symbol (called the\nconfiguration) being scanned. This is the so-called\ndeterminacy condition\n (Section 3).\n These a-machines are contrasted with the so-called choice\nmachines for which the next state depends on the decision of an\nexternal device or operator (Turing 1936–7: 232). A Turing\nmachine is capable of three types of action: \nThe ‘program’ of a Turing machine can then be written as a\nfinite set of quintuples of the form:  \nWhere \\(q_i\\) is the current state, \\(S_j\\) the content of the square\nbeing scanned, \\(S_{i,j}\\) the new content of the square; \\(M_{i,j}\\)\nspecifies whether the machine is to move one square to the left, to\nthe right or to remain at the same square, and \\(q_{i,j}\\) is the next\nstate of the machine. These quintuples are also called the transition\nrules of a given machine. The Turing machine \\(T_{\\textrm{Simple}}\\)\nwhich, when started from a blank tape, computes the sequence\n\\(S_0S_1S_0S_1\\ldots\\) is then given by\n Table 1. \nTable 1: Quintuple representation of\n\\(T_{\\textrm{Simple}}\\) \nNote that \\(T_{\\textrm{Simple}}\\) will never enter a configuration\nwhere it is scanning \\(S_1\\) so that two of the four quintuples are\nredundant. Another typical format to represent Turing machines and\nwhich was also used by Turing is the transition table.\n Table 2\n gives the transition table of \\(T_{\\textrm{Simple}}\\). \nTable 2: Transition table for\n\\(T_{\\textrm{Simple}}\\) \nWhere current definitions of Turing machines usually have only one\ntype of symbols (usually just 0 and 1; it was proven by Shannon that\nany Turing machine can be reduced to a binary Turing machine (Shannon\n1956)) Turing, in his original definition of so-called computing\nmachines, used two kinds of symbols: the figures which\nconsist entirely of 0s and 1s and the so-called symbols of the\nsecond kind. These are differentiated on the Turing machine tape\nby using a system of alternating squares of figures and symbols of the\nsecond kind. One sequence of alternating squares contains the figures\nand is called the sequence of F-squares. It contains the\nsequence computed by the machine; the other is called the\nsequence of E-squares. The latter are used to mark\nF-squares and are there to “assist the memory”\n(Turing 1936–7: 232). The content of the E-squares is\nliable to change. F-squares however cannot be changed which\nmeans that one cannot implement algorithms whereby earlier computed\ndigits need to be changed. Moreover, the machine will never print a\nsymbol on an F-square if the F-square preceding it has\nnot been computed yet. This usage of F and E-squares can\nbe quite useful (see\n Sec. 2.3)\n but, as was shown by Emil L. Post, it results in a number of\ncomplications (see\n Sec. 1.2). \nThere are two important things to notice about the Turing machine\nsetup. The first concerns the definition of the machine itself, namely\nthat the machine’s tape is potentially infinite. This\ncorresponds to an assumption that the memory of the machine is\n(potentially) infinite. The second concerns the definition of Turing\ncomputable, namely that a function will be Turing computable if there\nexists a set of instructions that will result in a Turing machine\ncomputing the function regardless of the amount of time it takes. One\ncan think of this as assuming the availability of potentially infinite\ntime to complete the computation. \nThese two assumptions are intended to ensure that the definition of\ncomputation that results is not too narrow. This is, it ensures that\nno computable function will fail to be Turing-computable solely\nbecause there is insufficient time or memory to complete the\ncomputation. It follows that there may be some Turing computable\nfunctions which may not be carried out by any existing computer,\nperhaps because no existing machine has sufficient memory to carry out\nthe task. Some Turing computable functions may not ever be computable\nin practice, since they may require more memory than can be built\nusing all of the (finite number of) atoms in the universe. If we\nmoreover assume that a physical computer is a finite realization of\nthe Turing machine, and so that the Turing machine functions as a good\nformal model for the computer, a result which shows that a function is\nnot Turing computable is very strong, since it implies that no\ncomputer that we could ever build could carry out the computation. In\nSection 2.4, it is shown that there are functions which are not\nTuring-computable. \nTuring’s definition was standardized through (some of)\nPost’s modifications of it in Post 1947. In that paper Post\nproves that a certain problem from mathematics known as Thue’s\nproblem or the word problem for semi-groups is not Turing computable\n(or, in Post’s words, recursively unsolvable). Post’s main\nstrategy was to show that if it were decidable then the following\ndecision problem from Turing 1936–7 would also be decidable: \nPRINT? The problem to decide for every Turing machine\nM whether or not it will ever print some symbol (for instance,\n0). \nIt was however proven by Turing that PRINT? is not\nTuring computable and so the same is true of Thue’s problem. \nWhile the uncomputability of PRINT? plays a central\nrole in Post’s proof, Post believed that Turing’s proof of\nthat was affected by the “spurious Turing convention”\n(Post 1947: 9), viz. the system of F and E-squares.\nThus, Post introduced a modified version of the Turing machine. The\nmost important differences between Post’s and Turing’s\ndefinition are: \nPost’s Turing machine, when in a given state, either prints or\nmoves and so its transition rules are more ‘atomic’ (it\ndoes not have the composite operation of moving and printing). This\nresults in the quadruple notation of Turing machines, where each\nquadruple is in one of the three forms of\n Table 3: \nTable 3: Post’s Quadruple\nnotation \nNote that Post’s reformulation of the Turing machine is very\nmuch rooted in his Post 1936. (Some of) Post’s modifications of Turing’s\ndefinition became part of the definition of the Turing machine in\nstandard works such as Kleene 1952 and Davis 1958. Since that time,\nseveral (logically equivalent) definitions have been introduced.\nToday, standard definitions of Turing machines are, in some respects,\ncloser to Post’s Turing machines than to Turing’s\nmachines. In what follows we will use a variant on the standard\ndefinition from Minsky 1967 which uses the quintuple notation but has no E and\nF-squares and includes a special halting state H. It\nalso has only two move operations, viz., L and R and so\nthe action whereby the machine merely prints is not used. When the\nmachine is started, the tape is blank except for some finite portion\nof the tape. Note that the blank square can also be represented as a\nsquare containing the symbol \\(S_0\\) or simply 0. The finite content\nof the tape will also be called the dataword on the tape. \nTalk of “tape” and a “read-write head” is\nintended to aid the intuition (and reveals something of the time in\nwhich Turing was writing) but plays no important role in the\ndefinition of Turing machines. In situations where a formal analysis\nof Turing machines is required, it is appropriate to spell out the\ndefinition of the machinery and program in more mathematical terms.\nPurely formally a Turing machine can be specified as a quadruple \\(T =\n(Q,\\Sigma, s, \\delta)\\) where: \n\\(\\delta\\) is a transition function determining the next move:  \nThe transition function for the machine T is a function from\ncomputation states to computation states. If \\(\\delta(q_i,S_j) =\n(S_{i,j},D,q_{i,j})\\), then when the machine’s state is \\(q_j\\),\nreading the symbol \\(S_j\\), \\(T\\) replaces \\(S_j\\) by \\(S_{i,j}\\),\nmoves in direction \\(D \\in \\{L,R\\}\\) and goes to state\n\\(q_{i,j}\\). \nWe introduce a representation which allows us to describe the behavior\nor dynamics of a Turing machine \\(T_n\\), relying on the notation of\nthe complete configuration (Turing 1936–7: 232) also\nknown today as instantaneous description (ID) (Davis 1982:\n6). At any\nstage of the computation of \\(T_{i}\\) its ID is given by: \nSo, given some Turing machine T which is in state \\(q_{i}\\)\nscanning the symbol \\(S_{j}\\), its ID is given by \\(Pq_{i}S_{j}Q\\)\nwhere P and Q are the finite words to the left and right\nhand side of the square containing the symbol \\(S_{j}\\).\n Figure 1\n gives a visual representation of an ID of some Turing machine\nT in state \\(q_i\\) scanning the tape. \nFigure 1: A complete configuration of\nsome Turing machine T \nThe notation thus allows us to capture the developing behavior of the\nmachine and its tape through its consecutive IDs.\n Figure 2\n gives the first few consecutive IDs of \\(T_{\\textrm{Simple}}\\) using\na graphical representation. \nFigure 2: The dynamics of\n\\(T_{\\textrm{Simple}}\\) graphical representation \nThe animation can be started by clicking on the picture. One can also\nexplicitly print the consecutive IDs, using their symbolic\nrepresentations. This results in a state-space diagram of the behavior\nof a Turing machine. So, for \\(T_{\\textrm{Simple}}\\) we get (Note that\n\\(\\overline{0}\\) means the infinite repetition of 0s):  \nAs explained in\n Sec. 1.1,\n Turing machines were originally intended to formalize the notion of\ncomputability in order to tackle a fundamental problem of mathematics.\nIndependently of Turing, Alonzo Church gave a different but logically\nequivalent formulation (see\n Sec. 4).\n Today, most computer scientists agree that Turing’s, or any\nother logically equivalent, formal notion captures all\ncomputable problems, viz. for any computable problem, there is a\nTuring machine which computes it. This is known as the\nChurch-Turing thesis, Turing’s thesis (when\nthe reference is only to Turing’s work) or Church’s\nthesis (when the reference is only to Church’s work). \nIt implies that, if accepted, any problem not computable by a Turing\nmachine is not computable by any finite means whatsoever. Indeed,\nsince it was Turing’s ambition to capture “[all] the\npossible processes which can be carried out in computing a\nnumber” (Turing 1936–7: 249), it follows that, if we\naccept Turing’s analysis: \nIn this section, examples will be given which illustrate the\ncomputational power and boundaries of the Turing machine\nmodel. Section 3 then discusses some philosophical issues related to\nTuring’s thesis. \nIn order to speak about a Turing machine that does something useful\nfrom the human perspective, we will have to provide an interpretation\nof the symbols recorded on the tape. For example, if we want to design\na machine which will compute some mathematical function, addition say,\nthen we will need to describe how to interpret the ones and zeros\nappearing on the tape as numbers. \nIn the examples that follow we will represent the number n as\na block of \\(n+1\\) copies of the symbol ‘1’ on the tape.\nThus we will represent the number 0 as a single ‘1’ and\nthe number 3 as a block of four ‘1’s. This is called\nunary notation. \nWe will also have to make some assumptions about the configuration of\nthe tape when the machine is started, and when it finishes, in order\nto interpret the computation. We will assume that if the function to\nbe computed requires n arguments, then the Turing machine\nwill start with its head scanning the leftmost ‘1’ of a\nsequence of n blocks of ‘1’s. The blocks of\n‘1’s representing the arguments must be separated by a\nsingle occurrence of the symbol ‘0’. For example, to\ncompute the sum \\(3+4\\), a Turing machine will start in the\nconfiguration shown in\n Figure 3. \nFigure 3: Initial configuration for a\ncomputation over two numbers n and m \nHere the supposed addition machine takes two arguments representing\nthe numbers to be added, starting at the leftmost 1 of the first\nargument. The arguments are separated by a single 0 as required, and\nthe first block contains four ‘1’s, representing the\nnumber 3, and the second contains five ‘1’s, representing\nthe number 4. \nA machine must finish in standard configuration too. There must be a\nsingle block of symbols (a sequence of 1s representing some number or\na symbol representing another kind of output) and the machine must be\nscanning the leftmost symbol of that sequence. If the machine\ncorrectly computes the function then this block must represent the\ncorrect answer. \nAdopting this convention for the terminating configuration of a Turing\nmachine means that we can compose machines by identifying the final\nstate of one machine with the initial state of the next. \n\n Table 4\n gives the transition table of a Turing machine \\(T_{\\textrm{Add}_2}\\)\nwhich adds two natural numbers n and m. We assume the\nmachine starts in state \\(q_1\\) scanning the leftmost 1 of\n\\(n+1\\). \nTable 4: Transition table for\n\\(T_{\\textrm{Add}_2}\\) \nThe idea of doing an addition with Turing machines when using unary\nrepresentation is to shift the leftmost number n one square to\nthe right. This is achieved by erasing the leftmost 1 of \\(n +1\\)\n(this is done in state \\(q_1\\)) and then setting the 0 between \\(n+1\\)\nand \\(m+1\\) to 1 (state \\(q_2\\)). We then have \\(n + m + 2\\) and so we\nstill need to erase one additional 1. This is done by erasing the\nleftmost 1 (states \\(q_3\\) and \\(q_4\\)).\n Figure 4\n shows this computation for \\(3 + 4\\). \nFigure 4: The computation of \\(3+4\\) by\n\\(T_{\\textrm{Add}_2}\\) \nWe can generalize \\(T_{\\textrm{Add}_2}\\) to a Turing machine\n\\(T_{\\textrm{Add}_i}\\) for the addition of an arbitrary number\ni of integers \\(n_1, n_2,\\ldots, n_j\\). We assume\nagain that the machine starts in state \\(q_1\\) scanning the leftmost 1\nof \\(n_1+1\\). The transition table for such a machine\n\\(T_{\\textrm{Add}_i}\\) is given in\n Table 5. \nTable 5: Transition table for\n\\(T_{\\textrm{Add}_i}\\) \nThe machine \\(T_{\\textrm{Add}_i}\\) uses the principle of shifting the\naddends to the right which was also used for \\(T_{\\textrm{Add}_2}\\).\nMore particularly, \\(T_{add_i}\\) computes the sum of \\(n_1 + 1\\),\n\\(n_2 + 1\\),… \\(n_i+1\\) from left to right, viz. it computes\nthis sum as follows:  \nThe most important difference between \\(T_{\\textrm{Add}_2}\\) and\n\\(T_{\\textrm{Add}_i}\\) is that \\(T_{\\textrm{Add}_i}\\) needs to verify\nif the leftmost addend \\(N_j, 1 < j \\leq i\\) is equal to\n\\(N_i\\). This is achieved by checking whether the first 0 to the right\nof \\(N_j\\) is followed by another 0 or not (states \\(q_2\\) and\n\\(q_3\\)). If it is not the case, then there is at least one more\naddend \\(n_{j+2}\\) to be added. The machine moves to the leftmost 1 of\n\\(N_j\\) (\\(q_5\\)), removes that first 1 (\\(q_1\\)) and starts again in\n\\(q_2\\). If \\(N_j = N_i\\), the machine erases the leftmost 1 of\n\\(N_i\\) (\\(q_6\\)) and moves back to the leftmost 1 of \\(N_i - 1 = n_1\n+ n_2 + \\ldots + n_i\\). \nTuring’s original paper is concerned with computable (real)\nnumbers. A (real) number is Turing computable if there exists a\nTuring machine which computes an arbitrarily precise approximation to\nthat number. All of the algebraic numbers (roots of polynomials with\nalgebraic coefficients) and many transcendental mathematical\nconstants, such as e and \\(\\pi\\) are Turing-computable.\nTuring gave several examples of classes of numbers computable by\nTuring machines (see section 10 Examples of large classes of\nnumbers which are computable of Turing 1936–7) as a\nheuristic argument showing that a wide diversity of classes of numbers\ncan be computed by Turing machines. \nOne might wonder however in what sense computation with numbers, viz.\ncalculation, captures non-numerical but computable problems\nand so how Turing machines capture all general and effective\nprocedures which determine whether something is the case or not.\nExamples of such problems are: \nIn general, these problems are of the form: \nAn important challenge of both theoretical and concrete advances in\ncomputing (often at the interface with other disciplines) has become\nthe problem of providing an interpretation of X such that it\ncan be tackled computationally. To give just one concrete example, in\ndaily computational practices it might be important to have a method\nto decide for any digital “source” whether or not it can\nbe trusted and so one needs a computational interpretation of\ntrust. \nThe characteristic function of a predicate is a function\nwhich has the value TRUE or FALSE when given appropriate arguments. In\norder for such functions to be computable, Turing relied on\nGödel’s insight that these kind of problems can be encoded\nas a problem about numbers (See\n Gödel’s incompleteness theorem\n and the next\n Sec. 2.3)\n In Turing’s wording: \nThe expression “there is a general process for determining\n…” has been used [here] […] as equivalent to\n“there is a machine which will determine …”. This\nusage can be justified if and only if we can justify our definition of\n“computable”. For each of these “general\nprocess” problems can be expressed as a problem concerning a\ngeneral process for determining whether a given integer n has a\nproperty \\(G(n)\\) [e.g. \\(G(n)\\) might mean “n is\nsatisfactory” or “n is the Gödel\nrepresentation of a provable formula”], and this is equivalent\nto computing a number whose n-th figure is 1 if \\(G(n)\\) is\ntrue and 0 if it is false. (1936–7: 248) \nIt is the possibility of coding the “general process”\nproblems as numerical problems that is essential to Turing’s\nconstruction of the universal Turing machine and its use within a\nproof that shows there are problems that cannot be computed by a\nTuring machine. \nThe universal Turing machine which was constructed to prove the\nuncomputability of certain problems, is, roughly speaking, a Turing\nmachine that is able to compute what any other Turing machine\ncomputes. Assuming that the Turing machine notion fully captures\ncomputability (and so that Turing’s thesis is valid), it is\nimplied that anything which can be “computed”, can also be\ncomputed by that one universal machine. Conversely, any problem that\nis not computable by the universal machine is considered to be\nuncomputable. \nThis is the rhetorical and theoretical power of the universal machine\nconcept, viz. that one relatively simple formal device captures all\n“the possible processes which can be carried out in\ncomputing a number” (Turing 1936–7). It is also one\nof the main reasons why Turing has been retrospectively\nidentified as one of the founding fathers of computer science (see\n Section 5). \nSo how to construct a universal machine U out of the set of\nbasic operations we have at our disposal? Turing’s approach is\nthe construction of a machine U which is able to (1)\n‘understand’ the program of any other machine\n\\(T_{n}\\) and, based on that “understanding”, (2)\n‘mimic’ the behavior of \\(T_{n}\\). To this end, a method\nis needed which allows to treat the program and the behavior of\n\\(T_n\\) interchangeably since both aspects are manipulated on the same\ntape and by the same machine. This is achieved by Turing in two basic\nsteps: the development of (1) a notational method (2) a set of\nelementary functions which treats that notation—independent of\nwhether it is formalizing the program or the behavior of\n\\(T_n\\)—as text to be compared, copied down, erased, etc. In\nother words, Turing develops a technique that allows to treat program\nand behavior on the same level. \nGiven some machine \\(T_n\\), Turing’s basic idea is to construct\na machine \\(T_n'\\) which, rather than directly printing the output\nof \\(T_n\\), prints out the successive complete configurations or\ninstantaneous descriptions of \\(T_n\\). In order to achieve this,\n\\(T_n'\\): \n[…] could be made to depend on having the rules of operation\n[…] of [\\(T_n\\)] written somewhere within itself […]\neach step could be carried out by referring to these rules. (Turing\n1936–7: 242) \nIn other words, \\(T_n'\\) prints out the successive complete\nconfigurations of \\(T_n\\) by having the program of \\(T_n\\) written on\nits tape. Thus, Turing needs a notational method which makes it\npossible to ‘capture’ two different aspects of a Turing\nmachine on one and the same tape in such a way they can be treated\nby the same machine, viz.: \nThus, a first and perhaps most essential step, in the construction of\nU are the quintuple and complete configuration notation and\nthe idea of putting them on the same tape. More particularly, the tape\nis divided into two regions which we will call the A and\nB region here. The A region contains a notation of the\n‘program’ of \\(T_n\\) and the B region a notation\nfor the successive complete configurations of \\(T_n\\). In\nTuring’s paper they are separated by an additional symbol\n“::”. \nTo simplify the construction of U and in order to encode any\nTuring machine as a unique number, Turing develops a third notation\nwhich permits to express the quintuples and complete configurations\nwith letters only. This is determined by [Note that we use\nTuring’s original encoding. Of course, there is a broad variety\nof possible encodings, including binary encodings]: \nUsing this method, each quintuple of some Turing machine \\(T_n\\) can\nbe expressed in terms of a sequence of capital letters and so the\n‘program’ of any machine \\(T_{n}\\) can be expressed by the\nset of symbols A, C, D, R, L, N and ;. This is the so-called\nStandard Description (S.D.) of a Turing machine. Thus, for\ninstance, the S.D. of \\(T_{\\textrm{Simple}}\\) is:  \nThis is, essentially, Turing’s version of\n Gödel numbering.\n Indeed, as Turing shows, one can easily get a numerical description\nrepresentation or Description Number (D.N.) of a Turing\nmachine \\(T_{n}\\) by replacing: \nThus, the D.N. of \\(T_{\\textrm{Simple}}\\) is:  \nNote that every machine \\(T_n\\) has a unique D.N.; a D.N. represents\none and one machine only. \nClearly, the method used to determine the \\(S.D.\\) of some machine\n\\(T_n\\) can also be used to write out the successive complete\nconfigurations of \\(T_n\\). Using “:” as a separator\nbetween successive complete configurations, the first few complete\nconfigurations of \\(T_{\\textrm{Simple}}\\) are:  \nHaving a notational method to write the program and successive\ncomplete configurations of some machine \\(T_n\\) on one and the same\ntape of some other machine \\(T_n'\\) is the first step in\nTuring’s construction of U. However, U should also\nbe able to “emulate” the program of \\(T_n\\) as written in\nregion A so that it can actually write out its successive\ncomplete configurations in region B. Moreover it should be\npossible to “take out and exchange[…] [the rules of operations of some Turing machine] for\nothers” (Turing 1936–7: 242). Viz.,\nit should be able not just to calculate but also to compute, an issue\nthat was also dealt with by others such as Church, Gödel and Post\nusing their own formal devices. It should, for instance, be able to\n“recognize” whether it is in region A or B\nand it should be able to determine whether or not a certain sequence\nof symbols is the next state \\(q_i\\) which needs to be executed. \nThis is achieved by Turing through the construction of a sequence of\nTuring computable problems such as: \nTuring develops a notational technique, called skeleton\ntables, for these functions which serves as a kind of shorthand\nnotation for a complete Turing machine table but can be easily used to\nconstruct more complicated machines from previous ones. The technique\nis quite reminiscent of the recursive technique of composition (see:\n recursive functions). \nTo illustrate how such functions are Turing computable, we discuss one\nsuch function in more detail, viz. the compare function. It is\nconstructed on the basis of a number of other Turing computable\nfunctions which are built on top of each other. In order to understand\nhow these functions work, remember that Turing used a system of\nalternating F and E-squares where the F-squares\ncontain the actual quintuples and complete configurations and the\nE-squares are used as a way to mark off certain parts of the\nmachine tape. For the comparing of two sequences \\(S_1\\) and \\(S_2\\),\neach symbol of \\(S_1\\) will be marked by some symbol a and each\nsymbol of \\(S_2\\) will be marked by some symbol b. \nTuring defined nine different functions to show how the compare\nfunction can be computed with Turing machines: \n\\(\\textrm{COMPARE}\\_\\textrm{ALL}(q_j,q_n,a,b)\\) The machine compares\nthe sequences A and B marked with a and b\nrespectively. This is done by repeatedly computing COMPARE_ERASE on a and\nb. If A and B are equal, all a’s and\nb’s will have been erased and the machine moves to state\n\\(q_j\\), else, it will move to state \\(q_n\\). It is computed by\n\n\\[\\textrm{COMPARE}\\_\\textrm{ERASE}(\\textrm{COMPARE}\\_\\textrm{ALL}(q_j,q_n,a,b),q_j,q_n,a,b)\\]\n\n and so by recursively calling\n\\(\\textrm{COMPARE}\\_\\textrm{ALL}\\). \nIn a similar manner, Turing defines the following functions: \nUsing the basic functions COPY, REPLACE and COMPARE, Turing constructs\na universal Turing machine. \nBelow is an outline of the universal Turing machine indicating how\nthese basic functions indeed make possible universal computation. It\nis assumed that upon initialization, U has on its tape the S.D.\nof some Turing machine \\(T_n\\). Remember that Turing uses the system\nof alternating F and E-squares and so, for instance, the\nS.D. of \\(T_{\\textrm{Simple}}\\) will be written on the tape of\nU as:  \nwhere “_” indicates an unmarked E-square. \nFIND_NEXT_STATE: The machine first marks (1) with y the\nconfiguration \\(q_{CC,i}S_{CC,j}\\) of the rightmost (and so last)\ncomplete configuration computed by U in the B part of\nthe tape and (2) with x the configuration \\(q_{q,m}S_{q,n}\\) of\nthe leftmost quintuple which is not preceded by a marked (with the\nletter z) semicolon in the A part of the tape. The two\nconfigurations are compared. If they are identical, the machine moves\nto MARK_OPERATIONS, if not, it marks the semicolon preceding\n\\(q_{q,m}S_{q,n}\\) with z and goes to FIND_NEXT_STATE. This is\neasily achieved using the function COMPARE_ALL which means that,\nwhatever the outcome of the comparison, the marks x and\ny will be erased. For instance, suppose that \\(T_n =\nT_{\\textrm{Simple}}\\) and that the last complete configuration of\n\\(T_{\\textrm{Simple}}\\) as computed by U is:  \nThen U will move to region A and determine that the\ncorresponding quintuple is:  \nMARK_OPERATIONS: The machine U marks the operations that it\nneeds to execute in order to compute the next complete configuration\nof \\(T_n\\). The printing and move (L,R, N) operations are marked with\nu and the next state with y. All marks z are\nerased. Continuing with our example, U will mark\n\\(\\eqref{quint_univ}\\) as follows:  \nMARK_COMPCONFIG: The last complete configuration of \\(T_n\\) as\ncomputed by U is marked into four regions: the configuration\n\\(q_{CC,i}S_{CC,j}\\) itself is left unmarked; the symbol just\npreceding it is marked with an x and the remaining symbols to\nthe left or marked with v. Finally, all symbols to the right,\nif any, are marked with w and a “:” is printed to\nthe right of the rightmost symbol in order to indicate the beginning\nof the next complete configuration of \\(T_n\\) to be computed by\nU. Continuing with our example, \\(\\eqref{CC_univ}\\) will be\nmarked as follows by U:  \nU then goes to PRINT \nPRINT_COMPLETE_CONFIGURATION. U prints the next complete\nconfiguration and erases all marks u, v, w, x, y. It then\nreturns to FIND_NEXT_STATE. U first searches for the rightmost\nletter u, to check which move is needed (R, L, N) and\nerases the mark u for R, L, N. Depending on the value\nL, R or N will then write down the next complete\nconfiguration by applying COPY\\(_5\\) to u, v, w, x, y. The move\noperation (L, R, N) is accounted for by the particular\ncombination of u, v, w, x, y:  Following our example, since \\(T_{\\textrm{Simple}}\\)\nneeds to move right, the new rightmost complete configursiation of\n\\(T_{\\textrm{Simple}}\\) written on the tape of U is:  \nSince we have that for this complete configuration the square being\nscanned by \\(T_{\\textrm{Simple}}\\) is one that was not included in the\nprevious complete configuration (viz. \\(T_{\\textrm{Simple}}\\) has\nreached beyond the rightmost previous point) the complete\nconfiguration as written out by U is in fact incomplete. This\nsmall defect was corrected by Post (Post 1947) by including an additional instruction in the function\nused to mark the complete configuration in the next round. \nAs is clear, Turing’s universal machine indeed requires that\nprogram and ‘data’ produced by that program are\nmanipulated interchangeably, viz. the program and its productions are\nput next to each other and treated in the same manner, as sequences of\nletters to be copied, marked, erased and compared. \nTuring’s particular construction is quite intricate with its\nreliance on the F and E-squares, the use of a rather\nlarge set of symbols and a rather arcane notation used to describe the\ndifferent functions discussed above. Since 1936 several modifications\nand simplifications have been implemented. The removal of the\ndifference between F and E-squares was already discussed\nin\n Section 1.2\n and it was proven by Shannon that any Turing machine, including the\nuniversal machine, can be reduced to a binary Turing machine (Shannon\n1956). Since the 1950s, there has been quite some research on what\ncould be the smallest possible universal devices (with respect to the\nnumber of states and symbols) and quite some “small”\nuniversal Turing machines have been found. These results are usually\nachieved by relying on other equivalent models of computability such\nas, for instance, tag systems. For a survey on research into small\nuniversal devices (see Margenstern 2000; Woods & Neary 2009). \nAs explained, the purpose of Turing’s paper was to show that the\nEntscheidungsproblem for first-order logic is not computable. The same\nresult was achieved independently by Church (1936a, 1936b) using a different kind of formal device which is logically\nequivalent to a Turing machine (see\n Sec. 4).\n The result went very much against what Hilbert had hoped to achieve\nwith his finitary and formalist program. Indeed, next to\nGödel’s incompleteness results, they broke much of\nHilbert’s dream of making mathematics void of\nIgnorabimus and which was explicitly expressed in the\nfollowing words of Hilbert: \nThe true reason why Comte could not find an unsolvable problem, lies\nin my opinion in the assertion that there exists no unsolvable\nproblem. Instead of the stupid Ignorabimus, our solution should be: We\nmust know. We shall know. (1930: 963) [translation by the author]  \nNote that the solvability Hilbert is referring to here concerns\nsolvability of mathematical problems in general and not just\nmechanically solvable. It is shown however in Mancosu et al. 2009 (p.\n94), that this general aim of solving every mathematical problem,\nunderpins two particular convictions of Hilbert namely that (1) the\naxioms of number theory are complete and (2) that there are no\nundecidable problems in mathematics. \nSo, how can one show, for a particular decision problem\n\\(\\textrm{D}_i\\), that it is not computable? There are two main\nmethods: \nToday, one usually relies on the first method while it is evident that\nin the absence of a problem \\(\\textrm{D}_{\\textrm{uncomp}}\\), Turing\nbut also Church and Post (see\n Sec. 4)\n had to rely on the direct approach. \nThe notion of reducibility has its origins in the work of Turing and\nPost who considered several variants of computability (Post 1947;\nTuring 1939). The concept was later appropriated in the context of\ncomputational complexity theory and is today one of the basic concepts\nof both computability and computational complexity theory (Odifreddi\n1989; Sipser 1996). Roughly speaking, a reduction of a problem \\(D_i\\)\nto a problem \\(D_j\\) comes down to providing an effective procedure\nfor translating every instance \\(d_{i,m}\\) of the problem \\(D_i\\) to\nan instance \\(d_{j,n}\\) of \\(D_j\\) in such a way that an effective\nprocedure for solving \\(d_{j,n}\\) also yields an effective procedure\nfor solving \\(d_{i,m}\\). In other words, if \\(D_i\\) reduces to \\(D_j\\)\nthen, if \\(D_i\\) is uncomputable so is \\(D_j\\). Note that the\nreduction of one problem to another can also be used in decidability\nproofs: if \\(D_i\\) reduces to \\(D_j\\) and \\(D_j\\) is known to be\ncomputable then so is \\(D_i\\). \nIn the absence of D\\(_{\\textrm{uncomp}}\\) a very\ndifferent approach was required and Church, Post and Turing each used\nmore or less the same approach to this end (Gandy 1988). First of all,\none needs a formalism which captures the notion of computability.\nTuring proposed the Turing machine formalism to this end. A second\nstep is to show that there are problems that are not computable within\nthe formalism. To achieve this, a uniform process U\nneeds to be set-up relative to the formalism which is able to compute\nevery computable number. One can then use (some form of)\ndiagonalization in combination with U to derive a\ncontradiction. Diagonalization was introduced by Cantor to show that\nthe set of real numbers is “uncountable” or not\ndenumerable. A variant of the method was used also by Gödel in\nthe proof of his\n first incompleteness theorem. \nRecall that in Turing’s original version of the Turing machine,\nthe machines are computing real numbers. This implied that a\n“well-behaving” Turing machine should in fact never halt\nand print out an infinite sequence of figures. Such machines were\nidentified by Turing as circle-free. All other machines are\ncalled circular machines. A number n which is the D.N.\nof a circle-free machine is called satisfactory. \nThis basic difference is used in Turing’s proof of the\nuncomputability of: \nCIRC? The problem to decide for every number n\nwhether or not it is satisfactory \nThe proof of the uncomputability of CIRC? uses the\nconstruction of a hypothetical and circle-free machine \\(T_{decide}\\)\nwhich computes the diagonal sequence of the set of all computable\nnumbers computed by the circle-free machines. Hence, it relies for its\nconstruction on the universal Turing machine and a hypothetical\nmachine that is able to decide CIRC? for each number\nn given to it. It is shown that the machine \\(T_{decide}\\)\nbecomes a circular machine when it is provided with its own\ndescription number, hence the assumption of a machine which is capable\nof solving CIRC? must be false. \nBased on the uncomputability of CIRC?, Turing then\nshows that also PRINT? is not computable. More\nparticularly he shows that if PRINT? were to be\ncomputable, also CIRC? would be decidable, viz. he\nrephrases PRINT? in such a way that it becomes the\nproblem to decide for any machine whether or not it will print an\ninfinity of symbols which would amount to deciding\nCIRC?. \nFinally, based on the uncomputability of PRINT?\nTuring shows that the Entscheidungsproblem is not decidable. This is\nachieved by showing: \nIt thus follows from the uncomputability of PRINT?,\nthat the Entscheidungsproblem is not computable. \nGiven Turing’s focus on computable real numbers, his base\ndecision problem is about determining whether or not some Turing\nmachine will not halt and so is not quite the same as the\nmore well-known halting problem: \nTuring’s problem PRINT? is in fact very close\nto HALT? (see Davis 1958: Chapter 5, Theorem\n2.3). \nA popular proof of HALT? goes as follows. Assume that\nHALT? is computable. Then it should be possible to\nconstruct a Turing machine which decides, for each machine \\(T_i\\) and\nsome input w for \\(T_i\\) whether or not \\(T_i\\) will halt on\nw. Let us call this machine \\(T_{H}\\). More particularly, we\nhave: \nWe now define a second machine \\(T_D\\) which relies on the assumption\nthat the machine \\(T_H\\) can be constructed. More particularly, we\nhave:  \nIf we now set \\(T_i\\) to \\(T_D\\) we end up with a contradiction: if\n\\(T_D\\) halts it means that \\(T_D\\) does not halt and vice versa. A\npopular but quite informal variant of this proof was given by\nChristopher Strachey in the context of programming (Strachey\n1965). \nAs is clear from\n Sections 1.1\n and\n 1.2,\n there is a variety of definitions of the Turing machine. One can use\na quintuple or quadruple notation; one can have different types of\nsymbols or just one; one can have a two-way infinite or a one-way\ninfinite tape; etc. Several other less obvious modifications have been\nconsidered and used in the past. These modifications can be of two\nkinds: generalizations or restrictions. These do not result in\n“stronger” or “weaker” models. Viz. these\nmodified machines compute no more and no less than the Turing\ncomputable functions. This adds to the robustness of the Turing\nmachine definition. \nIn his short 1936 note Post considers machines that either mark or unmark a square which\nmeans we have only two symbols \\(S_0\\) and \\(S_1\\) but he did not\nprove that this formulation captures exactly the Turing computable\nfunctions. It was Shannon who proved that for any Turing machine\nT with n symbols there is a Turing machine with two\nsymbols that simulates T (Shannon 1956). He also showed that\nfor any Turing machine with m states, there is a Turing machine\nwith only two states that simulates it. \nNon-erasing machines are machines that can only overprint \\(S_0\\). In\nMoore 1952, it was mentioned that Shannon proved that non-erasing\nmachines can compute what any Turing machine computes. This result was\ngiven in a context of actual digital computers of the 50s which relied\non punched tape (and so, for which, one cannot erase). Shannon’s\nresult however remained unpublished. It was Wang who published the\nresult (Wang 1957). \nIt was shown by Minsky that for every Turing machine there is a\nnon-writing Turing machine with two tapes that simulates it. \nInstead of one tape one can consider a Turing machine with multiple\ntapes. This turned out the be very useful in several different\ncontexts. For instance, Minsky, used two-tape non-writing Turing machines to prove that a certain decision problem defined by Post (the decision problem for tag systems) is\nnon-Turing computable (Minsky 1961). Hartmanis and Stearns then,\nin their founding paper for computational complexity theory, proved\nthat any n-tape Turing machine reduces to a single tape Turing\nmachine and so anything that can be computed by an n-tape or\nmultitape Turing machine can also be computed by a single tape Turing\nmachine, and conversely (Hartmanis & Stearns 1965). They used\nmultitape machines because they were considered to be closer to actual\ndigital computers. \nAnother variant is to consider Turing machines where the tape is not\none-dimensional but n-dimensional. This variant too reduces to\nthe one-dimensional variant. \nAn apparently more radical reformulation of the notion of Turing\nmachine is that of non-deterministic Turing machines. As explained in\n 1.1,\n one fundamental condition of Turing’s machines is the so-called\ndeterminacy condition, viz. the idea that at any given moment, the\nmachine’s behavior is completely determined by the configuration\nor state it is in and the symbol it is scanning. Next to these, Turing\nalso mentions the idea of choice machines for which the next state is\nnot completely determined by the state and symbol pair. Instead, some\nexternal device makes a random choice of what to do next.\nNon-deterministic Turing machines are a kind of choice machines: for\neach state and symbol pair, the non-deterministic machine makes an\narbitrary choice between a finite (possibly zero) number of states.\nThus, unlike the computation of a deterministic Turing machine, the\ncomputation of a non-deterministic machine is a tree of possible\nconfiguration paths. One way to visualize the computation of a\nnon-deterministic Turing machine is that the machine spawns an exact\ncopy of itself and the tape for each alternative available transition,\nand each machine continues the computation. If any of the machines\nterminates successfully, then the entire computation terminates and\ninherits that machine’s resulting tape. Notice the word\nsuccessfully in the preceding sentence. In this formulation, some\nstates are designated as accepting states and when the\nmachine terminates in one of these states, then the computation is\nsuccessful, otherwise the computation is unsuccessful and any other\nmachines continue in their search for a successful outcome. The\naddition of non-determinism to Turing machines does not alter the\nextent of Turing-computability. Non-determinism was introduced for\nfinite automata in the paper, Rabin & Scott 1959, where it is also\nshown that adding non-determinism does not result in more powerful\nautomata. Non-deterministic Turing machines are an important model in\nthe context of\n computational complexity theory. \nWeak Turing machines are machines where some word over the alphabet is\nrepeated infinitely often to the left and right of the input.\nSemi-weak machines are machines where some word is repeated infinitely\noften either to the left or right of the input. These machines are\ngeneralizations of the standard model in which the initial tape\ncontains some finite word (possibly nil). They were introduced to\ndetermine smaller universal machines. Watanabe was the first to define\na universal semi-weak machine with six states and five symbols\n(Watanabe 1961). Recently, a number of researchers have determined\nseveral small weak and semi-weak universal Turing machines (e.g.,\nWoods & Neary 2007; Cook 2004) \nBesides these variants on the Turing machine model, there are also\nvariants that result in models which capture, in some well-defined\nsense, more than the (Turing)-computable functions. Examples of such\nmodels are oracle machines (Turing 1939), infinite-time Turing\nmachines (Hamkins & Lewis 2008) and accelerating Turing machines\n(Copeland 2002). There are various reasons for introducing such\nstronger models. Some are well-known models of computability or\nrecursion theory and are used in the theory of higher-order recursion\nand relative computability (oracle machines); others, like the\naccelerating machines, were introduced in the context of\n supertasks\n and the idea of providing physical models that “compute”\nfunctions which are not Turing-computable. \nIn its original context, Turing’s identification between the\ncomputable numbers and Turing machines was aimed at proving that the\nEntscheidungsproblem is not a computable problem and so not a\nso-called “general process” problem (Turing 1936–7:\n248). The basic assumption to be made for this result is that our\n“intuitive” notion of computability can be formally\ndefined as Turing computability and so that there are no\n“computable” problems that are not Turing computable. But\nwhat was Turing’s “intuitive” notion of\ncomputability and how can we be sure that it really covers all\ncomputable problems, and, more generally, all kinds of computations?\nThis is a very basic question in the\n philosophy of computer science. \nAt the time Turing was writing his paper, the modern computer was not\ndeveloped yet and so rephrasings of Turing’s thesis which\nidentify Turing computability with computability by a modern computer\nare interpretations rather than historically correct statements of\nTuring’s thesis. The existing computing machines at the time\nTuring wrote his paper, such as the differential analyzer or desk\ncalculators, were quite restricted in what they could compute and were\nused in a context of human computational practices (Grier 2007). It is\nthus not surprising that Turing did not attempt to formalize machine\ncomputation but rather human computation and so computable problems in\nTuring’s paper become computable by human means. This is very\nexplicit in Section 9 of Turing 1936–7 where he shows that\nTuring machines are a ‘natural’ model of (human)\ncomputation by analyzing the process of human computation. The\nanalysis results in a kind of abstract human ‘computor’\nwho fulfills a set of different conditions that are rooted in\nTuring’s recognition of a set of human limitations which\nrestrict what we can compute (of our sensory apparatus but also of our\nmental apparatus). This ‘computor’ computes (real) numbers\non an infinite one-dimensional tape divided into squares [Note: Turing\nassumed that the reduction of the 2-dimensional character of the paper\na human mathematician usually works on “is not essential of\ncomputation” (Turing 1936–7: 249)]. It has the following\nrestrictions (Gandy 1988; Sieg 1994): \nIt is this so-called “direct appeal to intuition”\n(1936–7: 249) of Turing’s analysis and resulting model\nthat explain why the Turing machine is today considered by many as the\nbest standard model of computability (for a strong statement of this\npoint of view, see Soare 1996). Indeed, from the above set of\nconditions one can quite easily derive Turing’s machines. This\nis achieved basically by analyzing the restrictive conditions into\n“‘simple operations’ which are so elementary that it\nis not easy to imagine them further divided” (Turing\n1936–7: 250). \nNote that while Turing’s analysis focuses on human computation,\nthe application of his identification between (human) computation and\nTuring machine computation to the Entscheidungsproblem suggests that\nhe did not consider the possibility of a model of computation\nthat somehow goes “beyond” human computation and is\ncapable of providing an effective and general procedure which solves\nthe Entscheidungsproblem. If that would have been the case, he would\nnot have considered the Entscheidungsproblem to be uncomputable. \nThe focus on human computation in Turing’s analysis of\ncomputation, has led researchers to extend Turing’s analysis to\ncomputation by physical devices. This results in (versions of) the\nphysical Church-Turing thesis. Robin Gandy focused on extending\nTuring’s analysis to discrete mechanical devices (note that he\ndid not consider analog machines). More particularly, like Turing,\nGandy starts from a basic set of restrictions of computation by\ndiscrete mechanical devices and, on that basis, develops a new model\nwhich he proved to be reducible to the Turing machine model. This work\nis continued by Wilfried Sieg who proposed the framework of Computable\nDynamical Systems (Sieg 2008). Others have considered the possibility\nof “reasonable” models from physics which\n“compute” something that is not Turing computable. See for\ninstance Aaronson, Bavarian, & Gueltrini 2016 (Other Internet\nResources) in which it is shown that if closed timelike curves\nwould exist, the halting problem would become solvable with finite\nresources. Others have proposed alternative models for computation\nwhich are inspired by the Turing machine model but capture specific\naspects of current computing practices for which the Turing machine\nmodel is considered less suited. One example here are the persistent\nTuring machines intended to capture interactive processes. Note\nhowever that these results do not show that there are\n“computable” problems that are not Turing\ncomputable. These and other related proposals have been considered by\nsome authors as reasonable models of computation that somehow compute\nmore than Turing machines. It is the latter kind of statements that\nbecame affiliated with research on so-called hypercomputation\nresulting in the early 2000s in a rather fierce debate in the computer\nscience community, see, e.g., Teuscher 2004 for various positions. \nAs is clear, strictly speaking, Turing’s thesis is not provable,\nsince, in its original form, it is a claim about the relationship\nbetween a formal and a vague or intuitive concept. By consequence,\nmany consider it as a thesis or a definition. The thesis would be\nrefuted if one would be able to provide an intuitively acceptable\neffective procedure for a task that is not Turing-computable. This\nfar, no such counterexample has been found. Other independently\ndefined notions of computability based on alternative foundations,\nsuch as\n recursive functions\n and abacus machines have also been shown to be equivalent to Turing\ncomputability. These equivalences between quite different formulations\nindicate that there is a natural and robust notion of computability\nunderlying our understanding. Given this apparent robustness of our\nnotion of computability, some have proposed to avoid the notion of a\nthesis altogether and instead propose a set of axioms used to sharpen\nthe informal notion. There are several approaches, most notably, an\napproach of structural axiomatization where computability itself is\naxiomatized (Sieg 2008) and one whereby an axiomatization is given\nfrom which the Church-Turing thesis can be derived (Dershowitz &\nGurevich 2008). \nBesides the Turing machine, several other models were introduced\nindependently of Turing in the context of research into the foundation\nof mathematics which resulted in theses that are logically equivalent\nto Turing’s thesis. For each of these models it was proven that\nthey capture the Turing computable functions. Note that the\ndevelopment of the modern computer stimulated the development of other\nmodels such as register machines or Markov algorithms. More recently,\ncomputational approaches in disciplines such as biology or physics,\nresulted in bio-inspired and physics-inspired models such as Petri\nnets or quantum Turing machines. A discussion of such models, however,\nlies beyond the scope of this entry. \nThe original formulation of general\n recursive functions can be\n found in Gödel 1934, which built on a suggestion by Herbrand. In\n Kleene 1936 a simpler definition was given and in Kleene 1943 the\n standard form which uses the so-called minimization or\n \\(\\mu\\)-operator was introduced. For more information, see the entry\n on\n recursive functions. \nChurch used the definition of general recursive functions to state his\nthesis: \nChurch’s thesis Every effectively calculable\nfunction is general recursive \nIn the context of recursive function one uses the notion of recursive\nsolvability and unsolvability rather than Turing computability and\nuncomputability. This terminology is due to Post (1944). \nChurch’s λ-calculus has its origin in the papers (Church\n1932, 1933) and which were intended as a logical foundation for\nmathematics. It was Church’s conviction at that time that this\ndifferent formal approach might avoid Gödel incompleteness (Sieg\n1997: 177). However, the logical system proposed by Church was proven\ninconsistent by his two PhD students Stephen C. Kleene and Barkley\nRosser and so they started to focus on a subpart of that logic which\nwas basically the λ-calculus. Church, Kleene and Rosser started\nto λ-define any calculable function they could think of and\nquite soon Church proposed to define effective calculability in terms\nof λ-definability. However, it was only after Church, Kleene\nand Rosser had established that general recursiveness and\nλ-definability are equivalent that Church announced his thesis\npublicly and in terms of general recursive functions rather than\nλ-definability (Davis 1982; Sieg 1997). \nIn λ-calculus there are only two types of symbols. The three\nprimitive symbols λ, (, ) also called the improper symbols, and\nan infinite list of variables. There are three rules to define the\nwell-formed formulas of λ-calculus, called\nλ-formulas. \nThe λ-formulas, or well-formed formulas of λ-calculus\nare all and only those formulas that result from (repeated)\napplication of these three rules. \nThere are three operations or rules of conversion. Let us define\n\\(\\textrm{S}_{\\mathbf{N}}^{x}\\mathbf{M}|\\) as standing for the formula\nthat results by substitution of N for x in\nM. \nChurch introduces the following abbreviations to define the natural\nnumbers in λ-calculus:  \nUsing this definition, it is possible to λ-define\nfunctions over the positive integers. A function F of one\npositive integer is λ-definable if we can find a\nλ-formula F, such that if \\(F(m) = n\\) and\nm and n are λ-formulas\nstanding for the integers m and n, then the\nλ-formula \\(\\{\\mathbf{F}\\} (\\mathbf{m})\\) can be\nconverted to n by applying the conversion\nrules of λ-calculus. Thus, for example, the successor function\nS, first introduced by Church, can be λ-defined as\nfollows:  \nTo give an example, applying S to the λ-formula standing\nfor 2, we get:  \nToday, λ-calculus is considered to be a basic model in the\ntheory of programming. \nAround 1920–21 Emil Post developed different but related types\nof production systems in order to develop a syntactical form which\nwould allow him to tackle the decision problem for first-order logic.\nOne of these forms are Post canonical systems C which became\nlater known as Post production systems. \nA canonical system consists of a finite alphabet \\(\\Sigma\\), a finite\nset of initial words \\(W_{0,0}\\), \\(W_{0,1}\\),…, \\(W_{0,n}\\)\nand a finite set of production rules of the following form: \nThe symbols g are a kind of metasymbols: they correspond to\nactual sequences of letters in actual productions. The symbols\nP are the operational variables and so can represent any\nsequence of letters in a production. So, for instance, consider a\nproduction system over the alphabet \\(\\Sigma = \\{a,b\\}\\) with initial\nword:  \nand the following production rule: \nThen, starting with \\(W_0\\), there are three possible ways to apply\nthe production rule and in each application the variables \\(P_{1,i}\\)\nwill have different values but the values of the g’s are fixed.\nAny set of finite sequences of words that can be produced by a\ncanonical system is called a canonical set. \nA special class of canonical forms defined by Post are normal systems.\nA normal system N consists of a finite alphabet \\(\\Sigma\\), one\ninitial word \\(W_0 \\in \\Sigma^{\\ast}\\) and a finite set of production\nrules, each of the following form: \nAny set of finite sequences of words that can be produced by a normal\nsystem is called a normal set. Post was able to show that for\nany canonical set C over some alphabet \\(\\Sigma\\) there is a\nnormal set N over an alphabet \\(\\Delta\\) with \\(\\Sigma\n\\subseteq \\Delta\\) such that \\(C = N \\cap \\Sigma^{\\ast}\\). It was his\nconviction that (1) any set of finite sequences that can be generated\nby finite means can be generated by canonical systems and (2) the\nproof that for every canonical set there is a normal set which\ncontains it, which resulted in Post’s thesis I: \nPost’s thesis I (Davis 1982) Every set of\nfinite sequences of letters that can be generated by finite processes\ncan also be generated by normal systems. More particularly, any set of\nwords on an alphabet \\(\\Sigma\\) which can be generated by a finite\nprocess is of the form \\(N \\cap \\Sigma^{\\ast}\\), with N a\nnormal set. \nPost realized that “[for the thesis to obtain its full\ngenerality] a complete analysis would have to be made of all the\npossible ways in which the human mind could set up finite processes\nfor generating sequences” (Post 1965: 408) and it is quite\nprobable that the formulation 1 given in Post 1936 and which is almost\nidentical to Turing’s machines is the result of such an\nanalysis. \nPost production systems became important formal devices in computer\nscience and, more particularly, formal language theory (Davis 1989;\nPullum 2011). \nIn 1936 Post published a short note from which one can derive\nPost’s second thesis (De Mol 2013): \nPost’s thesis II Solvability of a\nproblem in the intuitive sense coincides with solvability by\nformulation 1 \nFormulation 1 is very similar to Turing machines but the\n‘program’ is given as a list of directions which a human\nworker needs to follow. Instead of a one-way infinite tape,\nPost’s ‘machine’ consists of a two-way infinite\nsymbol space divided into boxes. The idea is that a worker is working\nin this symbol space, being capable of a set of five primitive acts\n(\\(O_{1}\\) mark a box, \\(O_{2}\\) unmark a box, \\(O_{3}\\) move one box\nto the left, \\(O_{4}\\) move one box to the right, \\(O_{5}\\)\ndetermining whether the box he is in is marked or unmarked), following\na finite set of directions \\(d_{1}\\),…, \\(d_{n}\\) where each\ndirection \\(d_{i}\\) always has one of the following forms: \nPost also defined a specific terminology for his formulation 1 in\norder to define the solvability of a problem in terms of formulation\n1. These notions are applicability, finite-1-process, 1-solution and\n1-given. Roughly speaking these notions assure that a decision problem\nis solvable with formulation 1 on the condition that the solution\ngiven in the formalism always terminates with a correct solution. \nTuring is today one of the most celebrated figures of computer\nscience. Many consider him as the father of computer science and the\nfact that the main award in the computer science community is called\nthe Turing award is a clear indication of that (Daylight 2015). This\nwas strengthened by the Turing centenary celebrations from 2012, which\nwere largely coordinated by S. Barry Cooper. This resulted not only in\nan enormous number of scientific events around Turing but also a\nnumber of initiatives that brought the idea of Turing as the father of\ncomputer science also to the broader public (Bullynck, Daylight, &\nDe Mol 2015). Amongst Turing’s contributions which are today\nconsidered as pioneering, the 1936 paper on Turing machines stands out\nas the one which has the largest impact on computer science. However,\nrecent historical research shows also that one should treat the impact\nof Turing machines with great care and that one should be careful in\nretrofitting the past into the present. \nToday, the Turing machine and its theory are part of the theoretical\nfoundations of computer science. It is a standard reference in\nresearch on foundational questions such as: \nIt is also one of the main models for research into a broad range of\nsubdisciplines in theoretical computer science such as: variant and\nminimal models of computability, higher-order computability,\n computational complexity theory,\n algorithmic information theory, etc. This significance of the Turing\nmachine model for theoretical computer science has at least two\nhistorical roots. \nFirst of all, there is the continuation of the work in mathematical\nlogic from the 1920s and 1930s by people like Martin Davis—who\nis a student of Post and Church—and Kleene. Within that\ntradition, Turing’s work was of course well-known and the Turing\nmachine was considered as the best model of computability given. Both\nDavis and Kleene published a book in the 1950s on these topics (Kleene\n1952; Davis 1958) which soon became standard references not just for\nearly computability theory but also for more theoretical reflections\nin the late 1950s and 1960s on computing. \nSecondly, one sees that in the 1950s there is a need for theoretical\nmodels to reflect on the new computing machines, their abilities and\nlimitations and this in a more systematic manner. It is in that\ncontext that the theoretical work already done was picked up. One\nimportant development is automata theory in which one can situate,\namongst others, the development of other machine models like the\nregister machine model or the Wang B machine model which are,\nultimately, rooted in Turing’s and Post’s machines; there\nare the minimal machine designs discussed in\n Section 5.2;\n and there is the use of Turing machines in the context of what would\nbecome the origins of formal language theory, viz the study of\ndifferent classes of machines with respect to the different\n“languages” they can recognize and so also their\nlimitations and strengths. It are these more theoretical developments\nthat contributed to the establishment of\n computational complexity theory\n in the 1960s. Of course, besides Turing machines, other models also\nplayed and play an important role in these developments. Still, within\ntheoretical computer science it is mostly the Turing machine which\nremains the model, even today. Indeed, when in 1965 one of the\nfounding papers of computational complexity theory (Hartmanis &\nStearns 1965) is published, it is the multitape Turing machine which\nis introduced as the standard model for the computer. \nIn several accounts, Turing has been identified not just as the father\nof computer science but as the father of the modern computer. The\nclassical story for this more or less goes as follows: the blueprint\nof the modern computer can be found in von Neumann’s EDVAC\ndesign and today classical computers are usually described as having a\nso-called von Neumann architecture. One fundamental idea of the EDVAC\ndesign is the so-called stored-program idea. Roughly speaking this\nmeans the storage of instructions and data in the same memory allowing\nthe manipulation of programs as data. There are good reasons for\nassuming that von Neumann knew the main results of Turing’s\npaper (Davis 1988). Thus, one could argue that the stored-program\nconcept originates in Turing’s notion of the universal Turing\nmachine and, singling this out as the defining feature of the modern\ncomputer, some might claim that Turing is the father of the modern\ncomputer. Another related argument is that Turing was the first who\n“captured” the idea of a general-purpose machine through\nhis notion of the universal machine and that in this sense he also\n“invented” the modern computer (Copeland & Proudfoot\n2011). This argument is then strengthened by the fact that Turing was\nalso involved with the construction of an important class of computing\ndevices (the Bombe) used for decrypting the German Enigma code and\nlater proposed the design of the ACE (Automatic Computing Engine)\nwhich was explicitly identified as a kind of physical realization of\nthe universal machine by Turing himself: \nSome years ago I was researching on what might now be described as an\ninvestigation of the theoretical possibilities and limitations of\ndigital computing machines. […] Machines such as the ACE may be\nregarded as practical versions of this same type of machine. (Turing\n1947) \nNote however that Turing already knew the ENIAC and EDVAC designs and\nproposed the ACE as a kind of improvement on that design (amongst\nothers, it had a simpler hardware architecture). \nThese claims about Turing as the inventor and/or father of the\ncomputer have been scrutinized by some historians of computing\n(Daylight 2014; Haigh 2013; Haigh 2014; Mounier-Kuhn 2012), mostly in the wake of the Turing centenary and\nthis from several perspectives. Based on that research it is clear\nthat claims about Turing being the inventor of the modern computer\ngive a distorted and biased picture of the development of the modern\ncomputer. At best, he is one of the many who made a contribution to\none of the several historical developments (scientific, political,\ntechnological, social and industrial) which resulted, ultimately, in\n(our concept of) the modern computer. Indeed, the “first”\ncomputers are the result of a wide number of innovations and so are\nrooted in the work of not just one but several people with diverse\nbackgrounds and viewpoints. \nIn the 1950s then the (universal) Turing machine starts to become an\naccepted model in relation to actual computers and is used as a tool\nto reflect on the limits and potentials of general-purpose computers\nby both engineers, mathematicians and logicians. More particularly,\nwith respect to machine designs, it was the insight that only a few\nnumber of operations were required to built a general-purpose machine\nwhich inspired in the 1950s reflections on minimal machine\narchitectures. Frankel, who (partially) constructed the MINAC stated\nthis as follows: \nOne remarkable result of Turing’s investigation is that he was\nable to describe a single computer which is able to compute\nany computable number. He called this machine a universal\ncomputer. It is thus the “best possible” computer\nmentioned. \n[…] This surprising result shows that in examining the question\nof what problems are, in principle, solvable by computing machines, we\ndo not need to consider an infinite series of computers of greater and\ngreater complexity but may think only of a single machine.  \nEven more surprising than the theoretical possibility of such a\n“best possible” computer is the fact that it need not be\nvery complex. The description given by Turing of a universal computer\nis not unique. Many computers, some of quite modest complexity,\nsatisfy the requirements for a universal computer. (Frankel 1956:\n635) \nThe result was a series of experimental machines such as the MINAC,\nTX-0 (Lincoln Lab) or the ZERO machine (van der Poel) which in their\nturn became predecessors of a number of commercial machines. It is\nworth pointing out that also Turing’s ACE machine design fits\ninto this philosophy. It was also commercialized as the BENDIX G15\nmachine (De Mol, Bullynck, & Daylight 2018). \nOf course, by minimizing the machine instructions, coding or\nprogramming became a much more complicated task. To put it in\nTuring’s words who clearly realized this trade-off between code\nand (hard-wired) instructions when designing the ACE: “[W]e have\noften simplified the circuit at the expense of the code” (Turing\n1947).\nAnd indeed, one sees that with these early minimal designs, much\neffort goes into developing more efficient coding strategies. It is\nhere that one can also situate one historical root of making the\nconnection between the universal Turing machine and the important\nprinciple of the interchangeability between hardware and programs. \nToday, the universal Turing machine is by many still considered as the\nmain theoretical model of the modern computer especially in relation\nto the so-called von Neumann architecture. Of course, other models\nhave been introduced for other architectures such as the Bulk\nsynchronous parallel model for parallel machines or the persistent\nTuring machine for modeling interactive problems. \nThe idea that any general-purpose machine can, in principle, be\nmodeled as a universal Turing machine also became an important\nprinciple in the context of automatic programming in the 1950s\n(Daylight 2015). In the machine design context it was the minimizing\nof the machine instructions that was the most important consequence of\nthat viewpoint. In the programming context then it was about the idea\nthat one can built a machine that is able to\n‘mimic’’ the behavior of any other machine and so,\nultimately, the interchangeability between machine hardware and\nlanguage implementations. This is introduced in several forms in the\n1950s by people like John W. Carr III and Saul Gorn—who were\nalso actively involved in the shaping of the Association for\nComputing Machinery (ACM)—as the unifying theoretical idea\nfor automatic programming which indeed is about the (automatic)\n“translation” of higher-order to lower-level, and,\nultimately, machine code. Thus, also in the context of programming,\nthe universal Turing machine starts to take on its foundational role\nin the 1950s (Daylight 2015). \nWhereas the Turing machine is and was a fundamental theoretical model\ndelimiting what is possible and not on the general level, it did not\nhave a real impact on the syntax and semantics of programming\nlanguages. In that context it were rather λ-calculus and Post\nproduction systems that had an effect (though also here one should be\ncareful in overstating the influence of a formal model on a\nprogramming practice). In fact, Turing machines were often regarded as\nmachine models rather than as a model for programming: \nTuring machines are not conceptually different from the automatic\ncomputers in general use, but they are very poor in their control\nstructure. […] Of course, most of the theory of computability\ndeals with questions which are not concerned with the particular ways\ncomputations are represented. It is sufficient that computable\nfunctions be represented somehow by symbolic expressions, e.g.,\nnumbers, and that functions computable in terms of given functions be\nsomehow represented by expressions computable in terms of the\nexpressions representing the original functions. However, a practical\ntheory of computation must be applicable to particular algorithms.\n(McCarthy 1963: 37) \nThus one sees that the role of the Turing machine for computer science\nshould be situated rather on the theoretical level: the universal\nmachine is today by many still considered as the model for the modern\ncomputer while its ability to mimic machines through its manipulation\nof programs-as-data is one of the basic principles of modern\ncomputing. Moreover, its robustness and naturalness as a model of\ncomputability have made it the main model to challenge if one is\nattacking versions of the so-called (physical) Church-Turing\nthesis.","contact.mail":"liesbeth.demol@univ-lille3.fr","contact.domain":"univ-lille3.fr"}]
