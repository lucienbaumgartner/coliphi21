[{"date.published":"2003-08-27","date.changed":"2018-11-02","url":"https://plato.stanford.edu/entries/logic-ai/","author1":"Richmond Thomason","entry":"logic-ai","body.text":"\n\n\n\nArtificial Intelligence (referred to hereafter by its nickname,\n“AI”) is the subfield of Computer Science devoted to developing\nprograms that enable computers to display behavior that can (broadly)\nbe characterized as\n intelligent.[1]\n Most research in AI is devoted to\nfairly narrow applications, such as planning or speech-to-speech\ntranslation in limited, well defined task domains. But substantial\ninterest remains in the long-range goal of building generally\nintelligent, autonomous\n agents,[2]\n even if the goal of fully human-like intelligence is elusive and\nis seldom pursued explicitly and as such.\n\n \n\n\nThroughout its relatively short history, AI has been heavily\ninfluenced by logical ideas. AI has drawn on many research\nmethodologies:  the value and relative importance of logical formalisms\nis questioned by some leading practitioners, and has been debated in\nthe literature from time to\n time.[3]\n But most members of the AI community\nwould agree that logic has an important role to play in at least some\ncentral areas of AI research, and an influential minority considers\nlogic to be the most important factor in enabling strategic,\nfundamental advances.\n\n\n\nThe relations between AI and philosophical logic are part of a\nlarger story. It is hard to find a major philosophical theme that\ndoesn’t become entangled with issues having to do with reasoning.\nImplicatures, for instance, have to correspond to inferences that can\nbe carried out by a rational interpreter of discourse. Whatever\ncausality is, causal relations should be inferrable in everyday common\nsense settings. Whatever belief is, it should be possible for rational\nagents to make plausible inferences about the beliefs of other agents.\nThe goals and standing constraints that inform a rational agent’s\nbehavior must permit the formation of reasonable plans.\n\n\n\nIn each of these cases, compatibility with an acceptable account of\nthe relevant reasoning is essential for a successful philosophical\ntheory. But the methods in the contemporary philosophical inventory are\ntoo crude to provide anything like an adequate account of reasoning\nthat is this complex and this entangled in broad world knowledge.\n\n\n\nBringing an eclectic set of conceptual tools to the problem of\nidealized reasoning in realistic settings, and using computers to model\nand test the theories, research in AI has transformed the study of\nreasoning—especially of practical, common sense reasoning. This\nprocess and its outcome is well documented in\n Russell & Norvig 2010.\n\n\n\nThe new insights and theories that have emerged from AI are of great\npotential value in informing and constraining many areas of\nphilosophical inquiry. The special case of philosophical logic that\nforms the theme of this article may provide support for the more\ngeneral point. Although logic in AI grew out of philosophical logic,\nin its new setting it has produced new theories and ambitious programs\nthat would not have been possible outside of a community devoted to\nbuilding full-scale computational models of rational agency.\n\n\n\nThis entry assumes an audience consisting primarily of\nphilosophers who have little or no familiarity with AI. The\nentry concentrates on the issues that arise when\nlogic is used in understanding problems in intelligent reasoning and\nguiding the design of mechanized reasoning systems. Logic in AI is by\nnow a very large and not very well demarcated field—nothing\nlike complete coverage has been achieved here. \n Sections 3\n and\n Section 4\n provide an overview with some historical and\ntechnical details concerning nonmonotonic logic and reasoning about\naction and change, a topic that is not only central in AI but that\nshould be of considerable interest to philosophers. The remaining\nsections provide brief and more or less inadequate sketches of\nselected topics, with references to the primary literature.\n\n\n\n Minker 2000b is a comprehensive\ncollection of survey papers and original contributions to the field of\nlogic-based AI, with extensive references to the literature.  Jack Minker’s\nintroduction, Minker 2000a, is a useful\norientation to the field. This volume is \na good beginning point for readers who wish to pursue this topic\nfurther.  Brachman\n& Levesque 2004a provides an introduction to the field of\nknowledge representation in textbook form.  \nDavis 1991a and\nMueller 2006a are book-length treatments\nof the challenging problem of formalizing commonsense reasoning.  \nAntonelli 2012a is a good entry point\nfor readers interested in nonmonotonic logic, and\nShanahan 2009a is a useful discussion of\nthe frame problem.  \nWooldridge 2000a deals with logical\nformalizations of rational agents.\n\n\n\nContents of this entry:\n\n\n\nTheoretical computer science developed out of logic, the theory of\ncomputation (if this is to be considered a different subject from\nlogic), and some related areas of\n mathematics.[4]\n So theoretically minded\ncomputer scientists are well informed about logic even when they aren’t\nlogicians. Computer scientists in general are familiar with the idea\nthat logic provides techniques for analyzing the inferential properties\nof languages, and with the distinction between a high-level logical\nanalysis of a reasoning problem and its implementations. Logic, for\ninstance, can provide a specification for a programming language by\ncharacterizing a mapping from programs to the computations that they\nlicense. A compiler that implements the language can be incomplete, or\neven unsound, as long as in some sense it approximates the logical\nspecification. This makes it possible for the involvement of logic in\nAI applications to vary from relatively weak uses in which the logic\ninforms the implementation process with analytic insights, to strong\nuses in which the implementation algorithm can be shown to be sound and\ncomplete. In some cases, a working system is inspired by ideas from\nlogic and then acquires features that at first seem logically problematic\nbut can later be explained by developing new ideas in logical theory.\nThis sort of thing has happened, for instance, in logic\nprogramming. \n\nIn particular, logical theories in AI are independent from\nimplementations. They can be used to provide insights into the\nreasoning problem without directly informing the implementation. Direct\nimplementations of ideas from logic—theorem-proving and\nmodel-construction techniques—are used in AI, but the AI theorists who\nrely on logic to model their problem areas are free to use other\nimplementation techniques as well. Thus, in\n Moore 1995b\n (Chapter 1), Robert C.\nMoore distinguishes three uses of logic in AI; as a tool of analysis,\nas a basis for knowledge representation, and as a programming\nlanguage. \n\nA large part of the effort of developing limited-objective reasoning\nsystems goes into the management of large, complex bodies of\ndeclarative information. It is generally recognized in AI that it is\nimportant to treat the representation of this information, and the\nreasoning that goes along with it, as a separate task, with its own\nresearch problems. \n\nThe evolution of expert systems illustrates the point. The earliest\nexpert systems, such as MYCIN (a program that reasons\nabout bacterial infections, see\n Buchanan & Shortliffe 1984),\nwere based entirely on large systems of procedural rules, with no\nseparate representation of the background knowledge—for instance, the\ntaxonomy of the infectious organisms about which the system reasoned\nwas not represented. \n\nLater generation expert systems show a greater modularity in their\ndesign. A separate knowledge representation component is useful for\nsoftware engineering purposes—it is much better to have a single\nrepresentation of a general fact that can have many different uses,\nsince this makes the system easier to develop and to modify. And this\ndesign turns out to be essential in enabling these systems to deliver\nexplanations as well as mere\n conclusions.[5] \n\nIn response to the need to design this declarative component, a\nsubfield of AI known as knowledge representation emerged\nduring the 1980s. Knowledge representation deals primarily with the\nrepresentational and reasoning challenges of this separate component.\nThe best place to get a feel for this subject is the proceedings of the\nmeetings that are now held every other year: see\n Brachman et al. 1989,\n Allen et al. 1991,\n Nebel et al. 1992,\n Doyle et al. 1994,\n Aiello et al. 1996,\n Cohn et al. 1998,\n Cohn et al. 2000,\n Fensel et al. 2002,\n Dubois et al. 2004,\n Doherty et al. 2006,\n Brewka & Lang 2008,\n Lin et al. 2010,\n Eiter et al. 2012,\n Baral et al. 2012,\n and\n Baral et al. 2016. \n\nTypical articles in the proceedings of the KR and Reasoning\nconferences deal with the following topics. \n\nThese topics hardly overlap at all with the contents of the Journal\nof Symbolic Logic, the principal research archive for mathematical\nlogic. But there is substantial overlap in theoretical emphasis with\nThe Journal of Philosophical Logic, where topics such as tense\nlogic, epistemic logic, logical approaches to practical reasoning,\nbelief change, and vagueness account for a large percentage of the\ncontributions. Very few JPL publications, however, deal with\ncomplexity theory or with potential applications to automated\nreasoning.  \n\nA history of philosophical logic is yet to be written.  \nThough philosophical logic has traditionally been distinguised from\nmathematical logic, the distinction may well be incidental in relation\nto the overall goals of the subject, since technical rigor and the use\nof mathematical methods seem to be essential in all areas of logical\nresearch. However, the distinction between the two subfields has been\nmagnified by differences in the sorts of professional training that\nare available to logicians, and by the views of individuals on what is\nimportant for the field.  \n\nThe statement of policy presented in\nJournal of Symbolic Logic (1936, Volume 1, No. 1) lists\nbringing together the mathematicians and philosophers working in logic\namong the goals of the new journal. Probably at this time both the\nmathematicians and the philosophers shared a sense that their subject\nwas considered to be somewhat marginal by their colleagues, and may\nhave felt a primary loyalty to logic as a subject rather than to any\nacademic discipline.  Articles in the first volume of the JSL\nwere divided about equally between professional mathematicians and\nphilosophers, and the early volumes of the JSL do not show\nany strong differences between the two groups as to topic. \n\nThis situation changed in the 1960s. The 1969 volume of the\nJSL contained 39 articles by mathematicians, and only nine by\nphilosophers. By the early 1970s, many philosophers felt that\nphilosophical papers on logic were unlikely to be accepted by the\nJSL, and that if they were accepted they were unlikely to be\nread by philosophers. At this point, the goals of the two groups had\ndiverged considerably. Mathematicians were pursuing the development of\nan increasingly technical and complex body of methods and theorems.\nMany philosophers felt that this pursuit was increasingly irrelevant to\nthe goal of illuminating philosophical issues. These divisions led to\nthe founding of the Journal of Philosophical Logic in 1972.\nThe list of sample topics in the first issue included: \n\nMost of the articles over the subsequent 28 years of the JPL\nbelong to the first of these four categories. But the description with\nwhich this list begins is not particularly illuminating: why should\nthese particular topics be of interest to philosophers? Their most\nimportant shared feature is a sense that despite successes in\nformalizing areas of mathematical logic, the scope of logic remained\nseverely limited. There are unsolved problems in formalizing the\nnonmathematical sciences that seem to require thinking through new and\ndifferent logical issues (quantum logic and the logic of induction,\nfor instance). The remaining topics cover a part, at least, of the\neven more pressing problems involved in extending logical theory to\nnonscientific reasoning. The dominant goal, then, of philosophical\nlogic is the extension of logical methods to nonmathematical reasoning\ndomains. This goal has a theoretical dimension if (as many\nphilosophical logicians seem to feel) it requires reworking and\nextending logical formalisms.  \n\nThe development and testing of applications, such as the problem of\nformalizing the reasoning involved in getting to the airport, posed as\na challenge in\n McCarthy 1959\n (see\n Section 2.2,\n below), doesn’t even appear as a\ncategory in the list of JPL topics, and in fact most of the\nphilosophical logic literature is theoretical in nature, and is tested\nusing philosophical techniques. Essentially, this means that the\ntheories are motivated and tested with small-scale, artificial\nexamples, selected by the theoreticians. These examples usually serve\nmore as demonstrations or illustrations than as tests. \n\nThe rough comparison in\n Section 1.2\n of the\ncontents of the main publications for research in logical AI and\nphilosophical logic suggests the following picture. Theoretical work in\nlogical AI and in philosophical logic overlap to a large extent. Both\nare interested in developing nonmetamathematical applications of logic,\nand the core topics are very similar. This overlap is due not only to\ncommonality of interest, but to direct influence of philosophical logic\non logical AI; there is ample evidence, as we will see, that the first\ngeneration at least of AI logicists read and were influenced by the\nliterature in philosophical logic. \n\nSince that point, the specialties have diverged. New logical\ntheories have emerged in logical AI (nonmonotonic logic is the most\nimportant example) which are not widely known in philosophical logic.\nOther differences are due to the AI community’s interest in the\ntheoretical analysis of algorithms and, of course, with their sense of\nthe importance of implementations. Some have to do with the emerging\ndevelopment in computer science of ambitious applications using\nunprecedentedly large bodies of logical axioms. The sheer size of these\napplications produces new problems and new methodologies. And other\ndifferences originate in the interest of philosophical logicians in\nsome topics (metaphysical topics, for instance) that are primarily\ninspired by purely philosophical considerations. \n\nConcern for applications can be a great influence on how research is\ncarried out and presented. The tradition in philosophical logic\npredates applications in automated reasoning, and to this day remains\nrelatively uninterested in such applications. The methodology depends\non intuitions, but without any generally accepted methodology for\narticulating and deploying these intuitions. And the ideas are\nillustrated and informed by artificial, small-scale\n examples.[6]\n In general,\nthe philosophical literature does not deal with implementability or\nefficiency of the reasoning, or indeed with any features of the\nreasoning process. And it is hard to find cases in which the\nphilosophical theories are illustrated or tested with realistic,\nlarge-scale reasoning problems. \n\nThese differences, however, are much more a matter of style than of\nsubstance or of strategic research goals. It is difficult to think\nthrough the details of the reasoning process without the computational\ntools to make the process concrete, and difficult to develop\nlarge-scale formalizations of reasoning problems without computational\ntools for entering, testing, and maintaining the formalizations.\nBecause the core theoretical topics (modal, conditional and temporal\nlogic, belief revision, and the logic of context) are so similar, and\nbecause the ultimate goal (the formalization of nonmathematical\nreasoning) is the same, one can see logic in AI as a continuous\nextension of the philosophical logic tradition. \n\nThe early influence of philosophical logic on logic in AI was\nprofound. The bibliography of\n McCarthy & Hayes 1969,\n one\nof the most influential early papers in logical AI, illustrates the\npoint well. There are 58 citations in the bibliography. Of these, 35\nrefer to the philosophical logic literature. (There are 17 computer\nscience citations, one mathematical logic citation, one economics\ncitation, and one psychology citation.) This paper was written at a\ntime when there were hardly any references to logical AI in the\ncomputer science literature. Naturally, as logical AI has matured and\ndeveloped as a branch of computer science, the proportion of\ncross-disciplinary citations has decreased. A sampling of articles from\nthe first Knowledge Representation conference,\n Brachman et al. 1989,\n held in\n1989, shows only 12 philosophical logic citations out of a total of 522\nsampled citations; a sampling of articles from\n Cohn et al. 1998,\n held in 1998,\nshows 23 philosophical logic citations out of a total of 468\nsampled.[7] \n\nDespite the dramatic decrease in quantity of explicit citations, the\ncontemporary literature in logical AI reflects an indirect acquaintance\nwith the earlier literature in philosophical logic, since many of the\ncomputational papers that are explicitly cited in the modern works were\ninfluenced by this literature. Of course, the influence becomes\nincreasingly distant as time passes, and this trend is accelerated by\nthe fact that new theoretical topics have been invented in logical AI\nthat were at best only dimly prefigured in the philosophical\nliterature. \n\nAlthough philosophical logic is now a relatively small field in\ncomparison to logical AI, it remains a viable area of research, with new\nwork appearing regularly. But references to contemporary research in\nphilosophical logic are rare in the AI literature. Similarly, the\npapers currently published in The Journal of Philosophical\nLogic, at least, do not show much influence from\n AI.[8]\n In Europe,\nthe lines are harder to draw between professional divisions among\nlogicians: some European journals, especially the Journal of Logic,\nLanguage, and Information, are successful in maintaining a focus\nin logic while attracting authors from all the disciplines in which\nlogic is represented. \n\nThe importance of applications in logical AI, and the scale of these\napplications, represents a new methodology for logic—one that would\nhave been impossible without mechanized reasoning. This methodology\nforces theoreticians to think through problems on a new scale and at a\nnew level of detail, and this in turn has a profound effect on the\nresulting theories. The effects of this methodology will be illustrated\nin the sections below, dealing with various topics in logical AI. But\nthe point is illustrated well by reasoning about action and change.\nThis topic was investigated in the philosophical literature. Reasoning\nabout change, at least, is part of tense logic, and the consequences of\naction are investigated in the literature on “seeing to it that”; see,\nfor instance,\n Belnap 1996.\n The latter\ntheory has no very robust account of action. The central construct is a\nvariation on a branching-time modality of the sort that has been\nfamiliar since\n Prior 1967.\n Although it\nrepresents an interesting development in philosophical logic, the scale\nof the accomplishment is very different from the research tradition in\nlogical AI reported in\n Section 4,\n below. The\nformalisms in this tradition not only support the formalization of\ncomplex, realistic planning problems, but provide entirely new insights\ninto reasoning about the causal effects of actions, the persistence of\nstates, and the interactions between actions and continuous physical\nprocesses. Developments such as this would have been impossible without\nthe interactions between the logical theories and large-scale,\npractical applications in automated planning. \n\nIn\n Carnap 1955,\n Rudolf Carnap attempted\nto clarify intensional analyses of linguistic meaning, and to justify\nfrom a methodological point of view, by imagining how the analysis\ncould be applied to the linguistic usage of a hypothetical robot.\nCarnap hoped that the fact that we could imagine ourselves to know the\ninternal structure of the robot would help to make the case for an\nempirical science of semantics more plausible. This hope proved to be\nunjustified; the philosophical issue that concerned Carnap remains\ncontroversial to this day, and thought experiments with robots have not\nproved to be particularly rewarding in addressing it. Real robots,\nthough, with real\n applications,[9]\n are a very different matter. Though it\nis hard to tell whether they will prove to be helpful in clarifying\nfundamental philosophical problems, they provide a laboratory for logic\nthat is revolutionary in its potential impact on the subject. They\nmotivate the development of entirely new logical theories that should\nprove to be as important for philosophy as the fundamental\ndevelopments in logic of the late nineteenth century proved to be. \n\nThe emergence of separate mathematical and philosophical\nsubspecialties within logic was not an entirely healthy thing for the\nfield. The process of making mathematical logic rigorous and of\ndemonstrating the usefulness of the techniques in achieving mathematical\nends that was pursued so successfully in the first half of the\ntwentieth century represents a coherent refinement of logical\nmethodology. All logicians should be pleased and proud that logic is\nnow an area with a body of results and problems that is as substantial\nand challenging as those associated with most areas of mathematics. \n\nBut these methodological advances were gained at the expense of\ncoverage. In the final analysis, logic deals with reasoning—and\nrelatively little of the reasoning we do is mathematical, while almost\nall of the mathematical reasoning that nonmathematicians do is mere\ncalculation. To have both rigor and scope, logic needs to keep its\nmathematical and its philosophical side united in a single discipline.\nIn recent years, neither the mathematical nor the philosophical\nprofessions—and this is especially true in the United\nStates—have done a great deal to promote this unity. But the\nneeds of Computer Science provide strong unifying motives. The\nprofessional standards for logical research in Computer Science\ncertainly require rigor, but the field also puts its practitioners into\ncontact with reasoning domains that are not strictly mathematical, and\ncreates needs for innovative logical theorizing. \n\nThe most innovative and ambitious area of Computer Science, in terms\nof its coverage of reasoning, and the one that is closest in spirit to\nphilosophical logic, is AI. This article will attempt to provide an\nintroduction, for outsiders who are familiar with logic, to the aspects\nof AI that are closest to the philosophical logic tradition. This area\nof logic deserves, and urgently needs, to be studied by historians.  But\nsuch a study will not be found here. \n\nThe most influential figure in logical AI is John McCarthy. McCarthy\nwas one of the founders of AI, and consistently advocated a research\nmethodology that uses logical techniques to formalize the reasoning\nproblems that AI needs to solve. All but the most recent work in\nMcCarthy’s research program can be found in\n Lifschitz 1990a,\n which also contains an\nintroduction to McCarthy’s work\n Lifschitz 1990b;\n for additional historical background, see\n Israel 1991. \n\nMcCarthy’s methodological position has not changed substantially\nsince it was first articulated in\n McCarthy 1959\n and elaborated and amended\nin\n McCarthy & Hayes 1969.\n The motivation for using logic is that—even if the\neventual implementations do not directly and simply use logical\nreasoning techniques like theorem proving—a logical formalization\nhelps us to understand the reasoning problem itself. The claim is that\nwithout an understanding of what the reasoning problems are, it will\nnot be possible to implement their solutions. Plausible as this\nPlatonic argument may seem, it is in fact controversial in the context\nof AI; an alternative methodology would seek to learn or evolve the\ndesired behaviors. The representations and reasoning that this\nmethodology would produce might well be too complex to characterize or\nto understand at a conceptual level. \n\nFrom\n McCarthy & Hayes 1969,\n it is clear that McCarthy thought of his methodology for AI as\noverlapping to a large extent with traditional philosophy, but adding\nto it the need to inform the design of programs capable of manifesting\ngeneral intelligence. This idea is not uncongenial to some philosophers\n(see, for instance,\n Carnap 1956\n(pp. 244–247) and\n Pollock 1995).\n In\npractice, the actual theories that have emerged from McCarthy’s\nmethodology are influenced most strongly by work in philosophical\nlogic, and the research tradition in logical AI represents a more or\nless direct development of this work, with some changes in emphasis.\nThis review will concentrate on logical AI in relation to philosophical\nlogic, without further comment on relations to philosophy in general or\nto the feasibility of developing human-level intelligent systems. \n\nMcCarthy’s long-term objective was to formalize common sense\nreasoning, the prescientific reasoning that is used in dealing\nwith everyday problems. An early example of such a problem, mentioned\nin\n McCarthy 1959,\n is getting from\nhome to the airport. Other examples include: \n\nStated baldly, the goal of formalizing common sense would probably\nseem outrageous to most philosophers, who are trained to think of\ncommon sense as rather elusive. But whether or not the ultimate goal\nis appropriate and achievable, the specific formalization projects\nthat have emerged from this program have been successful in several\nways.  They have succeeded in breaking new territory for logic by\nextending the scope of the reasoning problems to which logical\ntechniques can be successfully applied. They have demonstrated that\nlogical techniques can contribute usefully to the solution of specific\nAI problems—planning is the most successful of these, but some\nsuccess has been achieved in other areas as\nwell.[11] They\nform the basis of one approach to developing complete, autonomous\nagents.[12] And\nthey have illuminated many specific forms of nonscientific\nreasoning—for instance, qualitative reasoning about the behavior\nof physical devices.[13]  Although McCarthy has advocated this program of formalization\nsince 1959—an almost prehistorical date for AI—the program has\nnot been taken on board and pursued by a dedicated community until\nrecently.  Research in this area has gained much momentum since 1990.\nFor further discussion and details, see \n Section 8. \n\nAristotle believed that most reasoning, including reasoning about\nwhat to do and about sublunary natural phenomena, dealt with things\nthat hold “always or for the most part.” But Aristotelian logic deals\nonly with patterns of inference that hold without exception. We find at\nthe very beginning of logic a discrepancy between the scope of logical\ntheory and common sense reasoning. Nonmonotonic logic is the first\nsustained attempt within logical theory to remedy this discrepancy. As\nsuch, it represents a potential for a sweeping expansion of the scope\nof logic, as well as a significant body of technical results. \n\nThe consequence relations of classical logics are monotonic. That\nis, if a set Γ of formulas implies a consequence C then\na larger set Γ ∪ A will also imply C. A\nlogic is nonmonotonic if its consequence relation lacks this\nproperty. Preferred models provide a general way to induce a\nnonmonotonic consequence relation. Invoke a function that for each\nΓ produces a subset\n MΓ of the models of Γ; in general,\nwe will expect\n MΓ to\nbe a proper subset of these models. We then say that Γ implies\nC if C is satisfied by every model in\n MΓ. As long as we do not\nsuppose that\n MΓ∪{A} ⊆\n MΓ, we can easily have an\nimplication relation between Γ and C without imposing\nthis relation on supersets of\n Γ.[14] \n\nThis model theoretic behavior corresponds to expectation-guided\nreasoning, where the expectations allow certain cases to be neglected.\nHere is an important difference between common sense and mathematics.\nMathematicians are trained to reject a proof by cases unless the cases\nexhaust all the possibilities; but typical instances of common sense\nreasoning neglect some alternatives. In fact, it is reasonable to\nroutinely ignore improbable possibilities. Standing in a kitchen in\nCalifornia, wondering if there is time to wash the dishes before leaving\nfor work, one might not take the possibility of an earthquake into\naccount. \n\nThere seem to be many legitimate reasons for neglecting certain\ncases in common sense reasoning. A qualitative judgment that the\nprobability of a case is negligible is one reason. But, for instance,\nin a planning context it may be reasonable to ignore even nonnegligible\nprobabilities, as long as there is no practical point in planning on\nthese cases. \n\nThe motivations for nonmonotonicity seem to involve a number of\ncomplex factors; probability (perhaps in some qualitative sense),\nnormality, expectations that are reasonable in the sense that one can’t\nbe reasonably blamed for having them, mutual acceptance, and factors\nhaving to do with limited rationality.  It may well be that no one has\nsucceeded in disentangling and clarifying these motivating\nconsiderations. In the early stages of its emergence in logical AI,\nmany researchers seem to have thought of nonmonotonic reasoning as a\ngeneral method for reasoning about uncertainty; but by the end of the\n1980s, implementations of fully quantitative probabilistic reasoning\nwere not only possible in principle, but were clearly preferable in\nmany sorts of applications to methods involving nonmonotonic logic. A\nplausible and realistic rationale for nonmonotonic logic has to fit it\ninto a broader picture of reasoning about uncertainty that also\nincludes probabilistic\n reasoning.[15] \n\nThree influential papers on nonmonotonic logic appeared in 1980:\n McDermott & Doyle 1980,\n Reiter 1980,\n and\n McCarthy 1980.\n In each case, the\nformalisms presented in these papers were the result of a gestation\nperiod of several years or more. To set out the historical influences\naccurately, it would be necessary to interview the authors, and this \nhas not been done. However, there seem to have been two motivating factors:\nstrategic considerations having to do with the long-range goals of AI,\nand much more specific, tactical considerations arising from the\nanalysis of the reasoning systems that were being deployed in the\n1970s. \n\n Section 2.2\n drew attention to McCarthy’s\nproposed goal of formalizing common sense reasoning. The brief\ndiscussion above in\n Section 3.1\n suggests that\nmonotonicity may be an obstacle in pursuing this goal. An additional\nmotive was found in\n Minsky 1974,\n which was\nwidely read at the time. This paper presents an assortment of\nchallenges for AI, focusing at the outset on the problem of natural\nlanguage\n understanding.[16]\n Minsky advocates frame-based\nknowledge representation\n techniques[17]\n and (conceiving of the use of these\nrepresentations as an alternative to logic), he throws out a number of\nloosely connected challenges for the logical approach, including the\nproblem of building large-scale representations, of reasoning\nefficiently, of representing control knowledge, and of providing for\nthe flexible revision of defeasible beliefs. In retrospect, \nmost AI researchers would tend to agree that these problems are general\nchallenges to any research program in AI (including the one Minsky\nhimself advocated at the time) and that logical techniques are an\nimportant element in addressing some, perhaps all, of the issues. (For\ninstance, a well structured logical design can be a great help in\nscaling up knowledge representation.) \n\nMinsky apparently intended to provide general arguments against\nlogical methods in AI, but\n McDermott & Doyle 1980\n and\n McCarthy 1980\n interpret\n Minsky 1974\n as a challenge that can be met by\ndeveloping logics that lack the monotonicity property. Perhaps\nunintentionally, the paper seems to have provided some incentive\nto the nonmonotonic logicians by stressing monotonicity as a source of\nthe alleged shortcomings of logic. In fact, the term\n‘monotonicity’ apparently makes its first appearance in\nprint in Minsky’s 1974 paper. \n\nThe development of nonmonotonic logic also owes a great deal to the\napplied side of AI. In fact, the need for a nonmonotonic analysis of a\nnumber of AI applications was as persuasive as the strategic\nconsiderations urged by McCarthy, and in many ways more influential on\nthe shape of the formalisms that emerged. Here, we mention three\nsuch applications that appear to have been important for some of the\nearly nonmonotonic logicians: belief revision, closed-world reasoning,\nand planning. \n\nIn a TMS, part of the support for a belief can consist in the\nabsence of some other belief. This introduces nonmonotonicity.\nFor instance, it provides for defaults; that Wednesday is the default\nday for scheduling a meeting means the belief that the meeting will be\non Wednesday depends on the absence of special-case beliefs\nentailing that it will not be on Wednesday. \n\nThe TMS algorithm and its refinements had a significant impact on AI\napplications, and this created the need for a logical analysis. (In\neven fairly simple cases, it can be hard in the absence of analytic\ntools to see what consequences a TMS should deliver.) This provided a\nnatural and highly specific challenge for those seeking to develop a\nnonmonotonic logic. The TMS also provided specific intuitions: the idea\nthat the key to nonmonotonicity has to do with inferences based on\nunprovability was important for the modal approaches to nonmonotonic\nlogic and for default logic. And the TMS’s emphasis on interactions\nbetween arguments began a theme in nonmonotonic logic that remains\nimportant to this day. (See the discussion of argument-based\napproaches, in\n Section 3.4,\n below.) \n\nThe study of databases belongs to computer science, not specifically to\nAI. But one of the research paradigms in the scientific analysis of\ndatabases uses logical models of the representations and reasoning\n (see\n Minker 1997\n for a recent survey of the\nfield), and this area has interacted with logical AI. The deductive\ndatabase paradigm was taking shape at about the same time that many AI\nresearchers were thinking through the problems of nonmonotonic logic,\nand provided several specific examples of nonmonotonic reasoning that\ncalled for analyses. Of these, perhaps the most important is the\nclosed-world assumption, according to which—at least as\nfar as simple facts are concerned, represented in the database as\npositive or negative literals—the system assumes that it knows\nall that there is to be known. It is the closed world assumption that\njustifies a negative answer to a query “Is there a direct flight\nfrom Detroit to Bologna?” when the system finds no such flight in\nits data. This is another case of inference from the absence of a\nproof; a negative is proved, in effect, by the failure of a systematic\nattempt to prove the positive. This idea, which was investigated in\npapers such as\n Reiter 1978\n and\n Clark 1978\n also provided a challenge for\nnonmonotonic logics, as well as specific intuitions—note that\nagain, the idea of inference rules depending on the absence of a proof\nis present here.  \n\nRational planning is impossible without the ability to reason about\nthe outcomes of a series of contemplated actions. Predictive reasoning\nof this sort is local; in a complex world with many features, we\nassume that most things will be unchanged by the performance of an\naction. But this locality has proved to be difficult to formalize. The\nproblem of how to formalize this “causal\ninertia”[18] is known as the Frame\nProblem.  \n\nIt is very natural to suppose that inertia holds by default;\nvariables are unchanged by the performance of an action unless there is\na special reason to think that they will change. This suggests that\nnonmonotonic temporal formalisms should provide an appropriate\nfoundation for reasoning about action and change. So attempts to\nformalize the reasoning needed in planning also created a need for\nnonmonotonic logics. One of the earliest attempts to formalize\nnonmonotonic reasoning,\n Sandewall 1972,\naddresses the frame problem. Inertial defaults are an especially\nimportant and instructive case study; no more will be said about them\nhere, since they are discussed in detail in\n Section 4.4,\n below. \n\nThe three 1980 papers mentioned at the beginning of\n Section 3.2\n represent three approaches to\nnonmonotonic logic that remain important subfields to this day:\ncircumscription (McCarthy), modal approaches (Doyle\n& McDermott) and default logic (Reiter). \n\nIn\n McCarthy 1993a,\n McCarthy urges\nus, when considering the early history of circumscription, to take into\naccount a group of three papers: McCarthy\n 1986,\n 1980,\n and\n 1987.\n The first paper connects the\nstrategic ideas of\n McCarthy & Hayes 1969\n with the need for a nonmonotonic logic, and\nsketches the logical ideas of domain circumscription, which is\nnow classified as the simplest case of circumscription. The second\npaper provides more thorough logical foundations, and introduces the\nmore general and powerful predicate circumscription approach.\nThe third paper concentrates on developing techniques for formalizing\nchallenging common sense examples. \n\nAll forms of circumscription involve restricting attention to models\nin which certain sets are minimized; for this reason, circumscription\ncan be grouped with the preferred models approaches to nonmonotonicity:\nsee\n Section 3.4,\n below. McCarthy’s\nformalism is fairly conservative; though it raises interesting logical\nissues in higher-order logic and complexity, it uses familiar logical\nframeworks. And much of the focus is on the development of\nformalization techniques. The other varieties of nonmonotonic logic,\nincluding default logic and the modal nonmonotonic logics, raise issues\nof the sort that are familiar to philosophical logicians, having to do\nwith the design of new logics, the systematic investigation of\nquestions concerning validity, and managing the proliferation of\nalternative logics. \n\nAs the discussion above of truth maintenance indicated, it is very\nnatural to think of nonmonotonic inferences as being hedged.\nThat is, a nonmonotonic inference may require not merely the presence\nof a set of proved conclusions, but the absence of certain\nother conclusions. The general form of such a rule is: \n\nAn important special case of DR is a normal\ndefault, a simple rule to the effect that C holds by\ndefault, conditionally on assumptions\nA1,…,An. This can be\nformalized by taking the condition that must be absent to simply be the\nnegation of the conclusion. \n\nAt first sight, it is somewhat perplexing how to formalize this\nnotion of nonmonotonic inference, since it seems to require a circular\ndefinition of provability that can’t be replaced with an inductive\ndefinition, as in the nonmonotonic case. The difficulty with the early\ntheory of\n Sandewall 1972\n is that it does\nnot address this difficulty successfully.\n McDermott & Doyle 1980\n and\n Reiter 1980\n use fixpoint definitions to\nsolve the problem. In both cases, the logical task is (1) to develop a\nformalism in which rules like DR can be expressed, and\n(2) to define the relation between a theory DT (which may\nincorporate such rules) and the theories E which could count\nas reasonable consequences of DT. In the terminology that\nlater became standard, we need to define the relation between a theory\nDT and its extensions. \n\nIn retrospect, we can identify two sorts of approaches to\nnonmonotonic logic: those based on preference and those based\non conflict. Theories of the first sort (like circumscription)\ninvolve a relatively straightforward modification of the ordinary\nmodel-theoretic definition of logical consequence that takes into\naccount a preference relation over models. Theories of the second sort\n(like default logic) involve a more radical rethinking of logical\nideas. The possibility of multiple extensions—different possible\ncoherent, inferentially complete conclusion sets that can be drawn from\na single set of premises—means that we have to think of logical\nconsequence not as a function taking a set of axioms into its logical\nclosure, but as a relation between a set of axioms and\nalternative logical closures. Since logical consequence is so\nfundamental, this represents a major theoretical departure. With\nmultiple extensions, we can still retrieve a consequence relation\nbetween a theory and a formula in various ways, the simplest being to\nsay that DT nonmonotonically implies C if C\nis a member of every extension of DT. Still, the\nconflict-based account of consequence provides a much richer underlying\nstructure than the preferential one. \n\nReiter approaches the formalization problem conservatively.\nNonmonotonicity is not expressed in the language of default logic,\nwhich is the same as the language of first-order logic. But a theory\nmay involve a set of default rules—rules of the form\nDR.\n Reiter 1980\nprovides a fixpoint definition of the extensions of such a theory, and\ndevelops the theoretical groundwork for the approach, proving a number\nof the basic theorems. \n\nOf these theorems, we mention one in particular, which will be used\nin\n Section 4.5,\n in connection with the Yale\nShooting Anomaly. The idea is to take a conjectured extension (which\nwill be a set T*) and to use this set for consistency checks\nin a proof-like process that successively applies default rules in\n<W,D> to stages that begin with\nW. \n\nWe define a default proof process\nT0,T1,… for W,\nD, relative to T*, as follows. \n\nthat is nonvacuously applicable to Ti\nrelative to T*, and let  \n\nIn other words, as long as we can nonvacuously close the stage we are\nworking on under an applicable default, we do so; otherwise, we do\nnothing. A theorem of Reiter’s says that, under these circumstances:  \n\nThus, we can show that T is an extension by (1) using\nT for consistency checks in a default reasoning process from\n<W, D>, (2) taking the limit T′\nof this process, and (3) verifying that in fact T′ =\nT.  \n\nThe modal approach represents a “higher level of nonmonotonic\ninvolvement” than default logic. The unprovability construct is\nrepresented explicitly in the language, by means of a modal operator\nL informally interpreted as ‘provable’ (or, as\n in\n McDermott & Doyle 1980,\n by\nthe dual of this operator).[19]\n Although McDermott and Doyle’s\nterminology is different from Reiter’s, the logical ideas are very\nsimilar—the essence of their approach, like Reiter’s, is a\nfixpoint definition of the extensions of a nonmonotonic logic.\nIncorporating nonmonoticity in the object language creates some\nadditional complexities, which in the early modal approach show up\nmainly in proliferation of the logics and difficulties in evaluating\nthe merits of the alternatives. As better foundations for the modal\napproach emerged, it became possible to prove the expected theorems\nconcerning equivalence of modal formalisms with default\n logic.[20] \n\nReiter’s paper\n Reiter 1980\n appears to\nhave developed primarily out of tactical considerations. The earlier\npaper\n Reiter 1978\n is largely concerned\nwith providing an account of database queries. Unlike the other seminal\npapers in nonmonotonic logic, Reiter’s shows specific influence from\nthe earlier and independent work on nonmonotonicity in logic\nprogramming—the work seems to have been largely inspired by the\nneed to provide logical foundations for the nonmonotonic reasoning\nfound in deductive databases. Doyle and McDermott’s paper shows both\nstrategic and tactical motivation—citing the earlier literature\nin logicist AI, it motivates nonmonotonic logic as part of a program of\nmodeling common sense rationality. But the theory is also clearly\ninfluenced by the need to provide a formal account of truth\nmaintenance. \n\nNonmonotonic logic is a complex, robust research field. Providing a\nsurvey of the subject is made difficult by the fact that there are many\ndifferent foundational paradigms for formalizing nonmonotonic\nreasoning, and the relations between these paradigms is not simple. An\nadequate account of even a significant part of the field requires a\nsomething like a book-length treatment. A number of books are\navailable, including\n Łukaszewicz 1990,\n Brewka 1991,\n Besnard 1992,\n Marek & Truszczynski 1994,\n Antoniou 1997,\n Brewka et al. 1997,\n Schlechta 1997,\n Antonelli 2005,\n Makinson 2005b,\n and\n Horty 2012.\n Two\ncollections are especially useful:\n Ginsberg 1987\n and\n Gabbay et al. 1994.\n The former is a useful source for readers interested in the\nearly history of the subject, and has an excellent introduction. The\nhandbook chapters in\n Gabbay et al. 1994\n provide overviews of important topics and approaches.\nMy current recommendation for readers interested in a quick, readable\nintroduction to the topic would be\n Brewka et al. 1997\n and self-selected chapters of\n Gabbay et al. 1994.\nSome other sources include \nBochman 2004,\nMakinson 2005,\nAntoniou and Wang 2007,\nBochman 2007,\n and\nSchlechta 2007.\nWe rely\non these references for technical background, and will concentrate on\nintellectual motivation, basic ideas, and potential long-term\nsignifgicance for logic. \n\nAt the outset in\n Section 3.1,\n it was mentioned how\npreferred models could be used to characterize a nonmonotonic\nconsequence relation. This general model theory of nonmonotonicity\nemerged in\n Shoham 1988,\n five years after\nthe work discussed in\n Section 3.2,\n and\nrepresents a much more general and abstract approach.  \n\nPreferential semantics relies on a function S taking a set\nK of models into a subset S(K) of\nK. The crucial definition of preferential entailment\nstipulates that A is a (nonmonotonic) consequence of Γ\nif every model M of S(Models(Γ))\nimplies A. Shoham’s theory is based on a partial order\n≼ over models: S(K) can then be characterized\nas the set of models in K that are ≼-minimal\nin K. To ensure that no set can preferentially entail a\ncontradiction unless it classically entails a contradiction, infinite\ndescending ≼ chains need to be disallowed. \n\nThis treatment of nonmonotonicity is similar to the earlier modal\nsemantic theories of conditionals—the similarities are\nparticularly evident using the more general theories of conditional\nsemantics, such as the one presented in\n Chellas 1975.\n Of course, the consequence\nrelation of the classical conditional logics is monotonic, and\nconditional semantics uses possible worlds, not models. But the\nleft-nonmonotonicity of conditionals (the fact that A\n C does not imply\n[A\n ∧\n B]\n C) creates issues that\nparallel those in nonmonotonic logics. Early work in nonmonotonic logic\ndoes not seem to be aware of the analogy with conditional logic. But\nthe interrelations between the two have become an important theme more\nrecently; see, for instance,\n Gärdenfors & Makinson 1994,\n Boutilier 1992,\n Pearl 1994,\n Gabbay 1995,\n Benferat et al. 1997,\n Delgrande 1998,\n Arlo-Costa & Shapiro 1992,\n Alcourrón 1995,\n Asher 1995,\n and\n Thomason 2007. \n\nPreference semantics raises an opportunity for formulating and\nproving representation theorems relating conditions over preference\nrelations to properties of the abstract consequence relation. This line\nof investigation began with\n Lehmann & Magidor 1992. \n\nNeither Doyle or McDermott pursued the modal approach much beyond\nthe initial stages of\n McDermott 1982,\nand\n McDermott & Doyle 1980.\n With a helpful suggestion from Robert Stalnaker (see\n Stalnaker 1993),\n however, Robert C. Moore\nproduced a modal theory that improves in many ways on the earlier\nideas. Moore gives the modal operator of his system an epistemic\ninterpretation, based on the conception of a default rule as one that\nlicenses a conclusion for a reasoning agent unless something that the\nagent knows blocks the conclusion. In Moore’s autoepistemic\nlogic, an extension E of a theory T is a\nsuperset of T that is stable, i.e., that is\ndeductively closed, and that satisfies the following two rules: \n\nIt is also usual to impose a groundedness condition on\nautoepistemic extensions of T, ensuring that every member of\nan extension has some reason tracing back to T. Various such\nconditions have been considered; the simplest one restricts extensions\nto those satisfying  \n\nAutoepistemic logic remains a popular approach to nonmonotonic logic,\nin part because of its usefulness in providing theoretical foundations\nfor logic programming. For more recent references, see\n Marek & Truszczynski 1991,\n Moore 1995b,\n Marek & Truszczynski 1989,\n Konolige 1994,\n Antoniou 1997,\n Moore 1993,\n and\n Deneker et al. 2003. \n\nEpistemic logic has inspired other approaches to nonmonotonic logic.\nLike other modal theories of nonmonotonicity, these use modality to\nreflect consistency in the object language, and so allow default rules\nalong the lines of DR to be expressed. But instead of\nconsistency, these use ignorance. See\n Halpern & Moses 1985\n and\n Levesque 1987\n for variations on this idea.\nThese theories are explained, and compared to other nonmonotonic\nlogics, in\n Meyer & van der Hoek 1995.\n In more recent work, Levesque’s ideas are systematically\npresented and applied to the theory of knowledge bases in\n Levesque & Lakemeyer 2000. \n\nThis brief historical introduction to nonmonotonic logic leaves\nuntouched a number of general topics that might well be of\ninterest to a nonspecialist. These include graph-based and\nproof-theoretic approaches to nonmonotonic logic, results that\ninterrelate the various formalisms, complexity results, tractable\nspecial cases of nonmonotonic reasoning, relations between nonmonotonic\nand abductive reasoning, relations to probability logics, the logical\nintuitions and apparent patterns of validity underlying nonmonotonic\nlogics, and the techniques used to formalize domains using nonmonotonic\nlogics. For these and other topics, the reader is referred to the\nliterature. As a start, the chapters in\n Gabbay et al. 1994\n are highly\nrecommended. \n\nTime and temporal reasoning have been associated with logic since the\norigins of scientific logic with Aristotle. The idea of a logic of\ntense in the modern sense has been familiar since at least the work of\nJan Łukasiewicz (see, for instance,\n Łukasiewicz 1970),\n but the shape\nof what is commonly known as tense logic was standardized by Arthur\nPrior’s work in the 1950s and 1960s: see Prior\n 1956,\n 1967,\n 1968.[22]\n As the topic was developed in\nphilosophical logic, tense logic proved to be a species of modal logic;\nPrior’s work was heavily influenced by both Hintikka and Kripke, and by\nthe idea that the truth of tense-logical formulas is relative to\nworld-states or temporal stages of the world; these are the\ntense-theoretic analogues of the timeless possible worlds of ordinary\nmodal logic. Thus, the central logical problems and techniques of tense\nlogic were borrowed from modal logic. For instance, it became a\nresearch theme to work out the relations between axiomatic systems and\nthe corresponding model theoretic constraints on temporal orderings.\nSee, for instance,\n Burgess 1984\n and\n van Benthem 1983. \n\nPriorian tense logic shares with modal logic a technical\nconcentration on issues that arise from using the first-order theory of\nrelations to explain the logical phenomena, an expectation that the\nimportant temporal operators will be quantifiers over world-states, and\na rather remote and foundational approach to actual specimens of\ntemporal reasoning. Of course, these temporal logics do yield\nvalidities, such as \n\n(if A, then it was the case that A was going to be\nthe case), which certainly are intuitively valid. But at most, these\ncan only play a broadly foundational role in accounting for realistic\nreasoning about time. It is hard to think of realistic examples in\nwhich they play a leading part.  \n\nThis characteristic, of course, is one that modal logic shares with\nmost traditional and modern logical theories; the connection with\neveryday reasoning is rather weak. Although modern logical techniques\ndo account with some success for the reasoning involved in verifying\nmathematical proofs and logic puzzles, they do not explain other cases\nof technical or common sense reasoning with much detail or\nplausibility. Even in cases like legal reasoning, where logicians and\nlogically-minded legal theorists have put much effort into formalizing\nthe reasoning, the utility of the results is controversial. \n\nPlanning problems provide one of the most fruitful showcases for\ncombining logical analysis with AI applications. On the one hand there\nare many practically important applications of automated planning, and\non the other logical formalizations of planning are genuinely helpful\nin understanding the problems, and in designing algorithms. \n\nThe classical representation of an AI planning problem, as described\nin\n Amarel 1968,\n evidently originates in\nearly work of Herbert Simon’s, published in a 1966 CMU technical\nreport,\n Simon 1966.\n In such a problem, an\nagent in an initial world-state is equipped with a set of\nactions, which are thought of as partial functions\ntransforming world-states into world-states. Actions are feasible only\nin world-states that meet certain constraints.  (These constraints are\nnow called the “preconditions” of the action.)  A planning\nproblem then becomes a search for a series of feasible actions that\nsuccessively transform the initial world-state into a desired\nworld-state. \n\nThe Situation Calculus, developed by John McCarthy, is the\norigin of most of the later work in formalizing reasoning about action\nand change. It was first described in 1969, in\n McCarthy 1983;\n the earliest generally\naccessible publication on the topic is\n McCarthy & Hayes 1969. \n\nApparently, Priorian tense logic had no influence on\n Amarel 1968.\n But there is no important\ndifference between Amarel’s world-states and those of Priorian tense\nlogic. The “situations” of the Situation Calculus are these same\nworld-states, under a new\n name.[23]\n They resemble possible worlds in\nmodal logic in providing abstract locations that support a consistent\nand complete collection of truths. As in tense logic, these locations\nare ordered, and change is represented by the variation in truths from\none location to another. The crucial difference between the Situation\nCalculus and tense logic is that change in the situation is\ndynamic—changes do not merely occur, but occur for a\nreason. \n\nThis difference, of course, is inspired by the intended use of the\nSituation Calculus: it is meant to formalize Simon’s representation of\nthe planning problem, in which a single agent reasons about the\nscenarios in which a series of actions is\n performed.[24]\n In this\nmodel, what drives change is the performance of actions, so the\nfundamental model theoretic relation is the relation \n\nbetween an action a, an initial situation s in which a is performed,\nand a resulting situation s′ immediately subsequent to the\nperformance of the action. Usually (though this is not absolutely\nnecessary) the deterministic assumption is made that s′ is\nunique. In general, actions can be successfully performed only under\ncertain limited circumstances. This could be modeled by allowing for\ncases in which there is no s′ such that\nRESULT(a,s,s′). But usually, it is assumed that\nRESULT is in fact a total function, but that in cases in\nwhich s does not meet the “preconditions” of a, there are no\nrestrictions on the s′ satisfying\nRESULT(a,s,s′), so that the causal effects of a\nwill be entirely unconstrained in such cases.  \n\nA planning problem starts with a limited repertoire of actions\n(where sets of preconditions and effects are associated with each\naction), an initial situation, and a goal (which can be treated as a\nformula). A planning problem is a matter of finding a sequence of\nactions that will achieve the goal, given the initial situation. That\nis, given a goal G and initial situation s, the problem will\nconsist of finding a sequence s1,...,sn of\nactions which will transform s into a final situation that satisfies\nG. This means (assuming that RESULT is a\nfunction) that G will be satisfied by the situation\nsn, where s0 = s and si+1 is\nthe s′ such that\nRESULT(ai+1,si,s′).\nThe planning problem is in effect a search for a sequence of actions\nmeeting these conditions. The success conditions for the search can be\ncharacterized in a formalism like the Situation Calculus, which allows\ninformation about the results of actions to be expressed. \n\nNothing has been said up till now about the actual language of the\nSituation Calculus. The crucial thing is how change is to be expressed.\nWith tense logic in mind, it would be natural to invoke a modality like\n[a]A,\n with the truth condition \n\nThis formalization, in the style of dynamic logic, is in fact a leading\ncandidate; see\n Section 4.7,\n below.  \n\nBut\n McCarthy & Hayes 1969\n deploys a language that is much closer to first-order logic.\n(This formalization style is characteristic of McCarthy’s work; see\n McCarthy 1979.)\n Actions are treated as\nindividuals. And certain propositions whose truth values can change\nover time (propositional fluents) are also treated as\nindividuals. Where s is a situation and f is a fluent,\nHolds(f,s) says that f is true in s. \n\nSince the pioneering work of the nineteenth and early twentieth\ncentury logicians, the process of formalizing mathematical domains has\nlargely become a matter of routine. Although (as with set theory) there\nmay be controversies about what axioms and logical infrastructure best\nserve to formalize an area of mathematics, the methods of formalization\nand the criteria for evaluating them are relatively unproblematic. This\nmethodological clarity has not been successfully extended to other\ndomains; even the formalization of the empirical sciences presents\ndifficult problems that have not yet been\n resolved.[25] \n\n The formalization of temporal reasoning, and in particular of\nreasoning about actions and plans, is the best-developed successful\nextension of modern formalization techniques to domains other than\nmathematical theories.  This departure has required the creation of\nnew methodologies.  One methodological innovation will emerge in\n Section 4.5:\nthe development of a library of scenarios for\ntesting the adequacy of various formalisms, and the creation of\nspecialized domains like the blocks-world domain (mentioned above,\nin Section 4.2) that serve a laboratories for testing ideas. \nFor more on the blocks world, see\n Genesereth & Nilsson 1987;\n Davis 1991.\n McCarthy’s ideas about\nelaboration tolerance\n McCarthy 1999\n provide one interesting attempt to provide a criterion for the\nadequacy of formalizations. Another idea that has emerged in the course of\nformalizing common sense domains is the importance of an explicit\nontology; see, for instance,\n Fikes 1996 and\n Lenat & Guha 1989.\n Another is the potential usefulness of explicit\nrepresentations of context; see\n Guha, 1991.\n  Another is the use of simulation techniques: see,\nfor instance, \n Johnstone & Williamson 2007\n \n\nTo tell whether a plan achieves its goal, you need to see whether\nthe goal holds in the plan’s final state. Doing this requires\npredictive reasoning, a type of reasoning that was neglected\nin the tense-logical literature. As in mechanics, prediction involves\nthe inference of later states from earlier ones. But (in the case of\nsimple planning problems at least) the dynamics are determined by\nactions rather than by differential equations. The investigation of\nthis qualitative form of temporal reasoning, and of related sorts of\nreasoning (e.g., plan recognition, which seeks to infer goals from\nobserved actions, and narrative explanation, which seeks to fill in\nimplicit information in a temporal narrative) is one of the most\nimpressive chapters in the brief history of common sense logicism. \n\nThe essence of prediction is the problem of inferring what holds in\nthe situation that ensues from performing an action, given information\nabout the initial situation. It is often assumed that the agent has complete\nknowledge about the initial situation—this assumption is usual in\nclassical formalizations of \nplanning.[26] \n\nA large part of the qualitative dynamics that is needed for planning\nconsists in inferring what does not change. Take a simple plan\nto type the word ‘cat’ using word processing software: \nthe natural plan is to first enter ‘c’, then enter ‘a’,\nthen enter ‘t’. Part of one’s confidence in this plan is that\nthe actions are independent: for instance, entering ‘a’\ndoes not also erase the ‘c’. The required inference can be\nthought of as a form of inertia. The Frame Problem is\nthe problem of how to formalize the required inertial reasoning. \n\nThe Frame Problem was named and introduced in\n McCarthy & Hayes 1969.\nUnlike most of the philosophically interesting technical problems to\nemerge in AI, it has attracted the interest of philosophers; most of\nthe relevant papers, and background information, can be found in\n Ford & Pylyshyn 1996;\n Pylyshyn 1987.\n Both of these volumes\ndocument interactions between AI and philosophy. \n\nThe quality of these interactions is discouraging. Like any realistic\ncommon sense reasoning problem, the Frame Problem is open-ended, and\ncan depend on a wide variety of circumstances. If you put $20 in a\nwallet, put the wallet in your pocket, and go to the store, you can\nsafely assume that the $20 is still in the wallet.  But if you leave the\n$20 on the counter at the store while shopping, you can’t safely\nassume it will be there later.  This may account for the\ntemptation that makes some \nphilosophers[27]\n want to construe the Frame Problem\nvery broadly, so that very soon it becomes indiscernible from the\nproblem of formalizing general common sense in arbitrary domains. Such\na broad construal may serve to introduce speculative discussions\nconcerning the nature of AI, but it loses all contact with the\ngenuine, new logical problems in temporal reasoning that have been\ndiscovered by the AI community. It provides a forum for repeating some\nfamiliar philosophical themes, but it brings nothing new to\nphilosophy. This way of interpreting the Frame Problem is disappointing, because\nphilosophy can use all the help it can get; the AI community has\nsucceeded in extending and enriching the application of logic to\ncommon sense reasoning in dramatic ways that are highly relevant to\nphilosophy. The clearest account of these developments to be found in\nthe volumes edited by Pylyshyn is\n Morgenstern 1996.\n An extended treatment can be found in\n Shanahan 1997;\n also see\n Sandewall 1994 and\n Shanahan 2009. \n The purely logical Frame Problem can be solved using monotonic logic,\nby simply writing explicit axioms stating what does not\nchange when an action is performed. This technique can be successfully\napplied to quite complex formalization\nproblems.[28]\nBut nonmonotonic solutions to the framework have been\nextensively investigated and deployed; these lead to new and\ninteresting lines of logical development. \n\nSome philosophers\n (Fodor 1987,\n Lormand 1996)\n have felt that contrived\npropositions will pose special difficulties in connection with the\nFrame Problem. As Shanahan points out\n Shanahan 1997\n [p. 24]) Fodor’s\n“fridgeon” example is readily formalized in the Situation Calculus and\nposes no special problems. However, as Lormand suggests, Goodman’s\nexamples\n Goodman, 1946\n do create\nproblems if they are admitted as fluents; there will be anomalous\nextensions in which objects change from green to blue in order to\npreserve their grueness. \n\nThis is one of the few points about the Frame Problem made by a philosopher \nthat raises a genuine difficulty for\nthe formal solutions. But the difficulty is peripheral, since the\nexample is not realistic. Recall that fluents are represented as\nfirst-order individuals. Although fluents are situation-dependent\nfunctions, an axiom of comprehension is certainly not assumed for\nfluents. In fact, it is generally supposed that the domain of fluents\nwill be a very limited set of the totality of situation-dependent\nfunctions; typically, it will be a relatively small finite set of\nvariables representing features of the domain considered to be\nimportant.  In particular cases these will be chosen in much the same\nway that a set of variables is chosen in statistical modeling. \n\nThere doesn’t seem to be a systematic account in the AI literature of\nhow to choose an appropriate set of fluents, but it would certainly be\npart of such an account that all fluents should correspond to\nprojectable predicates, in Goodman’s sense. \n\nThe idea behind nonmonotonic solutions to the Frame Problem is to\ntreat inertia as a default; changes are assumed to occur only if there\nis some special reason for them to occur. In an action-centered account\nof change, this means that absence of change is assumed when an action\nis performed unless a reason for the change can be found in axioms for\nthe action. \n\nFor explicitness, let’s use Reiter’s default logic to illustrate\nthe formalization. Recall that in Reiter’s theory, defaults are\nrepresented as rules, not formulas, so that they are not subject to\nquantification. To formalize inertia, then, we need to use default rule\nschemata. For each fluent f, action a, and situation s, the set of\nthese schemata will include an instance of the following schema: \n\nThis way of doing things makes any case in which a fluent changes\ntruth value a prima facie anomaly. But it follows from\nReiter’s account of extensions that such defaults are overridden when\nthey conflict with the monotonic theory of situation dynamics. So if,\nfor instance, there is a monotonic causal axiom for the action\nblacken ensuring that blackening a block will make it black in\nthe resulting situation, then the appropriate instance of\nIR will be inefficacious, and there will be no\nextension in which a white block remains white when it is\nblackened. \n\nThe Frame Problem somehow managed to capture the attention of a wide\ncommunity—but if one is interested in understanding the complex\nproblems that arise in generalizing formalisms like the Situation\nCalculus, while at the same time ensuring that they deliver plausible\nsolutions to a wide variety of scenarios, it is more useful to consider\na larger range of problems. For the AI community, the larger problems\ninclude the Frame Problem itself, the Qualification Problem, the\nRamification Problem, generalizability along a number of important\ndimensions including incomplete information, concurrency (multiple\nagents), and continuous change, and finally a large assortment of\nspecific challenges such as the scenarios mentioned later in this\nsection. \n\nThe Qualification Problem arises generally in connection\nwith the formalization of common sense generalizations. Typically,\nthese involve exceptions, and these exceptions—especially if\none is willing to entertain far-fetched circumstances—can\niterate endlessly. The same phenomenon, under a label like ‘the\nproblem of ceteris paribus generalizations’, is familiar\nfrom analytic philosophy. It also comes up in the semantics of\ngeneric constructions found in natural languages.[29] In a sense, this\nproblem is addressed at a general level by nonmonotonic logics,\nwhich—though they do not provide a way to enumerate\nexceptions—do allow common sense generalizations to be\nformulated as defaults, as well as enabling further qualifications to\nbe added nondestructively. Ideally, then, the initial generalization\ncan be stated as an axiom and qualifications can be added\nincrementally in the form of further axioms. \n\nThe Qualification Problem was raised in\n McCarthy 1986,\n where it was motivated\nchiefly by generalizations concerning the consequences of actions;\nMcCarthy considers in some detail the generalization that turning the\nignition key in an automobile will start the car. Much the same point,\nin fact, can be made about virtually any action, including stacking one\nblock on another—the standard action that is used to illustrate\nthe Situation Calculus. A circumscriptive approach to the Qualification\nProblem is presented in\n Lifschitz 1987;\nthis explicitly introduces the precondition relation between an action\nand its preconditions into the formalism, and circumscriptively\nminimizes preconditions, eliminating from preferred models any “unknown\npreconditions” that might render an action inefficacious. \n\nSeveral dimensions of the Qualification Problem remain as broad,\nchallenging research problems. For one thing, not every nonmonotonic\nlogic provides graceful mechanisms for qualification. Default logic,\nfor instance, does not deliver the intuitively desired conclusions.\nSuppose one formalizes the common sense generalization that if you press\nthe ‘a’ key on a computer it will type ‘a’ as\na normal default: \n\nIf we then formalize the exception to this generalization that if you\npress the ‘a’ key while the Alt key is depressed the\ncursor moves to the beginning of the current sentence as a normal\ndefault along the same lines, we get two extensions: one in\nwhich pressing ‘a’ while the Alt key is depressed moves\nthe cursor and another in which it adds ‘a’ to the\ntext. \n\nThe problem is that default logic does not provide for more specific\ndefaults to override ones that are more general. This principle of\nspecificity has been discussed at length in the literature.\nIncorporating it in a nonmonotonic logic can complicate the theory\nconsiderably; see, for instance,\n Asher & Morreau 1991\n and\n Horty 1994.\n And,\nas\n Elkan 1995\n points out, the Qualification\nProblem raises computational issues.  \n\nRelatively little attention has been given to the Qualification\nProblem for characterizing actions, in comparison with other problems\nin temporal reasoning. In particular, the standard accounts of\nunsuccessful actions are somewhat unintuitive. In the\nformalization of\n Lifschitz 1987,\n for instance, actions with some unsatisfied preconditions are only\ndistinguished from actions whose preconditions all succeed in that\nthe conventional effects of the action will only be ensured when the\npreconditions are met. It is as if an action of spending $1,000,000\ncan be performed at any moment—although if you don’t have the\nmoney, no effects in particular will be guaranteed. \n [30] And there\nis no distinction between actions that cannot even be attempted (like\nboarding a plane in London when you are in Sydney), actions that can\nbe attempted, but in which the attempt can be expected to go wrong\n(like making a withdrawal when you have insufficient funds), actions\nthat can be attempted with reasonable hope of success, and actions\nthat can be attempted with guaranteed success. As J.L. Austin made\nclear in\n Austin 1961,\n the ways in which actions can be\nattempted, and in which attempted actions can fail, are a well\ndeveloped part of common sense reasoning. Obviously, in contemplating a\nplan containing actions that may fail, one may need to reason about the\nconsequences of failure. Formalizing the pathology of actions,\nproviding a systematic theory of ways in which actions and the plans\nthat contain them can go wrong, would be a useful addition to planning\nformalisms, and one that would illuminate important themes in\nphilosophy. \n\nThe challenge posed by the Ramification Problem\n(characterized first in\n Finger 1987)\n is\nto formalize the indirect consequences of actions, where “indirect”\neffects are not\n delayed,[31]\n but are temporally immediate but causally derivative. If one walks into\na room, the direct effect is that one is now in the room. There are also\nmany indirect effects: for instance, that one’s shirt also is now in the\nroom. \n\n You can see from this that the formulation of the problem\npresupposes a distinction between direct consequences of actions (ones\nthat attach directly to an action, and that are ensured by the\nsuccessful performance of the action) and other consequences. This\nassumption is generally accepted without question in the AI literature\non action formalisms. You can make a good case for its common sense\nplausibility—for instance, many of our words for actions\n(‘to warm’, to ‘lengthen’, ‘to\nensure’) are derived from the effects that are conventionally\nassociated with them. And in these cases, success is entailed: if\nsomeone has warmed something, this entails that it became warm.\n[32] A typical\nexample is discussed in\n Lin 1995:\na certain suitcase has two locks, and is open if and only if both locks\nare open. Then (assuming that actions are not performed concurrently)\nopening one lock will open the suitcase if and only if the other lock\nis open. Here, opening a lock is an action, with direct consequences;\nopening a suitcase is not an action, it is an indirect effect. \n\nObviously, the Ramification Problem is intimately connected with the\nFrame Problem. In approaches that adopt nonmonotonic solutions to the\nFrame Problem, inertial defaults will need to be overridden by\nconclusions about ramifications in order to obtain correct results. In\ncase the left lock of the suitcase is open, for instance, and an action\nof opening the right lock is performed, then the default conclusion\nthat the suitcase remains closed needs somehow to be suppressed. Some\napproaches to the Ramification Problem depend on the development of\ntheories of common sense causation, and therefore are closely related\nto the causal approaches to reasoning about time and action discussed\nbelow in\n Section 4.6.\n See, for\n instance,\n Giunchiglia et al. 1997,\n Thielscher 1989,\n Lin 1995. \n\nPhilosophical logicians have been content to illustrate their ideas\nwith relatively small-scale examples. The formalization of even\nlarge-scale mathematical theories is relatively unproblematic. Logicist\nAI is the first branch of logic to undertake the task of formalizing\nlarge examples involving nontrivial common sense reasoning. In doing\nso, the field has had to invent new methods. An important part of the\nmethodology that has emerged in formalizing action and change is the\nprominence that is given to challenges, posed in the form of\nscenarios. These scenarios represent formalization problems\nwhich usually involve relatively simple, realistic examples designed to\nchallenge the logical theories in specific ways. Typically, there will\nbe clear common sense intuitions about the inferences that should be\ndrawn in these cases. The challenge is to design a logical formalism\nthat will provide general, well-motivated solutions to these benchmark\nproblems. \n\nAmong the many scenarios that have been discussed in the literature\nare the Baby Scenario, the Bus Ride Scenario, the Chess Board Scenario,\nthe Ferryboat Connection Scenario, the Furniture Assembly Scenario, the\nHiding Turkey Scenario, the Kitchen Sink Scenario, the Russian Turkey\nScenario, the Stanford Murder Mystery, the Stockholm Delivery Scenario,\nthe Stolen Car Scenario, the Stuffy Room Scenario, the Ticketed Car\nScenario, the Walking Turkey Scenario, and the Yale Shooting Anomaly.\nAccounts of these can be found in\n Shanahan 1997\n and\n Sandewall 1994;\n see\nespecially\n Sandewall 1994[Chapters 2\n and 7]. \n\nMany of these scenarios are designed to test advanced problems that\nwill not be discussed here—for instance, challenges dealing with\nmultiple agents, or with continuous changes. Here, we concentrate\non one of the earliest, and probably the most subtle of these\nscenarios: the Yale Shooting Anomaly, first reported in\n Hanks & McDermott 1985\n and\npublished in\n Hanks & McDermott 1986;\n Hanks & McDermott 1987. \n\nThe Yale Shooting Anomaly involves three actions: load,\nshoot, and wait. A propositional fluent\nLoaded tracks whether a certain pistol is loaded; another\nfluent, Alive, tracks whether a certain person, Fred, is\nalive. load has no preconditions; its only effect is\nLoaded. The fluent shoot has Loaded as its\nonly precondition and Alive as a negative effect;\nwait has no preconditions and no effects. \n\nCausal information regarding the axioms is formalized as\nfollows. \n\nThere is no Wait Axiom—that is, wait has no\npreconditions and no effects. \n\nWe will formalize the inertial reasoning in this scenario using a\nnonmonotonic logic—to be specific, we use Reiter’s default logic.\nThe set D of defaults for this theory consists of all\ninstances of the inertial schema IR. In the initial\nsituation, Fred is alive and the pistol is unloaded. \n\nThe monotonic theory W of the scenario consists of: (1) the\naction axioms Load, Shoot 1 and\nShoot 2 and (2) the initial conditions\nIC1 and IC2. Let s1 =\nRESULT(load,s0), s2 =\nRESULT(wait,s1), and s3 =\nRESULT(shoot,s2).  \n\nThe Yale Shooting Anomaly arises because this theory\nallows an extension in which the actions are load;\nshoot; wait, and in the final situation\ns3, the pistol is unloaded and Fred is alive. The\ninitial situation in the Anomaly and the three actions, with their\nresulting situations, can be pictured as follows. \n\nThe natural, expected outcome of these axioms is that the pistol is\nloaded and Fred is alive after waiting, so that shooting yields a final\noutcome in which Fred is not alive and the pistol is unloaded. There is\nno problem in showing that this corresponds to an extension; the\nproblem is the presence of the other, anomalous extension, which looks\nlike this. \n\nHere is a narrative version of this extension. At first, Fred is\nalive and the pistol is unloaded. After loading, the pistol is loaded\nand Fred remains alive. After waiting, the pistol becomes unloaded and\nFred remains alive. Shooting is then vacuous since the pistol is\nunloaded, so finally, after shooting, Fred remains alive and the pistol\nremains unloaded. \n\nThe best way to see clearly that this is an extension is to work\nthrough the proof. Less formally, though, you can see that the expected\nextension violates just one default: the frame default for\nAlive is violated when Fred changes state in the last step.\nBut the anomalous extension also violates only one default: the frame\ndefault for Loaded is violated when the pistol spontaneously\nbecomes unloaded while waiting. So, if you just go by the number of\ndefaults that are violated, both extensions are equally good. \n\nThe Yale Shooting Anomaly represents a major obstacle in developing\na theory of predictive reasoning. A plausible, well-motivated logical\nsolution to the Frame Problem runs afoul of a simple, crisp example in\nwhich it clearly delivers the wrong results. Naturally, the literature\nconcerning the Yale Shooting Problem is extensive. Surveys of some of\nthis work, with bibliographical references, can be found in\n Shanahan 1997;\n Morgenstern 1996. \n\nMany formalisms have been proposed to deal with the problems\nsurveyed in the previous section. Some are more or less neglected\ntoday. Several are still advocated and defended by leading experts;\nsome of these are associated with research groups who are not only\ninterested in developments of logical theory, but in applications in\nplanning and cognitive robotics. \n\nThe leading approaches provide solutions to the main problems\nmentioned in\n Section 4.5,\n and to many of the\nscenarios designed to test and illustrate theories of reasoning about\naction and change. It is commonly agreed that good solutions need to be\ngeneralizable to more complex cases than the early planning formalisms,\nand that in particular the solutions they offer should be deployable\neven when continuous time, concurrent actions, and various kinds of\nignorance are allowed. Also, it is generally agreed that the formalisms\nshould support several kinds of reasoning, and, in particular, not only\nprediction and plan verification but retrodiction, i.e.,\nconstruction of a sequence of states and actions from partial\ninformation, presented in narrative form. \n\nWe describe four approaches here: (1) Features and fluents\n(Sandewall), (2) Motivated Action Theory (Morgenstern and Stein), (3)\nState Minimization in the Event Calculus (Shanahan) and (4) Causal\nTheories (Lifschitz and others). The accounts of the first three in\nwhat follows will be fairly brief; fortunately, each approach is well\ndocumented in a single reference. The fourth approach is most likely\nto be interesting to philosophers and to contain elements that will be\nof lasting importance regardless of future developments in this\narea. \n\nThis approach, described in\n Sandewall 1994,\n uses preference semantics as a way to organize nonmonotonic\nsolutions to the problems of reasoning about action and change. Rather\nthan introducing a single logical framework, Sandewall considers a\nnumber of temporal logics, including ones that use discrete,\ncontinuous, and branching time. The properties of the logics are\nsystematically tested against a large suite of test scenarios.  \n\nThis theory grew out of direct consideration of the problems in\ntemporal reasoning described above in\n Section 4.5,\n and especially the Yale Shooting Scenario. In\n Morgenstern & Stein 1994,\nMorgenstern and Stein seek to find a general, intuitively motivated\nlogical framework that solves the difficulties. They settle on the idea\nthat unmotivated actions are to be minimized, where an action\n(“actions” construed generally enough to include any change) can be\nmotivated directly, e.g. by an axiom, or indirectly, through chains of\nmotivations. The key technical idea of the paper is a (rather\ncomplicated) definition of motivation in an interval-based temporal\nlogic. In\n Morgenstern 1996,\nMorgenstern presents a summary of the theory, along with reasons for\nrejecting its causal rivals. The most important of these reasons is\nthat these theories, based on the Situation Calculus, do not appear to\ngeneralize to cases allowing for concurrency and ignorance. She also\ncites the failure of early causal theories to deal with retrodiction.  \n\nIn\n Baker 1989,\n Andrew Baker presented a\nsolution to the version of the Yale Shooting problem in the Situation\nCalculus, using a circumscriptive inertial axiom. The very brief\naccount of circumscription above in\n Section 3\nindicated that circumscription uses preferred models in which the\nextensions of certain predicates are minimized. In the course of this\nminimization, a set of parameters (including, of course, the predicates\nto be minimized) is allowed to vary; the rest are held constant. Which\nparameters vary and which are held constant is determined by the\napplication.  \n\nIn the earliest circumscriptive solutions to the Frame Problem, the\ninertial rule CIR is stated using an abnormality\npredicate. \n\nThis axiom uses a biconditional, so that it can be used for\nretrodiction; this is typical of the more recent formulations of common\nsense inertia. In circumscribing, the abnormality predicate is\nminimized while the Holds predicate is allowed to vary and all\nother parameters are fixed. This formalization succumbs to the Yale\nShooting Anomaly in much the same way that default logic does.\n(Circumscription does not involve multiple extensions, so the problem\nemerges as the nonderivability of the conclusion that Fred is alive\nafter the occurrence of the shooting.) \n\nIn Baker’s reformulation of the problem, separate axioms ensure the\nexistence of a situation corresponding to each Boolean combination of\nfluents, and the RESULT function is allowed to vary,\nwhile the Holds predicate is held constant. In this setting,\nthe RESULT function needs to be specified for\n“counterfactual”actions—in particular, for shooting as well as\nfor waiting in the Yale Shooting Anomaly. It is this feature that\neliminates the incorrect model for that scenario; for details, see\n Baker 1989\n and\n Shanahan 1997,\n Chapter 6. \n\nThis idea, which Shanahan calls “State-Based Minimization,” is\ndeveloped and extended in\n Shanahan 1997,\nin the context of a temporal logic deriving from the Event Calculus of\nKowalski and Sergot; see\n Kowalski & Sergot 1986.\n Shanahan’s formalism has the advantage of being\nclosely connected to implementations using logic programming. \n\nRecall that in the anomalous model of the Yale Shooting Scenario the\ngun becomes unloaded after the performance of the wait action,\nan action which has no conventional effects—the unloading, then,\nis uncaused. In the context of a nonmonotonic logic—and without\nsuch a logic, the Yale Shooting Anomaly would not arise—it is\nvery natural to formalize this by treating uncaused eventualities as\nabnormalities to be minimized.  \n\nThis strategy was pursued by Hector Geffner in\n Geffner 1992\n 1990,\n where he formalizes this simple causal\nsolution to the Yale Shooting Anomaly. But the solution is presented in\nthe context of an ambitious general project in nonmonotonic logic that\nnot only develops properties of the preferred model approach and shows\nhow to apply it to a number of reasoning problems, but that relates\nnonmonotonic logic to probabilities, using ideas deriving from\n Adams 1975.\n In\n Geffner 1992,\n the causal theory is sketched;\nit is not developed to show its adequacy in dealing with the battery of\nproblems presented above, and in particular the Ramification Problem is\nleft untouched. \n\nThe work beginning with\n Lifschitz 1987\n has contributed to a sustained line of research in the causal\napproach—not only by Lifschitz and students of his such as Enrico\nGiunchiglia and Hudson Turner, but by researchers at other sites. For\nwork in this area, and further references, see\n Thielscher 1989,\n Gustaffson & Doherty 1996,\n Baral 1995,\n Nakashima et al. 1997,\n Lifschitz 1997,\n Giunchiglia & Lifschitz 1998,\n Lin 1995,\n Haugh 1987,\n Lifschitz 1998,\n Turner 1999,\n McCain & Turner 1995,\n Elkan 1991,\n McCain & Turner 1997,\n Thielscher 1996,\n and\n Gelfond & Lifschitz 1998. \n\nHere, we briefly describe some of theories developed by the\nTexas Action Group, leading up to the causal solution presented in\n Turner 1999.\n Turner returns to the ideas\n of\n Geffner 1992,\n but places them in a simpler\nlogical setting and applies them to the formalization of more complex\nscenarios that illustrate the interactions of causal inertia with other\nconsiderations, especially the Ramification Problem. \n\nRamification is induced by the presence of static laws\nwhich relate the direct consequences of actions to other changes. \nLet’s use a car-starting scenario to illustrate the difficulties. There\nis one action, turn-on, which turns on the ignition; let’s\nsuppose that this action has no preconditions. There is a fluent\nIg tracking whether the ignition is on, a fluent Dead\ntracking whether the battery is dead, and a fluent Run\ntracking whether the engine is running. A static law says that if the\nignition is on and the battery isn’t dead, the engine is running.\n(Let’s suppose that every other source of failure has already been\neliminated in this scenario; the only possible reason for not starting\nis the battery.) We want to consider a transition in which\nturn-on is performed in a situation in which the ignition is\nnot on, the battery is not dead, and the car isn’t running. \n\nOf course, we want a planning agent to be able to infer in such a\ncase that a performance of turn-on will result in a situation\nin which the ignition is on, the battery isn’t dead, and the engine is\nrunning. But contraposition of laws makes it difficult to devise a\nprincipled solution. Informally, this difficulty is this: we can\nconclude by contraposing our only static law that if the ignition is on\nand the engine isn’t running, then the battery is dead. This law not\nonly is true in our scenario, but would be used to explain a failed\nattempt to start the car. But if we allow it to be used for prediction,\nthen it is hard to see how to rule out an outcome of turn-on\nin which the ignition is on, the battery is dead, and the engine isn’t\nrunning. The battery is dead in this outcome because of causal inertia.\nThe engine isn’t running because of the contraposed causal law. \n\nReaders who want to explore in some detail the\nproblems of embedding a nonmonotonic solution to the Frame Problem in\nrelatively expressive action languages  can look to\n Gelfond & Lifschitz 1998.\nThis paper presents an\nincreasingly powerful and sophisticated series of action languages.\nTheir language incorporates an ad hoc or at least purely\nsyntactic solution to the Ramification Problem. \n\nA theory in language\n B\n consists of\ntwo sets of axioms: \n\nGelfond and Lifschitz impose a weak closure condition on static\nlaws: where s is a set of literals, s is\nrestricted-closed with respect to a\n B theory\n T,\n RBClT(s)\nif and only\nif every literal that would be added by starting with s and\nforward-chaining through the static laws of\n B\n is already in s. In other words: \n\nThe closure operation induced by this condition is used in defining\nRESULT(a,s) for the language\n B.\n Of course, this solution treats logically equivalent\nstatic laws differently; for instance, a theory whose only static law\nis [P ∧ ¬Q] →R will in general\ndetermine a very different RESULT function than one\nwhose only static law is [P ∧ ¬R] →Q. \n\nThis has some somewhat counterintuitive effects. In the car-starting\nscenario, Gelfond and Lifschitz’ language\n B\n indeed yields the desired conclusion that the car will\nstart when there is one dynamic law, \n\nwhen the only static law is  \n\nwhen the initial state is s = {¬Ig,\n¬Dead, ¬Run}, and when the action\nturn-on is performed.  \n\nHowever, if we add a true static law saying that if the ignition is\non and the engine isn’t running the battery is dead, we get an anomaly.\nWith the addition of this law, there is a model in which preserving the\nfact that the car is not running makes the battery become dead when the\nignition is turned on. \n\nIf the two static laws,\n[Ig ∧ ¬Dead] → Run\nand [Ig ∧ ¬Run]\n→ Dead, are reformulated using causal language, the\nformer sounds correct, while the second is distinctly problematic:\ncompare \n\nwith  \n\nThis makes it very plausible to suppose that the source of the\nproblem is a representation of underlying causal information in action\nlanguage\n B\n that is somehow\ninadequate. \n\nGelfond and Lifschitz go on to describe another action\n language,\n C, which invokes an explicit notion of\ncausality—motivated, in all likelihood, in part by the need to provide a\nmore principled solution to the problem. Instead of describing that\nlanguage, we now discuss the similar theory of\n Turner 1999. \n\nTurner’s idea is to treat Caused as a modal operator\n[ c ], making this the\nbasis of a modal nonmonotonic logic. In the preferred models of this\nlogic, the caused propositions coincide with the propositions that are\ntrue, and this must be the only possibility consistent with the\nextensional part of the model. To make this more explicit, recall that\nin the possible worlds interpretation of S5, it is\npossible to identify possible worlds with state descriptions,\nwhich we can represent as sets I of literals (atomic formulas\nand their negations). Making this identification, then, we can think of\na model as a pair <I, S>, where S is a\nset of interpretations (complete, consistent sets of literals)\nincluding I. The modal operator\n[ c ] is given the\nstandard semantics: where S is a set of interpretations and\nwhere I ∈ S, \nS ⊨I [ c \n] A if and only if \nS ⊨I′ A for all\nI′ ∈S. <I, S>\nsatisfies a set of formulas T if and only if\nS ⊨I A for all\nA ∈ T. \n\nTurner’s preferred models of T are the pairs\n<I, S> such that: (1) <I,\nS> satisfies T, (2) S = {I}, and\n(3) <I, S> is the unique interpretation\n<I′, S′> meeting conditions (1) and\n(2) with I′ = I. Condition (2) guarantees the\n“universality of causation”; it validates A ↔\n[ c ]A.\nCondition (3) “grounds” causality in noncausal information (in the\nmodels in which we are interested, this will be information about the\noccurrence of events), in the strongest sense: it is uniquely\ndetermined by this information. \n\nAlthough it is not evident from the formulation, Turner’s account of\npreferred models is related to the constructions of more general\nnonmonotonic logics, such as default logic. Consult\n Turner 1999\n for details. \n\nThe axioms that specify the effects of actions treat these effects\nas caused; for instance, the axiom schema for loading would read as\nfollows: \n\nRamifications of the immediate effects of actions are also treated as\ncaused. And the nonmonotonic inertial axiom schemata take the form  \n[[[ c ]Holds(f,s)]\n ∧\nHolds(f,RESULT(a,s))] →\n[ c ]Holds(f,R\nESULT(a,s)) \nand  \n[[[ c ]¬Holds(f,s)]\n ∧\n¬Holds(f,RESULT(a,s))] → \n     \n  [ c ]¬Holds(f,RESULT(a,s)).\n \n\nThus, a true proposition can be caused either because it is the direct\nor indirect effect of an action, or because it involves the persistence\nof a caused proposition. Initial conditions are also considered to be\ncaused, by stipulation.  \n\nTo illustrate the workings of this approach, let’s consider the\nsimplest case of inertia: we have a language with just one constant\ndenoting a fluent, f, and one action-denoting constant, wait.\nAs in the Yale Shooting Problem, there are no axioms for wait;\nthis action can always be performed and has no associated effects. Let\ns1 be RESULT(wait,s\n0). The theory T contains an initial condition for\nf, Holds(f,s0) and a statement that the initial\ncondition is caused,\n[ c ] Holds(f,s\n0), as well as the inertial schemata. \n\nTwo models of T satisfy conditions (1) and (2): where\nI1 = {Holds(f,s0),\nHolds(f,s1)} and I2 =\n{Holds(f,s0),\n¬Holds(f,s1)}. \n\nM1 is the intended model, in which nothing\nchanges. It satisfies Condition (3), since if\n<I1, S> satisfies T it\nsatisfies\n[ c ]Holds(f,s\n1) by the inertial axiom \n\nTherefore, S = {I1}. \n\nM2 is an anomalous model, in which the fluent\nceases spontaneously. This model does not satisfy Condition (3), since\nM3 =\n<I2,{I1,\nI2}> also satisfies T; in particular,\nit satisfies the inertial axiom for f because it fails to satisfy\nHolds(f,s1). So, while M1 is a\npreferred model, M2 is not. \n\nTurner’s approach avoids the problem of contraposition by giving\ncausal relations the form \n\nWhen contraposed, this becomes  \n\nwhich does not have the form of a causal law. This solution is less\nthan fully satisfactory at solving the intuitive difficulties, because\nTurner’s semantics seems intuitively to validate formulas such as \n\nand in this form, contraposition would be restored. The task of\nclarifying the foundations of causal theories of action and change may\nnot yet be complete. \n\nBut the apparent usefulness of a “principle of universal causality”\nin accounting for a range of problems in qualitative common sense\nreasoning will be tantalizing to philosophers. And the causal theory,\nas initiated by Geffner and developed by Turner, has many interesting\ndetailed features. For instance, while philosophical work on causality\nhas concentrated on the causal relation, this work in logical AI shows\nthat a great deal can be done by using only a nonrelational causal\npredicate. \n\nThe relation between causality and conditionals can be explored and\nexploited in various ways.  Lewis 1977\nundertakes to account for causality in terms of conditionals.  In the\nreverse direction, Lent and Thomason 2015\nuses Turner’s causal approach to construct models for conditional logics,\nin the restricted case where the antecedent is the conjunction of an\naction expression and simple situational conditions.  The motivation for\nthis idea is than an explicit solution to the frame problem automatically\nprovides a semantics for such conditionals. \n\nMorgenstern’s two chief criticisms of the causal approach to\nreasoning about actions are that it does not give an adequate account\nof explanation[34]\n and that the logical context in\nwhich it works (the Situation Calculus) is limited. As work on the\napproach continues, progress is being made in these areas. But the\nconstraints that a successful logic of action and change must meet are\nso complex that it seems to be a reasonable research methodology to\nconcentrate initially on a restricted logical setting. \n\nFor another approach to nonmonotonic causal reasoning, based on\ninput-output logics (Makinson\n& van der Torre 2000), see Bochman\n2004. \n\nAlthough for many AI logicists, the goal of action formalisms is to\nilluminate an important aspect of common sense reasoning, most of their\nresearch is uninformed by an important source of insights into the\ncommon sense view of time—namely, natural language. Linguists\nconcerned with the semantics of temporal constructions in natural\nlanguage, like the AI community, have begun with ideas from\nphilosophical logic but have discovered that these ideas need to be\nmodified in order to deal with the phenomena. A chief discovery of the\nAI logicists has been the importance of actions and their relation to\nchange. Similarly, an important discovery of the “natural language\nlogicists” has been the importance of different kinds of events\n(including structured composite events) in interpreting natural\nlanguage. From work such as this the idea of “natural language\nmetaphysics” (see, for instance,\n Bach 1989)\nhas emerged.  \n\nThe goal of articulating a logical framework tailored to a\nrepresentational system that is motivated by systematic evidence about\nmeanings in natural languages is not acknowledged by all linguistic\nsemanticists. Nevertheless, it is a significant theme in the linguistic\nliterature. This goal is remarkably similar to those of the common\nsense logicists, but the research methodology is entirely\ndifferent. \n\nCan the insights of these separate traditions be reconciled and\nunified? Is it possible to constrain theories of temporal\nrepresentations and reasoning with the insights and research\nmethodologies of both traditions? In\n Steedman 1995\n (and\n 2000,\n listed in the Other\nInternet Resources Section), these important questions are addressed,\nand a theory is developed that extends action formalisms like the\nSituation Calculus, and that incorporates many of the insights from\nlinguistic semantics. The project reported in\n Steedman 2000\n is still incomplete, but the\nresults reported there make a convincing case that the event-based\nideas from linguistics can be fruitfully combined with the\naction-centered formalisms in the AI literature. The possibility of\nthis unification is one of the most exciting logical developments in\nthis area, bringing together as it does two independent descendants of\nthe earlier work in the logic of time. \n\nIn\n Section 4.6,\n we traced the reasons\nfor the development of theories incorporating causality in work on\nreasoning about action and change. This is not the only area of AI in\nwhich causality has emerged. Causality figures in qualitative reasoning\nabout devices; for Herbert Simon’s important work in this area, which\ngoes back to the 1950s, see\n Simon 1952;\n 1977;\n Iwasaki & Simon 1986.\n Both these\ntraditions are important. But the most robust and highly developed\nprogram in AI relating to causality is that of Judea Pearl and his\nstudents and associates, which derives from statistical techniques\nknown as structural equation models.  \nHalpern and Pearl 2001 introduced\nthe idea that causal relations among events could be inferred from\nthese models: Bayesian belief networks could be interpreted as \ncausal networks. \n\nPearl’s program has developed into a far-reaching campaign to\nrehabilitate causality in statistical thinking. We shall not discuss this\ntopic here. For one thing, this survey omits probabilistic reasoning in\nAI. For another, Pearl’s views on causality are systematically and\ncomprehensively presented in a recent book-length study,\n Pearl 2000. \n\nBut it is important to point out that the work on causality discussed\n in\n Section 4.6\n and Pearl’s ideas do share\nsome common themes. On both approaches: action is central for\ncausality. Also there is a focus on causality as a tool in reasoning\nthat is necessitated in part by limited resources. Another important\ntheme is the deployment and systematic study of formalisms in which\ncausality is related to other constructs (in particular, to probability\nand to qualitative change) and a variety of realistic reasoning\nproblems are addressed. \n\nThese commonalities provide reason to hope that we will see a\nscience of causality emerging from the AI research, unifying the\ncontributions of the probabilistic, the qualitative physics, and the\nnonmonotonic traditions, and illuminating the various phases of causal\nreasoning. \n\n A recent landmark in this direction is \nHalpern 2016, which develops and\napplies the general theory of event causality that arises from the\ncausal network approach.  Although Halpern is a computer scientist,  a\nlarge part of this book is philosophical, exploring notions such\nas blame and explanation.  But the book also explores practical\napplications of the approach that would not occur to philosophers,\nin areas such as software fault diagnosis. \n\nWhether you take causality to be a fundamental construct in natural\nscience, or a fundamental common sense phenomenon, depends on whether\nyou have in mind an idealized nature described by differential\nequations or you have in mind the view of nature we have to take in\norder to act, either in everyday situations, or for that matter in\ndesigning experiments in the laboratory. The fact that, as Bertrand\nRussell noted (see\n Russell 1957),\ncausality is not to be found as a theoretical primitive in contemporary\nphysical theories is at odds with its seeming importance in so many\nfamiliar areas of reasoning. The rigorous theories emerging in AI that\nare beginning to illuminate the workings of causality are important not\nonly in themselves, but in their potentiality to illuminate wider\nphilosophical issues. \n\nThe precomputational literature in philosophical logic relating to\nspatial reasoning is very sparse in relation, for instance, to the\ntemporal literature. The need to support computational reasoning about\nspace, however, in application areas such as motion planning and\nmanipulation in physical space, the indexing and retrieval of images,\ngeographic information systems, diagrammatic reasoning, and the design\nof high-level graphics programs has led to new interest in spatial\nrepresentations and spatial reasoning. Of course, the geometrical\ntradition provides an exceptionally strong mathematical resource for\nthis enterprise. But as in many other AI-related areas, it is not clear\nthat the available mathematical theories are appropriate for informing\nthese applications, and many computer scientists have felt it\nworthwhile to develop new foundations. Some of this work is closely\nrelated to the research in qualitative reasoning mentioned above in\n Section 2.2,\n and in some cases has been carried\nout by the same individuals.  \n\nThe literature in spatial reasoning is extensive; for references to\nsome areas not discussed here, see\n Stock 1997,\n Kapur & Mundy 1988,\n Hammer 1995,\n Wilson 1998,\n Osherson & Lasnik 1990,\n Renz & Nebel 1999,\n Yeap & Jeffries 1999,\n Forbus et al. 1991,\n Chen 1990,\n Burger & Bhanu 1992,\n Allwein & Barwise 1996,\n Glasgow et al. 1995,\n and\n Kosslyn 1990.\n Here, we discuss only one\ntrend, which is closely connected with parallel work in philosophical\nlogic. \n\nQualitative approaches to space were introduced into the logical\nliterature early in the twentieth century by Lesniewski; see\n Lesniewski 1916,\n which presents the idea\nof a mereology, or qualitative theory of the part-whole\nrelation between physical individuals. This idea of a logical theory of\nrelations among regions or the objects that occupy them, which does not\ndepend on construing regions as sets of points, remained an active area\nof philosophical logic, even though it attracted relatively few\nresearchers. More recent work in the philosophical literature,\nespecially\n Casati & Varzi 1999,\n Simons 1987,\n Casati & Varzi 1996,\n Clarke 1981,\n Clarke 1985,\n as directly influential on\ncurrent computational work. \n\nThe Regional Connection Calculus (RCC), developed by computer\nscientists at the University of Leeds, is based on a primitive\nC relating regions of space: the intended interpretation of\nC(x, y) is that the intersection of the\nclosures of the values of x and y is nonempty.\n (See\n Cohn et al. 1997,\n Cohn 1996)\n for details and references.) One\narea of research concerns the definability of shapes in RCC. The extent\nof what can be defined with this simple primitive is surprising, but\nthe technicalities quickly become complex; see, for instance,\n Gotts 1994,\n Gotts 1996).\n The work cited in\n Cohn et al. 1997\n describes constraint propagation techniques and\nencodings in intuitionistic propositional logic as ways of supporting\nimplemented reasoning based on RCC and some of its extensions. More\nrecent work based on RCC addresses representation and reasoning about\nmotion, which of course combines spatial and temporal issues; see\n Wolter & Zakharyaschev 2000).\n For more information about qualitative theories of movement,\nwith references to other approaches, see\n Galton 1997. \n\nEpistemic logic is another area in which logic in computer science\nhave been influenced by philosophical logic. The classical source for\nepistemic logic is\n Hintikka 1962,\n in which Jaakko Hintikka\nshowed that a modal approach to single-agent epistemic attitudes could\nbe informative and rewarding. This work discusses at length the\nquestion of exactly which constraints are appropriate for knowledge and\nbelief, when these attitudes are viewed as explicated by a model\ntheoretic relation over possible worlds; in both cases, Hintikka argues\nfor S4 type operators.  \n\nIn several papers (including\n McCarthy 1979),\n John McCarthy has recommended an approach to formalizing\nknowledge that uses first-order logic, but that quantifies explicitly\nover such things as individual concepts. In this section, we discuss\nthe approach taken by most computer scientists, who, unlike McCarthy,\nuse a modal language to formalize propositional attitudes. \n\nThe logical aspects of modal epistemic logic were not significantly\ndeveloped after Hintikka’s 1962 presentation; instead, the\nphilosophical literature (which is not extensive, compared with many\nother topics in the area) concentrates on the issue of\nhyperintensionality, or closure of epistemic attitudes under\nlogical consequence. This topic is especially challenging, turning out\nto be closely related to the semantic paradoxes, and the philosophical\nliterature is inconclusive. Intuitions seem to conflict, and it is\ndifficult to find ways to model the important phenomena using logical\ntechniques.[35] \n\n Fagin et al. 1984\n begins a\ntradition in computational logic that revives the modal approach to\nepistemic logic, developing generalized logical foundations and\napplications that had not occurred to the philosophers. The technical\nidea is to simplify the modality, using S5 (or deontic\nS5 for belief), but to introduce multiple agents, and\nto concentrate on reasoning having to do with agents’ attitudes about\none another’s attitudes. Such logics have direct applications in the\nanalysis of distributed systems, dynamic systems in which\nchange is effected by message actions, which change the knowledge of\nagents according to rules determined by a communications\nprotocol. \n\nAs such, this work belongs to a separate area of computer science,\nbut one that overlaps to some extent with AI. Later, this work has\ninteracted with a research tradition in economics that is concerned\nwith the role of knowledge in games and bargaining; see, for\n instance,\n Geanakopolos 1994;\n Osborne & Rubenstein 1994\n[Chapter 5]. \n\nFor some reason, the multi-agent case did not occur to philosophical\nlogicians.[36]\n This is another example of the way\nin which need for an application (in this case, the need for a theory\nof distributed systems) provided the inspiration for an important\nlogical development. The logical details are extensively and systematically \nrecorded in\n Fagin et al. 1995;\n this is\nessential reading for anyone seriously interested in this topic. \n\nMuch of the interdisciplinary work in applications of the logic of\nknowledge is reported in the proceedings of a series of conferences\ninitiated in 1986 with\n Halpern 1986.\n These\nconferences record one of the most successful collaborations of\nphilosophers with logicians in Computer Science, although the group of\ninvolved philosophers has been relatively small. The focus of the\nconferences has gradually shifted from Computer Science to\nEconomics. \n\nAI applications deal with with knowledge in the form of stored\nrepresentations, and the tradition in AI with which we are concerned\nhere thinks of reasoning as the manipulation of symbolic\nrepresentations. Also, it is mainly due to AI that the problem of\nlimited rationality has become a topic of serious interest, providing a\ncounterbalance to the idealizations of philosophy and\neconomics.[37]\n So you would think that a logical\nmodel of propositional attitudes that is committed to closure under\nlogical consequence would be highly unpopular in AI. But this is not\nso; the possible worlds approach to attitudes is not only the leading\ntheory in the areas discussed in\n Fagin et al. 1995,\n but has even been advocated in robotics\napplications; see\n Rosenschein & Kaelbling 1995;\n Rosenschein 1989.\n Nevertheless, the issue of hyperintensionality has been\ninvestigated in the AI literature; see\n Perlis 1985;\n Konolige 1986;\n Lakemeyer 1997;\n Levesque 1984).\n Though there are some new\npositive results here, the AI work in this area has, for the most\npart, been as inconclusive as that in philosophy. \n\nThe philosophical literature on a related topic, the logic of\nperception, has not been extensive; the main reference is\n Hintikka 1970.[38]\n But sensation is\naddressed in recent work in the AI literature which is concerned with\ndeveloping logical frameworks for general-purpose applications in\nrobotics. The main idea in this area is to add sensing actions to the\nrepertoire of a planning formalism of the sort discussed in\n Section 4.\n The earliest work in this area was carried\nout in the 1980s by Robert Moore; see\n Moore 1995b;\n Moore 1985.\n For some of the\ncontemporary work in cognitive robotics, see\n Baral et al. 2000,\n Bacchus et al. 1999,\n Golden & Weld, 1996,\n Pirri & Finzi 1999,\n and\n Thielscher 2000. \nJohn McCarthy’s explicit long-term goal—the\nformalization of common sense knowledge—has been adopted and\npursued by a relatively small, but active, subcommunity of AI\nresearchers.  A larger group (those involved in knowledge\nrepresentation, cognitive robotics, and qualitative physics) can be\nconsidered to work on specialized projects that support the larger\ngoal.  Anything like a formalization of common sense is so far from\nbeing accomplished that—if it is achievable at all—it is\nnot even possible to estimate when we could expect the task to be\ncompleted.  However, since 2001 (the date of a symposium on common\nsense reasoning held at The Courant Institute—see\n The Common Sense 2001 Homepage) something like\na cooperative, sustained effort in this direction has begun to\nemerge.  This effort is yielding a better sense of how to develop a\nworkable methodology for formalizing common sense, and of how to\ndivide the larger problem up into more manageable parts.  Many of the\npapers presented at this conference were presented in expanded form\nin 2004 in an issue of Artificial Intelligence.  The\nintroduction to this issue, \nDavis & Morgenstern 2004, provides a useful survey and\nappreciation of the general research area.\n\t \nThis cooperative formalization effort (1) seeks to account for\nmany areas of knowledge, and at the same time (2) attempts to see how\nthis formalized knowledge can be brought to bear on moderately\ncomplex common-sense reasoning problems.  The first book-length\ntreatment of this topic, Davis 1991,\ndivides the general problem into the following subtopics. \n\nSeveral of these topics overlap with concerns of the\nqualitative physics and qualitative reasoning community.  Although it\ncan be hard to tell where common sense ends and physics begins, the\nformalization of common sense reasoning can be seen as a more general\nformalization project that can draw on a tradition in qualitative\nphysics that has gone through many years of development and by now is\nfairly \n mature.[39] \n\n And a few of them overlap with\nthe work on the formalization of planning that was described above in\nSection 4.  Minds and society, however, are new and\ndifferent topics; the former has to do with common sense psychology\nand its application in introspective and interpersonal reasoning, and\nthe latter, of course, should have to do with social and political\nknowledge and reasoning, but this is the least-developed area of\nformalized common sense knowledge: the chapter on this topic in Davis 1991 is very brief, and discusses\nmutual attitudes and communication.  More recently, Andrew S. Gordon\nand Jerry Hobbs have undertaken a large-scale, ambitious\nformalization of common-sense psychology.  See, for instance, \n Hobbs & Gordon 2005. \n \nA more recent book-length treatment of the formalization of\ncommon sense, \n Mueller, 2006, follows a similar\npattern.  More than half of the book is devoted to reasoning about\nactions and change.  There are short chapters on space and mental\nstates, and a longer treatment of default reasoning.  Although\nlogical techniques and formalization methods take center stage in\nthis book, it also contains material on nonlogical methods and\non implementations related to the  formalizations. \nEven when attempted on a moderate scale, the formalization of\ncommon sense knowledge puts considerable pressure on the resources of\neven the most powerful logical systems that were devised for the\nformalization of mathematics.  As we tried to show in discussing the\nspecial case of action and planning in Section 4,\nthis pressure may lead us to seek logics that can facilitate the\nformalization projects: for instance, nonmonotomic logics and logics\nthat explicitly represent context.  \n\t \nWhen larger-scale formalizations are attempted, other\nchallenges arise that are similar to those that software engineering\ntries to address.  Even fairly small programs and systems of axioms\nare difficult to comprehend and can be highly unpredicable, yielding\nunexpected consequences and unanticipated interactions.  The creation\nand use of larger programs and formalizations raises questions of how\nto enable teams of developers to produce coherent results when\nmodules are integrated, how to maintain and test large systems, and\nhow to use knowledge sources such as dictionaries and knowledge bases\nto automatically generate axioms.   \nYou can think of the philosophical methodology of providing\nanalyses as a collection of attempts to formalize or partially\nformalize various common sense notions.  These attempts are far\nsmaller in scale, less systematic, and more heterogeneous than the\nparallel effort that is emerging in AI.  Philosophers have never\nchosen a specific domain comparable to the planning domain and\nmounted a sustained attempt to formalize it, along with a companion\neffort to develop appropriate logics.  And no matter how complex the\nnotions with which they which they are concerned, philosophers have\nnever allowed their analyses to grow to the complexity where \nmethodological issues arise similar to those that apply to the\ndevelopment and maintenance of large software systems.  \nThe techniques emerging in AI are of great potential\nsignificance for philosophy because it is easy to suspect that many\nphilosophically important phenomena have the sort of complexity that\ncan only be dealt with by accepting the problems that go along with\ndeveloping complex formalizations.  Limitations of the philosophical\nmethods that were used throughout the twentieth century and are still\nin use may make it impossible to produce theories that do justice to\nthe subject matter.   \nIt would therefore be a great mistake for philosophers to\ndisparage and ignore the large-scale formalizations that are\nbeginning to emerge in AI because these efforts begin to raise\nengineering issues.  It may well be that, although philosophy\nrequires us to address complex phenomena in a rigorous way, the\ntraditional philosophical methods are capable of doing justice\nto the complexity.  Methods that promise to do this are worth taking\nseriously. \nAmong other methods borrowed from computer science, \nthe common sense reasoning community has sought to develop \nsuites of “benchmark problems”.  The idea is to publicize\nproblems that are difficult but not impossibly difficult, to\nencourage the community to create solutions, and compare\nthe solutions.\n \nProbably the best-studied problem to date is Ernest Davis’\n“egg-cracking problem.”  This is formulated as follows on the\nCommon Sense Reasoning Problem webpage.\n \nAlong with the problem itself  three solutions are posted:\n Shanahan 2004, \n Lifschitz 1998b,\n and a version of\n  Morgenstern 2001.  Comparing the\nsolutions is instructive: similarities outweigh differences.  All the\nauthors think of this as a planning problem, and use a versions of\nthe Situation Calculus or the Event Calculus in the formalization.\nEach axiomatization is modular, with, for instance, separate modules\ndevoted to the relevant geometrical and material properties.  Each\nauthor provides a “proof of concept” for the formalization by showing\nthat the axioms support a proof of the correctness of a plan to crack\nthe egg in the simple case.  None of the authors considers all of\nDavis’ elaborations of the problem, but the axioms are framed with\nelaboration in mind and some elaborations are considered.  It isn’t\nclear whether any of the authors actually implemented the\nformalization (for instance, using a theorem prover).  \nThe egg-cracking case raises the problem of how to evaluate\nmoderately large formalizations of common sense problems.\nMorgenstern and Shanahan express this issue explicitly.  Morgenstern\nsuggests that the important criteria are (1) Epistemological adequacy\n(correspondence to intuitive reasoning, as experienced by people who\nengage in it), (2) Faithfulness to the real world, (3) Reusability,\nand (4) Elaboration tolerance.  It isn’t clear whether the first two\nof these criteria are too subjective to be useful.  To these,\nShanahan adds (5) Usability, which probably is presupposed by\nMorgenstern’s third criterion.\n \nThere is anecdotal evidence that the larger AI community is\nsomewhat skeptical about such research projects—or, if not\nskeptical, at least puzzled about how to evaluate them.  In\nconsidering these doubts, it is necessary to appreciate the\ncomplexity of these formalization problems, and the preliminary and\ntentative status of the research program.  Nevertheless, this\ncriticism has some legitimacy, the common sense reasoning community\nis sensitive to these criticisms, and is working to develop and\nrefine the methods and criteria for evaluating this work. \nAs long as formalization problems remain relatively simple,\nwe can treat formalization as an art rather than as a discipline with\na well-articulated methodology.  But the trends we’ve been\ndiscussing show that formalization of even moderate-sized realistic\ncommon sense reasoning problems is not merely an art.  Just as\nprogramming systems, expert systems and knowledge bases have created\ncorresponding software engineering disciplines, large-scale\nformalization projects require a carefully thought through and tested\nmethodology.\n \n\nOver the last twenty-five years or so, many profound relations have\nemerged between logic and grammar. Computational linguistics (or\nnatural language processing) is a branch of AI, and it is fairly\nnatural to classify some of these developments under logic and AI. But\nmany of them also belong to an independent tradition in logical\nfoundations of linguistics; and in many cases it is hard (and\npointless) to attempt a classification. This sketch will concentrate on\ndevelopments that have to do with reasoning about linguistics; other\napplications of logic to linguistics are described in\n van Benthem & ter Meulen 1996. \n\nGrammar formalisms—special-purpose systems for the description\nof linguistic systems and subsystems—can be thought of as logics\ndesigned to axiomatize the association of linguistic structures with\nstrings of symbols. You might be able to infer from such a system, for\ninstance, that ‘assignments’ is the plural form of the\nnominalization of the verb ‘assign’. So you can look at the\nprocess of parsing a string of words—of finding the\nlinguistic structures, if any, that are associated with it—as a\nsearch for a proof in a certain logical system. \n\nThis approach has been highly successful as an analytic tool. It\nmakes model-theoretic techniques applicable to linguistic reasoning,\nThis makes the underlying reasoning problems much more transparent, and\nmakes it possible to apply many well-developed areas of logic to\ngrammar formalisms. For more information on these topics, see\n Buszkowski 1996;\n Shieber 1992. \n\nThe usefulness and scope of logical methods in relation to\nlinguistics is greatly increased by the development of techniques for\nanalyzing the way information attaches to linguistic units. It is very\nnatural to represent the information attaching, say, to a lexical item\nin the form of a set of functions (or attributes) that produce values\nin some linguistic domain. A pronoun x may have a number, a\nperson, and a case: if x = ‘we’ then \n\nIn more general cases, the values of these functions may themselves be\nlinguistic units that take on values for certain attributes.  \n\nAllowing these functions to be partial provides a useful\ninformational representation of the stages of a linguistic parse; much\nof the work of parsing involves completing this partial information,\nsubject to constraints imposed by linguistic agreement conditions.\nFeature structures—sets of identities that serve to evaluate\nlinguistic features—have a natural algebraic interpretation, and\nthere is an elegant treatment of their logic. For more information and\nreferences, see\n Rounds 1996. \n\nThe reasoning associated with discourse is the probably the least\nwell understood area of computational linguistics. Although logical\ntechniques do not yet play a major role in discourse, they seem to\noffer one of the most promising ways of providing a uniform account of\nthe many forms of reasoning that are involved in generating and\ninterpreting language in interactive conversation. \n\nWe  briefly mention three contributions to this area. Building on\nthe fact that the rules governing conversation are exception-ridden,\nAlex Lascarides and Nicholas Asher have developed techniques for\nformalizing discourse phenomena based on nonmonotonic logic; see\n Asher & Lascarides 1994,\n Asher & Lascarides 1997.\n Jerry\nHobbs and various co-workers look at the inference processes used in\ndiscourse as abductive, and propose to formalize abduction as\na search for a proof in which certain “low-cost” assumptions may be\nmade which serve as data or additional axioms for the proof.\n Hobbs et al. 1993\n shows how an\nimpressive range of discourse phenomena can be formalized using this\nidea. In practice, this abductive account looks rather similar to that\nof Lascarides and Asher, because it involves deploying axioms about\ndiscourse (in the form of Horn clause rules supplemented with weights\ngiving the assumption costs of premises) that in effect are\nnonmonotonic. \n\nIn more recent work, Matthew Stone shows in\n Stone 1998\n how modal logic can inform the\ncomplex reasoning involved in natural language generation. Generating a\ncoherent, appropriately phrased text that usefully performs a\ntask-oriented communication task is difficult to formalize because it\nrequires the integration of complex and sophisticated domain\ninformation with discourse planning, user modeling, and linguistic\nconstraints. Stone shows that modal logic can be used to modularize the\nformalization of the information required in this task; he also shows\nhow modal theorem proving can be used to implement the reasoning. \n\nTraditionally, the task of representing large amounts of domain\ninformation for general-purpose reasoning has been one of the most\nimportant areas of knowledge representation. Systems that exploit the\nintuitive taxonomic organization of domains are useful for this\npurpose; taxonomic hierarchies not only help to organize the process of\nknowledge acquisition, but provide a useful connection to rule-based\nreasoning.[41] \n\nFor domains in which complex definitions are a natural way to\norganize information, knowledge engineering services based on\ndefinitions of concepts have been extremely successful. Like\nvariable-free versions of first-order logic (see, for instance,\n Quine 1960,\n these systems are centered on\nconcepts or first-order predicates, and provide a number of mechanisms\nfor their definition. The fundamental algorithm associated with these\ntaxonomic logics is a classifier which inputs a system of\ndefinitions and outputs the entailment relations between defined and\nprimitive concepts. For background on these systems, see\n Woods & Schmolze 1992\n and\n Brachman et al. 1991. \n\nThe simplest taxonomic logics can be regarded as subsystems of\nfirst-order logic with complex predicates; but they have been extended\nin many ways, and the issues raised by many of these extensions overlap\nin many cases with topics in philosophical logic. \n\nMuch more complex logical issues arise when the organization of a\ndomain into hierarchies is allowed to have exceptions. One way to\napproach this topic is to explore how to make a taxonomic logic\nnonmonotonic; but nonmonotonic inheritance is a topic in its\nown right. Although there are strong affinities to nonmonotonic logic,\nnonmonotonic logic relies more heavily on graph-based representations\nthan on traditional logical ideas, and seems to provide a much\nfiner-grained approach to nonmonotonic reasoning that raises entirely\nnew issues, and which quickly becomes problematic. For this reason,\nsystems of nonmonotonic inheritance tend to be expressively weak, and\ntheir relations to the more powerful nonmonotonic logic has never been\nfully clarified. For background on this topic, see\n Thomason 1992,\n Horty 1994. \n\nIn the tradition in philosophical logic dealing with contextual\neffects on the interpretation of expressions, as well as in the more\nrecent tradition in dynamic logic, context is primarily formalized as\nan assignment of values to variables, and the language is designed to\nmake explicit reasoning about context either very limited or outright\nimpossible. \n\nConcern in AI about the representation of large and apparently\nheterogeneous domains and about the integration of disparate knowledge\nsources, as well as interests in formalizing common sense of the sort\ndiscussed in\n Section 2.2,\n above, have led to\ninterest in the AI community in formalizing languages that take context\ninto account more explicitly. \n\nIn\n McCarthy 1993b,\n McCarthy\nrecommends the study of languages containing a construct \n\nwhere ist is read “is-true.” This is analogous to the\nHolds construct of the situation calculus—but now\nc stands for a context, and φ is a possibly complex\npropositional representation, which many (including McCarthy) take to\nrefer to a sentence.  \n\nThere are analogies here both to modal logic and to languages with an\nexplicit truth predicate. But the applications that are envisioned for\na logic of context create opportunities and problems that are in many\nways new.  Work on the logic of context subsequent to McCarthy’s\noriginal suggestion, includes \nMcCarthy &\nBuvac 1998, Guha 1991, and some of\nthe papers in the conference volumes\nAkman et al. 2001,\n Bouquet et al. 1999.\nFor extensions of Richard Montague’s Intensional Logic motivated by\nMcCarthy’s suggestions, see Thomason 2003\nand 2005. \n\nFor some reason, work on the explicit formalization of context hasn’t\nbeen actively pursued by the computational community beyond this point.\nPhilosophical interest in context, and especially in the interaction\nof context with propositional attitudes and modals, continues to be\nstrong; but the very general logical frameworks for context that \nMcCarthy envisioned have yet not been taken up by philosophers. \n\nThere is reason to hope that the combination of logical\nmethods with planning applications in AI can enable the development of\na far more comprehensive and adequate theory of practical reasoning\nthan has heretofore been possible. As with many problems having to do\nwith common sense reasoning, the scale and complexity of the\nformalizations that are required are beyond the traditional techniques\nof philosophical logic. However, with computational methods of\nimplementing and testing the formalizations and with areas such as\ncognitive robotics to serve as laboratories for developing and testing\nideas, we can hope to radically advance a problem that has seen little\nprogress since it was first proposed by Aristotle: the problem of\ndevising a formalization of practical reasoning that is genuinely\napplicable to common sense reasoning problems.  \n\nThe classical work in deontic logic that was begun by von Wright\n(see\n von Wright 1983)\n is one source of\nideas; see\n (Horty 2001,\n van der Torre 1997).\n In fact, as the\nmore recent work in deontic logic shows, nonmonotonic logic provides a\nnatural and useful supplement to the classical deontic logic.  One\nrecent work (Horty 2012) seeks \nto base deontic logic on a prioritized version of Reiter’s default logic.\n \n\nAn even more robust account of practical reasoning begins to emerge\nwhen these ideas are supplemented with work on the foundations of\nplanning and reasoning about action that were discussed in\n Section 4,\n above. But this development can be pursued\neven further, by extending the formalism to include preferences and\nintentions.[42] \n\nUltimately, what is needed is a model of an intelligent reasoning\nand acting agent. Developing such a model need not be entirely a matter\nof logic, but according to one school of thought, logic has a central\nrole to play in it; see, for instance,\n Baral & Gelfond 2000,\n Wobcke et al. 1998,\n Rao & Georgeff 1991,\n Burkhard et al. 1998).","contact.mail":"rthomaso@umich.edu","contact.domain":"umich.edu"}]
