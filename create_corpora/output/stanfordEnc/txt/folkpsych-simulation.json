[{"date.published":"1997-12-08","date.changed":"2017-03-28","url":"https://plato.stanford.edu/entries/folkpsych-simulation/","author1":"Luca Barlassina","author1.info":"https://www.umsl.edu/~philo/People/Faculty/index.html","entry":"folkpsych-simulation","body.text":"\n\n\nThe capacity for “mindreading” is understood in philosophy\nof mind and cognitive science as the capacity to represent, reason\nabout, and respond to others’ mental states. Essentially the\nsame capacity is also known as “folk psychology”,\n“Theory of Mind”, and “mentalizing”. An\nexample of everyday mindreading: you notice that Tom’s\nfright embarrassed Mary and surprised Bill,\nwho had believed that Tom wanted to try\neverything. Mindreading is of crucial importance for our social life:\nour ability to predict, explain, and/or coordinate with others’\nactions on countless occasions crucially relies on representing their\nmental states. For instance, by attributing to Steve the desire for a\nbanana and the belief that there are no more bananas at home but there\nare some left at the local grocery store, you can: (i)\nexplain why Steve has just left home; (ii) predict\nwhere Steve is heading; and (iii) coordinate your behavior\nwith his (meet him at the store, or prepare a surpise party while he\nis gone). Without mindreading, (i)–(iii) do not come\neasily—if they come at all. That much is fairly uncontroversial.\nWhat is controversial is how to explain mindreading. That is, how do\npeople arrive at representing others’ mental states? This is the\nmain question to which the Simulation (or, mental simulation) Theory\n(ST) of mindreading offers an answer.\n\n\nCommon sense has it that, in many circumstances, we arrive at\nrepresenting others’ mental states by putting ourselves in their\nshoes, or taking their perspective. For example, I can try to figure\nout my chess opponent’s next decision by imagining what I would\ndecide if I were in her place. (Although we may also speak of this as\na kind of empathy, that term must be understood here without\nany implication of sympathy or benevolence.)\n\n\nST takes this commonsensical idea seriously and develops it into a\nfully-fledged theory. At the core of the theory, we find the thesis\nthat mental simulation plays a central role in mindreading:\nwe typically arrive at representing others’ mental states by\nsimulating their mental states in our own mind. So, to figure out my\nchess opponent’s next decision, I mentally switch roles with her\nin the game. In doing this, I simulate her relevant beliefs\nand goals, and then feed these simulated mental states into\nmy decision-making mechanism and let the mechanism produce a simulated\ndecision. This decision is projected on or attributed to the\nopponent. In other words, the basic idea of ST is that if the\nresources our own brain uses to guide our own behavior can be modified\nto work as representations of other people’s mental states, then\nwe have no need to store general information about what makes people\ntick: we just do the ticking for them. Accordingly, ST challenges the\nTheory-Theory of mindreading (TT), the view that a tacit psychological\ntheory underlies the ability to represent and reason about\nothers’ mental states. While TT maintains that mindreading is an\ninformation-rich and theory-driven process, ST sees it as\ninformationally poor and process driven (Goldman 1989).\n\n\nThis entry is organized as follows. In section 1 (The Origins and\nVarieties of ST), we briefly reconstruct ST’s history and\nelaborate further on ST’s main theoretical aims. We then go on\nto explain the very idea of mental simulation (section 2: What is\nMeant by “Mental Simulation”?) In section 3 (Two Types of\nSimulation Processes), we consider the cognitive architecture\nunderlying mental simulation and introduce the distinction between\nhigh-level and low-level simulation processes. In section 4 (The Role\nof Mental Simulation in Mindreading), we discuss what role mental\nsimulation is supposed to play in mindreading, according to ST. This\ndiscussion carries over to section 5 (Simulation Theory and\nTheory-Theory), where we contrast the accounts of mindreading given by\nST and TT. Finally, section 6 (Simulation Theory: Pros and Cons)\nexamines some of the main arguments in favour of and against ST as\ntheory of mindreading.\n\nThe idea that we often arrive at representing other people’s\nmental states by mentally simulating those states in ourselves has a\ndistinguished history in philosophy and the human sciences. Robert\nGordon (1995) traces it back to David Hume (1739) and Adam\nSmith’s (1759) notion of sympathy; Jane Heal (2003) and Gordon\n(2000) find simulationist themes in the Verstehen approach to\nthe philosophy of history (e.g., Dilthey 1894); Alvin Goldman (2006)\nconsiders Theodor Lipps’s (1903) account of empathy\n(Einfühlung) as a precursor of the notion of mental\nsimulation. \nIn its modern guise, ST was established in 1986, with the publication\nof Robert Gordon’s “Folk Psychology as Simulation”\nand Jane Heal’s “Replication and Functionalism”.\nThese two articles criticized the Theory-Theory and introduced ST as a\nbetter account of mindreading. In his article, Gordon discussed\npsychological findings concerning the development of the capacity to\nrepresent others’ false beliefs. This attracted the interest of\ndevelopmental psychologists, especially Paul Harris (1989, 1992), who\npresented empirical support for ST, and Alison Gopnik (Gopnik &\nWellman 1992) and Joseph Perner (Perner & Howes 1992), who argued\nagainst it—Perner has since come to defend a hybrid version of\nST (Perner & Kühberger 2005). \nAlvin Goldman was an early and influential defender of ST (1989) and\nhas done much to give the theory its prominence. His work with the\nneuroscientist Vittorio Gallese (Gallese & Goldman 1998) was the\nfirst to posit an important connection between ST and the newly\ndiscovered mirror neurons. Goldman’s 2006 book Simulating\nMinds is the clearest and most comprehensive account to date of\nthe relevant philosophical and empirical issues. Among other\nphilosophical proponents of ST, Gregory Currie and Susan Hurley have\nbeen influential. \nSince the late 1980s, ST has been one of the central players in the\nphilosophical, psychological, and neuroscientific discussions of\nmindreading. It has however been argued that the fortunes of ST have\nhad a notable negative consequence: the expression “mental\nsimulation” has come to be used broadly and in a variety of\nways, making “Simulation Theory” a blanket term lumping\ntogether many distinct approaches to mindreading. Stephen Stich and\nShaun Nichols (1997) already urged dropping it in favor of a\nfiner-grained terminology. There is some merit to this. ST is in fact\nbetter conceived of as a family of theories rather than a\nsingle theory. All the members of the family agree on the thesis that\nmental simulation, rather than a body of knowledge about other minds,\nplays a central role in mindreading. However, different members of the\nfamily can differ from one another in significant respects. \nOne fundamental area of disagreement among Simulation Theorists is the\nvery nature of ST—what kind of theory ST is supposed to\nbe—and what philosophers can contribute to it. Some Simulation\nTheorists take the question “How do people arrive at\nrepresenting others’ mental states?” as a straightforward\nempirical question about the cognitive processes and mechanisms\nunderlying mindreading (Goldman 2006; Hurley 2008). According to them,\nST is thus a theory in cognitive science, to which\nphilosophers can contribute exactly as theoretical physicists\ncontribute to physics:  \ntheorists specialize in creating and tweaking theoretical structures\nthat comport with experimental data, whereas experimentalists have the\nprimary job of generating the data. (Goldman 2006: 22)  \nOther philosophical defenders of ST, however, do not conceive of\nthemselves as theoretical cognitive scientists at all. For example,\nHeal (1998) writes that:  \nit is commonly taken that the inquiry into … the extent of\nsimulation in psychological understanding is empirical, and that\nscientific investigation is the way to tell whether ST … is\ncorrect. But this perception is confused. It is an a priori\ntruth … that simulation must be given a substantial role in our\npersonal-level account of psychological understanding. (Heal 1998:\n477–478)  \nAdjudicating this meta-philosophical dispute goes well beyond the aim\nof this entry. To be as inclusive as we can, we shall adopt a\n“balanced diet” approach: we shall discuss the extent to\nwhich ST is supported by empirical findings from psychology and\nneuroscience, and, at the same time, we shall dwell on\n“purely philosophical” problems concerning ST. We leave to\nthe reader the task of evaluating which aspects should be put at the\ncentre of the inquiry. \nImportantly, even those who agree on the general nature of ST might\ndisagree on other crucial issues. We will focus on what are typically\ntaken to be the three most important bones of contention among\nSimulation Theorists: what is meant by “mental\nsimulation”?\n (section 2).\n What types of simulation processes are there?\n (section 3).\n What is the role of mental simulation in mindreading?\n (section 4).\n After having considered what keeps Simulation Theorists apart, we\nshall move to discuss what holds them together, i.e., the opposition\nto the Theory-Theory of mindreading\n (section 5\n and\n section 6).\n This should give the reader a sense of the “unity amidst\ndiversity” that characterizes ST. \nIn common parlance, we talk of putting ourselves in others’\nshoes, or empathizing with other people. This talk is typically\nunderstood as adopting someone else’s point of view, or\nperspective, in our imagination. For example, it is quite natural to\ninterpret the request “Try to show some empathy for John!”\nas asking you to use your imaginative capacity to consider the world\nfrom John’s perspective. But what is it for someone to\nimaginatively adopt someone else’s perspective? To a first\napproximation, according to Simulation Theorists, it consists of\nmentally simulating, or re-creating, someone\nelse’s mental states. Currie and Ravenscroft (2002) make this\npoint quite nicely:  \nImagination enables us to project ourselves into another situation and\nto see, or think about, the world from another perspective. These\nsituations and perspectives … might be those of another actual\nperson, [or] the perspective we would have on things if we believed\nsomething we actually don’t believe, [or] that of a fictional\ncharacter. … Imagination recreates the mental states of\nothers. (Currie & Ravenscroft 2002: 1, emphasis added).  \nThus, according to ST, empathizing with John’s sadness consists\nof mentally simulating his sadness, and adopting Mary’s\npolitical point of view consists of mentally simulating her political\nbeliefs. This is the intuitive and general sense of mental\nsimulation that Simulation Theorists have in mind. \nNeedless to say, this intuitive characterization of “mental\nsimulation” is loose. What exactly does it mean to say that a\nmental state is a mental simulation of another mental state? Clearly,\nwe need a precise answer to this question, if the notion of mental\nsimulation is to be the fundamental building block of a theory.\nSimulation Theorists, however, differ over how to answer this\nquestion. The central divide concerns whether “mental\nsimulation” should be defined in terms of resemblance\n(Heal 1986, 2003; Goldman 2006, 2008a) or in terms of reuse\n(Hurley 2004, 2008; Gallese & Sinigaglia 2011). We consider these\ntwo proposals in turn. \nThe simplest interpretation of “mental simulation” in\nterms of resemblance goes like this: \n(RES-1) Token state M* is a mental simulation of token state\nM if and only if: \nTwo clarifications are in order. First, we will elaborate on the\n“significant respects” in which a mental state has to\nresemble another mental state in due course (see, in particular,\n section 3).\n For the moment, it will suffice to mention some relevant dimensions\nof resemblance: similar functional role; similar content; similar\nphenomenology; similar neural basis (an important discussion of this\ntopic is Fisher 2006). Second, RES-1 defines “mental\nsimulation” as a dyadic relation between\nmental states (the relation being a mental simulation of).\nHowever, the expression “mental simulation” is also often\nused to pick out a monadic property of mental states\n—the property being a simulated mental state (as will\nbecome clear soon, “simulated mental state” does not refer\nhere to the state which is simulated, but to the state that does the\nsimulating). For example, it is common to find in the literature\nsentences like “M* is a mental simulation”. To\navoid ambiguities, we shall adopt the following terminological\nconventions: \nIt follows from this that, strictly speaking,\n RES-1\n is a definition of “mental simulation of”. Throughout\nthis entry, we shall characterize “simulated mental state”\nin terms of “mental simulation of”: we shall say that if\nM* is a mental simulation of M, then\nM* is a simulated mental\n state.[1] \nWith these clarifications in place, we will consider the strengths and\nweaknesses of\n RES-1.\n Suppose that Lisa is seeing a yellow banana. At the present moment,\nthere is no yellow banana in my own surroundings; thus, I cannot have\nthat (type of) visual experience. Still, I can visualize what\nLisa is seeing. Intuitively, my visual imagery of a yellow banana is a\nmental simulation of Lisa’s visual experience.\n RES-1\n captures this, given that both my visual imagery and Lisa’s\nvisual experience are mental states and the former resembles the\nlatter. \n\n RES-1,\n however, faces an obvious problem (Goldman 2006). The resemblance\nrelation is symmetric: for any x and y, if x\nresembles y, then y resembles x. Accordingly, it\nfollows from\n RES-1\n that Lisa’s visual experience is a mental simulation of my\nvisual imagery. But this is clearly wrong. There is no sense in which\none person’s perceptual experience can be a mental simulation of\nanother person’s mental imagery (see Ramsey 2010 for other\ndifficulties with\n RES-1). \nIn order to solve this problem, Goldman (2006) proposes the following\nresemblance-based definition of “mental simulation\nof”: \n(RES-2) Token state M* is a mental simulation of token state\nM if and only if: \nUnder the plausible assumption that one of the functions of visual\nimagery is to resemble visual experiences, RES-2 correctly predicts\nthat my visual imagery of a yellow banana counts as a mental\nsimulation of Lisa’s visual experience. At the same time, since\nvisual experiences do not have the function of resembling visual\nimages, RES-2 does not run into the trouble of categorizing the former\nas a mental simulation of the latter. \nClearly,\n RES-2\n is a better definition of “mental simulation of” than\n RES-1.\n Hurley (2008), however, argued that it won’t do either, since\nit fails to distinguish ST from its main competitor, i.e., the\nTheory-Theory (TT), according to which mindreading depends on a body\nof information about mental states and processes\n (section 5).\n The crux of Hurley’s argument is this. Suppose that a token\nvisual image V* resembles a token visual experience V\nand, in doing so, fulfils one of its functions. In this case,\n RES-2\n is satisfied. But now suppose further that visualization works like a\ncomputer simulation: it generates its outputs on the basis of a body\nof information about vision. On this assumption,\n RES-2\n still categorizes V* as a mental simulation of V,\neven though V* has been generated by exactly the kind of\nprocess described by TT: a theory-driven and information-rich\nprocess. \nAccording to Hurley (who follows here a suggestion by Currie &\nRavenscroft 2002), the solution to this difficulty lies in the\nrealization that “the fundamental … concept of simulation\nis reuse, not resemblance” (Hurley 2008: 758, emphasis\nadded). Hurley’s reuse-based definition of “mental\nsimulation of” can be articulated as follows: \n(REU) Token state M* is a mental simulation of token state\nM if and only if: \nTo have a full understanding of REU, we need to answer three\nquestions: (a) What is a cognitive process? (b) What is a\ncognitive mechanism? (c) What is the difference between\nusing and reusing a certain cognitive mechanism?\nLet’s do it! \nIt is a commonplace that explanation in cognitive science is\nstructured into different levels. Given our aims, we can illustrate\nthis idea through the classical tri-level hypothesis\nformulated by David Marr (1982). Suppose that one wants to explain a\ncertain cognitive capacity, say, vision (or mindreading, or\nmoral judgment). The first level of explanation, the most abstract\none, consists in describing what the cognitive capacity\ndoes—what task it performs, what problem it solves, what\nfunction it computes. For example, the task performed by vision is\nroughly “to derive properties of the world from images of\nit” (Marr 1982: 23). The second level of analysis specifies\nhow the task is accomplished: what algorithm our mind uses to\ncompute the function. Importantly, this level of analysis abstracts\nfrom the particular physical structures that implement the algorithm\nin our head. It is only at the third level of analysis that the\ndetails of the physical implementation of the algorithm in\nour brain are spelled out. \nWith these distinctions at hand, we can answer questions (a) and (b).\nA cognitive process is a cognitive capacity considered as an\ninformation-processing activity and taken in abstraction from its\nphysical implementation. Thus, cognitive processes are individuated in\nterms of what function they perform and/or in terms of what algorithms\ncompute these functions (fair enough, the “and/or” is a\nvery big deal, but it is something we can leave aside here). This\nmeans that the same (type of) cognitive process can be multiply\nrealized in different physical structures. For example, parsing\n(roughly, the cognitive process that assigns a grammatical structure\nto a string of signs) can be implemented both by a human brain and a\ncomputer. On the contrary, cognitive mechanisms are\nparticular (types of) physical structures—e.g., a certain part\nof the brain—implementing certain cognitive processes. More\nprecisely, cognitive mechanisms are organized structures carrying out\ncognitive processes in virtue of how their constituent parts interact\n(Bechtel 2008; Craver 2007; Machamer et al. 2000). \nWe now turn to question (c), which concerns the distinction between\nuse and reuse of a cognitive mechanism. At a first approximation, a\ncognitive mechanism is used when it performs its primary\nfunction, while it is reused when it is activated to perform\na different, non-primary function. For example, one is using\none’s visual mechanism when one employs it to see, while one is\nreusing it when one employs it to conjure up a visual image\n(see Anderson 2008, 2015 for further discussion of the notion of\nreuse). All this is a bit sketchy, but it will do. \nLet’s now go back to\n REU.\n The main idea behind it is that whether a mental state is a mental\nsimulation of another mental state depends on the cognitive\nprocesses generating these two mental states, and on the\ncognitive mechanisms implementing such cognitive processes.\nMore precisely, in order for mental state M* to be a mental\nsimulation of mental state M, it has to be case that:\n(i) cognitive processes P* and P, which\nrespectively generate M* and M, are both implemented by\nthe same (type of) cognitive mechanism C; (ii) P is\nimplemented by the use of C, while P* is\nimplemented by the reuse of C.  \nNow that we know what\n REU\n means, we can consider whether it fares better than\n RES-2\n in capturing the nature of the relation of mental simulation. It\nwould seem so. Consider this hypothetical scenario. Lisa is seeing a\nyellow banana, and her visual experience has been generated by\ncognitive process V1, which has been implemented by\nthe use of her visual mechanism. I am visualizing a yellow banana, and\nmy visual image has been generated by cognitive process\nV2, which has been implemented by the reuse of my\nvisual mechanism. Rosanna-the-Super-Reasoner is also visualizing a\nyellow banana, but her visual image has been generated by an\ninformation-rich cognitive process: a process drawing upon\nRosanna’s detailed knowledge of vision and implemented by her\nincredibly powerful reasoning mechanism.\n REU\n correctly predicts that my visual image is a mental simulation of\nLisa’s visual experience, but not vice versa. More importantly,\nit also predicts that Rosanna’s visual image does not count as a\nmental simulation of Lisa’s visual experience, given that\nRosanna’s cognitive process was not implemented by the reuse of\nthe visual mechanism. In this way,\n REU\n solves the problem faced by\n RES-2\n in distinguishing ST from TT. \nShould we then conclude that “mental simulation of” has to\nbe defined in terms of reuse, rather than in terms of resemblance?\nGoldman (2008a) is still not convinced. Suppose that while Lisa is\nseeing a yellow banana, I am using my visual mechanism to visualize\nthe Golden Gate Bridge. Now, even though Lisa’s visual\nexperience and my visual image have been respectively generated by the\nuse and the reuse of the visual mechanism, it would be bizarre to say\nthat my mental state is a mental simulation of Lisa’s. Why?\nBecause my mental state doesn’t resemble Lisa’s (she is\nseeing a yellow banana; I am visualizing the Golden Gate Bridge!)\nThus—Goldman concludes—resemblance should be taken as\nthe central feature of mental simulation. \nIn order to overcome the difficulties faced by trying to define\n“mental simulation of” in terms of either\nreplication or reuse, philosophers have built on the insights\nof both RES and\n REU\n and have proposed definitions that combine replication and reuse\nelements (Currie & Ravenscroft 2002; in recent years, Goldman\nhimself seems to have favoured a mixed account; see Goldman 2012a).\nHere is one plausible definition: \n(RES+REU) Token state M* is a mental simulation of token state\nM if and only if: \nRES+REU has at least three important virtues. The first is\nthat it solves all the aforementioned problems for RES and\n REU—we\n leave to the reader the exercise of showing that this is indeed the\ncase. \nThe second is that it fits nicely with an idea that loomed\nlarge in the simulationist literature: the idea that simulated\nmental states are “pretend” (“as if”,\n“quasi-”) states—imperfect copies of, surrogates\nfor, the “genuine” states normally produced by a certain\ncognitive mechanism, obtained by taking this cognitive mechanism\n“off-line”. Consider the following case. Frank is in front\nof Central Café (and believes that he is\nthere). He desires to drink a beer and believes that\nhe can buy one at Central Café. When he feeds these\nmental states into his decision-making mechanism, the mechanism\nimplements a decision-making process, which outputs the\ndecision to enter the café. In this case,\nFrank’s decision-making mechanism was\n“on-line”—i.e., he used it; he employed it\nfor its primary function. My situation is different. I don’t\nbelieve I am in front of Central Café, nor do I desire\nto drink a beer right now. Still, I can imagine believing and desiring\nso. When I feed these imagined states into my decision-making\nmechanism, I am not employing it for its primary function. Rather, I\nam taking it off-line (I am reusing it). As a result, the\ncognitive process implemented by my mechanism will output a merely\nimagined decision to enter the café. Now, it seems fair to\nsay that my imagined decision resembles Frank’s decision (more\non this in\n section 3).\n If you combine this with how these two mental states have been\ngenerated, the result is that my imagined decision is a mental\nsimulation of Frank’s decision, and thus it is a simulated\nmental state. It is also clear why Frank’s decision is\ngenuine, while my simulated mental state is just a\npretend decision: all else being equal, Frank’s\ndecision to enter Central Café will cause him to enter\nthe café; on the contrary, no such behaviour will result from\nmy simulated decision. I have not really decided so. Mine was\njust a quasi-decision—an imperfect copy of, a surrogate\nfor, Frank’s genuine decision. \nAnd here is\n RES+REU’s\n third virtue. So far, we have said that “mental\nsimulation” can either pick out a dyadic relation between mental\nstates or a monadic property of mental states. In fact, its ambiguity\nruns deeper than this, since philosophers and cognitive scientists\nalso use “mental simulation” to refer to a monadic\nproperty of cognitive processes, namely, the property\nbeing a (mental) simulation process (or: “process of\nmental simulation”, “simulational process”,\n“simulative process”, etc.) As a first stab, a (mental)\nsimulation process is a cognitive process generating simulated mental\nstates.\n RES+REU\n has the resources to capture this usage of “mental\nsimulation” too. Indeed,\n RES+REU\n implicitly contains the following definition of\n“simulation process”: \n(PROC): Token process P* is a (mental) simulation process if\nand only if: \nGo back to the case in which Lisa was having a visual experience of a\nyellow banana, while I was having a visual image of a yellow banana.\nOur two mental states resembled one another, but different cognitive\nprocesses generated them: seeing in Lisa’s case, and\nvisualizing in my case. Moreover, Lisa’s\nseeing was implemented by the use of the visual mechanism,\nwhile my visualizing was implemented by its reuse. According\nto PROC, the latter cognitive process, but not the former, was thus a\nsimulation process. \nTo sum up,\n RES+REU\n captures many of the crucial features that Simulation Theorists\nascribe to mental simulation. For this reason, we shall adopt it as\nour working definition of “mental simulation\nof”—consequently, we shall adopt PROC as a definition of\n“simulated mental\n state”.[2]\n We can put this into a diagram. \nFigure 1 \nThe hexagon at the bottom depicts a cognitive mechanism C (it\ncould be, say, the visual mechanism). When C is used (arrow on\nthe left), it implements cognitive process P (say, seeing);\nwhen it is re-used (arrow on the right), it implements cognitive\nprocess P* (say, visualizing). P generates mental state\nM (say, a visual experience of a red tomato), while P*\ngenerates mental state M* (say, a visual image of a red\ntomato). These two mental states (M and M*) resemble one\nanother. Given this: M* is a mental simulation of\nM; M* is a simulated mental state; and\nP* is a simulation\n process.[3] \nIn this section, we shall finally consider three worries raised for\nadopting\n RES+REU\n as a definition of “mental simulation of”. If you have\nalready had enough of\n RES+REU,\n please feel free to move straight to\n section 3. \nHeal (1994) pointed out a problem with committing ST to a particular\naccount of the cognitive mechanisms that underlie it. Suppose that the\nhuman mind contains two distinct decision-making mechanisms:\nMec1, which takes beliefs and desires as input, and generates\ndecisions as output; and Mec2, which works by following\nexactly the same logical principles as Mec1, but takes\nimagined beliefs and imagined desires as input and generates imagined\ndecisions as output. Consider again Frank’s decision to enter\nCentral Café and my imagined decision to do so.\nAccording to the two mechanisms hypothesis, Frank desired to drink a\nbeer and believed that he could buy one at Central\nCafé, fed these mental states into Mec1, which\ngenerated the decision to enter the café. As for me, I fed the\nimagined desire to drink a beer and the imagined belief that I could\nbuy one at Central Café into a distinct (type of)\nmechanism, i.e., Mec2, which generated the imagined decision\nto enter Central Café. Here is the question: does my\nimagined decision to enter Central Café count as a\nmental simulation of Frank’s decision to do so? If your answer\nis “Yes, it does”, then\n RES+REU\n is in trouble, since my imagined decision was not generated by\nreusing the same (type of) cognitive mechanism that Frank used to\ngenerate his decision; his decision was generated by Mec1, my\nimagined decision by Mec2. Thus, Heal concludes, a definition\n“mental simulation of” should not contain any commitment\nabout cognitive mechanisms—it should not make any implementation\nclaim—but should be given at a more abstract level of\ndescription. \nIn the face of this difficulty, a defender of\n RES+REU\n can say the following. First, she might reject the intuition that, in\nthe two mechanisms scenario, my imagined decision counts as a mental\nsimulation of Frank’s decision. At a minimum, she might say that\nthis scenario does not elicit any robust intuition in one direction or\nthe other: it is not clear whether these two mental states stand in\nthe relation being a mental simulation of. Second, she might\ndownplay the role of intuitions in the construction of a definition\nfor “mental simulation of” and cognate notions. In\nparticular, if she conceives of ST as an empirical theory in cognitive\nscience, she will be happy to discount the evidential value of\nintuitions if countervailing theoretical considerations are\navailable. This, e.g., is Currie and Ravenscroft’s (2002)\nposition, who write that  \nthere are two reasons … why the Simulation Theorist should\nprefer [a one mechanism hypothesis]: … first, the postulation\nof two mechanisms is less economical than the postulation of one;\nsecond, … we have very good reasons to think that\nimagination-based decision making does not operate in isolation from\nthe subject’s real beliefs and desires. … If imagination\nand belief operate under a system of inferential apartheid—as\nthe two-mechanisms view has it—how could this happen? (Currie\n& Ravenscroft 2002: 67–68) \nA second worry has to do with the fact that\n RES+REU\n appears to be too liberal. Take this case. Yesterday, Angelina had\nthe visual experience of a red apple. On the night of June 15, 1815,\nNapoleon conjured up the visual image of a red apple. Angelina used\nher visual mechanism to see, while Napoleon reused his to imagine. If\nwe add to this that Napoleon’s mental state resembled\nAngelina’s,\n RES+REU\n predicts that Napoleon’s (token) visual image was a mental\nsimulation of Angelina’s (token) visual experience. This might\nstrike one as utterly bizarre. In fact, not only did Napoleon not\nintend to simulate Angelina’s experience: he could\nnot even have intended to do it. After all, Angelina was born\nroughly 150 years after Napoleon’s death. By the same token, it\nis also impossible that Napoleon’s visual image has been\ncaused by Angelina’s visual experience. As a matter of\nfact, the visual image Napoleon had on the night of June 15, 1815 is\nentirely disconnected from the visual experience that\nAngelina had yesterday. Thus, how could the former be a mental\nsimulation of the latter? If you think about it, the problem is even\nworse than this.\n RES+REU\n has it that Napoleon’s visual image of a red apple is a mental\nsimulation of all the visual experiences of a red apple that\nhave obtained in the past, that are currently obtaining, and that will\nobtain in the future. Isn’t that absurd? \nAgain, a defender of\n RES+REU\n can give a two-fold answer. First, she can develop an argument that\nthis is not absurd at all. Intuitively, the following principle seems\nto be true: \n(TYPE): the mental state type visual image of a red apple is\na mental simulation of the mental state type visual experience of\na red apple. \nIf TYPE is correct, then the following principle has to be true as\nwell: \n(TOKEN): Any token mental state of the type visual image of a red\napple is a mental simulation of every token mental state of the\ntype visual experience of a red apple. \nBut TOKEN entails that Napoleon’s (token) visual image of a red\napple is a mental simulation of Angelina’s (token) visual\nexperience of a red apple, which is exactly what\n RES+REU\n predicts. Thus,\n RES+REU’s\n prediction, rather than being absurd, independently follows from\nquite intuitive assumptions. Moreover, even though TOKEN and\n RES+REU\n make the same prediction about the Napoleon-Angelina case, TOKEN is\nnot entailed by\n RES+REU,\n since the latter contains a restriction on how visual images\nhave to be generated. Thus, if one finds TOKEN intuitively acceptable,\nit is hard to see how one can find\n RES+REU\n to be too liberal. \nThe second component of the answer echoes one of the answers given to\nHeal: for a Simulation Theorist who conceives of ST as a theory in\ncognitive science, intuitions have a limited value in assessing a\ndefinition of “mental simulation of”. In fact, the main\naim of this definition is not that of capturing folk intuitions, but\nrather that of offering a clear enough picture of the relation of\nmental simulation on the basis of which an adequate theory of\nmindreading can be built. So, if the proposed definition fails, say,\nto help distinguishing ST from TT, or is of limited use in\ntheory-building, or is contradicted by certain important results from\ncognitive science, then one has a good reason to abandon it. On the\ncontrary, it should not be a cause for concern if\n RES+REU\n does not match the folk concept MENTAL SIMULATION OF. The notion\n“mental simulation of” is a term of art—like, say,\nthe notions of I-Language or of Curved Space. These notions\ndo poorly match the folk concepts of language and space, but\nlinguists and physicists do not take this to be a problem. The same\napplies to the notion of mental simulation. \nAnd here is the third and final worry.\n RES+REU\n is supposed to be a definition of “mental simulation of”\non the basis of which a theory of mindreading can be built. However,\nneither\n RES+REU\n nor\n PROC\n make any reference to the idea of representing others’ mental\nstates. Thus, how could these definitions help us to construct a\nSimulation Theory of mindreading? The answer is simple: they\nwill help us exactly as a clear definition of\n“computation”, which has nothing to do with how the mind\nworks, helped to develop the Computational Theory of Mind (see entry\non\n computational theory of mind). \nHere is another way to make the point. ST is made up of two\ndistinct claims: the first is that mental simulation is\npsychologically real, i.e., that there are mental states and processes\nsatisfying\n RES+REU\n and\n PROC.\n The second claim is that mental simulation plays a central role in\nmindreading. Clearly, the second claim cannot be true if the first is\nfalse. However, the second claim can be false even if the first claim\nis true: mental simulation could be psychologically real, but play no\nrole in mindreading at all. Hence, Simulation Theorists have to do\nthree things. First, they have to establish that mental simulation is\npsychologically real. We consider this issue in\n section 3.\n Second, they have to articulate ST as a theory of\nmindreading. That is, they have to spell out in some detail the\ncrucial role that mental simulation is supposed to play in\nrepresenting others’ mental states, and contrast the resulting\ntheory with other accounts of mindreading. We dwell on this in\nsections\n 4\n and\n 5.\n Finally, Simulation Theorists have to provide evidence in support of\ntheir theory of mindreading—that is, they have to give us good\nreasons to believe that mental simulation does play a crucial role in\nrepresenting others’ mental states. We discuss this issue in\n section 6. \nNow that we have definitions of “mental simulation of” and\ncognate notions, it is time to consider which mental states and\nprocesses satisfy them, if any. Are there really simulated mental\nstates? That is, are there mental states generated by the\nreuse of cognitive mechanisms? And do these mental states\nresemble the mental states generated by the use of such\nmechanisms? For example, is it truly the case that visual images are\nmental simulations of visual experiences? What about\ndecisions, emotions, beliefs, desires, and bodily sensations? Can our\nminds generate simulated counterparts of all these types of mental\nstates? In this section, we consider how Simulation Theorists have\ntackled these problems. We will do so by focusing on the following\nquestion: are there really simulation processes (as defined\nby\n PROC)?\n If the answer to this question is positive, it follows that there are\nmental states standing in the relation of mental simulation (as\ndefined by\n RES+REU),\n and thus simulated mental states. \nFollowing Goldman (2006), it has become customary among Simulation\nTheorists to argue for the existence of two types of simulation\nprocesses: high-level simulation processes and\nlow-level simulation processes (see, however, de Vignemont\n2009). By exploring this distinction, we begin to articulate the\ncognitive architecture underlying mental simulation proposed by\nST. \nHigh-level simulation processes are cognitive processes with the\nfollowing features: (a) they are typically conscious, under voluntary\ncontrol, and stimulus-independent; (b) they satisfy\n PROC,\n that is, they are implemented by the reuse of a certain\ncognitive mechanism, C, and their output states\nresemble the output states generated by the use of\n C.[4]\n Here are some cognitive processes that, according to Simulation\nTheorists, qualify as high-level simulation processes. Visualizing:\nthe cognitive process generating visual images (Currie 1995; Currie\n& Ravenscroft 2002; Goldman 2006); motor imagination: the\ncognitive process generating imagined bodily movements and actions\n(Currie & Ravenscroft 1997, 2002; Goldman 2006); imagining\ndeciding: the cognitive process generating decision-like imaginings\n(Currie & Ravenscroft 2002); imagining believing: the cognitive\nprocess generating belief-like imaginings (Currie & Ravenscroft\n2002); imagining desiring: the cognitive process generating\ndesire-like imaginings (Currie 2002). In what follows, we shall\nconsider a couple of them in some detail. \nVisualizing first. It is not particularly hard to see why visualizing\nsatisfies condition (a). Typically: one can decide to visualize (or\nstop visualizing) something; the process is not driven by perceptual\nstimuli; and at least some parts of the visualization process are\nconscious. There might be cases in which visualizing is not under\nvoluntary control, is stimulus-driven and, maybe, even entirely\nunconscious. This, however, is not a problem, since we know that there\nare clear cases satisfying (a). \nUnsurprisingly, the difficult task for Simulation Theorists is to\nestablish that visualizing has feature (b), that is: it is implemented\nby the reuse of the visual mechanism; and its outputs (that\nis, visual images) resemble genuine visual experiences.\nSimulation Theorists maintain that they have strong empirical evidence\nsupporting the claim that visualizing satisfies\n PROC.\n Here is a sample (this and further evidence is extensively discussed\nin Currie 1995, Currie & Ravenscroft 2002, and in Goldman\n2006): \nOn this basis, Simulation Theorists conclude that visualizing is\nindeed implemented by the reuse of the visual mechanism\n(evidence i and ii) and that its outputs, i.e., visual images, do\nresemble visual experiences (evidence iii, iv, and v). Thus,\nvisualizing is a process that qualifies as high-level simulation, and\nvisual images are simulated mental states. \nVisual images are mental simulations of perceptual states. Are there\nhigh-level simulation processes whose outputs instead are mental\nsimulations of propositional attitudes? (If you think that visual\nexperiences are propositional attitudes, you can rephrase the question\nas follows: are there high-level simulation processes whose outputs\nare mental simulations of non-sensory states?) Three candidate\nprocesses have received a fair amount of attention in the\nsimulationist literature: imagining desiring, imagining deciding, and\nimagining believing. The claims made by Simulation Theorists about\nthese cognitive processes and their output states have generated an\nintense debate (Doggett & Egan 2007; Funkhouser & Spaulding\n2009; Kieran & Lopes 2003; Nichols 2006a, 2006b; Nichols &\nStich 2003; Velleman 2000). We do not have space to review it here\n(two good entry points are the introduction to Nichols 2006a and the\nentry on\n imagination).\n Rather, we shall confine ourselves to briefly illustrating the\nsimulationist case in favour of the thesis that imagining believing is\na high-level simulation process. \nI don’t believe that Rome is in France, but I can imagine\nbelieving it. Imagining believing typically is a conscious,\nstimulus-independent process, under voluntary control. Thus, imagining\nbelieving satisfies condition (a). In order for it to count as an\ninstance of high-level simulation process, it also needs to\nhave feature (b), that is: (b.i) its outputs (i.e., belief-like\nimaginings) have to resemble genuine beliefs in some\nsignificant respects; (b.ii) it has to be implemented by the\nreuse of the cognitive mechanism (whose use implements the\ncognitive process) that generates genuine beliefs—let us call it\n“the belief-forming mechanism”. Does imagining believing\nsatisfy (b)? Currie and Ravenscroft (2002) argue in favour of (b.i).\nBeliefs are individuated in terms of their content and functional\nrole. Belief-like imaginings—Currie and Ravenscroft\nsay—have the same content and a similar functional role to their\ngenuine counterparts. For example, the belief that Rome is in France\nand the belief-like imagining that Rome is in France have exactly the\nsame propositional content: that Rome is in France. Moreover,\nbelief-like imaginings mirror the inferential role of genuine beliefs.\nIf one believes both that Rome is in France and that French is the\nlanguage spoken in France, one can infer the belief that French is the\nlanguage spoken in Rome. Analogously, from the belief-like imagining\nthat Rome is in France and the genuine belief that French is the\nlanguage spoken in France, one can infer the belief-like imagining\nthat French is the language spoken in Rome. So far, so good (but see\nNichols 2006b). \nWhat about (b.ii)? Direct evidence bearing on it is scarce. However,\nSimulation Theorists can give an argument along the following lines.\nFirst, one owes an explanation of why belief-like imaginings are,\nwell, belief-like—as we have said above, it seems that they have\nthe same type of content as, and a functional role similar to, genuine\nbeliefs. A possible explanation for this is that both types of mental\nstates are generated by (cognitive processes implemented by) the same\ncognitive mechanism. Second, it goes without saying that our mind\ncontains a mechanism for generating beliefs (the belief-forming\nmechanism), and that there must be some mechanism or another in charge\nof generating belief-like imaginings. It is also well known that\ncognitive mechanisms are evolutionary costly to build and maintain.\nThus, evolution might have adopted the parsimonious strategy of\nredeploying a pre-existing mechanism (the belief-forming mechanism)\nfor a non-primary function, i.e., generating belief-like\nimaginings—in general, this hypothesis is also supported by the\nidea that neural reuse is one of the fundamental organizational\nprinciple of the brain (Anderson 2008). If one puts these two strands\nof reasoning together, one gets a prima facie case for the\nclaim that imagining believing is implemented by the reuse of the\nbelief-forming mechanism—that is, a prima facie case\nfor the conclusion that imagining believing satisfies (b.ii). Since\nimagining believing appears also to satisfy (b.i) and (a), lacking\nevidence to the contrary, Simulation Theorists are justified in\nconsidering it to be a high-level simulation process. \nLet’s take stock. We have examined a few suggested instances of\nhigh-level simulation processes. If Simulation Theorists are correct,\nthey exhibit the following commonalities: they satisfy\n PROC\n (this is why they are simulation processes); they are\ntypically conscious, under voluntary control, and stimulus-independent\n(this is why they are high-level). Do they have some other\nimportant features in common? Yes, they do—Simulation Theorists\nsay. They all are under the control of a single cognitive\nmechanism: imagination (more precisely, Currie & Ravenscroft\n(2002) talk of Re-Creative Imagination, while Goldman (2006, 2009)\nuses the expression “Enactment Imagination”). The\nfollowing passage will give you the basic gist of the proposal: \nWhat is distinctive to high-level simulation is the psychological\nmechanism … that produces it, the mechanism of imagination.\nThis psychological system is capable of producing a wide variety of\nsimulational events: simulated seeings (i.e., visual imagery),\n… simulated motor actions (motor imagery), simulated beliefs,\n… and so forth. … In producing simulational outputs,\nimagination does not operate all by itself. … For example, it\nrecruits parts of the visual system to produce visual imagery\n…. Nonetheless, imagination “‘takes the\nlead”’ in directing or controlling the other systems it\nenlists for its project. (Goldman 2009: 484–85) \nHere is another way to make the point. We already know that, according\nto ST, visualizing is implemented by the reuse of the visual\nmechanism. In the above passage, Goldman adds that the reuse of the\nvisual mechanism is initiated, guided and controlled by imagination.\nThe same applies, mutatis mutandis, to all cases of\nhigh-level simulation processes. For example, in imagining hearing,\nimagination “gets in control” of the auditory mechanism,\ntakes it off-line, and (re)uses it to generate simulated auditory\nexperiences. Goldman (2012b, Goldman & Jordan 2013) supports this\nclaim by making reference to neuroscientific data indicating that the\nsame core brain network, the so-called “default network”,\nsubserves all the following self-projections: prospection (projecting\noneself into one’s future); episodic memory (projecting oneself\ninto one’s past); perspective taking (projecting oneself into\nother minds); and navigation (projecting oneself into other places)\n(see Buckner & Carroll 2007 for a review). These different\nself-projections presumably involve different high-level simulation\nprocesses. However, they all have something in common: they all\ninvolve imagination-based perspectival shifts. Therefore, the fact\nthat there is one brain network common to all these self-projections\nlends some support to the claim that there is one common cognitive\nmechanism, i.e., imagination, which initiates, guides, and controls\nall high-level simulation processes.  \nIf Goldman is right, and all high-level simulation processes are\nguided by imagination, we can then explain why, in our common\nparlance, we tend to describe high-level simulation processes and\noutputs in terms of imaginings, images, imagery, etc. More\nimportantly, we can also explain why high-level simulation processes\nare conscious, under voluntary control, and stimulus-independent.\nThese are, after all, typical properties of imaginative processes.\nHowever, there are simulation processes that typically are neither\nconscious, nor under voluntary control, nor stimulus independent. This\nindicates that they are not imagination-based. It is to this other\ntype of simulation processes that we now turn. \nLow-level simulation processes are cognitive processes with these\nfeatures: (a*) they are typically unconscious, automatic, and\nstimulus-driven; (b) they satisfy\n PROC,\n that is, they are implemented by the reuse of a certain\ncognitive mechanism, C, and their output states\nresemble the output states generated by the use of C.\nWhat cognitive processes are, according to ST, instances of low-level\nsimulation? The answer can be given in two words: mirroring processes.\nClarifying what these two words mean, however, will take some\ntime. \nThe story begins at the end of the 1980s in Parma, Italy, where the\nneuroscientist Giacomo Rizzolatti and his team were investigating the\nproperties of the neurons in the macaque monkey ventral premotor\ncortex. Through single-cell recording experiments, they discovered\nthat the activity of neurons in the area F5 is correlated with\ngoal-directed motor actions and not with particular movements\n(Rizzolatti et al. 1988). For example, some F5 neurons fire when the\nmonkey grasps an object, regardless of whether the monkey uses the\nleft or the right hand. A plausible interpretation of these results is\nthat neurons in monkey area F5 encode motor intentions (i.e.,\nthose intentions causing and guiding actions like reaching, grasping,\nholding, etc.) and not mere kinematic instructions (i.e.,\nthose representations specifying the fine-grained motor details of an\naction). (In-depth philosophical analyses of the notion of motor\nintention can be found in: Brozzo forthcoming; Butterfill &\nSinigaglia 2014; Pacherie 2000). This was an already interesting\nresult, but it was not what the Parma group became famous for. Rather,\ntheir striking discovery happened a few years later, helped by\nserendipity. Researchers were recording the activity of F5 neurons in\na macaque monkey performing an object-retrieval task. In between\ntrials, the monkey stood still and watched an experimenter setting up\nthe new trial, with microelectrodes still measuring the monkey’s\nbrain activity. Surprisingly, some of the F5 neurons turned out to\nfire when the monkey saw the experimenter grasping and\nplacing objects. This almost immediately led to new experiments, which\nrevealed that a portion of F5 neurons not only fire when the monkey\nperforms a certain goal-directed motor action (say, bringing a piece\nof food to the mouth), but also when it sees another agent performing\nthe same (type of) action (di Pellegrino et al. 1992; Gallese et al.\n1996; Rizzolatti et al. 1996). For this reason, these neurons were\naptly called “mirror neurons”, and it was\nproposed that they encode motor intentions both during action\nexecution and action observation (Rizzolatti & Sinigaglia 2007,\nforthcoming). Later studies found mirror neurons also in the macaque\nmonkey inferior parietal lobule (Gallese et al. 2002), which together\nwith the ventral premotor cortex constitutes the monkey cortical\nmirror neuron circuit (Rizzolatti & Craighero 2004). \nSubsequent evidence suggested that an action mirror\nmechanism—that is, a cognitive mechanism that gets\nactivated both when an individual performs a certain goal-directed\nmotor action and when she sees another agent performing the same\naction—also exists in the human brain (for reviews, see\nRizzolatti & Craighero 2004, and Rizzolatti & Sinigaglia\nforthcoming). In fact, it appears that there are mirror\nmechanisms in the human brain outside the action domain as well:\na mirror mechanism for disgust (Wicker et al. 2003), one for pain\n(Singer at al. 2004; Avenanti et al. 2005), and one for touch\n(Blakemore et al. 2005). Given the variety of mirror mechanisms, it is\nnot easy to give a definition that fits them all. Goldman (2008b) has\nquite a good one though, and we will draw from it: a cognitive\nmechanism is a mirror mechanism if and only if it gets activated both\nwhen an individual undergoes a certain mental event\nendogenously and when she perceives a sign that\nanother individual is undergoing the same (type of) mental event. For\nexample, the pain mirror mechanism gets activated both when\nindividuals experience “a painful stimulus and … when\nthey observe a signal indicating that [someone else] is receiving a\nsimilar pain stimulus” (Singer et al. 2004: 1157). \nHaving introduced the notions of mirror neuron and mirror mechanism,\nwe can define the crucial notion of this section: mirroring\nprocess. We have seen that mirror mechanisms can get activated in\ntwo distinct modes: (i) endogenously; (ii) in the perception mode. For\nexample, my action mirror mechanism gets endogenously activated when I\ngrasp a mug, while it gets activated in the perception mode when I see\nyou grasping a mug. Following again Goldman (2008b), let us say that a\ncognitive process is a mirroring process if and only if it is\nconstituted by the activation of a mirror mechanism in the\nperception mode. For example, what goes on in my brain when I see\nyou grasping a mug counts as a mirroring process. \nNow that we know what mirroring processes are, we can return to our\ninitial problem—i.e., whether they are low-level simulation\nprocesses (remember that a cognitive process is a low-level simulation\nprocess if and only if: (a*) it is typically unconscious, automatic,\nand stimulus-driven; (b) it satisfies\n PROC).\n For reasons of space, we will focus on disgust mirroring only. \nWicker et al. (2003) carried out an fMRI study in which participants\nfirst observed videos of disgusted facial expressions and subsequently\nunderwent a disgust experience via inhaling foul odorants. It turned\nout that the same neural area—the left anterior\ninsula—that was preferentially activated during the experience\nof disgust was also preferentially activated during the observation of\nthe disgusted facial expressions. These results indicate the existence\nof a disgust mirror mechanism. Is disgust mirroring\n(the activation of the disgust mirror mechanism in the perception\nmode) a low-level simulation process? Simulation Theorists answer in\nthe affirmative. \nHere is why disgust mirroring satisfies (a*): the process is\nstimulus-driven: it is sensitive to certain perceptual\nstimuli (disgusted facial expressions); it is automatic; and\nit is typically unconscious (even though its output, i.e.,\n“mirrored disgust”, is sometimes conscious). What about\ncondition (b)? Presumably, the primary (evolutionary) function of the\ndisgust mechanism is to produce a disgust response to spoiled food,\ngerms, parasites etc. (Rozin et al. 2008). In the course of evolution,\nthis mechanism could have been subsequently co-opted to also get\nactivated by the perception (of a sign) that someone else is\nexperiencing disgust, in order to facilitate social learning of food\npreferences (Gariépy et al. 2014). If this is correct, then\ndisgust mirroring is implemented by the reuse of the disgust\nmechanism (by employing this mechanism for a function different than\nits primary one). Moreover, the output of disgust mirroring\nresembles the genuine experience of disgust in at least two\nsignificant respects: first, both mental states have the same neural\nbasis; second, when conscious, they share a similar phenomenology.\nAccordingly, (b) is satisfied. By putting all this together,\nSimulation Theorists conclude that disgust mirroring is a low-level\nsimulation process, and mirrored disgust is a simulated mental state\n(Goldman 2008b; Barlassina 2013) \nIn the previous section, we examined the case for the psychological\nreality of mental simulation. We now turn to ST as a theory of\nmindreading. We will tackle two main issues: the extent to which\nmindreading is simulation-based, and how simulation-based mindreading\nworks. \nST proposes that mental simulation plays a central role in\nmindreading, i.e., it plays a central role in the capacity to\nrepresent and reason about others’ mental state. What does\n“central” mean here? Does it mean the central\nrole, with other contributors to mindreading being merely peripheral?\nThis is an important question, since in recent years there have been\nproposed hybrid models according to which both mental simulation and\ntheorizing play important roles in mindreading (see\n section 5.2).\n  \nA possible interpretation of the claim that mental simulation plays a\ncentral role in representing others’ mental states is that\nmindreading events are always simulation-based, even if they\nsometimes also involve theory. Some Simulation Theorists, however,\nreject this interpretation, since they maintain that there are\nmindreading events in which mental simulation plays no role at all\n(Currie & Ravenscroft 2002). For example, if I know that Little\nJimmy is happy every time he finds a dollar, and I also know that he\nhas just found a dollar, I do not need to undergo any simulation\nprocess to conclude that Little Jimmy is happy right now. I just need\nto carry out a simple logical inference. \nHowever, generalizations like, “Little Jimmy is happy every time\nhe finds a dollar,” are ceteris paribus rules. People\nreadily recognize exceptions: for example, we recognize situations in\nwhich Jimmy would probably not be happy even if he found a dollar,\nincluding some in which finding a dollar might actually make him\nunhappy. Rather than applying some additional or more complex rules\nthat cover such situations, it is arguable that putting ourselves in\nJimmy's situation and using “good common sense” alerts us\nto to these exceptions and overrides the rule. If that is correct,\nthen simulation is acting as an overseer or governor even when people\nappear to be simply applying rules.  \nGoldman (2006) suggests that we cash out the central role of mental\nsimulation in representing others’ mental states as follows:\nmindreading is often simulation-based. Goldman’s\nsuggestion, however, turns out to be empty, since he explicitly\nrefuses to specify what “often” means in this context.\n \nHow often is often? Every Tuesday, Thursday, and Saturday? Precisely\nwhat claim does ST mean to make? It is unreasonable to demand a\nprecise answer at this time. (Goldman 2006: 42; see also Goldman 2002;\nJeannerod & Pacherie 2004) \nPerhaps a better way to go is to characterize the centrality of mental\nsimulation for mindreading not in terms of frequency of use,\nbut in terms of importance. Currie and Ravenscroft make the\nvery plausible suggestion that “one way to see how important a\nfaculty is for performing a certain task is to examine what happens\nwhen the faculty is lacking or damaged” (Currie &\nRavenscroft 2002: 51). On this basis, one could say that mental\nsimulation plays a central role in mindreading if and only\nif: if one’s simulational capacity (i.e., the capacity to\nundergo simulation processes/simulated mental states) were impaired,\nthen one’s mindreading capacity would be significantly\nimpaired.  \nAn elaboration of this line of thought comes from Gordon (2005)—\nsee also Gordon (1986, 1996) and Peacocke (2005)—who argues that\nsomeone lacking the capacity for mental simulation would not be able\nto represent mental states as such, since she is incapable of\nrepresenting anyone as having a mind in the first place.\nGordon’s argument is essentially as follows: \nWe represent something as having a mind, as having mental states and\nprocesses, only if we represent it as a subject (“subject of\nexperience,” in formulations of “the hard problem of\nconsciousness”), where “a subject” is understood as\na generic “I”. This distinguishes it from a “mere\nobject” (and also is a necessary condition for a more benevolent\nsort of empathy). \nTo represent something as another “I” is to represent it\nas a possible target of self-projection: as something one might (with\nvarying degrees of success) imaginatively put oneself in the place of.\n(Of course, one can fancifully put oneself in the place of just about\nanything—a suspension bridge, even; but that is not a\nreductio ad absurdum, because one can also fancifully\nrepresent just about anything as having a mind.)  \nIt is not clear, however, what consequences Gordon’s conceptual\nargument would have for mindreading, if any. Even if a capacity to\nself-project were needed for representing mental states as such, would\nlack of this capacity necessarily impair mindreading? That is,\ncouldn't one predict explain, predict, and coordinate behavior using a\ntheory of internal states, without conceptualizing these as states of\nan I or subject? As a more general point, Simulation Theorists have\nnever provided a principled account of what would constitute a\n“significant impairment” of mindreading capacity. \nTo cut a long story short, ST claims that mental simulation plays a\ncentral role in mindreading, but at the present stage its proponents\ndo not agree on what this centrality exactly amounts to. We will come\nback to this issue in\n section 5,\n when we shall discuss the respective contributions of mental\nsimulation and theorizing in mindreading. \nWe now turn to a different problem: how does mental simulation\ncontribute to mindreading when it does? That is, how does\nsimulation-based mindreading work? Here again, Simulation Theorists\ndisagree about what the right answer is. In what follows, we explore\nsome dimensions of disagreement. \nSome Simulation Theorists defend a strong view of\nsimulation-based mindreading (Gordon 1986, 1995, 1996; Gallese et al.\n2004; Gallese & Sinigaglia 2011). They maintain that many\nsimulation-based mindreading events are (entirely)\nconstituted by mental simulation events (where mental\nsimulation events are simulated mental states or simulation\nprocesses). In other words, some Simulation Theorists claim that,\non many occasions, the fact that a subject S is\nrepresenting someone else’s mental states is nothing over and\nabove the fact that S is undergoing a mental simulation event:\nthe former fact reduces to the latter. For example, Lisa’s\nundergoing a mirrored disgust experience as a result of observing\nJohn’s disgusted face would count as a mindreading event:\nLisa’s simulated mental state would represent John’s\ndisgust (Gallese et al. 2004). Let us call this “the\nConstitution View”. \nWe shall elaborate on the details of the Constitution View in\n section 4.3.\n Before doing that, we consider an argument that has been directed\nagainst it over and over again, and which is supposed to show that the\nConstitution View is a non-starter (Fuller 1995; Heal 1995; Goldman\n2008b; Jacob 2008, 2012). Lacking a better name, we will call it\n“the Anti-Constitution argument”. Here it is. By\ndefinition, a mindreading event is a mental event in which a subject,\nS, represents another subject, Q, as having a certain\nmental state M. Now—the argument continues—the\nonly way in which S can represent Q as having\nM is this: S has to employ the concept of that\nmental state and form the judgment, or the belief,\nthat Q is in M. Therefore, a mindreading event is\nidentical to an event of judging that someone else has a certain\nmental state (where this entails the application of mentalistic\nconcepts). It follows from this that mental simulation events cannot\nbe constitutive of mindreading events, since the former events are not\nevents of judging that someone else has a certain mental state. An\nexample should clarify the matter. Consider Lisa again, who is\nundergoing a mirrored disgust experience as a result of\nobserving John’s disgusted face. Clearly, undergoing such a\nsimulated disgust experience is a different mental event from\njudging that John is experiencing disgust. Therefore,\nLisa’s mental simulation does not constitute a mindreading\nevent. \nIn\n section 4.3,\n we will discuss how the defenders of the Constitution View have\nresponded to this argument. Suppose for the moment that the\nAnti-Constitution argument is sound. What alternative pictures of\nsimulation-based mindreading are available? Those Simulation Theorists\nwho reject the Constitution View tend to endorse the Causation\nView, according to which mental simulation events never\nconstitute mindreading events, but only causally contribute\nto them. The best developed version of this view is Goldman’s\n(2006) Three-Stage Model (again, this is our label, not his),\nwhose basic structure is as follows: \nSTAGE 1. Mental simulation: Subject S undergoes a\nsimulation process, which outputs a token simulated mental state\nm*. \nSTAGE 2. Introspection: S introspects m* and\ncategorizes/conceptualizes it as (a state of type)\nM. \nSTAGE 3. Judgment: S attributes (a state of type)\nM to another subject, Q, through the judgment\nQ is in M. \n(The causal relations among these stages are such that: STAGE 1 causes\nSTAGE 2, and STAGE 2 in turn causes STAGE 3. See Spaulding 2012 for a\ndiscussion of the notion of causation in this context.) \nHere is our trite example. On the basis of observing John’s\ndisgusted facial expression, Lisa comes to judge that John is\nhaving a disgust experience. How did she arrive at the formation of\nthis judgment? Goldman’s answer is as follows. The observation\nof John’s disgusted facial expression triggered a disgust\nmirroring process in Lisa, resulting in Lisa’s undergoing a\nmirrored disgust experience (STAGE 1). This caused Lisa to introspect\nher simulated disgust experience and to categorize it as a disgust\nexperience (STAGE 2) (the technical notion of introspection used by\nGoldman will be discussed in\n section 4.4).\n This, in turn, brought about the formation of the judgment John\nis having a disgust experience (STAGE 3). Given that, according\nto Goldman, mindreading events are identical to events of judging that\nsomeone else has a certain mental state, it is only this last stage of\nLisa’s cognitive process that constitutes a mindreading event.\nOn the other hand, the previous two stages were merely causal\ncontributors to it. But mental simulation entirely took place at STAGE\n1. This is why the Three-Stage Model is a version of the Causation\nView: according to the model, mental simulation events causally\ncontribute to, but do not constitute, mindreading events. \nThe main strategy adopted by the advocates of the Constitution View in\nresponding to the Anti-Constitution argument consists in impugning the\nidentification of mindreading events with events of judging\nthat someone else has a certain mental state. A prominent version of\nthis position is Gordon’s (1995, 1996) Radical\nSimulationism, according to which representing someone\nelse’s mental states does not require the formation of\njudgments involving the application of mentalistic\nconcepts. Rather, Gordon proposes that the main bulk of\nmindreading events are non-conceptual representations of\nothers’ mental states, where these non-conceptual\nrepresentations are constituted by mental simulation events. If this\nis true, many mindreading events are constituted by mental\nsimulation events, and thus the Constitution View is correct. \nThe following case should help to get Radical Simulationism across.\nSuppose that I want to represent the mental state that an\nindividual—call him “Mr Tees”—is in right now.\nAccording to Gordon, there is a false assumption behind the idea that,\nin order to do so, I need to form a judgment with the content Mr\nTees is in M (where “M” is a\nplaceholder for a mentalistic concept). The false assumption is that\nthe only thing that I can do is to simulate myself in Mr\nTees’s situation. As Gordon points out, it is also possible for\nme to simulate Mr Tees in his situation. And if I do so, my\nvery simulation of Mr Tees constitutes a representation of\nhis mental state, without the need of forming any judgment. This is\nhow Gordon makes his point: \nTo simulate Mr Tees in his situation requires an egocentric shift, a\nrecentering of my egocentric map on Mr Tees. He becomes in my\nimagination the referent of the first person pronoun “I”.\n… Such recentering is the prelude to transforming myself in\nimagination into Mr Tees as much as actors become the characters they\nplay. … But once a personal transformation has been\naccomplished, … I am already representing him as being\nin a certain state of mind. (Gordon 1995: 55–56) \nIt is important to stress the dramatic difference between\nGordon’s Radical Simulationism and Goldman’s Three-Stage\nModel. According to the latter, mental simulation events causally\ncontribute to representing other people’s mental states,\nbut the mindreading event proper is always constituted by a\njudgment (or a belief). Moreover, Goldman maintains that the ability\nto form such judgments requires both the capacity to\nintrospect one’s own mental states (more in this in\n section 4.4)\n and possession of mentalistic concepts. None of this is true\nof Radical Simulationism. Rather, Gordon proposes that, in the large\nmajority of cases, it is the very mental simulation event itself that\nconstitutes a representation of someone else’s mental\nstates. Furthermore, since such mental simulation events neither\nrequire the capacity for introspection nor possession of mentalistic\nconcepts, Radical Simulationism entails the surprising conclusion that\nthese two features play at best a very minor role in mindreading. A\ntestable corollary is that social interaction often relies on an\nunderstanding of others that does not require the explicit application\nof mental state concepts. \nFrom what we have said so far, one could expect that Gordon should\nagree with Goldman on at least one point. Clearly, Gordon has to admit\nthat there are some cases of mindreading in which a subject\nattributes a mental state to someone else through a judgment involving\nthe application of mentalistic concepts. Surely, Gordon cannot deny\nthat there are occasions in which we think things like Mary\nbelieves that John is late or Pat desires to visit\nLisbon. Being a Simulation Theorist, Gordon will also presumably\nbe eager to maintain that many such mindreading events are based on\nmental simulation events. But if Gordon admits that much, should he\nnot also concede that Goldman’s Three-Stage Model is the right\naccount of at least those simulation-based mindreading\nevents? Surprising as it may be, Gordon still disagrees. \nGordon (1995) accepts that there are occasions in which a subject\narrives at a judgment about someone else’s mental state\non the basis of some mental simulation event. He might also concede to\nGoldman that such a judgment involves mentalistic concepts (but see\nGordon’s 1995 distinction between comprehending and\nuncomprehending ascriptions). Contra Goldman,\nhowever, Gordon argues that introspection plays no role at\nall in the generation of these judgments. Focusing on a specific\nexample will help us to clarify this further disagreement between\nGoldman and Gordon. \nSuppose that I know that Tom believes that (1) and (2): \nOn this basis, I attribute to Tom the further belief that (3): \nGoldman’s Three-Stage Model explains this mindreading act in the\nfollowing way. FIRST STAGE: I imagine believing what Tom believes\n(i.e., I imagine believing that (1) and (2)); I then feed those\nbelief-like imaginings into my reasoning mechanism (in the off-line\nmode); as a result, my reasoning mechanism outputs the imagined belief\nthat (3). The SECOND STAGE of the process consists in introspecting\nthis simulated belief and categorizing it as a belief.\nCrucially, in Goldman’s model, “introspection” does\nnot merely refer to the capacity to self-ascribe mental states.\nRather, it picks out a distinctive cognitive method for\nself-ascription, a method which is typically described as\nnon-inferential and quasi-perceptual (see the\nsection Inner sense accounts in the entry on\n self-knowledge).\n In particular, Goldman (2006) characterizes introspection as a\ntransduction process that takes the neural properties of a mental\nstate token as input and outputs a categorization of the type of\nstate. In the case that we are considering, my introspective mechanism\ntakes the neural properties of my token simulated belief as input and\ncategorizes it as a belief as output. After all this, the\nTHIRD STAGE occurs: I project the categorized belief onto Tom, through\nthe judgment Tom believes that Fido enjoys watching\nTV. (You might wonder where the content of Tom’s\nbelief comes from. Goldman (2006) has a story about that too, but we\nwill leave this aside). \nWhat about Gordon? How does he explain, in a simulationist fashion but\nwithout resorting to introspection, the passage from knowing that Tom\nbelieves that (1) and (2) to judging that Tom believes that (3)?\nAccording to Gordon, the first step in the process is, of course,\nimagining to be Tom—thus believing, in the context\nof the simulation, that (1) and (2). This results (again in the\ncontext of the simulation) in the formation of the belief that (3).\nBut how do I now go about discovering that *I*, Tom, believe that (3)?\nHow can one perform such a self-ascription if not via introspection? A\nsuggestion given by Gareth Evans will show us how—Gordon\nthinks. \nEvans (1982) famously argued that we answer the question “Do I\nbelieve that p?” by answering another question, namely\n“Is it the case that p?” In other words, according\nto Evans, we ascribe beliefs to ourselves not by introspecting, or by\n“looking inside”, but by looking “outside” and\ntrying to ascertain how the world is. If, e.g., I want to know whether\nI believe that Manchester is bigger than Sheffield, I just ask myself\n“Is Manchester bigger than Sheffield?” If I answer in the\naffirmative, then I believe that Manchester is bigger than Sheffield.\nIf I answer in the negative, then I believe that Manchester is\nnot bigger than Sheffield. If I do not know what to answer,\nthen I do not have any belief with regard to this subject\nmatter. \nGordon (1986, 1995) maintains that this self-ascription\nstrategy—which he labels “the ascent\nroutine” (Gordon 2007)—is also the strategy that we\nemploy, in the context of a simulation, to determine the mental states\nof the simulated agent: \nIn a simulation of O, I settle the question of whether O\nbelieves that p by simply asking … whether it is the\ncase that p. That is, I simply concern myself with the\nworld—O’s world, the world from O’s\nperspective. … Reporting O’s beliefs is\njust reporting what is there. (Gordon 1995: 60) \nSo, this is how, in Gordon’s story, I come to judge that Tom has\nthe belief that Fido enjoys watching TV. In the context of the\nsimulation, *I* asked *myself* (where both “*I*” and\n“*myself*” in fact refer to Tom) whether *I* believe that\nFido enjoys watching TV. And *I* answered this question by answering\nanother question, namely, whether it is the case that Fido enjoys\nwatching TV. Given that, from *my* perspective, Fido enjoys watching\nTV (after all, from *my* perspective, Fido is a dog and all dogs enjoy\nwatching TV), *I* expressed my belief by saying: “Yes,\n*I*, Tom, believe that Fido enjoys watching TV”. As you can see,\nin such a story, introspection does not do anything. (We will come\nback to the role of introspection in mindreading in\n section 6.2). \nIn sections 2, 3, and 4 we dwelt upon the “internal”\ndisagreements among Simulation Theorists. It goes without saying that\nsuch disagreements are both wide and deep. In fact, different\nSimulation Theorists give different answers to such fundamental\nquestions as: “What is mental simulation?”, “How\ndoes mental simulation contribute to mindreading?, ‘What is the\nrole of introspection in mindreading?” In light of such\ndifferences of opinion in the simulationist camp, one might conclude\nthat, after all, Stich and Nichols (1997) were right when saying that\nthere is no such thing as the Simulation Theory. However, if\none considers what is shared among Simulation Theorists, one will\nrealize that there is unity amidst this diversity. A good way to\nreveal the commonalities among different versions of ST is by\ncontrasting ST with its arch-enemy, i.e., the Theory-Theory of\nmindreading. This is what we do in the next section. \nST is only one of several accounts of mindreading on the market. A\nrough-and-ready list of the alternatives should at least include: the\nIntentional Stance Theory (Dennett 1987; Gergely & Csibra 2003;\nGergely et al. 1995); Interactionism (Gallagher 2001; Gallagher &\nHutto 2008; De Jaegher at al. 2010); and the Theory-Theory (Gopnik\n& Wellman 1992; Gopnik & Meltzoff 1997; Leslie 1994; Scholl\n& Leslie 1999). In this entry, we will discuss the Theory-Theory\n(TT) only, given that the TT-ST controversy has constituted the focal\npoint of the debate on mindreading during the last 30 years or so. \nAs suggested by its name, the Theory-Theory proposes that mindreading\nis grounded by the possession of a Theory of Mind (“a folk\npsychology”)—i.e., it is based on the tacit knowledge of\nthe following body of information: a number of “folk” laws\nor principles connecting mental states with sensory stimuli,\nbehavioural responses, and other mental states. Here are a couple of\nputative examples: \nLaw of sight: If S is in front of object O,\nS directs her eye-gaze to O, S’s visual\nsystem is properly functioning, and the environmental conditions are\noptimal, then ceteris paribus S will see O. \nLaw of the practical syllogism: If S desires a certain\noutcome G and S believes that by performing a certain\naction A she will obtain G, then ceteris\nparibus S will decide to perform A. \nThe main divide among Theory-Theorists concerns how the Theory of Mind\nis acquired—i.e., it concerns where this body of knowledge comes\nfrom. According to the Child-Scientist Theory-Theory (Gopnik &\nWellman 1992; Gopnik & Meltzoff 1997), a child constructs a Theory\nof Mind exactly as a scientist constructs a scientific theory: she\ncollects evidence, formulates explanatory hypotheses, and revises\nthese hypotheses in the light of further evidence. In other words,\n“folk” laws and principles are obtained through hypothesis\ntesting and revision—a process that, according to proponents of\nthis view, is guided by a general-purpose, Bayesian learning mechanism\n(Gopnik & Wellman 2012). On the contrary, the Nativist\nTheory-Theory (Carruthers 2013; Scholl & Leslie 1999) argues that\na significant part of the Theory of Mind is innate, rather than\nlearned. More precisely, Nativists typically consider the core of the\nTheory of Mind as resulting from the maturation of a cognitive module\nspecifically dedicated to representing mental states \nThese disagreements notwithstanding, the main tenet of TT is clear\nenough: attributions of mental states to other people are guided by\nthe possession of a Theory of Mind. For example, if I know that you\ndesire to buy a copy of The New York Times and I know that\nyou believe that if you go to News & Booze you can buy a\ncopy, then I can use the Law of the Practical Syllogism to\ninfer that you will decide to go to News & Booze. \nTT has been so popular among philosophers and cognitive scientists\nthat the explanation it proposes has ended up being the name of the\nvery phenomenon to be explained: on many occasions, scholars use the\nexpression “Theory of Mind” as a synonym of\n“mindreading”. Simulation Theorists, however, have never\nbeen particularly impressed by this. According to them, there is\nno need to invoke the tacit knowledge of a Theory of Mind to\naccount for mindreading, since a more parsimonious\nexplanation is available: we reuse our own cognitive\nmechanisms to mentally simulate others’ mental states. For\nexample, why do I need to know the Law of the Practical\nSyllogism, if I can employ my own decision-making mechanism\n(which I have anyway) to simulate your decision? It is\nuneconomical—Simulation Theorists say—to resort\nto an information-rich strategy, if an information-poor strategy will\ndo equally as well. \nThe difference between TT and ST can be further illustrated through a\nnice example given by Stich and Nichols (1992). Suppose that you want\nto predict the behavior of an airplane in certain atmospheric\nconditions. You can collect the specifications of the airplane and\ninfer, on the basis of aerodynamic theory, how the airplane\nwill behave. Alternatively, you can build a model of the airplane and\nrun a simulation. The former scenario approximates the way in\nwhich TT describes our capacity to represent others’ mental\nstates, while the latter approximates ST. Two points need to be\nstressed, though. First, while knowledge of aerodynamic theory is\nexplicit, TT says that our knowledge of the Theory of Mind is\ntypically implicit (or tacit). That is, someone who knows\naerodynamic theory is aware of the theory’s laws and\nprinciples and is able to report them correctly, while the laws and\nprinciples constituting one’s Theory of Mind typically lie\noutside awareness and reportability. Second, when we run a simulation\nof someone else’s mental states, we do not need to build a\nmodel: we are the model—that is, we use our own\nmind as a model of others’ minds. \nSimulation Theorists maintain that the default state for the\n“model” is one in which the simulator simply makes no\nadjustments when simulating another individual. That is, ST has it\nthat we are automatically disposed to attribute to a target mental\nstates no different from our own current states. This would often\nserve adequately in social interaction between people who are\ncooperating or competing in what is for practical purposes the same\nsituation. We tend to depart from this default when we perceive\nrelevant differences between others’ situations and our own. In\nsuch cases, we might find ourselves adjusting for situational\ndifferences by putting ourselves imaginatively in what we consider the\nother’s situation to be. \nWe might also make adjustments for individual differences. An\nacquaintance will soon be choosing between candidate a and\ncandidate b in an upcoming election. To us, projecting\nourselves imaginatively into that voting situation, the choice is\nglaringly obvious: candidate a, by any reasonable criteria.\nBut then we may wonder whether this imaginative projection into the\nvoting situation adequately represents our acquaintance in\nthat situation. We might recall things the person has said, or\npeculiarities of dress style, diet, or entertainment, that might seem\nrelevant. Internalizing such behavior ourselves, trying to “get\nbehind” it as an actor might get behind a scripted role, we\nmight then put, as it were, a different person into the voting\nsituation, one who might choose candidate b. \nSuch a transformation would require quarantining some of our own\nmental states, preferences, and dispositions, inhibiting them so that\nthey do not contaminate our off-line decision-making in the role of\nthe other. Such inhibition of one's own mental states would be\ncognitively demanding. For that reason, ST predicts that mindreading\nwill be subject to egocentric errors—that is, it\npredicts that we will often attribute to a target the mental state\nthat we would have if we were in the target’s situation, rather\nthan the state the target is actually in (Goldman 2006). In\n section 6.2,\n we shall discuss whether this prediction is borne out by the\ndata. \nOn the face of it, ST and TT could not be more different from one\nanother. Some philosophers, however, have argued that, on closer\ninspection, ST collapses into TT, thus revealing itself as a form of\nTT in disguise. The collapse argument was originally formulated by\nDaniel Dennett (1987): \nIf I make believe I am a suspension bridge and wonder what I will do\nwhen the wind blows, what “comes to my mind” in my\nmake-believe state depends on… my knowledge of\nphysics… Why should my making believe I have your beliefs be\nany different? In both cases, knowledge of the imitated object is\nneeded to drive the… “simulation”, and the\nknowledge must be… something like a theory. (Dennett\n1987: 100–101, emphasis added) \nDennett’s point is clear. If I imagine being, say, a bridge,\nwhat I imagine will depend on my theory of bridges. Suppose that I\nhave a folk theory of bridges that contains the following principle:\n“A bridge cannot sustain a weight superior to its own\nweight”. In this case, if I imagine an elephant weighing three\ntons walking over a bridge weighing two tons, I will imagine the\nbridge collapsing. Since my “bridge-simulation” is\nentirely theory-driven, “simulation” is a\nmisnomer. The same carries over to “simulating other\npeople”s mental states’, Dennett says. If I try to imagine\nyour mental states, what I imagine will depend entirely on my Theory\nof Mind. Therefore, the label “mental simulation” is\nmisleading. \nHeal (1986) and Goldman (1989) promptly replied to Dennett. Fair\nenough, if a system S tries to simulate the state of a\nradically different system Q (e.g., if a human being tries to\nsimulate the state of a bridge), then S’s simulation must\nbe guided by a theory. However, if a system S tries to simulate\nthe state of a relevantly similar system S*, then\nS’s simulation can be entirely process-driven:\nto simulate the state which S* is in, S simply has to\nrun in itself a process similar to the one S* underwent. Given\nthat, for all intents and purposes, human beings are relevantly\nsimilar to each other, a human being can mentally simulate what\nfollows from having another human being’s mental states without\nresorting to a body of theoretical knowledge about the mind’s\ninner workings. She will just need to reuse her own cognitive\nmechanisms to implement a simulation process. \nThis reply invited the following response (Jackson 1999). If the\npossibility of process-driven simulation is grounded in the similarity\nbetween the simulator and the simulated, then I have to assume that\nyou are relevantly similar to me, when I mentally simulate your mental\nstates. This particular assumption, in turn, will be derived from a\ngeneral principle—something like “Human beings\nare psychologically similar”. Therefore, mental simulation is\ngrounded in the possession of a theory. The threat of collapse is\nback! One reply to Jackson’s arguments is as follows (for other\nreplies see Goldman 2006): the fact that process-driven simulation is\ngrounded in the similarity among human beings does not entail\nthat, in order to run a simulation, a simulator must know (or believe,\nor assume) that such similarity obtains; no more, indeed, than the\nfact that the solubility of salt is grounded in the molecular\nstructure of salt entails that a pinch of salt needs to know chemistry\nto dissolve in water. \nGranting that ST and TT are distinct theories, we can now ask a\ndifferent question: are the theories better off individually or should\nthey join forces somehow? Let us be more explicit. Can ST on its own\noffer an adequate account of mindreading (or at least of the great\nmajority of its episodes)? And what about TT? A good number of\ntheorists now believe that neither ST nor TT alone will do. Rather,\nmany would agree that these two theories need to cooperate, if they\nwant to reach a satisfactory explanation of mindreading. Some authors\nhave put forward TT-ST hybrid models, i.e., models in which the tacit\nknowledge of a Theory of Mind is the central aspect of mindreading,\nbut it is in many cases supplemented by simulation processes\n(Botterill & Carruthers 1999; Nichols & Stich 2003). Other\nauthors have instead defended ST-TT hybrid models, namely, accounts of\nmindreading where the pride of place is given to mental simulation,\nbut where the possession of a Theory of Mind plays some non-negligible\nrole nonetheless (Currie & Ravenscroft 2002; Goldman 2006; Heal\n2003). Since this entry is dedicated to ST, we will briefly touch upon\none instance of the latter variety of hybrid account. \nHeal (2003) suggested that the domain of ST is restricted to those\nmental processes involving rational transitions among\ncontentful mental states. To wit, Heal maintains that mental\nsimulation is the cognitive routine that we employ to represent other\npeople’s rational processes, i.e., those cognitive\nprocesses which are sensitive to the semantic content of the mental\nstates involved. On the other hand,  \nwhen starting point and/or outcome are [states] without content,\nand/or the connection is not [rationally] intelligible, there is no\nreason … to suppose that the process … can be simulated.\n(Heal 2003: 77) \nAn example will clarify the matter. Suppose that I know that you\ndesire to eat sushi, and that you believe that you can order sushi by\ncalling Yama Sushi. To reach the conclusion that you will\ndecide to call Yama Sushi, I only need to imagine desiring\nand believing what you desire and believe, and to run a simulated\ndecision-making process in myself. No further knowledge is required to\npredict your decision: simulation alone will do the job. Consider, on\nthe other hand, the situation in which I know that you took a certain\ndrug and I want to figure out what your mental states will be. In this\ncase—Heal says—my prediction cannot be based on mental\nsimulation. Rather, I need to resort to a body of information about\nthe likely psychological effects of that drug, i.e., I have to resort\nto a Theory of Mind (fair enough, I can also take the drug myself, but\nthis will not count as mental simulation). This, according to Heal,\ngeneralizes to all cases in which a mental state is the input or the\noutput of a mere causal process. In those cases, mental\nsimulation is ineffective and should be replaced by theorizing. Still,\nthose cases do not constitute the central part of mindreading. In\nfact, many philosophers and cognitive scientists would agree that the\ncrucial component of human mindreading is the ability to reason about\nothers’ propositional attitudes. And this is exactly\nthe ability that, according to Heal, should be explained in term of\nmental simulation. This is why Heal’s proposal counts as an\nST-TT hybrid, rather than the other way around. \nST has sparked a lively debate, which has been going on since the end\nof the 1980s. This debate has dealt with a great number of theoretical\nand empirical issues. On the theoretical side, we have seen\nphilosophical discussions of the relation between ST and functionalism\n(Gordon 1986; Goldman 1989; Heal 2003; Stich & Ravenscroft 1992),\nand of the role of tacit knowledge in cognitive explanations (Davies\n1987; Heal 1994; Davies & Stone 2001), just to name a few.\nExamples of empirical debates are: how to account for mindreading\ndeficits in Autism Spectrum Disorders (Baron-Cohen 2000; Currie &\nRavenscroft 2002), or how to explain the evolution of mindreading\n(Carruthers 2009; Lurz 2011). It goes without saying that discussing\nall these bones of contention would require an entire book (most\nprobably, a series of books). In the last section of this\nentry, we confine ourselves to briefly introducing the reader to a\nsmall sample of the main open issues concerning ST. \nWe wrote that ST proposes that mirroring processes (i.e., activations\nof mirror mechanisms in the perception mode): (A) are\n(low-level) simulation processes, and (B) contribute (either\nconstitutively or causally) to mindreading (Gallese et al. 2004;\nGallese & Goldman 1998; Goldman 2006, 2008b; Hurley 2005). Both\n(A) and (B) have been vehemently contested by ST’s\nopponents. \nBeginning with (A), it has been argued that mirroring processes do not\nqualify as simulation processes, because they fail to satisfy the\ndefinition of “simulation process” (Gallagher 2007;\nHerschbach 2012; Jacob 2008; Spaulding 2012) and/or because they are\nbetter characterized in different terms, e.g., as enactive perceptual\nprocesses (Gallagher 2007) or as elements in an information-rich\nprocess (Spaulding 2012). As for (B), the main worry runs as follows.\nGranting that mirroring processes are simulation processes, what\nevidence do we have for the claim that they contribute to mindreading?\nThis, in particular, has been asked with respect to the role of\nmirroring processes in “action understanding” (i.e., the\ninterpretation of an agent’s behavior in terms of the\nagent’s intentions, goals, etc.). After all, the neuroscientific\nevidence just indicates that action mirroring correlates with\nepisodes of action understanding, but correlation is not causation,\nlet alone constitution. In fact, there are no studies examining\nwhether disruption of the monkey mirror neuron circuit results in\naction understanding deficits, and the evidence on human action\nunderstanding following damage to the action mirror mechanism is\ninconclusive at best (Hickok 2009). In this regard, some authors have\nsuggested that the most plausible hypothesis is instead that action\nmirroring follows (rather than causes or constitutes) the\nunderstanding of others’ mental states (Csibra 2007; Jacob\n2008). For example, Jacob (2008) proposes that the job of mirroring\nprocesses in the action domain is just that of computing a\nrepresentation of the observed agent’s next movement,\non the basis of a previous representation of the\nagent’s intention. Similar deflationary accounts of the action\nmirror mechanism have been given by Brass et al. (2007), Hickok\n(2014), and Vannuscorps and Caramazza (2015)—these accounts\ntypically take the STS (superior temporal sulcus, a brain region\nlacking mirror neurons) to be the critical neural area for action\nunderstanding. \nThere are various ways to respond to these criticisms. A strong\nresponse argues that they are based on a misunderstanding of the\nrelevant empirical findings, as well as on a mischaracterization of\nthe role that ST attributes to the action mirror mechanism in action\nunderstanding (Rizzolatti & Sinigaglia 2010, 2014). A weaker\nresponse holds that the focus on action understanding is a bit of a\nred herring, given that the most robust evidence in support of the\ncentral role played by mirroring processes in mindreading comes from\nthe emotion domain (Goldman 2008b). We will consider the weaker\nresponse here. \nGoldman and Sripada (2005) discuss a series of paired deficits in\nemotion production and face-based emotion mindreading. These\ndeficits—they maintain—are best explained by the\nhypothesis that one attributes emotions to someone else through\nsimulating these emotions in oneself: when the ability to undergo the\nemotion breaks down, the mindreading capacity breaks down as well.\nBarlassina (2013) elaborates on this idea by considering\nHuntington’s Disease (HD), a neurodegenerative disorder\nresulting in, among other things, damage to the disgust mirror\nmechanism. As predicted by ST, the difficulties individuals with HD\nhave in experiencing disgust co-occur with an impairment in\nattributing disgust to someone else on the basis of observing her\nfacial expression—despite perceptual abilities and knowledge\nabout disgust being preserved in this clinical population. Individuals\nsuffering from HD, however, exhibit an intact capacity for disgust\nmindreading on the basis of non-facial visual stimuli. For this\nreason, Barlassina concludes by putting forward an ST-TT hybrid model\nof disgust mindreading on the basis of visual stimuli. \nST’s central claim is that we reuse our own cognitive\nmechanisms to arrive at a representation of other\npeople’s mental states. This claim raises a number of issues\nconcerning how ST conceptualizes the self-other relation. We will\ndiscuss a couple of them. \nGallagher (2007: 355) writes that  \ngiven the large diversity of motives, beliefs, desires, and behaviours\nin the world, it is not clear how a simulation process … can\ngive me a reliable sense of what is going on in the other\nperson’s mind.  \nThere are two ways of interpreting Gallagher’s worry. First, it\ncan be read as saying that if mindreading is based on mental\nsimulation, then it is hard to see how mental state attributions could\nbe epistemically justified. This criticism, however, misses\nthe mark entirely, since ST is not concerned with whether mental state\nattributions count as knowledge, but only with how, as a matter of\nfact, we go about forming such attributions. A second way to\nunderstand Gallagher’s remarks is this: as a matter of\nfact, we are pretty successful in understanding other minds;\nhowever, given the difference among individual minds, this pattern of\nsuccesses cannot be explained in terms of mental simulation. \nST has a two-tier answer to the second reading of Gallagher’s\nchallenge. First, human beings are very similar with regard to\ncognitive processes such as perception, theoretical reasoning,\npractical reasoning, etc. For example, there is a very high\nprobability that if both you and I look at the same scene, we will\nhave the same visual experience. This explains why, in the large\nmajority of cases, I can reuse my visual mechanism to successfully\nsimulate your visual experiences. Second, even though we are quite\ngood at recognizing others’ mental states, we are nonetheless\nprone to egocentric errors, i.e., we tend to attribute to a\ntarget the mental state that we would undergo if we were in\nthe target’s situation, rather than the actual mental state the\ntarget is in (Goldman 2006). A standard example is the curse of\nknowledge bias, where we take for granted that other people know\nwhat we know (Birch & Bloom 2007). ST has a straightforward\nexplanation of such egocentric errors (Gordon 1995; Goldman 2006): if\nwe arrive at attributing mental states via mental simulation, the\nattribution accuracy will depend on our capacity to\n“quarantine” our genuine mental states, when they do not\nmatch the target’s, and to replace them with more appropriate\nsimulated mental states. This “adjustment” process,\nhowever, is a demanding one, because our genuine mental states exert a\npowerful tendency. Thus, Gallagher is right when he says that, on some\noccasions, “if I project the results of my own simulation onto\nthe other, I understand only myself in that other’s situation,\nbut I don’t understand the other” (Gallagher 2007: 355).\nHowever, given how widespread egocentric errors are, this counts as a\npoint in favour of ST, rather than as an argument against it (but see\nde Vignemont & Mercier 2016, and Saxe 2005). \nCarruthers (1996, 2009, 2011) raises a different problem for ST: no\nversion of ST can adequately account for self-attributions of mental\nstates. Recall that, according to Goldman (2006), simulation-based\nmindreading is a three-stage process in which we first mentally\nsimulate a target’s mental state, we then introspect\nand categorize the simulated mental state, and we finally attribute\nthe categorized state to the target. Since Goldman’s model has\nit that attributions of mental states to others asymmetrically depend\non the ability to introspect one’s own mental states, it\npredicts that: (A) introspection is (ontogenetically and\nphylogenetically) prior to the ability to represent others’\nmental states; (B) there are cases in which introspection works just\nfine, but where the ability to represent others’ mental states\nis impaired (presumably, because the mechanism responsible for\nprojecting one’s mental states to the target is damaged).\nCarruthers (2009) argues that neither (A) nor (B) are borne out by the\ndata. The former because there are no creatures that have\nintrospective capacities but at the same time lack the ability to\nrepresent others’ mental states; the latter because there are no\ndissociation cases in which an intact capacity for introspection is\npaired with an impairment in the ability to represent others’\nmental states. \nHow might a Simulation Theorist respond to this objection? As we said\nin\n section 4,\n Gordon’s (1986, 1995, 1996) Radical Simulationism does\nnot assign any role to introspection in mindreading. Rather, Gordon\nproposes that self-ascriptions are guided by ascent routines through\nwhich we answer the question “Do I believe that p?”\nby answering the lower-order question “Is it the case that\np?” Carruthers (1996, 2011) thinks that this won’t\ndo either. Here is one of the many problems that Carruthers raises for\nthis suggestion—we can call it “The Scope\nProblem”:  \nthis suggestion appears to have only a limited range of application.\nFor even if it works for the case of belief, it is very hard to see\nhow one might extend it to account for our knowledge of our own goals,\ndecisions, or intentions—let alone our knowledge of our own\nattitudes of wondering, supposing, fearing, and so on. (Carruthers\n2011: 81) \nCarruthers’ objections are important and deserve to be taken\nseriously. To discuss them, however, we would need to introduce a lot\nof further empirical evidence and many complex philosophical ideas\nabout self-knowledge. This is not a task that we can take up here (the\ninterested reader is encouraged to read, in addition to Gordon (2007)\nand Goldman (2009), the SEP entries on\n self-knowledge\n and on\n introspection).\n The take-home message should be clear enough nonetheless: anybody who\nputs forward an account of mindreading should remember that such an\naccount has to cohere with a plausible story about the cognitive\nmechanisms underlying self-attribution. \nThe development of mindreading capacities in children has been one of\nthe central areas of empirical investigation. In particular,\ndevelopmental psychologists have put a lot of effort into detailing\nhow the ability to attribute false beliefs to others develops. Until\n2005, the central experimental paradigm to test this ability was the\nverbal false belief task (Wimmer & Perner 1983). Here is\na classic version of it. A subject is introduced to two dolls, Sally\nand Anne, and three objects: Sally’s ball, a basket, and a box.\nSally puts her ball in the basket and leaves the scene. While Sally is\naway, Anne takes the ball out of the basket and puts it into the box.\nSally then returns. The subject is asked where she thinks Sally will\nlook for the ball. The correct answer, of course, is that Sally will\nlook inside the basket. To give this answer, the subject has to\nattribute to Sally the false belief that the ball is in the\nbasket. A number of experiments have found that while four-year old\nchildren pass this task, three-year old children fail it (for a\nreview, see Wellman et al. 2001). For a long time, the mainstream\ninterpretation of these findings has been that children acquire the\nability to attribute false beliefs only around their fourth birthday\n(but see Clements & Perner 1994 and Bloom & German 2000). \nIn 2005, this developmental timeline was called into question.\nKristine Onishi and Renée Baillargeon (2005) published the\nresult of a non-verbal version of the false belief task,\nwhich they administered to 15-month old infants. The experiment\ninvolves three steps. First, the infants see a toy between two boxes,\none yellow and one green, and then an actor hiding the toy inside the\ngreen box. Next, the infants see the toy sliding out of the green box\nand hiding inside the yellow box. In the true belief condition (TB),\nthe actor notices that the toy changes location, while in the false\nbelief condition (FB) she does not. Finally, half of the infants see\nthe actor reaching into the green box, while the other half sees the\nactor reaching into the yellow box. According to the\nviolation-of-expectation paradigm, infants reliably look for\na longer time at unexpected events. Therefore, if the infants expected\nthe actor to search for the toy on the basis of the\nactor’s belief about its location, then when the actor had a\ntrue belief that the toy was hidden in one box, the infants\nshould look longer when the actor reached into the other box instead.\nConversely, the infants should look longer at one box when the actor\nfalsely believed that the toy was hidden in the other box.\nStrikingly, these predictions were confirmed in both the (TB) and (FB)\nconditions. On this basis, Onishi and Baillargeon (2005) concluded\nthat children of 15 months possess the capacity to represent\nothers’ false beliefs. \nThis and subsequent versions of non-verbal false belief tasks\nattracted a huge amount of interest (at the current stage of research,\nthere is evidence that sensitivity to others’ false beliefs is\npresent in infants as young as 7 months—for a review, see\nBaillargeon at al. 2016). Above all, the following two questions have\nbeen widely discussed: why do children pass the non-verbal false\nbelief task at such an early age, but do not pass the verbal version\nbefore the age of 4? Does passing the non-verbal false belief task\nreally indicate the capacity to represent others’ false beliefs?\n(Perner & Ruffman 2005; Apperly & Butterfill 2009; Baillargeon\net al. 2010; Carruthers 2013; Helming et al. 2014). \nGoldman and Jordan (2013) maintain that ST has a good answer to both\nquestions. To begin with, they argue that it is implausible to\nattribute to infants such sophisticated meta-representational\nabilities as the ability to represent others’ false beliefs.\nThus, Goldman and Jordan favour a deflationary view, according to\nwhich infants are sensitive to others’ false beliefs,\nbut do not represent them as such. In particular, they\npropose that rather than believing that another subject S\n(falsely) believes that p, infants simply imagine how the\nworld is from S’s perspective—that is, they simply\nimagine that p is the case. This—Goldman and Jordan\nsay—is a more primitive psychological competence than\nmindreading, since it does not involve forming a judgment about\nothers’ mental states. This brings us to Goldman and\nJordan’s answer to the question “why do children pass the\nverbal false belief task only at four?” Passing this task\nrequires fully-fledged mindreading abilities and executive functions\nsuch as inhibitory control. It takes quite a lot of time—around\n3 to 4 years—before these functions and abilities come\nonline. \nSince the late 1980s, ST has received a great amount of attention from\nphilosophers, psychologists, and neuroscientists. This is not\nsurprising. Mindreading is a central human cognitive capacity, and ST\nchalleges some basic assumptions about the cognitive processes and\nneural mechanisms underlying human social behavior. Moreover, ST\ntouches upon a number of major philosophical problems, such as the\nrelation between self-knowledge and knowledge of other minds, and the\nnature of mental concepts, including the concept of mind itself. In\nthis entry, we have considered some of the fundamental empirical and\nphilosophical issues surrounding ST. Many of them remain open. In\nparticular, while the consensus view is now that both mental\nsimulation and theorizing play important role in mindreading, the\ncurrently available evidence falls short of establishing what their\nrespective roles are. In other words, it is likely that we shall end\nup adopting a hybrid model of mindreading that combines ST and TT,\nbut, at the present stage, it is very difficult to predict what this\nhybrid model will look like. Hopefully, the joint work of philosophers\nand cognitive scientists will help to settle the matter.","contact.mail":"l.barlassina@sheffield.ac.uk","contact.domain":"sheffield.ac.uk"},{"date.published":"1997-12-08","date.changed":"2017-03-28","url":"https://plato.stanford.edu/entries/folkpsych-simulation/","author1":"Luca Barlassina","author1.info":"https://www.umsl.edu/~philo/People/Faculty/index.html","entry":"folkpsych-simulation","body.text":"\n\n\nThe capacity for “mindreading” is understood in philosophy\nof mind and cognitive science as the capacity to represent, reason\nabout, and respond to others’ mental states. Essentially the\nsame capacity is also known as “folk psychology”,\n“Theory of Mind”, and “mentalizing”. An\nexample of everyday mindreading: you notice that Tom’s\nfright embarrassed Mary and surprised Bill,\nwho had believed that Tom wanted to try\neverything. Mindreading is of crucial importance for our social life:\nour ability to predict, explain, and/or coordinate with others’\nactions on countless occasions crucially relies on representing their\nmental states. For instance, by attributing to Steve the desire for a\nbanana and the belief that there are no more bananas at home but there\nare some left at the local grocery store, you can: (i)\nexplain why Steve has just left home; (ii) predict\nwhere Steve is heading; and (iii) coordinate your behavior\nwith his (meet him at the store, or prepare a surpise party while he\nis gone). Without mindreading, (i)–(iii) do not come\neasily—if they come at all. That much is fairly uncontroversial.\nWhat is controversial is how to explain mindreading. That is, how do\npeople arrive at representing others’ mental states? This is the\nmain question to which the Simulation (or, mental simulation) Theory\n(ST) of mindreading offers an answer.\n\n\nCommon sense has it that, in many circumstances, we arrive at\nrepresenting others’ mental states by putting ourselves in their\nshoes, or taking their perspective. For example, I can try to figure\nout my chess opponent’s next decision by imagining what I would\ndecide if I were in her place. (Although we may also speak of this as\na kind of empathy, that term must be understood here without\nany implication of sympathy or benevolence.)\n\n\nST takes this commonsensical idea seriously and develops it into a\nfully-fledged theory. At the core of the theory, we find the thesis\nthat mental simulation plays a central role in mindreading:\nwe typically arrive at representing others’ mental states by\nsimulating their mental states in our own mind. So, to figure out my\nchess opponent’s next decision, I mentally switch roles with her\nin the game. In doing this, I simulate her relevant beliefs\nand goals, and then feed these simulated mental states into\nmy decision-making mechanism and let the mechanism produce a simulated\ndecision. This decision is projected on or attributed to the\nopponent. In other words, the basic idea of ST is that if the\nresources our own brain uses to guide our own behavior can be modified\nto work as representations of other people’s mental states, then\nwe have no need to store general information about what makes people\ntick: we just do the ticking for them. Accordingly, ST challenges the\nTheory-Theory of mindreading (TT), the view that a tacit psychological\ntheory underlies the ability to represent and reason about\nothers’ mental states. While TT maintains that mindreading is an\ninformation-rich and theory-driven process, ST sees it as\ninformationally poor and process driven (Goldman 1989).\n\n\nThis entry is organized as follows. In section 1 (The Origins and\nVarieties of ST), we briefly reconstruct ST’s history and\nelaborate further on ST’s main theoretical aims. We then go on\nto explain the very idea of mental simulation (section 2: What is\nMeant by “Mental Simulation”?) In section 3 (Two Types of\nSimulation Processes), we consider the cognitive architecture\nunderlying mental simulation and introduce the distinction between\nhigh-level and low-level simulation processes. In section 4 (The Role\nof Mental Simulation in Mindreading), we discuss what role mental\nsimulation is supposed to play in mindreading, according to ST. This\ndiscussion carries over to section 5 (Simulation Theory and\nTheory-Theory), where we contrast the accounts of mindreading given by\nST and TT. Finally, section 6 (Simulation Theory: Pros and Cons)\nexamines some of the main arguments in favour of and against ST as\ntheory of mindreading.\n\nThe idea that we often arrive at representing other people’s\nmental states by mentally simulating those states in ourselves has a\ndistinguished history in philosophy and the human sciences. Robert\nGordon (1995) traces it back to David Hume (1739) and Adam\nSmith’s (1759) notion of sympathy; Jane Heal (2003) and Gordon\n(2000) find simulationist themes in the Verstehen approach to\nthe philosophy of history (e.g., Dilthey 1894); Alvin Goldman (2006)\nconsiders Theodor Lipps’s (1903) account of empathy\n(Einfühlung) as a precursor of the notion of mental\nsimulation. \nIn its modern guise, ST was established in 1986, with the publication\nof Robert Gordon’s “Folk Psychology as Simulation”\nand Jane Heal’s “Replication and Functionalism”.\nThese two articles criticized the Theory-Theory and introduced ST as a\nbetter account of mindreading. In his article, Gordon discussed\npsychological findings concerning the development of the capacity to\nrepresent others’ false beliefs. This attracted the interest of\ndevelopmental psychologists, especially Paul Harris (1989, 1992), who\npresented empirical support for ST, and Alison Gopnik (Gopnik &\nWellman 1992) and Joseph Perner (Perner & Howes 1992), who argued\nagainst it—Perner has since come to defend a hybrid version of\nST (Perner & Kühberger 2005). \nAlvin Goldman was an early and influential defender of ST (1989) and\nhas done much to give the theory its prominence. His work with the\nneuroscientist Vittorio Gallese (Gallese & Goldman 1998) was the\nfirst to posit an important connection between ST and the newly\ndiscovered mirror neurons. Goldman’s 2006 book Simulating\nMinds is the clearest and most comprehensive account to date of\nthe relevant philosophical and empirical issues. Among other\nphilosophical proponents of ST, Gregory Currie and Susan Hurley have\nbeen influential. \nSince the late 1980s, ST has been one of the central players in the\nphilosophical, psychological, and neuroscientific discussions of\nmindreading. It has however been argued that the fortunes of ST have\nhad a notable negative consequence: the expression “mental\nsimulation” has come to be used broadly and in a variety of\nways, making “Simulation Theory” a blanket term lumping\ntogether many distinct approaches to mindreading. Stephen Stich and\nShaun Nichols (1997) already urged dropping it in favor of a\nfiner-grained terminology. There is some merit to this. ST is in fact\nbetter conceived of as a family of theories rather than a\nsingle theory. All the members of the family agree on the thesis that\nmental simulation, rather than a body of knowledge about other minds,\nplays a central role in mindreading. However, different members of the\nfamily can differ from one another in significant respects. \nOne fundamental area of disagreement among Simulation Theorists is the\nvery nature of ST—what kind of theory ST is supposed to\nbe—and what philosophers can contribute to it. Some Simulation\nTheorists take the question “How do people arrive at\nrepresenting others’ mental states?” as a straightforward\nempirical question about the cognitive processes and mechanisms\nunderlying mindreading (Goldman 2006; Hurley 2008). According to them,\nST is thus a theory in cognitive science, to which\nphilosophers can contribute exactly as theoretical physicists\ncontribute to physics:  \ntheorists specialize in creating and tweaking theoretical structures\nthat comport with experimental data, whereas experimentalists have the\nprimary job of generating the data. (Goldman 2006: 22)  \nOther philosophical defenders of ST, however, do not conceive of\nthemselves as theoretical cognitive scientists at all. For example,\nHeal (1998) writes that:  \nit is commonly taken that the inquiry into … the extent of\nsimulation in psychological understanding is empirical, and that\nscientific investigation is the way to tell whether ST … is\ncorrect. But this perception is confused. It is an a priori\ntruth … that simulation must be given a substantial role in our\npersonal-level account of psychological understanding. (Heal 1998:\n477–478)  \nAdjudicating this meta-philosophical dispute goes well beyond the aim\nof this entry. To be as inclusive as we can, we shall adopt a\n“balanced diet” approach: we shall discuss the extent to\nwhich ST is supported by empirical findings from psychology and\nneuroscience, and, at the same time, we shall dwell on\n“purely philosophical” problems concerning ST. We leave to\nthe reader the task of evaluating which aspects should be put at the\ncentre of the inquiry. \nImportantly, even those who agree on the general nature of ST might\ndisagree on other crucial issues. We will focus on what are typically\ntaken to be the three most important bones of contention among\nSimulation Theorists: what is meant by “mental\nsimulation”?\n (section 2).\n What types of simulation processes are there?\n (section 3).\n What is the role of mental simulation in mindreading?\n (section 4).\n After having considered what keeps Simulation Theorists apart, we\nshall move to discuss what holds them together, i.e., the opposition\nto the Theory-Theory of mindreading\n (section 5\n and\n section 6).\n This should give the reader a sense of the “unity amidst\ndiversity” that characterizes ST. \nIn common parlance, we talk of putting ourselves in others’\nshoes, or empathizing with other people. This talk is typically\nunderstood as adopting someone else’s point of view, or\nperspective, in our imagination. For example, it is quite natural to\ninterpret the request “Try to show some empathy for John!”\nas asking you to use your imaginative capacity to consider the world\nfrom John’s perspective. But what is it for someone to\nimaginatively adopt someone else’s perspective? To a first\napproximation, according to Simulation Theorists, it consists of\nmentally simulating, or re-creating, someone\nelse’s mental states. Currie and Ravenscroft (2002) make this\npoint quite nicely:  \nImagination enables us to project ourselves into another situation and\nto see, or think about, the world from another perspective. These\nsituations and perspectives … might be those of another actual\nperson, [or] the perspective we would have on things if we believed\nsomething we actually don’t believe, [or] that of a fictional\ncharacter. … Imagination recreates the mental states of\nothers. (Currie & Ravenscroft 2002: 1, emphasis added).  \nThus, according to ST, empathizing with John’s sadness consists\nof mentally simulating his sadness, and adopting Mary’s\npolitical point of view consists of mentally simulating her political\nbeliefs. This is the intuitive and general sense of mental\nsimulation that Simulation Theorists have in mind. \nNeedless to say, this intuitive characterization of “mental\nsimulation” is loose. What exactly does it mean to say that a\nmental state is a mental simulation of another mental state? Clearly,\nwe need a precise answer to this question, if the notion of mental\nsimulation is to be the fundamental building block of a theory.\nSimulation Theorists, however, differ over how to answer this\nquestion. The central divide concerns whether “mental\nsimulation” should be defined in terms of resemblance\n(Heal 1986, 2003; Goldman 2006, 2008a) or in terms of reuse\n(Hurley 2004, 2008; Gallese & Sinigaglia 2011). We consider these\ntwo proposals in turn. \nThe simplest interpretation of “mental simulation” in\nterms of resemblance goes like this: \n(RES-1) Token state M* is a mental simulation of token state\nM if and only if: \nTwo clarifications are in order. First, we will elaborate on the\n“significant respects” in which a mental state has to\nresemble another mental state in due course (see, in particular,\n section 3).\n For the moment, it will suffice to mention some relevant dimensions\nof resemblance: similar functional role; similar content; similar\nphenomenology; similar neural basis (an important discussion of this\ntopic is Fisher 2006). Second, RES-1 defines “mental\nsimulation” as a dyadic relation between\nmental states (the relation being a mental simulation of).\nHowever, the expression “mental simulation” is also often\nused to pick out a monadic property of mental states\n—the property being a simulated mental state (as will\nbecome clear soon, “simulated mental state” does not refer\nhere to the state which is simulated, but to the state that does the\nsimulating). For example, it is common to find in the literature\nsentences like “M* is a mental simulation”. To\navoid ambiguities, we shall adopt the following terminological\nconventions: \nIt follows from this that, strictly speaking,\n RES-1\n is a definition of “mental simulation of”. Throughout\nthis entry, we shall characterize “simulated mental state”\nin terms of “mental simulation of”: we shall say that if\nM* is a mental simulation of M, then\nM* is a simulated mental\n state.[1] \nWith these clarifications in place, we will consider the strengths and\nweaknesses of\n RES-1.\n Suppose that Lisa is seeing a yellow banana. At the present moment,\nthere is no yellow banana in my own surroundings; thus, I cannot have\nthat (type of) visual experience. Still, I can visualize what\nLisa is seeing. Intuitively, my visual imagery of a yellow banana is a\nmental simulation of Lisa’s visual experience.\n RES-1\n captures this, given that both my visual imagery and Lisa’s\nvisual experience are mental states and the former resembles the\nlatter. \n\n RES-1,\n however, faces an obvious problem (Goldman 2006). The resemblance\nrelation is symmetric: for any x and y, if x\nresembles y, then y resembles x. Accordingly, it\nfollows from\n RES-1\n that Lisa’s visual experience is a mental simulation of my\nvisual imagery. But this is clearly wrong. There is no sense in which\none person’s perceptual experience can be a mental simulation of\nanother person’s mental imagery (see Ramsey 2010 for other\ndifficulties with\n RES-1). \nIn order to solve this problem, Goldman (2006) proposes the following\nresemblance-based definition of “mental simulation\nof”: \n(RES-2) Token state M* is a mental simulation of token state\nM if and only if: \nUnder the plausible assumption that one of the functions of visual\nimagery is to resemble visual experiences, RES-2 correctly predicts\nthat my visual imagery of a yellow banana counts as a mental\nsimulation of Lisa’s visual experience. At the same time, since\nvisual experiences do not have the function of resembling visual\nimages, RES-2 does not run into the trouble of categorizing the former\nas a mental simulation of the latter. \nClearly,\n RES-2\n is a better definition of “mental simulation of” than\n RES-1.\n Hurley (2008), however, argued that it won’t do either, since\nit fails to distinguish ST from its main competitor, i.e., the\nTheory-Theory (TT), according to which mindreading depends on a body\nof information about mental states and processes\n (section 5).\n The crux of Hurley’s argument is this. Suppose that a token\nvisual image V* resembles a token visual experience V\nand, in doing so, fulfils one of its functions. In this case,\n RES-2\n is satisfied. But now suppose further that visualization works like a\ncomputer simulation: it generates its outputs on the basis of a body\nof information about vision. On this assumption,\n RES-2\n still categorizes V* as a mental simulation of V,\neven though V* has been generated by exactly the kind of\nprocess described by TT: a theory-driven and information-rich\nprocess. \nAccording to Hurley (who follows here a suggestion by Currie &\nRavenscroft 2002), the solution to this difficulty lies in the\nrealization that “the fundamental … concept of simulation\nis reuse, not resemblance” (Hurley 2008: 758, emphasis\nadded). Hurley’s reuse-based definition of “mental\nsimulation of” can be articulated as follows: \n(REU) Token state M* is a mental simulation of token state\nM if and only if: \nTo have a full understanding of REU, we need to answer three\nquestions: (a) What is a cognitive process? (b) What is a\ncognitive mechanism? (c) What is the difference between\nusing and reusing a certain cognitive mechanism?\nLet’s do it! \nIt is a commonplace that explanation in cognitive science is\nstructured into different levels. Given our aims, we can illustrate\nthis idea through the classical tri-level hypothesis\nformulated by David Marr (1982). Suppose that one wants to explain a\ncertain cognitive capacity, say, vision (or mindreading, or\nmoral judgment). The first level of explanation, the most abstract\none, consists in describing what the cognitive capacity\ndoes—what task it performs, what problem it solves, what\nfunction it computes. For example, the task performed by vision is\nroughly “to derive properties of the world from images of\nit” (Marr 1982: 23). The second level of analysis specifies\nhow the task is accomplished: what algorithm our mind uses to\ncompute the function. Importantly, this level of analysis abstracts\nfrom the particular physical structures that implement the algorithm\nin our head. It is only at the third level of analysis that the\ndetails of the physical implementation of the algorithm in\nour brain are spelled out. \nWith these distinctions at hand, we can answer questions (a) and (b).\nA cognitive process is a cognitive capacity considered as an\ninformation-processing activity and taken in abstraction from its\nphysical implementation. Thus, cognitive processes are individuated in\nterms of what function they perform and/or in terms of what algorithms\ncompute these functions (fair enough, the “and/or” is a\nvery big deal, but it is something we can leave aside here). This\nmeans that the same (type of) cognitive process can be multiply\nrealized in different physical structures. For example, parsing\n(roughly, the cognitive process that assigns a grammatical structure\nto a string of signs) can be implemented both by a human brain and a\ncomputer. On the contrary, cognitive mechanisms are\nparticular (types of) physical structures—e.g., a certain part\nof the brain—implementing certain cognitive processes. More\nprecisely, cognitive mechanisms are organized structures carrying out\ncognitive processes in virtue of how their constituent parts interact\n(Bechtel 2008; Craver 2007; Machamer et al. 2000). \nWe now turn to question (c), which concerns the distinction between\nuse and reuse of a cognitive mechanism. At a first approximation, a\ncognitive mechanism is used when it performs its primary\nfunction, while it is reused when it is activated to perform\na different, non-primary function. For example, one is using\none’s visual mechanism when one employs it to see, while one is\nreusing it when one employs it to conjure up a visual image\n(see Anderson 2008, 2015 for further discussion of the notion of\nreuse). All this is a bit sketchy, but it will do. \nLet’s now go back to\n REU.\n The main idea behind it is that whether a mental state is a mental\nsimulation of another mental state depends on the cognitive\nprocesses generating these two mental states, and on the\ncognitive mechanisms implementing such cognitive processes.\nMore precisely, in order for mental state M* to be a mental\nsimulation of mental state M, it has to be case that:\n(i) cognitive processes P* and P, which\nrespectively generate M* and M, are both implemented by\nthe same (type of) cognitive mechanism C; (ii) P is\nimplemented by the use of C, while P* is\nimplemented by the reuse of C.  \nNow that we know what\n REU\n means, we can consider whether it fares better than\n RES-2\n in capturing the nature of the relation of mental simulation. It\nwould seem so. Consider this hypothetical scenario. Lisa is seeing a\nyellow banana, and her visual experience has been generated by\ncognitive process V1, which has been implemented by\nthe use of her visual mechanism. I am visualizing a yellow banana, and\nmy visual image has been generated by cognitive process\nV2, which has been implemented by the reuse of my\nvisual mechanism. Rosanna-the-Super-Reasoner is also visualizing a\nyellow banana, but her visual image has been generated by an\ninformation-rich cognitive process: a process drawing upon\nRosanna’s detailed knowledge of vision and implemented by her\nincredibly powerful reasoning mechanism.\n REU\n correctly predicts that my visual image is a mental simulation of\nLisa’s visual experience, but not vice versa. More importantly,\nit also predicts that Rosanna’s visual image does not count as a\nmental simulation of Lisa’s visual experience, given that\nRosanna’s cognitive process was not implemented by the reuse of\nthe visual mechanism. In this way,\n REU\n solves the problem faced by\n RES-2\n in distinguishing ST from TT. \nShould we then conclude that “mental simulation of” has to\nbe defined in terms of reuse, rather than in terms of resemblance?\nGoldman (2008a) is still not convinced. Suppose that while Lisa is\nseeing a yellow banana, I am using my visual mechanism to visualize\nthe Golden Gate Bridge. Now, even though Lisa’s visual\nexperience and my visual image have been respectively generated by the\nuse and the reuse of the visual mechanism, it would be bizarre to say\nthat my mental state is a mental simulation of Lisa’s. Why?\nBecause my mental state doesn’t resemble Lisa’s (she is\nseeing a yellow banana; I am visualizing the Golden Gate Bridge!)\nThus—Goldman concludes—resemblance should be taken as\nthe central feature of mental simulation. \nIn order to overcome the difficulties faced by trying to define\n“mental simulation of” in terms of either\nreplication or reuse, philosophers have built on the insights\nof both RES and\n REU\n and have proposed definitions that combine replication and reuse\nelements (Currie & Ravenscroft 2002; in recent years, Goldman\nhimself seems to have favoured a mixed account; see Goldman 2012a).\nHere is one plausible definition: \n(RES+REU) Token state M* is a mental simulation of token state\nM if and only if: \nRES+REU has at least three important virtues. The first is\nthat it solves all the aforementioned problems for RES and\n REU—we\n leave to the reader the exercise of showing that this is indeed the\ncase. \nThe second is that it fits nicely with an idea that loomed\nlarge in the simulationist literature: the idea that simulated\nmental states are “pretend” (“as if”,\n“quasi-”) states—imperfect copies of, surrogates\nfor, the “genuine” states normally produced by a certain\ncognitive mechanism, obtained by taking this cognitive mechanism\n“off-line”. Consider the following case. Frank is in front\nof Central Café (and believes that he is\nthere). He desires to drink a beer and believes that\nhe can buy one at Central Café. When he feeds these\nmental states into his decision-making mechanism, the mechanism\nimplements a decision-making process, which outputs the\ndecision to enter the café. In this case,\nFrank’s decision-making mechanism was\n“on-line”—i.e., he used it; he employed it\nfor its primary function. My situation is different. I don’t\nbelieve I am in front of Central Café, nor do I desire\nto drink a beer right now. Still, I can imagine believing and desiring\nso. When I feed these imagined states into my decision-making\nmechanism, I am not employing it for its primary function. Rather, I\nam taking it off-line (I am reusing it). As a result, the\ncognitive process implemented by my mechanism will output a merely\nimagined decision to enter the café. Now, it seems fair to\nsay that my imagined decision resembles Frank’s decision (more\non this in\n section 3).\n If you combine this with how these two mental states have been\ngenerated, the result is that my imagined decision is a mental\nsimulation of Frank’s decision, and thus it is a simulated\nmental state. It is also clear why Frank’s decision is\ngenuine, while my simulated mental state is just a\npretend decision: all else being equal, Frank’s\ndecision to enter Central Café will cause him to enter\nthe café; on the contrary, no such behaviour will result from\nmy simulated decision. I have not really decided so. Mine was\njust a quasi-decision—an imperfect copy of, a surrogate\nfor, Frank’s genuine decision. \nAnd here is\n RES+REU’s\n third virtue. So far, we have said that “mental\nsimulation” can either pick out a dyadic relation between mental\nstates or a monadic property of mental states. In fact, its ambiguity\nruns deeper than this, since philosophers and cognitive scientists\nalso use “mental simulation” to refer to a monadic\nproperty of cognitive processes, namely, the property\nbeing a (mental) simulation process (or: “process of\nmental simulation”, “simulational process”,\n“simulative process”, etc.) As a first stab, a (mental)\nsimulation process is a cognitive process generating simulated mental\nstates.\n RES+REU\n has the resources to capture this usage of “mental\nsimulation” too. Indeed,\n RES+REU\n implicitly contains the following definition of\n“simulation process”: \n(PROC): Token process P* is a (mental) simulation process if\nand only if: \nGo back to the case in which Lisa was having a visual experience of a\nyellow banana, while I was having a visual image of a yellow banana.\nOur two mental states resembled one another, but different cognitive\nprocesses generated them: seeing in Lisa’s case, and\nvisualizing in my case. Moreover, Lisa’s\nseeing was implemented by the use of the visual mechanism,\nwhile my visualizing was implemented by its reuse. According\nto PROC, the latter cognitive process, but not the former, was thus a\nsimulation process. \nTo sum up,\n RES+REU\n captures many of the crucial features that Simulation Theorists\nascribe to mental simulation. For this reason, we shall adopt it as\nour working definition of “mental simulation\nof”—consequently, we shall adopt PROC as a definition of\n“simulated mental\n state”.[2]\n We can put this into a diagram. \nFigure 1 \nThe hexagon at the bottom depicts a cognitive mechanism C (it\ncould be, say, the visual mechanism). When C is used (arrow on\nthe left), it implements cognitive process P (say, seeing);\nwhen it is re-used (arrow on the right), it implements cognitive\nprocess P* (say, visualizing). P generates mental state\nM (say, a visual experience of a red tomato), while P*\ngenerates mental state M* (say, a visual image of a red\ntomato). These two mental states (M and M*) resemble one\nanother. Given this: M* is a mental simulation of\nM; M* is a simulated mental state; and\nP* is a simulation\n process.[3] \nIn this section, we shall finally consider three worries raised for\nadopting\n RES+REU\n as a definition of “mental simulation of”. If you have\nalready had enough of\n RES+REU,\n please feel free to move straight to\n section 3. \nHeal (1994) pointed out a problem with committing ST to a particular\naccount of the cognitive mechanisms that underlie it. Suppose that the\nhuman mind contains two distinct decision-making mechanisms:\nMec1, which takes beliefs and desires as input, and generates\ndecisions as output; and Mec2, which works by following\nexactly the same logical principles as Mec1, but takes\nimagined beliefs and imagined desires as input and generates imagined\ndecisions as output. Consider again Frank’s decision to enter\nCentral Café and my imagined decision to do so.\nAccording to the two mechanisms hypothesis, Frank desired to drink a\nbeer and believed that he could buy one at Central\nCafé, fed these mental states into Mec1, which\ngenerated the decision to enter the café. As for me, I fed the\nimagined desire to drink a beer and the imagined belief that I could\nbuy one at Central Café into a distinct (type of)\nmechanism, i.e., Mec2, which generated the imagined decision\nto enter Central Café. Here is the question: does my\nimagined decision to enter Central Café count as a\nmental simulation of Frank’s decision to do so? If your answer\nis “Yes, it does”, then\n RES+REU\n is in trouble, since my imagined decision was not generated by\nreusing the same (type of) cognitive mechanism that Frank used to\ngenerate his decision; his decision was generated by Mec1, my\nimagined decision by Mec2. Thus, Heal concludes, a definition\n“mental simulation of” should not contain any commitment\nabout cognitive mechanisms—it should not make any implementation\nclaim—but should be given at a more abstract level of\ndescription. \nIn the face of this difficulty, a defender of\n RES+REU\n can say the following. First, she might reject the intuition that, in\nthe two mechanisms scenario, my imagined decision counts as a mental\nsimulation of Frank’s decision. At a minimum, she might say that\nthis scenario does not elicit any robust intuition in one direction or\nthe other: it is not clear whether these two mental states stand in\nthe relation being a mental simulation of. Second, she might\ndownplay the role of intuitions in the construction of a definition\nfor “mental simulation of” and cognate notions. In\nparticular, if she conceives of ST as an empirical theory in cognitive\nscience, she will be happy to discount the evidential value of\nintuitions if countervailing theoretical considerations are\navailable. This, e.g., is Currie and Ravenscroft’s (2002)\nposition, who write that  \nthere are two reasons … why the Simulation Theorist should\nprefer [a one mechanism hypothesis]: … first, the postulation\nof two mechanisms is less economical than the postulation of one;\nsecond, … we have very good reasons to think that\nimagination-based decision making does not operate in isolation from\nthe subject’s real beliefs and desires. … If imagination\nand belief operate under a system of inferential apartheid—as\nthe two-mechanisms view has it—how could this happen? (Currie\n& Ravenscroft 2002: 67–68) \nA second worry has to do with the fact that\n RES+REU\n appears to be too liberal. Take this case. Yesterday, Angelina had\nthe visual experience of a red apple. On the night of June 15, 1815,\nNapoleon conjured up the visual image of a red apple. Angelina used\nher visual mechanism to see, while Napoleon reused his to imagine. If\nwe add to this that Napoleon’s mental state resembled\nAngelina’s,\n RES+REU\n predicts that Napoleon’s (token) visual image was a mental\nsimulation of Angelina’s (token) visual experience. This might\nstrike one as utterly bizarre. In fact, not only did Napoleon not\nintend to simulate Angelina’s experience: he could\nnot even have intended to do it. After all, Angelina was born\nroughly 150 years after Napoleon’s death. By the same token, it\nis also impossible that Napoleon’s visual image has been\ncaused by Angelina’s visual experience. As a matter of\nfact, the visual image Napoleon had on the night of June 15, 1815 is\nentirely disconnected from the visual experience that\nAngelina had yesterday. Thus, how could the former be a mental\nsimulation of the latter? If you think about it, the problem is even\nworse than this.\n RES+REU\n has it that Napoleon’s visual image of a red apple is a mental\nsimulation of all the visual experiences of a red apple that\nhave obtained in the past, that are currently obtaining, and that will\nobtain in the future. Isn’t that absurd? \nAgain, a defender of\n RES+REU\n can give a two-fold answer. First, she can develop an argument that\nthis is not absurd at all. Intuitively, the following principle seems\nto be true: \n(TYPE): the mental state type visual image of a red apple is\na mental simulation of the mental state type visual experience of\na red apple. \nIf TYPE is correct, then the following principle has to be true as\nwell: \n(TOKEN): Any token mental state of the type visual image of a red\napple is a mental simulation of every token mental state of the\ntype visual experience of a red apple. \nBut TOKEN entails that Napoleon’s (token) visual image of a red\napple is a mental simulation of Angelina’s (token) visual\nexperience of a red apple, which is exactly what\n RES+REU\n predicts. Thus,\n RES+REU’s\n prediction, rather than being absurd, independently follows from\nquite intuitive assumptions. Moreover, even though TOKEN and\n RES+REU\n make the same prediction about the Napoleon-Angelina case, TOKEN is\nnot entailed by\n RES+REU,\n since the latter contains a restriction on how visual images\nhave to be generated. Thus, if one finds TOKEN intuitively acceptable,\nit is hard to see how one can find\n RES+REU\n to be too liberal. \nThe second component of the answer echoes one of the answers given to\nHeal: for a Simulation Theorist who conceives of ST as a theory in\ncognitive science, intuitions have a limited value in assessing a\ndefinition of “mental simulation of”. In fact, the main\naim of this definition is not that of capturing folk intuitions, but\nrather that of offering a clear enough picture of the relation of\nmental simulation on the basis of which an adequate theory of\nmindreading can be built. So, if the proposed definition fails, say,\nto help distinguishing ST from TT, or is of limited use in\ntheory-building, or is contradicted by certain important results from\ncognitive science, then one has a good reason to abandon it. On the\ncontrary, it should not be a cause for concern if\n RES+REU\n does not match the folk concept MENTAL SIMULATION OF. The notion\n“mental simulation of” is a term of art—like, say,\nthe notions of I-Language or of Curved Space. These notions\ndo poorly match the folk concepts of language and space, but\nlinguists and physicists do not take this to be a problem. The same\napplies to the notion of mental simulation. \nAnd here is the third and final worry.\n RES+REU\n is supposed to be a definition of “mental simulation of”\non the basis of which a theory of mindreading can be built. However,\nneither\n RES+REU\n nor\n PROC\n make any reference to the idea of representing others’ mental\nstates. Thus, how could these definitions help us to construct a\nSimulation Theory of mindreading? The answer is simple: they\nwill help us exactly as a clear definition of\n“computation”, which has nothing to do with how the mind\nworks, helped to develop the Computational Theory of Mind (see entry\non\n computational theory of mind). \nHere is another way to make the point. ST is made up of two\ndistinct claims: the first is that mental simulation is\npsychologically real, i.e., that there are mental states and processes\nsatisfying\n RES+REU\n and\n PROC.\n The second claim is that mental simulation plays a central role in\nmindreading. Clearly, the second claim cannot be true if the first is\nfalse. However, the second claim can be false even if the first claim\nis true: mental simulation could be psychologically real, but play no\nrole in mindreading at all. Hence, Simulation Theorists have to do\nthree things. First, they have to establish that mental simulation is\npsychologically real. We consider this issue in\n section 3.\n Second, they have to articulate ST as a theory of\nmindreading. That is, they have to spell out in some detail the\ncrucial role that mental simulation is supposed to play in\nrepresenting others’ mental states, and contrast the resulting\ntheory with other accounts of mindreading. We dwell on this in\nsections\n 4\n and\n 5.\n Finally, Simulation Theorists have to provide evidence in support of\ntheir theory of mindreading—that is, they have to give us good\nreasons to believe that mental simulation does play a crucial role in\nrepresenting others’ mental states. We discuss this issue in\n section 6. \nNow that we have definitions of “mental simulation of” and\ncognate notions, it is time to consider which mental states and\nprocesses satisfy them, if any. Are there really simulated mental\nstates? That is, are there mental states generated by the\nreuse of cognitive mechanisms? And do these mental states\nresemble the mental states generated by the use of such\nmechanisms? For example, is it truly the case that visual images are\nmental simulations of visual experiences? What about\ndecisions, emotions, beliefs, desires, and bodily sensations? Can our\nminds generate simulated counterparts of all these types of mental\nstates? In this section, we consider how Simulation Theorists have\ntackled these problems. We will do so by focusing on the following\nquestion: are there really simulation processes (as defined\nby\n PROC)?\n If the answer to this question is positive, it follows that there are\nmental states standing in the relation of mental simulation (as\ndefined by\n RES+REU),\n and thus simulated mental states. \nFollowing Goldman (2006), it has become customary among Simulation\nTheorists to argue for the existence of two types of simulation\nprocesses: high-level simulation processes and\nlow-level simulation processes (see, however, de Vignemont\n2009). By exploring this distinction, we begin to articulate the\ncognitive architecture underlying mental simulation proposed by\nST. \nHigh-level simulation processes are cognitive processes with the\nfollowing features: (a) they are typically conscious, under voluntary\ncontrol, and stimulus-independent; (b) they satisfy\n PROC,\n that is, they are implemented by the reuse of a certain\ncognitive mechanism, C, and their output states\nresemble the output states generated by the use of\n C.[4]\n Here are some cognitive processes that, according to Simulation\nTheorists, qualify as high-level simulation processes. Visualizing:\nthe cognitive process generating visual images (Currie 1995; Currie\n& Ravenscroft 2002; Goldman 2006); motor imagination: the\ncognitive process generating imagined bodily movements and actions\n(Currie & Ravenscroft 1997, 2002; Goldman 2006); imagining\ndeciding: the cognitive process generating decision-like imaginings\n(Currie & Ravenscroft 2002); imagining believing: the cognitive\nprocess generating belief-like imaginings (Currie & Ravenscroft\n2002); imagining desiring: the cognitive process generating\ndesire-like imaginings (Currie 2002). In what follows, we shall\nconsider a couple of them in some detail. \nVisualizing first. It is not particularly hard to see why visualizing\nsatisfies condition (a). Typically: one can decide to visualize (or\nstop visualizing) something; the process is not driven by perceptual\nstimuli; and at least some parts of the visualization process are\nconscious. There might be cases in which visualizing is not under\nvoluntary control, is stimulus-driven and, maybe, even entirely\nunconscious. This, however, is not a problem, since we know that there\nare clear cases satisfying (a). \nUnsurprisingly, the difficult task for Simulation Theorists is to\nestablish that visualizing has feature (b), that is: it is implemented\nby the reuse of the visual mechanism; and its outputs (that\nis, visual images) resemble genuine visual experiences.\nSimulation Theorists maintain that they have strong empirical evidence\nsupporting the claim that visualizing satisfies\n PROC.\n Here is a sample (this and further evidence is extensively discussed\nin Currie 1995, Currie & Ravenscroft 2002, and in Goldman\n2006): \nOn this basis, Simulation Theorists conclude that visualizing is\nindeed implemented by the reuse of the visual mechanism\n(evidence i and ii) and that its outputs, i.e., visual images, do\nresemble visual experiences (evidence iii, iv, and v). Thus,\nvisualizing is a process that qualifies as high-level simulation, and\nvisual images are simulated mental states. \nVisual images are mental simulations of perceptual states. Are there\nhigh-level simulation processes whose outputs instead are mental\nsimulations of propositional attitudes? (If you think that visual\nexperiences are propositional attitudes, you can rephrase the question\nas follows: are there high-level simulation processes whose outputs\nare mental simulations of non-sensory states?) Three candidate\nprocesses have received a fair amount of attention in the\nsimulationist literature: imagining desiring, imagining deciding, and\nimagining believing. The claims made by Simulation Theorists about\nthese cognitive processes and their output states have generated an\nintense debate (Doggett & Egan 2007; Funkhouser & Spaulding\n2009; Kieran & Lopes 2003; Nichols 2006a, 2006b; Nichols &\nStich 2003; Velleman 2000). We do not have space to review it here\n(two good entry points are the introduction to Nichols 2006a and the\nentry on\n imagination).\n Rather, we shall confine ourselves to briefly illustrating the\nsimulationist case in favour of the thesis that imagining believing is\na high-level simulation process. \nI don’t believe that Rome is in France, but I can imagine\nbelieving it. Imagining believing typically is a conscious,\nstimulus-independent process, under voluntary control. Thus, imagining\nbelieving satisfies condition (a). In order for it to count as an\ninstance of high-level simulation process, it also needs to\nhave feature (b), that is: (b.i) its outputs (i.e., belief-like\nimaginings) have to resemble genuine beliefs in some\nsignificant respects; (b.ii) it has to be implemented by the\nreuse of the cognitive mechanism (whose use implements the\ncognitive process) that generates genuine beliefs—let us call it\n“the belief-forming mechanism”. Does imagining believing\nsatisfy (b)? Currie and Ravenscroft (2002) argue in favour of (b.i).\nBeliefs are individuated in terms of their content and functional\nrole. Belief-like imaginings—Currie and Ravenscroft\nsay—have the same content and a similar functional role to their\ngenuine counterparts. For example, the belief that Rome is in France\nand the belief-like imagining that Rome is in France have exactly the\nsame propositional content: that Rome is in France. Moreover,\nbelief-like imaginings mirror the inferential role of genuine beliefs.\nIf one believes both that Rome is in France and that French is the\nlanguage spoken in France, one can infer the belief that French is the\nlanguage spoken in Rome. Analogously, from the belief-like imagining\nthat Rome is in France and the genuine belief that French is the\nlanguage spoken in France, one can infer the belief-like imagining\nthat French is the language spoken in Rome. So far, so good (but see\nNichols 2006b). \nWhat about (b.ii)? Direct evidence bearing on it is scarce. However,\nSimulation Theorists can give an argument along the following lines.\nFirst, one owes an explanation of why belief-like imaginings are,\nwell, belief-like—as we have said above, it seems that they have\nthe same type of content as, and a functional role similar to, genuine\nbeliefs. A possible explanation for this is that both types of mental\nstates are generated by (cognitive processes implemented by) the same\ncognitive mechanism. Second, it goes without saying that our mind\ncontains a mechanism for generating beliefs (the belief-forming\nmechanism), and that there must be some mechanism or another in charge\nof generating belief-like imaginings. It is also well known that\ncognitive mechanisms are evolutionary costly to build and maintain.\nThus, evolution might have adopted the parsimonious strategy of\nredeploying a pre-existing mechanism (the belief-forming mechanism)\nfor a non-primary function, i.e., generating belief-like\nimaginings—in general, this hypothesis is also supported by the\nidea that neural reuse is one of the fundamental organizational\nprinciple of the brain (Anderson 2008). If one puts these two strands\nof reasoning together, one gets a prima facie case for the\nclaim that imagining believing is implemented by the reuse of the\nbelief-forming mechanism—that is, a prima facie case\nfor the conclusion that imagining believing satisfies (b.ii). Since\nimagining believing appears also to satisfy (b.i) and (a), lacking\nevidence to the contrary, Simulation Theorists are justified in\nconsidering it to be a high-level simulation process. \nLet’s take stock. We have examined a few suggested instances of\nhigh-level simulation processes. If Simulation Theorists are correct,\nthey exhibit the following commonalities: they satisfy\n PROC\n (this is why they are simulation processes); they are\ntypically conscious, under voluntary control, and stimulus-independent\n(this is why they are high-level). Do they have some other\nimportant features in common? Yes, they do—Simulation Theorists\nsay. They all are under the control of a single cognitive\nmechanism: imagination (more precisely, Currie & Ravenscroft\n(2002) talk of Re-Creative Imagination, while Goldman (2006, 2009)\nuses the expression “Enactment Imagination”). The\nfollowing passage will give you the basic gist of the proposal: \nWhat is distinctive to high-level simulation is the psychological\nmechanism … that produces it, the mechanism of imagination.\nThis psychological system is capable of producing a wide variety of\nsimulational events: simulated seeings (i.e., visual imagery),\n… simulated motor actions (motor imagery), simulated beliefs,\n… and so forth. … In producing simulational outputs,\nimagination does not operate all by itself. … For example, it\nrecruits parts of the visual system to produce visual imagery\n…. Nonetheless, imagination “‘takes the\nlead”’ in directing or controlling the other systems it\nenlists for its project. (Goldman 2009: 484–85) \nHere is another way to make the point. We already know that, according\nto ST, visualizing is implemented by the reuse of the visual\nmechanism. In the above passage, Goldman adds that the reuse of the\nvisual mechanism is initiated, guided and controlled by imagination.\nThe same applies, mutatis mutandis, to all cases of\nhigh-level simulation processes. For example, in imagining hearing,\nimagination “gets in control” of the auditory mechanism,\ntakes it off-line, and (re)uses it to generate simulated auditory\nexperiences. Goldman (2012b, Goldman & Jordan 2013) supports this\nclaim by making reference to neuroscientific data indicating that the\nsame core brain network, the so-called “default network”,\nsubserves all the following self-projections: prospection (projecting\noneself into one’s future); episodic memory (projecting oneself\ninto one’s past); perspective taking (projecting oneself into\nother minds); and navigation (projecting oneself into other places)\n(see Buckner & Carroll 2007 for a review). These different\nself-projections presumably involve different high-level simulation\nprocesses. However, they all have something in common: they all\ninvolve imagination-based perspectival shifts. Therefore, the fact\nthat there is one brain network common to all these self-projections\nlends some support to the claim that there is one common cognitive\nmechanism, i.e., imagination, which initiates, guides, and controls\nall high-level simulation processes.  \nIf Goldman is right, and all high-level simulation processes are\nguided by imagination, we can then explain why, in our common\nparlance, we tend to describe high-level simulation processes and\noutputs in terms of imaginings, images, imagery, etc. More\nimportantly, we can also explain why high-level simulation processes\nare conscious, under voluntary control, and stimulus-independent.\nThese are, after all, typical properties of imaginative processes.\nHowever, there are simulation processes that typically are neither\nconscious, nor under voluntary control, nor stimulus independent. This\nindicates that they are not imagination-based. It is to this other\ntype of simulation processes that we now turn. \nLow-level simulation processes are cognitive processes with these\nfeatures: (a*) they are typically unconscious, automatic, and\nstimulus-driven; (b) they satisfy\n PROC,\n that is, they are implemented by the reuse of a certain\ncognitive mechanism, C, and their output states\nresemble the output states generated by the use of C.\nWhat cognitive processes are, according to ST, instances of low-level\nsimulation? The answer can be given in two words: mirroring processes.\nClarifying what these two words mean, however, will take some\ntime. \nThe story begins at the end of the 1980s in Parma, Italy, where the\nneuroscientist Giacomo Rizzolatti and his team were investigating the\nproperties of the neurons in the macaque monkey ventral premotor\ncortex. Through single-cell recording experiments, they discovered\nthat the activity of neurons in the area F5 is correlated with\ngoal-directed motor actions and not with particular movements\n(Rizzolatti et al. 1988). For example, some F5 neurons fire when the\nmonkey grasps an object, regardless of whether the monkey uses the\nleft or the right hand. A plausible interpretation of these results is\nthat neurons in monkey area F5 encode motor intentions (i.e.,\nthose intentions causing and guiding actions like reaching, grasping,\nholding, etc.) and not mere kinematic instructions (i.e.,\nthose representations specifying the fine-grained motor details of an\naction). (In-depth philosophical analyses of the notion of motor\nintention can be found in: Brozzo forthcoming; Butterfill &\nSinigaglia 2014; Pacherie 2000). This was an already interesting\nresult, but it was not what the Parma group became famous for. Rather,\ntheir striking discovery happened a few years later, helped by\nserendipity. Researchers were recording the activity of F5 neurons in\na macaque monkey performing an object-retrieval task. In between\ntrials, the monkey stood still and watched an experimenter setting up\nthe new trial, with microelectrodes still measuring the monkey’s\nbrain activity. Surprisingly, some of the F5 neurons turned out to\nfire when the monkey saw the experimenter grasping and\nplacing objects. This almost immediately led to new experiments, which\nrevealed that a portion of F5 neurons not only fire when the monkey\nperforms a certain goal-directed motor action (say, bringing a piece\nof food to the mouth), but also when it sees another agent performing\nthe same (type of) action (di Pellegrino et al. 1992; Gallese et al.\n1996; Rizzolatti et al. 1996). For this reason, these neurons were\naptly called “mirror neurons”, and it was\nproposed that they encode motor intentions both during action\nexecution and action observation (Rizzolatti & Sinigaglia 2007,\nforthcoming). Later studies found mirror neurons also in the macaque\nmonkey inferior parietal lobule (Gallese et al. 2002), which together\nwith the ventral premotor cortex constitutes the monkey cortical\nmirror neuron circuit (Rizzolatti & Craighero 2004). \nSubsequent evidence suggested that an action mirror\nmechanism—that is, a cognitive mechanism that gets\nactivated both when an individual performs a certain goal-directed\nmotor action and when she sees another agent performing the same\naction—also exists in the human brain (for reviews, see\nRizzolatti & Craighero 2004, and Rizzolatti & Sinigaglia\nforthcoming). In fact, it appears that there are mirror\nmechanisms in the human brain outside the action domain as well:\na mirror mechanism for disgust (Wicker et al. 2003), one for pain\n(Singer at al. 2004; Avenanti et al. 2005), and one for touch\n(Blakemore et al. 2005). Given the variety of mirror mechanisms, it is\nnot easy to give a definition that fits them all. Goldman (2008b) has\nquite a good one though, and we will draw from it: a cognitive\nmechanism is a mirror mechanism if and only if it gets activated both\nwhen an individual undergoes a certain mental event\nendogenously and when she perceives a sign that\nanother individual is undergoing the same (type of) mental event. For\nexample, the pain mirror mechanism gets activated both when\nindividuals experience “a painful stimulus and … when\nthey observe a signal indicating that [someone else] is receiving a\nsimilar pain stimulus” (Singer et al. 2004: 1157). \nHaving introduced the notions of mirror neuron and mirror mechanism,\nwe can define the crucial notion of this section: mirroring\nprocess. We have seen that mirror mechanisms can get activated in\ntwo distinct modes: (i) endogenously; (ii) in the perception mode. For\nexample, my action mirror mechanism gets endogenously activated when I\ngrasp a mug, while it gets activated in the perception mode when I see\nyou grasping a mug. Following again Goldman (2008b), let us say that a\ncognitive process is a mirroring process if and only if it is\nconstituted by the activation of a mirror mechanism in the\nperception mode. For example, what goes on in my brain when I see\nyou grasping a mug counts as a mirroring process. \nNow that we know what mirroring processes are, we can return to our\ninitial problem—i.e., whether they are low-level simulation\nprocesses (remember that a cognitive process is a low-level simulation\nprocess if and only if: (a*) it is typically unconscious, automatic,\nand stimulus-driven; (b) it satisfies\n PROC).\n For reasons of space, we will focus on disgust mirroring only. \nWicker et al. (2003) carried out an fMRI study in which participants\nfirst observed videos of disgusted facial expressions and subsequently\nunderwent a disgust experience via inhaling foul odorants. It turned\nout that the same neural area—the left anterior\ninsula—that was preferentially activated during the experience\nof disgust was also preferentially activated during the observation of\nthe disgusted facial expressions. These results indicate the existence\nof a disgust mirror mechanism. Is disgust mirroring\n(the activation of the disgust mirror mechanism in the perception\nmode) a low-level simulation process? Simulation Theorists answer in\nthe affirmative. \nHere is why disgust mirroring satisfies (a*): the process is\nstimulus-driven: it is sensitive to certain perceptual\nstimuli (disgusted facial expressions); it is automatic; and\nit is typically unconscious (even though its output, i.e.,\n“mirrored disgust”, is sometimes conscious). What about\ncondition (b)? Presumably, the primary (evolutionary) function of the\ndisgust mechanism is to produce a disgust response to spoiled food,\ngerms, parasites etc. (Rozin et al. 2008). In the course of evolution,\nthis mechanism could have been subsequently co-opted to also get\nactivated by the perception (of a sign) that someone else is\nexperiencing disgust, in order to facilitate social learning of food\npreferences (Gariépy et al. 2014). If this is correct, then\ndisgust mirroring is implemented by the reuse of the disgust\nmechanism (by employing this mechanism for a function different than\nits primary one). Moreover, the output of disgust mirroring\nresembles the genuine experience of disgust in at least two\nsignificant respects: first, both mental states have the same neural\nbasis; second, when conscious, they share a similar phenomenology.\nAccordingly, (b) is satisfied. By putting all this together,\nSimulation Theorists conclude that disgust mirroring is a low-level\nsimulation process, and mirrored disgust is a simulated mental state\n(Goldman 2008b; Barlassina 2013) \nIn the previous section, we examined the case for the psychological\nreality of mental simulation. We now turn to ST as a theory of\nmindreading. We will tackle two main issues: the extent to which\nmindreading is simulation-based, and how simulation-based mindreading\nworks. \nST proposes that mental simulation plays a central role in\nmindreading, i.e., it plays a central role in the capacity to\nrepresent and reason about others’ mental state. What does\n“central” mean here? Does it mean the central\nrole, with other contributors to mindreading being merely peripheral?\nThis is an important question, since in recent years there have been\nproposed hybrid models according to which both mental simulation and\ntheorizing play important roles in mindreading (see\n section 5.2).\n  \nA possible interpretation of the claim that mental simulation plays a\ncentral role in representing others’ mental states is that\nmindreading events are always simulation-based, even if they\nsometimes also involve theory. Some Simulation Theorists, however,\nreject this interpretation, since they maintain that there are\nmindreading events in which mental simulation plays no role at all\n(Currie & Ravenscroft 2002). For example, if I know that Little\nJimmy is happy every time he finds a dollar, and I also know that he\nhas just found a dollar, I do not need to undergo any simulation\nprocess to conclude that Little Jimmy is happy right now. I just need\nto carry out a simple logical inference. \nHowever, generalizations like, “Little Jimmy is happy every time\nhe finds a dollar,” are ceteris paribus rules. People\nreadily recognize exceptions: for example, we recognize situations in\nwhich Jimmy would probably not be happy even if he found a dollar,\nincluding some in which finding a dollar might actually make him\nunhappy. Rather than applying some additional or more complex rules\nthat cover such situations, it is arguable that putting ourselves in\nJimmy's situation and using “good common sense” alerts us\nto to these exceptions and overrides the rule. If that is correct,\nthen simulation is acting as an overseer or governor even when people\nappear to be simply applying rules.  \nGoldman (2006) suggests that we cash out the central role of mental\nsimulation in representing others’ mental states as follows:\nmindreading is often simulation-based. Goldman’s\nsuggestion, however, turns out to be empty, since he explicitly\nrefuses to specify what “often” means in this context.\n \nHow often is often? Every Tuesday, Thursday, and Saturday? Precisely\nwhat claim does ST mean to make? It is unreasonable to demand a\nprecise answer at this time. (Goldman 2006: 42; see also Goldman 2002;\nJeannerod & Pacherie 2004) \nPerhaps a better way to go is to characterize the centrality of mental\nsimulation for mindreading not in terms of frequency of use,\nbut in terms of importance. Currie and Ravenscroft make the\nvery plausible suggestion that “one way to see how important a\nfaculty is for performing a certain task is to examine what happens\nwhen the faculty is lacking or damaged” (Currie &\nRavenscroft 2002: 51). On this basis, one could say that mental\nsimulation plays a central role in mindreading if and only\nif: if one’s simulational capacity (i.e., the capacity to\nundergo simulation processes/simulated mental states) were impaired,\nthen one’s mindreading capacity would be significantly\nimpaired.  \nAn elaboration of this line of thought comes from Gordon (2005)—\nsee also Gordon (1986, 1996) and Peacocke (2005)—who argues that\nsomeone lacking the capacity for mental simulation would not be able\nto represent mental states as such, since she is incapable of\nrepresenting anyone as having a mind in the first place.\nGordon’s argument is essentially as follows: \nWe represent something as having a mind, as having mental states and\nprocesses, only if we represent it as a subject (“subject of\nexperience,” in formulations of “the hard problem of\nconsciousness”), where “a subject” is understood as\na generic “I”. This distinguishes it from a “mere\nobject” (and also is a necessary condition for a more benevolent\nsort of empathy). \nTo represent something as another “I” is to represent it\nas a possible target of self-projection: as something one might (with\nvarying degrees of success) imaginatively put oneself in the place of.\n(Of course, one can fancifully put oneself in the place of just about\nanything—a suspension bridge, even; but that is not a\nreductio ad absurdum, because one can also fancifully\nrepresent just about anything as having a mind.)  \nIt is not clear, however, what consequences Gordon’s conceptual\nargument would have for mindreading, if any. Even if a capacity to\nself-project were needed for representing mental states as such, would\nlack of this capacity necessarily impair mindreading? That is,\ncouldn't one predict explain, predict, and coordinate behavior using a\ntheory of internal states, without conceptualizing these as states of\nan I or subject? As a more general point, Simulation Theorists have\nnever provided a principled account of what would constitute a\n“significant impairment” of mindreading capacity. \nTo cut a long story short, ST claims that mental simulation plays a\ncentral role in mindreading, but at the present stage its proponents\ndo not agree on what this centrality exactly amounts to. We will come\nback to this issue in\n section 5,\n when we shall discuss the respective contributions of mental\nsimulation and theorizing in mindreading. \nWe now turn to a different problem: how does mental simulation\ncontribute to mindreading when it does? That is, how does\nsimulation-based mindreading work? Here again, Simulation Theorists\ndisagree about what the right answer is. In what follows, we explore\nsome dimensions of disagreement. \nSome Simulation Theorists defend a strong view of\nsimulation-based mindreading (Gordon 1986, 1995, 1996; Gallese et al.\n2004; Gallese & Sinigaglia 2011). They maintain that many\nsimulation-based mindreading events are (entirely)\nconstituted by mental simulation events (where mental\nsimulation events are simulated mental states or simulation\nprocesses). In other words, some Simulation Theorists claim that,\non many occasions, the fact that a subject S is\nrepresenting someone else’s mental states is nothing over and\nabove the fact that S is undergoing a mental simulation event:\nthe former fact reduces to the latter. For example, Lisa’s\nundergoing a mirrored disgust experience as a result of observing\nJohn’s disgusted face would count as a mindreading event:\nLisa’s simulated mental state would represent John’s\ndisgust (Gallese et al. 2004). Let us call this “the\nConstitution View”. \nWe shall elaborate on the details of the Constitution View in\n section 4.3.\n Before doing that, we consider an argument that has been directed\nagainst it over and over again, and which is supposed to show that the\nConstitution View is a non-starter (Fuller 1995; Heal 1995; Goldman\n2008b; Jacob 2008, 2012). Lacking a better name, we will call it\n“the Anti-Constitution argument”. Here it is. By\ndefinition, a mindreading event is a mental event in which a subject,\nS, represents another subject, Q, as having a certain\nmental state M. Now—the argument continues—the\nonly way in which S can represent Q as having\nM is this: S has to employ the concept of that\nmental state and form the judgment, or the belief,\nthat Q is in M. Therefore, a mindreading event is\nidentical to an event of judging that someone else has a certain\nmental state (where this entails the application of mentalistic\nconcepts). It follows from this that mental simulation events cannot\nbe constitutive of mindreading events, since the former events are not\nevents of judging that someone else has a certain mental state. An\nexample should clarify the matter. Consider Lisa again, who is\nundergoing a mirrored disgust experience as a result of\nobserving John’s disgusted face. Clearly, undergoing such a\nsimulated disgust experience is a different mental event from\njudging that John is experiencing disgust. Therefore,\nLisa’s mental simulation does not constitute a mindreading\nevent. \nIn\n section 4.3,\n we will discuss how the defenders of the Constitution View have\nresponded to this argument. Suppose for the moment that the\nAnti-Constitution argument is sound. What alternative pictures of\nsimulation-based mindreading are available? Those Simulation Theorists\nwho reject the Constitution View tend to endorse the Causation\nView, according to which mental simulation events never\nconstitute mindreading events, but only causally contribute\nto them. The best developed version of this view is Goldman’s\n(2006) Three-Stage Model (again, this is our label, not his),\nwhose basic structure is as follows: \nSTAGE 1. Mental simulation: Subject S undergoes a\nsimulation process, which outputs a token simulated mental state\nm*. \nSTAGE 2. Introspection: S introspects m* and\ncategorizes/conceptualizes it as (a state of type)\nM. \nSTAGE 3. Judgment: S attributes (a state of type)\nM to another subject, Q, through the judgment\nQ is in M. \n(The causal relations among these stages are such that: STAGE 1 causes\nSTAGE 2, and STAGE 2 in turn causes STAGE 3. See Spaulding 2012 for a\ndiscussion of the notion of causation in this context.) \nHere is our trite example. On the basis of observing John’s\ndisgusted facial expression, Lisa comes to judge that John is\nhaving a disgust experience. How did she arrive at the formation of\nthis judgment? Goldman’s answer is as follows. The observation\nof John’s disgusted facial expression triggered a disgust\nmirroring process in Lisa, resulting in Lisa’s undergoing a\nmirrored disgust experience (STAGE 1). This caused Lisa to introspect\nher simulated disgust experience and to categorize it as a disgust\nexperience (STAGE 2) (the technical notion of introspection used by\nGoldman will be discussed in\n section 4.4).\n This, in turn, brought about the formation of the judgment John\nis having a disgust experience (STAGE 3). Given that, according\nto Goldman, mindreading events are identical to events of judging that\nsomeone else has a certain mental state, it is only this last stage of\nLisa’s cognitive process that constitutes a mindreading event.\nOn the other hand, the previous two stages were merely causal\ncontributors to it. But mental simulation entirely took place at STAGE\n1. This is why the Three-Stage Model is a version of the Causation\nView: according to the model, mental simulation events causally\ncontribute to, but do not constitute, mindreading events. \nThe main strategy adopted by the advocates of the Constitution View in\nresponding to the Anti-Constitution argument consists in impugning the\nidentification of mindreading events with events of judging\nthat someone else has a certain mental state. A prominent version of\nthis position is Gordon’s (1995, 1996) Radical\nSimulationism, according to which representing someone\nelse’s mental states does not require the formation of\njudgments involving the application of mentalistic\nconcepts. Rather, Gordon proposes that the main bulk of\nmindreading events are non-conceptual representations of\nothers’ mental states, where these non-conceptual\nrepresentations are constituted by mental simulation events. If this\nis true, many mindreading events are constituted by mental\nsimulation events, and thus the Constitution View is correct. \nThe following case should help to get Radical Simulationism across.\nSuppose that I want to represent the mental state that an\nindividual—call him “Mr Tees”—is in right now.\nAccording to Gordon, there is a false assumption behind the idea that,\nin order to do so, I need to form a judgment with the content Mr\nTees is in M (where “M” is a\nplaceholder for a mentalistic concept). The false assumption is that\nthe only thing that I can do is to simulate myself in Mr\nTees’s situation. As Gordon points out, it is also possible for\nme to simulate Mr Tees in his situation. And if I do so, my\nvery simulation of Mr Tees constitutes a representation of\nhis mental state, without the need of forming any judgment. This is\nhow Gordon makes his point: \nTo simulate Mr Tees in his situation requires an egocentric shift, a\nrecentering of my egocentric map on Mr Tees. He becomes in my\nimagination the referent of the first person pronoun “I”.\n… Such recentering is the prelude to transforming myself in\nimagination into Mr Tees as much as actors become the characters they\nplay. … But once a personal transformation has been\naccomplished, … I am already representing him as being\nin a certain state of mind. (Gordon 1995: 55–56) \nIt is important to stress the dramatic difference between\nGordon’s Radical Simulationism and Goldman’s Three-Stage\nModel. According to the latter, mental simulation events causally\ncontribute to representing other people’s mental states,\nbut the mindreading event proper is always constituted by a\njudgment (or a belief). Moreover, Goldman maintains that the ability\nto form such judgments requires both the capacity to\nintrospect one’s own mental states (more in this in\n section 4.4)\n and possession of mentalistic concepts. None of this is true\nof Radical Simulationism. Rather, Gordon proposes that, in the large\nmajority of cases, it is the very mental simulation event itself that\nconstitutes a representation of someone else’s mental\nstates. Furthermore, since such mental simulation events neither\nrequire the capacity for introspection nor possession of mentalistic\nconcepts, Radical Simulationism entails the surprising conclusion that\nthese two features play at best a very minor role in mindreading. A\ntestable corollary is that social interaction often relies on an\nunderstanding of others that does not require the explicit application\nof mental state concepts. \nFrom what we have said so far, one could expect that Gordon should\nagree with Goldman on at least one point. Clearly, Gordon has to admit\nthat there are some cases of mindreading in which a subject\nattributes a mental state to someone else through a judgment involving\nthe application of mentalistic concepts. Surely, Gordon cannot deny\nthat there are occasions in which we think things like Mary\nbelieves that John is late or Pat desires to visit\nLisbon. Being a Simulation Theorist, Gordon will also presumably\nbe eager to maintain that many such mindreading events are based on\nmental simulation events. But if Gordon admits that much, should he\nnot also concede that Goldman’s Three-Stage Model is the right\naccount of at least those simulation-based mindreading\nevents? Surprising as it may be, Gordon still disagrees. \nGordon (1995) accepts that there are occasions in which a subject\narrives at a judgment about someone else’s mental state\non the basis of some mental simulation event. He might also concede to\nGoldman that such a judgment involves mentalistic concepts (but see\nGordon’s 1995 distinction between comprehending and\nuncomprehending ascriptions). Contra Goldman,\nhowever, Gordon argues that introspection plays no role at\nall in the generation of these judgments. Focusing on a specific\nexample will help us to clarify this further disagreement between\nGoldman and Gordon. \nSuppose that I know that Tom believes that (1) and (2): \nOn this basis, I attribute to Tom the further belief that (3): \nGoldman’s Three-Stage Model explains this mindreading act in the\nfollowing way. FIRST STAGE: I imagine believing what Tom believes\n(i.e., I imagine believing that (1) and (2)); I then feed those\nbelief-like imaginings into my reasoning mechanism (in the off-line\nmode); as a result, my reasoning mechanism outputs the imagined belief\nthat (3). The SECOND STAGE of the process consists in introspecting\nthis simulated belief and categorizing it as a belief.\nCrucially, in Goldman’s model, “introspection” does\nnot merely refer to the capacity to self-ascribe mental states.\nRather, it picks out a distinctive cognitive method for\nself-ascription, a method which is typically described as\nnon-inferential and quasi-perceptual (see the\nsection Inner sense accounts in the entry on\n self-knowledge).\n In particular, Goldman (2006) characterizes introspection as a\ntransduction process that takes the neural properties of a mental\nstate token as input and outputs a categorization of the type of\nstate. In the case that we are considering, my introspective mechanism\ntakes the neural properties of my token simulated belief as input and\ncategorizes it as a belief as output. After all this, the\nTHIRD STAGE occurs: I project the categorized belief onto Tom, through\nthe judgment Tom believes that Fido enjoys watching\nTV. (You might wonder where the content of Tom’s\nbelief comes from. Goldman (2006) has a story about that too, but we\nwill leave this aside). \nWhat about Gordon? How does he explain, in a simulationist fashion but\nwithout resorting to introspection, the passage from knowing that Tom\nbelieves that (1) and (2) to judging that Tom believes that (3)?\nAccording to Gordon, the first step in the process is, of course,\nimagining to be Tom—thus believing, in the context\nof the simulation, that (1) and (2). This results (again in the\ncontext of the simulation) in the formation of the belief that (3).\nBut how do I now go about discovering that *I*, Tom, believe that (3)?\nHow can one perform such a self-ascription if not via introspection? A\nsuggestion given by Gareth Evans will show us how—Gordon\nthinks. \nEvans (1982) famously argued that we answer the question “Do I\nbelieve that p?” by answering another question, namely\n“Is it the case that p?” In other words, according\nto Evans, we ascribe beliefs to ourselves not by introspecting, or by\n“looking inside”, but by looking “outside” and\ntrying to ascertain how the world is. If, e.g., I want to know whether\nI believe that Manchester is bigger than Sheffield, I just ask myself\n“Is Manchester bigger than Sheffield?” If I answer in the\naffirmative, then I believe that Manchester is bigger than Sheffield.\nIf I answer in the negative, then I believe that Manchester is\nnot bigger than Sheffield. If I do not know what to answer,\nthen I do not have any belief with regard to this subject\nmatter. \nGordon (1986, 1995) maintains that this self-ascription\nstrategy—which he labels “the ascent\nroutine” (Gordon 2007)—is also the strategy that we\nemploy, in the context of a simulation, to determine the mental states\nof the simulated agent: \nIn a simulation of O, I settle the question of whether O\nbelieves that p by simply asking … whether it is the\ncase that p. That is, I simply concern myself with the\nworld—O’s world, the world from O’s\nperspective. … Reporting O’s beliefs is\njust reporting what is there. (Gordon 1995: 60) \nSo, this is how, in Gordon’s story, I come to judge that Tom has\nthe belief that Fido enjoys watching TV. In the context of the\nsimulation, *I* asked *myself* (where both “*I*” and\n“*myself*” in fact refer to Tom) whether *I* believe that\nFido enjoys watching TV. And *I* answered this question by answering\nanother question, namely, whether it is the case that Fido enjoys\nwatching TV. Given that, from *my* perspective, Fido enjoys watching\nTV (after all, from *my* perspective, Fido is a dog and all dogs enjoy\nwatching TV), *I* expressed my belief by saying: “Yes,\n*I*, Tom, believe that Fido enjoys watching TV”. As you can see,\nin such a story, introspection does not do anything. (We will come\nback to the role of introspection in mindreading in\n section 6.2). \nIn sections 2, 3, and 4 we dwelt upon the “internal”\ndisagreements among Simulation Theorists. It goes without saying that\nsuch disagreements are both wide and deep. In fact, different\nSimulation Theorists give different answers to such fundamental\nquestions as: “What is mental simulation?”, “How\ndoes mental simulation contribute to mindreading?, ‘What is the\nrole of introspection in mindreading?” In light of such\ndifferences of opinion in the simulationist camp, one might conclude\nthat, after all, Stich and Nichols (1997) were right when saying that\nthere is no such thing as the Simulation Theory. However, if\none considers what is shared among Simulation Theorists, one will\nrealize that there is unity amidst this diversity. A good way to\nreveal the commonalities among different versions of ST is by\ncontrasting ST with its arch-enemy, i.e., the Theory-Theory of\nmindreading. This is what we do in the next section. \nST is only one of several accounts of mindreading on the market. A\nrough-and-ready list of the alternatives should at least include: the\nIntentional Stance Theory (Dennett 1987; Gergely & Csibra 2003;\nGergely et al. 1995); Interactionism (Gallagher 2001; Gallagher &\nHutto 2008; De Jaegher at al. 2010); and the Theory-Theory (Gopnik\n& Wellman 1992; Gopnik & Meltzoff 1997; Leslie 1994; Scholl\n& Leslie 1999). In this entry, we will discuss the Theory-Theory\n(TT) only, given that the TT-ST controversy has constituted the focal\npoint of the debate on mindreading during the last 30 years or so. \nAs suggested by its name, the Theory-Theory proposes that mindreading\nis grounded by the possession of a Theory of Mind (“a folk\npsychology”)—i.e., it is based on the tacit knowledge of\nthe following body of information: a number of “folk” laws\nor principles connecting mental states with sensory stimuli,\nbehavioural responses, and other mental states. Here are a couple of\nputative examples: \nLaw of sight: If S is in front of object O,\nS directs her eye-gaze to O, S’s visual\nsystem is properly functioning, and the environmental conditions are\noptimal, then ceteris paribus S will see O. \nLaw of the practical syllogism: If S desires a certain\noutcome G and S believes that by performing a certain\naction A she will obtain G, then ceteris\nparibus S will decide to perform A. \nThe main divide among Theory-Theorists concerns how the Theory of Mind\nis acquired—i.e., it concerns where this body of knowledge comes\nfrom. According to the Child-Scientist Theory-Theory (Gopnik &\nWellman 1992; Gopnik & Meltzoff 1997), a child constructs a Theory\nof Mind exactly as a scientist constructs a scientific theory: she\ncollects evidence, formulates explanatory hypotheses, and revises\nthese hypotheses in the light of further evidence. In other words,\n“folk” laws and principles are obtained through hypothesis\ntesting and revision—a process that, according to proponents of\nthis view, is guided by a general-purpose, Bayesian learning mechanism\n(Gopnik & Wellman 2012). On the contrary, the Nativist\nTheory-Theory (Carruthers 2013; Scholl & Leslie 1999) argues that\na significant part of the Theory of Mind is innate, rather than\nlearned. More precisely, Nativists typically consider the core of the\nTheory of Mind as resulting from the maturation of a cognitive module\nspecifically dedicated to representing mental states \nThese disagreements notwithstanding, the main tenet of TT is clear\nenough: attributions of mental states to other people are guided by\nthe possession of a Theory of Mind. For example, if I know that you\ndesire to buy a copy of The New York Times and I know that\nyou believe that if you go to News & Booze you can buy a\ncopy, then I can use the Law of the Practical Syllogism to\ninfer that you will decide to go to News & Booze. \nTT has been so popular among philosophers and cognitive scientists\nthat the explanation it proposes has ended up being the name of the\nvery phenomenon to be explained: on many occasions, scholars use the\nexpression “Theory of Mind” as a synonym of\n“mindreading”. Simulation Theorists, however, have never\nbeen particularly impressed by this. According to them, there is\nno need to invoke the tacit knowledge of a Theory of Mind to\naccount for mindreading, since a more parsimonious\nexplanation is available: we reuse our own cognitive\nmechanisms to mentally simulate others’ mental states. For\nexample, why do I need to know the Law of the Practical\nSyllogism, if I can employ my own decision-making mechanism\n(which I have anyway) to simulate your decision? It is\nuneconomical—Simulation Theorists say—to resort\nto an information-rich strategy, if an information-poor strategy will\ndo equally as well. \nThe difference between TT and ST can be further illustrated through a\nnice example given by Stich and Nichols (1992). Suppose that you want\nto predict the behavior of an airplane in certain atmospheric\nconditions. You can collect the specifications of the airplane and\ninfer, on the basis of aerodynamic theory, how the airplane\nwill behave. Alternatively, you can build a model of the airplane and\nrun a simulation. The former scenario approximates the way in\nwhich TT describes our capacity to represent others’ mental\nstates, while the latter approximates ST. Two points need to be\nstressed, though. First, while knowledge of aerodynamic theory is\nexplicit, TT says that our knowledge of the Theory of Mind is\ntypically implicit (or tacit). That is, someone who knows\naerodynamic theory is aware of the theory’s laws and\nprinciples and is able to report them correctly, while the laws and\nprinciples constituting one’s Theory of Mind typically lie\noutside awareness and reportability. Second, when we run a simulation\nof someone else’s mental states, we do not need to build a\nmodel: we are the model—that is, we use our own\nmind as a model of others’ minds. \nSimulation Theorists maintain that the default state for the\n“model” is one in which the simulator simply makes no\nadjustments when simulating another individual. That is, ST has it\nthat we are automatically disposed to attribute to a target mental\nstates no different from our own current states. This would often\nserve adequately in social interaction between people who are\ncooperating or competing in what is for practical purposes the same\nsituation. We tend to depart from this default when we perceive\nrelevant differences between others’ situations and our own. In\nsuch cases, we might find ourselves adjusting for situational\ndifferences by putting ourselves imaginatively in what we consider the\nother’s situation to be. \nWe might also make adjustments for individual differences. An\nacquaintance will soon be choosing between candidate a and\ncandidate b in an upcoming election. To us, projecting\nourselves imaginatively into that voting situation, the choice is\nglaringly obvious: candidate a, by any reasonable criteria.\nBut then we may wonder whether this imaginative projection into the\nvoting situation adequately represents our acquaintance in\nthat situation. We might recall things the person has said, or\npeculiarities of dress style, diet, or entertainment, that might seem\nrelevant. Internalizing such behavior ourselves, trying to “get\nbehind” it as an actor might get behind a scripted role, we\nmight then put, as it were, a different person into the voting\nsituation, one who might choose candidate b. \nSuch a transformation would require quarantining some of our own\nmental states, preferences, and dispositions, inhibiting them so that\nthey do not contaminate our off-line decision-making in the role of\nthe other. Such inhibition of one's own mental states would be\ncognitively demanding. For that reason, ST predicts that mindreading\nwill be subject to egocentric errors—that is, it\npredicts that we will often attribute to a target the mental state\nthat we would have if we were in the target’s situation, rather\nthan the state the target is actually in (Goldman 2006). In\n section 6.2,\n we shall discuss whether this prediction is borne out by the\ndata. \nOn the face of it, ST and TT could not be more different from one\nanother. Some philosophers, however, have argued that, on closer\ninspection, ST collapses into TT, thus revealing itself as a form of\nTT in disguise. The collapse argument was originally formulated by\nDaniel Dennett (1987): \nIf I make believe I am a suspension bridge and wonder what I will do\nwhen the wind blows, what “comes to my mind” in my\nmake-believe state depends on… my knowledge of\nphysics… Why should my making believe I have your beliefs be\nany different? In both cases, knowledge of the imitated object is\nneeded to drive the… “simulation”, and the\nknowledge must be… something like a theory. (Dennett\n1987: 100–101, emphasis added) \nDennett’s point is clear. If I imagine being, say, a bridge,\nwhat I imagine will depend on my theory of bridges. Suppose that I\nhave a folk theory of bridges that contains the following principle:\n“A bridge cannot sustain a weight superior to its own\nweight”. In this case, if I imagine an elephant weighing three\ntons walking over a bridge weighing two tons, I will imagine the\nbridge collapsing. Since my “bridge-simulation” is\nentirely theory-driven, “simulation” is a\nmisnomer. The same carries over to “simulating other\npeople”s mental states’, Dennett says. If I try to imagine\nyour mental states, what I imagine will depend entirely on my Theory\nof Mind. Therefore, the label “mental simulation” is\nmisleading. \nHeal (1986) and Goldman (1989) promptly replied to Dennett. Fair\nenough, if a system S tries to simulate the state of a\nradically different system Q (e.g., if a human being tries to\nsimulate the state of a bridge), then S’s simulation must\nbe guided by a theory. However, if a system S tries to simulate\nthe state of a relevantly similar system S*, then\nS’s simulation can be entirely process-driven:\nto simulate the state which S* is in, S simply has to\nrun in itself a process similar to the one S* underwent. Given\nthat, for all intents and purposes, human beings are relevantly\nsimilar to each other, a human being can mentally simulate what\nfollows from having another human being’s mental states without\nresorting to a body of theoretical knowledge about the mind’s\ninner workings. She will just need to reuse her own cognitive\nmechanisms to implement a simulation process. \nThis reply invited the following response (Jackson 1999). If the\npossibility of process-driven simulation is grounded in the similarity\nbetween the simulator and the simulated, then I have to assume that\nyou are relevantly similar to me, when I mentally simulate your mental\nstates. This particular assumption, in turn, will be derived from a\ngeneral principle—something like “Human beings\nare psychologically similar”. Therefore, mental simulation is\ngrounded in the possession of a theory. The threat of collapse is\nback! One reply to Jackson’s arguments is as follows (for other\nreplies see Goldman 2006): the fact that process-driven simulation is\ngrounded in the similarity among human beings does not entail\nthat, in order to run a simulation, a simulator must know (or believe,\nor assume) that such similarity obtains; no more, indeed, than the\nfact that the solubility of salt is grounded in the molecular\nstructure of salt entails that a pinch of salt needs to know chemistry\nto dissolve in water. \nGranting that ST and TT are distinct theories, we can now ask a\ndifferent question: are the theories better off individually or should\nthey join forces somehow? Let us be more explicit. Can ST on its own\noffer an adequate account of mindreading (or at least of the great\nmajority of its episodes)? And what about TT? A good number of\ntheorists now believe that neither ST nor TT alone will do. Rather,\nmany would agree that these two theories need to cooperate, if they\nwant to reach a satisfactory explanation of mindreading. Some authors\nhave put forward TT-ST hybrid models, i.e., models in which the tacit\nknowledge of a Theory of Mind is the central aspect of mindreading,\nbut it is in many cases supplemented by simulation processes\n(Botterill & Carruthers 1999; Nichols & Stich 2003). Other\nauthors have instead defended ST-TT hybrid models, namely, accounts of\nmindreading where the pride of place is given to mental simulation,\nbut where the possession of a Theory of Mind plays some non-negligible\nrole nonetheless (Currie & Ravenscroft 2002; Goldman 2006; Heal\n2003). Since this entry is dedicated to ST, we will briefly touch upon\none instance of the latter variety of hybrid account. \nHeal (2003) suggested that the domain of ST is restricted to those\nmental processes involving rational transitions among\ncontentful mental states. To wit, Heal maintains that mental\nsimulation is the cognitive routine that we employ to represent other\npeople’s rational processes, i.e., those cognitive\nprocesses which are sensitive to the semantic content of the mental\nstates involved. On the other hand,  \nwhen starting point and/or outcome are [states] without content,\nand/or the connection is not [rationally] intelligible, there is no\nreason … to suppose that the process … can be simulated.\n(Heal 2003: 77) \nAn example will clarify the matter. Suppose that I know that you\ndesire to eat sushi, and that you believe that you can order sushi by\ncalling Yama Sushi. To reach the conclusion that you will\ndecide to call Yama Sushi, I only need to imagine desiring\nand believing what you desire and believe, and to run a simulated\ndecision-making process in myself. No further knowledge is required to\npredict your decision: simulation alone will do the job. Consider, on\nthe other hand, the situation in which I know that you took a certain\ndrug and I want to figure out what your mental states will be. In this\ncase—Heal says—my prediction cannot be based on mental\nsimulation. Rather, I need to resort to a body of information about\nthe likely psychological effects of that drug, i.e., I have to resort\nto a Theory of Mind (fair enough, I can also take the drug myself, but\nthis will not count as mental simulation). This, according to Heal,\ngeneralizes to all cases in which a mental state is the input or the\noutput of a mere causal process. In those cases, mental\nsimulation is ineffective and should be replaced by theorizing. Still,\nthose cases do not constitute the central part of mindreading. In\nfact, many philosophers and cognitive scientists would agree that the\ncrucial component of human mindreading is the ability to reason about\nothers’ propositional attitudes. And this is exactly\nthe ability that, according to Heal, should be explained in term of\nmental simulation. This is why Heal’s proposal counts as an\nST-TT hybrid, rather than the other way around. \nST has sparked a lively debate, which has been going on since the end\nof the 1980s. This debate has dealt with a great number of theoretical\nand empirical issues. On the theoretical side, we have seen\nphilosophical discussions of the relation between ST and functionalism\n(Gordon 1986; Goldman 1989; Heal 2003; Stich & Ravenscroft 1992),\nand of the role of tacit knowledge in cognitive explanations (Davies\n1987; Heal 1994; Davies & Stone 2001), just to name a few.\nExamples of empirical debates are: how to account for mindreading\ndeficits in Autism Spectrum Disorders (Baron-Cohen 2000; Currie &\nRavenscroft 2002), or how to explain the evolution of mindreading\n(Carruthers 2009; Lurz 2011). It goes without saying that discussing\nall these bones of contention would require an entire book (most\nprobably, a series of books). In the last section of this\nentry, we confine ourselves to briefly introducing the reader to a\nsmall sample of the main open issues concerning ST. \nWe wrote that ST proposes that mirroring processes (i.e., activations\nof mirror mechanisms in the perception mode): (A) are\n(low-level) simulation processes, and (B) contribute (either\nconstitutively or causally) to mindreading (Gallese et al. 2004;\nGallese & Goldman 1998; Goldman 2006, 2008b; Hurley 2005). Both\n(A) and (B) have been vehemently contested by ST’s\nopponents. \nBeginning with (A), it has been argued that mirroring processes do not\nqualify as simulation processes, because they fail to satisfy the\ndefinition of “simulation process” (Gallagher 2007;\nHerschbach 2012; Jacob 2008; Spaulding 2012) and/or because they are\nbetter characterized in different terms, e.g., as enactive perceptual\nprocesses (Gallagher 2007) or as elements in an information-rich\nprocess (Spaulding 2012). As for (B), the main worry runs as follows.\nGranting that mirroring processes are simulation processes, what\nevidence do we have for the claim that they contribute to mindreading?\nThis, in particular, has been asked with respect to the role of\nmirroring processes in “action understanding” (i.e., the\ninterpretation of an agent’s behavior in terms of the\nagent’s intentions, goals, etc.). After all, the neuroscientific\nevidence just indicates that action mirroring correlates with\nepisodes of action understanding, but correlation is not causation,\nlet alone constitution. In fact, there are no studies examining\nwhether disruption of the monkey mirror neuron circuit results in\naction understanding deficits, and the evidence on human action\nunderstanding following damage to the action mirror mechanism is\ninconclusive at best (Hickok 2009). In this regard, some authors have\nsuggested that the most plausible hypothesis is instead that action\nmirroring follows (rather than causes or constitutes) the\nunderstanding of others’ mental states (Csibra 2007; Jacob\n2008). For example, Jacob (2008) proposes that the job of mirroring\nprocesses in the action domain is just that of computing a\nrepresentation of the observed agent’s next movement,\non the basis of a previous representation of the\nagent’s intention. Similar deflationary accounts of the action\nmirror mechanism have been given by Brass et al. (2007), Hickok\n(2014), and Vannuscorps and Caramazza (2015)—these accounts\ntypically take the STS (superior temporal sulcus, a brain region\nlacking mirror neurons) to be the critical neural area for action\nunderstanding. \nThere are various ways to respond to these criticisms. A strong\nresponse argues that they are based on a misunderstanding of the\nrelevant empirical findings, as well as on a mischaracterization of\nthe role that ST attributes to the action mirror mechanism in action\nunderstanding (Rizzolatti & Sinigaglia 2010, 2014). A weaker\nresponse holds that the focus on action understanding is a bit of a\nred herring, given that the most robust evidence in support of the\ncentral role played by mirroring processes in mindreading comes from\nthe emotion domain (Goldman 2008b). We will consider the weaker\nresponse here. \nGoldman and Sripada (2005) discuss a series of paired deficits in\nemotion production and face-based emotion mindreading. These\ndeficits—they maintain—are best explained by the\nhypothesis that one attributes emotions to someone else through\nsimulating these emotions in oneself: when the ability to undergo the\nemotion breaks down, the mindreading capacity breaks down as well.\nBarlassina (2013) elaborates on this idea by considering\nHuntington’s Disease (HD), a neurodegenerative disorder\nresulting in, among other things, damage to the disgust mirror\nmechanism. As predicted by ST, the difficulties individuals with HD\nhave in experiencing disgust co-occur with an impairment in\nattributing disgust to someone else on the basis of observing her\nfacial expression—despite perceptual abilities and knowledge\nabout disgust being preserved in this clinical population. Individuals\nsuffering from HD, however, exhibit an intact capacity for disgust\nmindreading on the basis of non-facial visual stimuli. For this\nreason, Barlassina concludes by putting forward an ST-TT hybrid model\nof disgust mindreading on the basis of visual stimuli. \nST’s central claim is that we reuse our own cognitive\nmechanisms to arrive at a representation of other\npeople’s mental states. This claim raises a number of issues\nconcerning how ST conceptualizes the self-other relation. We will\ndiscuss a couple of them. \nGallagher (2007: 355) writes that  \ngiven the large diversity of motives, beliefs, desires, and behaviours\nin the world, it is not clear how a simulation process … can\ngive me a reliable sense of what is going on in the other\nperson’s mind.  \nThere are two ways of interpreting Gallagher’s worry. First, it\ncan be read as saying that if mindreading is based on mental\nsimulation, then it is hard to see how mental state attributions could\nbe epistemically justified. This criticism, however, misses\nthe mark entirely, since ST is not concerned with whether mental state\nattributions count as knowledge, but only with how, as a matter of\nfact, we go about forming such attributions. A second way to\nunderstand Gallagher’s remarks is this: as a matter of\nfact, we are pretty successful in understanding other minds;\nhowever, given the difference among individual minds, this pattern of\nsuccesses cannot be explained in terms of mental simulation. \nST has a two-tier answer to the second reading of Gallagher’s\nchallenge. First, human beings are very similar with regard to\ncognitive processes such as perception, theoretical reasoning,\npractical reasoning, etc. For example, there is a very high\nprobability that if both you and I look at the same scene, we will\nhave the same visual experience. This explains why, in the large\nmajority of cases, I can reuse my visual mechanism to successfully\nsimulate your visual experiences. Second, even though we are quite\ngood at recognizing others’ mental states, we are nonetheless\nprone to egocentric errors, i.e., we tend to attribute to a\ntarget the mental state that we would undergo if we were in\nthe target’s situation, rather than the actual mental state the\ntarget is in (Goldman 2006). A standard example is the curse of\nknowledge bias, where we take for granted that other people know\nwhat we know (Birch & Bloom 2007). ST has a straightforward\nexplanation of such egocentric errors (Gordon 1995; Goldman 2006): if\nwe arrive at attributing mental states via mental simulation, the\nattribution accuracy will depend on our capacity to\n“quarantine” our genuine mental states, when they do not\nmatch the target’s, and to replace them with more appropriate\nsimulated mental states. This “adjustment” process,\nhowever, is a demanding one, because our genuine mental states exert a\npowerful tendency. Thus, Gallagher is right when he says that, on some\noccasions, “if I project the results of my own simulation onto\nthe other, I understand only myself in that other’s situation,\nbut I don’t understand the other” (Gallagher 2007: 355).\nHowever, given how widespread egocentric errors are, this counts as a\npoint in favour of ST, rather than as an argument against it (but see\nde Vignemont & Mercier 2016, and Saxe 2005). \nCarruthers (1996, 2009, 2011) raises a different problem for ST: no\nversion of ST can adequately account for self-attributions of mental\nstates. Recall that, according to Goldman (2006), simulation-based\nmindreading is a three-stage process in which we first mentally\nsimulate a target’s mental state, we then introspect\nand categorize the simulated mental state, and we finally attribute\nthe categorized state to the target. Since Goldman’s model has\nit that attributions of mental states to others asymmetrically depend\non the ability to introspect one’s own mental states, it\npredicts that: (A) introspection is (ontogenetically and\nphylogenetically) prior to the ability to represent others’\nmental states; (B) there are cases in which introspection works just\nfine, but where the ability to represent others’ mental states\nis impaired (presumably, because the mechanism responsible for\nprojecting one’s mental states to the target is damaged).\nCarruthers (2009) argues that neither (A) nor (B) are borne out by the\ndata. The former because there are no creatures that have\nintrospective capacities but at the same time lack the ability to\nrepresent others’ mental states; the latter because there are no\ndissociation cases in which an intact capacity for introspection is\npaired with an impairment in the ability to represent others’\nmental states. \nHow might a Simulation Theorist respond to this objection? As we said\nin\n section 4,\n Gordon’s (1986, 1995, 1996) Radical Simulationism does\nnot assign any role to introspection in mindreading. Rather, Gordon\nproposes that self-ascriptions are guided by ascent routines through\nwhich we answer the question “Do I believe that p?”\nby answering the lower-order question “Is it the case that\np?” Carruthers (1996, 2011) thinks that this won’t\ndo either. Here is one of the many problems that Carruthers raises for\nthis suggestion—we can call it “The Scope\nProblem”:  \nthis suggestion appears to have only a limited range of application.\nFor even if it works for the case of belief, it is very hard to see\nhow one might extend it to account for our knowledge of our own goals,\ndecisions, or intentions—let alone our knowledge of our own\nattitudes of wondering, supposing, fearing, and so on. (Carruthers\n2011: 81) \nCarruthers’ objections are important and deserve to be taken\nseriously. To discuss them, however, we would need to introduce a lot\nof further empirical evidence and many complex philosophical ideas\nabout self-knowledge. This is not a task that we can take up here (the\ninterested reader is encouraged to read, in addition to Gordon (2007)\nand Goldman (2009), the SEP entries on\n self-knowledge\n and on\n introspection).\n The take-home message should be clear enough nonetheless: anybody who\nputs forward an account of mindreading should remember that such an\naccount has to cohere with a plausible story about the cognitive\nmechanisms underlying self-attribution. \nThe development of mindreading capacities in children has been one of\nthe central areas of empirical investigation. In particular,\ndevelopmental psychologists have put a lot of effort into detailing\nhow the ability to attribute false beliefs to others develops. Until\n2005, the central experimental paradigm to test this ability was the\nverbal false belief task (Wimmer & Perner 1983). Here is\na classic version of it. A subject is introduced to two dolls, Sally\nand Anne, and three objects: Sally’s ball, a basket, and a box.\nSally puts her ball in the basket and leaves the scene. While Sally is\naway, Anne takes the ball out of the basket and puts it into the box.\nSally then returns. The subject is asked where she thinks Sally will\nlook for the ball. The correct answer, of course, is that Sally will\nlook inside the basket. To give this answer, the subject has to\nattribute to Sally the false belief that the ball is in the\nbasket. A number of experiments have found that while four-year old\nchildren pass this task, three-year old children fail it (for a\nreview, see Wellman et al. 2001). For a long time, the mainstream\ninterpretation of these findings has been that children acquire the\nability to attribute false beliefs only around their fourth birthday\n(but see Clements & Perner 1994 and Bloom & German 2000). \nIn 2005, this developmental timeline was called into question.\nKristine Onishi and Renée Baillargeon (2005) published the\nresult of a non-verbal version of the false belief task,\nwhich they administered to 15-month old infants. The experiment\ninvolves three steps. First, the infants see a toy between two boxes,\none yellow and one green, and then an actor hiding the toy inside the\ngreen box. Next, the infants see the toy sliding out of the green box\nand hiding inside the yellow box. In the true belief condition (TB),\nthe actor notices that the toy changes location, while in the false\nbelief condition (FB) she does not. Finally, half of the infants see\nthe actor reaching into the green box, while the other half sees the\nactor reaching into the yellow box. According to the\nviolation-of-expectation paradigm, infants reliably look for\na longer time at unexpected events. Therefore, if the infants expected\nthe actor to search for the toy on the basis of the\nactor’s belief about its location, then when the actor had a\ntrue belief that the toy was hidden in one box, the infants\nshould look longer when the actor reached into the other box instead.\nConversely, the infants should look longer at one box when the actor\nfalsely believed that the toy was hidden in the other box.\nStrikingly, these predictions were confirmed in both the (TB) and (FB)\nconditions. On this basis, Onishi and Baillargeon (2005) concluded\nthat children of 15 months possess the capacity to represent\nothers’ false beliefs. \nThis and subsequent versions of non-verbal false belief tasks\nattracted a huge amount of interest (at the current stage of research,\nthere is evidence that sensitivity to others’ false beliefs is\npresent in infants as young as 7 months—for a review, see\nBaillargeon at al. 2016). Above all, the following two questions have\nbeen widely discussed: why do children pass the non-verbal false\nbelief task at such an early age, but do not pass the verbal version\nbefore the age of 4? Does passing the non-verbal false belief task\nreally indicate the capacity to represent others’ false beliefs?\n(Perner & Ruffman 2005; Apperly & Butterfill 2009; Baillargeon\net al. 2010; Carruthers 2013; Helming et al. 2014). \nGoldman and Jordan (2013) maintain that ST has a good answer to both\nquestions. To begin with, they argue that it is implausible to\nattribute to infants such sophisticated meta-representational\nabilities as the ability to represent others’ false beliefs.\nThus, Goldman and Jordan favour a deflationary view, according to\nwhich infants are sensitive to others’ false beliefs,\nbut do not represent them as such. In particular, they\npropose that rather than believing that another subject S\n(falsely) believes that p, infants simply imagine how the\nworld is from S’s perspective—that is, they simply\nimagine that p is the case. This—Goldman and Jordan\nsay—is a more primitive psychological competence than\nmindreading, since it does not involve forming a judgment about\nothers’ mental states. This brings us to Goldman and\nJordan’s answer to the question “why do children pass the\nverbal false belief task only at four?” Passing this task\nrequires fully-fledged mindreading abilities and executive functions\nsuch as inhibitory control. It takes quite a lot of time—around\n3 to 4 years—before these functions and abilities come\nonline. \nSince the late 1980s, ST has received a great amount of attention from\nphilosophers, psychologists, and neuroscientists. This is not\nsurprising. Mindreading is a central human cognitive capacity, and ST\nchalleges some basic assumptions about the cognitive processes and\nneural mechanisms underlying human social behavior. Moreover, ST\ntouches upon a number of major philosophical problems, such as the\nrelation between self-knowledge and knowledge of other minds, and the\nnature of mental concepts, including the concept of mind itself. In\nthis entry, we have considered some of the fundamental empirical and\nphilosophical issues surrounding ST. Many of them remain open. In\nparticular, while the consensus view is now that both mental\nsimulation and theorizing play important role in mindreading, the\ncurrently available evidence falls short of establishing what their\nrespective roles are. In other words, it is likely that we shall end\nup adopting a hybrid model of mindreading that combines ST and TT,\nbut, at the present stage, it is very difficult to predict what this\nhybrid model will look like. Hopefully, the joint work of philosophers\nand cognitive scientists will help to settle the matter.","contact.mail":"gordon@umsl.edu","contact.domain":"umsl.edu"}]
