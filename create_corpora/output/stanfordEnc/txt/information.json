[{"date.published":"2012-10-26","date.changed":"2020-08-18","url":"https://plato.stanford.edu/entries/information/","author1":"Pieter Adriaans","author1.info":"http://www.uva.nl/en/profile/a/d/p.w.adriaans/p.w.adriaans.html","entry":"information","body.text":"\n\n\nPhilosophy of Information deals with the philosophical analysis of the\nnotion of information both from a historical and a systematic\nperspective. With the emergence of the empiricist theory of knowledge\nin early modern philosophy, the development of various mathematical\ntheories of information in the twentieth century and the rise of\ninformation technology, the concept of “information” has\nconquered a central place in the sciences and in society. This\ninterest also led to the emergence of a separate branch of philosophy\nthat analyzes information in all its guises (Adriaans & van\nBenthem 2008a,b; Lenski 2010; Floridi 2002, 2011). Information has\nbecome a central category in both the sciences and the humanities and\nthe reflection on information influences a broad range of\nphilosophical disciplines varying from logic (Dretske 1981; van\nBenthem & van Rooij 2003; van Benthem 2006, see the entry on\n logic and information),\n epistemology (Simondon 1989) to ethics (Floridi 1999) and esthetics\n(Schmidhuber 1997a; Adriaans 2008) to ontology (Zuse 1969; Wheeler\n1990; Schmidhuber 1997b; Wolfram 2002; Hutter 2010). \n\n\nThere is no consensus about the exact nature of the field of\nphilosophy of information. Several authors have proposed a more or\nless coherent philosophy of information as an attempt to rethink\nphilosophy from a new perspective: e.g., quantum physics\n(Mugur-Schächter 2002), \n logic (Brenner 2008), semantic information (Floridi 2011; Adams &\nde Moraes 2016, see the entry on\n semantic conceptions of information), \ncommunication and message systems (Capurro &\nHolgate 2011) and meta-philosophy (Wu 2010, 2016). Others (Adriaans\n& van Benthem 2008a; Lenski 2010) see it more as a technical\ndiscipline with deep roots in the history of philosophy and\nconsequences for various disciplines like methodology, epistemology\nand ethics. Whatever one’s interpretation of the nature of\nphilosophy of information is, it seems to imply an ambitious research\nprogram consisting of many sub-projects varying from the\nreinterpretation of the history of philosophy in the context of modern\ntheories of information, to an in depth analysis of the role of\ninformation in science, the humanities and society as a whole. \n\n\nThe term “information” in colloquial speech is currently\npredominantly used as an abstract mass-noun used to denote any amount\nof data, code or text that is stored, sent, received or manipulated in\nany medium. The detailed history of both the term\n“information” and the various concepts that come with it\nis complex and for the larger part still has to be written (Seiffert\n1968; Schnelle 1976; Capurro 1978, 2009; Capurro & Hjørland\n2003). The exact meaning of the term “information” varies\nin different philosophical traditions and its colloquial use varies\ngeographically and over different pragmatic contexts. Although an\nanalysis of the notion of information has been a theme in Western\nphilosophy from its early inception, the explicit analysis of\ninformation as a philosophical concept is recent, and dates back to\nthe second half of the twentieth century. At this moment it is clear\nthat information is a pivotal concept in the sciences and humanities\nand in our every day life. Everything we know about the world is based\non information we received or gathered and every science in principle\ndeals with information. There is a network of related concepts of\ninformation, with roots in various disciplines like physics,\nmathematics, logic, biology, economy and epistemology. All these\nnotions cluster around two central properties: \n\n\n\nInformation is extensive. Central is the concept\nof additivity: the combination of two independent datasets\nwith the same amount of information contains twice as much\ninformation as the separate individual datasets. The notion of\nextensiveness emerges naturally in our interactions with the world\naround us when we count and measure objects and structures. Basic\nconceptions of more abstract mathematical entities, like sets,\nmultisets and sequences, were developed early in history on the basis\nof structural rules for the manipulation of symbols (Schmandt-Besserat\n1992). The mathematical formalisation of extensiveness in terms of the\nlog function took place in the context of research in to\nthermodynamics in the nineteenth (Boltzmann 1866) and early twentieth\ncentury (Gibbs 1906). When coded in terms of more advanced\nmulti-dimensional numbers systems (complex numbers, quaternions,\noctonions) the concept of extensiveness generalizes in to more subtle\nnotions of additivity that do not meet our everyday intuitions. Yet\nthey play an important role in recent developments of information\ntheory based on quantum physics (Von Neumann 1932; Redei &\nStöltzner 2001, see entry on\n quantum entanglement and information).\n\n\n Information reduces uncertainty. The amount of\ninformation we get grows linearly with the amount by which it reduces\nour uncertainty until the moment that we have received all possible\ninformation and the amount of uncertainty is zero. The relation\nbetween uncertainty and information was probably first formulated by\nthe empiricists (Locke 1689; Hume 1748). Hume explicitly observes that\na choice from a larger selection of possibilities gives more\ninformation. This observation reached its canonical mathematical\nformulation in the function proposed by Hartley (1928) that defines\nthe amount of information we get when we select an element from a\nfinite set. The only mathematical function that unifies these two\nintuitions about extensiveness and probability is the one that defines\nthe information in terms of the negative log of the probability:\n\\(I(A)= -\\log P(A)\\) (Shannon 1948; Shannon & Weaver 1949,\nRényi 1961). \n\n\n\nThe elegance of this formula however does not shield us from the\nconceptual problems it harbors. In the twentieth century various\nproposals for formalization of concepts of information were made: \n\nQualitative Theories of Information\n\nSemantic Information: Bar-Hillel and Carnap\ndeveloped a theory of semantic Information (1953). Floridi (2002,\n2003, 2011) defines semantic information as well-formed, meaningful\nand truthful data. Formal entropy based definitions of information\n(Fisher, Shannon, Quantum, Kolmogorov) work on a more general level\nand do not necessarily measure information in meaningful truthful\ndatasets, although one might defend the view that in order to be\nmeasurable the data must be well-formed (for a discussion see\n section 6.6 on Logic and Semantic Information). \nSemantic information is close to our everyday naive notion of\ninformation as something that is conveyed by true statements about the\nworld. \n\nInformation as a state of an agent: the formal\nlogical treatment of notions like knowledge and belief was initiated\nby Hintikka (1962, 1973). Dretske (1981) and van Benthem & van\nRooij (2003) studied these notions in the context of information\ntheory, cf. van Rooij (2003) on questions and answers, or Parikh & Ramanujam\n(2003) on general messaging. Also Dunn seems to have this notion in\nmind when he defines information as “what is left of knowledge\nwhen one takes away belief, justification and truth” (Dunn\n2001: 423; 2008). Vigo proposed a Structure-Sensitive Theory of\nInformation based on the complexity of concept acquisition by agents\n(Vigo 2011, 2012). \n\n\nQuantitative Theories of Information\n\nNyquist’s function: Nyquist (1924) was\nprobably the first to express the amount of “intelligence”\nthat could be transmitted given a certain line speed of a telegraph\nsystems in terms of a log function: \\(W= k \\log m\\), where W is\nthe speed of transmission, K is a constant, and m are\nthe different voltage levels one can choose from.\n\nFisher information: the amount of information\nthat an observable random variable X carries about an unknown\nparameter \\(\\theta\\) upon which the probability of X depends\n(Fisher 1925).\n\nThe Hartley function: (Hartley 1928, Rényi\n1961, Vigo 2012). The amount of information we get when we select an\nelement from a finite set S under uniform distribution is the\nlogarithm of the cardinality of that set. \n\nShannon information: the entropy, H, of a\ndiscrete random variable X is a measure of the amount of\nuncertainty associated with the value of X (Shannon 1948;\nShannon & Weaver 1949).\n\nKolmogorov complexity: the information in a\nbinary string x is the length of the shortest program p\nthat produces x on a reference universal Turing machine\nU (Turing 1937; Solomonoff 1960, 1964a,b, 1997; Kolmogorov\n1965; Chaitin 1969, 1987).\n\nEntropy measures in Physics: Although they are\nnot in all cases strictly measures of information, the different\nnotions of entropy defined in physics are closely related to\ncorresponding concepts of information. We mention Boltzmann\nEntropy (Boltzmann, 1866) closely related to the Hartley Function\n(Hartley 1928), Gibbs Entropy (Gibbs 1906) formally\nequivalent to Shannon entropy and various generalizations like\nTsallis Entropy (Tsallis 1988) and Rényi\nEntropy (Rényi 1961). \n\nQuantum Information: The qubit is a\ngeneralization of the classical bit and is described by a quantum\nstate in a two-state quantum-mechanical system, which is formally\nequivalent to a two-dimensional vector space over the complex numbers\n(Von Neumann 1932; Redei & Stöltzner 2001).\n\n\nUntil recently the possibility of a unification of these theories was\ngenerally doubted (Adriaans & van Benthem 2008a), but after two\ndecades of research, perspectives for unification seem better. \n\n\nThe contours of a unified concept of information emerges along the\nfollowing lines: \n\n Philosophy of information is a sub-discipline of philosophy,\nintricately related to the philosophy of logic and mathematics.\nPhilosophy of semantic information (Floridi 2011,\nD’Alfonso 2012, Adams & de Moraes, 2016) again is a\nsub-discipline of philosophy of information (see the informational map\nin the entry on\n semantic conceptions of information).\n From this perspective philosophy of information is interested in the\ninvestigation of the subject at the most general level: data,\nwell-formed data, environmental data etc. Philosophy of semantic\ninformation adds the dimensions of meaning and\ntruthfulness. It is possible to interpret quantitative\ntheories of information in the framework of a philosophy of semantic\ninformation (see\n section 6.5 for an in-depth discussion). \n\n Various quantitative concepts of information are\nassociated with different narratives (counting, receiving messages,\ngathering information, computing) rooted in the same basic\nmathematical framework. Many problems in philosophy of information\ncenter around related problems in philosophy of mathematics.\nConversions and reductions between various formal models have been\nstudied (Cover & Thomas 2006; Grünwald & Vitányi\n2008; Bais & Farmer 2008). The situation that seems to emerge is\nnot unlike the concept of energy: there are various formal\nsub-theories about energy (kinetic, potential, electrical, chemical,\nnuclear) with well-defined transformations between them. Apart from\nthat, the term “energy” is used loosely in colloquial\nspeech. \n\n Agent based concepts of information emerge\nnaturally when we extend our interest from simple measurement and\nsymbol manipulation to the more complex paradigm of an agent with\nknowledge, beliefs, intentions and freedom of choice. They are\nassociated with the deployment of other concepts of information. \n\nThe emergence of a coherent theory to measure information\nquantitatively in the twentieth century is closely related to the\ndevelopment of the theory of computing. Central in this context are\nthe notions of Universality, Turing\nequivalence and Invariance: because the\nconcept of a Turing system defines the notion of a universal\nprogrammable computer, all universal models of computation seem to\nhave the same power. This implies that all possible measures of\ninformation definable for universal models of computation (Recursive\nFunctions, Turing Machine, Lambda Calculus etc.) are\ninvariant modulo an additive constant. This gives a perspective on a unified theory of\ninformation that might dominate the research program for the years to\ncome.\n\nThe lack of preciseness and the universal usefulness of the term\n“information” go hand in hand. In our society, in which we\nexplore reality by means of instruments and installations of ever\nincreasing complexity (telescopes, cyclotrons) and communicate via\nmore advanced media (newspapers, radio, television, SMS, the\nInternet), it is useful to have an abstract mass-noun for the\n“stuff” that is created by the instruments and that\n“flows” through these media. Historically this general\nmeaning emerged rather late and seems to be associated with the rise\nof mass media and intelligence agencies (Devlin & Rosenberg 2008;\nAdriaans & van Benthem 2008b). \nIn present colloquial speech the term information is used in various\nloosely defined and often even conflicting ways. Most people, for\ninstance, would consider the following inference prima facie\nto be valid: \nIf I get the information that p then I know that p. \nThe same people would probably have no problems with the statement\nthat “Secret services sometimes distribute false\ninformation”, or with the sentence “The information\nprovided by the witnesses of the accident was vague and\nconflicting”. The first statement implies that information\nnecessarily is true, while the other statements allow for the\npossibility that information is false, conflicting and vague. In\neveryday communication these inconsistencies do not seem to create\ngreat trouble and in general it is clear from the pragmatic context\nwhat type of information is designated. These examples suffice to\nargue that references to our intuitions as speakers of the English\nlanguage are of little help in the development of a rigorous\nphilosophical theory of information. There seems to be no pragmatic\npressure in everyday communication to converge to a more exact\ndefinition of the notion of information. \nUntil the second half of the twentieth century almost no modern\nphilosopher considered “information” to be an important\nphilosophical concept. The term has no lemma in the well-known\nencyclopedia of Edwards (1967) and is not mentioned in Windelband\n(1903). In this context the interest in “Philosophy of\nInformation” is a recent development. Yet, with hindsight from\nthe perspective of a history of ideas, reflection on the notion of\n“information” has been a predominant theme in the history\nof philosophy. The reconstruction of this history is relevant for the\nstudy of information. \nA problem with any “history of ideas” approach is the\nvalidation of the underlying assumption that the concept one is\nstudying has indeed continuity over the history of philosophy. In the\ncase of the historical analysis of information one might ask whether\nthe concept of “informatio” discussed by\nAugustine has any connection to Shannon information, other than a\nresemblance of the terms. At the same time one might ask whether\nLocke’s “historical, plain method” is an important\ncontribution to the emergence of the modern concept of information\nalthough in his writings Locke hardly uses the term\n“information” in a technical sense. As is shown below,\nthere is a conglomerate of ideas involving a notion of information\nthat has developed from antiquity till recent times, but further study\nof the history of the concept of information is necessary.  \nAn important recurring theme in the early philosophical analysis of\nknowledge is the paradigm of manipulating a piece of wax: either by\nsimply deforming it, by imprinting a signet ring in it or by writing\ncharacters on it. The fact that wax can take different shapes and\nsecondary qualities (temperature, smell, touch) while the volume\n(extension) stays the same, make it a rich source of analogies,\nnatural to Greek, Roman and medieval culture, where wax was used both\nfor sculpture, writing (wax tablets) and encaustic painting. One finds\nthis topic in writings of such diverse authors as Democritus, Plato,\nAristotle, Theophrastus, Cicero, Augustine, Avicenna, Duns Scotus,\nAquinas, Descartes and Locke. \nIn classical philosophy “information” was a technical\nnotion associated with a theory of knowledge and ontology that\noriginated in Plato’s (427–347 BCE) theory of forms,\ndeveloped in a number of his dialogues (Phaedo, Phaedrus,\nSymposium, Timaeus, Republic). Various imperfect individual\nhorses in the physical world could be identified as horses, because\nthey participated in the static atemporal and aspatial idea of\n“horseness” in the world of ideas or forms. When later\nauthors like Cicero (106–43 BCE) and Augustine (354–430\nCE) discussed Platonic concepts in Latin they used the terms\ninformare and informatio as a translation for\ntechnical Greek terms like eidos (essence), idea\n(idea), typos (type), morphe (form) and\nprolepsis (representation). The root “form” still\nis recognizable in the word in-form-ation (Capurro &\nHjørland 2003). Plato’s theory of forms was an attempt to\nformulate a solution for various philosophical problems: the theory of\nforms mediates between a static (Parmenides, ca. 450 BCE) and a\ndynamic (Herakleitos, ca. 535–475 BCE) ontological conception of\nreality and it offers a model to the study of the theory of human\nknowledge. According to Theophrastus (371–287 BCE) the analogy\nof the wax tablet goes back to Democritos (ca. 460–380/370 BCE)\n(De Sensibus 50). In the Theaetetus (191c,d) Plato\ncompares the function of our memory with a wax tablet in which our\nperceptions and thoughts are imprinted like a signet ring stamps\nimpressions in wax. Note that the metaphor of imprinting symbols in\nwax is essentially spatial (extensive) and can not easily be\nreconciled with the aspatial interpretation of ideas supported by\nPlato. \nOne gets a picture of the role the notion of “form” plays\nin classical methodology if one considers Aristotle’s\n(384–322 BCE) doctrine of the four causes. In Aristotelian\nmethodology understanding an object implied understanding four\ndifferent aspects of it: Material Cause:: that as the result of whose\npresence something comes into being—e.g., the bronze of a statue\nand the silver of a cup, and the classes which contain these Formal Cause:: the form or pattern; that is, the\nessential formula and the classes which contain it—e.g., the\nratio 2:1 and number in general is the cause of the octave-and the\nparts of the formula. Efficient Cause:: the source of the first\nbeginning of change or rest; e.g., the man who plans is a cause, and\nthe father is the cause of the child, and in general that which\nproduces is the cause of that which is produced, and that which\nchanges of that which is changed. Final Cause:: the same as “end”; i.e.,\nthe final cause; e.g., as the “end” of walking is\nhealth. For why does a man walk?  “To be healthy”, we say,\nand by saying this we consider that we have supplied the\ncause. (Aristotle, Metaphysics 1013a) \nNote that Aristotle, who rejects Plato’s theory of forms as\natemporal aspatial entities, still uses “form” as a\ntechnical concept. This passage states that knowing the form or\nstructure of an object, i.e., the information, is a necessary\ncondition for understanding it. In this sense information is a crucial\naspect of classical epistemology. \nThe fact that the ratio 2:1 is cited as an example also illustrates\nthe deep connection between the notion of forms and the idea that the\nworld was governed by mathematical principles. Plato believed under\ninfluence of an older Pythagorean (Pythagoras 572–ca. 500 BCE)\ntradition that “everything that emerges and happens in the\nworld” could be measured by means of numbers (Politicus\n285a). On various occasions Aristotle mentions the fact that Plato\nassociated ideas with numbers (Vogel 1968: 139). Although\nformal mathematical theories about information only emerged in the\ntwentieth century, and one has to be careful not to interpret the\nGreek notion of a number in any modern sense, the idea that\ninformation was essentially a mathematical notion, dates back to\nclassical philosophy: the form of an entity was conceived as a\nstructure or pattern that could be described in terms of numbers. Such\na form had both an ontological and an epistemological aspect: it\nexplains the essence as well as the understandability of the object.\nThe concept of information thus from the very start of philosophical\nreflection was already associated with epistemology, ontology and\nmathematics. \nTwo fundamental problems that are not explained by the classical\ntheory of ideas or forms are 1) the actual act of knowing an object\n(i.e., if I see a horse in what way is the idea of a horse activated\nin my mind) and 2) the process of thinking as manipulation of ideas.\nAristotle treats these issues in De Anime, invoking the\nsignet-ring-impression-in-wax analogy: \nBy a “sense” is meant what has the power of receiving into\nitself the sensible forms of things without the matter. This must be\nconceived of as taking place in the way in which a piece of wax takes\non the impress of a signet-ring without the iron or gold; we say that\nwhat produces the impression is a signet of bronze or gold, but its\nparticular metallic constitution makes no difference: in a similar way\nthe sense is affected by what is coloured or flavoured or sounding,\nbut it is indifferent what in each case the substance is; what alone\nmatters is what quality it has, i.e., in what ratio its constituents\nare combined. (De Anime, Book II, Chp. 12) \nHave not we already disposed of the difficulty about interaction\ninvolving a common element, when we said that mind is in a sense\npotentially whatever is thinkable, though actually it is nothing until\nit has thought? What it thinks must be in it just as characters may be\nsaid to be on a writing-tablet on which as yet nothing actually stands\nwritten: this is exactly what happens with mind. (De Anime,\nBook III, Chp. 4) \nThese passages are rich in influential ideas and can with hindsight be\nread as programmatic for a philosophy of information: the process of\ninformatio can be conceived as the imprint of characters on a\nwax tablet (tabula rasa), thinking can be analyzed in terms\nof manipulation of symbols. \nThroughout the Middle Ages the reflection on the concept of\ninformatio is taken up by successive thinkers. Illustrative\nfor the Aristotelian influence is the passage of Augustine in De\nTrinitate book XI. Here he analyzes vision as an analogy for the\nunderstanding of the Trinity. There are three aspects: the corporeal\nform in the outside world, the informatio by the sense of\nvision, and the resulting form in the mind. For this process of\ninformation Augustine uses the image of a signet ring making an\nimpression in wax (De Trinitate, XI Cap 2 par 3). Capurro\n(2009) observes that this analysis can be interpreted as an early\nversion of the technical concept of “sending a message” in\nmodern information theory, but the idea is older and is a common topic\nin Greek thought (Plato Theaetetus 191c,d; Aristotle De\nAnime, Book II, Chp. 12, Book III, Chp. 4; Theophrastus De\nSensibus 50). \nThe tabula rasa notion was later further developed in the\ntheory of knowledge of Avicenna (c. 980–1037 CE):  \nThe human intellect at birth is rather like a tabula rasa, a\npure potentiality that is actualized through education and comes to\nknow. Knowledge is attained through empirical familiarity with objects\nin this world from which one abstracts universal concepts. (Sajjad\n2006\n [Other Internet Resources [hereafter OIR]])\n  \nThe idea of a tabula rasa development of the human mind was\nthe topic of a novel Hayy ibn Yaqdhan by the Arabic Andalusian\nphilosopher Ibn Tufail (1105–1185 CE, known as\n“Abubacer” or “Ebn Tophail” in the West). This\nnovel describes the development of an isolated child on a deserted\nisland. A later translation in Latin under the title Philosophus\nAutodidactus (1761) influenced the empiricist John Locke in the\nformulation of his tabula rasa doctrine. \nApart from the permanent creative tension between theology and\nphilosophy, medieval thought, after the rediscovery of\nAristotle’s Metaphysics in the twelfth century inspired\nby Arabic scholars, can be characterized as an elaborate and subtle\ninterpretation and development of, mainly Aristotelian, classical\ntheory. Reflection on the notion of informatio is taken up,\nunder influence of Avicenna, by thinkers like Aquinas (1225–1274\nCE) and Duns Scotus (1265/66–1308 CE). When Aquinas discusses\nthe question whether angels can interact with matter he refers to the\nAristotelian doctrine of hylomorphism (i.e., the theory that substance\nconsists of matter (hylo (wood), matter) and form\n(morphè)). Here Aquinas translates this as the\nin-formation of matter (informatio materiae) (Summa\nTheologiae, 1a 110 2; Capurro 2009). Duns Scotus refers to\ninformatio in the technical sense when he discusses\nAugustine’s theory of vision in De Trinitate, XI Cap 2\npar 3 (Duns Scotus, 1639, “De imagine”,\nOrdinatio, I, d.3, p.3). \nThe tension that already existed in classical philosophy between\nPlatonic idealism(universalia ante res) and Aristotelian\nrealism (universalia in rebus) is recaptured as the problem\nof universals: do universal qualities like “humanity” or\nthe idea of a horse exist apart from the individual entities that\ninstantiate them? It is in the context of his rejection of universals\nthat Ockham (c. 1287–1347 CE) introduces his well-known razor:\nentities should not be multiplied beyond necessity. Throughout their\nwritings Aquinas and Scotus use the Latin terms informatio\nand informare in a technical sense, although this terminology\nis not used by Ockham. \nThe history of the concept of information in modern philosophy is\ncomplicated. Probably starting in the fourteenth century the term\n“information” emerged in various developing European\nlanguages in the general meaning of “education” and\n“inquiry”. The French historical dictionary by Godefroy\n(1881) gives action de former, instruction, enquête,\nscience, talent as early meanings of “information”.\nThe term was also used explicitly for legal inquiries\n(Dictionnaire du Moyen Français (1330–1500)\n2015). Because of this colloquial use the term\n“information” loses its association with the concept of\n“form” gradually and appears less and less in a formal\nsense in philosophical texts. \nAt the end of the Middle Ages society and science are changing\nfundamentally (Hazard 1935; Ong 1958; Dijksterhuis 1986). In a long\ncomplex process the Aristotelian methodology of the four causes was\ntransformed to serve the needs of experimental science: \nIn this changing context the analogy of the wax-impression is\nreinterpreted. A proto-version of the modern concept of information as\nthe structure of a set or sequence of simple ideas is developed by the\nempiricists, but since the technical meaning of the term\n“information” is lost, this theory of knowledge is never\nidentified as a new “theory of information”. \nThe consequence of this shift in methodology is that only phenomena\nthat can be explained in terms of mechanical interaction between\nmaterial bodies can be studied scientifically. This implies in a\nmodern sense: the reduction of intensive properties to measurable\nextensive properties. For Galileo this insight is programmatic:  \nTo excite in us tastes, odors, and sounds I believe that nothing is\nrequired in external bodies except shapes, numbers, and slow or rapid\nmovements. (Galileo 1623 [1960: 276)\n \nThese insights later led to the doctrine of the difference between\nprimary qualities (space, shape, velocity) and secondary qualities\n(heat, taste, color etc.). In the context of philosophy of information\nGalileo’s observations on the secondary quality of\n“heat” is of particular importance since they lay the\nfoundations for the study of thermodynamics in the nineteenth century:\n \nHaving shown that many sensations which are supposed to be qualities\nresiding in external objects have no real existence save in us, and\noutside ourselves are mere names, I now say that I am inclined to\nbelieve heat to be of this character. Those materials which produce\nheat in us and make us feel warmth, which are known by the general\nname of “fire,” would then be a multitude of minute\nparticles having certain shapes and moving with certain velocities.\n(Galileo 1623 [1960: 277) \nA pivotal thinker in this transformation is René Descartes\n(1596–1650 CE). In his Meditationes, after\n“proving” that the matter (res extensa) and mind\n(res cogitans) are different substances (i.e., forms of being\nexisting independently), the question of the interaction between these\nsubstances becomes an issue. The malleability of wax is for Descartes\nan explicit argument against influence of the res extensa on\nthe res cogitans (Meditationes II, 15). The fact\nthat a piece of wax loses its form and other qualities easily when\nheated, implies that the senses are not adequate for the\nidentification of objects in the world. True knowledge thus can only\nbe reached via “inspection of the mind”. Here the wax\nmetaphor that for more than 1500 years was used to explain\nsensory impression is used to argue against the possibility\nto reach knowledge via the senses. Since the essence of the res\nextensa is extension, thinking fundamentally can not be\nunderstood as a spatial process. Descartes still uses the terms\n“form” and “idea” in the original scholastic\nnon-geometric (atemporal, aspatial) sense. An example is the short\nformal proof of God’s existence in the second answer to Mersenne\nin the Meditationes de Prima Philosophia \nI use the term idea to refer to the form of any given\nthought, immediate perception of which makes me aware of the thought.\n\n(Idea nomine intelligo cujuslibet cogitationis formam\nillam, per cujus immediatam perceptionem ipsius ejusdem cogitationis\nconscious sum)  \nI call them “ideas” says Descartes  \nonly in so far as they make a difference to the mind itself when they\ninform that part of the brain.\n\n(sed tantum quatenus mentem ipsam in illam cerebri partem\nconversam informant). (Descartes, 1641, Ad\nSecundas Objections, Rationes, Dei existentiam & anime\ndistinctionem probantes, more Geometrico dispositae.) \nBecause the res extensa and the res cogitans are\ndifferent substances, the act of thinking can never be emulated in\nspace: machines can not have the universal faculty of reason.\nDescartes gives two separate motivations: \nOf these the first is that they could never use words or other signs\narranged in such a manner as is competent to us in order to declare\nour thoughts to others: (…) The second test is, that although\nsuch machines might execute many things with equal or perhaps greater\nperfection than any of us, they would, without doubt, fail in certain\nothers from which it could be discovered that they did not act from\nknowledge, but solely from the disposition of their organs: for while\nreason is an universal instrument that is alike available on every\noccasion, these organs, on the contrary, need a particular arrangement\nfor each particular action; whence it must be morally impossible that\nthere should exist in any machine a diversity of organs sufficient to\nenable it to act in all the occurrences of life, in the way in which\nour reason enables us to act. (Discourse de la\nméthode, 1647) \nThe passage is relevant since it directly argues against the\npossibility of artificial intelligence and it even might be\ninterpreted as arguing against the possibility of a universal Turing\nmachine: reason as a universal instrument can never be emulated in\nspace. This conception is in opposition to the modern concept of\ninformation which as a measurable quantity is essentially spatial,\ni.e., extensive (but in a sense different from that of Descartes). \nDescartes does not present a new interpretation of the notions of form\nand idea, but he sets the stage for a debate about the nature of ideas\nthat evolves around two opposite positions: Rationalism: The Cartesian notion that ideas are\ninnate and thus a priori. This form of rationalism implies an\ninterpretation of the notion of ideas and forms as atemporal,\naspatial, but complex structures i.e., the idea of “a\nhorse” (i.e., with a head, body and legs). It also matches well\nwith the interpretation of the knowing subject as a created being\n(ens creatu). God created man after his own image and thus\nprovided the human mind with an adequate set of ideas to understand\nhis creation. In this theory growth, of knowledge is a priori\nlimited. Creation of new ideas ex nihilo is impossible. This\nview is difficult to reconcile with the concept of experimental\nscience. Empiricism: Concepts are constructed in the\nmind a posteriori on the basis of ideas associated with\nsensory impressions. This doctrine implies a new interpretation of the\nconcept of idea as: \nwhatsoever is the object of understanding when a man thinks …\nwhatever is meant by phantasm, notion, species, or whatever it is\nwhich the mind can be employed about when thinking. (Locke 1689, bk I,\nch 1, para 8) \nHere ideas are conceived as elementary building blocks\nof human knowledge and reflection. This fits well with the demands of\nexperimental science. The downside is that the mind can never\nformulate apodeictic truths about cause and effects and the essence of\nobserved entities, including its own identity. Human knowledge becomes\nessentially probabilistic (Locke 1689: bk I, ch. 4, para 25). \nLocke’s reinterpretation of the notion of idea as a\n“structural placeholder” for any entity present in the\nmind is an essential step in the emergence of the modern concept of\ninformation. Since these ideas are not involved in the justification\nof apodeictic knowledge, the necessity to stress the atemporal and\naspatial nature of ideas vanishes. The construction of concepts on the\nbasis of a collection of elementary ideas based in sensorial\nexperience opens the gate to a reconstruction of knowledge as an\nextensive property of an agent: more ideas implies more probable\nknowledge. \nIn the second half of the seventeenth century formal theory of\nprobability is developed by researchers like Pascal (1623–1662),\nFermat (1601 or 1606–1665) and Christiaan Huygens\n(1629–1695). The work De ratiociniis in ludo aleae of\nHuygens was translated in to English by John Arbuthnot (1692). For\nthese authors, the world was essentially mechanistic and thus\ndeterministic, probability was a quality of human knowledge caused by\nits imperfection: \nIt is impossible for a Die, with such determin’d force and\ndirection, not to fall on such determin’d side, only I\ndon’t know the force and direction which makes it fall on such\ndetermin’d side, and therefore I call it Chance, wich is nothing\nbut the want of art;… (John Arbuthnot Of the Laws of\nChance (1692), preface)  \nThis text probably influenced Hume, who was the first to marry formal\nprobability theory with theory of knowledge: \nThough there be no such thing as Chance in the world; our ignorance of\nthe real cause of any event has the same influence on the\nunderstanding, and begets a like species of belief or opinion.\n(…) If a dye were marked with one figure or number of spots on\nfour sides, and with another figure or number of spots on the two\nremaining sides, it would be more probable, that the former would turn\nup than the latter; though, if it had a thousand sides marked in the\nsame manner, and only one side different, the probability would be\nmuch higher, and our belief or expectation of the event more steady\nand secure. This process of the thought or reasoning may seem trivial\nand obvious; but to those who consider it more narrowly, it may,\nperhaps, afford matter for curious speculation. (Hume 1748: Section\nVI, “On probability” 1) \nHere knowledge about the future as a degree of belief is measured in\nterms of probability, which in its turn is explained in terms of the\nnumber of configurations a deterministic system in the world can have.\nThe basic building blocks of a modern theory of information are in\nplace. With this new concept of knowledge empiricists laid the\nfoundation for the later development of thermodynamics as a reduction\nof the secondary quality of heat to the primary qualities of\nbodies. \nAt the same time the term “information” seems to have lost\nmuch of its technical meaning in the writings of the empiricists so\nthis new development is not designated as a new interpretation of the\nnotion of “information”. Locke sometimes uses the phrase\nthat our senses “inform” us about the world and\noccasionally uses the word “information”.  \nFor what information, what knowledge, carries this proposition in it,\nviz. “Lead is a metal” to a man who knows the complex idea\nthe name lead stands for? (Locke 1689: bk IV, ch 8, para 4)  \nHume seems to use information in the same casual way when he observes:\n \nTwo objects, though perfectly resembling each other, and even\nappearing in the same place at different times, may be numerically\ndifferent: And as the power, by which one object produces another, is\nnever discoverable merely from their idea, it is evident cause and\neffect are relations, of which we receive information from experience,\nand not from any abstract reasoning or reflection. (Hume 1739: Part\nIII, section 1) \nThe empiricists methodology is not without problems. The biggest issue\nis that all knowledge becomes probabilistic and a posteriori.\nImmanuel Kant (1724–1804) was one of the first to point out that\nthe human mind has a grasp of the meta-concepts of space, time and\ncausality that itself can never be understood as the result of a mere\ncombination of “ideas”. What is more, these intuitions\nallow us to formulate scientific insights with certainty: i.e., the\nfact that the sum of the angles of a triangle in Euclidean space is\n180 degrees. This issue cannot be explained in the empirical\nframework. If knowledge is created by means of combination of ideas\nthen there must exist an a priori synthesis of ideas in the\nhuman mind. According to Kant, this implies that the human mind can\nevaluate its own capability to formulate scientific judgments. In his\nKritik der reinen Vernunft (1781) Kant developed\ntranscendental philosophy as an investigation of the necessary\nconditions of human knowledge. Although Kant’s transcendental\nprogram did not contribute directly to the development of the concept\nof information, he did influence research in to the foundations of\nmathematics and knowledge relevant for this subject in the nineteenth\nand twentieth century: e.g., the work of Frege, Husserl, Russell,\nBrouwer, L. Wittgenstein, Gödel, Carnap, Popper and Quine. \nThe history of the term “information” is intricately\nrelated to the study of central problems in epistemology and ontology\nin Western philosophy. After a start as a technical term in classical\nand medieval texts the term “information” almost vanished\nfrom the philosophical discourse in modern philosophy, but gained\npopularity in colloquial speech. Gradually the term obtained the\nstatus of an abstract mass-noun, a meaning that is orthogonal to the\nclassical process-oriented meaning. In this form it was picked up by\nseveral researchers (Fisher 1925; Shannon 1948) in the twentieth\ncentury who introduced formal methods to measure\n“information”. This, in its turn, lead to a revival of the\nphilosophical interest in the concept of information. This complex\nhistory seems to be one of the main reasons for the difficulties in\nformulating a definition of a unified concept of information that\nsatisfies all our intuitions. At least three different meanings of the\nword “information” are historically relevant: “Information” as the process of being\ninformed.\nThis is the oldest meaning one finds in the writings of authors\nlike Cicero (106–43 BCE) and Augustine (354–430 CE) and it\nis lost in the modern discourse, although the association of\ninformation with processes (i.e., computing, flowing or sending a\nmessage) still exists. In classical philosophy one could say that when\nI recognize a horse as such, then the “form” of a horse is\nplanted in my mind. This process is my “information” of\nthe nature of the horse. Also the act of teaching could be referred to\nas the “information” of a pupil. In the same sense one\ncould say that a sculptor creates a sculpture by\n“informing” a piece of marble. The task of the sculptor is\nthe “information” of the statue (Capurro &\nHjørland 2003). This process-oriented meaning survived quite\nlong in western European discourse: even in the eighteenth century\nRobinson Crusoe could refer to the education of his servant Friday as\nhis “information” (Defoe 1719: 261). It is also used in\nthis sense by Berkeley: “I love information upon all subjects\nthat come in my way, and especially upon those that are most\nimportant” (Alciphron Dialogue 1, Section 5, Paragraph\n6/10, see Berkeley 1732). “Information” as a state of an\nagent,\ni.e., as the result of the process of being informed. If one\nteaches a pupil the theorem of Pythagoras then, after this process is\ncompleted, the student can be said to “have the information\nabout the theorem of Pythagoras”. In this sense the term\n“information” is the result of the same suspect form of\nsubstantiation of a verb (informare \\(\\gt\\)\ninformatio) as many other technical terms in philosophy\n(substance, consciousness, subject, object). This sort of\nterm-formation is notorious for the conceptual difficulties it\ngenerates. Can one derive the fact that I “have”\nconsciousness from the fact that I am conscious? Can one derive the\nfact that I “have” information from the fact that I have\nbeen informed? The transformation to this modern substantiated meaning\nseems to have been gradual and seems to have been general in Western\nEurope at least from the middle of the fifteenth century. In the\nrenaissance a scholar could be referred to as “a man of\ninformation”, much in the same way as we now could say that\nsomeone received an education (Adriaans & van Benthem 2008b;\nCapurro & Hjørland 2003). In “Emma” by Jane\nAusten one can read: “Mr. Martin, I suppose, is not a man of\ninformation beyond the line of his own business. He does not\nread” (Austen 1815: 21). “Information” as the disposition to\ninform,\ni.e., as a capacity of an object to inform an agent. When the act\nof teaching me Pythagoras’ theorem leaves me with information\nabout this theorem, it is only natural to assume that a text in which\nthe theorem is explained actually “contains” this\ninformation. The text has the capacity to inform me when I read it. In\nthe same sense, when I have received information from a teacher, I am\ncapable of transmitting this information to another student. Thus\ninformation becomes something that can be stored and measured. This\nlast concept of information as an abstract mass-noun has gathered wide\nacceptance in modern society and has found its definitive form in the\nnineteenth century, allowing Sherlock Homes to make the following\nobservation: “… friend Lestrade held information in his\nhands the value of which he did not himself know” (“The\nAdventure of the Noble Bachelor”, Conan Doyle 1892). The\nassociation with the technical philosophical notions like\n“form” and “informing” has vanished from the\ngeneral consciousness although the association between information and\nprocesses like storing, gathering, computing and teaching still\nexist. \nWith hindsight many notions that have to do with optimal code systems,\nideal languages and the association between computing and processing\nlanguage have been recurrent themes in the philosophical reflection\nsince the seventeenth century. \nOne of the most elaborate proposals for a universal\n“philosophical” language was made by bishop John Wilkins (Maat 2004):\n“An Essay towards a Real Character, and a Philosophical\nLanguage” (1668). Wilkins’ project consisted of an\nelaborate system of symbols that supposedly were associated with\nunambiguous concepts in reality. Proposals such as these made\nphilosophers sensitive to the deep connections between language and\nthought. The empiricist methodology made it possible to conceive the\ndevelopment of language as a system of conventional signs in terms of\nassociations between ideas in the human mind. The issue that currently\nis known as the symbol grounding problem (how do arbitrary\nsigns acquire their inter-subjective meaning) was one of the most\nheavily debated questions in the eighteenth century in the context of\nthe problem of the origin of languages. Diverse thinkers as Vico,\nCondillac, Rousseau, Diderot, Herder and Haman made contributions. The\ncentral question was whether language was given a priori (by\nGod) or whether it was constructed and hence an invention of man\nhimself. Typical was the contest issued by the Royal Prussian Academy\nof Sciences in 1769: \nEn supposant les hommes abandonnés à leurs\nfacultés naturelles, sont-ils en état d’inventer\nle langage? Et par quels moyens parviendront-ils\nd’eux-mêmes à cette invention?  \nAssuming men abandoned to their natural faculties, are they able to\ninvent language and by what means will they come to this\n invention?[1] \nThe controversy raged on for over a century without any conclusion and\nin 1866 the Linguistic Society of Paris (Société de\nLinguistique de Paris) banished the issue from its arena.\n [2] \nPhilosophically more relevant is the work of Leibniz (1646–1716)\non a so-called characteristica universalis: the notion of a\nuniversal logical calculus that would be the perfect vehicle for\nscientific reasoning. A central presupposition in Leibniz’\nphilosophy is that such a perfect language of science is in principle\npossible because of the perfect nature of the world as God’s\ncreation (ratio essendi = ration cognoscendi, the\norigin of being is the origin of knowing). This principle was rejected\nby Wolff (1679–1754) who suggested more heuristically oriented\ncharacteristica combinatoria (van Peursen 1987). These ideas\nhad to wait for thinkers like Boole (1854, An Investigation of the\nLaws of Thought), Frege (1879, Begriffsschrift), Peirce\n(who in 1886 already suggested that electrical circuits could be used\nto process logical operations) and Whitehead and Russell\n(1910–1913, Principia Mathematica) to find a more\nfruitful treatment. \nThe fact that frequencies of letters vary in a language was known\nsince the invention of book printing. Printers needed many more\n“e”s and “t”s than “x”s or\n“q”s to typeset an English text. This knowledge was used\nextensively to decode ciphers since the seventeenth century (Kahn\n1967; Singh 1999). In 1844 an assistant of Samuel Morse, Alfred Vail,\ndetermined the frequency of letters used in a local newspaper in\nMorristown, New Jersey, and used them to optimize Morse code. Thus the\ncore of theory of optimal codes was already established long before\nShannon developed its mathematical foundation (Shannon 1948; Shannon\n& Weaver 1949). Historically important but philosophically less\nrelevant are the efforts of Charles Babbage to construct computing\nmachines (Difference Engine in 1821, and the Analytical Engine\n1834–1871) and the attempt of Ada Lovelace (1815–1852) to\ndesign what is considered to be the first programming language for the\nAnalytical Engine. \nThe simplest way of representing numbers is via a unary\nsystem. Here the length of the representation of a number is\nequal to the size of the number itself, i.e., the number\n“ten” is represented as “\\\\\\\\\\\\\\\\\\\\”. The\nclassical Roman number system is an improvement since it contains\ndifferent symbols for different orders of magnitude (one = I, ten = X,\nhundred = C, thousand = M). This system has enormous drawbacks since\nin principle one needs an infinite amount of symbols to code the\nnatural numbers and because of this the same mathematical operations\n(adding, multiplication etc.) take different forms at different orders\nof magnitude. Around 500 CE the number zero was invented in India.\nUsing zero as a placeholder we can code an infinity of numbers with a\nfinite set of symbols (one = I, ten = 10, hundred = 100, thousand =\n1000 etc.). From a modern perspective an infinite number of position\nsystems is possible as long as we have 0 as a placeholder and a finite\nnumber of other symbols. Our normal decimal number system has ten\ndigits “0, 1, 2, 3, 4, 5, 6, 7, 8, 9” and represents the\nnumber two-hundred-and-fifty-five as “255”. In a binary\nnumber system we only have the symbols “0” and\n“1”. Here two-hundred-and-fifty-five is represented as\n“11111111”. In a hexadecimal system with 16 symbols (0, 1,\n2, 3, 4, 5, 6, 7, 8, 9, a, b, c, d, e, f) the same number can be\nwritten as “ff”. Note that the length of these\nrepresentations differs considerable. Using this representation,\nmathematical operations can be standardized irrespective of the order\nof magnitude of numbers we are dealing with, i.e., the possibility of\na uniform algorithmic treatment of mathematical functions (addition,\nsubtraction, multiplication and division etc.) is associated with such\na position system. \nThe concept of a positional number system was brought to Europe by the\nPersian mathematician al-Khwarizmi (ca. 780–ca. 850 CE). His\nmain work on numbers (ca. 820 CE) was translated into Latin as\nLiber Algebrae et Almucabola in the twelfth century, which\ngave us amongst other things the term “algebra”. Our word\n“algorithm” is derived from Algoritmi, the Latin\nform of his name. Positional number systems simplified commercial and\nscientific calculations. \nIn 1544 Michael Stifel introduced the concept of the exponent of a\nnumber in Arithmetica integra (1544). Thus 8 can be written\nas \\(2^3\\) and 25 as \\(5^2\\). The notion of an exponent immediately\nsuggests the notion of a logarithm as its inverse function: \\(\\log_b\nb^a) = a\\). Stifel compared the arithmetic sequence: \nin which the term 1 have a difference of 1 with the geometric\nsequence: \nin which the terms have a ratio of 2. The exponent notation allowed\nhim to rewrite the values of the second table as: \nwhich combines the two tables. This arguably was the first logarithmic\ntable. A more definitive and practical theory of logarithms is\ndeveloped by John Napier (1550–1617) in his main work (Napier\n1614). He coined the term logarithm (logos + arithmetic: ratio of\nnumbers). As is clear from the match between arithmetic and geometric\nprogressions, logarithms reduce products to sums: \nThey also reduce divisions to differences: \nand powers to products:  \nAfter publication of the logarithmic tables by Briggs (1624) this new\ntechnique of facilitating complex calculations rapidly gained\npopularity. \nGalileo (1623) already had suggested that the analysis of phenomena\nlike heat and pressure could be reduced to the study of movements of\nelementary particles. Within the empirical methodology this could be\nconceived as the question how the sensory experience of the secondary\nquality of heat of an object or a gas could be reduced to movements of\nparticles. Bernoulli (Hydrodynamica published in 1738) was\nthe first to develop a kinetic theory of gases in which\nmacroscopically observable phenomena are described in terms of\nmicrostates of systems of particles that obey the laws of Newtonian\nmechanics, but it was quite an intellectual effort to come up with an\nadequate mathematical treatment. Clausius (1850) made a conclusive\nstep when he introduced the notion of the mean free path of a particle\nbetween two collisions. This opened the way for a statistical\ntreatment by Maxwell who formulated his distribution in 1857, which\nwas the first statistical law in physics. The definitive formula that\ntied all notions together (and that is engraved on his tombstone,\nthough the actual formula is due to Planck) was developed by\nBoltzmann: \nIt describes the entropy S of a system in terms of the\nlogarithm of the number of possible microstates W, consistent\nwith the observable macroscopic states of the system, where k\nis the well-known Boltzmann constant. In all its simplicity the value\nof this formula for modern science can hardly be overestimated. The\nexpression “\\(\\log W\\)” can, from the perspective of\ninformation theory, be interpreted in various ways: \nThus it connects the additive nature of logarithm with the extensive\nqualities of entropy, probability, typicality and information and it\nis a fundamental step in the use of mathematics to analyze nature.\nLater Gibbs (1906) refined the formula: \nwhere \\(p_i\\) is the probability that the system is in the\n\\(i^{\\textrm{th}}\\) microstate. This formula was adopted by Shannon\n(1948; Shannon & Weaver 1949) to characterize the communication\nentropy of a system of messages. Although there is a close connection\nbetween the mathematical treatment of entropy and information, the\nexact interpretation of this fact has been a source of controversy\never since (Harremoës & Topsøe 2008; Bais & Farmer\n2008). \nThe modern theories of information emerged in the middle of the\ntwentieth century in a specific intellectual climate in which the\ndistance between the sciences and parts of academic philosophy was\nquite big. Some philosophers displayed a specific anti-scientific\nattitude: Heidegger, “Die Wissenschaft denkt\nnicht.” On the other hand the philosophers from the Wiener\nKreis overtly discredited traditional philosophy as dealing with\nillusionary problems (Carnap 1928). The research program of logical\npositivism was a rigorous reconstruction of philosophy based on a\ncombination of empiricism and the recent advances in logic. It is\nperhaps because of this intellectual climate that early important\ndevelopments in the theory of information took place in isolation from\nmainstream philosophical reflection. A landmark is the work of Dretske\nin the early eighties (Dretske 1981). Since the turn of the century,\ninterest in Philosophy of Information has grown considerably, largely\nunder the influence of the work of Luciano Floridi on semantic\ninformation. Also the rapid theoretical development of quantum\ncomputing and the associated notion of quantum information have had it\nrepercussions on philosophical reflection. \nThe research program of logical positivism of the Wiener Kreis in the\nfirst half of the twentieth century revitalized the older project of\nempiricism. Its ambition was to reconstruct scientific knowledge on\nthe basis of direct observations and logical relation between\nstatements about those observations. The old criticism of Kant on\nempiricism was revitalized by Quine (1951). Within the framework of\nlogical positivism induction was invalid and causation could never be\nestablished objectively. In his Logik der Forschung (1934)\nPopper formulates his well-known demarcation criterion and he\npositions this explicitly as a solution to Hume’s problem of\ninduction (Popper 1934 [1977: 42]). Scientific theories formulated as\ngeneral laws can never be verified definitively, but they can be\nfalsified by only one observation. This implies that a theory is\n“more” scientific if it is richer and provides more\nopportunity to be falsified: \nThus it can be said that the amount of empirical information conveyed\nby a theory, or its empirical content, increases with its\ndegree of falsifiability. (Popper 1934 [1977: 113], emphasis in\noriginal)  \nThis quote, in the context of Popper’s research program, shows\nthat the ambition to measure the amount of empirical information\nin scientific theory conceived as a set of logical statements was\nalready recognized as a philosophical problem more than a decade\nbefore Shannon formulated his theory of information. Popper is aware\nof the fact that the empirical content of a theory is related to its\nfalsifiability and that this in its turn has a relation with the\nprobability of the statements in the theory. Theories with more\nempirical information are less probable. Popper distinguishes\nlogical probability from numerical probability\n(“which is employed in the theory of games and chance, and in\nstatistics”; Popper 1934 [1977: 119]). In a passage that is\nprogrammatic for the later development of the concept of information\nhe defines the notion of logical probability: \nThe logical probability of a statement is complementary to its\nfalsifiability: it increases with decreasing degree of\nfalsifiability. The logical probability 1 corresponds to the degree 0\nof falsifiability and vice versa. (Popper 1934 [1977: 119],\nemphasis in original)  \nIt is possible to interpret numerical probability as applying to a\nsubsequence (picked out from the logical probability relation) for\nwhich a system of measurement can be defined, on the basis of\nfrequency estimates. (Popper 1934 [1977: 119], emphasis in original)\n \nPopper never succeeded in formulating a good formal theory to measure\nthis amount of information although in later writings he suggests that\nShannon’s theory of information might be useful (Popper 1934\n[1977], 404 [Appendix IX, from 1954]).\nThese issues were later developed\nin philosophy of science. Theory of conformation studies induction\ntheory and the way in which evidence “supports” a certain\ntheory (Huber 2007\n [OIR]).\n Although the work of Carnap motivated important developments in both\nphilosophy of science and philosophy of information the connection\nbetween the two disciplines seems to have been lost. There is no\nmention of information theory or any of the more foundational work in\nphilosophy of information in Kuipers (2007a), but the two disciplines\ncertainly have overlapping domains. (See, e.g., the discussion of the\nso-called Black Ravens Paradox by Kuipers (2007b) and Rathmanner &\nHutter (2011).)  \nIn two landmark papers Shannon (1948; Shannon & Weaver 1949)\ncharacterized the communication entropy of a system of messages\nA: \nHere \\(p_i\\) is the probability of message i in A. This\nis exactly the formula for Gibb’s entropy in physics. The use of\nbase-2 logarithms ensures that the code length is measured in bits\n(binary digits). It is easily seen that the communication entropy of a\nsystem is maximal when all the messages have equal probability and\nthus are typical.  \nThe amount of information I in an individual message x\nis given by:  \nThis formula, that can be interpreted as the inverse of the Boltzmann\nentropy, covers a number of our basic intuitions about\ninformation: \nInformation as the negative log of the probability is the only\nmathematical function that exactly fulfills these constraints (Cover\n& Thomas 2006). Shannon offers a theoretical framework in which\nbinary strings can be interpreted as words in a (programming) language\ncontaining a certain amount of information (see\n 3.1 Languages).\n The expression \\(-\\log p_x\\) exactly gives the length of an optimal\ncode for message x and as such formalizes the old intuition\nthat codes are more efficient when frequent letters get shorter\nrepresentations (see\n 3.2 Optimal codes).\n Logarithms as a reduction of multiplication to addition (see\n 3.3 Numbers)\n are a natural representation of extensive properties of systems and\nalready as such had been used by physicists in the nineteenth century\n(see\n 3.4 Physics). \nOne aspect of information that Shannon’s definition explicitly\ndoes not cover is the actual content of the messages interpreted as\npropositions. So the statement “Jesus was Caesar” and\n“The moon is made of green cheese” may carry the same\namount of information while their meaning is totally different. A\nlarge part of the effort in philosophy of information has been\ndirected to the formulation of more semantic theories of information\n(Bar-Hillel & Carnap 1953; Floridi 2002, 2003, 2011). Although\nShannon’s proposals at first were almost completely ignored by\nphilosophers it has in the past decennia become apparent that their\nimpact on philosophical issues is big. Dretske (1981) was one of the\nfirst to analyze the philosophical implications of Shannon’s\ntheory, but the exact relation between various systems of logic and\ntheory of information are still unclear (see\n6.6 Logic and Semantic Information).  \nThis problem of relating a set of statements to a set of observations\nand defining the corresponding probability was taken up by Carnap\n(1945, 1950). He distinguished two forms of probability:\nProbability\\(_1\\) or “degree of confirmation” \\(P_1 (h ;\ne)\\) is a logical relation between two sentences, a\nhypothesis h and a sentence e reporting a series of\nobservations. Statements of this type are either analytical or\ncontradictory. The second form, Probability\\(_2\\) or “relative\nfrequency”, is the statistical concept. In the words of his\nstudent Solomonoff (1997): \nCarnap’s model of probability started with a long sequence of\nsymbols that was a description of the entire universe. Through his own\nformal linguistic analysis, he was able to assign a priori\nprobabilities to any possible string of symbols that might represent\nthe universe. \nThe method for assigning probabilities Carnap used, was not universal\nand depended heavily on the code systems used. A general theory of\ninduction using Bayes’ rule can only be developed when we can\nassign a universal probability to “any possible string” of\nsymbols. In a paper in 1960 Solomonoff (1960, 1964a,b) was the first\nto sketch an outline of a solution for this problem. He formulated the\nnotion of what is now called a universal probability\ndistribution: consider the set of all possible finite strings to\nbe programs for a universal Turing machine U and define the\nprobability of a string x of symbols in terms of the length of\nthe shortest program p that outputs x on U. \nThis notion of Algorithmic Information Theory was invented\nindependently somewhat later separately by Kolmogorov (1965) and\nChaitin (1969). Levin (1974) developed a mathematical expression of\nthe universal a priori probability as a universal (that is,\nmaximal) lower semicomputable semimeasure M, and showed that\nthe negative logarithm of \\(M(x)\\) coincides with the Kolmogorov\ncomplexity of x up to an additive logarithmic term. The actual\ndefinition of the complexity measure is:  \nKolmogorov complexity The algorithmic complexity of a\nstring x is the length \\(\\cal{l}(p)\\) of the smallest program\np that produces x when it runs on a universal Turing\nmachine U, noted as \\(U(p)=x\\):  \nAlgorithmic Information Theory (a.k.a. Kolmogorov complexity theory)\nhas developed into a rich field of research with a wide range of\ndomains of applications many of which are philosophically relevant (Li\n& Vitányi 2019): \nThere are also down-sides: \nLevin complexity The Levin complexity of a string\nx is the sum of the length \\(\\cal{l}(p)\\) and the logarithm of\nthe computation time of the smallest program p that produces\nx when it runs on a universal Turing machine U, noted as\n\\(U(p)=x\\):  \nAlgorithmic Information Theory has gained rapid acceptance as a\nfundamental theory of information. The well-known introduction in\nInformation Theory by Cover and Thomas (2006) states:\n“… we consider Kolmogorov complexity (i.e., AIT) to be\nmore fundamental than Shannon entropy” (2006: 3). \nThe idea that algorithmic complexity theory is a foundation for a\ngeneral theory of artificial intelligence (and theory of knowledge)\nhas already been suggested by Solomonoff (1997) and Chaitin (1987).\nSeveral authors have defended that data compression is a general\nprinciple that governs human cognition (Chater & Vitányi\n2003; Wolff 2006). Hutter (2005, 2007a,b) argues that Solomonoff’s formal and\ncomplete theory essentially solves the induction problem. Hutter\n(2007a) and Rathmanner & Hutter (2011) enumerate a plethora of\nclassical philosophical and statistical problems around induction and\nclaim that Solomonoff’s theory solves or avoids all these\nproblems. Probably because of its technical nature, the theory has\nbeen largely ignored by the philosophical community. Yet, it stands\nout as one of the most fundamental contributions to information theory\nin the twentieth century and it is clearly relevant for a number of\nphilosophical issues, such as the problem of induction. \nIn a mathematical sense information is associated with measuring\nextensive properties of classes of systems with finite but unlimited\ndimensions (systems of particles, texts, codes, networks, graphs,\ngames etc.). This suggests that a uniform treatment of various\ntheories of information is possible. In the Handbook of Philosophy of\nInformation three different forms of information are distinguished\n(Adriaans & van Benthem 2008b): Information-A:\nKnowledge, logic, what is conveyed in informative answers Information-B:\nProbabilistic, information-theoretic, measured quantitatively Information-C:\nAlgorithmic, code compression, measured quantitatively \nBecause of recent development the connections between Information-B\n(Shannon) and Information-C (Kolmogorov) are reasonably well\nunderstood (Cover & Thomas 2006). The historical material\npresented in this article suggests that reflection on Information-A\n(logic, knowledge) is historically much more interwoven than was\ngenerally known up till now. The research program of logical\npositivism can with hindsight be characterized as the attempt to marry\na possible worlds interpretation of logic with probabilistic reasoning\n(Carnap 1945, 1950; Popper 1934; for a recent approach see Hutter et\nal. 2013). Modern attempt to design a Bayesian epistemology (Bovens\n& Hartmann 2003) do not seem to be aware of the work done in the\nfirst half of the twentieth century. However, an attempt to unify\nInformation-A and Information-B seems a viable exercise. Also the\nconnection between thermodynamics and information theory have become\nmuch closer, amongst others, due to the work of Gell-Mann & Lloyd\n(2003) (see also: Bais and Farmer 2008). Verlinde (2011, 2017) even\npresented a reduction of gravity to information (see the entry on\n information processing and thermodynamic entropy).\n  \nWith respect to the main definitions of the concept of information,\nlike Shannon Information, Kolmogorov complexity, semantic information\nand quantum information, a unifying approach to a philosophy of\ninformation is possible, when we interpret it as an extension to the\nphilosophy of mathematics. The answer to questions like “What is\ndata?” and “What is information?” then evolves from\none’s answer to the related questions like “What is a\nset?” and “What is a number?” With hindsight one can\nobserve that many open problems in the philosophy of mathematics\nrevolve around the notion of information.  \nIf we look at the foundations of information and computation there are\ntwo notions that are crucial: the concept of a data set and the\nconcept of an algorithm. Once we accept these notions as fundamental\nthe rest of the theory data and computation unfolds quite naturally.\nOne can “plug in” one’s favorite epistemological or\nmetaphysical stance here, but this does not really affect foundational\nissues in the philosophy of computation and information. One might\nsustain a Formalist, Platonic or intuitionistic view of the\nmathematical universe (see entry on\n philosophy of mathematics)\n and still agree on the basic notion of what effective computation is.\nThe theory of computing, because of its finitistic and constructivist\nnature, seems to live more or less on the common ground in which these\ntheories overlap.  \nInformation as a scientific concept emerges naturally in the context\nof our every day dealing with nature when we measure things. Examples\nare ordinary actions like measuring the size of an object with a\nstick, counting using our fingers, drawing a straight line using a\npiece of rope. These processes are the anchor points of abstract\nconcepts like length, distance, number, straight line that form the\nbuilding blocks of science. The fact that these concepts are rooted in\nour concrete experience of reality guarantees their applicability and\nusefulness. The earliest traces of information processing evolved\naround the notions of counting, administration and accountancy. \nExample: Tally sticks\n\nOne of the most elementary information measuring devices is unary\ncounting using a tally stick. Tally sticks were already used\naround 20,000 years ago. When a hypothetical prehistoric hunter killed\na deer he could have registered this fact by making a scratch\n“|” on a piece of wood. Every stroke on such a stick\nrepresents an object/item/event.  The process of unary counting is\nbased on the elementary operation of\ncatenation of symbols into sequences. This measuring\nmethod illustrates a primitive version of the concept of\nextensiveness of information: the length of the sequences is\na measure for the amount of items counted. Note that such a sequential\nprocess of counting is non-commutative and non-associative. If\n“|” is our basic symbol and \\(\\oplus\\) our concatenation\noperator then a sequence of signs has the form:  \nA new symbol is always concatenated at the end of the sequence.  \nThis example helps to understand the importance of context in\nthe analysis of information. In itself a scratch on a stick may have\nno meaning at all, but as soon as we decide that such a scratch\nrepresents another object or event it becomes a\nmeaningful symbol. When we manipulate it in such a context we\nprocess information. In principle a simple scratch can represent any\nevent or object we like: symbols are conventional. \nDefinition: A symbol is a mark, sign or word\nthat indicates, signifies, or is understood as representing an idea,\nobject, or relationship.\n  \nSymbols are the semantic anchors by which symbol manipulating systems\nare tied to the world. Observe that the meta-statement:  \nThe symbol “|” signifies object y.  \nif true, specifies semantic information: \nSymbol manipulation can take many forms and is not restricted to\nsequences. Many examples of different forms of information processing\ncan be found in prehistoric times.  \nExample: Counting sheep in Mesopotamia\n\nWith the process of urbanization, early accounting systems emerged in\nMesopotamia around 8000 BCE using clay tokens to administer cattle\n(Schmandt-Besserat 1992). Different shaped tokens were used for\ndifferent types of animals, e.g., sheep and goats. After the\nregistration the tokens were packed in a globular clay container, with\nmarks representing their content on the outside. The container was\nbaked to make the registration permanent. Thus early forms of writing\nemerged. After 4000 BCE the tokens were mounted on a string to\npreserve the order.  \nThe historical transformation from sets to strings is important. It is\na more sophisticated form of coding of information. Formally we can\ndistinguish several levels of complexity of token combination: \nSequences of symbols code more information than multisets and\nmultisets are more expressive than sets. Thus the emergence of writing\nitself can be seen as a quest to find the most expressive\nrepresentation of administrative data. When measuring information in\nsequences of messages it is important to distinguish the aspects of\nrepetition, order and grouping. The\nextensive aspects of information can be studied in terms of such\nstructural operations (see entry on\n substructural logics).\n We can study sets of messages in terms of operators defined on\nsequences of symbols.  \nDefinition: Suppose m, n, o,\np, … are symbols and \\(\\oplus\\) is a tensor or\nconcatentation operator. We define the class of sequences:\n \nObservation: \nSystems of sequences with contraction, commutativity and associativity\nbehave like sets. Consider the equation:  \nWhen we model the sets as two sequences \\((p \\oplus q)\\) and \\((p\n\\oplus r)\\), the corresponding implication is:  \nProof: \nThe structural aspects of sets, multisets and strings can be\nformulated in terms of these properties:  Sets:  \nSequences of messages collapse into sets under contraction,\ncommutativity and associativity. A set is a\ncollection of objects in which each element occurs only once:  \nand for which order is not relevant: \nSets are associated with our normal everyday naive concept of\ninformation as new, previously unknown, information. We only\nupdate our set if we get a message we have not seen previously. This\nnotion of information is forgetful both with respect to\nsequence and frequency. The set of messages cannot be reconstructed.\nThis behavior is associated with the notion of extensionality\nof sets: we are only interested in equality of elements, not in\nfrequency. Multisets:  \nSequences of messages collapse into multisets under\ncommutativity and associativity. A multiset is a\ncollection of objects in which the same element can occur multiple\ntimes \nand for which order is not relevant: \nMultisets are associated with a resource sensitive concept of\ninformation defined in Shannon Information. We are\ninterested in the frequency of the messages. This concept is\nforgetful with regards to sequence. We update our set every\ntime we get a message, but we forget the structure of the sequence.\nThis behavior is associated with the notion of extensiveness\nof information: we are both interested in equality of elements, and in\nfrequency. Sequences:  \nSequences are associative. Sequences are ordered multisets: \\(aba \\neq\nbaa\\). The whole structure of the sequence of a message is stored.\nSequences are associated with Kolmogorov complexity defined\nas the length of a sequence of symbols.  \nSets may be interpreted as spaces in which objects can move freely.\nWhen the same objects are in each others vicinity they collapse in to\none object. Multisets can be interpreted as spaces in which objects\ncan move freely, with the constraint that the total number of objects\nstays constant. This is the standard notion of extensiveness: the\ntotal volume of a space stays constant, but the internal structure may\ndiffer. Sequences may be interpreted as spaces in which objects have a\nfixed location. In general a sequence contains more information than\nthe derived multiset, which contains more information than the\nassociated set. \nObservation: The interplay between the notion of\nsequences and multisets can be interpreted as a formalisation of the\nmalleability of a piece of wax that pervades history of\nphilosophy as the paradigm of information. Different sequences (forms)\nare representations of the same multiset (matter). The volume of the\npiece of wax (length of the string) is constant and thus a measure for\nthe amount of information that can be represented in the wax (i.e.in\nthe sequence of symbols). In terms of quantum physics the stability of\nthe piece of wax seems to be an emergent property: the statistical\ninstability of objects on an atomic level seem to even out when large\nquantities of them are manipulated.  \nThe notion of a set in mathematics is considered to be fundamental.\nAny identifiable collection of discrete objects can be considered to\nbe a set. The relation between theory of sets and the concept of\ninformation becomes clear when we analyze the basic statement:  \nWhich reads the object e is an element of the set A.\nObserve that this statement, if true, represents a piece of semantic\ninformation. It is wellformed, meaningful and truthful. (see entry on\n semantic conceptions of information)\n The concept of information is already at play in the basic building\nblocks of mathematics.The philosophical question “What are\nsets?” the answer to the ti esti question, is\ndetermined implicitly by the Zermelo-Fraenkel axioms (see\nentry on\n set theory),\n the first of which is that of extensionality:  \nTwo sets are equal if they have the same elements.  \nThe idea that mathematical concepts are defined implicitly by a set of\naxioms was proposed by Hilbert but is not uncontroversial (see entry\non the\n Frege-Hilbert controversy).\n The fact that the definition is implicit entails that we only have\nexamples of what sets are without the possibility to\nformulate any positive predicate that defines them. Elements of a set\nare not necessarily physical, nor abstract, nor spatial or temporal,\nnor simple, nor real. The only prerequisite is the possibility to\nformulate clear judgments about membership. This implicit definition\nof the notion of a set is not unproblematic. We might define objects\nthat at first glance seem to be proper sets, which after scrutiny\nappear to be internally inconsistent. This is the basis for:  \nRussell’s paradox:\n This paradox, which motivated a lot of research into the foundations\nof mathematics, is a variant of the liars paradox attributed to the\nCretan philosopher Epeimenides (ca. 6 BCE) who apparently stated that\nCretans always lie. The crux of these paradoxes lies in the\ncombination of the notions of: Universality,\nNegation, and Self-reference. \nAny person who is not Cretan can state that all Cretans always lie.\nFor a Cretan this is not possible because of the universal negative\nself-referential nature of the statement. If the statement is true, he\nis not lying which makes the statement untrue: a real paradox based on\nself contradiction. Along the same lines Russel coined the concept of\nthe set of all sets that are not member of themselves, for\nwhich membership cannot be determined. Apparently the set of all\nsets is an inadmissible object within set theory. In general\nthere is in philosophy and mathematics a limit to the extent in which\na system can verify statements about itself within the system.\n(For further discussion, see the entry on\n Russell’s paradox.)\n \nThe implicit definition of the concepts of sets, entails that the\nclass is essentially open itself. There are mathematical\ndefinitions of objects of which it is unclear or highly controversial\nwhether they define a set or not.  \nModern philosophy of mathematics starts with the Frege-Russell theory\nof numbers (Frege 1879, 1892, Goodstein 1957, see entry on\n alternative axiomatic set theories)\n in terms of sets. If we accept the notion of a class of objects as\nvalid and fundamental, together with the notion of a one-to-one\ncorrespondence between classes of objects, then we can define numbers\nas sets of equinumerous classes.  \nDefinition: Two sets Aand B are\nequinumerous, \\(A \\sim B\\), if there exists a one-to-one\ncorrespondence between them, i.e., a function \\(f: A \\rightarrow B\\)\nsuch that for every \\(a \\in A\\) there is exactly one \\(f(a) \\in B\\).\n \nAny set of, say four, objects then becomes a representation of the\nnumber 4 and for any other set of objects we can establish membership\nto the equivalence class defining the number 4 by defining a one to\none correspondence to our example set. \nDefinition: If A is a finite set, then\n\\(\\mathcal{S}_A = \\{X \\mid X \\sim A \\}\\) is the class of all sets\nequinumerous with A. The associated generalization\noperation is the cardinality function: \\(|A|\n=\\mathcal{S}_A = \\{X \\mid X \\sim A \\} = n\\). This defines a natural\nnumber \\(|A|= n \\in \\mathbb{N}\\) associated with the set\nA.  \nWe can reconstruct large parts of the mathematical universe by\nselecting appropriate mathematical example objects to populate it,\nbeginning with the assumption that there is a single unique empty set\n\\(\\emptyset\\) which represents the number 0. This gives us the\nexistence of a set with only one member \\(\\{\\varnothing\\}\\) to\nrepresent the number 1 and repeating this construction,\n\\(\\{\\varnothing,\\{\\varnothing\\}\\}\\) for 2, the whole set of natural\nnumbers \\(\\mathbb{N}\\) emerges. Elementary arithmetic then is defined\non the basis of Peano’s axioms:\n  \nThe fragment of the mathematical universe that emerges is relatively\nuncontroversial and both Platonists and constructivists might agree on\nits basic merits. On the basis of Peano’s axioms we can define\nmore complex functions like addition and multiplication which are\nclosed on \\(\\mathbb{N}\\) and the inverse functions, subtraction and\ndivision, which are not closed and lead to the set of whole numbers\n\\(\\mathbb{Z}\\) and the rational numbers \\(\\mathbb{Q}\\).  \nWe can define the concept of information for a number n by\nmeans of an unspecified function \\(I(n)\\). We observe that addition\nand multiplication specify multisets: both are\nnon-contractive and commutative and\nassociative. Suppose we interpret the tensor operator\n\\(\\oplus\\) as multiplication \\(\\times\\). It is natural to define the\nsemantics for \\(I(m \\times n)\\) in terms of addition. If we\nget both messages m and n, the total amount of\ninformation in the combined messages is the sum of the amount of\ninformation in the individual messages. This leads to the following\nconstraints:  \nDefinition: Additivity Constraint:  \nFurthermore we want bigger numbers to contain more information than\nsmaller ones, which gives a:  \nDefinition: Monotonicity Constraint:  \nWe also want to select a certain number a as our basic unit\nof measurement: \nDefinition: Normalization Constraint:  \nThe following theorem is due to Rényi (1961): \nTheorem: The Logarithm is the only mathematical\noperation that satisfies Additivity, Monotonicity and Normalisation.\n \nObservation: The logarithm \\(\\log_a n\\) of a number\nn characterizes our intuitions about the concept of information\nin a number n exactly. When we decide that 1)\nmultisets are the right formalisation of the notion of extensiveness,\nand 2) multiplication is the right operation to express additivity,\nthen the logarithm is the only measurement function that satisfies our\nconstraints.  \nWe define:  \nDefinition: For all natural numbers \\(n \\in\n\\mathbb{N}^{+}\\) \nFor finite sets we can now specify the amount of information we get\nwhen we know a certain element of a set conditional to knowing the set\nas a whole. \nDefinition: Suppose S is a finite set and we\nhave:  \nthen,  \ni.e., the log of the cardinality of the set.  \nThe bigger the set, the harder the search is, the more information we\nget when we find what we are looking for. Conversely, without any\nfurther information the probability of selecting a certain\nelement of S is \\(p_S(x) = \\frac{1}{|S|}\\). The associated\nfunction is the so-called Hartley function:  \nDefinition: If a sample from a finite set S uniformly\nat random is picked, the information revealed after the outcome is\nknown is given by the Hartley function (Hartley 1928):  \nThe combination of these definitions gives a theorem that ties\ntogether the notions of conditional information and probability:  \nUnification Theorem: If S is a finite set\nthen,  \nThe information about an element x of a set S\nconditional to the set is equal to the log of the probability that we select this\nelement x under uniform distribution, which is a measure of our\nignorance if we know the set but not which element of the set\nis to be selected.  \nObservation: Note that the Hartley function unifies\nthe concepts of entropy defined by Boltzmann \\(S = k \\log\nW\\), where W is the cardinality of the set of micro states of\nsystem S, with the concept of Shannon information\n\\(I_S(x) = - \\log p(x)\\). If we consider S to be a set of\nmessages, then the probability that we select an element x from\nthe set (i.e., get a message from S ) under uniform\ndistribution pis \\(\\frac{1}{|S|}\\). \\(H_0(S)\\) is also known as\nthe Hartley Entropy of S.  \nUsing these results we define the conditional amount of\ninformation in a subset of a finite set as: \nDefinition: If A is a finite set and B\nis an arbitrary subset \\(B \\subset A\\), with \\(|A|=n\\) and \\(|B|=k\\)\nwe have:  \nThis is just an application of our basic definition of information:\nthe cardinality of the class of subsets of A with size k\nis \\({n \\choose k}\\).  \nThe formal properties of the concept of probability are specified by\nthe Kolmogorov Axioms of Probability:  \nDefinition: \\(P(E)\\) is the probability P that\nsome event E occurs. \\((\\Omega, F,P)\\), with \\(P(\\Omega)=1\\),\nis a probability space, with sample space \\(\\Omega\\),\nevent space and probability measure.  \nLet \\(P(E)\\) be the probability P that some event E\noccurs. Let \\((\\Omega, F,P)\\), with \\(P(\\Omega)=1\\), be a\nprobability space, with sample space \\(\\Omega\\), event\nspace F and probability measure P. \nOne of the consequences is monotonicity: if \\(A \\subseteq B\\)\nimplies \\(P(A) \\leq P(B)\\). Note that this is the same notion of\nadditivity as defined for the concept of information. At subatomic\nlevel the Kolmogorov Axiom of additivity loses its validity in favor\nof a more subtle notion (see\n section 5.3).\n  \nFrom a philosophical point of view the importance of this construction\nlies in the fact that it leads to an ontologically neutral concept of\ninformation based on a very limited robust base of axiomatic\nassumptions:  \nThis shows how Shannon’s theory of information and\nBoltzmann’s notion of entropy are rooted in more fundamental\nmathematical concepts. The notions of a set of messages or a\nset of micro states are specializations of the more general\nmathematical concept of a set. The concept of information\nalready exists on this more fundamental level. Although many open\nquestions still remain, specifically in the context of the relation\nbetween information theory and physics, perspectives on a unified\ntheory of information now look better than at the beginning of the\ntwenty-first century.  \nThe definition of the amount of information in a number in therms of\nlogarithms allows us to classify other mathematical functions in terms\nof their capacity to process information. The Information\nEfficiency of a function is the difference between the amount of\ninformation in the input of a function and the amount of information\nin the output (Adriaans 2016\n [OIR]).\n It allows us to measure how information flows through a set\nof functions. We use the shorthand \\(f(\\overline{x})\\) for\n\\(f(x_1,x_2,\\dots,x_k)\\): \nDefinition: Information Efficiency of a\nFunction: Let \\(f: \\mathbb{N}^k \\rightarrow \\mathbb{N}\\) be a\nfunction of k variables. We have:  \nIn general deterministic information processing systems do not\ncreate new information. They only process it. The\nfollowing fundamental theorem about the interaction between\ninformation and computation is due to Adriaans and Van Emde Boas\n(2011): \nTheorem: Deterministic programs do not expand\ninformation.  \nThis is in line with both Shannon’s theory and Kolmogorov\ncomplexity. The outcome of a deterministic program is always the same,\nso the probability of the outcome is 1 which gives under\nShannon’s theory, 0 bits of new information. Likewise\nfor Kolmogorov complexity, the output of a program can never be more\ncomplex than the length of the program itself, plus a constant. This\nis analyzed in depth in Adriaans and Van Emde Boas (2011). In a\ndeterministic world it is the case that if:  \nThe essence of information is uncertainty and a message that occurs\nwith probability “1” contains no information. The fact\nthat it might take a long time to compute the number is irrelevant as\nlong as the computation halts. Infinite computations are studied in\nthe theory of Scott domains (Abramsky & Jung 1994).  \nEstimating the information efficiency of elementary functions is not\ntrivial. The primitive recursive functions (see entry on\n recursive functions)\n have one information expanding operation, the increment operation, one\ninformation discarding operation, choosing, all the others\nare information neutral. The information efficiency of more complex\noperations is defined by a combination of counting and choosing. From\nan information efficiency point of view the elementary arithmetical\nfunctions are complex families of functions that describe computations\nwith the same outcome, but with different computational histories. \nSome arithmetical operations expand information, some have constant\ninformation and some discard information. During the execution of\ndeterministic programs expansion of information may take place, but,\nif the program is effective, the descriptive complexity of the output\nis limited. The flow of information is determined by the succession of\ntypes of operations, and by the balance between the complexity of the\noperations and the number of variables.  \nWe briefly discuss the information efficiency of the two basic\nrecursive functions on two variables and their coding\npossibilities: \nAddition \nAddition is associated with information storage in terms of\nsequences or strings of symbols. It is information\ndiscarding for natural numbers bigger than 1. We have \\(\\delta(a\n+ b) \\lt 0\\) since \\(\\log (a + b) \\lt \\log a + \\log b\\). Still,\naddition has information preserving qualities. If we add numbers with\ndifferent log units we can reconstruct the frequency of the units from\nthe resulting number: \n\n\\[\\begin{align}\n232 & = 200 + 30 + 2 \\\\\n& =  (2 \\times 10^2) + (3 \\times 10^1) + (2 \\times 10^0)\\\\\n& = 100 + 100 + 10 + 10 + 10 + 1 + 1\n\\end{align}\n\\] \n \nSince the information in the building blocks, 100, 10 and 1, is given\nthe number representation can still be reconstructed. This implies\nthat natural numbers code in terms of addition of powers of \nk in principle two types of information: value and\nfrequency. We can use this insight to code complex typed\ninformation in single natural numbers. Basically it allows us\nto code any natural numbers in a string of symbols of length \\(\\lceil\n\\log_k n \\rceil \\), which specifies a quantitative measure for the\namount of information in a number in terms of the length of its code.\nSee\n section 3.3\n for a historical analysis of the importance of the discovery of\nposition systems for information theory.  \nMultiplication is by definition information\nconserving. We have: \\(\\delta(a \\times b) = 0\\), since \\(\\log (a\n\\times b) = \\log a + \\log b\\). Still multiplication does not preserve\nall information in its input: the order of the operation is lost. This\nis exactly what we want from an operator that characterizes an\nextensive measure: only the extensive qualities of the\nnumbers are preserved. If we multiply two numbers \\(3 \\times 4\\), then\nthe result, 12, allows us to reconstruct the original computation, in\nso far as we can reduce all its components to their most elementary\nvalues: \\(2 \\times 2 \\times 3 = 12\\). This leads to the observation\nthat some numbers act as information building blocks of other\nnumbers, which gives us the concept of a prime number:  \nDefinition: A prime number is a number that\nis only divisible by itself or 1.  \nThe concept of a prime number gives rise to the Fundamental\nTheorem of Arithmetic: \nTheorem: Every natural number n greater than 1\nis a product of a multiset \\(A_p\\) of primes, and this\nmultiset is unique for n.  \nThe Fundamental Theorem of Arithmetic can be seen as a theorem about\nconservation of information: for every natural number there is a set\nof natural numbers that contains exactly that same amount of\ninformation. The factors of a number form a so-called\nmultiset: a set that may contain multiple copies of the same\nelement: e.g., the number 12 defines the multiset \\(\\{2,2,3\\}\\) in\nwhich the number 2 occurs twice. This makes multisets a powerful\ndevice for coding information since it codes qualitative information\n(i.e., the numbers 2 and 3) as well as quantitative information (i.e.,\nthe fact that the number 2 occurs twice and the number 3 only once).\nThis implies that natural numbers in terms of multiplication of\nprimes also code two types of information: value and\nfrequency. Again we can use this insight to code complex\ntyped information in single natural numbers.  \nPosition based number representations using addition of powers are\nstraightforward and easy to handle and form the basis of most of our\nmathematical functions. This is not the case for coding systems based\non multiplication. Many of the open questions in the philosophy of\nmathematics and information arise in the context of the concepts of\nthe Fundamental Theorem of Arithmetic and Primes. We give a short\noverview:  (Ir)regularity of the set of primes.\nSince antiquity it is known that there is an infinite number of\nprimes. The proof is simple. Suppose the set of primes P is\nfinite. Now multiply all elements of P and add 1. The resulting\nnumber cannot be divided by any member of P, so P is\nincomplete. An estimation of the density of the prime numbers given by\nthe Prime Number Theorem (see entry in Encyclopaedia\nBritannica on Prime Number Theorem\n [OIR]).\n It states that the gaps between primes in the set of natural numbers\nof size n is roughly \\( \\ln n\\), where \\(\\ln\\) is the natural\nlogarithm based on Euler’s number e. A refinement of the\ndensity estimation is given by the so-called Riemannn\nhypothesis, formulated by him in 1859 (Goodman and Weisstein 2019\n [OIR]),\n which is commonly regarded as deepest unsolved problems in\nmathematics, although most mathematicians consider the hypothesis to\nbe true. (In)efficiency of Factorization.\nSince multiplication conserves information the function is, to an\nextent, reversible. The process of finding the unique set of primes\nfor a certain natural number n is called\nfactorization. Observe that the use of the term\n“only” in the definition of a prime number implies that\nthis is in fact a negative characterization: a number\nn is prime if there exists no number between 1 and n\nthat divides it. This gives us an effective procedure for\nfactorization of a number n (simply try to divide n by\nall numbers between 1 and \\(n)\\), but such techniques are not\nefficient. \nIf we use a position system to represent the number n then the\nprocess of identifying factors of n by trial and error will\ntake a deterministic computer program at most n trials which\ngives a computation time exponential in the length of the\nrepresentation of the number which is \\(\\lceil \\log n \\rceil \\).\nFactorization by trial and error of a relatively simple number, of,\nsay, two hundred digits, which codes a rather small message, could\neasily take a computer of the size of our whole universe longer than\nthe time passed since the big bang. So, although theoretically\nfeasible, such algorithms are completely unpractical.  \nFactorization is possibly an example of so-called trapdoor\none-to-one function which is easy to compute from one side but very\ndifficult in its inverse. Whether factorization is really difficult,\nremains an open question, although most mathematicians believe the\nproblem to be hard.\nNote that factorization in this context can be seen as the process of\ndecoding a message. If factorization is hard it can be used as an\nencryption technique. Classical encryption techniques, like RSA, are based on multiplying\ncodes with large prime numbers. Suppose Alice has a message encoded as\na large number m and she knows Bob has access to a large prime\np. She sends the number \\(p \\times m = n\\) to Bob. Since Bob\nknows p he can easily reconstruct m by computing \\(m =\nn/p\\). Since factorization is difficult any other person that receives\nthe message n will have a hard time reconstructing\nm. Primality testing versus Factorization.\nAlthough at this moment efficient techniques for factorization on\nclassical computers are not known to exist, there is an efficient\nalgorithm that decides for us whether a number is prime or not: the\nso-called AKS primality test (Agrawal et al. 2004). So, we might know\na number is not prime, while we still do not have access to its set of\nfactors.  Classical- versus Quantum Computing.\nTheoretically factorization is efficient on quantum computers using\nShor’s algorithm (Shor 1997). This algorithm has a non-classical\nquantum subroutine, embedded in a deterministic classical program.\nCollections of quantum bits can be modeled in terms of complex higher\ndimensional vector-spaces, that, in principle, allow us to analyze an\nexponential number \\(2^n\\) of correlations between collections of\nn objects. Currently it is not clear whether larger quantum\ncomputers will be stable enough to facilitate practical applications,\nbut that the world at quantum level has relevant computational\npossibilities can not be doubted anymore, e.g., quantum random\ngenerators are available as a commercial product (see\nWikipedia entry on Hardware random number generator\n [OIR]).\n As soon as viable quantum computers become available almost all of\nthe current encryption techniques become useless, although they can be\nreplaced by quantum versions of encryption techniques (see\nthe entry on Quantum Computiong). \nThere is an infinite number of observations we can make about the set\n\\(\\mathbb{N}\\) that are not implied directly by the axioms, but\ninvolve a considerable amount of computation.  \nIn a landmark paper in 1931 Kurt Gödel proved that any consistent\nformal system that contains elementary arithmetic is fundamentally\nincomplete in the sense that it contains true statements that cannot\nbe proved within the system. In a philosophical context this implies\nthat the semantics of a formal system rich enough to contain\nelementary mathematics cannot be defined in terms of mathematical\nfunctions within the system, i.e., there are statements that contain\nsemantic information about the system in the sense of being\nwell-formed, meaningful and truthful\nwithout being provable.  \nCentral is the concept of a Recursive Function. (see entry on\n recursive functions).\n Such functions are defined on numbers. Gödel’s notion of a\nrecursive function is closest to what we would associate with\ncomputation in every day life. Basically they are elementary\narithmetical functions operating on natural numbers like addition,\nsubtraction, multiplication and division and all other functions that\ncan be defined on top of these.  \nWe give the basic structure of the proof. Suppose F is a formal\nsystem, with the following components:  \nAssume furthermore that F is consistent, i.e., it will never\nderive false statements form true ones. In his proof Gödel used\nthe coding possibilities of multiplication to construct an image of\nthe system (see the discussion of \n Gödel numbering\n from the entry on Gödel’s Incompleteness Theorems).\n According to the fundamental theorem of arithmetic \n any number can be uniquely factored in to its primes. This defines a\none-to-one relationship between multisets of numbers and numbers: the\nnumber 12 can be constructed on the basis of the multiset\n\\(\\{2,2,3\\}\\) as \\(12=2 \\times 2\\times 3\\) and vice versa. This allows\nus to code any sequence of symbols as a specific individual number in\nthe following way: \nOn the basis of this we can code any sequence of symbols as a\nso-called Gödel number, e.g., the number:  \ncodes the multiset \\(\\{2,3,3,5,5,7\\}\\), which represents the string\n“abba” under the assumption \\(a=1\\), \\(b=2\\). With this\nobservation conditions close to those that lead to the paradox of\nRussel are satisfied: elementary arithmetic itself is rich enough to\nexpress: Universality, Negation, and\nSelf-reference. \nSince arithmetic is consistent this does not lead to paradoxes, but to\nincompleteness. By a construction related to the liars paradox\nGödel proved that such a system must contain statements that are\ntrue but not provable: there are true sentences of the form “I\nam not provable”.  \nTheorem: Any formal system that contains elementary\narithmetic is fundamentally incomplete. It contains\nstatements that are true but not provable.  \nIn the context of philosophy of information the incompleteness of\nmathematics is a direct consequence of the rich possibilities of the\nnatural numbers to code information. In principle any deterministic\nformal system can be represented in terms of elementary arithmetical\nfunctions. Consequently, If such a system itself contains arithmetic\nas a sub system, it contains a infinite chain of endomorphisms (i.e.,\nimages of itself). Such a system is capable of reasoning about its own\nfunctions and proofs but since it is consistent (and thus the\nconstruction of paradoxes is not possible within the system) it is by\nnecessity incomplete.  \nRecursive functions are abstract relations defined on natural numbers.\nIn principle they can be defined without any reference to space and\ntime. Such functions must be distinguished from the\noperations that we use to compute them. These operations\nmainly depend on the type of symbolic representations that we\nchoose for them. We can represent the number seven as unary number\n\\(|||||||\\), binary number 111, Roman number VII, or Arabic number 7\nand depending on our choice other types of sequential symbol\nmanipulation can be used to compute the addition two plus five is\nseven, which can be represented as: \n\n\\[\n\\begin{align}\n|| + ||||| & = ||||||| \\\\\n10 + 101 &  = 111 \\\\\n\\textrm{II} + \\textrm{V} & = \\textrm{VII}\\\\\n2 + 5 &= 7 \\\\\n\\end{align}\n\\]  \n\n Consequently we can\nread these four sentences as four statements of the same\nmathematical truth, or as statements specifying the results of four\ndifferent operations.  \nObservation: There are (at least) two different\nperspectives from which we can study the notion of computation. The\nsemantics of the symbols is different under these interpretations.\n \nThis leads to the following tentative definition: \nDefinition: Deterministic Computing on a Macroscopic\nScale can be defined as the local, sequential, manipulation of\ndiscrete objects according to deterministic rules.  \nIn nature there are many other ways to perform such computations. One\ncould use an abacus, study chemical processes or simply manipulate\nsequences of pebbles on a beach. The fact that the objects we\nmanipulate are discrete together with the observation that the dataset\nis self-referential implies that the data domain is in principle\nDedekind Infinite: \nDefinition: A set S is Dedekind Infinite if it\nhas a bijection \\(f: S \\rightarrow S^{\\prime}\\) to a proper subset\n\\(S^{\\prime} \\subset S\\).  \nSince the data elements are discrete and finite the data domain will\nbe countable infinite and therefore isomorphic to the set of natural\nnumbers.  \nDefinition: An infinite set S is\ncountable if there exists a bijection with the set of natural\nnumbers \\(\\mathbb{N}\\).  \nFor infinite countable sets the notion of information is defined as\nfollows:  \nDefinition: Suppose S is countable and\ninfinite and the function \\(f:S \\rightarrow \\mathbb{N}\\) defines a\none-to-one correspondence, then: \n\n\\[I(a\\mid S,f) = \\log f(a)\\]\n\n i.e., the amount of\ninformation in an index of a in S given f.  \nNote that the correspondence f is specified explicitly. As soon\nas such an index function is defined for a class of objects in the\nreal world, the manipulation of these objects can be interpreted a\nform of computing. \nOnce we choose a finite set of symbols and our operational rules the system starts\nto produce statements about the world. \nObservation: The meta-sentence: \nThe sign “0” is the symbol for zero. \nspecifies semantic information in the same sense as the\nstatement \\(e \\in A\\) does for sets (see\n section 6.6).\n The statement is wellformed, meaningful and\ntruthful.  \nWe can study symbol manipulation in general on an abstract level,\nwithout any semantic implications. Such a theory was published by Alan\nTuring (1912–1954). Turing developed a general theory of\ncomputing focusing on the actual operations on symbols a mathematician\nperforms (Turing 1936). For him a computer was an abstraction of a\nreal mathematician sitting behind a desk, receiving problems written\ndown on an in-tray (the inut), solving them according to fixed rules\n(the process) and leaving them to be picked up in an out-tray (the\noutput).  \nTuring first formulated the notion of a general theory of computing\nalong these lines. He proposed abstract machines that operate on\ninfinite tapes with three symbols: blank \\((b)\\), zero \\((0)\\) and one\n\\((1)\\). Consequently the data domain for Turing machines is the set\nof relevant tape configurations, which can be associated with the set\nof binary strings, consisting of zero’s and one’s. The\nmachines can read and write symbols on the tape and they have a\ntransition function that determines their actions under various\nconditions. On an abstract level Turing machines operate like\nfunctions.  \nDefinition: If \\(T_i\\) is a Turing machine\nwith index i and x is a string of zero’s and\none’s on the tape that function as the input then\n\\(T_i(x)\\) indicates the tape configuration after the machine has\nstopped, i.e., its output.  \nThere is an infinite number of Turing machines. Turing discovered that\nthere are so-called universal Turing machines \\(U_j\\) that can emulate\nany other Turing machine \\(T_i\\).  \nDefinition: The expression \\(U_j(\\overline{T_i}x)\\)\ndenotes the result of the emulation of the computation \\(T_i(x)\\) by\n\\(U_j\\) after reading the self-delimiting description\n\\(\\overline{T_i}\\) of machine \\(T_j\\).  \nThe self-delimiting code is necessary because the input for \\(U_j\\) is\ncoded as one string \\(\\overline{T_i}x\\). The universal machine \\(U_j\\)\nseparates the input string \\(\\overline{T_i}x\\) in to its two\nconstituent parts: the description of the machine \\(\\overline{T_i}\\)\nand the input for this machine x.  \nThe self-referential nature of general computational systems allows us\nto construct machines that emulate other machines. This suggests the\npossible existence of a ‘super machine’ that emulates all\npossible computations on all possible machines and predicts their\noutcome. Using a technique called diagonalization, where one analyzes\nan enumeration of all possible machines running on descriptions of all\npossible machines, Turing proved that such a machine can not exist.\nMore formally:  \nTheorem: There is no Turing machine that predicts for\nany other Turing machine whether it stops on a certain input or not.\n \nThis implies that for a certain universal machine \\(U_i\\) the set of\ninputs on which it stops in finite time, is uncomputable. In recent years the notion of infinite computations on Turing machines has also been studied (Hamkins and Lewis 2000.) Not every\nmachine will stop on every input, but in some case infinite computations compute useful output (consider the infinite expansion of the number pi).  \nDefinition: The Halting set is the set of\ncombinations of Turing machines \\(T_i\\) and inputs x such that\nthe computation \\(T_i(x)\\) stops.  \nThe existence of universal Turing machines indicates that the class\nembodies a notion of universal computing: any computation\nthat can be performed on a specific Turing machine can also be\nperformed on any other universal Turing machine. This is the\nmathematical foundation of the concept of a general programmable\ncomputer. These observations have bearing on the theory of\ninformation: certain measures of information, like Kolmogorov\ncomplexity, are defined, but not computable.  \nThe proof of the existence uncomputable functions in the class of\nTuring machines is similar to the incompleteness result of Gödel\nfor elementary arithmetic. Since Turing machines were defined to study\nthe notion of computation and thus contain elementary arithmetic. The\nclass of Turing machines is in itself rich enough to express:\nUniversality, Negation and Self-reference.\nConsequently Turing machines can model universal negative statements\nabout themselves. Turing’s uncomputability proof is also\nmotivated by the liars paradox, and the notion of a machine that stops\non a certain input is similar to the notion of a proof that exists for\na certain statement. At the same time Turing machines satisfy the\nconditions of Gödel’s theorem: they can be modeled as a\nformal system F that contains elementary Peano arithmetic. \nObservation: Since they can emulate each other, the\nRecursive Function Paradigm and the Symbol Manipulation\nParadigm have the same computational strength. Any\nfunction that can be computed in one paradigm can also by definition\nbe computed in the other.  \nThis insight can be generalized: \nDefinition: An infinite set of computational\nfunctions is Turing complete if it has the same computational\npower as the general class of Turing machines. In this case it is\ncalled Turing equivalent. Such a system is, like the class of Turing\nmachines, universal: it can emulate any computable function.  \nThe philosophical implications of this observation are strong and\nrich, not only for the theory of computing but also for our\nunderstanding of the concept of information.  \nThere is an intricate ration between the notion of universal computing\nand that of information. Precisely the fact that Turing Systems are\nuniversal allows us to say that they process information, because\ntheir universality entails invariance: \nSmall Invariance Theorem: The concept of information\nin a string x measured as the length of the smallest string of\nsymbols s of a program for a universal Turing machine U\nsuch that \\(U(s)= x\\) is invariant, modulo an additive constant, under selection of\ndifferent universal Turing machines  \nProof: The proof is simple and relevant for\nphilosophy of information. Let \\(l(x)\\) be the length of the string of\nsymbols x. Suppose we have two different universal Turing\nmachines \\(U_j\\) and \\(U_k\\). Since they are universal they can both\nemulate the computation \\(T_i(x)\\) of Turing machine \\(T_i\\) on input\nx: \nHere \\(l(\\overline{T}_i^j)\\) is the length of the code for \\(T_i\\) on\n\\(U_j\\) and \\(l(\\overline{T}_i^k)\\) is the length of the code for\n\\(T_i\\) on \\(U_k\\). Suppose \\(l(\\overline{T}_i^jx) \\ll\nl(\\overline{T}_i^kx)\\), i.e., the code for \\(T_i\\) on \\(U_k\\) is much\nless efficient that on \\(U_j\\). Observe that the code for \\(U_j\\) has\nconstant length, i.e., \\(l(\\overline{U}_j^k)=c\\). Since \\(U_k\\) is\nuniversal we can compute:  \nThe length of the input for this computation is:  \nConsequently the specification of the input for the computation\n\\(T_i(x)\\) on the universal machine \\(U_k\\) never needs to longer than\na constant. \\(\\Box\\)  \nThis proof forms the basis of the theory of Kolmogorov complexity and\nis originally due to Solomonoff (1964a,b) and discovered independently\nby Kolmogorov (1965) and Chaitin (1969). Note that this notion of\ninvariance can be generalized over the class of Turing Complete\nSystems: \nBig Invariance Theorem: The concept of information\nmeasured in terms of the length of the input of a computation is invariant, modulo an additive constant, for for Turing Complete systems.  \nProof: Suppose we have a Turing Complete system\nF. By Definition any computation \\(T_i(x)\\) on a Turing machine\ncan be emulated in F and vice versa. There will be a special\nuniversal Turing machine \\(U_F\\) that emulates the computation\n\\(T_i(x)\\) in F: \\(U_F(\\overline{T}_i^Fx)\\). In principle\n\\(\\overline{T}_i^F\\) might use a very inefficient way to code programs\nsuch that \\(\\overline{T}_i^F\\) can have any length. Observe that the\ncode for any other universal machine \\(U_j\\) emulated by \\(U_F\\) has\nconstant length, i.e., \\(l(\\overline{U}_j^F)=c\\). Since \\(U_F\\) is\nuniversal we can also compute:  \nThe length of the input for this computation is: \n\n\\[l(\\overline{U}_j^F \\ \\overline{T}_i^jx) = c + l(\\overline{T}_i^jx)\\] \n\nConsequently the specification of the input for the computation\n\\(T_i(x)\\) on the universal machine \\(U_F\\) never needs to be longer\nthan a constant. \\(\\Box\\)  \nHow strong this result is becomes clear when we analyze the class of\nTuring complete systems in more detail. In the first half of the\ntwentieth century three fundamentally different proposals for a\ngeneral theory of computation were formulated: Gödel’s\nrecursive functions ( Gödel 1931), Turing’s automata\n(Turing 1937) and Church’s Lambda Calculus (Church 1936). Each\nof these proposals in its own way clarifies aspects of the notion of\ncomputing. Later much more examples followed. The class of Turing\nequivalent systems is diverse. Apart from obvious candidates like all\ngeneral purpose programming languages (C, Fortran, Prolog, etc.) it\nalso contains some unexpected elements like various games (e.g.,\nMagic: The Gathering [Churchill 2012\n OIR]).\n The table below gives an overview of some conceptually interesting\nsystems:  \nAn overview of some Turing Complete systems  \nWe make the following:  \nObservation: The class of Turing equivalent systems\nis open, because it is defined in terms of purely operational mappings\nbetween computations.  \nA direct consequence of this observation is:  \nObservation: The general theory of computation and\ninformation defined by the class of Complete Turing machines is\nontologically neutral.  \nIt is not possible to derive any necessary qualities of computational\nsystems and data domains beyond the fact that they are general\nmathematical operations and structures. Data domains on which Turing\nequivalent systems are defined are not necessarily physical, nor\ntemporal, nor spatial, not binary or digital. At any moment a new\nmember for the class can be introduced. We know that there are\ncomputational systems that are weaker than the class of Turing\nmachines (e.g., regular languages). We cannot rule out the possibility\nthat one-day we come across a system that is stronger. The thesis that\nsuch a system does not exist is known as the Church-Turing thesis (see\nentry on\n Church-Turing thesis): \nChurch-Turing Thesis: The class of Turing machines\ncharacterizes the notion of algorithmic computing exactly.  \nWe give an overview of the arguments for and against the thesis: \nArguments in favor of the thesis: The theory of Turing\nmachines seems to be the most general theory possible that we can\nformulate since it is based on a very limited set of assumptions about\nwhat computing is. The fact that it is universal also points in the\ndirection of its generality. It is difficult to conceive in what sense\na more powerful system could be “more” universal. Even if\nwe could think of such a more powerful system, the in- and output for\nsuch a system would have to be finite and discrete and the computation\ntime also finite. So, in the end, any computation would have the form\nof a finite function between finite data sets, and, in principle, all\nsuch relations can be modeled on Turing machines. The fact that all\nknown systems of computation we have defined so far have the same\npower also corroborates the thesis.  \nArguments against the thesis: The thesis is, in its present\nform, unprovable. The class of Turing Complete systems is open. It is\ndefined on the basis of the existence of equivalence relations between known\nsystems. In this sense it does not define the notion of computing\nintrinsically. It doesn’t not provide us with a philosophical\ntheory that defines what computing exactly is. Consequently\nit does not allow us to exclude any system from the class a\npriori. At any time a proposal for a notion of computation might\nemerge that is fundamentally stronger. What is more, nature provides\nus with stronger notions of computing in the form of quantum\ncomputing. Quantum bits are really a generalization of the normal\nconcept of bits that is associated with symbol manipulation, although\nin the end quantum computing does not seem to necessitate us to\nredefine the notion of computing so far. We can never rule out that\nresearch in physics, biology or chemistry will define systems that\nwill force us to do so. Indeed various authors have suggested such\nsystems but there is currently no consensus on convincing candidates\n(Davis 2006). Dershowitz and Gurevich (2008) claim to have vindicated\nthe hypothesis, but this result is not generally accepted (see the\ndiscussion on “Computability – What would it mean to\ndisprove the Church-Turing thesis”, in the\n Other Internet Resources [OIR]).\n  \nBeing Turing complete seems to be quite a natural condition for a\n(formal) system. Any system that is sufficiently rich to represent the\nnatural numbers and elementary arithmetical operations is Turing\ncomplete. What is needed is a finite set of operations defined on a\nset of discrete finite data elements that is rich enough to make the\nsystem self-referential: its operations can be described by its data\nelements. This explains, in part, why we can use mathematics to\ndescribe our world. The abstract notion of computation defined as\nfunctions on numbers in the abstract world mathematics and the\nconcrete notion of computing by manipulation objects in our every day\nworld around us coincide. The concepts of information end computation\nimplied by the Recursive Function Paradigm and the Symbol\nManipulation Paradigm are the same.  \nObservation: If one accepts the fact that the\nChurch-Turing thesis is open, this implies that the question about the\nexistence of a universal notion of information is also open. At this\nstage of the research it is not possible to specify the a\npriori conditions for such a general theory.  \nWe have a reasonable understanding of the concept of classical\ncomputing, but the implications of quantum physics for computing and\ninformation may determine the philosophical research agenda for\ndecades to come if not longer. Still it is already clear that the\nresearch has repercussions for traditional philosophical positions:\nthe Laplacian view (Laplace 1814 [1902]) that the universe is\nessentially deterministic seems to be falsified by empirical\nobservations. Quantum random generators are commercially available\n(see Wikipedia entry on Hardware random number generator\n [OIR])\n and quantum fluctuations do affect neurological, biological and\nphysical processes at a macroscopic scale (Albrecht & Phillips\n2014). Our universe is effectively a process that generates\ninformation permanently. Classical deterministic computing seems to be\ntoo weak a concept to understand its structure.  \nStandard computing on a macroscopic scale can be defined as local,\nsequential, manipulation of discrete objects according to\ndeterministic rules. Is has a natural interpretation in\noperations on the set of natural numbers N and a natural\nmeasurement function in the log operation \\(\\log: \\mathbb{N}\n\\rightarrow \\mathbb{R}\\) associating a real number to every natural\nnumber. The definition gives us an adequate information measure for\ncountable infinite sets, including number classes like the integers\n\\(\\mathbb{Z}\\), closed under subtraction, and the rational\nnumbers \\(\\mathbb{Q}\\), closed under division. \nThe operation of multiplication with the associated\nlogarithmic function characterizes our intuitions about\nadditivity of the concept of information exactly. It leads to a\nnatural bijection between the set of natural numbers \\(\\mathbb{N}\\)\nand the set of multisets of numbers (i.e., sets of prime factors). The\nnotion of a multiset is associated with the properties of\ncommutativity and associativity. This program can be\nextended to other classes of numbers when we study division algebras\nin higher dimensions. The following table gives an overview of some\nrelevant number classes together with the properties of the\noperation of multiplication for these classes:  \nThe table is ordered in terms of increasing generality. Starting from\nthe set of natural numbers \\(\\mathbb{N}\\), various extensions are\npossible taking into account closure under subtraction,\n\\(\\mathbb{Z}\\), and division, \\(\\mathbb{Q}\\). This are the number\nclasses for which we have adequate finite symbolic representations on\na macroscopic scale. For elements of the real numbers \\(\\mathbb{R}\\)\nsuch a representations are not available. The real numbers\n\\(\\mathbb{R}\\) introduce the aspect of manipulation of infinite\namounts of information in one operation. \nObservation: For almost all \\(e \\in\n\\mathbb{R}\\) we have \\(I(e) = \\infty\\).  \nMore complex division algebras can be defined when we introduce\nimaginary numbers as negative squares \\(i^2 = -1\\). We can now define\ncomplex numbers: \\(a + bi\\), where a is the real part and\n\\(bi\\) the imaginary part. Complex numbers can be interpreted as\nvectors in a two dimensional plane. Consequently they lack the notion\nof a strict linear order between symbols. Addition is quite\nstraightforward: \nMultiplication follows the normal distribution rule but the result is\nless intuitive since it involves a negative term generated by\n\\(i^2\\): \nIn this context multiplication ceases to be a purely extensive\noperation: \nMore complicated numbers systems with generalizations of this type of\nmultiplication in 4 and 8 dimensions can be defined. Kervaire (1958)\nand Bott & Milnor (1958) independently proved that the only four\ndivision algebras built on the reals are \\(\\mathbb{R}\\),\n\\(\\mathbb{C}\\), \\(\\mathbb{H}\\) and \\(\\mathbb{O}\\), so the table gives\na comprehensive view of all possible algebra’s that define a\nnotion of extensiveness. For each of the number classes in the table a\nseparate theory of information measurement, based on the properties of\nmultiplication, can be developed. For the countable classes\n\\(\\mathbb{N}\\), \\(\\mathbb{Z}\\) and \\(\\mathbb{Q}\\) these theories ware\nequivalent to the standard concept of information implied by the\nnotion of Turing equivalence. Up to the real numbers these theories\nsatisfy our intuitive notions of extensiveness of information. For\ncomplex numbers the notion of information efficiency of\nmultiplication is destroyed. The quaternions lack the property of\ncommutativity and the octonions that of\nassociativity. These models are not just abstract\nconstructions since the algebras play an important role in our\ndescriptions of nature: \nWe briefly discuss the application of vector spaces in quantum\nphysics. Classical information is measured in bits. Implementation of\nbits in nature involves macroscopic physical systems with at least two\ndifferent stable states and a low energy reversible transition process\n(i.e., switches, relays, transistors). The most fundamental way to\nstore information in nature on an atomic level involves qubits. The\nqubit is described by a state vector in a two-level quantum-mechanical\nsystem, which is formally equivalent to a two-dimensional vector space\nover the complex numbers (Von Neumann 1932; Nielsen & Chuang\n2000). Quantum algorithms have, in some cases, a fundamentally lower\ncomplexity (e.g., Shor’s algorithm for factorization of integers\n(Shor 1997)). \nDefinition: The quantum bit, or\nqubit, is a generalization of the classical bit. The quantum\nstate of qubit is represented as the linear superposition of two\northonormal basis vectors: \nHere the so-called Dirac or “bra-ket” notion is used:\nwhere \\(\\ket{0}\\) and \\(\\ket{1}\\) are pronounced as “ket\n0” and “ket 1”. The two vectors together form the\ncomputational basis \\(\\{\\ket{0}, \\ket{1}\\}\\), which\ndefines a vector in a two-dimensional Hilbert space. A\ncombination of n qubits is represented by a superposition\nvector in a \\(2^n\\) dimensional Hilbert space, e.g.: \n A pure qubit is a coherent superposition of the\nbasis states:  \nwhere \\(\\alpha\\) and \\(\\beta\\) are complex numbers, with the\nconstraint: \nIn this way the values can be interpreted as probabilities:\n\\(|\\alpha|^2\\) is the probability that the qubit has value 0 and\n\\(|\\beta|^2\\) is the probability that the qubit has value 1.  \nUnder this mathematical model our intuitions about computing as local,\nsequential, manipulation of discrete objects according to\ndeterministic rules evolve in to a much richer paradigm: \nFrom this analysis it is clear that the description of our universe at\nvery small (and very large) scales involves mathematical models that\nare alien to our experience of reality in everyday life. The\nproperties that allow us to understand the world (the existence of\nstable, discrete objects that preserve their identity in space and\ntime) seem to be emergent aspects of a much more complex\nreality that is incomprehensible to us outside its mathematical\nformulation. Yet, at a macroscopic level, the universe facilitates\nelementary processes, like counting, measuring lengths, and the\nmanipulation of symbols, that allow us to develop a consistent\nhierarchy of mathematical models some of which seems to describe the\ndeeper underlying structure of reality.  \nIn a sense the same mathematical properties that drove the development\nof elementary accounting systems in Mesopotamia four thousand years\nago, still help us to penetrate in to the world of subatomic\nstructures. In the past decennia information seems to have become a\nvital concept in physics. Seth Lloyd and others (Zuse 1969; Wheeler\n1990; Schmidhuber 1997b; Wolfram 2002; Hutter 2010) have analyzed\ncomputational models of various physical systems. The notion of\ninformation seems to play a major role in the analysis of black holes\n(Lloyd & Ng 2004; Bekenstein 1994\n [OIR]).\n Erik Verlinde (2011, 2017) has proposed a theory in which gravity is\nanalyzed in terms of information. For the moment these models seem to\nbe purely descriptive without any possibility of empirical\nverification.  \nSome of the fundamental issues in philosophy of Information are\nclosely related to existing philosophical problems, others seem to be\nnew. In this paragraph we discuss a number of observations that may\ndetermine the future research agenda. Some relevant questions are: \nSince Frege most mathematicians seem to believe that the answer to the\nfirst question is positive (Frege 1879, 1892). The descriptions\n“The morning star” and “The evening star” are\nassociated with procedures to identify the planet Venus, but\nthey do not give access to all information about the object itself. If\nthis were so the discovery that the evening star is in fact also the\nmorning star would be uninformative. If we want to maintain this\nposition we get into conflict, because in terms of information theory\nthe answer to the second question is negative (see\n section 5.1.7).\n Yet this observation is highly counter intuitive, because it implies\nthat we never can construct new information on the basis of\ndeterministic computation, which leads to the third question. These\nissues cluster around one of the fundamental open problems of\nPhilosophy of Information:  \nOpen problem What is the interaction between\nInformation and Computation?  \nWhy would we compute at all, if according to our known information\nmeasures, deterministic computing does not produce new information?\nThe question could be rephrased as: should we use Kolmogorov or Levin\ncomplexity (Levin 1973, 1974, 1984) as our basic information measure?\nIn fact both choices lead to relevant, but fundamentally different,\ntheories of information. When using the Levin measure, computing\ngenerates information and the answer to the three questions above is a\n“yes”, when using Kolmogorov this is not the case. The\nquestions are related to many problems both in mathematics and\ncomputer science. Related issues like approximation, computability and\npartial information are also studied in the context of Scott domains\n(Abramsky & Jung 1994). Below we discuss some relevant\nobservations.  \nThe essence of information is the fact that it reduces uncertainty.\nThis observation leads to problems in opaque contexts, for instance,\nwhen we search an object. This is illustrated by Meno’s paradox\n(see entry on\n epistemic paradoxes): \nAnd how will you enquire, Socrates, into that which you do not\nknow? What will you put forth as the subject of enquiry? And if you\nfind what you want, how will you ever know that this is the thing\nwhich you did not know? (Plato, Meno, 80d1-4)  \nThe paradox is related to other open problems in computer science and\nphilosophy. Suppose that John is looking for a unicorn. It is very\nunlikely that unicorns exist, so, in terms of Shannon’s theory,\nJohn gets a lot of information if he finds one. Yet from a descriptive\nKolmogorov point of view, John does not get new information, since he\nalready knows what unicorns are. The related paradox of systematic\nsearch might be formulated as follows: \nAny information that can be found by means of systematic search has no\nvalue, since we are certain to find it, given enough time.\nConsequently information only has value as long as we are uncertain\nabout its existence, but then, since we already know what we are\nlooking for, we get no new information when we find out that it\nexists.  \nExample: Goldbach conjectured in 1742\nthat every even number bigger than\n2 could be written as the sum of two primes. Until today this\nconjecture remains unproved. Consider the term “The first number\nthat violates Goldbach’s conjecture”. It does not give us\nall information about the number, since the number might not exist.\nThe prefix “the first” ensures the description, if it\nexists, is unique, and it gives us an algorithm to find the number. It\nis a partial uniquely identifying description. This algorithm\nis only effective if the number really exists, otherwise it\nwill run forever. If we find the number this will be great news, but\nfrom the perspective of descriptive complexity the number itself will\nbe totally uninteresting, since we already know the relevant\nproperties to find it. Observe that, even if we have a number n\nthat is a counter example to Goldbach’s conjecture, it might be\ndifficult to verify this: we might have to check almost all primes \\(\n\\leq n\\). This can be done effectively (we will always get a\nresult) but not, as far as we know, efficiently (it might\ntake “close” to n different computations) .  \nA possible solution is to specify the constraint that it is\nillegal to measure the information content of an object in\nterms of partial descriptions, but this would destroy our theory of\ndescriptive complexity. Note that the complexity of an object is the\nlength of the shortest program that produces an object on a universal\nTuring machine. In this sense the phrase “the first number that\nviolates Goldbach’s conjecture” is a perfect description\nof a program, and it adequately measures the descriptive complexity of\nsuch a number. The short description reflects the fact that the\nnumber, if it exists, is very special, and thus it has a high\npossibility to occur in some mathematical context.  \nThere are relations which well-studied philosophical problems like the\nAnselm’s ontological argument for God’s existence and the\nKantian counter claim that existence is not a predicate. In order to\navoid similar problems Russell proposed to interpret unique\ndescriptions existentially (Russell 1905): A sentence like “The\nking of France is bald” would have the following logical\nstructure:  \nThis interpretation does not help us to analyze decision problems that\ndeal with existence. Suppose the predicate L is true of\nx if I’m looking for x, then the logical structure\nof the phrase “I’m looking for the king of France”\nwould be:  \ni.e., if the king of France does not exist it cannot be true that I am\nlooking for him, which is unsatisfactory. Kripke (1971) criticized\nRussell’s solution and proposed his so-called causal theory of\nreference in which a name get its reference by an initial act of\n“baptism”. It then becomes a rigid designator\n(see entry on\n rigid designators)\n that can be followed back to that original act via causal chains. In\nthis way ad hoc descriptions like “John was the fourth\nperson to step out of the elevator this morning” can establish a\nsemantics for a name. \nIn the context of mathematics and information theory the corresponding\nconcept is that of names, constructive predicates and ad-hoc\npredicates of numbers. For any number there will be in principle an\ninfinite number of true statements about that number. Since elementary\narithmetic is incomplete there will be statements about numbers that\nare true but unprovable. In the limit a vanishing fragment of numbers\nwill have true predicates that actually compress their description.\nConsider the following statements: \nThe first statement simply specifies a name for a number. The second\nstatement gives a partial description that is constructive,\ninformation compressing and unique. The 1000th Fibonacci number has\n209 digits, so the description “the 1000th Fibonacci\nnumber” is much more efficient than the actual name of the\nnumber. Moreover, we have an algorithm to construct the number. This\nmight not be that case for the description in the third statement. We\ndo not know whether the first number that violates the Goldbach\nconjecture exists, but if it does, the description might well be\nad hoc and thus gives us no clue to construct the number.\nThis rise to the conjecture that there are data compressing\neffective ad hoc descriptions: \nConjecture: There exist numbers that are compressed\nby non-constructive unique effective descriptions, i.e., the validity\nof the description can be checked effectively given the number, but\nthe number cannot be constructed effectively from the description,\nexcept by means of systematic search.  \nThe conjecture is a more general variant of the so-called\nP vs. NP thesis (see\n section 6.3).\n If one replaces the term “effective” with the term\n“efficient” one gets a formulation of the \\(\\textrm{P}\n\\neq \\textrm{NP}\\) thesis. \nWhen we restrict ourselves to effective search in finite sets, the\nproblem of partial descriptions, and construction versus search\nremain. It seems natural to assume that when one has a definition of a\nset of numbers, then one also has all the information about the\nmembers of the set and about its subsets, but this is not true. In\ngeneral the computation of the amount of information in a set of\nnumbers is a highly non-trivial issue. We give some results: \nLemma A subset \\(A \\subset S\\) of a set S can\ncontain more information conditional to the set than the set itself.\n \nProof: Consider the set S of all natural\nnumbers smaller than n. The descriptive complexity of this set\nin bits is \\( \\log_2 n + c\\). Now construct A by selecting half\nof the elements of S randomly. Observe that:  \nWe have: \nThe conditional descriptive complexity of this set will be: \\(I(A\\mid\nS) \\approx n + c \\gg \\log n + c\\). \\(\\Box\\) \nA direct consequence is that we can lose information when we merge two\nsets. An even stronger result is:  \nLemma: An element of a set can contain more\ninformation than the set itself.  \nProof: Consider the set S of natural numbers\nsmaller then \\(2^n\\). The cardinality of S is \\(2^n\\). The\ndescriptive complexity of this set is \\(\\log n + c\\) bits, but for\nhalf of the elements of S we need n bits to describe\nthem. \\(\\Box\\) \nIn this case the description of the set itself is highly compressible,\nbut it still contains non-compressible elements. When we merge or\nsplit sets of numbers, or add or remove elements, the effects on the\namount of information are in general hard to predict and might even be\nuncomputable:  \nTheorem: Information is not monotone under set\ntheoretical operations  \nProof: Immediate consequence of the lemmas above.\n\\(\\Box\\) \nThis shows how the notion of information pervades our everyday life.\nWhen John has two apples in his pocket it seems that he can do\nwhatever he wants with them, but, in fact, as soon as he chooses one\nof the two, he has created (new) information. The consequences for\nsearch problems are clear: we can always effectively perform bounded\nsearch on the elements and the set of subsets of a set. Consequently\nwhen we search for such a set of subsets by means of partial\ndescriptions then the result generates (new) information. This\nanalysis prima facie appears to force us to accept that in mathematics\nthere are simple descriptions that allow us to identify complex\nobjects by means of systematic search. When we look for the object we\nhave only little information about it, when we finally find it our\ninformation increases to the set of full facts about the object\nsearched. This is in conflict with our current theories of information\n(Shannon and Kolmogorov): any description that allows us to identify\nan object effectively by deterministic search contains all relevant\ninformation about the object. The time complexity of the search\nprocess then is irrelevant.  \nIn the past decennia mathematicians have been pondering about a\nrelated question: suppose it would be easy to check whether I\nhave found what I’m looking for, how hard can it be to find such\nan object? In mathematics and computer science there seems to be a\nconsiderable class of decision problems that cannot be solved\nconstructively in polynomial time, \\(t(x)=x^c\\), where c is a\nconstant and x is the length of the input), but only through\nsystematic search of a large part of the solution space, which might\ntake exponential time, \\(t(x)=c^x\\). This difference roughly coincides\nwith the separation of problems that are computationally feasible from\nthose that are not. \nThe issue of the existence of such problems has been framed as the\npossible equivalence of the class P of decision problems\nthat can be solved in time polynomial to the input to the class\nNP of problems for which the solution can be checked in\ntime polynomial to the input. (Garey & Johnson 1979; see also Cook\n2000\n [OIR]\n for a good introduction.)  \nExample: A well-known example in the class\nNP is the so-called subset sum problem: given a\nfinite set of natural numbers S, is there a subset\n\\(S^{\\prime}\\subseteq S\\) that sums up to some number k? It is\nclear that when someone proposes a solution \\(X \\subseteq S\\) to this\nproblem we can easily check whether the elements of X add up to\nk, but we might have to check almost all subsets of S in\norder to find such a solution ourselves.  \nThis is an example of a so-called decision problem. The answer is a\nsimple “yes” or “no”, but it might be hard to\nfind the answer. Observe that the formulation of the question\nconditional to S has descriptive complexity \\(\\log k + c\\),\nwhereas most random subsets of S have a conditional descriptive\ncomplexity of \\(|S|\\). So any subset \\(S^{\\prime}\\) that adds up to\nk might have a descriptive complexity that is bigger then the\nformulation of the search problem. In this sense search seems to\ngenerate information. The problem is that if such a set exists the\nsearch process is bounded, and thus effective, which means that the\nphrase “the first subset of S that adds up to\nk” is an adequate description. If \\(\\textrm{P} =\n\\textrm{NP}\\) then the Kolmogorov complexity and the Levin complexity\nof the set \\(S^{\\prime}\\) we find roughly coincide, if \\(P \\neq\n\\textit{NP}\\) then in some cases \\(Kt(S^{\\prime}) \\gg K(S^{\\prime})\\).\nBoth positions, the theory that search generates new information and\nthe theory that it does not, are counterintuitive from different\nperspectives. \nThe P vs. NP problem, that appears to be\nvery hard, has been a rich source of research in computer science and\nmathematics although relatively little has been published on its\nphilosophical relevance. That a solution might have profound\nphilosophical impact is illustrated by a quote from Scott\nAaronson: \nIf P = NP, then the world would be a profoundly different place than\nwe usually assume it to be. There would be no special value in\n“creative leaps,” no fundamental gap between solving a\nproblem and recognizing the solution once it’s found. Everyone\nwho could appreciate a symphony would be Mozart; everyone who could\nfollow a step-by-step argument would be Gauss…. \n(Aaronson 2006 – in the Other Internet Resources) \nIn fact, if \\(\\textrm{P}=\\textrm{NP}\\) then every object that has a description that is not too large and easy to check is also easy to find.  \nIn current scientific methodology the sequential aspects of the\nscientific process are formalized in terms of the empirical cycle,\nwhich according to de Groot (1969) has the following stages: \nIn the context of information theory the set of observations will be a\ndata set and we can construct models by observing regularities in this\ndata set. Science aims at the construction of true models of our\nreality. It is in this sense a semantical venture. In the 21-st\ncentury the process of theory formation and testing will for the\nlargest part be done automatically by computers working on large\ndatabases with observations. Turing award winner Jim Grey framed the\nemerging discipline of e-science as the fourth data-driven paradigm of\nscience. The others are empirical, theoretical and computational. As\nsuch the process of automatic theory construction on the basis of data\nis part of the methodology of science and consequently of philosophy\nof information (Adriaans & Zantinge 1996; Bell, Hey, & Szalay\n2009; Hey, Tansley, and Tolle 2009). Many well-known learning\nalgorithms, like decision tree induction, support vector machines,\nnormalized information distance and neural networks, use entropy based\ninformation measures to extract meaningful and useful models out of\nlarge data bases. The very name of the discipline Knowledge Discovery\nin Databases (KDD) is witness to the ambition of the Big Data research\nprogram. We quote: \nAt an abstract level, the KDD field is concerned with the development\nof methods and techniques for making sense of data. The basic problem\naddressed by the KDD process is one of mapping low-level data (which\nare typically too voluminous to understand and digest easily) into\nother forms that might be more compact (for example, a short report),\nmore abstract (for example, a descriptive approximation or model of\nthe process that generated the data), or more useful (for example, a\npredictive model for estimating the value of future cases). At the\ncore of the process is the application of specific data-mining methods\nfor pattern discovery and extraction. (Fayyad, Piatetsky-Shapiro,\n& Smyth 1996: 37) \nMuch of the current research focuses on the issue of selecting an\noptimal computational model for a data set. The theory of Kolmogorov\ncomplexity is an interesting methodological foundation to study\nlearning and theory construction as a form of data compression. The\nintuition is that the shortest theory that still explains the data is\nalso the best model for generalization of the observations. A crucial\ndistinction in this context is the one between one- and two-part\ncode optimization:  \nOne-part Code Optimization: The methodological\naspects of the theory of Kolmogorov complexity become clear if we\nfollow its definition. We begin with a well-formed dataset y\nand select an appropriate universal machine \\(U_j\\). The expression\n\\(U_j(\\overline{T_i}x)= y\\) is a true sentence that gives us\ninformation about y. The first move in the development of a\ntheory of measurement is to force all expressiveness to the\ninstructional or procedural part of the sentence by a restriction to\nsentences that describe computations on empty input:  \nThis restriction is vital for the proof of invariance. From this, in\nprinciple infinite, class of sentences we can measure the length when\nrepresented as a program. We select the ones (there might be more than\none) of the form \\(\\overline{T_i}\\) that are shortest. The length\n\\(\\mathit{l}(\\overline{T_i})\\) of such a shortest description is a\nmeasure for the information content of y. It is asymptotic in\nthe sense that, when the data set y grows to an infinite\nlength, the information content assigned by the choice of another\nTuring machine will never vary by more than a constant in the limit.\nKolmogorov complexity measures the information content of a data set\nin terms of the shortest description of the set of instructions that\nproduces the data set on a universal computing device. \nTwo-part Code Optimization: Note that by restricting\nourselves to programs with empty input and the focus on the length\n of programs instead of their content we gain the\nquality of invariance for our measure, but we also lose a lot of\nexpressiveness. The information in the actual program that produces\nthe data set is neglected. Subsequent research therefore has focused\non techniques to make the explanatory power, hidden in the Kolmogorov\ncomplexity measure, explicit.  \nA possible approach is suggested by an interpretation of Bayes’\nlaw. If we combine Shannon’s notion of an optimal code with\nBayes’ law, we get a rough theory about optimal model selection.\nLet \\(\\mathcal{H}\\) be a set of hypotheses and let x be a data\nset. Using Bayes’ law, the optimal computational model under\nthis distribution would be:  \nThis is equivalent to optimizing:  \nHere \\(-\\log P(M)\\) can be interpreted as the length of the optimal\nmodel code in Shannon’s sense and \\(- \\log P(x\\mid M)\\) as\nthe length of the optimal data-to-model code; i.e., the data\ninterpreted with help of the model. This insight is canonized in the\nso-called: \nMinimum Description Length (MDL) Principle: The best\ntheory to explain a data set is the one that minimizes the sum in bits\nof a description of the theory (model code) and of the data set\nencoded with the theory (the data to model code).  \nThe MDL principle is often referred to as a modern version of\nOckham’s razor (see entry on\n William of Ockham),\n although in its original form Ockham’s razor is an ontological\nprinciple and has little to do with data compression. In many cases\nMDL is a valid heuristic tool and the mathematical properties of the\ntheory have been studied extensively (Grünwald 2007). Still MDL,\nOckham’s razor and two-part code optimization have been the\nsubject of considerable debate in the past decennia (e.g., Domingos\n1998; McAllister 2003).  \nThe philosophical implications of the work initiated by Solomonoff,\nKolmogorov and Chaitin in the sixties of the 20-th century are\nfundamental and diverse. The universal distribution m proposed\nby Solomonoff, for instance, codifies all possible mathematical\nknowledge and when updated on the basis of empirical observations\nwould in principle converge to an optimal scientific model of our\nworld. In this sense the choice of a universal Turing machine as basis\nfor our theory of information measurement has philosophical\nimportance, specifically for methodology of science. A choice for a\nuniversal Turing machine can be seen as a choice of a set of\nbias for our methodology. There are roughly two schools: \nBoth approaches have their value. For rigid mathematical proofs the\npoor machine approach is often best. For practical applications on\nfinite data sets the rich model strategy often gets much better\nresults, since a poor machine would have to “re-invent the\nwheel” every time it compresses a data set. This leads to the\nconclusion that Kolmogorov complexity inherently contains a theory\nabout scientific bias and as such implies a methodology in which the\nclass of admissible universal models should be explicitly formulated\nand motivated a priori. In the past decennia there have been\na number of proposals to define a formal unit of measurement of the\namount of structural (or model-) information in a data set. \nThree intuitions dominate the research. A string is\n“interesting” when …  \nSuch models penalize both maximal entropy and low information content.\nThe exact relationship between these intuitions is unclear. The\nproblem of meaningful information has been researched extensively in\nthe past years, but the ambition to formulate a universal method for\nmodel selection based on compression techniques seems to be misguided:\n \nObservation: A measure of meaningful information\nbased on two-part code optimization can never be invariant in\nthe sense of Kolmogorov complexity (Bloem et al. 2015). \n \nThis appears to be the case even if we restrict ourselves to weaker\ncomputational models like total functions, but more research is\nnecessary. There seems to be no a priori mathematical\njustification for the approach, although two-part code optimization\ncontinues to be a valid approach in an empirical setting of data sets\nthat have been created on the basis of repeated\nobservations. Phenomena that might be related to a theory of\nstructural information and that currently are ill-understood are:\nphase transitions in the hardness of satisfiability problems related\nto their complexity (Simon & Dubois 1989; Crawford & Auton\n1993) and phase transitions in the expressiveness of Turing machines\nrelated to their complexity (Crutchfield & Young 1989, 1990;\nLangton 1990; Dufort & Lumsden 1994).  \nMany basic concepts of information theory were developed in the\nnineteenth century in the context of the emerging science of\nthermodynamics. There is a reasonable understanding of the\nrelationship between Kolmogorov Complexity and Shannon information (Li\n& Vitányi 2008; Grünwald & Vitányi 2008;\nCover & Thomas 2006), but the unification between the notion of\nentropy in thermodynamics and Shannon-Kolmogorov information is very\nincomplete apart from some very ad hoc insights\n(Harremoës & Topsøe 2008; Bais & Farmer 2008).\nFredkin and Toffoli (1982) have proposed so-called billiard ball\ncomputers to study reversible systems in thermodynamics (Durand-Lose\n2002) (see the entry on\n information processing and thermodynamic entropy).\n Possible theoretical models could with high probability be\ncorroborated with feasible experiments (e.g., Joule’s adiabatic\nexpansion, see Adriaans 2008).  \nQuestions that emerge are: \nThese problems seem to be hard because 150 years of research in\nthermodynamics still leaves us with a lot of conceptual unclarities in\nthe heart of the theory of thermodynamics itself (see entry on\n thermodynamic asymmetry in time). \nReal numbers are not accessible to us in finite computational\nprocesses yet they do play a role in our analysis of thermodynamic\nprocesses. The most elegant models of physical systems are based on\nfunctions in continuous spaces. In such models almost all points in\nspace carry an infinite amount of information. Yet, the cornerstone of\nthermodynamics is that a finite amount of space has finite entropy.\nThere is, on the basis of the theory of quantum information, no\nfundamental reason to assume that the expressiveness of real numbers\nis never used in nature itself on this level. This problem is related\nto questions studied in philosophy of mathematics (an intuitionistic\nversus a more platonic view). The issue is central in some of the more\nphilosophical discussions on the nature of computation and information\n(Putnam 1988; Searle 1990). The problem is also related to the notion\nof phase transitions in the description of nature (e.g.,\nthermodynamics versus statistical mechanics) and to the idea of levels\nof abstraction (Floridi 2002).  \nIn the past decade some progress has been made in the analysis of\nthese questions. A basic insight is that the interaction between time\nand computational processes can be understood at an abstract\nmathematical level, without the burden of some intended physical\napplication (Adriaans & van Emde Boas 2011). Central is the\ninsight that deterministic programs do not generate new information.\nConsequently deterministic computational models of physical systems\ncan never give an account of the growth of information or entropy in\nnature: \nObservation: The Laplacian assumption that the\nuniverse can be described as a deterministic computer is, given the\nfundamental theorem of Adriaans and van Emde Boas (2011) and the\nassumption that quantum physics as a essentially stochastic\ndescription of the structure of our reality, incorrect.  \nA statistical reduction of thermodynamics to a deterministic theory\nlike Newtonian physics leads to a notion of entropy that is\nfundamentally different from the information processed by\ndeterministic computers. From this perspective the mathematical models\nof thermodynamics, which are basically differential equations on\nspaces of real numbers, seem to operate on a level that is not\nexpressive enough. More advanced mathematical models, taking in to\naccount quantum effects, might resolve some of the conceptual\ndifficulties. At a subatomic level nature seems to be inherently\nprobabilistic. If probabilistic quantum effects play a role in the\nbehavior of real billiard balls, then the debate whether entropy\nincreases in an abstract gas, made out of ideal balls, seems a bit\nacademic. There is reason to assume that stochastic phenomena at\nquantum level are a source of probability at a macroscopic scale\n(Albrecht & Phillips 2014). From this perspective the universe is\na constant source of, literally, astronomical amounts of information\nat any scale.  \nLogical and computational approaches to the understanding of\ninformation both have their roots in the “linguistic turn”\nthat characterized the philosophical research in the beginning of the\ntwentieth century and the elementary research questions originate from\nthe work of Frege (1879, 1892, see the entry on\n logic and information).\n The ambition to quantify information in sets of true\nsentences, as apparent in the work of researchers like Popper,\nCarnap, Solomonoff, Kolmogorov, Chaitin, Rissanen, Koppel,\nSchmidthuber, Li, Vitányi and Hutter is an inherently semantic\nresearch program. In fact, Shannon’s theory of information is\nthe only modern approach that explicitly claims to be non-semantic.\nMore recent quantitative information measures like Kolmogorov\ncomplexity (with its ambition to codify all scientific knowledge in\nterms of a universal distribution) and quantum information (with its\nconcept of observation of physical systems) inherently assume\na semantic component. At the same time it is possible to develop\nquantitative versions of semantic theories (see entry on\n semantic conceptions of information).\n  \nThe central intuition of algorithmic complexity theory that an\nintension or meaning of an object can be a computation, was originally\nformulated by Frege (1879, 1892). The expressions “1 + 4”\nand “2 + 3” have the same extension (Bedeutung)\n“5”, but a different intension (Sinn). In this\nsense one mathematical object can have an infinity of different\nmeanings. There are opaque contexts in which such a distinction is\nnecessary. Consider the sentence “John knows that \\(\\log_2 2^2 =\n2\\)”. Clearly the fact that \\(\\log_2 2^2\\) represents a specific\ncomputation is relevant here. The sentence “John knows that \\(2\n= 2\\)” seems to have a different meaning. \nDunn (2001, 2008) has pointed out that the analysis of information in\nlogic is intricately related to the notions of intension and\nextension. The distinction between intension and extension is already\nanticipated in the \n Port Royal Logic (1662)\nand the writings of Mill (1843),\nBoole (1847) and Peirce (1868) but was systematically introduced in\nlogic by Frege (1879, 1892). In a modern sense the extension of a\npredicate, say “X is a bachelor”, is simply the set\nof bachelors in our domain. The intension is associated with the\nmeaning of the predicate and allows us to derive from the fact that\n“John is a bachelor” the facts that “John is\nmale” and “John is unmarried”. It is clear that this\nphenomenon has a relation with both the possible world interpretation\nof modal operators and the notion of information. A bachelor is by\nnecessity also male, i.e., in every possible world in which John is a\nbachelor he is also male, consequently: If someone gives me the\ninformation that John is a bachelor I get the information that he is\nmale and unmarried for free.  \nThe possible world interpretation of modal operators (Kripke 1959) is\nrelated to the notion of “state description” introduced by\nCarnap (1947). A state description is a conjunction that contains\nexactly one of each atomic sentence or its negation (see\n section 4.3).\n The ambition to define a good probability measure for state\ndescriptions was one of the motivations for Solomonoff (1960, 1997) to\ndevelop algorithmic information theory. From this perspective\nKolmogorov complexity, with its separation of data types (programs,\ndata, machines) and its focus on true sentences describing effects of\nprocesses is basically a semantic theory. This is immediately clear if\nwe evaluate the expression: \nAs is explained in\n section 5.2.1\n the expression \\(U_j(\\overline{T_i}x)\\) denotes the result of the\nemulation of the computation \\(T_i(x)\\) by \\(U_j\\) after reading the\nself-delimiting description \\(\\overline{T_i}\\) of machine \\(T_j\\).\nThis expression can be interpreted as a piece of semantic\ninformation in the context of the informational map (See\nentry on\n semantic conceptions of information)\n as follows:  \nThe logical structure of the sentence \\(U_j(\\overline{T_i}x)= y\\) is\ncomparable to a true sentence like:  \nIn the context of empirical observations on planet earth, the bright\nstar you can see in the morning in the eastern sky is Venus  \nMutatis mutandis one could develop the following\ninterpretation: \\(U_j\\) can be seen as a context that, for instance,\ncodifies a bias for scientific observations on earth,\ny is the extension Venus, \\(\\overline{T_i}x\\) is the\nintension “the bright star you can see in the morning\nin the eastern sky”. The intension consists of \\(T_i\\), which\ncan be interpreted as some general astronomical observation routine\n(e.g., instructional data), and x provides the well-formed data\nthat tells one where to look (bright star in the morning in the\neastern sky).  \nThis suggests a possible unification between more truth oriented\ntheories of information and computational approaches in terms of the\ninformational map presented in the entry of\n semantic conceptions of information.\n We delineate some research questions: \nEver since Descartes, the idea that the meaningful world, we perceive\naround us, can be reduced to physical processes has been a predominant\ntheme in western philosophy. The corresponding philosophical\nself-reflection in history neatly follows the technical developments\nfrom: Is the human mind an automaton, to is the mind a Turing machine\nand, eventually, is the mind a quantum computer? It is not the place\nhere to discuss these matters extensively, but the corresponding\nproblem in philosophy of information is relevant: \nOpen problem: Can meaning be reduced to computation?\n \nThe question is interwoven with more general issues in philosophy and\nits answer directly forces a choice between a more\npositivistic or a more hermeneutical approach to\nphilosophy, with consequences for theory of knowledge, metaphysics,\naesthetics and ethics. It also effects direct practical decisions we\ntake on a daily basis. Should the actions of a medical doctor be\nguided by evidence based medicine or by the notion of\ncaritas? Is a patient a conscious human being that wants to\nlead a meaningful life, or is he ultimately just a system that needs\nto be repaired?  \nThe idea that meaning is essentially a computational phenomenon may\nseem extreme, but here are many discussions and theories in science,\nphilosophy and culture that implicitly assume such a view. In popular\nculture, e.g., there is a remarkable collection of movies and books in\nwhich we find evil computers that are conscious of themselves (2001,\nA Space Odyssey), individuals that upload their consciousness\nto a computer (1992, The Lawnmower Man), and fight battles in\nvirtual realities (1999, The Matrix). In philosophy the\nposition of Bostrom (2003), who defends the view that it is very\nlikely that we already live in a computer simulation, is illustrative.\nThere are many ways to argue the pros and cons of the reduction of\nmeaning to computation. We give an overview of possible arguments for\nthe two extreme positions:  \nMeaning is an emergent aspect of computation: Science is our\nbest effort to develop a valid objective theoretical description of\nthe universe based on intersubjectively verifiable repeated\nobservations. Science tells us that our reality at a small scale\nconsists of elementary particles whose behavior is described by exact\nmathematical models. At an elementary level these particles interact\nand exchange information. These processes are essentially\ncomputational. At this most basic level of description there is no\nroom for a subjective notion of meaning. There is no reason to deny\nthat we as human being experience a meaningful world, but as such this\nmust be an emergent aspect of nature. At a fundamental level it does\nnot exist. We can describe our universe as a big quantum computer. We\ncan estimate the information storage content of our universe to be\n\\(10^{92}\\) bits and the number of computational steps it made since\nthe big bang as \\(10^{123}\\) (Lloyd 2000; Lloyd & Ng 2004). As\nhuman beings we are just subsystems of the universe with an estimated\ncomplexity of roughly \\(10^{30}\\) bits. It might be technically\nimpossible, but there seems to be no theoretical objection against the\nidea that we can in principle construct an exact copy of a human\nbeing, either as a direct physical copy or as a simulation in a\ncomputer. Such an “artificial” person will experience a\nmeaningful world, but the experience will be emergent.  \nMeaning is ontologically rooted in our individual experience of\nthe world and thus irreducible: The reason scientific theories\neliminate most semantic aspects of our world, is caused by the very\nnature of methodology of science itself. The essence of meaning and\nthe associated emotions is that they are rooted in our individual\nexperience of the world. By focusing on repeated observations of\nsimilar events by different observers scientific methodology excludes\nthe possibility of an analysis of the concept of meaning a\npriori. Empirical scientific methodology is valuable in the sense\nthat it allows us to abstract from the individual differences of\nconscious observers, but there is no reason to reduce our ontology to\nthe phenomena studied by empirical science. Isolated individual events\nand observations are by definition not open to experimental analysis\nand this seems to be the point of demarcation between science and the\nhumanities. In disciplines like history, literature, visual art and\nethics we predominantly analyze individual events and individual\nobjects. The closer these are to our individual existence, the more\nmeaning they have for us. There is no reason to doubt the fact that\nsentences like “Guernica is a masterpiece that shows the\natrocities of war” or “McEnroe played such an inspired\nmatch that he deserved to win” uttered in the right context\nconvey meaningful information. The view that this information content\nultimately should be understood in terms of computational processes\nseems too extreme to be viable.  \nApart from that, a discipline like physics, that until recently\noverlooked about 68% of the energy in the universe and 27% of the\nmatter, that has no unified theory of elementary forces and only\nexplains the fundamental aspects of our world in terms of mathematical\nmodels that lack any intuitive foundation, for the moment does not\nseem to converge to a model that could be an adequate basis for a\nreductionistic metaphysics.  \nAs soon as one defines information in terms of true statements, some\nmeanings become computational and others lack that feature. In the\ncontext of empirical science we can study groups of researchers that\naim at the construction of theories generalizing structural\ninformation in data sets of repeated observations. Such processes of\ntheory construction and intersubjective verification and\nfalsification have an inherent computational component. In fact,\nthis notion of intersubjective verification seems an essential element\nof mathematics. This is the main cause of the fact that central\nquestions of humanities are not open for quantitative analysis: We can\ndisagree on the question whether one painting is more beautiful than\nthe other, but not on the fact that there are two paintings.  \nIt is clear that computation as a conceptual model pays a role in many\nscientific disciplines varying from cognition (Chater &\nVitányi 2003), to biology (see entry on\n biological information)\n and physics (Lloyd & Ng 2004; Verlinde 2011, 2017). Extracting\nmeaningful models out of data sets by means of computation is the\ndriving force behind the Big Data revolution (Adriaans & Zantinge\n1996; Bell, Hey, & Szalay 2009; Hey, Tansley, & Tolle 2009).\nEverything that multinationals like Google and Facebook\n“know” about individuals is extracted from large data\nbases by means of computational processes, and it cannot be denied\nthat this kind of “knowledge” has a considerable amount of\nimpact on society. The research question “How can we construct\nmeaningful data out of large data sets by means of computation?”\nis a fundamental meta-problem of science in the twenty-first century\nand as such part of philosophy of information, but there is no strict\nnecessity for a reductionistic view.  \nThe first domain that could benefit from philosophy of information is\nof course philosophy itself. The concept of information potentially\nhas an impact on almost all philosophical main disciplines, ranging\nfrom logic, theory of knowledge, to ontology and even ethics and\nesthetics (see introduction above). Philosophy of science and\nphilosophy of information, with their interest in the problem of\ninduction and theory formation, probably both could benefit from\ncloser cooperation (see\n 4.1 Popper: Information as degree of falsifiability).\n The concept of information plays an important role in the history of\nphilosophy that is not completely understood (see\n 2. History of the term and the concept of information).\n  \nAs information has become a central issue in almost all of the\nsciences and humanities this development will also impact\nphilosophical reflection in these areas. Archaeologists, linguists,\nphysicists, astronomers all deal with information. The first thing a\nscientist has to do before he can formulate a theory is gathering\ninformation. The application possibilities are abundant. Datamining\nand the handling of extremely large data sets seems to be an essential\nfor almost every empirical discipline in the twenty-first century. \nIn biology we have found out that information is essential for the\norganization of life itself and for the propagation of complex\norganisms (see entry on\n biological information).\n One of the main problems is that current models do not explain the\ncomplexity of life well. Valiant has started a research program that\nstudies evolution as a form of computational learning (Valiant 2009)\nin order to explain this discrepancy. Aaronson (2013) has argued\nexplicitly for a closer cooperation between complexity theory and\nphilosophy. \nUntil recently the general opinion was that the various notions of\ninformation were more or less isolated but in recent years\nconsiderable progress has been made in the understanding of the\nrelationship between these concepts. Cover and Thomas (2006), for\ninstance, see a perfect match between Kolmogorov complexity and\nShannon information. Similar observations have been made by\nGrünwald and Vitányi (2008). Also the connections that\nexist between the theory of thermodynamics and information theory have\nbeen studied (Bais & Farmer 2008; Harremoës &\nTopsøe 2008) and it is clear that the connections between\nphysics and information theory are much more elaborate than a mere\nad hoc similarity between the formal treatment of entropy and\ninformation suggests (Gell-Mann & Lloyd 2003; Verlinde (2011,\n2017). Quantum computing is at this moment not developed to a point\nwhere it is effectively more powerful than classical computing, but\nthis threshold might be passed in the coming years. From the point of\nview of philosophy many conceptual problems of quantum physics and\ninformation theory seem to merge into one field of related questions:\n \nThe notion of information has become central in both our society and\nin the sciences. Information technology plays a pivotal role in the\nway we organize our lives. It also has become a basic category in the\nsciences and the humanities. Philosophy of information, both as a\nhistorical and a systematic discipline, offers a new perspective on\nold philosophical problems and also suggests new research domains.","contact.mail":"P.W.Adriaans@uva.nl","contact.domain":"uva.nl"}]
