[{"date.published":"2018-12-03","url":"https://plato.stanford.edu/entries/scientific-reproducibility/","author1":"Fiona Fidler","author1.info":"https://johnericwilcox.weebly.com","entry":"scientific-reproducibility","body.text":"\n\n\nThe terms “reproducibility crisis” and “replication\ncrisis” gained currency in conversation and in print over the\nlast decade (e.g., Pashler & Wagenmakers 2012), as disappointing\nresults emerged from large scale reproducibility projects in various\nmedical, life and behavioural sciences (e.g., Open Science\nCollaboration, OSC 2015). In 2016, a poll conducted by the journal\nNature reported that more than half (52%) of scientists\nsurveyed believed science was facing a “replication\ncrisis” (Baker 2016). More recently, some authors have moved to\nmore positive terms for describing this episode in science; for\nexample, Vazire (2018) refers instead to a “credibility\nrevolution” highlighting the improved methods and open science\npractices it has motivated.\n\n\nThe crisis often refers collectively to at least the following things:\n\n\n the virtual absence of replication studies in the published\nliterature in many scientific fields (e.g., Makel, Plucker, &\nHegarty 2012), \n\nwidespread failure to reproduce results of published studies in\nlarge systematic replication projects (e.g., OSC 2015; Begley &\nEllis 2012), \n\nevidence of publication bias (Fanelli 2010a), \n\na high prevalence of “questionable research\npractices”, which inflate the rate of false positives in the\nliterature (Simmons, Nelson, & Simonsohn 2011; John, Loewenstein,\n& Prelec 2012; Agnoli et al. 2017; Fraser et al. 2018), and \n\nthe documented lack of transparency and completeness in the\nreporting of methods, data and analysis in scientific publication\n(Bakker & Wicherts 2011; Nuijten et al. 2016). \n\nThe associated open science reform movement aims to rectify conditions\nthat led to the crisis. This is done by promoting activities such as\ndata sharing and public pre-registration of studies, and by advocating\nstricter editorial policies around statistical reporting including\npublishing replication studies and statistically non-significant\nresults.\n\n\nThis review consists of four distinct parts. First, we look at the\nterm “reproducibility” and related terms like\n“repeatability” and “replication”, presenting\nsome definitions and conceptual discussion about the epistemic\nfunction of different types of replication studies. Second, we\ndescribe the meta-science research that has established and\ncharacterised the reproducibility crisis, including large scale\nreplication projects and surveys of questionable research practices in\nvarious scientific communities. Third, we look at attempts to address\nepistemological questions about the limitations of replication, and\nwhat value it holds for scientific inquiry and the accumulation of\nknowledge. The fourth and final part describes some of the many\ninitiatives the open science reform movement has proposed (and in many\ncases implemented) to improve reproducibility in science. In addition,\nwe reflect there on the values and norms which those reforms embody,\nnoting their relevance to the debate about the role of values in the\nphilosophy of science.\n\nA starting point in any philosophical exploration of reproducibility\nand related notions is to consider the conceptual question of what\nsuch notions mean. According to some (e.g., Cartwright 1991), the\nterms “replication”, “reproduction” and\n“repetition” denote distinct concepts, while others use\nthese terms interchangeably (e.g., Atmanspacher & Maasen 2016a).\nDifferent disciplines can have different understandings of these terms\ntoo. In computational disciplines, for example,\nreproducibility often refers to the ability to reproduce\ncomputations alone, that is, it relates exclusively to sharing and\nsufficiently annotating data and code (e.g., Peng 2011, 2015). In\nthose disciplines, replication describes the redoing of whole\nexperiments (Barba 2017, Other Internet Resources). In psychology and\nother social and life\nsciences, however, reproducibility may refer to either the\nredoing of computations, or the redoing of experiments. The\nReproducibility Projects, coordinated by the Center for Open Science,\nredo entire studies, data collection and analysis. A recent funding\nprogram announcement by DARPA (US Defense Advanced Research Programs\nAgency) distinguished between reproducibility and replicability, where\nthe former refers to computational reproducibility and the latter to\nthe redoing of experiments. Here we use all three\nterms—“replication”, “reproduction” and\n“repetition”—interchangeably, unless explicitly\ndescribing the distinctions of other authors. \nWhen describing a study as “replicable”, people could have\nin mind either of at least two different things. The first is that the\nstudy is replicable in principle the sense that it can be\ncarried out again, particularly when its methods, procedures and\nanalysis are described in a sufficiently detailed and transparent way.\nThe second is that the study is replicable in that sense that it can\nbe carried out again and, when this happens, the replication study\nwill successfully produce the same or sufficiently\nsimilar results as the original. A study may be replicable in the\nformer sense but not in the second sense: one might be able to\nreplicate the methods, procedures and analysis of a study, but fail to\nsuccessfully replicate the results of the original study. Similarly,\nwhen people talk of a “replication”, they could also have\nin mind two different things: the replication of the methods,\nprocedures and analysis of a study (irrespective of the results) or,\nalternatively, the replication of such methods, procedures and\nanalysis as well as the results. \nArguably, most typologies of replication make more or less\nfine-grained distinctions between direct replication (which\nclosely follow the original study to verify results) and\nconceptual replications (which deliberately alter important\nfeatures of the study to generalize findings or to test the underlying\nhypothesis in a new way). As suggested, this distinction may not\nalways be known by these terms. For example, roughly the same\ndistinction is referred to as exact and inexact\nreplication by Keppel (1982); concrete and\nconceptual replication by Sargent (1981), and literal,\noperational and constructive replication by Lykken (1968).\nComputational reproducibility is most often direct (reproducing\nparticular analysis outcomes from the same data set using the same\ncode and software), but it can also be conceptual (analysing the same\nraw data set with alternative approaches, different models or\nstatistical frameworks). For an example of a conceptual computational\nreproducibility study, see Silberzahn and Uhlmann 2015. \nWe do not attempt to resolve these disciplinary differences or to\ncreate a new typology of replication, and instead we will provide a\nlimited snapshot of the conceptual terrain by surveying three existing\ntypologies—from Stefan Schmidt (2009), from Omar Gómez,\nNatalia Juristo, and Sira Vegas (2010) and from Hans Radder.\nSchmidt’s account has been influential and widely-cited in\npsychology and social sciences, where the replication crisis\nliterature is heavily concentrated. Gómez, Juristo, and\nVegas’s (2010) typology of replication is based on a\nmultidisciplinary survey of over 18 scholarly classifications of\nreplication studies which collectively contain more than 79 types of\nreplication. Finally, Radder’s (1996, 2003, 2006, 2009, 2012)\ntypology is perhaps best known within philosophy of science\nitself. \nSchmidt outlines five functions of replication studies in the social\nsciences: \nModifying Hendrik’s (1991) classes of variables that define a\nresearch space, Schmidt (2009) presents four classes of variables\nwhich may be altered or held constant in order for a given replication\nstudy to fulfil one of the above functions. The four classes are: \nSchmidt then systematically works through examples of how each\nfunction can be achieved by altering and/or holding a different class\nor classes of variable constant. For example, to fulfil the function\nof controlling for sampling error\n (Function 1),\n one should alter only variables regarding participant recruitment\n(Class 3), attempting to keep variables in all other classes as close\nto the original study as possible. To control for artefacts\n (Function 2),\n one should alter variables concerning the context and dependent\nvariable measures (variables in Classes 2 and 4 respectively), but\nkeep variables in 1 and 3 (information conveyed to participants and\nparticipant recruitment) as close to the original as possible.\nSchmidt, like most other authors in this area, acknowledges the\npractical limits of being able to hold all else constant. Controlling\nfor fraud\n (Function 3)\n is served by the same arrangements as controlling for artefacts\n (Function 2).\n In Schmidt’s account, controlling for sampling error, artefacts\nand fraud (Functions 1 to 3) are connected by a theme of confirming\nthe results of the original study. Functions 4 and 5 go beyond\nthis—generalizing to new populations\n (Function 4)\n which is served by changes to participant recruitment (Class 3) and\nconfirming the underlying hypothesis\n (Function 5),\n which served by changes to the information conveyed, the context and\ndependant variable measures (Classes 1, 2 and 4 respectively) but not\nchanges to participant recruitment (Class 3, although Schmidt\nacknowledges that holding the latter class of variables constant\nwhilst varying everything else is often practically impossible).\nAttempts to enable verification of the underlying research hypothesis\n(i.e., to fulfil Function 5) alone are what Schmidt classifies as\nconceptual replications, following Rosenthal (1991). Attempts\nto fulfil the other four functions are considered variants of\ndirect replications. \nIn summary, for Schmidt, direct replications control for sampling\nerror, artifacts, and fraud, and provide information about the\nreliability and validity of prior empirical work. Conceptual\nreplications help corroborate the underlying theory or substantive (as\nopposed to statistical) hypothesis in question and the extent to which\nthey generalize in new circumstances and situations. In practice,\ndirect and conceptual replications lie on a continuum, with\nreplication studies varying more or less compared to the original on\npotentially a great number of dimensions. \nGómez, Juristo, and Vega’s (2010) survey of the\nliterature in 18 disciplines identified 79 types of replication, not\nall of which they considered entirely distinct. They identify five\nmain ways in which a replication study may diverge from an initial\nstudy. With some similarities to Schimdt’s four classes\nabove: \nA change in any one or combination of these elements in a replication\nstudy corresponds to different purposes underlying the study, and\nthereby establishes a different kind of validity. Like Schmidt et al.\nthen systematically work through how changes to each of the above work\nto fulfil different epistemic functions. \nRadder (1996, 2003, 2006, 2009, 2012) distinguishes three types of\nreproducibility. One is the reproducibility of what Radder calls an\nexperiment’s material realization. Using one of\nRadder’s own examples as an illustration, two people may carry\nout the same actions to measure the mass of an object. Despite doing\nthe same actions, person A regards themselves as measuring the\nobject’s Newtonian mass while person B regards themselves\nas measuring the object’s Einsteinian mass. Here, then, the\nactions or material realization of the experimental procedure can be\nreproduced, but the theoretical descriptions of their significance\ndiffer. Radder, however, does not specify what is required for one\nmaterial realisation to be a reproduction of another, a pertinent\nquestion, especially since, as Radder himself affirms, no reproduction\nwill be exactly the same as any other reproduction (1996:\n82–83). \nA second type of reproducibility is the reproducibility of an\nexperiment, given a fixed theoretical description. For\nexample, a social scientist might conduct two experiments to examine\nsocial conformity. In one experiment, a young child might be\ninstructed to give an answer to a question before a group of other\nchildren who are, unknown to the former child, instructed to give\nwrong answers to the same question. In another experiment, an adult\nmight be instructed to give an answer to a question before a group of\nother adults who are, unknown to the former adult, instructed to give\nwrong answers to the same question. If the child and the adult give a\nwrong answer that conforms to the answers of others, then the social\nscientist might interpret the result as exemplifying social\nconformity. For Radder, the theoretical description of the experiment\nmight be fixed, specifying that if some people in a\nparticipant’s surroundings give intentionally false answers to\nthe question, then the genuine participant will conform to the\nbehaviour of their peers. However, the material realization of these\nexperiments differs insofar as one concerns children and the other\nadults. It is difficult to see how, in this example at least, this\ndiffers from what either Schmidt or Gómez, Juristo, and Vegas\nwould refer to as establishing generalizability to a different\npopulation (Schmidt’s [2009]\n Class 3\n and\n Function 5;\n Gómez, Juristo, and Vegas’s [2010]\n way 5\n and\n Function 4). \nThe third kind of reproducibility is what Radder calls\nreplicability. This is where experimental procedures differ\nto produce the same experimental result (otherwise known as a\nsuccessful replication). For example, Radder notes that multiple\nexperiments might obtain the result “a fluid of type f\nhas a boiling point b”, despite having different kinds of\nthermometers by which to measure this boiling point (2006:\n113–114). \nSchmidt (2009) points out that the difference between Radder’s\nsecond and third types of reproducibility is small in comparison to\ntheir differences to the first type. He consequently suggests his\nalternative distinction between direct and conceptual replication,\npresumably intending a conceptual replication to cover Radder’s\nsecond and third types. \nIn summary, whilst Gómez, Juristo, and Vegas’s typology\ndraws distinctions in slightly different places to Schmidt’s,\nits purpose is arguably the same—to explain what types of\nalterations in replication studies fulfil different scientific goals,\nsuch as establishing internal validity or the extent of generalization\nand so on. With the exception of his discussion of reproducing the\nmaterial realization, Radder’s other two categories can perhaps\nbe seen as fitting within the larger range of functions described by\nSchmidt and Gómez et al., who both acknowledge that in\npractice, direct and conceptual replications lie on a noisy continuum.\n \nIn psychology, the origin of the reproducibility crisis is often\nlinked to Daryl Bem’s (2011) paper which reported empirical\nevidence for the existence of “psi”, otherwise known as\nExtra Sensory Perception (ESP). This paper passed through the standard\npeer review process and was published in the high impact Journal\nof Personality and Social Psychology. The controversial nature of\nthe findings inspired three independent replication studies, each of\nwhich failed to reproduce Bem’s results. However, these\nreplication studies were rejected from four different journals,\nincluding the journal that had originally published Bem’s study,\non the grounds that the replications were not original or novel\nresearch. They were eventually published in PLoS ONE\n(Ritchie, Wiseman, & French 2012). This created controversy in the\nfield, and was interpreted by many as demonstrating how publication\nbias impeded science’s self-correction mechanism. In medicine,\nthe origin of the crisis is often attributed to Ioannidis’\n(2005) paper “Why most published findings are false”. The\npaper offered formal arguments about inflated rates of false positives\nin the literature—where a “false positive” result\nclaims a relationship exists between phenomena when it in fact does\nnot (e.g., a claim that consuming a drug is correlated with symptom\nrelief when it in fact is not). Ioannidis’ (2005) also reported\nvery low (11%) empirical reproducibility rates from a set of\npre-clinical trial replications at Amgen, later independently\npublished by Begley and Ellis (2012). In all disciplines, the\nreplication crisis is also more generally linked to earlier criticisms\nof Null Hypothesis Significance Testing (e.g., Szucs & Ioannidis\n2017), which pointed out the neglect of statistical power (e.g., Cohen\n1962, 1994) and a failure to adequately distinguish statistical and\nsubstantive hypotheses (e.g., Meehl 1967, 1978). This is discussed\nfurther below. \nIn response to the events above, a new field identifying as\nmeta-science (or meta-research) has become\nestablished over the last decade (Munafò et al. 2017).\nMunafò et al. define meta-science as “the scientific\nstudy of science itself” (2017: 1). In October 2015, Ioannidis,\nFanelli, Dunne, and Goodman identified over 800 meta-science papers\npublished in the five-month period from January to May that year, and\nestimated that the relevant literature was accruing at the rate of\napproximately 2,000 papers each year. Referring to the same bodies of\nwork with slightly different terms, Ioannidis et al. define\n“meta-research” as  \nan evolving scientific discipline that aims to evaluate and improve\nresearch practices. It includes thematic areas of methods, reporting,\nreproducibility, evaluation, and incentives (how to do, report,\nverify, correct, and reward science). (2015: 1)  \nMultiple research centres dedicated to this work now exist, including,\nfor example, the Tilburg University Meta-Research Center in\npsychology, the Meta-Research Innovation Center at Stanford (METRICS),\nand others listed in Ioannidis et al. 2015 (see\n Other Internet Resources).\n Relevant research in medical fields is also covered in Stegenga\n2018. \nProjects that self-identify as meta-science or meta-research\ninclude: \nThe most well known of these projects is undoubtedly the\nReproducibility Project: Psychology, coordinated by the now Center for\nOpen Science in Charlottesville, VA (then the Open Science\nCollaboration). It involved 270 crowd sourced researchers in 64\ndifferent institutions in 11 different countries. Researchers\nattempted direct replications of 100 studies published in three\nleading psychology journals in the year 2008. Each study was\nreplicated only once. Replications attempted to follow original\nprotocols as closely as possible, though some differences were\nunavoidable (e.g., some replication studies were done with European\nsamples when the original studies used US samples). In almost all\ncases, replication studies used larger sample sizes that the original\nstudies and therefore had greater statistical power—that is, a\ngreater probability of correctly rejecting the null hypothesis (i.e.,\nthat no relationship exists) when the hypothesis is false. A number of\nmeasures of reproducibility were reported: \nThere have been objections to the implementation and interpretation of\nthis project, most notably by Gilbert et al. (2016), who took issue\nwith the extent to which the replications studies were indeed direct\nreplications. For example, Gilbert et al. highlighted 6 specific\nexamples of “low fidelity protocols”, that is, where\nreplication studies differed in their view substantially from the\noriginal (in one case, using a European sample rather than a US sample\nof participants). However, Anderson et al. (2016) explained in a reply\nthat in half of those cases, the authors of the original study had\nendorsed the replication as being direct or close to on relevant\ndimensions and that furthermore, that independently rated similarity\nbetween original and replication studies failed to predict replication\nsuccess. Others (e.g., Etz & Vandekerckhove 2016) have applied\nBayesian reanalysis to the OSC’s (2015) data and conclude that\nup to 75% (as opposed to the OSC’s 36–47%) of replications\ncould be considered successful. However, they do note that in many\ncases this is only with very weak evidence (i.e., Bayes factors of\n<10). They too conclude that the failure to reproduce many effects\nis indeed explained by the overestimation of effect sizes, itself a\nproduct of publication bias. A Reproducibility Project: Cancer Biology\n(also coordinated by the Center for Open Science) is currently\nunderway (Errington et al. 2014), originally attempting to replicate\n50 of the highest impact studies in Cancer Biology published between\n2010–2012. This project has recently announced it will complete\nwith only 18 replication studies, as too few originals reported enough\ninformation to proceed with full replications (Kaiser 2018). Results\nof the first 10 studies are reportedly mixed, with only 5 being\nconsidered “mostly repeatable” (Kaiser 2018). \nThe Many Labs project (Klein et al. 2014) coordinated 36 independent\nreplications of 13 classic psychology phenomena (from 12 studies, that\nis, one study tested two effects), including anchoring, sunk cost bias\nand priming, amongst other well-known effects in psychology. In terms\nof matching statistical significance, the project demonstrated that 11\nout of 13 effects could be successful replicated. It also showed great\nvariation in many of the effect sizes across the 36 replications. \nIn biomedical research, there have also been a number of large scale\nreproducibility projects. An early one by Begley and Ellis (2012, but\ndiscussed earlier in Ioannidis 2005) attempted to replicate 56\nlandmark pre-clinical trials and reported an alarming reproducibility\nrate of only 11%, that is, only 6 of the 56 results could be\nsuccessfully reproduced. Subsequent attempts at large scale\nreplications in this field have produced more optimistic estimates,\nbut routinely failed to successfully reproduce more than half of the\npublished results. Freedman et al. (2015) report five replication\nprojects by independent groups of researchers which produce\nreproducibility estimates ranging from 22% to 49%. They estimate the\ncost of irreproducible research in US biomedical science alone to be\nin the order of USD$28 billion per year. A reproducibility project in\nExperimental Philosophy is an exception to the general trend,\nreporting reproducibility rates of 70% (Cova et al. forthcoming). \nFinally, the Social Science Replication Project (SSRP) redid 21\nexperimental social science studies published in the journals Nature\nand Science between 2010 and 2015. Depending on the measure taken, the\nreplication success rate was 57–67% (Camerer et al. 2018). \nThe causes of irreproducible results are largely the same across\ndisciplines we have mentioned. This is not surprising given that they\nstem from problems with statistical methods, publishing practices and\nthe incentive structures created in a “publish or perish”\nresearch culture, all of which are largely shared, at least in the\nlife and behavioral sciences. \nWhilst replication is often casually referred to as a cornerstone of\nthe scientific method, direct replication studies (as they might be\nunderstood from Schmidt or Gómez, Juristo, and Vegas’s\ntypologies above) are a rare event in the published literature of some\nscientific disciplines, most notably the life and social sciences. For\nexample, such replication attempts constitute roughly 1% of the\npublished psychology literature (Makel, Plucker, & Hegarty 2012).\nThe proportion in published ecology and evolution literature is even\nsmaller (Kelly 2017, Other Internet Resources). \nThis virtual absence of replication studies in the literature can\nexplained by the fact that many scientific journals have historically\nhad explicit policies against publishing replication studies (Mahoney\n1985)—thus giving rise to a “publication bias”. Over\n70% of editors from 79 social science journals said they preferred new\nstudies over replications and over 90% said they would did not\nencourage the submission of replication studies (Neuliep &\nCrandall 1990). In addition, many science funding bodies also fund\nonly “novel”, “original” and/or\n“groundbreaking” research (Schmidt 2009). \nA second type of publication bias has also played a substantial role\nin the reproducibility crisis, namely a bias towards\n“statistically significant” or “positive”\nresults. Unlike the bias against replication studies, this is rarely\nan explicitly stated policy of a journal. Publication bias towards\nstatistically significant findings has a long history, and was first\ndocumented in psychology by Sterling (1959). Developments in text\nmining techniques have led to more comprehensive estimates. For\nexample, Fanelli’s work has demonstrated the extent of\npublication bias in various disciplines, and the proportions of\nstatistically significant results given below are from his 2010a\npaper. He has also documented the increase of this bias over time\n(2012) and explored the causes of the bias, including the relationship\nbetween publication bias and a publish or perish research culture\n(2010b). \nIn many disciplines (e.g., psychology, psychiatry, materials science,\npharmacology and toxicology, clinical medicine, biology and\nbiochemistry, economics and business, microbiology and genetics) the\nproportion of statistically significant results is very high, close to\nor exceeding 90% (Fanelli 2010a). This is despite the fact that in\nmany of these fields, the average statistical power is low—that\nis, the average probability that a study will correctly reject the\nnull hypothesis is low. For example, in psychology the proportion of\npublished results that are statistically significant is 92% despite\nthe fact that the average power of studies in this field to detect\nmedium effect sizes (arguably typical of the discipline) is roughly\n44% (Szucs & Ioannidis 2017). If there was no bias towards\npublishing statistically significant results, the proportion of\nsignificant results should roughly match the average statistical power\nof the discipline. The excess in statistical significance (in this\ncase, the difference between 92% and 44%) is therefore an indicator\nthe strength of the bias. For a second example, in ecology,\nenvironment and plant and animal sciences the proportion of\nstatistically significant results is 74% and 78% respectively,\nadmittedly lower than in psychology. However, the most recent estimate\nof the statistical power, again of medium effect sizes, of ecology and\nanimal behaviour is 23–26% (Smith, Hardy, & Gammell 2011)\n(An earlier more optimistic assessment was 40–47%, Jennions\n& Møller, 2003.) For a third example, the proportion of\nstatistically significant results in neuroscience and behaviour is\n85%. Our best estimate of the statistical power in neuroscience is at\nbest 31%, with a lower bound estimate of 8% (Button et al. 2013). The\nassociated file-drawer problem (Rosenthal 1979)—where\nresearchers relegate failed statistically non-significant studies to\ntheir file drawers, hidden from public view—has long been\nestablished in psychology and others disciplines, and is known to lead\nto distortions in meta-analysis (where a “meta-analysis”\nis a study which analyses results across multiple other studies). \nIn addition to creating the file-drawer problem described above,\npublication bias has been held at least partially responsible for the\nhigh prevalence of Questionable Research Practices (QRPs) uncovered in\nboth self-report survey research (John, Loewenstein, & Prelec\n2012; Agnoli 2017 et al. 2017; Fraser\net al. 2018) and in journal studies that have detected, for example,\nunusual distributions of p values (Masicampo & Lalande\n2012; Hartgerink et al. 2016). Pressure to publish, now ubiquitous\nacross academic institutions, means that researchers often cannot\nafford to simply assign “failed” or statistically\nnon-significant studies to the file drawer, so instead they p\nhack and cherry-pick results (as discussed below) back to\nsignificance, and back into the published literature. Simmons, Nelson,\nand Simonsohn (2011) explained and demonstrated with simulated results\nhow engaging in such practices inflates the false positive error rate\nof the published literature, leading to a lower rate of reproducible\nresults. \n“P hacking” refers to a set of practices which\ninclude: checking the statistical significance of results before\ndeciding whether to collect more data; stopping data collection early\nbecause results have reached statistical significance; deciding\nwhether to exclude data points (e.g., outliers) only after checking\nthe impact on statistical significance and not reporting the impact of\nthe data exclusion; adjusting statistical models, for instance by\nincluding or excluding covariates based on the resulting strength of\nthe main effect of interest; and rounding of a p value to meet\na statistical significance threshold (e.g., presenting 0.053 as\nP < .05). “Cherry picking” includes failing to\nreport dependent or response variables or relationships that did not\nreach statistical significance or other threshold and/or failing to\nreport conditions or treatments that did not reach statistical\nsignificance or other threshold. “HARKing” (Hypothesising\nAfter Results are Known) includes presenting ad hoc and/or unexpected\nfindings as though they had been predicted all along (Kerr 1998); and\npresenting exploratory work as though it was confirmatory hypothesis\ntesting (Wagenmakers et al. 2012). Five of the most widespread QRPs\nare listed below in Table 1 (from Fraser et al. 2018), with associated\nsurvey measures of prevalence. \nTable 1: The prevalence of some common\nQuestionable Research Practices. Percentage (with 95% confidence\nintervals) of researches who reported having used the QRP at least\nonce (adapted from Fraser et al. 2018) \n#cherry picking,\n*p hacking,\n^HARKing \nNull Hypothesis Significance Testing (NHST)—discussed\nabove—is a commonly diagnosed cause of the current replication\ncrisis (see Szucs & Ioannidis 2017). The ubiquitous nature of NHST\nin life and behavioural sciences is well documented, most recently by\nCristea and Ioannidis (2018). This is important pre-condition for\nestablishing its role as a cause, since it could not be a cause if its\nactual use was rare. The dichotomous nature of NHST facilitates\npublication bias (Meehl 1967, 1978). For example, the language of\naccept and reject in hypothesis testing maps conveniently on to\nacceptance and rejection of manuscripts, a fact that led Rosnow and\nRosenthal (1989) to decry that “surely God loves the .06 nearly\nas much as the .05” (1989: 1277). Techniques that failed to\nenshrine a dichotomous threshold would be harder to employ in service\nof publication bias. For example, a case has been made that estimation\nusing effect sizes and confidence intervals (introduced above) would\nbe less prone to being used in service of publication bias (Cumming\n2012, Cumming and Calin-Jageman 2017. \nAs already mentioned, the average statistical power in various\ndisciplines is low. Not only is power often low, but it is virtually\nnever reported; less than 10% of published studies in psychology\nreport statistical power and even fewer in ecology do (Fidler et al.\n2006). Explanations for the widespread neglect of statistical power\noften highlight the many common misconceptions and fallacies\nassociated with p values (e.g., Haller & Krauss 2002;\nGigerenzer 2018). For example, the inverse probability\n fallacy[1]\n has been used to explain why so many researchers fail to calculate\nand report statistical power (Oakes 1986). \nIn 2017, a group of 72 authors proposed in a Nature Human\nBehaviour paper that alpha level in statistical significance\ntesting be lowered to 0.005 (as opposed to the current standard of\n0.05) to improve the reproducibility rate of published research\n(Benjamin et al. 2018). A reply from a different set of 88 authors was\npublished in the same journal, arguing against this proposal and\nstating instead that researchers should justify their alpha level\nbased on context (Lakens et al. 2018). Several other replies have\nfollowed, including a call from Andrew Gelman and colleagues to\nabandon statistical significance altogether (McShane et al. 2018, \nOther Internet Resources). The\nexchange has become known on social media as the Alpha Wars\n(e.g., in the Barely Significant blog,\n Other Internet Resources)).\n Independently, the American Statistical Association released a\nstatement on the use of p values for the first time in its\nhistory, cautioning against their overinterpretation and pointing out\nthe limits of the information they offer about replication (Wasserman\n& Lazar 2016) and devoted their association’s 2017 annual\nconvention to the theme “Scientific Method for the\n21st Century: A World Beyond \\(p <0.05\\)” (see\n Other Internet Resources).\n  \nA number of recent high-profile cases of scientific fraud have\ncontributed considerably to the amount of press around the\nreproducibility crisis in science. Often these cases (e.g., Diederik\nStapel in psychology) are used as a hook for media coverage, even\nthough the crisis itself has very little to do with scientific fraud.\n(Note also that the Questionable Research Practices above are not\ntypically counted as “fraud” or even “scientific\nmisconduct” despite their ethically dubious status.) For\nexample, Fang, Grant Steen, and Casadevall (2012) estimated that 43%\nof retracted articles in biomedical research are withdrawn because of\nfraud. However, roughly half a million biomedical articles are\npublished annually and only 400 of those are retracted (Oransky 2016,\nfounder of the website RetractionWatch), so this amounts to a very\nsmall proportion of the literature (approximately 0.1%). There are, of\ncourse, many cases of pharmaceutical companies exercising financial\npressure on scientists and the publishing industry that raise\nspeculation about how many undetected (or unretracted) cases there may\nstill be in the literature. Having said that, there is widespread\nconsensus amongst scientists in the field that the main cause of the\ncurrent reproducibility crisis is the current incentive structure in\nscience (publication bias, publish or perish, non-transparent\nstatistical reporting, lack of rewards for data sharing). Whilst this\nincentive structure can push some to scientific fraud, it appears to\nbe a very small proportion.  \nMany scientists believe that replication is epistemically valuable in\nsome way, that is to say, that replication serves a useful function in\nenhancing our knowledge, understanding or beliefs about reality. This\nsection first discusses a problem about the epistemic value of\nreplication studies—called the “experimenters\nregress”—and it then considers the claim that replication\nplays an epistemically valuable role in distinguishing scientific\ninquiry. It lastly examines a recent attempt to formalise the logic of\nreplication in a Bayesian framework. \nCollins (1985) articulated a widely discussed problem that is now\nknown as the experimenters’ regress. He initially lays\nout the problem in the context of measurement (Collins 1985: 84).\nSuppose a scientist is trying to determine the accuracy of a\nmeasurement device and also the accuracy of a measurement result.\nPerhaps, for example, a scientist is using a thermometer to measure\nthe temperature of a liquid, and it delivers a particular measurement\nresult, say, 12 degrees Celsius. \nThe problem arises because of the interdependence of the accuracy of\nthe measurement result and the accuracy of the measurement device: to\nknow whether a particular measurement result is accurate, we need to\ntest it against a measurement result that is previously known to be\naccurate, but to know that the result is accurate, we need to know\nthat it has been obtained via an accurate measuring device, and so on.\nThis, according to Collins, creates a “circle” which he\nrefers to as the “experimenters’ regress”. \nCollins extends the problem to scientific replication more generally.\nSuppose that an experiment B is a replication study of an\ninitial experiment A, and that B’s result\napparently conflicts with A’s result. This seeming\nconflict may have one of two interpretations: \nThe regress poses a problem about how to choose between these\ninterpretations, a problem which threatens the epistemic value of\nreplication studies if there are no rational grounds for choosing in a\nparticular way. Determining whether one experiment is a proper\nreplication of another is complicated by the facts that scientific\nwriting conventions often omit precise details of experimental\nmethodology (Collins 2016), and, furthermore, much of the knowledge\nthat scientists require to execute experiments is tacit and\n“cannot be fully explicated or absolutely established”\n(Collins 1985: 73). \nIn the context of experimental methodology, Collins wrote: \nTo know an experiment has been well conducted, one needs to know\nwhether it gives rise to the correct outcome. But to know what the\ncorrect outcome is, one needs to do a well-conducted experiment. But\nto know whether the experiment has been well conducted…! (2016:\n66; ellipses original) \nCollins holds that in such cases where a conflict of results arises,\nscientists tend to fraction into two groups, each holding opposing\ninterpretations of the results. According to Collins, where such\ngroups are “determined” and the “controversy runs\ndeep” (Collins 2016: 67), the dispute between the groups cannot\nbe resolved via further experimentation, for each additional result is\nsubject to the problem posed by the experimenters’\n regress.[2]\n In such cases, Collins claims that particular non-epistemic factors\nwill partly determine which interpretation becomes the lasting view:\n \nthe career, social, and cognitive interests of the scientists, their\nreputations and that of their institutions, and the perceived utility\nfor future work. (Franklin & Collins 2016: 99) \nFranklin was the most vociferous opponent of Collins, although recent\ncollaboration between the two has fostered some agreement (Collins\n2016). Franklin presented a set of strategies for validating\nexperimental results, all of which relate to “rational\nargument” on epistemic grounds (Franklin 1989: 459; 1994).\nExamples include, for instance, appealing to experimental checks on\nmeasurement devices or eliminating potential sources of error in the\nexperiment (Franklin & Collins 2016). He claimed that the fact\nthat such strategies were evidenced in scientific practice\n“argues against those who believe that rational arguments plays\nlittle, if any, role” in such validation (Franklin 1989: 459),\nwith Collins being an example. He interprets Collins as suggesting\nthat the strategies for resolving debates of the validation of results\nare social factors or “culturally accepted practices”\n(Franklin, 1989: 459) which do not provide reasons to underpin\nrational belief about results. Franklin (1994) further claims that\nCollins conflates the difficulty in successfully executing\nexperiments with the difficulty of demonstrating that\nexperiments have been executed, with Feest (2016) interpreting him to\nsay that although such execution requires tacit knowledge, one can\nnevertheless appeal to strategies to demonstrate the validity of\nexperimental findings. \nFeest (2016) examines a case study involving debates about the Mozart\neffect in psychology (which, roughly speaking, is the effect whereby\nlistening to Mozart beneficially affects some aspect of intelligence\nor brain structure). Like Collins, she agrees that there is a problem\nin determining whether conflicting results suggest a putative\nreplication experiment is not a proper replication attempt, in part\nbecause there is uncertainty about whether scientific concepts such as\nthe Mozart effect have been appropriately operationalised in earlier\nor later experimental contexts. Unlike Collins (on her\ninterpretation), however, she does not think that this uncertainty\narises because scientists have inescapably tacit knowledge of the\nlinguistic rules about the meaning and application of concepts like\nthe Mozart effect. Rather the uncertainty arises because such concepts\nare still themselves developing and because of assumptions about the\nworld that are required to successfully draw inferences from\nit. Experimental methodology then serves to reveal the previously\ntacit assumptions about the application of concepts and the legitimacy\nof inferences, assumptions which are then susceptible to scrutiny. \nFor example, in her study of the Mozart effect, she notes that\nreplication studies of the Mozart effect failed to find that Mozart\nmusic had a beneficial influence on spatial abilities. Rauscher, who\nwas the first to report results supporting the Mozart effect,\nsuggested that the later studies were not proper replications of her\nstudy (Rauscher, Shaw, and Ky 1993, 1995). She clarified that the\nMozart effect applied only to a particular category\nof spatial abilities (spatio-temporal processes) and that the later\nstudies operationalised the Mozart effect in terms of different\nspatial abilities (spatial recognition). Here, then, there was a\ndifficulty in determining whether to interpret failed replication\nresults as evidence against the initial results or rather as an\nindication that the replication studies were not proper replications.\nFeest claims this difficulty arose because of tacit knowledge or\nassumptions: assumptions about the application of the Mozart effect\nconcept to different kinds of spatial abilities, about whether the\nworld is such that Mozart music has an effect on such abilities and\nabout whether the failure of Mozart to impact other kinds of spatial\nabilities warrants the inference that the Mozart effect does not\nexist. Contra Collins, however, experimental methodology enabled the\nexplication and testing of these assumptions, thus allowing scientists\nto overcome the interpretive impasse. \nAgainst this background, her overall argument is that scientists often\nare and should be sceptical towards each other’s results.\nHowever, this is not because of inescapably tacit knowledge and the\ninevitable failure of epistemic strategies for validating results.\nRather, it is at least in part because of varying tacit assumptions\nthat researchers have about the meaning of concepts, about the world\nand about what to draw inferences from it. Progressive experimentation\nserves to reveal these tacit assumptions which can then be\nscrutinised, leading to the accumulation of knowledge. \nThere is also other philosophical literature on the\nexperimenters’ regress, including Teira’s (2013) paper\narguing that particular experimental debiasing procedures are\ndefensible against the regress from a contractualist perspective,\naccording to which self-interested scientists have reason to adopt\ngood methodological standards. \nThere is a widespread belief that science is distinct from other\nknowledge accumulation endeavours, and some have suggested that\nreplication distinguishes (or is at least essential to) science in\nthis respect. (See also the entry on \n  science and pseudo-science.).\nAccording to the Open Science Collaboration, “Reproducible\nresearch practices are at the heart of sound research and integral to\nthe scientific method.” (OSC 2015: 7). Schmidt echoes this theme:\n“To confirm results or hypotheses by a repetition procedure is\nat the basis of any scientific conception” (2009: 90). Braude\n(1979) goes so far as to say that reproducibility is a\n“demarcation criterion between science and nonscience”\n(1979: 2). Similarly, Nosek, Spies, and Motyl state that: \n[T]he scientific method differentiates itself from other approaches by\npublicly disclosing the basis of evidence for a claim…. In\nprinciple, open sharing of methodology means that the entire body of\nscientific knowledge can be reproduced by anyone. (2012: 618) \nIf replication played such an essential or distinguishing role in\nscience, we might expect it to be a prominent theme in the history of\nscience. Steinle (2016) considers the extent to which it is such a\ntheme. He presents a variety of cases from the history of science\nwhere replication played very different roles, although he understands\n“replication” narrowly to refer to when an experiment is\nre-run by different researchers. He claims that the role and\nvalue of replication in experimental replication is “much more\ncomplex than easy textbook accounts make us believe” (2016: 60),\nparticularly since each scientific inquiry is always tied to a variety\nof contextual considerations that can affect the importance of\nreplication. Such considerations include the relationship between\nexperimental results and the background of accepted theory at the\ntime, the practical and resource constraints on pursuing replication\nand the perceived credibility of the researchers. These contextual\nfactors, he claims, mean that replication was a key or even overriding\ndeterminant of acceptance of research claims in some cases, but not in\nothers. \nFor example, sometimes replication was sufficient to embrace a\nresearch claim, even if it conflicted with the background of accepted\ntheory and left theoretical questions unresolved. A case of this is\nhigh-temperature superconductivity, the effect whereby an electric\ncurrent can pass with zero resistance through a conductor at\nrelatively high temperatures. In 1986, physicists Georg Bednorz and\nAlex Müller reported finding a material which acted as a\nsuperconductor at 35 kelvin (−238 degrees Celsius). Scientists\naround the world successfully replicated the effect, and Bednorz and\nMuller were then awarded with a Nobel Prize in Physics a year after\ntheir announcement. This case is remarkable since not only did their\neffect contradict the accepted physical theory at the time, but there\nis still no extant theory that adequately explains the effects which\nthey reported (Di Bucchianico, 2014). \nAs a contrasting example, however, sometimes claims were accepted\nwithout any replication. In the 1650s, German scientist Otto von\nGuericke designed and operated the world’s first vacuum pump\nthat would visibly suck air out of a larger space. He performed\nexperiments with his device to various audiences. Yet the replication\nof his experiments by others would have been very difficult, if not\nimpossible: not only was Guericke’s pump both expensive and\ncomplicated to build, but it was also unlikely that his descriptions\nof it sufficed to enable anyone to build the pump and to consequently\nreplicate his findings. Despite this, Steinle claims that “no\ndoubts were raised about his results”, probably as a results of\nhis “public performances that could be witnessed by a large\nnumber of participants” (2016: 55). \nSteinle takes such historical cases to provide normative guidance for\nunderstanding the epistemic value as replication as context-sensitive:\nwhether replication is necessary or sufficient for establishing a\nresearch claim will depend on a variety of considerations, such as\nthose mentioned earlier. He consequently eschews wide-reaching claims,\nsuch as those that “it’s all about replicability” or\nthat “replicability does not decide anything” (2016:\n60). \nEarp and Trafimow (2015) attempt to formalise the way in which\nreplication is epistemically valuable, and they do this using a\nBayesian framework to explicate the inferences drawn from replication\nstudies. They present the framework in a context similar to that of\nCollins (1985), noting that “it is well-nigh impossible to say\nconclusively what [replication results] mean” (Earp &\nTrafimow, 2015: 3). But while replication studies are often not\nconclusive, they do believe that such studies can be\ninformative, and their Bayesian framework depicts how this is\nso. \nThe framework is set out with an example. Suppose an aficionado of\nResearcher A is highly confident that anything said by\nResearcher A is true. Some other researcher, Researcher\nB, then attempts to replicate an experiment by Researcher\nA, and Researcher B find results that conflict with\nthose of Researcher A. Earp and Trafimow claim that the\naficionado might continue to be confident in Researcher\nA’s findings, but the aficionado’s confidence is\nlikely to slightly decrease. As the number of failed replication\nattempts increases, the aficionado’s confidence accordingly\ndecreases, eventually falling below 50% and thereby placing more\nconfidence in the replication failures than in the findings initially\nreported by Researcher A. \nHere, then, suppose we are interested in the probability that the\noriginal result reported by Researcher A is true given\nResearcher B’s first replication failure. Earp and\nTrafimow represent this probability with the notation \\(p(T\\mid F)\\)\nwhere p is a probability function, T represents the\nproposition that the original result is true and F represents\nResearcher B’s replication failure. According to\nBayes’s theorem below, this probability is calculable from the\naficionado’s degree of confidence that the original result is\ntrue prior to learning of the replication failure \\(p(T)\\), their\ndegree of expectation of the replication failure on the condition that\nthe original result is true \\(p(T\\mid F)\\), and the degree to which they would\nunconditionally expect a replication failure prior to learning of the\nreplication failure \\(p(F)\\): \nRelatedly, we could instead be interested in the confidence ratio that\nthe original result is true or false given the failure to replicate.\nThis ratio is representable as \\(\\frac{p(T\\mid F)}{p(\\nneg T\\mid F)}\\)\nwhere \\(\\nneg T\\) represents the proposition that the original result\nis false. According to the standard Bayesian probability calculus,\nthis ratio in turn is related to a product of ratios concerning  \nThis relation is expressed in the equation: \nNow Earp and Trafimow assign some values to the terms on the\nright-hand of the equation for (2). Supposing that the aficionado is\nconfident in the original results, they set the ratio\n\\(\\frac{p(T)}{p(\\nneg T)}\\) to 50, meaning that the aficionado is\ninitially fifty times more confident that the results are true than\nthat the results are false. \nThey also set the ratio \\(\\frac{p(F\\mid T)}{p(F\\mid \\nneg T)}\\). about\nthe conditional expectation of a replication failure to 0.5, meaning\nthat the aficionado is considerably less confident that there will be\na replication failure if the original result is true than if it is\nfalse. They point out that the extent to which the aficionado is less\nconfident depends on the quality of so-called auxiliary\nassumptions about the replication experiment. Here, auxiliary\nassumptions are assumptions which enable one to infer that particular\nthings should be observable if the theory under test is true. The\nintuitive idea is that the higher the quality of the assumptions about\na replication study, the more one would expect to observe a successful\nreplication if the original result was true. While they do not specify\nprecisely what makes such auxiliary assumptions have high\n“quality” in this context, presumably this quality\nconcerns the extent to which the assumptions are probably true and the\nextent to which the replication experiment is an appropriate test of\nthe veracity of the original results if the assumptions are true. \nOnce the ratios on the right-hand of equation (2) are set as such, one\ncan see that a replication failure would reduce one’s confidence\nin the original results: \nHere, then, a replication failure would reduce the aficionado’s\nconfidence that the original result was true so that the aficionado\nwould be only 25 times more confident that the result is true given a\nfailure (as per \\(\\frac{p(T\\mid F)}{p(\\nneg T\\mid F)}\\)) rather than\n50 times more confident that it is true (as per \\(\\frac{p(T)}{p(\\nneg\nT)}\\)). \nNevertheless, the aficionado may still be confident that the original\nresult is true, but we can see how such confidence would decrease with\nsuccessive replication failures. More formally, let \\(F_N\\) be the\nlast replication failure in a sequence of N replication\nfailures \\(\\langle F_1,F_2,\\ldots,F_N\\rangle\\). Then, the\naficionado’s confidence in the original result given the\nNth replication failure is expressible in the\n equation:[3] \nFor example, suppose there are 10 replication failures, and so\n\\(N=10\\). Suppose further that the confidence ratios for the\nreplication failures are set such that: \nThen, \nHere, then, the aficionado’s confidence in the original result\ndecreases so that they are more confident that it was false than that\nit was true. Hence, on Earp and Trafimow’s Bayesian account,\nsuccessive replication failures can progressively erode one’s\nconfidence that an original result is true, even if one was initially\nhighly confident in the original result and even if no single\nreplication failure by itself was\n conclusive.[4] \nSome putative merits of Earp and Trafimow’s account, then, are\nthat it provides a formalisation whereby replication attempts are\ninformative even if they are not conclusive, and furthermore, the\nformalisation provides a role for both quantity of replication\nattempts as well as auxiliary assumptions about the replications. \nThe aforementioned meta-science has unearthed a range of problems\nwhich give rise to the reproducibility crisis, and the open science\nmovement has proposed or promoted various solutions—or\nreforms—for these problems. These reforms can be grouped into\nfour categories: (a) methods and training, (b) reporting and\ndissemination, (c) peer review processes, and (d) evaluating new\nincentive structures (loosely following the categories used by\nMunafò et al. 2017 and Ioannidis et al. 2015). In subsections\n4.1–4.4 below, we present a non-exhaustive list of initiatives\nin each of the above categories. These initiatives are reflections of\nvarious values and norms that are at the heart of the open science\nmovement, and we discuss these values and norms in 4.5. \nThere has long been philosophical debate about what role values do and\nshould play in science (Churchman 1948; Rudner 1953; Douglas 2016),\nand the reproducibility crisis is intimately connected to questions\nabout the operations of, and interconnections between, such values. In\nparticular, Nosek et al. (2017) argue that there is a tension between truth and\npublishability. More specifically, for reasons discussed in section 2\nabove, the accuracy of scientific results are compromised by the value\nwhich journals place on novel and positive results and, consequently,\nby scientists who value career success to seek to exclusively publish\nsuch results in these journals. Many others in addition to Nosek et\nal. (Hackett 2005; Martin 1992; Sovacool 2008) have taken also take issue\nwith the value which journals and funding bodies have placed on\nnovelty. \nSome might interpret the tension as a manifestation of how epistemic\nvalues (such as truth and replicability) can be compromised by\n(arguably) non-epistemic values, such the value of novel, interesting\nor surprising results. Epistemic values are typically taken to be\nvalues that, in the words of Steel “promote the acquisition of\ntrue beliefs” (2010: 18; see also Goldman 1999). Canonical\nexamples of epistemic values include the predictive accuracy and\ninternal consistency of a theory. Epistemic values are often\ncontrasted with putative non-epistemic or non-cognitive values, which\ninclude ethical or social values like, for example, the novelty of a\ntheory or its ability to improve well-being by lessening power\ninequalities (Longino 1996). Of course, there is no complete consensus\nas to precisely what counts as an epistemic or non-epistemic value\n(Rooney 1992; Longino 1996). Longino, for example, claims that, other\nthings being equal, novelty counts in favour of accepting a theory,\nand convincingly argues that, in some contexts, it can serve as a\n“protection against unconscious perpetuation of the sexism and\nandrocentrism” in traditional science (1997: 22). However, she\ndoes not discuss novelty specifically in the context of the\nreproducibility crisis. \nGiner-Sorolla (2012), however, does discuss novelty in the context of\nthe crisis, and he offers another perspective on its value. He claims\nthat one reason novelty has been used to define what is publishable or\nfundable is that it is relatively easy for researchers to establish\nand for reviewers and editors to detect. Yet, Giner-Sorolla argues,\nnovelty for its own sake perhaps should not be valued, and should in\nfact be recognized as merely an operationalisation of a deeper\nconcept, such as “ability to advance the field” (567).\nGiner-Sorolla goes on to point out how such shallow\noperationalisations of important concepts often lead to problems, for\nexample, using statistical significance to measure the importance of\nresults, or measuring the quality of research by how well outcomes fit\nwith experimenters’ prior expectations. \nValues are closely connected to discussions about norms in the open\nscience movement. Vazire (2018) and others invoke norms of\nscience—communality, universalism, disinterestedness and\norganised skepticism—in setting the goals for open science,\nnorms originally articulated by Robert Merton (1942). Each such norm\narguably reflects a value which Merton advocated, and each norm may be\nopposed by a counternorm which denotes behaviour that is in conflict\nwith the norm. For example, the norm of communality (which Merton\ncalled “communism”) reflects the value of collaboration\nand the common ownership of scientific goods since the norm recommends\nsuch collaboration and common ownership. Advocates of open science see\nsuch norms, and the values which they reflect, as an aim for open\nscience. For example, the norm of communality is reflected in sharing\nand making data open, and in open access publishing. In contrast, the\ncounternorm of secrecy is associated with a closed, for profit\npublishing system (Anderson et al. 2010). Likewise, assessing\nscientific work on its merits upholds the norm of\nuniversalism—that the evaluation of research claims should not\ndepend on the socio-demographic characteristics of the proponents of\nsuch claims. In contrast, assessing work by the age, the status, the\ninstitution or the metrics of the journal it is published in reflects\na counternorm of particularism. \nVazire (2018) and others have argued that, at the moment, scientific\npractice is dominated by counternorms and that a move to Mertonian\nnorms is a goal of the open science reform movement. In particular,\nself-interestedness, as opposed to the norm of disinterestedness,\nmotivates p-hacking and other Questionable Research Practices.\nSimilarly, a desire to protect one’s professional reputation\nmotivates resistance to having one’s work replicated by others\n(Vazire 2018). This in turn reinforces a counternorm of organized\ndogmatism rather than organized skepticism which, according to Merton,\ninvolves the “temporary suspension of judgment and the detached\nscrutiny of beliefs” (Merton, 1973). \nAnderson et al.’s (2010) focus groups and surveys of scientists\nsuggest that scientists do want to adhere to Merton’s norms but\nthat the current incentive structure of science makes this difficult.\nChanging the structure of penalty and reward systems within science to\npromote communality, universalism, disinterestedness and organized\nskepticism instead of their counternorms is an ongoing challenge for\nthe open science reform movement. As Pashler and Wagenmakers (2012)\nhave said:  \nreplicability problems will not be so easily overcome, as they reflect\ndeep-seated human biases and well-entrenched incentives that shape the\nbehavior of individuals and institutions. (2012: 529) \nThe effort to promote such values and norms has generated heated\ncontroversy. Some early responses to the Reproducibility Project:\nPsychology and Many Labs projects were highly critical, not just of\nthe substance of the nature and process of the work. Calls for\nopenness were interpreted as reflecting mistrust, and attempts to\nreplicate others’ work as personal attacks (e.g., Schnail 2014\nin\n Other Internet Resources).\n Nosek, Spies, & Motyl (2012) argue that calls for openness should\nnot be interepreted as mistrust:  \nOpening our research process will make us feel accountable to do our\nbest to get it right; and, if we do not get it right, to increase the\nopportunities for others to detect the problems and correct them.\nOpenness is not needed because we are untrustworthy; it is needed\nbecause we are human. (2012: 626) \nExchanges related to this have become known as the tone\ndebate.[] \nThe subject of reproducibility is associated with a turbulent period\nin contemporary science. This period has called for a re-evaluation of\nthe values, incentives, practices and structures which underpin\nscientific inquiry. While the meta-science has painted a bleak picture\nof reproducibility in some fields, it has also inspired a parallel\nmovement to strengthen the foundations of science. However, more\nprogress is to be made, especially in understanding the solutions to\nthe reproducibility crisis. In this regard, there are fruitful avenues\nfor future research, including a deeper exploration of the role that\nepistemic and non-epistemic values can or should play in scientific\ninquiry.","contact.mail":"fidlerfm@unimelb.edu.au","contact.domain":"unimelb.edu.au"},{"date.published":"2018-12-03","url":"https://plato.stanford.edu/entries/scientific-reproducibility/","author1":"Fiona Fidler","author1.info":"https://johnericwilcox.weebly.com","entry":"scientific-reproducibility","body.text":"\n\n\nThe terms “reproducibility crisis” and “replication\ncrisis” gained currency in conversation and in print over the\nlast decade (e.g., Pashler & Wagenmakers 2012), as disappointing\nresults emerged from large scale reproducibility projects in various\nmedical, life and behavioural sciences (e.g., Open Science\nCollaboration, OSC 2015). In 2016, a poll conducted by the journal\nNature reported that more than half (52%) of scientists\nsurveyed believed science was facing a “replication\ncrisis” (Baker 2016). More recently, some authors have moved to\nmore positive terms for describing this episode in science; for\nexample, Vazire (2018) refers instead to a “credibility\nrevolution” highlighting the improved methods and open science\npractices it has motivated.\n\n\nThe crisis often refers collectively to at least the following things:\n\n\n the virtual absence of replication studies in the published\nliterature in many scientific fields (e.g., Makel, Plucker, &\nHegarty 2012), \n\nwidespread failure to reproduce results of published studies in\nlarge systematic replication projects (e.g., OSC 2015; Begley &\nEllis 2012), \n\nevidence of publication bias (Fanelli 2010a), \n\na high prevalence of “questionable research\npractices”, which inflate the rate of false positives in the\nliterature (Simmons, Nelson, & Simonsohn 2011; John, Loewenstein,\n& Prelec 2012; Agnoli et al. 2017; Fraser et al. 2018), and \n\nthe documented lack of transparency and completeness in the\nreporting of methods, data and analysis in scientific publication\n(Bakker & Wicherts 2011; Nuijten et al. 2016). \n\nThe associated open science reform movement aims to rectify conditions\nthat led to the crisis. This is done by promoting activities such as\ndata sharing and public pre-registration of studies, and by advocating\nstricter editorial policies around statistical reporting including\npublishing replication studies and statistically non-significant\nresults.\n\n\nThis review consists of four distinct parts. First, we look at the\nterm “reproducibility” and related terms like\n“repeatability” and “replication”, presenting\nsome definitions and conceptual discussion about the epistemic\nfunction of different types of replication studies. Second, we\ndescribe the meta-science research that has established and\ncharacterised the reproducibility crisis, including large scale\nreplication projects and surveys of questionable research practices in\nvarious scientific communities. Third, we look at attempts to address\nepistemological questions about the limitations of replication, and\nwhat value it holds for scientific inquiry and the accumulation of\nknowledge. The fourth and final part describes some of the many\ninitiatives the open science reform movement has proposed (and in many\ncases implemented) to improve reproducibility in science. In addition,\nwe reflect there on the values and norms which those reforms embody,\nnoting their relevance to the debate about the role of values in the\nphilosophy of science.\n\nA starting point in any philosophical exploration of reproducibility\nand related notions is to consider the conceptual question of what\nsuch notions mean. According to some (e.g., Cartwright 1991), the\nterms “replication”, “reproduction” and\n“repetition” denote distinct concepts, while others use\nthese terms interchangeably (e.g., Atmanspacher & Maasen 2016a).\nDifferent disciplines can have different understandings of these terms\ntoo. In computational disciplines, for example,\nreproducibility often refers to the ability to reproduce\ncomputations alone, that is, it relates exclusively to sharing and\nsufficiently annotating data and code (e.g., Peng 2011, 2015). In\nthose disciplines, replication describes the redoing of whole\nexperiments (Barba 2017, Other Internet Resources). In psychology and\nother social and life\nsciences, however, reproducibility may refer to either the\nredoing of computations, or the redoing of experiments. The\nReproducibility Projects, coordinated by the Center for Open Science,\nredo entire studies, data collection and analysis. A recent funding\nprogram announcement by DARPA (US Defense Advanced Research Programs\nAgency) distinguished between reproducibility and replicability, where\nthe former refers to computational reproducibility and the latter to\nthe redoing of experiments. Here we use all three\nterms—“replication”, “reproduction” and\n“repetition”—interchangeably, unless explicitly\ndescribing the distinctions of other authors. \nWhen describing a study as “replicable”, people could have\nin mind either of at least two different things. The first is that the\nstudy is replicable in principle the sense that it can be\ncarried out again, particularly when its methods, procedures and\nanalysis are described in a sufficiently detailed and transparent way.\nThe second is that the study is replicable in that sense that it can\nbe carried out again and, when this happens, the replication study\nwill successfully produce the same or sufficiently\nsimilar results as the original. A study may be replicable in the\nformer sense but not in the second sense: one might be able to\nreplicate the methods, procedures and analysis of a study, but fail to\nsuccessfully replicate the results of the original study. Similarly,\nwhen people talk of a “replication”, they could also have\nin mind two different things: the replication of the methods,\nprocedures and analysis of a study (irrespective of the results) or,\nalternatively, the replication of such methods, procedures and\nanalysis as well as the results. \nArguably, most typologies of replication make more or less\nfine-grained distinctions between direct replication (which\nclosely follow the original study to verify results) and\nconceptual replications (which deliberately alter important\nfeatures of the study to generalize findings or to test the underlying\nhypothesis in a new way). As suggested, this distinction may not\nalways be known by these terms. For example, roughly the same\ndistinction is referred to as exact and inexact\nreplication by Keppel (1982); concrete and\nconceptual replication by Sargent (1981), and literal,\noperational and constructive replication by Lykken (1968).\nComputational reproducibility is most often direct (reproducing\nparticular analysis outcomes from the same data set using the same\ncode and software), but it can also be conceptual (analysing the same\nraw data set with alternative approaches, different models or\nstatistical frameworks). For an example of a conceptual computational\nreproducibility study, see Silberzahn and Uhlmann 2015. \nWe do not attempt to resolve these disciplinary differences or to\ncreate a new typology of replication, and instead we will provide a\nlimited snapshot of the conceptual terrain by surveying three existing\ntypologies—from Stefan Schmidt (2009), from Omar Gómez,\nNatalia Juristo, and Sira Vegas (2010) and from Hans Radder.\nSchmidt’s account has been influential and widely-cited in\npsychology and social sciences, where the replication crisis\nliterature is heavily concentrated. Gómez, Juristo, and\nVegas’s (2010) typology of replication is based on a\nmultidisciplinary survey of over 18 scholarly classifications of\nreplication studies which collectively contain more than 79 types of\nreplication. Finally, Radder’s (1996, 2003, 2006, 2009, 2012)\ntypology is perhaps best known within philosophy of science\nitself. \nSchmidt outlines five functions of replication studies in the social\nsciences: \nModifying Hendrik’s (1991) classes of variables that define a\nresearch space, Schmidt (2009) presents four classes of variables\nwhich may be altered or held constant in order for a given replication\nstudy to fulfil one of the above functions. The four classes are: \nSchmidt then systematically works through examples of how each\nfunction can be achieved by altering and/or holding a different class\nor classes of variable constant. For example, to fulfil the function\nof controlling for sampling error\n (Function 1),\n one should alter only variables regarding participant recruitment\n(Class 3), attempting to keep variables in all other classes as close\nto the original study as possible. To control for artefacts\n (Function 2),\n one should alter variables concerning the context and dependent\nvariable measures (variables in Classes 2 and 4 respectively), but\nkeep variables in 1 and 3 (information conveyed to participants and\nparticipant recruitment) as close to the original as possible.\nSchmidt, like most other authors in this area, acknowledges the\npractical limits of being able to hold all else constant. Controlling\nfor fraud\n (Function 3)\n is served by the same arrangements as controlling for artefacts\n (Function 2).\n In Schmidt’s account, controlling for sampling error, artefacts\nand fraud (Functions 1 to 3) are connected by a theme of confirming\nthe results of the original study. Functions 4 and 5 go beyond\nthis—generalizing to new populations\n (Function 4)\n which is served by changes to participant recruitment (Class 3) and\nconfirming the underlying hypothesis\n (Function 5),\n which served by changes to the information conveyed, the context and\ndependant variable measures (Classes 1, 2 and 4 respectively) but not\nchanges to participant recruitment (Class 3, although Schmidt\nacknowledges that holding the latter class of variables constant\nwhilst varying everything else is often practically impossible).\nAttempts to enable verification of the underlying research hypothesis\n(i.e., to fulfil Function 5) alone are what Schmidt classifies as\nconceptual replications, following Rosenthal (1991). Attempts\nto fulfil the other four functions are considered variants of\ndirect replications. \nIn summary, for Schmidt, direct replications control for sampling\nerror, artifacts, and fraud, and provide information about the\nreliability and validity of prior empirical work. Conceptual\nreplications help corroborate the underlying theory or substantive (as\nopposed to statistical) hypothesis in question and the extent to which\nthey generalize in new circumstances and situations. In practice,\ndirect and conceptual replications lie on a continuum, with\nreplication studies varying more or less compared to the original on\npotentially a great number of dimensions. \nGómez, Juristo, and Vega’s (2010) survey of the\nliterature in 18 disciplines identified 79 types of replication, not\nall of which they considered entirely distinct. They identify five\nmain ways in which a replication study may diverge from an initial\nstudy. With some similarities to Schimdt’s four classes\nabove: \nA change in any one or combination of these elements in a replication\nstudy corresponds to different purposes underlying the study, and\nthereby establishes a different kind of validity. Like Schmidt et al.\nthen systematically work through how changes to each of the above work\nto fulfil different epistemic functions. \nRadder (1996, 2003, 2006, 2009, 2012) distinguishes three types of\nreproducibility. One is the reproducibility of what Radder calls an\nexperiment’s material realization. Using one of\nRadder’s own examples as an illustration, two people may carry\nout the same actions to measure the mass of an object. Despite doing\nthe same actions, person A regards themselves as measuring the\nobject’s Newtonian mass while person B regards themselves\nas measuring the object’s Einsteinian mass. Here, then, the\nactions or material realization of the experimental procedure can be\nreproduced, but the theoretical descriptions of their significance\ndiffer. Radder, however, does not specify what is required for one\nmaterial realisation to be a reproduction of another, a pertinent\nquestion, especially since, as Radder himself affirms, no reproduction\nwill be exactly the same as any other reproduction (1996:\n82–83). \nA second type of reproducibility is the reproducibility of an\nexperiment, given a fixed theoretical description. For\nexample, a social scientist might conduct two experiments to examine\nsocial conformity. In one experiment, a young child might be\ninstructed to give an answer to a question before a group of other\nchildren who are, unknown to the former child, instructed to give\nwrong answers to the same question. In another experiment, an adult\nmight be instructed to give an answer to a question before a group of\nother adults who are, unknown to the former adult, instructed to give\nwrong answers to the same question. If the child and the adult give a\nwrong answer that conforms to the answers of others, then the social\nscientist might interpret the result as exemplifying social\nconformity. For Radder, the theoretical description of the experiment\nmight be fixed, specifying that if some people in a\nparticipant’s surroundings give intentionally false answers to\nthe question, then the genuine participant will conform to the\nbehaviour of their peers. However, the material realization of these\nexperiments differs insofar as one concerns children and the other\nadults. It is difficult to see how, in this example at least, this\ndiffers from what either Schmidt or Gómez, Juristo, and Vegas\nwould refer to as establishing generalizability to a different\npopulation (Schmidt’s [2009]\n Class 3\n and\n Function 5;\n Gómez, Juristo, and Vegas’s [2010]\n way 5\n and\n Function 4). \nThe third kind of reproducibility is what Radder calls\nreplicability. This is where experimental procedures differ\nto produce the same experimental result (otherwise known as a\nsuccessful replication). For example, Radder notes that multiple\nexperiments might obtain the result “a fluid of type f\nhas a boiling point b”, despite having different kinds of\nthermometers by which to measure this boiling point (2006:\n113–114). \nSchmidt (2009) points out that the difference between Radder’s\nsecond and third types of reproducibility is small in comparison to\ntheir differences to the first type. He consequently suggests his\nalternative distinction between direct and conceptual replication,\npresumably intending a conceptual replication to cover Radder’s\nsecond and third types. \nIn summary, whilst Gómez, Juristo, and Vegas’s typology\ndraws distinctions in slightly different places to Schmidt’s,\nits purpose is arguably the same—to explain what types of\nalterations in replication studies fulfil different scientific goals,\nsuch as establishing internal validity or the extent of generalization\nand so on. With the exception of his discussion of reproducing the\nmaterial realization, Radder’s other two categories can perhaps\nbe seen as fitting within the larger range of functions described by\nSchmidt and Gómez et al., who both acknowledge that in\npractice, direct and conceptual replications lie on a noisy continuum.\n \nIn psychology, the origin of the reproducibility crisis is often\nlinked to Daryl Bem’s (2011) paper which reported empirical\nevidence for the existence of “psi”, otherwise known as\nExtra Sensory Perception (ESP). This paper passed through the standard\npeer review process and was published in the high impact Journal\nof Personality and Social Psychology. The controversial nature of\nthe findings inspired three independent replication studies, each of\nwhich failed to reproduce Bem’s results. However, these\nreplication studies were rejected from four different journals,\nincluding the journal that had originally published Bem’s study,\non the grounds that the replications were not original or novel\nresearch. They were eventually published in PLoS ONE\n(Ritchie, Wiseman, & French 2012). This created controversy in the\nfield, and was interpreted by many as demonstrating how publication\nbias impeded science’s self-correction mechanism. In medicine,\nthe origin of the crisis is often attributed to Ioannidis’\n(2005) paper “Why most published findings are false”. The\npaper offered formal arguments about inflated rates of false positives\nin the literature—where a “false positive” result\nclaims a relationship exists between phenomena when it in fact does\nnot (e.g., a claim that consuming a drug is correlated with symptom\nrelief when it in fact is not). Ioannidis’ (2005) also reported\nvery low (11%) empirical reproducibility rates from a set of\npre-clinical trial replications at Amgen, later independently\npublished by Begley and Ellis (2012). In all disciplines, the\nreplication crisis is also more generally linked to earlier criticisms\nof Null Hypothesis Significance Testing (e.g., Szucs & Ioannidis\n2017), which pointed out the neglect of statistical power (e.g., Cohen\n1962, 1994) and a failure to adequately distinguish statistical and\nsubstantive hypotheses (e.g., Meehl 1967, 1978). This is discussed\nfurther below. \nIn response to the events above, a new field identifying as\nmeta-science (or meta-research) has become\nestablished over the last decade (Munafò et al. 2017).\nMunafò et al. define meta-science as “the scientific\nstudy of science itself” (2017: 1). In October 2015, Ioannidis,\nFanelli, Dunne, and Goodman identified over 800 meta-science papers\npublished in the five-month period from January to May that year, and\nestimated that the relevant literature was accruing at the rate of\napproximately 2,000 papers each year. Referring to the same bodies of\nwork with slightly different terms, Ioannidis et al. define\n“meta-research” as  \nan evolving scientific discipline that aims to evaluate and improve\nresearch practices. It includes thematic areas of methods, reporting,\nreproducibility, evaluation, and incentives (how to do, report,\nverify, correct, and reward science). (2015: 1)  \nMultiple research centres dedicated to this work now exist, including,\nfor example, the Tilburg University Meta-Research Center in\npsychology, the Meta-Research Innovation Center at Stanford (METRICS),\nand others listed in Ioannidis et al. 2015 (see\n Other Internet Resources).\n Relevant research in medical fields is also covered in Stegenga\n2018. \nProjects that self-identify as meta-science or meta-research\ninclude: \nThe most well known of these projects is undoubtedly the\nReproducibility Project: Psychology, coordinated by the now Center for\nOpen Science in Charlottesville, VA (then the Open Science\nCollaboration). It involved 270 crowd sourced researchers in 64\ndifferent institutions in 11 different countries. Researchers\nattempted direct replications of 100 studies published in three\nleading psychology journals in the year 2008. Each study was\nreplicated only once. Replications attempted to follow original\nprotocols as closely as possible, though some differences were\nunavoidable (e.g., some replication studies were done with European\nsamples when the original studies used US samples). In almost all\ncases, replication studies used larger sample sizes that the original\nstudies and therefore had greater statistical power—that is, a\ngreater probability of correctly rejecting the null hypothesis (i.e.,\nthat no relationship exists) when the hypothesis is false. A number of\nmeasures of reproducibility were reported: \nThere have been objections to the implementation and interpretation of\nthis project, most notably by Gilbert et al. (2016), who took issue\nwith the extent to which the replications studies were indeed direct\nreplications. For example, Gilbert et al. highlighted 6 specific\nexamples of “low fidelity protocols”, that is, where\nreplication studies differed in their view substantially from the\noriginal (in one case, using a European sample rather than a US sample\nof participants). However, Anderson et al. (2016) explained in a reply\nthat in half of those cases, the authors of the original study had\nendorsed the replication as being direct or close to on relevant\ndimensions and that furthermore, that independently rated similarity\nbetween original and replication studies failed to predict replication\nsuccess. Others (e.g., Etz & Vandekerckhove 2016) have applied\nBayesian reanalysis to the OSC’s (2015) data and conclude that\nup to 75% (as opposed to the OSC’s 36–47%) of replications\ncould be considered successful. However, they do note that in many\ncases this is only with very weak evidence (i.e., Bayes factors of\n<10). They too conclude that the failure to reproduce many effects\nis indeed explained by the overestimation of effect sizes, itself a\nproduct of publication bias. A Reproducibility Project: Cancer Biology\n(also coordinated by the Center for Open Science) is currently\nunderway (Errington et al. 2014), originally attempting to replicate\n50 of the highest impact studies in Cancer Biology published between\n2010–2012. This project has recently announced it will complete\nwith only 18 replication studies, as too few originals reported enough\ninformation to proceed with full replications (Kaiser 2018). Results\nof the first 10 studies are reportedly mixed, with only 5 being\nconsidered “mostly repeatable” (Kaiser 2018). \nThe Many Labs project (Klein et al. 2014) coordinated 36 independent\nreplications of 13 classic psychology phenomena (from 12 studies, that\nis, one study tested two effects), including anchoring, sunk cost bias\nand priming, amongst other well-known effects in psychology. In terms\nof matching statistical significance, the project demonstrated that 11\nout of 13 effects could be successful replicated. It also showed great\nvariation in many of the effect sizes across the 36 replications. \nIn biomedical research, there have also been a number of large scale\nreproducibility projects. An early one by Begley and Ellis (2012, but\ndiscussed earlier in Ioannidis 2005) attempted to replicate 56\nlandmark pre-clinical trials and reported an alarming reproducibility\nrate of only 11%, that is, only 6 of the 56 results could be\nsuccessfully reproduced. Subsequent attempts at large scale\nreplications in this field have produced more optimistic estimates,\nbut routinely failed to successfully reproduce more than half of the\npublished results. Freedman et al. (2015) report five replication\nprojects by independent groups of researchers which produce\nreproducibility estimates ranging from 22% to 49%. They estimate the\ncost of irreproducible research in US biomedical science alone to be\nin the order of USD$28 billion per year. A reproducibility project in\nExperimental Philosophy is an exception to the general trend,\nreporting reproducibility rates of 70% (Cova et al. forthcoming). \nFinally, the Social Science Replication Project (SSRP) redid 21\nexperimental social science studies published in the journals Nature\nand Science between 2010 and 2015. Depending on the measure taken, the\nreplication success rate was 57–67% (Camerer et al. 2018). \nThe causes of irreproducible results are largely the same across\ndisciplines we have mentioned. This is not surprising given that they\nstem from problems with statistical methods, publishing practices and\nthe incentive structures created in a “publish or perish”\nresearch culture, all of which are largely shared, at least in the\nlife and behavioral sciences. \nWhilst replication is often casually referred to as a cornerstone of\nthe scientific method, direct replication studies (as they might be\nunderstood from Schmidt or Gómez, Juristo, and Vegas’s\ntypologies above) are a rare event in the published literature of some\nscientific disciplines, most notably the life and social sciences. For\nexample, such replication attempts constitute roughly 1% of the\npublished psychology literature (Makel, Plucker, & Hegarty 2012).\nThe proportion in published ecology and evolution literature is even\nsmaller (Kelly 2017, Other Internet Resources). \nThis virtual absence of replication studies in the literature can\nexplained by the fact that many scientific journals have historically\nhad explicit policies against publishing replication studies (Mahoney\n1985)—thus giving rise to a “publication bias”. Over\n70% of editors from 79 social science journals said they preferred new\nstudies over replications and over 90% said they would did not\nencourage the submission of replication studies (Neuliep &\nCrandall 1990). In addition, many science funding bodies also fund\nonly “novel”, “original” and/or\n“groundbreaking” research (Schmidt 2009). \nA second type of publication bias has also played a substantial role\nin the reproducibility crisis, namely a bias towards\n“statistically significant” or “positive”\nresults. Unlike the bias against replication studies, this is rarely\nan explicitly stated policy of a journal. Publication bias towards\nstatistically significant findings has a long history, and was first\ndocumented in psychology by Sterling (1959). Developments in text\nmining techniques have led to more comprehensive estimates. For\nexample, Fanelli’s work has demonstrated the extent of\npublication bias in various disciplines, and the proportions of\nstatistically significant results given below are from his 2010a\npaper. He has also documented the increase of this bias over time\n(2012) and explored the causes of the bias, including the relationship\nbetween publication bias and a publish or perish research culture\n(2010b). \nIn many disciplines (e.g., psychology, psychiatry, materials science,\npharmacology and toxicology, clinical medicine, biology and\nbiochemistry, economics and business, microbiology and genetics) the\nproportion of statistically significant results is very high, close to\nor exceeding 90% (Fanelli 2010a). This is despite the fact that in\nmany of these fields, the average statistical power is low—that\nis, the average probability that a study will correctly reject the\nnull hypothesis is low. For example, in psychology the proportion of\npublished results that are statistically significant is 92% despite\nthe fact that the average power of studies in this field to detect\nmedium effect sizes (arguably typical of the discipline) is roughly\n44% (Szucs & Ioannidis 2017). If there was no bias towards\npublishing statistically significant results, the proportion of\nsignificant results should roughly match the average statistical power\nof the discipline. The excess in statistical significance (in this\ncase, the difference between 92% and 44%) is therefore an indicator\nthe strength of the bias. For a second example, in ecology,\nenvironment and plant and animal sciences the proportion of\nstatistically significant results is 74% and 78% respectively,\nadmittedly lower than in psychology. However, the most recent estimate\nof the statistical power, again of medium effect sizes, of ecology and\nanimal behaviour is 23–26% (Smith, Hardy, & Gammell 2011)\n(An earlier more optimistic assessment was 40–47%, Jennions\n& Møller, 2003.) For a third example, the proportion of\nstatistically significant results in neuroscience and behaviour is\n85%. Our best estimate of the statistical power in neuroscience is at\nbest 31%, with a lower bound estimate of 8% (Button et al. 2013). The\nassociated file-drawer problem (Rosenthal 1979)—where\nresearchers relegate failed statistically non-significant studies to\ntheir file drawers, hidden from public view—has long been\nestablished in psychology and others disciplines, and is known to lead\nto distortions in meta-analysis (where a “meta-analysis”\nis a study which analyses results across multiple other studies). \nIn addition to creating the file-drawer problem described above,\npublication bias has been held at least partially responsible for the\nhigh prevalence of Questionable Research Practices (QRPs) uncovered in\nboth self-report survey research (John, Loewenstein, & Prelec\n2012; Agnoli 2017 et al. 2017; Fraser\net al. 2018) and in journal studies that have detected, for example,\nunusual distributions of p values (Masicampo & Lalande\n2012; Hartgerink et al. 2016). Pressure to publish, now ubiquitous\nacross academic institutions, means that researchers often cannot\nafford to simply assign “failed” or statistically\nnon-significant studies to the file drawer, so instead they p\nhack and cherry-pick results (as discussed below) back to\nsignificance, and back into the published literature. Simmons, Nelson,\nand Simonsohn (2011) explained and demonstrated with simulated results\nhow engaging in such practices inflates the false positive error rate\nof the published literature, leading to a lower rate of reproducible\nresults. \n“P hacking” refers to a set of practices which\ninclude: checking the statistical significance of results before\ndeciding whether to collect more data; stopping data collection early\nbecause results have reached statistical significance; deciding\nwhether to exclude data points (e.g., outliers) only after checking\nthe impact on statistical significance and not reporting the impact of\nthe data exclusion; adjusting statistical models, for instance by\nincluding or excluding covariates based on the resulting strength of\nthe main effect of interest; and rounding of a p value to meet\na statistical significance threshold (e.g., presenting 0.053 as\nP < .05). “Cherry picking” includes failing to\nreport dependent or response variables or relationships that did not\nreach statistical significance or other threshold and/or failing to\nreport conditions or treatments that did not reach statistical\nsignificance or other threshold. “HARKing” (Hypothesising\nAfter Results are Known) includes presenting ad hoc and/or unexpected\nfindings as though they had been predicted all along (Kerr 1998); and\npresenting exploratory work as though it was confirmatory hypothesis\ntesting (Wagenmakers et al. 2012). Five of the most widespread QRPs\nare listed below in Table 1 (from Fraser et al. 2018), with associated\nsurvey measures of prevalence. \nTable 1: The prevalence of some common\nQuestionable Research Practices. Percentage (with 95% confidence\nintervals) of researches who reported having used the QRP at least\nonce (adapted from Fraser et al. 2018) \n#cherry picking,\n*p hacking,\n^HARKing \nNull Hypothesis Significance Testing (NHST)—discussed\nabove—is a commonly diagnosed cause of the current replication\ncrisis (see Szucs & Ioannidis 2017). The ubiquitous nature of NHST\nin life and behavioural sciences is well documented, most recently by\nCristea and Ioannidis (2018). This is important pre-condition for\nestablishing its role as a cause, since it could not be a cause if its\nactual use was rare. The dichotomous nature of NHST facilitates\npublication bias (Meehl 1967, 1978). For example, the language of\naccept and reject in hypothesis testing maps conveniently on to\nacceptance and rejection of manuscripts, a fact that led Rosnow and\nRosenthal (1989) to decry that “surely God loves the .06 nearly\nas much as the .05” (1989: 1277). Techniques that failed to\nenshrine a dichotomous threshold would be harder to employ in service\nof publication bias. For example, a case has been made that estimation\nusing effect sizes and confidence intervals (introduced above) would\nbe less prone to being used in service of publication bias (Cumming\n2012, Cumming and Calin-Jageman 2017. \nAs already mentioned, the average statistical power in various\ndisciplines is low. Not only is power often low, but it is virtually\nnever reported; less than 10% of published studies in psychology\nreport statistical power and even fewer in ecology do (Fidler et al.\n2006). Explanations for the widespread neglect of statistical power\noften highlight the many common misconceptions and fallacies\nassociated with p values (e.g., Haller & Krauss 2002;\nGigerenzer 2018). For example, the inverse probability\n fallacy[1]\n has been used to explain why so many researchers fail to calculate\nand report statistical power (Oakes 1986). \nIn 2017, a group of 72 authors proposed in a Nature Human\nBehaviour paper that alpha level in statistical significance\ntesting be lowered to 0.005 (as opposed to the current standard of\n0.05) to improve the reproducibility rate of published research\n(Benjamin et al. 2018). A reply from a different set of 88 authors was\npublished in the same journal, arguing against this proposal and\nstating instead that researchers should justify their alpha level\nbased on context (Lakens et al. 2018). Several other replies have\nfollowed, including a call from Andrew Gelman and colleagues to\nabandon statistical significance altogether (McShane et al. 2018, \nOther Internet Resources). The\nexchange has become known on social media as the Alpha Wars\n(e.g., in the Barely Significant blog,\n Other Internet Resources)).\n Independently, the American Statistical Association released a\nstatement on the use of p values for the first time in its\nhistory, cautioning against their overinterpretation and pointing out\nthe limits of the information they offer about replication (Wasserman\n& Lazar 2016) and devoted their association’s 2017 annual\nconvention to the theme “Scientific Method for the\n21st Century: A World Beyond \\(p <0.05\\)” (see\n Other Internet Resources).\n  \nA number of recent high-profile cases of scientific fraud have\ncontributed considerably to the amount of press around the\nreproducibility crisis in science. Often these cases (e.g., Diederik\nStapel in psychology) are used as a hook for media coverage, even\nthough the crisis itself has very little to do with scientific fraud.\n(Note also that the Questionable Research Practices above are not\ntypically counted as “fraud” or even “scientific\nmisconduct” despite their ethically dubious status.) For\nexample, Fang, Grant Steen, and Casadevall (2012) estimated that 43%\nof retracted articles in biomedical research are withdrawn because of\nfraud. However, roughly half a million biomedical articles are\npublished annually and only 400 of those are retracted (Oransky 2016,\nfounder of the website RetractionWatch), so this amounts to a very\nsmall proportion of the literature (approximately 0.1%). There are, of\ncourse, many cases of pharmaceutical companies exercising financial\npressure on scientists and the publishing industry that raise\nspeculation about how many undetected (or unretracted) cases there may\nstill be in the literature. Having said that, there is widespread\nconsensus amongst scientists in the field that the main cause of the\ncurrent reproducibility crisis is the current incentive structure in\nscience (publication bias, publish or perish, non-transparent\nstatistical reporting, lack of rewards for data sharing). Whilst this\nincentive structure can push some to scientific fraud, it appears to\nbe a very small proportion.  \nMany scientists believe that replication is epistemically valuable in\nsome way, that is to say, that replication serves a useful function in\nenhancing our knowledge, understanding or beliefs about reality. This\nsection first discusses a problem about the epistemic value of\nreplication studies—called the “experimenters\nregress”—and it then considers the claim that replication\nplays an epistemically valuable role in distinguishing scientific\ninquiry. It lastly examines a recent attempt to formalise the logic of\nreplication in a Bayesian framework. \nCollins (1985) articulated a widely discussed problem that is now\nknown as the experimenters’ regress. He initially lays\nout the problem in the context of measurement (Collins 1985: 84).\nSuppose a scientist is trying to determine the accuracy of a\nmeasurement device and also the accuracy of a measurement result.\nPerhaps, for example, a scientist is using a thermometer to measure\nthe temperature of a liquid, and it delivers a particular measurement\nresult, say, 12 degrees Celsius. \nThe problem arises because of the interdependence of the accuracy of\nthe measurement result and the accuracy of the measurement device: to\nknow whether a particular measurement result is accurate, we need to\ntest it against a measurement result that is previously known to be\naccurate, but to know that the result is accurate, we need to know\nthat it has been obtained via an accurate measuring device, and so on.\nThis, according to Collins, creates a “circle” which he\nrefers to as the “experimenters’ regress”. \nCollins extends the problem to scientific replication more generally.\nSuppose that an experiment B is a replication study of an\ninitial experiment A, and that B’s result\napparently conflicts with A’s result. This seeming\nconflict may have one of two interpretations: \nThe regress poses a problem about how to choose between these\ninterpretations, a problem which threatens the epistemic value of\nreplication studies if there are no rational grounds for choosing in a\nparticular way. Determining whether one experiment is a proper\nreplication of another is complicated by the facts that scientific\nwriting conventions often omit precise details of experimental\nmethodology (Collins 2016), and, furthermore, much of the knowledge\nthat scientists require to execute experiments is tacit and\n“cannot be fully explicated or absolutely established”\n(Collins 1985: 73). \nIn the context of experimental methodology, Collins wrote: \nTo know an experiment has been well conducted, one needs to know\nwhether it gives rise to the correct outcome. But to know what the\ncorrect outcome is, one needs to do a well-conducted experiment. But\nto know whether the experiment has been well conducted…! (2016:\n66; ellipses original) \nCollins holds that in such cases where a conflict of results arises,\nscientists tend to fraction into two groups, each holding opposing\ninterpretations of the results. According to Collins, where such\ngroups are “determined” and the “controversy runs\ndeep” (Collins 2016: 67), the dispute between the groups cannot\nbe resolved via further experimentation, for each additional result is\nsubject to the problem posed by the experimenters’\n regress.[2]\n In such cases, Collins claims that particular non-epistemic factors\nwill partly determine which interpretation becomes the lasting view:\n \nthe career, social, and cognitive interests of the scientists, their\nreputations and that of their institutions, and the perceived utility\nfor future work. (Franklin & Collins 2016: 99) \nFranklin was the most vociferous opponent of Collins, although recent\ncollaboration between the two has fostered some agreement (Collins\n2016). Franklin presented a set of strategies for validating\nexperimental results, all of which relate to “rational\nargument” on epistemic grounds (Franklin 1989: 459; 1994).\nExamples include, for instance, appealing to experimental checks on\nmeasurement devices or eliminating potential sources of error in the\nexperiment (Franklin & Collins 2016). He claimed that the fact\nthat such strategies were evidenced in scientific practice\n“argues against those who believe that rational arguments plays\nlittle, if any, role” in such validation (Franklin 1989: 459),\nwith Collins being an example. He interprets Collins as suggesting\nthat the strategies for resolving debates of the validation of results\nare social factors or “culturally accepted practices”\n(Franklin, 1989: 459) which do not provide reasons to underpin\nrational belief about results. Franklin (1994) further claims that\nCollins conflates the difficulty in successfully executing\nexperiments with the difficulty of demonstrating that\nexperiments have been executed, with Feest (2016) interpreting him to\nsay that although such execution requires tacit knowledge, one can\nnevertheless appeal to strategies to demonstrate the validity of\nexperimental findings. \nFeest (2016) examines a case study involving debates about the Mozart\neffect in psychology (which, roughly speaking, is the effect whereby\nlistening to Mozart beneficially affects some aspect of intelligence\nor brain structure). Like Collins, she agrees that there is a problem\nin determining whether conflicting results suggest a putative\nreplication experiment is not a proper replication attempt, in part\nbecause there is uncertainty about whether scientific concepts such as\nthe Mozart effect have been appropriately operationalised in earlier\nor later experimental contexts. Unlike Collins (on her\ninterpretation), however, she does not think that this uncertainty\narises because scientists have inescapably tacit knowledge of the\nlinguistic rules about the meaning and application of concepts like\nthe Mozart effect. Rather the uncertainty arises because such concepts\nare still themselves developing and because of assumptions about the\nworld that are required to successfully draw inferences from\nit. Experimental methodology then serves to reveal the previously\ntacit assumptions about the application of concepts and the legitimacy\nof inferences, assumptions which are then susceptible to scrutiny. \nFor example, in her study of the Mozart effect, she notes that\nreplication studies of the Mozart effect failed to find that Mozart\nmusic had a beneficial influence on spatial abilities. Rauscher, who\nwas the first to report results supporting the Mozart effect,\nsuggested that the later studies were not proper replications of her\nstudy (Rauscher, Shaw, and Ky 1993, 1995). She clarified that the\nMozart effect applied only to a particular category\nof spatial abilities (spatio-temporal processes) and that the later\nstudies operationalised the Mozart effect in terms of different\nspatial abilities (spatial recognition). Here, then, there was a\ndifficulty in determining whether to interpret failed replication\nresults as evidence against the initial results or rather as an\nindication that the replication studies were not proper replications.\nFeest claims this difficulty arose because of tacit knowledge or\nassumptions: assumptions about the application of the Mozart effect\nconcept to different kinds of spatial abilities, about whether the\nworld is such that Mozart music has an effect on such abilities and\nabout whether the failure of Mozart to impact other kinds of spatial\nabilities warrants the inference that the Mozart effect does not\nexist. Contra Collins, however, experimental methodology enabled the\nexplication and testing of these assumptions, thus allowing scientists\nto overcome the interpretive impasse. \nAgainst this background, her overall argument is that scientists often\nare and should be sceptical towards each other’s results.\nHowever, this is not because of inescapably tacit knowledge and the\ninevitable failure of epistemic strategies for validating results.\nRather, it is at least in part because of varying tacit assumptions\nthat researchers have about the meaning of concepts, about the world\nand about what to draw inferences from it. Progressive experimentation\nserves to reveal these tacit assumptions which can then be\nscrutinised, leading to the accumulation of knowledge. \nThere is also other philosophical literature on the\nexperimenters’ regress, including Teira’s (2013) paper\narguing that particular experimental debiasing procedures are\ndefensible against the regress from a contractualist perspective,\naccording to which self-interested scientists have reason to adopt\ngood methodological standards. \nThere is a widespread belief that science is distinct from other\nknowledge accumulation endeavours, and some have suggested that\nreplication distinguishes (or is at least essential to) science in\nthis respect. (See also the entry on \n  science and pseudo-science.).\nAccording to the Open Science Collaboration, “Reproducible\nresearch practices are at the heart of sound research and integral to\nthe scientific method.” (OSC 2015: 7). Schmidt echoes this theme:\n“To confirm results or hypotheses by a repetition procedure is\nat the basis of any scientific conception” (2009: 90). Braude\n(1979) goes so far as to say that reproducibility is a\n“demarcation criterion between science and nonscience”\n(1979: 2). Similarly, Nosek, Spies, and Motyl state that: \n[T]he scientific method differentiates itself from other approaches by\npublicly disclosing the basis of evidence for a claim…. In\nprinciple, open sharing of methodology means that the entire body of\nscientific knowledge can be reproduced by anyone. (2012: 618) \nIf replication played such an essential or distinguishing role in\nscience, we might expect it to be a prominent theme in the history of\nscience. Steinle (2016) considers the extent to which it is such a\ntheme. He presents a variety of cases from the history of science\nwhere replication played very different roles, although he understands\n“replication” narrowly to refer to when an experiment is\nre-run by different researchers. He claims that the role and\nvalue of replication in experimental replication is “much more\ncomplex than easy textbook accounts make us believe” (2016: 60),\nparticularly since each scientific inquiry is always tied to a variety\nof contextual considerations that can affect the importance of\nreplication. Such considerations include the relationship between\nexperimental results and the background of accepted theory at the\ntime, the practical and resource constraints on pursuing replication\nand the perceived credibility of the researchers. These contextual\nfactors, he claims, mean that replication was a key or even overriding\ndeterminant of acceptance of research claims in some cases, but not in\nothers. \nFor example, sometimes replication was sufficient to embrace a\nresearch claim, even if it conflicted with the background of accepted\ntheory and left theoretical questions unresolved. A case of this is\nhigh-temperature superconductivity, the effect whereby an electric\ncurrent can pass with zero resistance through a conductor at\nrelatively high temperatures. In 1986, physicists Georg Bednorz and\nAlex Müller reported finding a material which acted as a\nsuperconductor at 35 kelvin (−238 degrees Celsius). Scientists\naround the world successfully replicated the effect, and Bednorz and\nMuller were then awarded with a Nobel Prize in Physics a year after\ntheir announcement. This case is remarkable since not only did their\neffect contradict the accepted physical theory at the time, but there\nis still no extant theory that adequately explains the effects which\nthey reported (Di Bucchianico, 2014). \nAs a contrasting example, however, sometimes claims were accepted\nwithout any replication. In the 1650s, German scientist Otto von\nGuericke designed and operated the world’s first vacuum pump\nthat would visibly suck air out of a larger space. He performed\nexperiments with his device to various audiences. Yet the replication\nof his experiments by others would have been very difficult, if not\nimpossible: not only was Guericke’s pump both expensive and\ncomplicated to build, but it was also unlikely that his descriptions\nof it sufficed to enable anyone to build the pump and to consequently\nreplicate his findings. Despite this, Steinle claims that “no\ndoubts were raised about his results”, probably as a results of\nhis “public performances that could be witnessed by a large\nnumber of participants” (2016: 55). \nSteinle takes such historical cases to provide normative guidance for\nunderstanding the epistemic value as replication as context-sensitive:\nwhether replication is necessary or sufficient for establishing a\nresearch claim will depend on a variety of considerations, such as\nthose mentioned earlier. He consequently eschews wide-reaching claims,\nsuch as those that “it’s all about replicability” or\nthat “replicability does not decide anything” (2016:\n60). \nEarp and Trafimow (2015) attempt to formalise the way in which\nreplication is epistemically valuable, and they do this using a\nBayesian framework to explicate the inferences drawn from replication\nstudies. They present the framework in a context similar to that of\nCollins (1985), noting that “it is well-nigh impossible to say\nconclusively what [replication results] mean” (Earp &\nTrafimow, 2015: 3). But while replication studies are often not\nconclusive, they do believe that such studies can be\ninformative, and their Bayesian framework depicts how this is\nso. \nThe framework is set out with an example. Suppose an aficionado of\nResearcher A is highly confident that anything said by\nResearcher A is true. Some other researcher, Researcher\nB, then attempts to replicate an experiment by Researcher\nA, and Researcher B find results that conflict with\nthose of Researcher A. Earp and Trafimow claim that the\naficionado might continue to be confident in Researcher\nA’s findings, but the aficionado’s confidence is\nlikely to slightly decrease. As the number of failed replication\nattempts increases, the aficionado’s confidence accordingly\ndecreases, eventually falling below 50% and thereby placing more\nconfidence in the replication failures than in the findings initially\nreported by Researcher A. \nHere, then, suppose we are interested in the probability that the\noriginal result reported by Researcher A is true given\nResearcher B’s first replication failure. Earp and\nTrafimow represent this probability with the notation \\(p(T\\mid F)\\)\nwhere p is a probability function, T represents the\nproposition that the original result is true and F represents\nResearcher B’s replication failure. According to\nBayes’s theorem below, this probability is calculable from the\naficionado’s degree of confidence that the original result is\ntrue prior to learning of the replication failure \\(p(T)\\), their\ndegree of expectation of the replication failure on the condition that\nthe original result is true \\(p(T\\mid F)\\), and the degree to which they would\nunconditionally expect a replication failure prior to learning of the\nreplication failure \\(p(F)\\): \nRelatedly, we could instead be interested in the confidence ratio that\nthe original result is true or false given the failure to replicate.\nThis ratio is representable as \\(\\frac{p(T\\mid F)}{p(\\nneg T\\mid F)}\\)\nwhere \\(\\nneg T\\) represents the proposition that the original result\nis false. According to the standard Bayesian probability calculus,\nthis ratio in turn is related to a product of ratios concerning  \nThis relation is expressed in the equation: \nNow Earp and Trafimow assign some values to the terms on the\nright-hand of the equation for (2). Supposing that the aficionado is\nconfident in the original results, they set the ratio\n\\(\\frac{p(T)}{p(\\nneg T)}\\) to 50, meaning that the aficionado is\ninitially fifty times more confident that the results are true than\nthat the results are false. \nThey also set the ratio \\(\\frac{p(F\\mid T)}{p(F\\mid \\nneg T)}\\). about\nthe conditional expectation of a replication failure to 0.5, meaning\nthat the aficionado is considerably less confident that there will be\na replication failure if the original result is true than if it is\nfalse. They point out that the extent to which the aficionado is less\nconfident depends on the quality of so-called auxiliary\nassumptions about the replication experiment. Here, auxiliary\nassumptions are assumptions which enable one to infer that particular\nthings should be observable if the theory under test is true. The\nintuitive idea is that the higher the quality of the assumptions about\na replication study, the more one would expect to observe a successful\nreplication if the original result was true. While they do not specify\nprecisely what makes such auxiliary assumptions have high\n“quality” in this context, presumably this quality\nconcerns the extent to which the assumptions are probably true and the\nextent to which the replication experiment is an appropriate test of\nthe veracity of the original results if the assumptions are true. \nOnce the ratios on the right-hand of equation (2) are set as such, one\ncan see that a replication failure would reduce one’s confidence\nin the original results: \nHere, then, a replication failure would reduce the aficionado’s\nconfidence that the original result was true so that the aficionado\nwould be only 25 times more confident that the result is true given a\nfailure (as per \\(\\frac{p(T\\mid F)}{p(\\nneg T\\mid F)}\\)) rather than\n50 times more confident that it is true (as per \\(\\frac{p(T)}{p(\\nneg\nT)}\\)). \nNevertheless, the aficionado may still be confident that the original\nresult is true, but we can see how such confidence would decrease with\nsuccessive replication failures. More formally, let \\(F_N\\) be the\nlast replication failure in a sequence of N replication\nfailures \\(\\langle F_1,F_2,\\ldots,F_N\\rangle\\). Then, the\naficionado’s confidence in the original result given the\nNth replication failure is expressible in the\n equation:[3] \nFor example, suppose there are 10 replication failures, and so\n\\(N=10\\). Suppose further that the confidence ratios for the\nreplication failures are set such that: \nThen, \nHere, then, the aficionado’s confidence in the original result\ndecreases so that they are more confident that it was false than that\nit was true. Hence, on Earp and Trafimow’s Bayesian account,\nsuccessive replication failures can progressively erode one’s\nconfidence that an original result is true, even if one was initially\nhighly confident in the original result and even if no single\nreplication failure by itself was\n conclusive.[4] \nSome putative merits of Earp and Trafimow’s account, then, are\nthat it provides a formalisation whereby replication attempts are\ninformative even if they are not conclusive, and furthermore, the\nformalisation provides a role for both quantity of replication\nattempts as well as auxiliary assumptions about the replications. \nThe aforementioned meta-science has unearthed a range of problems\nwhich give rise to the reproducibility crisis, and the open science\nmovement has proposed or promoted various solutions—or\nreforms—for these problems. These reforms can be grouped into\nfour categories: (a) methods and training, (b) reporting and\ndissemination, (c) peer review processes, and (d) evaluating new\nincentive structures (loosely following the categories used by\nMunafò et al. 2017 and Ioannidis et al. 2015). In subsections\n4.1–4.4 below, we present a non-exhaustive list of initiatives\nin each of the above categories. These initiatives are reflections of\nvarious values and norms that are at the heart of the open science\nmovement, and we discuss these values and norms in 4.5. \nThere has long been philosophical debate about what role values do and\nshould play in science (Churchman 1948; Rudner 1953; Douglas 2016),\nand the reproducibility crisis is intimately connected to questions\nabout the operations of, and interconnections between, such values. In\nparticular, Nosek et al. (2017) argue that there is a tension between truth and\npublishability. More specifically, for reasons discussed in section 2\nabove, the accuracy of scientific results are compromised by the value\nwhich journals place on novel and positive results and, consequently,\nby scientists who value career success to seek to exclusively publish\nsuch results in these journals. Many others in addition to Nosek et\nal. (Hackett 2005; Martin 1992; Sovacool 2008) have taken also take issue\nwith the value which journals and funding bodies have placed on\nnovelty. \nSome might interpret the tension as a manifestation of how epistemic\nvalues (such as truth and replicability) can be compromised by\n(arguably) non-epistemic values, such the value of novel, interesting\nor surprising results. Epistemic values are typically taken to be\nvalues that, in the words of Steel “promote the acquisition of\ntrue beliefs” (2010: 18; see also Goldman 1999). Canonical\nexamples of epistemic values include the predictive accuracy and\ninternal consistency of a theory. Epistemic values are often\ncontrasted with putative non-epistemic or non-cognitive values, which\ninclude ethical or social values like, for example, the novelty of a\ntheory or its ability to improve well-being by lessening power\ninequalities (Longino 1996). Of course, there is no complete consensus\nas to precisely what counts as an epistemic or non-epistemic value\n(Rooney 1992; Longino 1996). Longino, for example, claims that, other\nthings being equal, novelty counts in favour of accepting a theory,\nand convincingly argues that, in some contexts, it can serve as a\n“protection against unconscious perpetuation of the sexism and\nandrocentrism” in traditional science (1997: 22). However, she\ndoes not discuss novelty specifically in the context of the\nreproducibility crisis. \nGiner-Sorolla (2012), however, does discuss novelty in the context of\nthe crisis, and he offers another perspective on its value. He claims\nthat one reason novelty has been used to define what is publishable or\nfundable is that it is relatively easy for researchers to establish\nand for reviewers and editors to detect. Yet, Giner-Sorolla argues,\nnovelty for its own sake perhaps should not be valued, and should in\nfact be recognized as merely an operationalisation of a deeper\nconcept, such as “ability to advance the field” (567).\nGiner-Sorolla goes on to point out how such shallow\noperationalisations of important concepts often lead to problems, for\nexample, using statistical significance to measure the importance of\nresults, or measuring the quality of research by how well outcomes fit\nwith experimenters’ prior expectations. \nValues are closely connected to discussions about norms in the open\nscience movement. Vazire (2018) and others invoke norms of\nscience—communality, universalism, disinterestedness and\norganised skepticism—in setting the goals for open science,\nnorms originally articulated by Robert Merton (1942). Each such norm\narguably reflects a value which Merton advocated, and each norm may be\nopposed by a counternorm which denotes behaviour that is in conflict\nwith the norm. For example, the norm of communality (which Merton\ncalled “communism”) reflects the value of collaboration\nand the common ownership of scientific goods since the norm recommends\nsuch collaboration and common ownership. Advocates of open science see\nsuch norms, and the values which they reflect, as an aim for open\nscience. For example, the norm of communality is reflected in sharing\nand making data open, and in open access publishing. In contrast, the\ncounternorm of secrecy is associated with a closed, for profit\npublishing system (Anderson et al. 2010). Likewise, assessing\nscientific work on its merits upholds the norm of\nuniversalism—that the evaluation of research claims should not\ndepend on the socio-demographic characteristics of the proponents of\nsuch claims. In contrast, assessing work by the age, the status, the\ninstitution or the metrics of the journal it is published in reflects\na counternorm of particularism. \nVazire (2018) and others have argued that, at the moment, scientific\npractice is dominated by counternorms and that a move to Mertonian\nnorms is a goal of the open science reform movement. In particular,\nself-interestedness, as opposed to the norm of disinterestedness,\nmotivates p-hacking and other Questionable Research Practices.\nSimilarly, a desire to protect one’s professional reputation\nmotivates resistance to having one’s work replicated by others\n(Vazire 2018). This in turn reinforces a counternorm of organized\ndogmatism rather than organized skepticism which, according to Merton,\ninvolves the “temporary suspension of judgment and the detached\nscrutiny of beliefs” (Merton, 1973). \nAnderson et al.’s (2010) focus groups and surveys of scientists\nsuggest that scientists do want to adhere to Merton’s norms but\nthat the current incentive structure of science makes this difficult.\nChanging the structure of penalty and reward systems within science to\npromote communality, universalism, disinterestedness and organized\nskepticism instead of their counternorms is an ongoing challenge for\nthe open science reform movement. As Pashler and Wagenmakers (2012)\nhave said:  \nreplicability problems will not be so easily overcome, as they reflect\ndeep-seated human biases and well-entrenched incentives that shape the\nbehavior of individuals and institutions. (2012: 529) \nThe effort to promote such values and norms has generated heated\ncontroversy. Some early responses to the Reproducibility Project:\nPsychology and Many Labs projects were highly critical, not just of\nthe substance of the nature and process of the work. Calls for\nopenness were interpreted as reflecting mistrust, and attempts to\nreplicate others’ work as personal attacks (e.g., Schnail 2014\nin\n Other Internet Resources).\n Nosek, Spies, & Motyl (2012) argue that calls for openness should\nnot be interepreted as mistrust:  \nOpening our research process will make us feel accountable to do our\nbest to get it right; and, if we do not get it right, to increase the\nopportunities for others to detect the problems and correct them.\nOpenness is not needed because we are untrustworthy; it is needed\nbecause we are human. (2012: 626) \nExchanges related to this have become known as the tone\ndebate.[] \nThe subject of reproducibility is associated with a turbulent period\nin contemporary science. This period has called for a re-evaluation of\nthe values, incentives, practices and structures which underpin\nscientific inquiry. While the meta-science has painted a bleak picture\nof reproducibility in some fields, it has also inspired a parallel\nmovement to strengthen the foundations of science. However, more\nprogress is to be made, especially in understanding the solutions to\nthe reproducibility crisis. In this regard, there are fruitful avenues\nfor future research, including a deeper exploration of the role that\nepistemic and non-epistemic values can or should play in scientific\ninquiry.","contact.mail":"wilcoxje@stanford.edu","contact.domain":"stanford.edu"}]
