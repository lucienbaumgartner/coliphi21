[{"date.published":"2010-09-07","url":"https://plato.stanford.edu/entries/typelogical-grammar/","author1":"Michael Moortgat","entry":"typelogical-grammar","body.text":"\n\n\n\nTypelogical grammars are substructural logics, designed for reasoning\nabout the composition of form and meaning in natural language. At\nthe core of these grammars are residuated families of type-forming\noperations; a hierarchy of typelogical grammars results from the\nchoices one makes with respect to the structural properties of the\ntype-forming operations, and the means one introduces to control the\ngrammatical resource management.  Computational semantics is obtained\nfrom the Curry-Howard interpretation of categorial derivations.\nParsing and natural language processing is modeled in terms of\nsuitably refined versions of the proof nets of linear logic.\n\nTypelogical grammar has its roots in two seminal papers written by\nJim Lambek half a century ago (Lambek 1958, 1961). In these papers,\nthe author set himself the aim “to obtain an effective rule (or\nalgorithm) for distinguishing sentences from non-sentences, which\nworks not only for the formal languages of interest to the\nmathematical logician, but also for natural languages\n[…]”. To realize this goal, the familiar parts of speech\n(nouns, verbs, …) are turned into formulas of a logic — a\nlogic designed to reason about grammatical composition.  The judgement\nwhether a phrase is wellformed, under this view, is the outcome of a\nprocess of deduction.  A decision procedure for such judgements is\nobtained by casting the grammar logic in the format of a Gentzen-style\nsequent calculus.  The sequent presentation is extremely simple: the\nlogic of grammar is a logic without structural rules.  Contraction and\nWeakening are dropped; their presence would entail that wellformedness\nis unaffected by arbitrary copying or deletion of grammatical\nmaterial.  To take into account also word order and phrase structure\ninformation, Lambek further removes the structural rules of Exchange\nand (in the 1961 paper) Associativity. At the time of their publication, these ideas did not resonate;\ntheir impact on the field of computational linguistics dates from\nthe 1980s. Two factors have played an important role in this belated\nrecognition.  The first was the addition of a computational semantics\nfor categorial derivations along the lines of the Curry-Howard\nproofs-as-programs interpretation in van Benthem 1983, reprinted in\nBuszkowski et al. 1988. Lambek's typelogical grammars, so\nviewed, could be seen to realize Montague’s compositionality\nprogram in an uncompromising way, with the lambda calculus and type\ntheory providing powerful tools to study derivational and lexical\nsemantics. The second factor was the introduction of linear logic\n(Girard 1987), and with it, the surge of interest in substructural\nlogics. A key insight from linear logic injected into typelogical\ngrammar is the idea that structural rules can be reintroduced in a\ncontrolled form by means of so-called modalities: in moving to more\ndiscriminating type logics, no expressivity is lost. From a\ncomputational point of view, variants of the proof nets of linear\nlogic have provided the basis for typelogical natural language\nprocessing. Typelogical grammar, in its current incarnations, keeps the general\narchitecture of Lambek's original calculi, but extends this to a more\narticulate vocabulary of type-forming operations.  Of central\nimportance are the multiplicative operations, used to model\ngrammatical composition; these form the focus of this article. Next to\nthe multiplicatives one can consider additive or Boolean operations\nand first or second order quantification to handle phenomena of\nlexical ambiguity, type polymorphism and the management of feature\ninformation. Morrill 1994 is a good source of examples for such\nextensions. Outline of this article.  We start with the standard\nLambek systems. We study their model-theoretic and proof-theoretic\naspects (frame semantics, sequent calculus), and the relation between\nthe two (soundness, completeness). We characterize compositional\ninterpretation as a homomorphism relating a syntactic source calculus\nand a target calculus for meaning assembly. The mapping associates\nsyntactic derivations with semantic readings, expressed as terms of\nthe simply typed linear lambda calculus.  In §3 we turn to the\nexpressive limitations of the standard Lambek systems, in syntax and\nsemantics, and study how they can be addressed by systematically\nextending the categorial architecture in a modular way.\nGeneralizations concern the arity of the type-forming operations\n(binary composition operations versus unary control operators);\nmultimodal extensions where multiple families of type-forming\noperations co-exist and communicate via structural interaction\nprinciples; one-sorted vs multi-sorted logics (discontinuous calculi);\nsingle-conclusion vs multiple-conclusion systems (symmetric calculi);\nand more structured views of the syntax- semantics interface\n(continuation semantics). To close the panoramic tour, we present\nproof nets for the various typelogical systems studied in §3, and\nwe compare these systems with respect to expressivity and\ncomputational tractability. The type language to be considered in this section consists of a\nsmall set of atomic types, and is closed under the binary operations\nproduct, left and right division. Some notational conventions: in the\nspecification below, lower case p ranges over atomic types,\nupper case A, B over arbitrary types. We pronounce\nA\\B as ‘A under B’\nand B/A as ‘B over A’\nthus to make clear which part of the ‘fraction’ is the\ndenominator (A), and which is the numerator\n(B).   In categorizing linguistic expressions, atomic types stand for\nphrases that one can think of as grammatically ‘complete’.\nExamples, for English, could be s for declarative sentences\n(‘Mary dreams’,…), np for noun phrases\n(‘Mary’, ‘the girl’,…), n for\ncommon nouns (‘girl’, ‘smart girl’,…).\nDivision expresses incompleteness: an expression with\ntype A\\B (B/A) will produce a\nphrase of type B when it is put together with a phrase of\ntype A to its left (right).  Product types A\n⊗ B explicitly express the formation of a complex\nphrase out of constituent parts of type A and B in\nthat order.  So if ‘Mary’ is typed as np and the\nverb ‘dreams’ as np\\s, we\nobtain s for the combination ‘Mary dreams’ by\nmultiplying out these types: np ⊗\n(np\\s). Similarly for the combination ‘the\ngirl dreams’, where one would start from a\ntype np/n for the determiner and n for\n‘girl’: ((np/n) ⊗ n)\n⊗ (np\\s). To make this informal description of the interpretation of the type\nlanguage precise, we turn to modal logic. Frames F =\n(W,R), in the categorial setting, consist of a\nset W of linguistic resources (expressions,\n‘signs’), structured in terms of a ternary relation\nR. This ternary relation stands for grammatical composition, or\n‘Merge’ as it is known in the generative tradition: read\nRxyz as “the expression x is obtained by\nputting together the subexpressions y\nand z”. For comparison: in the Routley-Meyer semantics\nfor relevance logic, R would be the accessibility relation\ninterpreting the fusion operation. A model M is a pair (F,V)\nwhere V is an interpretation (valuation) sending atomic types\nto subsets of W.  For complex types, the valuation respects\nthe clauses below.  We write x ∈ V(A)\nas M,x ⊩ A or, if no confusion will\narise, x ⊩ A. A syntactic calculus for a categorial type language is a deductive\nsystem producing statements of the form A ⊢ B\n(‘B is derivable from A’).  A\nstatement A ⊢ B holds in a model\n(‘M ⊨ A ⊢ B’)\nif V(A) ⊆ V(B); it is valid\n(‘⊨ A ⊢\nB’) if it holds in all models. For the minimal grammar\nlogic, the set of derivations in the syntactic calculus is freely\ngenerated from the axiom and rules of inference below. This minimal\nsystem, for historical reasons, is known as NL (for\n‘Non-associative Lambek calculus’).  The\nfirst line states that derivability is a reflexive, transitive\nrelation, i.e., a preorder.  The bidirectional (‘if and only\nif’) inference rules of the second line characterize the product\nand division operations as a residuated triple as it is known in\nresiduation theory. (Galatos et al. 2007 provides good\nbackground reading.) The syntactic calculus, so defined, precisely\nfits the intended interpretation of the type language, in the sense of\nthe soundness and completeness result below. The completeness result for NL does not impose any restrictions on\nthe interpretation of the Merge\nrelation R. This means that the theorems\nand inference rules of the minimal grammar logic have the status of\ngrammatical invariants: properties of type combination that hold no\nmatter what the structural particularities of individual languages may\nbe.  Here are some examples of such universally valid principles. They\ncome in pairs, because of the left-right symmetry relating slash and\nbackslash. Given the universal freely generated syntactic calculus discussed\nabove, the task of providing a categorial grammar for\na particular language is reduced to specifying its lexicon:\nthe typelogical framework, in this sense, represents the culminating\npoint of ‘lexicalizing’ grammar formalisms.  A categorial\nlexicon is a relation associating each word with a finite number of\ntypes.  Given a lexicon Lex, a categorial grammar assigns\ntype B to a string of words w1 ··· \nwn iff for 1 ≤ i ≤ n,\n(wi,Ai) ∈ Lex,\nand X ⊢ B is provable in the syntactic\ncalculus where X is a product formula with\nyield A1\n··· An. The grammar can be\nconsidered adequate if the strings w1\n··· wn are indeed judged to be the wellformed\nphrases of type B. Lexical type assignments don't have to be handcrafted: Buszkowski\nand Penn (1990) model the acquisition of the lexicon as a\nprocess of solving type equations. Their unification-based algorithms\ntake function-argument structures as input (binary trees with a\ndistinguised head daughter); one obtains variations depending on\nwhether the solution should assign a unique type to every vocabulary\nitem (so-called rigid grammars), or whether one accepts multiple\nassignments (k-valued grammars, with k as the\nmaximal lexical ambiguity factor). Kanazawa (1998) studies learnable\nclasses of grammars from this perspective, in the sense of\nGold’s notion of identifiability ‘in the limit’.\nThis line of research is by now well established, for typelogical\ngrammars proper, and for the related pregroup formalism discussed\nunder §6. The radical lexicalism of typelogical grammar means there is no\nroom for construction specific rule stipulations: central grammatical\nnotions, rather than being postulated, must emerge from the type\nstructure. Here are some examples. Sequent calculus, decidability. How can we decide whether a\nstatement A ⊢ B is indeed a theorem of the\nsyntactic calculus?  In the presence of expanding rules, such as\nLifting, this is not immediately obvious: the transitivity inference\nfrom A ⊢ B and B ⊢ C\nto A ⊢ C involves a ‘lemma’\nformula B which disappears from the conclusion, and there is\nan infinite number of candidate lemma formulas to consider.  A key\npoint of Lambek's original papers consists in the reformulation of the\nsyntactic calculus in an equivalent Gentzen-style sequent format,\nwhich enjoys cut-elimination. Sequents for the grammatical base logic NL are\nstatements X ⇒ A, where X (the\nantecedent) is a structure, and A (the succedent) a type\nformula. The antecedent structure is required to be non-empty. A\nstructure is either a single formula, or a binary branching tree with\nformulas at the leaves.  In the sequent rules below, the tree\nstructure of the antecedent is indicated by bracketing\n(–,–).  The notation X[Y] refers to a\nstructure X containing a distinguished\nsubstructure Y.  For every connective (type-forming\noperation), there are two inference rules, introducing the connective\nto the left or to the right of the derivability arrow.  In this\nsequent presentation, as in Gentzen's propositional logics, Cut\nis an admissible rule: every proof of a theorem that makes use of Cut\ninferences can be transformed into an equivalent cut-free derivation.\nBackward-chaining proof search in the cut-free system respects the\nsubformula property and immediately yields a decision procedure. A landscape of calculi. From the pure residuation logic, one obtains\na hierarchy of calculi by attributing associativity and/or\ncommutativity properties to the composition operation ⊗. These\ncalculi reflect different views on how the grammatical material is\nstructured.  The table below summarizes the options.  \nFor reasons of historical precedence, the system of Lambek 1958, with\nan associative composition operation, is referred to as L; the\npure residuation logic NL, i.e., the non-associative version\nof L, was introduced later in Lambek 1961.  Addition of\ncommutativity turns L and NL into LP\nand NLP, respectively. LP, Lambek calculus with\nPermutation, was introduced in van Benthem 1983; in retrospect, this\nthe implication/fusion fragment of Multiplicative Intuitionistic\nLinear Logic (MILL). The commutative variant of NL has been\nstudied by Kandulski; it has not found significant linguistic\napplications. For the four systems in the hierarchy, one writes\n(N)L(P)* for the (non-conservative) extension\nwhich allows empty antecedents. \nIn the presentation of the syntactic calculus, the structural options\ntake the form of non-logical axioms (postulates), which one\nadds to the residuation laws.  In the sequent presentation, one has\ncorresponding structural rules.  The sequent calculus\nfor L(P) also allows for a sugared presentation where\nthe antecedent is treated as a list or a multiset of formulas; the\ninference steps for the structural rules can then be left implicit.\nWith respect to the frame semantics, each of the structural options\nintroduces a constraint on the interpretation of the Merge relation\nR⊗.  Completeness for the calculi with structural options is\nthen with respect to all frames respecting the relevant constraints\n(Došen 1992). The associative calculus L in fact allows\nmore specific models where we read Rxyz as x\n= y · z, with · a\nbinary concatenation operation.  Pentus (1995) presents a\nquite intricate proof that L is indeed complete with respect to\nsuch free semigroup models (or language models, as they are also\ncalled). A characteristic property of LP is the collapse of the\ndistinction between left and right division: A\\B\nand B/A become interderivable.  From\na syntactic point of view, the global availability of\nassociativity and/or commutativity in LP would entail that\narbitrary changes in constituent structure and/or word order cannot\naffect the well-formedness of an expression — a position that\nfew linguists will subscribe to.  Whether global associativity as we\nfind it in L is a viable option for syntax is open to dispute. Lambek\npoints to cases of overgeneration resulting from free rebracketing in\nhis 1961 paper, but later reverts to the associative view. In the\nfollowing section, we will see that LP is perfectly legitimate\nas a calculus of meaning composition. In §3, we discuss\ncontrolled forms of structural reasoning, anchored in lexical\ntype assignment, as an alternative to the problematic all-or-nothing\noptions. In order to give equal weight to syntactic and semantic\nconsiderations, we can set up a categorial grammar as a pair of\ncalculi linked by a compositional interpretation: a\nsyntactic source calculus, a semantic target\ncalculus, and a homomorphism mapping types and derivations of the\nsource to the corresponding types and derivations of the target. In\nthis section, we work out this architecture for the standard Lambek\ncalculi. In the following section, we will see how the extended\ncategorial type logics reflect different views on the division of\nlabour between syntax and semantics, and hence on the amount of work\nthat has to be done to achieve a compositional mapping between the\ntwo. The semantic study of categorial deduction started with van Benthem\n1983.  In line with the Curry-Howard ‘formulas-as-types,\nproofs-as-programs’ method, derivations in the various\ncategorial calculi are associated with terms of suitable fragments of\nthe lambda calculus. For the sake of simplicity, we restrict attention\nto the implicational vocabulary: the directional slashes for\n(N)L, and the non-directional implication\nof LP. \nLet us start with the Curry-Howard correspondence for LP. The\nchoice of LP as the calculus of natural language semantics captures\nthe fact that meaning composition is a resource-sensitive process: in\ninterpreting a derivation, no material can be copied or thrown away.\nLet us refer to the terms of the simply typed linear lambda calculus\nas Λ(Lin).  In the absence of Contraction and Weakening, this\nis the smallest set such that, given a denumerably infinite set of\nvariables Var, From Λ(Lin), one obtains Λ(LP), the terms in\nCurry-Howard correspondence with LP derivations, by adding one\nextra restriction, reflecting the requirement that sequent antecedents\nmust be non-empty: every subterm contains at least one free variable.\nThe derivations of LP are now read as judgements of a type\ninference system for Λ(LP).  Derivations are given in\nthe natural deduction style, with elimination and introduction rules\nfor the linear implication →. Judgements then are sequents of the\nform Γ ⊢ M : B.  Γ is a non-empty\nmultiset of typing declarations x1\n: A1, …,\nxn : An;\nthe Ai and B are linear implicative\ntypes, and M is a Λ(LP) term of type B\nbuilt out of the xi of\ntype Ai. Axiom sequents are of the\nform x : A ⊢ x : A. Elimination\nand Introduction rules for the linear implication are given below. In\nthe Introduction rule, x ∉ dom(Γ); in the\nElimination rule dom(Γ) ∩ dom(Δ) = ∅. The syntactic calculi (N)L, similarly, are in\nCurry-Howard correspondence with directional linear lambda\nterms (Wansing 1992). There is no standard notation. In the\ndirectional typing rules below, we use λr\nvs λl for the image of the slash and\nbackslash Introduction rules; in the case of the Elimination rules, we\nhave right and left function application, with the triangle pointing\nto the function. The antecedent is now a (bracketed) string of typing\ndeclarations xi : Ai. The source and target calculi each have their own set of basic\ntypes, motivated by syntactic and semantic considerations\nrespectively.  At the syntactic end, one could discriminate between\nsentences s, noun phrases np, common\nnouns n, for example.  At the semantic end, for a simple\nextensional interpretation, one could introduce two basic types, e and\nt. Set-theoretic interpretation for the semantic types is\nthen given in terms of a non-empty set of\nindividuals/entities E and the truth values {0,1}.\nDenotation domains Dom(A) for semantic types A are\nset up such that Dom(e) = E, Dom(t) = {0,1}\nand Dom(A\\B) = Dom(B/A) is the set\nof functions from Dom(A) to Dom(B). \nConsider next the homomorphic mapping from the syntactic source\ncalculus to the semantic target calculus. The mapping is specified at\nthe level of types and at the level of proofs (terms). Setting up the\nmapping at the level of atomic types as below, sentences denote truth\nvalues; (proper) noun phrases individuals; common nouns functions from\nindividuals to truth values. For complex syntactic types, the\ninterpretation function sends the two directional implications to the\nlinear implication of the target calculus. At the level of proofs\n(terms), likewise, the directional constructs of the syntactic source\ncalculus are identified in the interpretation. (We use a primed\nversion for the target variable corresponding to a source variable, as\na reminder of the fact that the type of these expressions is\ndifferent.) \nIn the previous sections, we have seen that the theorems of NL,\nL and LP form a proper inclusion hierarchy.  The\nsemantic counterpart is that in moving to more discriminating\nsyntactic calculi, more and more LP terms are lost in\ntranslation: desirable recipes for meaning assembly are often\nunobtainable as the image of (N)L proofs.  For the three\ncalculi, the table below gives a characteristic type transition.  The\nsecond row has the directional proof terms associated with the\n(N)L type transitions; the third row gives the\ncorresponding LP term, obtained via the translation\nhomomorphism, in the case of (N)L, or directly for\nthe LP case. Argument lowering is valid in NL, hence also in\nL, LP. Function composition depends on associativity; it\nis invalid in NL, valid in L(P). Function\ncomposition has been used to build up partial semantic\nrepresentations, running in parallel with the incremental\n(left-to-right) processing of a string. Argument raising, finally, is\nvalid only in LP, i.e., this form of meaning assembly cannot be\nobtained as the image of a derivation in any of the syntactic calculi\n(N)L. But, as we will see below, this transition plays\nan important role in the analysis of natural language scope\nambiguities. Derivational versus lexical semantics.  The constraints\nimposed by resource-sensitivity put severe limitations on the\nexpressivity of the derivational semantics. To some extent, these can\nbe overcome at the level of lexical semantics. The\nCurry-Howard term associated with a derivation, seen as a program for\nmeaning assembly, abstracts from the contribution of particular\nlexical items.  The semantic specification for a lexical item of\ntype A takes the form of a lambda term of the corresponding\nsemantic type A′, but this lambda term is no longer\nrequired to be linear.  Below some examples of non-linear lexical\nmeaning recipes.  The specification for the reflexive pronoun\n‘himself’ is a pure combinator (a closed term): it\nidentifies the first and second argument of a binary relation.  The\nterms for the relative pronoun ‘that’ or for the\ndeterminers ‘a, some’ have an abstraction that binds two\noccurrences of a variable, so as to compute the intersection of their\ntwo (e → t) arguments (noun and verb phrase),\nand to test the intersection for non-emptiness in the case of\n‘some’. The interplay between lexical and derivational aspects of meaning\nassembly is illustrated with the example below. Notice that the linear\nproof term reflects the derivational history (modulo directionality);\nafter substitution of the lexical recipes and β conversion this\ntransparency is lost. The full encapsulation of lexical semantics is\none of the strong attractions of the categorial approach. The strategy of combining resource-sensitivity at the level of\nderivational semantics with non-linear lexical meaning assignments is\nnot a general solution for the expressive limitations of the syntactic\ncalculi originally conceived by Lambek. A type of problem that\nnecessitates extensions of the derivational machinery itself has to do\nwith the inability of (N)L to\nestablish discontinuous dependencies between grammatical\nresources in a satisfactory way.  Such dependencies manifest\nthemselves in two situations. Extraction.  These are phenomena which generative grammar\nrefers to as ‘overt displacement’, as we find it\nin wh ‘movement’ constructions.  Compare\nthe wh phrases ‘what — annoys Mary’ versus\n‘what Mary put — there’, as they would occur in a\ncontext ‘I know what annoys Mary’ or ‘I know what\nMary put there’.  The dash marks the spot where a noun phrase\nwould fit in a normal declarative sentence: ‘this record annoys\nMary’, ‘Mary put the record there’.  Assigning the\ntype wh/(np\\s) to ‘what’, we\ncan derive wh for the first example, with a np\nhypothesis in the left-peripheral subject position.  If this\nhypothesis (the ‘gap’) occurs internally, as in\nthe second example, it is unreachable for (N)L: one\ncannot derive np\\s for ‘Mary — put\nthere’ (nor s/np, for that matter, which would\nrequire a right-peripheral gap). Infixation. Typical examples would be cases of non-local\nsemantic construal, as in ‘Alice knows someone is\ncheating’, with a wide-scope reading for ‘someone’:\n‘there is a particular person x such that Alice\nthinks x is cheating’. The quantifier phrase\n‘someone’ occupies the structural position where in fact\nthe np-type hypothesis is needed, and realizes its semantic\neffect at some higher sentential level. With a type-assignment\ns/(np\\s) for ‘someone’, there\nis no derivation producing the non-local reading. In generative\ngrammar, one would call the non-local construal an instance of\n‘covert’ movement. The extensions of the basic calculus\nto be discussed in the sections below represent the strategies that\nare being pursued to find principled solutions to these problems of\ndiscontinuous dependencies. The key idea of Multimodal Categorial Grammars, introduced in the\n1990s, is simple: instead of a single family of residuated\ntype-forming operations, one considers multiple families, co-existing\nin one logic. To discriminate between these families, type-forming\noperations are decorated with a mode index\n/i, ⊗i, \\i;\nlikewise for the interpreting Merge\nrelations Ri. Each family has the\nsame logical rules (the residuation laws). But they can\ndiffer in their structural properties. In particular, they can\ninteract in terms of structural rules that mix different\nmodes.  At the level of the interpretation, such interaction\nprinciples introduce frame constraints, similar to the constraints\nthat went with the move from NL to L and LP. For an example, let us return to the case of non-peripheral\nextraction ‘(what) Mary put — there’.  Assume we\nhave two modes of composition: ⊗c and\n⊗d.  Regular valency requirements are\nexpressed in terms of the slashes for the\n⊗c product.  Discontinuous dependencies are\nhandled by structural postulates of mixed associativity and\ncommutativity, and an inclusion postulate expressing the relative\n‘strength’ of ⊗c and\n⊗d. The type-assignment to the pronoun\n‘what’ provides access to these postulates, so that it can\nestablish a link with a non-subject np hypothesis, also in a\nsentence-internal position. We can now succesfully demonstrate that the phrase ‘Mary put\n— there’ is of\ntype s/dnp, as shown in the\nsketch of a derivation below. By means of the mixed interaction\npostulates, the np hypothesis finds its way to the\nnon-peripheral position where it is required by the verb\n‘put’. Once it has reached its home position, the\ninclusion step switches from the structurally permissive mode d to the\nc mode expressing regular subcategorization. Through multimodal interaction principles, one avoids the\novergeneration that would result from global Associativity\nand/or Commutativity options for a single composition operation\n⊗. Undisciplined use of multimodality, however, could easily\nlead to the introduction of construction-specific mode distinctions,\nthus compromising one of the main attractions of the typelogical\napproach.  The theory of structural control operators to be discussed\nbelow offers a more principled solution to the type of problem\nmultimodal approaches address. Control operators.  Linear Logic, by dropping the structural\nrules of Contraction and Weakening, splits the conjunction\n‘&’ of classical logic in a multiplicative and an\nadditive connective.  This move to a resource-sensitive logic can be\nobtained without sacrificing expressivity: the unary modalities\n‘!’,‘?’ allow one to reintroduce\nContraction/Weakening in a controlled form and thus to recover\nclassical logic from within the linear setting. A similar strategy can be followed within typelogical grammar\n(Moortgat 1996, Kurtonina 1995).  The type language is extended with a\npair of unary operators which we will write ◊,□; formulas\nnow are built out of atomic formulas p with the unary and\nbinary type-forming operations.  As with the binary vocabulary, we\nstart with a minimal logic for ◊,□; facilities for\nstructural control can then be added in the form of extra postulates.\nThe truth conditions below characterize the control operators ◊\nand □ as inverse duals with respect to a binary accessibility\nrelation R◊. This interpretation turns them\ninto a residuated pair, just like composition and the left and right\ndivision operations. We saw that in the minimal logic NL completeness with\nrespect to the frame semantics for composition and its residuals\ndoesn't impose restrictions on the interpretation of the merge\nrelation R⊗. The same holds\nfor R◊ in the pure residuation logic of\n◊,□. From the residuation laws, one easily derives\nmonotonicity of the control operators (A ⊢ B\nimplies ◊A ⊢ ◊B and □A\n⊢ □B); their compositions satisfy\n◊□A ⊢ A and A ⊢\n□◊A. These properties can be put to use in refining\nlexical type assignment so that selectional dependencies are\ntaken into account.  Compare the effect of an\nassignment A/B\nversus A/◊□B. The former will produce an\nexpression of type A in composition both with expressions of\ntype B and ◊□B, the latter only with the\nmore specific of these two, ◊□B. An expression typed as\n□◊B will resist composition with\neither A/B or A/◊□B.\nBernardi and Szabolcsi (2008) present accounts of licensing of\npolarity-sensitive expressions and scope construal based on (a\ngeneralization of) this modalisation strategy. For sequent presentation of ◊,□, antecedent tree\nstructures now are built out of formulas by means of unary and binary\ntree-building operations, (–) and (–,–).  The\nresiduation pattern then gives rise to the following left and right\nintroduction rules.  Cut elimination carries over straightforwardly to\nthe extended system, and with it decidability and the subformula\nproperty. Controlling structural resource management.  Let us turn\nthen to the use of ◊,□ as control devices. The control can\ntake two forms: one is to license access to a structural\noption that would be overgenerating if globally available; the second\nis to occasionally block a structural capability of the\ngrammar. For the licensing type of control, compare the multimodal\ntreatment of extraction in terms of a distinction\n⊗c vs ⊗d, with\nthe ◊-controlled version below, which relies on a single binary\ncomposition operation, and narrows down the ‘movement’\npotential entirely to ◊-marked resources. The derivation sketch below illustrates the interplay between\nlogical and structural reasoning. As long as the gap subformula\n◊□np carries the licensing ◊, the structural\nrules are applicable; as soon as it has found the appropriate\nstructural position where it is selected by the transitive verb, it\ncan be used as a regular np, given the type transition\n◊□np ⊢ np which obviates the need\nfor the inclusion postulate of the earlier account.  Related analyses\nof extraction based on the ‘!’ modality of Linear Logic\ncan be found in (Barry et al. 1991), with the dereliction\nrule !A ⊢ A taking the place of\n◊□A ⊢ A for gap introduction. For the type of control where a structural transformation has to be\nblocked, we can turn to extraction islands: phrases that do\nnot allow a gap, such as adjuncts. Compare ‘Mary fell asleep\nduring the concert’, with the adjunct phrase ‘during the\nconcert’ of type s\\s, with the ill-formed\n‘I know what Mary fell asleep during —’. To make the\nadjunct inaccessible, one can assign ‘during’ the type\n(□(s\\s))/np: for the □ to be\neliminated, the whole adjunct phrase has to be enclosed by a\nstructural ◊, which then acts as a barrier for extraction. Without\nsuch a barrier, the extraction postulates as formulated above would\nallow the ill-formed sentence to be derived.  This strategy of\nprojecting island barriers from lexical type assignments was\nintroduced by Morrill; see the discussion of ‘structural\ninhibition’ in Morrill 1994. The two uses of the control operators illustrated here give rise to\na general theory of substructural communication relating the different\ntype logics in the categorial landscape. Let Source and Target be\nneighbouring logics differing in some structural option, for\nexample NL versus L, or L\nversus LP. Kurtonina and Moortgat (1997) establish a set of\nembedding theorems of the following form: In the case where the Target logic is more discriminating than the\nSource, the translation implements control of the licensing\ntype. The other direction of communication obtains when the Target\nsystem is less discriminating than the Source.  The modal decoration\neffected by the translation in this case blocks the applicability of\nstructural rules. Frame constraints, term assignment.  The frame semantics for\nthe pure residuation logic does not impose restrictions on the\ninterpretation of the R◊\nand R⊗ relations.  In the case of extended\nversions with modally controlled structural postulates, there is a\nframe constraint for each structural postulate, and completeness holds\nwith respect to appropriately restricted frames. Depicting R⊗ with a branching\nand R◊ with a non-branching node, in the case\nof the modally controlled extraction postulates discussed here, we\nhave the constraint that for\nall x,y,z,r,s,t\ncomposed as in the input configuration below, there are alternative\ninternal points s′, t′ connecting\nthe root r to the leaves x,y,z. For the mapping between the modally extended syntactic source\ncalculus and the semantic target calculus LP, we have two\noptions.  The first is to treat ◊,□ purely as syntactic\ncontrol devices.  One then sets (◊A)′ =\n(□A)′ = A′, and the inference\nrules affecting the modalities leave no trace in the LP term\nassociated with a derivation. The second is to actually provide\ndenotation domains for the new types, and to extend the term language\naccordingly.  For the minimal logic of ◊,□, one can turn to\nWansing (2002), who develops a set-theoretic interpretation of minimal\ntemporal intuitionistic logic.  The temporal modalities of future\npossibility and past necessity are indistinguishable from the control\noperators ◊,□, prooftheoretically and as far as their\nrelational interpretation is concerned.  Morrill (1990) gives an\ninterpretation in intensional lambda calculus for a stronger S5\nmodality, which assumes a universal accessibility relation at the\nlevel of the frame semantics.  Denotations for modalized\nformulas A, in this setting, are functions from indices\n(situations, reference points) to A denotations. Discussion. The control operators discussed here bear a\nstrong resemblance to the syntactic control features used in Stabler's\nalgebraic formulation of Minimalist Grammars (Stabler 1997, 1999).\nThese features, like ◊,□, come in matching pairs of a\nlicensor ‘+f’ and a licensee\n‘−f’ that have to cancel for a derivation\nto be successful; movement is triggered by the presence of an\nappropriate licensor feature.  For an explicit comparison of the\nmimimalist and the typelogical view on structural control, see Vermaat\n(2004). Whether one wants the full expressivity of modally controlled\nstructural rules is a matter of debate.  Vermaat (2006) takes the\nstrong position that the cross-linguistic possibilities for\ndisplacement can be fully captured in terms of the right branch\nextraction postulates discussed above, together with their mirror\nimages for extraction from left branches.  Under this view, which is\nconsistent with the radical lexicalism of the original categorial\nsystems, this postulate set is fixed at the level of Universal\nGrammar, and variation is reduced to the language-specific lexica that\ndetermine which of the available extraction patterns are\nactivated. The discontinuous Lambek calculi that have been developed by\nMorrill and co-workers (see Morrill et al. 2007, 2009) and\nChapter 6 of the forthcoming monograph (Morrill 2010) extend the\nassociative Lambek calculus L. We saw that L is the\nlogic of strings composed by concatenation.  The discontinuous calculi\nenrich the ontology with a notion of split strings:\nexpressions consisting of detached parts, as in the idiom ‘take\n— to task’. To build the phrase ‘take someone to\ntask’, one wraps the discontinuous expression around\nits object. In this particular example, there is a single point of\ndiscontinuity, but one can also think of cases with more than one\nsplit point. This naturally leads to two notions of discontinuous\ncomposition: a deterministic view, where wrapping targets a particular\nsplit point, and a non-deterministic view where the targeted split\npoint is arbitrary. In the case of expressions with a single split\npoint, these two notions coincide. The vocabulary of DL (Discontinuous Lambek Calculus)\nconsists of residuated families of unary and binary type-forming\noperations.  A representative sample is given below.  For the binary\ncase, in addition to the concatenation product of L and the\nresidual slash operations, we have a discontinuous (wrapping) product\n⊙, with residual infixation ↓ and extraction ↑\noperations.  For the deterministic interpretation, the discontinuous\ntype-forming operations have an indexed form\n↑k, ⊙k,\n↓k explicitly referring to the k-th\nsplit point of their interpretants.  The function of the unary\noperations is to control the creation and removal of split points.  As\nin the binary case, we have non-deterministic operations\n(bridge ∧, split ∨) and indexed forms\n(∧k,∨k)\nwith a deterministic interpretation. On the model-theoretic side, an innovative feature of DL is\nthe move to a multi-sorted interpretation.  The key notion is\nthat of a graded algebra: a freely generated algebra\n(W,·,1, ⎵) where the monoid\n(W,·,1) of the interpretation of L* is\naugmented with a distinguished generator ⎵ called the separator.\nThe sort of an expression s, σ(s), is\ngiven by the number of occurrences of the separator in it. Expressions\nof the nullary sort are the familiar strings for the language models\nof L. Expressions of sort n > 0 are split\nstrings, with n marked positions where other expressions can\nbe substituted. Interpretation of types is now relativized to sorted\ndomains Wi = {s | σ(s)\n= i} for i ≥ 0. Frames, accordingly, are sorted\nstructures\n({Wi}i∈N,Rbridge,Rwrap,·,{bridgek}k∈N*,{wrapk}k∈N*)\nwith n + 1-ary relations for the n-ary type-forming\noperations with a non-deterministic interpretation and n-ary\noperations (functions) for the n-place deterministic\nvocabulary items.  The operation · : Wi\n× Wj\n→ Wi+j here is the sorted\nversion of the concatenation operation of L.  An interpretation for DL associates atomic types of\nsort i to subsets of Wi. Interpretation\nclauses for the new complex types are standard. In the light of the\nillustrations below, we give the clauses for non-deterministic\nbridge/split, and the wrapping family. The sort of the types can be\nreadily computed from the sort information for the interpreting\noperations/relations. On the proof-theoretic side, a cut-elimination theorem for a\nsequent presentation of DL establishes decidability.\nThe DL sequent rules are shown to be sound with respect to the\nintended interpretation; a completeness result so far has not been\nobtained.  For the mapping from the syntactic source\ncalculus DL to the semantic target calculus LP, the\nunary type-forming operations are considered inert: the inference\nrules for these connectives, consequently, leave no trace in\nthe LP proof term associated with a derivation in the syntactic\nsource calculus. The continuous and discontinuous families, for the\nrest, are treated exactly alike.  Specifically, the infixation and\nextraction operations are mapped to LP function types, like the\nslashes. Illustration.  DL has been successfully applied to a great\nnumber of discontinuous dependencies, both of the overt and of the\ncovert type. The non-deterministic operations have been used to model\nparticle shift and complement alternation constructions.  The\ndeterministic operations of sort 1 (single split point) are used in\nthe analysis of non-peripheral extraction, discontinuous idioms,\ngapping and ellipsis, quantifier scope construal, reflexivisation,\npied-piping and the Dutch cross- serial dependencies, among\nothers. We illustrate the non-deterministic use of DL with English\nparticle shift, using labeled natural deduction format to display\nderivations, with items Form-Meaning:Type. A verb-particle\ncombination ‘call — up’ can be lexically typed as\n∨(np\\s) ↑ np of sort 2, with an\ninternal split point, and a right-peripheral one. Elimination of the\nnon-deterministic extraction operation ↑ offers a choice as to\nwhether wrapping will affect the first or the second split point. The\nfirst option is displayed below. The remaining separator is removed in\nthe elimination step for ∨, with the continuous verb phrases\n‘call Mary up’ or ‘call up Mary’ as\nresult. For an example involving covert discontinuity, consider quantifier\nscope construal.  DL provides a uniform type assignment to\ngeneralized quantifier expressions such as ‘everyone’,\n‘someone’: (s ↑ np)\n↓ s.  In the syntactic source calculus, this type\nassignment allows a quantifier phrase QP to occupy any position that\ncould be occupied by a regular non-quantificational noun phrase.\nSemantically, the image of the ↑ Introduction rule at the level\nof the semantic target calculus LP binds an np type\nhypothesis at the position that was occupied by the QP (the a\n− x : np premise, with a\nand x structural and semantic variables for the np\nhypothesis).  The image of the ↓ Elimination rule applies the\nterm representing the QP meaning to this abstract.  Scope ambiguities\narise from derivational ambiguity in the source\ncalculus DL. The derivation below results in a non-local\nreading ‘there is a particular x such that Mary\nthinks x left’.  Looking upward from the conclusion,\nthe last rule applied is ↓ Elimination, which means the QP takes\nscope at the main clause level.  An alternative derivation, producing\nthe local scope reading, would have the / Elimination rule for\n‘thinks’: (np\\s)/s as the last\nstep. Discussion.  The basis for the DL extensions is the\nassociative calculus L. As we saw above, a global insensitivity\nto phrase structure is a source of overgeneration, unless blocked by\nexplicit island modalities.  In the development of DL nothing\nseems to hinge on the associativity of the base system: it would seem\nentirely feasible, in other words, to develop DL as an\nextension of a non-associative basis, allowing for the representation\nof constituent-structure information. In the interpreting frames, one\nwould then start from a graded groupoid rather than a\nmonoid. The fact that the DL framework can readily accommodate\na string or tree-based point of view testifies to the versatility of\nthe approach. The extensions of the Syntactic Calculus that we studied in the\nprevious sections all obey an “intuitionistic”\nrestriction: in statements\nA1,…,An\n⊢ B the antecedent can consist of multiple formulas\n(configured as a ⊗ tree in the non-associative case, a list or\na multiset in the case of L, LP), the succedent is a\nsingle formula. In a remarkable paper antedating Linear Logic by five years,\nGrishin (1983) presents a number of extensions of the Lambek calculus\nwith the common characteristic that derivability holds between\nhypotheses A1,…,An,\ntaken together by means of a multiplicative conjunction,\nand multiple\nconclusions B1,…,Bm,\ncombined by means of a multiplicative disjunction. Linguistic\nexploration of Grishin's ideas has started in recent years.  In this\nsection, we introduce a multiple-conclusion categorial system that is\nalready well-studied: LG (for Lambek-Grishin calculus).  We\npresent syntax and relational semantics of the system; in the next\nsection, we turn to its computational interpretation. LG consists of a symmetric version of the pure residuation\nlogic NL together with structure-preserving interaction\nprinciples relating the conjunctive and disjunctive operations.  We\ndiscuss these components in turn. For symmetry, the inventory of\ntype-forming operations is doubled: in addition to the NL\noperations ⊗,\\,/ (product, left and right division), there is a\nsecond family ⊕,⊘,⦸: coproduct, right and left\ndifference.  The two families are related by an arrow reversal\nsymmetry δ, which translates type formulas according to the\ntable below. At the level of derivability, we then have A ⊢ B iff\nδ(B) ⊢ δ(A): for every theorem or rule of NL,\nwe also find its image under δ in LG. A note about the\nnotation: we read B ⦸ A as ‘B\nfrom A’ and A ⊘ B as\n‘A less B’, i.e., the quantity that is\nsubtracted is put under the circled (back)slash, just as we have the\ndenominator under the (back)slash in the case of left and right\ndivision types. In a formulas-as-types spirit, we will feel free to\nrefer to the division operations also as implications, and to the\ndifference operations as coimplications. Communication between the product and coproduct families requires\nthe addition of interaction principles to the (dual) residuation laws.\nThe principles above take the form of inference rules obtained from\nthe following recipe: from A ⊗ B\n⊢ C ⊕ D in the premise, one selects a\nproduct and a coproduct term; in the conclusion,\none simultaneously introduces the residual operations for the\nremaining two terms. Using the (dual) residuation laws, one derives\nthe following patterns from the interaction principles.\nAlternatively, one can take (P1)–(P4) as primitive postulates,\nand obtain the interaction principles as derived inference rules,\nusing transitivity and the (dual) residuation laws. Derivability patterns of the form (P1)–(P4) have been called\nlinear distributivity principles — linear, because they do not\nduplicate any of the terms involved. In LG, in addition to\nbeing linear, they respect the word order and phrase structure\ninformation encoded in the non-commutative, non-associative\ntype-forming operations. Illustration. The interaction principles of LG\nprovide a particularly direct way of capturing phenomena that depend\non infixation rather than concatenation.  The derivations\nbelow (using monotonicity, and (dual) residuation steps) show how the\nsame pieces of information (the same premises) can be used either to\nintroduce an implication B\\C on the left of the\nturnstile, or a coimplication B ⊘ C on the\nright.  The first option leads to a rule of Application — the\ncentral composition operation of standard Lambek calculus. The second\noption leads to a Co-Application variant.  Although these two rules\nare derived from the same premises, there is an important difference\nbetween them.  When the implication B\\C composes\nwith its argument, it must stay external to X.  In the case\nof the coimplication, when X is some product of\nfactors A1,…,An, the\nconditions for the interaction principles are met.  This means that\nthe coimplication B ⊘ C will be able to\ndescend into the phrase X and associate with any of the\nconstituent parts Ai into a formula (B\n⊘ C) ⦸ Ai. In general, an expression of type (B ⊘ C)\n⦸ A behaves locally as an A within a context\nof type B; it then acts as a function transforming B\ninto C. We illustrate with the example of non-local scope\nconstrual for which we have seen the DL analysis in the previous\nsection. The key point is the lexical type assignment to the\ngeneralized quantifier expression, instantiating (B\n⊘ C) ⦸ A as (s\n⊘ s) ⦸ np. The semantic interpretation\nof this derivation will be discussed below. Completeness, decidability. Relational models for LG\nare given in terms of two interpreting\nrelations: R⊗ for multiplicative\nconjunction (merge, fusion), and R⊕ for\nmultiplicative disjunction (fission). The truth conditions for\nco-product and the difference operations are given below. Completeness of LG with respect to this interpretation is proved in\nKurtonina and Moortgat 2010.  The minimal symmetric system (without\ninteraction principles) has fission R⊕ and\nmerge R⊗ as distinct relations, without\nimposing restrictions on their interpretation.  In the presense of the\ninteraction principles, their interpretation is related in terms of\nframe constraints. The distributivity principle (A\n⦸ B) ⊗ C ⊢ A ⦸\n(B ⊗ C), for example, corresponds to the\nconstraint that for\nevery x,y,z,w,v, if we\nhave a configuration R⊗xyz\nand R⊕vwy, then there exists an\nalternative internal point t such\nthat R⊕twx\nand R⊗tvz.  For decidability,\nMoortgat (2009) gives a sequent presentation of LG in the\nformat of a Display Calculus allowing cut-free proof search. Discussion. LG is a weaker logic than CNL, the\nClassical Non-Associative Lambek Calculus of de Groote and Lamarche\n2002. In the latter system, like in classical linear logic, we have an\ninvolutive negation and De Morgan dualities turning multiplicative\nconjunction (times) and disjunction (par) into interexpressible\noperations. With respect to the linguistic applications, one can\ncompare\nLG with the Discontinuous Calculus of §3.2.  Whereas the\nlatter provides uniform analyses of both the extraction and the\ninfixation type of discontinuities, the distributivity principles\nof LG are primarily addressing the infixation variety.  It may\nbe interesting in this respect to note that Grishin 1983 proposes a\nsecond group of distributivity principles — converses of the\nones discussed above.  Whether these could play a role in the analysis\nof overt displacement remains to be seen.  From a formal point of\nview, each of the groups of distributivities is a conservative\nextension of the basic symmetric calculus.  But the combination of the\ntwo (i.e., the distributivity principles as invertible rules)\ninduces partial associativity/commutativity of the (co)product\noperations, i.e., structure-preservation is lost. In the previous sections, the emphasis was on extending the\ncapabilities of the syntactic source calculi. In this section, we look\nat developments that put more structure in the mapping between the\nsource and target calculi. The syntax-semantics mapping, as discussed in §2.2,\nis rigid: once we have determined its action on the atomic\ntypes of the syntactic source calculus, everything is fixed. This\nrigidity is problematic as soon as we have a class of syntactic\nexpressions with a non-uniform contribution to meaning assembly.  Noun\nphrases are a case in point.  Mapping np to semantic\ntype e is appropriate for proper nouns and definite\ndescriptions whose denotations can be taken to be individuals.  But\nfor quantifying expressions like ‘someone’, ‘no\nstudent’, an interpretation of type e is inadequate.\nThe rigid syntax-semantics mapping thus forces one to assign these\nexpressions a higher-order type in the syntax, for\nexample s/(np\\s), so as to obtain a\nsemantic type with the right kind of denotation as the translation\nimage, (e → t) → t. If one wants\nto avoid this semantically motivated complication of the syntax, one\ncould set np′ = (e → t)\n→ t. But now the effect will be that simple transitive\nverbs (np\\s)/np, which one would like to\ntreat as binary relations e → e\n→ t by default, are associated with third-order\ninterpretations instead. Flexible interpretation. An attractive alternative to this rigid\nview of the syntax-semantics interface, is the flexible interpretation\nof Hendriks 1993.  In this approach, a type of the syntactic source\ncalculus is associated with an infinite set of LP types: one of\nthese is the default semantic type associated with the syntactic\nsource type, the others are derived from this default by means\nof type shifting laws. Similarly, a source calculus term is\ntranslated into an infinite set of target semantic terms; one of these\nis the basic translation (not necessarily of the default\nsemantic type for the relevant syntactic category), the others are\nderived by the Curry-Howard term image of the typing shifting laws.\nThe type shifting principles used are (i) argument lowering, (ii)\nvalue raising, and (iii) argument raising. The first two correspond to\nvalid type transitions of NL; argument raising is the crucial\nprinciple that goes beyond the expressivity of the syntactic source\ncalculi, as we saw in §2.2. As an example of flexible interpretation, compare ‘John loves\nMary’ and ‘Everyone loves someone’.  Syntactically,\nall noun phrases are typed as np, and the verb as\n(np\\s)/np. ‘John’ and\n‘Mary’ are interpreted with constants of type e,\n‘loves’ with a constant of type e\n→ e → t, the default semantic types for\nthe syntactic source types.  But ‘everyone’ and\n‘someone’ are mapped to constants of type (e\n→ t) → t. Interpreting the syntactic\nderivation for ‘Everyone loves someone’, the semantic\ntypes no longer match for simple function application: the verb\nexpects two e type arguments, but instead finds two arguments\nof type (e → t) → t. There are\ntwo ways of accommodating these higher-order arguments by means of\nargument raising: depending on whether one raises first the object\nargument and then the subject, or vice versa, one obtains two readings\nfor one and the same syntactic derivation: a reading corresponding to\nthe surface order, with ∀ taking scope over ∃, and a\nreading with the inverted scope order. The correspondence between\nsyntax and semantics, under this flexible view, has become\na relation, rather than being functionally\ndetermined by the translation homomorphism of §2.2. Continuations.  It has become clear in recent years, that\nthe core ideas of Hendriks' type-shifting account of derivational\nambiguity can be insightfully recast in terms of a\ncontinuation-passing-style interpretation (CPS), as developed within\ncomputer science, and the different evaluation strategies\navailable for such interpretation. The use of continuations in natural\nlanguage semantics has been pioneered by de Groote (2001b) and Barker\n(2002). In the theory of programming languages, a continuation is a\nrepresentation of the control state, i.e., the future of the\ncomputation to be performed. By adding the control state as an\nexplicit parameter in the interpretation, it becomes possible for a\nprogram to manipulate its continuation, and thus to express control\nconstructs that would otherwise be unavailable.  Technically,\ncontinuation semantics makes use of a designated response type for the\nultimate result of a computation; a continuation for an expression of\ntype A is a function that takes an A value to the\nresponse type. In the application to natural language semantics, the\nresponse type is generally identified with the type of truth\nvalues t, i.e., the type assigned to complete sentences. Barker 2004 shows how to obtain the type-shifting scope ambiguity\nfor ‘Everyone loves someone’ in terms of a\ncontinuation-passing-style translation.  The key idea is that in\nmapping the source calculus to the semantic target calculus, all types\nare parameterized with an extra continuation argument.  We use a new\ntranslation function (·)* for this, which composes the mapping\nfrom syntactic to semantic types with the continuization.  A source\nlanguage type A is now associated with a so-called\ncomputation in the target language, i.e., a function acting\non its own continuation: A* = (A′\n→ t) → t. At the level of proofs, given the above interpretation of\ntypes, the task is to find an LP proof for the sequent below,\nthe image of the \\ and / elimination rules\nfor A, A\\B ⊢ B\nand B/A, A ⊢ B.\nWhereas in the source calculus there is only one way of putting\ntogether an A\\B (or B/A) function\nwith its\nA argument, in the target calculus there is a choice as to\nthe evaluation order: do we want to first evaluate the\ntranslation image of the argument, then that of the function, or the\nother way around.  We write ·v for the\nfirst option (call-by-value) and ·n for the\nsecond (call-by-name).  In the target language, m\nand n are variables of type A′\n→ B′ and A′\nrespectively; k is the resulting B′ →\nt continuation. The continuation-passing-style interpretation, like the\ntype-shifting approach, makes it possible to assign syntactic\ntype np both to proper names and to quantificational noun\nphrases: in the target calculus, the translation of np has\nthe appropriate semantic type (e → t)\n→ t.  But the lifting strategy is now generalized to all\nsource types: a transitive verb (np\\s)/np)\nis mapped to ((e → e → t)\n→ t) → t, etc.  For the translation of\nlexical constants of type A, the default recipe is\nλk.(k c), c a non-logical\nconstant of type A′.  The default translation simply\npasses the semantic value for these lexical items to the continuation\nparameter k. But quantificational noun phrases effectively\nexploit the continuation parameter: they take scope over\ntheir continuation, leading to lexical recipes\nλk.(∀\nλx.(k x)),\nλk.(∃\nλx.(k x)) for ‘everyone’\nand ‘someone’. The choice between the evaluation\nstrategies, in combination with these lexical recipes, then results in\ndifferent interpretations for a single derivation in the\nsource calculus M = (everyone⊳(loves⊲someone)),\nwith ·v producing the surface scope\nconstrual, and ·n the inverted scope\nreading. The above example only uses the slash Elimination of NL in\nthe syntactic source calculus.  An important motivation for the\nintroduction of continuations is that they make it possible to give a\nconstructive interpretation to classical (as opposed to\nintuitionistic) logic; see Sørensen and Urzyczyn 2006 for\ndiscussion. It will be no surprise, then, that also the\nmultiple-conclusion symmetric categorial grammar LG has a\nnatural interpretation in the continuation-passing-style (Bernardi and\nMoortgat 2010, Moortgat 2009).  In the example we gave above, source\ntypes A are provided with a single continuation parameter. In\nthe CPS translation for LG types, the continuization of\nsyntactic source types is performed recursively.  Let us\nwrite V(A) for target language values of\ntype A, K(A) for continuations, i.e.,\nfunctions V(A) → R,\nand C(A) for computations, i.e.,\nfunctions K(A) → R, where R\nis the response type.  For an LG syntactic source\ntype A, the call-by-value CPS translation yields an LP\nvalue V(A) as follows.  V(p)\n= p for atomic types, At the level of proofs (and the terms in Curry-Howard\ncorrespondence with them), the CPS translation turns the\nmultiple-conclusion source derivations into\nsingle-conclusion LP derivations. The translation respects the\ninvariant below. An active output formula A (marked off by\nthe vertical bar in the box below) maps to a computation\nC(A), an active input formula to a\ncontinuation K(A). A cut then is interpreted as the\napplication of C(A) to K(A). Discussion.  Continuation-based approaches are now available\nfor a number of recalcitrant phenomena that would seem to defy a\ncompositional treatment.  Examples, at the sentential level, include\nthe treatment of in situ scope construal and wh questions of\nShan and Barker 2006, where crossover and superiority violations are\nexplained in terms of a preference of the human processor for a\nleft-to-right evaluation strategy; donkey anaphora (Barker and Shan\n2008)); quantifier scope ambiguities for the call-by-value and\ncall-by-name interpretation of LG are investigated in Bernardi and\nMoortgat 2010. At the discourse level, de Groote (2006) gives a\ntype-theoretic analysis of dynamic phenomena, modeling propositions as\nfunctions over a sentence's left and right context (continuation). In the previous sections, we have seen how the different categorial\ncalculi can be presented as proof systems with a sequent-based\ndecision procedure.  Naive proof search in these systems, however, is\ninefficient from a computational point of view: in general, there will\nbe multiple ways to construct alternative proofs that differ only in\nthe order of the application of the inference rules, but produce the\nsame interpretation (the LP term associated with a derivation\nunder the interpretation homomorphism). This issue of\n‘spurious’ ambiguity can be tackled by the introduction of\nnormal form derivations (as in Hepple 1990, Hendriks 1993), compare\nfocused proof search regimes in linear logic), combined with the use\nof chart-based parsing methods, as in Hepple 1999, Capelletti 2007. An\nalternative for categorial ‘parsing-as-deduction’ is to\nleave the sequent calculus for what it is, and to switch to\na proof net approach.  Proof nets, originally developed in\nthe context of linear logic, use a representation of derivations that\nis inherently free of ‘spurious ambiguity’, i.e., the issue\nof irrelevant rule orderings simply doesn't arise. For a general\ntreatment of proof nets, we refer to the entry on \n linear Logic.\n Below we comment on aspects that are specifically addressing issues in\ncategorial grammar. Proof nets for the Lambek calculi. Proof nets for the\nassociative calculus L and the commutative variant LP\nhave been studied initially by Roorda (1991, 1992). To capture the\n‘intuitionistic’ nature of these systems, one works with\nformulas with polarities: input (‘given’)\npolarity for antecedent occurrences of a formula, versus output\n(‘to prove’) polarity for succedent\noccurrences. Correctness criteria identify proof nets among a wider\nclass of graphs: to the criteria of acyclicity and connectedness,\napplicable for linear logic in general, one adds planarity to\ncapture word order sensitivity: in the proof nets for L, axioms\nlinks cannot cross.  With respect to the syntax-semantics interface,\nde Groote and Retoré (1996) show that the lambda terms in\nCurry-Howard correspondence with LP derivations in the semantic\ntarget calculus can be read off from a proof net by specifying a set\nof ‘travel instructions’ for traversing a net; these\ninstructions then correspond step-by-step with the construction of the\nassociated lambda term. Incremental processing. Proof nets, considered statically as\ngraphs satisfying certain correctness criteria, remove spurious\nchoices relating to the order of rule applications in sequent calculi:\nin this respect, they represent a purely ‘declarative’\nview on categorial deductions. Johnson (1998) and Morrill (2000) have\npointed out that an alternative, ‘procedural’ view on the\nactual process of constructing a net makes perfect sense as well, and\noffers an attractive perspective on performance phenomena. Under this\ninterpretation, a net is built in a left-to-right incremental fashion\nby establishing possible linkings between the input/output literals of\nthe partial proof nets associated with lexical items as they occur in\nreal time. This suggests a simple complexity measure on an incremental\ntraversal, given by the number of unresolved dependencies between\nliterals.  This complexity measure correlates nicely with a number of\nwell-attested processing issues, such as the difficulty of center\nembedding, garden path effects, attachment preferences, and\npreferred scope construals in ambiguous constructions. First-order quantifiers.  The planarity condition singles\nout the non-commutative proof nets for L among the LP\nnets. To deal with the more structured categorial calculi discussed\nhere, the correctness criteria have to be refined. One strategy of\ndoing this is via a translation into MILL1 (first-order multiplicative\nintuitionistic linear logic) where one has proof nets with extra links\nfor existential and universal quantification over first order\nvariables. One can then use these variables in a way very similar to\nthe use of position arguments in Definite Clause Grammars as used in\nlogic programming. Moot and Piazza (2001) work out such translations\nfor L and NL. For the concatenation operations\nof L, one replaces the proposition letters (atomic formulas) by\ntwo-place terms, marking beginning and end of a continuous string. For\nnon-associative NL, one adds an extra variable to keep track of\nthe nesting depth of subformulas. For wrapping operations in the\nsimple discontinuous calculus DL (allowing a single split\npoint), Morrill and Fadda (2008) use four-place predicates.  In\ngeneral, we find a correlation here between the syntactic expressivity\nof the calculi and the number of variables needed to encode their\nstructural resource management. Nets and rewriting.  The multimodal and symmetric calculi of\n§3.1 and §3.3 pose a challenge to the proof net methods as\noriginally developed for linear logic.  In these systems we typically\nfind one-way structural rules, such as the extraction postulates for\novert displacement, or the Grishin distributivity laws in the case of\nLG: these one-way rules naturally suggest a notion of graph\nrewriting. A completely general proof net framework for the\nextended Lambek calculi, with a correctness criterion based on\nrewriting, has been developed in Moot and Puite 2002 and Moot\n2007. The basic building block for the Moot-Puite nets is a generalized\nnotion of a link, accommodating connectives of any arity.  A link is\ndetermined by its type (tensor or cotensor), its premises (a sequence\nP1,…,Pn, 0\n≤ n), its conclusions (a\nsequence C1,…,Cm, 0\n≤ m), and its main formula (which can be empty, in the\ncase of a neutral link, or one of the Pi\nor Ci). A proof structure is a set of\nlinks over a finite set of formulas such that every formula is at most\nonce the premise of a link and at most once the conclusion.  Formulas\nwhich are not the conclusion of any link are the hypotheses\nof the proof structures, whereas the formulas which are not the\npremise of any link are the conclusions.  An axiom formula is\na formula which is not the main formula of any\nlink.  Abstract proof structures are obtained by erasing all\nformulas on the internal nodes. A proof net is a proof\nstructure for which the abstract proof structure converts to a tensor\ntree — a rooted tree in the case of the intuitionistic systems,\npossibly an unrooted tree in the case of symmetric LG.  Proofs\nnets, so defined, can then be show to be the graphs that correspond to\nvalid derivations. The rewriting steps transforming a candidate abstract proof\nstructure into a proof net are of two kinds.  The logical\ncontractions correspond to identities A\n⊢ A, for complex formulas A; they reduce a\nconfiguration of a matching tensor and cotensor link to a point.\nThe structural conversions perform an internal rewiring of a\nproof structure with\nhypotheses H1,…,Hn and\nconclusions C1,…,Cm to\na structure with some permutation of the Hi as\nhypotheses and some permutation of the Ci as\nconclusions. Copying and deletion, in other words, are ruled out. As\nan example, the rewriting corresponding to one of the Grishin\ninteraction principles discussed in §3.3, allowing us to\ninfer C ⦸ A ⊢ D/B\nfrom A ⊗ B ⊢ C\n⊕ D. Hypotheses A, B are on the left,\nconclusions C, D on the right.  The net\nrepresentation brings out in a particularly clear way that these\nprinciples are structure-preserving: they leave the order of\nhypotheses and conclusions untouched. Reflecting on the above, one can say that the extensions of the\nsyntactic calculus reviewed here are motivated by the desire to find a\nproper balance between expressivity and computational\ntractability: on the side of expressivity, an adequate\ntypelogical framework should be able to recognize patterns beyond the\nstrictly context-free; ideally, one would like to keep such\nexpressivity compatible with a polynomial derivability problem.  Among\ncontemporary ‘lexicalized’ grammar formalisms, there is a\nremarkable convergence on systems that meet these requirements: the\nso-called ‘mildly context-sensitive’ systems (Joshi,\nVijay-Shanker, and Weir 1991).  Where can we situate the typelogical\nsystems discussed here with respect to recognizing capacity and\ncomplexity? The minimal system in the typelogical hierarchy NL has a\npolynomial recognition problem (see de Groote 1999, and Capelletti\n2007 for actual parsing algorithms), but it is strictly context-free\n(Kandulski 1988).  Extensions with global structural rules are\nunsatisfactory, both on the expressivity and on the complexity front.\nAs for L, Pentus (1993b, 2006) shows that it remains strictly\ncontext-free, whereas the addition of global associativity makes the\nderivability problem NP complete.  NP completeness already holds for\nthe product-free fragment of L (Savateev 2009).  Also\nfor LP, i.e., multiplicative intuitionistic linear logic, we\nhave NP completeness (Kanovich 1994).  With regard to recognizing\ncapacity, van Benthem (1995) shows that LP recognizes all\npermutation closures of context-free languages: a class which is too\nwide from the syntactic point of view.  As the logic of\nmeaning assembly, LP is a core component of the\ntypelogical inventory.  But as we saw in the discussion of the\nsyntax-semantics interface, we can restrict attention to the\nsublanguage of LP that forms the image of derivations in\nsyntactic calculi making interesting claims about word order and\nphrase structure. The situation of the multimodal and symmetric extensions is more\nintricate. Expressivity here is directly related to the kind of\nrestrictions one imposes on structural resource management. At one end\nof the spectrum, multimodality without structural rules does not lead\nus beyond context-free recognition: Jäger (2003) shows that the\npure residuation logic for n-ary families of type-forming\noperations stays strictly context-free. If one requires structural\nrules to be resource-sensitive (no copying or deletion) and,\nfor the unary modalities, non-expanding, one obtains the full\nexpressivity of context-sensitive grammars, and the PSPACE complexity\nthat goes with it (Moot 2002). (PSPACE consists of those problems \nsolvable using some polynomial amount of memory space.  See the entry on \n computability and complexity.\n If one imposes no restrictions on structural rules (specifically, if\none allows copying and deletion operations), unsurprisingly, one\nobtains the expressivity of unrestricted rewriting systems (Carpenter\n1999). A controlled use of copying is used in the analysis of\nanaphora resolution in Jäger 2005. The symmetric calculus LG without interaction\nprinciples is context-free, as shown in Bastenhof 2010.  For the\nsystem with the interaction principles of §3.3, Melissen\n(2009) shows that all languages which are the intersection of a\ncontext- free language and the permutation closure of a context-free\nlanguage are recognizable in LG. In this class, we find\ngeneralized forms of MIX (the language consisting of an equal number\nof symbols a, b, c, in any order), with\nequal multiplicity of k alphabet symbols in any order, and\ncounting dependencies\n an1…ank\nfor any number k of alphabet symbols.  Patterns of this type\nare recognized by Range Concatenation Grammars and Global Index\nGrammars; a comparison with these formalisms then might be useful to\nfix the upper bound of the recognizing capacity of LG, which is as yet\nunknown. With respect to computational complexity, Moot (2008) establishes a\ncorrespondence between Lexicalized Tree Adjoining Grammars on the one\nhand, and categorial grammars with the multimodal extraction\npostulates of §3.1 and a restricted set of LG grammars on\nthe other. For these grammars he obtains a polynomial parsability\nresult via a translation into Hyperedge Replacement Grammars.  In the\ncase of LG, the restriction requires the coimplications\n⊘,⦸ to occur in matching pairs in lexical type\nassignments.  The lexicon of the generalized MIX construction of\nMelissen 2009, and the type assignment used for quantifier phrases in\nthe analysis of scope construal in Bernardi and Moortgat 2010, do not\nrespect this restriction. For the general case of LG with the\ninteraction principles of §3.3, Bransen (2010) establishes\nNP-completeness.  The position of the discontinuous calculi of\n§3.2 in this hierarchy has to be determined: they\nrecognize more than the context-free languages, but it is not clear\nwhether they stay within the mildly context-sensitive family. Typelogical grammars as discussed here share a number of\ncharacteristics with a number of related formal grammar frameworks\nwhere types also play a key role.  We briefly discuss some of these\nrelatives, and provide pointers to the literature for readers who want\nto explore the connections further. Combinatory Categorial Grammar (CCG).  The CCG framework\n(see Steedman 2000 for a comprehensive presentation) takes its name\nfrom the Combinatory Logic of Curry and Feys.  Whereas the design of\ntypelogical grammars follows a logical agenda (sequent calculus,\nsoundness, completeness, etc), grammars in the CCG tradition are\npresented as finite sets of rules for type change and type\ncombination.  This rule inventory will typically include function\napplication schemata, type lifting, and functional composition, both\nof the kind valid within associative L (composing implicational\ntypes with the same directionality), and of the mixed kind (composing\nfunctors with different directionality). The development of CCG and Lambek style typelogical grammars in the\n1980s initially followed separate courses. More recently, there are\nsigns of convergence. An important factor has been the introduction of\nthe typelogical technique of multimodal control to fine-tune the\napplicability of the CCG combinatory schemata (Baldridge 2002, Kruijff\nand Baldridge 2003).  Multimodal typelogical grammar, from this\nperspective, plays the role of the underlying general logic which one\nuses to establish the validity of the CCG combinatory schemata,\nwhereas the particular choice of primitive combinators is motivated by\ncomputational efficiency considerations (Hoyt and Baldridge 2008). In\nterms of generative capacity, CCG is a member of the mildly context\nsensitive family of grammar formalisms. Polynomial parsability is\nobtained by imposing a bound on the functional composition\nschemata. Pregroup grammars.  Pregroups are an algebraic version of\ncompact bilinear logic (Lambek 1993) obtained by collapsing\nthe tensor and cotensor operations.  Pregroup grammars were introduced\nin Lambek 1999 as a simplification of the original Syntactic\nCalculus L. They have since been used to build computational\nfragments for a great variety of languages by Lambek and co-workers.\nA pregroup is a partially ordered monoid in which each\nelement a has a left and a right adjoint, al,\nar, satisfying ala → 1\n→ aal and aar → 1\n→ ara, respectively.  Type assignment takes\nthe form of associating a word with one or more elements from the free\npregroup generated by a partially ordered set of basic types.  For the\nconnection with categorial type formulas, one can use the translations\na/b = abl\nand b\\a = bra. Parsing, in the\npregroup setting, is extremely straightforward. Lambek (1999) proves\nthat one only has to perform the contractions\nreplacing ala and ala by the\nmultiplicative unit 1.  This is essentially a check for\nwell-bracketing — an operation that can be entrusted to a\npushdown automaton. The expansions 1 → aal\nand 1 → ara are needed to prove equations\nlike (ab)l\n= blal.  We have used the latter to\nobtain the pregroup version of the higher-order relative pronoun type\n(n\\n)/(s/np) in the example\nbelow. Compact bilinear logic is not a conservative extension of the\noriginal Syntactic Calculus.  Every sequent derivable in L has\na translation derivable in the corresponding pregroup, but the\nconverse is not true: the pregroup image of the types (a\n⊗ b)/c and a ⊗\n(b/c), for example,\nis a b cl, but these two types are\nnot interderivable in L. With respect to generative capacity, Buszkowski (2001) shows that\nthe pregroup grammars are equivalent to context-free grammars. They\nshare, in other words, the expressive limitations of the original\ncategorial grammars. To overcome these limitations different\nstrategies have been pursued, including lexical rules (metarules),\nderivational constraints, controlled forms of commutativity, and\nproducts of pregroups. The Studia Logica special issue\n(Buszkowski and Preller 2007) and the monograph Lambek 2008 give a\ngood picture of current research. Abstract Categorial Grammar.  The ACG framework (de Groote\n2001a) is a meta-theory of compositional grammar architectures. ACGs\nare built on higher-order linear signatures Σ =\n(A,C,τ), where A and C are\nfinite sets of atomic types and constants respectively, and τ a\nfunction assigning each constant a linear implicative type over\nA. Given a source signature Σ and a target signature\nΣ′, an interpretation is a mapping form Σ to\nΣ′ given by a pair of functions: η mapping the type\natoms of Σ to linear implicative types of Σ′ and\nθ mapping the constants of Σ to well-typed linear lambda\nterms of Σ′ in a way that is compatible with the mapping\non types. Using the terminology of compiler theory, one refers to the\nsource and target signatures as the abstract vocabulary and the\nconcrete vocabulary, respectively, and to the interpretive\nmapping as the lexicon. An ACG is then obtained by specifying\nan atomic type of Σ as the distinguished type of the\ngrammar. In the ACG setting, one can model the syntax-semantics interface in\nterms of the abstract versus object vocabulary distinction. But one\ncan also obtain surface form as the result of an interpretive\nmapping, using the canonical λ term encodings of strings and\ntrees and operations on them. ACG has given rise to an interesting\ncomplexity hierarchy for rewriting grammar formalisms encoded as ACGs:\ncontext-free grammars, tree-adjoining grammars, etc.; see for example\nde Groote and Pogodalla 2004.  Expressive power of these formalisms is\nmeasured in terms of the maximal order of the constants in the\nabstract vocabulary and of the object types interpreting the atomic\nabstract types. The study of ACG encodings of typelogical systems\nproper has started with Retoré and Salvati 2010; these authors\npresent an ACG construction for (product-free) NL. It will be clear from this description that the ACG architecture is\nclosely related to the compositional interpretation for categorial\ntype logics as discussed in this artile. A key difference is the\nnature of the ‘abstract syntax’, i.e. the source calculus\nfrom which interpretations are homomorphically derived. In the case of\nthe standard Lambek systems and the extended systems discussed in\n§3 above, the abstract syntax is a directional type\nlogic; in the case of ACG, one finds LP and the linear lambda\ncalculus both at the source and at the target end.  The debate as to\nwhether structural properties of language have to be accounted for at\nthe level of the abstract syntax has a long history, starting with\nCurry 1961; see Muskens 2007 for discussion.  The typelogical view\naccounts for word order universals at the level of its logical\nconstants, i.e., the type-forming operations, and the laws that\ngovern them.  The ACG view is more liberal in this respect: the\nderivation of surface form can be specified on a word-by-word\nbasis. Whether this more relaxed connection between abstract syntax\nand surface realisation is desirable, is a matter of debate. An\nattractive feature of ACG that has not been investigated\nsystematically within the typelogical setting is the connection\nbetween expressivity and order restrictions on the source constants\nand on the interpretive mapping.","contact.mail":"Michael.Moortgat@phil.uu.nl","contact.domain":"phil.uu.nl"}]
