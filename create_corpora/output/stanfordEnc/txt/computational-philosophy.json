[{"date.published":"2020-03-16","url":"https://plato.stanford.edu/entries/computational-philosophy/","author1":"Patrick Grim","author2":"Daniel Singer","entry":"computational-philosophy","body.text":"\n\n\nComputational philosophy is the use of mechanized computational\ntechniques to instantiate, extend, and amplify philosophical research.\nComputational philosophy is not philosophy of computers or\ncomputational techniques; it is rather philosophy using\ncomputers and computational techniques. The idea is simply to apply\nadvances in computer technology and techniques to advance discovery,\nexploration and argument within any philosophical area.\n\n\nAfter touching on historical precursors, this article discusses\ncontemporary computational philosophy across a variety of fields:\nepistemology, metaphysics, philosophy of science, ethics and social\nphilosophy, philosophy of language and philosophy of mind, often with\nexamples of operating software. Far short of any attempt at an\nexhaustive treatment, the intention is to introduce the spirit of each\napplication by using some representative examples.\n\nComputational philosophy is not an area or subdiscipline of philosophy\nbut a set of computational techniques applicable across many\nphilosophical areas. The idea is simply to apply computational\nmodeling and techniques to advance philosophical discovery,\nexploration and argument. One should not therefore expect a sharp\nbreak between computational and non-computational philosophy, nor a\nsharp break between computational philosophy and other computational\ndisciplines. \nThe past half-century has seen impressive advances in raw computer\npower as well as theoretical advances in automated theorem proving,\nagent-based modeling, causal and system dynamics, neural networks,\nmachine learning and data mining. What might contemporary\ncomputational technologies and techniques have to offer in advancing\nour understanding of issues in epistemology, ethics, social and\npolitical philosophy, philosophy of language, philosophy of mind,\nphilosophy of science, or philosophy of religion? Suggested by Leibniz\nand with important precursors in the history of formal logic, the idea\nis to apply new computational advances within long-standing areas of\nphilosophical interest. \nComputational philosophy is not the philosophy of\ncomputation, an area that asks about the nature of computation itself.\nAlthough applicable and informative regarding artificial intelligence,\ncomputational philosophy is not the philosophy of artificial\nintelligence. Nor is it an umbrella term for the questions about the\nsocial impact of computer use explored for example in philosophy of\ninformation, philosophy of technology, and computer ethics. More\ngenerally, there is no “of” that computational philosophy\ncan be said to be the philosophy of. Computational philosophy\nrepresents not an isolated topic area but the widespread application\nof whatever computer techniques are available across the full range of\nphilosophical topics. Techniques employed in computational philosophy\nmay draw from standard computer programming and software engineering,\nincluding aspects of artificial intelligence, neural networks, systems\nscience, complex adaptive systems, and a variety of computer modeling\nmethods. As a growing set of methodologies, it includes the prospect\nof computational textual analysis, big data analysis, and other\ntechniques as well. Its field of application is equally broad,\nunrestricted within the traditional discipline and domain of\nphilosophy. \nThis article is an introduction to computational philosophy rather\nthan anything like a complete survey. The goal is to offer a handful\nof suggestive examples across computational techniques and fields of\nphilosophical application. \nThe only way to rectify our reasonings is to make them as tangible as\nthose of the Mathematicians, so that we can find our error at a\nglance, and when there are disputes among persons, we can simply say:\nLet us calculate, without further ado, to see who is right. —Leibniz,\nThe Art of Discovery (1685 [1951: 51]) \nFormalization of philosophical argument has a history as old as\n logic.[1]\n Logic is the historical source and foundation of contemporary\n computing.[2]\n Our topic here is more specific: the application of contemporary\ncomputing to a range of philosophical questions. But that too has a\nhistory, evident in Leibniz’s vision of the power of\ncomputation. \nLeibniz is known for both the development of formal techniques in\nphilosophy and the design and production of actual computational\nmachinery. In 1642, the philosopher Blaise Pascal had invented the\nPascaline, designed to add with carry and subtract. Between 1673 and\n1720 Leibniz designed a series of calculating machines intended to\ninstantiate multiplication and division as well: the stepped reckoner,\nemploying what is still known as the Leibniz wheel (Martin 1925). The\nsole surviving Leibniz step reckoner was discovered in 1879 as workmen\nwere fixing a leaking roof at the University of Göttingen. In\ncorrespondence, Leibniz alluded to a cryptographic encoder and decoder\nusing the same mechanical principles. On the basis of those\ndescriptions, Nicholas Rescher has produced a working conjectural\nreconstruction (Rescher 2012). \nBut Leibniz had visions for the power of computation far beyond mere\narithmetic and cryptography. Leibniz’s 1666 Dissertatio De\nArte Combinatoria\ntrumpets the “art of combinations” as a method of\nproducing novel ideas and inventions as well as analyzing complex\nideas into simpler elements (Leibniz 1666 [1923]). Leibniz describes it as the “mother\nof inventions” that would lead to the “discovery of all\nthings”, with applications in logic, law, medicine, and physics.\nThe vision was of a set of formal methods applied within a perfect\nlanguage of pure concepts which would make possible the general\nmechanization of reason (Gray\n 2016).[3] \nThe specifics of Leibniz’s combinatorial vision \ncan be traced back to the mystical mechanisms of Raymond Llull circa 1308, combinatorial mechanisms lampooned in\nJonathan Swift’s Gulliver’s Travels of 1726 as allowing one to  \nwrite books in philosophy, poetry, politics, mathematics, and\ntheology, without the least assistance from genius or study. (Swift\n1726: 174, Lem 1964 [2013: 359])\n \nCombinatorial specifics aside, however, Leibniz’s vision of an\napplication of computational methods to substantive questions remains.\nIt is the vision of computational physics, computational biology,\ncomputational social science, and—in application to perennial\nquestions within philosophy—of computational philosophy. \nDespite Leibniz’s hopes for a single computational method that\nwould serve as a universal key to discovery, computational philosophy\ntoday is characterized by a number of distinct computational\napproaches to a variety of philosophical questions. Particular\nquestions and particular areas have simply seemed ripe for various\nmodels, methodologies, or techniques. Both attempts and results are\ntherefore scattered across a range of different areas. In what follows\nwe offer a survey of various explorations in computational\nphilosophy. \nComputational philosophy is perhaps most easily introduced by focusing\non applications of agent-based modeling to questions in social\nepistemology, social and political philosophy, philosophy of science,\nand philosophy of language. Sections 3.1 through 3.3 are therefore\nstructured around examples of agent-based modeling in these areas.\nOther important computational approaches and other areas are discussed\nin 3.4 through 3.6. \nTraditional epistemology—the epistemology of Plato, Hume,\nDescartes, and Kant—treats the acquisition and validation of\nknowledge on the individual level. The question for traditional\nepistemology was always how I as an individual can acquire\nknowledge of the objective world, when all I have to work with is my\nsubjective experience. Perennial questions of individual epistemology\nremain, but the last few decades have seen the rise of a very\ndifferent form of epistemology as well. Anticipated in early work by\nAlvin I. Goldman, Helen Longino, Philip Kitcher, and Miriam Solomon,\nsocial epistemology is now evident both within dedicated\njournals and across philosophy quite generally (Goldman 1987; Longino\n1990; Kitcher 1993; Solomon 1994a, 1994b; Goldman & Whitcomb 2011;\nGoldman & O’Connor 2001 [2019]; Longino 2019). I acquire my\nknowledge of the world as a member of a social group: a group that\nincludes those inquirers that constitute the scientific enterprise,\nfor example. In order to understand the acquisition and validation of\nknowledge we have to go beyond the level of individual epistemology:\nwe need to understand the social structure, dynamics, and process of\nscientific investigation. It is within this social turn in\nepistemology that the tools of computational\nmodelling—agent-based modeling in particular—become\nparticularly useful. (Klein, Marx and Fischbach 2018). \nThe following two sections use computational work on belief change as\nan introduction to agent-based modeling in social epistemology.\nClosely related questions regarding scientific communication are left\nto sections\n 3.2.2\n and\n 3.2.3. \nHow should we expect beliefs and opinions to change within a social\ngroup? How might they rationally change? The computational\napproach to these kinds of questions attempts to understand basic\ndynamics of the target phenomenon by building, running, and analyzing\nsimulations. Simulations may start with a model of interactive\ndynamics and initial conditions, which might include, for example, the\ninitial beliefs of individual agents and how prone those agents are to\nshare information and listen to others. The computer calculates\nsuccessive states of the model (“steps”) as a function\n(typically stochastic) of preceding stages. Researchers collect and\nanalyze simulation outputs, which might include, for example, the\ndistribution of beliefs in the simulated society after a certain\nnumber of rounds of communication. Because simulations typically\ninvolve many stochastic elements (which agents talk with which agents\nat what point in the simulation, what specific beliefs specific agents\nstart with, etc.), data is usually collected and analyzed across a\nlarge number of simulation runs. \nOne model of belief change and opinion polarization that has been of\nwide interest is that of Hegselmann and Krause (2002, 2005, 2006),\nwhich offers a clear and simple example of the application of\nagent-based techniques. \nOpinions in the Hegselmann-Krause model are mapped as numbers in the\n[0, 1] interval, with initial opinions spread uniformly at random in\nan artificial population. Individuals update their beliefs by taking\nan average of the opinions that are “close enough” to an\nagent’s own. As agents’ beliefs change, a different set of\nagents or a different set of values can be expected to influence\nfurther updating. A crucial parameter in the model is the threshold of\nwhat counts as “close enough” for actual\n influence.[4] \n\n Figure 1\n shows the changes in agent opinions over time in single runs with\nthresholds ε set at 0.01, 0.15, and 0.25 respectively. With a\nthreshold of 0.01, individuals remain isolated in a large number of\nsmall local groups. With a threshold of 0.15, the agents form two\npermanent groups. With a threshold of 0.25, the groups fuse into a\nsingle consensus opinion. These are typical representative cases, and\nruns vary slightly. As might be expected, all results depend on both\nthe number of individual agents and their initial random locations\nacross the opinion space. See the\n interactive simulation of the Hegselmann and Krause bounded confidence model\n in the Other Internet Resources section below. \n \nFigure 1: Example changes in opinion\nacross time from single runs with different threshold values\n\\(ε \\in \\{0.01, 0.15, 0.25\\}\\) in the Hegselmann and Krause\n(2002) model. [An\n extended description of figure 1\n is in the supplement.] \nAn illustration of average outcomes for different threshold values\nappears as\n figure 2.\n What is represented here is not change over time but rather the final\nopinion positions given different threshold values. As the threshold\nvalue climbs from 0 to roughly 0.20, there is an increasing number of\nresults with concentrations of agents at the outer edges of the\ndistribution, which themselves are moving inward. Between 0.22 and\n0.26 there is a quick transition from results with two final groups to\nresults with a single final group. For values still higher, the two\nsides are sufficiently within reach that they coalesce on a central\nconsensus, although the exact location of that final monolithic group\nchanges from run to run creating the fat central spike shown.\nHegselmann and Krause describe the progression of outcomes with an\nincreasing threshold as going through three phases:  \nAs the homogeneous and symmetric confidence interval increases we\ntransit from phase to phase. More exactly, we step from fragmentation\n(plurality) over polarisation (polarity) to consensus (conformity).\n(2002: 11, authors’ italics) \nFigure 2: Frequency of equilibrium opinion\npositions for different threshold values in the Hegselmann and Krause\nmodel scaled to [0, 100] (as original with axes relabeled; Hegselmann\nand Krause 2002). [An\n extended description of figure 2\n is in the supplement.] \nA number of models further refine the “bounded confidence”\nmechanisms of the Hegselmann Krause model. Deffuant et al., for\nexample, replace the sharp cutoff of influence in Hegselmann-Krause\nwith continuous influence values (Deffuant et al. 2002; Deffuant 2006;\nMeadows & Cliff 2012). Agents are again assigned both opinion\nvalues and threshold (“uncertainty”) ranges, but the\nextent to which the opinion of agent i is influential on\nagent j is proportional to the ratio of the overlap of their\nranges (opinion plus or minus threshold) over i’s\nrange. Opinion centers and threshold ranges are updated accordingly,\nresulting in the possibility of individuals with narrower and wider\nranges. Given the updating algorithm, influence may also be\nasymmetric: individuals with a narrower range of tolerance, which\nDeffuant et al. interpret as higher confidence or lower uncertainty,\nwill be more influential on individuals with a wider range than vice\nversa. The influence on polarization of “stubborn”\nindividuals who do not change, and of agents on extremes, has also\nbeen studied, showing a clear impact on the dynamics of belief change\nin the\n group.[5] \nEric Olsson and Sofi Angere have developed a sophisticated program in\nwhich the interaction of agents is modelled within a Bayesian network\nof both information and trust (Olsson 2011). Their program,\n Laputa\n (see Other Internet Resources) has a wide range of applications, one\nof which is a model of polarization interpreted in terms of the\nPersuasive Argument Theory in psychology and which replicates an\neffect seen in empirical studies: the increasing divergence of\npolarized groups (Lord, Ross, & Lepper 1979; Isenberg 1986; Olsson\n2013). Olsson raises the question of whether polarization may be\nepistemically rational, offering a positive answer. O’Connor and\nWeatherall (2018) and Singer et al. (2019) also argue that polarization\ncan be rational, using different models and perhaps different senses\nof polarization (Bramson et al. 2017). \nThe topic of polarization is anticipated in an earlier tradition of\ncellular automata models initiated by Robert Axelrod. The basic\npremise of Axelrod (1997) is that people tend to interact more with\nthose like themselves and tend to become more like those with whom\nthey interact. But if people come to share one another’s beliefs\n(or other cultural features) over time, why do we not observe complete\ncultural convergence? At the core of Axelrod’s model is a\nspatially instantiated imitative mechanism that produces cultural\nconvergence within local groups but also results in progressive\ndifferentiation and cultural isolation between groups. \n100 agents are arranged on a \\(10 \\times 10\\) lattice such as that\nillustrated in\n Figure 3.\n Each agent is connected to four others: top, bottom, left, and right.\nThe exceptions are those at the edges or corners of the array,\nconnected to only three and two neighbors, respectively. Agents in the\nmodel have multiple cultural “features”, each of which\ncarries one of multiple possible “traits”. One can think\nof the features as categorical variables and the traits as options or\nvalues within each category. For example, the first feature might\nrepresent culinary tradition, the second one the style of dress, the\nthird music, and so on. In the base configuration an agent’s\n“culture” is defined by five features \\((F = 5)\\) each\nhaving one of 10 traits \\((q =10),\\) numbered 0 through 9. Agent\nx might have \\(\\langle 8, 7, 2, 5, 4\\rangle\\) as a cultural\nsignature while agent y is characterized \\(\\langle 1, 4, 4, 8,\n4\\rangle\\). Agents are fixed in their lattice location and hence their\ninteraction partners. Agent interaction and imitation rates are\ndetermined by neighbor similarity, where similarity is measured as the\npercentage of feature positions that carry identical traits. With five\nfeatures, if a pair of agents share exactly one such element they are\n20% similar; if two elements match then they are 40% similar, and so\nforth. In the example just given, agents x and y and\nhave a similarity of 20% because they share only one feature. \nFigure 3: Typical initial set of\n“cultures” for a basic Axelrod-style model consisting of\n100 agents on a \\(10 \\times 10\\) lattice with five features and 10\npossible traits per agent. The marked sight shares two of five traits\nwith the site above it, giving it a cultural similarity score of 40%\n(Axelrod 1997). \nFor each iteration, the model picks at random an agent to be active\nand one of its neighbors. With probability equal to their cultural\nsimilarity, the two sites interact and the active agent changes one of\nits dissimilar elements to that of its neighbor. If agent \\(i =\n\\langle 8, 7, 2, 5, 4\\rangle\\) is chosen to be active and it is paired\nwith its neighbor agent \\(j = \\langle 8, 4, 9, 5, 1\\rangle,\\) for\nexample, the two will interact with a 40% probability because they\nhave two elements in common. If the interaction does happen, agent\ni changes one of its mismatched elements to match that of\nj, becoming perhaps \\(\\langle 8, 7, 2, 5, 1\\rangle.\\) This\nchange creates a similarity score of 60%, yielding an increased\nprobability of future interaction between the two. \nIn the course of approximately 80,000 iterations, Axelrod’s\nmodel produces large areas in which cultural features are identical:\nlocal convergence. It is also true, however, that arrays such as that\nillustrated do not typically move to full convergence. They instead\ntend to produce a small number of culturally isolated stable\nregions—groups of identical agents none of whom share features\nin common with adjacent groups and so cannot further interact. As an\narray develops, agents interact with increasing frequency with those\nwith whom they become increasingly similar, interacting less\nfrequently with the dissimilar agents. With only a mechanism of local\nconvergence, small pockets of similar agents emerge that move toward\ntheir own homogeneity and away from that of other groups. With the\nparameters described above, Axelrod reports a median of three stable\nregions at equilibrium. It is this phenomenon of global separation\nthat Axelrod refers to as “polarization”. See the\n interactive simulation of the Axelrod polarization model\n in the Other Internet Resources section below. \nAxelrod notes a number of intriguing results from the model, many of\nwhich have been further explored in later work. Results are very\nsensitive to the number of features F and traits q used\nas parameters, for example. Changing numbers of features and traits\nchanges the final number of stable regions in opposite directions: the\nnumber of stable regions correlates negatively with the number of\nfeatures F but positively with the number of traits q\n(Klemm et al. 2003). In Axelrod’s base case with \\(F = 5\\) and\n\\(q = 10\\) on a \\(10 \\times 10\\) lattice, the result is a median of\nthree stable regions. When q is increased from 10 to 15, the\nnumber of final regions increases from three to 20; increasing the\nnumber of traits increases the number of stable groups dramatically.\nIf the number of features F is increased to 15, in contrast,\nthe average number of stable regions drops to only 1.2 (Axelrod 1997).\nFurther explorations of parameters of population size, configuration,\nand dynamics, with measures of relative size of resultant groups,\nappear in Klemm et al. (2003a, b, c, 2005) and in Centola et al. (2007). \nOne result that computational modeling promises regarding a phenomenon\nsuch as opinion polarization is an understanding of the phenomenon\nitself: how real opinion polarization might happen, and how it might\nbe avoided. Another and very different outcome, however, is created by\nthe fact that computational modeling both offers and demands precision\nabout concepts and measures that may otherwise be lacking in theory.\nBramson et al. (2017), for example, argues that\n“polarization” has a range of possible meanings across the\nliterature in which it appears, different aspects of which are\ncaptured by different computational models with different\nmeasures. \nIn general, the social dynamics of belief change reviewed above treats\nbeliefs as items that spread by contact, much on the model of\ninfection dynamics (Grim, Singer, Reade, & Fisher 2015, though\nRiegler & Douven 2009\ncan be seen as an exception). Other attempts have been made to model\nbelief change in greater detail, motivated by reasons or\narguments. \nWith gestures toward earlier work by Phan Minh Dung (1995), Gregor Betz\nconstructs a model of belief change based on “dialectical\nstructures” of linked arguments (Betz 2013). Sentences and their\nnegations are represented as digits positive and negative, arguments\nas ordered sets of sentences, and two forms of links between\narguments: an attack relation in which a conclusion of one argument\ncontradicts a premise of another and support relations in which the\nconclusion of one argument is equivalent to the premise of another\n (Figure 4).\n A “position” on a dynamical structure, complete or\npartial, consists of an assignment of truth values T or F to the\nelements of the set of sentences involved. Consistent positions\nrelative to a structure are those in which contradictory sentences are\nsigned opposite truth values and every argument in which all premises\nare assigned T has a conclusion which is assigned T as well. Betz then\nmaps the space of coherent positions for a given dialectical structure\nas an undirected network, with links between positions that differ in\nthe truth-value of just one sentence of the set. \n\n \nFigure 4: A dialectical structure of\npropositions and their negations as positive and negative numbers,\nwith two complete positions indicated by values of T and F. The left\nassignment is consistent; the right assignment is not (after Betz\n2013). [An\n extended description of figure 4\n is in the supplement.] \nIn the simplest form of the model, two agents start with random\nassignments to a set of 20 sentences with consistent assignments to\ntheir negations. Arguments are added randomly, starting from a blank\nslate, and agents move to the coherent position closest to their\nprevious position, with a random choice in the case of a draw. In\nvariations on the basic structure, Betz considers (a) cases in which\nan initial background agreement is assumed, (b) cases of\n“controversial” argumentation, in which arguments are\nintroduced which support a proponent’s position or attack an\nopponent’s, and (c) in which up to six agents are involved. In\ntwo series of simulations, he tracks both the consensus-conduciveness\nof different parameters, and—with an assumption of a specific\nassignment as the “truth”—the truth-conduciveness of\ndifferent parameters. \nIn individual runs, depending on initial positions and arguments\nintroduced, Betz finds that argumentation of the sort modeled can\neither increase or decrease agreement, and can track the truth or lead\nastray. Averaging across many debates, however, Betz finds that\ncontroversial argumentation in particular is both consensus-conducive\nand better tracks the\n truth.[6] \nComputational models have been used in philosophy of science in two\nvery different respects: (a) as models of scientific theory, and (b)\nas models of the social interaction characteristic of collective\nscientific research. The next sections review some examples of\neach. \n“Computational philosophy of science” is enshrined as a\nbook title as early as Paul Thagard’s 1988. A central core of\nhis work is a connectionist ECHO program, which constructs network\nstructures of scientific explanation (Thagard 1992, 2012). From inputs\nof “explain”,, “contradict”,\n“data”, and “analogous” for the status and\nrelation of nodes, ECHO uses a set of principles of explanatory\ncoherence to construct a network of undirected excitatory and\ninhibitory links between nodes which “cohere” and those\nwhich “incohere”, respectively. If p1 through pm explain\nq, for example, all of p1 through pm cohere with q and\nwith each other, for example, though the weight of coherence is\ndivided by the number of p1 through pm. If p1 contradicts p2 or p1 and\np2 are parts of competing explanations for the same phenomenon, they\n“incohere”. \nStarting with initial node activations close to zero, the nodes of the\ncoherence network are synchronously updated in terms of their old\nactivation and weighted input from linked nodes, with\n“data” nodes set as a constant input of 1. Once the\nnetwork settles down to equilibrium, an explanatory hypothesis p1 is\ntaken to defeat another p2 if its activation value is higher—at\nleast generally, positive as opposed to negative\n (Figure 5). \nFigure 5: An ECHO network for hypotheses\nP1 and P2 and evidence units Q1 and Q2. Solid lines represent\nexcitatory links, the dotted line an inhibitory link. Because Q1 and\nQ2 are evidence nodes, they take a constant excitatory value of 1 from\nE. Started from values of .01 and following Thagard’s updating,\nP1 dominates P2 once the network has settled down: a hypothesis that\nexplains more dominates its alternative. Adapted from Thagard\n1992. \nThagard is able to show that such an algorithm effectively echoes a\nrange of familiar observations regarding theory selection. Hypotheses\nthat explain more defeat those that explain less, for example, and\nsimpler hypotheses are to be preferred. In contrast to simple\nPopperian refutation, ECHO abandons a hypothesis only when a\ndominating hypothesis is available. Thagard uses the basic approach of\nexplanatory coherence, instantiated in ECHO, in an analysis of a\nnumber of historical cases in the history of science, including the\nabandonment of phlogiston theory in favor of oxygen theory, the\nDarwinian revolution, and the eventual triumph of Wegener’s\nplate tectonics and continental drift. \nThe influence of Bayesian networks has been far more widespread, both\nacross disciplines and in technological application—application\nmade possible only with computers. Grounded in the work of Judea Pearl\n(1988, 2000; Pearl & Mackenzie 2018), Bayesian networks are\ndirected acyclic graphs in which nodes represent variables that can be\nread as either probabilities or degrees of belief and directed edges\nas conditional probabilities from “parent” to\n“child”. By the Markov convention, the value of a node is\nindependent of all other nodes that are not its descendants,\nconditional on its parents. A standard textbook example is shown in\n Figure 6. \nFigure 6: A standard example of a simple\nBayesian net. [An\n extended description of figure 6\n is in the supplement.] \nChanges of values at the nodes of a Bayesian network (in response to\nevidence, for example) are updated through belief propagation\nalgorithms applied at every node. The update of a response to input\nfrom a parent uses the conditional probabilities of the link. A\nparent’s response to input from a child uses the related\nlikelihood ratio (see also the supplement on Bayesian networks in\nBringsjord & Govindarajulu 2018 [2019]). Reading some variables as\nhypotheses and others as pieces of evidence, simple instances of core\nscientific concepts can easily be read off such a structure. Simple\nexplanation amounts to showing how the value of a variable\n“downstream” depends on the pattern\n“upstream”. Simple confirmation amounts to an increase in\nthe probability or degree of belief of a node h upstream\ngiven a piece of evidence e downstream. Evaluating competing\nhypotheses consists in calculating the comparative probability of\ndifferent patterns upstream. One clear reading of networks is as\ncausal graphs. \nAs Pearl notes, a Bayesian network is nothing more than a graphical\nrepresentation of a huge table of joint probabilities for the\nvariables involved (Pearl & Mackenzie 2018: 129). Given any\nsizable number of variables, however, calculation becomes humanly\nunmanageable—hence the crucial use of computers. The fact that\nBayesian networks are so computationally intensive is in fact a point\nthat Thagard makes against using them as models of human cognitive\nprocessing (Thagard 1992: 201). But that is not an objection against\nother philosophical interpretations. Application to philosophical\nquestions of causality in philosophy of science is detailed in\nSpirtes, Glymour, and Scheines (1993) and Sprenger and Hartmann\n(2019). Bayesian networks are now something of a standard in\nartificial intelligence, ubiquitous in its applications, and powerful\nalgorithms have been developed to extract causal networks from the\nmassive amounts of data available. \nIt should be no surprise that the computational studies of belief\nchange and opinion dynamics noted above blend smoothly into a range of\ncomputational studies in philosophy of science. Here a central\nmotivating question has been one of optimal investigatory structure:\nwhat pattern of scientific communication and cooperation, between what\nkinds of investigators, is best positioned to advance science? There\nare two strands of computational philosophy of science that attempt to\nwork toward an answer to this question. The first strand models the\neffect of communicative networks within groups. The second strand,\nleft to the next section, models the effects of cognitive diversity\nwithin groups. This section outlines what makes modeling of both sorts\npromising, but also notes limitations and some failures as well. \nOne might think that access to more data by more investigators would\ninevitably optimize the truth-seeking goals of communities of\ninvestigators. On that intuition, faster and more complete\ncommunication—the contemporary science of the\ninternet—would allow faster, more accurate, and more exploration\nof nature. Surprisingly, however, this first strand of modeling offers\nrobust arguments for the potential benefits of limited\ncommunication. \nIn the spirit of rational choice theory, much of this work was\ninspired by analytical work in economics on infinite populations by\nVenkatesh Bala and Sanjeev Goyal (1998), computationally implemented\nfor small populations in a finite context and with an eye to\nphilosophical implications by Kevin Zollman (2007, 2010a, 2010b). In Zollman’s model, Bayesian agents choose between a\ncurrent method \\(\\phi_1\\) and what is set as a better method\n\\(\\phi_2,\\) starting with random beliefs and allowing agents to pursue\nthe investigatory action with the highest subjective utility. Agents\nupdate their beliefs based on the results of their own testing\nresults—drawn from a distribution for that action—together\nwith results from the other agents to which they are communicatively\nconnected. A community is taken to have successfully learned when all\nagents converge on the better \\(\\phi_2.\\) \nZollman’s results are shown in\n Figure 7\n for the three simple networks shown in\n Figure 8.\n The communication network which performs the best is not the fully\nconnected network in which all investigators have access to all\nresults from all others, but the maximally distributed network\nrepresented by the ring. As Zollman also shows, this is also that\nconfiguration which takes the longest time to achieve convergence. See\n an interactive simulation of a simplified version of Zollman’s model\n in the Other Internet Resources section below. Figure 7: A 10 person ring, wheel, and\ncomplete graph. After Zollman (2010a). \nFigure 8: Learning results of computer\nsimulations: ring, wheel, and complete networks of Bayesian agents.\nAdapted from Zollman (2010a).\n [An\n extended description of figure 8\n is in the supplement.] \nOlsson and Angere’s Bayesian network Laputa (mentioned above)\nhas also been applied to the question of optimal networks for\nscientific communication. Their results essentially confirm\nZollman’s result, though sampled over a larger range of networks\n(Angere & Olsson 2017). Distributed networks with low connectivity\nare those that most reliably fix on the truth, though they are bound\nto do so more slowly. \nThe concept of an epistemic landscape has also emerged as of\ncentral importance in this strand of research. Analogous to a fitness\nlandscape in biology (Wright 1932), an epistemic landscape offers an\nabstract representation of ideal data that might in principle be\nobtained in testing a range of hypotheses (Grim 2006, 2009; Weisberg\n& Muldoon 2009; Hong & Page 2004, Page 2007).\n Figure 9\n uses the example of data that might be obtained by testing\nalternative medical treatments. In such a graph points in the\nchemotherapy-radiation plane represent particular hypotheses about the\nmost effective combination of radiation and chemotherapy. Graph height\nat each location represents some measure of success: the percentage of\npatients with 5-years survival on that treatment, for example. \nFigure 9: A three-dimensional epistemic\nlandscape. Points on the xz plane represent hypotheses regarding\noptimal combination of radiation and chemotherapy; graph height on the\ny axis represents some measure of success. [An\n extended description of figure 9\n is in the supplement.] \nAn epistemic landscape is intended to be an abstract representation of\nthe real-world phenomenon being explored. The key word, of course, is\n“abstract”: few would argue that such a model is fully\nrealistic either in terms of the simplicity of limited dimensions or\nthe precision in which one hypothesis has a distinctly higher value\nthan a close neighbor. As in all modeling, the goal is to represent as\nsimply as possible those aspects of a situation relevant to answering\na specific: in this case, the question of optimal scientific\norganization. Epistemic landscapes—even those this\nsimple—have been assumed to offer a promising start. As outlined\nbelow, however, one of the deeper conclusions that has emerged is how\nsensitive results can be to the specific topography of the epistemic\nlandscape. \nIs there a form of scientific communication which optimizes its\ntruth-seeking goals in exploration of a landscape? In a series of\nagent-based models, agents are communicatively linked explorers\nsituated at specific points on an epistemic landscape (Grim 2006;\nGrim, Singer et al. 2013). In such a design, simulation can be used to\nexplore the effect of network structure, the topography of the\nepistemic landscape, and the interaction of the two. \nThe simplest form of the results echo the pattern seen in different\nforms in Bala and Goyal (1998) and in Zollman\n(2010a, 2010b), here played out on epistemic landscapes. Agents start with\nrandom hypotheses as points on the x-axis of a two-dimensional\nlandscape. They compare their results (the height of the y axis at\nthat point) with those of the other agents to which they are\nnetworked. If a networked neighbor has a higher result, the agent\nmoves toward an approximation of that point (in the interval of a\n“shaking hand”) with an inertia factor (generally 50%, or\na move halfway). The process is repeated by all agents, progressively\nexploring the landscape in attempting to move toward more successful\nresults. \nOn “smooth” landscapes of the form of the first two graphs\nin\n Figure 10,\n agents in any of the networks shown in Figure 10 succeed in finding\nthe highest point on the landscape. Results become much more\ninteresting for epistemic landscapes that contain a “needle in a\nhaystack” as in the third graph in Figure 10. \nFigure 10: Two-dimensional epistemic\nlandscapes. Adapted from Grim (2009). ring radius 1 small world wheel hub random complete Figure 11: Sample networks. \nIn a ring with radius 1, each agent is connected with just its\nimmediate neighbors on each side. Using an inertia of 50% and a\n“shaking hand” interval of 8 on a 100-point landscape, 50\nagents in that configuration converge on the global maximum in the\n“needle in the haystack” landscape in 66% of simulation\nruns. If agents are connected to the two closest neighbors on each\nside, results drop immediately to 50% of runs in which agents find the\nglobal maximum. A small world network can be envisaged as a ring in\nwhich agents have a certain probability of “rewiring”:\nbreaking an existing link and establishing another one to some other\nagent at random (Watts & Strogatz 1998). If each of 50 agents has\na 9% probability of rewiring, the success rate of small worlds drops\nto 55%. Wheels and hubs have a 42% and 37% success rate, respectively.\nRandom networks with a 10% probability of connection between any two\nnodes score at 47%. The worst performing communication network on a\n“needle in a haystack” landscape is the “internet of\nscience” of a complete network in which everyone instantly sees\neveryone else’s result. \nExtensions of these results appear in Grim, Singer et al. (2013).\nThere a small sample of landscapes is replaced with a quantified\n“fiendishness index”, roughly representing the extent to\nwhich a landscape embodies a “needle in a haystack”.\nHigher fiendishness quantifies a lower probability that hill-climbing\nfrom a randomly chosen point “finds” the landscape’s\nglobal maximum. Landscapes, though still two-dimensional, are\n“looped” so as to avoid edge-effects also noted in\nHegselmann and Krause (2006). Here again results emphasize the\nepistemic advantages of ring-like or distributed network over fully\nconnected networks in the exploration of intuitively difficult\nepistemic landscapes. Distributed single rings achieve the highest\npercentage of cases in which the highest point on the landscape is\nfound, followed by all other network configurations. Total or\ncompletely connected networks show the worst results over all. Times\nto convergence are shown to be roughly though not precisely the\ninverse of these relationships. See\n the interactive simulation of a Grim and Singer et al.’s model\n in the Other Internet Resources section below. \nWhat all these models suggest is that it is distributed networks of\ncommunication between investigators, rather than full and immediate\ncommunication between all, that will—or at least\ncan—give us more accurate scientific outcomes. In the\nseventeenth century, scientific results were exchanged slowly, from\nperson to person, in the form of individual correspondence. In\ntoday’s science results are instantly available to everyone.\nWhat these models suggest is that the communication mechanisms of\nseventeenth century science may be more reliable than the highly\nconnected communications of today. Zollman draws the corollary\nconclusion that loosely connected communities made up of less informed\nscientists might be more reliable in seeking the truth than\ncommunities of more informed scientists that are better connected\n (Zollman 2010b). \nThe explanation is not far to seek. In all the models noted, more\nconnected networks produce inferior results because agents move too\nquickly to salient but sub-optimal positions: to local rather than\nglobal maxima. In the landscape models surveyed, connected networks\nresult in all investigators moving toward the same point, currently\nannounced to everyone as highest, skipping over large areas in the\nprocess—precisely where the “needle in the haystack”\nmight be hidden. In more distributed networks, local action results in\na far more even and effective exploration of widespread areas of the\nlandscape; exploration rather than exploitation (Holland 1975). \nHow should we structure the funding and communication structure of our\nscientific communities? It is clear both from these results in their\ncurrent form, and in further work along these general lines, that the\nanswer may well be “landscape”-relative: it may well\ndepend on what kind of question is at issue what form scientific\ncommunication ought to take. It may also depend on what desiderata are\nat issue. The models surveyed emphasize accuracy of results,\nabstractly modeled. All those surveyed concede that there is a clear\ntrade-off between accuracy of results and the speed of community\nconsensus\n (Zollman 2007;\n Zollman 2010b; Grim, Singer et al. 2013). But for many purposes, and\nreasons both ethical and practical, it may often be far better to work\nwith a result that is only roughly accurate but available today than\nto wait 10 years for a result that is many times more accurate but\narrives far too late. \nA second tradition of work in computational philosophy of science also\nuses epistemic landscapes, but attempts to model the effect not of\nnetwork structure but of the division of labor and diversity within\nscientific groups. An influential but ultimately flawed precursor in\nthis tradition is the work of Weisberg and Muldoon (2009). \nTwo views of Weisberg and Muldoon’s landscape appear in\n Figure 12.\n In their treatment, points on the base plane of the landscape\nrepresent “approaches”—abstract representations of\nthe background theories, methods, instruments and techniques used to\ninvestigate a particular research question. Heights at those points\nare taken to represent scientific significance (following Kitcher\n1993). \nFigure 12: Two visions of Weisberg and\nMuldoon’s landscape of scientific significance (height) at\ndifferent approaches to a research topic. \nThe agents that traverse this landscape are not networked, as in the\nearlier studies noted, except to the extent that they are influenced\nby agents with “approaches” near theirs on the landscape.\nWhat is significant about the Weisberg & Muldoon model, however,\nis that their agents are not homogeneous. Two types of agents play a\nprimary role. \n“Followers” take previous investigation of the territory\nby others into account in order to follow successful trends. If any\npreviously investigated points in their immediate neighborhood have a\nhigher significance than the point they stand on, they move to that\npoint (randomly breaking any\n tie).[7]\n Only if no neighboring investigated points have higher significance\nand uninvestigated point remain, followers move to one of those. \n“Mavericks” avoid previously investigated points much as\nfollowers prioritize them. Mavericks choose unexplored points\nin their neighborhoods, testing significance. If higher than their\ncurrent spot, they move to that point. \nWeisberg and Muldoon measure both the percentages of runs in which\ngroups of agents find the highest peak and the speed at which peaks\nare found. They report that the epistemic success of a population of\nfollowers is increased when mavericks are included, and that the\nexplanation for that effect lies in the fact that mavericks can\nprovide pathways for followers: “[m]avericks help many of the\nfollowers to get unstuck, and to explore more fruitful areas of the\nepistemic landscape” (for details see Weisberg & Muldoon\n2009: 247 ff). Against that background they argue for broad claims\nregarding the value for an epistemic community of combining different\nresearch strategies. The optimal division of labor that their model\nsuggests is “a healthy number of followers with a small number\nof mavericks”. \nCritics of Weisberg and Muldoon’s model argue that it is flawed by simple implementation errors in which >= was used in place of >, with the result that their software agents do not in fact operate in accord with their outlined strategies (Alexander, Himmelreich,& Thomson 2015). As\nimplemented, their followers tend to get trapped into oscillating\nbetween two equivalent spaces (often of value 0). According to the critics, when followers are properly implemented, it turns out that mavericks help the success of\na community solely in terms of discovery by the mavericks themselves,\nnot by getting followers “unstuck” who shouldn’t\nhave been stuck in the first place (see also Thoma 2015). If the critics are right, the Weisberg-Muldoon model as originally implemented proves inadequate as philosophical support for the claim that division of labor and strategic diversity are important epistemic drivers. There’s\n an interactive simulation of the Weisberg and Muldoon model, which includes a switch to change the >= to >,\n in the Other Internet Resources section below. \nCritics of the model don’t deny the general conclusion that\nWeisberg and Muldoon draw: that cognitive diversity or division of\ncognitive labor can favor social epistemic\n outcomes.[8]\n What they deny is that the Weisberg and Muldoon model adequately\nsupports that conclusion. A particularly intriguing model that does\nsupport that conclusion, built on a very different model of diversity,\nis that of Hong and Page (2004). But it also supports a point that\nAlexander et al. emphasize: that the advantages of cognitive\ndiversity can very much depend on the epistemic landscape being\nexplored. \nLu Hong and Scott Page work with a two-dimensional landscape of 2000\npoints, wrapped around as a loop. Each point is assigned a random\nvalue between 1 and 100. Their epistemic individuals explore that\nlandscape using heuristics composed of three ordered numbers between,\nsay, 1 and 12. An example helps. Consider an individual with heuristic\n\\(\\langle 2, 4, 7\\rangle\\) at point 112 on the landscape. He first\nuses his heuristic 2 to see if a point two to the right—at\n114—has a higher value than his current position. If so, he\nmoves to that point. If not, he stays put. From that point, whichever\nit is, he uses his heuristic 4 in order to see if a point 4 steps to\nthe right has a higher peak, and so forth. An agent circles through\nhis heuristic numbers repeatedly until he reaches a point from which\nnone within reach of his heuristic offers a higher value. The basic\ndynamic is illustrated in\n Figure 13. \nFigure 13: An example of exploration of\na landscape by an individual using heuristics as in Hong and Page\n(2004). Explored points can be read left to right. [An\n extended description of figure 13\n is in the supplement.] \nHong and Page score individuals on a given landscape in terms of the\naverage height they reach starting from each of the 2000 points. But\ntheir real target is the value of diversity in groups. With that in\nmind, they compare the performance of (a) groups composed of the 9\nindividuals with highest-scoring heuristics on a given landscape with\n(b) groups composed of 9 individuals with random heuristics on that\nlandscape. In each case groups function together in what has been\ntermed a “relay”. For each point on the 2000-point\nlandscape, the first individual of the group finds his highest\nreachable value. The next individual of the group starts from there,\nand so forth, circling through the individuals until a point is\nreached at which none can achieve a higher value. The score for the\ngroup as a whole is the average of values achieved in such a way\nacross all of the 2000 points \nWhat Hong and Page demonstrate in simulation is that groups with\nrandom heuristics routinely outperform groups composed entirely of the\n“best” individual performers. They christen their findings\nthe “Diversity Trumps Ability” result. In a replication of\ntheir study, the average maximum on the 2000-point terrain for the\ngroup of the 9 best individuals comes in at 92.53, with a median of\n92.67. The average for a group of 9 random individuals comes in at\n94.82, with a median of 94.83. Across 1000 runs in that replication, a\nhigher score was achieved by groups of random agents in 97.6% of all\ncases (Grim et al. 2019). See\n an interactive simulation of Hong and Page’s group deliberation model\n in the Other Internet Resources section below. Hong and Page also\noffer a mathematical theorem as a partial explanation of such a result\n(Hong & Page 2004). That component of their work has been attacked\nas trivial or irrelevant (Thompson 2014), though the attack itself has\ncome under criticism as well (Kuehn 2017, Singer 2019). \nThe Hong-Page model solidly demonstrates a general claim attempted in\nthe disputed Weisberg-Muldoon model: cognitive diversity can indeed be a\nsocial epistemic advantage. In application, however, the Hong-Page\nresult has sometimes been appealed to as support for much broader\nclaims: that diversity is always or quite generally of epistemic\nadvantage (Anderson 2006, Landemore 2013, Gunn 2014, Weymark 2015).\nThe result itself is limited in ways that have not always been\nacknowledged. In particular, it proves sensitive to the precise\ncharacter of the epistemic landscape employed. \nHong and Page’s landscape is one in which each of 2000 points is\ngiven a random value between 1 and 100: a purely random landscape. One\nconsequence of that fact is that the group of 9 best heuristics on\ndifferent random Hong-Page landscapes have essentially no correlation:\na high-performing individual on one landscape need have no carry-over\nto another. Grim et al. (2019) expands the Hong-Page model to\nincorporate other landscapes as well, in ways which challenge the\ngeneral conclusions regarding diversity that have been drawn from the\nmodel but which also suggest the potential for further interesting\napplications. \nAn easy way to “smooth” the Hong-Page landscapes is to\nassign random values not to every point on the 2000-point loop but\nevery second point, for example, with intermediate points taking an\naverage between those on each side. Where a random landscape has a\n“smoothness” factor of 0, this variation will have a\nrandomness factor of 1. A still “smoother” landscape of\ndegree 2 would be one in which slopes are drawn between random values\nassigned to every third point. Each degree of smoothness increases the\naverage value correlation between a point and its neighbors. Grim et\nal. consider landscapes of varying “smoothness” along\nroughly these lines, though with a randomization that avoids the\nlock-step intervals suggested (Grim et al. 2019). \nUsing Hong and Page’s parameters in other respects, it turns out\nthat the “Diversity Trumps Ability” result holds only for\nlandscapes with a smoothness factor less than 4. Beyond that point, it\nis “ability”—the performance of groups of the 9\nbest-performing individuals—that trumps\n“diversity”—the performance of groups of random\nheuristics. \nThe Hong-Page result is therefore very sensitive to the\n“smoothness” of the epistemic landscape modeled. As hinted\nin\n section 3.2.2,\n this is an indication from within the modeling tradition itself of\nthe danger of restricted and over-simple abstractions regarding\nepistemic landscapes. Moreover, the model’s sensitivity is not\nlimited to landscape smoothness: social epistemic success depends on\nthe pool of numbers from which heuristics are drawn as well, with\n“diversity” showing strength on smoother landscapes if the\npool of heuristics is expanded. Results also depend on whether social\ninteraction is modeled using of Hong-Page’s “relay”\nor an alternative dynamics in which individuals collectively (rather\nthan sequentially) announce their results, with all moving to the\nhighest point announced by any. Different landscape smoothnesses,\ndifferent heuristic pool sizes, and different interactive dynamics\nwill favor the epistemic advantages of different compositions of\ngroups, with different proportions of random and best-performing\nindividuals (Grim et al. 2019). \nWhat, then, is the conduct that ought to be adopted, the reasonable\ncourse of conduct, for this egoistic, naturally unsocial being, living\nside by side with similar beings? —Henry\nSidgwick, Outlines of the History\nof Ethics (1886: 162) \nHobbes’ Leviathan can be read as asking, with Sidgwick,\nhow cooperation can emerge in a society of egoists (Hobbes 1651).\nCooperation is thus a central theme in both ethics and\nsocial-political philosophy. \nGame theory has been a major tool in many of the philosophical\nconsiderations of cooperation, extended with computational\nmethodologies. Here the primary example is the Prisoner’s\nDilemma, a strategic interaction between two agents with a payoff\nmatrix in which joint cooperation gets a higher payoff than joint\ndefection, but the highest payoff goes to a player who defects when\nthe other player cooperates (see esp. Kuhn 1997 [2019]). Formally, the\nPrisoner’s Dilemma requires the value DC for defection against\ncooperation to be higher than CC for joint cooperation, with CC higher\nthan the payoff CD for cooperation against defection. In order to\navoid an advantage to alternating trade-offs, CC should also be higher\nthan \\((\\textrm{CD} + \\textrm{DC}) / 2.\\) A simple set of values that\nfits those requirements is shown in the matrix in\n Figure 14. \nFigure 14: A Prisoner’s Dilemma\npayoff matrix \nIt is clear in the “one-shot” Prisoner’s Dilemma\nthat defection is strictly dominant: whether the other player\ncooperates or defects, one gains more points by defecting. But if\ndefection always gives a higher payoff, what sense does it make to\ncooperate? In a Hobbesian population of egoists, with payoffs as in\nthe Prisoner’s Dilemma, it would seem that we should expect\nmutual defection as both a matter of course and the rational\noutcome—Hobbes’ “war of all against all”. How\ncould a population of egoists come to cooperate? How could the ethical\ndesideratum of cooperation arise and persist? \nA number of mechanisms have been shown to support the emergence of\ncooperation: kin selection (Fisher 1930; Haldane 1932), green beards\n(Hamilton 1964a,b; Dawkins 1976), secret handshakes (Robson 1990;\nWiseman & Yilankaya 2001), iterated games, spatialized and\nstructured interactions (Grim 1995; Skyrms 1996, 2004; Grim, Mar, & St. Denis\n1998; Alexander 2007), and noisy signals (Nowak & Sigmund\n1992). This section offers examples of the last two of these. \nIn the iterated Prisoner’s Dilemma, players repeat their\ninteractions, either in a fixed number of rounds or in an infinite or\nindefinite repetition. Robert Axelrod’s tournaments in the early\n1980s are the classic studies in the iterated prisoner’s\ndilemma, and early examples of the application of computational\ntechniques. Strategies for playing the Prisoner’s Dilemma were\nsolicited from experts in various fields, pitted against all others\n(and themselves) in round-robin competition over 200 rounds. Famously,\nthe strategy that triumphed was Tit for Tat, a simple strategy which\nresponds to cooperation from the other player on the previous round\nwith cooperation, responding to defection on the previous round with\ndefection. Even more surprisingly, Tit for Tat again came out in front\nin a second tournament, despite the fact that submitted strategies\nknew that Tit for Tat was the opponent to aim for. When those same\nstrategies were explored with replicator dynamics in place of\nround-robin competition, Tit for Tat again was the winner (Axelrod and\nHamilton 1981). Further work has tempered Tit for Tat’s\nreputation somewhat, emphasizing the constraints of Axelrod’s\ntournaments both in terms of structure and the strategies submitted\n(Kendall, Yao, & Chang 2007; Kuhn 1997 [2019]). \nA simple set of eight “reactive” strategies, in which a\nplayer acts solely on the basis of the opponent’s previous move,\nis shown in\n Figure 15.\n Coded with “1” for cooperate and “0” for\ndefect and three places representing first move i, response\nto cooperation on the other side c, and response to defection\non the other side d, these give us 8 strategies that include\nall defect, all cooperate, tit for tat as well as several other\nvariations.  \nFigure 15: 8 reactive strategies in the\nPrisoner’s Dilemma \nIf these strategies are played against each other and themselves, in\nthe manner of Axelrod’s tournaments, it is “all\ndefect” that is the clear winner. If agents imitate the most\nsuccessful strategy, a population will thus immediately go to All\nDefect—a game-theoretic image of Hobbes’ war of all\nagainst all, perhaps. \nConsider, however, a spatialized Prisoner’s Dilemma in the form\nof cellular automata, easily run and analyzed on a computer. Cells are\nassigned one of these eight strategies at random, play an iterated\ngame locally with their eight immediate neighbors in the array, and\nthen adopt the strategy of that neighbor (if any) that achieves a\nhigher total score. In this case, with the same 8 strategies,\noccupation of the array starts with a dominance by All Defect, but\nclusters of Tit for Tat grow to dominate the space\n (Figure 16).\n An interactive simulation in which one can choose which competing reactive strategies play in a spatialized array is available in the Other Internet Resources section\nbelow. \nFigure 16: Conquest by Tit for Tat in\nthe Spatialized Prisoner’s Dilemma. All defect is shown in\ngreen, Tit for Tat in gray (Grim, Mar, & St. Denis 1998) \nIn this case, there are two aspects to the emergence of cooperation in\nthe form of Tit for Tat. One is the fact that play is local:\nstrategies total points over just local interactions, rather than play\nwith all other cells. The other is that imitation is local as well:\nstrategies imitate their most successful neighbor, rather than that\nstrategy in the array that gained the most points. The fact that both\nconditions play out in the local structure of the lattice allows\nclusters of Tit for Tat to form and grow. In Axelrod’s\ntournaments it is particularly important that Tit for Tat does well in\nplay against itself; the same is true here. If either game interaction\nor strategy updating is made global rather than local, dominance goes\nto All Defect instead. One way in which cooperation can emerge, then,\nis through structured interactions (Grim 1995; Skyrms 1996, 2004; Grim, Mar, & St. Denis\n1998). Alexander (2007) offers a particularly thorough\ninvestigation of different interaction structures and different\ngames. \nMartin Nowak and Karl Sigmund offer a further variation that results\nin an even more surprising level of cooperation in the\nPrisoner’s Dilemma (Nowak & Sigmund 1992). The reactive\nstrategies outlined above are communicatively perfect strategies.\nThere is no noise in “hearing” a move as cooperation or\ndefection on the other side, and no “shaking hand” in\nresponse. In Tit for Tat a cooperation on the other side is flawlessly\nperceived as such, for example, and is perfectly responded to with\ncooperation. If signals are noisy or responses are less than flawless,\nhowever, Tit for Tat loses its advantage in play against itself. In\nthat case a chancy defection will set up a chain of mutual defections\nuntil a chancy cooperation reverses the trend. A “noisy”\nTit for Tat played against itself in an infinite game does no better\nthan a random strategy. \nNowak and Sigmund replace the “perfect” strategies of\n Figure 14\n with uniformly stochastic ones, reflecting a world of noisy signals\nand actions. The closest to All Defect will now be a strategy .01,\n.01, .01, indicating a strategy that has only a 99% chance of\ndefecting initially and in response to either cooperation or\ndefection. The closest to Tit for Tat will be a strategy .99, .99,\n.01, indicating merely a high probability of starting with cooperation\nand responding to cooperation with cooperation, defection with\ndefection. Using the mathematical fiction of an infinite game, Nowak\nand Sigmund are able to ignore the initial value. \nPitting a full range of stochastic strategies of this type against\neach other in a computerized tournament, using replicator dynamics in\nthe manner of Axelrod and Hamilton (1981), Nowak and Sigmund trace a\nprogressive evolution of strategies. Computer simulation shows\nimperfect All Defect to be an early winner, followed by Imperfect Tit\nfor Tat. But at that point dominance in the population goes to a still\nmore cooperative strategy which cooperates with cooperation 99% of the\ntime but cooperates even against defection 10% of the time. That\nstrategy is eventually dominated by one that cooperates against\ndefection 20% of the time, and then by one that cooperates against\ndefection 30% of the time. A replication of the Nowak and Sigmund\nresult is shown in\n Figure 17.\n Nowak and Sigmund show analytically that the most successful strategy\nin a world of noisy information will be “Generous Tit for\nTat”, with probabilities of \\(1 - ε\\) and 1/3 for\ncooperation against cooperation and defection respectively. \nFigure 17: Evolution toward Nowak and\nSigmund’s “Generous Tit for Tat” in a world of\nimperfect information (Nowak & Sigmund 1992). Population\nproportions are shown vertically for labelled strategies shown over\n12,000 generations for an initial pool of 121 stochastic strategies\n\\(\\langle c,d\\rangle\\) at .1 intervals, full value of 0 and 1 replaced\nwith 0.01 and 0.99 (Grim, Mar, & St. Denis 1998). [An\n extended description of figure 17\n is in the supplement.] \nHow can cooperation emerge in a society of self-serving egoists? In\nthe game-theoretic context of the Prisoner’s Dilemma, these\nresults indicate that iterated interaction, spatialization and\nstructured interaction, and noisy information can all facilitate\ncooperation, at least in the form of strategies such as Tit for Tat.\nWhen all three effects are combined, the result appears to be a level\nof cooperation even greater than that indicated in Nowak and Sigmund.\nWithin a spatialized Prisoner’s Dilemma using stochastic\nstrategies, it is strategies in the region of probabilities \\(1 -\nε\\) and 2/3 that emerge as optimal in the sense of having the\nhighest scores in play against themselves without being open to\ninvasion from small clusters of other strategies (Grim 1996; Grim, Mar\n& St. Denis 1998). \nThis outline has focused on some basic background regarding the\nPrisoner’s Dilemma and emergence of cooperation. More recently a\ngeneration of richer game-theoretic models has appeared, using a wider\nvariety of games of conflict and coordination and more closely tied to\nhistorical precedents in social and political philosophy. Newer\ngame-theoretic analyses of state of nature scenarios in Hobbes appear\nin Vanderschraaf (2006) and Chung (2015), extended with simulation to\ninclude Locke and Nozick in Bruner (forthcoming). \nThere is also a new body of work that extends game-theoretic modeling\nand simulation to questions of social inequity. Bruner (2017) shows that\nthe mere fact that one group is a minority in a population, and thus\ninteracts more frequently with majority than with minority members,\ncan result in its being disadvantaged where exchanges are\ncharacterized by bargaining in a Nash demand game (Young 1993). Termed\nthe “cultural Red King”, the effect has been further\nexplored through simulation, with links to experiment, and with\nextensions to questions of “intersectional disadvantage”,\nin which overlapping minority categories are in play (O’Connor\n2017;\n Mohseni, O’Connor, & Rubin 2019 [Other Internet Resources];\n O’Connor, Bright, & Bruner 2019). The relevance of this to\nthe focus of the previous section is made clear in Rubin and\nO’Connor (2018) and O’Connor and Bruner (2019), modeling\nminority disadvantage in scientific communities. \nIn computational simulations, game-theoretic cooperation has been\nappealed to as a model for aspects of both ethics in the sense of\nSidgwick and social-political philosophy on the model of Hobbes. That\nmodel is tied to game-theoretic assumptions in general, however, and\noften to the structure of the Prisoner’s Dilemma in particular\n(though Skyrms 2003 and Alexander 2007 are notable\nexceptions). With regard to a wide range of questions in social and\npolitical philosophy in particular, the limitations of game theory may\nseem unhelpfully abstract and artificial. \nWhile still abstract, there are other attempts to model questions in\nsocial political philosophy computationally. Here the studies\nmentioned earlier regarding polarization are relevant. There have also\nbeen recent attempts to address questions regarding epistemic\ndemocracy: the idea that among its other virtues, democratic\ndecision-making is more likely to track the truth. \nThere is a contrast, however, between open democratic decision-making,\nin which a full population takes part, and representative democracy,\nin which decision-making is passed up through a hierarchy of\nrepresentation. There is also a contrast between democracy seen as\npurely a matter of voting and as a deliberative process that in some\nway involves a population in wider discussion (Habermas 1992 [1996];\nAnderson 2006; Landemore 2013). \nFigure 18: The Condorcet result:\nprobability of a majority of different odd-numbered sizes being\ncorrect on a binary question with different homogeneous probabilities\nof individual members being correct. [An\n extended description of figure 18\n is in the supplement.] \nThe classic result for an open democracy and simple voting is the\nCondorcet jury theorem (Condorcet 1785). As long as each voter has a\nuniform an independent probability greater than 0.5 of getting an\nanswer right, the probability of a correct answer from a majority vote\nis significantly higher than that of any individual, and it quickly\nincreases with the size of the population\n (Figure 18). \nIt can be shown analytically that the basic thrust of the Condorcet\nresult remains when assumptions regarding uniform and independent\nprobabilities are relaxed (Boland, Proschan, & Tong 1989; Dietrich\n& Spiekermann 2013). The Condorcet result is significantly\nweakened, however, when applied in hierarchical representation, in\nwhich smaller groups first reach a majority verdict which is then\ncarried to a second level of representatives who use a majority vote\non that level (Boland 1989). More complicated questions regarding\ndeliberative dynamics and representation require simulation using\ncomputers. \nThe Hong-Page structure of group deliberation, outlined in the context\nof computational philosophy of science above, can also be taken as a\nmodel of “deliberative democracy” beyond a simple vote.\nThe success of deliberation in a group can be measured as the average\nvalue height of points found. In a representative instantiation of\nthis kind of deliberation, smaller groups of individuals first use\ntheir individual heuristics to explore a landscape collectively, then\nhanding their collective “best” for each point on the\nlandscape to a representative. In a second round of deliberation, the\nrepresentatives work from the results from their constituents in a\nsecond round of exploration. \nUnlike in the case of pure voting and the Condorcet result,\ncomputational simulations show that the use of a representative\nstructure does not dull the effect of deliberation on this model:\naverage scores for three groups of three in a representative structure\nare if anything slightly higher than average scores from an open\ndeliberation involving 9 agents (Grim, Bramson et al. forthcoming).\nResults like these show how computational models might help expand the\npolitical philosophical arguments for representative democracy. \nSocial and political philosophy appears to be a particularly promising\narea for big data and computational philosophy employing the data\nmining tools of computational social science, but as of this writing\nthat development remains largely a promise for the future. \nThe guiding idea of the interdisciplinary theme known as\n“complex systems” is that phenomena on a higher level can\n“emerge” from complex interactions on a lower level\n(Waldrop 1992, Kauffman 1995, Mitchell 2011, Krakauer 2019). The\nemergence of social outcomes from the interaction of individual\nchoices is a natural target, and agent-based modeling is a natural\ntool. \nOpinion polarization and the evolution of cooperation, outlined above,\nboth fit this pattern. A further classic example is the work of Thomas\nC. Schelling on residential segregation. A glance at demographic maps\nof American cities makes the fact of residential segregation obvious:\nethnic and racial groups appear as clearly distinguished patches\n (Figure 19).\n Is this an open and shut indication of rampant racism in American\nlife? \nFigure 19: A demographic map of Los\nAngeles. White households are shown in red, African-American in\npurple, Asian-American in green, and Hispanic in orange.\n (Fischer 2010 in Other Internet Resources)\n  \nSchelling attempted an answer to this question with an agent-based\nmodel that originally consisted of pennies and dimes on a checkerboard\narray (Schelling 1971, 1978), but which has been studied\ncomputationally in a number of variations. Two types of agents\n(Schelling’s pennies and dimes) are distributed at random across\na cellular automata lattice, with given preferences regarding their\nneighbors. In its original form, each agent has a threshold regarding\nneighbors of “their own kind”. At that threshold level and\nabove, agents remain in place. Should they not have that number of\nlike neighbors, they move to another spot (in some variations, a move\nat random, in others a move to the closest spot that satisfies their\nthreshold). \nWhat Schelling found was that residential segregation occurs even\nwithout a strong racist demand that all of one’s neighbors, or\neven most, are “of one’s kind”. Even when preference\nis that just a third of one’s neighbors are “of\none’s kind”, clear patches of residential segregation\nappear. The iterated evolution of such an array is shown in\n Figure 20.\n See\n the interactive simulation of this residential segregation model\n in the Other Internet Resources section below.  \nFigure 20: Emergence of residential\nsegregation in the Schelling model with preference threshold set at\n33% \nThe conclusion that Schelling is careful to draw from such a model is\nsimply that a low level of preference can be sufficient for\nresidential segregation. It does not follow that more egregious social\nand economic factors aren’t operative or even dominant in the\nresidential segregation we actually observe.  \nIn this case basic modeling assumptions have been challenged on\nempirical grounds. Elizabeth Bruch and Robert Mare use sociological\ndata on racial preferences, challenging the sharp cut-off employed in\nthe Schelling model (Bruch & Mare 2006). They claim on the basis\nof simulation that the Schelling effect disappears when more\nrealistically smooth preference functions are used instead. Their\nsimulations and the latter claim turn out to be in error (van de Rijt,\nSiegel, & Macy 2009), but the example of testing the robustness of\nsimple models with an eye to real data remains a valuable one. \nComputational modeling has been applied in philosophy of language\nalong two main lines. First, there are investigations of analogy and\nmetaphor using models of semantic webs that share a developmental\nhistory with some of the models of scientific theory outlined above.\nSecond, there are investigations of the emergence of signaling, which\nhave often used a game-theoretic base akin to some approaches to the\nemergence of cooperation discussed above. \nWordNet is a computerized lexical database for English built by George\nMiller in 1985 with a hierarchical structure of semantic categories\nintended to reflect empirical observations regarding human processing.\nA category “bird” includes a sub-category\n“songbirds” with “canary” as a particular, for\nexample, intended to explain the fact that subjects could more quickly\nprocess “canaries sing”—which involves traversing\njust one categorical step—than they could process\n“canaries fly” (Miller, Beckwith, Fellbaum, Gross, &\nMiller 1990). \nThere is a long tradition, across psychology, linguistics, and\nphilosophy, in which analogy and metaphor are seen as an important key\nto abstract reasoning and creativity (Black 1962; Hesse 1943 [1966];\nLakoff & Johnson 1980; Gentner 1982; Lakoff & Turner 1989).\nBeginning in the 1980s several notable attempts have been made to\napply computational tools in order to both understand and generate\nanalogies. Douglas Hofstadter and Melanie Mitchell’s Copycat,\ndeveloped as a model of high-level cognition, has\n“codelets” compete within a network in order to answer\nsimple questions of analogy: “abc is to abd as ijk is to\nwhat?” (Hofstadter 2008). Holyoak and Thagard envisage metaphors\nas analogies in which the source and target domain are semantically\ndistinct, calling for relational comparison between two semantic nets\n(Holyoak & Thagard 1989, 1995; see also Falkenhainer, Forbus,\n& Gentner 1989). In the Holyoak and Thagard model those\ncomparisons are constrained in a number of different ways that call\nfor coherence; their computational modeling for coherence in the case\nof metaphor was in fact a direct ancestor to Thagard’s coherence\nmodeling of scientific theory change discussed above (Thagard 1988,\n1992). \nEric Steinhart and Eva Kittay’s\n NETMET (see Other Internet Resources)\n offers an illustration of the relational approach to analogy and\nmetaphor. They use one semantic and inferential subnet related to\nbirth another related to the theory of ideas in the Theatetus. Each\nsubnet is categorized in terms of relations of containment,\nproduction, discarding, helping, passing, expressing and opposition.\nOn that basis NETMET generates metaphors including “Socrates is\na midwife”, “the mind is an intellectual womb”,\n“an idea is a child of the mind”, “some ideas are\nstillborn”, and the like (Steinhart 1994; Steinhart & Kittay\n1994). NETMET can be applied to large linguistic databases such as\nWordNet. \nSuppose we start without pre-existing meaning. Is it possible that\nunder favorable conditions, unsophisticated learning dynamics can\nspontaneously generate meaningful signaling? The answer is\naffirmative. —Brian Skyrms,\nSignals (2010: 19) \nDavid Lewis’ sender-receiver game is a cooperative game in which\na sender observes a state of nature and chooses a signal, a receiver\nobserves that signal and chooses an act, with both sender and receiver\nbenefiting from an appropriate coordination between state of nature\nand act (Lewis 1969). A number of researchers have explored both\nanalytic and computational models of signaling games with an eye to\nways in which initially arbitrary signals can come to function in ways\nthat start to look like meaning. \nCommunication can be seen as a form of cooperation, and here as in the\ncase of the emergence of cooperation the methods of (communicative)\nstrategy change seem less important than the interactive structure in\nwhich those strategies play out. Computer simulations show that simple\nimitation of a neighbor’s successful strategy, various forms of\nreinforcement learning, and training up of simple neural nets on\nsuccessful neighbors’ behaviors can all result in the emergence\nand spread of signaling systems, sometimes with different dialects\n(Zollman 2005; Grim, St.\nDenis & Kokalis 2002; Grim, Kokalis, Alai-Tafti, Kilb & St. Denis, 2004).[9]\n Development on a cellular automata grid produces communication with\nany of these techniques, even when the rewards are one-sided rather\nthan mutual in a strict Lewis signaling game, but structures of\ninteraction that facilitate communication can also co-evolve with the\ncommunication they facilitate as well (Skyrms 2010). Elliot Wagner\nextends the study of communication on interaction structures to other\nnetworks as well (Wagner 2009). \nOn an interpretation in terms of biological evolution, computationally\nemergent signaling of this sort can be seen as modeling communication\nin Vervet monkeys (Cheney & Seyfarth 1990) or even chemical\n“signals” in bacteria (Berleman, Scott, Chumley, &\nKirby 2008). If interpreted in terms of learned culture, particularly\nwith an eye to more complex signal combination, these have been\noffered as models of mechanisms at play in the development of human\nlanguage (Skyrms 2010).\n A simple interactive model in which signaling emerges in a situated population of agents harvesting food sources and avoiding predators\n is available in the Other Internet Resources section below. \nMany of our examples of computational philosophy have been examples of\nsimulation—often social simulation by way of agent-based\nmodeling. But there is also a strong tradition in which computation is\nused not in simulations but as a way of mechanizing and extending\nphilosophical argument (typically understood as deductive proof), with\napplications in philosophy of logic and ultimately in deontic logic,\nmetaphysics, and philosophy of\n religion.[10] \nEntitling a summer Dartmouth conference in 1956, the organizers coined\nthe term “artificial intelligence”. One of the high points\nof that conference was a computational program for the construction of\nlogical proofs, developed by Allen Newell and Herbert Simon at\nCarnegie Mellon and programmed by J. C. Shaw using the vacuum tubes of\nthe JOHNNIAC computer at the Institute for Advanced Study (Bringsjord\n& Govindarajulu 2018 [2019]). Newell and Simon’s\n“Logic Theorist” was given 52 theorems from chapter two of\nWhitehead and Russell’s Principia Mathematica (1910, 1912, 1913), of which it successfully\nproved 38, including a proof more elegant than one of Whitehead and\nRussell’s own (MacKenzie 1995, Loveland 1984, Davis 1957\n[1983]). Russell himself was impressed: \nI am delighted to know that Principia Mathematica can now be\ndone by machinery… I am quite willing to believe that\neverything in deductive logic can be done by machinery. (letter to\nHerbert Simon, 2 November 1956; quoted in O’Leary 1991: 52)  \nDespite possible claims to anticipation, the most compelling of which\nmay be Martin Davis’s 1950 computer implementation of Mojsesz\nPresburger’s decision procedure for a fragment of arithmetic\n(Davis 1957), the Logic Theorist is standardly regarded as the first\nautomated theorem-prover. Newell and Simon’s target, however,\nwas not so much a logic prover as a proof of concept for an\nintelligent or thinking machine. Having rejected geometrical proof as\ntoo reliant on diagrams, and chess as too hard, by Simon’s own\naccount they turned to logic because Principia Mathematica\nhappened to be on his\n shelf.[11] \nSimon and Newell’s primary target was not an optimized\ntheorem-prover but a “thinking machine” that in some way\nmatched human intelligence. They therefore relied in heuristics\nthought of as matching human strategies, an approach later ridiculed\nby Hao Wang: \nThere is no need to kill a chicken with a butcher’s knife, yet\nthe net impression is that Newell-Shaw-Simon failed even to kill the\nchicken…to argue the superiority of “heuristic”\nover algorithmic methods by choosing a particularly inefficient\nalgorithm seems hardly just. (Wang 1960: 3) \nLater theorem-provers were focused on proof itself rather than a model\nof human reasoning. By 1960 Hao Wang, Paul Gilmore, and Dag Prawitz\nhad developed computerized theorem-provers for the full first-order\npredicate calculus (Wang 1960, MacKenzie 1995). In the 1990s William\nMcCune developed Otter, a widely distributed and accessible prover for\nfirst-order logic (McCune & Wos 1997, Kalman 2001). A more recent\nincarnation is Prover9, coupled with search for models and\ncounter-examples in Mace4. Examples of Prover9 derivations are offered in Other Internet Resources. A contemporary alternative is Vampire, developed by Andrei Voronkov, Kryštof Hodere, and Alexander Rizanov (Riazanov & Voronkov 2002). \nTheorem-provers developed for higher-order logics, working from a\nvariety of approaches, include TPS (Andrews and Brown 2006), Leo-II\nand -III (Benzmüller, Sultana, Paulson, & Theiß 2015;\nSteen & Benzmüller 2018), and perhaps most prominently HOL\nand particularly development-friendly\n Isabelle/HOL\n (Gordon & Melham 1993; Paulson 1990). With clever implementation\nand extension, these also allow automation of aspects of modal,\ndeontic, epistemic, intuitionistic and paraconsistent logics, of\ninterest both in their own terms and in application within computer\nscience, robotics, and artificial intelligence (McRobbie 1991; Abe,\nAkama, & Nakamatsu 2015). \nWithin pure logic, Portararo (2001 [2019]) lists a number of results\nthat have been established using automated theorem-provers. It was\nconjectured for 50 years that a particular equation in a Robbins\nalgebra could be replaced by a simpler one, for example. Even Tarski\nhad failed in the attempt at proof, but McCune produced an automated\nproof in 1997 (McCune 1997). Shortest and simplest axiomatizations for\nimplicational fragments of modal logics S4 and S5 had been studied for\nyears as open questions, with eventual results by automated reasoning\nin 2002 (Ernst, Fitelson, Harris, & Wos\n 2002).[12] \nTheorem provers have been applied within deontic logics in the attempt\nto mechanize ethical reasoning and decision-making (Meyer &\nWierenga 1994; Van Den Hoven & Lokhorst 2002; Balbiani, Broersen,\n& Brunel 2009; Governatori & Sartor 2010; Benzmüller,\nParent, & van der Torre 2018; Benzmüller, Farjami, &\nParent, 2018). Alan Gewirth has argued that agents contradict their\nstatus as agents if they don’t accept a principle of generic\nconsistency—respecting the agency-necessary rights of\nothers—as a supreme principle of practical rationality (Gewirth\n1978; Beyleveld 1992, 2012). Fuenmayor and Benzmüller have shown\nthat even an ethical theory of this complexity can be formally encoded\nand assessed computationally (Fuenmayor & Benzmüller\n2018). \nOne of the major advances in computational philosophy has been the\napplication of theorem-provers to the analysis of classical\nphilosophical positions and arguments. From axioms of a metaphysical\nobject theory, Zalta and his collaborators use Prover9 and Mace to\nestablish theorems regarding possible worlds, such as the claim that\nevery possible world is maximal, modal theorems in Leibniz, and\nconsequences from Plato’s theory of Forms (Fitelson & Zalta\n2007; Alama, Oppenheimer, & Zalta 2015; Kirchner, Benzmüller,\n& Zalta 2019). \nVersions of the ontological argument have formed an important thread\nin recent work employing theorem provers, both because of their\ninherent interest and the technical challenges they bring with them.\nProver9 and Mace have again been used recently by Jack Horner in order\nto analyze a version of the ontological argument in Spinoza’s\nEthics (found invalid) and to propose an alternative (Horner\n2019). Significant work has been done on versions of Anselm’s\nontological argument (Oppenheimer & Zalta 2011; Garbacz 2012;\nRushby 2018). Christoph Benzmüller and his colleagues have\napplied higher-order theorem provers, including including Isabelle/HOL\nand their own Leo-II and Leo-III, in order to analyze a version of the\nontological argument found in the papers of Kurt Gödel\n(Benzmüller & Paelo 2016a, 2016b; Benzmüller, Weber,\n& Paleo 2017; Benzmüller & Fuenmayor 2018). A previously\nunnoticed inconsistency was found in Gödel’s original,\nthough avoided in Dana Scott’s transcription. Theorem-provers\nconfirmed that Gödel’s argument forces modal\ncollapse—all truths become necessary truths. Analysis with\ntheorem-provers makes it clear that variations proposed by C. Anthony\nAnderson and Melvin Fitting avoid that consequence, but in importantly\ndifferent ways (Benzmüller & Paleo 2014; Kirchner,\nBenzmüller, & Zalta\n 2019).[13] \nWork in metaphysics employing theorem-provers continues. Here of\nparticular note is Ed Zalta’s ambitious and long-term attempt to\nground metaphysics quite generally in computationally instantiated\nobject theory (Fitelson & Zalta 2007; Zalta 2020).\n A link to Zalta’s project can be found in the Other Internet Resources section below.\n  \nThe Dartmouth conference of 1956 is standardly taken as marking the\ninception of both the field and the term “artificial\nintelligence” (AI). There were, however, two distinct\ntrajectories apparent in that conference. Some of the participants\ntook as their goal to be the development of intelligent or thinking\nmachines, with perhaps an understanding of human processing as a\nbegrudging means to that end. Others took their goal to be a\nphilosophical and psychological understanding of human processing,\nwith the development of machines a means to that end. Those in the\nfirst group were quick to exploit linear programming: what came to be\nknown as “GOFAI”, or “good old-fashioned artificial\nintelligence”. Those in the second group rejoiced when\nconnectionist and neural net architectures came to maturity several\ndecades later, promising models directly built on and perhaps\nreflective of mechanisms in the human brain (Churchland 1995). \nAttempts to understand perception, conceptualization, belief change,\nand intelligence are all part of philosophy of mind. The use of\ncomputational models toward that end—the second strand\nabove—thus comes close to computational philosophy of mind.\nDaniel Dennett has come close to saying that AI is philosophy\nof mind: “a most abstract inquiry into the possibility of\nintelligence or knowledge” (Dennett 1979: 60; Bringsjord &\nGovindarajulu 2018 [2019]). \nThe bulk of AI research remains strongly oriented toward producing\neffective and profitable information processing, whether or not the\nresult offers philosophical understanding. So it is perhaps better not\nto identify AI with philosophy of mind, though AI has often been\nguided by philosophical conceptions and aspects of AI have proven\nfruitful for philosophical exploration. Philosophy of AI and\nphilosophy of mind inspired by and in response to\nAI, which are not the topic here, have both been far more common than\nphilosophy of mind developed with the techniques of AI. \nOne example of a program in artificial intelligence that was\nexplicitly conceived in philosophical terms and designed for\nphilosophical ends was the OSCAR project, developed by John Pollock\nbut cut short by his death (Pollock 1989, 1995, 2006). The goal of\nOSCAR was construction of a computational agent: an “artificial\nintellect”. At the core of OSCAR was implementation of a theory\nof rationality. Pollock was explicit regarding the intersection of AI\nand philosophy of mind in that project:  \nThe implementability of a theory of rationality is a necessary\ncondition for its correctness. This amounts to saying that philosophy\nneeds AI just as much as AI needs philosophy. (Pollock 1995: xii;\nBringsjord & Govindarajulu 2018 [2019]) \nAt the core of OSCAR’s rationality is implementation of\ndefeasible non-monotonic logic employing prima facie reasons and\npotential defeaters. Among its successes, Pollock claims an ability to\nhandle the lottery paradox and preface paradoxes. Informally, the fact\nthat we know that one of the many tickets in a lottery will win means\nthat we must treat “ticket 1 will not win…”,\n“ticket 2 will not win…” and the like not as items\nof knowledge but as defeasible beliefs for which we have strong prima\nfacie reasons. Pollock’s formal treatment in terms of collective\ndefeat is nicely outlined in a supplement on OSCAR in Bringsjord &\nGovindarajulu (2018 [2019]). \nThe sections above were intended to be an introduction to\ncomputational philosophy largely by example, emphasizing both the\nvariety of computational techniques employed and the spread of\nphilosophical topics to which they are applied. This final section is\ndevoted to the problems and prospects of computational philosophy. \nAlthough computational instantiations of logic are of an importantly\ndifferent character, simulation—including agent-based\nsimulation—plays a major role in much of computational\nphilosophy. Beyond philosophy, across all disciplines of its\napplication, simulation often raises suspicions. \nA standard suspicion of simulation in various fields is that it one\n“can prove anything” by manipulation of model structure\nand parameters. The worry is that an anticipated or desired effect\ncould always be “baked in”, programmed as an artefact of\nthe model itself. Production of a simulation would thus demonstrate\nnot the plausibility of a hypothesis or a fact about the world but\nmerely the cleverness of the programmer. In a somewhat different\ncontext, Rodney Brooks has written that the problem with simulations\nis that they are “doomed to succeed” (Brooks & Mataric\n1993). \nBut consider a similar critique of logical argument: that one\n“can prove anything” by careful choice of premises and\nrules of inference. The proper response in the case of logical\nargument is to concede the fact that a derivation for any proposition\ncan be produced from carefully chosen premises and rules, but to\nemphasize that it may be difficult or impossible to produce a\nderivation from agreed rules and clear and plausible premises. \nA similar response is appropriate here. The effectiveness of\nsimulation as argument depends on the strength of its assumptions and\nthe soundness of its mechanisms just as the effectiveness of logical\nproof depends on the strength of its premises and the validity of its\nrules of inference. The legitimate force of the critique, then, is not\nthat simulation is inherently untrustworthy but simply that the\nassumptions of any simulation are always open to further\nexamination. \nAnyone who has attempted computer simulation can testify that it is\noften extremely difficult or impossible to produce an expected effect,\nparticularly a robust effect across a plausible range of parameters\nand with a plausible basic mechanism. Like experiment, simulation can\ndemonstrate both the surprising fragility of a favored hypothesis and\nthe surprising robustness of an unexpected effect. \nFar from being “doomed to succeed”, simulations fail quite\nregularly in several important ways (Grim, Rosenberger, Rosenfeld,\nAnderson, & Eason 2013). Two standard forms of simulation failure\nare failure of verification and failure of validation (Kleijnen 1995;\nWindrum, Fabiolo, & Moneta 2007; Sargent 2013). Verification of a\nmodel demands assuring that it accurately reflects design intention.\nIf a computational model is intended to instantiate a particular\ntheory of belief change, for example, it fails verification if it does\nnot accurately represent the dynamics of that theory. Validation is\nperhaps the more difficult demand, particularly for philosophical\ncomputation: that the computational model adequately reflects those\naspects of the real world it is intended to capture or explain. \nIf its critics are right, a simple example of verification failure is the original Weisberg and\nMuldoon model of scientific exploration outlined above (Weisberg &\nMuldoon 2009). The model was intended to include two kinds of\nepistemic agents—followers and mavericks—with distinct\npatterns of exploration. Mavericks avoid previously investigated\npoints in their neighborhood. Followers move to neighboring points\nthat have been investigated but that have a higher significance. In\ncontrast to their description in the text, the critics argue, the software\nfor the model used “>=” in place of “>”\nat a crucial place, with the result that followers moved to\nneighboring points with a higher or equal significance, resulting in\ntheir often getting stuck in a very local oscillation (Alexander, Himmelreich, & Thomson 2015). If so, Weisberg and\nMuldoon’s original model fails to match its design\nintention—it fails verification—though some of their\ngeneral conclusions regarding epistemic diversity have been vindicated\nin further studies. \nValidation is a very different and more difficult demand: that a\nsimulation model adequately captures relevant aspects of what it is\nintended to model. A common critique of specific models is that they\nare too simple, leaving out some crucial aspect of the modeled\nphenomenon. When properly targeted, this can be an entirely\nappropriate critique. But what it calls for is not the abandonment of\nmodeling but better construction of a better model. \nIn time…the Cartographers Guilds struck a Map of the Empire\nwhose size was that of the Empire, and which coincided point for point\nwith it. The following Generations, saw that that vast Map was\nUseless…. (Jorge Luis Borges, “On Exactitude in\nScience”, 1946 [1998 English translation: 325]) \nBorges’ story is often quoted in illustration of the fact that\nno model—and no scientific theory—can include all\ncharacteristics of what it is intended to model (Weisberg 2013).\nModels and theories would be useless if they did: the purpose of both\ntheories and models is to present simpler representations or\nmechanisms that capture the relevant features or dynamics of\na phenomenon. What aspects of a phenomenon are in fact the relevant\naspects for understanding that phenomenon calls for evaluative input\noutside of the model. But where relevant aspects are omitted,\nirrelevant aspects included, or unrealistic or artificial constraints\nimposed, what a critique calls for is a better model (Martini &\nPinto 2017; Thicke forthcoming). \nThere is one aspect of validation that can sometimes be gauged at the\nlevel of modeling itself and with modeling tools alone. Where the\ntarget is some general phenomenon—opinion polarization or the\nemergence of communication, for example—a model which produces\nthat phenomenon within only a tiny range of parameters should be\nsuspicious. Our estimate of the parameters actually in play in the\nactual phenomenon may be merely intuitive or extremely rough, and the\nreal phenomenon may be ubiquitous in a wide range of settings. In such\na case, it would seem prima facie unlikely that a model which produced\na parallel effect within only a tiny window of parameters could be\ncapturing the general mechanism of a general phenomenon. In such cases\nrobustness testing is called for, a test for one aspect of validation\nthat can still be performed on the computer. To what extent do\nconclusions drawn from the modeling effect hold up under a range of\nparameter variations? \nThe Hong-Page model of the value of diversity in exploration, outlined\nabove, has been widely appealed to quite generally as support for\ncognitive diversity in groups. It has been cited in NASA internal\ndocuments, offered in support of diversity requirements at UCLA, and\nappears in an amicus curiae brief before the Supreme Court in\nsupport of promoting diversity in the armed forces (Fisher v. Univ. of\nTexas 2016). But the model is not robust across its several parameters\nto support sweepingly general claims that have been made on its basis\nregarding diversity and ability or expertise (Grim et al. 2019). Is that a problem internal to the model, or an external matter\nof its interpretation or application? There is much to be said for the\nlatter alternative. The model is and remains an interesting\none—interesting often in the ways in which it does show\nsensitivity to different parameters. Thus a failure of one aspect of\nvalidation—robustness—with an eye to one type of general\nclaim can also call for further modelling: modeling intended to\nexplore different effects in different contexts. Rosenstock, Bruner,\nand O’Connor (2017) offer a robustness test for the Zollman\nmodel outlined above. Borg, Frey, Šešelja, and\nStraßer (2018) offer new modeling grounded precisely in a\nrobustness critique of their predecessors. \nIt is noteworthy that the simulation failures mentioned have been\ndetected and corrected within the literature of simulation itself.\nThese are effective critiques within disciplines employing simulation,\nrather than from outside. An illustration of a such a case with both\nverification and validation in play is that of the Bruch and Mare\ncritique of the Schelling segregation model and the response to it in\nvan Rooij, Siegel, and Macy (Schelling 1971, 1978; Bruch & Mare\n2006; van de Rijt, Siegel, & Macy 2009). Many aspects of that\nmodel are clearly artificial: a limitation to two groups,\nspatialization on a cellular automata grid, and\n“unhappiness” or moving in terms of a sharp threshold\ncut-off of tolerance for neighbors of the other group. Bruch and Mare\noffered clear empirical evidence that residential preferences do not\nfit a sharp threshold. More importantly, they built a variation of the\nSchelling model in order to show that the Schelling effect disappeared\nwith more realistic preference profiles. What Bruch and Mare\nchallenged, in other words, was validation: not merely that\naspects of the target phenomenon of residential segregation were left\nout (as they would be in any model), but that relevant aspects were\nleft out: differences that made an important difference. Van de Rijt,\nSiegel, and Macy failed to understand why the smooth preference curves\nin Bruch and Mare’s data wouldn’t support rather than\ndefeat a Schelling effect. On investigation they found that they\nwould: Bruch and Mare’s validation claim against Schelling was\nitself founded in a programming error. De Rijt, Siegel and\nMacy’s verdict was that Bruch and Mare’s attack itself\nfailed model verification. \nIn the case of both Weisberg and Muldoon, and Bruch and Mare, original\ncode was made freely available to their critics. In both cases, the\noriginal authors recognized the problems revealed, though emphasizing\naspects of their work that survived the criticisms. Here again an\nimportant point is that critiques and responses of this type have\narisen and been addressed within philosophical and scientific\nsimulation itself, working toward better models and practices. \nPhilosophy at its best has always been in contact with the conceptual\nand scientific methodologies of its time. Computational philosophy can\nbe seen as a contemporary instantiation of that contact, crossing\ndisciplinary boundaries in order to both influence and benefit from\ndevelopments in computer science and artificial intelligence.\nIncorporation of new technologies and wider application within\nphilosophy can be expected and should be hoped for. \nThere is one extremely promising area in need of development within\ncomputational philosophy, though that area may also call for changes\nin conceptions of philosophy itself. Philosophy has classically been\nconceived as abstract rather than concrete, as seeking understanding\nat the most general level rather than specific prediction or\nretrodiction, often normative, and as operating in terms of logical\nargument and analysis rather than empirical data. The last of these\ncharacteristics, and to some extent the first, will have to be\nqualified if computational philosophy grows to incorporate a major\nbatch of contemporary techniques: those related to big data. \nExpansion of computational philosophy in the intersection with big\ndata seems an exciting prospect for social and political philosophy,\nin the analysis of belief change, and in understanding the social and\nhistorical dynamics of philosophy of science (Overton 2013; Pence\n& Ramsey 2018). A particular benefit would be better prospects for\nvalidation of a range of simulations and agent-based models, as\nemphasized above (Mäs 2019; Reijula & Kuorikoski 2019). If\ncomputational philosophy moves in that promising direction, however,\nit may take on a more empirical character in some respects. Emphasis\non general and abstract understanding and concern with the normative\nwill remain marks of a philosophical approach, but the membrane\nbetween some topic areas in philosophy and aspects of computational\nscience can be expected to become more permeable. \nDissolving these disciplinary boundaries may itself be a good in some\nrespects. The examples presented above make it clear that in\nincorporating (and contributing to) computational techniques developed\nin other areas, computational philosophy has long been\ncross-disciplinary. If our gain is a better understanding of the\ntopics that have long fascinated us, compromise in disciplinary\nboundaries and a change in our concept of philosophy seem a small\nprice to pay.","contact.mail":"patrick.grim@stonybrook.edu","contact.domain":"stonybrook.edu"},{"date.published":"2020-03-16","url":"https://plato.stanford.edu/entries/computational-philosophy/","author1":"Patrick Grim","author2":"Daniel Singer","entry":"computational-philosophy","body.text":"\n\n\nComputational philosophy is the use of mechanized computational\ntechniques to instantiate, extend, and amplify philosophical research.\nComputational philosophy is not philosophy of computers or\ncomputational techniques; it is rather philosophy using\ncomputers and computational techniques. The idea is simply to apply\nadvances in computer technology and techniques to advance discovery,\nexploration and argument within any philosophical area.\n\n\nAfter touching on historical precursors, this article discusses\ncontemporary computational philosophy across a variety of fields:\nepistemology, metaphysics, philosophy of science, ethics and social\nphilosophy, philosophy of language and philosophy of mind, often with\nexamples of operating software. Far short of any attempt at an\nexhaustive treatment, the intention is to introduce the spirit of each\napplication by using some representative examples.\n\nComputational philosophy is not an area or subdiscipline of philosophy\nbut a set of computational techniques applicable across many\nphilosophical areas. The idea is simply to apply computational\nmodeling and techniques to advance philosophical discovery,\nexploration and argument. One should not therefore expect a sharp\nbreak between computational and non-computational philosophy, nor a\nsharp break between computational philosophy and other computational\ndisciplines. \nThe past half-century has seen impressive advances in raw computer\npower as well as theoretical advances in automated theorem proving,\nagent-based modeling, causal and system dynamics, neural networks,\nmachine learning and data mining. What might contemporary\ncomputational technologies and techniques have to offer in advancing\nour understanding of issues in epistemology, ethics, social and\npolitical philosophy, philosophy of language, philosophy of mind,\nphilosophy of science, or philosophy of religion? Suggested by Leibniz\nand with important precursors in the history of formal logic, the idea\nis to apply new computational advances within long-standing areas of\nphilosophical interest. \nComputational philosophy is not the philosophy of\ncomputation, an area that asks about the nature of computation itself.\nAlthough applicable and informative regarding artificial intelligence,\ncomputational philosophy is not the philosophy of artificial\nintelligence. Nor is it an umbrella term for the questions about the\nsocial impact of computer use explored for example in philosophy of\ninformation, philosophy of technology, and computer ethics. More\ngenerally, there is no “of” that computational philosophy\ncan be said to be the philosophy of. Computational philosophy\nrepresents not an isolated topic area but the widespread application\nof whatever computer techniques are available across the full range of\nphilosophical topics. Techniques employed in computational philosophy\nmay draw from standard computer programming and software engineering,\nincluding aspects of artificial intelligence, neural networks, systems\nscience, complex adaptive systems, and a variety of computer modeling\nmethods. As a growing set of methodologies, it includes the prospect\nof computational textual analysis, big data analysis, and other\ntechniques as well. Its field of application is equally broad,\nunrestricted within the traditional discipline and domain of\nphilosophy. \nThis article is an introduction to computational philosophy rather\nthan anything like a complete survey. The goal is to offer a handful\nof suggestive examples across computational techniques and fields of\nphilosophical application. \nThe only way to rectify our reasonings is to make them as tangible as\nthose of the Mathematicians, so that we can find our error at a\nglance, and when there are disputes among persons, we can simply say:\nLet us calculate, without further ado, to see who is right. —Leibniz,\nThe Art of Discovery (1685 [1951: 51]) \nFormalization of philosophical argument has a history as old as\n logic.[1]\n Logic is the historical source and foundation of contemporary\n computing.[2]\n Our topic here is more specific: the application of contemporary\ncomputing to a range of philosophical questions. But that too has a\nhistory, evident in Leibniz’s vision of the power of\ncomputation. \nLeibniz is known for both the development of formal techniques in\nphilosophy and the design and production of actual computational\nmachinery. In 1642, the philosopher Blaise Pascal had invented the\nPascaline, designed to add with carry and subtract. Between 1673 and\n1720 Leibniz designed a series of calculating machines intended to\ninstantiate multiplication and division as well: the stepped reckoner,\nemploying what is still known as the Leibniz wheel (Martin 1925). The\nsole surviving Leibniz step reckoner was discovered in 1879 as workmen\nwere fixing a leaking roof at the University of Göttingen. In\ncorrespondence, Leibniz alluded to a cryptographic encoder and decoder\nusing the same mechanical principles. On the basis of those\ndescriptions, Nicholas Rescher has produced a working conjectural\nreconstruction (Rescher 2012). \nBut Leibniz had visions for the power of computation far beyond mere\narithmetic and cryptography. Leibniz’s 1666 Dissertatio De\nArte Combinatoria\ntrumpets the “art of combinations” as a method of\nproducing novel ideas and inventions as well as analyzing complex\nideas into simpler elements (Leibniz 1666 [1923]). Leibniz describes it as the “mother\nof inventions” that would lead to the “discovery of all\nthings”, with applications in logic, law, medicine, and physics.\nThe vision was of a set of formal methods applied within a perfect\nlanguage of pure concepts which would make possible the general\nmechanization of reason (Gray\n 2016).[3] \nThe specifics of Leibniz’s combinatorial vision \ncan be traced back to the mystical mechanisms of Raymond Llull circa 1308, combinatorial mechanisms lampooned in\nJonathan Swift’s Gulliver’s Travels of 1726 as allowing one to  \nwrite books in philosophy, poetry, politics, mathematics, and\ntheology, without the least assistance from genius or study. (Swift\n1726: 174, Lem 1964 [2013: 359])\n \nCombinatorial specifics aside, however, Leibniz’s vision of an\napplication of computational methods to substantive questions remains.\nIt is the vision of computational physics, computational biology,\ncomputational social science, and—in application to perennial\nquestions within philosophy—of computational philosophy. \nDespite Leibniz’s hopes for a single computational method that\nwould serve as a universal key to discovery, computational philosophy\ntoday is characterized by a number of distinct computational\napproaches to a variety of philosophical questions. Particular\nquestions and particular areas have simply seemed ripe for various\nmodels, methodologies, or techniques. Both attempts and results are\ntherefore scattered across a range of different areas. In what follows\nwe offer a survey of various explorations in computational\nphilosophy. \nComputational philosophy is perhaps most easily introduced by focusing\non applications of agent-based modeling to questions in social\nepistemology, social and political philosophy, philosophy of science,\nand philosophy of language. Sections 3.1 through 3.3 are therefore\nstructured around examples of agent-based modeling in these areas.\nOther important computational approaches and other areas are discussed\nin 3.4 through 3.6. \nTraditional epistemology—the epistemology of Plato, Hume,\nDescartes, and Kant—treats the acquisition and validation of\nknowledge on the individual level. The question for traditional\nepistemology was always how I as an individual can acquire\nknowledge of the objective world, when all I have to work with is my\nsubjective experience. Perennial questions of individual epistemology\nremain, but the last few decades have seen the rise of a very\ndifferent form of epistemology as well. Anticipated in early work by\nAlvin I. Goldman, Helen Longino, Philip Kitcher, and Miriam Solomon,\nsocial epistemology is now evident both within dedicated\njournals and across philosophy quite generally (Goldman 1987; Longino\n1990; Kitcher 1993; Solomon 1994a, 1994b; Goldman & Whitcomb 2011;\nGoldman & O’Connor 2001 [2019]; Longino 2019). I acquire my\nknowledge of the world as a member of a social group: a group that\nincludes those inquirers that constitute the scientific enterprise,\nfor example. In order to understand the acquisition and validation of\nknowledge we have to go beyond the level of individual epistemology:\nwe need to understand the social structure, dynamics, and process of\nscientific investigation. It is within this social turn in\nepistemology that the tools of computational\nmodelling—agent-based modeling in particular—become\nparticularly useful. (Klein, Marx and Fischbach 2018). \nThe following two sections use computational work on belief change as\nan introduction to agent-based modeling in social epistemology.\nClosely related questions regarding scientific communication are left\nto sections\n 3.2.2\n and\n 3.2.3. \nHow should we expect beliefs and opinions to change within a social\ngroup? How might they rationally change? The computational\napproach to these kinds of questions attempts to understand basic\ndynamics of the target phenomenon by building, running, and analyzing\nsimulations. Simulations may start with a model of interactive\ndynamics and initial conditions, which might include, for example, the\ninitial beliefs of individual agents and how prone those agents are to\nshare information and listen to others. The computer calculates\nsuccessive states of the model (“steps”) as a function\n(typically stochastic) of preceding stages. Researchers collect and\nanalyze simulation outputs, which might include, for example, the\ndistribution of beliefs in the simulated society after a certain\nnumber of rounds of communication. Because simulations typically\ninvolve many stochastic elements (which agents talk with which agents\nat what point in the simulation, what specific beliefs specific agents\nstart with, etc.), data is usually collected and analyzed across a\nlarge number of simulation runs. \nOne model of belief change and opinion polarization that has been of\nwide interest is that of Hegselmann and Krause (2002, 2005, 2006),\nwhich offers a clear and simple example of the application of\nagent-based techniques. \nOpinions in the Hegselmann-Krause model are mapped as numbers in the\n[0, 1] interval, with initial opinions spread uniformly at random in\nan artificial population. Individuals update their beliefs by taking\nan average of the opinions that are “close enough” to an\nagent’s own. As agents’ beliefs change, a different set of\nagents or a different set of values can be expected to influence\nfurther updating. A crucial parameter in the model is the threshold of\nwhat counts as “close enough” for actual\n influence.[4] \n\n Figure 1\n shows the changes in agent opinions over time in single runs with\nthresholds ε set at 0.01, 0.15, and 0.25 respectively. With a\nthreshold of 0.01, individuals remain isolated in a large number of\nsmall local groups. With a threshold of 0.15, the agents form two\npermanent groups. With a threshold of 0.25, the groups fuse into a\nsingle consensus opinion. These are typical representative cases, and\nruns vary slightly. As might be expected, all results depend on both\nthe number of individual agents and their initial random locations\nacross the opinion space. See the\n interactive simulation of the Hegselmann and Krause bounded confidence model\n in the Other Internet Resources section below. \n \nFigure 1: Example changes in opinion\nacross time from single runs with different threshold values\n\\(ε \\in \\{0.01, 0.15, 0.25\\}\\) in the Hegselmann and Krause\n(2002) model. [An\n extended description of figure 1\n is in the supplement.] \nAn illustration of average outcomes for different threshold values\nappears as\n figure 2.\n What is represented here is not change over time but rather the final\nopinion positions given different threshold values. As the threshold\nvalue climbs from 0 to roughly 0.20, there is an increasing number of\nresults with concentrations of agents at the outer edges of the\ndistribution, which themselves are moving inward. Between 0.22 and\n0.26 there is a quick transition from results with two final groups to\nresults with a single final group. For values still higher, the two\nsides are sufficiently within reach that they coalesce on a central\nconsensus, although the exact location of that final monolithic group\nchanges from run to run creating the fat central spike shown.\nHegselmann and Krause describe the progression of outcomes with an\nincreasing threshold as going through three phases:  \nAs the homogeneous and symmetric confidence interval increases we\ntransit from phase to phase. More exactly, we step from fragmentation\n(plurality) over polarisation (polarity) to consensus (conformity).\n(2002: 11, authors’ italics) \nFigure 2: Frequency of equilibrium opinion\npositions for different threshold values in the Hegselmann and Krause\nmodel scaled to [0, 100] (as original with axes relabeled; Hegselmann\nand Krause 2002). [An\n extended description of figure 2\n is in the supplement.] \nA number of models further refine the “bounded confidence”\nmechanisms of the Hegselmann Krause model. Deffuant et al., for\nexample, replace the sharp cutoff of influence in Hegselmann-Krause\nwith continuous influence values (Deffuant et al. 2002; Deffuant 2006;\nMeadows & Cliff 2012). Agents are again assigned both opinion\nvalues and threshold (“uncertainty”) ranges, but the\nextent to which the opinion of agent i is influential on\nagent j is proportional to the ratio of the overlap of their\nranges (opinion plus or minus threshold) over i’s\nrange. Opinion centers and threshold ranges are updated accordingly,\nresulting in the possibility of individuals with narrower and wider\nranges. Given the updating algorithm, influence may also be\nasymmetric: individuals with a narrower range of tolerance, which\nDeffuant et al. interpret as higher confidence or lower uncertainty,\nwill be more influential on individuals with a wider range than vice\nversa. The influence on polarization of “stubborn”\nindividuals who do not change, and of agents on extremes, has also\nbeen studied, showing a clear impact on the dynamics of belief change\nin the\n group.[5] \nEric Olsson and Sofi Angere have developed a sophisticated program in\nwhich the interaction of agents is modelled within a Bayesian network\nof both information and trust (Olsson 2011). Their program,\n Laputa\n (see Other Internet Resources) has a wide range of applications, one\nof which is a model of polarization interpreted in terms of the\nPersuasive Argument Theory in psychology and which replicates an\neffect seen in empirical studies: the increasing divergence of\npolarized groups (Lord, Ross, & Lepper 1979; Isenberg 1986; Olsson\n2013). Olsson raises the question of whether polarization may be\nepistemically rational, offering a positive answer. O’Connor and\nWeatherall (2018) and Singer et al. (2019) also argue that polarization\ncan be rational, using different models and perhaps different senses\nof polarization (Bramson et al. 2017). \nThe topic of polarization is anticipated in an earlier tradition of\ncellular automata models initiated by Robert Axelrod. The basic\npremise of Axelrod (1997) is that people tend to interact more with\nthose like themselves and tend to become more like those with whom\nthey interact. But if people come to share one another’s beliefs\n(or other cultural features) over time, why do we not observe complete\ncultural convergence? At the core of Axelrod’s model is a\nspatially instantiated imitative mechanism that produces cultural\nconvergence within local groups but also results in progressive\ndifferentiation and cultural isolation between groups. \n100 agents are arranged on a \\(10 \\times 10\\) lattice such as that\nillustrated in\n Figure 3.\n Each agent is connected to four others: top, bottom, left, and right.\nThe exceptions are those at the edges or corners of the array,\nconnected to only three and two neighbors, respectively. Agents in the\nmodel have multiple cultural “features”, each of which\ncarries one of multiple possible “traits”. One can think\nof the features as categorical variables and the traits as options or\nvalues within each category. For example, the first feature might\nrepresent culinary tradition, the second one the style of dress, the\nthird music, and so on. In the base configuration an agent’s\n“culture” is defined by five features \\((F = 5)\\) each\nhaving one of 10 traits \\((q =10),\\) numbered 0 through 9. Agent\nx might have \\(\\langle 8, 7, 2, 5, 4\\rangle\\) as a cultural\nsignature while agent y is characterized \\(\\langle 1, 4, 4, 8,\n4\\rangle\\). Agents are fixed in their lattice location and hence their\ninteraction partners. Agent interaction and imitation rates are\ndetermined by neighbor similarity, where similarity is measured as the\npercentage of feature positions that carry identical traits. With five\nfeatures, if a pair of agents share exactly one such element they are\n20% similar; if two elements match then they are 40% similar, and so\nforth. In the example just given, agents x and y and\nhave a similarity of 20% because they share only one feature. \nFigure 3: Typical initial set of\n“cultures” for a basic Axelrod-style model consisting of\n100 agents on a \\(10 \\times 10\\) lattice with five features and 10\npossible traits per agent. The marked sight shares two of five traits\nwith the site above it, giving it a cultural similarity score of 40%\n(Axelrod 1997). \nFor each iteration, the model picks at random an agent to be active\nand one of its neighbors. With probability equal to their cultural\nsimilarity, the two sites interact and the active agent changes one of\nits dissimilar elements to that of its neighbor. If agent \\(i =\n\\langle 8, 7, 2, 5, 4\\rangle\\) is chosen to be active and it is paired\nwith its neighbor agent \\(j = \\langle 8, 4, 9, 5, 1\\rangle,\\) for\nexample, the two will interact with a 40% probability because they\nhave two elements in common. If the interaction does happen, agent\ni changes one of its mismatched elements to match that of\nj, becoming perhaps \\(\\langle 8, 7, 2, 5, 1\\rangle.\\) This\nchange creates a similarity score of 60%, yielding an increased\nprobability of future interaction between the two. \nIn the course of approximately 80,000 iterations, Axelrod’s\nmodel produces large areas in which cultural features are identical:\nlocal convergence. It is also true, however, that arrays such as that\nillustrated do not typically move to full convergence. They instead\ntend to produce a small number of culturally isolated stable\nregions—groups of identical agents none of whom share features\nin common with adjacent groups and so cannot further interact. As an\narray develops, agents interact with increasing frequency with those\nwith whom they become increasingly similar, interacting less\nfrequently with the dissimilar agents. With only a mechanism of local\nconvergence, small pockets of similar agents emerge that move toward\ntheir own homogeneity and away from that of other groups. With the\nparameters described above, Axelrod reports a median of three stable\nregions at equilibrium. It is this phenomenon of global separation\nthat Axelrod refers to as “polarization”. See the\n interactive simulation of the Axelrod polarization model\n in the Other Internet Resources section below. \nAxelrod notes a number of intriguing results from the model, many of\nwhich have been further explored in later work. Results are very\nsensitive to the number of features F and traits q used\nas parameters, for example. Changing numbers of features and traits\nchanges the final number of stable regions in opposite directions: the\nnumber of stable regions correlates negatively with the number of\nfeatures F but positively with the number of traits q\n(Klemm et al. 2003). In Axelrod’s base case with \\(F = 5\\) and\n\\(q = 10\\) on a \\(10 \\times 10\\) lattice, the result is a median of\nthree stable regions. When q is increased from 10 to 15, the\nnumber of final regions increases from three to 20; increasing the\nnumber of traits increases the number of stable groups dramatically.\nIf the number of features F is increased to 15, in contrast,\nthe average number of stable regions drops to only 1.2 (Axelrod 1997).\nFurther explorations of parameters of population size, configuration,\nand dynamics, with measures of relative size of resultant groups,\nappear in Klemm et al. (2003a, b, c, 2005) and in Centola et al. (2007). \nOne result that computational modeling promises regarding a phenomenon\nsuch as opinion polarization is an understanding of the phenomenon\nitself: how real opinion polarization might happen, and how it might\nbe avoided. Another and very different outcome, however, is created by\nthe fact that computational modeling both offers and demands precision\nabout concepts and measures that may otherwise be lacking in theory.\nBramson et al. (2017), for example, argues that\n“polarization” has a range of possible meanings across the\nliterature in which it appears, different aspects of which are\ncaptured by different computational models with different\nmeasures. \nIn general, the social dynamics of belief change reviewed above treats\nbeliefs as items that spread by contact, much on the model of\ninfection dynamics (Grim, Singer, Reade, & Fisher 2015, though\nRiegler & Douven 2009\ncan be seen as an exception). Other attempts have been made to model\nbelief change in greater detail, motivated by reasons or\narguments. \nWith gestures toward earlier work by Phan Minh Dung (1995), Gregor Betz\nconstructs a model of belief change based on “dialectical\nstructures” of linked arguments (Betz 2013). Sentences and their\nnegations are represented as digits positive and negative, arguments\nas ordered sets of sentences, and two forms of links between\narguments: an attack relation in which a conclusion of one argument\ncontradicts a premise of another and support relations in which the\nconclusion of one argument is equivalent to the premise of another\n (Figure 4).\n A “position” on a dynamical structure, complete or\npartial, consists of an assignment of truth values T or F to the\nelements of the set of sentences involved. Consistent positions\nrelative to a structure are those in which contradictory sentences are\nsigned opposite truth values and every argument in which all premises\nare assigned T has a conclusion which is assigned T as well. Betz then\nmaps the space of coherent positions for a given dialectical structure\nas an undirected network, with links between positions that differ in\nthe truth-value of just one sentence of the set. \n\n \nFigure 4: A dialectical structure of\npropositions and their negations as positive and negative numbers,\nwith two complete positions indicated by values of T and F. The left\nassignment is consistent; the right assignment is not (after Betz\n2013). [An\n extended description of figure 4\n is in the supplement.] \nIn the simplest form of the model, two agents start with random\nassignments to a set of 20 sentences with consistent assignments to\ntheir negations. Arguments are added randomly, starting from a blank\nslate, and agents move to the coherent position closest to their\nprevious position, with a random choice in the case of a draw. In\nvariations on the basic structure, Betz considers (a) cases in which\nan initial background agreement is assumed, (b) cases of\n“controversial” argumentation, in which arguments are\nintroduced which support a proponent’s position or attack an\nopponent’s, and (c) in which up to six agents are involved. In\ntwo series of simulations, he tracks both the consensus-conduciveness\nof different parameters, and—with an assumption of a specific\nassignment as the “truth”—the truth-conduciveness of\ndifferent parameters. \nIn individual runs, depending on initial positions and arguments\nintroduced, Betz finds that argumentation of the sort modeled can\neither increase or decrease agreement, and can track the truth or lead\nastray. Averaging across many debates, however, Betz finds that\ncontroversial argumentation in particular is both consensus-conducive\nand better tracks the\n truth.[6] \nComputational models have been used in philosophy of science in two\nvery different respects: (a) as models of scientific theory, and (b)\nas models of the social interaction characteristic of collective\nscientific research. The next sections review some examples of\neach. \n“Computational philosophy of science” is enshrined as a\nbook title as early as Paul Thagard’s 1988. A central core of\nhis work is a connectionist ECHO program, which constructs network\nstructures of scientific explanation (Thagard 1992, 2012). From inputs\nof “explain”,, “contradict”,\n“data”, and “analogous” for the status and\nrelation of nodes, ECHO uses a set of principles of explanatory\ncoherence to construct a network of undirected excitatory and\ninhibitory links between nodes which “cohere” and those\nwhich “incohere”, respectively. If p1 through pm explain\nq, for example, all of p1 through pm cohere with q and\nwith each other, for example, though the weight of coherence is\ndivided by the number of p1 through pm. If p1 contradicts p2 or p1 and\np2 are parts of competing explanations for the same phenomenon, they\n“incohere”. \nStarting with initial node activations close to zero, the nodes of the\ncoherence network are synchronously updated in terms of their old\nactivation and weighted input from linked nodes, with\n“data” nodes set as a constant input of 1. Once the\nnetwork settles down to equilibrium, an explanatory hypothesis p1 is\ntaken to defeat another p2 if its activation value is higher—at\nleast generally, positive as opposed to negative\n (Figure 5). \nFigure 5: An ECHO network for hypotheses\nP1 and P2 and evidence units Q1 and Q2. Solid lines represent\nexcitatory links, the dotted line an inhibitory link. Because Q1 and\nQ2 are evidence nodes, they take a constant excitatory value of 1 from\nE. Started from values of .01 and following Thagard’s updating,\nP1 dominates P2 once the network has settled down: a hypothesis that\nexplains more dominates its alternative. Adapted from Thagard\n1992. \nThagard is able to show that such an algorithm effectively echoes a\nrange of familiar observations regarding theory selection. Hypotheses\nthat explain more defeat those that explain less, for example, and\nsimpler hypotheses are to be preferred. In contrast to simple\nPopperian refutation, ECHO abandons a hypothesis only when a\ndominating hypothesis is available. Thagard uses the basic approach of\nexplanatory coherence, instantiated in ECHO, in an analysis of a\nnumber of historical cases in the history of science, including the\nabandonment of phlogiston theory in favor of oxygen theory, the\nDarwinian revolution, and the eventual triumph of Wegener’s\nplate tectonics and continental drift. \nThe influence of Bayesian networks has been far more widespread, both\nacross disciplines and in technological application—application\nmade possible only with computers. Grounded in the work of Judea Pearl\n(1988, 2000; Pearl & Mackenzie 2018), Bayesian networks are\ndirected acyclic graphs in which nodes represent variables that can be\nread as either probabilities or degrees of belief and directed edges\nas conditional probabilities from “parent” to\n“child”. By the Markov convention, the value of a node is\nindependent of all other nodes that are not its descendants,\nconditional on its parents. A standard textbook example is shown in\n Figure 6. \nFigure 6: A standard example of a simple\nBayesian net. [An\n extended description of figure 6\n is in the supplement.] \nChanges of values at the nodes of a Bayesian network (in response to\nevidence, for example) are updated through belief propagation\nalgorithms applied at every node. The update of a response to input\nfrom a parent uses the conditional probabilities of the link. A\nparent’s response to input from a child uses the related\nlikelihood ratio (see also the supplement on Bayesian networks in\nBringsjord & Govindarajulu 2018 [2019]). Reading some variables as\nhypotheses and others as pieces of evidence, simple instances of core\nscientific concepts can easily be read off such a structure. Simple\nexplanation amounts to showing how the value of a variable\n“downstream” depends on the pattern\n“upstream”. Simple confirmation amounts to an increase in\nthe probability or degree of belief of a node h upstream\ngiven a piece of evidence e downstream. Evaluating competing\nhypotheses consists in calculating the comparative probability of\ndifferent patterns upstream. One clear reading of networks is as\ncausal graphs. \nAs Pearl notes, a Bayesian network is nothing more than a graphical\nrepresentation of a huge table of joint probabilities for the\nvariables involved (Pearl & Mackenzie 2018: 129). Given any\nsizable number of variables, however, calculation becomes humanly\nunmanageable—hence the crucial use of computers. The fact that\nBayesian networks are so computationally intensive is in fact a point\nthat Thagard makes against using them as models of human cognitive\nprocessing (Thagard 1992: 201). But that is not an objection against\nother philosophical interpretations. Application to philosophical\nquestions of causality in philosophy of science is detailed in\nSpirtes, Glymour, and Scheines (1993) and Sprenger and Hartmann\n(2019). Bayesian networks are now something of a standard in\nartificial intelligence, ubiquitous in its applications, and powerful\nalgorithms have been developed to extract causal networks from the\nmassive amounts of data available. \nIt should be no surprise that the computational studies of belief\nchange and opinion dynamics noted above blend smoothly into a range of\ncomputational studies in philosophy of science. Here a central\nmotivating question has been one of optimal investigatory structure:\nwhat pattern of scientific communication and cooperation, between what\nkinds of investigators, is best positioned to advance science? There\nare two strands of computational philosophy of science that attempt to\nwork toward an answer to this question. The first strand models the\neffect of communicative networks within groups. The second strand,\nleft to the next section, models the effects of cognitive diversity\nwithin groups. This section outlines what makes modeling of both sorts\npromising, but also notes limitations and some failures as well. \nOne might think that access to more data by more investigators would\ninevitably optimize the truth-seeking goals of communities of\ninvestigators. On that intuition, faster and more complete\ncommunication—the contemporary science of the\ninternet—would allow faster, more accurate, and more exploration\nof nature. Surprisingly, however, this first strand of modeling offers\nrobust arguments for the potential benefits of limited\ncommunication. \nIn the spirit of rational choice theory, much of this work was\ninspired by analytical work in economics on infinite populations by\nVenkatesh Bala and Sanjeev Goyal (1998), computationally implemented\nfor small populations in a finite context and with an eye to\nphilosophical implications by Kevin Zollman (2007, 2010a, 2010b). In Zollman’s model, Bayesian agents choose between a\ncurrent method \\(\\phi_1\\) and what is set as a better method\n\\(\\phi_2,\\) starting with random beliefs and allowing agents to pursue\nthe investigatory action with the highest subjective utility. Agents\nupdate their beliefs based on the results of their own testing\nresults—drawn from a distribution for that action—together\nwith results from the other agents to which they are communicatively\nconnected. A community is taken to have successfully learned when all\nagents converge on the better \\(\\phi_2.\\) \nZollman’s results are shown in\n Figure 7\n for the three simple networks shown in\n Figure 8.\n The communication network which performs the best is not the fully\nconnected network in which all investigators have access to all\nresults from all others, but the maximally distributed network\nrepresented by the ring. As Zollman also shows, this is also that\nconfiguration which takes the longest time to achieve convergence. See\n an interactive simulation of a simplified version of Zollman’s model\n in the Other Internet Resources section below. Figure 7: A 10 person ring, wheel, and\ncomplete graph. After Zollman (2010a). \nFigure 8: Learning results of computer\nsimulations: ring, wheel, and complete networks of Bayesian agents.\nAdapted from Zollman (2010a).\n [An\n extended description of figure 8\n is in the supplement.] \nOlsson and Angere’s Bayesian network Laputa (mentioned above)\nhas also been applied to the question of optimal networks for\nscientific communication. Their results essentially confirm\nZollman’s result, though sampled over a larger range of networks\n(Angere & Olsson 2017). Distributed networks with low connectivity\nare those that most reliably fix on the truth, though they are bound\nto do so more slowly. \nThe concept of an epistemic landscape has also emerged as of\ncentral importance in this strand of research. Analogous to a fitness\nlandscape in biology (Wright 1932), an epistemic landscape offers an\nabstract representation of ideal data that might in principle be\nobtained in testing a range of hypotheses (Grim 2006, 2009; Weisberg\n& Muldoon 2009; Hong & Page 2004, Page 2007).\n Figure 9\n uses the example of data that might be obtained by testing\nalternative medical treatments. In such a graph points in the\nchemotherapy-radiation plane represent particular hypotheses about the\nmost effective combination of radiation and chemotherapy. Graph height\nat each location represents some measure of success: the percentage of\npatients with 5-years survival on that treatment, for example. \nFigure 9: A three-dimensional epistemic\nlandscape. Points on the xz plane represent hypotheses regarding\noptimal combination of radiation and chemotherapy; graph height on the\ny axis represents some measure of success. [An\n extended description of figure 9\n is in the supplement.] \nAn epistemic landscape is intended to be an abstract representation of\nthe real-world phenomenon being explored. The key word, of course, is\n“abstract”: few would argue that such a model is fully\nrealistic either in terms of the simplicity of limited dimensions or\nthe precision in which one hypothesis has a distinctly higher value\nthan a close neighbor. As in all modeling, the goal is to represent as\nsimply as possible those aspects of a situation relevant to answering\na specific: in this case, the question of optimal scientific\norganization. Epistemic landscapes—even those this\nsimple—have been assumed to offer a promising start. As outlined\nbelow, however, one of the deeper conclusions that has emerged is how\nsensitive results can be to the specific topography of the epistemic\nlandscape. \nIs there a form of scientific communication which optimizes its\ntruth-seeking goals in exploration of a landscape? In a series of\nagent-based models, agents are communicatively linked explorers\nsituated at specific points on an epistemic landscape (Grim 2006;\nGrim, Singer et al. 2013). In such a design, simulation can be used to\nexplore the effect of network structure, the topography of the\nepistemic landscape, and the interaction of the two. \nThe simplest form of the results echo the pattern seen in different\nforms in Bala and Goyal (1998) and in Zollman\n(2010a, 2010b), here played out on epistemic landscapes. Agents start with\nrandom hypotheses as points on the x-axis of a two-dimensional\nlandscape. They compare their results (the height of the y axis at\nthat point) with those of the other agents to which they are\nnetworked. If a networked neighbor has a higher result, the agent\nmoves toward an approximation of that point (in the interval of a\n“shaking hand”) with an inertia factor (generally 50%, or\na move halfway). The process is repeated by all agents, progressively\nexploring the landscape in attempting to move toward more successful\nresults. \nOn “smooth” landscapes of the form of the first two graphs\nin\n Figure 10,\n agents in any of the networks shown in Figure 10 succeed in finding\nthe highest point on the landscape. Results become much more\ninteresting for epistemic landscapes that contain a “needle in a\nhaystack” as in the third graph in Figure 10. \nFigure 10: Two-dimensional epistemic\nlandscapes. Adapted from Grim (2009). ring radius 1 small world wheel hub random complete Figure 11: Sample networks. \nIn a ring with radius 1, each agent is connected with just its\nimmediate neighbors on each side. Using an inertia of 50% and a\n“shaking hand” interval of 8 on a 100-point landscape, 50\nagents in that configuration converge on the global maximum in the\n“needle in the haystack” landscape in 66% of simulation\nruns. If agents are connected to the two closest neighbors on each\nside, results drop immediately to 50% of runs in which agents find the\nglobal maximum. A small world network can be envisaged as a ring in\nwhich agents have a certain probability of “rewiring”:\nbreaking an existing link and establishing another one to some other\nagent at random (Watts & Strogatz 1998). If each of 50 agents has\na 9% probability of rewiring, the success rate of small worlds drops\nto 55%. Wheels and hubs have a 42% and 37% success rate, respectively.\nRandom networks with a 10% probability of connection between any two\nnodes score at 47%. The worst performing communication network on a\n“needle in a haystack” landscape is the “internet of\nscience” of a complete network in which everyone instantly sees\neveryone else’s result. \nExtensions of these results appear in Grim, Singer et al. (2013).\nThere a small sample of landscapes is replaced with a quantified\n“fiendishness index”, roughly representing the extent to\nwhich a landscape embodies a “needle in a haystack”.\nHigher fiendishness quantifies a lower probability that hill-climbing\nfrom a randomly chosen point “finds” the landscape’s\nglobal maximum. Landscapes, though still two-dimensional, are\n“looped” so as to avoid edge-effects also noted in\nHegselmann and Krause (2006). Here again results emphasize the\nepistemic advantages of ring-like or distributed network over fully\nconnected networks in the exploration of intuitively difficult\nepistemic landscapes. Distributed single rings achieve the highest\npercentage of cases in which the highest point on the landscape is\nfound, followed by all other network configurations. Total or\ncompletely connected networks show the worst results over all. Times\nto convergence are shown to be roughly though not precisely the\ninverse of these relationships. See\n the interactive simulation of a Grim and Singer et al.’s model\n in the Other Internet Resources section below. \nWhat all these models suggest is that it is distributed networks of\ncommunication between investigators, rather than full and immediate\ncommunication between all, that will—or at least\ncan—give us more accurate scientific outcomes. In the\nseventeenth century, scientific results were exchanged slowly, from\nperson to person, in the form of individual correspondence. In\ntoday’s science results are instantly available to everyone.\nWhat these models suggest is that the communication mechanisms of\nseventeenth century science may be more reliable than the highly\nconnected communications of today. Zollman draws the corollary\nconclusion that loosely connected communities made up of less informed\nscientists might be more reliable in seeking the truth than\ncommunities of more informed scientists that are better connected\n (Zollman 2010b). \nThe explanation is not far to seek. In all the models noted, more\nconnected networks produce inferior results because agents move too\nquickly to salient but sub-optimal positions: to local rather than\nglobal maxima. In the landscape models surveyed, connected networks\nresult in all investigators moving toward the same point, currently\nannounced to everyone as highest, skipping over large areas in the\nprocess—precisely where the “needle in the haystack”\nmight be hidden. In more distributed networks, local action results in\na far more even and effective exploration of widespread areas of the\nlandscape; exploration rather than exploitation (Holland 1975). \nHow should we structure the funding and communication structure of our\nscientific communities? It is clear both from these results in their\ncurrent form, and in further work along these general lines, that the\nanswer may well be “landscape”-relative: it may well\ndepend on what kind of question is at issue what form scientific\ncommunication ought to take. It may also depend on what desiderata are\nat issue. The models surveyed emphasize accuracy of results,\nabstractly modeled. All those surveyed concede that there is a clear\ntrade-off between accuracy of results and the speed of community\nconsensus\n (Zollman 2007;\n Zollman 2010b; Grim, Singer et al. 2013). But for many purposes, and\nreasons both ethical and practical, it may often be far better to work\nwith a result that is only roughly accurate but available today than\nto wait 10 years for a result that is many times more accurate but\narrives far too late. \nA second tradition of work in computational philosophy of science also\nuses epistemic landscapes, but attempts to model the effect not of\nnetwork structure but of the division of labor and diversity within\nscientific groups. An influential but ultimately flawed precursor in\nthis tradition is the work of Weisberg and Muldoon (2009). \nTwo views of Weisberg and Muldoon’s landscape appear in\n Figure 12.\n In their treatment, points on the base plane of the landscape\nrepresent “approaches”—abstract representations of\nthe background theories, methods, instruments and techniques used to\ninvestigate a particular research question. Heights at those points\nare taken to represent scientific significance (following Kitcher\n1993). \nFigure 12: Two visions of Weisberg and\nMuldoon’s landscape of scientific significance (height) at\ndifferent approaches to a research topic. \nThe agents that traverse this landscape are not networked, as in the\nearlier studies noted, except to the extent that they are influenced\nby agents with “approaches” near theirs on the landscape.\nWhat is significant about the Weisberg & Muldoon model, however,\nis that their agents are not homogeneous. Two types of agents play a\nprimary role. \n“Followers” take previous investigation of the territory\nby others into account in order to follow successful trends. If any\npreviously investigated points in their immediate neighborhood have a\nhigher significance than the point they stand on, they move to that\npoint (randomly breaking any\n tie).[7]\n Only if no neighboring investigated points have higher significance\nand uninvestigated point remain, followers move to one of those. \n“Mavericks” avoid previously investigated points much as\nfollowers prioritize them. Mavericks choose unexplored points\nin their neighborhoods, testing significance. If higher than their\ncurrent spot, they move to that point. \nWeisberg and Muldoon measure both the percentages of runs in which\ngroups of agents find the highest peak and the speed at which peaks\nare found. They report that the epistemic success of a population of\nfollowers is increased when mavericks are included, and that the\nexplanation for that effect lies in the fact that mavericks can\nprovide pathways for followers: “[m]avericks help many of the\nfollowers to get unstuck, and to explore more fruitful areas of the\nepistemic landscape” (for details see Weisberg & Muldoon\n2009: 247 ff). Against that background they argue for broad claims\nregarding the value for an epistemic community of combining different\nresearch strategies. The optimal division of labor that their model\nsuggests is “a healthy number of followers with a small number\nof mavericks”. \nCritics of Weisberg and Muldoon’s model argue that it is flawed by simple implementation errors in which >= was used in place of >, with the result that their software agents do not in fact operate in accord with their outlined strategies (Alexander, Himmelreich,& Thomson 2015). As\nimplemented, their followers tend to get trapped into oscillating\nbetween two equivalent spaces (often of value 0). According to the critics, when followers are properly implemented, it turns out that mavericks help the success of\na community solely in terms of discovery by the mavericks themselves,\nnot by getting followers “unstuck” who shouldn’t\nhave been stuck in the first place (see also Thoma 2015). If the critics are right, the Weisberg-Muldoon model as originally implemented proves inadequate as philosophical support for the claim that division of labor and strategic diversity are important epistemic drivers. There’s\n an interactive simulation of the Weisberg and Muldoon model, which includes a switch to change the >= to >,\n in the Other Internet Resources section below. \nCritics of the model don’t deny the general conclusion that\nWeisberg and Muldoon draw: that cognitive diversity or division of\ncognitive labor can favor social epistemic\n outcomes.[8]\n What they deny is that the Weisberg and Muldoon model adequately\nsupports that conclusion. A particularly intriguing model that does\nsupport that conclusion, built on a very different model of diversity,\nis that of Hong and Page (2004). But it also supports a point that\nAlexander et al. emphasize: that the advantages of cognitive\ndiversity can very much depend on the epistemic landscape being\nexplored. \nLu Hong and Scott Page work with a two-dimensional landscape of 2000\npoints, wrapped around as a loop. Each point is assigned a random\nvalue between 1 and 100. Their epistemic individuals explore that\nlandscape using heuristics composed of three ordered numbers between,\nsay, 1 and 12. An example helps. Consider an individual with heuristic\n\\(\\langle 2, 4, 7\\rangle\\) at point 112 on the landscape. He first\nuses his heuristic 2 to see if a point two to the right—at\n114—has a higher value than his current position. If so, he\nmoves to that point. If not, he stays put. From that point, whichever\nit is, he uses his heuristic 4 in order to see if a point 4 steps to\nthe right has a higher peak, and so forth. An agent circles through\nhis heuristic numbers repeatedly until he reaches a point from which\nnone within reach of his heuristic offers a higher value. The basic\ndynamic is illustrated in\n Figure 13. \nFigure 13: An example of exploration of\na landscape by an individual using heuristics as in Hong and Page\n(2004). Explored points can be read left to right. [An\n extended description of figure 13\n is in the supplement.] \nHong and Page score individuals on a given landscape in terms of the\naverage height they reach starting from each of the 2000 points. But\ntheir real target is the value of diversity in groups. With that in\nmind, they compare the performance of (a) groups composed of the 9\nindividuals with highest-scoring heuristics on a given landscape with\n(b) groups composed of 9 individuals with random heuristics on that\nlandscape. In each case groups function together in what has been\ntermed a “relay”. For each point on the 2000-point\nlandscape, the first individual of the group finds his highest\nreachable value. The next individual of the group starts from there,\nand so forth, circling through the individuals until a point is\nreached at which none can achieve a higher value. The score for the\ngroup as a whole is the average of values achieved in such a way\nacross all of the 2000 points \nWhat Hong and Page demonstrate in simulation is that groups with\nrandom heuristics routinely outperform groups composed entirely of the\n“best” individual performers. They christen their findings\nthe “Diversity Trumps Ability” result. In a replication of\ntheir study, the average maximum on the 2000-point terrain for the\ngroup of the 9 best individuals comes in at 92.53, with a median of\n92.67. The average for a group of 9 random individuals comes in at\n94.82, with a median of 94.83. Across 1000 runs in that replication, a\nhigher score was achieved by groups of random agents in 97.6% of all\ncases (Grim et al. 2019). See\n an interactive simulation of Hong and Page’s group deliberation model\n in the Other Internet Resources section below. Hong and Page also\noffer a mathematical theorem as a partial explanation of such a result\n(Hong & Page 2004). That component of their work has been attacked\nas trivial or irrelevant (Thompson 2014), though the attack itself has\ncome under criticism as well (Kuehn 2017, Singer 2019). \nThe Hong-Page model solidly demonstrates a general claim attempted in\nthe disputed Weisberg-Muldoon model: cognitive diversity can indeed be a\nsocial epistemic advantage. In application, however, the Hong-Page\nresult has sometimes been appealed to as support for much broader\nclaims: that diversity is always or quite generally of epistemic\nadvantage (Anderson 2006, Landemore 2013, Gunn 2014, Weymark 2015).\nThe result itself is limited in ways that have not always been\nacknowledged. In particular, it proves sensitive to the precise\ncharacter of the epistemic landscape employed. \nHong and Page’s landscape is one in which each of 2000 points is\ngiven a random value between 1 and 100: a purely random landscape. One\nconsequence of that fact is that the group of 9 best heuristics on\ndifferent random Hong-Page landscapes have essentially no correlation:\na high-performing individual on one landscape need have no carry-over\nto another. Grim et al. (2019) expands the Hong-Page model to\nincorporate other landscapes as well, in ways which challenge the\ngeneral conclusions regarding diversity that have been drawn from the\nmodel but which also suggest the potential for further interesting\napplications. \nAn easy way to “smooth” the Hong-Page landscapes is to\nassign random values not to every point on the 2000-point loop but\nevery second point, for example, with intermediate points taking an\naverage between those on each side. Where a random landscape has a\n“smoothness” factor of 0, this variation will have a\nrandomness factor of 1. A still “smoother” landscape of\ndegree 2 would be one in which slopes are drawn between random values\nassigned to every third point. Each degree of smoothness increases the\naverage value correlation between a point and its neighbors. Grim et\nal. consider landscapes of varying “smoothness” along\nroughly these lines, though with a randomization that avoids the\nlock-step intervals suggested (Grim et al. 2019). \nUsing Hong and Page’s parameters in other respects, it turns out\nthat the “Diversity Trumps Ability” result holds only for\nlandscapes with a smoothness factor less than 4. Beyond that point, it\nis “ability”—the performance of groups of the 9\nbest-performing individuals—that trumps\n“diversity”—the performance of groups of random\nheuristics. \nThe Hong-Page result is therefore very sensitive to the\n“smoothness” of the epistemic landscape modeled. As hinted\nin\n section 3.2.2,\n this is an indication from within the modeling tradition itself of\nthe danger of restricted and over-simple abstractions regarding\nepistemic landscapes. Moreover, the model’s sensitivity is not\nlimited to landscape smoothness: social epistemic success depends on\nthe pool of numbers from which heuristics are drawn as well, with\n“diversity” showing strength on smoother landscapes if the\npool of heuristics is expanded. Results also depend on whether social\ninteraction is modeled using of Hong-Page’s “relay”\nor an alternative dynamics in which individuals collectively (rather\nthan sequentially) announce their results, with all moving to the\nhighest point announced by any. Different landscape smoothnesses,\ndifferent heuristic pool sizes, and different interactive dynamics\nwill favor the epistemic advantages of different compositions of\ngroups, with different proportions of random and best-performing\nindividuals (Grim et al. 2019). \nWhat, then, is the conduct that ought to be adopted, the reasonable\ncourse of conduct, for this egoistic, naturally unsocial being, living\nside by side with similar beings? —Henry\nSidgwick, Outlines of the History\nof Ethics (1886: 162) \nHobbes’ Leviathan can be read as asking, with Sidgwick,\nhow cooperation can emerge in a society of egoists (Hobbes 1651).\nCooperation is thus a central theme in both ethics and\nsocial-political philosophy. \nGame theory has been a major tool in many of the philosophical\nconsiderations of cooperation, extended with computational\nmethodologies. Here the primary example is the Prisoner’s\nDilemma, a strategic interaction between two agents with a payoff\nmatrix in which joint cooperation gets a higher payoff than joint\ndefection, but the highest payoff goes to a player who defects when\nthe other player cooperates (see esp. Kuhn 1997 [2019]). Formally, the\nPrisoner’s Dilemma requires the value DC for defection against\ncooperation to be higher than CC for joint cooperation, with CC higher\nthan the payoff CD for cooperation against defection. In order to\navoid an advantage to alternating trade-offs, CC should also be higher\nthan \\((\\textrm{CD} + \\textrm{DC}) / 2.\\) A simple set of values that\nfits those requirements is shown in the matrix in\n Figure 14. \nFigure 14: A Prisoner’s Dilemma\npayoff matrix \nIt is clear in the “one-shot” Prisoner’s Dilemma\nthat defection is strictly dominant: whether the other player\ncooperates or defects, one gains more points by defecting. But if\ndefection always gives a higher payoff, what sense does it make to\ncooperate? In a Hobbesian population of egoists, with payoffs as in\nthe Prisoner’s Dilemma, it would seem that we should expect\nmutual defection as both a matter of course and the rational\noutcome—Hobbes’ “war of all against all”. How\ncould a population of egoists come to cooperate? How could the ethical\ndesideratum of cooperation arise and persist? \nA number of mechanisms have been shown to support the emergence of\ncooperation: kin selection (Fisher 1930; Haldane 1932), green beards\n(Hamilton 1964a,b; Dawkins 1976), secret handshakes (Robson 1990;\nWiseman & Yilankaya 2001), iterated games, spatialized and\nstructured interactions (Grim 1995; Skyrms 1996, 2004; Grim, Mar, & St. Denis\n1998; Alexander 2007), and noisy signals (Nowak & Sigmund\n1992). This section offers examples of the last two of these. \nIn the iterated Prisoner’s Dilemma, players repeat their\ninteractions, either in a fixed number of rounds or in an infinite or\nindefinite repetition. Robert Axelrod’s tournaments in the early\n1980s are the classic studies in the iterated prisoner’s\ndilemma, and early examples of the application of computational\ntechniques. Strategies for playing the Prisoner’s Dilemma were\nsolicited from experts in various fields, pitted against all others\n(and themselves) in round-robin competition over 200 rounds. Famously,\nthe strategy that triumphed was Tit for Tat, a simple strategy which\nresponds to cooperation from the other player on the previous round\nwith cooperation, responding to defection on the previous round with\ndefection. Even more surprisingly, Tit for Tat again came out in front\nin a second tournament, despite the fact that submitted strategies\nknew that Tit for Tat was the opponent to aim for. When those same\nstrategies were explored with replicator dynamics in place of\nround-robin competition, Tit for Tat again was the winner (Axelrod and\nHamilton 1981). Further work has tempered Tit for Tat’s\nreputation somewhat, emphasizing the constraints of Axelrod’s\ntournaments both in terms of structure and the strategies submitted\n(Kendall, Yao, & Chang 2007; Kuhn 1997 [2019]). \nA simple set of eight “reactive” strategies, in which a\nplayer acts solely on the basis of the opponent’s previous move,\nis shown in\n Figure 15.\n Coded with “1” for cooperate and “0” for\ndefect and three places representing first move i, response\nto cooperation on the other side c, and response to defection\non the other side d, these give us 8 strategies that include\nall defect, all cooperate, tit for tat as well as several other\nvariations.  \nFigure 15: 8 reactive strategies in the\nPrisoner’s Dilemma \nIf these strategies are played against each other and themselves, in\nthe manner of Axelrod’s tournaments, it is “all\ndefect” that is the clear winner. If agents imitate the most\nsuccessful strategy, a population will thus immediately go to All\nDefect—a game-theoretic image of Hobbes’ war of all\nagainst all, perhaps. \nConsider, however, a spatialized Prisoner’s Dilemma in the form\nof cellular automata, easily run and analyzed on a computer. Cells are\nassigned one of these eight strategies at random, play an iterated\ngame locally with their eight immediate neighbors in the array, and\nthen adopt the strategy of that neighbor (if any) that achieves a\nhigher total score. In this case, with the same 8 strategies,\noccupation of the array starts with a dominance by All Defect, but\nclusters of Tit for Tat grow to dominate the space\n (Figure 16).\n An interactive simulation in which one can choose which competing reactive strategies play in a spatialized array is available in the Other Internet Resources section\nbelow. \nFigure 16: Conquest by Tit for Tat in\nthe Spatialized Prisoner’s Dilemma. All defect is shown in\ngreen, Tit for Tat in gray (Grim, Mar, & St. Denis 1998) \nIn this case, there are two aspects to the emergence of cooperation in\nthe form of Tit for Tat. One is the fact that play is local:\nstrategies total points over just local interactions, rather than play\nwith all other cells. The other is that imitation is local as well:\nstrategies imitate their most successful neighbor, rather than that\nstrategy in the array that gained the most points. The fact that both\nconditions play out in the local structure of the lattice allows\nclusters of Tit for Tat to form and grow. In Axelrod’s\ntournaments it is particularly important that Tit for Tat does well in\nplay against itself; the same is true here. If either game interaction\nor strategy updating is made global rather than local, dominance goes\nto All Defect instead. One way in which cooperation can emerge, then,\nis through structured interactions (Grim 1995; Skyrms 1996, 2004; Grim, Mar, & St. Denis\n1998). Alexander (2007) offers a particularly thorough\ninvestigation of different interaction structures and different\ngames. \nMartin Nowak and Karl Sigmund offer a further variation that results\nin an even more surprising level of cooperation in the\nPrisoner’s Dilemma (Nowak & Sigmund 1992). The reactive\nstrategies outlined above are communicatively perfect strategies.\nThere is no noise in “hearing” a move as cooperation or\ndefection on the other side, and no “shaking hand” in\nresponse. In Tit for Tat a cooperation on the other side is flawlessly\nperceived as such, for example, and is perfectly responded to with\ncooperation. If signals are noisy or responses are less than flawless,\nhowever, Tit for Tat loses its advantage in play against itself. In\nthat case a chancy defection will set up a chain of mutual defections\nuntil a chancy cooperation reverses the trend. A “noisy”\nTit for Tat played against itself in an infinite game does no better\nthan a random strategy. \nNowak and Sigmund replace the “perfect” strategies of\n Figure 14\n with uniformly stochastic ones, reflecting a world of noisy signals\nand actions. The closest to All Defect will now be a strategy .01,\n.01, .01, indicating a strategy that has only a 99% chance of\ndefecting initially and in response to either cooperation or\ndefection. The closest to Tit for Tat will be a strategy .99, .99,\n.01, indicating merely a high probability of starting with cooperation\nand responding to cooperation with cooperation, defection with\ndefection. Using the mathematical fiction of an infinite game, Nowak\nand Sigmund are able to ignore the initial value. \nPitting a full range of stochastic strategies of this type against\neach other in a computerized tournament, using replicator dynamics in\nthe manner of Axelrod and Hamilton (1981), Nowak and Sigmund trace a\nprogressive evolution of strategies. Computer simulation shows\nimperfect All Defect to be an early winner, followed by Imperfect Tit\nfor Tat. But at that point dominance in the population goes to a still\nmore cooperative strategy which cooperates with cooperation 99% of the\ntime but cooperates even against defection 10% of the time. That\nstrategy is eventually dominated by one that cooperates against\ndefection 20% of the time, and then by one that cooperates against\ndefection 30% of the time. A replication of the Nowak and Sigmund\nresult is shown in\n Figure 17.\n Nowak and Sigmund show analytically that the most successful strategy\nin a world of noisy information will be “Generous Tit for\nTat”, with probabilities of \\(1 - ε\\) and 1/3 for\ncooperation against cooperation and defection respectively. \nFigure 17: Evolution toward Nowak and\nSigmund’s “Generous Tit for Tat” in a world of\nimperfect information (Nowak & Sigmund 1992). Population\nproportions are shown vertically for labelled strategies shown over\n12,000 generations for an initial pool of 121 stochastic strategies\n\\(\\langle c,d\\rangle\\) at .1 intervals, full value of 0 and 1 replaced\nwith 0.01 and 0.99 (Grim, Mar, & St. Denis 1998). [An\n extended description of figure 17\n is in the supplement.] \nHow can cooperation emerge in a society of self-serving egoists? In\nthe game-theoretic context of the Prisoner’s Dilemma, these\nresults indicate that iterated interaction, spatialization and\nstructured interaction, and noisy information can all facilitate\ncooperation, at least in the form of strategies such as Tit for Tat.\nWhen all three effects are combined, the result appears to be a level\nof cooperation even greater than that indicated in Nowak and Sigmund.\nWithin a spatialized Prisoner’s Dilemma using stochastic\nstrategies, it is strategies in the region of probabilities \\(1 -\nε\\) and 2/3 that emerge as optimal in the sense of having the\nhighest scores in play against themselves without being open to\ninvasion from small clusters of other strategies (Grim 1996; Grim, Mar\n& St. Denis 1998). \nThis outline has focused on some basic background regarding the\nPrisoner’s Dilemma and emergence of cooperation. More recently a\ngeneration of richer game-theoretic models has appeared, using a wider\nvariety of games of conflict and coordination and more closely tied to\nhistorical precedents in social and political philosophy. Newer\ngame-theoretic analyses of state of nature scenarios in Hobbes appear\nin Vanderschraaf (2006) and Chung (2015), extended with simulation to\ninclude Locke and Nozick in Bruner (forthcoming). \nThere is also a new body of work that extends game-theoretic modeling\nand simulation to questions of social inequity. Bruner (2017) shows that\nthe mere fact that one group is a minority in a population, and thus\ninteracts more frequently with majority than with minority members,\ncan result in its being disadvantaged where exchanges are\ncharacterized by bargaining in a Nash demand game (Young 1993). Termed\nthe “cultural Red King”, the effect has been further\nexplored through simulation, with links to experiment, and with\nextensions to questions of “intersectional disadvantage”,\nin which overlapping minority categories are in play (O’Connor\n2017;\n Mohseni, O’Connor, & Rubin 2019 [Other Internet Resources];\n O’Connor, Bright, & Bruner 2019). The relevance of this to\nthe focus of the previous section is made clear in Rubin and\nO’Connor (2018) and O’Connor and Bruner (2019), modeling\nminority disadvantage in scientific communities. \nIn computational simulations, game-theoretic cooperation has been\nappealed to as a model for aspects of both ethics in the sense of\nSidgwick and social-political philosophy on the model of Hobbes. That\nmodel is tied to game-theoretic assumptions in general, however, and\noften to the structure of the Prisoner’s Dilemma in particular\n(though Skyrms 2003 and Alexander 2007 are notable\nexceptions). With regard to a wide range of questions in social and\npolitical philosophy in particular, the limitations of game theory may\nseem unhelpfully abstract and artificial. \nWhile still abstract, there are other attempts to model questions in\nsocial political philosophy computationally. Here the studies\nmentioned earlier regarding polarization are relevant. There have also\nbeen recent attempts to address questions regarding epistemic\ndemocracy: the idea that among its other virtues, democratic\ndecision-making is more likely to track the truth. \nThere is a contrast, however, between open democratic decision-making,\nin which a full population takes part, and representative democracy,\nin which decision-making is passed up through a hierarchy of\nrepresentation. There is also a contrast between democracy seen as\npurely a matter of voting and as a deliberative process that in some\nway involves a population in wider discussion (Habermas 1992 [1996];\nAnderson 2006; Landemore 2013). \nFigure 18: The Condorcet result:\nprobability of a majority of different odd-numbered sizes being\ncorrect on a binary question with different homogeneous probabilities\nof individual members being correct. [An\n extended description of figure 18\n is in the supplement.] \nThe classic result for an open democracy and simple voting is the\nCondorcet jury theorem (Condorcet 1785). As long as each voter has a\nuniform an independent probability greater than 0.5 of getting an\nanswer right, the probability of a correct answer from a majority vote\nis significantly higher than that of any individual, and it quickly\nincreases with the size of the population\n (Figure 18). \nIt can be shown analytically that the basic thrust of the Condorcet\nresult remains when assumptions regarding uniform and independent\nprobabilities are relaxed (Boland, Proschan, & Tong 1989; Dietrich\n& Spiekermann 2013). The Condorcet result is significantly\nweakened, however, when applied in hierarchical representation, in\nwhich smaller groups first reach a majority verdict which is then\ncarried to a second level of representatives who use a majority vote\non that level (Boland 1989). More complicated questions regarding\ndeliberative dynamics and representation require simulation using\ncomputers. \nThe Hong-Page structure of group deliberation, outlined in the context\nof computational philosophy of science above, can also be taken as a\nmodel of “deliberative democracy” beyond a simple vote.\nThe success of deliberation in a group can be measured as the average\nvalue height of points found. In a representative instantiation of\nthis kind of deliberation, smaller groups of individuals first use\ntheir individual heuristics to explore a landscape collectively, then\nhanding their collective “best” for each point on the\nlandscape to a representative. In a second round of deliberation, the\nrepresentatives work from the results from their constituents in a\nsecond round of exploration. \nUnlike in the case of pure voting and the Condorcet result,\ncomputational simulations show that the use of a representative\nstructure does not dull the effect of deliberation on this model:\naverage scores for three groups of three in a representative structure\nare if anything slightly higher than average scores from an open\ndeliberation involving 9 agents (Grim, Bramson et al. forthcoming).\nResults like these show how computational models might help expand the\npolitical philosophical arguments for representative democracy. \nSocial and political philosophy appears to be a particularly promising\narea for big data and computational philosophy employing the data\nmining tools of computational social science, but as of this writing\nthat development remains largely a promise for the future. \nThe guiding idea of the interdisciplinary theme known as\n“complex systems” is that phenomena on a higher level can\n“emerge” from complex interactions on a lower level\n(Waldrop 1992, Kauffman 1995, Mitchell 2011, Krakauer 2019). The\nemergence of social outcomes from the interaction of individual\nchoices is a natural target, and agent-based modeling is a natural\ntool. \nOpinion polarization and the evolution of cooperation, outlined above,\nboth fit this pattern. A further classic example is the work of Thomas\nC. Schelling on residential segregation. A glance at demographic maps\nof American cities makes the fact of residential segregation obvious:\nethnic and racial groups appear as clearly distinguished patches\n (Figure 19).\n Is this an open and shut indication of rampant racism in American\nlife? \nFigure 19: A demographic map of Los\nAngeles. White households are shown in red, African-American in\npurple, Asian-American in green, and Hispanic in orange.\n (Fischer 2010 in Other Internet Resources)\n  \nSchelling attempted an answer to this question with an agent-based\nmodel that originally consisted of pennies and dimes on a checkerboard\narray (Schelling 1971, 1978), but which has been studied\ncomputationally in a number of variations. Two types of agents\n(Schelling’s pennies and dimes) are distributed at random across\na cellular automata lattice, with given preferences regarding their\nneighbors. In its original form, each agent has a threshold regarding\nneighbors of “their own kind”. At that threshold level and\nabove, agents remain in place. Should they not have that number of\nlike neighbors, they move to another spot (in some variations, a move\nat random, in others a move to the closest spot that satisfies their\nthreshold). \nWhat Schelling found was that residential segregation occurs even\nwithout a strong racist demand that all of one’s neighbors, or\neven most, are “of one’s kind”. Even when preference\nis that just a third of one’s neighbors are “of\none’s kind”, clear patches of residential segregation\nappear. The iterated evolution of such an array is shown in\n Figure 20.\n See\n the interactive simulation of this residential segregation model\n in the Other Internet Resources section below.  \nFigure 20: Emergence of residential\nsegregation in the Schelling model with preference threshold set at\n33% \nThe conclusion that Schelling is careful to draw from such a model is\nsimply that a low level of preference can be sufficient for\nresidential segregation. It does not follow that more egregious social\nand economic factors aren’t operative or even dominant in the\nresidential segregation we actually observe.  \nIn this case basic modeling assumptions have been challenged on\nempirical grounds. Elizabeth Bruch and Robert Mare use sociological\ndata on racial preferences, challenging the sharp cut-off employed in\nthe Schelling model (Bruch & Mare 2006). They claim on the basis\nof simulation that the Schelling effect disappears when more\nrealistically smooth preference functions are used instead. Their\nsimulations and the latter claim turn out to be in error (van de Rijt,\nSiegel, & Macy 2009), but the example of testing the robustness of\nsimple models with an eye to real data remains a valuable one. \nComputational modeling has been applied in philosophy of language\nalong two main lines. First, there are investigations of analogy and\nmetaphor using models of semantic webs that share a developmental\nhistory with some of the models of scientific theory outlined above.\nSecond, there are investigations of the emergence of signaling, which\nhave often used a game-theoretic base akin to some approaches to the\nemergence of cooperation discussed above. \nWordNet is a computerized lexical database for English built by George\nMiller in 1985 with a hierarchical structure of semantic categories\nintended to reflect empirical observations regarding human processing.\nA category “bird” includes a sub-category\n“songbirds” with “canary” as a particular, for\nexample, intended to explain the fact that subjects could more quickly\nprocess “canaries sing”—which involves traversing\njust one categorical step—than they could process\n“canaries fly” (Miller, Beckwith, Fellbaum, Gross, &\nMiller 1990). \nThere is a long tradition, across psychology, linguistics, and\nphilosophy, in which analogy and metaphor are seen as an important key\nto abstract reasoning and creativity (Black 1962; Hesse 1943 [1966];\nLakoff & Johnson 1980; Gentner 1982; Lakoff & Turner 1989).\nBeginning in the 1980s several notable attempts have been made to\napply computational tools in order to both understand and generate\nanalogies. Douglas Hofstadter and Melanie Mitchell’s Copycat,\ndeveloped as a model of high-level cognition, has\n“codelets” compete within a network in order to answer\nsimple questions of analogy: “abc is to abd as ijk is to\nwhat?” (Hofstadter 2008). Holyoak and Thagard envisage metaphors\nas analogies in which the source and target domain are semantically\ndistinct, calling for relational comparison between two semantic nets\n(Holyoak & Thagard 1989, 1995; see also Falkenhainer, Forbus,\n& Gentner 1989). In the Holyoak and Thagard model those\ncomparisons are constrained in a number of different ways that call\nfor coherence; their computational modeling for coherence in the case\nof metaphor was in fact a direct ancestor to Thagard’s coherence\nmodeling of scientific theory change discussed above (Thagard 1988,\n1992). \nEric Steinhart and Eva Kittay’s\n NETMET (see Other Internet Resources)\n offers an illustration of the relational approach to analogy and\nmetaphor. They use one semantic and inferential subnet related to\nbirth another related to the theory of ideas in the Theatetus. Each\nsubnet is categorized in terms of relations of containment,\nproduction, discarding, helping, passing, expressing and opposition.\nOn that basis NETMET generates metaphors including “Socrates is\na midwife”, “the mind is an intellectual womb”,\n“an idea is a child of the mind”, “some ideas are\nstillborn”, and the like (Steinhart 1994; Steinhart & Kittay\n1994). NETMET can be applied to large linguistic databases such as\nWordNet. \nSuppose we start without pre-existing meaning. Is it possible that\nunder favorable conditions, unsophisticated learning dynamics can\nspontaneously generate meaningful signaling? The answer is\naffirmative. —Brian Skyrms,\nSignals (2010: 19) \nDavid Lewis’ sender-receiver game is a cooperative game in which\na sender observes a state of nature and chooses a signal, a receiver\nobserves that signal and chooses an act, with both sender and receiver\nbenefiting from an appropriate coordination between state of nature\nand act (Lewis 1969). A number of researchers have explored both\nanalytic and computational models of signaling games with an eye to\nways in which initially arbitrary signals can come to function in ways\nthat start to look like meaning. \nCommunication can be seen as a form of cooperation, and here as in the\ncase of the emergence of cooperation the methods of (communicative)\nstrategy change seem less important than the interactive structure in\nwhich those strategies play out. Computer simulations show that simple\nimitation of a neighbor’s successful strategy, various forms of\nreinforcement learning, and training up of simple neural nets on\nsuccessful neighbors’ behaviors can all result in the emergence\nand spread of signaling systems, sometimes with different dialects\n(Zollman 2005; Grim, St.\nDenis & Kokalis 2002; Grim, Kokalis, Alai-Tafti, Kilb & St. Denis, 2004).[9]\n Development on a cellular automata grid produces communication with\nany of these techniques, even when the rewards are one-sided rather\nthan mutual in a strict Lewis signaling game, but structures of\ninteraction that facilitate communication can also co-evolve with the\ncommunication they facilitate as well (Skyrms 2010). Elliot Wagner\nextends the study of communication on interaction structures to other\nnetworks as well (Wagner 2009). \nOn an interpretation in terms of biological evolution, computationally\nemergent signaling of this sort can be seen as modeling communication\nin Vervet monkeys (Cheney & Seyfarth 1990) or even chemical\n“signals” in bacteria (Berleman, Scott, Chumley, &\nKirby 2008). If interpreted in terms of learned culture, particularly\nwith an eye to more complex signal combination, these have been\noffered as models of mechanisms at play in the development of human\nlanguage (Skyrms 2010).\n A simple interactive model in which signaling emerges in a situated population of agents harvesting food sources and avoiding predators\n is available in the Other Internet Resources section below. \nMany of our examples of computational philosophy have been examples of\nsimulation—often social simulation by way of agent-based\nmodeling. But there is also a strong tradition in which computation is\nused not in simulations but as a way of mechanizing and extending\nphilosophical argument (typically understood as deductive proof), with\napplications in philosophy of logic and ultimately in deontic logic,\nmetaphysics, and philosophy of\n religion.[10] \nEntitling a summer Dartmouth conference in 1956, the organizers coined\nthe term “artificial intelligence”. One of the high points\nof that conference was a computational program for the construction of\nlogical proofs, developed by Allen Newell and Herbert Simon at\nCarnegie Mellon and programmed by J. C. Shaw using the vacuum tubes of\nthe JOHNNIAC computer at the Institute for Advanced Study (Bringsjord\n& Govindarajulu 2018 [2019]). Newell and Simon’s\n“Logic Theorist” was given 52 theorems from chapter two of\nWhitehead and Russell’s Principia Mathematica (1910, 1912, 1913), of which it successfully\nproved 38, including a proof more elegant than one of Whitehead and\nRussell’s own (MacKenzie 1995, Loveland 1984, Davis 1957\n[1983]). Russell himself was impressed: \nI am delighted to know that Principia Mathematica can now be\ndone by machinery… I am quite willing to believe that\neverything in deductive logic can be done by machinery. (letter to\nHerbert Simon, 2 November 1956; quoted in O’Leary 1991: 52)  \nDespite possible claims to anticipation, the most compelling of which\nmay be Martin Davis’s 1950 computer implementation of Mojsesz\nPresburger’s decision procedure for a fragment of arithmetic\n(Davis 1957), the Logic Theorist is standardly regarded as the first\nautomated theorem-prover. Newell and Simon’s target, however,\nwas not so much a logic prover as a proof of concept for an\nintelligent or thinking machine. Having rejected geometrical proof as\ntoo reliant on diagrams, and chess as too hard, by Simon’s own\naccount they turned to logic because Principia Mathematica\nhappened to be on his\n shelf.[11] \nSimon and Newell’s primary target was not an optimized\ntheorem-prover but a “thinking machine” that in some way\nmatched human intelligence. They therefore relied in heuristics\nthought of as matching human strategies, an approach later ridiculed\nby Hao Wang: \nThere is no need to kill a chicken with a butcher’s knife, yet\nthe net impression is that Newell-Shaw-Simon failed even to kill the\nchicken…to argue the superiority of “heuristic”\nover algorithmic methods by choosing a particularly inefficient\nalgorithm seems hardly just. (Wang 1960: 3) \nLater theorem-provers were focused on proof itself rather than a model\nof human reasoning. By 1960 Hao Wang, Paul Gilmore, and Dag Prawitz\nhad developed computerized theorem-provers for the full first-order\npredicate calculus (Wang 1960, MacKenzie 1995). In the 1990s William\nMcCune developed Otter, a widely distributed and accessible prover for\nfirst-order logic (McCune & Wos 1997, Kalman 2001). A more recent\nincarnation is Prover9, coupled with search for models and\ncounter-examples in Mace4. Examples of Prover9 derivations are offered in Other Internet Resources. A contemporary alternative is Vampire, developed by Andrei Voronkov, Kryštof Hodere, and Alexander Rizanov (Riazanov & Voronkov 2002). \nTheorem-provers developed for higher-order logics, working from a\nvariety of approaches, include TPS (Andrews and Brown 2006), Leo-II\nand -III (Benzmüller, Sultana, Paulson, & Theiß 2015;\nSteen & Benzmüller 2018), and perhaps most prominently HOL\nand particularly development-friendly\n Isabelle/HOL\n (Gordon & Melham 1993; Paulson 1990). With clever implementation\nand extension, these also allow automation of aspects of modal,\ndeontic, epistemic, intuitionistic and paraconsistent logics, of\ninterest both in their own terms and in application within computer\nscience, robotics, and artificial intelligence (McRobbie 1991; Abe,\nAkama, & Nakamatsu 2015). \nWithin pure logic, Portararo (2001 [2019]) lists a number of results\nthat have been established using automated theorem-provers. It was\nconjectured for 50 years that a particular equation in a Robbins\nalgebra could be replaced by a simpler one, for example. Even Tarski\nhad failed in the attempt at proof, but McCune produced an automated\nproof in 1997 (McCune 1997). Shortest and simplest axiomatizations for\nimplicational fragments of modal logics S4 and S5 had been studied for\nyears as open questions, with eventual results by automated reasoning\nin 2002 (Ernst, Fitelson, Harris, & Wos\n 2002).[12] \nTheorem provers have been applied within deontic logics in the attempt\nto mechanize ethical reasoning and decision-making (Meyer &\nWierenga 1994; Van Den Hoven & Lokhorst 2002; Balbiani, Broersen,\n& Brunel 2009; Governatori & Sartor 2010; Benzmüller,\nParent, & van der Torre 2018; Benzmüller, Farjami, &\nParent, 2018). Alan Gewirth has argued that agents contradict their\nstatus as agents if they don’t accept a principle of generic\nconsistency—respecting the agency-necessary rights of\nothers—as a supreme principle of practical rationality (Gewirth\n1978; Beyleveld 1992, 2012). Fuenmayor and Benzmüller have shown\nthat even an ethical theory of this complexity can be formally encoded\nand assessed computationally (Fuenmayor & Benzmüller\n2018). \nOne of the major advances in computational philosophy has been the\napplication of theorem-provers to the analysis of classical\nphilosophical positions and arguments. From axioms of a metaphysical\nobject theory, Zalta and his collaborators use Prover9 and Mace to\nestablish theorems regarding possible worlds, such as the claim that\nevery possible world is maximal, modal theorems in Leibniz, and\nconsequences from Plato’s theory of Forms (Fitelson & Zalta\n2007; Alama, Oppenheimer, & Zalta 2015; Kirchner, Benzmüller,\n& Zalta 2019). \nVersions of the ontological argument have formed an important thread\nin recent work employing theorem provers, both because of their\ninherent interest and the technical challenges they bring with them.\nProver9 and Mace have again been used recently by Jack Horner in order\nto analyze a version of the ontological argument in Spinoza’s\nEthics (found invalid) and to propose an alternative (Horner\n2019). Significant work has been done on versions of Anselm’s\nontological argument (Oppenheimer & Zalta 2011; Garbacz 2012;\nRushby 2018). Christoph Benzmüller and his colleagues have\napplied higher-order theorem provers, including including Isabelle/HOL\nand their own Leo-II and Leo-III, in order to analyze a version of the\nontological argument found in the papers of Kurt Gödel\n(Benzmüller & Paelo 2016a, 2016b; Benzmüller, Weber,\n& Paleo 2017; Benzmüller & Fuenmayor 2018). A previously\nunnoticed inconsistency was found in Gödel’s original,\nthough avoided in Dana Scott’s transcription. Theorem-provers\nconfirmed that Gödel’s argument forces modal\ncollapse—all truths become necessary truths. Analysis with\ntheorem-provers makes it clear that variations proposed by C. Anthony\nAnderson and Melvin Fitting avoid that consequence, but in importantly\ndifferent ways (Benzmüller & Paleo 2014; Kirchner,\nBenzmüller, & Zalta\n 2019).[13] \nWork in metaphysics employing theorem-provers continues. Here of\nparticular note is Ed Zalta’s ambitious and long-term attempt to\nground metaphysics quite generally in computationally instantiated\nobject theory (Fitelson & Zalta 2007; Zalta 2020).\n A link to Zalta’s project can be found in the Other Internet Resources section below.\n  \nThe Dartmouth conference of 1956 is standardly taken as marking the\ninception of both the field and the term “artificial\nintelligence” (AI). There were, however, two distinct\ntrajectories apparent in that conference. Some of the participants\ntook as their goal to be the development of intelligent or thinking\nmachines, with perhaps an understanding of human processing as a\nbegrudging means to that end. Others took their goal to be a\nphilosophical and psychological understanding of human processing,\nwith the development of machines a means to that end. Those in the\nfirst group were quick to exploit linear programming: what came to be\nknown as “GOFAI”, or “good old-fashioned artificial\nintelligence”. Those in the second group rejoiced when\nconnectionist and neural net architectures came to maturity several\ndecades later, promising models directly built on and perhaps\nreflective of mechanisms in the human brain (Churchland 1995). \nAttempts to understand perception, conceptualization, belief change,\nand intelligence are all part of philosophy of mind. The use of\ncomputational models toward that end—the second strand\nabove—thus comes close to computational philosophy of mind.\nDaniel Dennett has come close to saying that AI is philosophy\nof mind: “a most abstract inquiry into the possibility of\nintelligence or knowledge” (Dennett 1979: 60; Bringsjord &\nGovindarajulu 2018 [2019]). \nThe bulk of AI research remains strongly oriented toward producing\neffective and profitable information processing, whether or not the\nresult offers philosophical understanding. So it is perhaps better not\nto identify AI with philosophy of mind, though AI has often been\nguided by philosophical conceptions and aspects of AI have proven\nfruitful for philosophical exploration. Philosophy of AI and\nphilosophy of mind inspired by and in response to\nAI, which are not the topic here, have both been far more common than\nphilosophy of mind developed with the techniques of AI. \nOne example of a program in artificial intelligence that was\nexplicitly conceived in philosophical terms and designed for\nphilosophical ends was the OSCAR project, developed by John Pollock\nbut cut short by his death (Pollock 1989, 1995, 2006). The goal of\nOSCAR was construction of a computational agent: an “artificial\nintellect”. At the core of OSCAR was implementation of a theory\nof rationality. Pollock was explicit regarding the intersection of AI\nand philosophy of mind in that project:  \nThe implementability of a theory of rationality is a necessary\ncondition for its correctness. This amounts to saying that philosophy\nneeds AI just as much as AI needs philosophy. (Pollock 1995: xii;\nBringsjord & Govindarajulu 2018 [2019]) \nAt the core of OSCAR’s rationality is implementation of\ndefeasible non-monotonic logic employing prima facie reasons and\npotential defeaters. Among its successes, Pollock claims an ability to\nhandle the lottery paradox and preface paradoxes. Informally, the fact\nthat we know that one of the many tickets in a lottery will win means\nthat we must treat “ticket 1 will not win…”,\n“ticket 2 will not win…” and the like not as items\nof knowledge but as defeasible beliefs for which we have strong prima\nfacie reasons. Pollock’s formal treatment in terms of collective\ndefeat is nicely outlined in a supplement on OSCAR in Bringsjord &\nGovindarajulu (2018 [2019]). \nThe sections above were intended to be an introduction to\ncomputational philosophy largely by example, emphasizing both the\nvariety of computational techniques employed and the spread of\nphilosophical topics to which they are applied. This final section is\ndevoted to the problems and prospects of computational philosophy. \nAlthough computational instantiations of logic are of an importantly\ndifferent character, simulation—including agent-based\nsimulation—plays a major role in much of computational\nphilosophy. Beyond philosophy, across all disciplines of its\napplication, simulation often raises suspicions. \nA standard suspicion of simulation in various fields is that it one\n“can prove anything” by manipulation of model structure\nand parameters. The worry is that an anticipated or desired effect\ncould always be “baked in”, programmed as an artefact of\nthe model itself. Production of a simulation would thus demonstrate\nnot the plausibility of a hypothesis or a fact about the world but\nmerely the cleverness of the programmer. In a somewhat different\ncontext, Rodney Brooks has written that the problem with simulations\nis that they are “doomed to succeed” (Brooks & Mataric\n1993). \nBut consider a similar critique of logical argument: that one\n“can prove anything” by careful choice of premises and\nrules of inference. The proper response in the case of logical\nargument is to concede the fact that a derivation for any proposition\ncan be produced from carefully chosen premises and rules, but to\nemphasize that it may be difficult or impossible to produce a\nderivation from agreed rules and clear and plausible premises. \nA similar response is appropriate here. The effectiveness of\nsimulation as argument depends on the strength of its assumptions and\nthe soundness of its mechanisms just as the effectiveness of logical\nproof depends on the strength of its premises and the validity of its\nrules of inference. The legitimate force of the critique, then, is not\nthat simulation is inherently untrustworthy but simply that the\nassumptions of any simulation are always open to further\nexamination. \nAnyone who has attempted computer simulation can testify that it is\noften extremely difficult or impossible to produce an expected effect,\nparticularly a robust effect across a plausible range of parameters\nand with a plausible basic mechanism. Like experiment, simulation can\ndemonstrate both the surprising fragility of a favored hypothesis and\nthe surprising robustness of an unexpected effect. \nFar from being “doomed to succeed”, simulations fail quite\nregularly in several important ways (Grim, Rosenberger, Rosenfeld,\nAnderson, & Eason 2013). Two standard forms of simulation failure\nare failure of verification and failure of validation (Kleijnen 1995;\nWindrum, Fabiolo, & Moneta 2007; Sargent 2013). Verification of a\nmodel demands assuring that it accurately reflects design intention.\nIf a computational model is intended to instantiate a particular\ntheory of belief change, for example, it fails verification if it does\nnot accurately represent the dynamics of that theory. Validation is\nperhaps the more difficult demand, particularly for philosophical\ncomputation: that the computational model adequately reflects those\naspects of the real world it is intended to capture or explain. \nIf its critics are right, a simple example of verification failure is the original Weisberg and\nMuldoon model of scientific exploration outlined above (Weisberg &\nMuldoon 2009). The model was intended to include two kinds of\nepistemic agents—followers and mavericks—with distinct\npatterns of exploration. Mavericks avoid previously investigated\npoints in their neighborhood. Followers move to neighboring points\nthat have been investigated but that have a higher significance. In\ncontrast to their description in the text, the critics argue, the software\nfor the model used “>=” in place of “>”\nat a crucial place, with the result that followers moved to\nneighboring points with a higher or equal significance, resulting in\ntheir often getting stuck in a very local oscillation (Alexander, Himmelreich, & Thomson 2015). If so, Weisberg and\nMuldoon’s original model fails to match its design\nintention—it fails verification—though some of their\ngeneral conclusions regarding epistemic diversity have been vindicated\nin further studies. \nValidation is a very different and more difficult demand: that a\nsimulation model adequately captures relevant aspects of what it is\nintended to model. A common critique of specific models is that they\nare too simple, leaving out some crucial aspect of the modeled\nphenomenon. When properly targeted, this can be an entirely\nappropriate critique. But what it calls for is not the abandonment of\nmodeling but better construction of a better model. \nIn time…the Cartographers Guilds struck a Map of the Empire\nwhose size was that of the Empire, and which coincided point for point\nwith it. The following Generations, saw that that vast Map was\nUseless…. (Jorge Luis Borges, “On Exactitude in\nScience”, 1946 [1998 English translation: 325]) \nBorges’ story is often quoted in illustration of the fact that\nno model—and no scientific theory—can include all\ncharacteristics of what it is intended to model (Weisberg 2013).\nModels and theories would be useless if they did: the purpose of both\ntheories and models is to present simpler representations or\nmechanisms that capture the relevant features or dynamics of\na phenomenon. What aspects of a phenomenon are in fact the relevant\naspects for understanding that phenomenon calls for evaluative input\noutside of the model. But where relevant aspects are omitted,\nirrelevant aspects included, or unrealistic or artificial constraints\nimposed, what a critique calls for is a better model (Martini &\nPinto 2017; Thicke forthcoming). \nThere is one aspect of validation that can sometimes be gauged at the\nlevel of modeling itself and with modeling tools alone. Where the\ntarget is some general phenomenon—opinion polarization or the\nemergence of communication, for example—a model which produces\nthat phenomenon within only a tiny range of parameters should be\nsuspicious. Our estimate of the parameters actually in play in the\nactual phenomenon may be merely intuitive or extremely rough, and the\nreal phenomenon may be ubiquitous in a wide range of settings. In such\na case, it would seem prima facie unlikely that a model which produced\na parallel effect within only a tiny window of parameters could be\ncapturing the general mechanism of a general phenomenon. In such cases\nrobustness testing is called for, a test for one aspect of validation\nthat can still be performed on the computer. To what extent do\nconclusions drawn from the modeling effect hold up under a range of\nparameter variations? \nThe Hong-Page model of the value of diversity in exploration, outlined\nabove, has been widely appealed to quite generally as support for\ncognitive diversity in groups. It has been cited in NASA internal\ndocuments, offered in support of diversity requirements at UCLA, and\nappears in an amicus curiae brief before the Supreme Court in\nsupport of promoting diversity in the armed forces (Fisher v. Univ. of\nTexas 2016). But the model is not robust across its several parameters\nto support sweepingly general claims that have been made on its basis\nregarding diversity and ability or expertise (Grim et al. 2019). Is that a problem internal to the model, or an external matter\nof its interpretation or application? There is much to be said for the\nlatter alternative. The model is and remains an interesting\none—interesting often in the ways in which it does show\nsensitivity to different parameters. Thus a failure of one aspect of\nvalidation—robustness—with an eye to one type of general\nclaim can also call for further modelling: modeling intended to\nexplore different effects in different contexts. Rosenstock, Bruner,\nand O’Connor (2017) offer a robustness test for the Zollman\nmodel outlined above. Borg, Frey, Šešelja, and\nStraßer (2018) offer new modeling grounded precisely in a\nrobustness critique of their predecessors. \nIt is noteworthy that the simulation failures mentioned have been\ndetected and corrected within the literature of simulation itself.\nThese are effective critiques within disciplines employing simulation,\nrather than from outside. An illustration of a such a case with both\nverification and validation in play is that of the Bruch and Mare\ncritique of the Schelling segregation model and the response to it in\nvan Rooij, Siegel, and Macy (Schelling 1971, 1978; Bruch & Mare\n2006; van de Rijt, Siegel, & Macy 2009). Many aspects of that\nmodel are clearly artificial: a limitation to two groups,\nspatialization on a cellular automata grid, and\n“unhappiness” or moving in terms of a sharp threshold\ncut-off of tolerance for neighbors of the other group. Bruch and Mare\noffered clear empirical evidence that residential preferences do not\nfit a sharp threshold. More importantly, they built a variation of the\nSchelling model in order to show that the Schelling effect disappeared\nwith more realistic preference profiles. What Bruch and Mare\nchallenged, in other words, was validation: not merely that\naspects of the target phenomenon of residential segregation were left\nout (as they would be in any model), but that relevant aspects were\nleft out: differences that made an important difference. Van de Rijt,\nSiegel, and Macy failed to understand why the smooth preference curves\nin Bruch and Mare’s data wouldn’t support rather than\ndefeat a Schelling effect. On investigation they found that they\nwould: Bruch and Mare’s validation claim against Schelling was\nitself founded in a programming error. De Rijt, Siegel and\nMacy’s verdict was that Bruch and Mare’s attack itself\nfailed model verification. \nIn the case of both Weisberg and Muldoon, and Bruch and Mare, original\ncode was made freely available to their critics. In both cases, the\noriginal authors recognized the problems revealed, though emphasizing\naspects of their work that survived the criticisms. Here again an\nimportant point is that critiques and responses of this type have\narisen and been addressed within philosophical and scientific\nsimulation itself, working toward better models and practices. \nPhilosophy at its best has always been in contact with the conceptual\nand scientific methodologies of its time. Computational philosophy can\nbe seen as a contemporary instantiation of that contact, crossing\ndisciplinary boundaries in order to both influence and benefit from\ndevelopments in computer science and artificial intelligence.\nIncorporation of new technologies and wider application within\nphilosophy can be expected and should be hoped for. \nThere is one extremely promising area in need of development within\ncomputational philosophy, though that area may also call for changes\nin conceptions of philosophy itself. Philosophy has classically been\nconceived as abstract rather than concrete, as seeking understanding\nat the most general level rather than specific prediction or\nretrodiction, often normative, and as operating in terms of logical\nargument and analysis rather than empirical data. The last of these\ncharacteristics, and to some extent the first, will have to be\nqualified if computational philosophy grows to incorporate a major\nbatch of contemporary techniques: those related to big data. \nExpansion of computational philosophy in the intersection with big\ndata seems an exciting prospect for social and political philosophy,\nin the analysis of belief change, and in understanding the social and\nhistorical dynamics of philosophy of science (Overton 2013; Pence\n& Ramsey 2018). A particular benefit would be better prospects for\nvalidation of a range of simulations and agent-based models, as\nemphasized above (Mäs 2019; Reijula & Kuorikoski 2019). If\ncomputational philosophy moves in that promising direction, however,\nit may take on a more empirical character in some respects. Emphasis\non general and abstract understanding and concern with the normative\nwill remain marks of a philosophical approach, but the membrane\nbetween some topic areas in philosophy and aspects of computational\nscience can be expected to become more permeable. \nDissolving these disciplinary boundaries may itself be a good in some\nrespects. The examples presented above make it clear that in\nincorporating (and contributing to) computational techniques developed\nin other areas, computational philosophy has long been\ncross-disciplinary. If our gain is a better understanding of the\ntopics that have long fascinated us, compromise in disciplinary\nboundaries and a change in our concept of philosophy seem a small\nprice to pay.","contact.mail":"singerd@phil.upenn.edu","contact.domain":"phil.upenn.edu"}]
