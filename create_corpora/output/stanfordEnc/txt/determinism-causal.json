[{"date.published":"2003-01-23","date.changed":"2016-01-21","url":"https://plato.stanford.edu/entries/determinism-causal/","author1":"Carl Hoefer","entry":"determinism-causal","body.text":"\n\n\nCausal determinism is, roughly speaking, the idea that every event is\nnecessitated by antecedent events and conditions together with the\nlaws of nature. The idea is ancient, but first became subject to\nclarification and mathematical analysis in the eighteenth century.\nDeterminism is deeply connected with our understanding of the physical\nsciences and their explanatory ambitions, on the one hand, and with\nour views about human free action on the other. In both of these\ngeneral areas there is no agreement over whether determinism is true\n(or even whether it can be known true or false), and what the import\nfor human agency would be in either case. \n\nIn most of what follows, I will speak simply of determinism,\nrather than of causal determinism. This follows recent\nphilosophical practice of sharply distinguishing views and theories of\nwhat causation is from any conclusions about the success or failure of\ndeterminism (cf. Earman, 1986; an exception is Mellor 1994). For the\nmost part this disengagement of the two concepts is appropriate. But\nas we will see later, the notion of cause/effect is not so easily\ndisengaged from much of what matters to us about determinism. \nTraditionally determinism has been given various, usually imprecise\ndefinitions. This is only problematic if one is investigating\ndeterminism in a specific, well-defined theoretical context; but it is\nimportant to avoid certain major errors of definition. In order to get\nstarted we can begin with a loose and (nearly) all-encompassing\ndefinition as follows: \nThe italicized phrases are elements that require further explanation\nand investigation, in order for us to gain a clear understanding of\nthe concept of determinism. \nThe roots of the notion of determinism surely lie in a very common\nphilosophical idea: the idea that everything can, in principle, be\nexplained, or that everything that is, has a sufficient\nreason for being and being as it is, and not otherwise. In other\nwords, the roots of determinism lie in what Leibniz named the\nPrinciple of Sufficient Reason. But since precise physical theories\nbegan to be formulated with apparently deterministic character, the\nnotion has become separable from these roots. Philosophers of science\nare frequently interested in the determinism or indeterminism of\nvarious theories, without necessarily starting from a view about\nLeibniz' Principle. \nSince the first clear articulations of the concept, there has been a\ntendency among philosophers to believe in the truth of some sort of\ndeterminist doctrine. There has also been a tendency, however, to\nconfuse determinism proper with two related notions:\npredictability and fate. \nFatalism is the thesis that all events (or in some versions, at least\nsome events) are destined to occur no matter what we do. The source of\nthe guarantee that those events will happen is located in the will of\nthe gods, or their divine foreknowledge, or some intrinsic\nteleological aspect of the universe, rather than in the unfolding of\nevents under the sway of natural laws or cause-effect relations.\nFatalism is therefore clearly separable from determinism, at least to\nthe extent that one can disentangle mystical forces and gods' wills\nand foreknowledge (about specific matters) from the notion of\nnatural/causal law. Not every metaphysical picture makes this\ndisentanglement possible, of course. But as a general matter, we can\nimagine that certain things are fated to happen, without this being\nthe result of deterministic natural laws alone; and we can imagine the\nworld being governed by deterministic laws, without anything at all\nbeing fated to occur (perhaps because there are no gods, nor\nmystical/teleological forces deserving the titles fate or\ndestiny, and in particular no intentional determination of\nthe “initial conditions” of the world). In a looser sense,\nhowever, it is true that under the assumption of determinism, one\nmight say that given the way things have gone in the past,\nall future events that will in fact happen are already\ndestined to occur. \nPrediction and determinism are also easy to disentangle, barring\ncertain strong theological commitments. As the following famous\nexpression of determinism by Laplace shows, however, the two are also\neasy to commingle: \nIn this century,\n Karl Popper (1982)\n defined determinism in terms of predictability also, in his book\nThe Open Universe. \nLaplace probably had God in mind as the powerful intelligence to whose\ngaze the whole future is open. If not, he should have: 19th\nand 20th century mathematical studies showed convincingly\nthat neither a finite, nor an infinite but embedded-in-the-world\nintelligence can have the computing power necessary to predict the\nactual future, in any world remotely like ours. But even if our aim is\nonly to predict a well-defined subsystem of the world, for a limited\nperiod of time, this may be impossible for any reasonable finite agent\nembedded in the world, as many studies of chaos (sensitive dependence\non initial conditions) show. Conversely, certain parts of the world\ncould be highly predictable, in some senses, without the\nworld being deterministic. When it comes to predictability of future\nevents by humans or other finite agents in the world, then,\npredictability and determinism are simply not logically connected at\nall.  \nThe equation of “determinism”with\n“predictability” is therefore a façon de\nparler that at best makes vivid what is at stake in determinism:\nour fears about our own status as free agents in the world. In\nLaplace's story, a sufficiently bright demon who knew how things stood\nin the world 100 years before my birth could predict every action,\nevery emotion, every belief in the course of my life. Were she then to\nwatch me live through it, she might smile condescendingly, as one who\nwatches a marionette dance to the tugs of strings that it knows\nnothing about. We can't stand the thought that we are (in some sense)\nmarionettes. Nor does it matter whether any demon (or even God) can,\nor cares to, actually predict what we will do: the existence of the\nstrings of physical necessity, linked to far-past states of\nthe world and determining our current every move, is what alarms us.\nWhether such alarm is actually warranted is a question well outside\nthe scope of this article (see Hoefer (2002a), Ismael (2016) and the\nentries on\n free will\n and\n incompatibilist theories of freedom).\n But a clear understanding of what determinism is, and how we might be\nable to decide its truth or falsity, is surely a useful starting point\nfor any attempt to grapple with this issue. We return to the issue of\nfreedom in section 6,\n Determinism and Human Action,\n below. \nRecall that we loosely defined causal determinism as follows, with\nterms in need of clarification italicized: \nWhy should we start so globally, speaking of the world, with\nall its myriad events, as deterministic? One might have thought that a\nfocus on individual events is more appropriate: an event E is\ncausally determined if and only if there exists a set of prior events\n{A, B, C …} that constitute a\n(jointly) sufficient cause of E. Then if all—or even\njust most—events E that are our human actions\nare causally determined, the problem that matters to us, namely the\nchallenge to free will, is in force. Nothing so global as states of\nthe whole world need be invoked, nor even a complete\ndeterminism that claims all events to be causally\ndetermined. \nFor a variety of reasons this approach is fraught with problems, and\nthe reasons explain why philosophers of science mostly prefer to drop\nthe word “causal” from their discussions of determinism.\nGenerally, as John Earman quipped (1986), to go this route is to\n“… seek to explain a vague\nconcept—determinism—in terms of a truly obscure\none—causation.” More specifically, neither philosophers'\nnor laymen's conceptions of events have any correlate in any\nmodern physical\n theory.[1]\n The same goes for the notions of cause and sufficient\ncause. A further problem is posed by the fact that, as is now\nwidely recognized, a set of events {A, B, C\n…} can only be genuinely sufficient to produce an\neffect-event if the set includes an open-ended ceteris\nparibus clause excluding the presence of potential disruptors\nthat could intervene to prevent E. For example, the start of\na football game on TV on a normal Saturday afternoon may be sufficient\nceteris paribus to launch Ted toward the fridge to grab a\nbeer; but not if a million-ton asteroid is approaching his house at\n.75c from a few thousand miles away, nor if his phone is\nabout to ring with news of a tragic nature, …, and so on.\n Bertrand Russell\n famously argued against the notion of cause along these lines (and\nothers) in 1912, and the situation has not changed. By trying to\ndefine causal determination in terms of a set of prior sufficient\nconditions, we inevitably fall into the mess of an open-ended list of\nnegative conditions required to achieve the desired sufficiency. \nMoreover, thinking about how such determination relates to free\naction, a further problem arises. If the ceteris paribus\nclause is open-ended, who is to say that it should not include the\nnegation of a potential disruptor corresponding to my freely deciding\nnot to go get the beer? If it does, then we are left saying\n“When A, B, C, … Ted will then\ngo to the fridge for a beer, unless D or E or\nF or … or Ted decides not to do so.” The\nmarionette strings of a “sufficient cause” begin to look\nrather tenuous. \nThey are also too short. For the typical set of prior events that can\n(intuitively, plausibly) be thought to be a sufficient cause of a\nhuman action may be so close in time and space to the agent, as to not\nlook like a threat to freedom so much as like enabling conditions. If\nTed is propelled to the fridge by {seeing the game's on; desiring to\nrepeat the satisfactory experience of other Saturdays; feeling a bit\nthirsty; etc}, such things look more like good reasons to have\ndecided to get a beer, not like external physical events far\nbeyond Ted's control. Compare this with the claim that {state of the\nworld in 1900; laws of nature} entail Ted's going to get the beer: the\ndifference is dramatic. So we have a number of good reasons for\nsticking to the formulations of determinism that arise most naturally\nout of physics. And this means that we are not looking at how a\nspecific event of ordinary talk is determined by previous events; we\nare looking at how everything that happens is determined by\nwhat has gone before. The state of the world in 1900 only entails that\nTed grabs a beer from the fridge by way of entailing the entire\nphysical state of affairs at the later time. \nThe typical explication of determinism fastens on the state of the\n(whole) world at a particular time (or instant), for a variety of\nreasons. We will briefly explain some of them. Why take the state of\nthe whole world, rather than some (perhaps very large) region, as our\nstarting point? One might, intuitively, think that it would be enough\nto give the complete state of things on Earth, say, or\nperhaps in the whole solar system, at t, to fix what happens\nthereafter (for a time at least). But notice that all sorts of\ninfluences from outside the solar system come in at the speed of\nlight, and they may have important effects. Suppose Mary looks up at\nthe sky on a clear night, and a particularly bright blue star catches\nher eye; she thinks “What a lovely star; I think I'll stay\noutside a bit longer and enjoy the view.” The state of the solar\nsystem one month ago did not fix that that blue light from Sirius\nwould arrive and strike Mary's retina; it arrived into the solar\nsystem only a day ago, let's say. So evidently, for Mary's actions\n(and hence, all physical events generally) to be fixed by the state of\nthings a month ago, that state will have to be fixed over a much\nlarger spatial region than just the solar system. (If no physical\ninfluences can go faster than light, then the state of things must be\ngiven over a spherical volume of space 1 light-month in radius.) \nBut in making vivid the “threat” of determinism, we often\nwant to fasten on the idea of the entire future of the world\nas being determined. No matter what the “speed limit” on\nphysical influences is, if we want the entire future of the world to\nbe determined, then we will have to fix the state of things over all\nof space, so as not to miss out something that could later come in\n“from outside” to spoil things. In the time of Laplace, of\ncourse, there was no known speed limit to the propagation of physical\nthings such as light-rays. In principle light could travel at any\narbitrarily high speed, and some thinkers did suppose that it was\ntransmitted “instantaneously.” The same went for the force\nof gravity. In such a world, evidently, one has to fix the state of\nthings over the whole of the world at a time t, in\norder for events to be strictly determined, by the laws of nature, for\nany amount of time thereafter. \nIn all this, we have been presupposing the common-sense Newtonian\nframework of space and time, in which the world-at-a-time is an\nobjective and meaningful notion. Below when we discuss determinism in\nrelativistic theories we will revisit this assumption. \nFor a wide class of physical theories (i.e., proposed sets of laws of\nnature), if they can be viewed as deterministic at all, they can be\nviewed as bi-directionally deterministic. That is, a\nspecification of the state of the world at a time t, along\nwith the laws, determines not only how things go after t, but\nalso how things go before t. Philosophers, while not exactly\nunaware of this symmetry, tend to ignore it when thinking of the\nbearing of determinism on the free will issue. The reason for this is\nthat we tend to think of the past (and hence, states of the world in\nthe past) as done, over, fixed and beyond our control.\nForward-looking determinism then entails that these past\nstates—beyond our control, perhaps occurring long before humans\neven existed—determine everything we do in our lives. It then\nseems a mere curious fact that it is equally true that the state of\nthe world now determines everything that happened in the\npast. We have an ingrained habit of taking the direction of both\ncausation and explanation as being past—-present, even when\ndiscussing physical theories free of any such asymmetry. We will\nreturn to this point shortly. \nAnother point to notice here is that the notion of things being\ndetermined thereafter is usually taken in an unlimited\nsense—i.e., determination of all future events, no matter how\nremote in time. But conceptually speaking, the world could be only\nimperfectly deterministic: things could be determined only,\nsay, for a thousand years or so from any given starting state of the\nworld. For example, suppose that near-perfect determinism were\nregularly (but infrequently) interrupted by spontaneous particle\ncreation events, which occur only once every thousand years in a\nthousand-light-year-radius volume of space. This unrealistic example\nshows how determinism could be strictly false, and yet the world be\ndeterministic enough for our concerns about free action to be\nunchanged. \nIn the loose statement of determinism we are working from, metaphors\nsuch as “govern” and “under the sway of” are\nused to indicate the strong force being attributed to the laws of\nnature. Part of understanding determinism—and especially,\nwhether and why it is metaphysically important—is getting clear\nabout the status of the presumed laws of nature. \nIn the physical sciences, the assumption that there are fundamental,\nexceptionless laws of nature, and that they have some strong sort of\nmodal force, usually goes unquestioned. Indeed, talk of laws\n“governing” and so on is so commonplace that it takes an\neffort of will to see it as metaphorical. We can characterize the\nusual assumptions about laws in this way: the laws of nature are\nassumed to be pushy explainers. They make things happen\nin certain ways , and by having this power, their existence lets\nus explain why things happen in certain ways. (For a defense\nof this perspective on laws, see Maudlin (2007)). Laws, we might say,\nare implicitly thought of as the cause of everything that\nhappens. If the laws governing our world are deterministic, then in\nprinciple everything that happens can be explained as following from\nstates of the world at earlier times. (Again, we note that even though\nthe entailment typically works in the future→past direction also,\nwe have trouble thinking of this as a legitimate explanatory\nentailment. In this respect also, we see that laws of nature are being\nimplicitly treated as the causes of what happens: causation,\nintuitively, can only go past→future.) \nInterestingly, philosophers tend to acknowledge the apparent threat\ndeterminism poses to free will, even when they explicitly reject the\nview that laws are pushy explainers. Earman (1986), for example,\nadvocates a theory of laws of nature that takes them to be simply the\nbest system of regularities that systematizes all the events in\nuniversal history. This is the Best Systems Analysis (BSA), with roots\nin the work of Hume, Mill and Ramsey, and most recently refined and\ndefended by David Lewis (1973, 1994) and by Earman (1984, 1986). (cf.\nentry on\n laws of nature).\n Yet he ends his comprehensive Primer on Determinism with a\ndiscussion of the free will problem, taking it as a still-important\nand unresolved issue. Prima facie this is quite puzzling, for\nthe BSA is founded on the idea that the laws of nature are\nontologically derivative, not primary; it is the events of universal\nhistory, as brute facts, that make the laws be what they are, and\nnot vice-versa. Taking this idea seriously, the actions of\nevery human agent in history are simply a part of the universe-wide\npattern of events that determines what the laws are for this world. It\nis then hard to see how the most elegant summary of this pattern, the\nBSA laws, can be thought of as determiners of human actions. The\ndetermination or constraint relations, it would seem, can go one way\nor the other, not both. \nOn second thought however it is not so surprising that broadly Humean\nphilosophers such as Ayer, Earman, Lewis and others still see a\npotential problem for freedom posed by determinism. For even if human\nactions are part of what makes the laws be what they are, this does\nnot mean that we automatically have freedom of the kind we\nthink we have, particularly freedom to have done otherwise\ngiven certain past states of affairs. It is one thing to say that\neverything occurring in and around my body, and everything\neverywhere else, conforms to Maxwell's equations and thus the Maxwell\nequations are genuine exceptionless regularities, and that because\nthey in addition are simple and strong, they turn out to be laws. It\nis quite another thing to add: thus, I might have chosen to do\notherwise at certain points in my life, and if I had, then Maxwell's\nequations would not have been laws. One might try to defend this\nclaim—unpalatable as it seems intuitively, to ascribe ourselves\nlaw-breaking power—but it does not follow directly from a Humean\napproach to laws of nature. Instead, on such views that deny laws most\nof their pushiness and explanatory force, questions about determinism\nand human freedom simply need to be approached afresh. \nA second important genre of theories of laws of nature holds that the\nlaws are in some sense necessary. For any such approach, laws\nare just the sort of pushy explainers that are assumed in the\ntraditional language of physical scientists and free will theorists.\nBut a third and growing class of philosophers holds that (universal,\nexceptionless, true) laws of nature simply do not exist.\nAmong those who hold this are influential philosophers such as Nancy\nCartwright, Bas van Fraassen, and John Dupré. For these\nphilosophers, there is a simple consequence: determinism is a false\ndoctrine. As with the Humean view, this does not mean that concerns\nabout human free action are automatically resolved; instead, they must\nbe addressed afresh in the light of whatever account of physical\nnature without laws is put forward. See Dupré (2001) for one\nsuch discussion. \nWe can now put our—still vague—pieces together.\nDeterminism requires a world that (a) has a well-defined state or\ndescription, at any given time, and (b) laws of nature that are true\nat all places and times. If we have all these, then if (a) and (b)\ntogether logically entail the state of the world at all other\ntimes (or, at least, all times later than that given in (a)), the\nworld is deterministic. Logical entailment, in a sense broad enough to\nencompass mathematical consequence, is the modality behind the\ndetermination in “determinism.” \nHow could we ever decide whether our world is deterministic or not?\nGiven that some philosophers and some physicists have held firm\nviews—with many prominent examples on each side—one would\nthink that it should be at least a clearly decidable\nquestion. Unfortunately, even this much is not clear, and the\nepistemology of determinism turns out to be a thorny and multi-faceted\nissue. \nAs we saw above, for determinism to be true there have to be\nsome laws of nature. Most philosophers and scientists since the\n17th century have indeed thought that there are. But in the\nface of more recent skepticism, how can it be proven that there are?\nAnd if this hurdle can be overcome, don't we have to know, with\ncertainty, precisely what the laws of our world are, in order\nto tackle the question of determinism's truth or falsity? \nThe first hurdle can perhaps be overcome by a combination of\nmetaphysical argument and appeal to knowledge we already have of the\nphysical world. Philosophers are currently pursuing this issue\nactively, in large part due to the efforts of the anti-laws minority.\nThe debate has been most recently framed by Cartwright in The\nDappled World (Cartwright 1999) in terms psychologically\nadvantageous to her anti-laws cause. Those who believe in the\nexistence of traditional, universal laws of nature are\nfundamentalists; those who disbelieve are\npluralists. This terminology seems to be becoming standard\n(see Belot 2001), so the first task in the epistemology of determinism\nis for fundamentalists to establish the reality of laws of nature (see\nHoefer 2002b). \nEven if the first hurdle can be overcome, the second, namely\nestablishing precisely what the actual laws are, may seem daunting\nindeed. In a sense, what we are asking for is precisely what\n19th and 20th century physicists sometimes set\nas their goal: the Final Theory of Everything. But perhaps, as Newton\nsaid of establishing the solar system's absolute motion, “the\nthing is not altogether desperate.” Many physicists in the past\n60 years or so have been convinced of determinism's falsity, because\nthey were convinced that (a) whatever the Final Theory is, it will be\nsome recognizable variant of the family of quantum mechanical\ntheories; and (b) all quantum mechanical theories are\nnon-deterministic. Both (a) and (b) are highly debatable, but the\npoint is that one can see how arguments in favor of these positions\nmight be mounted. The same was true in the 19th century,\nwhen theorists might have argued that (a) whatever the Final Theory\nis, it will involve only continuous fluids and solids governed by\npartial differential equations; and (b) all such theories are\ndeterministic. (Here, (b) is almost certainly false; see Earman\n(1986),ch. XI). Even if we now are not, we may in future be in a\nposition to mount a credible argument for or against determinism on\nthe grounds of features we think we know the Final Theory must\nhave. \nDeterminism could perhaps also receive direct\nsupport—confirmation in the sense of probability-raising, not\nproof—from experience and experiment. For theories (i.e.,\npotential laws of nature) of the sort we are used to in physics, it is\ntypically the case that if they are deterministic, then to the extent\nthat one can perfectly isolate a system and repeatedly impose\nidentical starting conditions, the subsequent behavior of the systems\nshould also be identical. And in broad terms, this is the case in many\ndomains we are familiar with. Your computer starts up every time you\nturn it on, and (if you have not changed any files, have no anti-virus\nsoftware, re-set the date to the same time before shutting down, and\nso on …) always in exactly the same way, with the same speed\nand resulting state (until the hard drive fails). The light comes on\nexactly 32 µsec after the switch closes (until the day\nthe bulb fails). These cases of repeated, reliable behavior obviously\nrequire some serious ceteris paribus clauses, are never\nperfectly identical, and always subject to catastrophic failure at\nsome point. But we tend to think that for the small deviations,\nprobably there are explanations for them in terms of\ndifferent starting conditions or failed isolation, and for the\ncatastrophic failures, definitely there are explanations in\nterms of different conditions. \nThere have even been studies of paradigmatically “chancy”\nphenomena such as coin-flipping, which show that if starting\nconditions can be precisely controlled and outside\ninterferences excluded, identical behavior results (see Diaconis,\nHolmes & Montgomery 2004). Most of these bits of evidence for\ndeterminism no longer seem to cut much ice, however, because of faith\nin quantum mechanics and its indeterminism. Indeterminist physicists\nand philosophers are ready to acknowledge that macroscopic\nrepeatability is usually obtainable, where phenomena are so\nlarge-scale that quantum stochasticity gets washed out. But they would\nmaintain that this repeatability is not to be found in experiments at\nthe microscopic level, and also that at least some failures\nof repeatability (in your hard drive, or coin-flipping experiments)\nare genuinely due to quantum indeterminism, not just failures to\nisolate properly or establish identical initial conditions. \nIf quantum theories were unquestionably indeterministic, and\ndeterministic theories guaranteed repeatability of a strong form,\nthere could conceivably be further experimental input on the question\nof determinism's truth or falsity. Unfortunately, the existence of\n Bohmian quantum theories\n casts strong doubt on the former point, while chaos theory\ncasts strong doubt on the latter. More will be said about each of\nthese complications below. \nIf the world were governed by strictly deterministic laws, might it\nstill look as though indeterminism reigns? This is one of the\ndifficult questions that chaos theory raises for the epistemology of\ndeterminism. \nA deterministic chaotic system has, roughly speaking, two salient\nfeatures: (i) the evolution of the system over a long time period\neffectively mimics a random or stochastic process—it lacks\npredictability or computability in some appropriate sense; (ii) two\nsystems with nearly identical initial states will have radically\ndivergent future developments, within a finite (and typically, short)\ntimespan. We will use “randomness” to denote the first\nfeature, and “sensitive dependence on initial conditions”\n(SDIC) for the latter. Definitions of chaos may focus on either or\nboth of these properties; Batterman (1993) argues that only (ii)\nprovides an appropriate basis for defining chaotic systems. \nA simple and very important example of a chaotic system in both\nrandomness and SDIC terms is the Newtonian dynamics of a pool table\nwith a convex obstacle (or obstacles) (Sinai 1970 and others). See\n Figure 1. \nFigure 1: Billiard table with convex\nobstacle \nThe usual idealizing assumptions are made: no friction, perfectly\nelastic collisions, no outside influences. The ball's trajectory is\ndetermined by its initial position and direction of motion. If we\nimagine a slightly different initial direction, the\ntrajectory will at first be only slightly different. And collisions\nwith the straight walls will not tend to increase very rapidly the\ndifference between trajectories. But collisions with the convex object\nwill have the effect of amplifying the differences. After\nseveral collisions with the convex body or bodies, trajectories that\nstarted out very close to one another will have become wildly\ndifferent—SDIC.  \nIn the example of the billiard table, we know that we are starting out\nwith a Newtonian deterministic system—that is how the idealized\nexample is defined. But chaotic dynamical systems come in a great\nvariety of types: discrete and continuous, 2-dimensional,\n3-dimensional and higher, particle-based and fluid-flow-based, and so\non. Mathematically, we may suppose all of these systems share SDIC.\nBut generally they will also display properties such as\nunpredictability, non-computability, Kolmogorov-random behavior, and\nso on—at least when looked at in the right way, or at the right\nlevel of detail. This leads to the following epistemic difficulty: if,\nin nature, we find a type of system that displays some or all of these\nlatter properties, how can we decide which of the following two\nhypotheses is true? \nIn other words, once one appreciates the varieties of chaotic\ndynamical systems that exist, mathematically speaking, it\nstarts to look difficult—maybe impossible—for us to ever\ndecide whether apparently random behavior in nature arises from\ngenuine stochasticity, or rather from deterministic chaos. Patrick\nSuppes (1993, 1996) argues, on the basis of theorems proven by\nOrnstein (1974 and later) that “There are processes which can\nequally well be analyzed as deterministic systems of classical\nmechanics or as indeterministic semi-Markov processes, no matter how\nmany observations are made.” And he concludes that\n“Deterministic metaphysicians can comfortably hold to their view\nknowing they cannot be empirically refuted, but so can indeterministic\nones as well.” (Suppes 1993, p. 254) For more recent works\nexploring the extent to which deterministic and indeterministic model\nsystems may be regarded as empirically indistinguishable, see Werndl\n(2016) and references therein. \nThere is certainly an interesting problem area here for the\nepistemology of determinism, but it must be handled with care. It may\nwell be true that there are some deterministic dynamical systems that,\nwhen viewed properly, display behavior indistinguishable from\nthat of a genuinely stochastic process. For example, using the\nbilliard table above, if one divides its surface into quadrants and\nlooks at which quadrant the ball is in at 30-second intervals, the\nresulting sequence is no doubt highly random. But this does not mean\nthat the same system, when viewed in a different way (perhaps\nat a higher degree of precision) does not cease to look random and\ninstead betray its deterministic nature. If we partition our billiard\ntable into squares 2 centimeters a side and look at which quadrant the\nball is in at .1 second intervals, the resulting sequence will be far\nfrom random. And finally, of course, if we simply look at the billiard\ntable with our eyes, and see it as a billiard table, there is\nno obvious way at all to maintain that it may be a truly random\nprocess rather than a deterministic dynamical system. (See Winnie\n(1996) for a nice technical and philosophical discussion of these\nissues. Winnie explicates Ornstein's and others' results in some\ndetail, and disputes Suppes' philosophical conclusions.) \nThe dynamical systems usually studied under the label of\n“chaos” are usually either purely abstract, mathematical\nsystems, or classical Newtonian systems. It is natural to wonder\nwhether chaotic behavior carries over into the realm of systems\ngoverned by quantum mechanics as well. Interestingly, it is much\nharder to find natural correlates of classical chaotic behavior in\ntrue quantum systems (see Gutzwiller 1990). Some, at least, of the\ninterpretive difficulties of quantum mechanics would have to be\nresolved before a meaningful assessment of chaos in quantum mechanics\ncould be achieved. For example, SDIC is hard to find in the\nSchrödinger evolution of a wavefunction for a system with finite\ndegrees of freedom; but in\n Bohmian quantum mechanics\n it is handled quite easily on the basis of particle trajectories (see\nDürr, Goldstein and Zhangì 1992). \nThe popularization of chaos theory in the relatively recent past\nperhaps made it seem self-evident that nature is full of genuinely\nchaotic systems. In fact, it is far from self-evident that such\nsystems exist, other than in an approximate sense. Nevertheless, the\nmathematical exploration of chaos in dynamical systems helps us to\nunderstand some of the pitfalls that may attend our efforts to know\nwhether our world is genuinely deterministic or not. \nLet us suppose that we shall never have the Final Theory of Everything\nbefore us—at least in our lifetime—and that we also remain\nunclear (on physical/experimental grounds) as to whether that Final\nTheory will be of a type that can or cannot be deterministic. Is there\nnothing left that could sway our belief toward or against determinism?\nThere is, of course: metaphysical argument. Metaphysical arguments on\nthis issue are not currently very popular. But philosophical fashions\nchange at least twice a century, and grand systemic metaphysics of the\nLeibnizian sort might one day come back into favor. Conversely, the\nanti-systemic, anti-fundamentalist metaphysics propounded by\nCartwright (1999) might also come to predominate. As likely as not,\nfor the foreseeable future metaphysical argument may be just as good a\nbasis on which to discuss determinism's prospects as any arguments\nfrom mathematics or physics. \nJohn Earman's Primer on Determinism (1986) remains the\nrichest storehouse of information on the truth or falsity of\ndeterminism in various physical theories, from classical mechanics to\nquantum mechanics and general relativity. (See also his recent update\non the subject, “Aspects of Determinism in Modern Physics”\n(2007)). Here I will give only a brief discussion of some key issues,\nreferring the reader to Earman (1986) and other resources for more\ndetail. Figuring out whether well-established theories are\ndeterministic or not (or to what extent, if they fall only a bit\nshort) does not do much to help us know whether our world is\nreally governed by deterministic laws; all our current best\ntheories, including General Relativity and the Standard Model of\nparticle physics, are too flawed and ill-understood to be mistaken for\nanything close to a Final Theory. Nevertheless, as Earman stressed,\nthe exploration is very valuable because of the way it enriches our\nunderstanding of the richness and complexity of determinism. \nDespite the common belief that classical mechanics (the theory that\ninspired Laplace in his articulation of determinism) is perfectly\ndeterministic, in fact the theory is rife with possibilities for\ndeterminism to break down. One class of problems arises due to the\nabsence of an upper bound on the velocities of moving objects. Below\nwe see the trajectory of an object that is accelerated unboundedly,\nits velocity becoming in effect infinite in a finite time. See\n Figure 2: \nFigure 2: An object accelerates so as to\nreach spatial infinity in a finite time \nBy the time t = t*, the object has literally disappeared from\nthe world—its world-line never reaches the t = t*\nsurface. (Never mind how the object gets accelerated in this way;\nthere are mechanisms that are perfectly consistent with classical\nmechanics that can do the job. In fact, Xia (1992) showed that such\nacceleration can be accomplished by gravitational forces from only 5\nfinite objects, without collisions. No mechanism is shown in these\ndiagrams.) This “escape to infinity,” while disturbing,\ndoes not yet look like a violation of determinism. But now recall that\nclassical mechanics is time-symmetric: any model has a time-inverse,\nwhich is also a consistent model of the theory. The time-inverse of\nour escaping body is playfully called a “space\ninvader.” \nFigure 3: A ‘space invader’\ncomes in from spatial infinity \nClearly, a world with a space invader does fail to be deterministic.\nBefore t = t*, there was nothing in the state of things to\nenable the prediction of the appearance of the invader at t =\nt*\n +.[2]\n One might think that the infinity of space is to blame for this\nstrange behavior, but this is not obviously correct. In finite,\n“rolled-up” or cylindrical versions of Newtonian\nspace-time space-invader trajectories can be constructed, though\nwhether a “reasonable” mechanism to power them exists is\nnot\n clear.[3] \nA second class of determinism-breaking models can be constructed on\nthe basis of collision phenomena. The first problem is that of\nmultiple-particle collisions for which Newtonian particle mechanics\nsimply does not have a prescription for what happens. (Consider three\nidentical point-particles approaching each other at 120 degree angles\nand colliding simultaneously. That they bounce back along their\napproach trajectories is possible; but it is equally possible for them\nto bounce in other directions (again with 120 degree angles between\ntheir paths), so long as momentum conservation is respected.) \nMoreover, there is a burgeoning literature of physical or\nquasi-physical systems, usually set in the context of classical\nphysics, that carry out supertasks (see Earman and Norton (1998) and\nthe entry on\n supertasks\n for a review). Frequently, the puzzle presented is to decide, on the\nbasis of the well-defined behavior before time t =\na, what state the system will be in at t =\na itself. A failure of CM to dictate a well-defined result\ncan then be seen as a failure of determinism. \nIn supertasks, one frequently encounters infinite numbers of\nparticles, infinite (or unbounded) mass densities, and other dubious\ninfinitary phenomena. Coupled with some of the other breakdowns of\ndeterminism in CM, one begins to get a sense that most, if not all,\nbreakdowns of determinism rely on some combination of the following\nset of (physically) dubious mathematical notions: {infinite space;\nunbounded velocity; continuity; point-particles; singular fields}. The\ntrouble is, it is difficult to imagine any recognizable\nphysics (much less CM) that eschews everything in the set. \nFigure 4: A ball may spontaneously start\nsliding down this dome, with no violation of Newton's laws.\n(Reproduced courtesy of John D. Norton and Philosopher's\nImprint) \nFinally, an elegant example of apparent violation of determinism in\nclassical physics has been created by John Norton (2003). As\nillustrated in\n Figure 4,\n imagine a ball sitting at the apex of a frictionless dome whose\nequation is specified as a function of radial distance from the apex\npoint. This rest-state is our initial condition for the system; what\nshould its future behavior be? Clearly one solution is for the ball to\nremain at rest at the apex indefinitely.  \nBut curiously, this is not the only solution under standard Newtonian\nlaws. The ball may also start into motion sliding down the\ndome—at any moment in time, and in any radial direction. This\nexample displays “uncaused motion” without, Norton argues,\nany violation of Newton's laws, including the First Law. And it does\nnot, unlike some supertask examples, require an infinity of particles.\nStill, many philosophers are uncomfortable with the moral Norton draws\nfrom his dome example, and point out reasons for questioning the\ndome's status as a Newtonian system (see e.g. Malament (2007)). \nTwo features of special relativistic physics make it perhaps the most\nhospitable environment for determinism of any major theoretical\ncontext: the fact that no process or signal can travel faster than the\nspeed of light, and the static, unchanging spacetime structure. The\nformer feature, including a prohibition against tachyons (hypothetical\nparticles travelling faster than\n light)[4]),\n rules out space invaders and other unbounded-velocity systems. The\nlatter feature makes the space-time itself nice and stable and\nnon-singular—unlike the dynamic space-time of General\nRelativity, as we shall see below. For source-free electromagnetic\nfields in special-relativistic space-time, a nice form of Laplacean\ndeterminism is provable. Unfortunately, interesting physics needs more\nthan source-free electromagnetic fields. Earman (1986) ch. IV surveys\nin depth the pitfalls for determinism that arise once things are\nallowed to get more interesting (e.g. by the addition of particles\ninteracting gravitationally). \nDefining an appropriate form of determinism for the context of general\nrelativistic physics is extremely difficult, due to both foundational\ninterpretive issues and the plethora of weirdly-shaped space-time\nmodels allowed by the theory's field equations. The simplest way of\ntreating the issue of determinism in GTR would be to state flatly:\ndeterminism fails, frequently, and in some of the most interesting\nmodels. Here we will briefly describe some of the most important\nchallenges that arise for determinism, directing the reader yet again\nto Earman (1986), and also Earman (1995) for more depth. \nIn GTR, we specify a model of the universe by giving a triple of three\nmathematical objects, <M,\ng,T>. M represents\na continuous “manifold”: that means a sort of unstructured\nspace (-time), made up of individual points and having smoothness or\ncontinuity, dimensionality (usually, 4-dimensional), and global\ntopology, but no further structure. What is the further structure a\nspace-time needs? Typically, at least, we expect the time-direction to\nbe distinguished from space-directions; and we expect there to be\nwell-defined distances between distinct points; and also a determinate\ngeometry (making certain continuous paths in M be\nstraight lines, etc.). All of this extra structure is coded into\ng, the metric field. So M and\ng together represent space-time. T\nrepresents the matter and energy content distributed around in\nspace-time (if any, of course). \nFor mathematical reasons not relevant here, it turns out to be\npossible to take a given model spacetime and perform a mathematical\noperation called a “hole diffeomorphism” h* on\nit; the diffeomorphism's effect is to shift around the matter content\nT and the metric g relative to the\ncontinuous manifold\n M.[5]\n If the diffeomorphism is chosen appropriately, it can move around\nT and g after a certain time t =\n0, but leave everything alone before that time. Thus, the new\nmodel represents the matter content (now h*\nT) and the metric (h*g) as\ndifferently located relative to the points of M making up\nspace-time. Yet, the new model is also a perfectly valid model of the\ntheory. This looks on the face of it like a form of indeterminism:\nGTR's equations do not specify how things will be distributed in\nspace-time in the future, even when the past before a given time\nt is held fixed. See\n Figure 5: \nFigure 5: “Hole”\ndiffeomorphism shifts contents of spacetime \nUsually the shift is confined to a finite region called the\nhole (for historical reasons). Then it is easy to see that the\nstate of the world at time t = 0 (and all the history that\ncame before) does not suffice to fix whether the future will be that\nof our first model, or its shifted counterpart in which events inside\nthe hole are different. \nThis is a form of indeterminism first highlighted by Earman and Norton\n(1987) as an interpretive philosophical difficulty for realism about\nGTR's description of the world, especially the point manifold\nM. They showed that realism about the manifold as a part of\nthe furniture of the universe (which they called “manifold\nsubstantivalism”) commits us to an automatic indeterminism in\nGTR (as described above), and they argued that this is unacceptable.\n(See\n the hole argument\n and Hoefer (1996) for one response on behalf of the space-time\nrealist, and discussion of other responses.) For now, we will simply\nnote that this indeterminism, unlike most others we are discussing in\nthis section, is empirically undetectable: our two models\n<M, g, T> and the\nshifted model <M, h*g,\nh*T> are empirically\nindistinguishable. \nThe separation of space-time structures into manifold and metric (or\nconnection) facilitates mathematical clarity in many ways, but also\nopens up Pandora's box when it comes to determinism. The indeterminism\nof the Earman and Norton hole argument is only the tip of the iceberg;\nsingularities make up much of the rest of the berg. In general terms,\na singularity can be thought of as a “place where things go\nbad” in one way or another in the space-time model. For example,\nnear the center of a Schwarzschild black hole, curvature increases\nwithout bound, and at the center itself it is undefined, which means\nthat Einstein's equations cannot be said to hold, which means\n(arguably) that this point does not exist as a part of the space-time\nat all! Some specific examples are clear, but giving a general\ndefinition of a singularity, like defining determinism itself in GTR,\nis a vexed issue (see Earman (1995) for an extended treatment;\nCallender and Hoefer (2001) gives a brief overview). We will not\nattempt here to catalog the various definitions and types of\nsingularity.  \nDifferent types of singularity bring different types of threat to\ndeterminism. In the case of ordinary black holes, mentioned above, all\nis well outside the so- called “event horizon”, which is\nthe spherical surface defining the black hole: once a body or light\nsignal passes through the event horizon to the interior region of the\nblack hole, it can never escape again. Generally, no violation of\ndeterminism looms outside the event horizon; but what about inside?\nSome black hole models have so-called “Cauchy horizons”\ninside the event horizon, i.e., surfaces beyond which determinism\nbreaks down. \nAnother way for a model spacetime to be singular is to have points or\nregions go missing, in some cases by simple excision. Perhaps the most\ndramatic form of this involves taking a nice model with a space-like\nsurface t = E (i.e., a well-defined part of the\nspace-time that can be considered “the state state of the world\nat time E”), and cutting out and throwing away this\nsurface and all points temporally later. The resulting spacetime\nsatisfies Einstein's equations; but, unfortunately for any\ninhabitants, the universe comes to a sudden and unpredictable end at\ntime E. This is too trivial a move to be considered a real\nthreat to determinism in GTR; we can impose a reasonable requirement\nthat space-time not “run out” in this way without some\nphysical reason (the spacetime should be “maximally\nextended”). For discussion of precise versions of such a\nrequirement, and whether they succeed in eliminating unwanted\nsingularities, see Earman (1995, chapter 2). \nThe most problematic kinds of singularities, in terms of determinism,\nare naked singularities (singularities not hidden behind an\nevent horizon). When a singularity forms from gravitational collapse,\nthe usual model of such a process involves the formation of an event\nhorizon (i.e. a black hole). A universe with an ordinary black hole\nhas a singularity, but as noted above, (outside the event horizon at\nleast) nothing unpredictable happens as a result. A naked singularity,\nby contrast, has no such protective barrier. In much the way that\nanything can disappear by falling into an excised-region singularity,\nor appear out of a white hole (white holes themselves are, in fact,\ntechnically naked singularities), there is the worry that anything at\nall could pop out of a naked singularity, without warning (hence,\nviolating determinism en passant). While most white hole\nmodels have Cauchy surfaces and are thus arguably\ndeterministic, other naked singularity models lack this property.\nPhysicists disturbed by the unpredictable potentialities of such\nsingularities have worked to try to prove various cosmic\ncensorship hypotheses that show—under (hopefully) plausible\nphysical assumptions—that such things do not arise by stellar\ncollapse in GTR (and hence are not liable to come into existence in\nour world). To date no very general and convincing forms of the\nhypothesis have been proven, so the prospects for determinism in GTR\nas a mathematical theory do not look terribly good. \nAs indicated above, QM is widely thought to be a strongly\nnon-deterministic theory. Popular belief (even among most physicists)\nholds that phenomena such as radioactive decay, photon emission and\nabsorption, and many others are such that only a\nprobabilistic description of them can be given. The theory\ndoes not say what happens in a given case, but only says what the\nprobabilities of various results are. So, for example, according to QM\nthe fullest description possible of a radium atom (or a chunk of\nradium, for that matter), does not suffice to determine when a given\natom will decay, nor how many atoms in the chunk will have decayed at\nany given time. The theory gives only the probabilities for a decay\n(or a number of decays) to happen within a given span of time.\nEinstein and others perhaps thought that this was a defect of the\ntheory that should eventually be removed, by a supplemental hidden\nvariable\n theory[6]\n that restores determinism; but subsequent work showed that no such\nhidden variables account could exist. At the microscopic level the\nworld is ultimately mysterious and chancy. \nSo goes the story; but like much popular wisdom, it is partly mistaken\nand/or misleading. Ironically, quantum mechanics is one of the best\nprospects for a genuinely deterministic theory in modern times!\nEverything hinges on what interpretational and philosophical decisions\none adopts. The fundamental law at the heart of non-relativistic QM is\nthe Schrödinger equation. The evolution of a wavefunction\ndescribing a physical system under this equation is normally taken to\nbe perfectly\n deterministic.[7]\n If one adopts an interpretation of QM according to which that's\nit—i.e., nothing ever interrupts Schrödinger evolution, and\nthe wavefunctions governed by the equation tell the complete physical\nstory—then quantum mechanics is a perfectly deterministic\ntheory. There are several interpretations that physicists and\nphilosophers have given of QM which go this way. (See the entry on\n quantum mechanics.) \nMore commonly—and this is part of the basis for the popular\nwisdom—physicists have resolved the\n quantum measurement problem\n by postulating that some process of “collapse of the\nwavefunction” occurs during measurements or observations that\ninterrupts Schrödinger evolution. The collapse process is usually\npostulated to be indeterministic, with probabilities for various\noutcomes, via Born's rule, calculable on the basis of a\nsystem's wavefunction. The once-standard Copenhagen interpretation of\nQM posits such a collapse. It has the virtue of solving certain\nproblems such as the infamous Schrödinger's cat paradox, but few\nphilosophers or physicists can take it very seriously unless they are\ninstrumentalists about the theory. The reason is simple: the collapse\nprocess is not physically well-defined, is characterised in terms of\nan anthropomorphic notion (measurement)and feels too ad\nhoc to be a fundamental part of nature's\n laws.[8] \nIn 1952 David Bohm created an alternative interpretation of non\nrelativistic QM—perhaps better thought of as an alternative\ntheory—that realizes Einstein's dream of a hidden variable\ntheory, restoring determinism and definiteness to micro-reality. In\n Bohmian quantum mechanics,\n unlike other interpretations, it is postulated that all particles\nhave, at all times, a definite position and velocity. In addition to\nthe Schrödinger equation, Bohm posited a guidance\nequation that determines, on the basis of the system's\nwavefunction and particles' initial positions and velocities, what\ntheir future positions and velocities should be. As much as any\nclassical theory of point particles moving under force fields, then,\nBohm's theory is deterministic. Amazingly, he was also able to show\nthat, as long as the statistical distribution of initial positions and\nvelocities of particles are chosen so as to meet a “quantum\nequilibrium” condition, his theory is empirically equivalent to\nstandard Copenhagen QM. In one sense this is a philosopher's\nnightmare: with genuine empirical equivalence as strong as Bohm\nobtained, it seems experimental evidence can never tell us which\ndescription of reality is correct. (Fortunately, we can safely assume\nthat neither is perfectly correct, and hope that our Final\nTheory has no such empirically equivalent rivals.) In other senses,\nthe Bohm theory is a philosopher's dream come true, eliminating much\n(but not all) of the weirdness of standard QM and restoring\ndeterminism to the physics of atoms and photons. The interested reader\ncan find out more from the link above, and references therein. \nThis small survey of determinism's status in some prominent physical\ntheories, as indicated above, does not really tell us anything about\nwhether determinism is true of our world. Instead, it raises a couple\nof further disturbing possibilities for the time when we do have the\nFinal Theory before us (if such time ever comes): first, we may have\ndifficulty establishing whether the Final Theory is deterministic or\nnot—depending on whether the theory comes loaded with unsolved\ninterpretational or mathematical puzzles. Second, we may have reason\nto worry that the Final Theory, if indeterministic, has an empirically\nequivalent yet deterministic rival (as illustrated by Bohmian quantum\nmechanics.) \nSome philosophers maintain that if determinism holds in our world,\nthen there are no objective chances in our world. And often\nthe word ‘chance’ here is taken to be synonymous with\n'probability', so these philosophers maintain that there are no\nnon-trivial objective probabilities for events in our world. (The\ncaveat “non-trivial” is added here because on some\naccounts, under determinism, all future events that actually happen\nhave probability, conditional on past history, equal to 1, and future\nevents that do not happen have probability equal to zero. Non-trivial\nprobabilities are probabilities strictly between zero and one.)\nConversely, it is often held, if there are laws of nature that are\nirreducibly probabilistic, determinism must be false. (Some\nphilosophers would go on to add that such irreducibly probabilistic\nlaws are the basis of whatever genuine objective chances obtain in our\nworld.) \nThe discussion of quantum mechanics in section 4 shows that it may be\ndifficult to know whether a physical theory postulates genuinely\nirreducible probabilistic laws or not. If a Bohmian version of QM is\ncorrect, then the probabilities dictated by the Born rule are not\nirreducible. If that is the case, should we say that the probabilities\ndictated by quantum mechanics are not objective? Or should we say that\nwe need to distinguish ‘chance’ and\n‘probabillity’ after all—and hold that not all\nobjective probabilities should be thought of as objective\nchances? The first option may seem hard to swallow, given the\nmany-decimal-place accuracy with which such probability-based\nquantities as half-lives and cross-sections can be reliably predicted\nand verified experimentally with QM. \nWhether objective chance and determinism are really incompatible or\nnot may depend on what view of the nature of laws is adopted. On a\n“pushy explainers” view of laws such as that\ndefended by Maudlin (2007), probabilistic laws are interpreted as\nirreducible dynamical transition-chances between allowed physical\nstates, and the incompatibility of such laws with determinism is\nimmediate. But what should a defender of a Humean view of laws, such\nas the BSA theory (section 2.4 above), say about probabilistic laws?\nThe first thing that needs to be done is explain how probabilistic\nlaws can fit into the BSA account at all, and this requires\nmodification or expansion of the view, since as first presented the\nonly candidates for laws of nature are true universal generalizations.\nIf ‘probability’ were a univocal, clearly understood\nnotion then this might be simple: We allow universal generalizations\nwhose logical form is something like: “Whenever conditions\nY obtain, Pr(A) = x”. But it is not\nat all clear how the meaning of ‘Pr’ should be understood\nin such a generalization; and it is even less clear what features the\nHumean pattern of actual events must have, for such a generalization\nto be held true. (See the entry on\n interpretations of probability\n and Lewis (1994).) \nHumeans about laws believe that what laws there are is a matter of\nwhat patterns are there to be discerned in the overall mosaic of\nevents that happen in the history of the world. It seems plausible\nenough that the patterns to be discerned may include not only strict\nassociations (whenever X, Y), but also stable\nstatistical associations. If the laws of nature can include either\nsort of association, a natural question to ask seems to be: why can't\nthere be non-probabilistic laws strong enough to ensure determinism,\nand on top of them, probabilistic laws as well? If a Humean wanted to\ncapture the laws not only of fundamental theories, but also\nnon-fundamental branches of physics such as (classical) statistical\nmechanics, such a peaceful coexistence of deterministic laws plus\nfurther probabilistic laws would seem to be desirable. Loewer (2004)\nand Frigg & Hoefer (2015) offer forms of this peaceful coexistence\nthat can be achieved within Lewis' version of the BSA account of\nlaws. \nIn the introduction, we noted the threat that determinism seems to\npose to human free agency. It is hard to see how, if the state of the\nworld 1000 years ago fixes everything I do during my life, I can\nmeaningfully say that I am a free agent, the author of my own actions,\nwhich I could have freely chosen to perform differently. After all, I\nhave neither the power to change the laws of nature, nor to change the\npast! So in what sense can I attribute freedom of choice to\nmyself? \nPhilosophers have not lacked ingenuity in devising answers to this\nquestion. There is a long tradition of\n compatibilists\n arguing that freedom is fully compatible with physical determinism; a\nprominent recent defender is John Fischer (1994, 2012). Hume went so\nfar as to argue that determinism is a necessary condition for\nfreedom—or at least, he argued that some causality principle\nalong the lines of “same cause, same effect” is required.\nThere have been equally numerous and vigorous responses by those who\nare not convinced. Can a clear understanding of what determinism is,\nand how it tends to succeed or fail in real physical theories, shed\nany light on the controversy? \nPhysics, particularly 20th century physics, does have one\nlesson to impart to the free will debate; a lesson about the\nrelationship between time and determinism. Recall that we\nnoticed that the fundamental theories we are familiar with, if they\nare deterministic at all, are time-symmetrically deterministic. That\nis, earlier states of the world can be seen as fixing all later\nstates; but equally, later states can be seen as fixing all earlier\nstates. We tend to focus only on the former relationship, but we are\nnot led to do so by the theories themselves. \nNor does 20th (21st) -century physics\ncountenance the idea that there is anything ontologically special\nabout the past, as opposed to the present and the future. In fact, it\nfails to use these categories in any respect, and teaches that in some\nsenses they are probably\n illusory.[9]\n So there is no support in physics for the idea that the past is\n“fixed” in some way that the present and future are not,\nor that it has some ontological power to constrain our actions that\nthe present and future do not have. It is not hard to uncover the\nreasons why we naturally do tend to think of the past as special, and\nassume that both physical causation and physical explanation work only\nin the past present/future direction (see the entry on\n thermodynamic asymmetry in time).\n But these pragmatic matters have nothing to do with fundamental\ndeterminism. If we shake loose from the tendency to see the past as\nspecial, when it comes to the relationships of determination, it may\nprove possible to think of a deterministic world as one in which each\npart bears a determining—or partial-determining—relation\nto other parts, but in which no particular part (region of space-time,\nevent or set of events, ...) has a special, privileged determining\nrole that undercuts the others. Hoefer (2002a) and Ismael (2016) use\nsuch considerations to argue in a novel way for the compatiblity of\ndeterminism with human free agency.","contact.mail":"carl.hoefer@ub.edu","contact.domain":"ub.edu"}]
