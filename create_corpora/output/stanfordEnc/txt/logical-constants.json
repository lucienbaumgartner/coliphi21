[{"date.published":"2005-05-16","date.changed":"2015-06-18","url":"https://plato.stanford.edu/entries/logical-constants/","author1":"John MacFarlane","author1.info":"http://johnmacfarlane.net/","entry":"logical-constants","body.text":"\n\n\n\nLogic is usually thought to concern itself only with features that\nsentences and arguments possess in virtue of their logical structures\nor forms. The logical form of a sentence or argument is\ndetermined by its syntactic or semantic structure and by the placement\nof certain expressions called “logical constants.”[1]\n Thus, for example, the\nsentences\n\nEvery boy loves some girl.\n\n\n\nand\n\nSome boy loves every girl.\n\n\n\nare thought to differ in logical form, even though they share a\ncommon syntactic and semantic structure, because they differ in the\nplacement of the logical constants “every” and “some”. By contrast, the\nsentences\n\nEvery girl loves some boy.\n\n\n\nand\n\nEvery boy loves some girl.\n\n\n\nare thought to have the same logical form, because “girl” and “boy”\nare not logical constants. Thus, in order to settle questions about\nlogical form, and ultimately about which arguments are logically valid\nand which sentences logically true, we must distinguish the “logical\nconstants” of a language from its nonlogical expressions.\n\n\n\nWhile it is generally agreed that signs for negation, conjunction,\ndisjunction, conditionality, and the first-order quantifiers should\ncount as logical constants, and that words like “red”, “boy”, “taller”,\nand “Clinton” should not, there is a vast disputed middle ground. Is\nthe sign for identity a logical constant? Are tense and modal operators\nlogical constants? What about “true”, the epsilon of set-theoretic\nmembership, the sign for mereological parthood, the second-order\nquantifiers, or the quantifier “there are infinitely many”? Is there a\ndistinctive logic of agency, or of knowledge? In these border areas our\nintuitions from paradigm cases fail us; we need something more\nprincipled.\n\n\n\nHowever, there is little philosophical consensus about the basis for\nthe distinction between logical and nonlogical expressions. Until this\nquestion is resolved, we lack a proper understanding of the scope and\nnature of logic, and of the significance of the distinction between the\n“formal” properties and relations logic studies and related but\nnon-formal ones. For example, the sentence\n\nIf Socrates is human and mortal, then he is\nmortal.\n\n\n\nis generally taken to be a logical truth, while the sentence\n\nIf Socrates is orange, then he is colored.\n\n\n\nis not, even though intuitively both are true, necessary, knowable\na priori, and analytic. What is the significance of the\ndistinction we are making between them in calling one but not the other\n“logically true”? A principled demarcation of logical constants might\noffer an answer to this question, thereby clarifying what is at stake\nin philosophical controversies for which it matters what counts as\nlogic (for example, logicism and structuralism in the philosophy of\nmathematics).\n\n\n\nThis article will discuss the problem of logical constants and\nsurvey the main approaches to solving or resolving it.\n\n\n\nThe most venerable approach to demarcating the logical constants\nidentifies them with the language’s syncategorematic signs:\nsigns that signify nothing by themselves, but serve to indicate how\nindependently meaningful terms are combined. This approach was natural\nin the context of the “term logics” that were dominant until the\nnineteenth century. All propositions were thought to be composed out of\npropositions of subject-predicate form by means of a small number of\nconnectives (“and”, “or”, “if …then”, and so on). In this framework,\nwords divide naturally into those that can be used as subjects or\npredicates (“categorematic” words) and those whose function is to\nindicate the relation between subject and predicate or between two\ndistinct subject-predicate propositions (“syncategorematic” words). For\nexample, “Socrates”, “runs”, “elephant”, and “large” are categorematic\nwords, while “only”, “every”, “necessarily”, and “or” are\nsyncategorematic. (For a more detailed account of the distinction, see\nKretzmann 1982, 211–214.) The syncategorematic words were naturally\nseen as indicating the structure or form of the\nproposition, while the categorematic words supplied its “matter.” Thus\nthe fourteenth-century logician Buridan writes: \n\nThe Fregean revolution in our conception of\n logical form\n made this way of demarcating the logical constants\nproblematic. Whereas the term logicians had seen every proposition as\ncomposed of subject and predicate terms linked together by\nsyncategorematic “glue,” Frege taught us to see sentences and\npropositions as built up recursively by functional application and\nfunctional abstraction (for a good account, see Dummett 1981, ch. 2).\nTo see the difference between the two approaches, consider the\nsentence \n\nA term logician would have regarded \\(\\refp{moby}\\) as composed of a subject\nterm (“boat”) and a predicate term (“thing smaller than Moby Dick”)\njoined together in a universal affirmative categorical form. Frege, by\ncontrast, would have regimented \\(\\refp{moby}\\) as \nwhich he would have analyzed as the result of applying the\nsecond-level function[2] to the first level functions and \n(The Greek letters \\(\\xi\\), \\(\\Phi\\), and \\(\\Psi\\) here indicate the\nfunctions’ argument places: lowercase Greek letters indicate places\nthat can be filled by proper names, while uppercase Greek letters\nindicate places that must be filled by function expressions like\n\\(\\refp{function-boat}\\)\nand \\(\\refp{function-smaller}\\).) He would have regarded \\(\\refp{function-smaller}\\)\nas itself the result of\n“abstracting” on the place occupied by “Shamu” in\n which in turn is the result of applying the function \nto “Shamu” and “Moby Dick”. Frege showed that by describing\nsentences and propositions this way, in terms of their\nfunction/argument composition, we can represent the logical relations\nbetween them in a far more complete, perspicuous, and systematic way\nthan was possible with the old subject/predicate model of propositional\nform. \n\nHowever, once we have thrown out the old subject/predicate model, we\ncan no longer identify the categorematic terms with the subject and\npredicate terms, as the medievals did. Nor can we think of the\nsyncategorematic terms as the expressions that have no “independent”\nsignificance, or as the “glue” that binds together the categorematic\nterms to form a meaningful whole. Granted, there is a sense in which\nthe “logical” function \\(\\refp{second-level}\\) is the glue that binds\ntogether \\(\\refp{function-boat}\\) and \\(\\refp{function-smaller}\\)\nto yield \\(\\refp{moby-frege}\\). But in the very same sense, the function\n\\(\\refp{smaller-than}\\) is the glue\nthat binds together “Shamu” and “Moby Dick” to yield\n\\(\\refp{shamu-smaller}\\). If we count\nall functional expressions as syncategorematic on the grounds that they\nare “incomplete” or “unsaturated” and thus not “independently\nmeaningful,” then the syncategorematic expressions will include not\njust connectives and quantifiers, but ordinary predicates. On the other\nhand, if we count all the functional expressions as categorematic, then\nthe syncategoremata will be limited to variables, parentheses, and\nother signs that serve to indicate functional application and\nabstraction. In neither case would the distinction be useful for\ndemarcating the logical constants. An intermediate proposal would be to\ncount first-level functions as categoremata and second-level functions\nas syncategoremata. That would make \\(\\refp{second-level}\\) syncategorematic and\n\\(\\refp{function-boat}\\) and \\(\\refp{function-smaller}\\) categorematic. However, not\nevery second-level function is\n(intuitively) “logical.” Consider, for example, the second-level\nfunction \n\nGranted, standard logical languages do not have a simple expression\nfor this function, but there is no reason in principle why we could not\nintroduce such an expression. Conversely, not every first-level\nfunction is (intuitively) nonlogical: for example, the identity\nrelation is usually treated as logical. \n\nIn sum, it is not clear how the distinction between categorematic\nand syncategorematic terms, so natural in the framework of a term\nlogic, can be extended to a post-Fregean function/argument conception\nof propositional structure. At any rate, none of the natural ways of\nextending the distinction seem apt for the demarcation of the logical\nconstants. Carnap concedes that the distinction between categorematic\nand syncategorematic expressions “seems more or less a matter of\nconvention” (1947, 6–7). However, the idea that logical constants are\nsyncategoremata does not wither away entirely with the demise of term\nlogics. Its influence can still be felt in Wittgenstein’s insistence\nthat the logical constants are like punctuation marks (1922,\n§5.4611),[3]\n in Russell’s claim that logical constants\nindicate logical form and not propositional constituents (1992, 98;\n1920, 199), and in the idea (found in Quine and Dummett) that the\nlogical constants of a language can be identified with its grammatical\nparticles. \n\nQuine and Dummett propose that the logical constants of a language\nare its grammatical particles—the expressions by means\nof which complex sentences are built up, step by step, from atomic\nones—while non-logical expressions are the simple expressions of\nwhich atomic sentences are composed (see Quine 1980, Quine 1986,\nDummett 1981, 21–2, and for discussion, Føllesdal 1980 and\nHarman 1984). On this conception, “[l]ogic studies the truth conditions\nthat hinge solely on grammatical constructions” (Quine 1980,\n 17).[4]\nThis criterion yields appropriate results when applied to the language\nof first-order logic (FOL) and other standard logical languages. In FOL\n(without identity), all singular terms and predicates are\nparadigm nonlogical constants, and all operators and\nconnectives are paradigm logical\n constants.[5] \n\nHowever, this nice coincidence of intuitively logical expressions and\ngrammatical particles in FOL cannot be taken as support for the\nQuine/Dummett proposal, because FOL was designed so that its\ngrammatical structure would reflect logical structure. It is easy\nenough to design other artificial languages for which the grammatical\ncriterion gives intuitively inappropriate results. For example, take\nstandard FOL and add a variable-binding operator “¢” whose\ninterpretation is “there is at least one cat such that ….” The\ngrammatical criterion counts “¢” as a logical constant, but\nsurely it is not one. \n\nMoreover, there are alternative ways of regimenting the grammar of\nFOL on which the standard truth-functional connectives are not\ngrammatical particles, but members of a small lexical category (Quine\n1986, 28–9). For example, instead of recognizing four grammatical\noperations that form one sentence from two sentences (one that takes\n\\(P\\) and \\(Q\\) and yields \\(\\cq{P \\vee Q}\\), one\nthat takes \\(P\\) and \\(Q\\) and yields \\(\\cq{P \\and Q}\\), and so\non), we could recognize a single grammatical operation that forms one\nsentence from two sentences and one connective. On this way of\nregimenting the grammar of FOL, \\(\\dq{\\and}\\) and\n \\(\\dq{\\vee}\\)\n would not count as grammatical particles. \n\nThe upshot is that a grammatical demarcation of logical constants will\nnot impose significant constraints on what counts as a logical\nconstant unless it is combined with some principle for limiting the\nlanguages to which it applies (excluding, for example, languages with\nthe operator “¢”) and privileging some regimentations of their\ngrammars over others (excluding, for example, regimentations that\ntreat truth-functional connectives as members of a small lexical\ncategory).  Quine’s own approach is to privilege the language best\nsuited for the articulation of scientific theories and the grammar\nthat allows the most economical representation of the truth conditions\nof its sentences. Thus the reason Quine holds that logic should\nrestrict itself to the study of inferences that are truth-preserving\nin virtue of their grammatical structures is not that he thinks there\nis something special about the grammatical particles (in an arbitrary\nlanguage), but rather that he thinks we should employ a language in\nwhich grammatical structure is a perspicuous guide to truth\nconditions: “what we call logical form is what grammatical form\nbecomes when grammar is revised so as to make for efficient general\nmethods of exploring the interdependence of sentences in respect of\ntheir truth values” (1980, 21). \n\nInstead of applying the grammatical criterion to artificial\nlanguages like FOL, one might apply it to natural languages like\nEnglish. One could then appeal to the work of empirical linguists for a\nfavored grammatical regimentation. Contemporary linguists posit a\nstructural representation called LF that resolves issues of scope and\nbinding crucial to semantic evaluation. But the question remains which\nlexical items in the LF should count as logical constants. Generalizing\nQuine’s proposal, one might identify the logical constants with members\nof small, “closed” lexical categories: for example, conjunctions and\ndeterminers. However, by this criterion prepositions in English would\ncount as logical constants (Harman 1984, 121). Alternatively, one might\nidentify the logical constants with members of functional\ncategories (including tense, complementizers, auxiliaries,\ndeterminers, and pronouns), and the nonlogical constants with members\nof substantive categories (including nouns, verbs, adjectives,\nadverbs, and prepositions) (for this terminology, see Chomsky 1995, 6,\n54 and Radford 2004, 41). If a distinction that plays an important role\nin a theory of linguistic competence should turn out to coincide (in\nlarge part) with our traditional distinction between logical and\nnonlogical constants, then this fact would stand in need of\nexplanation. Why should we treat inferences that are truth-preserving\nin virtue of their LF structures and functional words differently from\nthose that are truth-preserving in virtue of their LF structures and\nsubstantive words? Future work in linguistics, cognitive psychology and\nneurophysiology may provide the materials for an interesting answer to\nthis question, but for now it is important that the question be asked,\nand that we keep in mind the possibility of a sceptical answer. \n\nThe Quinean approach identifies the logical constants as the\nexpressions that play a privileged, “structural” role in a systematic\ngrammatical theory for a language. An alternative approach, due to\nQuine’s student\n Donald Davidson,\n identifies the logical constants as the expressions that play a\nprivileged, “structural” role in a systematic theory of\nmeaning for a language. A Davidsonian theory of meaning takes\nthe form of a\n Tarskian truth theory.\n Thus, it contains two kinds of axioms: base clauses that\nspecify the satisfaction conditions of atomic\n sentences,[6]\n and recursive\nclauses that specify the satisfaction conditions of complex\nsentences in terms of the satisfaction conditions of their proper\n parts.[7]\n For example: \n\nBase Clauses: \n\nRecursive Clauses: \n\nDavidson suggests that “[t]he logical constants may be identified as\nthose iterative features of the language that require a recursive\nclause in the characterization of truth or satisfaction” (1984, 71).\n(In our example, “or” and “some”.)  \n\nThis criterion certainly gives reasonable results when applied to\nstandard truth theories like the one above (although the sign for\nidentity once more gets counted as nonlogical). But as Davidson goes on\nto observe, “[l]ogical form, in this account, will of course be\nrelative to the choice of a metalanguage (with its logic) and a theory\nof truth” (1984, 71). Different truth theories can be given for the\nsame language, and they can agree on the truth conditions of whole\nsentences while differing in which expressions they treat in the\nrecursive clauses. Here are two examples (both discussed further in\nEvans 1976). \n\n1. We might have recursive clauses for “large” and other gradable\nadjectives, along these lines: For all assignments \\(a\\), terms \\(\\tau\\), and\n  sentences \\(\\phi\\),\n  \\(\\cq{\\tau \\text{ is a large } \\phi }\\)\n is satisfied by \\(a\\) iff Ref(\\(\\tau\\), \\(a\\))\n is a large satisfier of \\(\\phi\\) on \\(a\\). (cf. Evans 1976,\n 203) \n\nWe would in this case have to use a metalanguage with a stronger\nlogic, one that provides rules for manipulating “large satisfier of\n\\(\\phi\\) on \\(a\\).” (As Evans notes, all we would really need in\norder to derive T-sentences would be a rule licensing the derivation\nof\n\\(\\cq{\\tau \\text{ is a large satisfier of } \\phi \\text{ on } a}\\)\nfrom\n\\(\\cq{\\phi \\equiv \\psi}\\) and\n\\(\\cq{\\tau \\text{ is a larger satisfier of } \\psi \\text{ on } a}\\).) But\nsuch a metalanguage cannot be ruled out without begging the question\nabout the logicality of “large.” \n\n2. We might assign values to “and”, “or”, and the other\ntruth-functional connectives in the base clauses, allowing us\nto get by with a single generic recursive clause for truth-functional\nconnectives: \n\nBase: For all assignments \\(a\\), Ref(“or”, \\(a\\))\n= Boolean disjunction (the binary truth function that takes the value\nTrue when either argument is True, and False otherwise). \n\nRecursive: For all assignments \\(a\\), sentences \\(\\phi,\n\\psi\\), and truth-functional connectives \\(@\\), \\(\\cq{\\phi @ \\psi }\\)\nis satisfied\nby \\(a\\) iff Ref(\\(@, a\\))(Val(\\(\\phi, a\\)), Val(\\(\\psi,a\\))) = True\n(where Val(\\(\\phi, a\\)) = True if \\(\\phi\\) is\nsatisfied by \\(a\\), False if \\(\\phi\\) is not satisfied by\n\\(a\\)). (cf. Evans 1976, 214) \n\nThis approach requires a stronger metatheory than the usual\napproach, since it requires quantification over truth functions. But it\nis not clear why this is an objection. It is still possible to derive\nT-sentences whose right sides are no more ontologically committed than\nthe sentences named on their left sides, like \n\nSo it is hard to see how the use of functions here is any more\nobjectionable than Davidson’s own appeal to sequences or assignments of\nvalues to variables. \n\nIn sum, the problem with Davidson’s truth-theoretic proposal is much\nlike the problem discussed above with Quine’s grammatical proposal.\nWithout further constraints on the theory of meaning (or, in Quine’s\ncase, the grammar), it does not yield a definite criterion for logical\nconstancy. I do not mean to suggest that either Davidson or Quine was\ndeluded on this score. As we saw above, Quine appeals to pragmatic\nconsiderations to pick out a favored language and grammatical\nregimentation. No doubt Davidson would do the same, arguing (for\nexample) that the advantages of using a simple and well-understood\nlogic in the metalanguage outweigh any putative advantages of treating\n“large” and the like in recursive clauses. (For a recent defense of a\nDavidsonian criterion against Evans’s objections, see Lepore and Ludwig\n2002.) \n\nLogic, it seems, is not about anything in particular; relatedly, it is\napplicable everywhere, no matter what we are reasoning about. So it is\nnatural to suppose that the logical constants can be marked out as the\n“topic-neutral” expressions (Ryle 1954, 116; Peacocke 1976, 229; Haack\n1978, 5–6; McCarthy 1981, 504; Wright 1983, 133; Sainsbury 2001,\n365). We have reason to care about the topic-neutral expressions, and\nto treat them differently from others, because we are interested in\nlogic as a universal canon for reasoning, one that is\napplicable not just to reasoning about this or that domain, but to all\nreasoning. \n\nUnfortunately, the notion of topic neutrality is too vague to be of\nmuch help when it comes to the hard cases for which we need a\nprinciple of demarcation. Take arithmetic, for instance. Is it\ntopic-neutral? Well, yes: anything can be counted, so the theorems of\narithmetic will be useful in any field of inquiry. But then again, no:\narithmetic has its own special subject matter, the natural numbers and\nthe arithmetical relations that hold between them. The same can be\nsaid about set theory: on the one hand, anything we can reason about\ncan be grouped into sets; on the other hand, set theory seems to be\nabout a particular corner of the universe—the sets—and\nthus to have its own special “topic.” The general problem of which\nthese two cases are instances might be called the antinomy of\ntopic-neutrality. As George Boolos points out, the antinomy can\nbe pressed all the way to paradigm cases of logical constants: “it\nmight be said that logic is not so ‘topic-neutral’ as it\nis often made out to be: it can easily be said to be about the notions\nof negation, conjunction, identity, and the notions expressed by\n‘all’ and ‘some’, among others …”\n(1975, 517). It is plausible to think that the source of the antinomy\nis the vagueness of the notion of topic neutrality, so let us consider\nsome ways in which we might make this notion more precise. \n\nGilbert Ryle, who seems to have coined the expression\n“topic-neutral”, gives the following rough criterion: \n\nThere are, I suppose, a few paradigm cases of such expressions:\n“is”, for instance, and “if”. But the criterion gives little help when\nwe venture beyond these clear-cut cases. The problem is that one might\nanswer the question “what is this paragraph about?” at many different\nlevels of generality. Suppose I understand English badly, and I hear\nsomeone say: \n\nDo I have any clue as to what the paragraph is about? Well, surely I\nhave some clue. “Because” reveals that the passage is about\ncausal or explanatory relations. “It” reveals that the passage is about\nat least one object that is not known to be a person. The tense\noperator “was always” reveals that it is about events that occur in\ntime. “Might be” reveals that it is about the realm of the possible (or\nthe unknown), and not just the actual (or the known). Finally, “every”\nand “a few” reveal that it is about discrete, countable objects.\nPerhaps some of these words are not topic-neutral and should not be\nincluded in the domain of logic, but we certainly don’t want to rule\nout all of them. And Ryle’s criterion gives no guidance about\nwhere to draw the line. One might even suspect that there is no line,\nand that topic neutrality is a matter of degree, truth-functional\nexpressions being more topic-neutral than quantifiers, which are more\ntopic-neutral than tense and modal operators, which are more\ntopic-neutral than epistemic expressions, and so on (Lycan 1989). \n\nThe problem with Ryle’s account is its reliance on vague and\nunclarified talk of “aboutness.” If we had a precise philosophical\naccount of what it is for a statement to be about a particular\nobject or subject matter, then we could define a topic-neutral\nstatement as one that is not about anything—or, perhaps, one that\nis about everything indifferently. Here we might hope to appeal to\nNelson Goodman’s classic account of “absolute aboutness,” which implies\nthat logical truths are not absolutely about anything (1961, 256), or\nto David Lewis’s (1988) account of what it is for a proposition to be\nabout a certain subject matter, which implies that logical truths are\nabout every subject matter indifferently. However, neither\naccount is appropriate for our purpose. On Goodman’s account, “what a\nstatement is absolutely about will depend in part upon what logic is\npresupposed,” and hence upon which expressions are taken to be logical\nconstants (253–4), so it would be circular to appeal to Goodman’s\naccount of aboutness in a demarcation of the logical constants. On\nLewis’s account, all necessarily true propositions turn out to\nbe topic-neutral. But if there is any point to invoking topic\nneutrality in demarcating logic, it is presumably to distinguish the\nlogical truths from a wider class of necessary propositions, some of\nwhich are subject matter-specific. If we are willing to broaden the\nbounds of logic to encompass all necessary propositions (or,\nalternatively, all analytic sentences), then we might as well demarcate\nlogic as the realm of necessary truth (alternatively, analytic truth).\nIt is only if we want to distinguish the logical from the\ngenerically necessary, or to demarcate logic without appealing to modal\nnotions at all, that we need to invoke topic neutrality. And in neither\nof these cases will Lewis’s criterion of aboutness be of service. \n\nWe rejected Ryle’s criterion for topic-neutrality because it\nappealed to an unclarified notion of aboutness. We rejected Goodman’s\nexplication of aboutness because it assumed that the line between logic\nand non-logic had already been drawn. And we rejected Lewis’s account\nof aboutness because it did not distinguish logical truths from other\nkinds of necessary truths. How else might we cash out the idea that\nlogic is “not about anything in particular”? Two approaches have been\nprominent in the literature. \n\nThe first starts from the idea that what makes an expression\nspecific to a certain domain or topic is its capacity to\ndiscriminate between different individuals. For example, the\nmonadic predicate “is a horse”, the dyadic predicate “is taller than”,\nand the quantifier “every animal” all distinguish between Lucky Feet,\non the one hand, and the Statue of Liberty, on the other: \n\nOn the other hand, the monadic predicate “is a thing”, the dyadic\npredicate “is identical with”, and the quantifier “everything” do not\ndistinguish between Lucky Feet and the Statue of Liberty. In fact, they\ndo not distinguish between any two particular objects. As far\nas they are concerned, one object is as good as another and might just\nas well be switched with it. Expressions with this kind of indifference\nto the particular identities of objects might reasonably be said to be\ntopic-neutral. As we will see in the next section, this notion of topic\nneutrality can be cashed out in a mathematically precise way as\ninvariance under arbitrary permutations of a domain. It is in this\nsense that the basic concepts of arithmetic and set theory are not\ntopic-neutral, since they distinguish some objects (the empty set, the\nnumber 0) from others. \n\nThe second approach locates the topic neutrality of logic in its\nuniversal applicability. On this conception, logic is useful for the\nguidance and criticism of reasoning about any subject\nwhatsoever—natural or artefactual, animate or inanimate, abstract\nor concrete, normative or descriptive, sensible or merely\nconceptual—because it is intimately connected somehow with the\nvery conditions for thought or reasoning. This notion of topic\nneutrality is not equivalent to the one just discussed. It allows that\na science with its own proprietary domain of objects, like arithmetic\nor set theory, might still count as topic-neutral in virtue of its\ncompletely general applicability. Thus, Frege, who took arithmetic to\nbe about numbers, which he regarded as genuine objects, could still\naffirm its absolute topic neutrality: \n\nThe tradition of demarcating the logical constants as expressions\nthat can be characterized by purely inferential introduction and\nelimination rules can be seen as a way of capturing this notion of\ncompletely general applicability. For, plausibly, it is the fact that\nthe logical constants are characterizable in terms of notions\nfundamental to thought or reasoning (for example, valid inference) that\naccounts for their universal applicability. \n\nThe antinomy with which we started can now be resolved\nby disambiguating.  Arithmetic and set theory make distinctions among\nobjects, and so are not topic-neutral\nin the first sense, but they might\nstill be topic-neutral in the second sense, by virtue of their universal\napplicability to reasoning about any subject.\nWe are still faced with a decision about which of\nthese notions of topic neutrality is distinctive of logic. Let us postpone this\nproblem, however, until we have had a closer look at both notions. \n\nA number of philosophers have suggested that what is distinctive of\nlogical constants is their insensitivity to the particular identities\nof objects, or, more precisely, their invariance under\narbitrary permutations of the domain of objects (Mautner 1946;\nMostowski 1957, 13; Scott 1970, 160–161; McCarthy 1981, 1987; Tarski\n1986; van Benthem 1989; Sher 1991, 1996; McGee 1996). \n\nLet us unpack that phrase a bit. A permutation of a\ncollection of objects is a one-one mapping from that collection onto\nitself. Each object gets mapped to an object in the collection\n(possibly itself), and no two objects are mapped to the same object.\nFor example, the following mapping is a permutation of the first five\nletters of the alphabet: \n\nAnd the function \\(f(x) = x + 1\\) is a\npermutation of the set of integers onto itself. (Note, however, that a\npermutation need not be specifiable either by enumeration, as in our\nfirst example, or by a rule, as in our second.) \nThe extension of a predicate is invariant under a permutation of the domain if\nreplacing each of its members with the object to\nwhich the permutation maps it leaves us with the same set we\nstarted with.  Thus, for example, the extension of “is a\nletter between \\(\\textrm{A}\\) and \\(\\textrm{E}\\)” is invariant under the permutation of\nletters described above. By contrast, the\nextension of “is a vowel between \\(\\textrm{A}\\) and \\(\\textrm{E}\\)”,\nthe set \\(\\{\\textrm{A}, \\textrm{E}\\}\\),\nis not invariant under this permutation, which transforms it\nto a different set, \\(\\{\\textrm{C}, \\textrm{D}\\}\\).\n \nWe can make the notion of permutation invariance more precise\nas follows. Given a permutation \\(p\\) of objects on a domain \\(D\\),\nwe define a transformation \\(p^*\\) of\narbitrary types in the hierarchy: \n\nThese clauses can be applied recursively to define\ntransformations of sets of ordered tuples in \\(D\\) (the extensions\nof two-place predicates), sets\nof sets of objects in \\(D\\) (the extensions of unary first-order quantifiers),\nand so on. (For an introduction to\nthe type theoretic hierachy, see\nthe entry on Type Theory.) Where \\(x\\) is an\nitem in this hierarchy, we say that \\(x\\) is invariant\nunder a permutation \\(p\\) just in case \\(p^*(x) = x\\).\nTo return to our example above, the set \n\\(\\{\\textrm{A}, \\textrm{B}, \\textrm{C}, \\textrm{D}, \\textrm{E}\\}\\) is\ninvariant under all permutations of the letters \\(\\textrm{A}\\) through \\(\\textrm{E}\\):\nno matter how we switch these letters around, we end up with the same\nset.  But it is not invariant under all permutations of the entire\nalphabet.  For example, the permutation that switches the letters \\(\\textrm{A}\\)\nand \\(\\textrm{Z}\\), mapping all the other letters to themselves, transforms \n\\(\\{\\textrm{A}, \\textrm{B}, \\textrm{C}, \\textrm{D}, \\textrm{E}\\}\\) to \n\\(\\{\\textrm{Z}, \\textrm{B}, \\textrm{C}, \\textrm{D}, \\textrm{E}\\}\\).  The set containing all the letters,\nhowever, is invariant under all permutations of letters.  So is\nthe set of all sets containing at least two letters, and the relation\nof identity, which holds between each letter and itself. \n\nSo far we have defined permutation invariance for objects, tuples,\nand sets, but not for predicates, quantifiers, or other linguistic\nexpressions. But it is the latter, not the former, that we need to sort\ninto logical and nonlogical constants. The natural thought is that an\nexpression should count as permutation-invariant just in case its\nextension on each domain of objects is invariant under all permutations\nof that domain. (As usual, the extension of a name on a domain is the\nobject it denotes, the extension of a monadic predicate is the set of\nobjects in the domain to which it applies, and the extension of an\n\\(n\\)-adic predicate is the set of \\(n\\)-tuples of objects in\nthe domain to which it applies.) As it stands, this definition does not\napply to sentential connectives, which do not have\nextensions in the usual\n sense,[10]\n but it can be extended to cover them in a natural way (following\nMcGee 1996, 569). We can think of the semantic value of an \\(n\\)-ary\nquantifier or sentential connective \\(C\\) on a domain \\(D\\)\nas a function from \\(n\\)-tuples of sets of assignments (of values\nfrom \\(D\\) to the language’s variables) to sets of\nassignments. Where the input to the function is the \\(n\\)-tuple\nof sets of assignments that satisfy \\(\\phi_1, \\dots, \\phi_n\\),\nits output is the set of assignments\nthat satisfies \\(C\\phi_1 \\dots \\phi_n\\).\n(Check your understanding by thinking about how this works for\nthe unary connective \\(\\exists x\\).)\nWe can then define permutation\ninvariance for these semantic values as follows. Where \\(A\\) is a\nset of assignments and \\(p\\) is a permutation of a domain \\(D\\), let\n\\(p^\\dagger(A) = \\{ p \\circ a : a \\in\nA\\}\\).[11]\n Then if \\(e\\) is the semantic value of an \\(n\\)-place\nconnective or quantifier (in the sense defined above), \\(e\\) is\ninvariant under a permutation \\(p\\) just in case for any\n\\(n\\)-tuple \\(\\langle A_1, \\dots, A_n \\rangle\\) of\nsets of assignments, \\(p^\\dagger (e(\\langle A_1, \\dots, A_n\\rangle)) =\ne(\\langle p^\\dagger (A_1), \\dots, p^\\dagger (A_n)\\rangle\\)).\nAnd a connective or quantifier\nis permutation-invariant just in case its semantic value on each domain of\nobjects is invariant under all permutations of that domain. \n\nIt turns out that this condition does not quite suffice to weed out\nall sensitivity to particular features of objects, for it\nallows that a permutation-invariant constant might behave differently\non domains containing different kinds of objects. McGee (1996, 575)\ngives the delightful example of wombat disjunction, which\nbehaves like disjunction if the domain contains wombats and like\nconjunction otherwise. Sher’s fix, and McGee’s, is to consider not just\npermutations—bijections of the domain onto itself—but\narbitrary bijections of the domain onto another domain of equal\n cardinality.[12]\n For simplicity, we will ignore this\ncomplication in what follows and continue to talk of permutations. \n\nWhich expressions get counted as logical constants, on this\ncriterion? The monadic predicates “is a thing” (which applies to\neverything) and “is not anything” (which applies to nothing), the\nidentity predicate, the truth-functional connectives, and the standard\nexistential and universal quantifiers all pass the test. So do the\nstandard first-order binary quantifiers like “most” and “the” (see\n the entry on \n descriptions).\n Indeed, because cardinality is permutation-invariant, every\ncardinality quantifier is included, including “there are infinitely\nmany”, “there are uncountably many”, and others that are not\nfirst-order definable. Moreover, the second-order quantifiers count as\nlogical (at least on the standard semantics, in which they range over\narbitrary subsets of the domain), as do all higher-order\nquantifiers. On the other hand, all proper names are excluded, as are\nthe predicates “red”, “horse”, “is a successor of”, and “is a member\nof”, as well as the quantifiers “some dogs” and “exactly two natural\nnumbers”. So the invariance criterion seems to accord at least\npartially with common intuitions about logicality or topic neutrality,\nand with our logical practice. Two technical results allow us to be a\nbit more precise about the extent of this accord: Lindenbaum and\nTarski (1934–5) show that all of the relations definable in the\nlanguage of Principia Mathematica are\npermutation-invariant. Moving in the other direction, McGee (1996)\nshows that every permutation-invariant operation can be defined in\nterms of operations with an intuitively logical character (identity,\nsubstitution of variables, finite or infinite disjunction, negation,\nand finite or infinite existential quantification). He also\ngeneralizes the Lindenbaum-Tarski result by showing that every\noperation so definable is permutation invariant. \n\nAs Tarski and others have pointed out, the permutation invariance\ncriterion for logical constants can be seen as a natural generalization\nof Felix Klein’s (1893) idea that different geometries can be\ndistinguished by the groups of transformations under which their basic\nnotions are invariant. Thus, for example, the notions of Euclidean\ngeometry are invariant under similarity transformations, those of\naffine geometry under affine transformations, and those of topology\nunder bicontinuous transformations. In the same way, Tarski suggests\n(1986, 149), the logical notions are just those that are\ninvariant under the widest possible group of transformations: the group\nof permutations of the elements in the domain. Seen in this\nway, the logical notions are the end point of a chain of progressively\nmore abstract, “formal,” or topic-neutral notions defined by their\ninvariance under progressively wider groups of transformations of a\ndomain.[13] \n\nAs an account of the distinctive generality of logic, then,\npermutation invariance has much to recommend it. It is philosophically\nwell-motivated and mathematically precise, it yields results that\naccord with common practice, and it gives determinate rulings about\nsome borderline cases (for example, set-theoretic membership). Best of\nall, it offers hope for a sharp and principled demarcation of logic\nthat avoids cloudy epistemic and semantic terms like “about”,\n“analytic”, and “a\n priori”.[14] \n\nA limitation of the permutation invariance criterion (as it has been\nstated so far) is that it applies only to extensional operators and\nconnectives. It is therefore of no help in deciding, for instance,\nwhether the necessity operator in S4 modal logic or the H\noperator (“it has\nalways been the case that”) in temporal logic are bona\nfide logical constants, and these are among the questions that we\nwanted a criterion to resolve. However, the invariance criterion can be\nextended in a natural way to intensional operators. The usual strategy\nfor handling such operators semantically is to relativize truth not\njust to an assignment of values to variables, but also to a possible\nworld and a time. In such a framework, one might demand that logical\nconstants be insensitive not just to permutations of the domain of\nobjects, but to permutations of the domain of possible worlds and the\ndomain of times (see Scott 1970, 161, McCarthy 1981, 511–13, van\nBenthem 1989, 334). The resulting criterion is fairly stringent: it\ncounts the S5 necessity operator as a logical constant, but not the S4\nnecessity operator or the H operator in temporal logic.\nThe reason is that the latter two operators are sensitive to\nstructure on the domains of worlds and\ntimes—the “accessibility relation” in the former case, the\nrelation of temporal ordering in the latter—and this structure is\nnot preserved by all permutations of these\n domains.[15]\n (See the entries on \n modal logic\n and\n temporal logic.) \n\nOne might avoid this consequence by requiring only invariance under\npermutations that preserve the relevant structure on these domains\n(accessibility relations, temporal ordering). But one would then be\nfaced with the task of explaining why this structure deserves\nspecial treatment (cf. van Benthem 1989, 334). And if we are allowed to\nkeep some structure on the domain of worlds or times fixed, the\nquestion immediately arises why we should not also keep some structure\non the domain of objects fixed: for example, the set-theoretic\nmembership relation, the mereological part/whole relation, or the\ndistinction between existent and nonexistent objects (see the\nentry on free logics).\nWhatever resources we appeal to in answering this question\nwill be doing at least as much work as permutation invariance in the\nresulting demarcation of logical constants. \n\nIt may seem that the only principled position is to demand\ninvariance under all permutations. But even that position\nneeds justification, especially when one sees that it is possible to\nformulate even stricter invariance conditions. Feferman (1999) defines\na “similarity invariance” criterion that counts the truth-functional\noperators and first-order existential and universal quantifiers as\nlogical constants, but not identity, the first-order cardinality\nquantifiers, or the second-order quantifiers. Feferman’s criterion\ndraws the line between logic and mathematics much closer to the\ntraditional boundary than the permutation invariance criterion does.\nIndeed, one of Feferman’s criticisms of the permutation invariance\ncriterion is that it allows too many properly mathematical notions to\nbe expressed in purely logical terms.  Bonnay (2008) argues for\na different criterion, invariance under potential isomorphism, which\ncounts finite cardinality quantifiers and the notion of finiteness as\nlogical, while excluding the higher cardinality\nquantifiers—thus “[setting] the boundary between\nlogic and mathematics somewhere between arithmetic and set theory”\n(37; see Feferman 2010, §6, for further discussion).\nFeferman (2010) suggests that instead of relying solely\non invariance, we might combine invariance under permutations\nwith a separate absoluteness requirement, which captures the\ninsensitivity of logic to controversial set-theoretic theses like\naxioms of infinity.  He shows that the logical operations that\nare both permutation-invariant and absolutely definable\nwith respect to Kripke–Platek set theory without an\naxiom of infinity are just those definable in first-order logic.\n \n\nThere is another problem that afflicts any attempt to demarcate the\nlogical constants by appeal to mathematical properties like invariance.\nAs McCarthy puts it: “the logical status of an expression is not\nsettled by the functions it introduces, independently of how these\nfunctions are specified” (1981, 516). Consider a two-place\npredicate \\(\\dq{\\approx}\\), whose meaning is given by the following\ndefinition: \n\nAccording to the invariance criterion, \\(\\dq{\\approx}\\) is a logical\nconstant just in case its extension on every domain is invariant under\nevery permutation of that domain. On a domain \\(D\\) containing no\ntwo objects with exactly the same mass, \\(\\dq{\\approx}\\) has the same\nextension as \\(\\dq{=}\\)—the set \\(\\{ \\langle x, x \\rangle :\nx \\in D\\}\\)—and as we have seen, this extension\nis invariant under every permutation of the domain. Hence, if there is\nno domain containing two objects with exactly the same mass,\n\\(\\dq{\\approx}\\) counts as a logical constant, and\n\\(\\dq{\\forall x (x \\approx x)}\\) as a logical\ntruth.[16]\nBut it\nseems odd that the logical status of \\(\\dq{\\approx}\\) and\n\\(\\dq{\\forall x (x \\approx x)}\\) should depend on a matter of\ncontingent fact: whether there are distinct objects with identical\nmass. Do we really want to say that if we lived in a world in which no\ntwo objects had the same mass, \\(\\dq{\\approx}\\) would be a logical\n constant?[17] \n\nA natural response to this kind of objection would be to require\nthat the extension of a logical constant on every possible\ndomain of objects be invariant under every permutation of that domain,\nor, more generally, that a logical constant satisfy the permutation\ninvariance criterion as a matter of necessity. But this would\nnot get to the root of the problem. For consider the unary connective\n\\(\\dq{\\#}\\), defined by the clause \nAssuming that Kripke (1971; 1980) is right that water is necessarily\nH2O,\n\\(\\dq{\\#}\\) has the same extension as\n\\(\\dq{\\neg}\\) in every possible world, and so satisfies the permutation\ninvariance criterion as a matter of necessity (McGee 1996, 578). But intuitively, it does\nnot seem that \\(\\dq{\\#}\\) should be counted a logical\n constant.[18] \n\nOne might evade this counterexample by appealing to an\nepistemic modality instead of a metaphysical one. This is\nMcCarthy’s strategy (1987, 439). Even if it is metaphysically necessary\nthat water is H2O, there are\npresumably epistemically possible worlds, or information states, in\nwhich water is not H2O. So if we\nrequire that a logical constant be permutation invariant as a matter of\nepistemic necessity (or a priori), \\(\\dq{\\#}\\) does not count as a\nlogical constant. But even on this version of the criterion, a\nconnective like \\(\\dq{\\%}\\), defined by \nwould count as a logical constant (Gómez-Torrente\n2002, 21), assuming that it is epistemically necessary that there are\nno male widows. It may be tempting to solve this problem by\nappealing to a distinctively logical modality—requiring,\nfor example, that logical constants have permutation-invariant\nextensions as a matter of logical necessity. But we would then\nbe explicating the notion of a logical constant in terms of an obscure\nprimitive notion of logical necessity which we could not, on pain of\ncircularity, explicate by reference to logical constants. (McCarthy\n1998, §3 appeals explicitly to logical possibility and notices the\nthreat of circularity here.) \n\nMcGee’s strategy is to invoke semantic notions instead of modal\nones: he suggests that “[a] connective is a logical connective if and\nonly if it follows from the meaning of the connective that it is\ninvariant under arbitrary bijections” (McGee 1996, 578). But this\napproach, like McCarthy’s, seems to count \\(\\dq{\\%}\\) as a logical constant.\nAnd, like McCarthy’s, it requires appeal to a notion that does not seem\nany clearer than the notion of a logical constant: the notion of\nfollowing (logically?) from the meaning of the connective. \n\nSher’s response to the objection is radically different from McGee’s\nor McCarthy’s. She suggests that “logical terms are identified with\ntheir (actual) extensions,” so that \\(\\dq{\\#}\\), \\(\\dq{\\%}\\),\nand \\(\\dq{\\neg}\\) are just\ndifferent notations for the same term. More precisely: if these\nexpressions are used the way a logical constant must be used—as\nrigid\n designators[19]\n of their semantic values—then they can be identified with the\noperation of Boolean negation and hence with each other. “Qua\nquantifiers, ‘the number of planets’ and ‘9’ are\nindistinguishable” (Sher 1991, 64). But it is not clear what Sher can\nmean when she says that logical terms can be identified with their\nextensions. We normally individuate connectives\nintentionally, by the conditions for grasping them or the\nrules for their use, and not by the truth functions they express. For\nexample, we recognize a difference between \\(\\dq{\\and}\\), defined by\n and \\(\\dq{@}\\), defined by \neven though they express the same truth function. The distinction\nbetween these terms is not erased, as Sher seems to suggest, if we use\nthem as rigid designators for the truth functions they express. (That\n“Hesperus”, “Phosphorus”, and “the planet\nI actually saw near the horizon on the morning of November 1, 2004”\nall rigidly designate Venus\ndoes not entail that they have the same meaning.)\nThus Sher’s proposal can only be\nunderstood as a stipulation that if one of a pair of\ncoreferential rigid designators counts as a logical constant, the other\ndoes too. But it is not clear why we should accept this stipulation. It\ncertainly has some counterintuitive consequences: for example, that\n\\(\\dq{P \\vee \\#P}\\) is a logical\ntruth, at least when \\(\\dq{\\#}\\) is used rigidly (see Gómez-Torrente\n2002, 19, and the response in Sher 2003). \n\nIt is hard not to conclude from these discussions that the\npermutation invariance criterion gives at best a necessary condition\nfor logical constancy. Its main shortcoming is that it operates at the\nlevel of reference rather than the level of sense; it looks at the\nlogical operations expressed by the constants, but not at their\nmeanings. An adequate criterion, one might therefore expect, would\noperate at the level of sense, perhaps attending to the way we\ngrasp the meanings of logical constants. \n\nAt the end of the section on topic neutrality, we distinguished two\nnotions of topic neutrality. The first notion—insensitivity to\nthe distinguishing features of individuals—is effectively\ncaptured by the permutation invariance criterion. How might we capture\nthe second—universal applicability to all thought or reasoning,\nregardless of its subject matter? We might start by identifying certain\ningredients that must be present in anything that is to count as\nthought or reasoning, then class as logical any expression that can be\nunderstood in terms of these ingredients alone. That would ensure a\nspecial connection between the logical constants and thought or\nreasoning as such, a connection that would explain logic’s universal\napplicability. \n\nAlong these lines, it has been proposed that the logical constants\nare just those expressions that can be characterized by a set of purely\ninferential introduction and elimination\n rules.[20]\n To grasp the meaning\nof the conjunction connective \\(\\dq{\\and}\\), for example, it is arguably\nsufficient to learn that it is governed by the rules:\n\n\\begin{equation*}\n\\frac{A, B}{A \\and B}\n\\quad\n\\frac{A \\and B}{A}\n\\quad\n\\frac{A \\and B}{B}\n\\end{equation*}\n\nThus the meaning of \\(\\dq{\\and}\\) can be grasped by anyone who understands\nthe significance of the horizontal line in an inference rule. (Contrast\n\\(\\dq{\\%}\\) from the last section, which cannot be grasped by anyone who does\nnot understand what a male is and what a widow is.) Anyone who is\ncapable of articulate thought or reasoning at all should be able to\nunderstand these inference rules, and should therefore be in a position\nto grasp the meaning of \\(\\dq{\\and}\\). Or so the thought\n goes.[21] \n\nTo make such a proposal precise, we would have to make a number of\nadditional decisions: \n\nWe would have to decide whether to use natural deduction rules or\nsequent rules.  (See the entry on\nthe development of\n  proof theory.) \n\nIf we opted to use sequent rules, we would have to decide whether or\nnot to allow “substructure” (see the entry on\n substructural logics)\n and whether to allow multiple conclusions in the sequents. We would\nalso have to endorse a particular set of purely structural rules\n(rules not involving any expression of the language essentially). \n\nWe would have to specify whether it is introduction or elimination\nrules, or both, that are to characterize the meanings of logical\nconstants.[22]\n (In a sequent formulation, we would have to\ndistinguish between right and left introduction and elimination\nrules.) \n\nWe would have to allow for subpropositional structure in our rules,\nin order to make room for quantifier rules. \n\nWe would have to say when an introduction or elimination rule counts\nas “purely inferential,” to exclude rules like these:\n\n\\begin{equation*}\n\\frac{a~\\text{is red}}{Ra}\n\\quad\n\\frac{A, B, \\text{water is}~H_{2}O}{A * B}\n\\end{equation*}\n\nThe strictest criterion would allow only rules in which every sign,\nbesides a single instance of the constant being characterized, is\neither structural (like the comma) or schematic (like \\(\\dq{A}\\)).\nBut although this condition is met by the standard rules for\nconjunction, it is not met by the natural deduction introduction rule\nfor negation, which must employ either another logical constant\n(\\(\\dq{\\bot}\\)) or another instance of the negation sign than the one being\nintroduced. Thus one must either relax the condition for being “purely\ninferential” or add more structure (see especially Belnap 1982). \n\nDifferent versions of the inferential characterization approach make\ndifferent decisions about these matters, and these differences affect\nwhich constants get certified as “logical.” For example, if we use\nsingle-conclusion sequents with the standard rules for the constants,\nwe get the intuitionistic connectives, while if we use\nmultiple-conclusion sequents, we get the classical connectives (Kneale\n1956, 253). If we adopt Došen’s constraints on acceptable rules\n(Došen 1994, 280), the S4 necessity operator gets counted as a\nlogical constant, while if we adopt Hacking’s constraints, it doesn’t\n(Hacking 1979, 297). Thus, if we are to have any hope of deciding the\nhard cases in a principled way, we will have to motivate all of the\ndecisions that distinguish our version of the inferential\ncharacterization approach from the others. Here, however, we will avoid\ngetting into these issues of detail and focus instead on the basic\nidea. \n\nThe basic idea is that the logical constants are distinguished from\nother sorts of expressions by being “characterizable” in terms of\npurely inferential rules. But what does “characterizable” mean here? As\nGómez-Torrente (2002, 29) observes, it might be taken to require\neither the fixation of reference (semantic value) or the fixation of\nsense: \n\nSemantic value determination: A constant \\(c\\) is\ncharacterizable by rules \\(R\\) iff its being governed by\n\\(R\\) suffices to fix its reference or semantic value (for\nexample, the truth function it expresses), given certain semantic\nbackground assumptions (Hacking 1979, 299, 313). \n\n\nSense determination: A constant \\(c\\) is\ncharacterizable by rules \\(R\\) iff its being governed by \\(R\\) suffices\nto fix its sense: that is, one can grasp the sense of \\(c\\) simply by\nlearning that it is governed by \\(R\\) (Popper 1946–7, 1947; Kneale\n1956, 254–5; Peacocke 1987; Hodes 2004, 135). \n\nLet us consider these two versions of the inferential\ncharacterization approach in turn. \n\nHacking shows that, given certain background semantic assumptions\n(bivalence, valid inference preserves truth), any introduction and\nelimination rules meeting certain proof-theoretic conditions\n(subformula property, provability of elimination theorems for Cut,\nIdentity, and Weakening) will uniquely determine a semantics for the\nconstant they govern (Hacking 1979, 311–314). It is in this sense that\nthese rules “fix the meaning” of the constant: “they are such that if\nstrong semantic assumptions of a general kind are made, then the\nspecific semantics of the individual logical constants is thereby\ndetermined” (313). \n\nThe notion of determination of semantic value in a well-defined\nsemantic framework is, at least, clear—unlike the general notion\nof determination of sense. However, as Gómez-Torrente points\nout, by concentrating on the fixation of reference (or semantic value)\nrather than sense, Hacking opens himself up to an objection not unlike\nthe objection to permutation-invariance approaches we considered above\n(see also Sainsbury 2001, 369). Consider the quantifier \\(\\dq{W}\\),\nwhich means “not for all not …, if all are not male widows, and for\nall not …, if not all are not male widows” (Gómez-Torrente\n2002, 29). (It is important here that \\(\\dq{W}\\) is a primitive sign\nof the language, not one introduced by a definition in terms of\n\\(\\dq{\\forall}\\), \\(\\dq{\\neg}\\), “male”, and\n“widow”.) Since there are no male\nwidows, \\(\\dq{W}\\) has the same semantic value as our ordinary\nquantifier \\(\\dq{\\exists}\\). (As above, we can think of the semantic value of\na quantifier as a function from sets of assignments to sets of\nassignments.) Now let \\(R\\) be the standard introduction and\nelimination rules for \\(\\dq{\\exists}\\), and let \\(R'\\) be the\nresult of substituting \\(\\dq{W}\\) for \\(\\dq{\\exists}\\) in these rules.\nClearly, \\(R'\\) is no less “purely inferential” than\n\\(R\\). And if \\(R\\) fixes a semantic value for\n\\(\\dq{\\exists}\\), then \\(R'\\) fixes a semantic value—the very\nsame semantic value—for \\(\\dq{W}\\). So if logical constants are\nexpressions whose semantic values can be fixed by means of purely\ninferential introduction and elimination rules, \\(\\dq{W}\\) counts as\na logical constant if and only if \\(\\dq{\\exists}\\) does. \n\nYet intuitively there is an important difference between these\nconstants. We might describe it this way: whereas learning the rules\n\\(R\\) is sufficient to impart a full grasp of \\(\\dq{\\exists}\\), one could\nlearn the rules \\(R'\\) without fully understanding what is\nmeant by \\(\\dq{W}\\). To understand \\(\\dq{W}\\) one must know about\nthe human institution of marriage, and that accounts for our feeling\nthat \\(\\dq{W}\\) is not “topic-neutral” enough to be a logical\nconstant. However, this difference between \\(\\dq{W}\\) and \\(\\dq{\\exists}\\)\ncannot be discerned if we talk only of reference or semantic value; it\nis a difference in the senses of the two expressions. \n\nThe idea that introduction and/or elimination rules fix the\nsense of a logical constant is often motivated by talk of the\nrules as defining the constant.  Gentzen remarks that the natural\ndeduction rules “represent, as it were, the\n‘definitions’ of the symbols concerned, and the eliminations are no\nmore, in the final analysis, than the consequences of these definitions”\n(1935, §5.13; 1969, 80).\nHowever, a genuine definition would permit the constant to be\neliminated from every context in which it occurs (see the entry\non Definitions), and introduction and\nelimination rules for logical constants do not, in general, permit\nthis. For example, in an intuitionistic sequent calculus, there is no\nsequent (or group of sequents) not containing \\(\\dq{\\rightarrow}\\)\nthat is equivalent to the sequent \\(\\dq{A \\rightarrow B \\vdash C}\\).\nFor this reason, Kneale\n(1956, 257) says only that we can “treat” the rules as definitions,\nHacking (1979) speaks of the rules “not as defining but only as characterizing\nthe logical constants,” and Došen (1994) says that the rules\nprovide only an “analysis,” not a\n definition.[23] \n\nHowever, even if the rules are not “definitions,” there may still be\nsomething to say for the claim that they “fix the senses” of the\nconstants they introduce. For it may be that a speaker’s grasp of the\nmeaning of the constants consists in her mastery of these rules: her\ndisposition to accept inferences conforming to the rules as\n“primitively compelling” (Peacocke 1987, Hodes 2004). (A speaker finds\nan inference form primitively compelling just in case she finds it\ncompelling and does not take its correctness to require external\nratification, e.g. by inference.) If the senses of logical constants\nare individuated in this way by the conditions for their grasp, we can\ndistinguish between truth-functionally equivalent constants with\ndifferent meanings, like\n\\(\\dq{\\vee}\\), \\(\\dq{\\ddagger}\\), and \\(\\dq{\\dagger}\\),\nas defined below: \n\nTo understand \\(\\dq{\\vee}\\) one must find\nthe standard introduction rules primitively\ncompelling: \n\nTo understand \\(\\dq{\\ddagger}\\) one must find the\nfollowing elimination rule primitively compelling: \n\nFinally, to grasp the sense of \\(\\dq{\\dagger}\\) one must find these\nintroduction rules primitively compelling: \n\n\\(\\dq{\\vee}\\) and \\(\\dq{\\ddagger}\\) will count as\nlogical constants, because their sense-constitutive rules are purely\ninferential, while \\(\\dq{\\dagger}\\) will not, because its rules are not.\n(In the same way we can distinguish \\(\\dq{\\exists}\\) from\n\\(\\dq{W}\\).) Note that appropriately rewritten versions of \\(\\refp{or-intro}\\)\nwill hold for \\(\\dq{\\ddagger}\\) and \\(\\dq{\\dagger}\\); the difference is\nthat one can grasp \\(\\dq{\\ddagger}\\) and \\(\\dq{\\dagger}\\) (but not\n\\(\\dq{\\vee}\\)) without finding these rules primitively\ncompelling (Peacocke 1987, 156; cp. Sainsbury 2001, 370–1). \n\nSome critics have doubted that the introduction and elimination\nrules for the logical constants exhaust the aspects of the use of these\nconstants that must be mastered if one is to understand them. For\nexample, it has been suggested that in order to grasp the conditional\nand the universal quantifier, one must be disposed to treat certain\nkinds of inductive evidence as grounds for the assertion of\nconditionals and universally quantified claims (Dummett 1991, 275–8;\nGómez-Torrente 2002, 26–7; Sainsbury 2001, 370–1). It is not\nclear that these additional aspects of use can be captured in “purely\ninferential” rules, or that they can be derived from aspects of use\nthat can be so captured. \n\nIt is sometimes thought that Prior’s (1960) example of a\nconnective “tonk,” \nwhose rules permit\ninferring anything from anything, decisively refutes\nthe idea that the senses of logical constants are fixed\nby their introduction and/or elimination rules. But although\nPrior’s example\n(anticipated in Popper 1946–7, 284) certainly shows that not all sets of\nintroduction and elimination rules determine a coherent meaning for a\nlogical constant, it does not show that none do,\nor that the logical constants are\nnot distinctive in having their meanings determined in this way.\nFor some attempts to articulate conditions under\nwhich introduction and elimination rules do fix a meaning, see Belnap\n(1962), Hacking (1979, 296–8), Kremer (1988, 62–6),\nand Hodes (2004, 156–7). \nPrawitz (1985; 2005) argues that\n any formally suitable introduction rule can fix the meaning for a\nlogical constant. On Prawitz’s view, the lesson we learn from Prior is that we cannot\nalso stipulate an elimination rule, but must\njustify any proposed elimination rule by showing that there is a procedure\nfor rearranging any direct proof of the premises\nof the elimination rule into a direct proof of the conclusion.\nThus, we can stipulate\nthe introduction rule for “tonk”, but must then\ncontent ourselves with the strongest elimination rule for which such a procedure\nis available: \nOther philosophers reject Prawitz's (and Gentzen's) view that the introduction\nrules have priority in fixing the meanings of constants, but\nretain the idea that the introduction and elimination rules that fix the meaning of\na constant must be in harmony:\nthe elimination rules must not permit us to infer more from a compound sentence\nthan would be justified by the premises of the corresponding introduction rules\n(Dummett 1981, 396; Tennant 1987, 76-98).\n(For analyses of various notions of harmony, and their relation to notions like\nnormalizability and conservativeness, see Milne 1994, Read 2010, and Steinberger 2011.)\n \n\nThe proposals for demarcating logical constants that we have\nexamined so far have all been analytical demarcations. They\nhave sought to identify some favored property (grammatical\nparticlehood, topic neutrality, permutation invariance,\ncharacterizability by inferential rules, etc.) as a necessary and\nsufficient condition for an expression to be a logical constant. A\nfundamentally different strategy for demarcating the constants is to\nstart with a job description for logic and identify the\nconstants as the expressions that are necessary to do that job. For\nexample, we might start with the idea that the job of logic is to serve\nas a “framework for the deductive sytematization of scientific\ntheories” (Warmbrod 1999, 516), or to characterize mathematical\nstructures and represent mathematical reasoning (Shapiro 1991), or to\n“[express] explicitly within a language the features of the\nuse of that language that confer conceptual contents on the states,\nattitudes, performances, and expressions whose significances are\ngoverned by those practices” (Brandom 1994, xviii). Let us call\ndemarcations of this kind pragmatic demarcations. \n\nThere are some very general differences between the two kinds of\ndemarcations. Unlike analytical demarcations, pragmatic demarcations\nare guided by what Warmbrod calls a “requirement of minimalism”: \n\nOr, in Harman’s pithier formulation: “Count as logic only as much as\nyou have to” (Harman 1972, 79). Warmbrod uses this constraint to argue\nthat the theory of identity is not part of logic, on the grounds that it is\nnot needed to do the job he has identified for logic: “[w]e can\nsystematize the same sets of sentences by recognizing only the\ntruth-functional connectives and first-order quantifiers as constants,\ntreating ‘=’ as an ordinary predicate, and adopting\n appropriate axioms\nfor identity” (521; cf. Quine 1986, 63, 1980, 28). On similar grounds,\nboth Harman and Warmbrod argue that modal operators should not be\nconsidered part of\n logic.[24]\n Their point is not that identity or\nmodal operators lack some feature that the first-order quantifiers and\ntruth-functional operators possess, but merely that, since we\ncan get by without taking these notions to be part of our\nlogic, we should. Warmbrod and Tharp even explore the possibility of\ntaking truth-functional logic to be the whole of logic and viewing\nquantification theory as a non-logical theory (Warmbrod 1999, 525;\nTharp 1975, 18), though both reject this idea on pragmatic grounds. \n\nWhile pragmatic demarcations seek to minimize what counts as logic,\nanalytical demarcations are inclusive. They count as logical\nany expression that has the favored property. It is simply\nirrelevant whether an expression is required for a particular\npurpose: its logicality rests on features that it has independently of\nany use to which we might put it. \n\nRelatedly, pragmatic approaches tend to be holistic. Because it is\nwhole logical systems that can be evaluated as sufficient or\ninsufficient for doing the “job” assigned to logic, properties of\nsystems tend to be emphasized in pragmatic demarcations. For example,\nWagner (1987, 10–11) invokes Lindstrom’s theorem—that first-order\nlogic is the only logic that is either complete or compact and\nsatisfies the Löwenheim-Skolem theorem—in arguing that logic\nshould be limited to first-order logic, and Kneale and Kneale (1962,\n724, 741) invoke Gödel’s incompleteness theorems to similar\neffect. Although nothing about the idea of an analytical demarcation\nexcludes appeal to properties of whole systems, analytical demarcations\ntend to appeal to local properties of particular expressions\nrather than global systemic properties. \n\nFinally, on a pragmatic demarcation, what counts as logic may depend\non the current state of scientific and mathematical theory. If the\nadvance of science results in an increase or decrease in the resources\nneeded for deductive systematization of science (or whatever is the\nfavored task of logic), what counts as logic changes accordingly\n(Warmbrod 1999, 533). On an analytical demarcation, by contrast,\nwhether particular resources are logical depends only on whether they\nhave the favored property. If they do not, and if it turns out that\nthey are needed for the deductive systematization of theories, then the\nproper conclusion to draw is that logic alone is not adequate for this\ntask. \n\nNow that we have gotten a sense for the tremendous variety of\napproaches to the problem of logical constants, let us step back and\nreflect on the problem itself and its motivation. We can distinguish\nfour general attitudes toward the problem of logical constants: those\nof the Demarcater, the Debunker, the Relativist, and the Deflater. \n\nDemarcaters hold that the demarcation of logical constants\nis a genuine and important problem, whose solution can be expected to\nilluminate the nature and special status of logic. On their view, the\ntask of logic is to study features that arguments possess in virtue of\ntheir logical forms or\n structures.[25]\n Although there may be some sense in\nwhich the argument \nis a good or “valid” argument, it is not formally valid.  On\nthe Demarcater’s view, logicians who investigate the (non-formal) kind\nof “validity” possessed by \\(\\refp{chicago-north}\\) are straying from\nthe proper province\nof logic into some neighboring domain (here, geography or lexicography;\nin other cases, mathematics or metaphysics). For the Demarcater, then,\nunderstanding the distinction between logical and nonlogical constants\nis essential for understanding what logic is about. (For a forceful\nstatement of the Demarcater’s point of view, see Kneale 1956.) \n\nDebunkers, on the other hand, hold that the so-called\n“problem of logical constants” is a pseudoproblem (Bolzano 1929,\n§186; Lakoff 1970, 252–4; Coffa 1975; Etchemendy 1983, 1990,\nch. 9; Barwise and Feferman 1985, 6; Read 1994). They do not\ndispute that logicians have traditionally concerned themselves with\nargument forms in which a limited number of expressions occur\nessentially. What they deny is that these expressions and argument\nforms define the subject matter of logic. On their view, logic\nis concerned with validity simpliciter, not just validity that\nholds in virtue of a limited set of “logical forms.” The logician’s\nmethod for studying validity is to classify arguments by their\nforms, but these forms (and the logical constants that in part define\nthem) are logic’s tools, not its subject matter. The forms and\nconstants with which logicians are concerned at a particular point in\nthe development of logic are just a reflection of the logicians’\nprogress (up to that point) in systematically classifying valid\ninferences. Asking what is special about these forms and constants is\nthus a bit like asking what is special about the mountains that can be\nclimbed in a day: “The information so derived will be too closely\ndependent upon the skill of the climber to tell us much about\ngeography” (Coffa 1975, 114). What makes people logicians is not their\nconcern with “and”, “or”, and “not”, but their concern with validity,\nconsequence, consistency, and proof, and the distinctive methods they\nbring to their investigations. \n\nA good way to see the practical difference between Debunkers and\nDemarcaters is by contrasting their views on the use of counterexamples\nto show invalidity. Demarcaters typically hold that one can show an\nargument to be invalid by exhibiting another argument with the same\nlogical form that has true premises and a false conclusion. Of course,\nan argument will always instantiate multiple forms. For example, the\nargument can be seen as an instance of the propositional logical form as well as the more articulated form \nAs Massey (1975) reminds us, the fact that there are other arguments\nwith the form \\(\\refp{propform}\\) that have true premises and a false conclusion does\nnot show that \\(\\refp{firefighter}\\) is invalid (or even that it is\n“formally” invalid).\nThe Demarcater will insist that a genuine counterexample to the formal\nvalidity of \\(\\refp{firefighter}\\) would have to exhibit the full\nlogical structure of \\(\\refp{firefighter}\\), which is not \\(\\refp{propform}\\) but\n\\(\\refp{quantform}\\).  Thus the Demarcater’s\nuse of counterexamples to demonstrate the formal invalidity of\narguments presupposes a principled way of discerning the full\nlogical structure of an argument, and hence of distinguishing logical\nconstants from nonlogical\n constants.[26] \n\nThe Debunker, by contrast, rejects the idea that one of the many\nargument forms \\(\\refp{firefighter}\\) instantiates should be privileged as\nthe logical form of \\(\\refp{firefighter}\\). On the Debunker’s\nview, counterexamples never\nshow anything about a particular argument. All they show is that a\nform is invalid (that is, that it has invalid instances). To\nshow that a particular argument is invalid, one sort of Debunker\nholds, one needs to describe a possible situation in which the premises\nwould be true and the conclusion false, and to give a formal\ncounterexample is not to do that. \n\nThe Demarcater will object that the Debunker’s tolerant attitude\nleaves us with no coherent distinction between logic and other\ndisciplines. For surely it is the chemist, not the logician, who will\nbe called upon to tell us whether the following argument is a good\none: \nWithout a principled distinction between logical and nonlogical\nconstants, it seems, logic would need to be a kind of universal\nscience: not just a canon for inference, but an encyclopedia. If logic\nis to be a distinctive discipline, the Demarcater will argue, it must\nconcern itself not with all kinds of validity or goodness of arguments,\nbut with a special, privileged kind: formal validity. \n\nAgainst this, the Debunker might insist that deductive validity is a\nfeature arguments have by virtue of the meanings of the terms contained\nin them, so that anyone who understands the premises and conclusion of\nan argument must be in a position to determine, without recourse to\nempirical investigation, whether it is valid. On this conception, logic\nis the study of analytic truth, consequence, consistency, and\nvalidity. Because the relation between premise and conclusion in\n\\(\\refp{litmus}\\)\ndepends on empirical facts, not the meanings of terms, \\(\\refp{litmus}\\) is not\ndeductively\n valid.[27] \n\nThis response will not be available to those who have reservations\nabout the\n analytic/synthetic distinction.\n An important example is Tarski (1936a; 1936b; 1983;\n1987; 2002), who was much concerned to define logical truth and\nconsequence in purely mathematical terms, without appealing to suspect\nmodal or epistemic notions. On Tarski’s account, an argument is valid\njust in case there is no interpretation of its nonlogical constants on\nwhich the premises are true and the conclusion false. On this account,\nan argument containing no nonlogical constants is valid just in case it\nis materially truth-preserving (it is not the case that its premises\nare true and its conclusion false). Thus, as Tarski notes, if\nevery expression of a language counted as a logical constant,\nlogical validity would reduce to material truth preservation (or, on\nlater versions of Tarski’s definition, to material truth preservation\non every nonempty domain) (1983, 419). Someone who found this result\nintolerable might take it to show either that there must be a\nprincipled distinction between logical and nonlogical constants (the\nDemarcater’s conclusion), or that Tarski’s definition is misguided (the\nDebunker’s conclusion; see Etchemendy 1990, ch. 9). \n\nTarski’s own reaction was more cautious. After concluding that the\ndistinction is “certainly not quite arbitrary” (1983, 418), he\nwrites: \n\nHere Tarski is describing a position distinct from both the\nDemarcater’s position and the Debunker’s. The Relativist\nagrees with the Demarcater that logical consequence must be understood\nas formal consequence, and so presupposes a distinction\nbetween logical and nonlogical constants. But she agrees with the\nDebunker that we should not ask, “Which expressions are logical\nconstants and which are not?” The way she reconciles these apparently\nconflicting positions is by relativizing logical consequence\nto a choice of logical constants. For each set C of logical\nconstants, there will be a corresponding notion of\nC-consequence. None of these notions is to be identified with\nconsequence simpliciter; different ones are useful for\ndifferent purposes. In the limiting case, where every expression of the\nlanguage is taken to be a logical constant, we get material\nconsequence, but this is no more (and no less) the consequence\nrelation than any of the others. \n\nLike the Relativist, the Deflater seeks a moderate middle\nground between the Demarcater and the Debunker. The Deflater agrees\nwith the Demarcater that there is a real distinction between logical\nand nonlogical constants, and between formally and materially valid\narguments. She rejects the Relativist’s position that logical\nconsequence is a relative notion. But she also rejects the Demarcater’s\nproject of finding precise and illuminating necessary and sufficient\nconditions for logical constancy. “Logical constant”, she holds, is a\n“family resemblance” term, so we should not expect to uncover a hidden\nessence that all logical constants share. As Wittgenstein said about\nthe concept of number: “the strength of the thread does not reside in\nthe fact that some one fibre runs through its whole length, but in the\noverlapping of many fibres” (Wittgenstein 1958, §67). That does\nnot mean that there is no distinction between logical and nonlogical\nconstants, any more than our inability to give a precise definition of\n“game” means that there is no difference between games and other\nactivities. Nor does it mean that the distinction does not matter. What\nit means is that we should not expect a principled criterion for\nlogical constancy that explains why logic has a privileged\nepistemological or semantic status. (For a nice articulation of this\nkind of view, see Gómez-Torrente 2002.) \n\nThe debate between these four positions cannot be resolved here,\nbecause to some extent “the proof is in the pudding.” A compelling and\nilluminating account of logical constants—one that vindicated a\ndisciplinary segregation of \\(\\refp{chicago-north}\\) from\n\\(\\refp{firefighter}\\) by showing how these arguments are importantly\ndifferent—might give us reason to be Demarcaters. But it is\nimportant not to get so caught up in the debates between different\nDemarcaters, or between Demarcaters and Debunkers, that one loses sight\nof the other positions one might take toward the problem of logical\n constants. \n\nOther recent general discussions of the problem of logical constants\ninclude Peacocke 1976, McCarthy 1998, Warmbrod 1999, Sainsbury 2001,\nch. 6, and Gómez-Torrente 2002. Tarski 1936b is essential\nbackground to all of these. \n\nFor a discussion of grammatical criteria for logical terms, see\nQuine 1980 and Føllesdal’s (1980) reply. \n\nFor a discussion of the Davidsonian approach, see\nDavidson 1984, Evans 1976, Lycan 1989, Lepore and Ludwig 2002,\nand Edwards 2002. \n\nTarski 1986 is a brief and cogent exposition of the\npermutation-invariance approach. For elaboration and\ncriticism, see McCarthy 1981, van Bentham 1989,\nSher 1991, McGee 1996, Feferman 1999 and 2010, Bonnay 2008,\nand Dutilh Novaes 2014. Bonnay 2014 surveys recent work\nin this area.\n \n\nHacking 1979 and Peacocke 1987 are good representatives of the two\nversions of the inferential characterization approach discussed above.\nPopper’s papers (1946–7, 1947) are still worth reading; see\nSchroeder-Heister 1984 for critical discussion and Koslow 1999 for a\nmodern approach reminiscent of Popper’s. See also Kneale 1956, Kremer\n1988, Prawitz 1985 and 2005, Tennant 1987, ch. 9, Dummett 1991, ch. 11, Došen 1994, Hodes\n2004, and Read 2010. \n\nFor examples of pragmatic demarcations, see Wagner 1987 and\nWarmbrod 1999.  A different kind of pragmatic approach can\nbe found in Brandom (2000, ch. 1; 2008, ch. 2), who characterizes\nlogical vocabulary in terms of its expressive role. \n\nFor critiques of the whole project of demarcating the logical\nconstants, see Coffa 1975, Etchemendy (1983; 1990, ch. 9), and Read\n1994.","contact.mail":"jgm@berkeley.edu","contact.domain":"berkeley.edu"}]
