[{"date.published":"2014-12-20","date.changed":"2019-02-19","url":"https://plato.stanford.edu/entries/imprecise-probabilities/","author1":"Seamus Bradley","author1.info":"http://www.seamusbradley.net/","entry":"imprecise-probabilities","body.text":"\n\n\n\nIt has been argued that imprecise probabilities are a natural and\nintuitive way of overcoming some of the issues with orthodox precise\nprobabilities.  Models of this type have a long pedigree, and interest\nin such models has been growing in recent years.  This article\nintroduces the theory of imprecise probabilities, discusses the\nmotivations for their use and their possible advantages over the\nstandard precise model.  It then discusses some philosophical issues\nraised by this model.  There is also a historical appendix which\nprovides an overview of some important thinkers who appear sympathetic\nto imprecise probabilities.\n\n\nProbability theory has been a remarkably fruitful theory, with\napplications in almost every branch of science. In philosophy, some\nimportant applications of probability theory go by the name\nBayesianism; this has been an extremely successful\nprogram (see for example Howson and Urbach\n2006; Bovens and Hartmann 2003; Talbott 2008). But probability\ntheory seems to impute much richer and more determinate attitudes than\nseems warranted. What should your rational degree of belief be that\nglobal mean surface temperature will have risen by more than four\ndegrees by 2080? Perhaps it should be 0.75? Why not 0.75001? Why not\n0.7497? Is that event more or less likely than getting at least one\nhead on two tosses of a fair coin? It seems there are many events\nabout which we can (or perhaps should) take less precise attitudes\nthan orthodox probability requires. Among the reasons to question the\northodoxy, it seems that the insistence that states of belief be\nrepresented by a single real-valued probability function is quite an\nunrealistic idealisation, and one that brings with it some rather\nawkward consequences that we shall discuss later. Indeed, it has long\nbeen recognised that probability theory offers only a rather idealised\nmodel of belief. As far back as the mid-nineteenth century, we find\nGeorge Boole saying: It would be unphilosophical to affirm that the strength of that\nexpectation, viewed as an emotion of the mind, is capable of being\nreferred to any numerical standard. (Boole 1958\n[1854]: 244) For these, and many other reasons, there is growing interest\nin Imprecise Probability (IP) models. Broadly construed,\nthese are models of belief that go beyond the probabilistic orthodoxy\nin one way or another. IP models are used in a number of fields including: This article identifies a variety of motivations for IP models;\nintroduces various formal models that are broadly in this area; and\ndiscusses some open problems for these frameworks. The focus will be\non formal models of belief. Throughout the article I adopt the convention of discussing the\nbeliefs of an arbitrary intentional agent whom I shall call\n“you”. Prominent advocates of IP (including Good and\nWalley) adopt this convention. This article is about formal models of belief and as such, there\nneeds to be a certain amount of formal machinery introduced. There is\na set of states \\(\\Omega\\) which represents\nthe ways the world could\nbe. Sometimes \\(\\Omega\\) is described as the\nset of “possible worlds”. The objects of belief—the\nthings you have beliefs about—can be represented by subsets of\nthe set of ways the world could\nbe \\(\\Omega\\). We can identify a\nproposition \\(X\\) with the set of states which\nmake it true, or, with the set of possible worlds where it is true. If\nyou have beliefs about \\(X\\)\nand \\(Y\\) then you also have beliefs about\n“\\(X\\cap Y\\)”,\n“\\(X \\cup Y\\)” and\n“\\(\\neg X\\)”;\n“\\(X\\)\nand \\(Y\\)”,\n“\\(X\\)\nor \\(Y\\)” and “it is not the case\nthat \\(X\\)” respectively. The set of\nobjects of belief is the power set\nof \\(\\Omega\\), or\nif \\(\\Omega\\) is infinite, some measurable\nalgebra of the subsets of \\(\\Omega\\). The standard view of degree of belief is that degrees of belief are\nrepresented by real numbers and belief states by probability\nfunctions; this is a normative requirement. Probability functions are\nfunctions, \\(p\\), from the algebra of beliefs\nto real numbers satisfying: So if your belief state or doxastic state is represented\nby \\(p\\), then your degree of belief\nin \\(X\\) is the value assigned\nto \\(X\\) by \\(p\\);\nthat is, \\(p(X)\\). Further, learning in the Bayesian model of belief is effected\nby conditionalisation. If you learn a\nproposition \\(E\\) (and nothing further) then\nyour post-learning belief in \\(X\\) is given\nby \\(p(X\\mid E) = p(X\\cap E)/p(E)\\). The alternative approach that will be the main focus of this\narticle is the approach that represents belief by a set of\nprobability functions instead of a single probability. So instead of\nhaving some \\(p\\) represent your belief state,\nyou have \\(P\\), a set of such\nfunctions. van Fraassen (1990) calls\nthis your representor, Levi calls it a credal set. I\nwill discuss various ways you might interpret the representor later\nbut for now we can think of it as follows. Your representor is\na credal committee: each probability function in it\nrepresents the opinions of one member of a committee that,\ncollectively, represents your beliefs. From these concepts we can define some “summary\nfunctions” that are often used in discussions of imprecise\nprobabilities. Often, it is assumed that your degree of belief in a\nproposition, \\(X\\), is represented\nby \\(P(X) = \\{p(X) : p\\in P \\}\\). I will adopt\nthis notational convention, with the proviso that I don’t\ntake \\(P(X)\\) to be an adequate representation\nof your degree of belief\nin \\(X\\). Your lower envelope\nof \\(X\\)\nis: \\(\\underline{P}(X)=\\inf P(X)\\). Likewise,\nyour upper envelope\nis \\(\\overline{P}(X)=\\sup P(X)\\). They\nare conjugates of each other in the following\nsense: \\(\\overline{P}(X) = 1 - \\underline{P}(\\neg\nX)\\). The standard assumption about updating for sets of probabilities is\nthat your degree of belief in \\(X\\) after\nlearning \\(E\\) is given\nby \\(P(X\\mid E) = \\{p(X\\mid E), p\\in P, p(E)\n> 0\\}\\). Your belief state after having\nlearned \\(E\\)\nis \\(P(\\cdot\\mid E) = \\{p(\\cdot\\mid E), p\\in P,\np(E) > 0\\}\\). That is, by the set of conditional\nprobabilities. I would like to emphasise already that these summary\nfunctions—\\(P(\\cdot)\\), \\(\\underline{P}(\\cdot)\\)\nand \\(\\overline{P}(\\cdot)\\)—are not\nproperly representative of your belief. Information is missing from\nthe picture. This issue will be important later, in our discussion of\ndilation. We shall need to talk about decision making so we shall introduce a\nsimple model of decisions in terms of gambles. We can view bounded\nreal valued functions \\(f\\) as\n“gambles” that are functions from some\nset \\(\\Omega\\) to real numbers. A\ngamble \\(f\\) pays\nout \\(f(\\omega)\\)\nif \\(\\omega\\) is the true state. We assume\nthat you value each further unit of this good the same (the gambles’\npay outs are linear in utility) and you are indifferent to concerns of\nrisk. Your attitude to these gambles reflects your attitudes about how\nlikely the various contingencies in \\(\\Omega\\)\nare. That is, gambles that win big\nif \\(\\omega\\) look more attractive the more\nlikely you consider \\(\\omega\\) to be. In\nparticular, consider the indicator\nfunction \\(I_X\\) on a\nproposition \\(X\\) which\noutputs \\(1\\) if \\(X\\)\nis true at the actual world and \\(0\\)\notherwise. These are a particular kind of gamble, and your attitude\ntowards them straightforwardly reflects your degree of belief in the\nproposition. The more valuable you\nconsider \\(I_X\\), the more likely you\nconsider \\(X\\) to be. Call these indicator\ngambles. Gambles are evaluated with respect to their expected\nvalue. Call \\(E_{p}(f)\\) the expected value of\ngamble \\(f\\) with respect to\nprobability \\(p\\), and define it as:  How valuable you consider \\(f\\) to be in\nstate \\(\\omega\\) depends on how\nbig \\(f(\\omega)\\) is. How important the\ngoodness of \\(f\\)\nin \\(\\omega\\) is depends on how likely the\nstate is, measured by \\(p(\\omega)\\). The\nexpectation is then the sum of these probability-weighted\nvalues. See Briggs (2014) for more\ndiscussion of expected utility. Then we define \\(\\mathbf{E}_{P}(f)\\)\nas \\(\\mathbf{E}_{P}(f) = \\{E_{p}(f) : p\\in P\n\\}\\). That is, the set of expected values for members\nof \\(P\\). The same proviso holds\nof \\(\\mathbf{E}_{P}(f)\\) as held\nof \\(P(X)\\): that is, the extent to\nwhich \\(\\mathbf{E}_{P}(f)\\) fully represents\nyour attitude to the value of a gamble is open to question. I will\noften drop the subscript “\\(P\\)”\nwhen no ambiguity arises from doing so. Further technical details can\nbe found in the\n formal appendix. There are a number of distinctions that it is important to make in\nwhat follows. An important parameter in an IP theory is the normative force the\ntheory is supposed to have. Is imprecision obligatory or is it merely\npermissible? Is it always permissible/obligatory, or only\nsometimes? Or we might be interested in a purely descriptive project\nof characterising the credal states of actual agents, with no interest\nin normative questions. This last possibility will concern us little\nin this article. It is also helpful to distinguish belief itself from the\nelicitation of that belief and also from your introspective access to\nthose beliefs. The same goes for other attitudes (values, utilities\nand so on). It may be that you have beliefs that are not amenable to\n(precise) elicitation, in practice or even in principle. Likewise,\nyour introspective access to your own beliefs might be imperfect. Such\nimperfections could be a source of\nimprecision. Bradley (2009)\ndistinguishes many distinct sources of imperfect introspection. The\nimperfection could arise from your unawareness of the prospect in\nquestion, the boundedness of your reasoning, ignorance of relevant\ncontingencies, or because of conflict in your evidence or in your\nvalues (pp. 240–241). See Bradley and\nDrechsler (2014) for further discussion of types of\nuncertainty. There are a variety of aspects of a body of evidence that could\nmake a difference to how you ought to respond to it. We can\nask how much evidence there is (weight of evidence). We can\nask whether the evidence is balanced or whether it tells heavily in\nfavour of one hypothesis over another (balance of evidence). Evidence\ncan be balanced because it is incomplete: there simply isn’t\nenough of it. Evidence can also be balanced if it\nis conflicted: different pieces of evidence favour different\nhypotheses. We can further ask whether evidence tells us\nsomething specific—like that the bias of a coin is 2/3\nin favour of heads—or unspecific—like that the bias of a\ncoin is between 2/3 and 1 in favour of heads. This specificity should\nbe distinguished from vagueness or indeterminacy of evidence: that a\ncoin has bias about 2/3 is vague but specific, while that a\ncoin has bias definitely somewhere between 2/3 and 1 is determinate\nbut unspecific. Likewise, a credal state could be indeterminate,\nfuzzy, or it could be unspecific, or it could be both. It seems like\ndeterminate but unspecific belief states will be rarer than\nindeterminate ones. Isaac Levi (1974, 1985) makes a\ndistinction between “imprecise” credences and\n“indeterminate” credences (the scare quotes are indicating\nthat these aren’t uses of the terms “imprecise” and\n“indeterminate” that accord with the usage I adopt in this\narticle). The idea is that there are two distinct kinds of belief\nstate that might require a move to an IP representation of belief. An\n“imprecise” belief in Levi’s terminology is an imperfectly\nintrospected or elicited belief in mine, while an\n“indeterminate” belief is a (possibly) perfectly\nintrospected belief that is still indeterminate or unspecific (or\nboth). Levi argues that the interesting phenomenon is\n“indeterminate” credence. Walley\n(1991) also emphasises the distinction between cases where\nthere is a “correct” but unknown probability from cases of\n“indeterminacy”. There is a further question about the interpretation of IP that\ncross-cuts the above. This is the question of whether we\nunderstand \\(P\\) as a “complete”\nor “exhaustive” representation of your beliefs, or whether\nwe take the representation to be incomplete or non-exhaustive. Let’s\ntalk in terms of the betting interpretation for a moment. The\nexhaustive/non-exhaustive distinction can be drawn by asking the\nfollowing question: does \\(P\\) capture all\nand only your dispositions to bet or\ndoes \\(P\\) only partially capture your\ndispositions to bet? Walley emphasises this distinction and suggests\nthat most models are non-exhaustive. Partly because of Levi’s injunction to distinguish\n“imprecise” from “indeterminate” belief, some\nhave objected to the use of the term “imprecise\nprobability”. Using the above distinction between indeterminate,\nunspecific and imperfectly introspected belief, we can keep separate\nthe categories Levi wanted to keep separate all without using the term\n“imprecise”. We can then use “imprecise” as an\numbrella term to cover all these cases of lack of\nprecision. Conveniently, this allows us to stay in line with the\nwealth of formal work on “Imprecise Probabilities” which\nterm is used to cover cases of indeterminacy. This usage goes back at\nleast to Peter Walley’s influential book Statistical Reasoning\nwith Imprecise Probabilities (Walley\n1991). So, “Imprecise” is not quite right, but neither is\n“Probability” since the formal theory of IP is really\nabout previsions (sort of expectations) rather than just\nabout probability (expectations of indicator functions). Helpfully, if\nI abbreviate Imprecise Probability to “IP” then I can\nexploit some useful ambiguities. Let’s consider, in general terms, what sort of motivations one\nmight have for adopting models that fall under the umbrella of IP. The\nfocus will be on models of rational belief, since these are the models\nthat philosophers typically focus on, although it is worth noting that\nstatistical work using IP isn’t restricted to this\ninterpretation. Note that no one author endorses all of these\narguments, and indeed, some authors who are sympathetic to IP have\nexplicitly stated that they don’t consider certain of these arguments\nto be good (for example Mark Kaplan does not endorse the claim that\nconcerns about descriptive realism suggest allowing\nincompleteness). There are a number of examples of decision problems where we are\nintuitively drawn to go against the prescriptions of precise\nprobabilism. And indeed, many experimental subjects do seem to express\npreferences that violate the axioms. IP offers a way of representing\nthese intuitively plausible and experimentally observed choices as\nrational. One classic example of this is the Ellsberg\nproblem (Ellsberg 1961). I have an urn that contains ninety marbles. Thirty marbles are\nred. The remainder are blue or yellow in some unknown proportion. Consider the indicator gambles for various events in this\nscenario. Consider a choice between a bet that wins if the marble\ndrawn is red (I), versus a bet that wins if the marble drawn is blue\n(II). You might prefer I to II since I involves risk while II\ninvolves ambiguity. A prospect is risky if its outcome is\nuncertain but its outcomes occur with known probability. A prospect is\nambiguous if the outcomes occur with unknown or only partially known\nprobabilities. Now consider a choice between a bet that wins if the\nmarble drawn is not blue (III) versus a bet that wins if the marble\ndrawn is not red (IV). Now it is III that is ambiguous, while IV is\nunambiguous but risky, and thus IV might seem better to you if you\npreferred risky to ambiguous prospects. Such a pattern of preferences\n(I preferred to II but IV preferred to III) cannot be rationalised as\nthe choices of a precise expected utility maximiser. The gambles are\nsummarised in the table. Table 1: The Ellsberg bets. The urn\ncontains 30 red marbles and 60 blue/yellow marbles Let the probabilities for red, blue and yellow marbles\nbe \\(r\\), \\(b\\)\nand \\(y\\) respectively. If you were an\nexpected utility maximiser and preferred I to II,\nthen \\(r > b\\) and a preference for IV over\nIII entails that \\(r+y < y +b\\). No numbers\ncan jointly satisfy these two constraints. Therefore, no probability\nfunction is such that an expected utility maximiser with that\nprobability would choose in the way described above. While by no means\nuniversal, these preferences are a robust feature of many experimental\nsubjects’ response to this sort of\nexample (Camerer and Weber 1992; Fox and\nTversky 1995). Some experiments suggest that Ellsberg-type patterns of preference are rarer than normally recognised (Binmore et al. 2012; Voorhoeve et al. 2016). For more on ambiguity attitudes, see Trautmann and van der Kuilen (2016). The imprecise probabilist can model the situation as\nfollows: \\(P(R)=1/3, P(B)=P(Y)=[0,2/3]\\). Note\nthat this expression of the belief state misses out some important\ndetails. For example, for all \\(p\\in P\\), we\nhave \\(p(B)=2/3-p(Y)\\). For the point being made\nhere, this detail is not important. Modelling the ambiguity\nallows us to rationalise real agents’ preferences for bets on red. To\nflesh this story out would require a lot more to be said about\ndecision making, (see section 3.3) but the\nintuition is that aversion to ambiguity explains the preference for I\nover II and IV over III. As Steele (2007) points out, the\nabove analysis rationalises the Ellsberg choices only if we are\ndealing with genuinely indeterminate or unspecific beliefs. If we were\ndealing with a case of imperfectly introspected belief then there\nwould exist some \\(p\\) in the representor such\nthat rational choices maximise \\(E_{p}\\). For\nthe Ellsberg choices, there is no\nsuch \\(p\\). This view on the lessons of the Ellsberg game is not\nuncontroversial. Al-Najjar and Weinstein\n(2009) offer an alternative view on the interpretation of the\nEllsberg preferences. Their view is that the distinctive pattern of\nEllsberg choices is due to agents applying certain heuristics to solve\nthe decisions that assume that the odds are manipulable. In real-life\nsituations, if someone offers you a bet, you might think that they\nmust have some advantage over you in order for it to be worth their\nwhile offering you the bet. Such scepticism, appropriately modelled,\ncan yield the Ellsberg choices within a simple game theoretic precise\nprobabilistic model. Various arguments for (precise) probabilism assume that some\nrelation or other is complete. Whether this is a preference over acts,\nor some “qualitative probability ordering”, the relation\nis assumed to hold one way or the other between any two elements of\nthe domain. This hardly seems like it should be a principle of\nrationality, especially in cases of severe uncertainty. That\nis—to take the preference example—it is reasonable to have\nno preference in either direction. This is an importantly different\nattitude to being indifferent between the options. Mark\nKaplan argues this point as follows: Both when you are indifferent between \\(A\\)\nand \\(B\\) and when you are undecided\nbetween \\(A\\)\nand \\(B\\) you can be said not to prefer either\nstate of affairs to the other. Nonetheless, indifference and\nindecision are distinct. When you are indifferent\nbetween \\(A\\)\nand \\(B\\), your failure to prefer one to the\nother is born of a determination that they are equally\npreferable. When you are undecided, your failure to prefer one to the\nother is born of no such determination. (Kaplan\n1996: 5) There is a standard behaviourist response to the claim\nthat incomparability and indifference should be distinguished. In\nshort, the claim is that it is a distinction that cannot be inferred\nfrom actual agents’ choice behaviour. Ultimately, in a given choice\nsituation you must choose one of the options. Which you choose can be\ninterpreted as being (weakly) preferred. Joyce offers the following\ncriticism of this appeal to behaviourism. There are just too many things worth saying that cannot be said\nwithin the confines of strict behaviorism… The basic difficulty\nhere is that it is impossible to distinguish contexts in which an\nagent’s behavior really does reveal what she wants from contexts in\nwhich it does not without appealing to additional facts about her\nmental state… An even more serious shortcoming is behaviorism’s\ninability to make sense of rationalizing explanations of\nchoice behavior. (Joyce 1999: 21) On top of this, behaviourists cannot make sense of the fact that\nincomparable goods are insensitive to small\nimprovements. That is, if \\(A\\)\nand \\(B\\) are two goods that you have no\npreference between (for example, bets on propositions with imprecise\nprobabilities) and if \\(A^+\\) is a good\nslightly better than \\(A\\), then it might\nstill be incomparable with \\(B\\). This\ndistinguishes incomparability from indifference, since indifference\n“ties” will be broken by small improvements. So the claim\nthat there is no behavioural difference between indifference and\nincomparability is false. Kaplan argues that not only is violating the completeness axiom\npermissible, it is, in fact, sometimes obligatory. [M]y reason for rejecting as falsely precise the demand that you\nadopt a … set of preferences [that satisfy the preference\naxioms] is not the usual one. It is not that this demand is not\nhumanly satisfiable. For if that were all that was wrong, the\ndemand might still play a useful role as a regulative ideal—an\nideal which might then be legitimately invoked to get you to\n“solve” your decision problem as the orthodox Bayesian\nwould have you do. My complaint about the orthodox Bayesian demand is\nrather that it imposes the wrong regulative ideal. For if you have\n[such a] set of preferences then you have a determinate assignment of\n[\\(p\\)] to every hypothesis—and then you\nare not giving evidence its due. (Kaplan 1983:\n571) He notes that it is not the case that it is always unreasonable or\nimpossible for you to have precise beliefs: in that case precision\ncould serve as a regulative ideal. Precise probabilism does still\nserve as something of a regulative ideal, but it is the belief of an\nideal agent in an idealised evidential position. Idealised\nevidential positions are approximated by cases where you have a coin\nof a known bias. Precise probabilists and advocates of IP both agree\nthat precise probabilism is an idealisation, and a regulative\nideal. However, they differ as to what kind of idealisation is\ninvolved. Precise probabilists think that what precludes us from\nhaving precise probabilistic beliefs is merely a lack of computational\npower and introspective capacity. Imprecise probabilists think that\neven agents ideal in this sense might (and possibly should)\nfail to have precise probabilistic beliefs when they are not in an\nideal evidential position. At least some of the axioms of preference are not normative\nconstraints. We can now ask what can be proved in the absence of the\n“purely structural”—non-normative—axioms? This\nsurely gives us a handle on what is really required of the structure\nof belief. It seems permissible to fail to have a preference between two\noptions. Or it seems reasonable to fail to consider either of two\npossibilities more likely than the other. And these failures to assent\nto certain judgements is not the same as considering the two elements\nunder consideration to be on a par in any substantive\nsense. That said, precise probabilism is serving as a regulative\nideal. That is, precision might still be an unattained (possibly\nunattainable) goal that informs agents as to how they might improve\ntheir credences. Completeness of preference is what the thoroughly\ninformed agent ought to have. Without complete preference, standard\nrepresentation theorems don’t work. However, for\neach completion of the incomplete preference\nordering—for each complete ordering that extends the incomplete\npreference relation—the theorem follows. So if we consider the\nset of probability functions that are such that some completion of the\nincomplete preference is represented by that function, then we can\nconsider this set to be representing the beliefs associated with the\nincomplete preference. We also get, for each completion, a utility\nfunction unique up to linear transformation. This, in essence, was\nKaplan’s position (see Kaplan 1983;\n1996). Joyce (1999: 102–4) and\nJeffrey (1984: 138–41) both make\nsimilar claims. A particularly detailed argument along these lines for\ncomparative belief can be found in Hawthorne\n(2009). Indeed, this idea has a long and distinguished history\nthat goes back at least as far as B.O. Koopman\n(1940). I.J Good (1962), Terrence\nFine (1973) and Patrick\nSuppes (1974) all discussed ideas along\nthese lines. Seidenfeld, Schervish, and Kadane\n(1995) give a representation theorem for preference that don’t\nsatisfy completeness. (See Evren and Ok 2011;\nPedersen 2014; and Chu and Halpern 2008, 2004; for very general\nrepresentation theorems). Evidence influences belief. Joyce\n(2005) suggests that there is an important difference between\nthe weight of evidence and the balance of evidence. He argues that\nthis is a distinction that precise probabilists struggle to deal with\nand that the distinction is worth representing. This idea has been\nhinted at by a great many thinkers including J.M. Keynes, Rudolf\nCarnap, C.S. Pierce and Karl Popper (see\nreferences in Joyce 2005; Gärdenfors and Sahlin 1982). Here’s\nKeynes’ articulation of the intuition: As the relevant evidence at our disposal increases, the magnitude\nof the probability of the argument may either decrease or increase,\naccording as the new knowledge strengthens the unfavourable or the\nfavourable evidence; but something seems to have increased in\neither case,—we have a more substantial basis upon which to rest\nour conclusion. I express this by saying than an accession of new\nevidence increases the weight of an\nargument. (Keynes 1921: 78, Keynes’\nemphasis) Consider tossing a coin known to be fair. Let’s say you have seen\nthe outcome of a hundred tosses and roughly half have come up\nheads. Your degree of belief that the coin will land heads should be\naround a half. This is a case where there is weight of evidence behind\nthe belief. Now consider another case: a coin of unknown bias is to be\ntossed. That is, you have not seen any data on previous tosses. In the\nabsence of any relevant information about the bias, symmetry concerns\nmight suggest you take the chance of heads to be around a half. This\nopinion is different from the above one. There is no weight\nof evidence, but there is nothing to suggest that your attitudes\nto \\(H\\) and \\(T\\)\nshould be different. So, on balance, you should have the same\nbelief in both. However, these two different cases get represented as having the\nsame probabilistic belief,\nnamely \\(p(H)=p(T)=0.5\\). In the fair coin\ncase, this probability assignment comes from having evidence that\nsuggests that the chance of heads is a half, and the prescription to\nhave your credences match chances (ceteris paribus). In the\nunknown bias case, by contrast, one arrives at the same assignment in\na different way: nothing in your evidence supports one proposition\nover the other so some “principle of indifference”\nreasoning suggests that they should be assigned the same\ncredence (see Hájek 2011, for discussion of the\nprinciple of indifference). If we take seriously the “ambiguity aversion” discussed\nearlier, when offered the choice between betting on the fair coin’s\nlanding heads as opposed to the unknown-bias coin’s landing heads, it\ndoesn’t seem unreasonable to prefer the former. Recall the preference\nfor unambiguous gambles in the Ellsberg game\nin section 2.1. But if both coins have the same\nsubjective probabilities attached, what rationalises this preference\nfor betting on the fair coin? Joyce argues that there is a difference\nbetween these beliefs that is worth representing. IP does\nrepresent the difference. The first case is represented\nby \\(P(H)=\\{0.5\\}\\), while the second is\ncaptured by \\(P(H)=[0,1]\\). Scott Sturgeon puts this point nicely when he says: [E]vidence and attitude aptly based on it must match in\ncharacter. When evidence is essentially sharp, it warrants sharp\nor exact attitude; when evidence is essentially fuzzy—as it is\nmost of the time—it warrants at best a fuzzy attitude. In a\nphrase: evidential precision begets attitudinal precision; and\nevidential imprecision begets attitudinal\nimprecision. (Sturgeon 2008: 159 Sturgeon’s\nemphasis) Wheeler (2014) criticises Sturgeon on\nthis “character matching” thesis. However, an argument for\nIP based on the nature of evidence only requires that the character of\nthe evidence sometimes allows (or mandates?) imprecise belief and not\nthat the characters must always match. In\nopposition, Schoenfield (2012) argues\nthat evidence always supports precise credence, but that for reasons\nof limited computational capacity, real agents needn’t be required to\nhave precise credences. However, her argument only really supports the\nclaim that sometimes indeterminacy is due to complexity of\nthe evidence and computational complexity. She doesn’t have an\nargument against the claims Levi, Kaplan, Joyce and others make that\nthere are evidential situations that warrant imprecise attitudes. Strictly speaking, what we have here is only half the\nstory. There is a difference between the\nrepresentations of belief as regards weight and balance. But that\nstill leaves open the question of exactly what is representing the\nweight of evidence? What aspect of the belief reflects this\ndifference? One might be tempted to\nview \\(\\overline{P}(H)-\\underline{P}(H)\\) as a\nmeasure of the weight of evidence\nfor \\(H\\). Walley\n(1991) tentatively suggests as much. However, this would get\nwrong cases of conflicting evidence. (Imagine two equally reliable\nwitnesses: one tells you the coin is biased towards heads, the other\nsays the bias is towards tails.) The question of whether and\nhow IP does better than precise probabilism has not yet received an\nadequate answer. Researchers in IP have, however, made progress on\ndistinguishing cases where your beliefs happen to have certain\nsymmetry properties from cases where your beliefs capture evidence\nabout symmetries in the objects of belief. This is a\ndistinction that the standard precise model of belief fails to\ncapture (de Cooman and Miranda\n2007). The precise probabilist can respond to the weight/balance\ndistinction argument by pointing to the property\nof resiliency (Skyrms 2011)\nor stability (Leitgeb\n2014). The idea is that probabilities determined by the weight\nof evidence change less in response to new evidence than do\nprobabilities determined by balance of evidence alone. That is, if\nyou’ve seen a hundred tosses of the coin, seeing it land heads doesn’t\naffect your belief much, while if you’ve not seen any tosses of the\ncoin, seeing it land heads has a bigger effect on your beliefs. Thus,\nthe distinction is represented in the precise probabilistic framework\nin the conditional probabilities. The distinction, though, is one that\ncannot rationalise the preference for betting on the fair coin. One\ncould develop a resiliency-weighted expected value and claim that this\nis what you should maximise, but this would be as much of a departure\nfrom orthodox probabilism as IP is. If someone were to develop such a\ntheory, then its merits could be weighed against the merits of IP type\nmodels. Another potential precise response would be to suggest that there\nis weight of evidence for \\(H\\) if many\npropositions that are evidence for \\(H\\) are\nfully believed, or if there is a chance proposition\n(about \\(H\\)) that is near to fully\nbelieved. This is in contrast to cases of mere balance where few\npropositions that are evidence for \\(H\\) are\nfully believed, or where probability is spread out over a number of\nchance hypotheses. The same comments made above about resiliency apply\nhere: such distinctions can be made, but this doesn’t get us to a\ntheory that can rationalise ambiguity aversion. The phenomenon of dilation (section 3.1)\nsuggests that the kind of argument put forward in this section needs\nmore care and further elaboration. You are sometimes in a position where none of your evidence seems\nto speak for or against the truth of some proposition. Arguably, a\nreasonable attitude to take towards such a proposition is suspension\nof judgement. When there is little or no information on which to base our\nconclusions, we cannot expect reasoning (no matter how clever or\nthorough) to reveal a most probable hypothesis or a uniquely\nreasonable course of action. There are limits to the power of\nreason. (Walley 1991: 2) Consider a coin of unknown bias. The Bayesian agent must have a\nprecise belief about the coin’s landing heads on the next toss. Given\nthe complete lack of information about the coin, it seems like it\nwould be better just to suspend judgement. That is, it would be better\nnot to have any particular precise credence. It would be better to\navoid betting on the coin. But there just isn’t room in the Bayesian\nframework to do this. The probability function must output some\nnumber, and that number will sanction a particular set of bets as\ndesirable. Consider \\(\\underline{P}(X)\\) as\nrepresenting the degree to which the evidence\nsupports \\(X\\). Now\nconsider \\(I(X) = 1- (\\underline{P}(X) +\n\\underline{P}(\\neg X))\\). This measures the degree to which the\nevidence is silent\non \\(X\\). Huber\n(2009) points out that precise probabilism can then be\nunderstood as making the claim that no evidence is ever silent on any\nproposition. That is, \\(I(X)=0\\) for\nall \\(X\\). One can never suspend\njudgement. This is a nice way of seeing the strangeness of the precise\nprobabilist’s attitude to evidence. Huber is making this point about\nDempster-Shafer belief functions (see\n historical appendix,\n section 7), but it carries over to IP in general. The committed precise probabilist would respond that\nsetting \\(p(X)=0.5\\) is suspending\njudgement. This is the maximally noncommittal credence in the\ncase of a coin flip. More generally, suspending judgement should be\nunderstood in terms of maximising\nentropy (Jaynes 2003; Williamson 2010:\n49–72). The imprecise probabilist could argue that this\nonly seems to be the right way to be noncommittal if you are\nwedded to the precise probabilist representation of belief. That is,\nthe MaxEnt approach makes sense if you are already committed to\nrepresentation of belief by a single precise probability, but loses\nits appeal if credal sets are available. Suspending judgement is\nsomething you do when the evidence doesn’t determine your\ncredence. But for the precise probabilist, there is no way to signal\nthe difference between suspension of judgement and strong evidence of\nprobability half. This is just the weight/balance argument again. To make things more stark, consider the following delightfully odd\nexample from Adam Elga: A stranger approaches you on the street and starts pulling out\nobjects from a bag. The first three objects he pulls out are a\nregular-sized tube of toothpaste, a live jellyfish, and a travel-sized\ntube of toothpaste. To what degree should you believe that the next\nobject he pulls out will be another tube of\ntoothpaste? (2010: 1) In this case, unlike in the coin case, it really isn’t clear what\nintuition says about what would be the “correct” precise\nprobabilist suspension of judgement. What Maximum Entropy methods\nrecommend will depend on seemingly arbitrary choices about the formal\nlanguage used to model the situation. Williamson is well aware of this\nlanguage relativity problem. He argues that choice of a language\nencodes some of our evidence. Another response to this argument would be to take William James’\nresponse to W.K. Clifford (Clifford 1901; James\n1897). James argued that as long as your beliefs are consistent\nwith the evidence, then you are free to believe what you like. So\nthere is no need to ever suspend judgement. Thus, the precise\nprobabilist’s inability to do so is no real flaw. This attitude, which\nis sometimes called epistemic voluntarism, is close to the\nsort of subjectivism espoused by Bruno de Finetti, Frank\nRamsey and others. There does seem to be a case for an alternative method of\nsuspending judgement in order to allow you to avoid making any bets\nwhen your evidence is very incomplete, ambiguous or imprecise. If your\ncredences serve as your standard for the acceptability of bets, they\nshould allow for both sides of a bet to fail to be acceptable. A\nprecise probabilist cannot do this since if a bet has (precise)\nexpected value \\(e\\) then taking the other\nside of that bet (being the bookie) has expected\nvalue \\(-e\\). If acceptability is understood\nas nonnegative expectation, then at least one side of any bet is\nacceptable to a precise agent. This seems unsatisfactory. Surely\ngenuine suspension of judgement involves being unwilling to risk money\non the truth of a proposition at any odds. Inspired by the famous “Bertrand\nparadox”, Chandler (2014)\noffers a neat argument that the precise probabilist cannot jointly\nsatisfy two desiderata relating to suspension of judgment about a\nvariable. First desideratum: if you suspend judgement about the value\nof a bounded real variable \\(X\\), then it\nseems that different intervals of possible values\nfor \\(X\\) of the same size should be treated\nthe same by your epistemic state. Second desideratum:\nif \\(Y\\) essentially describes the same\nquantity as \\(X\\), then suspension of\njudgement about \\(X\\) should entail suspension\nof judgement about \\(Y\\). Let’s imagine now\nthat you have precise probabilities and that you suspend judgement\nabout \\(X\\). By the first desideratum, you\nhave a uniform distribution over values\nof \\(X\\). Now consider \\(Y =\n1/X\\). \\(Y\\) essentially describes the\nsame quantity that \\(X\\) did. But a uniform\ndistribution over \\(X\\) entails a non-uniform\ndistribution over \\(Y\\). So you do not suspend\njudgement over \\(Y\\). A real-world case of\nvariables so related is “ice residence time in clouds” and\n“ice fall rate in clouds”. These are inversely related,\nbut describe essentially the same element of a climate\nsystem (Stainforth et al. 2007:\n2154). So a precise probabilist cannot satisfy these reasonable desiderata\nof suspension of judgement. An imprecise probabilist can: for example,\nthe set of all probability functions\nover \\(X\\) satisfies both desiderata. There\nmay be more informative priors that also represent suspension of\njudgement, but it suffices for now to point out that IP seems better\nable to represent suspension of judgement than precise\nprobabilism. Section 5.5 of Walley\n(1991),\ndiscusses IP’s prospects as a method for\ndealing with suspension of judgement. Haenni et al. (2011) motivate\nimprecise probabilities by showing how they can arise from precise\nprobability judgements. That is, if you have a precise probability\nfor \\(X\\) and a precise probability\nfor \\(Y\\), then you can put bounds\non \\(p(X\\cap Y)\\) and \\(p(X\n\\cup Y)\\), even if you don’t know\nhow \\(X\\) and \\(Y\\)\nare related. These bounds give you intervals of possible probability\nvalues for the compound events. For example, you know that \\(p(X \\cap Y)\\) is bounded above by\n\\(p(X)\\) and by \\(p(Y)\\) and thus by \\(\\min\\{p(X),p(Y)\\}\\). If\n\\(p(X) \\gt 0.5\\) and \\(p(Y) \\gt 0.5\\) then \\(X\\) and \\(Y\\) must\noverlap. So \\(p(X\\cap Y)\\) is bounded below by \\(p(X)+p(Y)-1\\). But,\nby definition, \\(p(X \\cap Y)\\) is also bounded below by \\(0\\). So we\nhave the following result: if you know \\(p(X)\\) and you know \\(p(Y)\\),\nthen, you know Likewise, bounds can be put on \\(p(X \\cup Y)\\). \\(p(X\\cup Y)\\)\ncan’t be bigger than when \\(X\\) and \\(Y\\) are disjoint, so it is\nbounded above by \\(p(X)+p(Y)\\). It is also bounded above by \\(1\\), and\nthus by the minimum of those expressions. It is also bounded below by\n\\(p(X)\\) and by \\(p(Y)\\) and thus by their maximum. Putting this\ntogether, \n\nThese constraints are effectively what you get from de Finetti’s\nFundamental Theorem of Prevision (de Finetti\n1990 [1974]: 112; Schervish, Seidenfeld, and Kadane 2008). So if your evidence constrains your belief\nin \\(X\\) and in \\(Y\\),\nbut is silent on their interaction, then you will only be able to pin\ndown these compound events to certain intervals. Any choice of a\nparticular probability function will go beyond the evidence in\nassuming some particular evidential relationship\nbetween \\(X\\)\nand \\(Y\\). That\nis, \\(p(X)\\) and \\(p(X\\mid\nY)\\) will differ in a way that has no grounding in your\nevidence. What if the objective chances were not probabilities? If we endorse\nsome kind of connection between known objective chances and\nbelief—for example, a principle of direct inference or Lewis’\nPrincipal Principle (Lewis\n1986)—then we might have an additional reason to endorse\nimprecise probabilism. It seems to be a truth universally acknowledged\nthat chances ought to be probabilities, but it is a\n“truth” for which very little argument has been\noffered. For example, Schaffer (2007)\nmakes obeying the probability axioms one of the things required in\norder to play the “chance role”, but offers no argument\nthat this should be the case.  Joyce says “some have held\nobjective chances are not probabilities. This seems unlikely, but\nexplaining why would take us too far\nafield” (2009: 279,\nfn.17). Various other discussions of chance—for example\nin statistical mechanics (Loewer 2001; Frigg\n2008) or “Humean\nchance” (Lewis 1986,\n1994)—take for granted that chances should be precise and\nprobabilistic (Dardashti et al. 2014 is\nan exception). Obviously things are confused by the use of the concept\nof chance as a way of interpreting probability theory. There is,\nhowever, a perfectly good pre-theoretic notion of chance: this is what\nprobability theory was originally invented to reason about, after\nall. This pre-theoretic chance still seems like the sort of thing that\nwe should apportion our belief to, in some sense. And there is very\nlittle argument that chances must always be probabilities. If the\nchances were nonprobabilistic in a particular way, one might argue\nthat your credences ought to be nonprobabilistic in the same way. What\nform a chance-coordination norm should take if chances and credences\nwere to have non-probabilistic formal structures is currently an open\nproblem. I want to give a couple of examples of this idea. First consider\nsome physical process that doesn’t have a limiting frequency but has a\nfrequency that varies, always staying within some interval. This would\nbe a process that is chancy, but fairly predictable. It might be that\nthe best description of such a system is to just put bounds on its\nrelative frequency. Such processes have been studied using IP\nmodels (Kumar and Fine 1985; Grize and Fine\n1987; Fine 1988), and have been discussed as a potential source\nof imprecision in credence (Hájek and Smithson\n2012). A certain kind of non-standard understanding of a\nquantum-mechanical event leads naturally to upper probability\nmodels (Suppes and Zanotti 1991; Hartmann and\nSuppes 2010). John Norton has discussed the limits of\nprobability theory as a logic of induction, using an example which, he\nclaims, admits no reasonable probabilistic\nattitude (Norton 2007, 2008a,b). One\nmight hope that IP offers an inductive logic along the lines Norton\nsketches. Norton himself has expressed scepticism on this\nline (Norton 2007),\nalthough Benétreau-Dupin (2015) has defended IP as a candidate\nsystem for Norton’s project.\nFinally, particular views on vagueness might well prompt a rethinking of the formal structure of chance (Bradley 2016). Suppose we wanted our epistemology to apply not just to\nindividuals, but to “group agents” like committees,\ngovernments, companies, and so on. Such agents may be made up of\nmembers who disagree. Levi (1986, 1999)\nhas argued that representation of such conflict is better handled with\nsets of probabilities than with precise probabilities. There is a rich\nliterature on combining or aggregating the (probabilistic) opinions of\nmembers of groups (Genest and Zidek\n1986) but the outcome of such aggregation does not adequately\nrepresent the disagreement among the group. Some forms of\naggregation also fail to respect plausible constraints on group\nbelief. For example, if every member of the group agrees\nthat \\(X\\) and \\(Y\\)\nare probabilistically independent, then it seems plausible to require\nthat the group belief respects this unanimity. It is, however, well\nknown that linear pooling—a simple and popular form of\naggregation—does not respect this desideratum. Consider two\nprobability functions \\(p, q\\) such\nthat \\(p(X) = p(Y) = 1/3\\)\nand \\(p(X\\mid Y)=p(X)\\)\nwhile \\(q(X) = q(Y) = 2/3\\)\nand \\(q(X\\mid Y)=q(X)\\). Consider\naggregating these two probabilities by taking an unweighted average of\nthem: \\(r = p/2 + q/2\\). Now, calculation\nshows that \\(r(X\\cap Y) = 5/18\\)\nwhile \\(r(X)r(Y) = 1/4\\), thus demonstrating\nthat \\(r\\) does not\nconsider \\(X\\)\nand \\(Y\\) to be independent. So such an\naggregation method does not satisfy the above\ndesideratum (Kyburg and Pittarelli 1992; Cozman\n2012). For more on judgement aggregation in groups,\nsee List and Pettit (2011), especially\nchapter 2. \nElkin and Wheeler (2016) argue that resolving disagreement among precise probabilist peers should involve an imprecise probability. Stewart and Quintana (2018) argue that imprecise aggregation methods have some nice properties that no precise aggregation method do.\n If committee members have credences and utilities that\ndiffer among the group, then no precise probability-utility pair\ndistinct from the probabilities and utilities of the agents can\nsatisfy the Pareto\ncondition (Seidenfeld, Kadane, and\nSchervish 1989). The Pareto condition requires that the group\npreference respect agreement of preference among the group. That is,\nif all members of the group prefer \\(A\\)\nto \\(B\\) (that is, if each group member finds\nthat \\(A\\) has higher expected utility\nthan \\(B\\)) then the aggregate preference (as\ndetermined by the aggregate probability-utility pair) should satisfy\nthat preference. Since this “consensus preservation” is a\nreasonable requirement on aggregation, this result shows that precise\nmodels of group agents are problematic. Walley discusses an example of\na set of probabilities where each probability represents the beliefs\nof a member of a group, then \\(P\\) is an\nincomplete description of the beliefs of each agent, in the sense that\nif all members of \\(P\\) agree on something,\nthen that thing is something each agent believes. Sets of\nprobabilities allow us to represent an agent who\nis conflicted in their\njudgements (Levi 1986, 1999). Ideally rational agents may face choices where there is no best\noption available to them. Indeterminacy in probability judgement and\nunresolved conflicts between values lead to predicaments where at the\nmoment of choice the rational agent recognizes more than one such\npreference ranking of the available options in [the set of available\nchoices] to be permissible. (Levi 1999:\n510) Levi also argued that individual agents can be in conflict in the\nsame way as groups, and thus that individuals’ credal states are also\nbetter represented by sets of probabilities. (Levi also argued for\nthe convexity of credal states, which brings him into\nconflict with the above argument about independence (see\n historical appendix\nsection 3).) One doesn’t need to buy the claim that groups and\nindividuals must be modelled in the same way to take something away\nfrom this idea. One merely needs to accept the idea that an individual\ncan be conflicted in such a way that a reasonable\nrepresentation of her belief state—or belief and value\nstate—is in terms of sets of\nfunctions. Bradley (2009) calls members\nof such sets “avatars”. This suggests that we interpret an\nindividual’s credal set as a credal committee made up of her\navatars. This interpretation of the representor is due\nto Joyce (2011), though Joyce attributes\nit Adam Elga. This committee represents all the possible prior\nprobabilities you could have that are consistent with the\nevidence. Each credal committee member is a fully opinionated Jamesian\nvoluntarist. The committee as a whole, collectively, is a Cliffordian\nobjectivist. This section collects some problems for IP noted in\nthe literature. Consider two logically unrelated\npropositions \\(H\\)\nand \\(X\\). Now consider the four “state\ndescriptions” of this simple model as set out\nin Figure 1. So \\(a=H\\cap\nX\\) and so on. Now define \\(Y=a \\cup\nd\\). Alternatively, consider three propositions related in the\nfollowing way: \\(Y\\) is defined as\n“\\(H\\) if and only\nif \\(X\\)”. Figure 1: A diagram of the relationships after Seidenfeld (1994); \\(Y\\) is the shaded area Further imagine that \\(p(H\\mid X) = p(H) =\n1/2\\). No other relationships between the propositions hold\nexcept those required by logic and probability theory. It is\nstraightforward to verify that the above constraints require\nthat \\(p(Y) = 1/2\\). The probability\nfor \\(X\\), however, is unconstrained. Let’s imagine you were given the above information, and took your\nrepresentor to be the full set of probability functions that satisfied\nthese constraints. Roger White suggested an intuitive gloss on how you\nmight receive information about propositions so related and so\nconstrained (White 2010). White’s puzzle\ngoes like this. I have a proposition \\(X\\),\nabout which you know nothing at all. I have written whichever is true\nout of \\(X\\) and \\(\\neg\nX\\) on the Heads side of a fair coin. I have painted\nover the coin so you can’t see which side is heads. I then flip the\ncoin and it lands with the \\(X\\)\nuppermost. \\(H\\) is the proposition that the\ncoin lands heads up. \\(Y\\) is the proposition\nthat the coin lands with the\n“\\(X\\)” side up. Imagine if you had a precise prior that made you certain\nof \\(X\\) (this is compatible with the above\nconstraints since \\(X\\) was\nunconstrained). Seeing \\(X\\) land uppermost\nnow should be evidence that the coin has landed heads. The game set-up\nmakes it such that these apparently irrelevant instances of\nevidence can carry information. Likewise, being very\nconfident of \\(X\\)\nmakes \\(Y\\) very good evidence\nfor \\(H\\). If instead you were\nsure \\(X\\) was\nfalse, \\(Y\\) would be solid gold evidence\nof \\(H\\)’s falsity. So it seems\nthat \\(p(H\\mid Y)\\) is proportional to prior\nbelief in \\(X\\) (indeed, this can be proven\nrather easily). Given the way the events are related, observing\nwhether \\(X\\) or \\(\\neg\nX\\) landed uppermost is a noisy channel to learn about whether\nor not \\(H\\) landed uppermost. So let’s go back to the original imprecise case and consider what\nit means to have an imprecise belief\nin \\(X\\). Among other things, it means\nconsidering possible that \\(X\\) could be very\nlikely. It is consistent with your belief state\nthat \\(X\\) is such that if you knew what\nproposition \\(X\\) was, you would consider it\nvery likely. In this case, \\(Y\\) would be good\nevidence for \\(H\\). Note that in this case\nlearning that the coin landed \\(\\neg X\\)\nuppermost—call\nthis \\(Y'\\)—would be just as good\nevidence\nagainst \\(H\\). Likewise, \\(X\\)\nmight be a proposition that you would have very low credence in, and\nthus \\(Y\\) would be\nevidence against \\(H\\). Since you are in a state of ignorance with respect\nto \\(X\\), your representor contains\nprobabilities that take \\(Y\\) to be good\nevidence that \\(H\\) and probabilities that\ntake \\(Y\\) to be good evidence\nthat \\(\\neg H\\). So, despite the fact\nthat \\(P(H)=\\{1/2\\}\\) we\nhave \\(P(H\\mid Y) = [0,1]\\). This\nphenomenon—posteriors being wider than their priors—is\nknown as dilation. The phenomenon has been thoroughly investigated in\nthe mathematical literature (Walley 1991;\nSeidenfeld and Wasserman 1993; Herron, Seidenfeld, and Wasserman 1994;\nPedersen and Wheeler 2014). Levi and Seidenfeld reported\nan example of dilation to Good following Good\n(1967). Good mentioned this correspondence in his follow up\npaper (Good 1974). Recent interest in\ndilation in the philosophical community has been generated by White’s\npaper (White 2010). White considers dilation to be a problem since learning \\(Y\\)\ndoesn’t seem to be relevant to \\(H\\). That is, since\nyou are ignorant about \\(X\\), learning whether or not the coin landed\n\\(X\\) up doesn’t seem to tell you anything about whether the\ncoin landed heads up. It seems strange to argue that your belief in\n\\(H\\) should dilate from \\(1/2\\) to \\([0,1]\\) upon learning\n\\(Y\\). It feels as if this should just be irrelevant to\n\\(H\\). However, \\(Y\\) is only really irrelevant to \\(H\\) when\n\\(p(X)=1/2\\). Any other precise belief you might have in \\(X\\) is such\nthat \\(Y\\) now affects your posterior belief in\n\\(H\\). Figure 2 shows the situation for one\nparticular belief about how likely \\(X\\) is; for one particular \\(p\\in\nP\\). The horizontal line can shift up or down, depending on what the\ncommittee member we focus on believes about \\(X\\). \\(p(H\\mid Y)\\) is\na half only if the prior in \\(X\\) is also a half. However, the\nimprecise probabilist takes into account all the ways \\(Y\\) might\naffect belief in \\(H\\). Figure 2: A member of the credal committee (after Joyce (2011)) Consider a group of agents who each had precise credences in the\nabove coin case and differed in their priors\non \\(X\\). They would all start out with prior\nof a half in \\(H\\). After\nlearning \\(Y\\), these agents would differ in\ntheir posterior opinions about \\(H\\) based on\ntheir differing dispositions to update. The group belief would\ndilate. However, no agent in the group has acted in any way\nunreasonably. If we take Levi’s suggestion that individuals can be\nconflicted just like groups can, then it seems that individual agents\ncan have their beliefs dilate just like groups can. There are two apparent problems with dilation. First, the\nbelief-moving effect of apparently irrelevant evidence; and second,\nthe fact that learning some evidence can cause your belief-intervals\nto widen. The above comments speak to the first of\nthese. Pedersen and Wheeler\n(2014) also are focused on mitigating this worry. We\nturn now to the second worry. Even if we accept dilation as a fact of life for the imprecise\nprobabilist, it is still weird. Even if all of the above\nargument is accepted, it still seems strange to say that your belief\nin \\(H\\) is dilated, whatever you\nlearn. That is, whether you learn \\(Y\\)\nor \\(Y'\\), your posterior belief\nin \\(H\\) looks the\nsame: \\([0,1]\\). Or perhaps, what it shows to\nbe weird is that your initial credence was precise. \nHart and Titelbaum (2015) suggest that dilation is strange because conditionalising on a biconditional (which is, after all, what you are doing in the above example) is unintuitive even in the precise case. Whether all cases of dilation can be explained away in this manner remains to be seen.\nGong and Meng (2017) likewise see dilation as a problem of mis-specified statistical inference, rather than a problem for IP per se.\n Beyond this seeming strangeness, White suggests a specific way that\nbeing subject to dilation is an indicator of a defective\nepistemology. White suggests that dilation examples show that\nimprecise probabilities violate the Reflection\nPrinciple (van Fraassen 1984). The\nargument goes as follows:\n  given that you know now that whether you\nlearn \\(Y\\) or you\nlearn \\(Y'\\) your credence\nin \\(H\\) will\nbe \\([0,1]\\) (and you will certainly learn one\nor the other), your current credence in \\(H\\)\nshould also be \\([0,1]\\).\n  The general idea is that you should set your\n credences to what you expect your credences to be in the future. More\n specifically, your credence in \\(X\\) should\n be the expectation of your future possible credences\n in \\(X\\) over the things you might\n learn. Given that, for all the things you might learn in this example\n your credence in \\(H\\) would be the same, you\n should have that as your prior credence also. Your prior should be\n such that \\(P(H) = [0,1]\\). So having a\n precise prior credence in \\(H\\) to start with\n is irrational. That’s how the argument against dilation from\n reflection goes. Your prior \\(P\\) is not\n fully precise though. Consider \\(P(H \\cap\n Y)\\). That is, the prior belief in the conjunction is\n imprecise. So the alleged problem with dilation and reflection is not\n as simple as “your precise belief becomes imprecise”. The\n problem is “your precise\n belief in \\(H\\) becomes\n imprecise”; or rather, your precise belief\n in \\(H\\) as represented\n by \\(P(H)\\) becomes imprecise. The issue with reflection is more basic. What exactly does\nreflection require of imprecise probabilists in this case? Now, it is\nobviously the case that each credal committee member’s prior credence\nis its expectation over the possible future evidence (this is a\ntheorem of probability theory). But somehow, it is felt,\nthe credal state as a whole isn’t sensitive to reflection in\nthe way the principle requires. Each \\(p\\in\nP\\) satisfies the principle, but the awkward symmetries of the\nproblem conspire to make \\(P\\) as a whole\nviolate the principle. This looks to be the case if we focus\non \\(P(H)\\) as an adequate representation of\nthat part of the belief state. But as noted earlier, this is not an\nadequate way of understanding the credal state. Note that while\nlearning \\(Y\\) and\nlearning \\(Y'\\) both prompt revision to a\nstate where the posterior belief in \\(H\\) is\nrepresented as an interval by \\([0,1]\\), the\ncredal states as sets of probabilities are not the same. Call\nthe state after\nlearning \\(Y\\), \\(P'\\)\nand the state after\nlearning \\(Y'\\), \\(P''\\). So \\(P'\n= \\{p(\\cdot \\mid Y), p\\in P\\}\\)\nand \\(P'' = \\{p(\\cdot\\mid Y'), p\\in\nP\\}\\). While it is true that \\(P'(H) =\nP''(H)\\), \\(P' \\neq\nP''\\) as sets of probabilities, since\nif \\(p\\in P'\\)\nthen \\(p(Y) = 1\\) whereas\nif \\(p\\in P''\\)\nthen \\(p(Y) = 0\\). So one lesson we should\nlearn from dilation is that imprecise belief is represented\nby sets of functions rather than by a set-valued\nfunction (see also, Joyce 2011; Topey 2012;\nBradley and Steele 2014b). \nSo dilation can perhaps be tamed or rationalised, and the issue with reflection can be mitigated.\nBut there is still a puzzle that dilation raises:\nin the precise context we have a nice result – due to Good (1967) –\nthat says roughly that learning new information has positive expected value.\nInformation has positive value.\nThis result is, to some extent, undermined by dilation.\nBradley and Steele (2016) suggest that there is \nsome sense in which Good’s result can be partially salvaged in the IP setting.\n It seems that examples of dilation undermine the earlier claim that\nimprecise probabilities allow you to represent the difference between\nthe weight and balance of evidence\n(see section 2.3):\nlearning \\(Y\\) appears to give rise to a\nbelief which one would consider as representing less evidence\nsince it is more spread out. This is so because the prior credence in\nthe dilation case is precise, not through weight of evidence, but\nthrough the symmetry discussed earlier. We cannot take narrowness of\nthe interval \\([\\underline{P}(X),\n\\overline{P}(X)]\\) as a characterisation of weight of evidence\nsince the interval can be narrow for reasons other than because lots\nof evidence has been accumulated. So my earlier remarks on\nweight/balance should not be read as the claim that imprecise\nprobabilities can always represent the weight/balance\ndistinction. What is true is that there are cases where\nimprecise probabilities can represent the distinction in a way that\nimpacts on decision making. This issue is far from settled and more\nwork needs to be done on this topic. Imagine there are two live\nhypotheses \\(H_1\\)\nand \\(H_2\\). You have no idea how likely they\nare, but they are mutually exclusive and exhaustive. Then you acquire\nsome evidence \\(E\\). Some simple probability\ntheory shows that for every \\(p\\in P\\) we have\nthe following relationship (using \\(p_i = p(E\\mid\nH_i)\\) for \\(i=1,2\\)). If your prior in \\(H_1\\) is vacuous—if \\(P(H_1) =\n[0,1]\\)—then the above equation shows that your posterior is\nvacuous as well. That is, if \\(p(H_1) = 0\\) then \\(p(H_1\\mid E) =\n0\\) and likewise for \\(p(H_1) = 1 = p(H_1\\mid E)\\), and since the\nright hand side of the above equation is a continuous function of\n\\(p(H_1)\\), for every \\(r\\in [0,1]\\) there is some \\(p(H_1)\\) such\nthat \\(p(H_1\\mid E) = r\\). So \\(P(H_1\\mid E)=[0,1]\\). It seems like the imprecise probabilist cannot learn from vacuous\npriors. This problem of belief inertia goes back at least as far as\nLevi (1980), chapter 13.\nWalley also discusses the issue, but appears unmoved by it: \nhe says that vacuous posterior probabilities are just a\nconsequence of adopting a vacuous prior: The vacuous previsions really are rather trivial models. That seems\nappropriate for models of “complete ignorance” which is a\nrather trivial state of uncertainty. On the other hand, one cannot\nexpect such models to be very useful in practical problems,\nnotwithstanding their theoretical importance. If the vacuous\nprevisions are used to model prior beliefs about a statistical\nparameter for instance, they give rise to vacuous posterior\nprevisions… However, prior previsions that are close to vacuous\nand make nearly minimal claims about prior beliefs can lead to\nreasonable posterior previsions. (Walley 1991:\n93) Joyce (2011)\nand Rinard (2013) have both discussed\nthis problem. Rinard’s solution to it is to argue that this shows that\nthe vacuous prior is never a legitimate state of belief. Or rather,\nthat we only ever need to model your beliefs using non-vacuous priors,\neven if these are incomplete descriptions of your belief state. This\nis similar to Walley’s “non-exhaustive” representation of\nbelief. \nVallinder (2018) suggests that the problem of belief inertia is quite a general one.\nCastro and Hart (forthcoming) use the looming danger of belief inertia to argue against what I have called an \"objectivist\" interpretation of IP.\n An alternative solution to this\nproblem, (inspired by Wilson 2001; and Cattaneo\n2008; 2014), would modify the update rule in such a way that\nthose extreme priors that give extremely small likelihoods to the\nevidence are excised from the representor. More work would need to be\ndone to make this precise and show how exactly the response would\ngo. One important use that models of belief can be put to is as part of\na theory of rational decision. IP is no different. Decision making\nwith imprecise probabilities has some problems, however. The problem for IP decision making, in short, is that your credal\ncommittee can disagree on what the best course of action is, and when\nthey do, it is unclear how you should act (recall the definitions\nin section 1.1). Imagine betting on a coin of\nunknown bias. Consider the indicator gambles on heads and tails. Both\nbets have imprecise expectation \\([0,1]\\). How\nare you supposed to compare these expectations? The bets are\nincomparable. (If the coin case appears to have too much exploitable\nsymmetry, consider unit bets on Elga pulling toothpaste or jellyfish\nfrom his bag.) This incomparability, argues Williamson, leads to\ndecision making paralysis, and this highlights a flaw in the\nepistemology (2010: 70). This argument\nseems to be missing the point, however, if one of our motivations for\nIP is precisely to be able to represent such incompatibility of\nprospects (see section 2.2)! The\nincommensurability of options entailed by IP is not a bug, it’s a\nfeature. Decision making with imprecise probabilities is discussed\nby Seidenfeld\n(2004), Troffaes\n(2007), Seidenfeld, Schervish, and\nKadane (2010), Bradley\n(2015), Williams\n(2014), Huntley, Hable, and Troffaes\n(2014). A more serious worry confronts IP when you have to make sequences\nof decisions. There is a rich literature in economics on sequences of\ndecisions for agents who fail to be orthodox expected utility\nmaximisers (Seidenfeld 1988; 1994; Machina\n1989; Al-Najjar and Weinstein 2009, and the references\ntherein). This topic was brought to the attention\nof philosophers again after the publication of\nElga’s (2010) paper Subjective\nProbabilities Should Be Sharp which highlights the problem with a\nsimple decision example, although a very similar example appears\nin Hammond (1988) in relation to\nSeidenfeld’s discussion of Levi’s decision rule\n“E-admissibility” (Seidenfeld\n1988). A version of the problem is as follows. You are about to be offered\ntwo bets on a coin of unknown bias, \\(A\\)\nand \\(B\\), one after the other. The bets pay\nout as follows: If we assume you have beliefs represented\nby \\(P(H)=[0,1]\\), these bets have\nexpectations of \\([-10,15]\\). Refusing each\nbet has expectation of 0. So accepting and\nrefusing \\(A\\) are incomparable with respect\nto your beliefs. Likewise for \\(B\\). The\nproblem is that refusing both bets seems to be irrational,\nsince accepting both bets gets you a guaranteed payoff of 5. Elga\nargues that no decision rule for imprecise probabilities can rule out\nrefusing both bets. He then argues that this shows that imprecise\nprobabilities are bad epistemology. Neither argument\nworks. Chandler (2014)\nand Sahlin and Weirich (2014) both point\nout that a certain kind of imprecise decision rule does make refusing\nboth bets impermissible and Elga has acknowledged this in an erratum\nto his paper. Bradley and Steele (2014a)\nargue that decision rules that make refusing both bets merely\npermissible are legitimate ways to make imprecise decisions. They also\npoint out that the rule that Chandler, and Sahlin and Weirich advocate\nhas counterintuitive consequences in other decision problems. \n \nMoss (2015) relates\nElga-style IP decision problems to moral dilemmas and uses the analogy\nto explain the conflicting intuitions in Elga’s problem.\nSud (2014) and Rinard (2015)\nboth also offer alternative decision theories for imprecise probabilities.\nBradley (2019) argues that all three struggle to accommodate a version of the Ellsberg decisions discussed above.\n \nEven if\nElga’s argument worked and there were no good imprecise decision\nrules, that wouldn’t show that IP was a faulty model of belief. We\nwant to be able to represent the suspension of judgement on various\nthings, including on the relative goodness of a number of\noptions. Such incommensurability inevitably brings with it some\nproblems for sequential decisions (see, for\nexample, Broome 2000), but this is not an argument against the\nepistemology. As Bradley and Steele note, Elga’s argument—if it\nwere valid—could mutatis mutandis be used as an\nargument that there are no incommensurable goods and this seems too\nstrong.  Imprecise probabilities aren’t a radically new theory. They\nare merely a slight modification of existing models of belief for\nsituations of ambiguity. Often your credences will be precise enough,\nand your available actions will be such that you act more or\nless as if you were a strict Bayesian. One might analogize imprecise\nprobabilities as the “Theory of Relativity” to the strict\nBayesian “Newtonian Mechanics”: all but indistinguishable\nin all but the most extreme situations. This analogy goes deeper: in\nboth cases, the theories are “empirically\nindistinguishable” in normal circumstances, but they both differ\nradically in some conceptual respects. Namely, the role of absolute\nspace in Newtonian mechanics/GR; how to model ignorance in the\nstrict/imprecise probabilist case. Howson\n(2012) makes a similar analogy between modelling belief and\nmodels in science. Both involve some requirement to be somewhat\nfaithful to the target system, but in each case faithfulness must be\nweighed up against various theoretical virtues like simplicity,\ncomputational tractability and so\non. Likewise Hosni (2014) argues that\nwhat model of belief is appropriate is somewhat dependent on\ncontext. There is of course an important disanalogy in that models of\nbelief are supposed to be normative as well as descriptive,\nwhereas models in science typically only have to play a descriptive\nrole. Walley (1991) discusses a similar\nview but is generally sceptical of such an interpretation. One standard interpretation of the probability calculus is that\nprobabilities represent “degrees of belief” or\n“credences”. This is more or less the concept that under\nconsideration so far. But what is a degree of belief? There are a\nnumber of ways of cashing out what it is that a representation of\ndegree of belief is actually representing. One of the most straightforward understandings of degree of belief\nis that credences are interpreted in terms of an agent’s limiting\nwillingness to bet. This is an idea which goes back\nto Ramsey (1926)\nand de Finetti (1964, 1990 [1974]). The\nidea is that your credence in \\(X\\)\nis \\(\\alpha\\) just in\ncase \\(\\alpha\\) is the value at which you are\nindifferent between the gambles: This is the “betting interpretation”. This is the\ninterpretation behind Dutch book arguments: this interpretation of\nbelief makes the link between betting quotients and belief strong\nenough to sanction the Dutch book theorem’s claim\nthat beliefs must be probabilistic. Williamson in fact takes\nissue with IP because IP cannot be given this betting\ninterpretation (2010: 68–72). He\nargues that Smith’s and Walley’s contributions notwithstanding\n(see formal appendix), the\nsingle-value betting interpretation makes sense as a standard for\ncredence in a way that the one-sided betting interpretation\ndoesn’t. The idea is that you may refuse all bets unless they are at\nextremely favourable odds by your lights. Such behaviour doesn’t speak\nto your credences. However, if you were to offer a single\nvalue then this tells us something about your epistemic\nstate. There is something to this idea, but it must be traded off\nagainst the worry that forcing agents to have such single\nnumbers systematically misrepresents their epistemic states. As Kaplan\nputs it The mere fact that you nominate \\(0.8\\)\nunder the compulsion to choose some determinate value for\n[\\(p(X)\\)] hardly means that you have\na reason to choose \\(0.8\\). The\northodox Bayesian is, in short, guilty of advocating false\nprecision. (Kaplan 1983: 569, Kaplan’s\nemphasis) A related interpretation of credence is to understand credence as\nbeing just a representation of an agent’s dispositions to act. This\ninterpretation sees credence as that function such that your elicited\npreferences and observed actions can be represented as those of an\nexpected utility maximiser with respect to that probability\nfunction (Briggs 2014: section 2.2). Your\ncredences just are that function that represents you as a\nrational agent. For precise probabilism, “rational agent”\nmeans “expected utility maximiser”. For imprecise\nprobabilism, rational agent must mean something slightly different. A\nslightly more sophisticated version of this sort of idea is to\nunderstand credence to be exactly that component of the preference\nstructure that the probability function represents in the\nrepresentation theorem. Recall the discussion of incompleteness\n(section 2.2). IP represents you as the agent\nconflicted between all the \\(p \\in P\\) such\nthat unless the \\(p\\) agree\nthat \\(X\\) is better\nthan \\(Y\\) or vice versa, you find them\nincomparable. What a representation theorem actually proves is a\nmatter of some dispute (see Zynda 2000; Hájek\n2008; Meacham and Weisberg 2011). One might take the view that credence is modelling some kind of\nmental or psychological quantity in the head. Strength of belief is a\nreal psychological quantity and it is this that credence should\nmeasure. Unlike the above views, this interpretation of credence isn’t\neasy to operationalise. It also seems like this understanding of\nstrength of belief distances credence from its role in understanding\ndecision making. The above behaviourist views take belief’s\nrole in decision making to be central to or even definitional of what\nbelief is. This psychological interpretation seems to divorce belief\nfrom decision. Whether there are such stable neurological structures\nis also a matter of some controversy (Fumagalli\n2013; Smithson and Pushkarskaya 2015). A compromise between the behaviourist views and the psychological\nviews is to say that belief is characterised in part by its\nrole in decision making. This leaves room for belief to play an\nimportant role in other things, like assertion or reasoning and\ninference. So the answer to the question “What is degree of\nbelief?” is: “Degree of belief is whatever psychological\nfactors play the role imputed to belief in decision making contexts,\nassertion behaviour, reasoning and inference”. There is room in\nthis characterisation to understand credence as measuring some sort of\npsychological quantity that causally relates to action, assertion and\nso on. This is a sort of functionalist reading of what belief\nis. Eriksson and Hájek (2007) argue that\n“degree of belief” should just be taken as a primitive\nconcept in epistemology. The above attempts to characterise degree of\nbelief then fill in the picture of the role degree of belief\nplays. So now we have a better idea of what it is that a model of belief\nshould do. But which part of our model of belief is representing which\npart of the belief state? The first thing to say is\nthat \\(P(X)\\) is not an adequate\nrepresentation of the belief in \\(X\\). That\nis, one of the values of the credal set approach is that it can\ncapture certain kinds of non-logical relationships between\npropositions that are lost when focusing on, say, the associated set\nof probability values. For example, consider tossing a coin of unknown\nbias. \\(P(H)=P(T)=[0,1]\\), but this fails to\nrepresent the important fact\nthat \\(p(H)=1-p(T)\\) for\nall \\(p\\in P\\). Or that getting a heads on the\nfirst toss is at least as likely as heads on two consecutive\ntosses. These facts that aren’t captured by the sets-of-values view\ncan play an important role in reasoning and decision. \\(P(X)\\) might be a good enough\nrepresentation of belief for some purposes. For example in the\nEllsberg game these sets of probability values (and their associated\nsets of expectations) are enough to rationalise the non-probabilistic\npreferences. How good the representation needs to be depends on what\nit will be used for. Representing the sun as a point mass is a good\nenough representation for basic orbital calculations, but obviously\ninadequate if you are studying coronal mass ejections, solar flares or\nother phenomena that depend on details of the internal dynamics of the\nsun. Imprecise probabilities is a theory born of our limitations as\nreasoning agents, and of limitations in our evidence base. If only we\nhad better evidence, a single probability function would do. But since\nour evidence is weak, we must use a set. In a way, the same is true of\nprecise probabilism. If only we knew the truth, we could represent\nbelief with a truth-valuation function, or just a set of sentences\nthat are fully believed. But since there are truths we don’t know, we\nmust use a probability to represent our intermediate confidence. And\nindeed, the same problem arises for the imprecise probabilist. Is it\nreasonable to assume that we know what set of probabilities best\nrepresents the evidence? Perhaps we should have a set of sets of\nprobabilities… Similar problems arise for theories of\nvagueness (Sorensen 2012). We objected\nto precise values for degrees of belief, so why be content with\nsets-valued beliefs with precise boundaries? This is the problem of\n“higher-order vagueness” recast as a problem for imprecise\nprobabilism. Why is sets of probabilities the right level to stop the\nregress at? Why not sets of sets? Why not second-order probabilities?\nWhy not single probability\nfunctions? Williamson (2014)\nmakes this point, and argues that a single precise probability is the\ncorrect level at which to get off the “uncertainty\nescalator”. Williamson advocates the betting interpretation of\nbelief, and his argument here presupposes that interpretation. But the\npoint is still worth addressing: for a particular interpretation of\nwhat belief is, what sort of level of uncertainty is appropriate. For\nthe functionalist interpretation suggested above, this is\nsomething of a pragmatic choice. The further we allow this regress to\ncontinue, the harder it is to deal with these belief-representing\nobjects. So let’s not go further than we need. We have seen arguments above that IP does have some advantage over\nprecise probabilism, in the capacity to represent suspending\njudgement, the difference between weight and balance of evidence and\nso on. So we must go at least this far up the uncertainty\nescalator. But for the sake of practicality we need not go any\nfurther, even though there are hierarchical Bayes models that would\ngive us a well-defined theory of higher-order models of belief. This\nis, ultimately, a pragmatic argument. Actual human belief states are\nprobably immensely complicated neurological patterns with all the\nattendant complexity, interactivity, reflexivity and vagueness. We\nare modelling belief, so it is about choosing a model at the\nright level of complexity. If you are working out the trajectory of a\ncannonball on earth, you can safely ignore the gravitational influence\nof the moon on the cannonball. Likewise, there will be contexts where\nsimple models of belief are appropriate: perhaps your belief state is\njust a set of sentences of a language, or perhaps just a single\nprobability function. If, however, you are modelling the tides, then\nthe gravitational influence of the moon needs to be involved: the\nmodel needs to be more complex. This suggests that an adequate model of\nbelief under severe uncertainty may need to move beyond the single\nprobability paradigm. But a pragmatic argument says that we should\nonly move as far as we need to. So while you need to model the moon to\nget the tides right, you can get away without having Venus in your\nmodel. This relates to the contextual nature of appropriateness for\nmodels of belief mentioned earlier. If one were attempting to\nprovide a complete formal characterisation of the ontology of\nbelief, then these regress worries would be significantly harder to\navoid. Let’s imagine that we had a second order\nprobability \\(\\mu\\) defined over the set of\n(first order) probabilities \\(P\\). We could\nthen reduce uncertainty to a single function\nby \\(p^*(X) = \\sum_P \\mu(p)p(X)\\)\n(if \\(P\\) is finite, in the interests of\nkeeping things simple I discuss only this case). Now\nif \\(p^*(X)\\) is what is used in decision\nmaking, then there is no real sense in which we have a genuine IP\nmodel, and it cannot rationalise the Ellsberg choice, nor can it give\nrise to incomparability. If there is some alternative use\nthat \\(\\mu\\) is put to, a use that allows\nincomparability and that rationalises Ellsberg choices, then it might\nbe a genuine rival to credal sets, but it represents just as much of a\ndeparture from the orthodox theory as IP does. Gärdenfors and Sahlin’s Unreliable Probabilities model\nenriches a basic IP approach with a “reliability index”\n(see the\n historical appendix).\n Lyon (2017)\nenriches the standard IP picture in a different way: he adds a\nprivileged “best guess” probability. This modification\nallows for better aggregation of elicited IP estimates. How best to\ninterpret such a model is still an open question. Other enriched IP\nmodels are no doubt available. There are, as we have seen, certain structural properties that are\nnecessary conditions on rational belief. What exactly these are\ndepends on your views. However, there are further ways of assessing\nbelief. Strongly believing true things and strongly believing the\nnegations of false things seem like good-making-features of\nbeliefs. For the case of precise credences, we can make this\nprecise. There is a large literature on “scoring rules”:\nmethods for measuring how good a probability is relative to the actual\nstate of the world (Brier 1950; Savage 1971;\nJoyce 2009; Pettigrew 2011). These are numerical methods of\nmeasuring how good a probability is given the true state of the\nworld. For the case of imprecise probabilities, however, the situation\nlooks bleak. No real valued scoring rule for imprecise probabilities\ncan have the desirable property of being strictly\nproper (Seidenfeld, Schervish, and Kadane\n2012). Schoenfield (2017) presents a simple version of the result. Since strict propriety is a desirable property of a\nscoring rule (Bröcker and Smith 2007; Joyce\n2009; Pettigrew 2011), this failing is serious. So further work\nis needed to develop a well-grounded theory of how to assess imprecise\nprobabilities.\nMayo-Wilson and Wheeler (2016) provide a neater version of the proof, and offer a property weaker than strict propriety that an imprecise probability scoring rule can satisfy.\nCarr (2015) and Konek (forthcoming) both present positive suggestions for moving forward with imprecise scoring rules.\nLevinstein (forthcoming) suggests that the problem really only arises for determinately imprecise credences, but not for indeterminate credence. \n\nImprecise probabilities offer a model of rational belief that does\naway with some of the idealisation required by the orthodox precise\nprobability approach. Many motivations for such a move have been put\nforward, and many views on IP have been discussed. There are still\nseveral open philosophical questions relating to IP, and this is\nlikely to be a rich field of research for years to come.","contact.mail":"S.C.Bradley@leeds.ac.uk","contact.domain":"leeds.ac.uk"}]
