[{"date.published":"2009-09-15","url":"https://plato.stanford.edu/entries/information-entropy/","author1":"Owen Maroney","author1.info":"https://www.philosophy.ox.ac.uk/people/owen-maroney","entry":"information-entropy","body.text":"\n\n\n\nAre principles of information processing necessary to demonstrate the\nconsistency of statistical mechanics? Does the physical implementation\nof a computational operation have a fundamental thermodynamic cost,\npurely by virtue of its logical properties? These two questions lie at\nthe centre of a large body of literature concerned with the Szilard\nengine (a variant of the Maxwell's demon thought experiment),\nLandauer's principle (supposed to embody the fundamental principle of\nthe thermodynamics of computation) and possible connections between\nthe two. A variety of attempts to answer these questions have\nillustrated many open questions in the foundations of statistical\nmechanics.\n\n\n\nMaxwell's demon was first mentioned in a letter written to Tait in\n1867. Maxwell was one amongst a number of researchers in the\ndeveloping field of thermodynamics who was interested in seeking an\nunderstanding of thermal phenomena in terms of an underlying atomic\nphysics. However, unlike Boltzmann and Clausius, who were attempting\nto prove the law of entropy increase from such atomic physics, Maxwell\nhad realised that if thermodynamics was ultimately grounded in atomic\ntheory, then the second law of thermodynamics could have only a\nstatistical validity. \n\nThe standard demon is supposed to be able to create a temperature\ndifference in a gas, without expending work. The gas is in a\ncontainer, divided in two by an insulated partition, but there is a\nhole in the partition just large enough for a single molecule to pass\nthrough. The gas has been allowed to equilibrate at some well defined\ntemperature, and as a result the average kinetic energy of each\nmolecule is (3/2)kT (we ignore internal degrees of\nfreedom and assume the gas is monatomic), where\nT is the absolute (Kelvin) temperature scale and k\nis Boltzmann's constant. \n\nThe demon is equipped with a shutter that is able to block the\nhole. If a molecule that is moving faster than average approaches the\nhole from the left hand side, the demon closes the hole with the\nshutter, and the molecule is elastically reflected back to the\nleft. If a molecule approaches from the left hand side, but moving\nslower than average, the demon leaves the hole unblocked, and the\nmolecule proceeds through to the right. When a molecule approaches the\nhole from the right hand side, the demon's sorting procedure is\nreversed: slow molecules are blocked and fast molecules are allowed to\npass through.  The result is a gradual accumulation of faster\nmolecules on the left and slower molecules on the right. Further\ncollisions between molecules will distribute this kinetic energy\nthroughout each side, resulting in the gas in the left side getting\nhotter and the right side getting cooler. As the collisions with the\nshutter are elastic, and moving the shutter is frictionless, no work\nis performed by the demon. The temperature difference that develops\ncould be exploited by a conventional heat engine to extract work, in\nviolation of second law of thermodynamics. \n\nA simpler demon could be constructed simply by always blocking\nmolecules coming from the left and never blocking molecules approaching\nfrom the right. This pressure demon would cause a pressure difference\nto develop between the two sides of the partition and again, a\nconventionally operating engine could exploit such a difference to\nextract work. \n\nHis thought experiment was intended to demonstrate the possibility\nof a gas evolving from a higher to a lower entropy state. The later\ntime reversal and recurrence arguments of Loschmidt and Poincare\nsimilarly challenged the H-theorem of Boltzmann (see Uffink (2006),\nSection 4, for a discussion of these arguments), but time reversal\nrequires great sensitivity and recurrence takes a very long time.\nAlthough they show the law of entropy increase is not absolute, we\nmight still be surprised to actually witness a large decrease in\nentropy happen through these means. With the demon we to appear to need\nanother explanation of why such systems are not seen to occur\nspontaneously in nature, or why we cannot arrange for such a decrease\nin entropy to occur. \n\nMaxwell's original discussions emphasised that the demon needed to\nhave powers of perception and handling the individual molecules far\ngreater than our own. This makes its operation simply a matter of scale\nand the statistical nature of the second law not probabilistic, but\ndue to our inability to discriminate the exact state of a large number\nof particles (similar to our inability to exploit Loschmidt's\nreversibility objection). This leaves open the possibility of a device\nwhich could discriminate fluctuations in individual atomic velocities\nand it is not clear that any probabilistic argument would prevent work\nbeing extracted from this. The explanation of Brownian motion by\nEinstein in 1905, as the effect of statistical mechanical fluctuations,\nmade them appear directly observable and open to exploitation. Earman\nand Norton (1998) includes a historical review of early attempts to\ndesign such devices. \n\nSmoluchowski (1914) is generally credited with having proposed the\nexplanation that prevents their operation. He replaced the demon by a\nphysical device, in this case a gentle spring that presses the trapdoor\nagainst the side of the partition. The combined spring and trapdoor is\nsupposed to act as a valve. The trapdoor is held shut if a molecule\ncollides from the left, but is opened by a fast collision from the\nright, so that a pressure difference develops. However, the spring is a\nsystem with its own kinetic and potential energy. The collisions will\ntransfer energy to the spring, making it oscillate. When the internal\nenergy of the spring matches the temperature of the gas, it is flapping\nback and forth, effectively randomly. It becomes as likely as not to be\nspontaneously open when a molecule arrives from the left and to be\nswinging shut when a molecule arrives from the right. This balance\nmeans pressure or temperature differences are no more likely to occur\nthan they would spontaneously if the hole was simply left open. If the\ninternal energy of the spring is out of equilibrium with the gas, it\nwill be heated (or cooled) by the collisions with the molecules until\nequilibrium is reached. \n\nThe trapdoor may appear to violate the second law over short\nperiods, but the behaviour is such that it is not violated in the long\nrun. Smoluchowski suggested that a modified second law should express\nthe inability of a device to produce continuous, reliable reductions in\nentropy. \n\nSmoluchowski also left the possibility open of an exception even to\na modified second law: \n\nSzilard (1929) attempted to investigate this special case of\nintelligently operated devices by considering a box containing only a\nsingle molecule. He argued that in order to achieve the entropy\nreduction, the intelligent being must acquire knowledge of which\nfluctuation occurs and so must perform a measurement. The second law\nwould not be threatened provided there was a compensating cost to\nperforming this measurement, regardless of the character of the\nintelligent being. \n\nThe Szilard engine consists of a box, containing a single molecule, in\nthermal contact with a heat bath, and a partition. Thermal contact\ntransfers energy, through random fluctuations, back and forth between\nthe molecule and the heat bath. The molecule bounces randomly\nthroughout the box with this thermal energy. \n\nThe partition is capable of being inserted into the box, dividing it\ninto two separate volumes, and is also capable of sliding,\nfrictionlessly, along the box to the left or to the right. When the\npartition is inserted in the box, collisions with the molecule exert a\npressure on the partition. If the partition moves in the direction of\nthe pressure, it may be coupled to a pulley and the force used to lift\na weight. If the partition moves against the pressure, this requires\nthe coupled pulley to lower a weight, working against the pressure\nexerted by the molecule. \n\nIf the partition is inserted at the middle point, the molecule is\ncaught on one side or the other with equal probability. Now if it is known\nwhich side the molecule is on, it is possible to connect up the\npartition to a pulley and extract work. Based upon the ideal gas law,\nPV=NkT, for the case\nwhere\nN=1, it is a standard calculation to show that the maximum\nwork extracted as the partition moves to the side of the box\nis kT  ln 2.  As the molecule\nis assumed to be in thermal contact with the heat bath at all times,\nthe kinetic energy of the molecule is maintained at\n(3/2)kT, and the work extracted is drawn from the\nheat bath. Once the partition reaches the side of the box, it can be\nremoved and the cycle has been completed. Heat has been extracted from\nthe heat bath and converted into work, with apparent certainty.  The\nprocess may be repeated indefinitely to continue the certain\nextraction of work. If this succeeds, Smoluchowski's modified second\nlaw appears to be violated. \n\nSzilard's analysis emphasised the necessity of knowing\nwhich side the molecule is on, for work to be extracted. Without this\nit would not be possible to know in which direction the partition needs\nto move. This connects the apparent second law violation to the state\nof a demon's knowledge. Szilard argued that the second law would be\nsaved if the acquisition of knowledge by the demon came with a\ncompensating entropy cost. Much of the succeeding literature is\nconcerned with whether, and how, this knowledge comes with the cost\nthat Szilard believed, or with whether such knowledge is, in fact,\nnecessary for the engine's operation. \n\nLandauer (1961) investigated the question of what are the physical\nlimitations on building a device to implement a computation. At the\ntime he wrote, an influential body of work had been developed, by\nBrillouin (1951, 1956), Gabor (1964) and Rothstein (1951), arguing\nthat the acquisition of information through a measurement required a\ndissipation of at least\nkT  ln 2\nenergy for each bit of information gathered. von Neumann (1949) had\nalso suggested on the basis of Szilard's work, that every act of\ninformation processing was necessarily accompanied by this level of\nenergy dissipation. \n\nStarting by representing logical operations as abstract maps defined\nfrom one set of discrete logical states to another set, Landauer argued\nthat a physical system that was designed to implement the logical\noperation must have physical states to correspond to the logical\nstates. He then distinguished between logically reversible and\nlogically irreversible operations: an operation is logically reversible\nif the input state can be uniquely identified from the output\nstate. \n\nThe NOT operation, for example, is a logically reversible operation.\nIf the output is logical state one, then the input must have been\nlogical state zero, and vice versa. An example of a logically\nirreversible operation is the AND operation. If the output is logical\nstate zero, then there are three possible combinations of input logical\nstates that could have produced that output: (zero, zero); (zero, one);\nand (one, zero). \n\nAs logically reversible operations need to be 1:1 maps, Landauer\nargued they can be implemented by physical devices which do not\ncompress the physical state space. Logically irreversible operations\nreduce the logical state space, so must compress the physical state\nspace. Landauer argued that this must be accompanied by a corresponding\nentropy increase in the environment, in the form of heat dissipation.\nMost familiar logical operations are irreversible and so by this\nargument must generate heat. \n\nTo quantify the heat generation Landauer considered the most basic\nlogically irreversible operation to be resetting a bit. This operation\ntakes two input logical states, (conventionally zero and one) and\nalways outputs logical state zero (in some papers reset to one is\nconsidered instead). \n\nFor a physical implementation of this operation, it is usual to\nconsider a device similar to the Szilard engine. A box, in thermal\ncontact with a heat bath, contains a single molecule and a partition\nwhich divides the box in two. If the molecule is on the left hand side,\nthen the physical state represents logical state zero and if the\nmolecule is on the right hand side, it represents logical state\none. \n\nThe partition is then removed from the centre of the box, so the\nmolecule is free to travel throughout the box. The partition is\ninserted into the far right hand side of the box and, maintaining\nthermal contact, is slowly moved to the centre of the box. Once again,\ncollisions with the molecule exert a pressure on the partition,\nrequiring work to be performed, and the energy from the work is\ntransferred via the molecule to heat in the heat bath. Standard\ncalculations show this requires at least\nkT  ln 2 work. One expression\nof what has now become known as “Landauer's principle” is\nthat there are no possible physical implementations of the resetting\noperation that can do better that this, that is, reset a bit to zero\nconverting less than\nkT  ln 2\nof work into heat. \n\nLandauer referred to this operation as resetting, although in much of\nthe succeeding literature this has become known as erasure. This has\ncaused some confusion as “erasing” information could be\ntaken just to mean an operation which destroys the original\ninformation, without necessarily leaving the system determinately in\nthe zero state. An example of such\n‘erasure-by-destruction’ would be to simply remove the\npartition from the box, wait long enough for thermalisation to\nrandomise the location of the molecule, then reinsert the partition in\nthe centre of the box.  Such an operation would clearly destroy the\noriginal information, represented by the original location of the\nmolecule, but requires no work to be performed. However, it is also\nclearly not an implementation of the reset operation. \n\nLandauer's argument suggests that, purely from the abstract properties\nof a logical operation one can deduce a thermodynamic constraint upon\nany physical system which is required to act as an embodiment of that\nlogical operation. It suggests there is a non-trivial thermodynamics\nof computation, a non-trivial connection between abstract logical\nfunctions and their physical implementation in thermal systems. \n\nLandauer did not, in his 1961 paper, directly address the question\nof whether a measurement was logically reversible or irreversible, but\nonly questioned whether the concept of a measurement had been defined\nsufficiently well in the work of Brillouin and others. At that point in\ntime, he regarded logical irreversibility as an essential part of\ncomputation and believed this was responsible for a necessary minimum\nheat generation in information processing. His arguments were presented\nas making more precise the arguments of Brillouin and von Neumann. \n\nBennett (1973) built upon Landauer's work, but argued that logical\nirreversibility could be avoided in computation in general. In\n(Bennett 1982) he argued that measurement could also be represented by\na logically reversible process, avoiding any need to generate\nheat. This represented a major change from von Neumann and Brillouin's\narguments, and Bennett's presentation of Landauer's principle rapidly\nbecame accepted as the fundamental principle of the thermodynamics of\ncomputation. \n\nThe literature devoted to analysing the second law of thermodynamics\nin the context of statistical mechanics starts with the development of\nstatistical mechanics in the late 19th century. Considerable confusion\nhas arisen simply due to the fact that as the subject has developed,\nthe meaning of key terms have changed or become ambiguous. When one\npaper speaks of the second law being violated or of entropy decreasing\nand another of it being saved or being non-decreasing, it is not\nnecessarily the case that they are referring to the same things. While\na review of the foundations of thermal physics is beyond the scope of\nthis entry (though see related entries by\n Sklar\n and\n Uffink),\n some important distinctions\nneed be borne in mind. \n\nEven in phenomenological thermodynamics, the definition of\nthermodynamic entropy is difficult to make precise and may be\napproached in a number of ways (see (Uffink 2001) for an extensive\ntreatment of this issue). The traditional approach is based upon the\nwork of Carnot, Kelvin and Clausius, one version of which will be\ngiven here. \n\nA closed thermodynamic system has contact with the rest of the world\nonly through work and heat exchange. Work performed upon the system\nlowers a weight through a gravitational potential, while work extracted\nis used to raise the weight through the potential. This work may be\nperformed through the manipulation of externally controllable\nparameters of the system (such as adjusting the volume of a sealed box\ncontaining a gas) or may be by other means (such as driving a paddle\nwheel within the gas, stirring the gas). Heat is exchanged with heat\nbaths brought into thermal contact with the system. Many heat baths may\nbe used, and they may be at different temperatures to each other. A\nclosed cycle is a sequence of operations that leaves the system in the\nsame thermodynamic state at the end of the sequence as it was at the\nstart of the sequence, but may change the position of the weight in the\ngravitational potential, and may involve quantities of heat being\ndeposited in, or extracted from, the individual heat baths. \n\nIt was empirically observed that in any closed cycle, whose sole\nresult is the generation of heats,\nQi, \nin heat baths at temperatures\nTi,\n(requiring work\nW=∑i Qi \nto be performed), the Clausius inequality: \n\nholds. Both the Kelvin version of the second law of\nthermodynamics: \n\nand the Clausius version: \n\nare special cases of this inequality. Kelvin and Clausius, building on\nthe work of Carnot, suggested that the inequality must hold, for all\nclosed cycles, as a general law. The temperature scale is the absolute\ntemperature scale (up to a positive multiplicative constant\nrescaling: T′=aT,\nwith a>0), which may be measured with an ideal gas\nthermometer. \n\nNow let us suppose there exists a process that transforms the system\nfrom a thermodynamic state A to thermodynamic\nstate B, while generating\nheats qi in heat baths at\ntemperatures Ti, and an opposite process,\nfrom B to A, generates\nheats q′i, in the same heat baths,\nin such a way that the equality: \n\nis reached.  It follows from the inequality that, if there is any\nprocess that transforms the system from state A to\nstate B, while generating\nheats Qi in heat baths at\ntemperatures Ti, then: \n\nand so: \n\nThe term \n\ndefines a minimum quantity associated with the heats generated by any\npossible process that transforms the system from state\nA to state B.\nClausius had the insight that this could be used to define a function\nof the thermodynamic state, through the measurement of heat transferred\nto heat baths, as the system changes between two states. The function,\nthe thermodynamic entropy\nSΘ, is defined by \n\nFor any other process, \n\nso for any process to be possible: \n\nAn adiabatic process (one which does not generate any heat)\nfrom state A to state B is therefore only possible\nif it is entropy increasing: SΘ(A) ≤ SΘ(B). \n\nThis definition, of thermodynamic entropy, depends upon cyclic\nprocesses that can reach the equality, which are called\nreversible processes. The existence of such processes between\nthermodynamic states allows the entropy differences between those\nstates to be determined and, by extension to all states, defines a\nthermodynamic entropy function that is globally unique (up to a\nrescaling\nS′ = a−1 S + b,\nwhere a and\nb are constants, and\na is the multiplicative constant from the\ntemperature scale). It may be noted that if there exist states that\ncannot be connected by a reversible process, it is still always\npossible to define an entropy function that satisfies \n\nfor all possible processes, but its value will not be determined\nuniquely (i.e., there will exist a number of functions that satisfy\nthe inequality). \n\nTo reach the equality for a cycle generally requires\nquasistatic reversible processes. These are processes for\nwhich the system goes through infinitesimally small changes in the\nstate variables (such as the temperature, the volume and the pressure\nof a gas) and for which the change can go in either direction with\nequal and opposite infinitesimal heat exchanges with heat baths. These\nheat exchanges are generally only reversible if the system is in\nthermal equilibrium with the heat bath. \n\nFor the changes in the state variables to be infinitesimal, the\nstate space must be continuous. A sequence of states will then be\nrepresented by a continuous curve in the state space. The curve\nconnecting A to B\nthrough these infinitesimal changes replaces the summation with an\nintegral. The\nTi can\nbe replaced by the temperature of the system\nT, and the heat,\ndQ, is now the heat absorbed by the system,\nto give: \n\nThe Clausius inequality ensures that this value is the same for all\nquasistatic reversible paths from A to B. It should\nbe noted that a quasistatic reversible path is an idealisation that is\nreachable only in the limit of infinitely slow processes. \n\nThis thermodynamic entropy is a consistently defined single valued\nfunction of the thermodynamic state only if the Clausius inequality\nholds. If a Maxwellian demon exists, however, then it would appear\npossible the Clausius inequality would not hold. To\ninvestigate further it is necessary to consider statistical mechanical\ngeneralisations of entropy. \n\nFor statistical mechanics we need to consider a microscopic state\nspace and a dynamical evolution of states in that space. Classically\nthis will be a phase space, with an N-body system having\n3N position degrees of freedom and 3N momentum\ndegrees of freedom. A single point in the phase space corresponds to\nthe combined physical state of all the N bodies. The dynamics\nis almost always supposed to be Hamiltonian. A Hamiltonian flow\npreserves the measure\ndX3NdP3N. \nThis measure can be used to define the volume,\nVR, of a\nregion of phase space, R, as: \n\nA very important consequence of this is Liouville's Theorem, which\nshows that the volume of phase space occupied by a set of states does\nnot change when that set of states evolves through a Hamiltonian\nevolution. \n\nFor quantum mechanical systems, the microscopic state space is a\nHilbert space. Dynamic evolution is generally through a unitary\noperator, but with doubly stochastic transitions if wavefunction\ncollapse occurs. Volumes of regions of the state space are associated\nwith the dimensionality of the smallest subspace containing the\nregion, and an analogue of Liouville's Theorem holds for unitary\nevolution (doubly stochastic transitions may increase, but cannot\ndecrease, this state space volume). For the most part there is little\ndifference between the classical and quantum treatments of Szilard's\nengine and Landauer's principle and, unless stated otherwise, we will\nuse the classical treatment. We will consider some of the suggested\ndifferences in Section 5. \n\nThe Boltzmann entropy,\nSB = k lnW, \nis widely regarded as being the most natural analog within statistical\nmechanics for the thermodynamic entropy. It is a property of an\nindividual microstate. The state space is divided up into a number of\ndistinct regions and SB is defined in \nterms of the volume, W, of the region of state space to which\nthe microstate belongs. All microstates within a given region have the\nsame Boltzmann entropy. \n\nThere are many approaches to defining the division of the state\nspace into distinct regions. The most common group together sets of\nmicrostates that match criteria such as being macroscopically or\nobservationally indistinguishable or are accessible over time to the\nmicrostate evolution. For the systems considered here, these approaches\ngenerally define the same regions. We may conventionally refer to these\nregions as macrostates, while acknowledging this terminology is a\nlittle inappropriate when describing systems consisting only of a\nsingle molecule. In the case of the Szilard engine, for example, the\nmacrostate of the system, when the partition is absent, consists of the\nset of all the microstates for which the molecule is in the box. When\nthe partition is inserted in the box, the macrostate is the set of all\nthe microstates for which the position of the molecule is on the same\nside of the partition as the actual location of the molecule. We will\nsometimes refer to the Boltzmann entropy of a macrostate: this is\nsimply the Boltzmann entropy of the microstates within that\nmacrostate. \n\nThe Boltzmann entropy, SB, is not \nguaranteed to be non-decreasing. While decreases of \nSB are known to be possible through the\nreversibility and recurrence objections to Boltzmann's \nH-theorem, such decreases would be seen as surprising if they\noccurred in practice. While an individual microstate can evolve from a\nhigh volume macrostate to a low volume macrostate, Liouville's Theorem\nguarantees that only a fraction of the microstates from the larger\nmacrostate can end up in the smaller macrostate under a Hamiltonian\nevolution. From the logarithmic form of Boltzmann entropy the\nproportion, by volume, is\np ≤ eΔSB/k\n(with ΔSB the reduction in the \nBoltzmann entropy between the two macrostates). If it can be assumed\nthat the probability of the microstate being in a given subregion of\nthe macrostate is proportional to the phase space volume of the\nsubregion, this gives Einstein's fluctuation formula. Although\nwidespread use is made of this assumption, its justification is one of\nthe more significant challenges for the foundations of statistical\nmechanics. \n\nBy Liouville's Theorem, if all the microstates (up to a set of measure\nzero) in an initial macrostate evolve into the same final macrostate,\nthen the Boltzmann entropy of the final macrostate cannot be less than\nthe Boltzmann entropy of the initial macrostate. This is a\nmacroscopically deterministic process. A macroscopically\nindeterministic process is one in which microstates starting in the\nsame initial macrostate end up in different final macrostates. Penrose\n(1970, Chapters V, VI) analysed the problem of the change in Boltzmann\nentropies for these kinds of processes in detail. After attempting a\njustification of probabilities being proportional to phase space\nvolume, he argued that even the average of the Boltzmann\nentropy can decrease for macroscopically indeterministic\nprocesses. For a system initially in a macrostate with Boltzmann\nentropy SB, and evolving, with\nprobability pi, into a\nmacrostate i with Boltzmann\nentropy SBi, it is possible that\n∑i piSBi < SB\n(it is even possible that\n∀i SBi < SB).\nHowever, Penrose also showed that this decrease is bounded:\n∑i pi (SBi  − k ln pi) ≥ SB.\nHe suggested that, when there is a probability distribution over\ndistinct macrostates, a modified, statistical entropy,\nSP  =  ∑i pi (SBi  − k ln pi),\nshould be used. This statistical entropy is non-decreasing even for\nmacroscopically indeterministic processes. \n\nThe Gibbs approach to statistical mechanics is based upon probability\ndistributions, p(X3N, P3N),\nover the state space, rather than the properties of individual\nmicrostates. The Gibbs entropy of the distribution is defined by\nSG = −k ∫ p(X3N, P3N) ln p(X3N, P3N) dX3NdP3N.\nIf the state space is divided into a number of distinct macrostates,\nRi, then the Gibbs entropy of the\nith macrostate is \nwith\np(X3N, P3N | i) = p(X3N, P3N, i) / pi\nand\npi = ∫Ri p(X3N, P3N) dX3N dP3N.\nThis gives\nSG = ∑i pi(SGi −k ln pi).\n(N.B.\np(X3N, P3N, i) = 0\noutside of\nRi and\np(X3N, P3N, i) = p(X3N, P3N)\ninside of\nRi.) \n\nIt is a consequence of Liouville's Theorem that this entropy is\nconstant under Hamiltonian flows. While this guarantees entropy is\nnon-decreasing, it presents a problem for providing an account of the\nappearance of entropy increase. The standard approach to dealing with\nthis is called coarse graining. Coarse graining replaces the\nprobability distribution over each macrostate,\np(X3N, P3N | i),\nwith a ‘smoother’ probability distribution,\np′(X3N, P3N | i),\nTypically the ‘uniform’ distribution: inside Ri and\np′(X3N, P3N | i) = 0\noutside Ri. \nThe coarse grained entropy for the macrostate now satisfies \nThe entropy of the overall coarse grained probability  distribution\np′(X3N, P3N) = ∑i p′(X3N, P3N | i) pi\nis \nCoarse graining the probability distribution avoids Liouville's\nTheorem, and successive coarse grainings increase the coarse grained\nGibbs entropy. The justification for coarse graining is usually\nattributed to our observations being insensitive to the fine grained\nstructure of the original probability distribution\np(X3N, P3N | i).\nThe acceptability of this practice is one of the major problems for\nthe Gibbs approach to statistical mechanics. \n\nFor Szilard's engine and Landauer's principle, it is largely agreed\nthat numerical evaluations allow us to set\nSBi = SGi = S′Gi\nfor all macrostates, by the adjustment of a single additive constant\n(this is equivalent to saying that entropy differences\nbetween macrostates are the same for each definition of\nentropy). Where there is no uncertainty over the macroscopic state, it\ncan also be assumed\nSB = SP = SG = S′G.\nThis has allowed a large amount of discussion to take place in the\nliterature referring to ‘entropy’, without\nspecifying which entropy is meant. Although this practice can\ncause confusion, it avoids unnecessarily cumbersome language in\nsituations where the different entropies are in agreement.  However,\nwhen an argument is only valid for a particular entropy, or\nwhen these entropies disagree, it should be clearly stated which\nentropy is involved. \n\nAtomic physics allows the possibility of evolutions to states that\nwould usually be characterised as having a lower thermodynamic\nentropy.  An exorcism of Maxwell's demon, therefore, cannot be taken\nto be the claim that this entropy cannot go down, as this is\ndefinitely possible.  Smoluchowski proposed, not an outright exorcism\nof Maxwell's demon, but an argument that Maxwell's demon could not\nviolate a modified second law. He offered a modified formulation of\nthe second law which states the demon is unable to reliably,\ncontinuously produce work. Such a demon, while not exorcised, might be\nconsidered “tamed”. A tame demon could produce (in the\nterminology of Earman and Norton (1998)) “straight”\nviolations of the second law, but not “embellished”\nviolations, where the straight violation is exploited in such a way as\nto reliably, continuously produce such work. \n\nSuch a formulation leaves much to be clarified: reliability is not\nclearly defined and the requirement that the demon only fail in the\ninfinite time limit seems to allow arbitrarily large violations on any\nfinite time scale we might care about, with probabilities as close to\none as we please, provided they do not actually reach one. A common\nreworking of the modified form, but which is clearly stronger, is that\nthe demon cannot operate a cycle, in finite time, in which the\nexpectation value for work produced is positive, as repetition of such\na cycle could produce arbitrarily large amounts of work with\nprobability arbitrarily close to one. \n\nNot only are these formulations inequivalent, they leave open the\npossibility of other types of violation. Is there a demon which\ncontinuously, reliably produces work without ever completing a cycle or\nperhaps a demon which produces arbitrarily large amounts of work and\ncompletes a cycle with a probability arbitrarily close to one, but\nwhich still does not succeed on average as it faces a catastrophically\nlarge failure on the remote possibility that it fails? Whether a cycle\nneeds to complete with certainty, or just with probability arbitrarily\nclose to one, whether a completed cycle means the system must return to\nexactly its initial state or whether it need only return to an\nequivalent state, would seem to require clarification as part of such a\nmodified law. \n\nFormulations of modified second laws in terms of entropy must first\nidentify the entropy function being used. Here the problem is not so\nmuch to define a modified law. One may easily define ones that are\nalready known to be true (the fine grained Gibbs entropy cannot\ndecrease) or false (the Boltzmann entropy cannot decrease, or cannot\ndecrease on average). The challenge must be to show that a given\nmodified law is, in fact, the law one should really be concerned about.\nIts violation would show that untamed demons clearly exist, while its\nproof would ensure that all demons are tamed. \n\nIn this entry a constrained violation is one in which the\nunmodified second law is violated, but with some constraint upon the\nform of the violation so that some form of modified second law\nmay still be possible. An unconstrained violation is one in\nwhich all attempts to construct meaningful modifications of the second\nlaw are also invalidated. Although this is still ambiguous, there seems\ngeneral agreement that a cycle which could complete in finite time and\nwith probability one, and have no other effect than to convert a\nquantity of heat into work, would certainly constitute an unconstrained\nviolation. \n\nSzilard's own answer to the dilemma posed by his engine was to\npresume that a demon could not operate the device continuously and\nreliably. He then tried to deduce where it must go wrong. He imposed\nthe postulate that the second law must be obeyed and after eliminating\nall other sources of entropy production, he argued that a minimum\nentropy production occurs during measurement. The postulated second law\nis a modified one, requiring that the average entropy\nproduction in a measurement process must equal the average\nentropy reduction as a result of that measurement. Although Szilard did\nnot identify a specific definition of entropy, it is clear from the\ncontext that he was considering the entropy of specific macrostates.\nThe most significant aspect of Szilard's argument is, in effect, that\nif statistical mechanics is incompatible with the operation of untamed\ndemons, then it is required that there be an entropic cost\nassociated with the demon's acquisition of information. \n\nWhile Szilard gave a specific example of a measurement process that\nwas supposed to demonstrate the entropic cost, he did not attempt a\ngeneral argument that no physical measurement process could do better.\nInstead the argument went that if one did exist, it would lead to an\nuntamed demon. In light of later arguments, it should also be pointed\nout that it is not, in fact, measurement but the erasure of the\nmeasurement outcome that generates the entropy production in Szilard's\nexample. \n\nSzilard's argument was developed further after Shannon identified\nthat the measure p ln p had operational\nsignificance for information theory, suggestive of a deeper connection\nbetween entropy and information. To illustrate the idea further, both\nGabor (1964) and Brillouin (1951) constructed specific models of\ndissipative measurement that involve shining a light into one side of\nthe engine to see whether that side contains the molecule. The light is\nscattered if the molecule is on that side, but is not scattered if the\nmolecule is not present. However, if the system, including the\nelectromagnetic field itself, is in thermal equilibrium then there is\nbackground radiation at that temperature with a blackbody spectrum. To\nsee the scattered light it must be distinguishable from the background\nradiation. This requires a photon with energy much higher than the mean\nenergy of the blackbody spectrum,\n so\n ℏ ν ≫ k T.\nA photon must be used whose energy is greater than the energy gained\nfrom the operation of the engine, thereby precluding a net conversion\nof heat into work. \n\nBrillouin, in particular, attempted to develop this idea into a\ngeneral theory of the relationship between information and entropy.\nBrillouin now identified entropy with the Gibbs entropy of a system. He\ndistinguished two kinds of information: ‘bound’ information and ‘free’\ninformation. Bound information refers strictly to information which is\nembodied in the states of physical systems. Information which is, for\nexample, only contained in someone's mind, according to Brillouin, is\nfree, not bound. Performing a measurement could reduce the\nthermodynamic entropy of a system, but to do so requires the creation\nof an equivalent quantity of bound information in the device which\nholds the outcome of the measurement. However, it is not entirely clear\nif Brillouin's argument was that the creation of bound information is\nnecessarily associated with compensating entropy production, as the\narguments based upon scattered photons suggest, or if the bound\ninformation in itself is an additional term that must be added to\nnormal entropy to produce a generalised second law. \n\nA counter-argument to Szilard was developed originally by Popper,\nalthough it first appears in print with Feyerabend (1966) and has been\nrediscovered repeatedly. Its goal is to reject the idea that\nstatistical mechanical entropy is a subjective quantity. The primary\npurpose of the counter-argument is to show that work can be extracted\nwithout needing to have an intelligent being involved. The concept of\ninformation is argued not to carry any real burden in understanding the\nSzilard engine. The measurement can be performed, and the operation of\nthe engine effected, without there needing to be a demon to find out\nthe result of the measurement. The description of the measurement\noutcome as information is superfluous. \n\nThe Popper-Szilard engine attaches a pulley and weight to each side of\nthe partition, but positions a floor beneath the engine in such a way\nthat when the partition is in the centre of the box, both weights are\nresting on the floor and the pulley is taut. The partition has a hole\nin it, allowing the molecule access to both sides of the engine. At an\narbitrary time, the hole is blocked, without any measurement performed\non the location of the molecule. The collisions of the molecule\nagainst the partition will now exert a force on one or the other\nweight, lifting it against gravity. \n\nWhat is frequently left unclear in discussions of these devices is\nwhether a compensating entropy increase must still take place and if\nnot, what implications this has. To some, most notably Feyerabend, no\ncompensation need occur and the argument is continued to claim\nexplicitly the second law of thermodynamics is unconditionally\nviolated. To others, such as Popper, it indicates that the second law\nof thermodynamics is only applicable to large systems with many degrees\nof freedom. The engine is used to demonstrate the domain of validity of\nthermodynamics, in a similar manner to Maxwell's original demon. From\nthis point of view, the Smoluchowski trapdoor may still be viewed as a\nthermodynamic system but the Szilard engine is not. Advances in\ntechnology, such as in nano-technology and quantum computation, may\nrender this argument problematical, as the ability to construct\nreliable devices manipulating the states of individual molecules will,\nif there is not some additional physical constraint, eventually allow\nthe construction of macroscopically large numbers of such devices. If\neach device could reliably, continuously extract even microscopic\namounts of heat, then in aggregate there would again be the production\nof macroscopically significant quantities of work. \n\nLandauer's work led indirectly to criticisms of the Szilard argument\nalthough Landauer did not directly address the Szilard engine, or\nMaxwell's demon. According to his own comments in (Landauer 1986), it\nappears he believed that logically irreversible operations were a\nnecessary part of a computation and these generated heat. Although it\nwas known to be possible to simulate logically irreversible operations\nwith logically reversible ones, this came with its own cost of\nadditional bits of information needing to be stored. To avoid this\nstorage, and to complete a thermodynamic cycle, required the additional\nbits to be reset to zero, with a corresponding entropy cost. \n\nA similar argument was developed from first principles by Penrose\n(1970, Chapters V and VI, with particular reference to VI.3), who had\ndeduced that the Boltzmann entropy could go down on average, but\nonly during macroscopically indeterministic processes. This\nreduction was bounded, such that, if a macroscopically indeterministic\nprocess started in a determinate macrostate of Boltzmann entropy\nS0, and\nproduced distinct macrostates of Boltzmann entropy\nSi, with\nprobability\npi,\n then\nS0 − 〈 Si 〉 ≤ −k 〈 ln pi 〉.\nHe further argued this meant that a process which started with\ndistinct macrostates of Boltzmann entropy\nSi, each\noccurring with probability\npi,\ncould not end up in a macrostate with Boltzmann entropy\nSfwith certainty, unless\nSf − 〈 Si 〉 ≥ −k 〈 ln pi 〉\n as, otherwise, the two processes in succession could lead to a\nmacroscopically\ndeterministic process with decreasing Boltzmann entropy\n(Sf<S0). \n\nPenrose then directly applied this to the problem of the Szilard\nengine. The insertion of the partition corresponds to a\nmacroscopically indeterministic process. After the insertion the\nmolecule is in one of two possible macrostates, in either case\nreducing the Boltzmann entropy\nby k ln 2. However, this reduction cannot be\ndirectly exploited to\nextract k T ln 2 heat and leave\nthe molecule in a macrostate occupying the entire box again.  If there\nwas a process which could directly exploit this reduction, it could\ncombine with the insertion of the partition to be a macroscopically\ndeterministic process that reduces the Boltzmann entropy (via the\nextraction of heat from the heat bath). Penrose had already argued,\nfrom first principles, that a macroscopically deterministic process\ncannot reduce the Boltzmann entropy. \n\nHe then considered adding a demon, and applied the same argument to\nthe combined system of engine and demon. Now he concluded work can be\nextracted leaving the molecule in a macrostate occupying the entire\nbox, but only by leaving the demon indeterministically in one of a\nnumber of macrostates. The additional statistical entropy of the\nprobability distribution over the demon's macrostates compensates for\nthe entropy reduction in the heat bath. Eliminating the statistical\ndistribution over the demon's states is possible only with the cost of\nan entropy increase in the heat bath. \n\nPenrose concluded that the demon needs to measure the location of\nthe molecule, store that location in its memory, and can then extract\nthe work. At the end of the operation, the demon retains the memory of\nwhere the molecule was located, maintaining the indeterministic\noutcome. He considered a demon repeating this process many times,\ngradually filling its memory up with the outcomes of previous\nmeasurements. If the demon's memory is finite, eventually it will run\nout of space, leading to one of two possibilities: either the demon\nceases to be able to operate or the demon must reset some of its memory\nback to zero, which requires a corresponding increase in Boltzmann\nentropy elsewhere. \n\nPenrose's solution was effectively rediscovered independently by\nBennett (1982), after Bennett had demonstrated that logically\nreversible computation could avoid the necessity of storing large\nquantities of additional information (Bennett 1973). Bennett presented\nan explicit physical model for reversible measurement. In logical\nterms, he represented a measurement by the state of a measuring system\nbecoming correlated to the state of the measured system. The measuring\nsystem starts in a fixed state (which can be taken to be logical state\nzero) and moves through a correlated interaction, into a copy of the\nlogical state, zero or one, of the system being measured. For\n(measuring system, measured system), a combined input logical state of\n(zero, zero) is left unaffected with the output logical state (zero,\nzero), while a combined input logical state of (zero, one) becomes the\noutput logical state (one, one). As the combined output state can be\nused to uniquely identify the combined input state, the operation is\nlogically reversible. \n\nFor the physical process, Bennett made the molecule in the Szilard\nengine diamagnetic and the measuring device a one domain ferromagnet\nwith a fixed initial polarisation. With careful manipulation, he argued\nit is possible to use the perturbation of the magnetic field by the\ndiamagnet, to correlate the polarisation of the ferromagnet\nnon-dissipatively to the diamagnet's location on one side or other of\nthe engine. By the same kind of manipulation, he argued that to reset\nthe polarisation of the ferromagnet to its initial state\nwithout using the correlated location of the diamagnet, there\nis a cost of k T ln 2 in heat\ngeneration, in accordance with Landauer's principle. Extracting work\nfrom the Szilard engine allows the diamagnetic molecule to move freely\nthroughout the engine, so losing the correlation. The heat generating\nreset operation is then the only way to restore the ferromagnet to its\ninitial polarisation. \n\nWith the ferromagnet representing the demon's memory, Bennett had\ngiven an explicit counter-example to the arguments of Szilard and\nBrillouin, that measurement is necessarily dissipative. At the same\ntime he argued that the process of resetting the demon's memory is a\nnecessarily logically irreversible step, which incurs the heat\ngeneration cost identified by Landauer, and this cost saves a modified\nform of the second law. Although Bennett gave a particular model for\nresetting the demons memory, no direct proof was given that there can\nbe no better way. Unlike Penrose's argument, Landauer's principle seems\nto be assumed, rather than derived. \n\nSupporters of this resolution argue that the demonless engines of\nPopper et. al. are also examples of this kind of process. In each case\nof a demonless engine, there is another mechanism that is left in one\nof two distinct possible states after the work is extracted, depending\nupon which side of the box the molecule was located. In the case of the\nPopper-Szilard engine, the partition is on the left or right hand side\nand a different weight has been lifted. This state still encodes the\ninformation about which outcome occurs. To truly have completed a\ncycle, reliably, it is necessary to restore the partition to the centre\nof the box, regardless of the side to which it moved, and to have a\ndefinite, single weight raised. If Landauer's principle is correct,\nthis is a resetting operation and necessarily incurs a cost that, on\naverage, offsets the work extracted from the operation of the\nengine. \n\nThe Landauer-Penrose-Bennett resolution depends for its consistency\nin continuing to take into account a probability distribution over the\npossible outcomes of the measurement, after the measurement has\noccurred. There are many reasons why one might feel that this raises\nproblems. For example, a Boltzmannian approach to statistical mechanics\ntakes the entropy of a system to depend only upon a confined region of\nthe microstate space, usually either that of microstates compatible\nwith a given macrostate, or accessible to the microstate over time.\nProbability distributions over inaccessible or macroscopically distinct\nregions should not, from this point of view, have thermodynamic\nsignificance. From an opposite point of view, a subjective Bayesian\nmight argue probability distributions are only expressions of an\nagent's uncertainty. If the intelligent demon is supposed to be the\nagent, it can hardly be uncertain of its own state once it has\nperformed the measurement. \n\nZurek (1989a; 1989b) proposed a development of the\nLandauer-Penrose-Bennett position, building on a suggestion of\nBennett's, that would attempt to address some of these concerns and\nalso whether a sufficiently cleverly programmed computer might be able\nto do better than Landauer's principle would suggest. If a demon\nperformed a large number of cycles without erasing its memory, as\nPenrose suggested, then its memory would contain a randomly generated\nbit string. Zurek's suggestion was that the Boltzmannian entropy of the\nphysical states representing the bit string needed to have added to it\nthe algorithmic complexity of the bit string itself and it is this\ntotal entropy that is the subject of a modified second law. \n\nThe algorithmic complexity of a bit string is a measure of how much\na given bit string can be compressed by a computer. A long, but\nalgorithmically simple, bit string can be compressed into a much\nshorter bit string. This shorter bit string could apparently be reset\nto zero, by resetting each individual bit, for a much lower cost than\nresetting each individual bit in the original longer bit string. A\nclever demon might, therefore, be able to compress its memory and so\navoid the full erasure cost that would be needed to ensure a modified\nsecond law holds. \n\nHowever, long randomly generated bit strings are, with high\nprobability, incompressible, so unless the demon was very lucky, this\ncould not work. Furthermore, the average algorithmic complexity of a\nstatistical mixture of bit strings is not less than the Shannon\ninformation of the statistical distribution. It can be shown,\ntherefore, that even if the best compression algorithms are available\nthe clever demon cannot do better, on average, than simply resetting\neach bit individually in its memory. \n\nZurek's proposed total entropy, given by the sum of the Boltzmannian\nentropy and the algorithmic complexity, is a property of the individual\nmicrostates, rather than of the probability distribution over the\noutcomes. It can decrease during macroscopically indeterministic\nprocesses, if a particularly algorithmically simple sequence of\noutcomes happens to occur. However, unlike the Boltzmann entropy, even\nduring macroscopically indeterministic processes it does not decrease\non average. \n\nEarman and Norton (1999) presented a dilemma for all attempts to\nexorcise the Szilard engine by information theoretic arguments. Would\nbe exorcists who want to use information theory to save the second law\nmust, they urge, choose between “sound” and\n“profound” horns of a dilemma. \n\nThe sound horn proposes a resolution in which the demon, as well as\nthe system, are assumed to be ‘canonical thermal systems' and so\nsubject to, a possibly modified form of, the second law. By imposing\nsome form of the second law, consideration of what information\nprocessing a demon may need to undertake would lead to a conclusion as\nto what the thermodynamic cost of that information processing must be.\nThis approach lead Szilard to conclude that information acquisition had\nan intrinsic thermodynamic cost. It is possible to read some papers on\nthe use of Landauer's principle in the Szilard engine in a similar way:\nthe justification for believing that the resetting operation must carry\na thermodynamic cost is for, otherwise, even a modified second law\nwould be false. \n\nThis kind of eliminative argument has significant weaknesses. All\npossible sources of entropy increase are eliminated and it is concluded\nwhatever is left must be responsible for any remaining entropy\nincrease. It is hard to see how it can be considered an explanation of\nwhy demons do not exist, as by imposing the second law in the\nfirst place, one has simply ruled out their possible existence. At most\nit can provide a demonstration of the consistency of assuming some\nparticular operation has a thermodynamic cost attached to it. It is\nalso hard to see what confidence one can have in the conclusions of\nsuch an argument. Szilard, Gabor and Brillouin thought that such\narguments led to the conclusion that information acquisition had an\nintrinsic cost. By contrast, Landauer, Penrose and Bennett concluded\nthat information acquisition does not have an intrinsic cost, but\ninformation erasure does. What are the grounds for supposing Landauer's\nprinciple is any more secure than Szilard's argument? \n\nA sound resolution proceeds by assuming a second law to be true, and\ndeducing the consequences for information processing. The other horn of\nthe dilemma, a profound resolution, introduces Landauer's principle (or\nsome other information theoretic principle) as an independent axiom,\nthat cannot be derived within statistical mechanics. By adding this\nadditional axiom, a modified second law may be derived, and the absence\nof untamed demons deduced. This in itself raises questions. It would\nsuggest that statistical mechanics is incomplete without being\nsupplemented with this new principle of information processing. If\nLandauer's principle is genuinely the reason the Szilard engine fails,\na profound resolution implies that without it, statistical mechanics\nwould allow unconstrained violations of the second law. Again,\nif we have no other independent grounds for believing in Landauer's\nprinciple, then how can we be confident that a clever device cannot be\nconstructed to produce such an unconstrained violation? \n\nThere is, of course, another response to the Szilard engine, and\nMaxwell's demon in general. It is to argue that these demons do, in\nfact, exist. This has been suggested in recent years by Albert (2000)\nand developed by Hemmo and Shenker (2007). The principal argument\ndepends on the observation that, in macroscopically indeterministic\nprocesses, the Boltzmann entropy can go down. Adopting a Boltzmannian\npoint of view, Albert argued that the Boltzmannian entropy of the\nmacrostate that the system inhabits is the only meaningful measure of\nthermodynamic entropy. The fact that in indeterministic processes it\ncan be made to go down systematically is therefore just a fact of the\nworld. In the Szilard engine, inserting the partition in the centre of\nthe box is just such an indeterministic process that reduces the\nBoltzmann entropy regardless of which side the molecule is located. As\nthe demonless engines show, extracting the work from this reduction is\npossible, without the intervention of intelligence. \n\nHemmo and Shenker considered whether this work extraction requires a\nrecord to remain of the initial location of the molecule, as supporters\nof the Landauer-Penrose-Bennett resolution would suggest. They argue\nthat this is not necessary. Instead they perform an operation\nequivalent to ‘erasure-by-destruction’, on the auxiliary system. This\ndestroys any remaining macroscopic trace of the information about where\nthe molecule was located, without incurring any thermodynamic cost (see\nalso Maroney (2005), who considered similar processes from within a\nGibbsian framework), but leaves the auxiliary in one of several low\nBoltzmann entropy macrostates. \n\nHowever, it seems also clearly acknowledged in these arguments that\nthe reduction can only be achieved through macroscopically\nindeterministic processes. The system and auxiliary cannot\nboth be restored to their initial macrostate with certainty,\nwithout heat being generated. It would appear that the violation is\nstill constrained and the demon may be tame. Albert, at least, appeared\nto acknowledge this, and the corresponding possibility that a modified\nsecond law may still be possible. \n\nFinally, it may be noted that it is possible to conceive of untamed\nMaxwell's demons, that produce unconstrained violations of the second\nlaw, by simply modifying the microscopic laws in order to make it so.\nZhang and Zhang (1992) provided an example of this replacing the\npartition in the centre of Maxwell's box with a velocity dependant\npotential capable of creating a pressure difference in the gas. This\npotential is non-Hamiltonian, so Liouville's Theorem does not hold. The\nphase volume of the macrostate of the gas can then be compressed and\nboth the Boltzmann and Gibbs entropies can be systematically reduced\neven in macroscopically deterministic processes. Such examples, while\nnot necessarily being presented as candidates for demons that can be\nconstructed in the real world, are nevertheless useful in clarifying\nwhat attendant assumptions are being made when exorcisms are\nproposed. \n\nIn his earliest paper, Landauer derived the cost of performing the\nreset operation by simply assuming that there is an equivalence\nbetween the Shannon information of a distribution of logical states,\nand thermodynamic entropy of a physical system that can represent\nthose logical states. A reduction in the Shannon information content\nof the logical states would then reduce the thermodynamic entropy of\nthe physical system. Landauer then further assumed that the second law\nholds true and that this reduction must produce a thermodynamic\nentropy increase elsewhere. Viewed in this way, his argument appears\nto contain elements both of the profound and the sound horns of Earman\nand Norton's (1998, 1999) dilemma. It follows the profound horn in\nidentifying Shannon information with thermodynamic entropy and the\nsound horn in assuming the validity of the second law. \n\nIt is less clear that Landauer himself thought he was introducing a\nnew principle. It seems more plausible that he was taking for granted\nthe Gibbsian approach to statistical mechanics, which identified\np ln p as the statistical\nmechanical entropy long before Shannon's work, and the non-decrease of\nthis entropy through coarse-graining. As such, the validity of\nLandauer's principle would still require the structure of Gibbsian\nstatistical mechanics to be self-consistent and the appropriate\nrepresentation of thermal systems. At the very least this cannot be\ntaken for granted unless it has already been established that untamed\ndemons do not exist and so the unquestioned use of Landauer's\nprinciple in exorcisms of the demon would still appear to be\ncircular. \n\nNevertheless, this cannot be taken to imply that analysing the\nthermodynamics of computation is an altogether pointless task. The\nquestion of whether or not one can deduce thermodynamic consequences of\nphysically implementing logical operations, from the structure of the\nlogical operation itself, may be a well-founded question (although it\nclearly cannot be wholly divorced from what fundamental physics says\nabout the construction of such devices). Furthermore, were it to be\nshown that Landauer's principle was not correct, and that\ninformation could be reset at arbitrarily low cost, it would seem that\neither untamed demons must be possible or some further source\nof heat generation must be discovered in the operation of Szilard's\nengine. \n\nAt one extreme, von Neumann argued that any logical operation\nnecessarily generated a minimum quantity of heat. Gabor and Brillouin\nhad argued that measurement, at least, generated heat. Landauer and\nBennett argued that only logically irreversible operations need\ngenerate heat, but that measurement is logically reversible. This was\nbacked up by the presentation of physical processes that could, in\nprinciple, perform a measurement with arbitrarily little heat\ngeneration. Bennett argued that while the measurement processes\ndiscussed by Gabor and Brillouin, using scattered light, generate\nheat, they were incorrect to generalise this to the claim that all\nmeasurement process must necessarily do the same. However, a similar\ncharge may be levelled at Landauer's principle. Although explicit\nmodels for resetting operations have been constructed that get\narbitrarily close to a minimum heat generation\nof k T ln 2, what is needed is\na proof that no physical process can do better. A literature now\nexists examining the strength of this claim, and in particular what\nare the physical and statistical assumptions underpinning their\narguments. \n\nSchematically, the simplest of arguments in favour of Landauer's\nprinciple is based directly on volume of phase space arguments and\nLiouville's Theorem. \n\nThe microscopic state space of the system and environment\nis assumed to be divided into logical, or information bearing, degrees\nof freedom and environmental, or non-information bearing degrees of\nfreedom. These can be simplified to just two dimensions. In the figure,\nthe horizontal axis represents the information bearing degrees of\nfreedom. The system being in logical state zero or logical state one\ncorresponds to the microscopic state lying within a particular region\nof the information bearing degree of freedom. The reset to zero\noperation is required to be a Hamiltonian evolution of the state space\nof the system and environment that leaves the system in logical state\nzero, regardless of the initial logical state. \n\nAs the region of the state space available to the logical degrees of\nfreedom is reduced by a factor of two, Liouville's Theorem requires\nthat the region of state space available to the environmental degrees\nof freedom must have doubled. It is then argued that this must involve\nheat generation in the environment. This last step requires a little\njustification. In the Boltzmannian approach, the temperature of a\nmacrostate is generally defined by the formula \n If this system is very large, it is assumed\nthat its temperature undergoes negligible changes from the absorption\nof small quantities of heat. It follows that if the physical volume is\nkept fixed, so that the only change in energy is through heating, then\nthe heat and entropy are related by\n ΔQ = T ΔSB.\n Systems that satisfy these conditions may be considered to be good\nheat baths. A doubling in phase volume of the heat bath implies an\nincrease in Boltzmann entropy of k ln 2, so\nrequires heat absorption\nof kT ln 2. \n\nAlthough this simple argument seems compelling, doubts may easily be\nraised. The representation of the logical states and the environment is\nso simplified that it should at least raise concerns as to what may\nhave been lost in this simplification. Shenker (2000) and Hemmo and\nShenker (2007) raise a number of such issues, as part of their\narguments over the existence of a demon. There seems no reason to\nrequire the environment to absorb heat, or even be in a macrostate\nwhich has twice the phase volume. Instead, it could be in one of two\ndistinct regions, corresponding to macroscopically distinct, but\notherwise quite unimportant, degrees of freedom (such as the location\nof a shoe dropped on the floor). All that is required is that the\nenvironmental degrees of freedom must end in one of a number of\ndistinct macrostates, whose total phase volume over all\npossible macrostates must be twice that of the its original macrostate.\nThe use of an ‘erasure by destruction’ operation can then ensure that\nthe specific macrostates have no trace of the original information. \n\nHowever, as noted above in Section 3.6, Hemmo and Shenker\nacknowledge that some tighter formulations of Landauer's principle\nexist that prevent this kind of move (although they question the\nreasonableness of these formulations): if one requires that the\nresetting operation leaves unchanged the macrostate of any\nenvironmental degree of freedom that is not a heat bath, then the\nLiouvillean argument seems to hold. \n\nThe earliest attempts to construct general, rigorous proofs of\nLandauer's principle, from within statistical mechanics, were by\nShizume (1995) and Piechocinska (2000). (Although Penrose (1970,\nChapter VI) does, in effect, derive an entropic cost for the reset\noperation, it appears his work was motivated quite independently of\nLandauer's). Shizume's proof relies upon treating thermal contact as a\nGaussian white noise field in a Langevin equation, while Piechocinska\nassumes Hamiltonian dynamics and a heat bath that is initially\nuncorrelated with the system and represented by a Gibbs canonical\ndistribution. \n\nThese proofs, and those that follow on from them, are in the\ntradition of making standard assumptions that are generally accepted\nwithin the context of practical calculations in statistical mechanics.\nIt is taken for granted that a probability distribution over at least\nsome region of state space is a meaningful characterisation of the\nstate of a system, and further constraints are usually assumed about\nwhat kind of probability distribution this could be. From the point of\nview of investigating questions about the fundamental consistency of\nstatistical mechanics, this may appear unsatisfactory. If the aim is\nmore modest, of simply investigating the generality of the\ncircumstances under which Landauer's principle might hold, such proofs\nmay still be seen to be of value. It is important to distinguish\nbetween, for example, the rather hard problem of providing an\nexplanation of why thermalised systems should be represented\nby a canonical distribution over the accessible states space, from the\nsomewhat more easy task of deriving the consequences of the empirically\njustified observation that thermalised systems are well\nrepresented by a canonical distribution over the accessible state\nspace. \n\nPiechocinska provided proofs for both the classical and quantum\ncases. A brief sketch of Piechocinska's quantum proof is possible.\nFirst simply define a function\n\nΓ(i, f, m, n)= ln pi\n−  ln pf − β(En − Em),\n where\npi is\nthe occupation probability of an initial state\ni of the system,\npf the\noccupation probability of a final state f,\nbeta the dispersion parameter of the canonical distribution of the heat\nbath, and\nEm and\nEn the\nenergies of eigenstates of the heat bath. No particular physical\nsignificance is attached to this function. However, from the\nrequirement that the overall evolution be unitary, it can be shown\n〈 e−Γ 〉=1\n\nand so by  convexity\n〈 Γ 〉 ≥ 0. The mean heat generated in the heat bath can\nthen easily be shown to\n satisfy\n〈 Q 〉 ≥ −ΔH k T ln 2 \nwhere\n ΔH =\n ∑ ipi log pi\n −\n ∑ fpf log pf\n is the change in Shannon information over the states of the system\nduring the operation. If there are assumed to be two equiprobable\ninput states, the requirement that the operation be a resetting\noperation and leave the system determinately in a specific output\nstate leads to ΔH = −1 and the usual expression\nof Landauer's principle is obtained. \n\nThis considers only pure quantum states to represent logical states,\nalthough in the classical case her proof allows logical states to be\nrepresented by macrostates. Maroney (2009) generalised the method\nfurther, to input and output states represented by macrostates with\nvariable mean internal energies, entropies and temperatures and\nconsidered the consequences for more general logical operations than\nthe reset operation. Turgut (2009), building on Penrose's work,\nderived similar—though apparently more powerful—results\nusing the Gibbs microcanonical distribution approach. \n\nGroisman, Ladyman, Short and Presnell (2007) presented a\nphenomenological argument in favour of the proposition that all\nlogically irreversible operations are thermodynamically irreversible.\nIn this they were responding in part to Norton's (2005) criticism of\nthe generality of previous proofs of Landauer's principle (including\nPiechocinska's) and in part to Maroney's (2005) argument that the\nequality in Piechocinska's proof is reachable, and when reached the\nheat is generated in a thermodynamically reversible fashion. \n\nTheir intention seems to have been to have a general proof that can\nbe shown to hold independently of any particular model of physical\nprocess of resetting. To achieve this without presuming something about\nthe physics which governs a device that performs the resetting, they\nexplicitly accepted the sound approach to Earman and Norton's Sound vs.\nProfound dilemma. This means they assumed that a particular modified\nform of the second law of thermodynamics must hold true. The modified\nversion that they adopted is that there are no cyclic processes, whose\nsole result is to restore a system to its initial state and having a\nstrictly positive expectation value for the conversion of heat into\nwork. \n\nTheir method was, in essence, equivalent to that of Szilard and\nBennett. They considered the operation of the Szilard engine, using a\ndevice to measure and store the location of the molecule, and a\ncorrelated operation to extract heat from it. Then they reset the\ndevice. Unlike Bennett, they did not assume that resetting has a cost,\nand argue that this cost saves the second law. Rather, like Szilard,\nthey imposed a modified form of the second law, and from it deduced\nthat resetting has a cost. The generality of their claim is based upon\nthe fact that they made no attempt to characterise the device\nphysically, although they did assume it was capable of performing\nnon-dissipative measurements. \n\nEarly criticisms of Landauer's principle tended to focus on the\nclaim that logically reversible operations could be implemented in a\nthermodynamically reversible fashion, and instead to defend the\nposition of von Neumann, Gabor and Brillouin (see the references in\nLandauer's response (Porod 1988) and in (Bennett 2003) for examples of\nthis debate). The increasingly explicit models developed by workers\nsuch as Bennett, Fredkin and Toffoli have now generally been considered\nto have established that thermodynamically reversible computation is\nindeed possible, if implemented by logically reversible computers. \n\nRecent criticisms (Shenker 2000 [in Other Internet Resources], Norton\n2005) focussed upon whether heat generation must necessarily be\nassociated with logically irreversible operations. A particular\nobjection raised by Shenker and a similar objection by Norton,\nconcerned the use of probability distributions over macroscopically\ndistinct states. Both Shenker and Norton argued that the thermodynamic\nentropy of a system is only properly defined by considering the region\nof state space accessible to the given microstate. For physical\nrepresentations of logical states it is essential that the physical\nsystem cannot jump from one logical state to another. Consequently,\nregions of state space corresponding to different logical states are\nnot accessible to one another, or as Shenker referred to it, they are\nnot interaccessible. In Landauer's original paper, he simply\nidentified the p ln p term of the\nprobability distribution over the distinct logical states with\nthermodynamic entropy. More developed Gibbsian approaches still\ncalculate the Gibbs entropy of a statistical distribution over regions\nof state space which are not interaccessible.  Norton argued in detail\nthat this is an illegitimate calculation, and that the resulting\nentropy has nothing to do with thermodynamic entropy. \n\nThe objection extends far beyond the scope of Landauer's principle.\nIt shares much with general Boltzmannian arguments against the Gibbs\napproach to statistical mechanics. However, it is questionable whether\nproofs following the approach of Piechocinska are genuinely vulnerable\nto the accusation. Piechocinska did not, in fact, attribute a\nthermodynamic entropy to the Gibbs entropy measure (entropy was barely\neven mentioned in her paper) nor, contrary to some criticisms, did she\nassume a specific model of how the reset operation is performed, at\nleast in the quantum case. The assumption that generates the\nlogarithmic form of Landauer's principle is that heat baths are\ninitially canonically distributed (an assumption which Norton, at\nleast, appeared willing to concede) combined with the requirement that\na single Hamiltonian (or unitary, in the case of quantum theory)\ndynamics is used to describe the combined evolution of the system and\nenvironment, independantly of the input logical state. Similar comments\nmay be made about (Maroney 2009), while (Turgut 2009) managed to go\nfurther and deduce a stronger constraint, which implies the usual\nversion of Landauer's principle, but without even needing to consider\nprobability distributions over the input logical states. \n\nA more concrete counterexample is that of Allahverdyan and\nNieuwenhuizen (2001), where they argued it is possible to effect an\nerasure process with a lower cost than Landauer's principle suggests,\nbut only in the low temperature quantum regime. A typical assumption in\nstatistical mechanical proofs is that the interaction energy between a\nsystem and a heat bath can be treated as negligible (at least before\nthe start and after the end of the process). Allahverdyan and\nNieuwenhuizen's example exploits this by considering situations where\nthe temperature is sufficiently low that the interaction energy can no\nlonger be treated as negligible when compared to\nkT. In this situation the heat bath can no longer be\ntreated as a canonical distribution, independently of the system and\nthe standard proofs do not hold. It should be noted that while Zhang\nand Zhang's non-Hamiltonian demon is not considered actually\nphysically possible, Allahverdyan and Nieuwenhuizen explicitly claimed\nthat in the low temperature regime, Landauer's principle can, in fact,\nbe broken. \n\nThe relationship between quantum theory, measurement and\nirreversibility is a complex one, and considering the effect of\nquantum theory on the argument has lead to sometimes surprising claims\nbeing made. \n\nVon Neumann explicitly referred to Szilard's argument in (von\nNeumann 1932, Chapter V.2), when discussing the irreversibility of\nwavefunction collapse on measurement, although it is unclear precisely\nwhat role this was intended to play. Both Gabor and Brillouin's\nmeasurement procedures, using light, required the quantised treatment\nof the electromagnetic field to produce dissipation. Gabor stated quite\nclearly his belief that measurement by classical electromagnetic fields\ncould take place non-dissipatively and would lead to unconstrained\nviolations of the second law. The Landauer-Penrose-Bennett argument\ndoes not require measurements to generate heat, so for them, classical\nstatistical mechanics need not lead to untamed demons. Although it\nmight still be argued that quantum electrodynamics dissipates heat if\nused for measurement, the fact that some physical processes for\nmeasurement dissipate heat does not undermine Bennett's claim that\nother physical processes can perform measurements\nnon-dissipatively. \n\nStill, wavefunction collapse is frequently associated with\nthermodynamic irreversibility. It might seem there is a contradiction\nhere, as measurement, in the Szilard engine, is supposed to lead to the\npossibility of a decrease in entropy, while measurement, in quantum\ntheory, is supposed to increase entropy. Alternatively, it may be\nwondered if the entropy increase in wavefunction collapse offsets the\nreduction from measurement. These thoughts are largely a confusion\nwhich can be easily remedied. A projective, von Neumann, measurement\nwill increase the Gibbs-von Neumann entropy of a density matrix unless\nthe observable being measured commutes with the density matrix, in\nwhich case the measurement will leave it constant. This, however,\napplies to the density matrix that describes the statistical\ndistribution over all the measurement outcomes. The subensemble density\nmatrix that corresponds to a particular outcome having occurred can\nstill have a lower Gibbs-von Neumann entropy than the ensemble over all\nmeasurement outcomes. \n\nZurek (1986) is the most prominent example of attempts to relate\nquantum measurement to the Szilard engine. Zurek suggested that a\nmolecule being in a superposition of being on either side of the\npartition, in the Szilard engine, is not subjective uncertainty as to\nwhich side the molecule is located. Instead it is an objective fact\nthat the quantum state occupies both sides. Even with the partition\npresent, the molecule occupies the entire box. Until a measurement is\nperformed, collapsing the quantum state to one side or the other, work\ncannot be extracted. Zurek still regarded the resetting operation as\nthe source of compensating entropy increase. This is justified as\nnecessary on the basis that the demon, having performed the measurement\nand extracted the work, is now in a statistical mixture of having\nobserved each of the possible outcomes. For the cost of resetting the\ndemon, Zurek appealed directly to Landauer's principle. \n\nThe logic here is hard to follow. If there was something ambiguous\nor troubling, in the classical situation, about having to refer to\nsubjective uncertainty over the location of the molecule while there is\nan objective fact on the matter, then it should be at least as\ntroubling to have to refer to the statistical mixture of the demon's\nstates when deciding to perform the resetting operation. Zurek did not\nseem to suggest that the demon is to be regarded as being in a\nsuperposition of measurement outcomes (if it were, then it could\npresumably be reset to a standard state with a lower cost). It seems\nassumed that there was a matter of fact about which outcome occurred\nand therefore which state the demon is in. Also ambiguous is whether we\nare to understand the measurement as a non-unitary wavefunction\ncollapse to one outcome or the other. If wavefunction collapse is to be\nregarded as a necessary component in understanding the absence of\ndemons, where does this leave no-collapse interpretations? \n\nIt is also important to bear in mind that all attempts to derive\nLandauer's principle, to date, have been based upon classical\ninformation processing. While it would appear that a lower bound, very\nsimilar in form to Landauer's principle, can be derived for quantum\ncomputational operations, unlike the classical case there appears no\nproof as yet of the existence of processes that can in principle reach\nthis bound. It remains possible, therefore, that quantum computations\nmay need to incur additional thermodynamic costs. This appears to be\ntrue even for the quantum analog of logically reversible operations:\nBennett's (1973) procedure for avoiding the cost of storing additional\nbits involves an operation which cannot in general be applied to\nquantum operations (Maroney 2004 [in Other Internet Resources]). Finally, as noted above,\nAllahverdyan and Nieuwenhuizen argued in the opposite direction, that\nthe derivations of this lower bound involve assumptions which can be\nviolated by quantum theory in the low temperature regime. \n\nThroughout the literature, there are widely different motivations\nand standards of proof accepted, so that at times it can be hard to see\nany coherent body of work. Maxwell's original intention was to\ndemonstrate that the second law had only limited validity, and that\nviolations were possible. Although he originally described the demon as a\nliving being, he later reduced its role to that of being a valve.\nSmoluchowski sought to tame the demon, by formulating a modified second\nlaw that it did not violate. His exorcism applied only to mechanical\ndevices and he left open the question of whether intelligent\nintervention could reverse the second law. As the literature developed,\nboth the scope of the exorcism and the nature of the demon changed. The\nconcept of intelligence became reduced to that of information\nprocessing performed by a mechanical device. Rather than investigate\nhow a tame demon might be constrained by physical laws the aim became\nto exclude demons altogether. Much of the resulting literature proceeds\non a case-by-case basis, where individual examples of demons are argued\nto fail and it is simply extrapolated that all demons will fail for\nsimilar reasons. \n\nSzilard's original intention was to analyse whether intelligence\ncould be used to defeat the second law. His argument did not analyse\nwhat was meant by intelligence. He argued that any being, however\nintelligent, would still have to perform measurements. If these\nmeasurements came with a sufficient entropic cost, the second law would\nbe safe without needing to consider further the constitution of an\nintelligent being. This clearly does not require intelligence to be\nreduced to performing measurements and processing the results of such\nmeasurements. \n\nUnder the analysis of von Neumann, Brillouin and Gabor, a more\ngeneral conception developed of information processing having a\nfundamental entropic cost. The possession of information could indeed\nbe regarded as reducing entropy and therefore capable of producing\nthermodynamic work. However, as the acquisition of information, through\nmeasurement, was still believed to require a dissipation of heat, there\nwas no threat to a generalised form of the second law. No explicit\nconsideration needed to be made regarding exactly what kind of being\nthe possessor of information had to be. \n\nLandauer's analysis led to the argument that measurement did not\nreduce the degrees of freedom needed to represent the combined system\nof object and measuring device, so did not lead to the necessity of\nheat generation. The models of measurement proposed by Szilard, Gabor\nand Brillouin were insufficiently general. Landauer's early papers did\nnot explicitly consider the Szilard engine and the problem of Maxwell's\ndemon, so they did not address what implications this had for the\nconsistency of statistical mechanics. It appears that Landauer believed\nthat logically reversible computation could not be achieved without\naccumulating a large amount of irrelevant information. That\naccumulation would ultimately require erasure, so that the cost of a\nlogically irreversible computation could not be avoided. It took\nBennett to demonstrate that this cost could be avoided in logically\nreversible computation. \n\nNevertheless Bennett also argued that the Szilard engine was a process\nwhere the logically irreversible erasure step could not be avoided to\ncomplete the cycle. The information acquired during the\nnon-dissipative measurement had to be stored in the state of the\ndemon's memory. While this information allowed the demon to extract\nwork from the engine, this memory could not be reset without incurring\nan erasure cost at least equal to the work extracted. Unlike the\nanalysis from Szilard to Brillouin, the consistency of statistical\nmechanics now requires us to say at least something about the demon\nitself as a physical being, constrained by a particular kind of\nphysical law. The demon may be able to perform measurements\nnon-dissipatively, but its memory must be representable by a\nphysical device, and this device must be included in the statistical\nmechanical description. This contrasts sharply with Brillouin.\nBrillouin characterised information as “bound” if it was\nembodied in the states of a physical device, but he explicitly stated\nthat information contained only in the mind was “free”,\nnot “bound”. \n\nThe relationship between entropy and information now becomes less\nclear. With the demon left out of the system, it is free to be\nconsidered an agent, who has information about the system. The\nuncertainty in the description of the system may be thought of as the\ndemon's lack of knowledge about the exact state of the system. If the\ndemon has more information, the entropy of the system is\nless. However, once the demon can acquire information\nnon-dissipatively, the entropy of the system goes down and the only\ncompensation seems to be an increase in the uncertainty of the demon's\nstate itself. \n\nIf the state of the demon's own mind must now be included\nin the system, with uncertainty over which state the demon is\nin, that raises the question: “Whose\nuncertainty?”. Perhaps it is the uncertainty the demon has at\nthe start of the process about which state it will be in? Perhaps the\nstatistical distribution should be abandoned, as the demon is not\nuncertain about its own state, but that another property—such as\nthe proposal of Zurek—has changed by a compensating amount?\nPerhaps the demon has been naturalised to the point that it no longer\nrepresents an intelligent agent, possessing and acting on information,\nand the uncertainty is now another, external agent's uncertainty? In\nthe latter case it would appear any hope of using the Szilard engine\nto answer Smoluchowski's original concern, about intelligent\nintervention, has been lost entirely. \n\nFor Penrose and Bennett's resolution to succeed, it is necessary that\nthe demon be conceived of as a particular type of physical system, as\nthe physical constitution of the demon's memory must now be taken into\naccount. If the demon is presumed to be representable by a Hamiltonian\nsystem, then it has been reduced to a mechanical device, not too\ndissimilar to Smoluchowski's trapdoor and spring. It is not so\nsurprising that being a Hamiltonian system, and subject to Liouville's\nTheorem, might naturally lay some physical constraint upon a demon,\nrendering it tamable. However, the existence and exact nature of this\nconstraint, and the possibility of a corresponding modified second\nlaw, remains open to debate and depends in a large measure on the\napproach taken to understanding statistical mechanics. \n\nIn the dissipationless measurements of Landauer et al., measurement\nstill requires the development of correlations between physical\nsystems, through interaction. The demonless engines of Popper et al.\nindicate that the only physical aspect of this process that is\nrelevant is precisely this correlated interaction, and that the\nquestion of whether the interaction is for the purpose of information\ngathering is not, in itself, of significance. While advocates of the\ninformation theoretic approach might argue that the\ncorrelation is information, the arguments of Maroney and of\nShenker and Hemmo would appear to challenge whether maintaining this\ncorrelation is even necessary to understand the operation of the\nengine. It may be asked whether the characterisation of the\ncorrelation as “information” is actually playing any role,\nor whether this is just a trivial relabelling exercise? If the term\ninformation adds nothing to the understanding of statistical mechanics\nthen it could be purged from the description (of course, this still\nleaves open the possibility that statistical mechanics has non-trivial\nimplications for the physics of information processing). \n\nBoth the Szilard engine and Landauer's principle seem to raise a\nsimilar problem about the relationship between knowledge and\nthermodynamic entropy: if one could know which side of the\nengine the molecule was located, one could extract work; if one could\nknow which logical state the device was in, one could set it\nto zero without work. Without this knowledge, it is necessary to design\na process that acts independently of the specific state the system is\nin. But it is plain that this does not tell us that, even\nwithout the knowledge, it is impossible to design a clever\nprocess that can still extract the work from engine without\ncompensation, or a clever process that can still reset the bit without\nwork. Hamiltonian mechanics and Liouville's Theorem seems to play a\nvital, if largely unnoticed, role. As Zhang and Zhang's demon\ndemonstrates, unconstrained violations of the second law are clearly\npossible given non-Hamiltonian flows and no appeal to information\ntheory or computation would seem able to avoid this.","contact.mail":"owen.maroney@philosophy.ox.ac.uk","contact.domain":"philosophy.ox.ac.uk"}]
