[{"date.published":"2002-10-21","date.changed":"2019-08-28","url":"https://plato.stanford.edu/entries/probability-interpret/","author1":"Alan Hájek","author1.info":"http://philosophy.anu.edu.au/profile/alan-hajek/","entry":"probability-interpret","body.text":"\n\n\nProbability is the most important concept in modern science,\nespecially as nobody has the slightest notion what it means.\n—Bertrand Russell, 1929 Lecture\n(cited in Bell 1945, 587)\n\n\n\n‘The Democrats will probably win the next election.’\n\n\n‘The coin is just as likely to land heads as tails.’\n\n\n‘There’s a 30% chance of rain tomorrow.’\n\n\n‘The probability that a radium atom decays in one year is\nroughly 0.0004.’\n\n\nOne regularly reads and hears probabilistic claims like these. But\nwhat do they mean? This may be understood as a metaphysical question\nabout what kinds of things are probabilities, or more generally as a\nquestion about what makes probability statements true or false. At a\nfirst pass, various interpretations of probability answer\nthis question, one way or another.\n\n\nHowever, there is also a stricter usage: an\n‘interpretation’ of a formal theory provides\nmeanings for its primitive symbols or terms, with an eye to turning\nits axioms and theorems into true statements about some subject. In\nthe case of probability, Kolmogorov’s axiomatization (which we\nwill see shortly) is the usual formal theory, and the so-called\n‘interpretations of probability’ usually interpret\nit. That axiomatization introduces a function\n‘\\(P\\)’ that has certain formal properties. We may\nthen ask ‘What is \\(P\\)?’. Several of the views that\nwe will discuss also answer this question, one way or another.\n\n\nOur topic is complicated by the fact that there are various\nalternative formalizations of probability. Moreover, as we will see,\nsome of the leading ‘interpretations of probability’ do\nnot obey all of Kolmogorov’s axioms, yet they have not\nlost their title for that. And various other quantities that have\nnothing to do with probability do satisfy Kolmogorov’s\naxioms, and thus are ‘interpretations’ of it in the strict\nsense: normalized mass, length, area, volume, and other quantities\nthat fall under the scope of measure theory, the abstract mathematical\ntheory that generalizes such quantities. Nobody seriously considers\nthese to be ‘interpretations of probability’, however,\nbecause they do not play the right role in our conceptual\napparatus.\n\n\nPerhaps we would do better, then, to think of the interpretations as\nanalyses of various concepts of probability. Or perhaps\nbetter still, we might regard them as explications of such\nconcepts, refining them to be fruitful for philosophical and\nscientific theorizing (à la Carnap 1950).\n\n\nHowever we think of it, the project of finding such interpretations is\nan important one. Probability is virtually ubiquitous. It plays a role\nin almost all the sciences. It underpins much of the social sciences\n— witness the prevalent use of statistical testing, confidence\nintervals, regression methods, and so on. It finds its way, moreover,\ninto much of philosophy. In epistemology, the philosophy of mind, and\ncognitive science, we see states of opinion being modeled by\nsubjective probability functions, and learning being modeled by the\nupdating of such functions. Since probability theory is central to\ndecision theory and game theory, it has ramifications for ethics and\npolitical philosophy. It figures prominently in such staples of\nmetaphysics as causation and laws of nature. It appears again in the\nphilosophy of science in the analysis of confirmation of theories,\nscientific explanation, and in the philosophy of specific scientific\ntheories, such as quantum mechanics, statistical mechanics,\nevolutionary biology, and genetics. It can even take center stage in\nthe philosophy of logic, the philosophy of language, and the\nphilosophy of religion. Thus, problems in the foundations of\nprobability bear at least indirectly, and sometimes directly, upon\ncentral scientific, social scientific, and philosophical concerns. The\ninterpretation of probability is one of the most important such\nfoundational problems.\n\nProbability theory was a relative latecomer in intellectual history.\nTo be sure, proto-probabilistic ideas concerning evidence and\ninference date back to antiquity (see Franklin 2001). However,\nprobability’s mathematical treatment had to wait until the\nFermat-Pascal correspondence, and their analysis of games of chance in\n17th century France. Its axiomatization had to wait still\nlonger, in Kolmogorov’s classic Foundations of the Theory of\nProbability (1933). Roughly, probabilities lie between 0 and 1\ninclusive, and they are additive. More formally, let \\(\\Omega\\) be a\nnon-empty set (‘the universal set’). A field (or\nalgebra) on \\(\\Omega\\) is a set \\(\\mathbf{F}\\) of subsets of\n\\(\\Omega\\) that has \\(\\Omega\\) as a member, and that is closed under\ncomplementation (with respect to \\(\\Omega)\\) and union. Let \\(P\\) be\na function from \\(\\mathbf{F}\\) to the real numbers obeying: \nCall \\(P\\) a probability function, and \\((\\Omega ,\n\\mathbf{F}, P)\\) a probability space. This is\nKolmogorov’s “elementary theory of probability”. \nThe assumption that \\(P\\) is defined on a field guarantees that\nthese axioms are non-vacuously instantiated, as are the various\ntheorems that follow from them. The non-negativity and normalization\naxioms are largely matters of convention, although it is non-trivial\nthat probability functions take at least the two values 0 and 1, and\nthat they have a maximal value (unlike various other measures, such as\nlength, volume, and so on, which are unbounded). We will return to\nfinite additivity at a number of points below.  \nWe may now apply the theory to various familiar cases. For example, we\nmay represent the results of tossing a single die once by the set\n\\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\), and we could let \\(\\mathbf{F}\\) be\nthe set of all subsets of \\(\\Omega\\). Under the natural assignment of\nprobabilities to members of \\(\\mathbf{F}\\), we obtain such welcome\nresults as the following:  \nand so on. \nWe could instead attach probabilities to members of a collection\n\\(\\mathbf{S}\\) of sentences of a formal language, closed\nunder (countable) truth-functional combinations, with the following\ncounterpart axiomatization: \nThe bearers of probabilities are sometimes also called\n“events”, “outcomes”, or\n“propositions”, but the underlying formalism remains the\nsame. More attention has been given to interpreting\n‘\\(P\\)’ than to interpreting its bearers; we will be\nconcerned with the former. \nNow let us strengthen our closure assumptions regarding\n\\(\\mathbf{F}\\), requiring it to be closed under complementation\nand countable union; it is then called a sigma field\n(or sigma algebra) on \\(\\Omega\\). It is controversial whether we\nshould strengthen finite additivity, as Kolmogorov does: \nKolmogorov comments that infinite probability spaces are idealized\nmodels of real random processes, and that he limits himself\narbitrarily to only those models that satisfy countable additivity.\nThis axiom is the cornerstone of the assimilation of probability\ntheory to measure theory. \nThe conditional probability of A given B is then given by the\nratio of unconditional probabilities: \nThis is often taken to be the definition of conditional\nprobability, although it should be emphasized that this is a technical\nusage of the term that may not align perfectly with a pretheoretical\nconcept that we might have (see Hájek, 2003). We recognize it\nin locutions such as “the probability that the die lands 1,\ngiven that it lands odd, is 1/3”, or “the probability that\nit will rain tomorrow, given that there are dark clouds in the sky\ntomorrow morning, is high”. It is the concept of the probability\nof something given or in the light of some piece of\nevidence or information. Indeed, some authors take conditional\nprobability to be the primitive notion, and axiomatize it directly\n(e.g. Popper 1959b, Rényi 1970, van Fraassen 1976, Spohn 1986,\nand Roeper and Leblanc 1999). \nThere are other formalizations that give up normalization; that give\nup countable additivity, and even additivity; that allow probabilities\nto take infinitesimal values (positive, but smaller than every\npositive real number); that allow probabilities to be imprecise\n— interval-valued, or more generally represented with sets of\nprecise probability functions; and that treat probabilities\ncomparatively rather than quantitatively. (See Fine 1974, Halpern\n2003, Cozman 2016, Fine 2016, Hawthorne 2016, Lyon 2016.) For now,\nhowever, when we speak of ‘the probability calculus’, we\nwill mean Kolmogorov’s approach, as is standard. See\nHájek and Hitchcock (2016b) for a relatively non-technical\nintroduction to it, intended for philosophers. \nGiven certain probabilities as inputs, the axioms and theorems allow\nus to compute various further probabilities. However, apart from the\nassignment of 1 to the universal set and 0 to the empty set, they are\nsilent regarding the initial assignment of\n probabilities.[1]\n For guidance with that, we need to turn to the interpretations of\nprobability. First, however, let us list some criteria of adequacy for\nsuch interpretations. \nWhat criteria are appropriate for assessing the cogency of a proposed\ninterpretation of probability? Of course, an interpretation should be\nprecise, unambiguous, non-circular, and use well-understood\nprimitives. But those are really prescriptions for good philosophizing\ngenerally; what do we want from our interpretations of\nprobability, specifically? We begin by following Salmon (1966,\n64), although we will raise some questions about his criteria, and\npropose some others. He writes: \nAscertainability. This criterion requires that there be some\nmethod by which, in principle at least, we can ascertain values of\nprobabilities. It merely expresses the fact that a concept of\nprobability will be useless if it is impossible in principle to find\nout what the probabilities are… \nApplicability. The force of this criterion is best expressed\nin Bishop Butler’s famous aphorism, “Probability is the\nvery guide of life.”… \nIt might seem that the criterion of admissibility goes without saying.\nThe word ‘interpretation’ is often used in such a way that\n‘admissible interpretation’ is a pleonasm. Yet it turns\nout that the criterion is non-trivial, and indeed if taken seriously\nwould rule out several of the leading interpretations of probability!\nAs we will see, some of them fail to satisfy countable additivity; for\nothers (certain propensity interpretations) the status of at least\nsome of the axioms is unclear. Nevertheless, we regard them as genuine\ncandidates. It should be remembered, moreover, that Kolmogorov’s\nis just one of many possible axiomatizations, and there is not\nuniversal agreement on which is ‘best’ (whatever that\nmight mean). Indeed, Salmon’s preferred axiomatization differs\nfrom\n Kolmogorov’s.[2]\n Thus, there is no such thing as admissibility tout court,\nbut rather admissibility with respect to this or that axiomatization.\nIt would be unfortunate if, perhaps out of an overdeveloped regard for\nhistory, one felt obliged to reject any interpretation that did not\nobey the letter of Kolmogorov’s laws and that was thus\n‘inadmissible’. In any case, if we found an inadmissible\ninterpretation that did a wonderful job of meeting the criteria of\nascertainability and applicability, then we should surely embrace\nit. \nSo let us turn to those criteria. It is a little unclear in the\nascertainability criterion just what “in principle”\namounts to, though perhaps some latitude here is all to the good.\nUnderstanding it in a way acceptable to a strict empiricist or a\nverificationist may be too restrictive. ‘Probability’ is\napparently a modal concept, and as such might be thought to\noutrun what actually occurs, let alone what is actually observed. \nMost of the work will be done by the applicability criterion. We must\nsay more (as Salmon indeed does) about what sort of a guide\nto life probability is supposed to be. Mass, length, area and volume\nare all useful concepts, and they are ‘guides to life’ in\nvarious ways (think how critical distance judgments can be to\nsurvival); moreover, they are admissible and ascertainable, so\npresumably it is the applicability criterion that will rule them out.\nPerhaps it is best to think of applicability as a cluster of criteria,\neach of which is supposed to capture something of probability’s\ndistinctive conceptual roles; moreover, we should not require that all\nof them be met by a given interpretation. They include: \nNon-triviality: an interpretation should make non-extreme\nprobabilities at least a conceptual possibility. For example, suppose\nthat we interpret ‘\\(P\\)’ as the truth\nfunction: it assigns the value 1 to all true sentences, and 0 to all\nfalse sentences. Then trivially, all the axioms come out true, so this\ninterpretation is admissible. We would hardly count it as an adequate\ninterpretation of probability, however, and so we\nneed to exclude it. It is essential to probability that, at least in\nprinciple, it can take intermediate values. All of the\ninterpretations that we will present meet this criterion, so we will\ndiscuss it no more. \nApplicability to frequencies: an interpretation should render\nperspicuous the relationship between probabilities and (long-run)\nfrequencies. Among other things, it should make clear why, by and\nlarge, more probable events occur more frequently than less probable\nevents. \nApplicability to rational beliefs: an interpretation should\nclarify the role that probabilities play in constraining the degrees\nof belief, or credences, of rational agents. Among other\nthings, knowing that one event is more probable than another, a\nrational agent will be more confident about the occurrence of the\nformer event. \nApplicability to rational decisions: an interpretation should\nmake clear how probabilities figure in rational decision-making. This\nseems especially apposite for a ‘guide to life’.  \nApplicability to ampliative inferences: an interpretation\nwill score bonus points if it illuminates the distinction between\n‘good’ and ‘bad’ ampliative inferences, while\nexplicating why both fall short of deductive inferences. \nApplicability to science: an interpretation should illuminate\nparadigmatic uses of probability in science (for example, in quantum\nmechanics and statistical mechanics). \nPerhaps there are further metaphysical desiderata that we\nmight impose on the interpretations. For example, there appear to be\nconnections between probability and modality. Events with\npositive probability can happen, even if they don’t.\nSome authors also insist on the converse condition that only\nevents with positive probability can happen, although this is more\ncontroversial — see our discussion of ‘regularity’\nin Section 3.3.4. (Indeed, in uncountable probability spaces this\ncondition will require the employment of infinitesimals, and will thus\ntake us beyond the standard Kolmogorov theory —\n‘standard’ both in the sense of being the orthodoxy, and\nin its employment of standard, as opposed to\n‘non-standard’ real numbers. See Skyrms 1980.) In any\ncase, our list is already long enough to help in our assessment of the\nleading interpretations on the market. \nBroadly speaking, there are arguably three main concepts of\nprobability: \nSome philosophers will insist that not all of these concepts are\nintelligible; some will insist that one of them is basic, and that the\nothers are reducible to it. Moreover, the boundaries between these\nconcepts are somewhat permeable. After all, ‘degree of\nconfidence’ is itself an epistemological concept, and as we will\nsee, it is thought to be rationally constrained both by evidential\nsupport relations and by attitudes to physical probabilities in the\nworld. And there are intramural disputes within the camps supporting\neach of these concepts, as we will also see. Be that as it may, it\nwill be useful to keep these concepts in mind. Sections 3.1 and 3.2\ndiscuss analyses of concept (1), classical and\nlogical/evidential probability; 3.3 discusses analyses of\nconcept (2), subjective probability; 3.4, 3.5, and 3.6\ndiscuss three analyses of concept (3), frequentist,\npropensity, and best-system interpretations. \nThe classical interpretation owes its name to its early and august\npedigree. It was championed by de Moivre and Laplace, and inchoate\nversions of it may be found in the works of Pascal, Bernoulli,\nHuygens, and Leibniz. It assigns probabilities in the absence of any\nevidence, or in the presence of symmetrically balanced evidence. The\nguiding idea is that in such circumstances, probability is shared\nequally among all the possible outcomes, so that the classical\nprobability of an event is simply the fraction of the total number of\npossibilities in which the event occurs. It seems especially well\nsuited to those games of chance that by their very design create such\ncircumstances — for example, the classical probability of a fair\ndie landing with an even number showing up is 3/6. It is often\npresupposed (usually tacitly) in textbook probability puzzles. \nHere is a classic statement by de Moivre: \nWe may ask a number of questions about this formulation. When are\nevents of the same kind? Intuitively, ‘heads’ and\n‘tails’ are equally likely outcomes of tossing a fair\ncoin; but if their kind is ‘ways the coin could land’,\nthen ‘edge’ should presumably be counted alongside them.\nThe “certain number of equally possible cases” and\n“that of all possible cases” are presumably finite\nnumbers. What, then, of probabilities in infinite spaces? Apparently,\nirrational-valued probabilities such as \\(1/\\sqrt{2}\\) are automatically\neliminated, and thus theories such as quantum mechanics that posit\nthem cannot be accommodated. (We will shortly see, however, that\nLaplace’s theory has been refined to handle infinite\nspaces.) \nWho are “we”, who “are equally uncertain”?\nDifferent people may be equally undecided about different things,\nwhich suggests that Laplace is offering a subjectivist interpretation\nin which probabilities vary from person to person depending on\ncontingent differences in their evidence. Yet he means to characterize\nthe objective probability assignment of a rational agent in an\nepistemically neutral position with respect to a set of “equally\npossible” cases. But then the proposal risks sounding empty: for\nwhat is it for an agent to be “equally uncertain”\nabout a set of cases, other than assigning them equal probability? \nThis brings us to one of the key objections to Laplace’s\naccount. The notion of “equally possible” cases faces the\ncharge of either being a category mistake (for\n‘possibility’ does not come in degrees), or circular (for\nwhat is meant is really ‘equally probable’). The notion is\nfinessed by the so-called ‘principle of indifference’, a\ncoinage due to Keynes (although he was no friend of the principle):\n“if there is no known reason for predicating of our subject one\nrather than another of several alternatives, then relatively to such\nknowledge the assertions of each of these alternatives have an equal\nprobability” (1921, 52–53). (The ‘principle of equal\nprobability’ would be a better name.) Thus, it might be claimed,\nthere is no circularity in the classical interpretation after all.\nHowever, this move may only postpone the problem, for there is still a\nthreat of circularity, albeit at a lower level. We have two cases\nhere: outcomes for which we have no evidence\n(“reason”) at all, and outcomes for which we have\nsymmetrically balanced evidence. There is no circularity in\nthe first case unless the notion of ‘evidence’ is itself\nprobabilistic; but artificial examples aside, it is doubtful that the\ncase ever arises. For example, we have a considerable fund of evidence\non coin tossing from the results of our own experiments, the testimony\nof others, our knowledge of some of the relevant physics, and so on.\nIn the second case, the threat of circularity is more apparent, for it\nseems that some sort of weighing of the evidence in favor of\neach outcome is required, and this seems to require a reference to\nprobability. Indeed, the most obvious characterization of\nsymmetrically balanced evidence is in terms of equality of conditional\nprobabilities: given evidence \\(E\\) and possible outcomes\n\\(O_1, O_2 , \\ldots ,O_n\\), the evidence is symmetrically balanced iff\n\\(P(O_1\\mid E) = P(O_2\\mid E) = \\ldots = P(O_n\\mid E)\\). Then it seems that\nprobabilities reside at the base of the interpretation after all.\nStill, it would be an achievement if all probabilities could be\nreduced to cases of equal probability. See Zabell (2016) for further\ndiscussion of the classical interpretation and the principle of\nindifference. \nAs we have seen, Laplace’s classical theory is restricted to\nfinite spaces, one for which there are only finitely many possible\noutcomes. When the spaces are countably infinite, the spirit of the\nclassical theory may be upheld by appealing to the\ninformation-theoretic principle of maximum entropy, a\ngeneralization of the principle of indifference championed by Jaynes\n(1968). Entropy is a measure of the lack of\n‘informativeness’ of a probability function. The more\nconcentrated is the function, the less is its entropy; the more\ndiffuse it is, the greater is its entropy. For a discrete assignment\nof probabilities \\(P = (p_1, p_2,\\ldots)\\), the entropy of \\(P\\) is\ndefined as: \n(For more explanation of this formula see the entry on\n Information.)\n  \nThe principle of maximum entropy enjoins us to select from the family\nof all probability functions consistent with our background knowledge\nthe function that maximizes this quantity. In the special case of\nchoosing the most uninformative probability function over a finite set\nof possible outcomes, this is just the familiar ‘flat’\nclassical assignment discussed previously. Things get more complicated\nin the infinite case, since there cannot be a flat assignment over\ndenumerably many outcomes, on pain of violating the standard\nprobability calculus (with countable additivity). Rather, the best we\ncan have are sequences of progressively flatter assignments, none of\nwhich is truly flat. We must then impose some further\nconstraint that narrows the field to a smaller family in which there\n\\(is\\) an assignment of maximum\n entropy.[3]\n This constraint has to be imposed from outside as background\nknowledge, but there is no general theory of which external constraint\nshould be applied when.  \nLet us turn now to uncountably infinite spaces. It is easy — all\ntoo easy — to assign equal probabilities to the points in such a\nspace: each gets probability 0. Non-trivial probabilities arise when\nuncountably many of the points are clumped together in larger sets. If\nthere are finitely many clumps, Laplace’s classical theory may\nbe appealed to again: if the evidence bears symmetrically on these\nclumps, each gets the same share of probability. \nEnter Bertrand’s paradoxes. They all arise in uncountable spaces\nand turn on alternative parametrizations of a given problem that are\nnon-linearly related to each other. Some presentations are needlessly\narcane; length and area suffice to make the point. The following\nexample (adapted from van Fraassen 1989) nicely illustrates how\nBertrand-style paradoxes work. A factory produces cubes with\nside-length between 0 and 1 foot; what is the probability that a\nrandomly chosen cube has side-length between 0 and 1/2 a foot? The\nclassical intepretation’s answer is apparently 1/2, as we\nimagine a process of production that is uniformly distributed over\nside-length. But the question could have been given an equivalent\nrestatement: A factory produces cubes with face-area between 0 and 1\nsquare-feet; what is the probability that a randomly chosen cube has\nface-area between 0 and 1/4 square-feet? Now the answer is apparently\n1/4, as we imagine a process of production that is uniformly\ndistributed over face-area. This is already disastrous, as we cannot\nallow the same event to have two different probabilities (especially\nif this interpretation is to be admissible!). But there is worse to\ncome, for the problem could have been restated equivalently again: A\nfactory produces cubes with volume between 0 and 1 cubic feet; what is\nthe probability that a randomly chosen cube has volume between 0 and\n1/8 cubic-feet? Now the answer is apparently 1/8, as we imagine a\nprocess of production that is uniformly distributed over volume. And\nso on for all of the infinitely many equivalent reformulations of the\nproblem (in terms of the fourth, fifth, … power of the length,\nand indeed in terms of every non-zero real-valued exponent of the\nlength). What, then, is the probability of the event in\nquestion? \nThe paradox arises because the principle of indifference can be used\nin incompatible ways. We have no evidence that favors the side-length\nlying in the interval [0, 1/2] over its lying in [1/2, 1], or vice\nversa, so the principle requires us to give probability 1/2 to each.\nUnfortunately, we also have no evidence that favors the face-area\nlying in any of the four intervals [0, 1/4], [1/4, 1/2], [1/2, 3/4],\nand [3/4, 1] over any of the others, so we must give probability 1/4\nto each. The event ‘the side-length lies in [0, 1/2]’,\nreceives a different probability when merely redescribed. And so it\ngoes, for all the other reformulations of the problem. We cannot meet\nany pair of these constraints simultaneously, let alone all of\nthem. \nJaynes attempts to save the principle of indifference and to extend\nthe principle of maximum entropy to the continuous case, with his\ninvariance condition: in two problems where we have the same\nknowledge, we should assign the same probabilities. He regards this as\na consistency requirement. For any problem, we have a group of\nadmissible transformations, those that change the problem into an\nequivalent form. Various details are left unspecified in the problem;\nequivalent formulations of it fill in the details in different ways.\nJaynes’ invariance condition bids us to assign equal\nprobabilities to equivalent propositions, reformulations of one\nanother that are arrived at by such admissible transformations of our\nproblem. Any probability assignment that meets this condition is\ncalled an invariant assignment. Ideally, our problem will\nhave a unique invariant assignment. To be sure, things will not always\nbe ideal; but sometimes they are, in which case this is surely\nprogress on Bertrand-style problems. \nAnd in any case, for many garden-variety problems such technical\nmachinery will not be needed. Suppose I tell you that a prize is\nbehind one of three doors, and you get to choose a door. This seems to\nbe a paradigm case in which the principle of indifference works well:\nthe probability that you choose the right door is 1/3. It seems\nimplausible that we should worry about some reparametrization of the\nproblem that would yield a different answer. To be sure,\nBertrand-style problems caution us that there are limits to the\nprinciple of indifference. But arguably we must just be careful not to\noverstate its applicability.  \nHow does the classical theory of probability fare with respect to our\ncriteria of adequacy? Let us begin with admissibility. (Laplacean)\nclassical probabilities obey non-negativity and normalization, but\nthey are only finitely additive (de Finetti 1974). So they do not obey\nthe full Kolmogorov probability calculus, but they provide an\ninterpretation of the elementary theory. \nClassical probabilities are ascertainable, assuming that the space of\npossibilities can be determined in principle. They bear a relationship\nto the credences of rational agents; the circularity concern, as we\nsaw above, is that the relationship is vacuous, and that rather than\nconstraining the credences of a rational agent in an\nepistemically neutral position, they merely record them. \nWithout supplementation, the classical theory makes no contact with\nfrequency information. However the coin happens to land in a sequence\nof trials, the possible outcomes remain the same. Indeed, even if we\nhave strong empirical evidence that the coin is biased towards heads\nwith probability, say, 0.6, it is hard to see how the unadorned\nclassical theory can accommodate this fact — for what now are\nthe ten possibilities, six of which are favorable to heads? Laplace\ndoes supplement the theory with his Rule of Succession: “Thus we\nfind that an event having occurred successively any number of times,\nthe probability that it will happen again the next time is equal to\nthis number increased by unity divided by the same number, increased\nby two units.” (1951, 19) That is: \nThus, inductive learning is possible — though not by classical\nprobabilities per se, but rather thanks to this further rule.\nAnd we must ask whether such learning can be captured once and for all\nby such a simple formula, the same for all domains and events. We will\nreturn to this question when we discuss the logical interpretation\nbelow. \nScience apparently invokes at various points probabilities that look\nclassical. Bose-Einstein statistics, Fermi-Dirac statistics, and\nMaxwell-Boltzmann statistics each arise by considering the ways in\nwhich particles can be assigned to states, and then applying the\nprinciple of indifference to different subdivisions of the set of\nalternatives, Bertrand-style. The trouble is that Bose-Einstein\nstatistics apply to some particles (e.g. photons) and not to others,\nFermi-Dirac statistics apply to different particles (e.g. electrons),\nand Maxwell-Boltzmann statistics do not apply to any known particles.\nNone of this can be determined a priori, as the classical\ninterpretation would have it. Moreover, the classical theory purports\nto yield probability assignments in the face of ignorance. But as Fine\n(1973) writes: \nThis brings us to one of the chief points of controversy regarding the\nclassical interpretation. Critics accuse the principle of indifference\nof extracting information from ignorance. Proponents reply that it\nrather codifies the way in which such ignorance should be\nepistemically managed — for anything other than an equal\nassignment of probabilities would represent the possession of some\nknowledge. Critics counter-reply that in a state of complete\nignorance, it is better to assign imprecise probabilities (perhaps\nranging over the entire [0, 1] interval), or to eschew the assignment\nof probabilities altogether. \nLogical theories of probability retain the classical\ninterpretation’s idea that probabilities can be determined a\npriori by an examination of the space of possibilities. However, they\ngeneralize it in two important ways: the possibilities may be assigned\nunequal weights, and probabilities can be computed whatever\nthe evidence may be, symmetrically balanced or not. Indeed, the\nlogical interpretation, in its various guises, seeks to encapsulate in\nfull generality the degree of support or confirmation that a piece of\nevidence \\(e\\) confers upon a given hypothesis \\(h\\), which\nwe may write as \\(c(h, e)\\). In doing so, it\ncan be regarded also as generalizing deductive logic and its notion of\nimplication, to a complete theory of inference equipped with the\nnotion of ‘degree of implication’ that relates \\(e\\)\nto \\(h\\). It is often called the theory of ‘inductive\nlogic’, although this is a misnomer: there is no requirement\nthat \\(e\\) be in any sense ‘inductive’ evidence for\n\\(h\\). ‘Non-deductive logic’ would be a better name,\nbut this overlooks the fact that deductive logic’s relations of\nimplication and incompatibility are also accommodated as extreme cases\nin which the confirmation function takes the values 1 and 0\nrespectively. In any case, it is significant that the logical\ninterpretation provides a framework for induction. \nEarly proponents of logical probability include Johnson (1921), Keynes\n(1921), and Jeffreys (1939/1998). However, by far the most systematic\nstudy of logical probability was by Carnap. His formulation of logical\nprobability begins with the construction of a formal language. In\n(1950) he considers a class of very simple languages consisting of a\nfinite number of logically independent monadic predicates (naming\nproperties) applied to countably many individual constants (naming\nindividuals) or variables, and the usual logical connectives. The\nstrongest (consistent) statements that can be made in a given language\ndescribe all of the individuals in as much detail as the expressive\npower of the language allows. They are conjunctions of complete\ndescriptions of each individual, each description itself a conjunction\ncontaining exactly one occurrence (negated or unnegated) of each\npredicate of the language. Call these strongest statements state\ndescriptions. \nAny probability measure \\(m(-)\\) over the state\ndescriptions automatically extends to a measure over all sentences,\nsince each sentence is equivalent to a disjunction of state\ndescriptions; m in turn induces a confirmation function\n\\(c(-, -)\\): \nThere are infinitely many candidates for \\(m\\), and hence\n\\(c\\), even for very simple languages. Carnap argues for his\nfavored measure “\\(m^*\\)” by insisting that the only\nthing that significantly distinguishes individuals from one another is\nsome qualitative difference, not just a difference in labeling. Call a\nstructure description a maximal set of state descriptions,\neach of which can be obtained from another by some permutation of the\nindividual names. \\(m^*\\) assigns each structure description equal\nmeasure, which in turn is divided equally among their constituent\nstate descriptions. It gives greater weight to homogenous state\ndescriptions than to heterogeneous ones, thus ‘rewarding’\nuniformity among the individuals in accordance with putatively\nreasonable inductive practice. The induced \\(c^*\\) allows\ninductive learning from experience. \nConsider, for example, a language that has three names, \\(a\\), \\(b\\)\nand \\(c\\), for individuals, and one predicate \\(F\\). For this\nlanguage, the state descriptions are: \nThere are four structure descriptions: \nThe measure \\(m^*\\) assigns numbers to the state descriptions as\nfollows: first, every structure description is assigned an equal\nweight, 1/4; then, each state description belonging to a given\nstructure description is assigned an equal part of the weight assigned\nto the structure description: \nNotice that \\(m^*\\) gives greater weight to the homogenous state\ndescriptions 1 and 8 than to the heterogeneous ones. This will\nmanifest itself in the inductive support that hypotheses can gain from\nappropriate evidence statements. Consider the hypothesis statement\n\\(h = Fc\\), true in 4 of the 8 state descriptions, with\na priori probability \\(m^*(h) = 1/2\\). Suppose we examine\nindividual “\\(a\\)” and find it has property \\(F\\) —\ncall this evidence \\(e\\).  Intuitively, \\(e\\) is favorable (albeit\nweak) inductive evidence for \\(h\\). We have: \\(m^*(h \\amp e) = 1/3,\\)\n\\(m^*(e) = 1/2\\), and hence \nThis is greater than the a priori probability\n\\(m^*(h) = 1/2\\), so the hypothesis has been confirmed.\nIt can be shown that in general \\(m^*\\) yields a degree of\nconfirmation \\(c^*\\) that allows learning from experience. \nNote, however, that infinitely many confirmation functions, defined by\nsuitable choices of the initial measure, allow learning from\nexperience. We do not have yet a reason to think that \\(c^*\\) is\nthe right choice. Carnap claims nevertheless that \\(c^*\\) stands\nout for being simple and natural. \nHe later generalizes his confirmation function to a continuum of\nfunctions \\(c_{\\lambda}\\). Define a family of predicates to\nbe a set of predicates such that, for each individual, exactly one\nmember of the set applies, and consider first-order languages\ncontaining a finite number of families. Carnap (1963) focuses on the\nspecial case of a language containing only one-place predicates. He\nlays down a host of axioms concerning the confirmation function \\(c\\),\nincluding those induced by the probability calculus itself, various\naxioms of symmetry (for example, that \\(c(h, e)\\) remains unchanged\nunder permutations of individuals, and of predicates of any family),\nand axioms that guarantee undogmatic inductive learning, and long-run\nconvergence to relative frequencies. They imply that, for a family\n\\(\\{P_n\\},\\) \\(n = 1, \\ldots,k\\) \\((k \\gt 2){:}\\) \nwhere \\(\\lambda\\) is a positive real number. The higher the value of\n\\(\\lambda\\), the less impact evidence has: induction from what is\nobserved becomes progressively more swamped by a classical-style equal\nassignment to each of the \\(k\\) possibilities regarding\nindividual \\(s + 1\\). \nI turn to various objections to Carnap’s program that have been\noffered in the literature, noting that this remains an area of lively\ndebate. (See Maher (2010) for rebuttals of some of these objections\nand for defenses of the program.) Firstly, is there a correct setting\nof \\(\\lambda\\), or said another way, how ‘inductive’ should\nthe confirmation function be? The concern here is that any particular\nsetting of \\(\\lambda\\) is arbitrary in a way that compromises\nCarnap’s claim to be offering a logical notion of\nprobability. Also, it turns out that for any such setting, a universal\nstatement in an infinite universe always receives zero confirmation,\nno matter what the (finite) evidence. Many find this counterintuitive,\nsince laws of nature with infinitely many instances can apparently be\nconfirmed. Earman (1992) discusses the prospects for avoiding the\nunwelcome result. \nSignificantly, Carnap’s various axioms of symmetry are hardly\nlogical truths. Moreover, Fine (1973, 202) argues that we cannot\nimpose further symmetry constraints that are seemingly just as\nplausible as Carnap’s, on pain of inconsistency. Goodman (1955)\ntaught us: that the future will resemble the past in some respect is\ntrivial; that it will resemble the past in all respects is\ncontradictory. And we may continue: that a probability assignment can\nbe made to respect some symmetry is trivial; that one can be made to\nrespect all symmetries is contradictory. This threatens the whole\nprogram of logical probability. \nAnother Goodmanian lesson is that inductive logic must be sensitive to\nthe meanings of predicates, strongly suggesting that a purely\nsyntactic approach such as Carnap’s is doomed. Scott and Krauss\n(1966) use model theory in their formulation of logical probability\nfor richer and more realistic languages than Carnap’s. Still,\nfinding a canonical language seems to many to be a pipe dream, at\nleast if we want to analyze the “logical probability” of\nany argument of real interest — either in science, or in\neveryday life. \nLogical probabilities are admissible. It is easily shown that they\nsatisfy finite additivity, and given that they are defined on finite\nsets of sentences, the extension to countable additivity is trivial.\nGiven a choice of language, the values of a given confirmation\nfunction are ascertainable; thus, if this language is rich enough for\na given application, the relevant probabilities are ascertainable. The\nwhole point of the theory of logical probability is to explicate\nampliative inference, although given the apparent arbitrariness in the\nchoice of language and in the setting of \\(\\lambda\\) — thus, in the\nchoice of confirmation function — one may wonder how well it\nachieves this. The problem of arbitrariness of the confirmation\nfunction also hampers the extent to which the logical interpretation\ncan truly illuminate the connection between probabilities and\nfrequencies. \nThe arbitrariness problem, moreover, stymies any compelling connection\nbetween logical probabilities and rational credences. And a further\nproblem remains even after the confirmation function has been chosen:\nif one’s credences are to be based on logical probabilities,\nthey must be relativized to an evidence statement, \\(e\\). Carnap\nrequires that \\(e\\) be one’s total\nevidence—the maximally specific information at one’s\ndisposal, the strongest proposition of which one is certain. But\nperhaps learning does not come in the form of such\n‘bedrock’ propositions, as Jeffrey (1992) has argued\n— maybe it rather involves a shift in one’s subjective\nprobabilities across a partition, without any cell of the partition\nbecoming certain. Then it may be that the strongest proposition of\nwhich one is certain is expressed by a tautology \\(T\\) —\nhardly an interesting notion of ‘total\n evidence’.[4] \nIn connection with the ‘applicability to science’\ncriterion, a point due to Lakatos is telling. By Carnap’s\nlights, the degree of confirmation of a hypothesis depends on the\nlanguage in which the hypothesis is stated and over which the\nconfirmation function is defined. But scientific progress often brings\nwith it a change in scientific language (for example, the addition of\nnew predicates and the deletion of old ones), and such a change will\nbring with it a change in the corresponding \\(c\\)-values. Thus,\nthe growth of science may overthrow any particular confirmation\ntheory. There is something of the snake eating its own tail here,\nsince logical probability was supposed to explicate the confirmation\nof scientific theories. \nWe have seen that the later Carnap relaxed his earlier aspiration to\nfind a unique confirmation function, allowing a continuum of\nsuch functions displaying a wide range of inductive cautiousness.\nVarious critics of logical probabilities believe that he did not go\nfar enough — that even his later systems constrain inductive\nlearning beyond what is rationally required. This recalls the classic\ndebate earlier in the 20th century between Keynes, a famous\nproponent of logical probabilities, and Ramsey, an equally famous\nopponent. Ramsey (1926; 1990) was skeptical of there being any\nnon-trivial relations of logical probability: he said that he could\nnot discern them himself, and that others disagree about them. This\nskepticism led him to formulate his enormously influential version of\nthe subjective interpretation of probability, to be discussed\nshortly. \nOne might insist, however, that there are non-trivial probabilistic\nevidential relations, even if they are not logical. It may\nnot be a matter of logic that the sun will probably rise\ntomorrow, given our evidence, yet there still seems to be an objective\nsense in which it probably will, given our evidence. In a crime\ninvestigation, there may be a fact of the matter of how strongly the\navailable evidence supports the guilt of various suspects. This does\nnot seem to be a matter of logic—nor of physics, nor of what\nanyone happens to think, nor of how the facts in the actual world turn\nout. It seems to be a matter, rather, of evidential\nprobabilities.  \nMore generally, Timothy Williamson (2000, 209) writes: \nWilliamson identifies one’s evidence with what one knows.\nHowever, one might adopt other conceptions of evidence, and one might\neven take evidential probabilities to link any two propositions\nwhatsoever. Williamson maintains that evidential probabilities are not\nlogical—in particular, they are not syntactically definable. He\nassumes an initial probability distribution \\(P\\), which\n“measures something like the intrinsic plausibility of\nhypotheses prior to investigation” (211). The evidential\nprobability of \\(h\\) on total evidence \\(e\\) is then given\nby \\(P(h\\mid e)\\). \nAre evidential probabilities admissible? Williamson says that “P\nwill be assumed to satisfy a standard set of axioms for the\nprobability calculus” (211). So admissibility is built into the\nvery specification of P. Are they ascertainable? He writes: \nThis might be understood as rejecting ascertainability as a criterion\nof adequacy.  \nHowever, some authors are skeptical that there are such things as\nevidential probabilities—e.g. Joyce (2004). He also argues that\nthere is more than one sense in which evidence tells for or against a\nhypothesis. Bacon (2014) allows that there are such things as\nevidential probabilities, but he argues that various puzzling results\nfollow from Williamson’s account of them, in virtue of its\nidentifying evidence with knowledge. Moreover, one may resist demands\nfor an operational definition of evidential probabilities,\nwhile seeking some further understanding of them in terms of other\ntheoretical concepts. For example, perhaps \\(P(h\\mid e)\\) is the\nsubjective probability that a perfectly rational agent with evidence\n\\(e\\) would assign to \\(h\\)? Williamson argues against this proposal;\nEder (forthcoming) defends it, and she offers several ways of\ninterpreting evidential probabilities in terms of ideal subjective\nprobabilities. If some such way is tenable, evidential probabilities\nwould presumably enjoy whatever applicability that such subjective\nprobabilities have. This brings us to our next interpretation of\nprobability.  \nNearly a century before Ramsey, De Morgan wrote: “By degree of\nprobability, we really mean, or ought to mean, degree of belief”\n(1847, 172). According to the subjective (or\npersonalist or  Bayesian) interpretation,\nprobabilities are degrees of confidence, or credences, or partial\nbeliefs of suitable agents. Thus, we really have many\ninterpretations of probability here— as many as there are\nsuitable agents. What makes an agent suitable? What we might call\nunconstrained subjectivism places no constraints on the\nagents — anyone goes, and hence anything goes. Various studies\nby psychologists are taken to show that people commonly violate the\nusual probability calculus in spectacular ways. (See, e.g., several\narticles in Kahneman et al. 1982.) We clearly do not have here an\nadmissible interpretation (with respect to any probability calculus),\nsince there is no limit to what degrees of confidence agents might\nhave. \nMore promising, however, is the thought that the suitable agents must\nbe, in a strong sense, rational. Following Ramsey, various\nsubjectivists have wanted to assimilate probability to logic by\nportraying probability as “the logic of partial belief”\n(1926; 1990, 53 and 55). A rational agent is required to be logically\nconsistent, now taken in a broad sense. These subjectivists argue that\nthis implies that the agent obeys the axioms of probability (although\nperhaps with only finite additivity), and that subjectivism is thus\n(to this extent) admissible. Before we can present this argument, we\nmust say more about what degrees of belief are. \nSubjective probabilities have long been analyzed in terms of betting\nbehavior. Here is a classic statement by de Finetti (1980): \nThis boils down to the following analysis: \nThe analysis presupposes that, for any \\(E\\), there is exactly\none such price — let’s call this your fair price\nfor the bet on \\(E\\). This presupposition may fail. There may be\nno such price — you may refuse to bet on \\(E\\) at all\n(perhaps unless coerced, in which case your genuine opinion about\n\\(E\\) may not be revealed), or your selling price may differ from\nyour buying price, as may occur if your probability for \\(E\\) is\nimprecise. There may be more than one fair price — you may find\na range of such prices acceptable, as may also occur if your\nprobability for \\(E\\) is imprecise. For now, however, let us\nwaive these concerns, and turn to an important argument that uses the\nbetting analysis purportedly to show that rational degrees of belief\nmust conform to the probability calculus (with at least finite\nadditivity). \nA Dutch book is a series of bets bought and sold at prices\nthat collectively guarantee loss, however the world turns out. Suppose\nwe identify your credences with your betting prices. Ramsey notes, and\nit can be easily proven (e.g., Skyrms 1984), that if your credences\nviolate the probability calculus, then you are susceptible to a Dutch\nbook. For example, suppose that you violate the additivity axiom by\nassigning \\(P(A \\cup B) \\lt P(A) + P(B)\\), where \\(A\\) and \\(B\\) are\nmutually exclusive. Then a cunning bettor could buy from you a bet on\n\\(A \\cup B\\) for \\(P(A \\cup B)\\) units, and sell you bets on \\(A\\) and\n\\(B\\) individually for \\(P(A)\\) and \\(P(B)\\) units respectively. He\npockets an initial profit of \\(P(A) + P(B) - P(A \\cup B)\\), and\nretains it whatever happens. Ramsey offers the following influential\ngloss: “If anyone’s mental condition violated these laws\n[of the probability calculus], his choice would depend on the precise\nform in which the options were offered him, which would be\nabsurd.” (1990, 78) The Dutch Book argument concludes:\nrationality requires your credences to obey the probability\ncalculus. \nEqually important, and often neglected, is the converse theorem that\nestablishes how you can avoid such a predicament. If your subjective\nprobabilities conform to the probability calculus, then no Dutch book\ncan be made against you (Kemeny 1955); your probability assignments\nare then said to be coherent. Williamson (1999) extends the\nDutch Book argument to countable additivity: if your credences violate\ncountable additivity, then you are susceptible to a Dutch book (with\ninfinitely many bets). Conformity to the full probability calculus\nthus seems to be necessary and sufficient for\n coherence.[5]\n We thus have an argument that rational credences provide an\ninterpretation of the full probability calculus, and thus an\nadmissible interpretation. Note, however, that de Finetti—the\narch subjectivist and proponent of the Dutch Book argument—was\nan opponent of countable additivity (e.g. in his 1974). See\nHájek (2009c) and the entry on\n Dutch Book arguments\n for various objections to Dutch Book arguments for conformity to the\nprobability calculus and for other putative norms on credences. \nBut let us return to the betting analysis of credences. It is an\nattempt to make good on Ramsey’s idea that probability “is\na measurement of belief qua basis of action” (67).\nWhile he regards the method of measuring an agent’s credences by\nher betting behavior as “fundamentally sound” (68), he\nrecognizes that it has its limitations. \nThe betting analysis gives an operational definition of subjective\nprobability, and indeed it inherits some of the difficulties of\noperationalism in general, and of behaviorism in particular. For\nexample, you may have reason to misrepresent your true opinion, or to\nfeign having opinions that in fact you lack, by making the relevant\nbets (perhaps to exploit an incoherence in someone else’s\nbetting prices). Moreover, as Ramsey points out, placing the very bet\nmay alter your state of opinion. Trivially, it does so regarding\nmatters involving the bet itself (e.g., you suddenly increase your\nprobability that you have just placed a bet). Less trivially, placing\nthe bet may change the world, and hence your opinions, in other ways.\nFor example, betting at high stakes on the proposition ‘I will\nsleep well tonight’ may suddenly turn you into an insomniac! And\nthen the bet may concern an event such that, were it to occur, you\nwould no longer value the pay-off the same way. (During the August 11,\n1999 solar eclipse in the UK, a man placed a bet that would have paid\na million pounds if the world came to an end.) \nThese problems stem largely from taking literally the notion of\nentering into a bet on \\(E\\), with its corresponding payoffs. The\nproblems may be avoided by identifying your degree of belief in a\nproposition with the betting price you regard as fair, whether or not\nyou enter into such a bet; it corresponds to the betting odds that you\nbelieve confer no advantage or disadvantage to either side of the bet\n(Howson and Urbach 1993). At your fair price, you should be\nindifferent between taking either\n side.[6] \nDe Finetti speaks of “an arbitrary sum” as the prize of\nthe bet on \\(E\\). The sum had better be potentially infinitely\ndivisible, or else probability measurements will be precise only up to\nthe level of ‘grain’ of the potential prizes. For example,\na sum that can be divided into only 100 parts will leave probability\nmeasurements imprecise beyond the second decimal place, conflating\nprobabilities that should be distinguished (e.g., those of a logical\ncontradiction and of ‘a fair coin lands heads 8 times in a\nrow’). More significantly, if utility is not a linear function\nof such sums, then the size of the prize will make a difference to the\nputative probability: winning a dollar means more to a pauper more\nthan it does to Bill Gates, and this may be reflected in their betting\nbehaviors in ways that have nothing to do with their genuine\nprobability assignments. De Finetti responds to this problem by\nsuggesting that the prizes be kept small; that, however, only creates\nthe opposite problem that agents may be reluctant to bother about\ntrifles, as Ramsey points out. \nBetter, then, to let the prizes be measured in utilities: after all,\nutility is infinitely divisible, and utility is a linear function of\nutility. While we’re at it, we should adopt a more liberal\nnotion of betting. After all, there is a sense in which every decision\nis a bet, as Ramsey observed. \nUtilities (desirabilities) of outcomes, their probabilities, and\nrational preferences are all intimately linked. The Port Royal\nLogic (Arnauld, 1662) showed how utilities and probabilities\ntogether determine rational preferences; de Finetti’s betting\nanalysis derives probabilities from utilities and rational\npreferences; von Neumann and Morgenstern (1944) derive utilities from\nprobabilities and rational preferences. And most remarkably, Ramsey\n(1926) (and later, Savage 1954 and Jeffrey 1966) derives both\nprobabilities and utilities from rational preferences\nalone. \nFirst, he defines a proposition to be ethically neutral\n— relative to an agent — if the agent is indifferent\nbetween the proposition’s truth and falsehood. The agent\ndoesn’t care about the ethically neutral proposition as such\n— it may be a means to an end that he might care about, but it\nhas no intrinsic value. (The result of a coin toss is typically like\nthis for most of us.) Now, there is a simple test for determining\nwhether, for a given agent, an ethically neutral proposition\n\\(N\\) has probability 1/2. Suppose that the agent prefers\n\\(A\\) to \\(B\\). Then \\(N\\) has probability 1/2 iff the\nagent is indifferent between the gambles: \nRamsey assumes that it does not matter what the candidates for \\(A\\)\nand \\(B\\) are. We may assign arbitrarily to \\(A\\) and \\(B\\) any two\nreal numbers \\(u(A)\\) and \\(u(B)\\) such that \\(u(A) \\gt u(B)\\),\nthought of as the desirabilities of \\(A\\) and \\(B\\)\nrespectively. Having done this for the one arbitrarily chosen pair\n\\(A\\) and \\(B\\), the utilities of all other propositions are\ndetermined. \nGiven various assumptions about the richness of the preference space,\nand certain ‘consistency assumptions’, he can define a\nreal-valued utility function of the outcomes \\(A, B\\),\netc — in fact, various such functions will represent the\nagent’s preferences. He is then able to define equality of\ndifferences in utility for any outcomes over which the agent has\npreferences. It turns out that ratios of utility-differences are\ninvariant — the same whichever representative utility function\nwe choose. This fact allows Ramsey to define degrees of belief as\nratios of such differences. For example, suppose the agent is\nindifferent between \\(A\\), and the gamble “\\(B\\) if\n\\(X, C\\) otherwise”. Then it follows from\nconsiderations of expected utility that her degree of belief in\n\\(X, P(X)\\), is given by: \nRamsey shows that degrees of belief so derived obey the probability\ncalculus (with finite additivity). \nSavage (1954) likewise derives probabilities and utilities from\npreferences among options that are constrained by certain putative\n‘consistency’ axioms. For a given set of such preferences,\nhe generates a class of utility functions, each a positive linear\ntransformation of the other (i.e. of the form \\(U_1 = aU_2 + b\\),\nwhere \\(a \\gt 0)\\), and a unique probability function. Together these\nare said to ‘represent’ the agent’s preferences, and\nthe result that they do so is called a ‘representation\ntheorem’.  Jeffrey (1966) refines Savage’s approach. The\nresult is a theory of decision according to which rational choice\nmaximizes ‘expected utility’, a certain\nprobability-weighted average of utilities. (See Buchak 2016 for more\ndiscussion.) Some of the difficulties with the behavioristic betting\nanalysis of degrees of belief can now be resolved by moving to an\nanalysis of degrees of belief that is functionalist in spirit. For\nexample, according to Lewis (1986a, 1994a), an agent’s credences\nare represented by the probability function belonging to a utility\nfunction/probability function pair that best rationalizes her\nbehavioral dispositions, rationality being given a decision-theoretic\nanalysis. Representation theorems (in one form or another)\nunderpin representation theorem arguments that rational\nagents’ credences obey the probability calculus: their\npreferences obey the requisite axioms, and thus their credences are\nrepresentable that way. However, as well as being representable\nprobabilistically, such agents’ credences are\nrepresentable non-probabilistically; why should the\nprobabilistic representation be privileged? See Zynda (2000),\nHájek (2008), and Meacham and Weisberg (2011) for this and\nother objections to representation theorem arguments.  \nThere is a deep issue that underlies all of these accounts of\nsubjective probability. They all presuppose the existence of necessary\nconnections between desire-like states and belief-like states,\nrendered explicit in the connections between preferences and\nprobabilities. In response, one might insist that such connections are\nat best contingent, and indeed can be imagined to be absent. Think of\nan idealized Zen Buddhist monk, devoid of any preferences, who\ndispassionately surveys the world before him, forming beliefs but no\ndesires. It could be replied that such an agent is not so easily\nimagined after all — even if the monk does not value worldly\ngoods, he will still prefer some things to others (e.g., truth to\nfalsehood). \nOnce desires enter the picture, they may also have unwanted\nconsequences. Again, how does one separate an agent’s enjoyment\nor disdain for gambling from the value she places on the gamble\nitself? Ironically, a remark that Ramsey makes in his critique of the\nbetting analysis seems apposite here: “The difficulty is like\nthat of separating two different co-operating forces” (1990,\n68). See Eriksson and Hájek (2007) for further criticism of\npreference-based accounts of credence. \nThe betting analysis makes subjective probabilities ascertainable to\nthe extent that an agent’s betting dispositions are\nascertainable. The derivation of them from preferences makes them\nascertainable to the extent that his or her preferences are known.\nHowever, it is unclear that an agent’s full set of preferences\nis ascertainable even to himself or herself. Here a lot of weight may\nneed to be placed on the ‘in principle’ qualification in\nthe ascertainability criterion. The expected utility representation\nmakes it virtually analytic that an agent should be guided by\nprobabilities — after all, the probabilities are her own, and\nthey are fed into the formula for expected utility in order to\ndetermine what it is rational for her to do. So the applicability to\nrational decision criterion is clearly met. \nBut do they function as a good guide? Here it is useful to\ndistinguish different versions of subjectivism. Orthodox\nBayesians in the style of de Finetti recognize no rational\nconstraints on subjective probabilities beyond: \nThis is a permissive epistemology, licensing doxastic states that we\nwould normally call crazy. Thus, you could assign probability 1 to\nthis sentence ruling the universe, while upholding such extreme\nsubjectivism. \nSome subjectivists impose the further rationality requirement of\nregularity: anything that is possible (in an appropriate\nsense) gets assigned positive probability. It is advocated by authors\nsuch as Jeffreys (1939/1998), Kemeny (1955), Edwards et al. (1963),\nShimony (1970), and Stalnaker (1970). It is meant to capture a form of\nopen-mindedness and responsiveness to evidence. But then, perhaps\nunintuitively, someone who assigns probability 0.999 to this sentence\nruling the universe can be judged rational, while someone who assigns\nit probability 0 is judged irrational. See, e.g., Levi (1978) for\nfurther opposition to regularity. \nProbabilistic coherence plays much the same role for degrees of belief\nthat consistency plays for ordinary, all-or-nothing beliefs.\nWhat an extreme subjectivist, even one who demands regularity, lacks\nis an analogue of truth, some yardstick for distinguishing\nthe ‘veridical’ probability assignments from the rest\n(such as the 0.999 one above), some way in which probability\nassignments are answerable to the world. It seems, then, that the\nsubjectivist needs something more. \nAnd various subjectivists offer more. Having isolated the\n“logic” of partial belief as conformity to the probability\ncalculus, Ramsey goes on to discuss what makes a degree of belief in a\nproposition reasonable. After canvassing several possible\nanswers, he settles upon one that focuses on habits of\nopinion formation — “e.g. the habit of proceeding from the\nopinion that a toadstool is yellow to the opinion that it is\nunwholesome” (50). He then asks, for a person with this habit,\nwhat probability it would be best for him to have that a given yellow\ntoadstool is unwholesome, and he answers that “it will in\ngeneral be equal to the proportion of yellow toadstools which are in\nfact unwholesome” (1990, 91). This resonates with more recent\nproposals (e.g., van Fraassen 1984, Shimony 1988) for evaluating\ndegrees of belief according to how closely they match the\ncorresponding relative frequencies — in the jargon, how well\ncalibrated they are. Since relative frequencies obey the\naxioms of probability (up to finite additivity), it is thought that\nrational credences, which strive to track them, should do so\n also.[7] \nHowever, rational credences may strive to track various things. For\nexample, we are often guided by the opinions of experts. We consult\nour doctors on medical matters, our weather forecasters on\nmeteorological matters, and so on. Gaifman (1988) coins the terms\n“expert assignment” and “expert probability”\nfor a probability assignment that a given agent strives to track:\n“The mere knowledge of the [expert] assignment will make the\nagent adopt it as his subjective probability” (193). This idea\nmay be codified as follows: \nwhere ‘\\(P\\)’ is the agent’s subjective\nprobability function, and ‘\\(pr(A)\\)’ is the\nassignment that the agent regards as expert. For example, if you\nregard the local weather forecaster as an expert on your local\nweather, and she assigns probability 0.1 to it raining tomorrow, then\nyou may well follow suit: \nMore generally, we might speak of an entire probability function as\nbeing such a guide for an agent over a specified set of propositions.\nVan Fraassen (1989, 198) gives us this definition: “If\n\\(P\\) is my personal probability function, then \\(q\\) is an\nexpert function for me concerning family \\(F\\) of\npropositions exactly if \\(P(A | q(A)\n= x) = x\\) for all propositions \\(A\\) in family\n\\(F\\).” \nLet us define a universal expert function for a\ngiven rational agent as one that would guide all of that\nagent’s probability assignments in this way: an expert function\nfor the agent concerning all propositions. van Fraassen (1984, 1995a),\nfollowing Goldstein (1983), argues that an agent’s future\nprobability functions are universal expert functions for that\nagent. He enshrines this idea in his Reflection Principle,\nwhere \\(P_t\\) is the agent’s probability\nfunction at time \\(t\\), and\n\\(P_{t+\\Delta}\\) is her function at a later\ntime \\(t+\\Delta\\): \nThe principle encapsulates a certain demand for ‘diachronic\ncoherence’ imposed by rationality. van Fraassen defends it with\na ‘diachronic’ Dutch Book argument (one that considers\nbets placed at different times), and by analogizing violations of it\nto the sort of pragmatic inconsistency that one finds in Moore’s\nparadox. \nWe may go still further. There may be universal expert functions for\nlarge classes of rational agents, and perhaps all of them. The\nPrinciple of Direct Probability regards the relative\nfrequency function as a universal expert function for all\nrational agents; we have already seen the importance that proponents\nof calibration place on it. Let \\(A\\) be an event-type, and let\nrelfreq\\((A)\\) be the relative frequency of \\(A\\)\n(in some suitable reference class). Then for any rational agent with\nprobability function \\(P\\), we have (cf. Hacking 1965): \nLewis (1980) posits a similar expert role for the objective chance\nfunction, ch, for all rational initial credences in his\nPrincipal Principle (here\n simplified[8]): \n‘\\(C\\)’ denotes the ‘ur’ credence\nfunction of an agent at the beginning of enquiry. This is an\nidealization that ensures that the agent does not have any\n“inadmissible” evidence that bears on \\(A\\) without\nbearing on the chance of \\(A\\). For example, a rational agent who\nsomehow knows that a particular coin toss lands heads is surely\nnot required to assign \nRather, this conditional probability should be 1, since she has\ninformation relevant to the outcome ‘heads’ that trumps\nits chance. The other expert principles surely need to be suitably\nqualified – otherwise they face analogous counterexamples. Yet\nstrangely, the Principal Principle is the only expert principle about\nwhich concerns about inadmissible evidence have been raised in the\nliterature.  \nI will say more about relative frequencies and chance shortly.  \nThe ultimate expert, presumably, is the truth function\n— the function that assigns 1 to all the true propositions and 0\nto all the false ones. Knowledge of its values should surely trump\nknowledge of the values assigned by human experts (including\none’s future selves), frequencies, or chances. Note that for any\nputative expert \\(q\\), \n— the truth of \\(A\\) overrides anything the expert might\nsay. So all of the proposed expert probabilities above should really\nbe regarded as defeasible. Joyce (1998) portrays the rational agent as\nestimating truth values, seeking to minimize a measure of distance\nbetween them and her probability assignments—that is, to\nmaximize the accuracy of those assignments. Generalizing a\ntheorem of de Finetti’s (1974), he shows that for any measure of\ndistance that satisfies certain intuitive properties, any agent who\nviolates the probability axioms could serve this epistemic goal better\nby obeying them instead, however the world turns out. In short,\nnon-probabilistic credences are accuracy-dominated by\nprobabilistic credences. This provides a “non-pragmatic”\nargument for probabilism (in contrast to the Dutch Book and\nrepresentation theorem arguments). \nThere are some unifying themes in these putative constraints on\nsubjective probability. An agent’s degrees of belief determine\nher estimates of certain quantities: the values of bets, or the\ndesirabilities of gambles more generally, or the probability\nassignments of various ‘experts’ — humans, relative\nfrequencies, objective chances, or truth values. The laws of\nprobability then are claimed to be constraints on these estimates:\nputative necessary conditions for minimizing her ‘losses’\nin a broad sense, be they monetary, or measured by distances from the\nassignments of these experts. \nWe have been gradually adding more and more constraints on rational\ncredences, putatively demanded by rationality. Recall that Carnap\nfirst assumed that there was a unique confirmation function, and then\nrelaxed this assumption to allow a plurality of such functions. We now\nseem to be heading in the opposite direction: starting with the\nextremely permissive orthodox Bayesianism, we are steadily reducing\nthe class of rationally permissible credence functions. So far the\nconstraints that we have admitted have not been especially\nevidence-driven. Objective Bayesians maintain that a\nrational agent’s credences are largely determined by her\nevidence.  \nHow large is “largely”? The lines of demarcation are not\nsharp, and subjective Bayesianism may be regarded as a somewhat\nindeterminate region on a spectrum of views that morph into objective\nBayesianism. At one end lies an extreme form of subjective\nBayesianism, according to which rational credences are constrained\nonly by the probability calculus (and updating by conditionalization).\nAt the other of the spectrum lies an extreme form of objective\nBayesianism, according to which rational probabilities are constrained\nto the point of uniqueness by one’s evidence—we may call\nthis the Uniqueness Thesis. But both objective Bayesians and\nsubjective Bayesians may adopt less extreme positions, and typically\ndo. For example, Jon Williamson (2010) is an objective Bayesian, but\nnot an extreme one. He adds to the probability calculus the\nconstraints of being calibrated with evidence, and otherwise\nequivocating between basic outcomes, especially appealing to versions\nof maximum entropy. As such, his view is a descendant of the classical\ninterpretation and its generalization due to Jaynes.  \nGamblers, actuaries and scientists have long understood that relative\nfrequencies bear an intimate relationship to probabilities. Frequency\ninterpretations posit the most intimate relationship of all: identity.\nThus, we might identify the probability of ‘heads’ on a\ncertain coin with the number of heads in a suitable sequence of tosses\nof the coin, divided by the total number of tosses. A simple version\nof frequentism, which we will call finite frequentism,\nattaches probabilities to events or attributes in a finite reference\nclass in such a straightforward manner: \nThus, finite frequentism bears certain structural similarities to the\nclassical interpretation, insofar as it gives equal weight to each\nmember of a set of events, simply counting how many of them are\n‘favorable’ as a proportion of the total. The crucial\ndifference, however, is that where the classical interpretation\ncounted all the possible outcomes of a given experiment,\nfinite frequentism counts actual outcomes. It is thus\ncongenial to those with empiricist scruples. It was developed by Venn\n(1876), who in his discussion of the proportion of births of males and\nfemales, concludes: “probability \\(is\\) nothing but that\nproportion” (p. 84, his\n emphasis).[9])\n Finite frequentism is often assumed, tacitly or explicitly, in\nstatistics and in the sciences more generally. \nFinite frequentism gives an operational definition of probability, and\nits problems begin there. For example, just as we want to allow that\nour thermometers could be ill-calibrated, and could thus give\nmisleading measurements of temperature, so we want to allow that our\n‘measurements’ of probabilities via frequencies could be\nmisleading, as when a fair coin lands heads 9 out of 10 times. More\nthan that, it seems to be built into the very notion of probability\nthat such misleading results can arise. Indeed, in many cases,\nmisleading results are guaranteed. Starting with a degenerate case:\naccording to the finite frequentist, a coin that is never tossed, and\nthat thus yields no actual outcomes whatsoever, lacks a probability\nfor heads altogether; yet a coin that is never measured does not\nthereby lack a diameter. Perhaps even more troubling, a coin that is\ntossed exactly once yields a relative frequency of heads of either 0\nor 1, whatever its bias. Or we can imagine a unique radiocative atom\nwhose probabilities of decaying at various times obey a continuous law\n(e.g. exponential); yet according to finite frequentism, with\nprobability 1 it decays at the exact time that it actually\ndoes, for its relative frequency of doing so is 1/1. Famous enough to\nmerit a name of its own, these are instances of the so-called\n‘problem of the single case’. In fact, many events are\nmost naturally regarded as not merely unrepeated, but in a strong\nsense unrepeatable — the 2020 presidential election,\nthe final game of the 2019 NBA play-offs, the Civil War,\nKennedy’s assassination, certain events in the very early\nhistory of the universe, and so on. Nonetheless, it seems natural to\nthink of non-extreme probabilities attaching to some, and perhaps all,\nof them. Worse still, some cosmologists regard it as a genuinely\nchancy matter whether our universe is open or closed (apparently\ncertain quantum fluctuations could, in principle, tip it one way or\nthe other), yet whatever it is, it is ‘single-case’ in the\nstrongest possible sense. \nThe problem of the single case is particularly striking, but we really\nhave a sequence of related problems: ‘the problem of the double\ncase’, ‘the problem of the triple case’ …\nEvery coin that is tossed exactly twice can yield only the relative\nfrequencies 0, 1/2 and 1, whatever its bias… A finite reference\nclass of size \\(n\\), however large \\(n\\) is, can only\nproduce relative frequencies at a certain level of\n‘grain’, namely \\(1/n\\). Among other things, this\nrules out irrational-valued probabilities; yet our best physical\ntheories say otherwise. Furthermore, there is a sense in which any of\nthese problems can be transformed into the problem of the single case.\nSuppose that we toss a coin a thousand times. We can regard this as a\nsingle trial of a thousand-tosses-of-the-coin experiment. Yet\nwe do not want to be committed to saying that that experiment\nyields its actual result with probability 1. \nThe problem of the single case is that the finite frequentist fails to\nsee intermediate probabilities in various places where others do.\nThere is also the converse problem: the frequentist sees intermediate\nprobabilities in various places where others do not. Our world has\nmyriad different entities, with myriad different attributes. We can\ngroup them into still more sets of objects, and then ask with which\nrelative frequencies various attributes occur in these sets. Many such\nrelative frequencies will be intermediate; the finite frequentist\nautomatically identifies them with intermediate probabilities. But it\nwould seem that whether or not they are genuine\nprobabilities, as opposed to mere tallies, depends on the\ncase at hand. Bare ratios of attributes among sets of disparate\nobjects may lack the sort of modal force that one might expect from\nprobabilities. I belong to the reference class consisting of myself,\nthe Eiffel Tower, the southernmost sandcastle on Santa Monica Beach,\nand Mt Everest. Two of these four objects are less than 7 feet tall, a\nrelative frequency of 1/2; moreover, we could easily extend this\nclass, preserving this relative frequency (or, equally easily, not).\nYet it would be odd to say that my probability of being less\nthan 7 feet tall, relative to this reference class, is 1/2, although\nit is perfectly acceptable (if uninteresting) to say that 1/2 of the\nobjects in the reference class are less than 7 feet tall. \nSome frequentists (notably Venn 1876, Reichenbach 1949, and von Mises\n1957 among others), partly in response to some of the problems above,\nhave gone on to consider infinite reference classes,\nidentifying probabilities with limiting relative frequencies\nof events or attributes therein. Thus, we require an infinite sequence\nof trials in order to define such probabilities. But what if the\nactual world does not provide an infinite sequence of trials of a\ngiven experiment? Indeed, that appears to be the norm, and perhaps\neven the rule. In that case, we are to identify probability with a\nhypothetical or counterfactual limiting relative\nfrequency. We are to imagine hypothetical infinite extensions of an\nactual sequence of trials; probabilities are then what the limiting\nrelative frequencies would be if the sequence were so\nextended. We might thus call this interpretation hypothetical\nfrequentism: \nNote that at this point we have left empiricism behind. A modal\nelement has been injected into frequentism with this invocation of a\ncounterfactual; moreover, the counterfactual may involve a radical\ndeparture from the way things actually are, one that may even require\nthe breaking of laws of nature. (Think what it would take for the coin\nin my pocket, which has only been tossed once, to be tossed infinitely\nmany times — never wearing out, and never running short of\npeople willing to toss it!) One may wonder, moreover, whether there is\nalways — or ever — a fact of the matter of what such\ncounterfactual relative frequencies are. \nLimiting relative frequencies, we have seen, must be relativized to a\nsequence of trials. Herein lies another difficulty. Consider an\ninfinite sequence of the results of tossing a coin, as it might be H,\nT, H, H, H, T, H, T, T, … Suppose for definiteness that the\ncorresponding relative frequency sequence for heads, which begins 1/1,\n1/2, 2/3, 3/4, 4/5, 4/6, 5/7, 5/8, 5/9, …, converges to 1/2. By\nsuitably reordering these results, we can make the sequence converge\nto any value in [0, 1] that we like. (If this is not obvious, consider\nhow the relative frequency of even numbers among positive integers,\nwhich intuitively ‘should’ converge to 1/2, can instead be\nmade to converge to 1/4 by reordering the integers with the even\nnumbers in every fourth place, as follows: 1, 3, 5, 2, 7, 9, 11, 4,\n13, 15, 17, 6, …) To be sure, there may be something natural\nabout the ordering of the tosses as given — for example, it may\nbe their temporal ordering. But there may be more than one\nnatural ordering. Imagine the tosses taking place on a train that\nshunts backwards and forwards on tracks that are oriented west-east.\nThen the spatial ordering of the results from west to east\ncould look very different. Why should one ordering be privileged over\nothers? \nA well-known objection to any version of frequentism is that\nrelative frequencies must be relativised to a\nreference class. Consider a probability concerning myself that I care\nabout — say, my probability of living to age 80. I belong to the\nclass of males, the class of non-smokers, the class of philosophy\nprofessors who have two vowels in their surname, … Presumably\nthe relative frequency of those who live to age 80 varies across (most\nof) these reference classes. What, then, is my probability of living\nto age 80? It seems that there is no single frequentist answer.\nInstead, there is my probability-qua-male, my\nprobability-qua-non-smoker, my probability-qua-male-non-smoker, and so\non. This is an example of the so-called reference class\nproblem for frequentism (although it can be argued that analogues\nof the problem arise for the other interpretations as\n well[10]).\n And as we have seen in the previous paragraph, the problem is only\ncompounded for limiting relative frequencies: probabilities must be\nrelativized not merely to a reference class, but to a sequence within\nthe reference class. We might call this the reference sequence\nproblem. \nThe beginnings of a solution to this problem would be to restrict our\nattention to sequences of a certain kind, those with certain desirable\nproperties. For example, there are sequences for which the limiting\nrelative frequency of a given attribute does not exist; Reichenbach\nthus excludes such sequences. Von Mises (1957) gives us a more\nthoroughgoing restriction to what he calls collectives\n— hypothetical infinite sequences of attributes (possible\noutcomes) of specified experiments that meet certain requirements.\nCall a place-selection an effectively specifiable method of\nselecting indices of members of the sequence, such that the selection\nor not of the index \\(i\\) depends at most on the first \\(i - 1\\)\nattributes. Von Mises imposes these axioms: \nAxiom of Randomness: the limiting relative frequency of each\nattribute in a collective \\(\\omega\\) is the same in any infinite\nsubsequence of \\(\\omega\\) which is determined by a place selection. \nThe probability of an attribute \\(A\\), relative to a collective\n\\(\\omega\\), is then defined as the limiting relative frequency of\n\\(A\\) in \\(\\omega\\). Note that a constant sequence such as H, H, H,\n…, in which the limiting relative frequency is the same in\nany infinite subsequence, trivially satisfies the axiom of\nrandomness. This puts some strain on the terminology — offhand,\nsuch sequences appear to be as non-random as they come\n— although to be sure it is desirable that probabilities be\nassigned even in such sequences. Be that as it may, there is a\nparallel between the role of the axiom of randomness in von\nMises’ theory and the principle of maximum entropy in the\nclassical theory: both attempt to capture a certain notion of\ndisorder. \nCollectives are abstract mathematical objects that are not empirically\ninstantiated, but that are nonetheless posited by von Mises to explain\nthe stabilities of relative frequencies in the behavior of actual\nsequences of outcomes of a repeatable random experiment. Church (1940)\nrenders precise the notion of a place selection as a recursive\nfunction. Nevertheless, the reference sequence problem remains:\nprobabilities must always be relativized to a collective, and for a\ngiven attribute such as ‘heads’ there are infinitely many.\nVon Mises embraces this consequence, insisting that the notion of\nprobability only makes sense relative to a collective. In particular,\nhe regards single case probabilities as nonsense: “We can say\nnothing about the probability of death of an individual even if we\nknow his condition of life and health in detail. The phrase\n‘probability of death’, when it refers to a single person,\nhas no meaning at all for us” (11). Some critics believe that\nrather than solving the problem of the single case, this merely\nignores it. And note that von Mises drastically understates the\ncommitments of his theory: by his lights, the phrase\n‘probability of death’ also has no meaning at all when it\nrefers to a million people, or a billion, or any finite number —\nafter all, collectives are infinite. More generally, it seems\nthat von Mises’ theory has the unwelcome consequence that\nprobability statements never have meaning in the real world, for\napparently all sequences of attributes are finite. He introduced the\nnotion of a collective because he believed that the regularities in\nthe behavior of certain actual sequences of outcomes are best\nexplained by the hypothesis that those sequences are initial segments\nof collectives. But this is curious: we know for any actual\nsequence of outcomes that they are not initial segments of\ncollectives, since we know that they are not initial segments of\ninfinite sequences.  \nLet us see how the frequentist interpretations fare according to our\ncriteria of adequacy. Finite relative frequencies of course satisfy\nfinite additivity. In a finite reference class, only finitely many\nevents can occur, so only finitely many events can have positive\nrelative frequency. In that case, countable additivity is satisfied\nsomewhat trivially: all but finitely many terms in the infinite sum\nwill be 0. Limiting relative frequencies violate countable additivity\n(de Finetti 1972, §5.22). Indeed, the domain of definition of\nlimiting relative frequency is not even a field, let alone a sigma\nfield (de Finetti 1972, §5.8). So such relative frequencies do\nnot provide an admissible interpretation of Kolmogorov’s axioms.\nFinite frequentism has no trouble meeting the ascertainability\ncriterion, as finite relative frequencies are in principle easily\ndetermined. The same cannot be said of limiting relative frequencies.\nOn the contrary, any finite sequence of trials (which, after all, is\nall we ever see) puts literally no constraint on the limit of an\ninfinite sequence; still less does an actual finite sequence\nput any constraint on the limit of an infinite hypothetical\nsequence, however fast and loose we play with the notion of ‘in\nprinciple’ in the ascertainability criterion. \nIt might seem that the frequentist interpretations resoundingly meet\nthe applicability to frequencies criterion. Finite frequentism meets\nit all too well, while hypothetical frequentism meets it in the wrong\nway. If anything, finite frequentism makes the connection between\nprobabilities and frequencies too tight, as we have already\nobserved. A fair coin that is tossed a million times is very\nunlikely to land heads exactly half the time; one\nthat is tossed a million and one times is even less likely to do so!\nFacts about finite relative frequencies should serve as evidence, but\nnot conclusive evidence, for the relevant probability\nassignments. Hypothetical frequentism fails to connect probabilities\nwith finite frequencies. It connects them with limiting relative\nfrequencies, of course, but again too tightly: for even in infinite\nsequences, the two can come apart. (A fair coin could land heads\nforever, even if it is highly unlikely to do so.) To be sure, science\nhas much interest in finite frequencies, and indeed working with them\nis much of the business of statistics. Whether it has any interest in\nhighly idealized, hypothetical extensions of actual sequences, and\nrelative frequencies therein, is another matter. The applicability to\nrational beliefs and to rational decisions go much the same way. Such\nbeliefs and decisions are guided by finite frequency information, but\nthey are not guided by information about limits of\nhypothetical frequencies, since one never has such information. For\nmuch more extensive critiques of finite frequentism and hypothetical\nfrequentism, see Hájek (1997) and Hájek (2009)\nrespectively, and La Caze (2016). \nLike the frequency interpretations, propensity\ninterpretations regard probabilities as objective properties of\nentities in the real world. Probability is thought of as a physical\npropensity, or disposition, or tendency of a given type of physical\nsituation to yield an outcome of a certain kind, or to yield a long\nrun relative frequency of such an outcome.  \nWhile Popper (1957) is often credited as being the pioneer of\npropensity interpretations, we already find the key idea in the\nwritings of Peirce (1910, 79–80): “I am, then, to define\nthe meaning of the statement that the probability, that if a\ndie be thrown from a dice box it will turn up a number divisible by\nthree, is one-third. The statement means that the die has a certain\n‘would-be’; and to say that the die has a\n‘would-be’ is to say that it has a property, quite\nanalogous to any habit that a man might have.” A\nman’s habit is a paradigmatic example of a disposition;\naccording to Peirce the die’s probability of landing 3 or 6 is\nan analogous disposition. We might think of various habits coming in\ndifferent degrees, measuring their various strengths. Analogously, the\ndie’s propensities to land various ways measure the strength of\nits dispositions to do so.  \nPeirce continues: “Now in order that the full effect of the\ndie’s ‘would-be’ may find expression, it is\nnecessary that the die should undergo an endless series of throws from\nthe dice box”, and he imagines the relative frequency of the\nevent-type in question oscilating from one side of 1/3 to another.\nThis again anticipates Popper’s view. But an important\ndifference is that Peirce regards the propensity as a property of the\ndie itself, whereas Popper attributes the propensity to the entire\nchance set-up of throwing the die. \nPopper (1957) is motivated by the desire to make sense of single-case\nprobability attributions that one finds in quantum mechanics—for\nexample ‘the probability that this radium atom decays in 1600\nyears is 1/2’. He develops the theory further in (1959a). For\nhim, a probability \\(p\\) of an outcome of a certain type is a\npropensity of a repeatable experiment to produce outcomes of that type\nwith limiting relative frequency \\(p\\). For instance, when we say\nthat a coin has probability 1/2 of landing heads when tossed, we mean\nthat we have a repeatable experimental set-up — the tossing\nset-up — that has a propensity to produce a sequence of outcomes\nin which the limiting relative frequency of heads is 1/2. With its\nheavy reliance on limiting relative frequency, this position risks\ncollapsing into von Mises-style frequentism according to some critics.\nGiere (1973), on the other hand, explicitly allows single-case\npropensities, with no mention of frequencies: probability is just a\npropensity of a repeatable experimental set-up to produce sequences of\noutcomes. This, however, creates the opposite problem to\nPopper’s: how, then, do we get the desired connection between\nprobabilities and frequencies? \nIt is thus useful to follow Gillies (2000a, 2016) in distinguishing\nlong-run propensity theories and single-case\npropensity theories: \nHacking (1965) and Gillies offer long-run (though not infinitely\nlong-run) propensity theories. Fetzer (1982, 1983) and Miller (1994)\noffer single-case propensity theories. So does Popper in a later work\n(1990), in which he regards propensities as “properties of\nthe whole physical situation and sometimes of the particular\nway in which a situation changes” (17). Note that\n‘propensities’ are categorically different things\ndepending on which sort of theory we are considering. According to the\nlong-run theories, propensities are tendencies to produce relative\nfrequencies with particular values, but the propensities are not\nmeasured by the probability values themselves; according to the\nsingle-case theories, the propensities are measured by the\nprobability values. According to Popper’s earlier view, for\nexample, a fair die has a propensity — an extremely\nstrong tendency — to land ‘3’ with long-run\nrelative frequency 1/6. The small value of 1/6 does not\nmeasure this tendency. According to Giere, on the other hand, the die\nhas a weak tendency to land ‘3’. The value of 1/6\ndoes measure this tendency. \nIt seems that those theories that tie propensities to frequencies do\nnot provide an admissible interpretation of the (full) probability\ncalculus, for the same reasons that relative frequencies do not. It is\nprima facie unclear whether single-case propensity theories\nobey the probability calculus or not. To be sure, one can\nstipulate that they do so, perhaps using that stipulation as\npart of the implicit definition of propensities. Still, it remains to\nbe shown that there really are such things — stipulating what a\nwitch is does not suffice to show that witches exist. Indeed, to\nclaim, as Popper does, that an experimental arrangement has a tendency\nto produce a given limiting relative frequency of a particular\noutcome, presupposes a kind of stability or uniformity in the workings\nof that arrangement (for the limit would not exist in a suitably\nunstable arrangement). But this is the sort of\n‘uniformity of nature’ presupposition that Hume argued\ncould not be known either a priori, or empirically. Now,\nappeals can be made to limit theorems — so called ‘laws of\nlarge numbers’ — whose content is roughly that under\nsuitable conditions, such limiting relative frequencies almost\ncertainly exist, and equal the single case propensities. Still, these\ntheorems make assumptions (e.g., that the trials are independent and\nidentically distributed) whose truth again cannot be known, and must\nmerely be postulated. \nPart of the problem here, say critics, is that we do not know enough\nabout what propensities are to adjudicate these issues. There is\nsome property of this coin tossing arrangement such that this\ncoin would land heads with a certain long-run frequency, say. But as\nHitchcock (2002) points out, “calling this property a\n‘propensity’ of a certain strength does little to indicate\njust what this property is.” Said another way, propensity\naccounts are accused of giving empty accounts of probability, à\nla Molière’s ‘dormative virtue’ (Sober 2000,\n64). Similarly, Gillies objects to single-case propensities on the\ngrounds that statements about them are untestable, and that they are\n“metaphysical rather than scientific” (825). Some might\nlevel the same charge even against long-run propensities, which are\nsupposedly distinct from the testable relative\nfrequencies. \nThis suggests that the propensity account has difficulty meeting the\napplicability to science criterion. Some propensity theorists (e.g.,\nGiere) liken propensities to physical magnitudes such as electrical\ncharge that are the province of science. But Hitchcock observes that\nthe analogy is misleading. We can only determine the general\nproperties of charge — that it comes in two varieties, that like\ncharges repel, and so on — by empirical investigation. What\ninvestigation, however, could tell us whether or not propensities are\nnon-negative, normalized and additive? (See also Eagle 2004.) \nMore promising, perhaps, is the idea that propensities are to play\ncertain theoretical roles, and that these place constraints on the way\nthey must behave, and hence what they could be (in the style of the\nRamsey/Lewis/‘Canberra plan’ approach to theoretical terms\n— see Lewis 1970 or Jackson 2000). The trouble here is that\nthese roles may pull in opposite directions, overconstraining\nthe problem. The first role, according to some, constrains them to\nobey the probability calculus (with finite additivity); the second\nrole, according to others, constrains them to violate it. \nOn the one hand, propensities are said to constrain the degrees of\nbelief, or credences, of a rational agent. Recall the\n‘applicability to rational beliefs’ criterion: an\ninterpretation should clarify the role that probabilities play in\nconstraining the credences of rational agents. One such putative role\nfor propensities is codified by Lewis’s ‘Principal\nPrinciple’. (See section 3.3.) The Principal Principle underpins\nan argument (Lewis 1980) that whatever they are, propensities must\nobey the usual probability calculus (with finite additivity). After\nall, it is argued, rational credences, which are guided by them,\ndo. \nOn the other hand, Humphreys (1985) gives an influential argument that\npropensities do not obey Kolmogorov’s probability\ncalculus. The idea is that the probability calculus implies\nBayes’ theorem, which allows us to reverse a\nconditional probability: \nYet propensities seem to be measures of ‘causal\ntendencies’, and much as the causal relation is asymmetric, so\nthese propensities supposedly do not reverse. Suppose that we have a\ntest for an illness that occasionally gives false positives and false\nnegatives. A given sick patient may have a (non-trivial) propensity to\ngive a positive test result, but it apparently makes no sense to say\nthat a given positive test result has a (non-trivial) propensity to\nhave come from a sick patient. Thus, we have an argument that whatever\nthey are, propensities must not obey the usual probability\ncalculus. ‘Humphreys’ paradox’, as it is known, is\nreally an argument against any formal account of propensities that has\nas a theorem: \nhowever one understands these conditional probabilities. The argument\nhas prompted Fetzer and Nute (in Fetzer 1981) to offer a\n“probabilistic causal calculus” that looks quite different\nfrom Kolmogorov’s\n calculus.[11]\n But one could respond more conservatively, as Lyon (2014) points out.\nFor example, Rényi’s axiomatization of primitive\nconditional probabilities does not have (∗) as a theorem, and thus\npropensities may conform to it despite Humphreys’ argument.\nNonetheless, Lyon offers “a more general problem for the\npropensity interpretation. There are all sorts of pairs of events that\nhave no propensity relations between them, and all three axiom\nsystems—Kolmogorov’s, Popper’s, and\nRényi’s—will sometimes force there to be\nconditional probabilities between them. This is not an argument that\nthere is no alternative axiom system that propensity theorists can\nadopt, but it is an argument that the three main contenders are not\nviable” (124). \nOr perhaps all this shows that the notion of ‘propensity’\nbifurcates: on the one hand, there are propensities that bear an\nintimate connection to relative frequencies and rational credences,\nand that obey the usual probability calculus (with finite additivity);\non the other hand, there are causal propensities that behave rather\ndifferently. In that case, there would be still more interpretations\nof probability than have previously been recognized. \nTraditionally, philosophers of probability have recognized five\nleading interpretations of probability—classical, logical,\nsubjectivist, frequentist, and propensity. But recently, so-called\nbest-system interpretations of chance have become\nincreasingly popular and important. While they bear some similarities\nto frequentist accounts, they avoid some of frequentism’s major\nfailings; and while they are sometimes assimilated to propensity\naccounts, they are really quite distinct. So they deserve separate\ntreatment. \nThe best-system approach was pioneered by Lewis (1994b). His analysis\nof chance is based on his account of laws of nature (1973),\nwhich in turn refines an account due to Ramsey (1928/1990). According\nto Lewis, the laws of nature are the theorems of the best\nsystematization of the universe—the true theory\nthat best combines the theoretical virtues of simplicity and\nstrength. These virtues trade off. It is easy for a theory to be\nsimple but not strong, by saying very little; it is easy for a theory\nto be strong but not simple, by conjoining lots of disparate facts.\nThe best theory balances simplicity and strength optimally—in\nshort, it is the most economical true theory. \nSo far, there is no mention of chances. Now, we allow probabilistic\ntheories to enter the competition. We are not yet in a position to\nspeak of such theories as being true. Instead, let us introduce\nanother theoretical virtue: fit. The more probable the actual\nhistory of the universe is by the lights of the theory, the better it\nfits that history. Now the theories compete according to how well they\ncombine simplicity, strength, and fit. The theorems of the winning\ntheory are the laws of nature. Some of these laws may be\nprobabilistic. The chances are the probabilities that are determined\nby these probabilistic laws. \nAccording to Lewis (1986b), intermediate chances are incompatible with\ndeterminism. Loewer (2004) agrees that intermediate\npropensities are incompatible with determinism, understanding\nthose to be essentially dynamical: “they specify the\ndegree to which one state has a tendency to cause another” (15).\nBut he argues that chances are best understood along Lewisian\nbest-system lines, and that there is no reason to limit them to\ndynamical chances. In particular, best-system chances may also attach\nto initial conditions: adding to the dynamical laws a\nprobability assignment, or distribution, over initial\nconditions may provide a substantial gain in strength with relatively\nlittle cost in simplicity. Science furnishes important examples of\ndeterministic theories with such initial-condition probabilities.\nAdding the so-called micro-canonical distribution to Newton’s\nlaws (and the assumption that the distant past had low entropy) yields\nall of statistical mechanics; adding the so-called quantum equilibrium\ndistribution to Bohm’s dynamical laws yields standard quantum\nmechanics. Indeed, this contact with actual science is one of the\nselling points of best-system analyses. See Schwarz (2016) for further\nselling points. \nAt first blush, best-systems analyses seem to score well on our\ncriteria of adequacy. They are admissible by definition: chances are\ndetermined by probabilistic laws (rather than by those expressed by\nsome other formalism). One could in principle ascertain values of\nprobabilities, since they supervene on what actually happens in the\nuniverse (though ‘in principle’ bears a heavy burden).\nApplicability to frequencies is secured through the role that\n‘fit’ plays. Schwarz (2014) offers a proof of the\nPrincipal Principle, which could be taken to undergird the\nbest-systems analyses’ applicability to rational beliefs and\nrational decisions. And we have just mentioned the\ninterpretation’s applicability to science.  \nThis approach solves, or at least eases, some of frequentism’s\nproblems. Progress can be made on the problem of the single case. The\nchances of a rare atom decaying in various time intervals may be\ndetermined by a more pervasive functional law, in which decay chances\nare given for a far wider range of atoms by plugging in a range of\nsettings of some other magnitude (e.g., atomic number). And simplicity\nmay militate in favour of this functional law being continuous, so\neven irrational-valued probabilities may be assigned. Moreover, bare\nratios of attributes among sets of disparate objects will not qualify\nas chances if they are not pervasive enough, for then a theory that\nassigns them probabilities will lose too much simplicity without\nsufficient gain in strength. \nHowever, some other problems for frequentism remain, and some new ones\nemerge, beginning with more basic problems for the Lewisian account of\nlawhood itself. Some of them are partly a matter of Lewis’s\nspecific formulation. Critics (e.g. van Fraassen 1989) question the\nrather nebulous notion of “balancing” simplicity and\nstrength, which are themselves somewhat sketchy. But arguably some\ntechnical story (e.g. information-theoretic) could be offered to\nprecisify them. Lewis himself worries that the exchange rate for such\nbalancing may depend partly on our psychology, in which case there is\nthe threat the laws themselves depend on our psychology, an\nunpalatable idealism about them. But he maintains that this threat is\nnot serious as long as “nature is kind”, and one theory is\nso robustly the front-runner that it remains so under any reasonable\nstandards for balancing. And again, perhaps technical tools can offer\nsome objectivity here. (See section 4 for a gesture at such\ntools.) \nMore telling is the concern that simplicity is language-relative, and\nindeed that any theory can be given the simplest specification\npossible: simply abbreviate it as \\(T\\)! Lewis replies that a\ntheory’s simplicity must be judged according to its\nspecification in a canonical language, in which all of the predicates\ncorrespond to natural properties. Thus, ‘green’\nmay well be eligible, but ‘grue’ surely is not. (See\nGoodman 1955.) Our abbreviation, then, has to be unpacked in terms of\nsuch a language, in which its true complexity will be revealed. But\nthis now involves a substantial metaphysical commitment to a\ndistinction between natural and unnatural properties, one that various\nempiricists (e.g. van Fraassen 1989) find objectionable. \nFurther problems arise with the refinement to handle probabilistic\nlaws. Again, some of them may be due to Lewis’s particular\nformulation. Elga (2004) observes that Lewis’s notion of fit is\nproblematic in various infinite universes—think of an infinite\nsequence of tosses of a coin. Offhand, it seems that the particular\ninfinite sequence that is actualized will be assigned probability\nzero by any plausible candidate theory that regards the\nprobability of heads as intermediate and the trials as independent.\nElga argues, moreover, that there are technical difficulties with\naddressing this problem with infinitesimal probabilities. However,\nperhaps we merely need a different understanding of\n‘fit’—perhaps understood as ‘typicality’\n(Elga), or perhaps one closer to that employed by statisticians with\n‘chi-squared’ tests of goodness of fit (Schwarz 2014). \nHoefer (2007) modifies Lewis’s best-system account in light of\nsome of these problems. Hoefer understands “best” as\n“best for us”, covering regularities that are of interest\nto us, using the language both of science and of daily life, without\nany special privilege bestowed upon natural properties. Moreover, the\n“best system” is now one of chances directly, rather than\nof laws. Thus, there may be chances associated with the punctuality of\ntrains, for example, without any presumption that there are any\nassociated laws. Hoefer follows Elga in understanding\n‘fit’ as ‘typicality’. Strength is a matter of\nthe size of the overall domain of the best system’s probability\nfunctions. Simplicity is to be understood in terms of elegant\nunification, and user-friendliness to beings like us. As a result,\nHoefer embraces the agent-centric nature of chances in his sense,\nregarding as essential the credence-guiding role for them that is\ncaptured by the Principal Principle. This is how his account meets the\n‘applicability to rational beliefs’ criterion.  \nHowever, some other problems for Lewis’s account may run deeper,\nthreatening best-system analyses more generally, and symptomatic of\nthe ghost of frequentism that still hovers behind such analyses. One\nproblem for frequentism that we saw strikes at the heart of any\nattempt to reduce chances to properties of patterns of outcomes. Such\noutcomes may be highly misleading regarding the true chances,\nbecause of their probabilistic nature. This is most vivid for\nevents that are single-case by any reasonable typing. Whether or our\nuniverse turns out to be open or closed, plausibly that outcome is\ncompatible with any underlying intermediate chance. The point\ngeneralizes, however pervasive the probabilistic pattern might be.\nPlausibly, a coin’s landing 9 heads out of 10 tosses is\ncompatible with any underlying intermediate chance for heads; and so\non. The pattern of outcomes that is instantiated may be a poor guide\nto the true chance. (See Hájek 2009 for further arguments\nagainst frequentism that carry over to best-system accounts.) \nAnother way of putting the concern is that best-system accounts\nmistake an idealized epistemology of chance for its metaphysics\n(though see Lewis’ insistence that this is not the case, in his\n1994). Such accounts single out three theoretical virtues—and\none may wonder why just those three—and reifies the\nprobabilities of a theory that displays the virtues to the highest\ndegree. But a probabilistic world may be recalcitrant to even the best\ntheorizing: nature may be unkind. \nIt should be clear from the foregoing that there is still much work to\nbe done regarding the interpretations of probability. Each\ninterpretation that we have canvassed seems to capture some crucial\ninsight into a concept of it, yet falls short of doing complete\njustice to this concept. Perhaps the full story about probability is\nsomething of a patchwork, with partially overlapping pieces and\nprinciples about how they ought to relate. In that sense, the above\ninterpretations might be regarded as complementary, although to be\nsure each may need some further refinement. My bet, for what it is\nworth, is that we will retain the distinct notions of physical\nlogical/evidential, and subjective probability, with a rich tapestry\nof connections between them.  \nThere are further signs of the rehabilitation of classical and logical\nprobability, and in particular the principle of indifference and the\nprinciple of maximum entropy, by authors such as Paris and\nVencovská (1997), Maher (2000, 2001), Bartha and Johns (2001),\nNovack (2010), White (2010), and Pettigrew (2016). Relevant here may\nalso be advances in information theory and complexity theory.\nInformation theory uses probabilities to define the information in a\nparticular event, the degree of uncertainty in a random variable, and\nthe mutual information between random variables (Shannon 1948, Shannon\n& Weaver 1949). This theory has been developed extensively to give\naccounts of complexity, optimal data compression and encoding\n(Kolmogorov 1965, Li and Vitanyi 1997, Cover and Thomas 2006; see the\nentry on\n information\n for more details). It is applied across the sciences, from its\nnatural home in computer science and communication theory, to physics\nand biology. Interpreting information in these areas goes hand-in-hand\nwith interpreting the underlying probabilities: each concept of\nprobability has a corresponding concept of information. For example,\nScarantino (2015) offers an account of ‘natural\ninformation’ in biology that is compatible with either a logical\ninterpretation of probability or objective Bayesian interpretation,\nwhile Kraemer (2015) offers one that rests on a finite frequency\ninterpretation.  \nInformation theory has also proved to be fruitful in the study of\nrandomness (Kolmogorov 1965, Martin-Löf 1966), which obviously is\nintimately related to the notion of probability – see Eagle\n(2016), and the entry on\n chance versus randomness.\n Refinements of our understanding of randomness, in turn, should have\na bearing on the frequency interpretations (recall von Mises’\nappeal to randomness in his definition of a ‘collective’),\nand on propensity accounts (especially those that make explicit ties\nto frequencies). Given the apparent connection between propensities\nand causation adumbrated in Section 3.5, powerful causal modelling\nmethods should also prove fruitful here. More generally, the theory of\ngraphical causal models (also known as Bayesian networks) uses\ndirected acyclic graphs to represent causal relationships in a system.\n(See Spirtes, Glymour and Scheines 1993, Pearl 2000, Woodward 2003.)\nThe graphs and the probabilities of the system’s variables\nharmonize in accordance with the causal Markov condition, a\nsophisticated version of Reichenbach’s slogan “no\ncorrelation without causation”. (See the entry on\n causal models\n for more details.) Thus again, each understanding of probability has\na counterpart understanding of causal networks.  \nRegarding best-system interpretations of chance, I noted that it is\nsomewhat unclear exactly what ‘simplicity’ and\n‘strength’ consist in, and exactly how they are to be\nbalanced. Perhaps insights from statistics and computer science may be\nhelpful here: approaches to statistical model selection, and in\nparticular the ‘curve-fitting’ problem, that attempt to\ncharacterize simplicity, and its trade-off with strength — e.g.,\nthe Akaike Information Criterion (see Forster and Sober 1994), the\nBayesian Information Criterion (see Kieseppä 2001), Minimum\nDescription Length theory (see Rissanen 1999) and Minimum Message\nLength theory (see Wallace and Dowe 1999).  \nPhysical probabilities are becoming even more crucial to scientific\ninquiry. Probabilities are not just used to characterize the support\ngiven to scientific theories by evidence; they appear essentially in\nthe content of the theories themselves. This has led to fertile\nphilosophical ground interpreting the probabilities in such theories.\nFor example, quantum mechanics has physical probabilities at the\nfundamental level. The interpretation of these probabilities is\nrelated to the interpretation of the theory itself (see the entry on\n philosophical issues in quantum theory).\n Statistical mechanics and evolutionary theory have non-fundamental\nobjective probabilities. Are they genuine chances? How can we account\nfor them? See Strevens (2003) and Lyon (2011) for discussion. However,\nSchwarz (2018) argues that these probabilities can and should be left\nuninterpreted. Loewer (2012) proposes that the Lewisian best system of\nour world is given by “the Mentaculus”—a\ncomplete probability map of the universe. This is Albert’s\n(2000) package of: \nAnother ongoing debate regarding physical probabilities concerns\nwhether chance is compatible with determinism—see, e.g.,\nSchaffer (2007), an incompatibilist, and Ismael (2009), a\ncompatibilist; see Frigg (2016) for an overview. Relatedly, an\nimportant approach to objective probability that has gained popularity\ninvolves the so-called method of arbitrary functions.\nOriginating with Poincaré (1896), it is a mathematical\ntechnique for determining probability functions for certain systems\nwith chaotic dynamical laws mapping input conditions to outcomes.\nRoughly speaking, the probabilities for the outcomes are relatively\ninsensitive to the probabilities over the various initial conditions\n— think of how the probabilities of outcomes of spins of a\nroulette wheel apparently do not depend on how the wheel is spun,\nsometimes vigorously, sometimes feebly. See Strevens (2003, 2013) for\ndetailed treatments of this approach.  \nThe subjectivist theory of probability is also thriving—indeed,\nit has been the biggest growth area among all the interpretations,\nthanks to the burgeoning of formal epistemology in the last couple of\ndecades. For each of the topics that I will briefly mention, I can\nonly cite a few representative works.  \nSince Joyce (1998), accuracy arguments for various Bayesian\nnorms have been especially influential. They include arguments for\nconditionalization (Greaves and Wallace 2006, Briggs and Pettigrew\nforthcoming), the Reflection Principle (Easwaran 2013), and the\nPrincipal Principle (Pettigrew 2016). This line of research continues\nto develop. And these norms themselves have received further\nattention—e.g. Schoenfield (2017) on conditionalization, and\nHall (1994, 2004), Ismael (2008) and Briggs (2009) on the Principal\nPrinciple.  \nYet for some problems, Bayesian modelling seems not to be sufficiently\nnuanced. A recently flourishing area has concerned modelling an\nagent’s self-locating credences, concerning who she is,\nor what time it is. The contents of such credences are usually taken\nto be richer than just propositions (thought of as sets of possible\nworlds); rather, they are finer-grained propositions (sets of centered\nworlds — see Lewis 1979). This in turn has ramifications for\nupdating rules, in particular calling conditionalization into\nquestion—see Meacham (2008). The so-called Sleeping Beauty\nproblem (Elga 2000) has generated much discussion in this regard. See\nTitelbaum (2012) for a comprehensive study and approach to such\nproblems. These continue to be fertile areas of research.  \nOn the other hand, there is another sense in which Bayesian modelling\nhas been regarded as too nuanced. It seems to be\npsychologically unrealistic to portray humans (rather than\nideally rational agents) as having degrees of belief that are\ninfinitely precise real numbers. Thus, there have been various\nattempts to ‘humanize’ Bayesianism, and this line of\nresearch is gaining momentum. For example, there has been a\nflourishing study of imprecise probability and imprecise decision\ntheory, in which credences need not be precise numbers—for\nexample, they could be sets of numbers, or intervals. See\nhttp://www.sipta.org/ for up-to-date research in this area. This\nresonates with recent work on whether imprecise probabilities are\nrationally required—Hájek and Smithson (2012) on the pro\nside, Schoenfield (2017) on the con side. The debate continues.  \nNor is it plausible that humans obey all the theorems of the\nprobability calculus—we are incoherent in all sorts of ways. The\nlast couple of decades have also seen research on degrees of\nincoherence—measuring the extent of departures from obedience to\nthe probability calculus—including Zynda (1996), Schervish,\nSeidenfeld and Kadane (2003), and De Bona and Staffel (2017, 2018).\nLin (2013) sees traditional epistemology’s notion of\nbelief as appropriate for humans who fall short of the\nBayesian ideal, but who nevertheless may obey various doxastic norms\nthat can be given Bayesian endorsement. He models everyday practical\nreasoning, with qualitative beliefs and desires, providing a\nqualitative decision theory and representation theorem. Easwaran\n(2016) takes humans to genuinely have all-or-nothing beliefs, but\noffers an instrumentalist justification for representing\nthose beliefs with probabilities.  \nIt also a fact of life that humans disagree with each other.\nHow should an agent modify her credences (if at all) when she\ndisagrees on some claim with an epistemic peer—someone\nwho has the same evidence as her, and whom she regards as equally good\nat evaluating that evidence? The literature on this topic is huge (see\nKopec and Titelbaum (2016) for a survey, and the entry on\n disagreement),\n and it connects in important ways with the interpretations of\nprobability. Intuitively, we feel that disagreement with an epistemic\npeer rationally calls for moving one’s opinion in the direction\nof theirs, since disagreement with a peer seems to be evidence that\none has made a mistake in evaluating one’s initial evidence. As\nKelly (2010) argues, this ‘conciliationist’ intuition\nappears to commit us to the evidential interpretation of probability,\nwith the common evidence bestowing a unique probability on the\ndisputed claim. (See Titelbaum 2016 for dissent; for a recent defense\nof the Uniqueness Thesis more generally, see Horowitz and Dogramaci\n2016; for a recent criticism, see Schoenfield 2014.) The intuition\nalso appears to commit us to probabilistic enkrasia: the view\nthat our credences are beholden to our attitudes about\nevidential probabilities, in much the same way as the Principal\nPrinciple portrays our credences as beholden to our attitudes about\nchances. (See Christensen 2013 and Elga 2010 for versions of\nprobabilistic enkrasia principles.) Let’s grant that\ndisagreement with a peer about some claim is evidence that one has\nmade a mistake regarding it. This should affect one’s opinion in\nit only if one’s attitude about the correct way to\nevaluate the evidence constrains one’s attitude about the claim.\nHowever, probabilistic enkrasia has been criticised (see Williamson\n2014; Lasonen-Aarnio 2015).  \nWe thus come back full circle to where we started. The classical and\nlogical/evidential interpretations sought to capture an objective\nnotion of probability that measures evidential support relations.\nEarly proponents of the subjective interpretation gave us a highly\npermissive notion of rational credences, constrained only by the\nprobability calculus. Less liberal subjectivists added further\nrationality constraints, with credences beholden to attitudes about\nphysical probabilities, and to evidential probabilities—at an\nextreme, to the point of uniqueness. The three kinds of concepts of\nprobability that we identified at the outset converge:\nepistemological, degrees of confidence, and physical. Future research\nwill doubtless explore further the relationships between\nthem—and how they provide guides to life.  \nKyburg (1970) contains a vast bibliography of the literature on\nprobability and induction pre-1970. Also useful for references before\n1967 is the bibliography for “Probability” in the\nMacmillan Encyclopedia of Philosophy. Earman (1992) and\nHowson and Urbach (1993) have more recent bibliographies, and give\ndetailed presentations of the Bayesian program. Skyrms (2000) is an\nexcellent introduction to the philosophy of probability. Von Plato\n(1994) is more technically demanding and more historically oriented,\nwith another extensive bibliography that has references to many\nlandmarks in the development of probability theory in the last\ncentury. Fine (1973) is still a highly sophisticated survey of and\ncontribution to various foundational issues in probability, with an\nemphasis on interpretations. More recent philosophical studies of the\nleading interpretations include Childers (2013), Gillies (2000b),\nGalavotti (2005), Huber (2019), and Mellor (2005). Hájek and\nHitchcock (2016a) is a collection of original survey articles on\nphilosophical issues related to probability. Section IV includes\nchapters on most of the major interpretations of probability. It also\nincludes coverage of the history of probability, Kolmogorov’s\nformalism and alternatives, and applications of probability in science\nand philosophy. Eagle (2010) is a valuable anthology of many\nsignificant papers in the philosophy of probability. Billingsley\n(1995) and Feller (1968) are classic, rather advanced textbooks on the\nmathematical theory of probability. Ross (2013) is less advanced and\nhas lots of examples.","contact.mail":"alan.hajek@anu.edu.au","contact.domain":"anu.edu.au"}]
