[{"date.published":"2019-07-30","url":"https://plato.stanford.edu/entries/paradox-stpetersburg/","author1":"Martin Peterson","author1.info":"http://www.martinpeterson.org","entry":"paradox-stpetersburg","body.text":"\n\n\nThe St. Petersburg paradox was introduced by Nicolaus Bernoulli in\n1713. It continues to be a reliable source for new puzzles and\ninsights in decision theory.\n\n\nThe standard version of the St. Petersburg paradox is derived from the\nSt. Petersburg game, which is played as follows: A fair coin is\nflipped until it comes up heads the first time. At that point the\nplayer wins \\(\\$2^n,\\) where n is the number of times the coin\nwas flipped. How much should one be willing to pay for playing this\ngame? Decision theorists advise us to apply the principle of\nmaximizing expected value. According to this principle, the value of\nan uncertain prospect is the sum total obtained by multiplying the\nvalue of each possible outcome with its probability and then adding up\nall the terms (see the entry on\n normative theories of rational choice: expected utility).\n In the St. Petersburg game the monetary values of the outcomes and\ntheir probabilities are easy to determine. If the coin lands heads on\nthe first flip you win $2, if it lands heads on the second flip you\nwin $4, and if this happens on the third flip you win $8, and so on.\nThe probabilities of the outcomes are \\(\\frac{1}{2}\\),\n\\(\\frac{1}{4}\\), \\(\\frac{1}{8}\\),…. Therefore, the expected\nmonetary value of the St. Petersburg game is \n\n\\[\\begin{align}\n\\frac{1}{2}\\cdot 2 + \\frac{1}{4}\\cdot 4 + \\frac{1}{8}\\cdot 8 + \\cdots &= 1+1+1+ \\cdots \\\\\n&= \\sum_{n=1}^{\\infty} \\left(\\frac{1}{2}\\right)^n \\cdot 2^n \\\\\n&= \\infty.\n\\end{align}\\]\n\n\n(Some would say that the sum approaches infinity, not that it\nis infinite. We will discuss this distinction in\n Section 2.)\n \n\nThe “paradox” consists in the fact that our best theory of\nrational choice seems to entail that it would be rational to pay\nany finite fee for a single opportunity to play the St.\nPetersburg game, even though it is almost certain that the player will\nwin a very modest amount. The probability is \\(\\frac{1}{2}\\) that the\nplayer wins no more than $2, and \\(\\frac{3}{4}\\) that he or she wins\nno more than $4.\n\n\nIn a strict logical sense, the St. Petersburg paradox is not a paradox\nbecause no formal contradiction is derived. However, to claim that a\nrational agent should pay millions, or even billions, for playing this\ngame seems absurd. So it seems that we, at the very least, have a\ncounterexample to the principle of maximizing expected value. If\nrationality forces us to liquidate all our assets for a single\nopportunity to play the St. Petersburg game, then it seems unappealing to be\nrational.\n\nThe St. Petersburg paradox is named after one of the leading\nscientific journals of the eighteenth century, Commentarii\nAcademiae Scientiarum Imperialis Petropolitanae [Papers of\nthe Imperial Academy of Sciences in Petersburg], in which Daniel\nBernoulli (1700–1782) published a paper entitled “Specimen\nTheoriae Novae de Mensura Sortis” [“Exposition of a New\nTheory on the Measurement of Risk”] in 1738. Daniel Bernoulli\nhad learned about the problem from his cousin Nicolaus I\n(1687–1759), who proposed an early but unnecessarily complex\nversion of the paradox in a letter to Pierre Rémond de Montmort\non 9 September 1713 (for this and related letters see J. Bernoulli\n1975). Nicolaus asked de Montmort to imagine an example in which an\nordinary dice is rolled until a 6 comes up: \n[W]hat is the expectation of B … if A promises to\nB to give him some coins in this progression 1, 2, 4, 8, 16\netc. or 1, 3, 9, 27 etc. or 1, 4, 9, 16, 25 etc. or 1, 8, 27, 64\ninstead of 1, 2, 3, 4, 5 etc. as beforehand. Although for the most\npart these problems are not difficult, you will find however something\nmost curious. (N. Bernoulli to Montmort, 9 September 1713) \nIt seems that Montmort did not immediately get Nicolaus’ point.\nMontmort responded that these problems  \nhave no difficulty, the only concern is to find the sum of the series\nof which the numerators being in the progression of squares, cubes,\netc. the denominators are in geometric progression. (Montmort to N.\nBernoulli, 15 November 1713)  \nHowever, he never performed any calculations. If he had, he would have\ndiscovered that the expected value of the first series (1, 2, 4, 8,\n16, etc.) is: \nFor this series it holds that \nso by applying the ratio test it is easy to verify that the series is\ndivergent. (This test was discovered by d’Alembert in 1768, so\nit might be unfair to criticize Montmort for not seeing this.)\nHowever, the mathematical argument presented by Nicolaus himself was\nalso a bit sketchy and would not impress contemporary mathematicians.\nThe good news is that his conclusion was correct: \nit would follow thence that B must give to A an infinite sum\nand even more than infinity (if it is permitted to speak thus) in\norder that he be able to make the advantage to give him some coins in\nthis progression 1, 2, 4, 8, 16 etc. (N. Bernoulli to Montmort, 20\nFebruary 1714) \nThe next important contribution to the debate was made by\nCramér in 1728. He read about Nicolaus’ original problem\nin a book published by Montmort and proposed a simpler and more\nelegant formulation in a letter to Nicolaus: \nIn order to render the case more simple I will suppose that A\nthrow in the air a piece of money, B undertakes to give him a\ncoin, if the side of Heads falls on the first toss, 2, if it is only\nthe second, 4, if it is the 3rd toss, 8, if it is the 4th toss, etc.\nThe paradox consists in this that the calculation gives for the\nequivalent that A must give to B an infinite sum, which\nwould seem absurd. (Cramér to N. Bernoulli, 21 May 1728) \nIn the very same letter, Cramér proposed a solution that\nrevolutionized the emerging field of decision theory. Cramér\npointed out that it is not the expected monetary value that\nshould guide the choices of a rational agent, but rather the\n“usage” that “men of good sense” can make of\nmoney. According to Cramér, twenty million is not worth more\nthan ten million, because ten million is enough for satisfying all\ndesires an agent may reasonably have: \nmathematicians value money in proportion to its quantity, and men of\ngood sense in proportion to the usage that they may make of it. That\nwhich renders the mathematical expectation infinite, is the prodigious\nsum that I am able to receive, if the side of Heads falls only very\nlate, the 100th or 1000th toss. Now this sum, if I reason as a\nsensible man, is not more for me, does not make more pleasure for me,\ndoes not engage me more to accept the game, than if it would be only\n10 or 20 million coins. (21 May 1728) \nThe point made by Cramér in this passage can be generalized.\nSuppose that the upper boundary of an outcome’s value is\n\\(2^m.\\) If so, that outcome will be obtained if the coin lands heads\non the mth flip. This means that the expected value\nof all the infinitely many possible outcomes in which the\ncoin is flipped more than m times will be finite: It is \\(2^m\\)\ntimes the probability that this happens, so it cannot exceed \\(2^m\\).\nTo this we have to add the aggregated value of the first m\npossible outcomes, which is obviously finite. Because the sum of any\ntwo finite numbers is finite, the expected value of\nCramér’s version of the St. Petersburg game is\nfinite. \nCramér was aware that it would be controversial to claim that\nthere exists an upper boundary beyond which additional riches do not\nmatter at all. However, he pointed out that his solution\nworks even if he the value of money is strictly increasing but the\nrelative increase gets smaller and smaller (21 May 1728):  \nIf one wishes to suppose that the moral value of goods was as the\nsquare root of the mathematical quantities … my moral\nexpectation will be \nThis is the first clear statement of what contemporary decision\ntheorists and economists refer to as decreasing marginal utility: The\nadditional utility of more money is never zero, but the richer you\nare, the less you gain by increasing your wealth further.\nCramér correctly calculated the expected utility (“moral\nvalue”) of the St. Petersburg game to be about 2.9 units for an\nagent whose utility of money is given by the root function. \nDaniel Bernoulli proposed a very similar idea in his famous 1738\narticle mentioned at the beginning of this section. Daniel argued that\nthe agent’s utility of wealth equals the logarithm of the\nmonetary amount, which entails that improbable but large monetary\nprizes will contribute less to the expected utility of the game than\nmore probable but smaller monetary amounts. As his article was about\nto be published, Daniel’s brother Nicolaus mentioned to him that\nCramér had proposed a very similar idea in 1728 (in the letter\nquoted above). In the final version of the text, Daniel openly\nacknowledged this:  \nIndeed I have found [Cramér’s] theory so similar to mine\nthat it seems miraculous that we independently reached such close\nagreement on this sort of subject. (Daniel Bernoulli 1738 [1954:\n33]) \nCramér’s remark about the agent’s decreasing\nmarginal utility of money solves the original version of the St.\nPetersburg paradox. However, modern decision theorists agree that this\nsolution is too narrow. The paradox can be restored by increasing the\nvalues of the outcomes up to the point at which the agent is fully\ncompensated for her decreasing marginal utility of money (see Menger\n1934 [1979]). The version of the St. Petersburg paradox discussed in\nthe modern literature can thus be formulated as follows: \nA fair coin is flipped until it comes up heads. At that point the\nplayer wins a prize worth \\(2^n\\) units of utility on the\nplayer’s personal utility scale, where n is the number of\ntimes the coin was flipped. \nNote that the expected utility of this gamble is infinite even if the\nagent’s marginal utility of money is decreasing. We can leave it\nopen exactly what the prizes consists of. It need not be money. \nIt is worth stressing that none of the prizes in the St. Petersburg\ngame have infinite value. No matter how many times the coin is\nflipped, the player will always win some finite amount of\nutility. The expected utility of the St. Petersburg game\nis not finite, but the actual outcome will always be finite.\nIt would thus be a mistake to dismiss the paradox by arguing that no\nactual prizes can have infinite utility. No actual infinities are\nrequired for constructing the paradox, only potential ones. (For a\ndiscussion of the distinction between actual and potential infinities,\nsee Linnebo and Shapiro 2019.) In discussions of the St. Petersburg\nparadox it is often helpful to interpret the term “infinite\nutility” as “not finite” but leave it to\nphilosophers of mathematics to determine whether it is or\nmerely approaches infinity. \nSome authors have discussed exactly what is problematic with the claim\nthat the expected utility of the modified St. Petersburg game is\ninfinite (read: not finite). Is it merely the fact that the fair price\nof the wager is “too high”, or is there something else\nthat prompts the worry? James M. Joyce notes that  \na wager of infinite utility will be strictly preferred to any\nof its payoffs since the latter are all finite. This is absurd given\nthat we are confining our attention to bettors who value wagers only\nas means to the end of increasing their fortune. (Joyce 1999: 37) \nJoyce’s point seems to be that an agent who pays the fair price\nof the wager will know for sure that she will actually be\nworse off after she has paid the fee. However, this seems to\npresuppose that actual infinities do exist. If only potential\ninfinities exist, then the player cannot “pay” an infinite\nfee for playing the game. If so, we could perhaps interpret Joyce as\nreminding us that no matter what finite amount the player actually\nwins, the expect utility will always be higher, meaning that it would\nhave been rational to pay even more. Decisions theorists analyze a\nmeans-ends notion of rationality, according to which it is rational to\ndo whatever is the best means to one’s end. The player thus\nknows that paying more than what one actually wins cannot be\nthe best means to the end of maximizing utility. This observation\nenables us to strengthen the original “paradox” (in which\nno formal contradiction is derived) into a stronger version consisting\nof three incompatible claims: \nMany discussions of the St. Petersburg paradox have focused on\n (1).\n As we will see in the next couple of sections, many scholars\nargue that the value of the St. Petersburg game is, for one reason or\nanother, finite. A rare exception is Hájek and Nover. They\noffer the following argument for accepting\n (1): \nThe St Petersburg game can be regarded as the limit of a sequence of\ntruncated St Petersburg games, with successively higher finite\ntruncation points—for example, the game is called off if heads\nis not reached by the tenth toss; by the eleventh toss; by the\ntwelveth toss;…. If we accept dominance reasoning, these\nsuccessive truncations can guide our assessment of the St Petersburg\ngame’s value: it is bounded below by each of their values, these\nbounds monotonically increasing. Thus we have a principled reason for\naccepting that it is worth paying any finite amount to play the St\nPetersburg game. (Hájek and Nover 2006: 706) \nAlthough they do not explicitly say so, Hájek and Nover would\nprobably reject\n (3). The least controversial claim is perhaps (2). It is, of course, logically possible that the coin keeps\nlanding tails every time it is flipped, even though an\ninfinite sequence of tails has probability 0. (For a discussion of\nthis possibility, see Williamson 2007.) Some events that have\nprobability 0 do actually occur, and in uncountable probability spaces\nit is impossible that all outcomes have a probability greater than 0.\nEven so, if the coin keeps landing tails every\ntime it is flipped, the agent wins 0 units of utility. So\n (2)\n would still hold true. \nSome authors claim that the St. Petersburg game should be dismissed\nbecause it rests on assumptions that can never be fulfilled. For\ninstance, Jeffrey (1983: 154) argues that “anyone who offers to\nlet the agent play the St. Petersburg gamble is a liar, for he is\npretending to have an indefinitely large bank”. Similar\nobjections were raised in the eighteenth century by Buffon and\nFontaine (see Dutka 1988). \nHowever, it is not clear why Jeffrey’s point about real-world\nconstraints would be relevant. What is wrong with evaluating a highly\nidealized game we have little reason to believe we will ever get to\nplay? Hájek and Smithson (2012) point out that the St\nPetersburg paradox is contagious in the following sense: As\nlong as you assign some nonzero probability to the hypothesis that the\nbank’s promise is credible, the expected utility will be\ninfinite no matter how low your credence in the hypothesis is. Any\nnonzero probability times infinity equals infinity, so any option in\nwhich you get to play the St. Petersburg game with a nonzero\nprobability has infinite expected utility. \nIt is also worth keeping in mind that the St. Petersburg game may not\nbe as unrealistic as Jeffrey claims. The fact that the bank does not\nhave an indefinite amount of money (or other assets) available\nbefore the coin is flipped should not be a problem. All that\nmatters is that the bank can make a credible promise to the\nplayer that the correct amount will be made available within a\nreasonable period of time after the flipping has been completed. How\nmuch money the bank has in the vault when the player plays the game is\nirrelevant. This is important because, as noted in section 2, the\namount the player actually wins will always be finite. We can thus\nimagine that the game works as follows: We first flip the coin, and\nonce we know what finite amount the bank owes the player, the CEO will\nsee to it that the bank raises enough money. \nIf this does not convince the player, we can imagine that the central\nbank issues a blank check in which the player gets to fill in the\ncorrect amount once the coin has been flipped. Because the check is\nissued by the central bank it cannot bounce. New money is\nautomatically created as checks issued by the central bank are\nintroduced in the economy. Jeffrey dismisses this version of the St.\nPetersburg game with the following argument: \n[Imagine that] Treasury department delivers to the winner a crisp new\nbillion billion dollar bill. Due to the resulting inflation, the\nmarginal desirabilities of such high payoffs would presumably be low\nenough to make the prospect of playing the game have finite expected\n[utility]. (Jeffrey 1983: 155) \nJeffrey is probably right that “a crisp new billion billion\ndollar bill” would trigger some inflation, but this seems to be\nsomething we could take into account as we construct the game. All\nthat matters is that the utilities in the payoff scheme are\nlinear. \nReaders who feel unconvinced by this argument may wish to imagine a\nversion of the St. Petersburg game in which the player is hooked up to\nNozick’s Experience Machine (see section 2.3 in the entry on\n hedonism).\n By construction, this machine can produce any pleasurable experience\nthe agent wishes. So once the coin has been flipped n times,\nthe Experience Machine will generate a pleasurable experience worth\n\\(2^n\\) units of utility on the player’s personal utility scale.\nAumann (1977) notes without explicitly mention the Experience Machine\nthat: \nThe payoffs need not be expressible in terms of a fixed finite number\nof commodities, or in terms of commodities at all […] the\nlottery ticket […] might be some kind of open-ended activity --\none that could lead to sensations that he has not heretofore\nexperienced. Examples might be religious, aesthetic, or emotional\nexperiences, like entering a monastery, climbing a mountain, or\nengaging in research with possibly spectacular results. (Aumann 1977:\n444) \nA possible example of the type of experience that Aumann has in mind\ncould be the number of days spent in Heaven. It is not clear why time\nspent in Heaven must have diminishing marginal utility.  \nAnother type of practical worry concerns the temporal dimension of the\nSt. Petersburg game. Brito (1975) claims that the coin flipping may\nsimply take too long time. If each flip takes n seconds, we\nmust make sure it would be possible to flip it sufficiently\nmany times before the player dies. Obviously, if there exists an upper\nlimit to how many times the coin can be flipped the expected utility\nwould be finite too. \nA straightforward response to this worry is to imagine that the\nflipping took place yesterday and was recorded on video. The first\nflip occurred at 11 p.m. sharp, the second flip \\(\\frac{60}{2}\\)\nminutes later, the third \\(\\frac{60}{4}\\) minutes after the second,\nand so on. The video has not yet been made available to anyone, but as\nsoon as the player has paid the fee for playing the game the video\nwill be placed in the public domain. Note that the coin could in\nprinciple have been flipped infinitely many times within a single\nhour. (This is an example of a “supertask”; see the entry on\n supertasks.) \nIt is true that this random experiment requires the coin to be flipped\nfaster and faster. At some point we would have to spin the coin faster\nthan the speed of light. This is not logically impossible\nalthough this assumption violates a contingent law of nature. If you\nfind this problematic, we can instead imagine that someone throws a\ndart on the real line between 0 and 1. The probability that the dart\nhits the first half of the interval, \\(\\left[0, \\frac{1}{2}\\right),\\)\nis \\(\\frac{1}{2}.\\) And the probability that the dart hits the next\nquarter, \\(\\left[\\frac{1}{2}, \\frac{3}{4}\\right),\\) is\n\\(\\frac{1}{4}\\), and so on. If “coin flips” are generated\nin this manner the random experiment will be over in no time at all.\nTo steer clear of the worry that no real-world dart is infinitely\nsharp we can define the point at which the dart hits the real line as\nfollows: Let a be the area of the dart. The point at which the\ndart hits the interval [0,1] is defined such that half of the area of\na is to the right of some vertical line through a and\nthe other half to the left the vertical line. The point at which the\nvertical line crosses the interval [0,1] is the outcome of the random\nexperiment. \nIn the contemporary literature on the St. Petersburg paradox practical\nworries are often ignored, either because it is possible to imagine\nscenarios in which they do not arise, or because highly idealized\ndecision problems with unbounded utilities and infinite state spaces\nare deemed to be interesting in their own right. \nArrow (1970: 92) suggests\nthat the utility function of a rational agent should be “taken\nto be a bounded function.… since such an assumption is needed\nto avoid [the St. Petersburg] paradox”. Basset (1987) makes a\nsimilar point; see also Samuelson (1977) and McClennen (1994). \nArrow’s point is that utilities must be bounded to avoid the St.\nPetersburg paradox and that traditional axiomatic accounts of the\nexpected utility principle guarantee this to be the case. The\nwell-known axiomatizations proposed by Ramsey (1926), von Neumann and\nMorgenstern (1947), and Savage (1954) do, for instance, all entail\nthat the decision maker’s utility function is bounded. (See\n section 2.3 in the entry on decision theory\n for an overview of von Neumann and Morgenstern’s\naxiomatization.) \nIf the utility function is bounded, then the expected utility of the\nSt. Petersburg game will of course be finite. But why do the axioms of\nexpected utility theory guarantee that the utility function is\nbounded? The crucial assumption is that rationally permissible\npreferences over lotteries are continuous. To explain the\nsignificance of this axiom it is helpful to introduce some symbols.\nLet \\(\\{pA, (1-p)B\\}\\) be the lottery that results in A with\nprobability p and B with probability \\(1-p\\). The\nexpression \\(A\\preceq B\\) means that the agent considers B to\nbe at least as good as A, i.e., weakly prefers B to\nA. Moreover, \\(A\\sim B\\) means that A and B are\nequi-preferred, and \\(A\\prec B\\) means that B is preferred to\nA. Consider: \nTo explain why this axiom entails that no object can have infinite\nvalue, suppose for reductio that A is a prize check\nworth $1, B is a check worth $2, and C is a prize to\nwhich the agent assigns infinite utility. The decision maker’s\npreference is \\(A\\prec B\\prec C\\), but there is no probability\np such that \\(\\{pA, (1-p)C\\sim B\\). Whenever p is\nnonzero the decision maker will strictly prefer \\(\\{pA, (1-p)C\\}\\) to\nB, and if p is 0 the decision maker will strictly prefer\nB. So because no object (lottery or outcome) can have infinite\nvalue, and a utility function is defined by the utilities it assigns\nto those objects (lotteries or outcomes), the utility function has to\nbe bounded. \nDoes this solve the St. Petersburg paradox? The answer depends on\nwhether we think a rational agent offered to play the St. Petersburg\ngame has any reason to accept the continuity axiom. A possible\nview is that anyone who is offered to play the St.\nPetersburg game has reason to reject the continuity axiom. Because the\nSt. Petersburg game has infinite utility, the agent has no reason to\nevaluate lotteries in the manner stipulated by this axiom. As\nexplained in Section 3, we can imagine unboundedly valuable\npayoffs. \nSome might object that the continuity axiom, as well as the\nother axioms proposed by von Neumann and Morgenstern (and Ramsey and\nSavage), are essential for defining utility in a\nmathematically precise manner. It would therefore be\nmeaningless to talk about utility if we reject the continuity\naxiom. This axiom is part of what it means to say that something has a\nhigher utility than something else. A good response could be to develop a theory of utility in which preferences over\nlotteries are not used for defining the meaning of the concept; see\nLuce (1959) for an early example of such a theory. Another response\ncould be to develop a theory of utility in which the continuity axiom\nis explicitly rejected; see Skala (1975). \nBuffon argued in 1777 that a rational decision maker should disregard the possibility of\nwinning lots of money in the St. Petersburg game because the\nprobability of doing so is very low. According to Buffon, some\nsufficiently improbable outcomes are “morally impossible”\nand should therefore be ignored. From a technical point of view, this\nsolution is very simple: The St. Petersburg paradox arises because the\ndecision maker is willing to aggregate infinitely many extremely\nvaluable but highly improbable outcomes, so if we restrict the set of\n“possible” outcomes by excluding sufficiently improbable\nones the expected utility will, of course, be finite. \nBut why should small probabilities be ignored? And how do we\ndraw the line between small probabilities that are beyond concern and\nothers that are not? Dutka summarizes Buffon’s lengthy answer as\nfollows: \nTo arrive at a suitable threshold value, [Buffon] notes that a\nfifty-six year old man, believing his health to be good, would\ndisregard the probability that he would die within twenty-four hours,\nalthough mortality tables indicate that the odds against his dying in\nthis period are only 10189 to 1. Buffon thus takes a probability of\n1/10,000 or less for an event as a probability which may be\ndisregarded. (Dutka 1988: 33) \nIs this a convincing argument? According to Buffon, we ought\nto ignore some small probabilities because people like him\n(56-year-old males) do in fact ignore them. Buffon can thus\nbe accused of attempting to derive an “ought” from an\n“is”. To avoid Hume’s no-ought-from-an-is objection,\nBuffon would have to add a premise to the effect that people’s\neveryday reactions to risk are always rational. But why should we\naccept such a premise? \nAnother objection is that if we ignore small probabilities, then we\nwill sometimes have to ignore all possible outcomes of an\nevent. Consider the following example: A regular deck of cards has 52\ncards, so it can be arranged in exactly 52! different ways. The\nprobability of any given arrangement is thus about 1 in \\(8 \\cdot\n10^{67}\\). This is a very small probability. (If one were to add six\ncards to the deck, then the number of possible orderings would exceed\nthe number of atoms in the known, observable universe.) However, every\ntime we shuffle a deck of cards, we know that exactly one of the\npossible outcomes will materialize, so why should we\nignore all such very improbable outcomes? \nNicholas J. J. Smith (2014) defends a modern version of Buffon’s solution. He\nbases his argument on the following principle: \nSmith points out that the order of the quantifiers in RNP is crucial.\nThe claim is that for every lottery there exists some probability\nthreshold \\(\\epsilon\\) below which all probabilities should be ignored,\nbut it would be a mistake to think that one and the same \\(\\epsilon\\)\nis applicable to every lottery. This is important because otherwise we\ncould argue that RNP allows us to combine thousands or millions of\nseparate events with a probability of less than \\(\\epsilon.\\) It would\nobviously make little sense to ignore, say, half a million\none-in-a-million events. By keeping in mind that that the appropriate\n\\(\\epsilon\\) may vary from case to case this worry can be\ndismissed. \nSmith also points out that if we ignore probabilities less than\n\\(\\epsilon,\\) then we have to increase some other probabilities to\nensure that all probabilities sum up to one, as required by the\nprobability axioms (see\n section 1 in the entry on interpretations of probability).\n Smith proposes a principle for doing this in a systematic manner. \nHowever, why should we accept RNP? What is the\nargument for accepting this controversial principle apart\nfrom the fact that it would solve the St. Petersburg paradox?\nSmith’s argument goes as follows: \nInfinite precision cannot be required: rather, in any given context,\nthere must be some finite tolerance—some positive threshold such\nthat ignoring all outcomes whose probabilities lie below this\nthreshold counts as satisfying the norm…. There is a norm of\ndecision theory which says to ignore outcomes whose probability is\nzero. Because this norm mentions a specific probability value (zero),\nit is the kind of norm where it makes sense to impose a tolerance:\nzero plus or minus \\(\\epsilon\\) (which becomes zero plus \\(\\epsilon,\\)\ngiven that probabilities are all between 0 and 1)… the idea\nbehind (RNP) is that in any actual context in which a decision is to\nbe made, one never needs to be infinitely precise in this\nway—that it never matters. There is (for each decision problem,\neach lottery therein, and each agent) some threshold such that the\nagent would not be irrational if she simply ignored outcomes whose\nprobabilities lie below that threshold. (Smith 2014:\n472–474) \nSuppose we accept the claim that infinite precision is not\nrequired in decision theory. This would entail, per\nSmith’s argument, that it is rationally permissible to\nignore probabilities smaller than \\(\\epsilon\\). However, to ensure that\nthe decision maker never pays a fortune for playing the St. Petersburg\ngame it seems that Smith would have to defend the stronger claim that\ndecision makers are rationally required to ignore small\nprobabilities, i.e., that it is not permissible to not ignore them.\nDecision makers who find themselves in agreement with Smith’s\nview run a risk of paying a very large amount for playing the St.\nPetersburg game without doing anything deemed to be irrational by RNP.\nThis point is important because it is arguably more difficult to show\nthat decision makers are rationally required to avoid\n“infinite precision” in decisions in which this is an\nattainable and fully realistic goal, such as the St. Petersburg game.\nFor a critique of RNP and a discussion of some related issues, see\nHájek (2014). \nAnother objection to RNP has been proposed by Yoaav Isaacs (2016). He\nshows that RNP together with an additional principle endorsed by Smith\n(Weak Consistency) entail that the decision maker will sometimes take\narbitrarily much risk for arbitrarily little reward. \nLara Buchak (2013) proposes what is arguably a more elegant version of\nthis solution. Her suggestion is that we should assign exponentially\nless weight to small probabilities as we calculate an\noption’s value. A possible weighting function r discussed\nby Buchak is \\(r(p) = p^2.\\) Her proposal is, thus, that if the\nprobability is \\(\\frac{1}{8}\\) that you win $8 in addition to what you\nalready have, and your utility of money increases linearly, then\ninstead of multiplying your gain in utility by \\(\\frac{1}{8},\\) you\nshould multiply it by \\((\\frac{1}{8})^2 =\\frac{1}{64}.\\) Moreover, if\nthe probability is \\(\\frac{1}{16}\\) that you win $16 in addition to\nwhat you already have, you should multiply your gain by\n\\(\\frac{1}{256},\\) and so on. This means that small probabilities\ncontribute very little to the risk-weighted expected\nutility. \nBuchak’s proposal vaguely resembles the familiar idea that our\nmarginal utility of money is decreasing. As stressed by Cramér\nand Daniel Bernoulli, more money is always better than less, but the\nutility gained from each extra dollar is decreasing. According to\nBuchak, the weight we should assign to an outcome’s probability\nis also nonlinear: Small probabilities matter less the smaller they\nare, and their relative importance decrease exponentially: \nThe intuition behind the diminishing marginal utility analysis of risk\naversion was that adding money to an outcome is of less value the more\nmoney the outcome already contains. The intuition behind the present\nanalysis of risk aversion is that adding probability to an outcome is\nof more value the more likely that outcome already is to obtain.\n(Buchak 2014: 1099.) \nBuchak notes that this move does not by itself solve the St.\nPetersburg paradox. For reasons that are similar to those Menger (1934\n[1979]) mentions in his comment on Bernoulli’s solution, the\nparadox can be reintroduced by adjusting the outcomes such that the\nsum increases linearly (for details, see Buchak 2013: 73–74).\nBuchak is, for this reason, also committed to RNP, i.e., the\ncontroversial assumption that there will be some probability so small\nthat it does not make any difference to the overall value of the\ngamble. \nAnother worry is that because Buchak rejects the principle of\nmaximizing expected utility and replaces it with the principle of\nrisk-weighted maximizing expected utility, many of the stock\nobjections decision theorists have raised against violations of the\nexpected utility principle can be raised against her principle as\nwell. For instance, if you accept the principle of risk-weighted\nmaximizing expected utility, you have to reject the independence\naxiom. This entails that you can be exploited in some cleverly\ndesigned pragmatic argument. See Briggs (2015) for a discussion of\nsome objections to Buchak’s theory. \nIn the Petrograd game introduced by Colyvan (2008) the player wins $1\nmore than in the St. Petersburg game regardless of how many times the\ncoin is flipped. So instead of winning 2 utility units if the coin\nlands heads on the first toss, the player wins 3; and so on. See\n Table 1. \nIt seems obvious that the Petrograd game is worth more than the St.\nPetersburg game. However, it is not easy to explain why. Both games\nhave infinite expected utility, so the expected utility principle\ngives the wrong answer. It is not true that the Petrograd game is\nworth more than the St. Petersburg game because its expected utility\nis higher; the two games have exactly the same expected utility. This\nshows that the expected utility principle is not universally\napplicable to all risky choices, which is an interesting observation\nin its own right. \nIs the Petrograd game worth more than the St. Petersburg game because\nthe outcomes of the Petrograd game dominate those of the St.\nPetersburg game? In this context, dominance means that the player will\nalways win $1 more regardless of which state of the world turns out to\nbe the true state, that is, regardless of how many times the coin is\nflipped. The problem is that it is easy to imagine versions of the\nPetrograd game to which the dominance principle would not be\napplicable. Imagine, for instance, a version of the Petrograd game\nthat is exactly like the one in\n Table 1\n except that for some very improbable outcome (say, if the coin lands\nheads for the first time on the 100th flip) the player wins\n1 unit less than in the St. Petersburg game. This game, the\nPetrogradskij game, does not dominate the St. Petersburg game.\nHowever, since it is almost certain that the player will be better off\nby playing the Petrogradskij game a plausible decision theory should\nbe able to explain why the Petrogradskij game is worth more than the\nSt. Petersburg game. \nColyvan claims that we can solve this puzzle by introducing a new\nversion of expected utility theory called Relative Expected Utility\nTheory (REUT). According to REUT we should calculate the difference in\nexpected utility between the two options for each possible outcome.\nFormally, the relative expected utility (\\(\\reu\\)) of act \\(A_k\\) over\n\\(A_l\\) is \nAccording to Colyvan, it is rational to choose \\(A_k\\) over \\(A_l\\) if\nand only if \\(\\reu(A_k,A_l) \\gt 0\\). \nColyvan’s REUT neatly explains why the Petrograd game is worth\nmore than the St. Petersburg game because the relative expected\nutility is 1. REUT also explains why the Petrogradskij game is worth\nmore than the St. Petersburg game: the difference in expected utility\nis \\(1 - (\\frac{1}{2})^{100}\\) which is > 0. \nHowever, Peterson (2013) notes that REUT cannot explain why the\nLeningradskij game is worth more than the Leningrad\ngame (see\n Table 2).\n The Leningradskij game is the version of the Petrograd game in which\nthe player in addition to receiving a finite number of units of\nutility also gets to play the St. Petersburg game (SP) if the coin\nlands heads up in the second round. In the Leningrad game the player\ngets to play the St. Petersburg game (SP) if the coin lands heads up\nin the third round. \nIt is obvious that the Leningradskij game is worth more than the\nLeningrad game because the probability that the player gets to play SP\nas a bonus (which has infinite expected utility) is higher. However,\nREUT cannot explain why. The difference in expected utility for the\nstate that occurs with probability \\(\\frac{1}{4}\\) in\n Table 2\n is \\(-\\infty\\) and it is \\(+\\infty\\) for the state that occurs with\nprobability \\(\\frac{1}{8}.\\) Therefore, because \\(p \\cdot \\infty =\n\\infty\\) for all positive probabilities \\(p\\), and “\\(\\infty -\n\\infty\\)” is undefined in standard analysis, REUT cannot be\napplied to these games. \nBartha (2016) proposes a more complex version of Colyvan’s\ntheory designed to address the worry outlined above. His suggestion is\nto ask the agent to compare a “problematic” game to a\nlottery between two other games. If, for instance,\nPetrograd+ is the game in which the player always wins 2\nunits more than in the St. Petersburg game regardless of how many\ntimes the coin is tossed, then the player could compare the Petrograd\ngame to a lottery between Petrograd+ and the St. Petersburg\ngame. By determining for what probabilities p a lottery in\nwhich one plays Petrograd+ with probability p and\nthe St. Petersburg game with probability \\(1-p\\) is better than\nplaying the Petrograd game for sure one can establish a measure of the\nrelative value of Petrograd as compared to Petrograd+ or\nSt. Petersburg. (For details, see Sect. 5 in Bartha 2016. See also\nColyvan and Hájek’s 2016 discussion of Bartha’s\ntheory.)  \nLet us also mention another, quite simple variation of the original\nSt. Petersburg game, which is played as follows (see Peterson 2015:\n87): A manipulated coin\nlands heads up with probability 0.4 and the player wins a prize worth\n\\(2^n\\) units of utility, where n is the number of times the\ncoin was tossed. This game, the Moscow game, is more likely to yield a\nlong sequence of flips and is therefore worth more than the St.\nPetersburg game, but the expected utility of both games is the same,\nbecause both games have infinite expected utility. It might be\ntempting to say that the Moscow game is more attractive because the\nMoscow game stochastically dominates the St. Petersburg game.\n(That one game stochastically dominates another game means that for\nevery possible outcome, the first game has at least as high a\nprobability of yielding a prize worth at least u units of\nutility as the second game; and for some u, the first game\nyields u with a higher probability than the second.) However,\nthe stochastic dominance principle is inapplicable to games in which\nthere is a small risk that the player wins a prize worth slightly less\nthan in the other game. We can, for instance, imagine that if the coin\nlands heads on the 100th flip the Moscow game pays one unit\nless than the St. Petersburg game; in this scenario neither game\nstochastically dominates the other. Despite this, it still seems\nreasonable to insist that the game that is almost certain to yield a\nbetter outcome (in the sense explained above) is worth more. The\nchallenge is to explain why in a robust and non-arbitrary way. \nThe Pasadena paradox introduced by Nover and Hájek (2004) is\ninspired by the St. Petersburg game, but the pay-off schedule is\ndifferent. As usual, a fair coin is flipped n times until it\ncomes up heads for the first time. If n is odd the player wins\n\\((2^n)/n\\) units of utility; however, if n is even the player\nhas to pay \\((2^n)/n\\) units. How much should one be willing\nto pay for playing this game? \nIf we sum up the terms in the temporal order in which the outcomes\noccur and calculate expected utility in the usual manner we find that\nthe Pasadena game is worth: \n\n\\[\\begin{align}\n\\frac{1}{2}\\cdot\\frac{2}{1} - \\frac{1}{4}\\cdot\\frac{4}{2} + \\frac{1}{8}\\cdot\\frac{8}{3}\n &- \\frac{1}{16}\\cdot\\frac{16}{4} + \\frac{1}{32}\\cdot\\frac{16}{5} - \\cdots  \\\\\n &= 1 - \\frac{1}{2} + \\frac{1}{3} - \\frac{1}{4} + \\frac{1}{5} - \\cdots \\\\\n &= \\sum_n \\frac{(-1)^{n-1}}{n}\n\\end{align}\\]\n\n  \nThis infinite sum converges to ln 2 (about 0.69 units of\nutility). However, Nover and Hájek point out that we would\nobtain a very different result if we were to rearrange the order in\nwhich the very same numbers are summed up. Here is one of many\npossible examples of this mathematical fact: \nThis is, of course, not news to mathematicians. The infinite sum\nproduced by the Pasadena game is known as the alternating harmonic\nseries, which is a conditionally convergent series. (A\nseries \\(a_n\\) is conditionally convergent if \\(\\sum_{j=1}^{\\infty}\na_n\\) converges but \\(\\sum_{j=1}^{\\infty} \\lvert a_n\\rvert\\)\ndiverges.) Because of a theorem known as the Riemann rearrangement\ntheorem, we know that if an infinite series is conditionally\nconvergent, then its terms can always be rearranged such the sum\nconverges to any finite number, or to \\(+\\infty\\) or to\n\\(-\\infty\\). \nNover and Hájek’s point is that it seems\narbitrary to sum up the terms in the Pasadena game in the\ntemporal order produced by the coin flips. To see why, it is helpful\nto imagine a slightly modified version of the game. In their original\npaper, Nover and Hájek ask us to imagine that: \nWe toss a fair coin until it lands heads for the first time. We have\nwritten on consecutive cards your pay-off for each possible outcome.\nThe cards read as follows: (Top card) If the first =heads is on toss\n#1, we pay you $2. […] By accident, we drop the cards, and\nafter picking them up and stacking them on the table, we find that\nthey have been rearranged. No matter, you say—obviously the game\nhas not changed, since the pay-off schedule remains the same. The\ngame, after all, is correctly and completely specified by the\nconditionals written on the cards, and we have merely changed the\norder in which the conditions are presented. (Nover and Hájek\n2004: 237–239) \nUnder the circumstances described here, we seem to have no\nreason to prefer any particular order in which to sum up the terms of\nthe infinite series. So is the expected value of Pasadena game \\(\\ln\n2\\) or \\(\\frac{1}{2}(\\ln 2)\\) or \\(\\frac{1}{3}\\) or \\(-\\infty\\) or\n345.68? All these suggestions seem equally arbitrary. Moreover, the\nsame holds true for the Altadena game, in which every payoff is\nincreased by one dollar. The Altadena game is clearly better than then\nPasadena game, but advocates of expected utility theory seem unable to\nexplain why. \nThe literature on the Pasadena game is extensive. See, e.g.,\nHájek and Nover (2006), Fine (2008), Smith (2014), and Bartha\n(2016). A particularly influential solution is due to\nEaswaran (2008). He introduces a distinction between a strong and a\nweak version of the expected utility principle, inspired by the\nwell-known distinction between the strong and weak versions of the law\nof large numbers. According to the strong law of large numbers, the\naverage utility of a game converges to its expected utility with\nprobability one as the number of iterations goes to infinity. The weak\nlaw of large numbers holds that for a sufficiently large set of trials\nthe probability can be made arbitrarily small that that the average\nutility will not differ from the expected utility by more than some\nsmall pre-specified amount. So according to the weak expected utility\nprinciple,  \nby fixing in advance a high enough number of n plays, the\naverage payoff per play can be almost guaranteed to be arbitrarily\nclose to ln 2,  \nwhile the strong version of the principle entails that  \nif one player keeps getting to decide whether to play again or quit,\nthen she can almost certainly guarantee as much profit as she wants,\nregardless of the (constant) price per play. (Easwaran 2008: 635)  \nEaswaran’s view is that the weak expected utility principle\nshould guide the agent’s choice and that the fair price to pay\nis ln 2. \nHowever, Easwaran’s solution cannot be generalized to other\ngames with slightly different payoff schemes. Bartha (2016: 805)\ndescribes a version of the Pasadena game that has no expected value.\nIn this game, the Arroyo game, the player wins \\(-1^{n+1}(n+1)\\) with\nprobability \\(p_n = \\frac{1}/{(n+1)}\\). If we calculate the expected\nutility in the order in which the outcomes are produced, we get the\nsame result as for the Pasadena game: \\(1 - \\frac{1}{2} + \\frac{1}{3}\n- \\frac{1}{4} \\cdots\\) For reasons explained (and proved) by Bartha,\nthe Arroyo game has no weak expected utility. \nIt is also worth keeping in mind that Pasadena-like scenarios can\narise in non-probabilistic contexts (see Peterson 2013). Imagine, for\ninstance, an infinite population in which the utility of individual\nnumber j is \\(\\frac{(-1)^{j-1}}{j}\\). What is the total utility\nof this population? Or imagine that you are the proud owner of a\nJackson Pollock painting. An art dealer tells you the overall\naesthetic value of the painting is the sum of some of its\nparts. You number the points in the painting with arbitrary\nnumbers 1, 2, 3, … (perhaps by writing down the numbers on\ncards and then dropping all cards on the floor); the aesthetic value\nof each point j is \\(\\frac{(-1)^{j-1}}{j}\\). What is the total\naesthetic value of the painting? These examples are non-probabilistic\nversions of the Pasadena problem, to which the expected utility\nprinciple is inapplicable. There is no uncertainty about any state\nof nature; the decision maker knows for sure\nwhat the world is like. This means that\nEaswaran’s distinction between weak and strong expectations is\nnot applicable. \nAlthough some of these problems may appear to be somewhat esoteric, we\ncannot dismiss them. All Pasadena-like problems are vulnerable to the\nsame contagion problem as the St Petersburg game (see\n section 2).\n Hájek and Smithson offer the following colorful\nillustration: \nYou can choose between pizza and Chinese for dinner. Each\noption’s desirability depends on how you weigh probabilistically\nvarious scenarios (burnt pizza, perfectly cooked pizza,…\nover-spiced Chinese, perfectly spiced Chinese…) and the\nutilities you accord them. Let us stipulate that neither choice\ndominates the other, yet it should be utterly straightforward for you\nto make a choice. But it is not if the expectations of pizza and\nChinese are contaminated by even a miniscule [sic] assignment of\ncredence to the Pasadena game. If the door is opened to it just a\ncrack, it kicks the door down and swamps all expected utility\ncalculations. You cannot even choose between pizza and Chinese.\n(Hájek and Smithson 2012: 42, emph. added.) \nColyvan (2006) suggests that we should bite the bullet on the Pasadena\ngame and accept that it has no expected utility. The\ncontagion problem shows that if we were to do so, we would have to\nadmit that the principle of maximizing expected utility would be\napplicable to nearly no decisions. Moreover, because the\ncontagion problem is equally applicable to all games discussed in this\nentry (St. Petersburg, Pasadena, Arroyo, etc.) it seems that all these\nproblems may require a unified solution. \nFor hundreds of years, decision theorists have agreed that rational\nagents should maximize expected utility. The discussion has mostly\nbeen focused on how to interpret this principle, especially for\nchoices in which the causal structure of the world is unusual.\nHowever, until recently no one has seriously questioned that the\nprinciple of maximizing expected utility is the right principle to\napply. The rich and growing literature on the many puzzles inspired by\nthe St. Petersburg paradox indicate that this might have been a\nmistake. Perhaps the principle of maximizing expected utility should\nbe replaced by some entirely different principle?","contact.mail":"martinpeterson@tamu.edu","contact.domain":"tamu.edu"}]
