[{"date.published":"2015-03-13","url":"https://plato.stanford.edu/entries/epistemic-game/","author1":"Eric Pacuit","author1.info":"http://www.philosophy.umd.edu/people/pacuit","author2.info":"http://www.philosophie1.uni-bayreuth.de/en/team/roy/","entry":"epistemic-game","body.text":"\n\nFoundational work in game theory aims at making explicit the\nassumptions that underlie the basic concepts of the\ndiscipline. Non-cooperative game theory is the study of individual,\nrational decision making in situations of strategic interaction. This\nentry presents the epistemic foundations of non-cooperative\ngame theory (this area of research is called epistemic game\ntheory).\n\nEpistemic game theory views rational decision making in games as\nsomething not essentially different from rational decision making\nunder uncertainty. As in Decision Theory (Peterson 2009), to choose\nrationally in a game is to select the “best” action in\nlight of one’s beliefs or information. In a decision problem,\nthe decision maker’s beliefs are about a passive state of\nnature, the state of which determines the consequences of her\nactions. In a game, the consequences of one’s decision depend on\nthe choices of the other agents involved in the situation\n(and possibly the state of nature). Recognizing this—i.e., that\none is interacting with other agents who try to choose the best course\nof action in the light of their own\nbeliefs—brings higher-order information into the\npicture. The players’ beliefs are no longer about a passive or\nexternal environment. They concern the choices and the\ninformation of the other players. What one expects of one’s\nopponents depends on what one thinks the others expect from her, and\nwhat the others expect from a given player depends on what they think\nher expectations about them are.\n\nThis entry provides an overview of the issues that arise when one\ntakes this broadly decision-theoretic view on rational decision making\nin games. After some general comments about information in games, we\npresent the formal tools developed in epistemic game theory and\nepistemic logic that have been used to understand the role of\nhigher-order information in interactive decision making. We then show\nhow these tools can be used to characterize known “solution\nconcepts” of games in terms of rational decision making in\nspecific informational contexts. Along the way, we highlight a number\nof philosophical issues that arise in this area.\n\nA game refers to any interactive situation involving a\ngroup of self-interested agents, or players. The defining\nfeature of a game is that the players are engaged in an\n“interdependent decision problem” (Schelling\n1960). Classically, the mathematical description of a game\nincludes following components: The players. In this entry, we only consider games with a\nfinite set of players. We use \\(\\Agt\\) to denote the set of players in a\ngame, and \\(i, j,\\ldots\\) to denote its elements. The feasible options (typically called actions\nor strategies) for each player. Again, we only consider games\nwith finitely many feasible options for each player. The players’ preferences over possible outcome. Here\nwe represent them as von Neumann-Morgenstern utility functions \\(u_i\\)\nassigning real-valued utilities to each outcome of the game. A game can have many other structural properties. It can\nbe represented as a single-shot or multi-stage decision problem, or it\ncan include simultaneous or stochastic moves. We start with games\nin strategic form without stochastic moves, and will\nintroduce more sophisticated games as we go along in the entry. In a\nstrategic game, each player \\(i\\) can choose from a (finite) set \\(S_i\\)\nof options, also called actions or strategies. The combination of all\nthe players’ choices, denoted \\({\\mathbf{s}}\\), is called\na strategy profile, or outcome of the game. We write\n\\({\\mathbf{s}}_i\\) for \\(i\\)’s component in \\({\\mathbf{s}}\\), and\n\\({\\mathbf{s}}_{-i}\\) for the profile of strategies for all agents other\nthan \\(i\\). Finally, we write \\(\\Pi_{i \\in \\Agt} S_i\\) for the set of all\nstrategy profiles of a given game. Putting everything together, a\nstrategic game is a tuple \\(\\langle \\A, \\{S_i, u_i\\}_{i\\in\\A}\\rangle\\)\nwhere \\(\\A\\) is a finite set of players, for each \\(i\\in\\A\\), \\(S_i\\) is a\nfinite set of actions and \\(u_i:\\Pi_{i\\in\\A} S_i\\rightarrow\\mathbb{R}\\)\nis player \\(i\\)’s utility function. The game in Figure 1 is an example\nof a game in strategic form. There are two players, Ann and Bob, and\neach has to choose between two options: \\(\\Agt = \\{Ann, Bob\\}\\),\n\\(S_{Ann} = \\{u, d\\}\\) and \\(S_{Bob} = \\{l, r\\}\\). The value of \\(u_{Ann}\\)\nand \\(u_{Bob}\\), representing their respective preferences over the\npossible outcomes of the game, are displayed in the cell of the\nmatrix. If Bob chooses \\(l\\), for instance, Ann prefers the outcome she\nwould get by choosing \\(u\\) to the one she would get by choosing \\(d\\),\nbut this preference is reversed in the case Bob chooses \\(r\\). This game\nis called a “pure coordination game” in the literature\nbecause the players have a clear interest in coordinating their\nchoices—i.e., on \\((u, l)\\) or \\((d, r)\\)—but they are\nindifferent about which way they coordinate their choices. Figure 1: A coordination game In a game, no single player has total control over which outcome\nwill be realized at the end of the interaction. This depends on the\ndecisions of all players. Such abstract models\nof interdependent decisions are capable of representing a\nwhole array of social situations, from strictly competitive to\ncooperative ones. See Ross (2010) for more details about classical\ngame theory and key references. The central analytic tool of classical game theory are solution\nconcepts. They provide a top-down perspective specifying which\noutcomes of a game are deemed “rational”. This can be\ngiven both a prescriptive or a predictive\nreading. Nash equilibrium is one of the most well-known solution\nconcepts, but we will encounter others below. In the game above, for\ninstance, there are two Nash equilibria in so-called “pure\nstrategies.”[1]\n These are the two coordination profiles: \\((u,\nl)\\) and \\((d, r)\\). From a prescriptive point of view, a solution concept is a set of\npractical recommendations—i.e., recommendations about what the\nplayers should do in a game. From a predictive point of view, solution\nconcepts describe what the players will actually do in certain\ninteractive situation. Consider again the pure strategy Nash\nequilibria in the above example. Under a prescriptive interpretation,\nit singles out what players should do in the game. That is,\nAnn and Bob should either play their component of \\((u, l)\\) or \\((d,\nr)\\). Under the predictive interpretation, these profiles are the ones\nthat one would expect to observe in a actual play of that game. This solution-concept-driven perspective on games faces many\nfoundational difficulties, which we do not survey here. The interested\nreader can consult Ross (2010), Bruin (2010), and Kadane & Larkey\n(1983) for a discussion. Epistemic game theory is a broad area of research encompassing a\nnumber of different mathematical frameworks that are used to analyze\ngames. The details of the frameworks are different, but they do share\na common perspective. In this Section, we discuss two key features of\nthis common perspective. This point of view is nicely explained by Robert Stalnaker: There is no special concept of rationality for decision making in a\nsituation where the outcomes depend on the actions of more than one\nagent. The acts of other agents are, like chance events, natural\ndisasters and acts of God, just facts about an uncertain world that\nagents have beliefs and degrees of belief about. The utilities of\nother agents are relevant to an agent only as information that,\ntogether with beliefs about the rationality of those agents, helps to\npredict their actions. (Stalnaker 1996: 136) In other words, epistemic game theory can be seen as an attempt to\nbring back the theory of decision making in games to its\ndecision-theoretic roots. In decision theory, the decision-making units are individuals with\npreferences over the possible consequences of their actions. Since the\nconsequence of a given action depend on the state of the environment,\nthe decision-maker’s beliefs about the state of the environment\nare crucial to assess the rationality of a particular decision. So,\nthe formal description of a decision problem includes the possible\noutcomes and states of the environment, the decision maker’s\npreferences over these outcome, and a description of the\ndecision maker’s beliefs about the state of nature\n(i.e., the decision maker’s doxastic state). A\ndecision-theoretic choice rule can be used to make\nrecommendations to the decision maker about what she should\nchoose (or to predict what the decision-maker will choose). A\nstandard example of a choice rule is maximization of (subjective)\nexpected utility, underlying the Bayesian view of\nrationality. It presupposes that the decision maker’s\npreferences and beliefs can be represented by numerical utilities and\nprobabilities, \nrespectively.[2]\n (We postpone the formal\nrepresentation of this, and the other choice rules such as weak and\nstrict dominance, until we have presented the formal models of beliefs\nin games in Section 2.) From an epistemic point of view, the classical ingredients of a\ngame (players, actions, outcomes, and preferences) are thus not enough\nto formulate recommendations or predictions about how the players\nshould or will choose. One needs to specify the (interactive) decision\nproblem the players are in, i.e., also the beliefs players\nhave about each other’s possible actions (and beliefs). In a\nterminology that is becoming increasingly popular in epistemic game\ntheory, games are played in specific contexts (Friedenberg\n& Meier 2010, Other Internet Resources), in which the players have\nspecific knowledge and/or beliefs about each other. The\nrecommendations and/or predictions that are appropriate for one\ncontext may not transfer to another, even if the underlying situation\nmay correspond to precisely the same strategic game. There are various types of information that a player has access to\nin a game situation. For instance, a player may have imperfect information about the play of the game (which moves have\nbeen played?); incomplete information about the structure of the game (what are\nthe actions/payoffs?); strategic information (what will the other players do?);\nor higher-order information (what are the other players\nthinking?). While all types of uncertainty may play a role in an epistemic\nanalysis of a game, a distinguishing feature of epistemic game theory\nis an insistence that rational decisions are assessed in terms of the\nplayers’ preferences and beliefs about what their\nopponents are going to do. Again we turn to Stalnaker to summarize\nthis point of view: …There are no special rules of rationality telling one what\nto do in the absence of degrees of belief [about the opponents’\nchoices], except this: decide what you believe, and then maximize\nexpected utility. (Stalnaker 1996: 136) The four types of uncertainty in games introduced above are\nconceptually important, but not necessarily exhaustive nor mutually\nexclusive. John Harsanyi, for instance, argued that all uncertainty\nabout the structure of the game, that is all possible incompleteness\nin information, can be reduced to uncertainty about the payoffs\n(Harsanyi 1967–68). (This was later formalized and proved by\nStuart and Hu 2002). In a similar vein, Kadane & Larkey (1982)\nargue that only strategic uncertainty is relevant for the assessment\nof decision in game situations. Contemporary epistemic game theory\ntakes the view that, although it may ultimately be reducible to\nstrategic uncertainty, making higher-order uncertainty explicit can\nclarify a great deal of what interactive or strategic rationality\nmeans. The crucial difference from the classical\n“solution-concept” analysis of a game is that epistemic\ngame theory takes a bottom-up perspective. Once the context of the\ngame is specified, the rational outcomes are derived, given\nassumptions about how the players are making their choices and what\nthey know and believe about how the others are choosing. In the\nremainder of this section, we briefly discuss some general issues that\narise from taking an epistemic perspective on games. We postpone\ndiscussion of higher-order and strategic uncertainty until Sections 3,\n4 and 5. It is standard in the game theory literature to distinguish three\nstages of the decision making process: ex ante, ex\ninterim and ex post. At one extreme is the ex\nante stage where no decision has been made yet. The other extreme\nis the ex post stage where the choices of all players are\nopenly disclosed. In between these two extremes is the ex\ninterim stage where the players have made their decisions, but\nthey are still uninformed about the decisions and intentions of the\nother players. These distinctions are not intended to be sharp. Rather, they\ndescribe various stages of information disclosure during the\ndecision-making process. At the ex-ante stage, little is\nknown except the structure of the game, who is taking part, and\npossibly (but not necessarily) some aspect of the agents’\ncharacter. At the ex-post stage the game is basically over:\nall player have made their decision and these are now irrevocably out\nin the open. This does not mean that all uncertainty is removed as an\nagent may remain uncertain about what exactly the others were\nexpecting of her. In between these two extreme stages lies a whole\ngradation of states of information disclosure that we loosely refer to\nas “the” ex-interim stage. Common to these stages\nis the fact that the agents have made a decision, although\nnot necessarily an irrevocable one. In this entry, we focus on the ex interim stage of\ndecision making. This is in line with much of the literature on the\nepistemic foundations of game theory as it allows for a\nstraightforward assessment of the agents’ rationality given\ntheir expectations about how their opponents will choose. Focusing on\nthe ex interim stage does raise some interesting questions\nabout possible correlations between a player’s strategy\nchoice, what Stalnaker (1999) calls “active knowledge”,\nand her information about the choices of others, her “passive\nknowledge” (idem). The question of how a player should\nreact, that is eventually revise her decision, upon learning that she\ndid not choose “rationally” is an interesting and\nimportant one, but we do not discuss it in the entry. Note that this\nquestion is different from the one of how agents should revise their\nbeliefs upon learning that others did not choose\nrationally. This second question is very relevant in games in which\nplayers choose sequentially, and will be addressed\nin Section 4.2.3. A natural question to ask about any mathematical model of\na game situation is how does the analysis change if the players\nare uncertain about some of the parameters of the model? This\nmotivated Harsanyi’s fundamental work introducing the notion of\na game-theoretic type and defining a Bayesian\ngame in Harsanyi 1967–68. Using these ideas, an\nextensive literature has developed that analyzes games in which\nplayers are uncertain about some aspect of the game. (Consult\nLeyton-Brown & Shoham (2008: ch. 7) for a concise summary of the\ncurrent state-of-affairs and pointers to the relevant literature.) One\ncan naturally wonder about the precise relationship between this\nliterature and the literature we survey in this entry on the epistemic\nfoundations of game theory. Indeed, the foundational literature we\ndiscuss here largely focuses on Harsanyi’s approach to modeling\nhigher-order beliefs (which we discuss in Section\n2.3). There are two crucial differences between the literature on\nBayesian games and the literature we discuss in this entry (cf. the\ndiscussion in Brandenburger 2010: sec. 4 and 5). In a Bayesian game, players are uncertain about the payoffs of the\ngame, what other players believe are the correct payoffs, what other\nplayers believe that the other players believe about the payoffs, and\nso on, and this is the only source of uncertainty. That is, the\nplayers’ (higher-order) beliefs about the payoffs in a game\ncompletely determine the (higher-order) beliefs about the other\naspects of the game. In particular, if a player comes to know\nthe payoffs of the other players, then that player is certain (and\ncorrect) about the possible (rational) choices of the other\nplayers.[3] It is assumed that all players choose optimally given their\ninformation. That is, all players choose a strategy that maximizes\ntheir expected utility given their beliefs about the game, beliefs\nabout what other players believe about the game, and so on. This\nmeans, in particular, that players do not entertain the possibility\nthat their opponents may choose “irrationally.” Note that these assumptions are not inherent in the formalism that\nHarsanyi used to represent the players’ beliefs in a game of\nincomplete information. Rather, they are better described as\nconventions followed by Harsanyi and subsequent researchers studying\nBayesian games. In a game with imperfect information (see Ross 2010 for a\ndiscussion), the players may not be perfectly informed about the moves\nof their opponents or the outcome of chance moves by nature. Games\nwith imperfect information can be pictured as follows: Figure 2 The interpretation is that the decision made at the first node\n(\\(d_0\\)) is forgotten, and so the decision maker is uncertain about\nwhether she is at node \\(d_1\\) or \\(d_2\\). See Osborne (2003: ch. 9 &\n10) for the general theory of games with imperfect information. In\nthis section, we briefly discuss a foundational issue that arises in\ngames with imperfect information. Kuhn (1953) introduced the distinction between perfect\nand imperfect recall in games with imperfect\ninformation. Roughly, players have perfect recall provided they\nremember all of their own past moves (see Bonanno 2004; Kaneko &\nKline 1995 for general discussions of the perfect recall\nassumption). It is standard in the game theory literature to assume\nthat all players have perfect recall (i.e., they may be uncertain\nabout previous choices of their opponents or nature, but they do\nremember their own moves). As we noted in Section 1.3, there are\ndifferent stages to the decision making process. Differences between\nthese stages become even more pronounced in extensive games in which\nthere is a temporal dimension to the game. There are two ways to think\nabout the decision making process in an extensive game (with imperfect\ninformation). The first is to focus on the initial “planning\nstage”. That is, initially, the players settle on a strategy\nspecifying the (possibly random) move they will make at each of their\nchoice nodes (this is the players’ global\nstrategy). Then, the players start making their respective moves\n(following the strategies which they have committed to without\nreconsidering their options at each choice node). Alternatively, we\ncan assume that the players make “local judgements” at\neach of their choice nodes, always choosing the best option given the\ninformation that is currently available to them. A well-known theorem\nof Kuhn (1953) shows that if players have perfect recall, then a\nstrategy is globally optimal if, and only if, it is locally optimal\n(see Brandenburger 2007 for a self-contained presentation of this\nclassic result). That is, both ways of thinking about the decision\nmaking process in extensive games (with imperfect information) lead to\nthe same recommendations/predictions. The assumption of perfect recall is crucial for Kuhn’s\nresult. This is demonstrated by the well-known absent-minded\ndriver’s problem of Piccione and Rubinstein\n(1997a). Interestingly, their example is one where a decision maker\nmay be tempted to change his strategy after the initial planning\nstage, despite getting no new information. They describe the\nexample as follows: An individual is sitting late at night in a bar planning his\nmidnight trip home. In order to get home he has to take the highway\nand get off at the second exit. Turning at the first exit leads into a\ndisastrous area (payoff 0). Turning at the second exit yields the\nhighest reward (payoff 4). If he continues beyond the second exit, he\ncannot go back and at the end of the highway he will find a motel\nwhere he can spend the night (payoff 1). The driver is absentminded\nand is aware of this fact. At an intersection, he cannot tell whether\nit is the first or the second intersection and he cannot remember how\nmany he has passed (one can make the situation more realistic by\nreferring to the 17th intersection). While sitting at the bar, all he\ncan do is to decide whether or not to exit at an\nintersection. (Piccione & Rubinstein 1997a: 7) The decision tree for the absent-minded driver is pictured\nbelow: Figure 3 This problem is interesting since it demonstrates that there is a\nconflict between what the decision maker commits to do while planning\nat the bar and what he thinks is best at the first intersection: Planning stage:\nWhile planning his trip home at the bar,\nthe decision maker is faced with a choice between “Continue;\nContinue” and “Exit”. Since he cannot distinguish\nbetween the two intersections, he cannot plan to “Exit” at\nthe second intersection (he must plan the same behavior at both \\(X\\)\nand \\(Y\\)). Since “Exit” will lead to the worst outcome\n(with a payoff of 0), the optimal strategy is “Continue;\nContinue” with a guaranteed payoff of 1. \nAction stage: \nWhen arriving at an intersection, the decision maker is faced with a\nlocal choice of either “Exit” or “Continue”\n(possibly followed by another decision). Now the decision maker knows\nthat since he committed to the plan of choosing “Continue”\nat each intersection, it is possible that he is at the second\nintersection. Indeed, the decision maker concludes that he is at the\nfirst intersection with probability 1/2. But then, his expected payoff\nfor “Exit” is 2, which is greater than the payoff\nguaranteed by following the strategy he previously committed to. Thus,\nhe chooses to “Exit”. This problem has been discussed by a number of different\nresearchers.[4]\n It is beyond the scope of this article to\ndiscuss the intricacies of the different analyses. An entire issue\nof Games and Economic Behavior (Volume 20, 1997) was devoted\nto the analysis of this problem. For a representative sampling of the\napproaches to this problem, see Kline (2002); Aumann, Hart, &\nPerry (1997); Board (2003); Halpern (1997); Piccione & Rubinstein\n(1997b). Mixed strategies play an important role in many game-theoretic\nanalyses. Let \\(\\Delta(X)\\) denote the set of probability measures over\nthe finite[5]\n set \\(X\\). A mixed strategy\nfor player \\(i\\), is an element \\(m_i\\in \\Delta(S_i)\\). If\n\\(m_i\\in\\Delta(S_i)\\) assigns probability 1 to an element \\(s_i\\in S_i\\),\nthen \\(m_i\\) is called a pure strategy (in such a case,\nI write \\(s_i\\) for \\(m_i\\)). Mixed strategies are incorporated into a\ngame-theoretic analysis as follows. Suppose that \\(G=\\langle N, \\{S_i,\nu_i\\}_{i\\in N}\\rangle\\) is a finite strategic game. The mixed\nextension of \\(G\\) is the strategic game in which the strategies\nfor player \\(i\\) are the mixed strategies in \\(G\\) (i.e., \\(\\Delta(S_i)\\))\nand the utility for player \\(i\\) (denoted \\(U_i\\)) of the joint mixed\nstrategy \\(m\\in \\Pi_{i\\in N} \\Delta(S_i)\\) is calculated in the obvious\nway (let \\(m(s)=m_1(s_1)\\cdot m_2(s_2)\\cdots m_n(s_n)\\) for \\(s\\in\n\\Pi_{i\\in N} S_i\\)): Thus, the solution space of a mixed extension of the game \\(G\\) is\nthe set \\(\\Pi_{i\\in N} \\Delta(S_i)\\). Despite their prominence in game theory, the interpretation of\nmixed strategies is controversial, as Ariel Rubinstein notes: We are reluctant to believe that our decisions are\nmade at random. We prefer to be able to point to a reason for each\naction we take. Outside of Las Vegas we do not spin\nroulettes. (Rubinstein 1991: 913). In epistemic game theory, it is more natural to\n work with an alternative interpretation of mixed strategies: A mixed\n strategy for player \\(i\\) is a representation of the beliefs\n of \\(i\\)’s opponent(s) about what she will do. This is nicely\n explained in Robert Aumann’s influential paper \n (Aumann 1987—see, especially, Section 6 of this paper \n  for a discussion of this interpretation of mixed strategies): An important feature of our approach is that it does not require\nexplicit randomization on the part of the players. Each player always\nchooses a definite pure strategy, with no attempt to randomize; the\nprobabilistic nature of the strategies reflects the uncertainties of\nother players about his choice. (Aumann 1987: 3) A model of a game is a structure that represents the\ninformational context of a given play of the game. The states, or\npossible worlds, in a game model describe a possible play of the\ngame and the specific information that influenced the\nplayers’ choices (which may be different at each state). This\nincludes each player’s “knowledge” of her own choice\nand opinions about the choices and “beliefs” of her\nopponents. A key challenge when constructing a model of a game is how\nto represent the different informational attitudes of the\nplayers. Researchers interested in the foundation of decision theory,\nepistemic and doxastic logic and, more recently, formal\nepistemology have developed many different formal models that can\ndescribe the many varieties of informational attitudes important for\nassessing the choice of a rational agent in a decision- or\ngame-theoretic situation. After discussing some general issues that arise when describing the\ninformational context of a game, we introduce the two main types of\nmodels that have been used to describe the players’ beliefs (and\nother informational attitudes) in a game situation: type\nspaces (Harsanyi 1967–68; Siniscalchi 2008) and the\nso-called Aumann- or Kripke-structures (Aumann\n1999a; Fagin, Halpern, Moses, & Vardi 1995). Although these two\napproaches have much in common, there are some important differences\nwhich we highlight below. A second, more fundamental, distinction\nfound in the literature is between “quantitative”\nstructures, representing “graded” attitudes (typically via\nprobability distributions), and “qualitative” structures\nrepresenting “all out” attitudes. Kripke structures are\noften associated with the former, and type spaces with the latter, but\nthis is not a strict classification. Informational contexts of games can include various forms of\nattitudes, from the classical knowledge and belief to robust\n(Stalnaker 1994) and strong (Battigalli & Siniscalchi 2002)\nbelief, each echoing in different notions of game-theoretical\nrationality. It is beyond the scope of this article to survey the\ndetails of this vast literature (cf. the next section for some\ndiscussion of this literature). Rather, we will introduce a general\ndistinction between hard and soft attitudes,\ndistinction mainly developed in dynamic epistemic logic (van Benthem\n2011), which proves useful in understanding the various philosophical\nissues raised by epistemic game theory. We call hard information, information that\nis veridical, fully introspective and not\nrevisable. This notion is intended to capture what the agents are\nfully and correctly certain of in a given interactive situation. At\nthe ex interim stage, for instance, the players have hard\ninformation about their own choice. They “know”\nwhich strategy they have chosen, they know that they know this, and no\nnew incoming information could make them change their opinion on\nthis. As this phrasing suggests, the term knowledge is often\nused, in absence of better terminology, to describe this very strong\ntype of informational attitude. Epistemic logicians and game theorist\nare well aware of the possible discrepancies between such hard\n“knowledge” and our intuitive or even philosophical\nunderstanding of this notion. In the present context is it sufficient\nto observe that hard information shares some of the\ncharacteristics that have been attributed to knowledge in the\nepistemological literature, for instance truthfulness. Furthermore,\nhard information might come closer to what has been called\n“implicit knowledge” (see Section\n5.3 below). In any case, it seems philosophically more\nconstructive to keep an eye on where the purported counter-intuitive\nproperties of hard information come into play in epistemic game\ntheory, rather than reject this notion as wrong or flawed at the\nupshot. Soft information is, roughly speaking, anything that is\nnot “hard”: it is not necessarily veridical, not\nnecessarily fully introspective and/or highly revisable in the\npresence of new information. As such, it comes much closer\nto beliefs. Once again, philosophical carefulness is in order\nhere. The whole range of informational attitudes that is labeled as\n“beliefs” indeed falls into the category of attitudes that\ncan be described as “regarding something as true”\n(Schwitzgebel 2010), among which beliefs, in the philosophical sense,\nseem to form a proper sub-category. The models introduced below describe the players’ hard and\nsoft information in interactive situations. They differ in their\nrepresentation of a state of the world, but they can all be broadly\ndescribed as “possible worlds models” familiar in much of\nthe philosophical logic literature. The starting point is a non-empty\n(finite or infinite) set \\(S\\) of states of nature describing\nthe exogenous parameters (i.e., facts about the physical\nworld) that do not depend on the agents’ uncertainties. Unless\notherwise specified, \\(S\\) is the set of possible outcomes of the games,\nthe set of all strategy \nprofiles.[6]\n Each player is assumed to\nentertain a number of possibilities, called possible\nworlds or simply (epistemic) states. These\n“possibilities” are intended to represent a possible way a\ngame situation may evolve. So each possibility will be associated with\na unique state of nature (i.e., there is a function from\npossible worlds to states of nature, but this function need not be\n1–1 or even onto). It is crucial for the analysis of rationality\nin games that there may be different possible worlds\nassociated with the same state of nature. Such possible worlds are\nimportant because they open the door to representing different state\nof information. Such state-based modeling naturally yields\na propositional view of the agents’ informational\nattitudes. Agents will have beliefs/knowledge\nabout propositions, which are also called events in\nthe game-theory literature, and are represented as sets of possible\nworlds. These basic modeling choices are not uncontroversial, but such\nissues are not our concern in this entry. We start with the models that are familiar to philosophical\nlogicians (van Benthem 2010) and computer scientists (Fagin et\nal. 1995). These models were introduced to game theory by Robert\nAumann (1976) in his seminal paper Agreeing to Disagree (see\nVanderschraaf & Sillari 2009, Section 2.3, for a discussion of\nthis result). First, some terminology: Given a set \\(W\\) of states, or\npossible worlds, let us call any subset \\(E\\subseteq W\\)\nan event or proposition. Given events \\(E\\subseteq W\\)\nand \\(F\\subseteq W\\), we use standard set-theoretic notation for\nintersection (\\(E\\cap F\\), read “\\(E\\) and \\(F\\)”), union\n(\\(E\\cup F\\), read “\\(E\\) or \\(F\\)”) and (relative) complement\n(\\(-{E}\\), read “not \\(E\\)”). We say that an event \\(E\\subseteq\nW\\) occurs at state \\(w\\) if \\(w\\in E\\). This terminology will be crucial\nfor studying the following models. \nDefinition 2.1 (Epistemic Model)\n Suppose that \\(G\\) is a strategic game, \\(S\\) is the set of strategy\nprofiles of \\(G\\), and \\(\\A\\) is the set of players. An epistemic\nmodel based on \\(S\\) and \\(\\Agt\\) is a triple \\(\\langle\nW,\\{\\Pi_i\\}_{i\\in\\Agt},\\sigma\\rangle\\), where \\(W\\) is a nonempty set,\nfor each \\(i\\in\\Agt\\), \\(\\Pi_i\\) is a\npartition[7]\n over \\(W\\) and \\(\\sigma:W\\rightarrow S\\).\n Epistemic models represent the informational context of a given\ngame in terms of possible configurations of states of the game and the\nhard information that the agents have about them. The function\n\\(\\sigma\\) assigns to each possible world a unique state of the game in\nwhich every ground fact is either true or false. If \\(\\sigma(w) =\n\\sigma(w')\\) then the two worlds \\(w,w'\\) will agree on all the ground\nfacts (i.e., what actions the players will choose) but, crucially, the\nagents may have different information in them. So, elements of \\(W\\)\nare richer, than the elements of \\(S\\) (more on this\nbelow). Given a state \\(w\\in W\\), the cell \\(\\Pi_i(w)\\) is called agent\n\\(i\\)’s information set. Following standard terminology,\nif \\(\\Pi_i(w)\\subseteq E\\), we say the agent \\(i\\) knows the\nevent \\(E\\) at state \\(w\\). Given an event \\(E\\), the event that agent \\(i\\)\nknows \\(E\\) is denoted \\(K_i(E)\\). Formally, we define for each agent \\(i\\)\na knowledge function assigning to every event \\(E\\) the event that the\nagent \\(i\\) knows \\(E\\): Definition 2.2 (Knowledge Function) \nLet \\(\\M=\\langle W,\\{\\Pi_i\\}_{i\\in\\A},\\sigma\\rangle\\) be an epistemic\nmodel. The knowledge function for agent \\(i\\) based on\n\\(\\M\\) is \\(K_i:\\pow(W)\\rightarrow\\pow(W)\\) with: where for any set \\(X\\), \\(\\pow(X)\\) is the powerset of \\(X\\). Remark 2.3\nIt is often convenient to\nwork with equivalence relations rather than partitions. In\nthis case, an epistemic model based on \\(S\\) and \\(\\Agt\\) can also be\ndefined as a triple \\(\\langle W,\\{\\sim_i\\}_{i\\in\\Agt},\\sigma \\rangle\\)\nwhere \\(W\\) and \\(\\sigma\\) are as above and for each \\(i\\in\\Agt\\),\n\\(\\sim_i\\subseteq W\\times W\\) is reflexive, transitive and\nsymmetric. Given such a model \\(\\langle W,\n\\{\\sim_i\\}_{i\\in\\Agt},\\sigma\\rangle\\), we write  for the equivalence class of \\(w\\). Since there is a\n1–1 correspondence between equivalence relations and\npartitions,[8]\n we will abuse notation and use \\(\\sim_i\\) and\n\\(\\Pi_i\\) interchangeably. \nApplying the above remark, an alternative definition of \\(K_i(E)\\) is\nthat \\(E\\) is true in all the states the agent \\(i\\) considers possible\n(according to \\(i\\)’s hard information). That is, \\(K_i(E)=\\{w\\mid\n[w]_i\\subseteq E\\}\\). Partitions or equivalence relations are intended to represent the\nagents’ hard information at each state. It is\nwell-known that the knowledge operator satisfies the properties of the\nepistemic logic \\(\\mathbf{S5}\\) (see Hendricks & Symons 2009 for a\ndiscussion). We do not discuss this and related issues here and\ninstead focus on how these models can be used to provide the\ninformational context of a game. An Example.  Consider the following coordination\n game between Ann (player 1) and Bob (player 2). As is well-known,\n there are two pure-strategy Nash equilibrium (\\((u,l)\\) and\n \\((d,r)\\)). Figure 4: A strategic coordination\ngame between Ann and Bob The utilities of the players are not important for us at this\nstage. To construct an epistemic model for this game, we need first to\nspecify what are the states of nature we will consider. For\nsimplicity, take them to be the set of strategy profiles\n\\(S=\\{(u,l),(d,l),(u,r),(d,l)\\}\\). The set of agents is of course\n\\(\\Agt=\\{A,B\\}\\). What will be the set of states \\(W\\)? We start by\nassuming \\(W=S\\), so there is exactly one possible world corresponding\nto each state of nature. This needs not be so, but here this will help\nto illustrate our point. There are many different partitions for Ann and Bob that we can use\nto complete the description of this simple epistemic model. Not all of\nthe partitions are appropriate for analyzing the ex interim\nstage of the decision-making process, though. For example, suppose\n\\(\\Pi_{A}=\\Pi_{B}=\\{W\\}\\) and consider the event \\(U=\\{(u,l),(u,r)\\}\\)\nrepresenting the situation where Ann chooses \\(u\\). Notice that\n\\(K_A(U)=\\emptyset\\) since for all \\(w\\in W\\), \\(\\Pi_A(w)\\not\\subseteq U\\),\nso there is no state where Ann knows that she chooses\n\\(u\\). This means that this model is appropriate for reasoning about\nthe ex ante stage rather than the ex interim\nstage. This is easily fixed with an additional technical assumption:\nSuppose \\(S\\) is a set of strategy profiles for some (strategic or\nextensive) game with players \\(\\A=\\{1,\\ldots,n\\}\\). A model \\(\\M=\\langle W, \\{\\Pi_i\\}_{i\\in \\A},\\sigma \\rangle\\) is said\nto be an ex interim epistemic model if for\nall \\(i\\in\\Agt\\) and \\(w,v\\in W\\), if \\(v\\in\\Pi_i(w)\\) then\n\\(\\sigma_i(w)=\\sigma_i(v)\\) where \\(\\sigma_i(w)\\) is the \\(i\\)th component of the\nstrategy profile \\(s\\in S\\) assigned to \\(w\\) by \\(\\sigma\\). An example of\nan ex interim epistemic model with states \\(W\\) is: \\(\\Pi_A=\\{\\{(u,l),(u,r)\\},\\{(d,l),(d,r)\\}\\}\\) and \\(\\Pi_B=\\{\\{(u,l),(d,l)\\},\\{(u,r),(d,r)\\}\\}\\). Note that this simply reinterprets the game matrix\nin Figure 1 as an epistemic model\nwhere the rows are Ann’s information sets and the columns are\nBob’s information sets. Unless otherwise stated, we will always\nassume that our epistemic models are ex interim. The class\nof ex interim epistemic models is very rich with models\ndescribing the (hard) information the agents have about their own\nchoices, the (possible) choices of the other players and\nhigher-order (hard) information (e.g., “Ann knows that Bob knows\nthat…”) about these decisions. We now look at the epistemic model described above in more\ndetail. We will often use the following diagrammatic representation of\nthe model to ease exposition. States are represented by nodes in a\ngraph where there is a (undirected) edge between states \\(w_i\\) and\n\\(w_j\\) when \\(w_i\\) and \\(w_j\\) are in the same partition cell. We use a\nsolid line labeled with \\(A\\) for Ann’s partition and a dashed\nline labeled with \\(B\\) for Bob’s partition (reflexive edges are\nnot represented for simplicity). The event \\(U=\\{w_1,w_3\\}\\)\nrepresenting the proposition “Ann decided to choose option\n\\(u\\)” is the shaded gray region: Figure 5 Notice that the following events are true at all states: \\(- K_B(U)\\): “Bob does not know that Ann decided to choose\n\\(u\\)” \\(K_B(K_A(U)\\vee K_A(-U))\\): “Bob knows that Ann knows whether\nshe has decided to choose \\(u\\)” \\(K_A(-K_B(U))\\): “Ann knows that Bob does not know that she\nhas decided to choose \\(u\\)” In particular, these events are true at state \\(w_1\\) where Ann has\ndecided to choose \\(u\\) (i.e., \\(w_1\\in U\\)). The first event makes sense\ngiven the assumptions about the available information at the ex\ninterim stage: each player knows their own choice but not the\nother players’ choices. The second event is a concrete example\nof another assumption about the available information: Bob has the\ninformation that Ann has, in fact, made some choice. But what\nwarrants Ann to conclude that Bob does not know she has chosen \\(u\\)\n(the third event)? This is a much more significant statement about\nwhat Ann knows about what Bob expects her to do. Indeed, in certain\ncontexts, Ann may have very good reasons to think it is possible that\nBob actually knows she will choose \\(u\\). We can find an ex\ninterim epistemic model where this event (\\(-K_A(-K_B(U))\\)) is\ntrue at \\(w_1\\), but this requires adding a new possible world: Figure 6 Notice that since \\(\\Pi_B(w')=\\{\\{w'\\}\\}\\subseteq U\\) we have \\(w'\\in\nK_B(U)\\). That is, Bob knows that Ann chooses \\(u\\) at state\n\\(w'\\). Finally, a simple calculation shows that \\(w_1\\in -K_A(-K_B(U))\\),\nas desired. Of course, we can question the other substantive\nassumptions built-in to this model (e.g., at \\(w_1\\), Bob knows\nthat Ann does not know he will choose \\(L\\)) and continue modifying the\nmodel. This raises a number of interesting conceptual and technical\nissues which we discuss in Section 7. So far we have looked at relational models of hard information. A\nsmall modification of these models allows us to model a softer\ninformational attitude. Indeed, by simply replacing the assumption of\nreflexivity of the relation \\(\\sim_i\\) with seriality (for each state\n\\(w\\) there is a state \\(v\\) such that \\(w\\sim_i v\\)), but keeping the other\naspects of the model the same, we can capture what epistemic logicians\nhave called “beliefs”. Formally,\na doxastic model is a tuple \\(\\langle W,\n\\{R_i\\}_{i\\in\\Agt},V\\rangle\\) where \\(W\\) is a nonempty set of states,\n\\(R_i\\) is a transitive, Euclidean and serial relation on \\(W\\) and \\(V\\) is\na valuation function (cf. Definition 2.1). This\nnotion of belief is very close to the above hard informational\nattitude and, in fact, shares all the properties of \\(K_i\\) listed above\nexcept Veracity (this is replaced with a weaker assumption\nthat agents are “consistent” and so cannot believe\ncontradictions). This points to a logical analysis of both\ninformational attitudes with various “bridge principles”\nrelating knowledge and belief (such as knowing something implies\nbelieving it or if an agent believes \\(\\phi\\) then the agent knows that\nhe believes it). However, we do not discuss this line of research here\nsince these models are not our preferred ways of representing the\nagents’ soft information (see, for example, Halpern 1991 and\nStalnaker 2006). Plausibility Orderings A key aspect of beliefs which is not yet represented in the above\nmodels is that they are revisable in the presence of new\ninformation. While there is an extensive literature on the theory of\nbelief revision in the “AGM” style (Alchourrón,\nGärdenfors, & Makinson 1985), we focus on how to extend an\nepistemic model with a representation of softer, revisable\ninformational attitudes. The standard approach is to include\na plausibility ordering for each agent: a preorder (reflexive\nand transitive) denoted \\(\\preceq_i\\,\\subseteq W\\times W\\). If\n\\(w\\preceq_i v\\) we say “player \\(i\\) considers \\(w\\) at least as\nplausible as \\(v\\).” For an event \\(X\\subseteq W\\), let denote the set of minimal elements of \\(X\\) according to\n\\(\\preceq_i\\). Thus while the \\(\\sim_i\\) partitions the set of possible\nworlds according to the agents’ hard information, the\nplausibility ordering \\(\\preceq_i\\) represents which of the possible\nworlds the agent considers more likely (i.e., it represents the\nplayers soft information). Definition 2.4 (Epistemic-Plausibility Models)\nSuppose\nthat \\(G\\) is a strategic game, \\(S\\) is the set of strategy profiles of\n\\(G\\), and \\(\\A\\) is the set of players. An epistemic-plausibility\nmodel is a tuple \\(\\langle W,\\{\\Pi_i\\}_{i\\in \\Agt},\n\\{\\preceq_i\\}_{i\\in \\Agt},\\sigma\\rangle\\) where \\(\\kripkemodel\\) is an\nepistemic model, \\(\\sigma:W\\rightarrow S\\) and for each \\(i\\in\\Agt\\),\n\\(\\preceq_i\\) is a \nwell-founded,[9]\n reflexive and transitive\nrelation on \\(W\\) satisfying the following properties, for all \\(w,v\\in\nW\\)   Remark 2.5\n Note that if \\(v\\not\\in\\Pi_i(w)\\) then\n\\(w\\not\\in \\Pi_i(v)\\). Hence, by property 1, \\(w\\not\\preceq_i v\\) and\n\\(v\\not\\preceq_i w\\). Thus, we have the following equivalence: \\(v\\in\n\\Pi_i(w)\\) iff \\(w\\preceq_i v\\) or \\(v\\preceq_i w\\). Local connectedness implies that \\(\\preceq_i\\) totally orders\n\\(\\Pi_i(w)\\) and well-foundedness implies that\n\\(Min_{\\preceq_i}(\\Pi_i(w))\\) is nonempty. This richer model allows us\nto formally define a variety of (soft) informational attitudes. We\nfirst need some additional notation: the plausibility relation\n\\(\\preceq_i\\) can be lifted to subsets of \\(W\\) as\nfollows[10] Suppose \\(\\M=\\plausmodel\\) is an epistemic-plausibility model,\nconsider the following operators (formally, each is a function from\n\\(\\pow(W)\\) to \\(\\pow(W)\\) similar to the knowledge operator defined\nabove): Belief: \\(B_i(E)=\\{w \\mid\nMin_{\\preceq_i}(\\Pi_i(w))\\subseteq E\\}\\) This is the usual notion\nof belief which satisfies the standard properties discussed above\n(e.g., consistency, positive and negative introspection). Robust Belief: \\(B_i^r(E)=\\{w \\mid v\\in E, \\mbox{ for all }\nv \\mbox{ with } w \\preceq_i v\\}\\)  So, \\(E\\) is robustly believed\nif it is true in all worlds more plausible then the current\nworld. This stronger notion of belief has also been\ncalled certainty by some authors (cf. Shoham &\nLeyton-Brown 2008: sec. 13.7). Strong Belief:  So, \\(E\\) is strongly believed provided it is epistemically possible and\nagent \\(i\\) considers any state in \\(E\\) more plausible\nthan any state in the complement of \\(E\\). It is not hard to see that if agent \\(i\\) knows that \\(E\\) then \\(i\\)\n(robustly, strongly) believes that \\(E\\). However, much more can be said\nabout the logical relationship between these different notions. (The\nlogic of these notions has been extensively studied by Alexandru\nBaltag and Sonja Smets in a series of articles, see Baltag & Smets\n2009 in Other Internet Resources for references.) As noted above, a crucial feature of these informational attitudes\nis that they may be defeated by appropriate evidence. In fact, we can\ncharacterize these attitudes in terms of the type of evidence which\ncan prompt the agent to adjust her beliefs. To make this precise, we\nintroduce the notion of a conditional belief: suppose\n\\(\\M=\\plausmodel\\) is an epistemic-plausibility model and \\(E\\) and \\(F\\)\nare events, then the conditional belief operator is\ndefined as follows: So, ‘\\(B_i^F\\)’ encodes what agent \\(i\\) will believe upon\nreceiving (possibly misleading) evidence that \\(F\\)\nis true. We conclude this section with an example to illustrate the above\nconcepts. Recall again the coordination game\nof Figure 4: there are two actions for\nplayer 1 (Ann), \\(u\\) and \\(d\\), and two actions for player 2 (Bob), \\(r\\)\nand \\(l\\). Again, the preferences (or utilities) of the players are not\nimportant at this stage since we are only interested in describing the\nplayers’ information. The following epistemic-plausibility model\nis a possible description of the players’ informational\nattitudes that can be associated with this game. The solid lines\nrepresent player 1’s informational attitudes and the dashed line\nrepresents player 2’s. The arrows correspond to the players\nplausibility orderings with an \\(i\\)-arrow from \\(w\\) to \\(v\\) meaning\n\\(v\\preceq_i w\\) (we do not draw all the arrows: each plausibility\nordering can be completed by filling in arrows that result from\nreflexivity and transitivity). The different regions represent the\nplayers’ hard information. Figure 7 Suppose that the actual state of play is \\(w_4\\). So, player 1 (Ann)\nchooses \\(u\\) and player 2 (Bob) chooses \\(r\\). Further, suppose that \\(L=\n\\{w_1,w_2,w_5\\}\\) is the event where where player 2 chooses \\(l\\)\n(similarly for \\(U\\), \\(D\\), and \\(R\\)) \\(B_1(L)\\): “player 1 believes that player 2 is choosing\n\\(L\\)” \\(B_1(B_2(U))\\): “player 1 believes that player 2 believes that\nplayer 1 chooses \\(u\\)” \\(B_1^{R} (- B_2(U))\\): “given that player 2 chooses \\(r\\),\nplayer 1 believes that player 2 does not believe she is choosing\n\\(u\\)” This last formula is interesting because it\n“pre-encodes” what player 1 would believe upon learning\nthat player 2 is choosing \\(R\\). Note that upon receiving\nthis true information, player 1 drops her belief that player\n2 believes she is choosing \\(u\\). The situation can be even more\ninteresting if there are statements in the language that reveal\nonly partial information about the player strategy\nchoices. Suppose that \\(E\\) is the event \\(\\{w_4,w_6\\}\\). Now \\(E\\) is true\nat \\(w_4\\) and player 2 believes that player 1 chooses \\(d\\)\ngiven that \\(E\\) is true (i.e., \\(w_4\\in B_2^E(D)\\)). So, player 1 can\n“bluff” by revealing the true (though partial) information\n\\(E\\). Probabilities The above models use a “crisp” notion of uncertainty,\ni.e., for each agent and state \\(w\\), any other state \\(v\\in W\\) either\nis or is not possible at/more plausible than \\(w\\). However, there is an\nextensive body of literature focused on graded,\nor quantitative, models of uncertainty (Huber 2009; Halpern\n2003). For instance, in the Game Theory literature it is standard to\nrepresent the players’ beliefs by probabilities (Aumann\n1999b; Harsanyi 1967–68). The idea is simple: replace the\nplausibility orderings with probability distributions: Definition 2.6 (Epistemic-Probability Model)\nSuppose that \\(G\\) is a strategic game, \\(S\\) is the set of\nstrategy profiles of \\(G\\), and \\(\\A\\) is the set of\nplayers. An epistemic-probabilistic model is a tuple where \\(\\kripkemodel\\) is an epistemic model and assigns to each state a probability measure over\n\\(W\\). Write \\(p_i^w\\) for the \\(i\\)’s probability measure at state\n\\(w\\). We make two natural assumptions\n(cf. Definition 2.4): For all \\(v\\in W\\), if \\(p_i^w(v)>0\\) then \\(p_i^w=p_i^v\\);\nand For all \\(v\\not\\in\\Pi_i(w)\\), \\(p_i^w(v)=0\\). Property 1 says that if \\(i\\) assigns a non-zero probability to state\n\\(v\\) at state \\(w\\) then the agent uses the same probability measure at\nboth states. This means that the players “know” their own\nprobability measures. The second property implies that players must\nassign a probability of zero to all states outside the current (hard)\ninformation cell. These models provide a very precise description of\nthe players’ hard and soft informational attitudes. However,\nnote that writing down a model requires us to specify a different\nprobability measure for each partition cell which can be quite\ncumbersome. Fortunately, the properties in the above definition imply\nthat, for each agent, we can view the agent’s probability\nmeasures as arising from one probability measure through\nconditionalization. Formally, for each \\(i\\in\\Agt\\), agent\n\\(i\\)’s (subjective) prior probability is any\nelement of \\(p_i\\in\\Delta(W)\\). Then, in order to define an\nepistemic-probability model we need only give for each agent\n\\(i\\in\\Agt\\), (1) a prior probability \\(p_i\\in\\Delta(W)\\) and (2) a\npartition \\(\\Pi_i\\) on \\(W\\) such that for each \\(w\\in W\\),\n\\(p_i(\\Pi_i(w))>0\\). The probability measures for each \\(i\\in\\Agt\\) are\nthen defined by: Of course, the side condition that for each \\(w\\in W\\),\n\\(p_i(\\Pi_i(w))>0\\) is important since we cannot divide by\nzero—this will be discussed in more detail in later\nsections. Indeed, (assuming \\(W\\) is \nfinite[11]) given any\nepistemic-plausibility model we can find, for each agent, a prior\n(possibly different ones for different agents) that generates the\nmodel as described above. This is not only a technical observation: it\nmeans that we are assuming that the players’ beliefs about the\noutcome of the situation are fixed ex ante with the ex\ninterim beliefs being derived through conditionalization on the\nagent’s hard information. (See Morris 1995 for an\nextensive discussion of the situation when there is a common\nprior.) We will return to these key assumptions throughout the\ntext. As above we can define belief operators, this time specifying the\nprecise degree to which an agent believes an event: Probabilistic belief: \\(B_i^r(E)=\\{w \\mid\np_i^w(E)=r\\}\\) Here, \\(r\\) can be any real number in the unit\ninterval; however, it is often enough to restrict attention to the\nrational numbers in the unit interval. Full belief: \\(B_i(E)=B_i^1(E)=\\{w \\mid p_i^w(E)=1\\}\\)\nSo, full belief is defined to belief with probability one. This is a\nstandard assumption in this literature despite a number of well-known\nconceptual difficulties (see Huber 2009 for an extensive discussion of\nthis and related issues). It is sometimes useful to work with the\nfollowing alternative characterization of full-belief (giving it a\nmore “modal” flavor): Agent \\(i\\) believes \\(E\\) at state \\(w\\)\nprovided all the states that \\(i\\) assigns positive probability to at\n\\(w\\) are in \\(E\\). Formally, These models have also been subjected to sophisticated logical\nanalyses (Fagin, Halpern, & Megiddo 1990; Heifetz & Mongin\n2001) complementing the logical frameworks discussed above (cf. Baltag\n& Smets 2006). We conclude this section with an example of an\nepistemic-probability model. Recall again the coordination game\nof Figure 4: there are two actions for\nplayer 1 (Ann), \\(u\\) and \\(d\\), and two actions for player 2 (Bob), \\(r\\)\nand \\(l\\). The preferences (or utilities) of the players are not\nimportant at this stage since we are only interested in describing the\nplayers’ information. Figure 8 The solid lines are Ann’s information partition and the\ndashed lines are Bob’s information partition. We further assume\nthere is a common prior \\(p_0\\) with the probabilities assigned to each\nstate written to the right of the state. Let \\(E=\\{w_2,w_5,w_6\\}\\) be an\nevent. Then, we have \\(B_1^{\\frac{1}{2}}(E)=\\{w \\mid p_0(E \\mid\n\\Pi_1(w))=\\frac{p_0(E\\cap\\Pi_1(w))}{p_0(\\Pi_1(w))}=\\frac{1}{2}\\}=\\{w_1,w_2,w_3\\}\\):\n“Ann assigns probability 1/2 to the event \\(E\\) given her\ninformation cell \\(\\Pi_1(w_1)\\). \\(B_2(E)=B_2^1(E)=\\{w_2,w_5,w_3,w_6\\}\\). In particular, note that at\n\\(w_6\\), the agent believes (with probability 1) that \\(E\\) is true, but\ndoes not know that \\(E\\) is true as \\(\\Pi_2(w_6)\\not\\subseteq\nE\\). So, there is a distinction between states the agent considers\npossible (given their “hard information”) and states to\nwhich players assign a non-zero probability. Let \\(U=\\{w_1,w_2,w_3\\}\\) be the event that Ann plays \\(u\\) and\n\\(L=\\{w_1,w_4\\}\\) the event that Bob plays \\(l\\). Then, we have \\(K_1(U)=U\\) and \\(K_2(L)=L\\): Both Ann and Bob know that strategy they\nhave chosen; \\(B_1^{\\frac{1}{2}}(L)=U\\): At all states where Ann plays \\(u\\), Ann\nbelieves that Bob plays \\(L\\) with probability 1/2; and \\(B_1(B_2^{\\frac{1}{2}}(U))=\\{w_1,w_2,w_3\\}=U\\): At all states where\nAnn plays \\(u\\), she believes that Bob believes with probability 1/2\nthat she is playing \\(u\\). An alternative approach to modeling beliefs was initiated by\nHarsanyi in his seminal paper (Harsanyi 1967–68). Rather than\n“possible worlds”, Harsanyi takes the notion of the\nplayers’ type as primitive. Formally, the players are\nassigned a nonempty set of types. Typically, players are assumed\nto know their own type but not the types of the other\nplayers. As we will see, each type can be associated with a specific\nhierarchy of belief   Definition 2.7 (Qualitative Type Space)\n A Qualitative type space for a\n(nonempty) set of states of nature \\(S\\) and agents \\(\\Agt\\) is a tuple\n\\(\\langle \\{T_i\\}_{i\\in\\A}, \\{\\lambda_i\\}_{i\\in\\Agt},S\\rangle\\) where\nfor each \\(i\\in\\Agt\\), \\(T_i\\) is a nonempty set and So, each type \\(t\\in T_i\\) is associated with a set of tuples\nconsisting of types of the other players and a state of nature. For\nsimplicity, suppose there are only two players, Ann and\nBob. Intuitively, \\((t',o')\\in\\lambda_{Ann}(t)\\) means that Ann’s\ntype \\(t\\) considers it possible that the outcome is \\(o'\\) and\nBob is of type \\(t'\\). Since the players’ uncertainty is directed\nat the choices and types of the other players, the\ninformational attitude captured by these models will certainly not\nsatisfy the Truth axiom. In fact, qualitative type spaces can be\nviewed as simply a “re-packaging” of the relational models\ndiscussed above (cf. Zvesper 2010 for a discussion). Consider again the running example of the coordination game between\nAnn and Bob (pictured in Figure 1). In\nthis case, the set of states of nature is\n\\(S=\\{(u,l),(d,l),(u,r),(d,r)\\}\\). In this context, it is natural to\nmodify the definition of the type functions \\(\\lambda_i\\) to account for\nthe fact that the players are only uncertain about the other\nplayers’ choices: let \\(S_A=\\{u,d\\}\\) and \\(S_B=\\{l,r\\}\\) and\nsuppose \\(T_A\\) and \\(T_B\\) are nonempty sets of types. Define \\(\\lambda_A\\)\nand \\(\\lambda_B\\) as follows: Suppose that there are two types for each player:\n\\(T_A=\\{t^A_1,t^A_2\\}\\) and \\(T_B=\\{t^B_1,t^B_2\\}\\). A convenient way to\ndescribe the maps \\(\\lambda_A\\) and \\(\\lambda_B\\) is: Figure 9 where a 1 in the \\((t',s)\\) entry of the above matrices corresponds\nto assuming \\((t',s)\\in\\lambda_i(t)\\) (\\(i=A,B\\)). What does it mean\nfor Ann (Bob) to believe an event \\(E\\) in a type structure?\nWe start with some intuitive observations about the above type\nstructure: Regardless of what type we assign to Ann, she believes that Bob\nwill choose \\(l\\) since in both matrices, \\(\\lambda_A(t_1^A)\\) and\n\\(\\lambda_A(t_2^A)\\), the only places where a 1 appears is under the \\(l\\)\ncolumn. So, fixing a type for Ann, in all of the situations Ann\nconsiders possible it is true that Bob chooses \\(l\\). If Ann is assigned the type \\(t_1^A\\), then she considers it possible\nthat Bob believes she will choose \\(u\\). Notice that type \\(t_1^A\\) has a\n1 in the row labeled \\(t^B_1\\), so she considers it possible that Bob is\nof type \\(t^B_1\\), and type \\(t^B_1\\) believes that Ann chooses \\(u\\) (the\nonly places where 1 appears is under the \\(u\\) column). If Ann is assigned the type \\(t_2^A\\), then Ann believes that Bob\nbelieves that Ann believes that Bob will choose \\(l\\). Note that type\n\\(t_2^A\\) “believes” that Bob will choose \\(l\\) and\nfurthermore \\(t_2^A\\) believes that Bob is of type \\(t^B_2\\) who in turn\nbelieves that Ann is of type \\(t^A_2\\). We can formalize the above informal observations using the\nfollowing notions: Fix a qualitative type space \\(\\langle\n\\{T_i\\}_{i\\in\\A}, \\{\\lambda_i\\}_{i\\in\\Agt},S\\rangle\\) for a (nonempty)\nset of states of nature \\(S\\) and agents \\(\\Agt\\). A (global) state, or possible\nworld is a tuple \\((t_1,t_2,\\ldots,t_n,s)\\) where \\(t_i\\in T_i\\)\nfor each \\(i=1,\\ldots,n\\) and \\(s\\in S\\). If \\(S=\\bigtimes S_i\\) is the set\nof strategy profiles for some game, then we write a possible world as:\n\\((t_1,s_1,t_2,s_2,\\ldots,t_n,s_n)\\) where \\(s_i\\in S_i\\) for each\n\\(i=1,\\ldots,n\\). Type spaces describe the players beliefs about the other\nplayers’ choices, so the notion of an event needs to be\nrelativized to an agent. An event for agent \\(i\\) is a\nsubset of \\(\\bigtimes_{j\\ne i}T_j\\times S\\). Again if \\(S\\) is a set of\nstrategy profiles (so \\(S=\\bigtimes S_i\\)), then an event for agent \\(i\\)\nis a subset of \\(\\bigtimes_{j\\ne i} (T_j\\times S_j)\\). Suppose that \\(E\\) is an event for agent \\(i\\), then we say that\nagent \\(i\\) believes \\(E\\) at \\((t_1,t_2,\\ldots,t_n,s)\\)\nprovided \\(\\lambda(t_1,s)\\subseteq E\\). In the specific example above, an event for Ann is a set\n\\(E\\subseteq T_B\\times S_B\\) and we can define the set of pairs\n\\((t^A,s^A)\\) that believe this event: similarly for Bob. Note that the event \\(B_A(E)\\) is an event for Bob\nand vice versa. A small change to the above definition of a\ntype space (Definition 2.7) allows us to\nrepresent probabilistic beliefs (we give the full definition\nhere for future reference): Definition 2.8 (Type Space)\nA type space for a (nonempty) set of states of\nnature \\(S\\) and agents \\(\\Agt\\) is a tuple \\(\\langle \\{T_i\\}_{i\\in\\A},\n\\{\\lambda_i\\}_{i\\in\\Agt},S\\rangle\\) where for each \\(i\\in\\Agt\\), \\(T_i\\) is\na nonempty set and where \\(\\Delta(\\bigtimes_{j\\ne i} T_j\\times S)\\) is the set of\nprobability measures on \\(\\bigtimes_{j\\ne i} T_j\\times S\\). Types and their associated image under \\(\\lambda_i\\) encode the\nplayers’ (probabilistic) information about the others’\ninformation. Indeed, each type is associated with a hierarchy of\nbelief. More formally, recall that an event \\(E\\) for a type \\(t_i\\) is a\nset of pairs \\((\\sigma_{-j}, t_{-j})\\), i.e., a set of strategy choices\nand types for all the other players. Given an event \\(E\\) for player\n\\(i\\), let \\(\\lambda_i(t_i)(E)\\) denote the sum of the probabilities that\n\\(\\lambda_i(t_i)\\) assigns to the elements of \\(E\\). The type \\(t_i\\) of\nplayer \\(i\\) is said to (all-out) believe the event \\(E\\)\nwhenever \\(\\lambda_i(t_i)(E) = 1\\). Conditional beliefs are computed in\nthe standard way: type \\(t_i\\) believes that \\(E\\) given \\(F\\) whenever: \nA state in a type structure is a tuple \\((\\sigma, t)\\) where\n\\(\\sigma\\) is a strategy profile and \\(t\\) is “type profile”,\na tuple of types, one for each player. Let \\(B_i(E) = \\{(\\sigma_{-j},\nt_{-j}) : t_i \\text{ believes that } E \\}\\) be the event (for \\(j\\)) that\n\\(i\\) believes that \\(E\\). Then agent \\(j\\) believes that \\(i\\) believes that\n\\(E\\) when \\(\\lambda_j(t_j)(B_i(E)) = 1\\). We can continue in this manner\ncomputing any (finite) level of such higher-order information. \nReturning again to our running example game where player 1 (Ann) has\ntwo available actions \\(\\{u,d\\}\\) and player 2 (Bob) has two available\nactions \\(\\{l,r\\}\\). The following type space describes the\nplayers’ information: there is one type for Ann (\\(t_1\\)) and two\nfor Bob (\\(t_2,t_2'\\)) with the corresponding probability measures given\nbelow: Figure 10: Ann's beliefs about\nBob Figure 11: Bob's belief about Ann In this example, since there is only one type for Ann, both of\nBob’s types are certain about Ann’s beliefs. If\nBob is of type \\(t_2\\) then he is certain Ann is choosing \\(u\\) while if\nhe is of type \\(t_2'\\) he thinks there is a 75% chance she plays\n\\(u\\). Ann assigns equal probability (\\(0.5\\)) to Bob’s types; and\nso, she believes it is equally likely that Bob is certain she plays\n\\(u\\) as Bob thinking there is a 75% chance she plays \\(u\\). The above\ntype space is a very compact description of the players’\ninformational attitudes. An epistemic-probabilistic model can describe\nthe same situation (here \\(p_i\\) for \\(i=1,2\\) is player \\(i\\)’s prior\nprobability): Figure 12 Some simple (but instructive!) calculations can convince us that\nthese two models represent the same situation. The more interesting\nquestion is how do these probabilistic models relate to the\nepistemic-doxastic models of Definition\n2.4. Here the situation is more complex. On the one hand,\nprobabilistic models with a graded notion of belief which is much more\nfine-grained than the “all-out” notion of belief discussed\nin the context of epistemic-doxastic models. On the other hand, in an\nepistemic-doxastic model, conditional believes are defined\nfor all events. In the above models, they are only defined\nfor events that are assigned nonzero probabilities. In other words,\nepistemic-probabilistic models do not describe what a player may\nbelieve upon learning something “surprising” (i.e.,\nsomething currently assigned probability zero). A number of extensions to basic probability theory have been\ndiscussed in the literature that address precisely this problem. We do\nnot go into details here about these approaches (a nice summary and\ndetailed comparison between different approaches can be found in\nHalpern (2010) and instead sketch the main ideas. The first approach\nis to use so-called Popper functions which\ntake conditional probability measures as primitive. That is,\nfor each non-empty event \\(E\\), there is a probability measure\n\\(p_E(\\cdot)\\) satisfying the usual Kolmogrov axioms (relativized to\n\\(E\\), so for example \\(p_E(E)=1\\)). A second approach assigns to each\nagent a finite sequence of probability measures \\((p_1,p_2,\\ldots,p_n)\\)\ncalled a lexicographic probability system. The idea is that\nto condition on \\(F\\), first find the first probability measure not\nassigning zero to \\(F\\) and use that measure to condition on\n\\(F\\). Roughly, one can see each of the probability measures in a\nlexicographic probability system as corresponding to a level of a\nplausibility ordering. We will return to these notions\nin Section 5.2. States in a game model not only represent the player’s\nbeliefs about what their opponents will do, but also\ntheir higher-order beliefs about what their opponents are\nthinking. This means that outcomes identified as\n“rational” in a particular informational context will\ndepend, in part, on these higher-order beliefs. Both game\ntheorists and logicians have extensively discussed different notions\nof knowledge and belief for a group, such as common knowledge and\nbelief. In this section, we briefly recount the standard definition of\ncommon knowledge. For more information and pointers to the relevant\nliterature, see Vanderschraaf & Sillari (2009) and Fagin et al.,\n(1995: ch. 6). Consider the statement “everyone in group \\(I\\) knows that\n\\(E\\)”. This is formally defined as follows: where \\(I\\) is any nonempty set of players. If \\(E\\) is common\nknowledge for the group \\(I\\), then not only does everyone in the group\nknow that \\(E\\) is true, but this fact is completely transparent to all\nmembers of the group. We first define \\(K_I^n(E)\\) for each \\(n\\ge 0\\) by\ninduction: Then, following Aumann (1976), common knowledge of\n\\(E\\) is defined as the following infinite conjunction: Unpacking the definitions, we have  The approach to defining common knowledge outlined above can be\nviewed as a recipe for defining common (robust/strong) belief (simply\nreplace the knowledge operators \\(K_i\\) with the appropriate belief\noperator). See Bonanno (1996) and Lismont & Mongin (1994, 2003)\nfor more information about the logic of common belief. Although we do\nnot discuss it in this entry, a probabilistic variant of common belief\nwas introduced by Monderer & Samet (1989). There are many philosophical issues that arise in decision theory,\nbut that is not our concern here. See Joyce 2004 and reference therein\nfor discussions of the main philosophical issues. This section\nprovides enough background on decision theory to understand the key\nresults of epistemic game theory presented in the remainder of this\nentry. Decision rules or choice rules determine what\neach individual player will, or should do, given her preferences and\nher information in a given context. In the epistemic game theory\nliterature the most commonly used choice rules are:\n(strict) dominance, maximization of expected utility\nand admissibility (also known as weak dominance). One can do\nepistemic analysis of games using alternative choice rules, e.g.,\nminmax regret (Halpern & Pass 2011). In this entry, we focus only\non the most common ones. Decision theorists distinguish between choice\nunder uncertainty and choice under risk. In the\nlatter case, the decision maker has probabilistic information about\nthe possible states of the world. In the former case, there is no such\ninformation. There is an extensive literature concerning decision\nmaking in both types of situations (see Peterson 2009 for a discussion\nand pointers to the relevant literature). In the setting of epistemic\ngame theory, the appropriate notion of a “rational choice”\ndepends on the type of game model used to describe the informational\ncontext of the game. So, in general, “rationality” should\nbe read as following a given choice rule. The general approach is to\nstart with a definition of an irrational choice (for\ninstance, one that is strictly dominated given one’s\nbeliefs), and then define rationality as not being irrational. Some\nauthors have recently looked at the consequences of lifting this\nsimplifying assumption (cf., the tripartite notion of\na categorization in Cubitt & Sugden (2011) and Pacuit\n& Roy (2011)), but the presentation of this goes beyond the scope\nof this entry. Finally, when the underlying notion of rationality\ngoes beyond maximization of expected utility, some authors\nhave reserved the word “optimal” to qualify decisions that\nmeet the latter requirement, but not necessarily the full requirements\nof rationality. See the remarks in Section\n5.2 for more on this. Maximization of expected utility is the most well-known choice rule\nin decision theory. Given an agent’s preferences (represented as\nutility functions) and beliefs (represented as subjective probability\nmeasures), the expected utility of an action, or option, is the sum of\nthe utilities of the outcomes of the action weighted by the\nprobability that they will occur (according to the agent’s\nbeliefs). The recommendation is to choose the action that maximizes\nthis weighted average. This idea underlies the Bayesian view\non practical rationality, and can be straightforwardly defined in type\nspaces.[12]\n We start by defining expected utility for a\nplayer in a game.  Suppose that \\(G=\\langle N, \\{S_i, u_i\\}_{i\\in N}\\rangle\\) is a\nstrategic game. A conjecture for player \\(i\\) is a\nprobability on the set \\(S_{-i}\\) of strategy profiles of \\(i\\)’s\nopponents. That is, a conjecture for player \\(i\\) is an element of\n\\(\\Delta(S_{-i})\\), the set of probability measures over\n\\(S_{-i}\\). The expected utility of \\(s_i\\in S_i\\) with\nrespect to a conjecture \\(p\\in \\Delta(S_{-i})\\) is defined as follows: A strategy \\(s_i\\in S_i\\) maximizes expected utility\nfor player \\(i\\) with respect to \\(p\\in \\Delta(S_{-i})\\) provided for all\n\\(s_i'\\in S_i\\), \\(EU(s_i,p)\\ge EU(s_i',p)\\). In such a case, we also say\n\\(s_i\\) is a best response to \\(p\\) in game \\(G\\). We now can define an event in a type space or epistemic-probability\nmodel where all players “choose rationally”, in the sense\nthat their choices maximize expected utility with respect to their\nbeliefs.  Let \\(G=\\langle N, \\{S_i, u_i\\}_{i\\in N}\\rangle\\) be a strategic\ngame and \\(\\T=\\langle \\{T_i\\}_{i\\in\\Agt},\n\\{\\lambda_i\\}_{i\\in\\Agt},S\\rangle\\) a type space for \\(G\\). Recall that\neach \\(t_i\\) is associated with a probability measure \\(\\lambda(t_i)\\in\n\\Delta(S_{-i}\\times T_{-i})\\). Then, for each \\(t_i\\in T_i\\), we can\ndefine a probability measure \\(p_{t_i}\\in \\Delta(S_{-i})\\) as follows: The set of states (pairs of strategy profiles and type profiles)\nwhere player \\(i\\) chooses rationally is then defined as: The event that all players are rational is Notice that here types, as opposed to players, maximize\nexpected utility. This is because in type structure, beliefs are\nassociated to types (see Section 2.3\nabove). The reader acquainted with decision theory will recognize that\nthis is just the standard notion of maximization of expected utility,\nwhere the space of uncertainty of each player, i.e., the possible\n“states of the world” on which the consequences of her\naction depend, is the possible combinations of types and strategy\nchoices of the other players. To illustrate the above definitions, consider the game\nin Figure 4 and the type space\nin Figure 11. The following calculations show that\n\\((u, t_1)\\in \\mathsf{Rat}_1\\) (\\(u\\) is the best response for player \\(1\\)\ngiven her beliefs defined by \\(t_1\\)): A similar calculation shows that \\((l, t_2)\\in \\mathsf{Rat}_2\\).  The definition of a rationality event is similar in an\nepistemic-probability model. For completeness, we give the formal\ndetails. Suppose that  is a strategic game and  is an\nepistemic probability models with each \\(p_i\\) a prior probability\nmeasure over \\(W\\). Each state \\(w\\in W\\), let Then, for each state \\(w\\in W\\), we define\na measure \\(p_w\\in \\Delta(S_{-i})\\) as follows: As above, and When a game model does not describe the players’\nprobabilistic beliefs, we are in a situation of choice\nunder uncertainty. The standard notion of “rational\nchoice” in this setting is based on dominance reasoning\n(Finetti 1974). The two standard notions of dominance are: \nDefinition 3.1 (Strict Dominance)\n  \nSuppose that \\(G=\\langle N, \\{S_i, u_i\\}_{i\\in N}\\rangle\\) is a\nstrategic game and \\(X\\subseteq S_{-i}\\). Let \\(m_i, m_i'\\in \\Delta(S_i)\\)\nbe two mixed strategies for player \\(i\\). The strategy\n\\(m_i\\) strictly dominates \\(m_i'\\) with respect to \\(X\\)\nprovided We say \\(m_i\\) is strictly dominated provided there\nis some \\(m_i'\\in \\Delta(S_i)\\) that strictly dominates \\(m_i\\). A strategy \\(m_i\\in \\Delta(S_i)\\) strictly dominates \\(m_i'\\in\n\\Delta(S_i)\\) provided \\(m_i\\) is better than \\(m_i'\\) (i.e., gives higher\npayoff to player \\(i\\)) no matter what the other players\ndo. There is also a weaker notion: Definition 3.2 (Weak Dominance)\nSuppose that \\(G=\\langle N, \\{S_i, u_i\\}_{i\\in N}\\rangle\\) is a\nstrategic game and \\(X\\subseteq S_{-i}\\). Let \\(m_i, m_i'\\in \\Delta(S_i)\\)\nbe two mixed strategies for player \\(i\\). The strategy\n\\(m_i\\) weakly dominates \\(m_i'\\) with respect to \\(X\\)\nprovided for all \\(s_{-i}\\in X\\), \\(U_i(m_i,s_{-i})\\ge U_i(m_i',s_{-i})\\) and there is some \\(s_{-i}\\in X\\) such that \\(U_i(m_i,s_{-i})>\nU_i(m_i',s_{-i})\\). We say \\(m_i\\) is weakly dominated provided there is\nsome \\(m_i'\\in \\Delta(S_i)\\) that weakly dominates \\(m_i\\). So, a mixed strategy \\(m_i\\) weakly dominates another strategy \\(m_i'\\)\nprovided \\(m_i\\) is at least as good as \\(m_i'\\) no matter what the other\nplayers do and there is at least one situation in which \\(m_i\\)\nis strictly better than \\(m_i'\\). Before we make use of these choice rules, we need to address two\npotentially confusing issues about these definitions. The definitions of strict and weak dominance are given in terms of\nmixed strategies even though we are assuming that players only\nselect pure strategies. That is, we are not considering\nsituations in which players explicitly randomize. In particular,\nrecall that only pure strategies are associated with states in a game\nmodel. Nonetheless, it is important to define strict/weak dominance in\nterms of mixed strategies because there are games in which a pure\nstrategy is strictly (weakly) dominated by a mixed strategy, but not\nby any of the other pure strategies. Even though it is important to consider situations in which a\nplayer’s pure strategy is strictly/weakly dominated by a mixed\nstrategy, we do not extend the above definitions to probabilities over\nthe opponents’ strategies. That is, we do not replace the above\ndefinition with \\(m_i\\) is strictly \\(p\\)-dominates \\(m_i'\\) with\nrespect to \\(X\\subseteq \\Delta(S_{-i})\\), provided for all \\(q\\in X\\),\n\\(U_i(m_i,q)>U_i(m_i',q)\\). This is because both definitions are\n equivalent. Obviously, \\(p\\)-strict dominance implies strict\n dominance. To see the converse, suppose that \\(m_i'\\) is dominated by\n \\(m_i\\) with respect to \\(X\\subseteq S_{-i}\\). We show that for all \\(q\\in\n \\Delta(X)\\), \\(U_i(m_i,q) > U_i(m_i',q)\\) (and so \\(m_i'\\) is\n \\(p\\)-strictly dominated by \\(m_i\\) with respect to \\(X\\)). Suppose that\n \\(q\\in \\Delta(X)\\). Then, The parameter \\(X\\) in the above definitions is intended to represent\nthe set of strategy profiles that the player \\(i\\) take to be\n“live possibilities”. Each state in an epistemic\n(-plausibility) model is associated with a such a set of strategy\nprofiles. Given a possible world \\(w\\) in a game model, let \\(S_{-i}(w)\\)\ndenote the set of states that player \\(i\\) “thinks” are\npossible. The precise definition depends on the type of game\nmodel: Epistemic models\nSuppose that  is a strategic game and  is an epistemic model of \\(G\\). For each player \\(i\\) and \\(w\\in W\\),\ndefine the set \\(S_{-i}(w)\\) as follows: \nEpistemic-Plausibility Models\nSuppose that is a strategic game and is an epistemic-plausibility model of \\(G\\). For each player \\(i\\) and \\(w\\in W\\),\ndefine the set \\(S_{-i}(w)\\) as follows: In either case, we say that a choice at state \\(w\\) is sd-rational\nfor player \\(i\\) at state \\(w\\) provided it is not strictly dominated with\nrespect to \\(S_{-i}(w)\\). The event in which \\(i\\) chooses rationality is\nthen defined as \nIn addition, we have \\(\\mathsf{Rat^{sd}}\\ :=\\ \\bigcap_{i\\in N}\\mathsf{Rat_i^{sd}}\\). \nSimilarly, we can define the set\nof states in which player \\(i\\) is playing a strategy that is not weakly\ndominated, denoted \\(\\mathsf{Rat_i^{wd}}\\) and \\(\\mathsf{Rat^{wd}}\\) using\nweak dominance. Knowledge of one’s own action, the trademark\nof ex-interim situations, plays an important role in the\nabove definitions. It enforces that \\(\\sigma_i(w') = \\sigma_i(w)\\)\nwhenever \\(w'\\in \\Pi_i(w)\\). This means that player \\(i\\)’s\nrationality is assessed on the basis of the result of\nher current choice according to different combinations of\nactions of the other players. An important special case is when the players consider all\nof their opponents’ strategies possible. It should be clear that\na rational player will never choose a strategy that is\nstrictly dominated with respect to \\(S_{-i}\\). That is, if \\(s_i\\) is\nstrictly dominated with respect that \\(S_{-i}\\), then there is no\ninformational context in which it is rational for player \\(i\\) to choose\n\\(s_i\\). This can be made more precise using the following well-known\nLemma.  Lemma 3.1\nSuppose that \\(G=\\langle N, \\{S_i, u_i\\}_{i\\in N}\\rangle\\) is a\nstrategic game. A strategy \\(s_i\\in S_i\\) is strictly dominated\n(possibly by a mixed strategy) with respect to \\(X\\subseteq S_{-i}\\) iff\nthere is no probability measure \\(p\\in \\Delta(X)\\) such that \\(s_i\\) is a\nbest response with respect to \\(p\\). The proof of this Lemma is given in\nthe supplement, Section 1. The general conclusion is that no dominated strategy can maximize\nexpected utility at a given state; and, conversely, if there is a\nstrategy that is not a best in a specific context, then it is not\nstrictly dominated. Similar facts hold about weak dominance, though the\nsituation is more subtle. The crucial observation is that there is a\ncharacterization of weak dominance in terms of best response to\ncertain types of probability measures. A probability measure \\(p\\in\n\\Delta(X)\\) is said to have full support (with respect\nto \\(X\\)) if \\(p\\) assigns positive probability to every element of \\(X\\)\n(formally, \\(supp(p)=\\{x\\in X \\mid p(x)>0\\}=X\\)). Let\n\\(\\Delta^{>0}(X)\\) be the set of full support probability measures on\n\\(X\\). A full support probability on \\(S_{-i}\\) means that player \\(i\\) does\nnot completely rule out (in the sense, that she assigns zero\nprobability to) any strategy profiles of her opponents. The following\nanalogue of Lemma 3.1 is also well-known: Lemma 3.2\nSuppose that \\(G=\\langle N, \\{S_i, u_i\\}_{i\\in\nN}\\rangle\\) is a strategic game. A strategy \\(s_i\\in S_i\\) is weakly\ndominated (possibly by a mixed strategy) with respect to \\(X\\subseteq\nS_{-i}\\) iff there is no full support probability measure \\(p\\in\n\\Delta^{>0}(X)\\) such that \\(s_i\\) is a best response with respect\nto \\(p\\). The proof of this Lemma is more involved. See Bernheim (1984:\nAppendix A) for a proof. In order for a strategy \\(s_i\\) to not be\nstrictly dominated, it is sufficient for \\(s_i\\) to be a best response\nto a belief, whatever that belief is, about the opponents’\nchoices. Admissibility requires something more: the strategy must be a\nbest response to a belief that does not explicitly rule-out any of the\nopponents’ choices. Comparing these two Lemmas, we see that\nstrict dominance implies weak dominance, but not necessarily vice\nversa. A strategy might not be a best response to any\nfull-support probability measure while being a best response to some\nparticular beliefs, those assigning probability one to a state where\nthe player is indifferent between the outcome of its present action\nand the potentially inadmissible one. There is another, crucial, difference between weak and strict\ndominance. The following observation is immediate from the definition\nof strict dominance: Observation 3.3\nIf \\(s_i\\) is strictly dominated with respect to \\(X\\) and\n\\(X'\\subseteq X\\), then \\(s_i\\) is strictly dominated with respect to\n\\(X'\\). If a strategy is strictly dominated, it remains so if the player\ngets more information about what her opponents (might) do. Thus, if a\nstrategy \\(s_i\\) is strictly dominated in a game \\(G\\) with respect to\nthe entire set of her opponents’ strategies \\(S_{-i}\\),\nthen it will never be rational (according to the above definitions) in\nany epistemic (-plausibility) model for \\(G\\). I.e., there are no\nbeliefs player \\(i\\) can have that makes \\(s_i\\) rational. The same\nobservation does not hold for weak dominance. The existential part of\nthe definition of weak dominance means that the analogue\nof Observation 3.3 does not hold for weak\ndominance: if \\(s_i\\) is weakly dominated with respect to \\(X\\) then it\nneed not be the case that \\(s_i\\) is weakly dominated with respect to\nsome \\(X'\\subseteq X\\). The epistemic approach to game theory focuses on the choices\nof individual decision makers in specific informational\ncontexts, assessed on the basis of decision-theoretic choice\nrules. This is a bottom-up, as opposed to the classical top-down,\napproach. Early work in this paradigm include Bernheim (1984) and\nPearce’s (1984) notion of rationalizability and\nAumann’s derivation of correlated equilibrium from the\nminimal assumption that the players are “Bayesian\nrational” (Aumann 1987). An important line of research in epistemic game theory asks under\nwhat epistemic conditions will players follow the\nrecommendations of particular solution concept? Providing such\nconditions is known as an epistemic characterization of a\nsolution concept. In this section, we present two fundamental epistemic\ncharacterization results. The first is a characterization of iterated\nremoval of strictly dominated strategies (henceforth ISDS), and the\nsecond is a characterization of backward induction. These epistemic\ncharacterization results are historically important. They mark the\nbeginning of epistemic game theory as we know it today. Furthermore,\nthey are also conceptually important. The developments in later\nsections build on the ideas presented in this section. The central result of epistemic game theory is that\n“rationality and common belief in rationality implies\niterated elimination of strictly dominated strategies.” This\nresult is already covered in Vanderschraaf & Sillari (2009). For\nthat reason, instead of focusing on the formal details, the emphasis\nhere will be on its significance for the epistemic foundations of game\ntheory. One important message is that the result highlights the\nimportance of higher-order information. Iterated elimination of strictly dominated strategies\n(ISDS) is a solution concept that runs as follows. First, remove from\nthe original game any strategy that is strictly dominated for player\n\\(i\\) (with respect to all of the opponents’ strategy\nprofiles). After having removed the strictly dominated strategies in\nthe original game, look at the resulting sub-game, remove the\nstrategies which have become strictly dominated there, and repeat this\nprocess until the elimination does not remove any strategies. The\nprofiles that survive this process are said to be iteratively\nnon-dominated. For example, consider the following strategic game: Figure 13 Note that \\(r\\) is strictly dominated for player \\(2\\) with respect to\n\\(\\{t, m, b\\}\\). Once \\(r\\) is removed from the game, we have \\(b\\) is\nstrictly dominated for player 1 with respect to \\(\\{l, c\\}\\). Thus,\n\\(\\{(t,l), (t,c), (m,l), (m,c)\\}\\) are iteratively undominated. That is,\niteratively removing strictly dominated strategies generates the\nfollowing sequence of games: Figure 14 For arbitrary large (finite) strategic games, if all players\nare rational and there is common belief that all\nplayers are rational, then they will choose a strategy that\nis iteratively non-dominated. The result is credited to Bernheim\n(1984) and Pearce (1984). See Spohn (1982) for an early version, and\nBrandenburger & Dekel (1987) for the relation with correlated\nequilibrium. Before stating the formal result, we illustrate the result with an\nexample. We start by describing an “informational context”\nof the above game. To that end, define a type space \\(\\T=\\langle \\{T_1,\nT_2\\}, \\{\\lambda_1,\\lambda_2\\}, S\\rangle\\), where \\(S\\) is the strategy\nprofiles in the above game, there are two types for player 1\n\\((T_1=\\{t_1, t_2\\})\\) and three types for player 2 \\((T_2=\\{s_1, s_2,\ns_3\\})\\). The type functions \\(\\lambda_i\\) are defined as follows: Figure 15 We then consider the pairs \\((s,t)\\) where \\(s\\in S_i\\) and \\(t\\in T_i\\)\nand identify  all the rational pairs (i.e., where \\(s\\) is a best\nresponse to \\(\\lambda_i(t)\\), see the previous section for a\ndiscussion): \\(\\mathsf{Rat_1}=\\{(t, t_1), (m, t_1), (b,t_2)\\}\\) \\(\\mathsf{Rat_2}=\\{(l, s_1), (c,s_1), (l, s_2), (c, s_2), (l, s_3)\n\\}\\) The next step is to identify the types that believe that\nthe other players are rational. In this context, belief\nmeans probability 1. For the type \\(t_1\\), we have\n\\(\\lambda_1(t_1)(\\mathsf{Rat}_2)=1\\); however, but \\((r,s_2)\\not\\in \\mathsf{Rat_2}\\), so \\(t_2\\) does not\nbelieve that player \\(2\\) is rational. This can be turned into an\niterative process as follows: Let \\(R_i^1=\\mathsf{Rat_i}\\). We first\nneed some notation. Suppose that for each \\(i\\), \\(R_i^n\\) has been\ndefined. Then, define \\(R_{-i}^n\\) as follows: \nFor each \\(n>1\\), define \\(R_i^n\\) inductively as follows: Thus, we have \\(R_1^2=\\{(t, t_1), (m, t_1)\\}\\). Note that \\(s_2\\)\nassigns non-zero probability to the pair \\((m,t_2)\\) which is not in\n\\(R_1^1\\), so \\(s_2\\) does not believe that \\(1\\) is rational. Thus, we have\n\\(R_2^2=\\{(l,s_1), (c,s_1),(l,s_3)\\}\\). Continuing with this process, we\nhave \\(R_1^2=R_1^3\\). However, \\(s_3\\) assigns non-zero probability to\n\\((b,t_2)\\) which is not in \\(R_1^2\\), so \\(R_2^3=\\{(l, s_1),\n(c,s_1)\\}\\). Putting everything together, we have \nThus, all the profiles that survive\niteratively removing strictly dominated strategies \n\\((\\{(t,l), (m,l), (t,c), (m,c)\\})\\) are consistent with \nstates where the players are rational and commonly believe they are\nrational. Note that, the above process need not generate all\nstrategies that survive iteratively removing strictly dominated\nstrategies. For example, consider a type space with a single type for\nplayer 1 assigning probability 1 to the single type of player 2 and\n\\(l\\), and the single type for player 2 assigning probability 1 to\nthe single type for player 1 and \\(u\\). Then, \\((u,l)\\) is the only\nstrategy profile in this model and obviously rationality and common\nbelief of rationality is satisfied. However, for any type space, if a\nstrategy profile is consistent with rationality and common belief of\nrationality, then it must be a strategy that is in the set of\nstrategies that survive iteratively removing strictly dominated\nstrategies. Theorem 4.1\nSuppose that \\(G\\) is a strategic game and \\(\\T\\) is any type space\nfor \\(G\\). If \\((s,t)\\) is a state in \\(\\T\\) in which all the players are\nrational and there is common belief of rationality—formally,\nfor each \\(i\\), —then \\(s\\) is a strategy profile that survives iteratively\nremoval of strictly dominated strategies. This result establishes sufficient conditions for ISDS. It\nhas also a converse direction: given any strategy profile that\nsurvives iterated elimination of strictly dominated strategies, there\nis a model in which this profile is played where all players are\nrational and this is common knowledge. In other words, one can\nalways view or interpret the choice of a strategy\nprofile that would survive the iterative elimination procedure as one\nthat results from common knowledge of rationality. Of course, this\nform of the converse is not particularly interesting as we can always\ndefine a type space where all the players assign probability 1 to the\ngiven strategy profile (and everyone playing their requisite\nstrategy). Much more interesting is the question whether\nthe entire set of strategy profiles that survive iteratively\nremoval of strictly dominated strategies is consistent with\nrationality and common belief in rationality. This is covered by the\nfollowing theorem of Brandenburger & Dekel (1987) (cf. also Tan\n& Werlang 1988): Theorem 4.2\nFor any game \\(G\\), there is a type structure for that game in\nwhich the strategy profiles consistent with rationality and common\nbelief in rationality is the set of strategies that survive\niterative removal of strictly dominated strategies. See Friedenberg & Keisler (2010) for the strongest versions of\nthe above results. Analogues of the above results have been proven\nusing different game models (e.g., epistemic models,\nepistemic-plausibility models, etc.). For example, see Apt &\nZvesper (2010) proofs of corresponding theorems using Kripke\nmodels. Many authors have pointed out the strength of the common belief\nassumption in the results of the previous section (see, e.g., Gintis\n2009; Bruin 2010). It requires that the players not only believe that\nthe others are not choosing an irrational strategy, but also to\nbelieve that everybody believes that nobody is choosing an irrational\nstrategy, and everyone believes that everyone believes that everyone\nbelieves that nobody is choosing an irrational strategy, and so on. It\nshould be noted, however, that this unbounded character is there only\nto ensure that the result holds for arbitrary finite\ngames. For a particular game and a model for it, a finite iteration\nof “everybody believes that” suffices to ensure a play\nthat survives the iterative elimination procedure. A possible reply to the criticism of the infinitary nature of the\ncommon belief assumption is that the result should be seen as the\nanalysis of a benchmark case, rather than a description of\ngenuine game playing situations or a prescription for what rational\nplayers should do (Aumann 2010). Indeed, common\nknowledge/belief of rationality has long been used as an informal\nexplanation of the idealizations underlying classical game-theoretical\nanalyses (Myerson 1991). The results above show that, once formalized,\nthis assumption does indeed lead to a classical solution concept,\nalthough, interestingly, not the well-known Nash equilibrium,\nas is often informally claimed in early game-theoretic\nliterature. Epistemic conditions for Nash equilibrium are presented\nin Section 5.1. The main message to take away from the results in the previous\nsection is: Strategic reasoning in games involves higher-order\ninformation. This means that, in particular, “Bayesian rationality” alone—i.e., maximization\nof expected utility—is not sufficient to ensure a strategy\nprofile is played that is iteratively undominated, in the general\ncase. In general, first-order belief of rationality will not do\neither. Exactly how many levels of beliefs is needed to guarantee\n“rational play” in game situations is still the subject of\nmuch debate (Kets 2014; Colman 2003; de Weerd, Verbrugge, &\nVerheij 2013; Rubinstein 1989). There are two further issues we need\nto address. First of all, how can agents arrive at a context where rationality\nis commonly believed? The above results do not answer that\nquestion. This has been the subject of recent work in Dynamic\nEpistemic Logic (van Benthem 2003). In this literature, this question\nis answered by showing that the agents can eliminate all higher-order\nuncertainty regarding each others’ rationality, and thus ensure\nthat no strategy is played that would not survive the iterated\nelimination procedure, by repeatedly and publicly\n“announcing” that they are not irrational. In\nother words, iterated public announcement of rationality makes the\nplayers’ expectations converge towards sufficient epistemic\nconditions to play iteratively non-dominated strategies. For more on\nthis dynamic view on solution epistemic characterization see van\nBenthem (2003); Pacuit & Roy (2011); van Benthem & Gheerbrant\n(2010); and van Benthem, Pacuit, & Roy (2011). Second of all, when there are more than two players, the above\nresults only hold if players can believe that the choices of their\nopponents are correlated (Bradenburger & Dekel 1987;\nBrandenburger & Friedenberg 2008). The following example from\nBrandenburger & Friedenberg (2008) illustrates this\npoint. Consider the following three person game where Ann’s\nstrategies are \\(S_A=\\{u,d\\}\\), Bob’s strategies are \\(S_B=\\{l,r\\}\\)\nand Charles’ strategies are \\(S_C=\\{x,y,z\\}\\) and their respective \npreferences for each outcome are given in the corresponding cell: Figure 16 Note that \\(y\\) is not strictly dominated for Charles. It is easy to\nfind a probability measure \\(p\\in\\Delta(S_A\\times S_B)\\) such that \\(y\\)\nis a best response to \\(p\\). Suppose that\n\\(p(u,l)=p(d,r)=\\frac{1}{2}\\). Then, \\(EU(x,p)=EU(z,p)=1.5\\) while\n\\(EU(y,p)=2\\). However, there is no probability measure \\(p\\in\n\\Delta(S_A\\times S_B)\\) such that \\(y\\) is a best response to \\(p\\) and\n\\(p(u,l)=p(u)\\cdot p(l)\\) (i.e., Charles believes that Ann and\nBob’s choices are independent). To see this, suppose that \\(a\\) is\nthe probability assigned to \\(u\\) and \\(b\\) is the probability assigned to\n\\(l\\). Then, we have: The expected utility of \\(y\\) is The expected utility of \\(x\\) is and The expected utility of \\(z\\) is There are three cases: Suppose that \\(a=1-a\\) (i.e., \\(a=1/2\\)). Then, Hence, \\(y\\) is not a best response. Suppose that \\(a>1-a\\). Then, Hence, \\(y\\) is not a best response. Suppose that \\(1-a>a\\). Then, Hence, \\(y\\) is not a best response. In all of the cases, \\(y\\) is not a best response. The second fundamental result analyzes the consequences of\nrationality and common belief/knowledge of rationality\nin extensive games (i.e., trees instead of matrices). Here,\nthe most well-known solution concept is the so-called subgame\nperfect equilibrium, also known as backward induction in\ngames of perfect information. The epistemic characterization of this\nsolution concept is in terms of “substantive rationality”\nand common belief that all players are substantively rational\n(cf. also Vanderschraaf & Sillari 2009: sec. 2.8). The main point\nthat we highlight in this section, which is by now widely acknowledged\nin the literature, is:  Belief revision policies play a key role\nin the epistemic analysis of extensive games\n The most well-known illustration of this is through the comparison\nof two apparently contradictory results regarding the consequences of\nassuming rationality and common knowledge of rationality in extensive\ngames. Aumann (1995) showed that this epistemic condition implies that\nthe players will play according to the backward induction solution\nwhile Stalnaker (1998) argued that this is not necessarily true. The\ncrucial difference between these two results is the way in which they\nmodel the players’ belief change upon (hypothetically) learning\nthat an opponent has deviated from the backward induction path. Extensive games make explicit the sequential structure of choices\nin a game situation. In this section, we focus on games of perfect\ninformation in which there is no uncertainty about earlier\nchoices in the game. These games are represented by tree-like\nstructures: \nDefinition 4.3 (Perfect Information Extensive Game)\nAn extensive game is a tuple \\(\\langle N, T, Act, \\tau, \\{ u_i \\}_{i\\in N}\\rangle\\), where\n \\(N\\) is a finite set of players; \\(T\\) is a tree describing the temporal structure of the game\nsituation: Formally, \\(T\\) consists of a set of nodes and an immediate\nsuccessor relation \\(\\rightarrowtail\\). Let \\(Z\\) denote the set of\nterminal nodes (i.e., nodes without any successors) and \\(V\\) the\nremaining nodes (called decision nodes). Let \\(v_0\\) denote the initial\nnode (i.e., the root of the tree). The edges at a\ndecision node \\(v\\in V\\) are each labeled with actions\nfrom a set \\(Act\\). Let \\(Act(v)\\) denote the set of actions available at\n\\(v\\). Let \\(\\rightsquigarrow\\) be the transitive closure of\n\\(\\rightarrowtail\\). \\(\\tau\\) is a turn function assigning a player to each node \\(v\\in V\\)\n(for a player \\(i\\in N\\), let \\(V_i=\\{v\\in V \\mid \\tau(v)=i\\}\\)). \\(u_i: Z\\rightarrow \\mathbb{R}\\) is the utility function for player\n\\(i\\) assigning real numbers to outcome nodes. A strategy is a term of art in extensive games. It\ndenotes a plan for every eventuality, which tells an agent what to do\nat all histories she is to play, even those which are excluded by the\nstrategy itself. Definition 4.4 (Strategies)\nA strategy for player \\(i\\) is a function \\(s_i:V_i\n\\rightarrow Act\\) where for all \\(v\\in V_i\\), \\(s_i(v)\\in Act(v)\\). A\nstrategy profile, denoted \\({\\mathbf{s}}\\), is an element of \\(\\Pi_{i\\in\nN} S_i\\). Given a strategy profile \\({\\mathbf{s}}\\), let \\({\\mathbf{s}}_i\\)\nbe player \\(i\\)’s component of \\({\\mathbf{s}}\\) and\n\\({\\mathbf{s}}_{-i}\\) the sequence of strategies form \\({\\mathbf{s}}\\) for\nall players except \\(i\\). Each strategy profile \\({\\mathbf{s}}\\) generates a path through an\nextensive game, where a path is a maximal sequence of nodes from the\nextensive game ordered by the immediate successor relations\n\\(\\rightarrowtail\\). We say that \\(v\\) is reached by a\nstrategy profile \\({\\mathbf{s}}\\) is \\(v\\) is on the path generated by\n\\({\\mathbf{s}}\\). Suppose that \\(v\\) is any node in an extensive game. Let\n\\(out(v,{\\mathbf{s}})\\) be the terminal node that is reached if,\nstarting at node \\(v\\), all the players move according to their\nrespective strategies in the profile \\({\\mathbf{s}}\\). Given a decision\nnode \\(v\\in V_i\\) for player \\(i\\), a strategy \\(s_i\\) for player \\(i\\), and a\nset \\(X\\subseteq S_{-i}\\) of strategy profiles of the opponents of \\(i\\),\nlet \\(Out_i(v,s_i, X)=\\{out(v, (s_i, s_{-i})) \\mid s_{-i}\\in X\\}\\). That\nis, \\(Out_i(v, s_i, X)\\) is the set of terminal nodes that may be\nreached if, starting at node \\(v\\), player \\(i\\) uses strategy \\(s_i\\) and\n\\(i\\)’s opponents use strategy profiles from \\(X\\). The following example of a perfect information extensive game will\nbe used to illustrate these concepts. The game is an instance of the\nwell-known centipede game, which has played an important role\nin the epistemic game theory literature on extensive games. Figure 17: An extensive game The decision nodes for \\(A\\) and \\(B\\) respectively are \\(V_A=\\{v_1,\nv_3\\}\\) and \\(V_B=\\{v_2\\}\\); and the outcome nodes are \\(O=\\{o_1, o_2,\no_3, o_4\\}\\). The labels of the edges in the above tree are the actions\navailable to each player. For instance, \\(Act(v_1)=\\{O_1, I_1\\}\\). There\nare four strategies for \\(A\\) and two strategies for \\(B\\). To simplify\nnotation, we denote the players’ strategies by the sequence of\nchoices at each of their decision nodes. For example, \\(A\\)’s\nstrategy \\(s_A^1\\) defined as \\(s_A^1(v_1)=O_1\\) and \\(s_A^1(v_3)=O_3\\) is\ndenoted by the sequence \\(O_1O_3\\). Thus, \\(A\\)’s strategies are:\n\\(s_A^1=O_1O_3\\), \\(s_A^2=O_1I_3\\), \\(s_A^3=I_1O_3\\) and\n\\(s_A^4=I_1I_3\\). Note that \\(A\\)’s strategy \\(s_A^2\\) specifies a\nmove at \\(v_3\\), even though the earlier move at \\(v_1\\), \\(O_1\\), means\nthat \\(A\\) will not be given a chance to move at \\(v_3\\). Similarly,\nBob’s strategies will be denoted by \\(s_B^1=O_2\\) and \\(s_B^2=I_2\\),\ngiving the actions chosen by \\(B\\) at his decision node. Then, for\nexample, \\(out(v_2,(s_A^2, s_B^2))=o_4\\). Finally, if \\(X=\\{s_A^1,\ns_A^4\\}\\), then \\(Out_B(v_2,s_B^2, X)=\\{o_3, o_4\\}\\). There are a variety of ways to describe the players’\nknowledge and beliefs in an extensive game. The game models vary\naccording to which epistemic attitudes are represented (e.g.,\nknowledge and/or various notions of beliefs) and precisely how the\nplayers’ disposition to revise their beliefs during a play of\nthe game is represented. Consult Battigalli, Di Tillio, & Samet\n(2013); Baltag, Smets, & Zvesper (2009); and Battigalli &\nSiniscalchi (2002) for a sampling of the different types of models\nfound in the literature. One of the simplest approaches is to use the epistemic models\nintroduced in Section 2.2 (cf.  Aumann 1995;\nHalpern 2001b). An epistemic model of an extensive game \\(G=\\langle N,\nT, Act, \\tau, \\{ u_i \\}_{i \\in N}\\rangle\\) is a tuple \\(\\langle W,\n\\{\\Pi_i\\}_{i\\in N}, \\sigma\\rangle \\) where \\(W\\) is a nonempty set of\nstates; for each \\(i\\in N\\), \\(\\Pi_i\\) is a partition on \\(W\\); and\n\\(\\sigma:W\\rightarrow \\Pi_{i\\in N} S_i\\) is a function assigning to each\nstate \\(w\\), a strategy profile from \\(G\\). If \\(\\sigma(w)={\\mathbf{s}}\\),\nthen we write \\(\\sigma_i(w)\\) for \\({\\mathbf{s}}_i\\) and \\(\\sigma_{-i}(w)\\)\nfor \\({\\mathbf{s}}_{-i}\\). As usual, we assume that players know their\nown strategies: for all \\(w\\in W\\), if \\(w'\\in \\Pi_i(w)\\), then\n\\(\\sigma_i(w)=\\sigma_i(w')\\). The rationality of a strategy at a decision node depends both on\nwhat actions the strategy prescribes at all future decision\nnodes and what the players know about the strategies that\ntheir opponents are following. Let \\(S_{-i}(w)=\\{\\sigma_{-i}(w') \\mid\nw'\\in \\Pi_i(w)\\}\\) be the set of strategy profiles of player\n\\(i\\)’s opponents that \\(i\\) thinks are possible at state \\(w\\). Then,\n\\(Out_i(v, s_i, S_{-i}(w))\\) is the set of outcomes that player \\(i\\)\nthinks are possible starting at node \\(v\\) if she follows strategy\n\\(s_i\\). Definition 4.5 (Rationality at a decision node)\nPlayer \\(i\\) is rational at node \\(v\\in V_i\\) in state\n\\(w\\) provided, for all strategies \\(s_i\\) such that \\(s_i\\ne\n\\sigma_i(w)\\), there is an \\(o'\\in Out_i(v, s_i, S_{-i}(w))\\) and \\(o\\in\nOut_i(v, \\sigma_i(w), S_{-i}(w))\\) such that \\(u_i(o)\\ge\nu_i(o')\\). So, a player \\(i\\) is rational at a decision node \\(v\\in V_i\\) in state\n\\(w\\) provided that \\(i\\) does not know that there is an alternative\nstrategy that would give her a higher payoff. Definition 4.6 (Substantive rationality)\nPlayer \\(i\\) is substantively rational at state \\(w\\)\nprovided for all decision nodes \\(v\\in V_i\\), \\(i\\) is rational at \\(v\\) in\nstate \\(w\\). We can define the event that player \\(i\\) is substantively rational\nis the standard way: \\(\\mathsf{Rat}_i=\\{w \\mid \\mbox{player } i\\) is\nsubstantively rational at state \\(w\\}\\); and so, the event that all\nplayers are substantively rational is \\(\\mathsf{Rat}=\\bigcap_{i\\in N}\n\\mathsf{Rat}_i\\). This notion of rationality at a decision node \\(v\\) is\nforward-looking in the sense that it only takes account of the\npossibilities that can arise from that point on in the\ngame. It does not take account of the previous moves leading to\n\\(v\\)—i.e., which choices have or could have lead to \\(v\\). We shall\nreturn to this in the discussion below. An important consequence of this is that the rationality of choices\nat nodes that are only followed by terminal nodes are independent of\nthe relevant player’s knowledge. Call a node\n\\(v\\) pre-terminal if all of \\(v\\)’s immediate\nsuccessors are terminal nodes. At such nodes, it does not matter what\nstrategies the player thinks are possible: If \\(v\\) is a pre-terminal\nnode and player \\(i\\) is moving at \\(v\\), then for all states in \\(w\\) in an\nepistemic model of the game and for all strategies \\(s_i\\in S_i\\),\n\\(Out_i(v, s_i, S_{-i}(w))=\\{s_i(v)\\}\\). This means, for example, that\nfor any state \\(w\\) in an epistemic model for the extensive game\nin Figure 17, the only strategies that are\nrational at node \\(v_3\\) in \\(w\\) are those that prescribe that \\(A\\)\nchooses \\(O_3\\) at node \\(v_3\\). Therefore, if \\(w\\in \\mathsf{Rat}_A\\), then\n\\(\\sigma_A(w)(v_3)=O_3\\). Whatever \\(A\\) knows, or rather knew about what\n\\(B\\) would do, if the game reaches the node \\(v_3\\), then the only\nrational choice for \\(A\\) is \\(O_3\\). Information about the rationality of players at pre-terminal nodes\nis very important for players choosing earlier in the game. Returning\nto the game in Figure 17, if \\(B\\) knows that \\(A\\)\nis substantively rational at a state \\(w\\) in an epistemic model of the\ngame, then \\(\\Pi_B(w)\\subseteq \\mathsf{Rat}_A\\). Given the above\nargument, this means that if \\(w'\\in \\Pi_B(w)\\), then\n\\(\\sigma_A(w')(v_3)=O_3\\). Thus, we have for any state \\(w\\) in an\nepistemic model of the game, and, of course, But then,\n\\((O_2)\\) is the only strategy that is rational for \\(B\\) at \\(v_2\\) in any\nstate \\(w\\) (this follows since \\(u_B(o_2)=2\\ge 1=u_B(o_3)\\)). This means\nthat if \\(w\\in \\mathsf{Rat}_B\\) and \\(\\Pi_B(w)\\subseteq \\mathsf{Rat}_A\\),\nthen \\(\\sigma_B(w)(v_2)=O_2\\). Finally, if \\(A\\) knows that \\(B\\) knows that\n\\(A\\) is substantively rational, then A similar argument shows that if \\(w\\in \\mathsf{Rat}_A\\) and \\(w\\in\nK_A(K_B(\\mathsf{Rat}_A))\\), then \\(\\sigma_A(w)(v_1)=O_1\\). The strategy profile \\((O_1O_3, O_2)\\) is the unique\npure-strategy sub-game perfect equilibrium (Selten 1975) of\nthe game in Figure 17. Furthermore, the\nreasoning that we went through in the previous paragraphs is very\nclose to backward induction algorithm. This algorithm can be\nused to calculate the sub-game perfect equilibrium in any perfect\ninformation game in which all players receive unique payoffs at each\noutcome.[13]\n The algorithm runs as follows: BI Algorithm\nAt terminal nodes, players already have the\nnodes marked with their utilities. At a non-terminal node \\(v\\), once\nall immediate successors are marked, the node is marked as follows:\nfind the immediate successor \\(d\\) that has the highest utility for\nplayer \\(\\tau(v)\\) (the players whose turn it is to move at \\(v\\)). Copy\nthe utilities from \\(d\\) onto \\(v\\). Given a marked game tree, the unique path that leads from the root\n\\(v_0\\) of the game tree to the outcome with the utilities that match\nthe utilities assigned to \\(v_0\\) is called the backward\ninduction path. In fact, the markings on each and every node\n(even nodes not on the backward induction path) defines a unique path\nthrough the game tree. These paths can be used to define strategies\nfor each player: At each decision node \\(v\\), choose the action that is\nconsistent with the path from \\(v\\). Let \\(BI\\) denote the\nresulting backward induction profile (where each\nplayer is following the strategy given by the backward induction\nalgorithm). Aumann (1995) showed that the above reasoning can be carried out\nfor any extensive game of perfect information. Theorem 4.7 (Aumann 1995)\nSuppose that \\(G\\) is an extensive game of perfect information and\n\\({\\mathbf{s}}\\) is a strategy profile for \\(G\\). The following are\nequivalent: There is a state \\(w\\) in an epistemic model of \\(G\\) such that\n\\(\\sigma(w) = {\\mathbf{s}}\\) and \\(w \\in C_N(\\mathsf{Rat})\\) (there is\ncommon knowledge that all players are substantively\nrational). \\({\\mathbf{s}}\\) is a sub-game perfect equilibrium of \\(G\\). This result has been extensively discussed. The standard ground of\ncontention is that common knowledge of rationality used in this\nargument seems self-defeating, at least intuitively. Recall\nthat we asked what would \\(B\\) do at node \\(v_2\\) under common knowledge\nof rationality, and we concluded that he would choose \\(O_2\\). But, if\nthe game ever reaches that state, then, by the theorem above, \\(B\\) has\nto conclude that either \\(A\\) is not rational, or that she does not know\nthat he is. Both violate common knowledge of rationality. Is there a\ncontradiction here? This entry will not survey the extensive\nliterature on this question. The reader can consult the references in\nBruin 2010. Our point here is rather that how one looks at this\npotential paradox hinges on the way the players will revise their\nbeliefs in “future” rationality in the light of observing\na move that would be “irrational” under common knowledge\nof rationality. Stalnaker (1996, 1998) offers a different perspective on backward\ninduction. The difference with Aumann’s analysis is best\nillustrated with the following example: Figure 18: An extensive game In the above game the backward induction profile is \\((I_1I_3, I_2)\\)\nleading to the outcome \\(o_4\\) with both players receiving a payoff of\n\\(3\\). Consider an epistemic model with a single state \\(w\\) where\n\\(\\sigma(w)=(O_1I_3,O_2)\\). This is not the backward induction profile,\nand so, by Aumann’s Theorem (Theorem\n4.7) it cannot be common knowledge among \\(A\\) and \\(B\\) at state\n\\(w\\) that both \\(A\\) and \\(B\\) are substantively rational. Recall that a strategy for a player \\(i\\) specifies choices\nat all decision nodes for \\(i\\), even those nodes that are\nimpossible to reach given earlier moves prescribed by the\nstrategy. Thus, strategies include “counterfactual”\ninformation about what players would do if they were given a chance to\nmove at each of their decision nodes. In the single state epistemic\nmodel, \\(B\\) knows that \\(A\\) is following the strategy \\(O_1I_3\\). This\nmeans that \\(B\\) knows two things about \\(A\\)’s choice behavior in\nthe game. The first is that \\(A\\) is choosing \\(O_1\\) initially. The\nsecond is that if \\(A\\) where given the opportunity to choose at \\(v_3\\),\nthen she would choose \\(I_3\\). Now, given \\(B\\)’s knowledge about\nwhat \\(A\\) is doing, there is a sense in which whatever \\(B\\) would\nchoose at \\(v_2\\), his choice is rational. This follows trivially\nsince \\(A\\)’s initial choice prescribed by her strategy at \\(w\\)\nmakes it impossible for \\(B\\) to move. Say that a player \\(i\\)\nis materially rational at a state \\(w\\) in an epistemic\nmodel of a game if \\(i\\) is rational at all decision nodes \\(v\\in V_i\\) in\nstate \\(w\\) that are reachable according to the strategy profile\n\\(\\sigma(w)\\). We have seen that \\(B\\) is trivially materially\nrational. Furthermore, \\(A\\) is materially rational since she knows that\n\\(B\\) is choosing \\(O_2\\) (i.e., \\(S_{-A}(w)=\\{O_2\\}\\)). Thus, \\(Out_A(v_1,\nO_1, S_{-i}(w))=\\{o_1\\}\\) and \\(Out_A(v_1, O_I, S_{-i}(w))=\\{o_2\\}\\); and\nso, \\(A\\)’s choice of \\(O_1\\) at \\(v_1\\) makes her materially rational\nat \\(w\\). The main point of contention between Aumann and Stalnaker\nboils down to whether the single state epistemic model includes enough\ninformation about what exactly \\(B\\) thinks about \\(A\\)’s choice at\n\\(v_3\\) when assessing the rationality of\n\\(B\\)’s hypothetical choice of \\(O_2\\) at \\(v_2\\). According to Aumann, \\(B\\) is not substantively rational: Since\n\\(S_{-B}(w)=\\{O_1I_3\\}\\), we have and and so, \\(B\\) is not\nrational at \\(v_2\\) in \\(w\\) (note that\n\\(u_B(o_4)=3>1=u_B(o_2)\\)). Stalnaker suggests that the players\nshould be endowed with a belief revision policy that\ndescribes which informational state they would revert to in case they\nwere to observe moves that are inconsistent with what they know about\ntheir opponents’ strategies. If \\(B\\) does learn that he can in\nfact move, then he has learned something about \\(A\\)’s\nstrategy. In particular, he now knows that she cannot be following any\nstrategy that prescribes that she chooses \\(O_1\\) at \\(v_1\\) (so, in\nparticular, she cannot be following the strategy \\(O_1I_3\\).) Suppose\nthat \\(B\\) is disposed to react to surprising information about\n\\(A\\)’s choice of strategy as follows: Upon learning that \\(A\\) is\nnot following a strategy in which she chooses \\(O_1\\) at \\(v_1\\), he\nconcludes that she is following strategy \\(I_1O_3\\). That is,\n\\(B\\)’s “belief revision policy” can be summarized as\nfollows: If \\(A\\) makes one “irrational move”, then she will\nmake another one. Stalnaker explains the apparent tension between this\nbelief revision policy and his knowledge that if \\(A\\) where given the\nopportunity to choose at \\(v_3\\), then she would choose \\(I_3\\) as\nfollows: To think there is something incoherent about this combination of\nbeliefs and belief revision policy is to confuse epistemic with causal\ncounterfactuals—it would be like thinking that because I believe\nthat if Shakespeare hadn’t written Hamlet, it would have never\nbeen written by anyone, I must therefore be disposed to conclude that\nHamlet was never written, were I to learn that Shakespeare was in fact\nnot its author. (Stalnaker 1996: 152) Then, with respect to \\(B\\)’s appropriately updated knowledge\nabout \\(A\\)’s choice at \\(v_3\\) (according to his specified belief\nrevision policy), his strategy \\(O_2\\) is in fact rational. According to\nStalnaker, the rationality of a choice at a node \\(v\\) should be\nevaluated in the (counterfactual) epistemic state the player would\nbe in if that node was reached. Assuming \\(A\\) knows that \\(B\\) is\nusing the belief revision policy described above, then \\(A\\) knows that\n\\(B\\) is substantively rational in Stalnaker’s sense. If the model\nincludes explicit information about the players’ belief revision\npolicy, then there can be common knowledge of substantive rationality\n(in Stalnaker’s sense) yet the players’ choices do not\nconform to the backward induction profile. In the previous section, we assumed that the players interpret an\nopponent’s deviation from expected play in an extensive game\n(e.g., deviation from the backward induction path) as an indication\nthat that player will choose “irrationally” at future\ndecision nodes. However, this is just one example of a belief revision\npolicy. It is not suggested that this is the belief revision policy\nthat players should adopt. Stalnaker’s central claim is\nthat models of extensive games should include a component that\ndescribes the players’ disposition to change their beliefs\nduring a play of the game, which may vary from model to model or even\namong the players in a single model: Faced with surprising behavior in the course of a game, the players\nmust decide what then to believe. Their strategies will be based on\nhow their beliefs would be revised, which will in turn be based on\ntheir epistemic priorities—whether an unexpected action should\nbe regarded as an isolated mistake that is thereby epistemically\nindependent of beliefs about subsequent actions, or whether it\nreveals, intentionally or inadvertently, something about the\nplayer’s expectations, and so about the way she is likely to\nbehave in the future. The players must decide, but the theorists\nshould not—at least they should not try to generalize about\nepistemic priorities that are meant to apply to any rational agent in\nall situations. (Stalnaker 1998: 54) One belief revision policy that has been extensively discussed in\nthe epistemic game theory literature is the rationalizability\nprinciple. Battigalli (1997) describes this belief revision\npolicy as follows: Rationalizability Principle\nA player should always try to interpret her information about the\nbehavior of her opponents assuming that they are not implementing\n‘irrational’ strategies. This belief revision policy is closely related to\nso-called forward induction reasoning. To illustrate,\nconsider the following imperfect information game: Figure 19 In the above game, \\(A\\) can either exit the game initially (by\nchoosing \\(e\\)) for a guaranteed payoff of \\(2\\) or decide to play a game\nof imperfect information with \\(B\\). Notice that \\(r_1\\) is strictly\ndominated by \\(e\\): No matter what \\(B\\) chooses at \\(v_3\\), \\(A\\) is better\noff choosing \\(e\\). This means that if \\(A\\) is following a rational\nstrategy, then she will not choose \\(r_1\\) at \\(v_1\\). According to the\nrationalizability principle, \\(B\\) is disposed to believe that \\(A\\) did\nnot choose \\(r_1\\) if he is given a chance to move. Thus, assuming that\n\\(B\\) knows the structure of the game and revises his beliefs according\nto the rationalizability principle, his only rational strategy is to\nchoose \\(l_2\\) at his informational cell (consisting of \\(\\{v_2,\nv_3\\})\\). If \\(A\\) can anticipate this reasoning, then her only rational\nstrategy is to choose \\(e\\) at \\(v_1\\). This is the forward induction\noutcome of the above game. Battigalli & Siniscalchi (2002) develop an epistemic analysis\nof forward induction reasoning in extensive games (cf. also, Stalnaker\n1998: sec. 6). They build on an idea of Stalnaker (1998, 1996) to\ncharacterize forward induction solution concepts in terms of\ncommon strong belief in rationality. We discussed the\ndefinition of “strong belief” in Section\n2.4. The mathematical representation of beliefs in Battigalli\n& Siniscalchi (2002) is different, although the underlying idea is\nthe same. A player strongly believes an event \\(E\\) provided she\nbelieves \\(E\\) is true at the beginning of the game (in the sense that\nshe assigns probability 1 to \\(E\\)) and continues to believe \\(E\\) as long\nas it is not falsified by the evidence. The evidence\navailable to a player in an extensive game consists of the\nobservations of the previous moves that are consistent with the\nstructure of the game tree—i.e., the paths through a game\ntree. A complete discussion of this approach is beyond the scope of\nthe entry. Consult Battigalli & Siniscalchi (2002); Baltag et\nal. (2009); Battigalli & Friedenberg (2012); Bonanno (2013); Perea\n(2012, 2014); and van Benthem & Gheerbrant (2010) for a discussion\nof this approach and alternative epistemic analyses of backward and\nforward induction. In this section, we present a number of results that build on the\nmethodology presented in the previous section. We discuss the\ncharacterization of the Nash equilibrium, incorporate considerations\nof weak dominance into the players’ reasoning and allow the\nplayers to be unaware, as opposed to uncertain,\nabout some aspects of the game. Iterated elimination of strictly dominated strategies is a very\nintuitive concept, but for many games it does not tell anything about\nwhat the players will or should choose. In coordination games\n(Figure 1 above) for instance, all\nprofiles, can be played under rationality and common belief of\nrationality. Looking again at Figure 1, one can\nask what would happen if Bob knew (that is had correct\nbeliefs about) Ann’s strategy choice? Intuitively, it is quite\nclear that his rational choice is to coordinate with her. If\nhe knows that she plays \\(t\\), for instance, then playing \\(l\\)\nis clearly the only rational choice for him, and similarly, if he\nknows that she plays \\(b\\), then \\(r\\) is the only rational choice. The\nsituation is symmetric for Ann. For instance, if she knows that Bob\nplays \\(l\\), then her only rational choice is to choose \\(t\\). More\nformally, the only states where Ann is rational and her\ntype knows (i.e., is correct and assigns probability 1 to)\nBob’s strategy choice and where Bob is also rational and his\ntype knows Ann’s strategy choices are states where they\nplay either \\((t,l)\\) or \\((b,r)\\), the pure-strategy Nash equilibria of\nthe game. A Nash equilibrium is a profile where no player has an\nincentive to unilaterally deviate from his strategy choice. In other\nwords, a Nash equilibrium is a combination of (possibly mixed)\nstrategies such that they all play their best response given the\nstrategy choices of the others. Again, \\((t,l)\\) and \\((b,r)\\) are the\nonly pure-strategy equilibria of the above coordination game. Nash\nequilibrium, and its numerous refinements, is arguably the game\ntheoretical solution concept that has been most used in game theory\n(Aumann & Hart 1994) and philosophy (e.g., famously in Lewis\n1969). The seminal result of Aumann & Brandenburger 1995 provides an\nepistemic characterization of the Nash equilibrium in terms\nof mutual knowledge of strategy choices (and the structure of\nthe game). See, also, Spohn (1982) for an early statement. Before\nstating the theorem, we discuss an example from Aumann &\nBrandenburger (1995) that illustrates the key ideas. Consider the\nfollowing coordination game: Figure 20 The two pure-strategy Nash equilibria are \\((u,l)\\) and \\((d,r)\\)\n(there is also a mixed-strategy equilibrium). As usual, we fix an\ninformational context for this game. Let \\(\\T\\) be a type space for the\ngame with three types for each player \\(T_A=\\{a_1,a_2, a_3\\}\\) and\n\\(T_B=\\{b_1,b_2,b_3\\}\\) with the following type functions: Figure 21 Consider the state \\((d,r,a_3,b_3)\\). Both \\(a_3\\) and \\(b_3\\) correctly\nbelieve (i.e., assign probability 1 to) that the outcome is \\((d,r)\\)\n(we have \\(\\lambda_A(a_3)(r)=\\lambda_B(b_3)(d)=1\\)). This fact is not\ncommon knowledge: \\(a_3\\) assigns a 0.5 probability to Bob being of type\n\\(b_2\\), and type \\(b_2\\) assigns a 0.5 probability to Ann playing\n\\(l\\). Thus, Ann does not know that Bob knows that she is playing \\(r\\)\n(here, “knowledge” is identified with “probability\n1” as it is in Aumann & Brandenburger 1995). Furthermore,\nwhile it is true that both Ann and Bob are rational, it is not common\nknowledge that they are rational. Indeed, the type \\(a_3\\) assigns a 0.5\nprobability to Bob being of type \\(b_2\\) and choosing \\(r\\); however, this\nis irrational since \\(b_2\\) believes that both of Ann’s options\nare equally probable. The example above is a situation where there is mutual knowledge of\nthe choices of the players. Indeed, it is not hard to see that in any\ntype space for a 2-player game \\(G\\), if \\((s,t)\\) is a state where there\nis mutual knowledge that player \\(i\\) is choosing \\(s_i\\) and the players\nare rational, then, \\(s\\) constitutes a (pure-strategy) Nash\nEquilibrium. There is a more general theorem concerning mixed strategy\nequilibrium. Recall that a conjecture for player \\(i\\) is a probability\nmeasure over the strategy choices of her opponents. Theorem 5.1  (Aumann & Brandenburger 1995: Theorem A)\nSuppose that \\(G\\) is a 2-person strategic game, \\((p_1,p_2)\\) are\nconjectures for players 1 and 2, and \\(\\T\\) is a type space for \\(G\\). If\n\\((s,t)\\) is a state in \\(\\T\\) where for \\(i=1,2\\), \\(t_i\\) assigns\nprobability 1 to the events (a) both players are rational (i.e.,\nmaximize expected utility), (b) the game is \\(G\\) and (c) for \\(i=1,2\\),\nplayer \\(i\\)’s conjecture is \\(p_i\\), then \\((p_1, p_2)\\) constitutes\na Nash equilibrium. The general version of this result, for arbitrary finite number of\nagents and allowing for mixed strategies, requires common\nknowledge of conjectures, i.e., of each player’s\nprobabilistic beliefs in the other’s choices. See Aumann &\nBrandenburger (1995: Theorem B) for precise formulation of the result,\nand, again, Spohn (1982) for an early version. See, also, Perea (2007)\nand Tan & Werlang (1988) for similar results about the Nash\nequilibrium. This epistemic characterization of Nash equilibrium requires\nmutual knowledge and rather than beliefs. The result fails\nwhen agents can be mistaken about the strategy choice of the\nothers. This has lead some authors to criticize this epistemic\ncharacterization: See Gintis (2009) and Bruin (2010), for\ninstance. How could the players ever know what the others are\nchoosing? Is it not contrary to the very idea of a game, where the\nplayers are free to choose whatever they want (Baltag et\nal. 2009)? One popular response to this criticism (Brandenburger 2010; Perea\n2012) is that the above result tells us something about Nash\nequilibrium as a solution concept, namely that it\nalleviates strategic uncertainty. Indeed, returning to the\nterminology introduced in Section 1.3, the\nepistemic conditions for Nash equilibrium are those that correspond to\nthe ex post state of information disclosure, “when all\nis said and done”, to put it figuratively. When players have\nreached full knowledge of what the others are going to do, there is\nnothing left to think about regarding the other players as rational,\ndeliberating agents. The consequences of each of the players’\nactions are now certain. The only task that remains is to compute\nwhich action is recommended by the adopted choice rule, and this does\nnot involve any specific information about the other players’\nbeliefs. Their choices are fixed, after all. The idea here is not to reject the epistemic characterization of\nNash Equilibrium on the grounds that it rests on unrealistic\nassumptions, but, rather, to view it as a lesson learned about Nash\nEquilibrium itself. From an epistemic point of view, where one is\nfocused on strategic reasoning about what others are going to\ndo and are thinking, this solution concepts might be of less\ninterest. There is another important lesson to draw from this epistemic\ncharacterization result. The widespread idea that game theory\n“assumes common knowledge of rationality”, perhaps in\nconjunction with the extensive use of equilibrium concepts in\ngame-theoretic analysis, has lead to misconception that the Nash\nEquilibrium either requires common knowledge of rationality,\nor that common knowledge of rationality is sufficient for the players\nto play according to a Nash equilibrium. To be sure, game theoretic\nmodels do assume that the structure of the game is common knowledge\n(though, see Section 5.3). Nonetheless, the\nabove result shows that both of these ideas are incorrect:\n \nCommon knowledge of rationality is neither necessary nor sufficient\nfor Nash Equilibrium.\n In fact, as we just stressed, Nash equilibrium can be played under\nfull uncertainty, and a fortiori under higher-order\nuncertainty, about the rationality of others. In recent years, a number of so-called “modal”\ncharacterizations of Nash Equilibrium have been proposed, mostly using\ntechniques from modal logic (see Hoek & Pauly 2007 for\ndetails). These results typically devise a modal logical language to\ndescribe games in strategic form, typically including modalities for\nthe players’ actions and preference, and show that the notion of\nprofile being a Nash Equilibrium language is definable in\nsuch a language. Most of these characterizations are not epistemic, and thus fall\noutside the scope of this entry. In context of this entry, it is\nimportant to note that most of these results aim at something\ndifferent than the epistemic characterization which we are discussing\nin this section. Mostly developed in Computer Sciences, these logical\nlanguages have been used to verify properties of multi-agents systems,\nnot to provide epistemic foundations to this solution\nconcept. However, note that in recent years, a number of logical\ncharacterizations of Nash equilibrium do explicitly use epistemic\nconcepts (see, for example, van Benthem et al. 2009; Lorini &\nSchwarzentruber 2010). It is not hard to find a game and an informational context where\nthere is at least one player without a unique “rational\nchoice”. How should a rational player incorporate the\ninformation that more than one action is classified as\n“choice-worthy” or “rationally permissible”\n(according to some choice rule) for her opponent(s)? In such a\nsituation, it is natural to require that the player does not rule\nout the possibility that her opponent will pick a\n“choice-worthy” option. More generally, the players should\nbe “cautious” about which of their opponents’\noptions they rule out. Assuming that the players’ beliefs are “cautious”\nis naturally related to weak dominance (recall the characterization of\nweak dominance, Section 3.2\nin which a strategy is weakly dominated iff it does not maximize\nexpected utility with respect to any full support probability\nmeasure). A key issue in epistemic game theory is the epistemic\nanalysis of iterated removal of weakly dominated strategies. Many\nauthors have pointed out puzzles surrounding such an analysis (Asheim\n& Dufwenberg 2003; Brandenburger, Friedenberg & Keisler 2008;\nCubitt & Sugden 1994; Samuelson 1992). For example, Samuelson\n(1992) showed (among other things) that the analogue of Theorem 4.1 is\nnot true for iterated removal of weakly dominated strategies. The main\nproblem is illustrated by the following game: Figure 22 In the above game, \\(d\\) is weakly dominated by \\(u\\) for Ann. If Bob\nknows that Ann is rational (in the sense that she will not choose a\nweakly dominated strategy), then he can rule out option \\(d\\). In the\nsmaller game, action \\(r\\) is now strictly dominated by \\(l\\) for Bob. If\nAnn knows that Bob is rational and that Bob knows that she is rational\n(and so, rules out option \\(d\\)), then she can rule out option\n\\(r\\). Assuming that the above reasoning is transparent to both Ann and\nBob, it is common knowledge that Ann will play \\(u\\) and Bob will play\n\\(l\\). But now, what is the reason for Bob to rule out the possibility\nthat Ann will play \\(d\\)? He knows that Ann knows that he is going to\nplay \\(l\\) and both \\(u\\) and \\(d\\) are best responses to \\(l\\). The problem\nis that assuming that the players’ beliefs are cautious\nconflicts with the logic of iterated removal of weakly dominated\nstrategies. This issue is nicely described in a well-known microeconomics\ntextbook: [T]he argument for deletion of a weakly dominated strategy for\nplayer \\(i\\) is that he contemplates the possibility that every strategy\ncombination of his rivals occurs with positive probability. However,\nthis hypothesis clashes with the logic of iterated deletion, which\nassumes, precisely that eliminated strategies are not expected to\noccur. (Mas-Colell, Winston, & Green 1995: 240) The extent of this conflict is nicely illustrated in Samuelson\n(1992). In particular, Samuelson (1992) shows that there is no\nepistemic-probability \nmodel[14]\n of the above game with\na state satisfying common knowledge of rationality (where\n“rationality” means that players do not choose weakly\ndominated strategies). Prima facie, this is puzzling: What\nabout the epistemic-probability model consisting of a single state \\(w\\)\nassigned the profile \\((u, l)\\)? Isn’t this a model of the above\ngame where there is a state satisfying common knowledge that the\nplayers do not choose weakly dominated strategies? The problem is that\nthe players do not have “cautious” beliefs in this model\n(in particular, Bob’s beliefs are not cautious in the sense\ndescribed below). Recall that having a cautious belief means that a\nplayer cannot know which options her opponent(s)\nwill \npick[15]\n from a set of choice-worthy options (in the\nabove game, if Ann knows that Bob is choosing \\(l\\), then both\n\\(u\\) and \\(d\\) are “choice-worthy”, so Bob\ncannot know that Ann is choosing \\(u\\)). This suggests an\nadditional requirement on a game model: Let \\(\\M=\\epprobmodel\\) be an\nepistemic-probability model. For each action \\(a\\in \\cup_{i\\in\\Agt}\nS_i\\), let \\({[\\![{a}]\\!]}=\\{w \\mid (\\sigma(w))_i=a\\}\\). If \\(a\\in S_i\\) is rational for player \\(i\\) at state\n\\(w\\), then for all players \\(j\\ne i\\), \\({[\\![{a}]\\!]}\\cap \\Pi_j(w)\\ne\n\\emptyset\\). This means that a player cannot know that her opponent\nwill not choose an action at a state \\(w\\) which is deemed rational\n(according to some choice rule). This property is called\n“privacy of tie-breaking” by Cubitt and Sugden (2011: 8)\nand “no extraneous beliefs” by Asheim and Dufwenberg\n(2003).[16]\n For an extended discussion of the above\nassumption see Cubitt & Sugden (2011). Given the above considerations, the epistemic analysis of iterated\nweak dominance is not a straightforward adaptation of the analysis of\niterated strict dominance discussed in the previous section. In\nparticular, any such analysis must resolve the conflict between\nstrategic reasoning where players rule out certain strategy\nchoices of their opponent(s) and admissibility considerations where\nplayers must consider all of their opponents’\noptions possible. A number of authors have developed\nframeworks that do resolve this conflict (Brandenburger et al. 2008;\nAsheim & Dufwenberg 2003; Halpern & Pass 2009). We sketch one\nof these solutions below: The key idea is to represent the players’ beliefs as\na lexicographic probability system (LPS). An LPS is a finite\nsequence of probability measures \\((p_1,p_2,\\ldots,p_n)\\) with supports\n(The support of a probability measure \\(p\\) defined on\na set of states \\(W\\) is the set of all states that have nonzero\nprobability; formally, \\(Supp(p)=\\{w \\mid p(w)>0\\}\\)) that do not\noverlap. This is interpreted as follows: if \\((p_1,\\ldots,p_n)\\)\nrepresents Ann’s beliefs, then \\(p_1\\) is Ann’s\n“initial hypothesis” about what Bob is going to do, \\(p_2\\)\nis Ann’s secondary hypothesis, and so on. In the above game, we\ncan describe Bob’s beliefs as follows: his initial hypothesis\nis that Ann will choose \\(U\\) with probability 1 and his secondary\nhypothesis is that she will choose \\(D\\) with probability 1. The\ninterpretation is that, although Bob does not rule out the possibility\nthat Ann will choose \\(D\\) (i.e., choose irrationally), he does consider\nit infinitely less likely than her choosing \\(U\\) (i.e.,\nchoosing rationally). So, representing beliefs as lexicographic probability measures\nresolves the conflict between strategic reasoning and the assumption\nthat players do not play weakly dominated strategies. However, there\nis another, more fundamental, issue that arises in the epistemic\nanalysis of iterated weak dominance: Under admissibility, Ann considers everything possible. But this is\nonly a decision-theoretic statement. Ann is in a game, so we imagine\nshe asks herself: “What about Bob? What does he consider\npossible?” If Ann truly considers everything possible, then it\nseems she should, in particular, allow for the possibility that Bob\ndoes not! Alternatively put, it seems that a full analysis of the\nadmissibility requirement should include the idea that other players\ndo not conform to the requirement.  (Brandenburger et al. 2008:\n313) There are two main ingredients to the epistemic characterization of\niterated weak dominance. The first is to represent the players’\nbeliefs as lexicographic probability systems. The second is to use a\nstronger notion of belief: A player assumes an event\n\\(E\\) provided \\(E\\) is infinitely more likely than \\(\\overline{E}\\) (on\nfinite spaces, this means each state in \\(E\\) is infinitely more likely\nthan states not in \\(E\\)). The key question is: What is the precise\nrelationship between the event “rationality and common\nassumption of rationality” and the strategies that survive\niterated removal of weakly dominated strategies? The precise answer\nturns out to be surprisingly subtle—the details are beyond the\nscope of this article (see Brandenburger et al. 2008). The game models introduced in Section 2 have\nbeen used to describe the uncertainty that the players have about what\ntheir opponents are going to do and are thinking in a game\nsituation. In the analyses provided thus far, the structure\nof the game (i.e., who is playing, what are the preferences of the\ndifferent players, and which actions are available) is assumed to be\ncommon knowledge among the players. However, there are many situations\nwhere the players do not have such complete information about\nthe game. There is no inherent difficulty in using the models\nfrom Section 2 to describe situations where\nplayers are not perfectly informed about the structure of the\ngame (for example, where there is some uncertainty about available\nactions). There is, however, a foundational issue that arises here. Suppose\nthat Ann considers it impossible that her opponent will\nchoose action \\(a\\). Now, there are many reasons why Ann would hold such\nan opinion. On the one hand, Ann may know something about what her\nopponent is going to do or is thinking which allows her to rule out\naction \\(a\\) as a live possibility—i.e., given all the evidence\nAnn has about her opponent, she concludes that action \\(a\\) is just not\nsomething her opponent will do. On the other hand, Ann may not even\nconceive of the possibility that her opponent will choose action\n\\(a\\). She may have a completely different model of the game in mind\nthan her opponents. The foundational question is: Can the game models\nintroduced in Section 2 faithfully represent\nthis latter type of uncertainty? The question is not whether one can formally describe what Ann\nknows and believes under the assumption that she considers it\nimpossible that her opponent will choose action \\(a\\). Indeed, an\nepistemic-probability model where Ann assigns probability zero to the\nevent that her opponent chooses action \\(a\\) is a perfectly good\ndescription of Ann’s epistemic state. The problem is that this\nmodel blurs an important distinction between Ann\nbeing unaware that action \\(a\\) is a live possibility and\nAnn ruling out that action \\(a\\) is a viable option for her\nopponent. This distinction is illustrated by the following snippet\nfrom the well-known Sherlock Holmes’ short story Silver Blaze\n(Doyle 1894): …I saw by the inspector’s face that his attention had\nbeen keenly aroused.  “You consider that to be\nimportant?” he [Inspector Gregory] asked.\n“Exceedingly so.” “Is there any point to which\nyou would wish to draw my attention?” “To the\ncurious incident of the dog in the night-time.” “The\ndog did nothing in the night-time.” “That was the\ncurious incident,” remarked Sherlock Holmes. The point is that Holmes is aware of a particular event (“the\ndog not barking”) and uses this to come to a conclusion. The\ninspector is not aware of this event, and so cannot (without\nHolmes’ help) come to the same conclusion. This is true of many\ndetective stories: clever detectives not only have the ability to\n“connect the dots”, but they are also aware of\nwhich dots need to be connected. Can we describe the inspector’s\nunawareness in an epistemic \nmodel?[17] Suppose that \\(U_i(E)\\) is the event that the player \\(i\\) is unaware\nof the event \\(E\\). Of course, if \\(i\\) is unaware of \\(E\\) then \\(i\\) does\nnot know that \\(E\\) is true (\\(U_i(E)\\subseteq\n\\overline{K_i(E)}\\), where \\(\\overline{X}\\) denotes the complement\nof the event \\(X\\)). Recall that in epistemic models (where the\nplayers’ information is described by partitions), we have the\nnegative introspection property: This means that if \\(i\\) is unaware of \\(E\\),\nthen \\(i\\) knows that she does not know that \\(E\\). Thus, to capture a\nmore natural definition of \\(U_i(E)\\) where we need to\nrepresent the players’ knowledge in a possibility\nstructure where the \\(K_i\\) operators do not necessarily satisfy\nnegative introspection. A possibility structure is a tuple \\(\\langle W,\n\\{P_i\\}_{i\\in\\A}, \\sigma\\rangle\\) where \\(P_i:W\\rightarrow \\pow(W)\\). The\nonly difference with an epistemic model is that the \\(P_i(w)\\) do not\nnecessarily form a partition of \\(W\\). We do not go into details\nhere—see Halpern (1999) for a complete discussion of possibility\nstructures and how they relate to epistemic models. The knowledge\noperator is defined as it is for epistemic models: for each event \\(E\\),\n\\(K_i(E)=\\{w \\mid P_i(w)\\subseteq E\\}\\). However, S. Modica and\nA. Rustichini (1994, 1999) argue that even the more general\npossibility structures cannot be used to describe a player’s\nunawareness. A natural definition of unawareness on possibility structures is: That is, an agent is unaware of \\(E\\) provided the agent does not\nknow that \\(E\\) obtains, does not know that she does not know that \\(E\\)\nobtains, and so on. Modica and Rustichini use a variant of the above\nSherlock Holmes story to show that there is a problem with this\ndefinition of unawareness. Suppose there are two signals: A dog barking (\\(d\\)) and a cat\nhowling (\\(c\\)). Furthermore, suppose there are three states \\(w_1\\),\n\\(w_2\\) in which the dog barks and \\(w_3\\) in which the cat howls. The\nevent that there is no intruder is \\(E=\\{w_1\\}\\) (the lack of the two\nsignals indicates that there was no\nintruder[18]).\n The following possibility structure\n(where there is an arrow from state \\(w\\) to state \\(v\\) provided \\(v\\in\nP(w)\\)) describes the inspector’s epistemic state: Figure 23 Consider the following calculations: \\(K(E)=\\{w_2\\}\\) (at \\(w_2\\), Watson knows there is a human intruder)\nand \\(-K(E)=\\{w_1,w_3\\}\\) \\(K(-K(E))=\\{w_3\\}\\) (at \\(w_3\\), Watson knows that she does not know\n\\(E\\)), and \\(-K(-K(E))=\\{w_1,w_2\\}\\). \\(-K(E)\\cap -K(-K(E))=\\{w_1\\}\\) and, in fact, \\(\\bigcap_{i=1}^\\infty\n(-K)^i(E)=\\{w_1\\}\\) Let \\(U(F)=\\bigcap_{i=1}^\\infty (-K)^i(F)\\). Then, \\(U(\\emptyset)=U(W)=U(\\{w_1\\})=U(\\{w_2,w_3\\})=\\emptyset\\) \\(U(E)=U(\\{w_3\\})=U(\\{w_1,w_3\\})=U(\\{w_1,w_2\\}=\\{w_1\\}\\) So, \\(U(E)=\\{w_1\\}\\) and \\(U(U(E))=U(\\{w_1\\})=\\emptyset\\). This means\nthat at state \\(w_1\\), the Inspector is unaware of \\(E\\), but is not\nunaware that he is unaware of \\(E\\). More generally, Dekel et al. (1998)\nshow that there is no nontrivial unawareness operator \\(U\\) satisfying\nthe following properties: \\(U(E) \\subseteq \\overline{K(E)}\\cap \\overline{K(E)}\\) \\(K(U(E))=\\emptyset\\) \\(U(E)\\subseteq U(U(E))\\) There is an extensive literature devoted to developing models that\ncan represent the players’ unawareness. See Board, Chung, &\nSchipper (2011); Chen, Ely, & Luo (2012); E. Dekel et al. (1998);\nHalpern (2001a); Halpern & Rego (2008); and Heifetz, Meier, &\nSchipper (2006) for a discussion of issues related to this entry. The\nUnawareness Bibliography (see Other Internet\nResources) has an up-to-date list of papers in this area. The first step in any epistemic analysis of a game is to describe\nthe players’ knowledge and beliefs using (a possible variant of)\none of the models introduced in Section 2. As we\nnoted already in Section 2.2, there will be\nstatements about what the players know and believe about the game\nsituation and about each other that are commonly known in some models\nbut not in others. In any particular structure, certain beliefs, beliefs about belief,\n…, will be present and others won’t be. So, there is an\nimportant implicit assumption behind the choice of a structure. This\nis that it is “transparent” to the players that the\nbeliefs in the type structure—and only those beliefs—are\npossible ….The idea is that there is a “context” to\nthe strategic situation (e.g., history, conventions, etc.) and this\n“context” causes the players to rule out certain\nbeliefs. (Brandenburger & Friedenberg 2010: 801) Ruling out certain configurations of beliefs\nconstitute substantive assumptions about the players’\nreasoning during the decision making process. In other words,\nsubstantive assumptions are about how, and how much, information is\nimparted to the agents, over and above those that are intrinsic to the\nmathematical formulation of the structures used to describe the\nplayers’ information. It is not hard to see that one always\nfinds substantive assumptions in finite structures: Given a countably\ninfinite set of atomic propositions, for instance, in finite\nstructures it will always be common knowledge that some logically\nconsistent combination of these basic facts are not realized,\nand a fortiori for logically consistent configurations of\ninformation and higher-order information about these basic facts. On\nthe other hand, monotonicity of the belief/knowledge operator is a\ntypical example of an assumption that is not\nsubstantive. More generally, there are no models of games, as we\ndefined in Section 2, where it is not common\nknowledge that the players believe all the logical consequences of\ntheir beliefs.[19] Can we compare models in terms of the number of substantive\nassumptions that are made? Are there models that make no, or at least\nas few as possible, substantive assumptions? These questions have been\nextensively discussed in the epistemic foundations of game\ntheory—see the discussion in Samuelson (1992) and the references\nin Moscati (2009). Intuitively, a structure without any substantive\nassumptions must represent all possible states of (higher-order)\ninformation. Whether such a structure exists will depend, in part, on\nhow the players’ informational attitudes are\nrepresented—e.g., as (conditional/lexicographic) probability\nmeasures or set-valued knowledge/belief functions. These questions\nhave triggered interest in the existence of “rich” models\ncontaining most, if not all, possible configurations of (higher-order)\nknowledge and beliefs. There are different ways to understand what it means for a\nstructure to minimize the substantive assumptions about the\nplayers’ higher-order information. We do not attempt a complete\noverview of this interesting literature here (see Brandenburger &\nKeisler (2006: sec. 11) and Siniscalchi (2008: sec. 3) for discussion\nand pointers to the relevant results). One approach considers the\nspace of all (Harsanyi type-/Kripke-/epistemic-plausibility-)\nstructures and tries to find a single structure that, in some suitable\nsense, “contains” all other structures. Such a structure,\noften called called a universal structure (or a terminal\nobject in the language of category theory), if it exists,\nincorporates any substantive assumption that an analyst can\nimagine. Such structure have been shown to exists for Harsanyi type\nspaces (Mertens & Zamir 1985; Brandenburger & Dekel 1993). For\nKripke structures, the question has been answered in the negative\n(Heifetz & Samet 1998; Fagin, Geanakoplos, Halpern, & Vardi\n1999; Meier 2005), with some qualifications regarding the language\nthat is used to describe them (Heifetz 1999; Roy & Pacuit\n2013). A second approach takes an internal perspective by asking\nwhether, for a fixed set of states or types, the agents are\nmaking any substantive assumptions about what their opponents know or\nbelieve. The idea is to identify (in a given model) a set of\npossible conjectures about the players. For example, in a\nknowledge structure based on a set of states \\(W\\) this might be the set\nof all subsets of \\(W\\) or the set definable subsets of \\(W\\) in some\nsuitable logical language. A space is said to be complete if\neach agent correctly takes into account each possible conjecture about\nher opponents. A simple counting argument shows that there cannot\nexist a complete structure when the set of conjectures is all\nsubsets of the set of states (Brandenburger 2003). However, there is a\ndeeper result here which we discuss below. Adam Brandenburger and H. Jerome Keisler (2006) introduce the\nfollowing two person, Russel-style paradox. The statement of the\nparadox involves two concepts: beliefs and\nassumptions. An assumption is a player’s strongest\nbelief: it is a set of states that implies all other beliefs at a\ngiven state. We will say more about the interpretation of an\nassumption below. Suppose there are two players, Ann and Bob, and\nconsider the following description of beliefs. A paradox arises by asking the question To ease the discussion, let \\(C\\) be Bob’s assumption in (S):\nthat is, \\(C\\) is the statement “Ann believes that Bob’s\nassumption is wrong.” So, (Q) asks whether \\(C\\) is true or\nfalse. We will argue that \\(C\\) is true if, and only if, \\(C\\) is\nfalse. Suppose that \\(C\\) is true. Then, Ann does believe that Bob’s\nassumption is wrong, and, by introspection, she believes that she\nbelieves this. That is to say, Ann believes that \\(C\\) is\ncorrect. Furthermore, according to (S), Ann believes that Bob’s\nassumption is \\(C\\). So, Ann, in fact, believes that Bob’s\nassumption is correct (she believes Bob’s assumption is \\(C\\) and\nthat \\(C\\) is correct). So, \\(C\\) is false. Suppose that \\(C\\) is false. This means that Ann believes that\nBob’s assumption is correct. That is, Ann believes that \\(C\\) is\ncorrect (By (S), Ann believes that Bob’s assumption is\n\\(C\\)). Furthermore, by (S), we have that Ann believes that Bob\nassumes that Ann believes that \\(C\\) is wrong. So, Ann believes\nthat she believes that \\(C\\) is correct and she believes that Bob\nassumption is that she believes that \\(C\\) is wrong. So, it is true that\nshe believes Bob’s assumptions is wrong (Ann believes that\nBob’s assumption is she believes that \\(C\\) is wrong, but\nshe believes that is wrong: she believes that \\(C\\) is\ncorrect). So, \\(C\\) is true. Brandenburger and Keisler formalize the above argument in order to\nprove a very strong impossibility result about the existence of\nso-called assumption-complete structures. We need some\nnotation to state this result. It will be most convenient to work in\nqualitative type spaces for two players\n(Definition 2.7). A qualitative type space\nfor two players (cf. Definition 2.7. The set\nof states is not important in what follows, so we leave it out) is a\nstructure \\(\\langle \\{T_A, T_B\\}, \\{\\lambda_A, \\lambda_B\\}\\rangle\\)\nwhere A set of conjectures about Ann is a subset\n\\(\\C_A\\subseteq \\pow(T_A)\\) (similarly, the set of conjectures about Bob\nis a subset \\(\\C_B\\subseteq \\pow(T_B)\\)). A structure \\(\\langle \\{T_A,\nT_B\\}, \\{\\lambda_A, \\lambda_B\\}\\rangle\\) is said to\nbe assumption-complete for the conjectures \\(\\C_A\\) and\n\\(\\C_B\\) provided for each conjecture in \\(\\C_A\\) there is a type that\nassumes that conjecture (similarly for Bob). Formally, for each\n\\(Y\\in\\C_B\\) there is a \\(t_0\\in T_A\\) such that \\(\\lambda_A(t_0)=Y\\), and\nsimilarly for Bob. As we remarked above, a simple counting argument\nshows that when \\(\\C_A=\\pow(T_A)\\) and \\(\\C_B=\\pow(T_B)\\), then\nassumption-complete models only exist in trivial cases. A much deeper\nresult is: Theorem 6.1 (Brandenburger & Keisler 2006: Theorem 5.4)\nThere is no assumption-complete type structure for the set of\nconjectures that contains the first-order definable subsets. See the supplement for a discussion of the proof of this theorem\n(see Section 2). Consult Pacuit (2007) and  Abramsky & Zvesper (2010) for an extensive analysis and\ngeneralization of this result. But, it is not all bad news: Mariotti,\nMeier, & Piccione (2005) construct a complete structure where the\nset of conjectures are compact subsets of some well-behaved\ntopological space. The epistemic view on games is that players should be seen as\nindividual decision makers, choosing what to do on the basis of their\nown preferences and the information they have in specific\ninformational contexts. What decision they will make—the\ndescriptive question—or what decision they should make—the\nnormative question, depends on the decision-theoretic choice rule that\nthe player use, or should use, in a given context. We conclude with\ntwo general methodological issues about epistemic game theory and some\npointers to further reading. Common knowledge of rationality is an informal assumption that game\ntheorists, philosophers and other social scientists often appeal to\nwhen analyzing social interactive situations. The epistemic program in\ngame theory demonstrates that there are many ways to understand what\nexactly it means to assume that there is “common\nknowledge/belief of rationality” in a game situation. Broadly speaking, much of the epistemic game theory literature is\nfocused on two types of projects. The goal of the first project is to\nmap out the relationship between different mathematical\nrepresentations of what the players know and believe about each other\nin a game situation. Research along these lines not only raises\ninteresting technical questions about how to compare and contrast\ndifferent mathematical models of the players’ epistemic states,\nbut it also highlights the benefits and limits of an epistemic\nanalysis of games. The second project addresses the nature of rational\nchoice in game situations. The importance of this project is nicely\nexplained by Wolfgang Spohn: …game theory…is, to put it strongly, confused about\nthe rationality concept appropriate to it, its assumptions about its\nsubjects (the players) are very unclear, and, as a consequence, it is\nunclear about the decision rules to be applied….The basic\ndifficulty in defining rational behavior in game situations is the\nfact that in general each player’s strategy will depend on his\nexpectations about the other players’ strategies. Could we\nassume that his expectations were given, then his problem of strategy\nchoice would become an ordinary maximization problem: he could simply\nchoose a strategy maximizing his own payoff on the assumption that the\nother players would act in accordance with his given expectations. But\nthe point is that game theory cannot regard the players’\nexpectations about each other’s behavior as given; rather, one\nof the most important problems for game theory is precisely to decide\nwhat expectations intelligent players can rationally entertain about\nother intelligent players’ behavior. (Spohn 1982: 267) Much of the work in epistemic game theory can be viewed as an\nattempt to use precise representations of the players’ knowledge\nand beliefs to help resolve some of the confusion alluded to in the\nabove quote. In an epistemic analysis of a game, the specific recommendations or\npredictions for the players’ choices are derived from\ndecision-theoretic choice rules. Maximization of expected utility, for\ninstance, underlies most of the results in the contemporary literature\non the epistemic foundations of game theory. From a methodological\nperspective, however, the choice rule that the modeler assumes the\nplayers are following is simply a parameter that can be varied. In\nrecent years, there have been some initial attempts to develop\nepistemic analyses with alternative choice rules, for\ninstance minregret Halpern & Pass (2009). The reader interested in more extensive coverage of all or some of\nthe topics discussed in this entry should consult the following\narticles and books. Logic in Games by Johan van Benthem: This book uses the\ntools of modal logic broadly conceived to discuss many of the issues\nraised in this entry (2014, MIT Press). The Language of Game Theory by Adam Brandenburger: A\ncollection of Brandenburger’s key papers on epistemic game\ntheory (2014, World Scientific Series in Economic Theory). Epistemic Game Theory by Eddie Dekel and Marciano\nSiniscalchi: A survey paper aimed at economists covering the main\ntechnical results of epistemic game theory (2014, Available online). Epistemic Game Theory: Reasoning and Choice by\nAndrés Perea: A non-technical introduction to epistemic game\ntheory (2012, Cambridge University Press). The Bounds of Reason: Game Theory and the Unification of the\nBehavioral Sciences by Herbert Gintis: This book offers a broad\noverview of the social and behavioral science using the ideas of\nepistemic game theory (2009, Princeton University Press).","contact.mail":"epacuit@umd.edu","contact.domain":"umd.edu"},{"date.published":"2015-03-13","url":"https://plato.stanford.edu/entries/epistemic-game/","author1":"Eric Pacuit","author1.info":"http://www.philosophy.umd.edu/people/pacuit","author2.info":"http://www.philosophie1.uni-bayreuth.de/en/team/roy/","entry":"epistemic-game","body.text":"\n\nFoundational work in game theory aims at making explicit the\nassumptions that underlie the basic concepts of the\ndiscipline. Non-cooperative game theory is the study of individual,\nrational decision making in situations of strategic interaction. This\nentry presents the epistemic foundations of non-cooperative\ngame theory (this area of research is called epistemic game\ntheory).\n\nEpistemic game theory views rational decision making in games as\nsomething not essentially different from rational decision making\nunder uncertainty. As in Decision Theory (Peterson 2009), to choose\nrationally in a game is to select the “best” action in\nlight of one’s beliefs or information. In a decision problem,\nthe decision maker’s beliefs are about a passive state of\nnature, the state of which determines the consequences of her\nactions. In a game, the consequences of one’s decision depend on\nthe choices of the other agents involved in the situation\n(and possibly the state of nature). Recognizing this—i.e., that\none is interacting with other agents who try to choose the best course\nof action in the light of their own\nbeliefs—brings higher-order information into the\npicture. The players’ beliefs are no longer about a passive or\nexternal environment. They concern the choices and the\ninformation of the other players. What one expects of one’s\nopponents depends on what one thinks the others expect from her, and\nwhat the others expect from a given player depends on what they think\nher expectations about them are.\n\nThis entry provides an overview of the issues that arise when one\ntakes this broadly decision-theoretic view on rational decision making\nin games. After some general comments about information in games, we\npresent the formal tools developed in epistemic game theory and\nepistemic logic that have been used to understand the role of\nhigher-order information in interactive decision making. We then show\nhow these tools can be used to characterize known “solution\nconcepts” of games in terms of rational decision making in\nspecific informational contexts. Along the way, we highlight a number\nof philosophical issues that arise in this area.\n\nA game refers to any interactive situation involving a\ngroup of self-interested agents, or players. The defining\nfeature of a game is that the players are engaged in an\n“interdependent decision problem” (Schelling\n1960). Classically, the mathematical description of a game\nincludes following components: The players. In this entry, we only consider games with a\nfinite set of players. We use \\(\\Agt\\) to denote the set of players in a\ngame, and \\(i, j,\\ldots\\) to denote its elements. The feasible options (typically called actions\nor strategies) for each player. Again, we only consider games\nwith finitely many feasible options for each player. The players’ preferences over possible outcome. Here\nwe represent them as von Neumann-Morgenstern utility functions \\(u_i\\)\nassigning real-valued utilities to each outcome of the game. A game can have many other structural properties. It can\nbe represented as a single-shot or multi-stage decision problem, or it\ncan include simultaneous or stochastic moves. We start with games\nin strategic form without stochastic moves, and will\nintroduce more sophisticated games as we go along in the entry. In a\nstrategic game, each player \\(i\\) can choose from a (finite) set \\(S_i\\)\nof options, also called actions or strategies. The combination of all\nthe players’ choices, denoted \\({\\mathbf{s}}\\), is called\na strategy profile, or outcome of the game. We write\n\\({\\mathbf{s}}_i\\) for \\(i\\)’s component in \\({\\mathbf{s}}\\), and\n\\({\\mathbf{s}}_{-i}\\) for the profile of strategies for all agents other\nthan \\(i\\). Finally, we write \\(\\Pi_{i \\in \\Agt} S_i\\) for the set of all\nstrategy profiles of a given game. Putting everything together, a\nstrategic game is a tuple \\(\\langle \\A, \\{S_i, u_i\\}_{i\\in\\A}\\rangle\\)\nwhere \\(\\A\\) is a finite set of players, for each \\(i\\in\\A\\), \\(S_i\\) is a\nfinite set of actions and \\(u_i:\\Pi_{i\\in\\A} S_i\\rightarrow\\mathbb{R}\\)\nis player \\(i\\)’s utility function. The game in Figure 1 is an example\nof a game in strategic form. There are two players, Ann and Bob, and\neach has to choose between two options: \\(\\Agt = \\{Ann, Bob\\}\\),\n\\(S_{Ann} = \\{u, d\\}\\) and \\(S_{Bob} = \\{l, r\\}\\). The value of \\(u_{Ann}\\)\nand \\(u_{Bob}\\), representing their respective preferences over the\npossible outcomes of the game, are displayed in the cell of the\nmatrix. If Bob chooses \\(l\\), for instance, Ann prefers the outcome she\nwould get by choosing \\(u\\) to the one she would get by choosing \\(d\\),\nbut this preference is reversed in the case Bob chooses \\(r\\). This game\nis called a “pure coordination game” in the literature\nbecause the players have a clear interest in coordinating their\nchoices—i.e., on \\((u, l)\\) or \\((d, r)\\)—but they are\nindifferent about which way they coordinate their choices. Figure 1: A coordination game In a game, no single player has total control over which outcome\nwill be realized at the end of the interaction. This depends on the\ndecisions of all players. Such abstract models\nof interdependent decisions are capable of representing a\nwhole array of social situations, from strictly competitive to\ncooperative ones. See Ross (2010) for more details about classical\ngame theory and key references. The central analytic tool of classical game theory are solution\nconcepts. They provide a top-down perspective specifying which\noutcomes of a game are deemed “rational”. This can be\ngiven both a prescriptive or a predictive\nreading. Nash equilibrium is one of the most well-known solution\nconcepts, but we will encounter others below. In the game above, for\ninstance, there are two Nash equilibria in so-called “pure\nstrategies.”[1]\n These are the two coordination profiles: \\((u,\nl)\\) and \\((d, r)\\). From a prescriptive point of view, a solution concept is a set of\npractical recommendations—i.e., recommendations about what the\nplayers should do in a game. From a predictive point of view, solution\nconcepts describe what the players will actually do in certain\ninteractive situation. Consider again the pure strategy Nash\nequilibria in the above example. Under a prescriptive interpretation,\nit singles out what players should do in the game. That is,\nAnn and Bob should either play their component of \\((u, l)\\) or \\((d,\nr)\\). Under the predictive interpretation, these profiles are the ones\nthat one would expect to observe in a actual play of that game. This solution-concept-driven perspective on games faces many\nfoundational difficulties, which we do not survey here. The interested\nreader can consult Ross (2010), Bruin (2010), and Kadane & Larkey\n(1983) for a discussion. Epistemic game theory is a broad area of research encompassing a\nnumber of different mathematical frameworks that are used to analyze\ngames. The details of the frameworks are different, but they do share\na common perspective. In this Section, we discuss two key features of\nthis common perspective. This point of view is nicely explained by Robert Stalnaker: There is no special concept of rationality for decision making in a\nsituation where the outcomes depend on the actions of more than one\nagent. The acts of other agents are, like chance events, natural\ndisasters and acts of God, just facts about an uncertain world that\nagents have beliefs and degrees of belief about. The utilities of\nother agents are relevant to an agent only as information that,\ntogether with beliefs about the rationality of those agents, helps to\npredict their actions. (Stalnaker 1996: 136) In other words, epistemic game theory can be seen as an attempt to\nbring back the theory of decision making in games to its\ndecision-theoretic roots. In decision theory, the decision-making units are individuals with\npreferences over the possible consequences of their actions. Since the\nconsequence of a given action depend on the state of the environment,\nthe decision-maker’s beliefs about the state of the environment\nare crucial to assess the rationality of a particular decision. So,\nthe formal description of a decision problem includes the possible\noutcomes and states of the environment, the decision maker’s\npreferences over these outcome, and a description of the\ndecision maker’s beliefs about the state of nature\n(i.e., the decision maker’s doxastic state). A\ndecision-theoretic choice rule can be used to make\nrecommendations to the decision maker about what she should\nchoose (or to predict what the decision-maker will choose). A\nstandard example of a choice rule is maximization of (subjective)\nexpected utility, underlying the Bayesian view of\nrationality. It presupposes that the decision maker’s\npreferences and beliefs can be represented by numerical utilities and\nprobabilities, \nrespectively.[2]\n (We postpone the formal\nrepresentation of this, and the other choice rules such as weak and\nstrict dominance, until we have presented the formal models of beliefs\nin games in Section 2.) From an epistemic point of view, the classical ingredients of a\ngame (players, actions, outcomes, and preferences) are thus not enough\nto formulate recommendations or predictions about how the players\nshould or will choose. One needs to specify the (interactive) decision\nproblem the players are in, i.e., also the beliefs players\nhave about each other’s possible actions (and beliefs). In a\nterminology that is becoming increasingly popular in epistemic game\ntheory, games are played in specific contexts (Friedenberg\n& Meier 2010, Other Internet Resources), in which the players have\nspecific knowledge and/or beliefs about each other. The\nrecommendations and/or predictions that are appropriate for one\ncontext may not transfer to another, even if the underlying situation\nmay correspond to precisely the same strategic game. There are various types of information that a player has access to\nin a game situation. For instance, a player may have imperfect information about the play of the game (which moves have\nbeen played?); incomplete information about the structure of the game (what are\nthe actions/payoffs?); strategic information (what will the other players do?);\nor higher-order information (what are the other players\nthinking?). While all types of uncertainty may play a role in an epistemic\nanalysis of a game, a distinguishing feature of epistemic game theory\nis an insistence that rational decisions are assessed in terms of the\nplayers’ preferences and beliefs about what their\nopponents are going to do. Again we turn to Stalnaker to summarize\nthis point of view: …There are no special rules of rationality telling one what\nto do in the absence of degrees of belief [about the opponents’\nchoices], except this: decide what you believe, and then maximize\nexpected utility. (Stalnaker 1996: 136) The four types of uncertainty in games introduced above are\nconceptually important, but not necessarily exhaustive nor mutually\nexclusive. John Harsanyi, for instance, argued that all uncertainty\nabout the structure of the game, that is all possible incompleteness\nin information, can be reduced to uncertainty about the payoffs\n(Harsanyi 1967–68). (This was later formalized and proved by\nStuart and Hu 2002). In a similar vein, Kadane & Larkey (1982)\nargue that only strategic uncertainty is relevant for the assessment\nof decision in game situations. Contemporary epistemic game theory\ntakes the view that, although it may ultimately be reducible to\nstrategic uncertainty, making higher-order uncertainty explicit can\nclarify a great deal of what interactive or strategic rationality\nmeans. The crucial difference from the classical\n“solution-concept” analysis of a game is that epistemic\ngame theory takes a bottom-up perspective. Once the context of the\ngame is specified, the rational outcomes are derived, given\nassumptions about how the players are making their choices and what\nthey know and believe about how the others are choosing. In the\nremainder of this section, we briefly discuss some general issues that\narise from taking an epistemic perspective on games. We postpone\ndiscussion of higher-order and strategic uncertainty until Sections 3,\n4 and 5. It is standard in the game theory literature to distinguish three\nstages of the decision making process: ex ante, ex\ninterim and ex post. At one extreme is the ex\nante stage where no decision has been made yet. The other extreme\nis the ex post stage where the choices of all players are\nopenly disclosed. In between these two extremes is the ex\ninterim stage where the players have made their decisions, but\nthey are still uninformed about the decisions and intentions of the\nother players. These distinctions are not intended to be sharp. Rather, they\ndescribe various stages of information disclosure during the\ndecision-making process. At the ex-ante stage, little is\nknown except the structure of the game, who is taking part, and\npossibly (but not necessarily) some aspect of the agents’\ncharacter. At the ex-post stage the game is basically over:\nall player have made their decision and these are now irrevocably out\nin the open. This does not mean that all uncertainty is removed as an\nagent may remain uncertain about what exactly the others were\nexpecting of her. In between these two extreme stages lies a whole\ngradation of states of information disclosure that we loosely refer to\nas “the” ex-interim stage. Common to these stages\nis the fact that the agents have made a decision, although\nnot necessarily an irrevocable one. In this entry, we focus on the ex interim stage of\ndecision making. This is in line with much of the literature on the\nepistemic foundations of game theory as it allows for a\nstraightforward assessment of the agents’ rationality given\ntheir expectations about how their opponents will choose. Focusing on\nthe ex interim stage does raise some interesting questions\nabout possible correlations between a player’s strategy\nchoice, what Stalnaker (1999) calls “active knowledge”,\nand her information about the choices of others, her “passive\nknowledge” (idem). The question of how a player should\nreact, that is eventually revise her decision, upon learning that she\ndid not choose “rationally” is an interesting and\nimportant one, but we do not discuss it in the entry. Note that this\nquestion is different from the one of how agents should revise their\nbeliefs upon learning that others did not choose\nrationally. This second question is very relevant in games in which\nplayers choose sequentially, and will be addressed\nin Section 4.2.3. A natural question to ask about any mathematical model of\na game situation is how does the analysis change if the players\nare uncertain about some of the parameters of the model? This\nmotivated Harsanyi’s fundamental work introducing the notion of\na game-theoretic type and defining a Bayesian\ngame in Harsanyi 1967–68. Using these ideas, an\nextensive literature has developed that analyzes games in which\nplayers are uncertain about some aspect of the game. (Consult\nLeyton-Brown & Shoham (2008: ch. 7) for a concise summary of the\ncurrent state-of-affairs and pointers to the relevant literature.) One\ncan naturally wonder about the precise relationship between this\nliterature and the literature we survey in this entry on the epistemic\nfoundations of game theory. Indeed, the foundational literature we\ndiscuss here largely focuses on Harsanyi’s approach to modeling\nhigher-order beliefs (which we discuss in Section\n2.3). There are two crucial differences between the literature on\nBayesian games and the literature we discuss in this entry (cf. the\ndiscussion in Brandenburger 2010: sec. 4 and 5). In a Bayesian game, players are uncertain about the payoffs of the\ngame, what other players believe are the correct payoffs, what other\nplayers believe that the other players believe about the payoffs, and\nso on, and this is the only source of uncertainty. That is, the\nplayers’ (higher-order) beliefs about the payoffs in a game\ncompletely determine the (higher-order) beliefs about the other\naspects of the game. In particular, if a player comes to know\nthe payoffs of the other players, then that player is certain (and\ncorrect) about the possible (rational) choices of the other\nplayers.[3] It is assumed that all players choose optimally given their\ninformation. That is, all players choose a strategy that maximizes\ntheir expected utility given their beliefs about the game, beliefs\nabout what other players believe about the game, and so on. This\nmeans, in particular, that players do not entertain the possibility\nthat their opponents may choose “irrationally.” Note that these assumptions are not inherent in the formalism that\nHarsanyi used to represent the players’ beliefs in a game of\nincomplete information. Rather, they are better described as\nconventions followed by Harsanyi and subsequent researchers studying\nBayesian games. In a game with imperfect information (see Ross 2010 for a\ndiscussion), the players may not be perfectly informed about the moves\nof their opponents or the outcome of chance moves by nature. Games\nwith imperfect information can be pictured as follows: Figure 2 The interpretation is that the decision made at the first node\n(\\(d_0\\)) is forgotten, and so the decision maker is uncertain about\nwhether she is at node \\(d_1\\) or \\(d_2\\). See Osborne (2003: ch. 9 &\n10) for the general theory of games with imperfect information. In\nthis section, we briefly discuss a foundational issue that arises in\ngames with imperfect information. Kuhn (1953) introduced the distinction between perfect\nand imperfect recall in games with imperfect\ninformation. Roughly, players have perfect recall provided they\nremember all of their own past moves (see Bonanno 2004; Kaneko &\nKline 1995 for general discussions of the perfect recall\nassumption). It is standard in the game theory literature to assume\nthat all players have perfect recall (i.e., they may be uncertain\nabout previous choices of their opponents or nature, but they do\nremember their own moves). As we noted in Section 1.3, there are\ndifferent stages to the decision making process. Differences between\nthese stages become even more pronounced in extensive games in which\nthere is a temporal dimension to the game. There are two ways to think\nabout the decision making process in an extensive game (with imperfect\ninformation). The first is to focus on the initial “planning\nstage”. That is, initially, the players settle on a strategy\nspecifying the (possibly random) move they will make at each of their\nchoice nodes (this is the players’ global\nstrategy). Then, the players start making their respective moves\n(following the strategies which they have committed to without\nreconsidering their options at each choice node). Alternatively, we\ncan assume that the players make “local judgements” at\neach of their choice nodes, always choosing the best option given the\ninformation that is currently available to them. A well-known theorem\nof Kuhn (1953) shows that if players have perfect recall, then a\nstrategy is globally optimal if, and only if, it is locally optimal\n(see Brandenburger 2007 for a self-contained presentation of this\nclassic result). That is, both ways of thinking about the decision\nmaking process in extensive games (with imperfect information) lead to\nthe same recommendations/predictions. The assumption of perfect recall is crucial for Kuhn’s\nresult. This is demonstrated by the well-known absent-minded\ndriver’s problem of Piccione and Rubinstein\n(1997a). Interestingly, their example is one where a decision maker\nmay be tempted to change his strategy after the initial planning\nstage, despite getting no new information. They describe the\nexample as follows: An individual is sitting late at night in a bar planning his\nmidnight trip home. In order to get home he has to take the highway\nand get off at the second exit. Turning at the first exit leads into a\ndisastrous area (payoff 0). Turning at the second exit yields the\nhighest reward (payoff 4). If he continues beyond the second exit, he\ncannot go back and at the end of the highway he will find a motel\nwhere he can spend the night (payoff 1). The driver is absentminded\nand is aware of this fact. At an intersection, he cannot tell whether\nit is the first or the second intersection and he cannot remember how\nmany he has passed (one can make the situation more realistic by\nreferring to the 17th intersection). While sitting at the bar, all he\ncan do is to decide whether or not to exit at an\nintersection. (Piccione & Rubinstein 1997a: 7) The decision tree for the absent-minded driver is pictured\nbelow: Figure 3 This problem is interesting since it demonstrates that there is a\nconflict between what the decision maker commits to do while planning\nat the bar and what he thinks is best at the first intersection: Planning stage:\nWhile planning his trip home at the bar,\nthe decision maker is faced with a choice between “Continue;\nContinue” and “Exit”. Since he cannot distinguish\nbetween the two intersections, he cannot plan to “Exit” at\nthe second intersection (he must plan the same behavior at both \\(X\\)\nand \\(Y\\)). Since “Exit” will lead to the worst outcome\n(with a payoff of 0), the optimal strategy is “Continue;\nContinue” with a guaranteed payoff of 1. \nAction stage: \nWhen arriving at an intersection, the decision maker is faced with a\nlocal choice of either “Exit” or “Continue”\n(possibly followed by another decision). Now the decision maker knows\nthat since he committed to the plan of choosing “Continue”\nat each intersection, it is possible that he is at the second\nintersection. Indeed, the decision maker concludes that he is at the\nfirst intersection with probability 1/2. But then, his expected payoff\nfor “Exit” is 2, which is greater than the payoff\nguaranteed by following the strategy he previously committed to. Thus,\nhe chooses to “Exit”. This problem has been discussed by a number of different\nresearchers.[4]\n It is beyond the scope of this article to\ndiscuss the intricacies of the different analyses. An entire issue\nof Games and Economic Behavior (Volume 20, 1997) was devoted\nto the analysis of this problem. For a representative sampling of the\napproaches to this problem, see Kline (2002); Aumann, Hart, &\nPerry (1997); Board (2003); Halpern (1997); Piccione & Rubinstein\n(1997b). Mixed strategies play an important role in many game-theoretic\nanalyses. Let \\(\\Delta(X)\\) denote the set of probability measures over\nthe finite[5]\n set \\(X\\). A mixed strategy\nfor player \\(i\\), is an element \\(m_i\\in \\Delta(S_i)\\). If\n\\(m_i\\in\\Delta(S_i)\\) assigns probability 1 to an element \\(s_i\\in S_i\\),\nthen \\(m_i\\) is called a pure strategy (in such a case,\nI write \\(s_i\\) for \\(m_i\\)). Mixed strategies are incorporated into a\ngame-theoretic analysis as follows. Suppose that \\(G=\\langle N, \\{S_i,\nu_i\\}_{i\\in N}\\rangle\\) is a finite strategic game. The mixed\nextension of \\(G\\) is the strategic game in which the strategies\nfor player \\(i\\) are the mixed strategies in \\(G\\) (i.e., \\(\\Delta(S_i)\\))\nand the utility for player \\(i\\) (denoted \\(U_i\\)) of the joint mixed\nstrategy \\(m\\in \\Pi_{i\\in N} \\Delta(S_i)\\) is calculated in the obvious\nway (let \\(m(s)=m_1(s_1)\\cdot m_2(s_2)\\cdots m_n(s_n)\\) for \\(s\\in\n\\Pi_{i\\in N} S_i\\)): Thus, the solution space of a mixed extension of the game \\(G\\) is\nthe set \\(\\Pi_{i\\in N} \\Delta(S_i)\\). Despite their prominence in game theory, the interpretation of\nmixed strategies is controversial, as Ariel Rubinstein notes: We are reluctant to believe that our decisions are\nmade at random. We prefer to be able to point to a reason for each\naction we take. Outside of Las Vegas we do not spin\nroulettes. (Rubinstein 1991: 913). In epistemic game theory, it is more natural to\n work with an alternative interpretation of mixed strategies: A mixed\n strategy for player \\(i\\) is a representation of the beliefs\n of \\(i\\)’s opponent(s) about what she will do. This is nicely\n explained in Robert Aumann’s influential paper \n (Aumann 1987—see, especially, Section 6 of this paper \n  for a discussion of this interpretation of mixed strategies): An important feature of our approach is that it does not require\nexplicit randomization on the part of the players. Each player always\nchooses a definite pure strategy, with no attempt to randomize; the\nprobabilistic nature of the strategies reflects the uncertainties of\nother players about his choice. (Aumann 1987: 3) A model of a game is a structure that represents the\ninformational context of a given play of the game. The states, or\npossible worlds, in a game model describe a possible play of the\ngame and the specific information that influenced the\nplayers’ choices (which may be different at each state). This\nincludes each player’s “knowledge” of her own choice\nand opinions about the choices and “beliefs” of her\nopponents. A key challenge when constructing a model of a game is how\nto represent the different informational attitudes of the\nplayers. Researchers interested in the foundation of decision theory,\nepistemic and doxastic logic and, more recently, formal\nepistemology have developed many different formal models that can\ndescribe the many varieties of informational attitudes important for\nassessing the choice of a rational agent in a decision- or\ngame-theoretic situation. After discussing some general issues that arise when describing the\ninformational context of a game, we introduce the two main types of\nmodels that have been used to describe the players’ beliefs (and\nother informational attitudes) in a game situation: type\nspaces (Harsanyi 1967–68; Siniscalchi 2008) and the\nso-called Aumann- or Kripke-structures (Aumann\n1999a; Fagin, Halpern, Moses, & Vardi 1995). Although these two\napproaches have much in common, there are some important differences\nwhich we highlight below. A second, more fundamental, distinction\nfound in the literature is between “quantitative”\nstructures, representing “graded” attitudes (typically via\nprobability distributions), and “qualitative” structures\nrepresenting “all out” attitudes. Kripke structures are\noften associated with the former, and type spaces with the latter, but\nthis is not a strict classification. Informational contexts of games can include various forms of\nattitudes, from the classical knowledge and belief to robust\n(Stalnaker 1994) and strong (Battigalli & Siniscalchi 2002)\nbelief, each echoing in different notions of game-theoretical\nrationality. It is beyond the scope of this article to survey the\ndetails of this vast literature (cf. the next section for some\ndiscussion of this literature). Rather, we will introduce a general\ndistinction between hard and soft attitudes,\ndistinction mainly developed in dynamic epistemic logic (van Benthem\n2011), which proves useful in understanding the various philosophical\nissues raised by epistemic game theory. We call hard information, information that\nis veridical, fully introspective and not\nrevisable. This notion is intended to capture what the agents are\nfully and correctly certain of in a given interactive situation. At\nthe ex interim stage, for instance, the players have hard\ninformation about their own choice. They “know”\nwhich strategy they have chosen, they know that they know this, and no\nnew incoming information could make them change their opinion on\nthis. As this phrasing suggests, the term knowledge is often\nused, in absence of better terminology, to describe this very strong\ntype of informational attitude. Epistemic logicians and game theorist\nare well aware of the possible discrepancies between such hard\n“knowledge” and our intuitive or even philosophical\nunderstanding of this notion. In the present context is it sufficient\nto observe that hard information shares some of the\ncharacteristics that have been attributed to knowledge in the\nepistemological literature, for instance truthfulness. Furthermore,\nhard information might come closer to what has been called\n“implicit knowledge” (see Section\n5.3 below). In any case, it seems philosophically more\nconstructive to keep an eye on where the purported counter-intuitive\nproperties of hard information come into play in epistemic game\ntheory, rather than reject this notion as wrong or flawed at the\nupshot. Soft information is, roughly speaking, anything that is\nnot “hard”: it is not necessarily veridical, not\nnecessarily fully introspective and/or highly revisable in the\npresence of new information. As such, it comes much closer\nto beliefs. Once again, philosophical carefulness is in order\nhere. The whole range of informational attitudes that is labeled as\n“beliefs” indeed falls into the category of attitudes that\ncan be described as “regarding something as true”\n(Schwitzgebel 2010), among which beliefs, in the philosophical sense,\nseem to form a proper sub-category. The models introduced below describe the players’ hard and\nsoft information in interactive situations. They differ in their\nrepresentation of a state of the world, but they can all be broadly\ndescribed as “possible worlds models” familiar in much of\nthe philosophical logic literature. The starting point is a non-empty\n(finite or infinite) set \\(S\\) of states of nature describing\nthe exogenous parameters (i.e., facts about the physical\nworld) that do not depend on the agents’ uncertainties. Unless\notherwise specified, \\(S\\) is the set of possible outcomes of the games,\nthe set of all strategy \nprofiles.[6]\n Each player is assumed to\nentertain a number of possibilities, called possible\nworlds or simply (epistemic) states. These\n“possibilities” are intended to represent a possible way a\ngame situation may evolve. So each possibility will be associated with\na unique state of nature (i.e., there is a function from\npossible worlds to states of nature, but this function need not be\n1–1 or even onto). It is crucial for the analysis of rationality\nin games that there may be different possible worlds\nassociated with the same state of nature. Such possible worlds are\nimportant because they open the door to representing different state\nof information. Such state-based modeling naturally yields\na propositional view of the agents’ informational\nattitudes. Agents will have beliefs/knowledge\nabout propositions, which are also called events in\nthe game-theory literature, and are represented as sets of possible\nworlds. These basic modeling choices are not uncontroversial, but such\nissues are not our concern in this entry. We start with the models that are familiar to philosophical\nlogicians (van Benthem 2010) and computer scientists (Fagin et\nal. 1995). These models were introduced to game theory by Robert\nAumann (1976) in his seminal paper Agreeing to Disagree (see\nVanderschraaf & Sillari 2009, Section 2.3, for a discussion of\nthis result). First, some terminology: Given a set \\(W\\) of states, or\npossible worlds, let us call any subset \\(E\\subseteq W\\)\nan event or proposition. Given events \\(E\\subseteq W\\)\nand \\(F\\subseteq W\\), we use standard set-theoretic notation for\nintersection (\\(E\\cap F\\), read “\\(E\\) and \\(F\\)”), union\n(\\(E\\cup F\\), read “\\(E\\) or \\(F\\)”) and (relative) complement\n(\\(-{E}\\), read “not \\(E\\)”). We say that an event \\(E\\subseteq\nW\\) occurs at state \\(w\\) if \\(w\\in E\\). This terminology will be crucial\nfor studying the following models. \nDefinition 2.1 (Epistemic Model)\n Suppose that \\(G\\) is a strategic game, \\(S\\) is the set of strategy\nprofiles of \\(G\\), and \\(\\A\\) is the set of players. An epistemic\nmodel based on \\(S\\) and \\(\\Agt\\) is a triple \\(\\langle\nW,\\{\\Pi_i\\}_{i\\in\\Agt},\\sigma\\rangle\\), where \\(W\\) is a nonempty set,\nfor each \\(i\\in\\Agt\\), \\(\\Pi_i\\) is a\npartition[7]\n over \\(W\\) and \\(\\sigma:W\\rightarrow S\\).\n Epistemic models represent the informational context of a given\ngame in terms of possible configurations of states of the game and the\nhard information that the agents have about them. The function\n\\(\\sigma\\) assigns to each possible world a unique state of the game in\nwhich every ground fact is either true or false. If \\(\\sigma(w) =\n\\sigma(w')\\) then the two worlds \\(w,w'\\) will agree on all the ground\nfacts (i.e., what actions the players will choose) but, crucially, the\nagents may have different information in them. So, elements of \\(W\\)\nare richer, than the elements of \\(S\\) (more on this\nbelow). Given a state \\(w\\in W\\), the cell \\(\\Pi_i(w)\\) is called agent\n\\(i\\)’s information set. Following standard terminology,\nif \\(\\Pi_i(w)\\subseteq E\\), we say the agent \\(i\\) knows the\nevent \\(E\\) at state \\(w\\). Given an event \\(E\\), the event that agent \\(i\\)\nknows \\(E\\) is denoted \\(K_i(E)\\). Formally, we define for each agent \\(i\\)\na knowledge function assigning to every event \\(E\\) the event that the\nagent \\(i\\) knows \\(E\\): Definition 2.2 (Knowledge Function) \nLet \\(\\M=\\langle W,\\{\\Pi_i\\}_{i\\in\\A},\\sigma\\rangle\\) be an epistemic\nmodel. The knowledge function for agent \\(i\\) based on\n\\(\\M\\) is \\(K_i:\\pow(W)\\rightarrow\\pow(W)\\) with: where for any set \\(X\\), \\(\\pow(X)\\) is the powerset of \\(X\\). Remark 2.3\nIt is often convenient to\nwork with equivalence relations rather than partitions. In\nthis case, an epistemic model based on \\(S\\) and \\(\\Agt\\) can also be\ndefined as a triple \\(\\langle W,\\{\\sim_i\\}_{i\\in\\Agt},\\sigma \\rangle\\)\nwhere \\(W\\) and \\(\\sigma\\) are as above and for each \\(i\\in\\Agt\\),\n\\(\\sim_i\\subseteq W\\times W\\) is reflexive, transitive and\nsymmetric. Given such a model \\(\\langle W,\n\\{\\sim_i\\}_{i\\in\\Agt},\\sigma\\rangle\\), we write  for the equivalence class of \\(w\\). Since there is a\n1–1 correspondence between equivalence relations and\npartitions,[8]\n we will abuse notation and use \\(\\sim_i\\) and\n\\(\\Pi_i\\) interchangeably. \nApplying the above remark, an alternative definition of \\(K_i(E)\\) is\nthat \\(E\\) is true in all the states the agent \\(i\\) considers possible\n(according to \\(i\\)’s hard information). That is, \\(K_i(E)=\\{w\\mid\n[w]_i\\subseteq E\\}\\). Partitions or equivalence relations are intended to represent the\nagents’ hard information at each state. It is\nwell-known that the knowledge operator satisfies the properties of the\nepistemic logic \\(\\mathbf{S5}\\) (see Hendricks & Symons 2009 for a\ndiscussion). We do not discuss this and related issues here and\ninstead focus on how these models can be used to provide the\ninformational context of a game. An Example.  Consider the following coordination\n game between Ann (player 1) and Bob (player 2). As is well-known,\n there are two pure-strategy Nash equilibrium (\\((u,l)\\) and\n \\((d,r)\\)). Figure 4: A strategic coordination\ngame between Ann and Bob The utilities of the players are not important for us at this\nstage. To construct an epistemic model for this game, we need first to\nspecify what are the states of nature we will consider. For\nsimplicity, take them to be the set of strategy profiles\n\\(S=\\{(u,l),(d,l),(u,r),(d,l)\\}\\). The set of agents is of course\n\\(\\Agt=\\{A,B\\}\\). What will be the set of states \\(W\\)? We start by\nassuming \\(W=S\\), so there is exactly one possible world corresponding\nto each state of nature. This needs not be so, but here this will help\nto illustrate our point. There are many different partitions for Ann and Bob that we can use\nto complete the description of this simple epistemic model. Not all of\nthe partitions are appropriate for analyzing the ex interim\nstage of the decision-making process, though. For example, suppose\n\\(\\Pi_{A}=\\Pi_{B}=\\{W\\}\\) and consider the event \\(U=\\{(u,l),(u,r)\\}\\)\nrepresenting the situation where Ann chooses \\(u\\). Notice that\n\\(K_A(U)=\\emptyset\\) since for all \\(w\\in W\\), \\(\\Pi_A(w)\\not\\subseteq U\\),\nso there is no state where Ann knows that she chooses\n\\(u\\). This means that this model is appropriate for reasoning about\nthe ex ante stage rather than the ex interim\nstage. This is easily fixed with an additional technical assumption:\nSuppose \\(S\\) is a set of strategy profiles for some (strategic or\nextensive) game with players \\(\\A=\\{1,\\ldots,n\\}\\). A model \\(\\M=\\langle W, \\{\\Pi_i\\}_{i\\in \\A},\\sigma \\rangle\\) is said\nto be an ex interim epistemic model if for\nall \\(i\\in\\Agt\\) and \\(w,v\\in W\\), if \\(v\\in\\Pi_i(w)\\) then\n\\(\\sigma_i(w)=\\sigma_i(v)\\) where \\(\\sigma_i(w)\\) is the \\(i\\)th component of the\nstrategy profile \\(s\\in S\\) assigned to \\(w\\) by \\(\\sigma\\). An example of\nan ex interim epistemic model with states \\(W\\) is: \\(\\Pi_A=\\{\\{(u,l),(u,r)\\},\\{(d,l),(d,r)\\}\\}\\) and \\(\\Pi_B=\\{\\{(u,l),(d,l)\\},\\{(u,r),(d,r)\\}\\}\\). Note that this simply reinterprets the game matrix\nin Figure 1 as an epistemic model\nwhere the rows are Ann’s information sets and the columns are\nBob’s information sets. Unless otherwise stated, we will always\nassume that our epistemic models are ex interim. The class\nof ex interim epistemic models is very rich with models\ndescribing the (hard) information the agents have about their own\nchoices, the (possible) choices of the other players and\nhigher-order (hard) information (e.g., “Ann knows that Bob knows\nthat…”) about these decisions. We now look at the epistemic model described above in more\ndetail. We will often use the following diagrammatic representation of\nthe model to ease exposition. States are represented by nodes in a\ngraph where there is a (undirected) edge between states \\(w_i\\) and\n\\(w_j\\) when \\(w_i\\) and \\(w_j\\) are in the same partition cell. We use a\nsolid line labeled with \\(A\\) for Ann’s partition and a dashed\nline labeled with \\(B\\) for Bob’s partition (reflexive edges are\nnot represented for simplicity). The event \\(U=\\{w_1,w_3\\}\\)\nrepresenting the proposition “Ann decided to choose option\n\\(u\\)” is the shaded gray region: Figure 5 Notice that the following events are true at all states: \\(- K_B(U)\\): “Bob does not know that Ann decided to choose\n\\(u\\)” \\(K_B(K_A(U)\\vee K_A(-U))\\): “Bob knows that Ann knows whether\nshe has decided to choose \\(u\\)” \\(K_A(-K_B(U))\\): “Ann knows that Bob does not know that she\nhas decided to choose \\(u\\)” In particular, these events are true at state \\(w_1\\) where Ann has\ndecided to choose \\(u\\) (i.e., \\(w_1\\in U\\)). The first event makes sense\ngiven the assumptions about the available information at the ex\ninterim stage: each player knows their own choice but not the\nother players’ choices. The second event is a concrete example\nof another assumption about the available information: Bob has the\ninformation that Ann has, in fact, made some choice. But what\nwarrants Ann to conclude that Bob does not know she has chosen \\(u\\)\n(the third event)? This is a much more significant statement about\nwhat Ann knows about what Bob expects her to do. Indeed, in certain\ncontexts, Ann may have very good reasons to think it is possible that\nBob actually knows she will choose \\(u\\). We can find an ex\ninterim epistemic model where this event (\\(-K_A(-K_B(U))\\)) is\ntrue at \\(w_1\\), but this requires adding a new possible world: Figure 6 Notice that since \\(\\Pi_B(w')=\\{\\{w'\\}\\}\\subseteq U\\) we have \\(w'\\in\nK_B(U)\\). That is, Bob knows that Ann chooses \\(u\\) at state\n\\(w'\\). Finally, a simple calculation shows that \\(w_1\\in -K_A(-K_B(U))\\),\nas desired. Of course, we can question the other substantive\nassumptions built-in to this model (e.g., at \\(w_1\\), Bob knows\nthat Ann does not know he will choose \\(L\\)) and continue modifying the\nmodel. This raises a number of interesting conceptual and technical\nissues which we discuss in Section 7. So far we have looked at relational models of hard information. A\nsmall modification of these models allows us to model a softer\ninformational attitude. Indeed, by simply replacing the assumption of\nreflexivity of the relation \\(\\sim_i\\) with seriality (for each state\n\\(w\\) there is a state \\(v\\) such that \\(w\\sim_i v\\)), but keeping the other\naspects of the model the same, we can capture what epistemic logicians\nhave called “beliefs”. Formally,\na doxastic model is a tuple \\(\\langle W,\n\\{R_i\\}_{i\\in\\Agt},V\\rangle\\) where \\(W\\) is a nonempty set of states,\n\\(R_i\\) is a transitive, Euclidean and serial relation on \\(W\\) and \\(V\\) is\na valuation function (cf. Definition 2.1). This\nnotion of belief is very close to the above hard informational\nattitude and, in fact, shares all the properties of \\(K_i\\) listed above\nexcept Veracity (this is replaced with a weaker assumption\nthat agents are “consistent” and so cannot believe\ncontradictions). This points to a logical analysis of both\ninformational attitudes with various “bridge principles”\nrelating knowledge and belief (such as knowing something implies\nbelieving it or if an agent believes \\(\\phi\\) then the agent knows that\nhe believes it). However, we do not discuss this line of research here\nsince these models are not our preferred ways of representing the\nagents’ soft information (see, for example, Halpern 1991 and\nStalnaker 2006). Plausibility Orderings A key aspect of beliefs which is not yet represented in the above\nmodels is that they are revisable in the presence of new\ninformation. While there is an extensive literature on the theory of\nbelief revision in the “AGM” style (Alchourrón,\nGärdenfors, & Makinson 1985), we focus on how to extend an\nepistemic model with a representation of softer, revisable\ninformational attitudes. The standard approach is to include\na plausibility ordering for each agent: a preorder (reflexive\nand transitive) denoted \\(\\preceq_i\\,\\subseteq W\\times W\\). If\n\\(w\\preceq_i v\\) we say “player \\(i\\) considers \\(w\\) at least as\nplausible as \\(v\\).” For an event \\(X\\subseteq W\\), let denote the set of minimal elements of \\(X\\) according to\n\\(\\preceq_i\\). Thus while the \\(\\sim_i\\) partitions the set of possible\nworlds according to the agents’ hard information, the\nplausibility ordering \\(\\preceq_i\\) represents which of the possible\nworlds the agent considers more likely (i.e., it represents the\nplayers soft information). Definition 2.4 (Epistemic-Plausibility Models)\nSuppose\nthat \\(G\\) is a strategic game, \\(S\\) is the set of strategy profiles of\n\\(G\\), and \\(\\A\\) is the set of players. An epistemic-plausibility\nmodel is a tuple \\(\\langle W,\\{\\Pi_i\\}_{i\\in \\Agt},\n\\{\\preceq_i\\}_{i\\in \\Agt},\\sigma\\rangle\\) where \\(\\kripkemodel\\) is an\nepistemic model, \\(\\sigma:W\\rightarrow S\\) and for each \\(i\\in\\Agt\\),\n\\(\\preceq_i\\) is a \nwell-founded,[9]\n reflexive and transitive\nrelation on \\(W\\) satisfying the following properties, for all \\(w,v\\in\nW\\)   Remark 2.5\n Note that if \\(v\\not\\in\\Pi_i(w)\\) then\n\\(w\\not\\in \\Pi_i(v)\\). Hence, by property 1, \\(w\\not\\preceq_i v\\) and\n\\(v\\not\\preceq_i w\\). Thus, we have the following equivalence: \\(v\\in\n\\Pi_i(w)\\) iff \\(w\\preceq_i v\\) or \\(v\\preceq_i w\\). Local connectedness implies that \\(\\preceq_i\\) totally orders\n\\(\\Pi_i(w)\\) and well-foundedness implies that\n\\(Min_{\\preceq_i}(\\Pi_i(w))\\) is nonempty. This richer model allows us\nto formally define a variety of (soft) informational attitudes. We\nfirst need some additional notation: the plausibility relation\n\\(\\preceq_i\\) can be lifted to subsets of \\(W\\) as\nfollows[10] Suppose \\(\\M=\\plausmodel\\) is an epistemic-plausibility model,\nconsider the following operators (formally, each is a function from\n\\(\\pow(W)\\) to \\(\\pow(W)\\) similar to the knowledge operator defined\nabove): Belief: \\(B_i(E)=\\{w \\mid\nMin_{\\preceq_i}(\\Pi_i(w))\\subseteq E\\}\\) This is the usual notion\nof belief which satisfies the standard properties discussed above\n(e.g., consistency, positive and negative introspection). Robust Belief: \\(B_i^r(E)=\\{w \\mid v\\in E, \\mbox{ for all }\nv \\mbox{ with } w \\preceq_i v\\}\\)  So, \\(E\\) is robustly believed\nif it is true in all worlds more plausible then the current\nworld. This stronger notion of belief has also been\ncalled certainty by some authors (cf. Shoham &\nLeyton-Brown 2008: sec. 13.7). Strong Belief:  So, \\(E\\) is strongly believed provided it is epistemically possible and\nagent \\(i\\) considers any state in \\(E\\) more plausible\nthan any state in the complement of \\(E\\). It is not hard to see that if agent \\(i\\) knows that \\(E\\) then \\(i\\)\n(robustly, strongly) believes that \\(E\\). However, much more can be said\nabout the logical relationship between these different notions. (The\nlogic of these notions has been extensively studied by Alexandru\nBaltag and Sonja Smets in a series of articles, see Baltag & Smets\n2009 in Other Internet Resources for references.) As noted above, a crucial feature of these informational attitudes\nis that they may be defeated by appropriate evidence. In fact, we can\ncharacterize these attitudes in terms of the type of evidence which\ncan prompt the agent to adjust her beliefs. To make this precise, we\nintroduce the notion of a conditional belief: suppose\n\\(\\M=\\plausmodel\\) is an epistemic-plausibility model and \\(E\\) and \\(F\\)\nare events, then the conditional belief operator is\ndefined as follows: So, ‘\\(B_i^F\\)’ encodes what agent \\(i\\) will believe upon\nreceiving (possibly misleading) evidence that \\(F\\)\nis true. We conclude this section with an example to illustrate the above\nconcepts. Recall again the coordination game\nof Figure 4: there are two actions for\nplayer 1 (Ann), \\(u\\) and \\(d\\), and two actions for player 2 (Bob), \\(r\\)\nand \\(l\\). Again, the preferences (or utilities) of the players are not\nimportant at this stage since we are only interested in describing the\nplayers’ information. The following epistemic-plausibility model\nis a possible description of the players’ informational\nattitudes that can be associated with this game. The solid lines\nrepresent player 1’s informational attitudes and the dashed line\nrepresents player 2’s. The arrows correspond to the players\nplausibility orderings with an \\(i\\)-arrow from \\(w\\) to \\(v\\) meaning\n\\(v\\preceq_i w\\) (we do not draw all the arrows: each plausibility\nordering can be completed by filling in arrows that result from\nreflexivity and transitivity). The different regions represent the\nplayers’ hard information. Figure 7 Suppose that the actual state of play is \\(w_4\\). So, player 1 (Ann)\nchooses \\(u\\) and player 2 (Bob) chooses \\(r\\). Further, suppose that \\(L=\n\\{w_1,w_2,w_5\\}\\) is the event where where player 2 chooses \\(l\\)\n(similarly for \\(U\\), \\(D\\), and \\(R\\)) \\(B_1(L)\\): “player 1 believes that player 2 is choosing\n\\(L\\)” \\(B_1(B_2(U))\\): “player 1 believes that player 2 believes that\nplayer 1 chooses \\(u\\)” \\(B_1^{R} (- B_2(U))\\): “given that player 2 chooses \\(r\\),\nplayer 1 believes that player 2 does not believe she is choosing\n\\(u\\)” This last formula is interesting because it\n“pre-encodes” what player 1 would believe upon learning\nthat player 2 is choosing \\(R\\). Note that upon receiving\nthis true information, player 1 drops her belief that player\n2 believes she is choosing \\(u\\). The situation can be even more\ninteresting if there are statements in the language that reveal\nonly partial information about the player strategy\nchoices. Suppose that \\(E\\) is the event \\(\\{w_4,w_6\\}\\). Now \\(E\\) is true\nat \\(w_4\\) and player 2 believes that player 1 chooses \\(d\\)\ngiven that \\(E\\) is true (i.e., \\(w_4\\in B_2^E(D)\\)). So, player 1 can\n“bluff” by revealing the true (though partial) information\n\\(E\\). Probabilities The above models use a “crisp” notion of uncertainty,\ni.e., for each agent and state \\(w\\), any other state \\(v\\in W\\) either\nis or is not possible at/more plausible than \\(w\\). However, there is an\nextensive body of literature focused on graded,\nor quantitative, models of uncertainty (Huber 2009; Halpern\n2003). For instance, in the Game Theory literature it is standard to\nrepresent the players’ beliefs by probabilities (Aumann\n1999b; Harsanyi 1967–68). The idea is simple: replace the\nplausibility orderings with probability distributions: Definition 2.6 (Epistemic-Probability Model)\nSuppose that \\(G\\) is a strategic game, \\(S\\) is the set of\nstrategy profiles of \\(G\\), and \\(\\A\\) is the set of\nplayers. An epistemic-probabilistic model is a tuple where \\(\\kripkemodel\\) is an epistemic model and assigns to each state a probability measure over\n\\(W\\). Write \\(p_i^w\\) for the \\(i\\)’s probability measure at state\n\\(w\\). We make two natural assumptions\n(cf. Definition 2.4): For all \\(v\\in W\\), if \\(p_i^w(v)>0\\) then \\(p_i^w=p_i^v\\);\nand For all \\(v\\not\\in\\Pi_i(w)\\), \\(p_i^w(v)=0\\). Property 1 says that if \\(i\\) assigns a non-zero probability to state\n\\(v\\) at state \\(w\\) then the agent uses the same probability measure at\nboth states. This means that the players “know” their own\nprobability measures. The second property implies that players must\nassign a probability of zero to all states outside the current (hard)\ninformation cell. These models provide a very precise description of\nthe players’ hard and soft informational attitudes. However,\nnote that writing down a model requires us to specify a different\nprobability measure for each partition cell which can be quite\ncumbersome. Fortunately, the properties in the above definition imply\nthat, for each agent, we can view the agent’s probability\nmeasures as arising from one probability measure through\nconditionalization. Formally, for each \\(i\\in\\Agt\\), agent\n\\(i\\)’s (subjective) prior probability is any\nelement of \\(p_i\\in\\Delta(W)\\). Then, in order to define an\nepistemic-probability model we need only give for each agent\n\\(i\\in\\Agt\\), (1) a prior probability \\(p_i\\in\\Delta(W)\\) and (2) a\npartition \\(\\Pi_i\\) on \\(W\\) such that for each \\(w\\in W\\),\n\\(p_i(\\Pi_i(w))>0\\). The probability measures for each \\(i\\in\\Agt\\) are\nthen defined by: Of course, the side condition that for each \\(w\\in W\\),\n\\(p_i(\\Pi_i(w))>0\\) is important since we cannot divide by\nzero—this will be discussed in more detail in later\nsections. Indeed, (assuming \\(W\\) is \nfinite[11]) given any\nepistemic-plausibility model we can find, for each agent, a prior\n(possibly different ones for different agents) that generates the\nmodel as described above. This is not only a technical observation: it\nmeans that we are assuming that the players’ beliefs about the\noutcome of the situation are fixed ex ante with the ex\ninterim beliefs being derived through conditionalization on the\nagent’s hard information. (See Morris 1995 for an\nextensive discussion of the situation when there is a common\nprior.) We will return to these key assumptions throughout the\ntext. As above we can define belief operators, this time specifying the\nprecise degree to which an agent believes an event: Probabilistic belief: \\(B_i^r(E)=\\{w \\mid\np_i^w(E)=r\\}\\) Here, \\(r\\) can be any real number in the unit\ninterval; however, it is often enough to restrict attention to the\nrational numbers in the unit interval. Full belief: \\(B_i(E)=B_i^1(E)=\\{w \\mid p_i^w(E)=1\\}\\)\nSo, full belief is defined to belief with probability one. This is a\nstandard assumption in this literature despite a number of well-known\nconceptual difficulties (see Huber 2009 for an extensive discussion of\nthis and related issues). It is sometimes useful to work with the\nfollowing alternative characterization of full-belief (giving it a\nmore “modal” flavor): Agent \\(i\\) believes \\(E\\) at state \\(w\\)\nprovided all the states that \\(i\\) assigns positive probability to at\n\\(w\\) are in \\(E\\). Formally, These models have also been subjected to sophisticated logical\nanalyses (Fagin, Halpern, & Megiddo 1990; Heifetz & Mongin\n2001) complementing the logical frameworks discussed above (cf. Baltag\n& Smets 2006). We conclude this section with an example of an\nepistemic-probability model. Recall again the coordination game\nof Figure 4: there are two actions for\nplayer 1 (Ann), \\(u\\) and \\(d\\), and two actions for player 2 (Bob), \\(r\\)\nand \\(l\\). The preferences (or utilities) of the players are not\nimportant at this stage since we are only interested in describing the\nplayers’ information. Figure 8 The solid lines are Ann’s information partition and the\ndashed lines are Bob’s information partition. We further assume\nthere is a common prior \\(p_0\\) with the probabilities assigned to each\nstate written to the right of the state. Let \\(E=\\{w_2,w_5,w_6\\}\\) be an\nevent. Then, we have \\(B_1^{\\frac{1}{2}}(E)=\\{w \\mid p_0(E \\mid\n\\Pi_1(w))=\\frac{p_0(E\\cap\\Pi_1(w))}{p_0(\\Pi_1(w))}=\\frac{1}{2}\\}=\\{w_1,w_2,w_3\\}\\):\n“Ann assigns probability 1/2 to the event \\(E\\) given her\ninformation cell \\(\\Pi_1(w_1)\\). \\(B_2(E)=B_2^1(E)=\\{w_2,w_5,w_3,w_6\\}\\). In particular, note that at\n\\(w_6\\), the agent believes (with probability 1) that \\(E\\) is true, but\ndoes not know that \\(E\\) is true as \\(\\Pi_2(w_6)\\not\\subseteq\nE\\). So, there is a distinction between states the agent considers\npossible (given their “hard information”) and states to\nwhich players assign a non-zero probability. Let \\(U=\\{w_1,w_2,w_3\\}\\) be the event that Ann plays \\(u\\) and\n\\(L=\\{w_1,w_4\\}\\) the event that Bob plays \\(l\\). Then, we have \\(K_1(U)=U\\) and \\(K_2(L)=L\\): Both Ann and Bob know that strategy they\nhave chosen; \\(B_1^{\\frac{1}{2}}(L)=U\\): At all states where Ann plays \\(u\\), Ann\nbelieves that Bob plays \\(L\\) with probability 1/2; and \\(B_1(B_2^{\\frac{1}{2}}(U))=\\{w_1,w_2,w_3\\}=U\\): At all states where\nAnn plays \\(u\\), she believes that Bob believes with probability 1/2\nthat she is playing \\(u\\). An alternative approach to modeling beliefs was initiated by\nHarsanyi in his seminal paper (Harsanyi 1967–68). Rather than\n“possible worlds”, Harsanyi takes the notion of the\nplayers’ type as primitive. Formally, the players are\nassigned a nonempty set of types. Typically, players are assumed\nto know their own type but not the types of the other\nplayers. As we will see, each type can be associated with a specific\nhierarchy of belief   Definition 2.7 (Qualitative Type Space)\n A Qualitative type space for a\n(nonempty) set of states of nature \\(S\\) and agents \\(\\Agt\\) is a tuple\n\\(\\langle \\{T_i\\}_{i\\in\\A}, \\{\\lambda_i\\}_{i\\in\\Agt},S\\rangle\\) where\nfor each \\(i\\in\\Agt\\), \\(T_i\\) is a nonempty set and So, each type \\(t\\in T_i\\) is associated with a set of tuples\nconsisting of types of the other players and a state of nature. For\nsimplicity, suppose there are only two players, Ann and\nBob. Intuitively, \\((t',o')\\in\\lambda_{Ann}(t)\\) means that Ann’s\ntype \\(t\\) considers it possible that the outcome is \\(o'\\) and\nBob is of type \\(t'\\). Since the players’ uncertainty is directed\nat the choices and types of the other players, the\ninformational attitude captured by these models will certainly not\nsatisfy the Truth axiom. In fact, qualitative type spaces can be\nviewed as simply a “re-packaging” of the relational models\ndiscussed above (cf. Zvesper 2010 for a discussion). Consider again the running example of the coordination game between\nAnn and Bob (pictured in Figure 1). In\nthis case, the set of states of nature is\n\\(S=\\{(u,l),(d,l),(u,r),(d,r)\\}\\). In this context, it is natural to\nmodify the definition of the type functions \\(\\lambda_i\\) to account for\nthe fact that the players are only uncertain about the other\nplayers’ choices: let \\(S_A=\\{u,d\\}\\) and \\(S_B=\\{l,r\\}\\) and\nsuppose \\(T_A\\) and \\(T_B\\) are nonempty sets of types. Define \\(\\lambda_A\\)\nand \\(\\lambda_B\\) as follows: Suppose that there are two types for each player:\n\\(T_A=\\{t^A_1,t^A_2\\}\\) and \\(T_B=\\{t^B_1,t^B_2\\}\\). A convenient way to\ndescribe the maps \\(\\lambda_A\\) and \\(\\lambda_B\\) is: Figure 9 where a 1 in the \\((t',s)\\) entry of the above matrices corresponds\nto assuming \\((t',s)\\in\\lambda_i(t)\\) (\\(i=A,B\\)). What does it mean\nfor Ann (Bob) to believe an event \\(E\\) in a type structure?\nWe start with some intuitive observations about the above type\nstructure: Regardless of what type we assign to Ann, she believes that Bob\nwill choose \\(l\\) since in both matrices, \\(\\lambda_A(t_1^A)\\) and\n\\(\\lambda_A(t_2^A)\\), the only places where a 1 appears is under the \\(l\\)\ncolumn. So, fixing a type for Ann, in all of the situations Ann\nconsiders possible it is true that Bob chooses \\(l\\). If Ann is assigned the type \\(t_1^A\\), then she considers it possible\nthat Bob believes she will choose \\(u\\). Notice that type \\(t_1^A\\) has a\n1 in the row labeled \\(t^B_1\\), so she considers it possible that Bob is\nof type \\(t^B_1\\), and type \\(t^B_1\\) believes that Ann chooses \\(u\\) (the\nonly places where 1 appears is under the \\(u\\) column). If Ann is assigned the type \\(t_2^A\\), then Ann believes that Bob\nbelieves that Ann believes that Bob will choose \\(l\\). Note that type\n\\(t_2^A\\) “believes” that Bob will choose \\(l\\) and\nfurthermore \\(t_2^A\\) believes that Bob is of type \\(t^B_2\\) who in turn\nbelieves that Ann is of type \\(t^A_2\\). We can formalize the above informal observations using the\nfollowing notions: Fix a qualitative type space \\(\\langle\n\\{T_i\\}_{i\\in\\A}, \\{\\lambda_i\\}_{i\\in\\Agt},S\\rangle\\) for a (nonempty)\nset of states of nature \\(S\\) and agents \\(\\Agt\\). A (global) state, or possible\nworld is a tuple \\((t_1,t_2,\\ldots,t_n,s)\\) where \\(t_i\\in T_i\\)\nfor each \\(i=1,\\ldots,n\\) and \\(s\\in S\\). If \\(S=\\bigtimes S_i\\) is the set\nof strategy profiles for some game, then we write a possible world as:\n\\((t_1,s_1,t_2,s_2,\\ldots,t_n,s_n)\\) where \\(s_i\\in S_i\\) for each\n\\(i=1,\\ldots,n\\). Type spaces describe the players beliefs about the other\nplayers’ choices, so the notion of an event needs to be\nrelativized to an agent. An event for agent \\(i\\) is a\nsubset of \\(\\bigtimes_{j\\ne i}T_j\\times S\\). Again if \\(S\\) is a set of\nstrategy profiles (so \\(S=\\bigtimes S_i\\)), then an event for agent \\(i\\)\nis a subset of \\(\\bigtimes_{j\\ne i} (T_j\\times S_j)\\). Suppose that \\(E\\) is an event for agent \\(i\\), then we say that\nagent \\(i\\) believes \\(E\\) at \\((t_1,t_2,\\ldots,t_n,s)\\)\nprovided \\(\\lambda(t_1,s)\\subseteq E\\). In the specific example above, an event for Ann is a set\n\\(E\\subseteq T_B\\times S_B\\) and we can define the set of pairs\n\\((t^A,s^A)\\) that believe this event: similarly for Bob. Note that the event \\(B_A(E)\\) is an event for Bob\nand vice versa. A small change to the above definition of a\ntype space (Definition 2.7) allows us to\nrepresent probabilistic beliefs (we give the full definition\nhere for future reference): Definition 2.8 (Type Space)\nA type space for a (nonempty) set of states of\nnature \\(S\\) and agents \\(\\Agt\\) is a tuple \\(\\langle \\{T_i\\}_{i\\in\\A},\n\\{\\lambda_i\\}_{i\\in\\Agt},S\\rangle\\) where for each \\(i\\in\\Agt\\), \\(T_i\\) is\na nonempty set and where \\(\\Delta(\\bigtimes_{j\\ne i} T_j\\times S)\\) is the set of\nprobability measures on \\(\\bigtimes_{j\\ne i} T_j\\times S\\). Types and their associated image under \\(\\lambda_i\\) encode the\nplayers’ (probabilistic) information about the others’\ninformation. Indeed, each type is associated with a hierarchy of\nbelief. More formally, recall that an event \\(E\\) for a type \\(t_i\\) is a\nset of pairs \\((\\sigma_{-j}, t_{-j})\\), i.e., a set of strategy choices\nand types for all the other players. Given an event \\(E\\) for player\n\\(i\\), let \\(\\lambda_i(t_i)(E)\\) denote the sum of the probabilities that\n\\(\\lambda_i(t_i)\\) assigns to the elements of \\(E\\). The type \\(t_i\\) of\nplayer \\(i\\) is said to (all-out) believe the event \\(E\\)\nwhenever \\(\\lambda_i(t_i)(E) = 1\\). Conditional beliefs are computed in\nthe standard way: type \\(t_i\\) believes that \\(E\\) given \\(F\\) whenever: \nA state in a type structure is a tuple \\((\\sigma, t)\\) where\n\\(\\sigma\\) is a strategy profile and \\(t\\) is “type profile”,\na tuple of types, one for each player. Let \\(B_i(E) = \\{(\\sigma_{-j},\nt_{-j}) : t_i \\text{ believes that } E \\}\\) be the event (for \\(j\\)) that\n\\(i\\) believes that \\(E\\). Then agent \\(j\\) believes that \\(i\\) believes that\n\\(E\\) when \\(\\lambda_j(t_j)(B_i(E)) = 1\\). We can continue in this manner\ncomputing any (finite) level of such higher-order information. \nReturning again to our running example game where player 1 (Ann) has\ntwo available actions \\(\\{u,d\\}\\) and player 2 (Bob) has two available\nactions \\(\\{l,r\\}\\). The following type space describes the\nplayers’ information: there is one type for Ann (\\(t_1\\)) and two\nfor Bob (\\(t_2,t_2'\\)) with the corresponding probability measures given\nbelow: Figure 10: Ann's beliefs about\nBob Figure 11: Bob's belief about Ann In this example, since there is only one type for Ann, both of\nBob’s types are certain about Ann’s beliefs. If\nBob is of type \\(t_2\\) then he is certain Ann is choosing \\(u\\) while if\nhe is of type \\(t_2'\\) he thinks there is a 75% chance she plays\n\\(u\\). Ann assigns equal probability (\\(0.5\\)) to Bob’s types; and\nso, she believes it is equally likely that Bob is certain she plays\n\\(u\\) as Bob thinking there is a 75% chance she plays \\(u\\). The above\ntype space is a very compact description of the players’\ninformational attitudes. An epistemic-probabilistic model can describe\nthe same situation (here \\(p_i\\) for \\(i=1,2\\) is player \\(i\\)’s prior\nprobability): Figure 12 Some simple (but instructive!) calculations can convince us that\nthese two models represent the same situation. The more interesting\nquestion is how do these probabilistic models relate to the\nepistemic-doxastic models of Definition\n2.4. Here the situation is more complex. On the one hand,\nprobabilistic models with a graded notion of belief which is much more\nfine-grained than the “all-out” notion of belief discussed\nin the context of epistemic-doxastic models. On the other hand, in an\nepistemic-doxastic model, conditional believes are defined\nfor all events. In the above models, they are only defined\nfor events that are assigned nonzero probabilities. In other words,\nepistemic-probabilistic models do not describe what a player may\nbelieve upon learning something “surprising” (i.e.,\nsomething currently assigned probability zero). A number of extensions to basic probability theory have been\ndiscussed in the literature that address precisely this problem. We do\nnot go into details here about these approaches (a nice summary and\ndetailed comparison between different approaches can be found in\nHalpern (2010) and instead sketch the main ideas. The first approach\nis to use so-called Popper functions which\ntake conditional probability measures as primitive. That is,\nfor each non-empty event \\(E\\), there is a probability measure\n\\(p_E(\\cdot)\\) satisfying the usual Kolmogrov axioms (relativized to\n\\(E\\), so for example \\(p_E(E)=1\\)). A second approach assigns to each\nagent a finite sequence of probability measures \\((p_1,p_2,\\ldots,p_n)\\)\ncalled a lexicographic probability system. The idea is that\nto condition on \\(F\\), first find the first probability measure not\nassigning zero to \\(F\\) and use that measure to condition on\n\\(F\\). Roughly, one can see each of the probability measures in a\nlexicographic probability system as corresponding to a level of a\nplausibility ordering. We will return to these notions\nin Section 5.2. States in a game model not only represent the player’s\nbeliefs about what their opponents will do, but also\ntheir higher-order beliefs about what their opponents are\nthinking. This means that outcomes identified as\n“rational” in a particular informational context will\ndepend, in part, on these higher-order beliefs. Both game\ntheorists and logicians have extensively discussed different notions\nof knowledge and belief for a group, such as common knowledge and\nbelief. In this section, we briefly recount the standard definition of\ncommon knowledge. For more information and pointers to the relevant\nliterature, see Vanderschraaf & Sillari (2009) and Fagin et al.,\n(1995: ch. 6). Consider the statement “everyone in group \\(I\\) knows that\n\\(E\\)”. This is formally defined as follows: where \\(I\\) is any nonempty set of players. If \\(E\\) is common\nknowledge for the group \\(I\\), then not only does everyone in the group\nknow that \\(E\\) is true, but this fact is completely transparent to all\nmembers of the group. We first define \\(K_I^n(E)\\) for each \\(n\\ge 0\\) by\ninduction: Then, following Aumann (1976), common knowledge of\n\\(E\\) is defined as the following infinite conjunction: Unpacking the definitions, we have  The approach to defining common knowledge outlined above can be\nviewed as a recipe for defining common (robust/strong) belief (simply\nreplace the knowledge operators \\(K_i\\) with the appropriate belief\noperator). See Bonanno (1996) and Lismont & Mongin (1994, 2003)\nfor more information about the logic of common belief. Although we do\nnot discuss it in this entry, a probabilistic variant of common belief\nwas introduced by Monderer & Samet (1989). There are many philosophical issues that arise in decision theory,\nbut that is not our concern here. See Joyce 2004 and reference therein\nfor discussions of the main philosophical issues. This section\nprovides enough background on decision theory to understand the key\nresults of epistemic game theory presented in the remainder of this\nentry. Decision rules or choice rules determine what\neach individual player will, or should do, given her preferences and\nher information in a given context. In the epistemic game theory\nliterature the most commonly used choice rules are:\n(strict) dominance, maximization of expected utility\nand admissibility (also known as weak dominance). One can do\nepistemic analysis of games using alternative choice rules, e.g.,\nminmax regret (Halpern & Pass 2011). In this entry, we focus only\non the most common ones. Decision theorists distinguish between choice\nunder uncertainty and choice under risk. In the\nlatter case, the decision maker has probabilistic information about\nthe possible states of the world. In the former case, there is no such\ninformation. There is an extensive literature concerning decision\nmaking in both types of situations (see Peterson 2009 for a discussion\nand pointers to the relevant literature). In the setting of epistemic\ngame theory, the appropriate notion of a “rational choice”\ndepends on the type of game model used to describe the informational\ncontext of the game. So, in general, “rationality” should\nbe read as following a given choice rule. The general approach is to\nstart with a definition of an irrational choice (for\ninstance, one that is strictly dominated given one’s\nbeliefs), and then define rationality as not being irrational. Some\nauthors have recently looked at the consequences of lifting this\nsimplifying assumption (cf., the tripartite notion of\na categorization in Cubitt & Sugden (2011) and Pacuit\n& Roy (2011)), but the presentation of this goes beyond the scope\nof this entry. Finally, when the underlying notion of rationality\ngoes beyond maximization of expected utility, some authors\nhave reserved the word “optimal” to qualify decisions that\nmeet the latter requirement, but not necessarily the full requirements\nof rationality. See the remarks in Section\n5.2 for more on this. Maximization of expected utility is the most well-known choice rule\nin decision theory. Given an agent’s preferences (represented as\nutility functions) and beliefs (represented as subjective probability\nmeasures), the expected utility of an action, or option, is the sum of\nthe utilities of the outcomes of the action weighted by the\nprobability that they will occur (according to the agent’s\nbeliefs). The recommendation is to choose the action that maximizes\nthis weighted average. This idea underlies the Bayesian view\non practical rationality, and can be straightforwardly defined in type\nspaces.[12]\n We start by defining expected utility for a\nplayer in a game.  Suppose that \\(G=\\langle N, \\{S_i, u_i\\}_{i\\in N}\\rangle\\) is a\nstrategic game. A conjecture for player \\(i\\) is a\nprobability on the set \\(S_{-i}\\) of strategy profiles of \\(i\\)’s\nopponents. That is, a conjecture for player \\(i\\) is an element of\n\\(\\Delta(S_{-i})\\), the set of probability measures over\n\\(S_{-i}\\). The expected utility of \\(s_i\\in S_i\\) with\nrespect to a conjecture \\(p\\in \\Delta(S_{-i})\\) is defined as follows: A strategy \\(s_i\\in S_i\\) maximizes expected utility\nfor player \\(i\\) with respect to \\(p\\in \\Delta(S_{-i})\\) provided for all\n\\(s_i'\\in S_i\\), \\(EU(s_i,p)\\ge EU(s_i',p)\\). In such a case, we also say\n\\(s_i\\) is a best response to \\(p\\) in game \\(G\\). We now can define an event in a type space or epistemic-probability\nmodel where all players “choose rationally”, in the sense\nthat their choices maximize expected utility with respect to their\nbeliefs.  Let \\(G=\\langle N, \\{S_i, u_i\\}_{i\\in N}\\rangle\\) be a strategic\ngame and \\(\\T=\\langle \\{T_i\\}_{i\\in\\Agt},\n\\{\\lambda_i\\}_{i\\in\\Agt},S\\rangle\\) a type space for \\(G\\). Recall that\neach \\(t_i\\) is associated with a probability measure \\(\\lambda(t_i)\\in\n\\Delta(S_{-i}\\times T_{-i})\\). Then, for each \\(t_i\\in T_i\\), we can\ndefine a probability measure \\(p_{t_i}\\in \\Delta(S_{-i})\\) as follows: The set of states (pairs of strategy profiles and type profiles)\nwhere player \\(i\\) chooses rationally is then defined as: The event that all players are rational is Notice that here types, as opposed to players, maximize\nexpected utility. This is because in type structure, beliefs are\nassociated to types (see Section 2.3\nabove). The reader acquainted with decision theory will recognize that\nthis is just the standard notion of maximization of expected utility,\nwhere the space of uncertainty of each player, i.e., the possible\n“states of the world” on which the consequences of her\naction depend, is the possible combinations of types and strategy\nchoices of the other players. To illustrate the above definitions, consider the game\nin Figure 4 and the type space\nin Figure 11. The following calculations show that\n\\((u, t_1)\\in \\mathsf{Rat}_1\\) (\\(u\\) is the best response for player \\(1\\)\ngiven her beliefs defined by \\(t_1\\)): A similar calculation shows that \\((l, t_2)\\in \\mathsf{Rat}_2\\).  The definition of a rationality event is similar in an\nepistemic-probability model. For completeness, we give the formal\ndetails. Suppose that  is a strategic game and  is an\nepistemic probability models with each \\(p_i\\) a prior probability\nmeasure over \\(W\\). Each state \\(w\\in W\\), let Then, for each state \\(w\\in W\\), we define\na measure \\(p_w\\in \\Delta(S_{-i})\\) as follows: As above, and When a game model does not describe the players’\nprobabilistic beliefs, we are in a situation of choice\nunder uncertainty. The standard notion of “rational\nchoice” in this setting is based on dominance reasoning\n(Finetti 1974). The two standard notions of dominance are: \nDefinition 3.1 (Strict Dominance)\n  \nSuppose that \\(G=\\langle N, \\{S_i, u_i\\}_{i\\in N}\\rangle\\) is a\nstrategic game and \\(X\\subseteq S_{-i}\\). Let \\(m_i, m_i'\\in \\Delta(S_i)\\)\nbe two mixed strategies for player \\(i\\). The strategy\n\\(m_i\\) strictly dominates \\(m_i'\\) with respect to \\(X\\)\nprovided We say \\(m_i\\) is strictly dominated provided there\nis some \\(m_i'\\in \\Delta(S_i)\\) that strictly dominates \\(m_i\\). A strategy \\(m_i\\in \\Delta(S_i)\\) strictly dominates \\(m_i'\\in\n\\Delta(S_i)\\) provided \\(m_i\\) is better than \\(m_i'\\) (i.e., gives higher\npayoff to player \\(i\\)) no matter what the other players\ndo. There is also a weaker notion: Definition 3.2 (Weak Dominance)\nSuppose that \\(G=\\langle N, \\{S_i, u_i\\}_{i\\in N}\\rangle\\) is a\nstrategic game and \\(X\\subseteq S_{-i}\\). Let \\(m_i, m_i'\\in \\Delta(S_i)\\)\nbe two mixed strategies for player \\(i\\). The strategy\n\\(m_i\\) weakly dominates \\(m_i'\\) with respect to \\(X\\)\nprovided for all \\(s_{-i}\\in X\\), \\(U_i(m_i,s_{-i})\\ge U_i(m_i',s_{-i})\\) and there is some \\(s_{-i}\\in X\\) such that \\(U_i(m_i,s_{-i})>\nU_i(m_i',s_{-i})\\). We say \\(m_i\\) is weakly dominated provided there is\nsome \\(m_i'\\in \\Delta(S_i)\\) that weakly dominates \\(m_i\\). So, a mixed strategy \\(m_i\\) weakly dominates another strategy \\(m_i'\\)\nprovided \\(m_i\\) is at least as good as \\(m_i'\\) no matter what the other\nplayers do and there is at least one situation in which \\(m_i\\)\nis strictly better than \\(m_i'\\). Before we make use of these choice rules, we need to address two\npotentially confusing issues about these definitions. The definitions of strict and weak dominance are given in terms of\nmixed strategies even though we are assuming that players only\nselect pure strategies. That is, we are not considering\nsituations in which players explicitly randomize. In particular,\nrecall that only pure strategies are associated with states in a game\nmodel. Nonetheless, it is important to define strict/weak dominance in\nterms of mixed strategies because there are games in which a pure\nstrategy is strictly (weakly) dominated by a mixed strategy, but not\nby any of the other pure strategies. Even though it is important to consider situations in which a\nplayer’s pure strategy is strictly/weakly dominated by a mixed\nstrategy, we do not extend the above definitions to probabilities over\nthe opponents’ strategies. That is, we do not replace the above\ndefinition with \\(m_i\\) is strictly \\(p\\)-dominates \\(m_i'\\) with\nrespect to \\(X\\subseteq \\Delta(S_{-i})\\), provided for all \\(q\\in X\\),\n\\(U_i(m_i,q)>U_i(m_i',q)\\). This is because both definitions are\n equivalent. Obviously, \\(p\\)-strict dominance implies strict\n dominance. To see the converse, suppose that \\(m_i'\\) is dominated by\n \\(m_i\\) with respect to \\(X\\subseteq S_{-i}\\). We show that for all \\(q\\in\n \\Delta(X)\\), \\(U_i(m_i,q) > U_i(m_i',q)\\) (and so \\(m_i'\\) is\n \\(p\\)-strictly dominated by \\(m_i\\) with respect to \\(X\\)). Suppose that\n \\(q\\in \\Delta(X)\\). Then, The parameter \\(X\\) in the above definitions is intended to represent\nthe set of strategy profiles that the player \\(i\\) take to be\n“live possibilities”. Each state in an epistemic\n(-plausibility) model is associated with a such a set of strategy\nprofiles. Given a possible world \\(w\\) in a game model, let \\(S_{-i}(w)\\)\ndenote the set of states that player \\(i\\) “thinks” are\npossible. The precise definition depends on the type of game\nmodel: Epistemic models\nSuppose that  is a strategic game and  is an epistemic model of \\(G\\). For each player \\(i\\) and \\(w\\in W\\),\ndefine the set \\(S_{-i}(w)\\) as follows: \nEpistemic-Plausibility Models\nSuppose that is a strategic game and is an epistemic-plausibility model of \\(G\\). For each player \\(i\\) and \\(w\\in W\\),\ndefine the set \\(S_{-i}(w)\\) as follows: In either case, we say that a choice at state \\(w\\) is sd-rational\nfor player \\(i\\) at state \\(w\\) provided it is not strictly dominated with\nrespect to \\(S_{-i}(w)\\). The event in which \\(i\\) chooses rationality is\nthen defined as \nIn addition, we have \\(\\mathsf{Rat^{sd}}\\ :=\\ \\bigcap_{i\\in N}\\mathsf{Rat_i^{sd}}\\). \nSimilarly, we can define the set\nof states in which player \\(i\\) is playing a strategy that is not weakly\ndominated, denoted \\(\\mathsf{Rat_i^{wd}}\\) and \\(\\mathsf{Rat^{wd}}\\) using\nweak dominance. Knowledge of one’s own action, the trademark\nof ex-interim situations, plays an important role in the\nabove definitions. It enforces that \\(\\sigma_i(w') = \\sigma_i(w)\\)\nwhenever \\(w'\\in \\Pi_i(w)\\). This means that player \\(i\\)’s\nrationality is assessed on the basis of the result of\nher current choice according to different combinations of\nactions of the other players. An important special case is when the players consider all\nof their opponents’ strategies possible. It should be clear that\na rational player will never choose a strategy that is\nstrictly dominated with respect to \\(S_{-i}\\). That is, if \\(s_i\\) is\nstrictly dominated with respect that \\(S_{-i}\\), then there is no\ninformational context in which it is rational for player \\(i\\) to choose\n\\(s_i\\). This can be made more precise using the following well-known\nLemma.  Lemma 3.1\nSuppose that \\(G=\\langle N, \\{S_i, u_i\\}_{i\\in N}\\rangle\\) is a\nstrategic game. A strategy \\(s_i\\in S_i\\) is strictly dominated\n(possibly by a mixed strategy) with respect to \\(X\\subseteq S_{-i}\\) iff\nthere is no probability measure \\(p\\in \\Delta(X)\\) such that \\(s_i\\) is a\nbest response with respect to \\(p\\). The proof of this Lemma is given in\nthe supplement, Section 1. The general conclusion is that no dominated strategy can maximize\nexpected utility at a given state; and, conversely, if there is a\nstrategy that is not a best in a specific context, then it is not\nstrictly dominated. Similar facts hold about weak dominance, though the\nsituation is more subtle. The crucial observation is that there is a\ncharacterization of weak dominance in terms of best response to\ncertain types of probability measures. A probability measure \\(p\\in\n\\Delta(X)\\) is said to have full support (with respect\nto \\(X\\)) if \\(p\\) assigns positive probability to every element of \\(X\\)\n(formally, \\(supp(p)=\\{x\\in X \\mid p(x)>0\\}=X\\)). Let\n\\(\\Delta^{>0}(X)\\) be the set of full support probability measures on\n\\(X\\). A full support probability on \\(S_{-i}\\) means that player \\(i\\) does\nnot completely rule out (in the sense, that she assigns zero\nprobability to) any strategy profiles of her opponents. The following\nanalogue of Lemma 3.1 is also well-known: Lemma 3.2\nSuppose that \\(G=\\langle N, \\{S_i, u_i\\}_{i\\in\nN}\\rangle\\) is a strategic game. A strategy \\(s_i\\in S_i\\) is weakly\ndominated (possibly by a mixed strategy) with respect to \\(X\\subseteq\nS_{-i}\\) iff there is no full support probability measure \\(p\\in\n\\Delta^{>0}(X)\\) such that \\(s_i\\) is a best response with respect\nto \\(p\\). The proof of this Lemma is more involved. See Bernheim (1984:\nAppendix A) for a proof. In order for a strategy \\(s_i\\) to not be\nstrictly dominated, it is sufficient for \\(s_i\\) to be a best response\nto a belief, whatever that belief is, about the opponents’\nchoices. Admissibility requires something more: the strategy must be a\nbest response to a belief that does not explicitly rule-out any of the\nopponents’ choices. Comparing these two Lemmas, we see that\nstrict dominance implies weak dominance, but not necessarily vice\nversa. A strategy might not be a best response to any\nfull-support probability measure while being a best response to some\nparticular beliefs, those assigning probability one to a state where\nthe player is indifferent between the outcome of its present action\nand the potentially inadmissible one. There is another, crucial, difference between weak and strict\ndominance. The following observation is immediate from the definition\nof strict dominance: Observation 3.3\nIf \\(s_i\\) is strictly dominated with respect to \\(X\\) and\n\\(X'\\subseteq X\\), then \\(s_i\\) is strictly dominated with respect to\n\\(X'\\). If a strategy is strictly dominated, it remains so if the player\ngets more information about what her opponents (might) do. Thus, if a\nstrategy \\(s_i\\) is strictly dominated in a game \\(G\\) with respect to\nthe entire set of her opponents’ strategies \\(S_{-i}\\),\nthen it will never be rational (according to the above definitions) in\nany epistemic (-plausibility) model for \\(G\\). I.e., there are no\nbeliefs player \\(i\\) can have that makes \\(s_i\\) rational. The same\nobservation does not hold for weak dominance. The existential part of\nthe definition of weak dominance means that the analogue\nof Observation 3.3 does not hold for weak\ndominance: if \\(s_i\\) is weakly dominated with respect to \\(X\\) then it\nneed not be the case that \\(s_i\\) is weakly dominated with respect to\nsome \\(X'\\subseteq X\\). The epistemic approach to game theory focuses on the choices\nof individual decision makers in specific informational\ncontexts, assessed on the basis of decision-theoretic choice\nrules. This is a bottom-up, as opposed to the classical top-down,\napproach. Early work in this paradigm include Bernheim (1984) and\nPearce’s (1984) notion of rationalizability and\nAumann’s derivation of correlated equilibrium from the\nminimal assumption that the players are “Bayesian\nrational” (Aumann 1987). An important line of research in epistemic game theory asks under\nwhat epistemic conditions will players follow the\nrecommendations of particular solution concept? Providing such\nconditions is known as an epistemic characterization of a\nsolution concept. In this section, we present two fundamental epistemic\ncharacterization results. The first is a characterization of iterated\nremoval of strictly dominated strategies (henceforth ISDS), and the\nsecond is a characterization of backward induction. These epistemic\ncharacterization results are historically important. They mark the\nbeginning of epistemic game theory as we know it today. Furthermore,\nthey are also conceptually important. The developments in later\nsections build on the ideas presented in this section. The central result of epistemic game theory is that\n“rationality and common belief in rationality implies\niterated elimination of strictly dominated strategies.” This\nresult is already covered in Vanderschraaf & Sillari (2009). For\nthat reason, instead of focusing on the formal details, the emphasis\nhere will be on its significance for the epistemic foundations of game\ntheory. One important message is that the result highlights the\nimportance of higher-order information. Iterated elimination of strictly dominated strategies\n(ISDS) is a solution concept that runs as follows. First, remove from\nthe original game any strategy that is strictly dominated for player\n\\(i\\) (with respect to all of the opponents’ strategy\nprofiles). After having removed the strictly dominated strategies in\nthe original game, look at the resulting sub-game, remove the\nstrategies which have become strictly dominated there, and repeat this\nprocess until the elimination does not remove any strategies. The\nprofiles that survive this process are said to be iteratively\nnon-dominated. For example, consider the following strategic game: Figure 13 Note that \\(r\\) is strictly dominated for player \\(2\\) with respect to\n\\(\\{t, m, b\\}\\). Once \\(r\\) is removed from the game, we have \\(b\\) is\nstrictly dominated for player 1 with respect to \\(\\{l, c\\}\\). Thus,\n\\(\\{(t,l), (t,c), (m,l), (m,c)\\}\\) are iteratively undominated. That is,\niteratively removing strictly dominated strategies generates the\nfollowing sequence of games: Figure 14 For arbitrary large (finite) strategic games, if all players\nare rational and there is common belief that all\nplayers are rational, then they will choose a strategy that\nis iteratively non-dominated. The result is credited to Bernheim\n(1984) and Pearce (1984). See Spohn (1982) for an early version, and\nBrandenburger & Dekel (1987) for the relation with correlated\nequilibrium. Before stating the formal result, we illustrate the result with an\nexample. We start by describing an “informational context”\nof the above game. To that end, define a type space \\(\\T=\\langle \\{T_1,\nT_2\\}, \\{\\lambda_1,\\lambda_2\\}, S\\rangle\\), where \\(S\\) is the strategy\nprofiles in the above game, there are two types for player 1\n\\((T_1=\\{t_1, t_2\\})\\) and three types for player 2 \\((T_2=\\{s_1, s_2,\ns_3\\})\\). The type functions \\(\\lambda_i\\) are defined as follows: Figure 15 We then consider the pairs \\((s,t)\\) where \\(s\\in S_i\\) and \\(t\\in T_i\\)\nand identify  all the rational pairs (i.e., where \\(s\\) is a best\nresponse to \\(\\lambda_i(t)\\), see the previous section for a\ndiscussion): \\(\\mathsf{Rat_1}=\\{(t, t_1), (m, t_1), (b,t_2)\\}\\) \\(\\mathsf{Rat_2}=\\{(l, s_1), (c,s_1), (l, s_2), (c, s_2), (l, s_3)\n\\}\\) The next step is to identify the types that believe that\nthe other players are rational. In this context, belief\nmeans probability 1. For the type \\(t_1\\), we have\n\\(\\lambda_1(t_1)(\\mathsf{Rat}_2)=1\\); however, but \\((r,s_2)\\not\\in \\mathsf{Rat_2}\\), so \\(t_2\\) does not\nbelieve that player \\(2\\) is rational. This can be turned into an\niterative process as follows: Let \\(R_i^1=\\mathsf{Rat_i}\\). We first\nneed some notation. Suppose that for each \\(i\\), \\(R_i^n\\) has been\ndefined. Then, define \\(R_{-i}^n\\) as follows: \nFor each \\(n>1\\), define \\(R_i^n\\) inductively as follows: Thus, we have \\(R_1^2=\\{(t, t_1), (m, t_1)\\}\\). Note that \\(s_2\\)\nassigns non-zero probability to the pair \\((m,t_2)\\) which is not in\n\\(R_1^1\\), so \\(s_2\\) does not believe that \\(1\\) is rational. Thus, we have\n\\(R_2^2=\\{(l,s_1), (c,s_1),(l,s_3)\\}\\). Continuing with this process, we\nhave \\(R_1^2=R_1^3\\). However, \\(s_3\\) assigns non-zero probability to\n\\((b,t_2)\\) which is not in \\(R_1^2\\), so \\(R_2^3=\\{(l, s_1),\n(c,s_1)\\}\\). Putting everything together, we have \nThus, all the profiles that survive\niteratively removing strictly dominated strategies \n\\((\\{(t,l), (m,l), (t,c), (m,c)\\})\\) are consistent with \nstates where the players are rational and commonly believe they are\nrational. Note that, the above process need not generate all\nstrategies that survive iteratively removing strictly dominated\nstrategies. For example, consider a type space with a single type for\nplayer 1 assigning probability 1 to the single type of player 2 and\n\\(l\\), and the single type for player 2 assigning probability 1 to\nthe single type for player 1 and \\(u\\). Then, \\((u,l)\\) is the only\nstrategy profile in this model and obviously rationality and common\nbelief of rationality is satisfied. However, for any type space, if a\nstrategy profile is consistent with rationality and common belief of\nrationality, then it must be a strategy that is in the set of\nstrategies that survive iteratively removing strictly dominated\nstrategies. Theorem 4.1\nSuppose that \\(G\\) is a strategic game and \\(\\T\\) is any type space\nfor \\(G\\). If \\((s,t)\\) is a state in \\(\\T\\) in which all the players are\nrational and there is common belief of rationality—formally,\nfor each \\(i\\), —then \\(s\\) is a strategy profile that survives iteratively\nremoval of strictly dominated strategies. This result establishes sufficient conditions for ISDS. It\nhas also a converse direction: given any strategy profile that\nsurvives iterated elimination of strictly dominated strategies, there\nis a model in which this profile is played where all players are\nrational and this is common knowledge. In other words, one can\nalways view or interpret the choice of a strategy\nprofile that would survive the iterative elimination procedure as one\nthat results from common knowledge of rationality. Of course, this\nform of the converse is not particularly interesting as we can always\ndefine a type space where all the players assign probability 1 to the\ngiven strategy profile (and everyone playing their requisite\nstrategy). Much more interesting is the question whether\nthe entire set of strategy profiles that survive iteratively\nremoval of strictly dominated strategies is consistent with\nrationality and common belief in rationality. This is covered by the\nfollowing theorem of Brandenburger & Dekel (1987) (cf. also Tan\n& Werlang 1988): Theorem 4.2\nFor any game \\(G\\), there is a type structure for that game in\nwhich the strategy profiles consistent with rationality and common\nbelief in rationality is the set of strategies that survive\niterative removal of strictly dominated strategies. See Friedenberg & Keisler (2010) for the strongest versions of\nthe above results. Analogues of the above results have been proven\nusing different game models (e.g., epistemic models,\nepistemic-plausibility models, etc.). For example, see Apt &\nZvesper (2010) proofs of corresponding theorems using Kripke\nmodels. Many authors have pointed out the strength of the common belief\nassumption in the results of the previous section (see, e.g., Gintis\n2009; Bruin 2010). It requires that the players not only believe that\nthe others are not choosing an irrational strategy, but also to\nbelieve that everybody believes that nobody is choosing an irrational\nstrategy, and everyone believes that everyone believes that everyone\nbelieves that nobody is choosing an irrational strategy, and so on. It\nshould be noted, however, that this unbounded character is there only\nto ensure that the result holds for arbitrary finite\ngames. For a particular game and a model for it, a finite iteration\nof “everybody believes that” suffices to ensure a play\nthat survives the iterative elimination procedure. A possible reply to the criticism of the infinitary nature of the\ncommon belief assumption is that the result should be seen as the\nanalysis of a benchmark case, rather than a description of\ngenuine game playing situations or a prescription for what rational\nplayers should do (Aumann 2010). Indeed, common\nknowledge/belief of rationality has long been used as an informal\nexplanation of the idealizations underlying classical game-theoretical\nanalyses (Myerson 1991). The results above show that, once formalized,\nthis assumption does indeed lead to a classical solution concept,\nalthough, interestingly, not the well-known Nash equilibrium,\nas is often informally claimed in early game-theoretic\nliterature. Epistemic conditions for Nash equilibrium are presented\nin Section 5.1. The main message to take away from the results in the previous\nsection is: Strategic reasoning in games involves higher-order\ninformation. This means that, in particular, “Bayesian rationality” alone—i.e., maximization\nof expected utility—is not sufficient to ensure a strategy\nprofile is played that is iteratively undominated, in the general\ncase. In general, first-order belief of rationality will not do\neither. Exactly how many levels of beliefs is needed to guarantee\n“rational play” in game situations is still the subject of\nmuch debate (Kets 2014; Colman 2003; de Weerd, Verbrugge, &\nVerheij 2013; Rubinstein 1989). There are two further issues we need\nto address. First of all, how can agents arrive at a context where rationality\nis commonly believed? The above results do not answer that\nquestion. This has been the subject of recent work in Dynamic\nEpistemic Logic (van Benthem 2003). In this literature, this question\nis answered by showing that the agents can eliminate all higher-order\nuncertainty regarding each others’ rationality, and thus ensure\nthat no strategy is played that would not survive the iterated\nelimination procedure, by repeatedly and publicly\n“announcing” that they are not irrational. In\nother words, iterated public announcement of rationality makes the\nplayers’ expectations converge towards sufficient epistemic\nconditions to play iteratively non-dominated strategies. For more on\nthis dynamic view on solution epistemic characterization see van\nBenthem (2003); Pacuit & Roy (2011); van Benthem & Gheerbrant\n(2010); and van Benthem, Pacuit, & Roy (2011). Second of all, when there are more than two players, the above\nresults only hold if players can believe that the choices of their\nopponents are correlated (Bradenburger & Dekel 1987;\nBrandenburger & Friedenberg 2008). The following example from\nBrandenburger & Friedenberg (2008) illustrates this\npoint. Consider the following three person game where Ann’s\nstrategies are \\(S_A=\\{u,d\\}\\), Bob’s strategies are \\(S_B=\\{l,r\\}\\)\nand Charles’ strategies are \\(S_C=\\{x,y,z\\}\\) and their respective \npreferences for each outcome are given in the corresponding cell: Figure 16 Note that \\(y\\) is not strictly dominated for Charles. It is easy to\nfind a probability measure \\(p\\in\\Delta(S_A\\times S_B)\\) such that \\(y\\)\nis a best response to \\(p\\). Suppose that\n\\(p(u,l)=p(d,r)=\\frac{1}{2}\\). Then, \\(EU(x,p)=EU(z,p)=1.5\\) while\n\\(EU(y,p)=2\\). However, there is no probability measure \\(p\\in\n\\Delta(S_A\\times S_B)\\) such that \\(y\\) is a best response to \\(p\\) and\n\\(p(u,l)=p(u)\\cdot p(l)\\) (i.e., Charles believes that Ann and\nBob’s choices are independent). To see this, suppose that \\(a\\) is\nthe probability assigned to \\(u\\) and \\(b\\) is the probability assigned to\n\\(l\\). Then, we have: The expected utility of \\(y\\) is The expected utility of \\(x\\) is and The expected utility of \\(z\\) is There are three cases: Suppose that \\(a=1-a\\) (i.e., \\(a=1/2\\)). Then, Hence, \\(y\\) is not a best response. Suppose that \\(a>1-a\\). Then, Hence, \\(y\\) is not a best response. Suppose that \\(1-a>a\\). Then, Hence, \\(y\\) is not a best response. In all of the cases, \\(y\\) is not a best response. The second fundamental result analyzes the consequences of\nrationality and common belief/knowledge of rationality\nin extensive games (i.e., trees instead of matrices). Here,\nthe most well-known solution concept is the so-called subgame\nperfect equilibrium, also known as backward induction in\ngames of perfect information. The epistemic characterization of this\nsolution concept is in terms of “substantive rationality”\nand common belief that all players are substantively rational\n(cf. also Vanderschraaf & Sillari 2009: sec. 2.8). The main point\nthat we highlight in this section, which is by now widely acknowledged\nin the literature, is:  Belief revision policies play a key role\nin the epistemic analysis of extensive games\n The most well-known illustration of this is through the comparison\nof two apparently contradictory results regarding the consequences of\nassuming rationality and common knowledge of rationality in extensive\ngames. Aumann (1995) showed that this epistemic condition implies that\nthe players will play according to the backward induction solution\nwhile Stalnaker (1998) argued that this is not necessarily true. The\ncrucial difference between these two results is the way in which they\nmodel the players’ belief change upon (hypothetically) learning\nthat an opponent has deviated from the backward induction path. Extensive games make explicit the sequential structure of choices\nin a game situation. In this section, we focus on games of perfect\ninformation in which there is no uncertainty about earlier\nchoices in the game. These games are represented by tree-like\nstructures: \nDefinition 4.3 (Perfect Information Extensive Game)\nAn extensive game is a tuple \\(\\langle N, T, Act, \\tau, \\{ u_i \\}_{i\\in N}\\rangle\\), where\n \\(N\\) is a finite set of players; \\(T\\) is a tree describing the temporal structure of the game\nsituation: Formally, \\(T\\) consists of a set of nodes and an immediate\nsuccessor relation \\(\\rightarrowtail\\). Let \\(Z\\) denote the set of\nterminal nodes (i.e., nodes without any successors) and \\(V\\) the\nremaining nodes (called decision nodes). Let \\(v_0\\) denote the initial\nnode (i.e., the root of the tree). The edges at a\ndecision node \\(v\\in V\\) are each labeled with actions\nfrom a set \\(Act\\). Let \\(Act(v)\\) denote the set of actions available at\n\\(v\\). Let \\(\\rightsquigarrow\\) be the transitive closure of\n\\(\\rightarrowtail\\). \\(\\tau\\) is a turn function assigning a player to each node \\(v\\in V\\)\n(for a player \\(i\\in N\\), let \\(V_i=\\{v\\in V \\mid \\tau(v)=i\\}\\)). \\(u_i: Z\\rightarrow \\mathbb{R}\\) is the utility function for player\n\\(i\\) assigning real numbers to outcome nodes. A strategy is a term of art in extensive games. It\ndenotes a plan for every eventuality, which tells an agent what to do\nat all histories she is to play, even those which are excluded by the\nstrategy itself. Definition 4.4 (Strategies)\nA strategy for player \\(i\\) is a function \\(s_i:V_i\n\\rightarrow Act\\) where for all \\(v\\in V_i\\), \\(s_i(v)\\in Act(v)\\). A\nstrategy profile, denoted \\({\\mathbf{s}}\\), is an element of \\(\\Pi_{i\\in\nN} S_i\\). Given a strategy profile \\({\\mathbf{s}}\\), let \\({\\mathbf{s}}_i\\)\nbe player \\(i\\)’s component of \\({\\mathbf{s}}\\) and\n\\({\\mathbf{s}}_{-i}\\) the sequence of strategies form \\({\\mathbf{s}}\\) for\nall players except \\(i\\). Each strategy profile \\({\\mathbf{s}}\\) generates a path through an\nextensive game, where a path is a maximal sequence of nodes from the\nextensive game ordered by the immediate successor relations\n\\(\\rightarrowtail\\). We say that \\(v\\) is reached by a\nstrategy profile \\({\\mathbf{s}}\\) is \\(v\\) is on the path generated by\n\\({\\mathbf{s}}\\). Suppose that \\(v\\) is any node in an extensive game. Let\n\\(out(v,{\\mathbf{s}})\\) be the terminal node that is reached if,\nstarting at node \\(v\\), all the players move according to their\nrespective strategies in the profile \\({\\mathbf{s}}\\). Given a decision\nnode \\(v\\in V_i\\) for player \\(i\\), a strategy \\(s_i\\) for player \\(i\\), and a\nset \\(X\\subseteq S_{-i}\\) of strategy profiles of the opponents of \\(i\\),\nlet \\(Out_i(v,s_i, X)=\\{out(v, (s_i, s_{-i})) \\mid s_{-i}\\in X\\}\\). That\nis, \\(Out_i(v, s_i, X)\\) is the set of terminal nodes that may be\nreached if, starting at node \\(v\\), player \\(i\\) uses strategy \\(s_i\\) and\n\\(i\\)’s opponents use strategy profiles from \\(X\\). The following example of a perfect information extensive game will\nbe used to illustrate these concepts. The game is an instance of the\nwell-known centipede game, which has played an important role\nin the epistemic game theory literature on extensive games. Figure 17: An extensive game The decision nodes for \\(A\\) and \\(B\\) respectively are \\(V_A=\\{v_1,\nv_3\\}\\) and \\(V_B=\\{v_2\\}\\); and the outcome nodes are \\(O=\\{o_1, o_2,\no_3, o_4\\}\\). The labels of the edges in the above tree are the actions\navailable to each player. For instance, \\(Act(v_1)=\\{O_1, I_1\\}\\). There\nare four strategies for \\(A\\) and two strategies for \\(B\\). To simplify\nnotation, we denote the players’ strategies by the sequence of\nchoices at each of their decision nodes. For example, \\(A\\)’s\nstrategy \\(s_A^1\\) defined as \\(s_A^1(v_1)=O_1\\) and \\(s_A^1(v_3)=O_3\\) is\ndenoted by the sequence \\(O_1O_3\\). Thus, \\(A\\)’s strategies are:\n\\(s_A^1=O_1O_3\\), \\(s_A^2=O_1I_3\\), \\(s_A^3=I_1O_3\\) and\n\\(s_A^4=I_1I_3\\). Note that \\(A\\)’s strategy \\(s_A^2\\) specifies a\nmove at \\(v_3\\), even though the earlier move at \\(v_1\\), \\(O_1\\), means\nthat \\(A\\) will not be given a chance to move at \\(v_3\\). Similarly,\nBob’s strategies will be denoted by \\(s_B^1=O_2\\) and \\(s_B^2=I_2\\),\ngiving the actions chosen by \\(B\\) at his decision node. Then, for\nexample, \\(out(v_2,(s_A^2, s_B^2))=o_4\\). Finally, if \\(X=\\{s_A^1,\ns_A^4\\}\\), then \\(Out_B(v_2,s_B^2, X)=\\{o_3, o_4\\}\\). There are a variety of ways to describe the players’\nknowledge and beliefs in an extensive game. The game models vary\naccording to which epistemic attitudes are represented (e.g.,\nknowledge and/or various notions of beliefs) and precisely how the\nplayers’ disposition to revise their beliefs during a play of\nthe game is represented. Consult Battigalli, Di Tillio, & Samet\n(2013); Baltag, Smets, & Zvesper (2009); and Battigalli &\nSiniscalchi (2002) for a sampling of the different types of models\nfound in the literature. One of the simplest approaches is to use the epistemic models\nintroduced in Section 2.2 (cf.  Aumann 1995;\nHalpern 2001b). An epistemic model of an extensive game \\(G=\\langle N,\nT, Act, \\tau, \\{ u_i \\}_{i \\in N}\\rangle\\) is a tuple \\(\\langle W,\n\\{\\Pi_i\\}_{i\\in N}, \\sigma\\rangle \\) where \\(W\\) is a nonempty set of\nstates; for each \\(i\\in N\\), \\(\\Pi_i\\) is a partition on \\(W\\); and\n\\(\\sigma:W\\rightarrow \\Pi_{i\\in N} S_i\\) is a function assigning to each\nstate \\(w\\), a strategy profile from \\(G\\). If \\(\\sigma(w)={\\mathbf{s}}\\),\nthen we write \\(\\sigma_i(w)\\) for \\({\\mathbf{s}}_i\\) and \\(\\sigma_{-i}(w)\\)\nfor \\({\\mathbf{s}}_{-i}\\). As usual, we assume that players know their\nown strategies: for all \\(w\\in W\\), if \\(w'\\in \\Pi_i(w)\\), then\n\\(\\sigma_i(w)=\\sigma_i(w')\\). The rationality of a strategy at a decision node depends both on\nwhat actions the strategy prescribes at all future decision\nnodes and what the players know about the strategies that\ntheir opponents are following. Let \\(S_{-i}(w)=\\{\\sigma_{-i}(w') \\mid\nw'\\in \\Pi_i(w)\\}\\) be the set of strategy profiles of player\n\\(i\\)’s opponents that \\(i\\) thinks are possible at state \\(w\\). Then,\n\\(Out_i(v, s_i, S_{-i}(w))\\) is the set of outcomes that player \\(i\\)\nthinks are possible starting at node \\(v\\) if she follows strategy\n\\(s_i\\). Definition 4.5 (Rationality at a decision node)\nPlayer \\(i\\) is rational at node \\(v\\in V_i\\) in state\n\\(w\\) provided, for all strategies \\(s_i\\) such that \\(s_i\\ne\n\\sigma_i(w)\\), there is an \\(o'\\in Out_i(v, s_i, S_{-i}(w))\\) and \\(o\\in\nOut_i(v, \\sigma_i(w), S_{-i}(w))\\) such that \\(u_i(o)\\ge\nu_i(o')\\). So, a player \\(i\\) is rational at a decision node \\(v\\in V_i\\) in state\n\\(w\\) provided that \\(i\\) does not know that there is an alternative\nstrategy that would give her a higher payoff. Definition 4.6 (Substantive rationality)\nPlayer \\(i\\) is substantively rational at state \\(w\\)\nprovided for all decision nodes \\(v\\in V_i\\), \\(i\\) is rational at \\(v\\) in\nstate \\(w\\). We can define the event that player \\(i\\) is substantively rational\nis the standard way: \\(\\mathsf{Rat}_i=\\{w \\mid \\mbox{player } i\\) is\nsubstantively rational at state \\(w\\}\\); and so, the event that all\nplayers are substantively rational is \\(\\mathsf{Rat}=\\bigcap_{i\\in N}\n\\mathsf{Rat}_i\\). This notion of rationality at a decision node \\(v\\) is\nforward-looking in the sense that it only takes account of the\npossibilities that can arise from that point on in the\ngame. It does not take account of the previous moves leading to\n\\(v\\)—i.e., which choices have or could have lead to \\(v\\). We shall\nreturn to this in the discussion below. An important consequence of this is that the rationality of choices\nat nodes that are only followed by terminal nodes are independent of\nthe relevant player’s knowledge. Call a node\n\\(v\\) pre-terminal if all of \\(v\\)’s immediate\nsuccessors are terminal nodes. At such nodes, it does not matter what\nstrategies the player thinks are possible: If \\(v\\) is a pre-terminal\nnode and player \\(i\\) is moving at \\(v\\), then for all states in \\(w\\) in an\nepistemic model of the game and for all strategies \\(s_i\\in S_i\\),\n\\(Out_i(v, s_i, S_{-i}(w))=\\{s_i(v)\\}\\). This means, for example, that\nfor any state \\(w\\) in an epistemic model for the extensive game\nin Figure 17, the only strategies that are\nrational at node \\(v_3\\) in \\(w\\) are those that prescribe that \\(A\\)\nchooses \\(O_3\\) at node \\(v_3\\). Therefore, if \\(w\\in \\mathsf{Rat}_A\\), then\n\\(\\sigma_A(w)(v_3)=O_3\\). Whatever \\(A\\) knows, or rather knew about what\n\\(B\\) would do, if the game reaches the node \\(v_3\\), then the only\nrational choice for \\(A\\) is \\(O_3\\). Information about the rationality of players at pre-terminal nodes\nis very important for players choosing earlier in the game. Returning\nto the game in Figure 17, if \\(B\\) knows that \\(A\\)\nis substantively rational at a state \\(w\\) in an epistemic model of the\ngame, then \\(\\Pi_B(w)\\subseteq \\mathsf{Rat}_A\\). Given the above\nargument, this means that if \\(w'\\in \\Pi_B(w)\\), then\n\\(\\sigma_A(w')(v_3)=O_3\\). Thus, we have for any state \\(w\\) in an\nepistemic model of the game, and, of course, But then,\n\\((O_2)\\) is the only strategy that is rational for \\(B\\) at \\(v_2\\) in any\nstate \\(w\\) (this follows since \\(u_B(o_2)=2\\ge 1=u_B(o_3)\\)). This means\nthat if \\(w\\in \\mathsf{Rat}_B\\) and \\(\\Pi_B(w)\\subseteq \\mathsf{Rat}_A\\),\nthen \\(\\sigma_B(w)(v_2)=O_2\\). Finally, if \\(A\\) knows that \\(B\\) knows that\n\\(A\\) is substantively rational, then A similar argument shows that if \\(w\\in \\mathsf{Rat}_A\\) and \\(w\\in\nK_A(K_B(\\mathsf{Rat}_A))\\), then \\(\\sigma_A(w)(v_1)=O_1\\). The strategy profile \\((O_1O_3, O_2)\\) is the unique\npure-strategy sub-game perfect equilibrium (Selten 1975) of\nthe game in Figure 17. Furthermore, the\nreasoning that we went through in the previous paragraphs is very\nclose to backward induction algorithm. This algorithm can be\nused to calculate the sub-game perfect equilibrium in any perfect\ninformation game in which all players receive unique payoffs at each\noutcome.[13]\n The algorithm runs as follows: BI Algorithm\nAt terminal nodes, players already have the\nnodes marked with their utilities. At a non-terminal node \\(v\\), once\nall immediate successors are marked, the node is marked as follows:\nfind the immediate successor \\(d\\) that has the highest utility for\nplayer \\(\\tau(v)\\) (the players whose turn it is to move at \\(v\\)). Copy\nthe utilities from \\(d\\) onto \\(v\\). Given a marked game tree, the unique path that leads from the root\n\\(v_0\\) of the game tree to the outcome with the utilities that match\nthe utilities assigned to \\(v_0\\) is called the backward\ninduction path. In fact, the markings on each and every node\n(even nodes not on the backward induction path) defines a unique path\nthrough the game tree. These paths can be used to define strategies\nfor each player: At each decision node \\(v\\), choose the action that is\nconsistent with the path from \\(v\\). Let \\(BI\\) denote the\nresulting backward induction profile (where each\nplayer is following the strategy given by the backward induction\nalgorithm). Aumann (1995) showed that the above reasoning can be carried out\nfor any extensive game of perfect information. Theorem 4.7 (Aumann 1995)\nSuppose that \\(G\\) is an extensive game of perfect information and\n\\({\\mathbf{s}}\\) is a strategy profile for \\(G\\). The following are\nequivalent: There is a state \\(w\\) in an epistemic model of \\(G\\) such that\n\\(\\sigma(w) = {\\mathbf{s}}\\) and \\(w \\in C_N(\\mathsf{Rat})\\) (there is\ncommon knowledge that all players are substantively\nrational). \\({\\mathbf{s}}\\) is a sub-game perfect equilibrium of \\(G\\). This result has been extensively discussed. The standard ground of\ncontention is that common knowledge of rationality used in this\nargument seems self-defeating, at least intuitively. Recall\nthat we asked what would \\(B\\) do at node \\(v_2\\) under common knowledge\nof rationality, and we concluded that he would choose \\(O_2\\). But, if\nthe game ever reaches that state, then, by the theorem above, \\(B\\) has\nto conclude that either \\(A\\) is not rational, or that she does not know\nthat he is. Both violate common knowledge of rationality. Is there a\ncontradiction here? This entry will not survey the extensive\nliterature on this question. The reader can consult the references in\nBruin 2010. Our point here is rather that how one looks at this\npotential paradox hinges on the way the players will revise their\nbeliefs in “future” rationality in the light of observing\na move that would be “irrational” under common knowledge\nof rationality. Stalnaker (1996, 1998) offers a different perspective on backward\ninduction. The difference with Aumann’s analysis is best\nillustrated with the following example: Figure 18: An extensive game In the above game the backward induction profile is \\((I_1I_3, I_2)\\)\nleading to the outcome \\(o_4\\) with both players receiving a payoff of\n\\(3\\). Consider an epistemic model with a single state \\(w\\) where\n\\(\\sigma(w)=(O_1I_3,O_2)\\). This is not the backward induction profile,\nand so, by Aumann’s Theorem (Theorem\n4.7) it cannot be common knowledge among \\(A\\) and \\(B\\) at state\n\\(w\\) that both \\(A\\) and \\(B\\) are substantively rational. Recall that a strategy for a player \\(i\\) specifies choices\nat all decision nodes for \\(i\\), even those nodes that are\nimpossible to reach given earlier moves prescribed by the\nstrategy. Thus, strategies include “counterfactual”\ninformation about what players would do if they were given a chance to\nmove at each of their decision nodes. In the single state epistemic\nmodel, \\(B\\) knows that \\(A\\) is following the strategy \\(O_1I_3\\). This\nmeans that \\(B\\) knows two things about \\(A\\)’s choice behavior in\nthe game. The first is that \\(A\\) is choosing \\(O_1\\) initially. The\nsecond is that if \\(A\\) where given the opportunity to choose at \\(v_3\\),\nthen she would choose \\(I_3\\). Now, given \\(B\\)’s knowledge about\nwhat \\(A\\) is doing, there is a sense in which whatever \\(B\\) would\nchoose at \\(v_2\\), his choice is rational. This follows trivially\nsince \\(A\\)’s initial choice prescribed by her strategy at \\(w\\)\nmakes it impossible for \\(B\\) to move. Say that a player \\(i\\)\nis materially rational at a state \\(w\\) in an epistemic\nmodel of a game if \\(i\\) is rational at all decision nodes \\(v\\in V_i\\) in\nstate \\(w\\) that are reachable according to the strategy profile\n\\(\\sigma(w)\\). We have seen that \\(B\\) is trivially materially\nrational. Furthermore, \\(A\\) is materially rational since she knows that\n\\(B\\) is choosing \\(O_2\\) (i.e., \\(S_{-A}(w)=\\{O_2\\}\\)). Thus, \\(Out_A(v_1,\nO_1, S_{-i}(w))=\\{o_1\\}\\) and \\(Out_A(v_1, O_I, S_{-i}(w))=\\{o_2\\}\\); and\nso, \\(A\\)’s choice of \\(O_1\\) at \\(v_1\\) makes her materially rational\nat \\(w\\). The main point of contention between Aumann and Stalnaker\nboils down to whether the single state epistemic model includes enough\ninformation about what exactly \\(B\\) thinks about \\(A\\)’s choice at\n\\(v_3\\) when assessing the rationality of\n\\(B\\)’s hypothetical choice of \\(O_2\\) at \\(v_2\\). According to Aumann, \\(B\\) is not substantively rational: Since\n\\(S_{-B}(w)=\\{O_1I_3\\}\\), we have and and so, \\(B\\) is not\nrational at \\(v_2\\) in \\(w\\) (note that\n\\(u_B(o_4)=3>1=u_B(o_2)\\)). Stalnaker suggests that the players\nshould be endowed with a belief revision policy that\ndescribes which informational state they would revert to in case they\nwere to observe moves that are inconsistent with what they know about\ntheir opponents’ strategies. If \\(B\\) does learn that he can in\nfact move, then he has learned something about \\(A\\)’s\nstrategy. In particular, he now knows that she cannot be following any\nstrategy that prescribes that she chooses \\(O_1\\) at \\(v_1\\) (so, in\nparticular, she cannot be following the strategy \\(O_1I_3\\).) Suppose\nthat \\(B\\) is disposed to react to surprising information about\n\\(A\\)’s choice of strategy as follows: Upon learning that \\(A\\) is\nnot following a strategy in which she chooses \\(O_1\\) at \\(v_1\\), he\nconcludes that she is following strategy \\(I_1O_3\\). That is,\n\\(B\\)’s “belief revision policy” can be summarized as\nfollows: If \\(A\\) makes one “irrational move”, then she will\nmake another one. Stalnaker explains the apparent tension between this\nbelief revision policy and his knowledge that if \\(A\\) where given the\nopportunity to choose at \\(v_3\\), then she would choose \\(I_3\\) as\nfollows: To think there is something incoherent about this combination of\nbeliefs and belief revision policy is to confuse epistemic with causal\ncounterfactuals—it would be like thinking that because I believe\nthat if Shakespeare hadn’t written Hamlet, it would have never\nbeen written by anyone, I must therefore be disposed to conclude that\nHamlet was never written, were I to learn that Shakespeare was in fact\nnot its author. (Stalnaker 1996: 152) Then, with respect to \\(B\\)’s appropriately updated knowledge\nabout \\(A\\)’s choice at \\(v_3\\) (according to his specified belief\nrevision policy), his strategy \\(O_2\\) is in fact rational. According to\nStalnaker, the rationality of a choice at a node \\(v\\) should be\nevaluated in the (counterfactual) epistemic state the player would\nbe in if that node was reached. Assuming \\(A\\) knows that \\(B\\) is\nusing the belief revision policy described above, then \\(A\\) knows that\n\\(B\\) is substantively rational in Stalnaker’s sense. If the model\nincludes explicit information about the players’ belief revision\npolicy, then there can be common knowledge of substantive rationality\n(in Stalnaker’s sense) yet the players’ choices do not\nconform to the backward induction profile. In the previous section, we assumed that the players interpret an\nopponent’s deviation from expected play in an extensive game\n(e.g., deviation from the backward induction path) as an indication\nthat that player will choose “irrationally” at future\ndecision nodes. However, this is just one example of a belief revision\npolicy. It is not suggested that this is the belief revision policy\nthat players should adopt. Stalnaker’s central claim is\nthat models of extensive games should include a component that\ndescribes the players’ disposition to change their beliefs\nduring a play of the game, which may vary from model to model or even\namong the players in a single model: Faced with surprising behavior in the course of a game, the players\nmust decide what then to believe. Their strategies will be based on\nhow their beliefs would be revised, which will in turn be based on\ntheir epistemic priorities—whether an unexpected action should\nbe regarded as an isolated mistake that is thereby epistemically\nindependent of beliefs about subsequent actions, or whether it\nreveals, intentionally or inadvertently, something about the\nplayer’s expectations, and so about the way she is likely to\nbehave in the future. The players must decide, but the theorists\nshould not—at least they should not try to generalize about\nepistemic priorities that are meant to apply to any rational agent in\nall situations. (Stalnaker 1998: 54) One belief revision policy that has been extensively discussed in\nthe epistemic game theory literature is the rationalizability\nprinciple. Battigalli (1997) describes this belief revision\npolicy as follows: Rationalizability Principle\nA player should always try to interpret her information about the\nbehavior of her opponents assuming that they are not implementing\n‘irrational’ strategies. This belief revision policy is closely related to\nso-called forward induction reasoning. To illustrate,\nconsider the following imperfect information game: Figure 19 In the above game, \\(A\\) can either exit the game initially (by\nchoosing \\(e\\)) for a guaranteed payoff of \\(2\\) or decide to play a game\nof imperfect information with \\(B\\). Notice that \\(r_1\\) is strictly\ndominated by \\(e\\): No matter what \\(B\\) chooses at \\(v_3\\), \\(A\\) is better\noff choosing \\(e\\). This means that if \\(A\\) is following a rational\nstrategy, then she will not choose \\(r_1\\) at \\(v_1\\). According to the\nrationalizability principle, \\(B\\) is disposed to believe that \\(A\\) did\nnot choose \\(r_1\\) if he is given a chance to move. Thus, assuming that\n\\(B\\) knows the structure of the game and revises his beliefs according\nto the rationalizability principle, his only rational strategy is to\nchoose \\(l_2\\) at his informational cell (consisting of \\(\\{v_2,\nv_3\\})\\). If \\(A\\) can anticipate this reasoning, then her only rational\nstrategy is to choose \\(e\\) at \\(v_1\\). This is the forward induction\noutcome of the above game. Battigalli & Siniscalchi (2002) develop an epistemic analysis\nof forward induction reasoning in extensive games (cf. also, Stalnaker\n1998: sec. 6). They build on an idea of Stalnaker (1998, 1996) to\ncharacterize forward induction solution concepts in terms of\ncommon strong belief in rationality. We discussed the\ndefinition of “strong belief” in Section\n2.4. The mathematical representation of beliefs in Battigalli\n& Siniscalchi (2002) is different, although the underlying idea is\nthe same. A player strongly believes an event \\(E\\) provided she\nbelieves \\(E\\) is true at the beginning of the game (in the sense that\nshe assigns probability 1 to \\(E\\)) and continues to believe \\(E\\) as long\nas it is not falsified by the evidence. The evidence\navailable to a player in an extensive game consists of the\nobservations of the previous moves that are consistent with the\nstructure of the game tree—i.e., the paths through a game\ntree. A complete discussion of this approach is beyond the scope of\nthe entry. Consult Battigalli & Siniscalchi (2002); Baltag et\nal. (2009); Battigalli & Friedenberg (2012); Bonanno (2013); Perea\n(2012, 2014); and van Benthem & Gheerbrant (2010) for a discussion\nof this approach and alternative epistemic analyses of backward and\nforward induction. In this section, we present a number of results that build on the\nmethodology presented in the previous section. We discuss the\ncharacterization of the Nash equilibrium, incorporate considerations\nof weak dominance into the players’ reasoning and allow the\nplayers to be unaware, as opposed to uncertain,\nabout some aspects of the game. Iterated elimination of strictly dominated strategies is a very\nintuitive concept, but for many games it does not tell anything about\nwhat the players will or should choose. In coordination games\n(Figure 1 above) for instance, all\nprofiles, can be played under rationality and common belief of\nrationality. Looking again at Figure 1, one can\nask what would happen if Bob knew (that is had correct\nbeliefs about) Ann’s strategy choice? Intuitively, it is quite\nclear that his rational choice is to coordinate with her. If\nhe knows that she plays \\(t\\), for instance, then playing \\(l\\)\nis clearly the only rational choice for him, and similarly, if he\nknows that she plays \\(b\\), then \\(r\\) is the only rational choice. The\nsituation is symmetric for Ann. For instance, if she knows that Bob\nplays \\(l\\), then her only rational choice is to choose \\(t\\). More\nformally, the only states where Ann is rational and her\ntype knows (i.e., is correct and assigns probability 1 to)\nBob’s strategy choice and where Bob is also rational and his\ntype knows Ann’s strategy choices are states where they\nplay either \\((t,l)\\) or \\((b,r)\\), the pure-strategy Nash equilibria of\nthe game. A Nash equilibrium is a profile where no player has an\nincentive to unilaterally deviate from his strategy choice. In other\nwords, a Nash equilibrium is a combination of (possibly mixed)\nstrategies such that they all play their best response given the\nstrategy choices of the others. Again, \\((t,l)\\) and \\((b,r)\\) are the\nonly pure-strategy equilibria of the above coordination game. Nash\nequilibrium, and its numerous refinements, is arguably the game\ntheoretical solution concept that has been most used in game theory\n(Aumann & Hart 1994) and philosophy (e.g., famously in Lewis\n1969). The seminal result of Aumann & Brandenburger 1995 provides an\nepistemic characterization of the Nash equilibrium in terms\nof mutual knowledge of strategy choices (and the structure of\nthe game). See, also, Spohn (1982) for an early statement. Before\nstating the theorem, we discuss an example from Aumann &\nBrandenburger (1995) that illustrates the key ideas. Consider the\nfollowing coordination game: Figure 20 The two pure-strategy Nash equilibria are \\((u,l)\\) and \\((d,r)\\)\n(there is also a mixed-strategy equilibrium). As usual, we fix an\ninformational context for this game. Let \\(\\T\\) be a type space for the\ngame with three types for each player \\(T_A=\\{a_1,a_2, a_3\\}\\) and\n\\(T_B=\\{b_1,b_2,b_3\\}\\) with the following type functions: Figure 21 Consider the state \\((d,r,a_3,b_3)\\). Both \\(a_3\\) and \\(b_3\\) correctly\nbelieve (i.e., assign probability 1 to) that the outcome is \\((d,r)\\)\n(we have \\(\\lambda_A(a_3)(r)=\\lambda_B(b_3)(d)=1\\)). This fact is not\ncommon knowledge: \\(a_3\\) assigns a 0.5 probability to Bob being of type\n\\(b_2\\), and type \\(b_2\\) assigns a 0.5 probability to Ann playing\n\\(l\\). Thus, Ann does not know that Bob knows that she is playing \\(r\\)\n(here, “knowledge” is identified with “probability\n1” as it is in Aumann & Brandenburger 1995). Furthermore,\nwhile it is true that both Ann and Bob are rational, it is not common\nknowledge that they are rational. Indeed, the type \\(a_3\\) assigns a 0.5\nprobability to Bob being of type \\(b_2\\) and choosing \\(r\\); however, this\nis irrational since \\(b_2\\) believes that both of Ann’s options\nare equally probable. The example above is a situation where there is mutual knowledge of\nthe choices of the players. Indeed, it is not hard to see that in any\ntype space for a 2-player game \\(G\\), if \\((s,t)\\) is a state where there\nis mutual knowledge that player \\(i\\) is choosing \\(s_i\\) and the players\nare rational, then, \\(s\\) constitutes a (pure-strategy) Nash\nEquilibrium. There is a more general theorem concerning mixed strategy\nequilibrium. Recall that a conjecture for player \\(i\\) is a probability\nmeasure over the strategy choices of her opponents. Theorem 5.1  (Aumann & Brandenburger 1995: Theorem A)\nSuppose that \\(G\\) is a 2-person strategic game, \\((p_1,p_2)\\) are\nconjectures for players 1 and 2, and \\(\\T\\) is a type space for \\(G\\). If\n\\((s,t)\\) is a state in \\(\\T\\) where for \\(i=1,2\\), \\(t_i\\) assigns\nprobability 1 to the events (a) both players are rational (i.e.,\nmaximize expected utility), (b) the game is \\(G\\) and (c) for \\(i=1,2\\),\nplayer \\(i\\)’s conjecture is \\(p_i\\), then \\((p_1, p_2)\\) constitutes\na Nash equilibrium. The general version of this result, for arbitrary finite number of\nagents and allowing for mixed strategies, requires common\nknowledge of conjectures, i.e., of each player’s\nprobabilistic beliefs in the other’s choices. See Aumann &\nBrandenburger (1995: Theorem B) for precise formulation of the result,\nand, again, Spohn (1982) for an early version. See, also, Perea (2007)\nand Tan & Werlang (1988) for similar results about the Nash\nequilibrium. This epistemic characterization of Nash equilibrium requires\nmutual knowledge and rather than beliefs. The result fails\nwhen agents can be mistaken about the strategy choice of the\nothers. This has lead some authors to criticize this epistemic\ncharacterization: See Gintis (2009) and Bruin (2010), for\ninstance. How could the players ever know what the others are\nchoosing? Is it not contrary to the very idea of a game, where the\nplayers are free to choose whatever they want (Baltag et\nal. 2009)? One popular response to this criticism (Brandenburger 2010; Perea\n2012) is that the above result tells us something about Nash\nequilibrium as a solution concept, namely that it\nalleviates strategic uncertainty. Indeed, returning to the\nterminology introduced in Section 1.3, the\nepistemic conditions for Nash equilibrium are those that correspond to\nthe ex post state of information disclosure, “when all\nis said and done”, to put it figuratively. When players have\nreached full knowledge of what the others are going to do, there is\nnothing left to think about regarding the other players as rational,\ndeliberating agents. The consequences of each of the players’\nactions are now certain. The only task that remains is to compute\nwhich action is recommended by the adopted choice rule, and this does\nnot involve any specific information about the other players’\nbeliefs. Their choices are fixed, after all. The idea here is not to reject the epistemic characterization of\nNash Equilibrium on the grounds that it rests on unrealistic\nassumptions, but, rather, to view it as a lesson learned about Nash\nEquilibrium itself. From an epistemic point of view, where one is\nfocused on strategic reasoning about what others are going to\ndo and are thinking, this solution concepts might be of less\ninterest. There is another important lesson to draw from this epistemic\ncharacterization result. The widespread idea that game theory\n“assumes common knowledge of rationality”, perhaps in\nconjunction with the extensive use of equilibrium concepts in\ngame-theoretic analysis, has lead to misconception that the Nash\nEquilibrium either requires common knowledge of rationality,\nor that common knowledge of rationality is sufficient for the players\nto play according to a Nash equilibrium. To be sure, game theoretic\nmodels do assume that the structure of the game is common knowledge\n(though, see Section 5.3). Nonetheless, the\nabove result shows that both of these ideas are incorrect:\n \nCommon knowledge of rationality is neither necessary nor sufficient\nfor Nash Equilibrium.\n In fact, as we just stressed, Nash equilibrium can be played under\nfull uncertainty, and a fortiori under higher-order\nuncertainty, about the rationality of others. In recent years, a number of so-called “modal”\ncharacterizations of Nash Equilibrium have been proposed, mostly using\ntechniques from modal logic (see Hoek & Pauly 2007 for\ndetails). These results typically devise a modal logical language to\ndescribe games in strategic form, typically including modalities for\nthe players’ actions and preference, and show that the notion of\nprofile being a Nash Equilibrium language is definable in\nsuch a language. Most of these characterizations are not epistemic, and thus fall\noutside the scope of this entry. In context of this entry, it is\nimportant to note that most of these results aim at something\ndifferent than the epistemic characterization which we are discussing\nin this section. Mostly developed in Computer Sciences, these logical\nlanguages have been used to verify properties of multi-agents systems,\nnot to provide epistemic foundations to this solution\nconcept. However, note that in recent years, a number of logical\ncharacterizations of Nash equilibrium do explicitly use epistemic\nconcepts (see, for example, van Benthem et al. 2009; Lorini &\nSchwarzentruber 2010). It is not hard to find a game and an informational context where\nthere is at least one player without a unique “rational\nchoice”. How should a rational player incorporate the\ninformation that more than one action is classified as\n“choice-worthy” or “rationally permissible”\n(according to some choice rule) for her opponent(s)? In such a\nsituation, it is natural to require that the player does not rule\nout the possibility that her opponent will pick a\n“choice-worthy” option. More generally, the players should\nbe “cautious” about which of their opponents’\noptions they rule out. Assuming that the players’ beliefs are “cautious”\nis naturally related to weak dominance (recall the characterization of\nweak dominance, Section 3.2\nin which a strategy is weakly dominated iff it does not maximize\nexpected utility with respect to any full support probability\nmeasure). A key issue in epistemic game theory is the epistemic\nanalysis of iterated removal of weakly dominated strategies. Many\nauthors have pointed out puzzles surrounding such an analysis (Asheim\n& Dufwenberg 2003; Brandenburger, Friedenberg & Keisler 2008;\nCubitt & Sugden 1994; Samuelson 1992). For example, Samuelson\n(1992) showed (among other things) that the analogue of Theorem 4.1 is\nnot true for iterated removal of weakly dominated strategies. The main\nproblem is illustrated by the following game: Figure 22 In the above game, \\(d\\) is weakly dominated by \\(u\\) for Ann. If Bob\nknows that Ann is rational (in the sense that she will not choose a\nweakly dominated strategy), then he can rule out option \\(d\\). In the\nsmaller game, action \\(r\\) is now strictly dominated by \\(l\\) for Bob. If\nAnn knows that Bob is rational and that Bob knows that she is rational\n(and so, rules out option \\(d\\)), then she can rule out option\n\\(r\\). Assuming that the above reasoning is transparent to both Ann and\nBob, it is common knowledge that Ann will play \\(u\\) and Bob will play\n\\(l\\). But now, what is the reason for Bob to rule out the possibility\nthat Ann will play \\(d\\)? He knows that Ann knows that he is going to\nplay \\(l\\) and both \\(u\\) and \\(d\\) are best responses to \\(l\\). The problem\nis that assuming that the players’ beliefs are cautious\nconflicts with the logic of iterated removal of weakly dominated\nstrategies. This issue is nicely described in a well-known microeconomics\ntextbook: [T]he argument for deletion of a weakly dominated strategy for\nplayer \\(i\\) is that he contemplates the possibility that every strategy\ncombination of his rivals occurs with positive probability. However,\nthis hypothesis clashes with the logic of iterated deletion, which\nassumes, precisely that eliminated strategies are not expected to\noccur. (Mas-Colell, Winston, & Green 1995: 240) The extent of this conflict is nicely illustrated in Samuelson\n(1992). In particular, Samuelson (1992) shows that there is no\nepistemic-probability \nmodel[14]\n of the above game with\na state satisfying common knowledge of rationality (where\n“rationality” means that players do not choose weakly\ndominated strategies). Prima facie, this is puzzling: What\nabout the epistemic-probability model consisting of a single state \\(w\\)\nassigned the profile \\((u, l)\\)? Isn’t this a model of the above\ngame where there is a state satisfying common knowledge that the\nplayers do not choose weakly dominated strategies? The problem is that\nthe players do not have “cautious” beliefs in this model\n(in particular, Bob’s beliefs are not cautious in the sense\ndescribed below). Recall that having a cautious belief means that a\nplayer cannot know which options her opponent(s)\nwill \npick[15]\n from a set of choice-worthy options (in the\nabove game, if Ann knows that Bob is choosing \\(l\\), then both\n\\(u\\) and \\(d\\) are “choice-worthy”, so Bob\ncannot know that Ann is choosing \\(u\\)). This suggests an\nadditional requirement on a game model: Let \\(\\M=\\epprobmodel\\) be an\nepistemic-probability model. For each action \\(a\\in \\cup_{i\\in\\Agt}\nS_i\\), let \\({[\\![{a}]\\!]}=\\{w \\mid (\\sigma(w))_i=a\\}\\). If \\(a\\in S_i\\) is rational for player \\(i\\) at state\n\\(w\\), then for all players \\(j\\ne i\\), \\({[\\![{a}]\\!]}\\cap \\Pi_j(w)\\ne\n\\emptyset\\). This means that a player cannot know that her opponent\nwill not choose an action at a state \\(w\\) which is deemed rational\n(according to some choice rule). This property is called\n“privacy of tie-breaking” by Cubitt and Sugden (2011: 8)\nand “no extraneous beliefs” by Asheim and Dufwenberg\n(2003).[16]\n For an extended discussion of the above\nassumption see Cubitt & Sugden (2011). Given the above considerations, the epistemic analysis of iterated\nweak dominance is not a straightforward adaptation of the analysis of\niterated strict dominance discussed in the previous section. In\nparticular, any such analysis must resolve the conflict between\nstrategic reasoning where players rule out certain strategy\nchoices of their opponent(s) and admissibility considerations where\nplayers must consider all of their opponents’\noptions possible. A number of authors have developed\nframeworks that do resolve this conflict (Brandenburger et al. 2008;\nAsheim & Dufwenberg 2003; Halpern & Pass 2009). We sketch one\nof these solutions below: The key idea is to represent the players’ beliefs as\na lexicographic probability system (LPS). An LPS is a finite\nsequence of probability measures \\((p_1,p_2,\\ldots,p_n)\\) with supports\n(The support of a probability measure \\(p\\) defined on\na set of states \\(W\\) is the set of all states that have nonzero\nprobability; formally, \\(Supp(p)=\\{w \\mid p(w)>0\\}\\)) that do not\noverlap. This is interpreted as follows: if \\((p_1,\\ldots,p_n)\\)\nrepresents Ann’s beliefs, then \\(p_1\\) is Ann’s\n“initial hypothesis” about what Bob is going to do, \\(p_2\\)\nis Ann’s secondary hypothesis, and so on. In the above game, we\ncan describe Bob’s beliefs as follows: his initial hypothesis\nis that Ann will choose \\(U\\) with probability 1 and his secondary\nhypothesis is that she will choose \\(D\\) with probability 1. The\ninterpretation is that, although Bob does not rule out the possibility\nthat Ann will choose \\(D\\) (i.e., choose irrationally), he does consider\nit infinitely less likely than her choosing \\(U\\) (i.e.,\nchoosing rationally). So, representing beliefs as lexicographic probability measures\nresolves the conflict between strategic reasoning and the assumption\nthat players do not play weakly dominated strategies. However, there\nis another, more fundamental, issue that arises in the epistemic\nanalysis of iterated weak dominance: Under admissibility, Ann considers everything possible. But this is\nonly a decision-theoretic statement. Ann is in a game, so we imagine\nshe asks herself: “What about Bob? What does he consider\npossible?” If Ann truly considers everything possible, then it\nseems she should, in particular, allow for the possibility that Bob\ndoes not! Alternatively put, it seems that a full analysis of the\nadmissibility requirement should include the idea that other players\ndo not conform to the requirement.  (Brandenburger et al. 2008:\n313) There are two main ingredients to the epistemic characterization of\niterated weak dominance. The first is to represent the players’\nbeliefs as lexicographic probability systems. The second is to use a\nstronger notion of belief: A player assumes an event\n\\(E\\) provided \\(E\\) is infinitely more likely than \\(\\overline{E}\\) (on\nfinite spaces, this means each state in \\(E\\) is infinitely more likely\nthan states not in \\(E\\)). The key question is: What is the precise\nrelationship between the event “rationality and common\nassumption of rationality” and the strategies that survive\niterated removal of weakly dominated strategies? The precise answer\nturns out to be surprisingly subtle—the details are beyond the\nscope of this article (see Brandenburger et al. 2008). The game models introduced in Section 2 have\nbeen used to describe the uncertainty that the players have about what\ntheir opponents are going to do and are thinking in a game\nsituation. In the analyses provided thus far, the structure\nof the game (i.e., who is playing, what are the preferences of the\ndifferent players, and which actions are available) is assumed to be\ncommon knowledge among the players. However, there are many situations\nwhere the players do not have such complete information about\nthe game. There is no inherent difficulty in using the models\nfrom Section 2 to describe situations where\nplayers are not perfectly informed about the structure of the\ngame (for example, where there is some uncertainty about available\nactions). There is, however, a foundational issue that arises here. Suppose\nthat Ann considers it impossible that her opponent will\nchoose action \\(a\\). Now, there are many reasons why Ann would hold such\nan opinion. On the one hand, Ann may know something about what her\nopponent is going to do or is thinking which allows her to rule out\naction \\(a\\) as a live possibility—i.e., given all the evidence\nAnn has about her opponent, she concludes that action \\(a\\) is just not\nsomething her opponent will do. On the other hand, Ann may not even\nconceive of the possibility that her opponent will choose action\n\\(a\\). She may have a completely different model of the game in mind\nthan her opponents. The foundational question is: Can the game models\nintroduced in Section 2 faithfully represent\nthis latter type of uncertainty? The question is not whether one can formally describe what Ann\nknows and believes under the assumption that she considers it\nimpossible that her opponent will choose action \\(a\\). Indeed, an\nepistemic-probability model where Ann assigns probability zero to the\nevent that her opponent chooses action \\(a\\) is a perfectly good\ndescription of Ann’s epistemic state. The problem is that this\nmodel blurs an important distinction between Ann\nbeing unaware that action \\(a\\) is a live possibility and\nAnn ruling out that action \\(a\\) is a viable option for her\nopponent. This distinction is illustrated by the following snippet\nfrom the well-known Sherlock Holmes’ short story Silver Blaze\n(Doyle 1894): …I saw by the inspector’s face that his attention had\nbeen keenly aroused.  “You consider that to be\nimportant?” he [Inspector Gregory] asked.\n“Exceedingly so.” “Is there any point to which\nyou would wish to draw my attention?” “To the\ncurious incident of the dog in the night-time.” “The\ndog did nothing in the night-time.” “That was the\ncurious incident,” remarked Sherlock Holmes. The point is that Holmes is aware of a particular event (“the\ndog not barking”) and uses this to come to a conclusion. The\ninspector is not aware of this event, and so cannot (without\nHolmes’ help) come to the same conclusion. This is true of many\ndetective stories: clever detectives not only have the ability to\n“connect the dots”, but they are also aware of\nwhich dots need to be connected. Can we describe the inspector’s\nunawareness in an epistemic \nmodel?[17] Suppose that \\(U_i(E)\\) is the event that the player \\(i\\) is unaware\nof the event \\(E\\). Of course, if \\(i\\) is unaware of \\(E\\) then \\(i\\) does\nnot know that \\(E\\) is true (\\(U_i(E)\\subseteq\n\\overline{K_i(E)}\\), where \\(\\overline{X}\\) denotes the complement\nof the event \\(X\\)). Recall that in epistemic models (where the\nplayers’ information is described by partitions), we have the\nnegative introspection property: This means that if \\(i\\) is unaware of \\(E\\),\nthen \\(i\\) knows that she does not know that \\(E\\). Thus, to capture a\nmore natural definition of \\(U_i(E)\\) where we need to\nrepresent the players’ knowledge in a possibility\nstructure where the \\(K_i\\) operators do not necessarily satisfy\nnegative introspection. A possibility structure is a tuple \\(\\langle W,\n\\{P_i\\}_{i\\in\\A}, \\sigma\\rangle\\) where \\(P_i:W\\rightarrow \\pow(W)\\). The\nonly difference with an epistemic model is that the \\(P_i(w)\\) do not\nnecessarily form a partition of \\(W\\). We do not go into details\nhere—see Halpern (1999) for a complete discussion of possibility\nstructures and how they relate to epistemic models. The knowledge\noperator is defined as it is for epistemic models: for each event \\(E\\),\n\\(K_i(E)=\\{w \\mid P_i(w)\\subseteq E\\}\\). However, S. Modica and\nA. Rustichini (1994, 1999) argue that even the more general\npossibility structures cannot be used to describe a player’s\nunawareness. A natural definition of unawareness on possibility structures is: That is, an agent is unaware of \\(E\\) provided the agent does not\nknow that \\(E\\) obtains, does not know that she does not know that \\(E\\)\nobtains, and so on. Modica and Rustichini use a variant of the above\nSherlock Holmes story to show that there is a problem with this\ndefinition of unawareness. Suppose there are two signals: A dog barking (\\(d\\)) and a cat\nhowling (\\(c\\)). Furthermore, suppose there are three states \\(w_1\\),\n\\(w_2\\) in which the dog barks and \\(w_3\\) in which the cat howls. The\nevent that there is no intruder is \\(E=\\{w_1\\}\\) (the lack of the two\nsignals indicates that there was no\nintruder[18]).\n The following possibility structure\n(where there is an arrow from state \\(w\\) to state \\(v\\) provided \\(v\\in\nP(w)\\)) describes the inspector’s epistemic state: Figure 23 Consider the following calculations: \\(K(E)=\\{w_2\\}\\) (at \\(w_2\\), Watson knows there is a human intruder)\nand \\(-K(E)=\\{w_1,w_3\\}\\) \\(K(-K(E))=\\{w_3\\}\\) (at \\(w_3\\), Watson knows that she does not know\n\\(E\\)), and \\(-K(-K(E))=\\{w_1,w_2\\}\\). \\(-K(E)\\cap -K(-K(E))=\\{w_1\\}\\) and, in fact, \\(\\bigcap_{i=1}^\\infty\n(-K)^i(E)=\\{w_1\\}\\) Let \\(U(F)=\\bigcap_{i=1}^\\infty (-K)^i(F)\\). Then, \\(U(\\emptyset)=U(W)=U(\\{w_1\\})=U(\\{w_2,w_3\\})=\\emptyset\\) \\(U(E)=U(\\{w_3\\})=U(\\{w_1,w_3\\})=U(\\{w_1,w_2\\}=\\{w_1\\}\\) So, \\(U(E)=\\{w_1\\}\\) and \\(U(U(E))=U(\\{w_1\\})=\\emptyset\\). This means\nthat at state \\(w_1\\), the Inspector is unaware of \\(E\\), but is not\nunaware that he is unaware of \\(E\\). More generally, Dekel et al. (1998)\nshow that there is no nontrivial unawareness operator \\(U\\) satisfying\nthe following properties: \\(U(E) \\subseteq \\overline{K(E)}\\cap \\overline{K(E)}\\) \\(K(U(E))=\\emptyset\\) \\(U(E)\\subseteq U(U(E))\\) There is an extensive literature devoted to developing models that\ncan represent the players’ unawareness. See Board, Chung, &\nSchipper (2011); Chen, Ely, & Luo (2012); E. Dekel et al. (1998);\nHalpern (2001a); Halpern & Rego (2008); and Heifetz, Meier, &\nSchipper (2006) for a discussion of issues related to this entry. The\nUnawareness Bibliography (see Other Internet\nResources) has an up-to-date list of papers in this area. The first step in any epistemic analysis of a game is to describe\nthe players’ knowledge and beliefs using (a possible variant of)\none of the models introduced in Section 2. As we\nnoted already in Section 2.2, there will be\nstatements about what the players know and believe about the game\nsituation and about each other that are commonly known in some models\nbut not in others. In any particular structure, certain beliefs, beliefs about belief,\n…, will be present and others won’t be. So, there is an\nimportant implicit assumption behind the choice of a structure. This\nis that it is “transparent” to the players that the\nbeliefs in the type structure—and only those beliefs—are\npossible ….The idea is that there is a “context” to\nthe strategic situation (e.g., history, conventions, etc.) and this\n“context” causes the players to rule out certain\nbeliefs. (Brandenburger & Friedenberg 2010: 801) Ruling out certain configurations of beliefs\nconstitute substantive assumptions about the players’\nreasoning during the decision making process. In other words,\nsubstantive assumptions are about how, and how much, information is\nimparted to the agents, over and above those that are intrinsic to the\nmathematical formulation of the structures used to describe the\nplayers’ information. It is not hard to see that one always\nfinds substantive assumptions in finite structures: Given a countably\ninfinite set of atomic propositions, for instance, in finite\nstructures it will always be common knowledge that some logically\nconsistent combination of these basic facts are not realized,\nand a fortiori for logically consistent configurations of\ninformation and higher-order information about these basic facts. On\nthe other hand, monotonicity of the belief/knowledge operator is a\ntypical example of an assumption that is not\nsubstantive. More generally, there are no models of games, as we\ndefined in Section 2, where it is not common\nknowledge that the players believe all the logical consequences of\ntheir beliefs.[19] Can we compare models in terms of the number of substantive\nassumptions that are made? Are there models that make no, or at least\nas few as possible, substantive assumptions? These questions have been\nextensively discussed in the epistemic foundations of game\ntheory—see the discussion in Samuelson (1992) and the references\nin Moscati (2009). Intuitively, a structure without any substantive\nassumptions must represent all possible states of (higher-order)\ninformation. Whether such a structure exists will depend, in part, on\nhow the players’ informational attitudes are\nrepresented—e.g., as (conditional/lexicographic) probability\nmeasures or set-valued knowledge/belief functions. These questions\nhave triggered interest in the existence of “rich” models\ncontaining most, if not all, possible configurations of (higher-order)\nknowledge and beliefs. There are different ways to understand what it means for a\nstructure to minimize the substantive assumptions about the\nplayers’ higher-order information. We do not attempt a complete\noverview of this interesting literature here (see Brandenburger &\nKeisler (2006: sec. 11) and Siniscalchi (2008: sec. 3) for discussion\nand pointers to the relevant results). One approach considers the\nspace of all (Harsanyi type-/Kripke-/epistemic-plausibility-)\nstructures and tries to find a single structure that, in some suitable\nsense, “contains” all other structures. Such a structure,\noften called called a universal structure (or a terminal\nobject in the language of category theory), if it exists,\nincorporates any substantive assumption that an analyst can\nimagine. Such structure have been shown to exists for Harsanyi type\nspaces (Mertens & Zamir 1985; Brandenburger & Dekel 1993). For\nKripke structures, the question has been answered in the negative\n(Heifetz & Samet 1998; Fagin, Geanakoplos, Halpern, & Vardi\n1999; Meier 2005), with some qualifications regarding the language\nthat is used to describe them (Heifetz 1999; Roy & Pacuit\n2013). A second approach takes an internal perspective by asking\nwhether, for a fixed set of states or types, the agents are\nmaking any substantive assumptions about what their opponents know or\nbelieve. The idea is to identify (in a given model) a set of\npossible conjectures about the players. For example, in a\nknowledge structure based on a set of states \\(W\\) this might be the set\nof all subsets of \\(W\\) or the set definable subsets of \\(W\\) in some\nsuitable logical language. A space is said to be complete if\neach agent correctly takes into account each possible conjecture about\nher opponents. A simple counting argument shows that there cannot\nexist a complete structure when the set of conjectures is all\nsubsets of the set of states (Brandenburger 2003). However, there is a\ndeeper result here which we discuss below. Adam Brandenburger and H. Jerome Keisler (2006) introduce the\nfollowing two person, Russel-style paradox. The statement of the\nparadox involves two concepts: beliefs and\nassumptions. An assumption is a player’s strongest\nbelief: it is a set of states that implies all other beliefs at a\ngiven state. We will say more about the interpretation of an\nassumption below. Suppose there are two players, Ann and Bob, and\nconsider the following description of beliefs. A paradox arises by asking the question To ease the discussion, let \\(C\\) be Bob’s assumption in (S):\nthat is, \\(C\\) is the statement “Ann believes that Bob’s\nassumption is wrong.” So, (Q) asks whether \\(C\\) is true or\nfalse. We will argue that \\(C\\) is true if, and only if, \\(C\\) is\nfalse. Suppose that \\(C\\) is true. Then, Ann does believe that Bob’s\nassumption is wrong, and, by introspection, she believes that she\nbelieves this. That is to say, Ann believes that \\(C\\) is\ncorrect. Furthermore, according to (S), Ann believes that Bob’s\nassumption is \\(C\\). So, Ann, in fact, believes that Bob’s\nassumption is correct (she believes Bob’s assumption is \\(C\\) and\nthat \\(C\\) is correct). So, \\(C\\) is false. Suppose that \\(C\\) is false. This means that Ann believes that\nBob’s assumption is correct. That is, Ann believes that \\(C\\) is\ncorrect (By (S), Ann believes that Bob’s assumption is\n\\(C\\)). Furthermore, by (S), we have that Ann believes that Bob\nassumes that Ann believes that \\(C\\) is wrong. So, Ann believes\nthat she believes that \\(C\\) is correct and she believes that Bob\nassumption is that she believes that \\(C\\) is wrong. So, it is true that\nshe believes Bob’s assumptions is wrong (Ann believes that\nBob’s assumption is she believes that \\(C\\) is wrong, but\nshe believes that is wrong: she believes that \\(C\\) is\ncorrect). So, \\(C\\) is true. Brandenburger and Keisler formalize the above argument in order to\nprove a very strong impossibility result about the existence of\nso-called assumption-complete structures. We need some\nnotation to state this result. It will be most convenient to work in\nqualitative type spaces for two players\n(Definition 2.7). A qualitative type space\nfor two players (cf. Definition 2.7. The set\nof states is not important in what follows, so we leave it out) is a\nstructure \\(\\langle \\{T_A, T_B\\}, \\{\\lambda_A, \\lambda_B\\}\\rangle\\)\nwhere A set of conjectures about Ann is a subset\n\\(\\C_A\\subseteq \\pow(T_A)\\) (similarly, the set of conjectures about Bob\nis a subset \\(\\C_B\\subseteq \\pow(T_B)\\)). A structure \\(\\langle \\{T_A,\nT_B\\}, \\{\\lambda_A, \\lambda_B\\}\\rangle\\) is said to\nbe assumption-complete for the conjectures \\(\\C_A\\) and\n\\(\\C_B\\) provided for each conjecture in \\(\\C_A\\) there is a type that\nassumes that conjecture (similarly for Bob). Formally, for each\n\\(Y\\in\\C_B\\) there is a \\(t_0\\in T_A\\) such that \\(\\lambda_A(t_0)=Y\\), and\nsimilarly for Bob. As we remarked above, a simple counting argument\nshows that when \\(\\C_A=\\pow(T_A)\\) and \\(\\C_B=\\pow(T_B)\\), then\nassumption-complete models only exist in trivial cases. A much deeper\nresult is: Theorem 6.1 (Brandenburger & Keisler 2006: Theorem 5.4)\nThere is no assumption-complete type structure for the set of\nconjectures that contains the first-order definable subsets. See the supplement for a discussion of the proof of this theorem\n(see Section 2). Consult Pacuit (2007) and  Abramsky & Zvesper (2010) for an extensive analysis and\ngeneralization of this result. But, it is not all bad news: Mariotti,\nMeier, & Piccione (2005) construct a complete structure where the\nset of conjectures are compact subsets of some well-behaved\ntopological space. The epistemic view on games is that players should be seen as\nindividual decision makers, choosing what to do on the basis of their\nown preferences and the information they have in specific\ninformational contexts. What decision they will make—the\ndescriptive question—or what decision they should make—the\nnormative question, depends on the decision-theoretic choice rule that\nthe player use, or should use, in a given context. We conclude with\ntwo general methodological issues about epistemic game theory and some\npointers to further reading. Common knowledge of rationality is an informal assumption that game\ntheorists, philosophers and other social scientists often appeal to\nwhen analyzing social interactive situations. The epistemic program in\ngame theory demonstrates that there are many ways to understand what\nexactly it means to assume that there is “common\nknowledge/belief of rationality” in a game situation. Broadly speaking, much of the epistemic game theory literature is\nfocused on two types of projects. The goal of the first project is to\nmap out the relationship between different mathematical\nrepresentations of what the players know and believe about each other\nin a game situation. Research along these lines not only raises\ninteresting technical questions about how to compare and contrast\ndifferent mathematical models of the players’ epistemic states,\nbut it also highlights the benefits and limits of an epistemic\nanalysis of games. The second project addresses the nature of rational\nchoice in game situations. The importance of this project is nicely\nexplained by Wolfgang Spohn: …game theory…is, to put it strongly, confused about\nthe rationality concept appropriate to it, its assumptions about its\nsubjects (the players) are very unclear, and, as a consequence, it is\nunclear about the decision rules to be applied….The basic\ndifficulty in defining rational behavior in game situations is the\nfact that in general each player’s strategy will depend on his\nexpectations about the other players’ strategies. Could we\nassume that his expectations were given, then his problem of strategy\nchoice would become an ordinary maximization problem: he could simply\nchoose a strategy maximizing his own payoff on the assumption that the\nother players would act in accordance with his given expectations. But\nthe point is that game theory cannot regard the players’\nexpectations about each other’s behavior as given; rather, one\nof the most important problems for game theory is precisely to decide\nwhat expectations intelligent players can rationally entertain about\nother intelligent players’ behavior. (Spohn 1982: 267) Much of the work in epistemic game theory can be viewed as an\nattempt to use precise representations of the players’ knowledge\nand beliefs to help resolve some of the confusion alluded to in the\nabove quote. In an epistemic analysis of a game, the specific recommendations or\npredictions for the players’ choices are derived from\ndecision-theoretic choice rules. Maximization of expected utility, for\ninstance, underlies most of the results in the contemporary literature\non the epistemic foundations of game theory. From a methodological\nperspective, however, the choice rule that the modeler assumes the\nplayers are following is simply a parameter that can be varied. In\nrecent years, there have been some initial attempts to develop\nepistemic analyses with alternative choice rules, for\ninstance minregret Halpern & Pass (2009). The reader interested in more extensive coverage of all or some of\nthe topics discussed in this entry should consult the following\narticles and books. Logic in Games by Johan van Benthem: This book uses the\ntools of modal logic broadly conceived to discuss many of the issues\nraised in this entry (2014, MIT Press). The Language of Game Theory by Adam Brandenburger: A\ncollection of Brandenburger’s key papers on epistemic game\ntheory (2014, World Scientific Series in Economic Theory). Epistemic Game Theory by Eddie Dekel and Marciano\nSiniscalchi: A survey paper aimed at economists covering the main\ntechnical results of epistemic game theory (2014, Available online). Epistemic Game Theory: Reasoning and Choice by\nAndrés Perea: A non-technical introduction to epistemic game\ntheory (2012, Cambridge University Press). The Bounds of Reason: Game Theory and the Unification of the\nBehavioral Sciences by Herbert Gintis: This book offers a broad\noverview of the social and behavioral science using the ideas of\nepistemic game theory (2009, Princeton University Press).","contact.mail":"Olivier.Roy@uni-bayreuth.de","contact.domain":"uni-bayreuth.de"}]
