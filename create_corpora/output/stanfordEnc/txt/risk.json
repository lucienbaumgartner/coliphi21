[{"date.published":"2007-03-13","date.changed":"2018-07-19","url":"https://plato.stanford.edu/entries/risk/","author1":"Sven Ove Hansson","author1.info":"http://people.kth.se/~soh/","entry":"risk","body.text":"\n\n\n\nSince the 1970s, studies of risk have grown into a major\ninterdisciplinary field of research. Although relatively few\nphilosophers have focused their work on risk, there are important\nconnections between risk studies and several philosophical\nsubdisciplines. This entry summarizes the most well-developed of these\nconnections and introduces some of the major topics in the philosophy\nof risk. It consists of six sections dealing with the definition of\nrisk and with treatments of risk related to epistemology, the\nphilosophy of science, the philosophy of technology, ethics, and the\nphilosophy of economics.\n\n\n\nIn non-technical contexts, the word “risk” refers, often\nrather vaguely, to situations in which it is possible but not certain\nthat some undesirable event will occur. In technical contexts, the\nword has several more specialized uses and meanings. Five of these are\nparticularly important since they are widely used across\ndisciplines: \n\nAn example of this usage is: “Lung cancer is one of the major\nrisks that affect smokers.”  \n\nAn example of this usage is: “Smoking is by far the most\nimportant health risk in industrialized countries.” (The\nunwanted event implicitly referred to here is a disease caused by\nsmoking.) Both (1) and (2) are qualitative senses of risk. The word\nalso has quantitative senses, of which the following is the oldest\none: \n\nThis usage is exemplified by the following statement: “The risk\nthat a smoker’s life is shortened by a smoking-related disease\nis about 50%.”  \n\nThe expectation value of a possible negative event is the product of\nits probability and some measure of its severity. It is common to use\nthe number of killed persons as a measure of the severity of an\naccident. With this measure of severity, the “risk” (in\nsense 4) associated with a potential accident is equal to the\nstatistically expected number of deaths. Other measures of severity\ngive rise to other measures of risk. \n\nAlthough expectation values have been calculated since the\n17th century, the use of the term “risk” in\nthis sense is relatively new. It was introduced into risk analysis in\nthe influential Reactor Safety Study, WASH-1400 (Rasmussen et al.,\n1975, Rechard 1999). Today it is the standard technical meaning of the term\n“risk” in many disciplines. It is regarded by some risk\nanalysts as the only correct usage of the term. \n\nIn addition to these five common meanings of “risk”\nthere are several other more technical meanings, which are\nwell-established in specialized fields of inquiry. Some of the major\ndefinitions of risk that are used in economic analysis will be\nintroduced below in section 6.1. \n\nAlthough most of the above-mentioned meanings of “risk”\nhave been referred to by philosophers, a large part of the\nphilosophical literature on risk refers to risk in the more informal\nsense that was mentioned at the beginning of this section, namely as a\nstate of affairs in which an undesirable event may or may not\noccur. Several philosophers have criticized the technical definitions\nof risk for being too limited and not covering all aspects that should\nbe included in risk assessments (Buchak 2014, Pritchard 2015,\nShrader-Frechette 1991). Linguistic evidence indicates that technical\ndefinitions of risk have had virtually no impact on the non-technical\nusage of the word (Boholm et al. 2016). \n\nTerminological note: Some philosophers distinguish between\n“subjective” and “objective”\nprobabilities. Others reserve the term “probability” for\nthe subjective notion. Here, the former terminology is used,\ni.e. “probability” can refer either to subjective\nprobability or to objective chances. \n\nWhen there is a risk, there must be something that is unknown or\nhas an unknown outcome. Therefore, knowledge about risk is knowledge\nabout lack of knowledge. This combination of knowledge and lack\nthereof contributes to making issues of risk complicated from an\nepistemological point of view. \n\nIn non-regimented usage, “risk” and\n“uncertainty” differ along the subjective—objective\ndimension. Whereas “uncertainty” seems to belong to the\nsubjective realm, “risk” has a strong objective component.\nIf a person does not know whether or not the grass snake is poisonous,\nthen she is in a state of uncertainty with respect to its ability to\npoison her. However, since this species has no poison there is no risk\nto be poisoned by it. The relationship between the two concepts\n“risk” and “uncertainty” seems to be in part\nanalogous to that between “truth” and\n“belief”. \n\nRegimented decision-theoretical usage differs from this. In decision\ntheory, a decision is said to be made “under risk” if the\nrelevant probabilities are available and “under\nuncertainty” if they are unavailable or only partially\navailable. Partially determined probabilities are sometimes expressed\nwith probability intervals, e.g., “the probability of rain\ntomorrow is between 0.1 and 0.4”.  (The term “decision\nunder ignorance” is sometimes used about the case when no\nprobabilistic information at all is available.)  \n\nAlthough this distinction between risk and uncertainty is\ndecision-theoretically useful, from an epistemological point of view it\nis in need of clarification. Only very rarely are probabilities known\nwith certainty. Strictly speaking, the only clear-cut cases of\n“risk” (known probabilities) seem to be idealized textbook\ncases that refer to devices such as dice or coins that are supposed to\nbe known with certainty to be fair. In real-life situations, even if we\nact upon a determinate probability estimate, we are not fully certain\nthat this estimate is exactly correct, hence there is uncertainty. It\nfollows that almost all decisions are made “under\nuncertainty”. If a decision problem is treated as a decision\n“under risk”, then this does not mean that the decision in\nquestion is made under conditions of completely known probabilities.\nRather, it means that a choice has been made to simplify the\ndescription of this decision problem by treating it as a case of known\nprobabilities. This is often a highly useful idealization in decision\ntheory. However, in practical applications it is important to\ndistinguish between those probabilities that can be treated as known\nand those that are uncertain and therefore much more in need of\ncontinuous updating. Typical examples of the former are the failure\nfrequencies of a technical component that are inferred from extensive\nand well-documented experience of its use. The latter case is\nexemplified by experts’ estimates of the expected failure\nfrequencies of a new type of component. \n\nA major problem in the epistemology of risk is how to deal with the\nsevere limitations that characterize our knowledge of the behaviour of\nunique complex systems that are essential for estimates of risk, such\nas the climate system, ecosystems, the world economy, etc. Each of\nthese systems contains so many components and potential interactions\nthat important aspects of it are unpredictable. However, in spite of\nthis fundamental uncertainty, reasonably reliable statements about\nsome aspects of these systems can be made. The epistemological status\nof such statements, and the nature of the uncertainty involved, are\nstill in need of further clarification (McKinney 1996,\nShrader-Frechette 1997). \n\nIn the risk sciences, it is common to distinguish between\n“objective risk” and “subjective risk”. The\nformer concept is in principle fairly unproblematic since it refers to\na frequentist interpretation of probability. The latter concept is\nmore ambiguous. In the early psychometric literature on risk (from the\n1970s), subjective risk was often conceived as a subjective estimate\nof objective risk. In more recent literature, a more complex picture\nhas emerged. Subjective appraisals of (the severity of) risk depend to\na large extent on factors that are not covered in traditional measures\nof objective risk (such as control and tampering with nature).  If the\nterms are taken in this sense, subjective risk is influenced by the\nsubjective estimate of objective risk, but cannot be identified with\nit. In the psychological literature, subjective risk is often\nconceived as the individual’s overall assessment of the seriousness of\na danger or alleged danger. Such individual assessments are commonly\ncalled “risk perception”, but strictly speaking the term\nis misleading. This is not a matter of perception, but rather a matter\nof attitudes and expectations. Subjective risk can be studied with\nmethods of attitude measurement and psychological scaling\n(Sjöberg 2004). \nThe role of values in science has been particularly controversial in\nissues of risk. Risk assessments have frequently been criticized for\ncontaining “hidden” values that induce a too high\nacceptance of risk (Cranor 2016; 2017; Intemann 2016; Heinzerling\n2000). There is also a discussion on the need to strengthen the impact\nof certain values in risk assessment, such as considerations of\njustice (Shrader-Frechette 2005a), human rights (Shrader-Frechette\n2005b), and the rights and welfare of future people (Caney 2009, Ng\n2005).\n\nIssues of risk have also given rise to heated debates on what levels of\nscientific evidence are needed for policy decisions. The proof\nstandards of science are apt to cause difficulties whenever science is\napplied to practical problems that require standards of proof or\nevidence other than those of science.  \n\nA decision to accept or reject a scientific statement (for instance an\nhypothesis) is in practice always subject to the possibility of\nerror. The chance of such an error is often called the inductive risk\n(Hempel 1965, 92). There are two major types of errors. The first of\nthese consists in concluding that there is a phenomenon or an effect\nwhen in fact there is none. This is called an error of type I (false\npositive). The second consists in missing an existing phenomenon or\neffect. This is called an error of type II (false negative). In the\ninternal dealings of science, errors of type I are in general regarded\nas more problematic than those of type II. The common scientific\nstandards of statistical significance substantially reduce the risk of\ntype I errors but do not protect against type II\nerrors (Shrader-Frechette 2008; John 2017). \n\nMany controversies on risk assessment concern the balance between\nrisks of type I and type II errors. Whereas science gives higher\npriority to avoiding type I errors than to avoiding type II errors,\nthe balance can shift when errors have practical consequences. This\ncan be seen from a case in which it is uncertain whether there is a\nserious defect in an airplane engine. A type II error, i.e., acting as\nif there were no such a defect when there is one, would in this case\nbe counted as more serious than a type I error, i.e., acting as if\nthere were such a defect when there is none. (The distinction between\ntype I and type II errors depends on the delimitation of the effect\nunder study. In discussions of risk, this delimitation is mostly\nuncontroversial; see Lemons et al. 1997, van den Belt and Gremmen\n2002.) \n\nIn this particular case it is fairly uncontroversial that avoidance of\ntype II error should be given priority over avoidance of type I\nerror. In other words, it is better to delay the flight and then find\nout that the engine was in good shape than to fly with an engine that\nturns out to malfunction. However, in other cases the balance between\nthe two error types is more controversial. Controversies are common,\nfor instance, over what degree of evidence should be required for\nactions against possible negative effects of chemical substances on\nhuman health and the environment. \n\nFigure 1. The use of scientific data for policy purposes. \n\nSuch controversies can be clarified with the help of a simple but\nillustrative model of how scientific data influence both scientific\njudgments and practical decisions (Hansson 2008). Scientific knowledge begins with\ndata that originate in experiments and other observations. (See Figure\n1.)  Through a process of critical assessment, these data give rise to\nthe scientific corpus (arrow 1). Roughly speaking, the corpus consists\nof those statements that could, for the time being, legitimately be\nmade without reservation in a (sufficiently detailed) textbook. When\ndetermining whether or not a scientific hypothesis should be accepted,\nfor the time being, as part of the corpus, the onus of proof falls on\nits adherents. Similarly, those who claim the existence of an as yet\nunproven phenomenon have the burden of proof. These proof standards\nare essential for the integrity of science.  \n\nThe most obvious way to use scientific information for policy-making\nis to employ information from the corpus (arrow 2). For many purposes,\nthis is the only sensible thing to do. However, in risk management\ndecisions exclusive reliance on the corpus may have unwanted\nconsequences.  Suppose that toxicological investigations are performed\non a substance that has not previously been studied from a\ntoxicological point of view. These investigations turn out to be\ninconclusive.  They give rise to science-based suspicions that the\nsubstance is dangerous to human health, but they do not amount to full\nscientific proof in the matter. Since the evidence is not sufficient\nto warrant an addition to the scientific corpus, this information\ncannot influence policies in the standard way (via arrows 1 and\n2). There is a strict requirement to avoid type I errors in the\nprocess represented by arrow 1, and this process filters out\ninformation that might in this case have been practically relevant and\njustified certain protective measures. \n\nIn cases like this, a direct road from data to policies is often taken\n(arrow 3). This means that a balance between type I and type II errors\nis determined in the particular case, based on practical\nconsiderations, rather than relying on the standard scientific\nprocedure with its strong emphasis on the avoidance of type I\nerrors. \n\nIt is essential to distinguish here between two kinds of\nrisk-related decision processes. One consists in determining which\nstatements about risks should be included in the scientific corpus. The\nother consists in determining how risk-related information should\ninfluence practical measures to protect health and the environment. It\nwould be a strange coincidence if the criteria of evidence in these two\ntypes of decisions were always the same. Strong reasons can be given\nfor strict standards of proof in science, i.e. high entry requirements\nfor the corpus. At the same time, there can be valid policy reasons to\nallow risk management decisions to be influenced by sound scientific\nindications of danger that are not yet sufficiently well-established to\nqualify for inclusion into the scientific corpus.  \n\nPolicy issues concerning risk have often been the targets of extensive\ndisinformation campaigns characterized by science denial and other\nforms of pseudoscience (Oreskes 2010). Several philosophers have been\nactive in the repudiation of invalid claims and the defence of science\nin risk-related issues (Cranor 2005, 2016, 2017; Goodwin 2009,\nProthero 2013, Shrader-Frechette 2014, Hansson 2017). Safety and the avoidance of risk are major concerns in practical\nengineering. Safety engineering has also increasingly become the\nsubject of academic investigations. However, these discussions are\nlargely fragmented between different areas of technology. The same\nbasic ideas or “safety philosophies” are discussed under\ndifferent names for instance in chemical, nuclear, and electrical\nengineering. Nevertheless, much of the basic thinking seems to be the\nsame in the different areas of safety engineering (Möller and\nHansson 2008).  \n\nSimple safety principles, often expressible as rules of thumbs, have a\ncentral role in safety engineering. Three of the most important of\nthese are inherent safety, safety factors, and multiple barriers. \n\nInherent safety, also called primary prevention, consists in the\nelimination of a hazard. It is contrasted with secondary prevention\nthat consists in reducing the risk associated with a hazard. For a\nsimple example, consider a process in which inflammable materials are\nused. Inherent safety would consist in replacing them by\nnon-inflammable materials. Secondary prevention would consist in\nremoving or isolating sources of ignition and/or installing\nfire-extinguishing equipment. As this example shows, secondary\nprevention usually involves added-on safety equipment. The major\nreason to prefer inherent safety to secondary prevention is that as\nlong as the hazard still exists, it can be realized by some\nunanticipated triggering event. Even with the best of control\nmeasures, if inflammable materials are present, some unforeseen chain\nof events can start a fire.  \n\nSafety factors are numerical factors employed or used as part of the\ndesign process for our houses, bridges, vehicles, tools, etc., in\norder to ensure that our constructions are stronger than the bare\nminimum expected to be required for their functions.  Elaborate\nsystems of safety factors have been specified in norms and\nstandards. A safety factor most commonly refers to the ratio between a\nmeasure of the maximal load not leading to the specified type of\nfailure and a corresponding measure of the maximal expected load. It\nis common to make bridges and other constructions strong enough to\nwithstand twice or three times the predicted maximal load. This means\nthat a safety factor of two or three is employed. \n\nSafety barriers are often arranged in chains. Ideally, each barrier is\nindependent of its predecessors so that if the first fails, then the\nsecond is still intact, etc. For example, in an ancient fortress, if\nthe enemy managed to pass the first wall, then additional layers would\nprotect the defending forces. Some engineering safety barriers follow\nthe same principle of concentric physical barriers. Others are\narranged serially in a temporal or functional rather than a spatial\nsense. One of the lessons that engineers learned from\nthe Titanic disaster is that improved construction of early\nbarriers is not of much help if it leads to neglect of the later\nbarriers (in that case lifeboats). \n\nThe major problem in the construction of safety barriers is to make\nthem as independent of each other as possible. If two or more barriers\nare sensitive to the same type of impact, then one and the same\ndestructive force can get rid of all of them in one swoop. For\ninstance, if three safety valves are installed in one and the same\nfactory hall, each with the probability 1/1,000 of failure, it does\nnot follow that the probability of all three failing is\n\\(1 \\times 10^{-9}\\). The three valves may all be destroyed in the\nsame fire, or damaged by the same mistake in maintenance\noperations. This is a common situation of many types of\nequipment.  \n\nInherent safety, safety factors, and multiple barriers have an\nimportant common feature: They all aim at protecting us not only\nagainst risks that can be assigned meaningful probability estimates,\nbut also against dangers that cannot be probabilized, such as the\npossibility that some unanticipated type of event gives rise to an\naccident. It remains, however, for philosophers of technology to\ninvestigate the principles underlying safety engineering more in\ndetail and to clarify how they relate to other principles of\nengineering design (Doorn and Hansson 2015).  \nMany attempts have been made to predict the risks of emerging and\nfuture technologies. The role of philosophers in these endeavours has\noften been to point out the difficulties and uncertainties involved in\nsuch predictions (Allhoff 2009; Gordijn 2005). Experience shows that\neven after extensive efforts to make a new product safe, there is a\nneed for post market surveillance (PMS) in order to discover\nunexpected problems. For instance, before the massive introduction of\nautomobile air bags around 1990, safety engineers performed laboratory\ntests of different crash scenarios with dummies representing a variety\nof body weights and configurations (including infants and pregnant\nwomen). But in spite of the adjustments of the construction that these\ntests gave rise to, inflated airbags caused a considerable number of\n(mostly minor) injuries. By carefully analyzing experiences from\nactual accidents, engineers were able to substantially reduce the\nfrequency and severity of such injuries (Wetmore 2008). For\npharmaceutical drugs and some medical devices, post market\nsurveillance is legally required in many jurisdictions. \n\nUntil recently, problems of risk have not been treated systematically in moral\nphilosophy. A possible defence of this limitation is that moral\nphilosophy can leave it to decision theory to analyse the complexities\nthat indeterminism and lack of knowledge give rise to in real life.\nAccording to the conventional division of labour between the two\ndisciplines, moral philosophy provides assessments of human behaviour\nin well-determined situations. Decision theory takes assessments of\nthese cases for given, adds the available probabilistic information,\nand derives assessments for rational behaviour in an uncertain and\nindeterministic world. On this view, no additional input of moral\nvalues is needed to deal with indeterminism or lack of knowledge,\nsince decision theory operates exclusively with criteria of\nrationality.  \n\nExamples are easily found that exhibit the problematic nature of this\ndivision between the two disciplines. Compare the act of throwing down\na brick on a person from a high building to the act of throwing down a\nbrick from a high building without first making sure that there is\nnobody beneath who can be hit by the brick. The moral difference\nbetween these two acts is not obviously expressible in a probability\ncalculus. An ethical analysis of the difference will have to refer to\nthe moral aspects of risk-taking as compared to intentional ill-doing.\nMore generally speaking, a reasonably complete account of the ethics\nof risk must distinguish between intentional and unintentional risk\nexposure and between voluntary risk-taking, risks imposed on a person\nwho accepts them, and risks imposed on a person who does not accept\nthem. This cannot be done in a framework that treats risks as\nprobabilistic mixtures of outcomes. In principle, these outcomes can\nbe so widely defined that they include all relevant moral aspects,\nincluding rights infringements as well as intentionality and other\npertinent mental states. However, this would still not cover the moral\nimplications of risk taking per se, since these are not\ninherent properties of any of the potential outcomes.  \n\nMethods of moral analysis are needed that can guide decisions on\nrisk-takings and risk-impositions. A first step is to investigate how\nstandard moral theories can deal with problems of risk that are\npresented in the same way as in decision theory, namely as the (moral)\nevaluation of probabilistic mixtures of (deterministic) scenarios. \n\nIn utilitarian theory, there are two obvious approaches to such\nproblems. One is the actualist solution. It consists in\nassigning to a (probabilistic) mixture of potential outcomes a utility\nthat is equal to the utility of the outcome that actually\nmaterializes. To exemplify this approach, consider a decision whether\nor not to reinforce a bridge before it is used for a single, very\nheavy transport. There is a 50% risk that the bridge will fall down if\nit is not reinforced. Suppose that a decision is made not to reinforce\nthe bridge and that everything goes well; the bridge is not\ndamaged. According to the actualist approach, the decision was\nright. This is, of course, contrary to common moral intuitions. \n\nThe other established utilitarian approach is the maximization of\nexpected utility. This means that the utility of a mixture of\npotential outcomes is defined as the probability-weighted average of\nthe utilities of these outcomes. \n\nThe expected utility criterion has been criticized along several\nlines. One criticism is that it disallows a common form of\ncautiousness, namely disproportionate avoidance of large disasters.\nFor example, provided that human deaths are valued equally and\nadditively, as most utilitarians are prone to do, this framework does\nnot allow that one prefers a probability of 1 in 1000 that one person\nwill die to a probability of 1 in 100000 that fifty persons will\ndie. The expected utility framework can also be criticized for\ndisallowing a common expression of strivings for fairness, namely\ndisproportionate avoidance of high-probability risks for particular\nindividuals. Hence, in the choice between exposing one person to a\nprobability of 0.9 to be killed and exposing each of one hundred\npersons to a probability of 0.01 of being killed, it requires that the\nformer alternative be chosen. In summary, expected utility\nmaximization prohibits what seem to be morally reasonable standpoints\non risk taking and risk imposition. \n\nHowever, it should be noted that the expected utility criterion does\nnot necessarily follow from utilitarianism. Utilitarianism in a wide\nsense (Scanlon 1982) is compatible with other ways of evaluating\nuncertain outcomes (most notably with actual consequence\nutilitarianism, but in principle also for instance with a maximin\ncriterion). Therefore, criticism directed against expected utility\nmaximization does not necessarily show a defect in utilitarian\nthinking. \n\nThe problem of dealing with risk in rights-based moral\ntheories was formulated by Robert Nozick: “Imposing how\nslight a probability of a harm that violates someone’s rights also\nviolates his rights?” (Nozick 1974, 74). \n\nAn extension of a rights-based moral theory to indeterministic cases\ncan be obtained by prescribing that if A has a right that\nB does not bring about a certain outcome, then A\nalso has a right that B does not perform any action that (at\nall) increases the probability of that outcome.  Unfortunately, such a\nstrict extension of rights is untenable in social\npractice. Presumably, A has the right not to be killed by\nB, but it would not be reasonable to extend this right to all\nactions by B that give rise to a very small increase in the\nrisk that A dies — such as driving a car in the town\nwhere A lives. Such a strict interpretation would make human\nsociety impossible.  \n\nHence, a right not to be risk-exposed will have to be defeasible so\nthat it can be overridden in some (but not necessarily all) cases when\nthe increase in probability is small. However, it remains to find a\ncredible criterion for when it should be overridden.  As Nozick\nobserved, a probability limit is not credible in “a tradition\nwhich holds that stealing a penny or a pin or anything from someone\nviolates his rights. That tradition does\nnot select a threshold measure of harm as a lower limit, in\nthe case of harms certain to occur” (Nozick 1974, 75). \n\nThe problem of dealing with risks in deontological theories is similar\nto the corresponding problem in rights-based theories. The duty not to\nharm other people can be extended to a duty not to perform actions\nthat increase their risk of being harmed. However, society as we know\nit is not possible without exceptions to this rule.  The determination\nof criteria for such exceptions is problematic in the same way as for\nrights-based theories. All reasonable systems of moral obligations\nwill contain a fairly general prohibition against actions that kill\nanother person. Such a prohibition can (and should) be extended to\nactions that involve a large risk that a person is killed.  However,\nit cannot be extended to all actions that lead to a small increase in\nthe risk that a person is killed, since in that case it could not be\nallowed for instance to drive a car. A limit must be drawn between\nreasonable and unreasonable impositions of risk. It seems as if such\ndelimitations will have to appeal to concepts, such as probabilities\nand/or the size of the benefits obtained by taking a risk, that are\nnot part of the internal resources of deontological theories. \n\nContract theories may appear somewhat more promising than\nthe theories discussed above. The criterion that they offer for the\ndeterministic case, namely consent among all those involved, can also\nbe applied to risky options. It could be claimed that risk impositions\nare acceptable if and only if they are supported by a consensus. Such a\nconsensus, as conceived in contract theories, is either actual or\nhypothetical.  \n\nActual consensus is unrealistic in a complex society in which everyone\nperforms actions with marginal but additive effects on many people’s\nlives. According to the criterion of actual consensus, any local\ncitizen will have a veto against anyone else who wants to drive a car\nin the town where she lives. In this way citizens can block each\nother, creating a society of stalemates.  \n\nHypothetical consensus has been developed as a criterion in contract\ntheory in order to deal with inter-individual problems. We are invited\nto consider a hypothetical initial situation in which the social order\nof a future society has not yet been decided. When its future citizens\nmeet to choose a social order, each of them is ignorant of her or his\nposition in any of the social arrangements which they can choose\namong. According to John Rawls’s theory of justice, they will then all\nopt for a maximin solution, i.e. a social order in which the worst\nposition that anyone can have in that society is as good as\npossible. In arguing for that solution, Rawls relied heavily on the\nassumption that none of the participants knows anything at all about\nthe probability that she will end up in one or other of the positions\nin a future social order (Rawls 1957, 1971, 1974). John Harsanyi, who\ndiscussed this problem prior to Rawls, assumed that the probability of\nfinding oneself in a particular social position is equal to the share\nof the population that will have the position in question, and that\nthis is also known by all participants. Hence, if a fifth of the\npopulation in a certain type of society will be migrant workers, then\neach participant in Harsanyi’s initial situation will assume that she\nhas a twenty per cent probability of becoming a migrant worker,\nwhereas none of the participants in Rawls’s initial situation will\nhave a clue what that probability can be. In Harsanyi’s initial\nsituation, the participants will choose the social order with the\nhighest expected utility (probability-weighted utility), thus taking\nall potential future positions into account rather than only the least\nfavourable one (Harsanyi 1953, 1955, 1975). \nHowever, in discussions about various risks in our existing societies\nwe do not have much use for the hypothetical initial situations of\ncontract theory. The risks and uncertainties in real life are of quite\na different nature than the hypothetical uncertainty (or ignorance)\nabout one’s own social position and conditions which is a crucial\nrequirement in the initial situation. The thought experiment of an\ninitial situation does not seem to provide us with any intellectual\ntools for the moral appraisal of risks in addition to those to which\nwe have access even without trying to think away who we are. \n\nIn summary, the problem of appraising risks from a moral point of view\ndoes not seem to have any satisfactory solution in the common versions\nof the above-mentioned types of moral theories. The following are\nthree possible elements of a solution: Additional discussions on the overall issue of risk acceptance can\nbe found in Macpherson 2008, Hansson 2013, and Oberdick 2014. Justice\nin risk impositions is discussed in Ferretti 2010 and Heyward &\nRoser 2016. Issues of rights and risks are discused in Thomson 1986\nand, with a particular emphasis on responsibilities, in Kermisch 2012\nand van de Poel, et al. 2012. \n\nDecision theory is concerned with determining the best way to achieve\nas valuable an outcome as possible, given the values that we have. In\ndecision theory, our values and goals are taken for given, and the\nanalysis concerns how best to achieve them to an as high degree as\npossible. Decision-making under risk and uncertainty is one of the\nmajor topics in decision theory. It is usually assumed that if the\nvalues of a set of potential outcomes are known (for instance from\nmoral philosophy), then purely instrumental considerations are\nsufficient for determining how best to act under risk or uncertainty\nin order to achieve the best possible result. (For a critical\ndiscussion of that presumption, see Hansson 2013, pp. 49–51.) \n\nThe values taken for given in decision-theoretical analysis can, but\nneed not, be moral values of the types that are developed and analyzed\nin moral philosophy. Decision theory has traditionally had a\npredilection for utilitarianism, whose structure is suitable for most\nformal models of decision-making. The standard decision-theoretical\napproach to risk is maximization of expected utility, which can be\nseen as a smooth extension of (act) utilitarianism. \n\nHowever, expected utility maximization does not rule\nunchallenged. Influential proposals have been made for alternative\ndecision rules. There are two major types of justification for such\nendeavours. First, examples have been put forward in which it would\nseem implausible to claim that expected utility maximization is the\nonly normatively reasonable decision rule (Allais 1953, Ellsberg\n1961). Secondly, numerous psychological experiments have shown that\nhuman decision-makers tend to deviate substantially from expected\nutility maximization. The first type of justification puts the\nnormative soundness of expected utility in question, where the second\nexposes its shortcomings as a descriptive model.  \n\nIn an important class of alternative decision rules, the probabilities\nused on expected utility calculations are replaced by some other\nnumbers (“decision weights”). This approach was proposed\nalready already in 1961 by William Fellner (1961). In most of these\nconstructions, all probabilities are tranformed by some transformation\nfunction r. Instead maximizing the standard expected utility\n\\[p(x) \\cdot u(x)\\] the agent will then maximize \\[r(p(x)) \\cdot\nu(x)\\] \nSeveral decision rules with this structure have been proposed. One of\nthe earliest was Handa (1977). Currently, the best know proposal in\nthis tradition is prospect theory (Kahneman and Tversky 1979), which\nwas developed in order to describe observations from psychological\ndecision experiments more accurately than in expected utility\ntheory. Prospect theory a is a fairly complex theory also containing\nother deviations from expected utility theory. The traditional focus\non outcomes is replaced by a focus on losses and gains, which are\ntreated asymmetrically.  \n\nA problem with the function r, as defined above, is that the\ntransformed probabilities which it gives rise to will not add up to 1\nexcept in the trivial case when r is the identity function\n(Fishburn 1978). To solve this problem, Quiggin (1982) introduced the\nrule of maximizing anticipated utility (also called utility\nwith rank dependent probabilities). Instead of replacing \\(p(x)\\) by a\nfunction of the individual probability, \\(r(p(x))\\), he replaced it by\na function that also depends on the other probabilities and utilities\ninvolved in the problem. The outcomes are first ordered from worst to\nbest, which results in a vector \\(\\langle x_1, x_2 ,\\ldots ,\nx_n\\rangle\\) of outcomes, such\nthat \\(u(x_1) \\leq u(x_2) \\leq \\ldots \\leq u(x_n)\\). A\ndecision weight can then be assigned to each outcome, taking into\naccount both its probability and its position in the ranked sequence\nof outcomes. Since the decision-weight can be different for outcomes\nwith the same probability, Fishburn’s trivialization result does\nnot apply here. There is evidence indicating that rank-dependent\nutility models may be more empirically adequate than prospect theory\n(Harrison and Ross 2017). \n\nSeveral other a models have been proposed that replace the\nprobabilities in expected utility maximization by some other type of\ndecision weight (Gilboa and Schmeidler 1994, Buchak 2014). \n\nRisks have a central role in economic activities. In capitalist market\neconomies, taking economic risks is an essential part of the role of\nthe entrepreneur. Decisions on investments and activities on financial\nmarkets can only be understood against the background of the risks\ninvolved. Therefore it is no surprise that modern economic theory,\nwith its emphasis on mathematical models of economic activities, has\ndeveloped several formal models of risk taking. \n\nPortfolio analysis, which was developed in the 1950s by Harry\nMarkowitz (1952), James Tobin (1958) and others, was an important step\nforward in the economic analysis of risk. These authors employed a\nsimple statistical measure, namely the standard deviation (or\nalternatively the variance, that is the square of the standard\ndeviation) as a measure of riskiness. Hence, in a comparison between\ntwo investment alternatives, the one whose economic outcome is\ncalculated to have the largest standard deviation is regarded as the\nmost risky one. In a comparison between different such alternatives,\neach of them can be characterized by two numbers, namely its\nexpectation value and its standard deviation or riskiness. Investors\ntypically prefer investments with as high expectation values and as\nlow riskiness as possible. However, investors differ in the relative\nweight that they assign to expectations respectively risk\navoidance. Given these decision weights, an individual’s optimal\nportfolio can be determined. \n\nSince the late 1960s, alternative measures of risk have been\ndeveloped. Perhaps the most influential of these was provided by\nMichael Rothschild and Joseph Stiglitz (1970): If we move probability mass\nfrom the centre to the tails of a probability distribution, while\nkeeping its mean unchanged, then we increase the risk associated with\nthe distribution. A measure based on this principle (mean preserving\nspread) can be constructed that has more attractive mathematical\nproperties than those of the older standard deviation measure. \nWe differ in our attitudes to risk. Some of us willingly take risks\nwhich others deem much too large. The notion of certainty-equivalent\noutcomes can be used to specify such differences. Consider a risky\noutcome X. Another outcome Y is a\ncertainty-equivalent outcome for X if and only if\n(1) Y involves no uncertainty and (2) X\nand Y are considered by the agent to be equally good. For\ninstance, let X be a lottery ticket with a 50 per cent chance\nof winning something to which you assign 10 utiles (utility units) and\na 50 per cent chance of winning nothing. The expected utility of this\nticket is 5 utiles. Now suppose that you are indifferent between\nreceiving 3 utiles for sure and receiving X. Then your\nattitude is risk averse. The general criterion for risk aversion (risk\navoidance, cautiousness) is that \\(CE(X) < EU(X)\\). Similarly, you are\nrisk neutral concerning X if \\(CE(X) = EU(X)\\) and you are\nrisk affine (risk seeking, risk loving) if \\(EU(X) < CE(X)\\). \n\nIn economics, risk aversion is usually related to\nmoney. Let X represent a lottery ticket with a 50 per cent\nchance of winning € 100, and suppose that you consider this\nticket to be worth € 30. We then have \\(EU(X) = u(100)/2\\), and\n\\(CE(X) = u(30)\\). This means that \\(u(30) = u(100)/2\\). If this is a\nconsistent pattern, then the utility of money, as illustrated in a\ndiagram with amounts x of money on the x axis and their\nutilities \\(u(x)\\) on the y axis, will be represented by a concave\n(hill-like) curve. Similarly, if a risk affine behaviour will be\nrepresented by a convex (valley-like curve). Provided that the utility\nfunction u is twice continuously differentiable, this can be\nexpressed more precisely in the form of the Arrow-Pratt measure of\nrisk aversion/affinity, according to which the agent’s risk\naversion at any point x is equal to \\(-u''(x)/u'(x)\\).\nHence, a person with the utility function \\(u_1\\) is more risk averse\nat a point x than one with utility function \\(u_2\\) if and\nonly if \\[-{u_1}''(x)/{u_1}'(x) > -{u_2}''(x)/{u_2}'(x)\\] (Arrow 1965,\nPratt 1964). The Arrow-Pratt measure has the advantage of being\ninvariant under transformations of the utility function that preserve\nthe preference relation that it represents (i.e. it is invariant under\nmultiplication of the utility with a positive constant and addition of\nan arbitrary constant). On the other hand, it can be questioned on\nphilosophical grounds whether risk attitudes can be adequately\nrepresented by variations in the utility of money. It can be argued\nthat cautiousness and the utility of money are two separate issues and\nthat they should therefore have independent representations. \n\nStudies in experimental economics reveal that actual agents often do\nnot conform with theoretically derived rationality criteria. One of\nthe most popular descriptive theories that tries to capture actual\nbehaviour under risk is prospect theory, which was developed by Daniel\nKahneman and Amos Tversky around 1980 (Tversky & Kahneman\n1986). It distinguishes between two stages in a decision process. In\nthe first phase, the editing phase, gains and losses in the\ndifferent options are identified. They are defined relative to some\nneutral reference point that is usually the current asset position. In\nthe second phase, the evaluation phase, the options are\nevaluated in a way that resembles expected utility analysis, but both\nutilities and probabilities are replaced by other, similar\nmeasures. Utility is replaced by a measure that is asymmetrical\nbetween gains and losses.  Objective probabilities are transformed by\na function that gives more weight to probability differences close to\nthe ends than to those near the centre of the distribution. Thus it\nmakes a greater difference to decrease the probability of a negative\noutcome from 2 to 1 per cent than to decrease it from 51 to 50\npercent.  \n\nProspect theory can explain some of the ways in which actual behaviour\ndeviates from theoretical models of rational behaviour under\nrisk. Hence the overweighting of probability changes close to zero or\nunity can be used to explain why people both buy insurance and buy\nlottery tickets. However, prospect theory is not plausible as a\nnormative theory for rational behaviour under risk. Probably,\nnormative and descriptive theories of risk will have to go in\ndifferent directions. \nRisk-benefit analysis (RBA), also called cost-benefit analysis (CBA)\nis a collection of decision-aiding techniques that weigh advantages\nagainst disadvantages in numerical terms. In a typical risk-benefit\nanalysis, multi-dimensional problems are reduced to a single\ndimension. This is achieved by assigning monetary values to all\npotential outcomes. Usually, uncertain outcomes are evaluated\naccording to the expected utility model. This means that the disvalue\nof a risk is obtained by multiplying the probability of the undesired\nevent by a monetary value representing its severity (Sen 2000;\nSunstein 2005). \nMost of the philosophical discussion on risk-benefit analysis has been\nfocused on the assignment of a monetary value to the loss of a human\nlife (MacLean 1994, Heinzerling 2000, 2002). It has been claimed that\nlives and money are incommensurable, and that the determination and\nuse of such “live values” express a lack of respect for\nhuman lives. Defenders of risk-benefit analysis have countered that\nthese values are just technical constructs representing what society\ntends to pay (alternative: ought to pay) in order to save a human\nlife. Risk-benefit analysis can help decision-makers to save as many\nlives as possible, given that they have a certain amount of resources\nthat they can assign to life-saving policies (Sunstein 2005). \nMany of the value assignments used in cost-benefit analysis are based\non estimates or measurements of (hypothetical) willingness to\npay. Such estimates will give more influence to affluent people since\nthey can pay more than others to have it their way. This can be\ncorrected with income-based adjustments of the reported willingness to\npay. However, there are considerable problems involved in the\nperformance and interpretation of willingness-to-pay\nstudies (Grüne-Yanoff 2009). \nRisk-benefit analysis gives rise to several other philosophical\nproblems of considerable philosophical interest (Hansson 2007). Due to\nits quantitative nature, it tends to leave out problems that are\ndifficult to quantify, such as risks of cultural impoverishment,\nsocial isolation, and increased tensions between social\nstrata. Furthermore, due to its aggregative structure, risk-benefit\nanalyses often leave out social justice and other distributional\naspects, although these are in fact accessible to quantitative\ntreatment.","contact.mail":"soh@kth.se","contact.domain":"kth.se"}]
