[{"date.published":"2009-02-20","date.changed":"2018-09-06","url":"https://plato.stanford.edu/entries/technology/","author1":"Maarten Franssen","author1.info":"http://www.tbm.tudelft.nl/en/about-faculty/departments/values-technology-and-innovation/sections/ethicsphilosophy-of-technology/staff/dr-mpm-maarten-franssen/","author2.info":"http://gjclokhorst.nl/","entry":"technology","body.text":"\n\n\nIf philosophy is the attempt “to understand how things in the\nbroadest possible sense of the term hang together in the broadest\npossible sense of the term”, as Sellars (1962) put it,\nphilosophy should not ignore technology. It is largely by technology\nthat contemporary society hangs together. It is hugely important not\nonly as an economic force but also as a cultural force. Indeed during\nthe last two centuries, when it gradually emerged as a discipline,\nphilosophy of technology has mostly been concerned with the meaning of\ntechnology for, and its impact on, society and culture, rather than\nwith technology itself. Mitcham (1994) calls this type of philosophy\nof technology “humanities philosophy of technology”\nbecause it accepts “the primacy of the humanities over\ntechnologies” and is continuous with the overall perspective of\nthe humanities (and some of the social sciences). Only recently a\nbranch of the philosophy of technology has developed that is concerned\nwith technology itself and that aims to understand both the practice\nof designing and creating artifacts (in a wide sense, including\nartificial processes and systems) and the nature of the things so\ncreated. This latter branch of the philosophy of technology seeks\ncontinuity with the philosophy of science and with several other\nfields in the analytic tradition in modern philosophy, such as the\nphilosophy of action and decision-making, rather than with the\nhumanities and social science.\n\n\nThe entry starts with a brief historical overview, then continues with\na presentation of the themes on which modern analytic philosophy of\ntechnology focuses. This is followed by a discussion of the societal\nand ethical aspects of technology, in which some of the concerns of\nhumanities philosophy of technology are addressed. This twofold\npresentation takes into consideration the development of technology as\nthe outcome of a process originating within and guided by the practice\nof engineering, by standards on which only limited societal control is\nexercised, as well as the consequences for society of the\nimplementation of the technology so created, which result from\nprocesses upon which only limited control can be exercised.\n\n\n\n\n\n\n\nPhilosophical reflection on technology is about as old as philosophy\nitself. Our oldest testimony is from ancient Greece. There are four\nprominent themes. One early theme is the thesis that technology learns\nfrom or imitates nature (Plato, Laws X 899a ff.). According\nto Democritus, for example, house-building and weaving were first\ninvented by imitating swallows and spiders building their nests and\nnets, respectively (Diels 1903 and Freeman 1948: 154).  Perhaps the\noldest extant source for the exemplary role of nature is Heraclitus\n(Diels 1903 and Freeman 1948: 112).  Aristotle referred to this\ntradition by repeating Democritus’ examples, but he did not\nmaintain that technology can only imitate nature: “generally\ntechnè in some cases completes what nature cannot\nbring to a finish, and in others imitates nature”\n(Physics II.8, 199a15; see also Physics II.2, and\nsee Schummer 2001 and this encyclopedia’s entry on\n episteme and techne\n for discussion). \nA second theme is the thesis that there is a fundamental ontological\ndistinction between natural things and artifacts. According to\nAristotle (Physics II.1), the former have their principles of\ngeneration and motion inside, whereas the latter, insofar as they are\nartifacts, are generated only by outward causes, namely human aims and\nforms in the human soul. Natural products (animals and their parts,\nplants, and the four elements) move, grow, change, and reproduce\nthemselves by inner final causes; they are driven by purposes of\nnature. Artifacts, on the other hand, cannot reproduce themselves.\nWithout human care and intervention, they vanish after some time by\nlosing their artificial forms and decomposing into (natural)\nmaterials. For instance, if a wooden bed is buried, it decomposes to\nearth or changes back into its botanical nature by putting forth a\nshoot. \nThe thesis that there is a fundamental difference between man-made\nproducts and natural substances has had a long-lasting influence. In\nthe Middle Ages, Avicenna criticized alchemy on the ground that it can\nnever produce ‘genuine’ substances\n(Briffault 1930: 147). Even today, some still\nmaintain that there is a difference between, for example, natural and\nsynthetic vitamin C. The modern discussion of this theme is taken up\nin\n Section 2.5. \nAristotle’s doctrine of the four causes—material, formal,\nefficient and final—can be regarded as a third early\ncontribution to the philosophy of technology. Aristotle explained this\ndoctrine by referring to technical artifacts such as houses and\nstatues (Physics II.3). The four causes are still very much\npresent in modern discussions related to the metaphysics of artifacts.\nDiscussions of the notion of function, for example, focus on its\ninherent teleological or ‘final’ character and the\ndifficulties this presents to its use in biology. And the notorious\ncase of the ship of Theseus—see this encyclopedia’s\nentries on\n material constitution,\n identity over time,\n relative identity,\n and\n sortals—was\n introduced in modern philosophy by Hobbes as showing a conflict\nbetween unity of matter and unity of form as principles of\nindividuation. This conflict is seen by many as characteristic of\nartifacts. David Wiggins (1980: 89) takes it even to be the defining\ncharacteristic of artifacts. \nA fourth point that deserves mentioning is the extensive employment of\ntechnological images by Plato and Aristotle. In his Timaeus,\nPlato described the world as the work of an Artisan, the Demiurge. His\naccount of the details of creation is full of images drawn from\ncarpentry, weaving, ceramics, metallurgy, and agricultural technology.\nAristotle used comparisons drawn from the arts and crafts to\nillustrate how final causes are at work in natural processes. Despite\ntheir negative appreciation of the life led by artisans, who they\nconsidered too much occupied by the concerns of their profession and\nthe need to earn a living to qualify as free individuals, both Plato\nand Aristotle found technological imagery indispensable for expressing\ntheir belief in the rational design of the universe (Lloyd 1973:\n61). \nAlthough there was much technological progress in the Roman empire and\nduring the Middle Ages, philosophical reflection on technology did not\ngrow at a corresponding rate. Comprehensive works such as\nVitruvius’ De architectura (first century BC) and\nAgricola’s De re metallica (1556) paid much attention\nto practical aspects of technology but little to philosophy. \nIn the realm of scholastic philosophy, there was an emergent\nappreciation for the mechanical arts. They were generally considered\nto be born of—and limited to—the mimicry of nature. This\nview was challenged when alchemy was introduced in the Latin West\naround the mid-twelfth century. Some alchemical writers such as Roger\nBacon were willing to argue that human art, even if learned by\nimitating natural processes, could successfully reproduce natural\nproducts or even surpass them (Newman 2004). The result was a philosophy of technology in which\nhuman art was raised to a level of appreciation not found in other\nwritings until the Renaissance. However, the last three decades of the\nthirteenth century witnessed an increasingly hostile attitude by\nreligious authorities toward alchemy that culminated eventually in the\ndenunciation Contra alchymistas, written by the inquisitor\nNicholas Eymeric in 1396 (Newman 2004). \nThe Renaissance led to a greater appreciation of human beings and\ntheir creative efforts, including technology. As a result,\nphilosophical reflection on technology and its impact on society\nincreased. Francis Bacon is generally regarded as the first modern\nauthor to put forward such reflection. His view, expressed in his\nfantasy New Atlantis (1627), was overwhelmingly positive.\nThis positive attitude lasted well into the nineteenth century,\nincorporating the first half-century of the industrial revolution. \nFor example, Karl Marx did not condemn the steam engine or the\nspinning mill for the vices of the bourgeois mode of production; he\nbelieved that ongoing technological innovation were necessary steps\ntoward the more blissful stages of socialism and communism of the\nfuture (see Bimber 1990 for a discussion of different views on the\nrole of technology in Marx’s theory of historical development,\nand see Van der Pot 1985 [1994/2004] for an extensive historical\noverview of appreciations of the development of technology). \nA turning point in the appreciation of technology as a socio-cultural\nphenomenon is marked by Samuel Butler’s Erewhon (1872),\nwritten under the influence of the Industrial Revolution, and\nDarwin’s On the Origin of Species (1859). Butler’s book gave an\naccount of a fictional country where all machines are banned and the\npossession of a machine or the attempt to build one is a capital\ncrime. The people of this country had become convinced by an argument\nthat ongoing technical improvements are likely to lead to a\n‘race’ of machines that will replace mankind as the\ndominant species on earth. \nDuring the last quarter of the nineteenth century and most of the\ntwentieth century a critical attitude predominated in philosophical\nreflection on technology. The representatives of this attitude were,\noverwhelmingly, schooled in the humanities or the social sciences and\nhad virtually no first-hand knowledge of engineering practice. Whereas\nBacon wrote extensively on the method of science and conducted\nphysical experiments himself, Butler, being a clergyman, lacked such\nfirst-hand knowledge. Ernst Kapp, who was the first to use the term\n‘philosophy of technology’ in his book Eine\nPhilosophie der Technik (1877 [2018]), was a philologist and\nhistorian.  Most of the authors who wrote critically about technology\nand its socio-cultural role during the twentieth century were\nphilosophers of a general outlook, such as Martin Heidegger (1954\n[1977]), Hans Jonas (1979 [1984]), Arnold Gehlen (1957 [1980]),\nGünther Anders (1956), and Andrew Feenberg (1999). Others had a\nbackground in one of the other humanities or in social science, such\nas literary criticism and social research in the case of Lewis Mumford\n(1934), law in the case of Jacques Ellul (1954 [1964]), political\nscience in the case of Langdon Winner (1977, 1980, 1983) and literary\nstudies in the case of Albert Borgmann (1984). The form of philosophy\nof technology constituted by the writings of these and others has been\ncalled by Carl Mitcham (1994) “humanities philosophy of\ntechnology”, because it takes its point of departure from the\nsocial sciences and the humanities rather than from the practice of\ntechnology, and it approaches technology accepting “the primacy\nof the humanities over technologies” (1994: 39), since\ntechnology originates from the goals and values of humans.\n \nHumanities philosophers of technology tend to take the phenomenon of\ntechnology itself largely for granted; they treat it as a ‘black\nbox’, a given, a unitary, monolithic, inescapable phenomenon.\nTheir interest is not so much to analyze and understand this\nphenomenon itself but to grasp its relations to morality (Jonas,\nGehlen), politics (Winner), the structure of society (Mumford), human\nculture (Ellul), the human condition (Hannah Arendt), or metaphysics\n(Heidegger). In this, these philosophers are almost all openly\ncritical of technology: all things considered, they tend to have a\nnegative judgment of the way technology has affected human society and\nculture, or at least they single out for consideration the negative\neffects of technology on human society and culture. This does not\nnecessarily mean that technology itself is pointed out as the\nprincipal cause of these negative developments. In the case of\nHeidegger, in particular, the paramount position of technology in\nmodern society is rather a symptom of something more fundamental,\nnamely a wrongheaded attitude towards Being which has been on the rise\nfor almost 25 centuries. It is therefore questionable whether\nHeidegger should be considered as a philosopher of technology,\nalthough within the traditional view he is considered to be among the\nmost important ones. Much the same could be said about Arendt, in\nparticular her discussion of technology in The Human\nCondition (1958), although her position in the canon of\nhumanities philosophy of technology is not as prominent. \nTo be sure, the work of these founding figures of humanities\nphilosophy of technology has been taken further by a second and third\ngeneration of scholars—in particular the work of Heidegger\nremains an important source of inspiration—but who in doing so\nhave adopted a more neutral rather than overall negative view of\ntechnology and its meaning for human life and culture. Notable\nexamples are Ihde (1979, 1993) and Verbeek (2000 [2005]). \nIn its development, humanities philosophy of technology continues to\nbe influenced not so much by developments in philosophy (e.g.,\nphilosophy of science, philosophy of action, philosophy of mind) but\nby developments in the social sciences and humanities. Although, for\nexample, Ihde and those who take their point of departure with him,\nposition their work as phenomenologist or postphenomenologist, there\ndoes not seem to be much interest in either the past or the present of\nthis diffuse notion in philosophy, and in particular not much interest\nin the far from easy question to what extent Heidegger can be\nconsidered a phenomenologist. Of particular significance has been the\nemergence of ‘Science and Technology Studies’ (STS) in the\n1980s, which studies from a broad social-scientific perspective how\nsocial, political, and cultural values affect scientific research and\ntechnological innovation, and how these in turn affect society,\npolitics, and culture. We discuss authors from humanities philosophy\nof technology in\n Section 3\non ‘Ethical and Social Aspects of Technology’, but do not\npresent separately and in detail the wide variety of views existing in\nthis field. For a detailed treatment Mitcham’s 1994 book\nprovides an excellent overview. Olsen, Selinger and Riis (2008) offer\na collection of more recent contributions; Scharff and Dusek (2003\n[2014]) and Kaplan (2004 [2009]) present comprehensive anthologies of\ntexts from this tradition. \nMitcham contrasts ‘humanities philosophy of technology’ to\n‘engineering philosophy of technology’, where the latter\nrefers to philosophical views developed by engineers or technologists\nas “attempts … to elaborate a technological\nphilosophy” (1994: 17). Mitcham discusses only a handful of\npeople as engineering philosophers of technology, however: Ernst Kapp,\nPeter Engelmeier, Friedrich Dessauer, and much more briefly Jacques\nLafitte, Gilbert Simondon, Hendrik van Riessen, Juan David\nGarcía Bacca, R. Buckminster Fuller and Mario Bunge. The label\nraises serious questions, however: several of them hardly classify as\n‘engineers or technologists’ and it is also not very clear\nhow the notion of ‘a technological philosophy’ should be\nunderstood. As philosophers these authors seem all to be rather\nisolated figures, whose work shows little overlap and who seem to be\nsharing mainly the absence of a ‘working relation’ with\nestablished philosophical disciplines. It is not so clear what sort of\nquestions and concerns underlie the notion of ‘engineering\nphilosophy of technology’. A larger role for systematic\nphilosophy could bring it quite close to some examples of humanities\nphilosophy of technology, for instance the work of Jacques Ellul,\nwhere the analyses would be rather similar and the remaining\ndifferences would be ones of attitude or appreciation. \nIn the next section we discuss in more detail a form of philosophy of\ntechnology that we consider to occupy, currently, the position of\nalternative to the humanities philosophy of technology. It emerged in\nthe 1960s and gained momentum in the past fifteen to twenty years.\nThis form of the philosophy of technology, which may be called\n‘analytic’, is not primarily concerned with the relations\nbetween technology and society but with technology itself. It\nexpressly does not look upon technology as a ‘black box’\nbut as a phenomenon that should be studied in detail. It regards\ntechnology perhaps not in its entirety as a practice but as something\ngrounded in a practice, basically the practice of engineering. It\nanalyses this practice, its goals, its concepts and its methods, and\nit relates its findings to various themes from philosophy. \nIn focusing on technology as a practice sustained by engineers,\nsimilar to the way philosophy of science focuses on the practice of\nscience as sustained by scientists, analytic philosophy of technology\ncould be thought to amount to the philosophy of engineering. Indeed\nmany of the issues related to design, discussed below in Sections\n 2.3\n and\n 2.4,\n could be singled out as forming the subject matter of the philosophy\nof engineering. The metaphysical issues discussed in Section\n 2.5\n could not, however, and analytic philosophy of technology is\ntherefore significantly broader than philosophy of engineering. The\nvery title of Philosophy of Technology and Engineering\nSciences (Meijers 2009), an extensive up-to-date overview, which\ncontains contributions to all of the topics treated in the next\nsection, expresses the view that technology and engineering do not\ncoincide. Which is not to say, however, that the book offers a clear\nconception of what makes technology different from engineering, or\nmore than engineering. In fact, the existence of humanities philosophy\nof technology and analytic philosophy of technology next to each other\nreflects a basic ambiguity in the notion of technology that the\nphilosophical work that has been going on has not succeeded in\nclarifying. \nTechnology can be said to have two ‘cores’ or\n‘dimensions’, which can be referred to as\ninstrumentality and productivity. Instrumentality\ncovers the totality of human endeavours to control their lives and\ntheir environments by interfering with the world in an instrumental\nway, by using things in a purposeful and clever way. Productivity\ncovers the totality of human endeavours to brings new things into\nexistence that can do certain things in a controlled and clever way.\nFor the study of instrumentality, however, it is in principle\nirrelevant whether or not the things that are made use of in\ncontrolling our lives and environments have been made by us first; if\nwe somehow could rely on natural objects to always be available to\nserve our purposes, the analysis of instrumentality and its\nconsequences for how we live our lives would not necessarily be\naffected. Likewise, for the analysis of what is involved in the making\nof artifacts, and how the notion of artifact and of something new\nbeing brought into existence are to be understood, it is to a large\nextent irrelevant how human life, culture and society are changed as a\nresult of the artifacts that are in fact produced. Clearly, humanities\nphilosophy of technology has until now been more attracted by the\ninstrumentality core whereas analytic philosophy of technology has\nmainly gone for the productivity core. But technology as one of the\nbasic phenomena of modern society, if not the most basic one, clearly\nis constituted by the processes centering on and involving both cores.\nIt has proved difficult, however, to come to an overarching approach\nin which the interaction between these two dimensions of technology\nare adequately dealt with—no doubt partly due to the great\ndifferences in philosophical orientation and methodology associated\nwith the two traditions and their separate foci. To improve this\nsituation is arguably the most urgent challenge that the field of\nphilosophy of technology as a whole is facing, since the continuation\nof the two orientations leading their separate lives threatens its\nunity and coherence as a discipline in the first place.\nNotwithstanding its centrality and urgency, the ambiguity noted here\nseems hardly to be confronted directly in the literature. It is\naddressed by Lawson (2008, 2017) and by Franssen and Koller\n(2016). \nAfter presenting the major issues of philosophical relevance in\ntechnology and engineering that are studied by analytic philosophers\nof technology in the next section, we discuss the problems and\nchallenges that technology poses for the society in which it is\npracticed in the third and final section. \nIt may come as a surprise to those new to the topic that the fields of\nphilosophy of science and philosophy of technology show such great\ndifferences, given that few practices in our society are as closely\nrelated as science and technology. Experimental science is nowadays\ncrucially dependent on technology for the realization of its research\nset-ups and for gathering and analyzing data. The phenomena that\nmodern science seeks to study could never be discovered without\nproducing them through technology. \nTheoretical research within technology has come to be often\nindistinguishable from theoretical research in science, making\nengineering science largely continuous with ‘ordinary’ or\n‘pure’ science. This is a relatively recent development,\nwhich started around the middle of the nineteenth century, and is\nresponsible for great differences between modern technology and\ntraditional, craft-like techniques. The educational training that\naspiring scientists and engineers receive starts off being largely\nidentical and only gradually diverges into a science or an engineering\ncurriculum. Ever since the scientific revolution of the seventeenth\ncentury, characterized by its two major innovations, the experimental\nmethod and the mathematical articulation of scientific theories,\nphilosophical reflection on science has focused on the method by which\nscientific knowledge is generated, on the reasons for thinking\nscientific theories to be true, or approximately true, and on the\nnature of evidence and the reasons for accepting one theory and\nrejecting another. Hardly ever have philosophers of science posed\nquestions that did not have the community of scientists, their\nconcerns, their aims, their intuitions, their arguments and choices,\nas a major target. In contrast it is only recently that the philosophy\nof technology has discovered the community of engineers. \nIt might be claimed that it is up to the philosophy of technology, and\nnot the philosophy of science, to target first of all the impact of\ntechnology—and with it science—on society and culture,\nbecause science affects society only through technology. This,\nhowever, will not do. Right from the start of the scientific\nrevolution, science affected human culture and thought fundamentally\nand directly, not with a detour through technology, and the same is\ntrue for later developments such as relativity, atomic physics and\nquantum mechanics, the theory of evolution, genetics, biochemistry,\nand the increasingly dominating scientific world view overall.\nPhilosophers of science overwhelmingly give the impression that they\nleave questions addressing the normative, social and cultural aspects\nof science gladly to other philosophical disciplines, or to historical\nstudies. There are exceptions, however, and things may be changing;\nPhilip Kitcher, to name but one prominent philosopher of science, has\nsince 2000 written books on the relation of science to politics,\nethics and religion (Kitcher 2001, 2011). \nThere is a major difference between the historical development of\nmodern technology as compared to modern science which may at least\npartly explain this situation, which is that science emerged in the\nseventeenth century from philosophy itself. The answers that Galileo,\nHuygens, Newton, and others gave, by which they initiated the alliance\nof empiricism and mathematical description that is so characteristic\nof modern science, were answers to questions that had belonged to the\ncore business of philosophy since antiquity. Science, therefore, kept\nthe attention of philosophers. Philosophy of science is a\ntransformation of epistemology in the light of the emergence of\nscience. The foundational issues—the reality of atoms, the\nstatus of causality and probability, questions of space and time, the\nnature of the quantum world—that were so lively discussed during\nthe end of the nineteenth and the beginning of the twentieth century\nare an illustration of this close relationship between scientists and\nphilosophers. No such intimacy has ever existed between those same\nphilosophers and technologists; their worlds still barely touch. To be\nsure, a case can be made that, compared to the continuity existing\nbetween natural philosophy and science, a similar continuity exists\nbetween central questions in philosophy having to do with human action\nand practical rationality and the way technology approaches and\nsystematizes the solution of practical problems. To investigate this\nconnection may indeed be considered a major theme for philosophy of\ntechnology, and more is said on it in Sections\n 2.3\n and\n 2.4.\n This continuity appears only by hindsight, however, and dimly, as the\nhistorical development is at most a slow convening of various strands\nof philosophical thinking on action and rationality, not a development\ninto variety from a single origin. Significantly it is only the\nacademic outsider Ellul who has, in his idiosyncratic way, recognized\nin technology the emergent single dominant way of answering all\nquestions concerning human action, comparable to science as the single\ndominant way of answering all questions concerning human knowledge\n(Ellul 1954 [1964]). But Ellul was not so much interested in\ninvestigating this relationship as in emphasizing and denouncing the\nsocial and cultural consequences as he saw them. It is all the more\nimportant to point out that humanities philosophy of technology cannot\nbe differentiated from analytic philosophy of technology by claiming\nthat only the former is interested in the social environment of\ntechnology. There are studies which are rooted in analytic philosophy\nof science but address specifically the relation of technology to\nsociety and culture, and equally the relevance of social relations to\npractices of technology, without taking an evaluative stand with\nrespect to technology; an example is B. Preston 2012. \nThe close relationship between the practices of science and technology\nmay easily keep the important differences between the two from view.\nThe predominant position of science in the philosophical field of\nvision made it difficult for philosophers to recognize that technology\nmerits special attention for involving issues that do not emerge in\nscience. This view resulting from this lack of recognition is often\npresented, perhaps somewhat dramatically, as coming down to a claim\nthat technology is ‘merely’ applied science. \nA questioning of the relation between science and technology was the\ncentral issue in one of the earliest discussions among analytic\nphilosophers of technology. In 1966, in a special issue of the journal\nTechnology and Culture, Henryk Skolimowski argued that\ntechnology is something quite different from science (Skolimowski\n1966). As he phrased it, science concerns itself with what is, whereas\ntechnology concerns itself with what is to be. A few years later, in\nhis well-known book The Sciences of the Artificial (1969),\nHerbert Simon emphasized this important distinction in almost the same\nwords, stating that the scientist is concerned with how things are but\nthe engineer with how things ought to be. Although it is difficult to\nimagine that earlier philosophers were blind to this difference in\norientation, their inclination, in particular in the tradition of\nlogical empiricism, to view knowledge as a system of statements may\nhave led to a conviction that in technology no knowledge claims play a\nrole that cannot also be found in science. The study of technology,\ntherefore, was not expected to pose new challenges nor hold surprises\nregarding the interests of analytic philosophy. \nIn contrast, Mario Bunge (1966) defended the view that technology\nis applied science, but in a subtle way that does justice to\nthe differences between science and technology. Bunge acknowledges\nthat technology is about action, but an action heavily underpinned by\ntheory—that is what distinguishes technology from the arts and\ncrafts and puts it on a par with science. According to Bunge, theories\nin technology come in two types: substantive theories, which provide\nknowledge about the object of action, and operative theories, which\nare concerned with action itself. The substantive theories of\ntechnology are indeed largely applications of scientific theories. The\noperative theories, in contrast, are not preceded by scientific\ntheories but are born in applied research itself. Still, as Bunge\nclaims, operative theories show a dependence on science in that in\nsuch theories the method of science is employed. This\nincludes such features as modeling and idealization, the use of\ntheoretical concepts and abstractions, and the modification of\ntheories by the absorption of empirical data through prediction and\nretrodiction. \nIn response to this discussion, Ian Jarvie (1966) proposed as\nimportant questions for a philosophy of technology what the\nepistemological status of technological statements is and how\ntechnological statements are to be demarcated from scientific\nstatements. This suggests a thorough investigation of the various\nforms of knowledge occurring in either practice, in particular, since\nscientific knowledge has already been so extensively studied, of the\nforms of knowledge that are characteristic of technology and are\nlacking, or of much less prominence, in science. A distinction between\n‘knowing that’—traditional propositional\nknowledge—and ‘knowing how’—non-articulated\nand even impossible-to-articulate knowledge—had been introduced\nby Gilbert Ryle (1949) in a different context. The notion of\n‘knowing how’ was taken up by Michael Polanyi under the\nname of tacit knowledge and made a central characteristic of\ntechnology (Polanyi 1958); the current state of the philosophical\ndiscussion is presented in this encyclopedia’s entry on\n knowledge how.\n However, emphasizing too much the role of unarticulated knowledge, of\n‘rules of thumb’ as they are often called, easily\nunderplays the importance of rational methods in technology. An\nemphasis on tacit knowledge may also be ill-fit for distinguishing the\npractices of science and technology because the role of tacit\nknowledge in science may well be more important than current\nphilosophy of science acknowledges, for example in concluding causal\nrelationships on the basis of empirical evidence. This was also an\nimportant theme in the writings of Thomas Kuhn on theory change in\nscience (Kuhn 1962). \nTo claim, with Skolimowski and Simon, that technology is about what is\nto be or what ought to be rather than what is may serve to distinguish\nit from science but will hardly make it understandable why so much\nphilosophical reflection on technology has taken the form of\nsocio-cultural critique. Technology is an ongoing attempt to bring the\nworld closer to the way one wishes it to be. Whereas science aims to\nunderstand the world as it is, technology aims to change the world.\nThese are abstractions, of course. For one, whose wishes concerning\nwhat the world should be like are realized in technology? Unlike\nscientists, who are often personally motivated in their attempts at\ndescribing and understanding the world, engineers are seen, not in the\nleast by engineers themselves, as undertaking their attempts to change\nthe world as a service to the public. The ideas on what is to be or\nwhat ought to be are seen as originating outside of technology itself;\nengineers then take it upon themselves to realize these ideas. This\nview is a major source for the widely spread picture of technology as\nbeing instrumental, as delivering instruments ordered from\n‘elsewhere’, as means to ends specified outside of\nengineering, a picture that has served further to support the claim\nthat technology is neutral with respect to values, discussed\nin\n Section 3.3.1.\n This view involves a considerable distortion of reality, however.\nMany engineers are intrinsically motivated to change the world; in\ndelivering ideas for improvement they are, so to speak, their own best\ncustomers. The same is true for most industrial companies,\nparticularly in a market economy, where the prospect of great profits\nis another powerful motivator. As a result, much technological\ndevelopment is ‘technology-driven’. \nTo understand where technology ‘comes from’, what drives\nthe innovation process, is of importance not only to those who are\ncurious to understand the phenomenon of technology itself but also to\nthose who are concerned about its role in society. Technology or\nengineering as a practice is concerned with the creation of artifacts\nand, of increasing importance, artifact-based services. The design\nprocess, the structured process leading toward that goal, forms\nthe core of the practice of technology. In the engineering literature,\nthe design process is commonly represented as consisting of a series\nof translational steps; see for this, e.g., Suh 2001. At the start are\nthe customer’s needs or wishes. In the first step these are\ntranslated into a list of functional requirements, which then\ndefine the design task an engineer, or a team of engineers, has to\naccomplish. The functional requirements specify as precisely as\npossible what the device to be designed must be able to do. This step\nis required because customers usually focus on just one or two\nfeatures and are unable to articulate the requirements that are\nnecessary to support the functionality they desire. In the second\nstep, the functional requirements are translated into design\nspecifications, which the exact physical parameters of crucial\ncomponents by which the functional requirements are going to be met.\nThe design parameters chosen to satisfy these requirements are\ncombined and made more precise such that a blueprint of the\ndevice results. The blueprint contains all the details that must be\nknown such that the final step to the process of manufacturing the\ndevice can take place. It is tempting to consider the blueprint as the\nend result of a design process, instead of a finished copy being this\nresult. However, actual copies of a device are crucial for the purpose\nof prototyping and testing. Prototyping and testing presuppose that\nthe sequence of steps making up the design process can and will often\ncontain iterations, leading to revisions of the design parameters\nand/or the functional requirements. Even though, certainly for\nmass-produced items, the manufacture of a product for delivery to its\ncustomers or to the market comes after the closure of the design\nphase, the manufacturing process is often reflected in the functional\nrequirements of a device, for example in putting restrictions on the\nnumber of different components of which the device consists. The\ncomplexity of a device will affect how difficult it will be to\nmaintain or repair it, and ease of maintenance or low repair costs are\noften functional requirements. An important modern development is that\nthe complete life cycle of an artifact is now considered to be the\ndesigning engineer’s concern, up till the final stages of the\nrecycling and disposal of its components and materials, and the\nfunctional requirements of any device should reflect this. From this\npoint of view, neither a blueprint nor a prototype can be considered\nthe end product of engineering design. \nThe biggest idealization that this scheme of the design process\ncontains is arguably located at the start. Only in a minority of cases\ndoes a design task originate in a customer need or wish for a\nparticular artifact. First of all, as already suggested, many design\ntasks are defined by engineers themselves, for instance, by noticing\nsomething to be improved in existing products. But more often than not\ndesign starts with a problem pointed out by some societal agent, which\nengineers are then invited to solve. Many such problems, however, are\nill-defined or wicked problems, meaning that it is not at all\nclear what the problem is exactly and what a solution to the problem\nwould consist in. The ‘problem’ is a situation that\npeople—not necessarily the people ‘in’ the\nsituation—find unsatisfactory, but typically without being able\nto specify a situation that they find more satisfactory in other terms\nthan as one in which the problem has been solved. In particular it is\nnot obvious that a solution to the problem would consist in some\nartifact, or some artifactual system or process, being made available\nor installed. Engineering departments all over the world advertise\nthat engineering is problem solving, and engineers easily seem\nconfident that they are best qualified to solve a problem when they\nare asked to, whatever the nature of the problem. This has led to the\nphenomenon of a technological fix, the solution of a problem\nby a technical solution, that is, the delivery of an artifact or\nartifactual process, where it is questionable, to say the least,\nwhether this solves the problem or whether it was the best way of\nhandling the problem. \nA candidate example of a technological fix for the problem of global\nwarming would be the currently much debated option of injecting\nsulfate aerosols into the stratosphere to offset the warming effect of\ngreenhouse gases such as carbon dioxide and methane. Such schemes of\ngeoengineering would allow us to avoid facing the—in all\nlikelihood painful—choices that will lead to a reduction of the\nemission of greenhouse gases into the atmosphere, but will at the same\ntime allow the depletion of the Earth’s reservoir of fossil\nfuels to continue. See for a discussion of technological fixing, e.g.,\nVolti 2009: 26–32. Given this situation, and its hazards, the\nnotion of a problem and a taxonomy of problems deserve to receive more\nphilosophical attention than they have hitherto received. \nThese wicked problems are often broadly social problems, which would\nbest be met by some form of ‘social action’, which would\nresult in people changing their behavior or acting differently in such\na way that the problem would be mitigated or even disappear\ncompletely. In defense of the engineering view, it could perhaps be\nsaid that the repertoire of ‘proven’ forms of social\naction is meager. The temptation of technical fixes could be\novercome—at least that is how an engineer might see it—by\nthe inclusion of the social sciences in the systematic development and\napplication of knowledge to the solution of human problems. This\nhowever, is a controversial view. Social engineering is to\nmany a specter to be kept at as large a distance as possible instead\nof an ideal to be pursued. Karl Popper referred to acceptable forms of\nimplementing social change as ‘piecemeal social\nengineering’ and contrasted it to the revolutionary but\ncompletely unfounded schemes advocated by, e.g., Marxism. In the entry\non\n Karl Popper,\n however, his choice of words is called ‘rather\nunfortunate’. The notion of social engineering, and its cogency,\ndeserves more attention that it is currently receiving. \nAn important input for the design process is scientific knowledge:\nknowledge about the behavior of components and the materials they are\ncomposed of in specific circumstances. This is the point where science\nis applied. However, much of this knowledge is not directly available\nfrom the sciences, since it often concerns extremely detailed behavior\nin very specific circumstances. This scientific knowledge is therefore\noften generated within technology, by the engineering sciences. But\napart from this very specific scientific knowledge, engineering design\ninvolves various other sorts of knowledge. In his book What\nEngineers Know and How They Know It (Vincenti 1990), the\naeronautical engineer Walter Vincenti gave a six-fold categorization\nof engineering design knowledge (leaving aside production and\noperation as the other two basic constituents of engineering\npractice). Vincenti distinguishes \nThe fourth category concerns the quantitative knowledge just referred\nto, and the third the theoretical tools used to acquire it. These two\ncategories can be assumed to match Bunge’s notion of substantive\ntechnological theories. The status of the remaining four categories is\nmuch less clear, however, partly because they are less familiar, or\nnot at all, from the well-explored context of science. Of these\ncategories, Vincenti claims that they represent prescriptive forms of\nknowledge rather than descriptive ones. Here, the activity of design\nintroduces an element of normativity, which is absent from scientific\nknowledge. Take such a basic notion as ‘operational\nprinciple’, which refers to the way in which the function of a\ndevice is realized, or, in short, how it works. This is still a purely\ndescriptive notion. Subsequently, however, it plays a role in\narguments that seek to prescribe a course of action to someone who has\na goal that could be realized by the operation of such a device. At\nthis stage, the issue changes from a descriptive to a prescriptive or\nnormative one. An extensive discussion of the various kinds of\nknowledge relevant to technology is offered by Houkes (2009). \nAlthough the notion of an operational principle—a term that\nseems to originate with Polanyi (1958)—is central to engineering\ndesign, no single clear-cut definition of it seems to exist. The issue\nof disentangling descriptive from prescriptive aspects in an analysis\nof the technical action and its constituents is therefore a task that\nhas hardly begun. This task requires a clear view on the extent and\nscope of technology. If one follows Joseph Pitt in his book\nThinking About Technology (1999) and defines technology\nbroadly as ‘humanity at work’, then to distinguish between\ntechnological action and action in general becomes difficult, and the\nstudy of technological action must absorb all descriptive and\nnormative theories of action, including the theory of practical\nrationality, and much of theoretical economics in its wake. There have\nindeed been attempts at such an encompassing account of human action,\nfor example Tadeusz Kotarbinski’s Praxiology (1965),\nbut a perspective of such generality makes it difficult to arrive at\nresults of sufficient depth. It would be a challenge for philosophy to\nspecify the differences among action forms and the reasoning grounding\nthem in, to single out three prominent fields of study, technology,\norganization and management, and economics. \nA more restricted attempt at such an approach is Ilkka\nNiiniluoto’s (1993). According to Niiniluoto, the theoretical\nframework of technology as the practice that is concerned with what\nthe world should be like rather than is, the framework that forms the\ncounterpoint to the descriptive framework of science, is design\nscience. The content of design science, the counterpoint to the\ntheories and explanations that form the content of descriptive\nscience, would then be formed by technical norms, statements\nof the form ‘If one wants to achieve X, one should do\nY’. The notion of a technical norm derives from Georg\nHenrik von Wright’s Norm and Action (1963). Technical\nnorms need to be distinguished from anankastic statements expressing\nnatural necessity, of the form ‘If X is to be achieved,\nY needs to be done’; the latter have a truth value but\nthe former have not. Von Wright himself, however, wrote that he did\nnot understand the mutual relations between these statements. Ideas on\nwhat design science is and can and should be are evidently related to\nthe broad problem area of practical rationality—see this\nencyclopedia’s entries on\n practical reason\n and\n instrumental rationality—and\n also to means-ends reasoning, discussed in the next section. \nDesign is an activity that is subject to rational scrutiny but in\nwhich creativity is considered to play an important role as well.\nSince design is a form of action, a structured series of decisions to\nproceed in one way rather than another, the form of rationality that\nis relevant to it is practical rationality, the rationality\nincorporating the criteria on how to act, given particular\ncircumstances. This suggests a clear division of labor between the\npart to be played by rational scrutiny and the part to be played by\ncreativity. Theories of rational action generally conceive their\nproblem situation as one involving a choice among various course of\naction open to the agent. Rationality then concerns the question how\nto decide among given options, whereas creativity concerns the\ngeneration of these options. This distinction is similar to the\ndistinction between the context of justification and the context of\ndiscovery in science. The suggestion that is associated with this\ndistinction, however, that rational scrutiny only applies in the\ncontext of justification, is difficult to uphold for technological\ndesign. If the initial creative phase of option generation is\nconducted sloppily, the result of the design task can hardly be\nsatisfactory. Unlike the case of science, where the practical\nconsequences of entertaining a particular theory are not taken into\nconsideration, the context of discovery in technology is governed by\nsevere constraints of time and money, and an analysis of the problem\nhow best to proceed certainly seems in order. There has been little\nphilosophical work done in this direction; an overview of the issues\nis given in Kroes, Franssen, and Bucciarelli (2009). \nThe ideas of Herbert Simon on bounded rationality (see, e.g., Simon\n1982) are relevant here, since decisions on when to stop generating\noptions and when to stop gathering information about these options and\nthe consequences when they are adopted are crucial in decision making\nif informational overload and calculative intractability are to be\navoided. However, it has proved difficult to further develop\nSimon’s ideas on bounded rationality since their conception in\nthe 1950s. Another notion that is relevant here is means-ends\nreasoning. In order to be of any help here, theories of means-ends\nreasoning should then concern not just the evaluation of given means\nwith respect to their ability to achieve given ends, but also the\ngeneration or construction of means for given ends. A comprehensive\ntheory of means-ends reasoning, however, is not yet available; for a\nproposal on how to develop means-ends reasoning in the context of\ntechnical artifacts, see Hughes, Kroes, and Zwart 2007. In the\npractice of technology, alternative proposals for the realization of\nparticular functions are usually taken from ‘catalogs’ of\nexisting and proven realizations. These catalogs are extended by\nongoing research in technology rather than under the urge of\nparticular design tasks. \nWhen engineering design is conceived as a process of decision making,\ngoverned by considerations of practical rationality, the next step is\nto specify these considerations. Almost all theories of practical\nrationality conceive of it as a reasoning process where a match\nbetween beliefs and desires or goals is sought. The desires or goals\nare represented by their value or utility for the decision maker, and\nthe decision maker’s problem is to choose an action that\nrealizes a situation that, ideally, has maximal value or utility among\nall the situations that could be realized. If there is uncertainty\nconcerning he situations that will be realized by a particular action,\nthen the problem is conceived as aiming for maximal expected\nvalue or utility. Now the instrumental perspective on technology\nimplies that the value that is at issue in the design process viewed\nas a process of rational decision making is not the value of the\nartifacts that are created. Those values are the domain of the\nusers of the technology so created. They are supposed to be\nrepresented in the functional requirements defining the design task.\nInstead the value to be maximized is the extent to which a particular\ndesign meets the functional requirements defining the design task. It\nis in this sense that engineers share an overall perspective on\nengineering design as an exercise in optimization. But\nalthough optimization is a value-orientated notion, it is not itself\nperceived as a value driving engineering design. \nThe functional requirements that define most design problems do not\nprescribe explicitly what should be optimized; usually they set levels\nto be attained minimally. It is then up to the engineer to choose how\nfar to go beyond meeting the requirements in this minimal sense.\nEfficiency, in energy consumption and use of materials first\nof all, is then often a prime value. Under the pressure of society,\nother values have come to be incorporated, in particular\nsafety and, more recently, sustainability. Sometimes\nit is claimed that what engineers aim to maximize is just one factor,\nnamely market success. Market success, however, can only be assessed\nafter the fact. The engineer’s maximization effort will instead\nbe directed at what are considered the predictors of market success.\nMeeting the functional requirements and being relatively efficient and\nsafe are plausible candidates as such predictors, but additional\nmethods, informed by market research, may introduce additional factors\nor may lead to a hierarchy among the factors. \nChoosing the design option that maximally meets all the functional\nrequirements (which may but need not originate with the prospective\nuser) and all other considerations and criteria that are taken to be\nrelevant, then becomes the practical decision-making problem to be\nsolved in a particular engineering-design task. This creates several\nmethodological problems. Most important of these is that the engineer\nis facing a multi-criteria decision problem. The various\nrequirements come with their own operationalizations in terms of\ndesign parameters and measurement procedures for assessing their\nperformance. This results in a number of rank orders or quantitative\nscales which represent the various options out of which a choice is to\nbe made. The task is to come up with a final score in which all these\nresults are ‘adequately’ represented, such that the option\nthat scores best can be considered the optimal solution to the design\nproblem. Engineers describe this situation as one where\ntrade-offs have to be made: in judging the merit of one\noption relative to other options, a relative bad performance on one\ncriterion can be balanced by a relatively good performance on another\ncriterion. An important problem is whether a rational method for doing\nthis can be formulated. It has been argued by Franssen (2005) that\nthis problem is structurally similar to the well-known problem of\nsocial choice, for which Kenneth Arrow proved his notorious\nimpossibility theorem in 1950, implying that no general rational\nsolution method exists for this problem. This poses serious problems\nfor the claim of engineers that their designs are optimal solutions,\nsince Arrow’s theorem implies that in most multi-criteria\nproblems the notion of ‘optimal’ cannot be rigorously\ndefined. \nThis result seems to except a crucial aspect of engineering activity\nfrom philosophical scrutiny, and it could be used to defend the\nopinion that engineering is at least partly an art, not a science.\nInstead of surrendering to the result, however, which has a\nsignificance that extends much beyond engineering and even beyond\ndecision making in general, we should perhaps conclude instead that\nthere is still a lot of work to be done on what might be termed,\nprovisionally, ‘approximative’ forms of reasoning. One\nform of reasoning to be included here is Herbert Simon’s bounded\nrationality, plus the related notion of ‘satisficing’.\nSince their introduction in the 1950s (Simon 1957) these two terms\nhave found wide usage, but we are still lacking a general theory of\nbounded rationality. It may be in the nature of forms of approximative\nreasoning such as bounded rationality that a general theory cannot be\nhad, but even a systematic treatment from which such an insight could\nemerge seems to be lacking. \nAnother problem for the decision-making view of engineering design is\nthat in modern technology almost all design is done by teams. Such\nteams are composed of experts from many different disciplines. Each\ndiscipline has its own theories, its own models of interdependencies,\nits own assessment criteria, and so forth, and the professionals\nbelonging to these disciplines must be considered as inhabitants of\ndifferent object worlds, as Louis Bucciarelli (1994) phrases\nit. The different team members are, therefore, likely to disagree on\nthe relative rankings and evaluations of the various design options\nunder discussion. Agreement on one option as the overall best one can\nhere be even less arrived at by an algorithmic method exemplifying\nengineering rationality. Instead, models of social interaction, such\nas bargaining and strategic thinking, are relevant here. An example of\nsuch an approach to an (abstract) design problem is presented by\nFranssen and Bucciarelli (2004). \nTo look in this way at technological design as a decision-making\nprocess is to view it normatively from the point of view of practical\nor instrumental rationality. At the same time it is descriptive in\nthat it is a description of how engineering methodology generally\npresents the issue how to solve design problems. From that somewhat\nhigher perspective there is room for all kinds of normative questions\nthat are not addressed here, such as whether the functional\nrequirements defining a design problem can be seen as an adequate\nrepresentation of the values of the prospective users of an artifact\nor a technology, or by which methods values such as safety and\nsustainability can best be elicited and represented in the design\nprocess. These issues will be taken up in\n Section 3. \nUnderstanding the process of designing artifacts is the theme in\nphilosophy of technology that most directly touches on the interests\nof engineering practice. This is hardly true for another issue of\ncentral concern to analytic philosophy of technology, which is the\nstatus and the character of artifacts. This is perhaps not unlike the\nsituation in the philosophy of science, where working scientists seem\nalso to be much less interested in investigating the status and\ncharacter of models and theories than philosophers are. \nArtifacts are man-made objects: they have an author (see Hilpinen 1992\nand Hilpinen’s article\n artifact\n in this encyclopedia). The artifacts that are of relevance to\ntechnology are, in particular, made to serve a purpose. This excludes,\nwithin the set of all man-made objects, on the one hand byproducts and\nwaste products and on the other hand works of art. Byproducts and\nwaste products result from an intentional act to make something but\njust not precisely, although the author at work may be well aware of\ntheir creation. Works of art result from an intention directed at\ntheir creation (although in exceptional cases of conceptual art, this\ndirectedness may involve many intermediate steps) but it is contested\nwhether artists include in their intentions concerning their work an\nintention that the work serves some purpose. A further discussion of\nthis aspect belongs to the philosophy of art. An interesting general\naccount has been presented by Dipert (1993). \nTechnical artifacts, then, are made to serve some purpose, generally\nto be used for something or to act as a component in a larger\nartifact, which in its turn is either something to be used or again a\ncomponent. Whether end product or component, an artifact is ‘for\nsomething’, and what it is for is called the artifact’s\nfunction. Several researchers have emphasized that an\nadequate description of artifacts must refer both to their status as\ntangible physical objects and to the intentions of the people engaged\nwith them. Kroes and Meijers (2006) have dubbed this view “the\ndual nature of technical artifacts”; its most mature formulation\nis Kroes 2012. They suggest that the two aspects are ‘tied\nup’, so to speak, in the notion of artifact function. This gives\nrise to several problems. One, which will be passed over quickly\nbecause little philosophical work seems to have been done concerning\nit, is that structure and function mutually constrain each other, but\nthe constraining is only partial. It is unclear whether a general\naccount of this relation is possible and what problems need to be\nsolved to arrive there. There may be interesting connections with the\nissue of multiple realizability in the philosophy of mind and with\naccounts of reduction in science; an example where this is explored is\nMahner and Bunge 2001. \nIt is equally problematic whether a unified account of the notion of\nfunction as such is possible, but this issue has received considerably\nmore philosophical attention. The notion of function is of paramount\nimportance for characterizing artifacts, but the notion is used much\nmore widely. The notion of an artifact’s function seems to refer\nnecessarily to human intentions. Function is also a key concept in\nbiology, however, where no intentionality plays a role, and it is a\nkey concept in cognitive science and the philosophy of mind, where it\nis crucial in grounding intentionality in non-intentional, structural\nand physical properties. Up till now there is no accepted general\naccount of function that covers both the intentionality-based notion\nof artifact function and the non-intentional notion of biological\nfunction—not to speak of other areas where the concept plays a\nrole, such as the social sciences. The most comprehensive theory, that\nhas the ambition to account for the biological notion, cognitive\nnotion and the intentional notion, is Ruth Millikan’s 1984; for\ncriticisms and replies, see B. Preston 1998, 2003; Millikan 1999;\nVermaas & Houkes 2003; and Houkes & Vermaas 2010. The\ncollection of essays edited by Ariew, Cummins and Perlman (2002)\npresents a recent introduction to the general topic of defining the\nnotion of function in general, although the emphasis is, as is\ngenerally the case in the literature on function, on biological\nfunctions. \nAgainst the view that, at least in the case of artifacts, the notion\nof function refers necessarily to intentionality, it could be argued\nthat in discussing the functions of the components of a larger device,\nand the interrelations between these functions, the intentional\n‘side’ of these functions is of secondary importance only.\nThis, however, would be to ignore the possibility of the\nmalfunctioning of such components. This notion seems to be\ndefinable only in terms of a mismatch between actual behavior and\nintended behavior. The notion of malfunction also sharpens an\nambiguity in the general reference to intentions when characterizing\ntechnical artifacts. These artifacts usually engage many people, and\nthe intentions of these people may not all pull in the same direction.\nA major distinction can be drawn between the intentions of the actual\nuser of an artifact for a particular purpose and the intentions of the\nartifact’s designer. Since an artifact may be used for a purpose\ndifferent from the one for which its designer intended it to be used,\nand since people may also use natural objects for some purpose or\nother, one is invited to allow that artifacts can have multiple\nfunctions, or to enforce a hierarchy among all relevant intentions in\ndetermining the function of an artifact, or to introduce a\nclassification of functions in terms of the sorts of determining\nintentions. In the latter case, which is a sort of middle way between\nthe two other options, one commonly distinguishes between the\nproper function of an artifact as the one intended by its\ndesigner and the accidental function of the artifact as the\none given to it by some user on private considerations. Accidental use\ncan become so common, however, that the original function drops out of\nmemory. \nClosely related to this issue to what extent use and design determine\nthe function of an artifact is the problem of characterizing artifact\nkinds. It may seem that we use functions to classify artifacts: an\nobject is a knife because it has the function of cutting, or more\nprecisely, of enabling us to cut. On closer inspection, however, the\nlink between function and kind-membership seems much less\nstraightforward. The basic kinds in technology are, for example,\n‘knife’, ‘aircraft’ and ‘piston’.\nThe members of these kinds have been designed in order to be used to\ncut something with, to transport something through the air and to\ngenerate mechanical movement through thermodynamic expansion. However,\none cannot create a particular kind of artifact just by designing\nsomething with the intention that it be used for some particular\npurpose: a member of the kind so created must actually be useful for\nthat purpose. Despite innumerable design attempts and claims, the\nperpetual motion machine is not a kind of artifact. A kind like\n‘knife’ is defined, therefore, not only by the intentions\nof the designers of its members that they each be useful for cutting\nbut also by a shared operational principle known to these designers,\nand on which they based their design. This is, in a different setting,\nalso defended by Thomasson, who in her characterization of what she in\ngeneral calls an artifactual kind says that such a kind is\ndefined by the designer’s intention to make something of that\nkind, by a substantive idea that the designer has of how this can be\nachieved, and by his or her largely successful achievement of it\n(Thomasson 2003, 2007). Qua sorts of kinds in which artifacts can be\ngrouped, a distinction must therefore be made between a kind like\n‘knife’ and a corresponding but different kind\n‘cutter’. A ‘knife’ indicates a particular way\na ‘cutter’ can be made. One can also cut, however, with a\nthread or line, a welding torch, a water jet, and undoubtedly by other\nsorts of means that have not yet been thought of. A\n‘cutter’ would then refer to a truly functional kind. As\nsuch, it is subject to the conflict between use and design: one could\nmean by ‘cutter’ anything than can be used for cutting or\nanything that has been designed to be used for cutting, by the\napplication of whatever operational principle, presently known or\nunknown. \nThis distinction between artifact kinds and functional kinds is\nrelevant for the status of such kinds in comparison to other notions\nof kinds. Philosophy of science has emphasized that the concept of\nnatural kind, such as exemplified by ‘water’ or\n‘atom’, lies at the basis of science. On the other hand it\nis generally taken for granted that there are no regularities that all\nknives or airplanes or pistons answer to. This, however, is loosely\nbased on considerations of multiple realizability that fully apply\nonly to functional kinds, not to artifact kinds. Artifact kinds share\nan operational principle that gives them some commonality in physical\nfeatures, and this commonality becomes stronger once a particular\nartifact kind is subdivided into narrower kinds. Since these kinds are\nspecified in terms of physical and geometrical parameters, they are\nmuch closer to the natural kinds of science, in that they support\nlaw-like regularities; see for a defense of this position (Soavi\n2009). A recent\ncollection of essays that discuss the metaphysics of artifacts and\nartifact kinds is Franssen, Kroes, Reydon and Vermaas 2014. \nThere is at least one additional technology-related topic that ought\nto be mentioned because it has created a good deal of analytic\nphilosophical literature, namely Artificial Intelligence and related\nareas. A full discussion of this vast field is beyond the scope of\nthis entry, however. Information is to be found in the entries on\n Turing machines,\n the Church-Turing thesis,\n computability and complexity,\n the Turing test,\n the Chinese room argument,\n the computational theory of mind,\n functionalism,\n multiple realizability, and\n the philosophy of computer science. \nIt was not until the twentieth century that the development of the\nethics of technology as a systematic and more or less independent\nsubdiscipline of philosophy started. This late development may seem\nsurprising given the large impact that technology has had on society,\nespecially since the industrial revolution. \nA plausible reason for this late development of ethics of technology\nis the instrumental perspective on technology that was mentioned in\n Section 2.2.\n This perspective implies, basically, a positive ethical assessment of\ntechnology: technology increases the possibilities and capabilities of\nhumans, which seems in general desirable. Of course, since antiquity,\nit has been recognized that the new capabilities may be put to bad use\nor lead to human hubris. Often, however, these undesirable\nconsequences are attributed to the users of technology, rather than\nthe technology itself, or its developers. This vision is known as the\ninstrumental vision of technology resulting in the so-called\nneutrality thesis. The neutrality thesis holds that technology is a\nneutral instrument that can be put to good or bad use by its users.\nDuring the twentieth century, this neutrality thesis met with severe\ncritique, most prominently by Heidegger and Ellul, who have been\nmentioned in this context in\n Section 2,\nbut also by philosophers from the Frankfurt School, such as Horkheimer\nand Adorno (1947 [2002]), Marcuse (1964), and Habermas (1968\n[1970]). \nThe scope and the agenda for ethics of technology to a large extent\ndepend on how technology is conceptualized. The second half of the\ntwentieth century has witnessed a richer variety of conceptualizations\nof technology that move beyond the conceptualization of technology as\na neutral tool, as a world view or as a historical necessity. This\nincludes conceptualizations of technology as a political phenomenon\n(Winner, Feenberg, Sclove), as a social activity (Latour, Callon, Bijker and others in the\narea of science and technology studies), as a cultural phenomenon\n(Ihde, Borgmann), as a professional activity (engineering ethics,\ne.g., Davis), and as a cognitive activity (Bunge, Vincenti). Despite\nthis diversity, the development in the second half of the twentieth\ncentury is characterized by two general trends. One is a move away\nfrom technological determinism and the assumption that technology is a\ngiven self-contained phenomenon which develops autonomously to an\nemphasis on technological development being the result of choices\n(although not necessarily the intended result). The other is a move\naway from ethical reflection on technology as such to ethical\nreflection of specific technologies and to specific phases in the\ndevelopment of technology. Both trends together have resulted in an\nenormous increase in the number and scope of ethical questions that\nare asked about technology. The developments also imply that ethics of\ntechnology is to be adequately empirically informed, not only about\nthe exact consequences of specific technologies but also about the\nactions of engineers and the process of technological development.\nThis has also opened the way to the involvement of other disciplines\nin ethical reflections on technology, such as Science and Technology\nStudies (STS) and Technology Assessment (TA). \nNot only is the ethics of technology characterized by a diversity of\napproaches, it might even be doubted whether something like a\nsubdiscipline of ethics of technology, in the sense of a community of\nscholars working on a common set of problems, exists. The scholars\nstudying ethical issues in technology have diverse backgrounds (e.g.,\nphilosophy, STS, TA, law, political science) and they do not always\nconsider themselves (primarily) ethicists of technology. To give the\nreader an overview of the field, three basic approaches or strands\nthat might be distinguished in the ethics of technology will be\ndiscussed. \nBoth cultural and political approaches build on the traditional\nphilosophy and ethics of technology of the first half of the twentieth\ncentury. Whereas cultural approaches conceive of technology as a\ncultural phenomenon that influences our perception of the world,\npolitical approaches conceive of technology as a political phenomenon,\ni.e., as a phenomenon that is ruled by and embodies institutional\npower relations between people. \nCultural approaches are often phenomenological in nature or at least\nposition themselves in relation to phenomenology as\npost-phenomenology. Examples of philosophers in this tradition are Don\nIhde, Albert Borgmann, Peter-Paul Verbeek and Evan Selinger (e.g.,\nBorgmann 1984; Ihde 1990; Verbeek 2000 [2005], 2011). The approaches are\nusually influenced by developments in STS, especially the idea that\ntechnologies contain a script that influences not only people’s\nperception of the world but also human behavior, and the idea of the\nabsence of a fundamental distinction between humans and non-humans,\nincluding technological artifacts (Akrich 1992; Latour 1992, 1993;\nIhde & Selinger 2003). The combination of both ideas has led some\nto claim that technology has (moral) agency, a claim that is discussed\nbelow in\n Section 3.3.1. \nPolitical approaches to technology mostly go back to Marx, who assumed\nthat the material structure of production in society, in which\ntechnology is obviously a major factor, determined the economic and\nsocial structure of that society. Similarly, Langdon Winner has argued\nthat technologies can embody specific forms of power and authority\n(Winner 1980). According to him, some technologies are inherently\nnormative in the sense that they require or are strongly compatible\nwith certain social and political relations. Railroads, for example,\nseem to require a certain authoritative management structure. In other\ncases, technologies may be political due to the particular way they\nhave been designed. Some political approaches to technology are\ninspired by (American) pragmatism and, to a lesser extent, discourse\nethics. A number of philosophers, for example, have pleaded for a\ndemocratization of technological development and the inclusion of\nordinary people in the shaping of technology (Winner 1983; Sclove\n1995; Feenberg 1999). \nAlthough political approaches have obviously ethical ramifications,\nmany philosophers who have adopted such approaches do not engage in\nexplicit ethical reflection on technology. An interesting recent\nexception, and an attempt to consolidate a number of recent\ndevelopments and to articulate them into a more general account of\nwhat an ethics of technology should look like, is the volume\nPragmatist Ethics for a Technological Culture (Keulartz et\nal. 2002). In this volume, the authors plead for a revival of the\npragmatist tradition in moral philosophy because it is better fit to\ndeal with a number of moral issues in technology. Instead of focusing\non how to reach and justify normative judgments about technology, a\npragmatist ethics focuses on how to recognize and trace moral problems\nin the first place. Moreover, the process of dealing with these\nproblems is considered more important than the outcome. \nEngineering ethics is a relatively new field of education and\nresearch. It started off in the 1980s in the United States, merely as\nan educational effort. Engineering ethics is concerned with “the\nactions and decisions made by persons, individually or collectively,\nwho belong to the profession of engineering” (Baum 1980: 1).\nAccording to this approach, engineering is a profession, in the same\nway as medicine is a profession. \nAlthough there is no agreement on how a profession exactly should be\ndefined, the following characteristics are often mentioned: \nTypical ethical issues that are discussed in engineering ethics are\nprofessional obligations of engineers as exemplified in, for example,\ncodes of ethics of engineers, the role of engineers versus managers,\ncompetence, honesty, whistle-blowing, concern for safety and conflicts\nof interest (Davis 1998, 2005; Martin & Schinzinger 2005; Harris,\nPritchard, & Rabins 2008). \nRecently, a number of authors have pleaded for broadening the\ntraditional scope of engineering ethics (e.g., Herkert 2001;, van de\nPoel & Royakkers 2011). This call for a broader approach derives\nfrom two concerns. One concern is that the traditional micro-ethical\napproach in engineering ethics tends to take the contexts in which\nengineers have to work for given, while major ethical issues pertain\nto how this context is ‘organized’. Another concern is\nthat the traditional micro-ethical focus tends to neglect issues\nrelating to the impact of technology on society or issues relating to\ndecisions about technology. Broadening the scope of engineering ethics\nwould then, among others, imply more attention for such issues as\nsustainability and social justice. \nThe last decades have witnessed an increase in ethical inquiries into\nspecific technologies. This may now be the largest of the three\nstrands discussed, especially given the rapid growth in\ntechnology-specific ethical inquiries in the last two decades. One of\nthe most visible new fields is probably computer ethics (e.g., Moor\n1985; Floridi 2010; Johnson 2009; Weckert 2007; van den Hoven &\nWeckert 2008), with more recently a focus on robotics, artificial\nintelligence, machine ethics, and the ethics of algorithms (Lin,\nAbney, & Jenkins 2017; Nucci & Santoni de Sio 2016;\nMittelstadt et al. 2016; Bostrom & Yudkowsky 2014; Wallach &\nAllen 2009). But biotechnology has spurred dedicated ethical\ninvestigations as well (e.g., Sherlock & Morrey 2002; P. Thompson\n2007). More traditional fields like architecture and urban planning\nhave also attracted specific ethical attention (Fox 2000). More\nrecently, nanotechnology and so-called converging technologies have\nled to the establishment of what is called nanoethics (Allhoff et al.\n2007). Other examples are the ethics of nuclear deterrence (Finnis et\nal. 1988), nuclear energy (Taebi & Roeser 2015) and geoengineering\n(C. Preston 2016). \nObviously the establishment of such new fields of ethical reflection\nis a response to social and technological developments. Still, the\nquestion can be asked whether the social demand is best met by\nestablishing new fields of applied ethics. This issue is in fact\nregularly discussed as new fields emerge. Several authors have for\nexample argued that there is no need for nanoethics because\nnanotechnology does not raise any really new ethical issues (e.g.,\nMcGinn 2010). The alleged absence of newness here is supported by the\nclaim that the ethical issues raised by nanotechnology are a variation\non, and sometimes an intensification of, existing ethical issues, but\nhardly really new, and by the claim that these issues can be dealt\nwith the existing theories and concepts from moral philosophy. For an\nearlier, similar discussion concerning the supposed new character of\nethical issues in computer engineering, see Tavani 2002. \nThe new fields of ethical reflection are often characterized as\napplied ethics, that is, as applications of theories, normative\nstandards, concepts and methods developed in moral philosophy. For\neach of these elements, however, application is usually not\nstraightforward but requires a further specification or revision. This\nis the case because general moral standards, concepts and methods are\noften not specific enough to be applicable in any direct sense to\nspecific moral problems. ‘Application’ therefore often\nleads to new insights which might well result in the reformulation or\nat least refinement of existing normative standards, concepts and\nmethods. In some cases, ethical issues in a specific field might\nrequire new standards, concepts or methods. Beauchamp and Childress\nfor example have proposed a number of general ethical principles for\nbiomedical ethics (Beauchamp & Childress 2001). These principles\nare more specific than general normative standards, but still so\ngeneral and abstract that they apply to different issues in biomedical\nethics. In computer ethics, existing moral concepts relating to for\nexample privacy and ownership has been redefined and adapted to deal\nwith issues which are typical for the computer age (Johnson 2003). New\nfields of ethical application might also require new methods for, for\nexample, discerning ethical issues that take into account relevant\nempirical facts about these fields, like the fact that technological\nresearch and development usually takes place in networks of people\nrather than by individuals (Zwart et al. 2006). Another more general\nissue that applies to many new technologies is how to deal with the\nuncertainties about (potential) social and ethical impacts that\ntypically surround new emerging technologies. Brey’s (2012)\nproposal for an anticipatory ethics may be seen as a reply to this\nchallenge. The issue of anticipation is also one of the central\nconcerns in the more recent interdisciplinary field of responsible\ninnovation (e.g., Owen et al. 2013). \nAlthough different fields of ethical reflection on specific\ntechnologies might well raise their own philosophical and ethical\nissues, it can be questioned whether this justifies the development of\nseparate subfields or even subdisciplines. One obvious argument might\nbe that in order to say something ethically meaningful about new\ntechnologies, one needs specialized and detailed knowledge of a\nspecific technology. Moreover such subfields allow interaction with\nrelevant non-philosophical experts in for example law, psychology,\neconomy, science and technology studies (STS) or technology assessment\n(TA). On the other side, it could also be argued that a lot can be\nlearned from interaction and discussion between ethicists specializing\nin different technologies, and a fruitful interaction with the two\nother strands discussed above (cultural and political approaches and\nengineering ethics). Currently, such interaction in many cases seems\nabsent, although there are of course exceptions. \nWe now turn to the description of some themes in the ethics of\ntechnology. We focus on a number of general themes that provide an\nillustration of general issues in the ethics of technology and the way\nthese are treated. \nOne important general theme in the ethics of technology is the\nquestion whether technology is value-laden. Some authors have\nmaintained that technology is value-neutral, in the sense that\ntechnology is just a neutral means to an end, and accordingly can be\nput to good or bad use (e.g., Pitt 2000). This view might have some\nplausibility in as far as technology is considered to be just a bare\nphysical structure. Most philosophers of technology, however, agree\nthat technological development is a goal-oriented process and that\ntechnological artifacts by definition have certain functions, so that\nthey can be used for certain goals but not, or far more difficulty or\nless effectively, for other goals. This conceptual connection between\ntechnological artifacts, functions and goals makes it hard to maintain\nthat technology is value-neutral. Even if this point is granted, the\nvalue-ladenness of technology can be construed in a host of different\nways. Some authors have maintained that technology can have moral\nagency. This claim suggests that technologies can autonomously and\nfreely ‘act’ in a moral sense and can be held morally\nresponsible for their actions. \nThe debate whether technologies can have moral agency started off in\ncomputer ethics (Bechtel 1985; Snapper 1985; Dennett 1997; Floridi\n& Sanders 2004) but has since broadened. Typically, the authors\nwho claim that technologies (can) have moral agency often redefine the\nnotion of agency or its connection to human will and freedom (e.g.,\nLatour 1993; Floridi & Sanders 2004, Verbeek 2011). A disadvantage\nof this strategy is that it tends to blur the morally relevant\ndistinctions between people and technological artifacts. More\ngenerally, the claim that technologies have moral agency sometimes\nseems to have become shorthand for claiming that technology is morally\nrelevant. This, however, overlooks the fact technologies can be\nvalue-laden in other ways than by having moral agency (see, e.g.,\nJohnson 2006; Radder 2009; Illies & Meijers 2009; Peterson &\nSpahn 2011). One might, for example, claim that technology enables (or\neven invites) and constrains (or even inhibits) certain human actions\nand the attainment of certain human goals and therefore is to some\nextent value-laden, without claiming moral agency for technological\nartifacts. A good overview of the debate can be found in Kroes and\nVerbeek 2014. \nThe debate about moral agency and technology is now particularly\nsalient with respect to the design of intelligent artificial agents.\nJames Moor (2006) has distinguished between four ways in which\nartificial agents may be or become moral agents: \nIt might perhaps never be possible to technologically design full\nethical agents, and if it were to become possible it might be\nquestionable whether it is morally desirable to do so (Bostrom &\nYudkowsky 2014). As Wallach and Allen (2009) have pointed out, the\nmain problem might not be to design artificial agents that can\nfunction autonomously and that can adapt themselves in interaction\nwith the environment, but rather to build enough, and the right kind\nof, ethical sensitivity into such machines. \nResponsibility has always been a central theme in the ethics of\ntechnology. The traditional philosophy and ethics of technology,\nhowever, tended to discuss responsibility in rather general terms and\nwere rather pessimistic about the possibility of engineers to assume\nresponsibility for the technologies they developed. Ellul, for\nexample, has characterized engineers as the high priests of\ntechnology, who cherish technology but cannot steer it. Hans Jonas\n(1979 [1984]) has argued that technology requires an ethics in which\nresponsibility is the central imperative because for the first time in\nhistory we are able to destroy the earth and humanity. \nIn engineering ethics, the responsibility of engineers is often\ndiscussed in relation to code of ethics that articulate specific\nresponsibilities of engineers. Such codes of ethics stress three types\nof responsibilities of engineers: (1) conducting the profession with\nintegrity and honesty and in a competent way, (2) responsibilities\ntowards employers and clients and (3) responsibility towards the\npublic and society. With respect to the latter, most US codes of\nethics maintain that engineers ‘should hold paramount the\nsafety, health and welfare of the public’. \nAs has been pointed out by several authors (Nissenbaum 1996; Johnson\n& Powers 2005; Swierstra & Jelsma 2006), it may be hard to\npinpoint individual responsibility in engineering. The reason is that\nthe conditions for the proper attribution of individual responsibility\nthat have been discussed in the philosophical literature (like freedom\nto act, knowledge, and causality) are often not met by individual\nengineers. For example, engineers may feel compelled to act in a\ncertain way due to hierarchical or market constraints, and negative\nconsequences may be very hard or impossible to predict beforehand. The\ncausality condition is often difficult to meet as well due to the long\nchain from research and development of a technology till its use and\nthe many people involved in this chain. Davis (2012) nevertheless\nmaintains that despite such difficulties individual engineers can and\ndo take responsibility. \nOne issue that is at stake in this debate is the notion of\nresponsibility. Davis (2012), and also for example Ladd (1991), argue\nfor a notion of responsibility that focuses less on blame and stresses\nthe forward-looking or virtuous character of assuming responsibility.\nBut many others focus on backward-looking notions of responsibility\nthat stress accountability, blameworthiness or liability. Zandvoort\n(2000), for example has pleaded for a notion of responsibility in\nengineering that is more like the legal notion of strict liability, in\nwhich the knowledge condition for responsibility is seriously\nweakened. Doorn (2012) compares three perspectives on responsibility\nascription in engineering—a merit-based, a right-based and a\nconsequentialist perspective—and argues that the\nconsequentialist perspective, which applies a forward-looking notion\nof responsibility, is most powerful in influencing engineering\npractice. \nThe difficulty of attributing individual responsibility may lead to\nthe Problem of Many Hands (PMH). The term was first coined by Dennis\nThompson (1980) in an article about the responsibility of public\nofficials. The term is used to describe problems with the ascription\nof individual responsibility in collective settings. Doorn (2010) has\nproposed a procedurals approach, based on Rawls’ reflective\nequilibrium model, to deal with the PMH; other ways of dealing with\nthe PMH include the design of institutions that help to avoid it or an\nemphasis on virtuous behavior in organizations (van de Poel, Royakers,\n& Zwart 2015). \nIn the last decades, increasingly attention is paid not only to\nethical issues that arise during the use of a technology, but also\nduring the design phase. An important consideration behind this\ndevelopment is the thought that during the design phase technologies,\nand their social consequences, are still malleable whereas during the\nuse phase technologies are more or less given and negative social\nconsequences may be harder to avoid or positive effects harder to\nachieve. \nIn computer ethics, an approach known as Value Sensitive Design (VSD)\nhas been developed to explicitly address the ethical nature of design.\nVSD aims at integrating values of ethical importance in engineering\ndesign in a systematic way (Friedman & Kahn 2003). The approach\ncombines conceptual, empirical and technical investigations. There is\nalso a range of other approaches aimed at including values in design.\n‘Design for X’ approaches in engineering aim at including\ninstrumental values (like maintainability, reliability and costs) but\nthey also include design for sustainability, inclusive design, and\naffective design (Holt & Barnes 2010). Inclusive design aims at\nmaking designs accessible to the whole population including, for\nexample, handicapped people and the elderly (Erlandson 2008).\nAffective design aims at designs that evoke positive emotions with the\nusers and so contributes to human well-being. Van de Hoven, Vermaas,\nand van de Poel 2015 gives a good overview of the state-of-the art of\nvalue sensitive design for various values and application domains. \nIf one tries to integrate values into design one may run into the\nproblem of a conflict of values. The safest car is, due to its weight,\nnot likely to be the most sustainability. Here safety and\nsustainability conflict in the design of cars. Traditional methods in\nwhich engineers deal with such conflicts and make trade-off between\ndifferent requirements for design include cost-benefit analysis and\nmultiple criteria analysis. Such methods are, however, beset with\nmethodological problems like those discussed in\n Section 2.4\n (Franssen 2005; Hansson 2007). Van de Poel (2009) discusses various\nalternatives for dealing with value conflicts in design including the\nsetting of thresholds (satisficing), reasoning about values,\ninnovation and diversity. \nThe risks of technology are one of the traditional ethical concerns in\nthe ethics of technology. Risks raise not only ethical issues but\nother philosophical issues, such as epistemological and\ndecision-theoretical issues as well (Roeser et al. 2012). \nRisk is usually defined as the product of the probability of an\nundesirable event and the effect of that event, although there are\nalso other definitions around (Hansson 2004b). In general it seems\ndesirable to keep technological risks as small as possible. The larger\nthe risk, the larger either the likeliness or the impact of an\nundesirable event is. Risk reduction therefore is an important goal in\ntechnological development and engineering codes of ethics often\nattribute a responsibility to engineers in reducing risks and\ndesigning safe products. Still, risk reduction is not always feasible\nor desirable. It is sometimes not feasible, because there are no\nabsolutely safe products and technologies. But even if risk reduction\nis feasible it may not be acceptable from a moral point of view.\nReducing risk often comes at a cost. Safer products may be more\ndifficult to use, more expensive or less sustainable. So sooner or\nlater, one is confronted with the question: what is safe enough? What\nmakes a risk (un)acceptable? \nThe process of dealing with risks is often divided into three stages:\nrisk assessment, risk evaluation and risk management. Of these, the\nsecond is most obviously ethically relevant. However, risk assessment\nalready involves value judgments, for example about which risks should\nbe assessed in the first place (Shrader-Frechette 1991). An important,\nand morally relevant, issue is also the degree of evidence that is\nneeded to establish a risk. In establishing a risk on the basis of a\nbody of empirical data one might make two kinds of mistakes. One can\nestablish a risk when there is actually none (type I error) or one can\nmistakenly conclude that there is no risk while there actually is a\nrisk (type II error). Science traditionally aims at avoiding type I\nerrors. Several authors have argued that in the specific context of\nrisk assessment it is often more important to avoid type II errors\n(Cranor 1990; Shrader-Frechette 1991). The reason for this is that\nrisk assessment not just aims at establishing scientific truth but has\na practical aim, i.e., to provide the knowledge on basis of which\ndecisions can be made about whether it is desirable to reduce or avoid\ncertain technological risks in order to protect users or the\npublic. \nRisk evaluation is carried out in a number of ways (see, e.g.,\nShrader-Frechette 1985). One possible approach is to judge the\nacceptability of risks by comparing them to other risks or to certain\nstandards. One could, for example, compare technological risks with\nnaturally occurring risks. This approach, however, runs the danger of\ncommitting a naturalistic fallacy: naturally occurring risks may\n(sometimes) be unavoidable but that does not necessarily make them\nmorally acceptable. More generally, it is often dubious to judge the\nacceptability of the risk of technology A by comparing it to the risk\nof technology B if A and B are not alternatives in a decision (for\nthis and other fallacies in reasoning about risks, see Hansson\n2004a). \nA second approach to risk evaluation is risk-cost benefit analysis,\nwhich is based on weighing the risks against the benefits of an\nactivity. Different decision criteria can be applied if a (risk) cost\nbenefit analysis is carried out (Kneese, Ben-David, and Schulze 1983).\nAccording to Hansson (2003: 306), usually the following criterion is\napplied:  \n… a risk is acceptable if and only if the total benefits that\nthe exposure gives rise to outweigh the total risks, measured as the\nprobability-weighted disutility of outcomes. \nA third approach is to base risk acceptance on the consent of people\nwho suffer the risks after they have been informed about these risks\n(informed consent). A problem of this approach is that technological\nrisks usually affect a large number of people at once. Informed\nconsent may therefore lead to a “society of stalemates”\n(Hansson 2003: 300). \nSeveral authors have proposed alternatives to the traditional\napproaches of risk evaluation on the basis of philosophical and\nethical arguments. Shrader-Frechette (1991) has proposed a number of\nreforms in risk assessment and evaluation procedures on the basis of a\nphilosophical critique of current practices. Roeser (2012) argues for\na role of emotions in judging the acceptability of risks. Hansson has\nproposed the following alternative principle for risk evaluation:  \nExposure of a person to a risk is acceptable if and only if this\nexposure is part of an equitable social system of risk-taking that\nworks to her advantage. (Hansson 2003: 305)  \nHansson’s proposal introduces a number of moral considerations\nin risk evaluation that are traditionally not addressed or only\nmarginally addressed. These are the consideration whether individuals\nprofit from a risky activity and the consideration whether the\ndistribution of risks and benefits is fair. \nSome authors have criticized the focus on risks in the ethics of\ntechnology. One strand of criticism argues that we often lack the\nknowledge to reliably assess the risks of a new technology before it\nhas come into use. We often do not know the probability that something\nmight go wrong, and sometimes we even do not know, or at least not\nfully, what might go wrong and what possible negative consequences may\nbe. To deal with this, some authors have proposed to conceive of the\nintroduction of new technology in society as a social experiment and\nhave urged to think about the conditions under which such experiments\nare morally acceptable (Martin & Schinzinger 2005; van de Poel\n2016). Another strand of criticism states that the focus on risks has\nled to a reduction of the impacts of technology that are considered\n(Swierstra & te Molder 2012). Only impacts related to safety and\nhealth, which can be calculated as risks, are considered, whereas\n‘soft’ impacts, for example of a social or psychological\nnature, are neglected, thereby impoverishing the moral evaluation of\nnew technologies.","contact.mail":"m.p.m.franssen@tudelft.nl","contact.domain":"tudelft.nl"},{"date.published":"2009-02-20","date.changed":"2018-09-06","url":"https://plato.stanford.edu/entries/technology/","author1":"Maarten Franssen","author1.info":"http://www.tbm.tudelft.nl/en/about-faculty/departments/values-technology-and-innovation/sections/ethicsphilosophy-of-technology/staff/dr-mpm-maarten-franssen/","author2.info":"http://gjclokhorst.nl/","entry":"technology","body.text":"\n\n\nIf philosophy is the attempt “to understand how things in the\nbroadest possible sense of the term hang together in the broadest\npossible sense of the term”, as Sellars (1962) put it,\nphilosophy should not ignore technology. It is largely by technology\nthat contemporary society hangs together. It is hugely important not\nonly as an economic force but also as a cultural force. Indeed during\nthe last two centuries, when it gradually emerged as a discipline,\nphilosophy of technology has mostly been concerned with the meaning of\ntechnology for, and its impact on, society and culture, rather than\nwith technology itself. Mitcham (1994) calls this type of philosophy\nof technology “humanities philosophy of technology”\nbecause it accepts “the primacy of the humanities over\ntechnologies” and is continuous with the overall perspective of\nthe humanities (and some of the social sciences). Only recently a\nbranch of the philosophy of technology has developed that is concerned\nwith technology itself and that aims to understand both the practice\nof designing and creating artifacts (in a wide sense, including\nartificial processes and systems) and the nature of the things so\ncreated. This latter branch of the philosophy of technology seeks\ncontinuity with the philosophy of science and with several other\nfields in the analytic tradition in modern philosophy, such as the\nphilosophy of action and decision-making, rather than with the\nhumanities and social science.\n\n\nThe entry starts with a brief historical overview, then continues with\na presentation of the themes on which modern analytic philosophy of\ntechnology focuses. This is followed by a discussion of the societal\nand ethical aspects of technology, in which some of the concerns of\nhumanities philosophy of technology are addressed. This twofold\npresentation takes into consideration the development of technology as\nthe outcome of a process originating within and guided by the practice\nof engineering, by standards on which only limited societal control is\nexercised, as well as the consequences for society of the\nimplementation of the technology so created, which result from\nprocesses upon which only limited control can be exercised.\n\n\n\n\n\n\n\nPhilosophical reflection on technology is about as old as philosophy\nitself. Our oldest testimony is from ancient Greece. There are four\nprominent themes. One early theme is the thesis that technology learns\nfrom or imitates nature (Plato, Laws X 899a ff.). According\nto Democritus, for example, house-building and weaving were first\ninvented by imitating swallows and spiders building their nests and\nnets, respectively (Diels 1903 and Freeman 1948: 154).  Perhaps the\noldest extant source for the exemplary role of nature is Heraclitus\n(Diels 1903 and Freeman 1948: 112).  Aristotle referred to this\ntradition by repeating Democritus’ examples, but he did not\nmaintain that technology can only imitate nature: “generally\ntechnè in some cases completes what nature cannot\nbring to a finish, and in others imitates nature”\n(Physics II.8, 199a15; see also Physics II.2, and\nsee Schummer 2001 and this encyclopedia’s entry on\n episteme and techne\n for discussion). \nA second theme is the thesis that there is a fundamental ontological\ndistinction between natural things and artifacts. According to\nAristotle (Physics II.1), the former have their principles of\ngeneration and motion inside, whereas the latter, insofar as they are\nartifacts, are generated only by outward causes, namely human aims and\nforms in the human soul. Natural products (animals and their parts,\nplants, and the four elements) move, grow, change, and reproduce\nthemselves by inner final causes; they are driven by purposes of\nnature. Artifacts, on the other hand, cannot reproduce themselves.\nWithout human care and intervention, they vanish after some time by\nlosing their artificial forms and decomposing into (natural)\nmaterials. For instance, if a wooden bed is buried, it decomposes to\nearth or changes back into its botanical nature by putting forth a\nshoot. \nThe thesis that there is a fundamental difference between man-made\nproducts and natural substances has had a long-lasting influence. In\nthe Middle Ages, Avicenna criticized alchemy on the ground that it can\nnever produce ‘genuine’ substances\n(Briffault 1930: 147). Even today, some still\nmaintain that there is a difference between, for example, natural and\nsynthetic vitamin C. The modern discussion of this theme is taken up\nin\n Section 2.5. \nAristotle’s doctrine of the four causes—material, formal,\nefficient and final—can be regarded as a third early\ncontribution to the philosophy of technology. Aristotle explained this\ndoctrine by referring to technical artifacts such as houses and\nstatues (Physics II.3). The four causes are still very much\npresent in modern discussions related to the metaphysics of artifacts.\nDiscussions of the notion of function, for example, focus on its\ninherent teleological or ‘final’ character and the\ndifficulties this presents to its use in biology. And the notorious\ncase of the ship of Theseus—see this encyclopedia’s\nentries on\n material constitution,\n identity over time,\n relative identity,\n and\n sortals—was\n introduced in modern philosophy by Hobbes as showing a conflict\nbetween unity of matter and unity of form as principles of\nindividuation. This conflict is seen by many as characteristic of\nartifacts. David Wiggins (1980: 89) takes it even to be the defining\ncharacteristic of artifacts. \nA fourth point that deserves mentioning is the extensive employment of\ntechnological images by Plato and Aristotle. In his Timaeus,\nPlato described the world as the work of an Artisan, the Demiurge. His\naccount of the details of creation is full of images drawn from\ncarpentry, weaving, ceramics, metallurgy, and agricultural technology.\nAristotle used comparisons drawn from the arts and crafts to\nillustrate how final causes are at work in natural processes. Despite\ntheir negative appreciation of the life led by artisans, who they\nconsidered too much occupied by the concerns of their profession and\nthe need to earn a living to qualify as free individuals, both Plato\nand Aristotle found technological imagery indispensable for expressing\ntheir belief in the rational design of the universe (Lloyd 1973:\n61). \nAlthough there was much technological progress in the Roman empire and\nduring the Middle Ages, philosophical reflection on technology did not\ngrow at a corresponding rate. Comprehensive works such as\nVitruvius’ De architectura (first century BC) and\nAgricola’s De re metallica (1556) paid much attention\nto practical aspects of technology but little to philosophy. \nIn the realm of scholastic philosophy, there was an emergent\nappreciation for the mechanical arts. They were generally considered\nto be born of—and limited to—the mimicry of nature. This\nview was challenged when alchemy was introduced in the Latin West\naround the mid-twelfth century. Some alchemical writers such as Roger\nBacon were willing to argue that human art, even if learned by\nimitating natural processes, could successfully reproduce natural\nproducts or even surpass them (Newman 2004). The result was a philosophy of technology in which\nhuman art was raised to a level of appreciation not found in other\nwritings until the Renaissance. However, the last three decades of the\nthirteenth century witnessed an increasingly hostile attitude by\nreligious authorities toward alchemy that culminated eventually in the\ndenunciation Contra alchymistas, written by the inquisitor\nNicholas Eymeric in 1396 (Newman 2004). \nThe Renaissance led to a greater appreciation of human beings and\ntheir creative efforts, including technology. As a result,\nphilosophical reflection on technology and its impact on society\nincreased. Francis Bacon is generally regarded as the first modern\nauthor to put forward such reflection. His view, expressed in his\nfantasy New Atlantis (1627), was overwhelmingly positive.\nThis positive attitude lasted well into the nineteenth century,\nincorporating the first half-century of the industrial revolution. \nFor example, Karl Marx did not condemn the steam engine or the\nspinning mill for the vices of the bourgeois mode of production; he\nbelieved that ongoing technological innovation were necessary steps\ntoward the more blissful stages of socialism and communism of the\nfuture (see Bimber 1990 for a discussion of different views on the\nrole of technology in Marx’s theory of historical development,\nand see Van der Pot 1985 [1994/2004] for an extensive historical\noverview of appreciations of the development of technology). \nA turning point in the appreciation of technology as a socio-cultural\nphenomenon is marked by Samuel Butler’s Erewhon (1872),\nwritten under the influence of the Industrial Revolution, and\nDarwin’s On the Origin of Species (1859). Butler’s book gave an\naccount of a fictional country where all machines are banned and the\npossession of a machine or the attempt to build one is a capital\ncrime. The people of this country had become convinced by an argument\nthat ongoing technical improvements are likely to lead to a\n‘race’ of machines that will replace mankind as the\ndominant species on earth. \nDuring the last quarter of the nineteenth century and most of the\ntwentieth century a critical attitude predominated in philosophical\nreflection on technology. The representatives of this attitude were,\noverwhelmingly, schooled in the humanities or the social sciences and\nhad virtually no first-hand knowledge of engineering practice. Whereas\nBacon wrote extensively on the method of science and conducted\nphysical experiments himself, Butler, being a clergyman, lacked such\nfirst-hand knowledge. Ernst Kapp, who was the first to use the term\n‘philosophy of technology’ in his book Eine\nPhilosophie der Technik (1877 [2018]), was a philologist and\nhistorian.  Most of the authors who wrote critically about technology\nand its socio-cultural role during the twentieth century were\nphilosophers of a general outlook, such as Martin Heidegger (1954\n[1977]), Hans Jonas (1979 [1984]), Arnold Gehlen (1957 [1980]),\nGünther Anders (1956), and Andrew Feenberg (1999). Others had a\nbackground in one of the other humanities or in social science, such\nas literary criticism and social research in the case of Lewis Mumford\n(1934), law in the case of Jacques Ellul (1954 [1964]), political\nscience in the case of Langdon Winner (1977, 1980, 1983) and literary\nstudies in the case of Albert Borgmann (1984). The form of philosophy\nof technology constituted by the writings of these and others has been\ncalled by Carl Mitcham (1994) “humanities philosophy of\ntechnology”, because it takes its point of departure from the\nsocial sciences and the humanities rather than from the practice of\ntechnology, and it approaches technology accepting “the primacy\nof the humanities over technologies” (1994: 39), since\ntechnology originates from the goals and values of humans.\n \nHumanities philosophers of technology tend to take the phenomenon of\ntechnology itself largely for granted; they treat it as a ‘black\nbox’, a given, a unitary, monolithic, inescapable phenomenon.\nTheir interest is not so much to analyze and understand this\nphenomenon itself but to grasp its relations to morality (Jonas,\nGehlen), politics (Winner), the structure of society (Mumford), human\nculture (Ellul), the human condition (Hannah Arendt), or metaphysics\n(Heidegger). In this, these philosophers are almost all openly\ncritical of technology: all things considered, they tend to have a\nnegative judgment of the way technology has affected human society and\nculture, or at least they single out for consideration the negative\neffects of technology on human society and culture. This does not\nnecessarily mean that technology itself is pointed out as the\nprincipal cause of these negative developments. In the case of\nHeidegger, in particular, the paramount position of technology in\nmodern society is rather a symptom of something more fundamental,\nnamely a wrongheaded attitude towards Being which has been on the rise\nfor almost 25 centuries. It is therefore questionable whether\nHeidegger should be considered as a philosopher of technology,\nalthough within the traditional view he is considered to be among the\nmost important ones. Much the same could be said about Arendt, in\nparticular her discussion of technology in The Human\nCondition (1958), although her position in the canon of\nhumanities philosophy of technology is not as prominent. \nTo be sure, the work of these founding figures of humanities\nphilosophy of technology has been taken further by a second and third\ngeneration of scholars—in particular the work of Heidegger\nremains an important source of inspiration—but who in doing so\nhave adopted a more neutral rather than overall negative view of\ntechnology and its meaning for human life and culture. Notable\nexamples are Ihde (1979, 1993) and Verbeek (2000 [2005]). \nIn its development, humanities philosophy of technology continues to\nbe influenced not so much by developments in philosophy (e.g.,\nphilosophy of science, philosophy of action, philosophy of mind) but\nby developments in the social sciences and humanities. Although, for\nexample, Ihde and those who take their point of departure with him,\nposition their work as phenomenologist or postphenomenologist, there\ndoes not seem to be much interest in either the past or the present of\nthis diffuse notion in philosophy, and in particular not much interest\nin the far from easy question to what extent Heidegger can be\nconsidered a phenomenologist. Of particular significance has been the\nemergence of ‘Science and Technology Studies’ (STS) in the\n1980s, which studies from a broad social-scientific perspective how\nsocial, political, and cultural values affect scientific research and\ntechnological innovation, and how these in turn affect society,\npolitics, and culture. We discuss authors from humanities philosophy\nof technology in\n Section 3\non ‘Ethical and Social Aspects of Technology’, but do not\npresent separately and in detail the wide variety of views existing in\nthis field. For a detailed treatment Mitcham’s 1994 book\nprovides an excellent overview. Olsen, Selinger and Riis (2008) offer\na collection of more recent contributions; Scharff and Dusek (2003\n[2014]) and Kaplan (2004 [2009]) present comprehensive anthologies of\ntexts from this tradition. \nMitcham contrasts ‘humanities philosophy of technology’ to\n‘engineering philosophy of technology’, where the latter\nrefers to philosophical views developed by engineers or technologists\nas “attempts … to elaborate a technological\nphilosophy” (1994: 17). Mitcham discusses only a handful of\npeople as engineering philosophers of technology, however: Ernst Kapp,\nPeter Engelmeier, Friedrich Dessauer, and much more briefly Jacques\nLafitte, Gilbert Simondon, Hendrik van Riessen, Juan David\nGarcía Bacca, R. Buckminster Fuller and Mario Bunge. The label\nraises serious questions, however: several of them hardly classify as\n‘engineers or technologists’ and it is also not very clear\nhow the notion of ‘a technological philosophy’ should be\nunderstood. As philosophers these authors seem all to be rather\nisolated figures, whose work shows little overlap and who seem to be\nsharing mainly the absence of a ‘working relation’ with\nestablished philosophical disciplines. It is not so clear what sort of\nquestions and concerns underlie the notion of ‘engineering\nphilosophy of technology’. A larger role for systematic\nphilosophy could bring it quite close to some examples of humanities\nphilosophy of technology, for instance the work of Jacques Ellul,\nwhere the analyses would be rather similar and the remaining\ndifferences would be ones of attitude or appreciation. \nIn the next section we discuss in more detail a form of philosophy of\ntechnology that we consider to occupy, currently, the position of\nalternative to the humanities philosophy of technology. It emerged in\nthe 1960s and gained momentum in the past fifteen to twenty years.\nThis form of the philosophy of technology, which may be called\n‘analytic’, is not primarily concerned with the relations\nbetween technology and society but with technology itself. It\nexpressly does not look upon technology as a ‘black box’\nbut as a phenomenon that should be studied in detail. It regards\ntechnology perhaps not in its entirety as a practice but as something\ngrounded in a practice, basically the practice of engineering. It\nanalyses this practice, its goals, its concepts and its methods, and\nit relates its findings to various themes from philosophy. \nIn focusing on technology as a practice sustained by engineers,\nsimilar to the way philosophy of science focuses on the practice of\nscience as sustained by scientists, analytic philosophy of technology\ncould be thought to amount to the philosophy of engineering. Indeed\nmany of the issues related to design, discussed below in Sections\n 2.3\n and\n 2.4,\n could be singled out as forming the subject matter of the philosophy\nof engineering. The metaphysical issues discussed in Section\n 2.5\n could not, however, and analytic philosophy of technology is\ntherefore significantly broader than philosophy of engineering. The\nvery title of Philosophy of Technology and Engineering\nSciences (Meijers 2009), an extensive up-to-date overview, which\ncontains contributions to all of the topics treated in the next\nsection, expresses the view that technology and engineering do not\ncoincide. Which is not to say, however, that the book offers a clear\nconception of what makes technology different from engineering, or\nmore than engineering. In fact, the existence of humanities philosophy\nof technology and analytic philosophy of technology next to each other\nreflects a basic ambiguity in the notion of technology that the\nphilosophical work that has been going on has not succeeded in\nclarifying. \nTechnology can be said to have two ‘cores’ or\n‘dimensions’, which can be referred to as\ninstrumentality and productivity. Instrumentality\ncovers the totality of human endeavours to control their lives and\ntheir environments by interfering with the world in an instrumental\nway, by using things in a purposeful and clever way. Productivity\ncovers the totality of human endeavours to brings new things into\nexistence that can do certain things in a controlled and clever way.\nFor the study of instrumentality, however, it is in principle\nirrelevant whether or not the things that are made use of in\ncontrolling our lives and environments have been made by us first; if\nwe somehow could rely on natural objects to always be available to\nserve our purposes, the analysis of instrumentality and its\nconsequences for how we live our lives would not necessarily be\naffected. Likewise, for the analysis of what is involved in the making\nof artifacts, and how the notion of artifact and of something new\nbeing brought into existence are to be understood, it is to a large\nextent irrelevant how human life, culture and society are changed as a\nresult of the artifacts that are in fact produced. Clearly, humanities\nphilosophy of technology has until now been more attracted by the\ninstrumentality core whereas analytic philosophy of technology has\nmainly gone for the productivity core. But technology as one of the\nbasic phenomena of modern society, if not the most basic one, clearly\nis constituted by the processes centering on and involving both cores.\nIt has proved difficult, however, to come to an overarching approach\nin which the interaction between these two dimensions of technology\nare adequately dealt with—no doubt partly due to the great\ndifferences in philosophical orientation and methodology associated\nwith the two traditions and their separate foci. To improve this\nsituation is arguably the most urgent challenge that the field of\nphilosophy of technology as a whole is facing, since the continuation\nof the two orientations leading their separate lives threatens its\nunity and coherence as a discipline in the first place.\nNotwithstanding its centrality and urgency, the ambiguity noted here\nseems hardly to be confronted directly in the literature. It is\naddressed by Lawson (2008, 2017) and by Franssen and Koller\n(2016). \nAfter presenting the major issues of philosophical relevance in\ntechnology and engineering that are studied by analytic philosophers\nof technology in the next section, we discuss the problems and\nchallenges that technology poses for the society in which it is\npracticed in the third and final section. \nIt may come as a surprise to those new to the topic that the fields of\nphilosophy of science and philosophy of technology show such great\ndifferences, given that few practices in our society are as closely\nrelated as science and technology. Experimental science is nowadays\ncrucially dependent on technology for the realization of its research\nset-ups and for gathering and analyzing data. The phenomena that\nmodern science seeks to study could never be discovered without\nproducing them through technology. \nTheoretical research within technology has come to be often\nindistinguishable from theoretical research in science, making\nengineering science largely continuous with ‘ordinary’ or\n‘pure’ science. This is a relatively recent development,\nwhich started around the middle of the nineteenth century, and is\nresponsible for great differences between modern technology and\ntraditional, craft-like techniques. The educational training that\naspiring scientists and engineers receive starts off being largely\nidentical and only gradually diverges into a science or an engineering\ncurriculum. Ever since the scientific revolution of the seventeenth\ncentury, characterized by its two major innovations, the experimental\nmethod and the mathematical articulation of scientific theories,\nphilosophical reflection on science has focused on the method by which\nscientific knowledge is generated, on the reasons for thinking\nscientific theories to be true, or approximately true, and on the\nnature of evidence and the reasons for accepting one theory and\nrejecting another. Hardly ever have philosophers of science posed\nquestions that did not have the community of scientists, their\nconcerns, their aims, their intuitions, their arguments and choices,\nas a major target. In contrast it is only recently that the philosophy\nof technology has discovered the community of engineers. \nIt might be claimed that it is up to the philosophy of technology, and\nnot the philosophy of science, to target first of all the impact of\ntechnology—and with it science—on society and culture,\nbecause science affects society only through technology. This,\nhowever, will not do. Right from the start of the scientific\nrevolution, science affected human culture and thought fundamentally\nand directly, not with a detour through technology, and the same is\ntrue for later developments such as relativity, atomic physics and\nquantum mechanics, the theory of evolution, genetics, biochemistry,\nand the increasingly dominating scientific world view overall.\nPhilosophers of science overwhelmingly give the impression that they\nleave questions addressing the normative, social and cultural aspects\nof science gladly to other philosophical disciplines, or to historical\nstudies. There are exceptions, however, and things may be changing;\nPhilip Kitcher, to name but one prominent philosopher of science, has\nsince 2000 written books on the relation of science to politics,\nethics and religion (Kitcher 2001, 2011). \nThere is a major difference between the historical development of\nmodern technology as compared to modern science which may at least\npartly explain this situation, which is that science emerged in the\nseventeenth century from philosophy itself. The answers that Galileo,\nHuygens, Newton, and others gave, by which they initiated the alliance\nof empiricism and mathematical description that is so characteristic\nof modern science, were answers to questions that had belonged to the\ncore business of philosophy since antiquity. Science, therefore, kept\nthe attention of philosophers. Philosophy of science is a\ntransformation of epistemology in the light of the emergence of\nscience. The foundational issues—the reality of atoms, the\nstatus of causality and probability, questions of space and time, the\nnature of the quantum world—that were so lively discussed during\nthe end of the nineteenth and the beginning of the twentieth century\nare an illustration of this close relationship between scientists and\nphilosophers. No such intimacy has ever existed between those same\nphilosophers and technologists; their worlds still barely touch. To be\nsure, a case can be made that, compared to the continuity existing\nbetween natural philosophy and science, a similar continuity exists\nbetween central questions in philosophy having to do with human action\nand practical rationality and the way technology approaches and\nsystematizes the solution of practical problems. To investigate this\nconnection may indeed be considered a major theme for philosophy of\ntechnology, and more is said on it in Sections\n 2.3\n and\n 2.4.\n This continuity appears only by hindsight, however, and dimly, as the\nhistorical development is at most a slow convening of various strands\nof philosophical thinking on action and rationality, not a development\ninto variety from a single origin. Significantly it is only the\nacademic outsider Ellul who has, in his idiosyncratic way, recognized\nin technology the emergent single dominant way of answering all\nquestions concerning human action, comparable to science as the single\ndominant way of answering all questions concerning human knowledge\n(Ellul 1954 [1964]). But Ellul was not so much interested in\ninvestigating this relationship as in emphasizing and denouncing the\nsocial and cultural consequences as he saw them. It is all the more\nimportant to point out that humanities philosophy of technology cannot\nbe differentiated from analytic philosophy of technology by claiming\nthat only the former is interested in the social environment of\ntechnology. There are studies which are rooted in analytic philosophy\nof science but address specifically the relation of technology to\nsociety and culture, and equally the relevance of social relations to\npractices of technology, without taking an evaluative stand with\nrespect to technology; an example is B. Preston 2012. \nThe close relationship between the practices of science and technology\nmay easily keep the important differences between the two from view.\nThe predominant position of science in the philosophical field of\nvision made it difficult for philosophers to recognize that technology\nmerits special attention for involving issues that do not emerge in\nscience. This view resulting from this lack of recognition is often\npresented, perhaps somewhat dramatically, as coming down to a claim\nthat technology is ‘merely’ applied science. \nA questioning of the relation between science and technology was the\ncentral issue in one of the earliest discussions among analytic\nphilosophers of technology. In 1966, in a special issue of the journal\nTechnology and Culture, Henryk Skolimowski argued that\ntechnology is something quite different from science (Skolimowski\n1966). As he phrased it, science concerns itself with what is, whereas\ntechnology concerns itself with what is to be. A few years later, in\nhis well-known book The Sciences of the Artificial (1969),\nHerbert Simon emphasized this important distinction in almost the same\nwords, stating that the scientist is concerned with how things are but\nthe engineer with how things ought to be. Although it is difficult to\nimagine that earlier philosophers were blind to this difference in\norientation, their inclination, in particular in the tradition of\nlogical empiricism, to view knowledge as a system of statements may\nhave led to a conviction that in technology no knowledge claims play a\nrole that cannot also be found in science. The study of technology,\ntherefore, was not expected to pose new challenges nor hold surprises\nregarding the interests of analytic philosophy. \nIn contrast, Mario Bunge (1966) defended the view that technology\nis applied science, but in a subtle way that does justice to\nthe differences between science and technology. Bunge acknowledges\nthat technology is about action, but an action heavily underpinned by\ntheory—that is what distinguishes technology from the arts and\ncrafts and puts it on a par with science. According to Bunge, theories\nin technology come in two types: substantive theories, which provide\nknowledge about the object of action, and operative theories, which\nare concerned with action itself. The substantive theories of\ntechnology are indeed largely applications of scientific theories. The\noperative theories, in contrast, are not preceded by scientific\ntheories but are born in applied research itself. Still, as Bunge\nclaims, operative theories show a dependence on science in that in\nsuch theories the method of science is employed. This\nincludes such features as modeling and idealization, the use of\ntheoretical concepts and abstractions, and the modification of\ntheories by the absorption of empirical data through prediction and\nretrodiction. \nIn response to this discussion, Ian Jarvie (1966) proposed as\nimportant questions for a philosophy of technology what the\nepistemological status of technological statements is and how\ntechnological statements are to be demarcated from scientific\nstatements. This suggests a thorough investigation of the various\nforms of knowledge occurring in either practice, in particular, since\nscientific knowledge has already been so extensively studied, of the\nforms of knowledge that are characteristic of technology and are\nlacking, or of much less prominence, in science. A distinction between\n‘knowing that’—traditional propositional\nknowledge—and ‘knowing how’—non-articulated\nand even impossible-to-articulate knowledge—had been introduced\nby Gilbert Ryle (1949) in a different context. The notion of\n‘knowing how’ was taken up by Michael Polanyi under the\nname of tacit knowledge and made a central characteristic of\ntechnology (Polanyi 1958); the current state of the philosophical\ndiscussion is presented in this encyclopedia’s entry on\n knowledge how.\n However, emphasizing too much the role of unarticulated knowledge, of\n‘rules of thumb’ as they are often called, easily\nunderplays the importance of rational methods in technology. An\nemphasis on tacit knowledge may also be ill-fit for distinguishing the\npractices of science and technology because the role of tacit\nknowledge in science may well be more important than current\nphilosophy of science acknowledges, for example in concluding causal\nrelationships on the basis of empirical evidence. This was also an\nimportant theme in the writings of Thomas Kuhn on theory change in\nscience (Kuhn 1962). \nTo claim, with Skolimowski and Simon, that technology is about what is\nto be or what ought to be rather than what is may serve to distinguish\nit from science but will hardly make it understandable why so much\nphilosophical reflection on technology has taken the form of\nsocio-cultural critique. Technology is an ongoing attempt to bring the\nworld closer to the way one wishes it to be. Whereas science aims to\nunderstand the world as it is, technology aims to change the world.\nThese are abstractions, of course. For one, whose wishes concerning\nwhat the world should be like are realized in technology? Unlike\nscientists, who are often personally motivated in their attempts at\ndescribing and understanding the world, engineers are seen, not in the\nleast by engineers themselves, as undertaking their attempts to change\nthe world as a service to the public. The ideas on what is to be or\nwhat ought to be are seen as originating outside of technology itself;\nengineers then take it upon themselves to realize these ideas. This\nview is a major source for the widely spread picture of technology as\nbeing instrumental, as delivering instruments ordered from\n‘elsewhere’, as means to ends specified outside of\nengineering, a picture that has served further to support the claim\nthat technology is neutral with respect to values, discussed\nin\n Section 3.3.1.\n This view involves a considerable distortion of reality, however.\nMany engineers are intrinsically motivated to change the world; in\ndelivering ideas for improvement they are, so to speak, their own best\ncustomers. The same is true for most industrial companies,\nparticularly in a market economy, where the prospect of great profits\nis another powerful motivator. As a result, much technological\ndevelopment is ‘technology-driven’. \nTo understand where technology ‘comes from’, what drives\nthe innovation process, is of importance not only to those who are\ncurious to understand the phenomenon of technology itself but also to\nthose who are concerned about its role in society. Technology or\nengineering as a practice is concerned with the creation of artifacts\nand, of increasing importance, artifact-based services. The design\nprocess, the structured process leading toward that goal, forms\nthe core of the practice of technology. In the engineering literature,\nthe design process is commonly represented as consisting of a series\nof translational steps; see for this, e.g., Suh 2001. At the start are\nthe customer’s needs or wishes. In the first step these are\ntranslated into a list of functional requirements, which then\ndefine the design task an engineer, or a team of engineers, has to\naccomplish. The functional requirements specify as precisely as\npossible what the device to be designed must be able to do. This step\nis required because customers usually focus on just one or two\nfeatures and are unable to articulate the requirements that are\nnecessary to support the functionality they desire. In the second\nstep, the functional requirements are translated into design\nspecifications, which the exact physical parameters of crucial\ncomponents by which the functional requirements are going to be met.\nThe design parameters chosen to satisfy these requirements are\ncombined and made more precise such that a blueprint of the\ndevice results. The blueprint contains all the details that must be\nknown such that the final step to the process of manufacturing the\ndevice can take place. It is tempting to consider the blueprint as the\nend result of a design process, instead of a finished copy being this\nresult. However, actual copies of a device are crucial for the purpose\nof prototyping and testing. Prototyping and testing presuppose that\nthe sequence of steps making up the design process can and will often\ncontain iterations, leading to revisions of the design parameters\nand/or the functional requirements. Even though, certainly for\nmass-produced items, the manufacture of a product for delivery to its\ncustomers or to the market comes after the closure of the design\nphase, the manufacturing process is often reflected in the functional\nrequirements of a device, for example in putting restrictions on the\nnumber of different components of which the device consists. The\ncomplexity of a device will affect how difficult it will be to\nmaintain or repair it, and ease of maintenance or low repair costs are\noften functional requirements. An important modern development is that\nthe complete life cycle of an artifact is now considered to be the\ndesigning engineer’s concern, up till the final stages of the\nrecycling and disposal of its components and materials, and the\nfunctional requirements of any device should reflect this. From this\npoint of view, neither a blueprint nor a prototype can be considered\nthe end product of engineering design. \nThe biggest idealization that this scheme of the design process\ncontains is arguably located at the start. Only in a minority of cases\ndoes a design task originate in a customer need or wish for a\nparticular artifact. First of all, as already suggested, many design\ntasks are defined by engineers themselves, for instance, by noticing\nsomething to be improved in existing products. But more often than not\ndesign starts with a problem pointed out by some societal agent, which\nengineers are then invited to solve. Many such problems, however, are\nill-defined or wicked problems, meaning that it is not at all\nclear what the problem is exactly and what a solution to the problem\nwould consist in. The ‘problem’ is a situation that\npeople—not necessarily the people ‘in’ the\nsituation—find unsatisfactory, but typically without being able\nto specify a situation that they find more satisfactory in other terms\nthan as one in which the problem has been solved. In particular it is\nnot obvious that a solution to the problem would consist in some\nartifact, or some artifactual system or process, being made available\nor installed. Engineering departments all over the world advertise\nthat engineering is problem solving, and engineers easily seem\nconfident that they are best qualified to solve a problem when they\nare asked to, whatever the nature of the problem. This has led to the\nphenomenon of a technological fix, the solution of a problem\nby a technical solution, that is, the delivery of an artifact or\nartifactual process, where it is questionable, to say the least,\nwhether this solves the problem or whether it was the best way of\nhandling the problem. \nA candidate example of a technological fix for the problem of global\nwarming would be the currently much debated option of injecting\nsulfate aerosols into the stratosphere to offset the warming effect of\ngreenhouse gases such as carbon dioxide and methane. Such schemes of\ngeoengineering would allow us to avoid facing the—in all\nlikelihood painful—choices that will lead to a reduction of the\nemission of greenhouse gases into the atmosphere, but will at the same\ntime allow the depletion of the Earth’s reservoir of fossil\nfuels to continue. See for a discussion of technological fixing, e.g.,\nVolti 2009: 26–32. Given this situation, and its hazards, the\nnotion of a problem and a taxonomy of problems deserve to receive more\nphilosophical attention than they have hitherto received. \nThese wicked problems are often broadly social problems, which would\nbest be met by some form of ‘social action’, which would\nresult in people changing their behavior or acting differently in such\na way that the problem would be mitigated or even disappear\ncompletely. In defense of the engineering view, it could perhaps be\nsaid that the repertoire of ‘proven’ forms of social\naction is meager. The temptation of technical fixes could be\novercome—at least that is how an engineer might see it—by\nthe inclusion of the social sciences in the systematic development and\napplication of knowledge to the solution of human problems. This\nhowever, is a controversial view. Social engineering is to\nmany a specter to be kept at as large a distance as possible instead\nof an ideal to be pursued. Karl Popper referred to acceptable forms of\nimplementing social change as ‘piecemeal social\nengineering’ and contrasted it to the revolutionary but\ncompletely unfounded schemes advocated by, e.g., Marxism. In the entry\non\n Karl Popper,\n however, his choice of words is called ‘rather\nunfortunate’. The notion of social engineering, and its cogency,\ndeserves more attention that it is currently receiving. \nAn important input for the design process is scientific knowledge:\nknowledge about the behavior of components and the materials they are\ncomposed of in specific circumstances. This is the point where science\nis applied. However, much of this knowledge is not directly available\nfrom the sciences, since it often concerns extremely detailed behavior\nin very specific circumstances. This scientific knowledge is therefore\noften generated within technology, by the engineering sciences. But\napart from this very specific scientific knowledge, engineering design\ninvolves various other sorts of knowledge. In his book What\nEngineers Know and How They Know It (Vincenti 1990), the\naeronautical engineer Walter Vincenti gave a six-fold categorization\nof engineering design knowledge (leaving aside production and\noperation as the other two basic constituents of engineering\npractice). Vincenti distinguishes \nThe fourth category concerns the quantitative knowledge just referred\nto, and the third the theoretical tools used to acquire it. These two\ncategories can be assumed to match Bunge’s notion of substantive\ntechnological theories. The status of the remaining four categories is\nmuch less clear, however, partly because they are less familiar, or\nnot at all, from the well-explored context of science. Of these\ncategories, Vincenti claims that they represent prescriptive forms of\nknowledge rather than descriptive ones. Here, the activity of design\nintroduces an element of normativity, which is absent from scientific\nknowledge. Take such a basic notion as ‘operational\nprinciple’, which refers to the way in which the function of a\ndevice is realized, or, in short, how it works. This is still a purely\ndescriptive notion. Subsequently, however, it plays a role in\narguments that seek to prescribe a course of action to someone who has\na goal that could be realized by the operation of such a device. At\nthis stage, the issue changes from a descriptive to a prescriptive or\nnormative one. An extensive discussion of the various kinds of\nknowledge relevant to technology is offered by Houkes (2009). \nAlthough the notion of an operational principle—a term that\nseems to originate with Polanyi (1958)—is central to engineering\ndesign, no single clear-cut definition of it seems to exist. The issue\nof disentangling descriptive from prescriptive aspects in an analysis\nof the technical action and its constituents is therefore a task that\nhas hardly begun. This task requires a clear view on the extent and\nscope of technology. If one follows Joseph Pitt in his book\nThinking About Technology (1999) and defines technology\nbroadly as ‘humanity at work’, then to distinguish between\ntechnological action and action in general becomes difficult, and the\nstudy of technological action must absorb all descriptive and\nnormative theories of action, including the theory of practical\nrationality, and much of theoretical economics in its wake. There have\nindeed been attempts at such an encompassing account of human action,\nfor example Tadeusz Kotarbinski’s Praxiology (1965),\nbut a perspective of such generality makes it difficult to arrive at\nresults of sufficient depth. It would be a challenge for philosophy to\nspecify the differences among action forms and the reasoning grounding\nthem in, to single out three prominent fields of study, technology,\norganization and management, and economics. \nA more restricted attempt at such an approach is Ilkka\nNiiniluoto’s (1993). According to Niiniluoto, the theoretical\nframework of technology as the practice that is concerned with what\nthe world should be like rather than is, the framework that forms the\ncounterpoint to the descriptive framework of science, is design\nscience. The content of design science, the counterpoint to the\ntheories and explanations that form the content of descriptive\nscience, would then be formed by technical norms, statements\nof the form ‘If one wants to achieve X, one should do\nY’. The notion of a technical norm derives from Georg\nHenrik von Wright’s Norm and Action (1963). Technical\nnorms need to be distinguished from anankastic statements expressing\nnatural necessity, of the form ‘If X is to be achieved,\nY needs to be done’; the latter have a truth value but\nthe former have not. Von Wright himself, however, wrote that he did\nnot understand the mutual relations between these statements. Ideas on\nwhat design science is and can and should be are evidently related to\nthe broad problem area of practical rationality—see this\nencyclopedia’s entries on\n practical reason\n and\n instrumental rationality—and\n also to means-ends reasoning, discussed in the next section. \nDesign is an activity that is subject to rational scrutiny but in\nwhich creativity is considered to play an important role as well.\nSince design is a form of action, a structured series of decisions to\nproceed in one way rather than another, the form of rationality that\nis relevant to it is practical rationality, the rationality\nincorporating the criteria on how to act, given particular\ncircumstances. This suggests a clear division of labor between the\npart to be played by rational scrutiny and the part to be played by\ncreativity. Theories of rational action generally conceive their\nproblem situation as one involving a choice among various course of\naction open to the agent. Rationality then concerns the question how\nto decide among given options, whereas creativity concerns the\ngeneration of these options. This distinction is similar to the\ndistinction between the context of justification and the context of\ndiscovery in science. The suggestion that is associated with this\ndistinction, however, that rational scrutiny only applies in the\ncontext of justification, is difficult to uphold for technological\ndesign. If the initial creative phase of option generation is\nconducted sloppily, the result of the design task can hardly be\nsatisfactory. Unlike the case of science, where the practical\nconsequences of entertaining a particular theory are not taken into\nconsideration, the context of discovery in technology is governed by\nsevere constraints of time and money, and an analysis of the problem\nhow best to proceed certainly seems in order. There has been little\nphilosophical work done in this direction; an overview of the issues\nis given in Kroes, Franssen, and Bucciarelli (2009). \nThe ideas of Herbert Simon on bounded rationality (see, e.g., Simon\n1982) are relevant here, since decisions on when to stop generating\noptions and when to stop gathering information about these options and\nthe consequences when they are adopted are crucial in decision making\nif informational overload and calculative intractability are to be\navoided. However, it has proved difficult to further develop\nSimon’s ideas on bounded rationality since their conception in\nthe 1950s. Another notion that is relevant here is means-ends\nreasoning. In order to be of any help here, theories of means-ends\nreasoning should then concern not just the evaluation of given means\nwith respect to their ability to achieve given ends, but also the\ngeneration or construction of means for given ends. A comprehensive\ntheory of means-ends reasoning, however, is not yet available; for a\nproposal on how to develop means-ends reasoning in the context of\ntechnical artifacts, see Hughes, Kroes, and Zwart 2007. In the\npractice of technology, alternative proposals for the realization of\nparticular functions are usually taken from ‘catalogs’ of\nexisting and proven realizations. These catalogs are extended by\nongoing research in technology rather than under the urge of\nparticular design tasks. \nWhen engineering design is conceived as a process of decision making,\ngoverned by considerations of practical rationality, the next step is\nto specify these considerations. Almost all theories of practical\nrationality conceive of it as a reasoning process where a match\nbetween beliefs and desires or goals is sought. The desires or goals\nare represented by their value or utility for the decision maker, and\nthe decision maker’s problem is to choose an action that\nrealizes a situation that, ideally, has maximal value or utility among\nall the situations that could be realized. If there is uncertainty\nconcerning he situations that will be realized by a particular action,\nthen the problem is conceived as aiming for maximal expected\nvalue or utility. Now the instrumental perspective on technology\nimplies that the value that is at issue in the design process viewed\nas a process of rational decision making is not the value of the\nartifacts that are created. Those values are the domain of the\nusers of the technology so created. They are supposed to be\nrepresented in the functional requirements defining the design task.\nInstead the value to be maximized is the extent to which a particular\ndesign meets the functional requirements defining the design task. It\nis in this sense that engineers share an overall perspective on\nengineering design as an exercise in optimization. But\nalthough optimization is a value-orientated notion, it is not itself\nperceived as a value driving engineering design. \nThe functional requirements that define most design problems do not\nprescribe explicitly what should be optimized; usually they set levels\nto be attained minimally. It is then up to the engineer to choose how\nfar to go beyond meeting the requirements in this minimal sense.\nEfficiency, in energy consumption and use of materials first\nof all, is then often a prime value. Under the pressure of society,\nother values have come to be incorporated, in particular\nsafety and, more recently, sustainability. Sometimes\nit is claimed that what engineers aim to maximize is just one factor,\nnamely market success. Market success, however, can only be assessed\nafter the fact. The engineer’s maximization effort will instead\nbe directed at what are considered the predictors of market success.\nMeeting the functional requirements and being relatively efficient and\nsafe are plausible candidates as such predictors, but additional\nmethods, informed by market research, may introduce additional factors\nor may lead to a hierarchy among the factors. \nChoosing the design option that maximally meets all the functional\nrequirements (which may but need not originate with the prospective\nuser) and all other considerations and criteria that are taken to be\nrelevant, then becomes the practical decision-making problem to be\nsolved in a particular engineering-design task. This creates several\nmethodological problems. Most important of these is that the engineer\nis facing a multi-criteria decision problem. The various\nrequirements come with their own operationalizations in terms of\ndesign parameters and measurement procedures for assessing their\nperformance. This results in a number of rank orders or quantitative\nscales which represent the various options out of which a choice is to\nbe made. The task is to come up with a final score in which all these\nresults are ‘adequately’ represented, such that the option\nthat scores best can be considered the optimal solution to the design\nproblem. Engineers describe this situation as one where\ntrade-offs have to be made: in judging the merit of one\noption relative to other options, a relative bad performance on one\ncriterion can be balanced by a relatively good performance on another\ncriterion. An important problem is whether a rational method for doing\nthis can be formulated. It has been argued by Franssen (2005) that\nthis problem is structurally similar to the well-known problem of\nsocial choice, for which Kenneth Arrow proved his notorious\nimpossibility theorem in 1950, implying that no general rational\nsolution method exists for this problem. This poses serious problems\nfor the claim of engineers that their designs are optimal solutions,\nsince Arrow’s theorem implies that in most multi-criteria\nproblems the notion of ‘optimal’ cannot be rigorously\ndefined. \nThis result seems to except a crucial aspect of engineering activity\nfrom philosophical scrutiny, and it could be used to defend the\nopinion that engineering is at least partly an art, not a science.\nInstead of surrendering to the result, however, which has a\nsignificance that extends much beyond engineering and even beyond\ndecision making in general, we should perhaps conclude instead that\nthere is still a lot of work to be done on what might be termed,\nprovisionally, ‘approximative’ forms of reasoning. One\nform of reasoning to be included here is Herbert Simon’s bounded\nrationality, plus the related notion of ‘satisficing’.\nSince their introduction in the 1950s (Simon 1957) these two terms\nhave found wide usage, but we are still lacking a general theory of\nbounded rationality. It may be in the nature of forms of approximative\nreasoning such as bounded rationality that a general theory cannot be\nhad, but even a systematic treatment from which such an insight could\nemerge seems to be lacking. \nAnother problem for the decision-making view of engineering design is\nthat in modern technology almost all design is done by teams. Such\nteams are composed of experts from many different disciplines. Each\ndiscipline has its own theories, its own models of interdependencies,\nits own assessment criteria, and so forth, and the professionals\nbelonging to these disciplines must be considered as inhabitants of\ndifferent object worlds, as Louis Bucciarelli (1994) phrases\nit. The different team members are, therefore, likely to disagree on\nthe relative rankings and evaluations of the various design options\nunder discussion. Agreement on one option as the overall best one can\nhere be even less arrived at by an algorithmic method exemplifying\nengineering rationality. Instead, models of social interaction, such\nas bargaining and strategic thinking, are relevant here. An example of\nsuch an approach to an (abstract) design problem is presented by\nFranssen and Bucciarelli (2004). \nTo look in this way at technological design as a decision-making\nprocess is to view it normatively from the point of view of practical\nor instrumental rationality. At the same time it is descriptive in\nthat it is a description of how engineering methodology generally\npresents the issue how to solve design problems. From that somewhat\nhigher perspective there is room for all kinds of normative questions\nthat are not addressed here, such as whether the functional\nrequirements defining a design problem can be seen as an adequate\nrepresentation of the values of the prospective users of an artifact\nor a technology, or by which methods values such as safety and\nsustainability can best be elicited and represented in the design\nprocess. These issues will be taken up in\n Section 3. \nUnderstanding the process of designing artifacts is the theme in\nphilosophy of technology that most directly touches on the interests\nof engineering practice. This is hardly true for another issue of\ncentral concern to analytic philosophy of technology, which is the\nstatus and the character of artifacts. This is perhaps not unlike the\nsituation in the philosophy of science, where working scientists seem\nalso to be much less interested in investigating the status and\ncharacter of models and theories than philosophers are. \nArtifacts are man-made objects: they have an author (see Hilpinen 1992\nand Hilpinen’s article\n artifact\n in this encyclopedia). The artifacts that are of relevance to\ntechnology are, in particular, made to serve a purpose. This excludes,\nwithin the set of all man-made objects, on the one hand byproducts and\nwaste products and on the other hand works of art. Byproducts and\nwaste products result from an intentional act to make something but\njust not precisely, although the author at work may be well aware of\ntheir creation. Works of art result from an intention directed at\ntheir creation (although in exceptional cases of conceptual art, this\ndirectedness may involve many intermediate steps) but it is contested\nwhether artists include in their intentions concerning their work an\nintention that the work serves some purpose. A further discussion of\nthis aspect belongs to the philosophy of art. An interesting general\naccount has been presented by Dipert (1993). \nTechnical artifacts, then, are made to serve some purpose, generally\nto be used for something or to act as a component in a larger\nartifact, which in its turn is either something to be used or again a\ncomponent. Whether end product or component, an artifact is ‘for\nsomething’, and what it is for is called the artifact’s\nfunction. Several researchers have emphasized that an\nadequate description of artifacts must refer both to their status as\ntangible physical objects and to the intentions of the people engaged\nwith them. Kroes and Meijers (2006) have dubbed this view “the\ndual nature of technical artifacts”; its most mature formulation\nis Kroes 2012. They suggest that the two aspects are ‘tied\nup’, so to speak, in the notion of artifact function. This gives\nrise to several problems. One, which will be passed over quickly\nbecause little philosophical work seems to have been done concerning\nit, is that structure and function mutually constrain each other, but\nthe constraining is only partial. It is unclear whether a general\naccount of this relation is possible and what problems need to be\nsolved to arrive there. There may be interesting connections with the\nissue of multiple realizability in the philosophy of mind and with\naccounts of reduction in science; an example where this is explored is\nMahner and Bunge 2001. \nIt is equally problematic whether a unified account of the notion of\nfunction as such is possible, but this issue has received considerably\nmore philosophical attention. The notion of function is of paramount\nimportance for characterizing artifacts, but the notion is used much\nmore widely. The notion of an artifact’s function seems to refer\nnecessarily to human intentions. Function is also a key concept in\nbiology, however, where no intentionality plays a role, and it is a\nkey concept in cognitive science and the philosophy of mind, where it\nis crucial in grounding intentionality in non-intentional, structural\nand physical properties. Up till now there is no accepted general\naccount of function that covers both the intentionality-based notion\nof artifact function and the non-intentional notion of biological\nfunction—not to speak of other areas where the concept plays a\nrole, such as the social sciences. The most comprehensive theory, that\nhas the ambition to account for the biological notion, cognitive\nnotion and the intentional notion, is Ruth Millikan’s 1984; for\ncriticisms and replies, see B. Preston 1998, 2003; Millikan 1999;\nVermaas & Houkes 2003; and Houkes & Vermaas 2010. The\ncollection of essays edited by Ariew, Cummins and Perlman (2002)\npresents a recent introduction to the general topic of defining the\nnotion of function in general, although the emphasis is, as is\ngenerally the case in the literature on function, on biological\nfunctions. \nAgainst the view that, at least in the case of artifacts, the notion\nof function refers necessarily to intentionality, it could be argued\nthat in discussing the functions of the components of a larger device,\nand the interrelations between these functions, the intentional\n‘side’ of these functions is of secondary importance only.\nThis, however, would be to ignore the possibility of the\nmalfunctioning of such components. This notion seems to be\ndefinable only in terms of a mismatch between actual behavior and\nintended behavior. The notion of malfunction also sharpens an\nambiguity in the general reference to intentions when characterizing\ntechnical artifacts. These artifacts usually engage many people, and\nthe intentions of these people may not all pull in the same direction.\nA major distinction can be drawn between the intentions of the actual\nuser of an artifact for a particular purpose and the intentions of the\nartifact’s designer. Since an artifact may be used for a purpose\ndifferent from the one for which its designer intended it to be used,\nand since people may also use natural objects for some purpose or\nother, one is invited to allow that artifacts can have multiple\nfunctions, or to enforce a hierarchy among all relevant intentions in\ndetermining the function of an artifact, or to introduce a\nclassification of functions in terms of the sorts of determining\nintentions. In the latter case, which is a sort of middle way between\nthe two other options, one commonly distinguishes between the\nproper function of an artifact as the one intended by its\ndesigner and the accidental function of the artifact as the\none given to it by some user on private considerations. Accidental use\ncan become so common, however, that the original function drops out of\nmemory. \nClosely related to this issue to what extent use and design determine\nthe function of an artifact is the problem of characterizing artifact\nkinds. It may seem that we use functions to classify artifacts: an\nobject is a knife because it has the function of cutting, or more\nprecisely, of enabling us to cut. On closer inspection, however, the\nlink between function and kind-membership seems much less\nstraightforward. The basic kinds in technology are, for example,\n‘knife’, ‘aircraft’ and ‘piston’.\nThe members of these kinds have been designed in order to be used to\ncut something with, to transport something through the air and to\ngenerate mechanical movement through thermodynamic expansion. However,\none cannot create a particular kind of artifact just by designing\nsomething with the intention that it be used for some particular\npurpose: a member of the kind so created must actually be useful for\nthat purpose. Despite innumerable design attempts and claims, the\nperpetual motion machine is not a kind of artifact. A kind like\n‘knife’ is defined, therefore, not only by the intentions\nof the designers of its members that they each be useful for cutting\nbut also by a shared operational principle known to these designers,\nand on which they based their design. This is, in a different setting,\nalso defended by Thomasson, who in her characterization of what she in\ngeneral calls an artifactual kind says that such a kind is\ndefined by the designer’s intention to make something of that\nkind, by a substantive idea that the designer has of how this can be\nachieved, and by his or her largely successful achievement of it\n(Thomasson 2003, 2007). Qua sorts of kinds in which artifacts can be\ngrouped, a distinction must therefore be made between a kind like\n‘knife’ and a corresponding but different kind\n‘cutter’. A ‘knife’ indicates a particular way\na ‘cutter’ can be made. One can also cut, however, with a\nthread or line, a welding torch, a water jet, and undoubtedly by other\nsorts of means that have not yet been thought of. A\n‘cutter’ would then refer to a truly functional kind. As\nsuch, it is subject to the conflict between use and design: one could\nmean by ‘cutter’ anything than can be used for cutting or\nanything that has been designed to be used for cutting, by the\napplication of whatever operational principle, presently known or\nunknown. \nThis distinction between artifact kinds and functional kinds is\nrelevant for the status of such kinds in comparison to other notions\nof kinds. Philosophy of science has emphasized that the concept of\nnatural kind, such as exemplified by ‘water’ or\n‘atom’, lies at the basis of science. On the other hand it\nis generally taken for granted that there are no regularities that all\nknives or airplanes or pistons answer to. This, however, is loosely\nbased on considerations of multiple realizability that fully apply\nonly to functional kinds, not to artifact kinds. Artifact kinds share\nan operational principle that gives them some commonality in physical\nfeatures, and this commonality becomes stronger once a particular\nartifact kind is subdivided into narrower kinds. Since these kinds are\nspecified in terms of physical and geometrical parameters, they are\nmuch closer to the natural kinds of science, in that they support\nlaw-like regularities; see for a defense of this position (Soavi\n2009). A recent\ncollection of essays that discuss the metaphysics of artifacts and\nartifact kinds is Franssen, Kroes, Reydon and Vermaas 2014. \nThere is at least one additional technology-related topic that ought\nto be mentioned because it has created a good deal of analytic\nphilosophical literature, namely Artificial Intelligence and related\nareas. A full discussion of this vast field is beyond the scope of\nthis entry, however. Information is to be found in the entries on\n Turing machines,\n the Church-Turing thesis,\n computability and complexity,\n the Turing test,\n the Chinese room argument,\n the computational theory of mind,\n functionalism,\n multiple realizability, and\n the philosophy of computer science. \nIt was not until the twentieth century that the development of the\nethics of technology as a systematic and more or less independent\nsubdiscipline of philosophy started. This late development may seem\nsurprising given the large impact that technology has had on society,\nespecially since the industrial revolution. \nA plausible reason for this late development of ethics of technology\nis the instrumental perspective on technology that was mentioned in\n Section 2.2.\n This perspective implies, basically, a positive ethical assessment of\ntechnology: technology increases the possibilities and capabilities of\nhumans, which seems in general desirable. Of course, since antiquity,\nit has been recognized that the new capabilities may be put to bad use\nor lead to human hubris. Often, however, these undesirable\nconsequences are attributed to the users of technology, rather than\nthe technology itself, or its developers. This vision is known as the\ninstrumental vision of technology resulting in the so-called\nneutrality thesis. The neutrality thesis holds that technology is a\nneutral instrument that can be put to good or bad use by its users.\nDuring the twentieth century, this neutrality thesis met with severe\ncritique, most prominently by Heidegger and Ellul, who have been\nmentioned in this context in\n Section 2,\nbut also by philosophers from the Frankfurt School, such as Horkheimer\nand Adorno (1947 [2002]), Marcuse (1964), and Habermas (1968\n[1970]). \nThe scope and the agenda for ethics of technology to a large extent\ndepend on how technology is conceptualized. The second half of the\ntwentieth century has witnessed a richer variety of conceptualizations\nof technology that move beyond the conceptualization of technology as\na neutral tool, as a world view or as a historical necessity. This\nincludes conceptualizations of technology as a political phenomenon\n(Winner, Feenberg, Sclove), as a social activity (Latour, Callon, Bijker and others in the\narea of science and technology studies), as a cultural phenomenon\n(Ihde, Borgmann), as a professional activity (engineering ethics,\ne.g., Davis), and as a cognitive activity (Bunge, Vincenti). Despite\nthis diversity, the development in the second half of the twentieth\ncentury is characterized by two general trends. One is a move away\nfrom technological determinism and the assumption that technology is a\ngiven self-contained phenomenon which develops autonomously to an\nemphasis on technological development being the result of choices\n(although not necessarily the intended result). The other is a move\naway from ethical reflection on technology as such to ethical\nreflection of specific technologies and to specific phases in the\ndevelopment of technology. Both trends together have resulted in an\nenormous increase in the number and scope of ethical questions that\nare asked about technology. The developments also imply that ethics of\ntechnology is to be adequately empirically informed, not only about\nthe exact consequences of specific technologies but also about the\nactions of engineers and the process of technological development.\nThis has also opened the way to the involvement of other disciplines\nin ethical reflections on technology, such as Science and Technology\nStudies (STS) and Technology Assessment (TA). \nNot only is the ethics of technology characterized by a diversity of\napproaches, it might even be doubted whether something like a\nsubdiscipline of ethics of technology, in the sense of a community of\nscholars working on a common set of problems, exists. The scholars\nstudying ethical issues in technology have diverse backgrounds (e.g.,\nphilosophy, STS, TA, law, political science) and they do not always\nconsider themselves (primarily) ethicists of technology. To give the\nreader an overview of the field, three basic approaches or strands\nthat might be distinguished in the ethics of technology will be\ndiscussed. \nBoth cultural and political approaches build on the traditional\nphilosophy and ethics of technology of the first half of the twentieth\ncentury. Whereas cultural approaches conceive of technology as a\ncultural phenomenon that influences our perception of the world,\npolitical approaches conceive of technology as a political phenomenon,\ni.e., as a phenomenon that is ruled by and embodies institutional\npower relations between people. \nCultural approaches are often phenomenological in nature or at least\nposition themselves in relation to phenomenology as\npost-phenomenology. Examples of philosophers in this tradition are Don\nIhde, Albert Borgmann, Peter-Paul Verbeek and Evan Selinger (e.g.,\nBorgmann 1984; Ihde 1990; Verbeek 2000 [2005], 2011). The approaches are\nusually influenced by developments in STS, especially the idea that\ntechnologies contain a script that influences not only people’s\nperception of the world but also human behavior, and the idea of the\nabsence of a fundamental distinction between humans and non-humans,\nincluding technological artifacts (Akrich 1992; Latour 1992, 1993;\nIhde & Selinger 2003). The combination of both ideas has led some\nto claim that technology has (moral) agency, a claim that is discussed\nbelow in\n Section 3.3.1. \nPolitical approaches to technology mostly go back to Marx, who assumed\nthat the material structure of production in society, in which\ntechnology is obviously a major factor, determined the economic and\nsocial structure of that society. Similarly, Langdon Winner has argued\nthat technologies can embody specific forms of power and authority\n(Winner 1980). According to him, some technologies are inherently\nnormative in the sense that they require or are strongly compatible\nwith certain social and political relations. Railroads, for example,\nseem to require a certain authoritative management structure. In other\ncases, technologies may be political due to the particular way they\nhave been designed. Some political approaches to technology are\ninspired by (American) pragmatism and, to a lesser extent, discourse\nethics. A number of philosophers, for example, have pleaded for a\ndemocratization of technological development and the inclusion of\nordinary people in the shaping of technology (Winner 1983; Sclove\n1995; Feenberg 1999). \nAlthough political approaches have obviously ethical ramifications,\nmany philosophers who have adopted such approaches do not engage in\nexplicit ethical reflection on technology. An interesting recent\nexception, and an attempt to consolidate a number of recent\ndevelopments and to articulate them into a more general account of\nwhat an ethics of technology should look like, is the volume\nPragmatist Ethics for a Technological Culture (Keulartz et\nal. 2002). In this volume, the authors plead for a revival of the\npragmatist tradition in moral philosophy because it is better fit to\ndeal with a number of moral issues in technology. Instead of focusing\non how to reach and justify normative judgments about technology, a\npragmatist ethics focuses on how to recognize and trace moral problems\nin the first place. Moreover, the process of dealing with these\nproblems is considered more important than the outcome. \nEngineering ethics is a relatively new field of education and\nresearch. It started off in the 1980s in the United States, merely as\nan educational effort. Engineering ethics is concerned with “the\nactions and decisions made by persons, individually or collectively,\nwho belong to the profession of engineering” (Baum 1980: 1).\nAccording to this approach, engineering is a profession, in the same\nway as medicine is a profession. \nAlthough there is no agreement on how a profession exactly should be\ndefined, the following characteristics are often mentioned: \nTypical ethical issues that are discussed in engineering ethics are\nprofessional obligations of engineers as exemplified in, for example,\ncodes of ethics of engineers, the role of engineers versus managers,\ncompetence, honesty, whistle-blowing, concern for safety and conflicts\nof interest (Davis 1998, 2005; Martin & Schinzinger 2005; Harris,\nPritchard, & Rabins 2008). \nRecently, a number of authors have pleaded for broadening the\ntraditional scope of engineering ethics (e.g., Herkert 2001;, van de\nPoel & Royakkers 2011). This call for a broader approach derives\nfrom two concerns. One concern is that the traditional micro-ethical\napproach in engineering ethics tends to take the contexts in which\nengineers have to work for given, while major ethical issues pertain\nto how this context is ‘organized’. Another concern is\nthat the traditional micro-ethical focus tends to neglect issues\nrelating to the impact of technology on society or issues relating to\ndecisions about technology. Broadening the scope of engineering ethics\nwould then, among others, imply more attention for such issues as\nsustainability and social justice. \nThe last decades have witnessed an increase in ethical inquiries into\nspecific technologies. This may now be the largest of the three\nstrands discussed, especially given the rapid growth in\ntechnology-specific ethical inquiries in the last two decades. One of\nthe most visible new fields is probably computer ethics (e.g., Moor\n1985; Floridi 2010; Johnson 2009; Weckert 2007; van den Hoven &\nWeckert 2008), with more recently a focus on robotics, artificial\nintelligence, machine ethics, and the ethics of algorithms (Lin,\nAbney, & Jenkins 2017; Nucci & Santoni de Sio 2016;\nMittelstadt et al. 2016; Bostrom & Yudkowsky 2014; Wallach &\nAllen 2009). But biotechnology has spurred dedicated ethical\ninvestigations as well (e.g., Sherlock & Morrey 2002; P. Thompson\n2007). More traditional fields like architecture and urban planning\nhave also attracted specific ethical attention (Fox 2000). More\nrecently, nanotechnology and so-called converging technologies have\nled to the establishment of what is called nanoethics (Allhoff et al.\n2007). Other examples are the ethics of nuclear deterrence (Finnis et\nal. 1988), nuclear energy (Taebi & Roeser 2015) and geoengineering\n(C. Preston 2016). \nObviously the establishment of such new fields of ethical reflection\nis a response to social and technological developments. Still, the\nquestion can be asked whether the social demand is best met by\nestablishing new fields of applied ethics. This issue is in fact\nregularly discussed as new fields emerge. Several authors have for\nexample argued that there is no need for nanoethics because\nnanotechnology does not raise any really new ethical issues (e.g.,\nMcGinn 2010). The alleged absence of newness here is supported by the\nclaim that the ethical issues raised by nanotechnology are a variation\non, and sometimes an intensification of, existing ethical issues, but\nhardly really new, and by the claim that these issues can be dealt\nwith the existing theories and concepts from moral philosophy. For an\nearlier, similar discussion concerning the supposed new character of\nethical issues in computer engineering, see Tavani 2002. \nThe new fields of ethical reflection are often characterized as\napplied ethics, that is, as applications of theories, normative\nstandards, concepts and methods developed in moral philosophy. For\neach of these elements, however, application is usually not\nstraightforward but requires a further specification or revision. This\nis the case because general moral standards, concepts and methods are\noften not specific enough to be applicable in any direct sense to\nspecific moral problems. ‘Application’ therefore often\nleads to new insights which might well result in the reformulation or\nat least refinement of existing normative standards, concepts and\nmethods. In some cases, ethical issues in a specific field might\nrequire new standards, concepts or methods. Beauchamp and Childress\nfor example have proposed a number of general ethical principles for\nbiomedical ethics (Beauchamp & Childress 2001). These principles\nare more specific than general normative standards, but still so\ngeneral and abstract that they apply to different issues in biomedical\nethics. In computer ethics, existing moral concepts relating to for\nexample privacy and ownership has been redefined and adapted to deal\nwith issues which are typical for the computer age (Johnson 2003). New\nfields of ethical application might also require new methods for, for\nexample, discerning ethical issues that take into account relevant\nempirical facts about these fields, like the fact that technological\nresearch and development usually takes place in networks of people\nrather than by individuals (Zwart et al. 2006). Another more general\nissue that applies to many new technologies is how to deal with the\nuncertainties about (potential) social and ethical impacts that\ntypically surround new emerging technologies. Brey’s (2012)\nproposal for an anticipatory ethics may be seen as a reply to this\nchallenge. The issue of anticipation is also one of the central\nconcerns in the more recent interdisciplinary field of responsible\ninnovation (e.g., Owen et al. 2013). \nAlthough different fields of ethical reflection on specific\ntechnologies might well raise their own philosophical and ethical\nissues, it can be questioned whether this justifies the development of\nseparate subfields or even subdisciplines. One obvious argument might\nbe that in order to say something ethically meaningful about new\ntechnologies, one needs specialized and detailed knowledge of a\nspecific technology. Moreover such subfields allow interaction with\nrelevant non-philosophical experts in for example law, psychology,\neconomy, science and technology studies (STS) or technology assessment\n(TA). On the other side, it could also be argued that a lot can be\nlearned from interaction and discussion between ethicists specializing\nin different technologies, and a fruitful interaction with the two\nother strands discussed above (cultural and political approaches and\nengineering ethics). Currently, such interaction in many cases seems\nabsent, although there are of course exceptions. \nWe now turn to the description of some themes in the ethics of\ntechnology. We focus on a number of general themes that provide an\nillustration of general issues in the ethics of technology and the way\nthese are treated. \nOne important general theme in the ethics of technology is the\nquestion whether technology is value-laden. Some authors have\nmaintained that technology is value-neutral, in the sense that\ntechnology is just a neutral means to an end, and accordingly can be\nput to good or bad use (e.g., Pitt 2000). This view might have some\nplausibility in as far as technology is considered to be just a bare\nphysical structure. Most philosophers of technology, however, agree\nthat technological development is a goal-oriented process and that\ntechnological artifacts by definition have certain functions, so that\nthey can be used for certain goals but not, or far more difficulty or\nless effectively, for other goals. This conceptual connection between\ntechnological artifacts, functions and goals makes it hard to maintain\nthat technology is value-neutral. Even if this point is granted, the\nvalue-ladenness of technology can be construed in a host of different\nways. Some authors have maintained that technology can have moral\nagency. This claim suggests that technologies can autonomously and\nfreely ‘act’ in a moral sense and can be held morally\nresponsible for their actions. \nThe debate whether technologies can have moral agency started off in\ncomputer ethics (Bechtel 1985; Snapper 1985; Dennett 1997; Floridi\n& Sanders 2004) but has since broadened. Typically, the authors\nwho claim that technologies (can) have moral agency often redefine the\nnotion of agency or its connection to human will and freedom (e.g.,\nLatour 1993; Floridi & Sanders 2004, Verbeek 2011). A disadvantage\nof this strategy is that it tends to blur the morally relevant\ndistinctions between people and technological artifacts. More\ngenerally, the claim that technologies have moral agency sometimes\nseems to have become shorthand for claiming that technology is morally\nrelevant. This, however, overlooks the fact technologies can be\nvalue-laden in other ways than by having moral agency (see, e.g.,\nJohnson 2006; Radder 2009; Illies & Meijers 2009; Peterson &\nSpahn 2011). One might, for example, claim that technology enables (or\neven invites) and constrains (or even inhibits) certain human actions\nand the attainment of certain human goals and therefore is to some\nextent value-laden, without claiming moral agency for technological\nartifacts. A good overview of the debate can be found in Kroes and\nVerbeek 2014. \nThe debate about moral agency and technology is now particularly\nsalient with respect to the design of intelligent artificial agents.\nJames Moor (2006) has distinguished between four ways in which\nartificial agents may be or become moral agents: \nIt might perhaps never be possible to technologically design full\nethical agents, and if it were to become possible it might be\nquestionable whether it is morally desirable to do so (Bostrom &\nYudkowsky 2014). As Wallach and Allen (2009) have pointed out, the\nmain problem might not be to design artificial agents that can\nfunction autonomously and that can adapt themselves in interaction\nwith the environment, but rather to build enough, and the right kind\nof, ethical sensitivity into such machines. \nResponsibility has always been a central theme in the ethics of\ntechnology. The traditional philosophy and ethics of technology,\nhowever, tended to discuss responsibility in rather general terms and\nwere rather pessimistic about the possibility of engineers to assume\nresponsibility for the technologies they developed. Ellul, for\nexample, has characterized engineers as the high priests of\ntechnology, who cherish technology but cannot steer it. Hans Jonas\n(1979 [1984]) has argued that technology requires an ethics in which\nresponsibility is the central imperative because for the first time in\nhistory we are able to destroy the earth and humanity. \nIn engineering ethics, the responsibility of engineers is often\ndiscussed in relation to code of ethics that articulate specific\nresponsibilities of engineers. Such codes of ethics stress three types\nof responsibilities of engineers: (1) conducting the profession with\nintegrity and honesty and in a competent way, (2) responsibilities\ntowards employers and clients and (3) responsibility towards the\npublic and society. With respect to the latter, most US codes of\nethics maintain that engineers ‘should hold paramount the\nsafety, health and welfare of the public’. \nAs has been pointed out by several authors (Nissenbaum 1996; Johnson\n& Powers 2005; Swierstra & Jelsma 2006), it may be hard to\npinpoint individual responsibility in engineering. The reason is that\nthe conditions for the proper attribution of individual responsibility\nthat have been discussed in the philosophical literature (like freedom\nto act, knowledge, and causality) are often not met by individual\nengineers. For example, engineers may feel compelled to act in a\ncertain way due to hierarchical or market constraints, and negative\nconsequences may be very hard or impossible to predict beforehand. The\ncausality condition is often difficult to meet as well due to the long\nchain from research and development of a technology till its use and\nthe many people involved in this chain. Davis (2012) nevertheless\nmaintains that despite such difficulties individual engineers can and\ndo take responsibility. \nOne issue that is at stake in this debate is the notion of\nresponsibility. Davis (2012), and also for example Ladd (1991), argue\nfor a notion of responsibility that focuses less on blame and stresses\nthe forward-looking or virtuous character of assuming responsibility.\nBut many others focus on backward-looking notions of responsibility\nthat stress accountability, blameworthiness or liability. Zandvoort\n(2000), for example has pleaded for a notion of responsibility in\nengineering that is more like the legal notion of strict liability, in\nwhich the knowledge condition for responsibility is seriously\nweakened. Doorn (2012) compares three perspectives on responsibility\nascription in engineering—a merit-based, a right-based and a\nconsequentialist perspective—and argues that the\nconsequentialist perspective, which applies a forward-looking notion\nof responsibility, is most powerful in influencing engineering\npractice. \nThe difficulty of attributing individual responsibility may lead to\nthe Problem of Many Hands (PMH). The term was first coined by Dennis\nThompson (1980) in an article about the responsibility of public\nofficials. The term is used to describe problems with the ascription\nof individual responsibility in collective settings. Doorn (2010) has\nproposed a procedurals approach, based on Rawls’ reflective\nequilibrium model, to deal with the PMH; other ways of dealing with\nthe PMH include the design of institutions that help to avoid it or an\nemphasis on virtuous behavior in organizations (van de Poel, Royakers,\n& Zwart 2015). \nIn the last decades, increasingly attention is paid not only to\nethical issues that arise during the use of a technology, but also\nduring the design phase. An important consideration behind this\ndevelopment is the thought that during the design phase technologies,\nand their social consequences, are still malleable whereas during the\nuse phase technologies are more or less given and negative social\nconsequences may be harder to avoid or positive effects harder to\nachieve. \nIn computer ethics, an approach known as Value Sensitive Design (VSD)\nhas been developed to explicitly address the ethical nature of design.\nVSD aims at integrating values of ethical importance in engineering\ndesign in a systematic way (Friedman & Kahn 2003). The approach\ncombines conceptual, empirical and technical investigations. There is\nalso a range of other approaches aimed at including values in design.\n‘Design for X’ approaches in engineering aim at including\ninstrumental values (like maintainability, reliability and costs) but\nthey also include design for sustainability, inclusive design, and\naffective design (Holt & Barnes 2010). Inclusive design aims at\nmaking designs accessible to the whole population including, for\nexample, handicapped people and the elderly (Erlandson 2008).\nAffective design aims at designs that evoke positive emotions with the\nusers and so contributes to human well-being. Van de Hoven, Vermaas,\nand van de Poel 2015 gives a good overview of the state-of-the art of\nvalue sensitive design for various values and application domains. \nIf one tries to integrate values into design one may run into the\nproblem of a conflict of values. The safest car is, due to its weight,\nnot likely to be the most sustainability. Here safety and\nsustainability conflict in the design of cars. Traditional methods in\nwhich engineers deal with such conflicts and make trade-off between\ndifferent requirements for design include cost-benefit analysis and\nmultiple criteria analysis. Such methods are, however, beset with\nmethodological problems like those discussed in\n Section 2.4\n (Franssen 2005; Hansson 2007). Van de Poel (2009) discusses various\nalternatives for dealing with value conflicts in design including the\nsetting of thresholds (satisficing), reasoning about values,\ninnovation and diversity. \nThe risks of technology are one of the traditional ethical concerns in\nthe ethics of technology. Risks raise not only ethical issues but\nother philosophical issues, such as epistemological and\ndecision-theoretical issues as well (Roeser et al. 2012). \nRisk is usually defined as the product of the probability of an\nundesirable event and the effect of that event, although there are\nalso other definitions around (Hansson 2004b). In general it seems\ndesirable to keep technological risks as small as possible. The larger\nthe risk, the larger either the likeliness or the impact of an\nundesirable event is. Risk reduction therefore is an important goal in\ntechnological development and engineering codes of ethics often\nattribute a responsibility to engineers in reducing risks and\ndesigning safe products. Still, risk reduction is not always feasible\nor desirable. It is sometimes not feasible, because there are no\nabsolutely safe products and technologies. But even if risk reduction\nis feasible it may not be acceptable from a moral point of view.\nReducing risk often comes at a cost. Safer products may be more\ndifficult to use, more expensive or less sustainable. So sooner or\nlater, one is confronted with the question: what is safe enough? What\nmakes a risk (un)acceptable? \nThe process of dealing with risks is often divided into three stages:\nrisk assessment, risk evaluation and risk management. Of these, the\nsecond is most obviously ethically relevant. However, risk assessment\nalready involves value judgments, for example about which risks should\nbe assessed in the first place (Shrader-Frechette 1991). An important,\nand morally relevant, issue is also the degree of evidence that is\nneeded to establish a risk. In establishing a risk on the basis of a\nbody of empirical data one might make two kinds of mistakes. One can\nestablish a risk when there is actually none (type I error) or one can\nmistakenly conclude that there is no risk while there actually is a\nrisk (type II error). Science traditionally aims at avoiding type I\nerrors. Several authors have argued that in the specific context of\nrisk assessment it is often more important to avoid type II errors\n(Cranor 1990; Shrader-Frechette 1991). The reason for this is that\nrisk assessment not just aims at establishing scientific truth but has\na practical aim, i.e., to provide the knowledge on basis of which\ndecisions can be made about whether it is desirable to reduce or avoid\ncertain technological risks in order to protect users or the\npublic. \nRisk evaluation is carried out in a number of ways (see, e.g.,\nShrader-Frechette 1985). One possible approach is to judge the\nacceptability of risks by comparing them to other risks or to certain\nstandards. One could, for example, compare technological risks with\nnaturally occurring risks. This approach, however, runs the danger of\ncommitting a naturalistic fallacy: naturally occurring risks may\n(sometimes) be unavoidable but that does not necessarily make them\nmorally acceptable. More generally, it is often dubious to judge the\nacceptability of the risk of technology A by comparing it to the risk\nof technology B if A and B are not alternatives in a decision (for\nthis and other fallacies in reasoning about risks, see Hansson\n2004a). \nA second approach to risk evaluation is risk-cost benefit analysis,\nwhich is based on weighing the risks against the benefits of an\nactivity. Different decision criteria can be applied if a (risk) cost\nbenefit analysis is carried out (Kneese, Ben-David, and Schulze 1983).\nAccording to Hansson (2003: 306), usually the following criterion is\napplied:  \n… a risk is acceptable if and only if the total benefits that\nthe exposure gives rise to outweigh the total risks, measured as the\nprobability-weighted disutility of outcomes. \nA third approach is to base risk acceptance on the consent of people\nwho suffer the risks after they have been informed about these risks\n(informed consent). A problem of this approach is that technological\nrisks usually affect a large number of people at once. Informed\nconsent may therefore lead to a “society of stalemates”\n(Hansson 2003: 300). \nSeveral authors have proposed alternatives to the traditional\napproaches of risk evaluation on the basis of philosophical and\nethical arguments. Shrader-Frechette (1991) has proposed a number of\nreforms in risk assessment and evaluation procedures on the basis of a\nphilosophical critique of current practices. Roeser (2012) argues for\na role of emotions in judging the acceptability of risks. Hansson has\nproposed the following alternative principle for risk evaluation:  \nExposure of a person to a risk is acceptable if and only if this\nexposure is part of an equitable social system of risk-taking that\nworks to her advantage. (Hansson 2003: 305)  \nHansson’s proposal introduces a number of moral considerations\nin risk evaluation that are traditionally not addressed or only\nmarginally addressed. These are the consideration whether individuals\nprofit from a risky activity and the consideration whether the\ndistribution of risks and benefits is fair. \nSome authors have criticized the focus on risks in the ethics of\ntechnology. One strand of criticism argues that we often lack the\nknowledge to reliably assess the risks of a new technology before it\nhas come into use. We often do not know the probability that something\nmight go wrong, and sometimes we even do not know, or at least not\nfully, what might go wrong and what possible negative consequences may\nbe. To deal with this, some authors have proposed to conceive of the\nintroduction of new technology in society as a social experiment and\nhave urged to think about the conditions under which such experiments\nare morally acceptable (Martin & Schinzinger 2005; van de Poel\n2016). Another strand of criticism states that the focus on risks has\nled to a reduction of the impacts of technology that are considered\n(Swierstra & te Molder 2012). Only impacts related to safety and\nhealth, which can be calculated as risks, are considered, whereas\n‘soft’ impacts, for example of a social or psychological\nnature, are neglected, thereby impoverishing the moral evaluation of\nnew technologies.","contact.mail":"g.j.c.lokhorst@tudelft.nl","contact.domain":"tudelft.nl"},{"date.published":"2009-02-20","date.changed":"2018-09-06","url":"https://plato.stanford.edu/entries/technology/","author1":"Maarten Franssen","author1.info":"http://www.tbm.tudelft.nl/en/about-faculty/departments/values-technology-and-innovation/sections/ethicsphilosophy-of-technology/staff/dr-mpm-maarten-franssen/","author2.info":"http://gjclokhorst.nl/","entry":"technology","body.text":"\n\n\nIf philosophy is the attempt “to understand how things in the\nbroadest possible sense of the term hang together in the broadest\npossible sense of the term”, as Sellars (1962) put it,\nphilosophy should not ignore technology. It is largely by technology\nthat contemporary society hangs together. It is hugely important not\nonly as an economic force but also as a cultural force. Indeed during\nthe last two centuries, when it gradually emerged as a discipline,\nphilosophy of technology has mostly been concerned with the meaning of\ntechnology for, and its impact on, society and culture, rather than\nwith technology itself. Mitcham (1994) calls this type of philosophy\nof technology “humanities philosophy of technology”\nbecause it accepts “the primacy of the humanities over\ntechnologies” and is continuous with the overall perspective of\nthe humanities (and some of the social sciences). Only recently a\nbranch of the philosophy of technology has developed that is concerned\nwith technology itself and that aims to understand both the practice\nof designing and creating artifacts (in a wide sense, including\nartificial processes and systems) and the nature of the things so\ncreated. This latter branch of the philosophy of technology seeks\ncontinuity with the philosophy of science and with several other\nfields in the analytic tradition in modern philosophy, such as the\nphilosophy of action and decision-making, rather than with the\nhumanities and social science.\n\n\nThe entry starts with a brief historical overview, then continues with\na presentation of the themes on which modern analytic philosophy of\ntechnology focuses. This is followed by a discussion of the societal\nand ethical aspects of technology, in which some of the concerns of\nhumanities philosophy of technology are addressed. This twofold\npresentation takes into consideration the development of technology as\nthe outcome of a process originating within and guided by the practice\nof engineering, by standards on which only limited societal control is\nexercised, as well as the consequences for society of the\nimplementation of the technology so created, which result from\nprocesses upon which only limited control can be exercised.\n\n\n\n\n\n\n\nPhilosophical reflection on technology is about as old as philosophy\nitself. Our oldest testimony is from ancient Greece. There are four\nprominent themes. One early theme is the thesis that technology learns\nfrom or imitates nature (Plato, Laws X 899a ff.). According\nto Democritus, for example, house-building and weaving were first\ninvented by imitating swallows and spiders building their nests and\nnets, respectively (Diels 1903 and Freeman 1948: 154).  Perhaps the\noldest extant source for the exemplary role of nature is Heraclitus\n(Diels 1903 and Freeman 1948: 112).  Aristotle referred to this\ntradition by repeating Democritus’ examples, but he did not\nmaintain that technology can only imitate nature: “generally\ntechnè in some cases completes what nature cannot\nbring to a finish, and in others imitates nature”\n(Physics II.8, 199a15; see also Physics II.2, and\nsee Schummer 2001 and this encyclopedia’s entry on\n episteme and techne\n for discussion). \nA second theme is the thesis that there is a fundamental ontological\ndistinction between natural things and artifacts. According to\nAristotle (Physics II.1), the former have their principles of\ngeneration and motion inside, whereas the latter, insofar as they are\nartifacts, are generated only by outward causes, namely human aims and\nforms in the human soul. Natural products (animals and their parts,\nplants, and the four elements) move, grow, change, and reproduce\nthemselves by inner final causes; they are driven by purposes of\nnature. Artifacts, on the other hand, cannot reproduce themselves.\nWithout human care and intervention, they vanish after some time by\nlosing their artificial forms and decomposing into (natural)\nmaterials. For instance, if a wooden bed is buried, it decomposes to\nearth or changes back into its botanical nature by putting forth a\nshoot. \nThe thesis that there is a fundamental difference between man-made\nproducts and natural substances has had a long-lasting influence. In\nthe Middle Ages, Avicenna criticized alchemy on the ground that it can\nnever produce ‘genuine’ substances\n(Briffault 1930: 147). Even today, some still\nmaintain that there is a difference between, for example, natural and\nsynthetic vitamin C. The modern discussion of this theme is taken up\nin\n Section 2.5. \nAristotle’s doctrine of the four causes—material, formal,\nefficient and final—can be regarded as a third early\ncontribution to the philosophy of technology. Aristotle explained this\ndoctrine by referring to technical artifacts such as houses and\nstatues (Physics II.3). The four causes are still very much\npresent in modern discussions related to the metaphysics of artifacts.\nDiscussions of the notion of function, for example, focus on its\ninherent teleological or ‘final’ character and the\ndifficulties this presents to its use in biology. And the notorious\ncase of the ship of Theseus—see this encyclopedia’s\nentries on\n material constitution,\n identity over time,\n relative identity,\n and\n sortals—was\n introduced in modern philosophy by Hobbes as showing a conflict\nbetween unity of matter and unity of form as principles of\nindividuation. This conflict is seen by many as characteristic of\nartifacts. David Wiggins (1980: 89) takes it even to be the defining\ncharacteristic of artifacts. \nA fourth point that deserves mentioning is the extensive employment of\ntechnological images by Plato and Aristotle. In his Timaeus,\nPlato described the world as the work of an Artisan, the Demiurge. His\naccount of the details of creation is full of images drawn from\ncarpentry, weaving, ceramics, metallurgy, and agricultural technology.\nAristotle used comparisons drawn from the arts and crafts to\nillustrate how final causes are at work in natural processes. Despite\ntheir negative appreciation of the life led by artisans, who they\nconsidered too much occupied by the concerns of their profession and\nthe need to earn a living to qualify as free individuals, both Plato\nand Aristotle found technological imagery indispensable for expressing\ntheir belief in the rational design of the universe (Lloyd 1973:\n61). \nAlthough there was much technological progress in the Roman empire and\nduring the Middle Ages, philosophical reflection on technology did not\ngrow at a corresponding rate. Comprehensive works such as\nVitruvius’ De architectura (first century BC) and\nAgricola’s De re metallica (1556) paid much attention\nto practical aspects of technology but little to philosophy. \nIn the realm of scholastic philosophy, there was an emergent\nappreciation for the mechanical arts. They were generally considered\nto be born of—and limited to—the mimicry of nature. This\nview was challenged when alchemy was introduced in the Latin West\naround the mid-twelfth century. Some alchemical writers such as Roger\nBacon were willing to argue that human art, even if learned by\nimitating natural processes, could successfully reproduce natural\nproducts or even surpass them (Newman 2004). The result was a philosophy of technology in which\nhuman art was raised to a level of appreciation not found in other\nwritings until the Renaissance. However, the last three decades of the\nthirteenth century witnessed an increasingly hostile attitude by\nreligious authorities toward alchemy that culminated eventually in the\ndenunciation Contra alchymistas, written by the inquisitor\nNicholas Eymeric in 1396 (Newman 2004). \nThe Renaissance led to a greater appreciation of human beings and\ntheir creative efforts, including technology. As a result,\nphilosophical reflection on technology and its impact on society\nincreased. Francis Bacon is generally regarded as the first modern\nauthor to put forward such reflection. His view, expressed in his\nfantasy New Atlantis (1627), was overwhelmingly positive.\nThis positive attitude lasted well into the nineteenth century,\nincorporating the first half-century of the industrial revolution. \nFor example, Karl Marx did not condemn the steam engine or the\nspinning mill for the vices of the bourgeois mode of production; he\nbelieved that ongoing technological innovation were necessary steps\ntoward the more blissful stages of socialism and communism of the\nfuture (see Bimber 1990 for a discussion of different views on the\nrole of technology in Marx’s theory of historical development,\nand see Van der Pot 1985 [1994/2004] for an extensive historical\noverview of appreciations of the development of technology). \nA turning point in the appreciation of technology as a socio-cultural\nphenomenon is marked by Samuel Butler’s Erewhon (1872),\nwritten under the influence of the Industrial Revolution, and\nDarwin’s On the Origin of Species (1859). Butler’s book gave an\naccount of a fictional country where all machines are banned and the\npossession of a machine or the attempt to build one is a capital\ncrime. The people of this country had become convinced by an argument\nthat ongoing technical improvements are likely to lead to a\n‘race’ of machines that will replace mankind as the\ndominant species on earth. \nDuring the last quarter of the nineteenth century and most of the\ntwentieth century a critical attitude predominated in philosophical\nreflection on technology. The representatives of this attitude were,\noverwhelmingly, schooled in the humanities or the social sciences and\nhad virtually no first-hand knowledge of engineering practice. Whereas\nBacon wrote extensively on the method of science and conducted\nphysical experiments himself, Butler, being a clergyman, lacked such\nfirst-hand knowledge. Ernst Kapp, who was the first to use the term\n‘philosophy of technology’ in his book Eine\nPhilosophie der Technik (1877 [2018]), was a philologist and\nhistorian.  Most of the authors who wrote critically about technology\nand its socio-cultural role during the twentieth century were\nphilosophers of a general outlook, such as Martin Heidegger (1954\n[1977]), Hans Jonas (1979 [1984]), Arnold Gehlen (1957 [1980]),\nGünther Anders (1956), and Andrew Feenberg (1999). Others had a\nbackground in one of the other humanities or in social science, such\nas literary criticism and social research in the case of Lewis Mumford\n(1934), law in the case of Jacques Ellul (1954 [1964]), political\nscience in the case of Langdon Winner (1977, 1980, 1983) and literary\nstudies in the case of Albert Borgmann (1984). The form of philosophy\nof technology constituted by the writings of these and others has been\ncalled by Carl Mitcham (1994) “humanities philosophy of\ntechnology”, because it takes its point of departure from the\nsocial sciences and the humanities rather than from the practice of\ntechnology, and it approaches technology accepting “the primacy\nof the humanities over technologies” (1994: 39), since\ntechnology originates from the goals and values of humans.\n \nHumanities philosophers of technology tend to take the phenomenon of\ntechnology itself largely for granted; they treat it as a ‘black\nbox’, a given, a unitary, monolithic, inescapable phenomenon.\nTheir interest is not so much to analyze and understand this\nphenomenon itself but to grasp its relations to morality (Jonas,\nGehlen), politics (Winner), the structure of society (Mumford), human\nculture (Ellul), the human condition (Hannah Arendt), or metaphysics\n(Heidegger). In this, these philosophers are almost all openly\ncritical of technology: all things considered, they tend to have a\nnegative judgment of the way technology has affected human society and\nculture, or at least they single out for consideration the negative\neffects of technology on human society and culture. This does not\nnecessarily mean that technology itself is pointed out as the\nprincipal cause of these negative developments. In the case of\nHeidegger, in particular, the paramount position of technology in\nmodern society is rather a symptom of something more fundamental,\nnamely a wrongheaded attitude towards Being which has been on the rise\nfor almost 25 centuries. It is therefore questionable whether\nHeidegger should be considered as a philosopher of technology,\nalthough within the traditional view he is considered to be among the\nmost important ones. Much the same could be said about Arendt, in\nparticular her discussion of technology in The Human\nCondition (1958), although her position in the canon of\nhumanities philosophy of technology is not as prominent. \nTo be sure, the work of these founding figures of humanities\nphilosophy of technology has been taken further by a second and third\ngeneration of scholars—in particular the work of Heidegger\nremains an important source of inspiration—but who in doing so\nhave adopted a more neutral rather than overall negative view of\ntechnology and its meaning for human life and culture. Notable\nexamples are Ihde (1979, 1993) and Verbeek (2000 [2005]). \nIn its development, humanities philosophy of technology continues to\nbe influenced not so much by developments in philosophy (e.g.,\nphilosophy of science, philosophy of action, philosophy of mind) but\nby developments in the social sciences and humanities. Although, for\nexample, Ihde and those who take their point of departure with him,\nposition their work as phenomenologist or postphenomenologist, there\ndoes not seem to be much interest in either the past or the present of\nthis diffuse notion in philosophy, and in particular not much interest\nin the far from easy question to what extent Heidegger can be\nconsidered a phenomenologist. Of particular significance has been the\nemergence of ‘Science and Technology Studies’ (STS) in the\n1980s, which studies from a broad social-scientific perspective how\nsocial, political, and cultural values affect scientific research and\ntechnological innovation, and how these in turn affect society,\npolitics, and culture. We discuss authors from humanities philosophy\nof technology in\n Section 3\non ‘Ethical and Social Aspects of Technology’, but do not\npresent separately and in detail the wide variety of views existing in\nthis field. For a detailed treatment Mitcham’s 1994 book\nprovides an excellent overview. Olsen, Selinger and Riis (2008) offer\na collection of more recent contributions; Scharff and Dusek (2003\n[2014]) and Kaplan (2004 [2009]) present comprehensive anthologies of\ntexts from this tradition. \nMitcham contrasts ‘humanities philosophy of technology’ to\n‘engineering philosophy of technology’, where the latter\nrefers to philosophical views developed by engineers or technologists\nas “attempts … to elaborate a technological\nphilosophy” (1994: 17). Mitcham discusses only a handful of\npeople as engineering philosophers of technology, however: Ernst Kapp,\nPeter Engelmeier, Friedrich Dessauer, and much more briefly Jacques\nLafitte, Gilbert Simondon, Hendrik van Riessen, Juan David\nGarcía Bacca, R. Buckminster Fuller and Mario Bunge. The label\nraises serious questions, however: several of them hardly classify as\n‘engineers or technologists’ and it is also not very clear\nhow the notion of ‘a technological philosophy’ should be\nunderstood. As philosophers these authors seem all to be rather\nisolated figures, whose work shows little overlap and who seem to be\nsharing mainly the absence of a ‘working relation’ with\nestablished philosophical disciplines. It is not so clear what sort of\nquestions and concerns underlie the notion of ‘engineering\nphilosophy of technology’. A larger role for systematic\nphilosophy could bring it quite close to some examples of humanities\nphilosophy of technology, for instance the work of Jacques Ellul,\nwhere the analyses would be rather similar and the remaining\ndifferences would be ones of attitude or appreciation. \nIn the next section we discuss in more detail a form of philosophy of\ntechnology that we consider to occupy, currently, the position of\nalternative to the humanities philosophy of technology. It emerged in\nthe 1960s and gained momentum in the past fifteen to twenty years.\nThis form of the philosophy of technology, which may be called\n‘analytic’, is not primarily concerned with the relations\nbetween technology and society but with technology itself. It\nexpressly does not look upon technology as a ‘black box’\nbut as a phenomenon that should be studied in detail. It regards\ntechnology perhaps not in its entirety as a practice but as something\ngrounded in a practice, basically the practice of engineering. It\nanalyses this practice, its goals, its concepts and its methods, and\nit relates its findings to various themes from philosophy. \nIn focusing on technology as a practice sustained by engineers,\nsimilar to the way philosophy of science focuses on the practice of\nscience as sustained by scientists, analytic philosophy of technology\ncould be thought to amount to the philosophy of engineering. Indeed\nmany of the issues related to design, discussed below in Sections\n 2.3\n and\n 2.4,\n could be singled out as forming the subject matter of the philosophy\nof engineering. The metaphysical issues discussed in Section\n 2.5\n could not, however, and analytic philosophy of technology is\ntherefore significantly broader than philosophy of engineering. The\nvery title of Philosophy of Technology and Engineering\nSciences (Meijers 2009), an extensive up-to-date overview, which\ncontains contributions to all of the topics treated in the next\nsection, expresses the view that technology and engineering do not\ncoincide. Which is not to say, however, that the book offers a clear\nconception of what makes technology different from engineering, or\nmore than engineering. In fact, the existence of humanities philosophy\nof technology and analytic philosophy of technology next to each other\nreflects a basic ambiguity in the notion of technology that the\nphilosophical work that has been going on has not succeeded in\nclarifying. \nTechnology can be said to have two ‘cores’ or\n‘dimensions’, which can be referred to as\ninstrumentality and productivity. Instrumentality\ncovers the totality of human endeavours to control their lives and\ntheir environments by interfering with the world in an instrumental\nway, by using things in a purposeful and clever way. Productivity\ncovers the totality of human endeavours to brings new things into\nexistence that can do certain things in a controlled and clever way.\nFor the study of instrumentality, however, it is in principle\nirrelevant whether or not the things that are made use of in\ncontrolling our lives and environments have been made by us first; if\nwe somehow could rely on natural objects to always be available to\nserve our purposes, the analysis of instrumentality and its\nconsequences for how we live our lives would not necessarily be\naffected. Likewise, for the analysis of what is involved in the making\nof artifacts, and how the notion of artifact and of something new\nbeing brought into existence are to be understood, it is to a large\nextent irrelevant how human life, culture and society are changed as a\nresult of the artifacts that are in fact produced. Clearly, humanities\nphilosophy of technology has until now been more attracted by the\ninstrumentality core whereas analytic philosophy of technology has\nmainly gone for the productivity core. But technology as one of the\nbasic phenomena of modern society, if not the most basic one, clearly\nis constituted by the processes centering on and involving both cores.\nIt has proved difficult, however, to come to an overarching approach\nin which the interaction between these two dimensions of technology\nare adequately dealt with—no doubt partly due to the great\ndifferences in philosophical orientation and methodology associated\nwith the two traditions and their separate foci. To improve this\nsituation is arguably the most urgent challenge that the field of\nphilosophy of technology as a whole is facing, since the continuation\nof the two orientations leading their separate lives threatens its\nunity and coherence as a discipline in the first place.\nNotwithstanding its centrality and urgency, the ambiguity noted here\nseems hardly to be confronted directly in the literature. It is\naddressed by Lawson (2008, 2017) and by Franssen and Koller\n(2016). \nAfter presenting the major issues of philosophical relevance in\ntechnology and engineering that are studied by analytic philosophers\nof technology in the next section, we discuss the problems and\nchallenges that technology poses for the society in which it is\npracticed in the third and final section. \nIt may come as a surprise to those new to the topic that the fields of\nphilosophy of science and philosophy of technology show such great\ndifferences, given that few practices in our society are as closely\nrelated as science and technology. Experimental science is nowadays\ncrucially dependent on technology for the realization of its research\nset-ups and for gathering and analyzing data. The phenomena that\nmodern science seeks to study could never be discovered without\nproducing them through technology. \nTheoretical research within technology has come to be often\nindistinguishable from theoretical research in science, making\nengineering science largely continuous with ‘ordinary’ or\n‘pure’ science. This is a relatively recent development,\nwhich started around the middle of the nineteenth century, and is\nresponsible for great differences between modern technology and\ntraditional, craft-like techniques. The educational training that\naspiring scientists and engineers receive starts off being largely\nidentical and only gradually diverges into a science or an engineering\ncurriculum. Ever since the scientific revolution of the seventeenth\ncentury, characterized by its two major innovations, the experimental\nmethod and the mathematical articulation of scientific theories,\nphilosophical reflection on science has focused on the method by which\nscientific knowledge is generated, on the reasons for thinking\nscientific theories to be true, or approximately true, and on the\nnature of evidence and the reasons for accepting one theory and\nrejecting another. Hardly ever have philosophers of science posed\nquestions that did not have the community of scientists, their\nconcerns, their aims, their intuitions, their arguments and choices,\nas a major target. In contrast it is only recently that the philosophy\nof technology has discovered the community of engineers. \nIt might be claimed that it is up to the philosophy of technology, and\nnot the philosophy of science, to target first of all the impact of\ntechnology—and with it science—on society and culture,\nbecause science affects society only through technology. This,\nhowever, will not do. Right from the start of the scientific\nrevolution, science affected human culture and thought fundamentally\nand directly, not with a detour through technology, and the same is\ntrue for later developments such as relativity, atomic physics and\nquantum mechanics, the theory of evolution, genetics, biochemistry,\nand the increasingly dominating scientific world view overall.\nPhilosophers of science overwhelmingly give the impression that they\nleave questions addressing the normative, social and cultural aspects\nof science gladly to other philosophical disciplines, or to historical\nstudies. There are exceptions, however, and things may be changing;\nPhilip Kitcher, to name but one prominent philosopher of science, has\nsince 2000 written books on the relation of science to politics,\nethics and religion (Kitcher 2001, 2011). \nThere is a major difference between the historical development of\nmodern technology as compared to modern science which may at least\npartly explain this situation, which is that science emerged in the\nseventeenth century from philosophy itself. The answers that Galileo,\nHuygens, Newton, and others gave, by which they initiated the alliance\nof empiricism and mathematical description that is so characteristic\nof modern science, were answers to questions that had belonged to the\ncore business of philosophy since antiquity. Science, therefore, kept\nthe attention of philosophers. Philosophy of science is a\ntransformation of epistemology in the light of the emergence of\nscience. The foundational issues—the reality of atoms, the\nstatus of causality and probability, questions of space and time, the\nnature of the quantum world—that were so lively discussed during\nthe end of the nineteenth and the beginning of the twentieth century\nare an illustration of this close relationship between scientists and\nphilosophers. No such intimacy has ever existed between those same\nphilosophers and technologists; their worlds still barely touch. To be\nsure, a case can be made that, compared to the continuity existing\nbetween natural philosophy and science, a similar continuity exists\nbetween central questions in philosophy having to do with human action\nand practical rationality and the way technology approaches and\nsystematizes the solution of practical problems. To investigate this\nconnection may indeed be considered a major theme for philosophy of\ntechnology, and more is said on it in Sections\n 2.3\n and\n 2.4.\n This continuity appears only by hindsight, however, and dimly, as the\nhistorical development is at most a slow convening of various strands\nof philosophical thinking on action and rationality, not a development\ninto variety from a single origin. Significantly it is only the\nacademic outsider Ellul who has, in his idiosyncratic way, recognized\nin technology the emergent single dominant way of answering all\nquestions concerning human action, comparable to science as the single\ndominant way of answering all questions concerning human knowledge\n(Ellul 1954 [1964]). But Ellul was not so much interested in\ninvestigating this relationship as in emphasizing and denouncing the\nsocial and cultural consequences as he saw them. It is all the more\nimportant to point out that humanities philosophy of technology cannot\nbe differentiated from analytic philosophy of technology by claiming\nthat only the former is interested in the social environment of\ntechnology. There are studies which are rooted in analytic philosophy\nof science but address specifically the relation of technology to\nsociety and culture, and equally the relevance of social relations to\npractices of technology, without taking an evaluative stand with\nrespect to technology; an example is B. Preston 2012. \nThe close relationship between the practices of science and technology\nmay easily keep the important differences between the two from view.\nThe predominant position of science in the philosophical field of\nvision made it difficult for philosophers to recognize that technology\nmerits special attention for involving issues that do not emerge in\nscience. This view resulting from this lack of recognition is often\npresented, perhaps somewhat dramatically, as coming down to a claim\nthat technology is ‘merely’ applied science. \nA questioning of the relation between science and technology was the\ncentral issue in one of the earliest discussions among analytic\nphilosophers of technology. In 1966, in a special issue of the journal\nTechnology and Culture, Henryk Skolimowski argued that\ntechnology is something quite different from science (Skolimowski\n1966). As he phrased it, science concerns itself with what is, whereas\ntechnology concerns itself with what is to be. A few years later, in\nhis well-known book The Sciences of the Artificial (1969),\nHerbert Simon emphasized this important distinction in almost the same\nwords, stating that the scientist is concerned with how things are but\nthe engineer with how things ought to be. Although it is difficult to\nimagine that earlier philosophers were blind to this difference in\norientation, their inclination, in particular in the tradition of\nlogical empiricism, to view knowledge as a system of statements may\nhave led to a conviction that in technology no knowledge claims play a\nrole that cannot also be found in science. The study of technology,\ntherefore, was not expected to pose new challenges nor hold surprises\nregarding the interests of analytic philosophy. \nIn contrast, Mario Bunge (1966) defended the view that technology\nis applied science, but in a subtle way that does justice to\nthe differences between science and technology. Bunge acknowledges\nthat technology is about action, but an action heavily underpinned by\ntheory—that is what distinguishes technology from the arts and\ncrafts and puts it on a par with science. According to Bunge, theories\nin technology come in two types: substantive theories, which provide\nknowledge about the object of action, and operative theories, which\nare concerned with action itself. The substantive theories of\ntechnology are indeed largely applications of scientific theories. The\noperative theories, in contrast, are not preceded by scientific\ntheories but are born in applied research itself. Still, as Bunge\nclaims, operative theories show a dependence on science in that in\nsuch theories the method of science is employed. This\nincludes such features as modeling and idealization, the use of\ntheoretical concepts and abstractions, and the modification of\ntheories by the absorption of empirical data through prediction and\nretrodiction. \nIn response to this discussion, Ian Jarvie (1966) proposed as\nimportant questions for a philosophy of technology what the\nepistemological status of technological statements is and how\ntechnological statements are to be demarcated from scientific\nstatements. This suggests a thorough investigation of the various\nforms of knowledge occurring in either practice, in particular, since\nscientific knowledge has already been so extensively studied, of the\nforms of knowledge that are characteristic of technology and are\nlacking, or of much less prominence, in science. A distinction between\n‘knowing that’—traditional propositional\nknowledge—and ‘knowing how’—non-articulated\nand even impossible-to-articulate knowledge—had been introduced\nby Gilbert Ryle (1949) in a different context. The notion of\n‘knowing how’ was taken up by Michael Polanyi under the\nname of tacit knowledge and made a central characteristic of\ntechnology (Polanyi 1958); the current state of the philosophical\ndiscussion is presented in this encyclopedia’s entry on\n knowledge how.\n However, emphasizing too much the role of unarticulated knowledge, of\n‘rules of thumb’ as they are often called, easily\nunderplays the importance of rational methods in technology. An\nemphasis on tacit knowledge may also be ill-fit for distinguishing the\npractices of science and technology because the role of tacit\nknowledge in science may well be more important than current\nphilosophy of science acknowledges, for example in concluding causal\nrelationships on the basis of empirical evidence. This was also an\nimportant theme in the writings of Thomas Kuhn on theory change in\nscience (Kuhn 1962). \nTo claim, with Skolimowski and Simon, that technology is about what is\nto be or what ought to be rather than what is may serve to distinguish\nit from science but will hardly make it understandable why so much\nphilosophical reflection on technology has taken the form of\nsocio-cultural critique. Technology is an ongoing attempt to bring the\nworld closer to the way one wishes it to be. Whereas science aims to\nunderstand the world as it is, technology aims to change the world.\nThese are abstractions, of course. For one, whose wishes concerning\nwhat the world should be like are realized in technology? Unlike\nscientists, who are often personally motivated in their attempts at\ndescribing and understanding the world, engineers are seen, not in the\nleast by engineers themselves, as undertaking their attempts to change\nthe world as a service to the public. The ideas on what is to be or\nwhat ought to be are seen as originating outside of technology itself;\nengineers then take it upon themselves to realize these ideas. This\nview is a major source for the widely spread picture of technology as\nbeing instrumental, as delivering instruments ordered from\n‘elsewhere’, as means to ends specified outside of\nengineering, a picture that has served further to support the claim\nthat technology is neutral with respect to values, discussed\nin\n Section 3.3.1.\n This view involves a considerable distortion of reality, however.\nMany engineers are intrinsically motivated to change the world; in\ndelivering ideas for improvement they are, so to speak, their own best\ncustomers. The same is true for most industrial companies,\nparticularly in a market economy, where the prospect of great profits\nis another powerful motivator. As a result, much technological\ndevelopment is ‘technology-driven’. \nTo understand where technology ‘comes from’, what drives\nthe innovation process, is of importance not only to those who are\ncurious to understand the phenomenon of technology itself but also to\nthose who are concerned about its role in society. Technology or\nengineering as a practice is concerned with the creation of artifacts\nand, of increasing importance, artifact-based services. The design\nprocess, the structured process leading toward that goal, forms\nthe core of the practice of technology. In the engineering literature,\nthe design process is commonly represented as consisting of a series\nof translational steps; see for this, e.g., Suh 2001. At the start are\nthe customer’s needs or wishes. In the first step these are\ntranslated into a list of functional requirements, which then\ndefine the design task an engineer, or a team of engineers, has to\naccomplish. The functional requirements specify as precisely as\npossible what the device to be designed must be able to do. This step\nis required because customers usually focus on just one or two\nfeatures and are unable to articulate the requirements that are\nnecessary to support the functionality they desire. In the second\nstep, the functional requirements are translated into design\nspecifications, which the exact physical parameters of crucial\ncomponents by which the functional requirements are going to be met.\nThe design parameters chosen to satisfy these requirements are\ncombined and made more precise such that a blueprint of the\ndevice results. The blueprint contains all the details that must be\nknown such that the final step to the process of manufacturing the\ndevice can take place. It is tempting to consider the blueprint as the\nend result of a design process, instead of a finished copy being this\nresult. However, actual copies of a device are crucial for the purpose\nof prototyping and testing. Prototyping and testing presuppose that\nthe sequence of steps making up the design process can and will often\ncontain iterations, leading to revisions of the design parameters\nand/or the functional requirements. Even though, certainly for\nmass-produced items, the manufacture of a product for delivery to its\ncustomers or to the market comes after the closure of the design\nphase, the manufacturing process is often reflected in the functional\nrequirements of a device, for example in putting restrictions on the\nnumber of different components of which the device consists. The\ncomplexity of a device will affect how difficult it will be to\nmaintain or repair it, and ease of maintenance or low repair costs are\noften functional requirements. An important modern development is that\nthe complete life cycle of an artifact is now considered to be the\ndesigning engineer’s concern, up till the final stages of the\nrecycling and disposal of its components and materials, and the\nfunctional requirements of any device should reflect this. From this\npoint of view, neither a blueprint nor a prototype can be considered\nthe end product of engineering design. \nThe biggest idealization that this scheme of the design process\ncontains is arguably located at the start. Only in a minority of cases\ndoes a design task originate in a customer need or wish for a\nparticular artifact. First of all, as already suggested, many design\ntasks are defined by engineers themselves, for instance, by noticing\nsomething to be improved in existing products. But more often than not\ndesign starts with a problem pointed out by some societal agent, which\nengineers are then invited to solve. Many such problems, however, are\nill-defined or wicked problems, meaning that it is not at all\nclear what the problem is exactly and what a solution to the problem\nwould consist in. The ‘problem’ is a situation that\npeople—not necessarily the people ‘in’ the\nsituation—find unsatisfactory, but typically without being able\nto specify a situation that they find more satisfactory in other terms\nthan as one in which the problem has been solved. In particular it is\nnot obvious that a solution to the problem would consist in some\nartifact, or some artifactual system or process, being made available\nor installed. Engineering departments all over the world advertise\nthat engineering is problem solving, and engineers easily seem\nconfident that they are best qualified to solve a problem when they\nare asked to, whatever the nature of the problem. This has led to the\nphenomenon of a technological fix, the solution of a problem\nby a technical solution, that is, the delivery of an artifact or\nartifactual process, where it is questionable, to say the least,\nwhether this solves the problem or whether it was the best way of\nhandling the problem. \nA candidate example of a technological fix for the problem of global\nwarming would be the currently much debated option of injecting\nsulfate aerosols into the stratosphere to offset the warming effect of\ngreenhouse gases such as carbon dioxide and methane. Such schemes of\ngeoengineering would allow us to avoid facing the—in all\nlikelihood painful—choices that will lead to a reduction of the\nemission of greenhouse gases into the atmosphere, but will at the same\ntime allow the depletion of the Earth’s reservoir of fossil\nfuels to continue. See for a discussion of technological fixing, e.g.,\nVolti 2009: 26–32. Given this situation, and its hazards, the\nnotion of a problem and a taxonomy of problems deserve to receive more\nphilosophical attention than they have hitherto received. \nThese wicked problems are often broadly social problems, which would\nbest be met by some form of ‘social action’, which would\nresult in people changing their behavior or acting differently in such\na way that the problem would be mitigated or even disappear\ncompletely. In defense of the engineering view, it could perhaps be\nsaid that the repertoire of ‘proven’ forms of social\naction is meager. The temptation of technical fixes could be\novercome—at least that is how an engineer might see it—by\nthe inclusion of the social sciences in the systematic development and\napplication of knowledge to the solution of human problems. This\nhowever, is a controversial view. Social engineering is to\nmany a specter to be kept at as large a distance as possible instead\nof an ideal to be pursued. Karl Popper referred to acceptable forms of\nimplementing social change as ‘piecemeal social\nengineering’ and contrasted it to the revolutionary but\ncompletely unfounded schemes advocated by, e.g., Marxism. In the entry\non\n Karl Popper,\n however, his choice of words is called ‘rather\nunfortunate’. The notion of social engineering, and its cogency,\ndeserves more attention that it is currently receiving. \nAn important input for the design process is scientific knowledge:\nknowledge about the behavior of components and the materials they are\ncomposed of in specific circumstances. This is the point where science\nis applied. However, much of this knowledge is not directly available\nfrom the sciences, since it often concerns extremely detailed behavior\nin very specific circumstances. This scientific knowledge is therefore\noften generated within technology, by the engineering sciences. But\napart from this very specific scientific knowledge, engineering design\ninvolves various other sorts of knowledge. In his book What\nEngineers Know and How They Know It (Vincenti 1990), the\naeronautical engineer Walter Vincenti gave a six-fold categorization\nof engineering design knowledge (leaving aside production and\noperation as the other two basic constituents of engineering\npractice). Vincenti distinguishes \nThe fourth category concerns the quantitative knowledge just referred\nto, and the third the theoretical tools used to acquire it. These two\ncategories can be assumed to match Bunge’s notion of substantive\ntechnological theories. The status of the remaining four categories is\nmuch less clear, however, partly because they are less familiar, or\nnot at all, from the well-explored context of science. Of these\ncategories, Vincenti claims that they represent prescriptive forms of\nknowledge rather than descriptive ones. Here, the activity of design\nintroduces an element of normativity, which is absent from scientific\nknowledge. Take such a basic notion as ‘operational\nprinciple’, which refers to the way in which the function of a\ndevice is realized, or, in short, how it works. This is still a purely\ndescriptive notion. Subsequently, however, it plays a role in\narguments that seek to prescribe a course of action to someone who has\na goal that could be realized by the operation of such a device. At\nthis stage, the issue changes from a descriptive to a prescriptive or\nnormative one. An extensive discussion of the various kinds of\nknowledge relevant to technology is offered by Houkes (2009). \nAlthough the notion of an operational principle—a term that\nseems to originate with Polanyi (1958)—is central to engineering\ndesign, no single clear-cut definition of it seems to exist. The issue\nof disentangling descriptive from prescriptive aspects in an analysis\nof the technical action and its constituents is therefore a task that\nhas hardly begun. This task requires a clear view on the extent and\nscope of technology. If one follows Joseph Pitt in his book\nThinking About Technology (1999) and defines technology\nbroadly as ‘humanity at work’, then to distinguish between\ntechnological action and action in general becomes difficult, and the\nstudy of technological action must absorb all descriptive and\nnormative theories of action, including the theory of practical\nrationality, and much of theoretical economics in its wake. There have\nindeed been attempts at such an encompassing account of human action,\nfor example Tadeusz Kotarbinski’s Praxiology (1965),\nbut a perspective of such generality makes it difficult to arrive at\nresults of sufficient depth. It would be a challenge for philosophy to\nspecify the differences among action forms and the reasoning grounding\nthem in, to single out three prominent fields of study, technology,\norganization and management, and economics. \nA more restricted attempt at such an approach is Ilkka\nNiiniluoto’s (1993). According to Niiniluoto, the theoretical\nframework of technology as the practice that is concerned with what\nthe world should be like rather than is, the framework that forms the\ncounterpoint to the descriptive framework of science, is design\nscience. The content of design science, the counterpoint to the\ntheories and explanations that form the content of descriptive\nscience, would then be formed by technical norms, statements\nof the form ‘If one wants to achieve X, one should do\nY’. The notion of a technical norm derives from Georg\nHenrik von Wright’s Norm and Action (1963). Technical\nnorms need to be distinguished from anankastic statements expressing\nnatural necessity, of the form ‘If X is to be achieved,\nY needs to be done’; the latter have a truth value but\nthe former have not. Von Wright himself, however, wrote that he did\nnot understand the mutual relations between these statements. Ideas on\nwhat design science is and can and should be are evidently related to\nthe broad problem area of practical rationality—see this\nencyclopedia’s entries on\n practical reason\n and\n instrumental rationality—and\n also to means-ends reasoning, discussed in the next section. \nDesign is an activity that is subject to rational scrutiny but in\nwhich creativity is considered to play an important role as well.\nSince design is a form of action, a structured series of decisions to\nproceed in one way rather than another, the form of rationality that\nis relevant to it is practical rationality, the rationality\nincorporating the criteria on how to act, given particular\ncircumstances. This suggests a clear division of labor between the\npart to be played by rational scrutiny and the part to be played by\ncreativity. Theories of rational action generally conceive their\nproblem situation as one involving a choice among various course of\naction open to the agent. Rationality then concerns the question how\nto decide among given options, whereas creativity concerns the\ngeneration of these options. This distinction is similar to the\ndistinction between the context of justification and the context of\ndiscovery in science. The suggestion that is associated with this\ndistinction, however, that rational scrutiny only applies in the\ncontext of justification, is difficult to uphold for technological\ndesign. If the initial creative phase of option generation is\nconducted sloppily, the result of the design task can hardly be\nsatisfactory. Unlike the case of science, where the practical\nconsequences of entertaining a particular theory are not taken into\nconsideration, the context of discovery in technology is governed by\nsevere constraints of time and money, and an analysis of the problem\nhow best to proceed certainly seems in order. There has been little\nphilosophical work done in this direction; an overview of the issues\nis given in Kroes, Franssen, and Bucciarelli (2009). \nThe ideas of Herbert Simon on bounded rationality (see, e.g., Simon\n1982) are relevant here, since decisions on when to stop generating\noptions and when to stop gathering information about these options and\nthe consequences when they are adopted are crucial in decision making\nif informational overload and calculative intractability are to be\navoided. However, it has proved difficult to further develop\nSimon’s ideas on bounded rationality since their conception in\nthe 1950s. Another notion that is relevant here is means-ends\nreasoning. In order to be of any help here, theories of means-ends\nreasoning should then concern not just the evaluation of given means\nwith respect to their ability to achieve given ends, but also the\ngeneration or construction of means for given ends. A comprehensive\ntheory of means-ends reasoning, however, is not yet available; for a\nproposal on how to develop means-ends reasoning in the context of\ntechnical artifacts, see Hughes, Kroes, and Zwart 2007. In the\npractice of technology, alternative proposals for the realization of\nparticular functions are usually taken from ‘catalogs’ of\nexisting and proven realizations. These catalogs are extended by\nongoing research in technology rather than under the urge of\nparticular design tasks. \nWhen engineering design is conceived as a process of decision making,\ngoverned by considerations of practical rationality, the next step is\nto specify these considerations. Almost all theories of practical\nrationality conceive of it as a reasoning process where a match\nbetween beliefs and desires or goals is sought. The desires or goals\nare represented by their value or utility for the decision maker, and\nthe decision maker’s problem is to choose an action that\nrealizes a situation that, ideally, has maximal value or utility among\nall the situations that could be realized. If there is uncertainty\nconcerning he situations that will be realized by a particular action,\nthen the problem is conceived as aiming for maximal expected\nvalue or utility. Now the instrumental perspective on technology\nimplies that the value that is at issue in the design process viewed\nas a process of rational decision making is not the value of the\nartifacts that are created. Those values are the domain of the\nusers of the technology so created. They are supposed to be\nrepresented in the functional requirements defining the design task.\nInstead the value to be maximized is the extent to which a particular\ndesign meets the functional requirements defining the design task. It\nis in this sense that engineers share an overall perspective on\nengineering design as an exercise in optimization. But\nalthough optimization is a value-orientated notion, it is not itself\nperceived as a value driving engineering design. \nThe functional requirements that define most design problems do not\nprescribe explicitly what should be optimized; usually they set levels\nto be attained minimally. It is then up to the engineer to choose how\nfar to go beyond meeting the requirements in this minimal sense.\nEfficiency, in energy consumption and use of materials first\nof all, is then often a prime value. Under the pressure of society,\nother values have come to be incorporated, in particular\nsafety and, more recently, sustainability. Sometimes\nit is claimed that what engineers aim to maximize is just one factor,\nnamely market success. Market success, however, can only be assessed\nafter the fact. The engineer’s maximization effort will instead\nbe directed at what are considered the predictors of market success.\nMeeting the functional requirements and being relatively efficient and\nsafe are plausible candidates as such predictors, but additional\nmethods, informed by market research, may introduce additional factors\nor may lead to a hierarchy among the factors. \nChoosing the design option that maximally meets all the functional\nrequirements (which may but need not originate with the prospective\nuser) and all other considerations and criteria that are taken to be\nrelevant, then becomes the practical decision-making problem to be\nsolved in a particular engineering-design task. This creates several\nmethodological problems. Most important of these is that the engineer\nis facing a multi-criteria decision problem. The various\nrequirements come with their own operationalizations in terms of\ndesign parameters and measurement procedures for assessing their\nperformance. This results in a number of rank orders or quantitative\nscales which represent the various options out of which a choice is to\nbe made. The task is to come up with a final score in which all these\nresults are ‘adequately’ represented, such that the option\nthat scores best can be considered the optimal solution to the design\nproblem. Engineers describe this situation as one where\ntrade-offs have to be made: in judging the merit of one\noption relative to other options, a relative bad performance on one\ncriterion can be balanced by a relatively good performance on another\ncriterion. An important problem is whether a rational method for doing\nthis can be formulated. It has been argued by Franssen (2005) that\nthis problem is structurally similar to the well-known problem of\nsocial choice, for which Kenneth Arrow proved his notorious\nimpossibility theorem in 1950, implying that no general rational\nsolution method exists for this problem. This poses serious problems\nfor the claim of engineers that their designs are optimal solutions,\nsince Arrow’s theorem implies that in most multi-criteria\nproblems the notion of ‘optimal’ cannot be rigorously\ndefined. \nThis result seems to except a crucial aspect of engineering activity\nfrom philosophical scrutiny, and it could be used to defend the\nopinion that engineering is at least partly an art, not a science.\nInstead of surrendering to the result, however, which has a\nsignificance that extends much beyond engineering and even beyond\ndecision making in general, we should perhaps conclude instead that\nthere is still a lot of work to be done on what might be termed,\nprovisionally, ‘approximative’ forms of reasoning. One\nform of reasoning to be included here is Herbert Simon’s bounded\nrationality, plus the related notion of ‘satisficing’.\nSince their introduction in the 1950s (Simon 1957) these two terms\nhave found wide usage, but we are still lacking a general theory of\nbounded rationality. It may be in the nature of forms of approximative\nreasoning such as bounded rationality that a general theory cannot be\nhad, but even a systematic treatment from which such an insight could\nemerge seems to be lacking. \nAnother problem for the decision-making view of engineering design is\nthat in modern technology almost all design is done by teams. Such\nteams are composed of experts from many different disciplines. Each\ndiscipline has its own theories, its own models of interdependencies,\nits own assessment criteria, and so forth, and the professionals\nbelonging to these disciplines must be considered as inhabitants of\ndifferent object worlds, as Louis Bucciarelli (1994) phrases\nit. The different team members are, therefore, likely to disagree on\nthe relative rankings and evaluations of the various design options\nunder discussion. Agreement on one option as the overall best one can\nhere be even less arrived at by an algorithmic method exemplifying\nengineering rationality. Instead, models of social interaction, such\nas bargaining and strategic thinking, are relevant here. An example of\nsuch an approach to an (abstract) design problem is presented by\nFranssen and Bucciarelli (2004). \nTo look in this way at technological design as a decision-making\nprocess is to view it normatively from the point of view of practical\nor instrumental rationality. At the same time it is descriptive in\nthat it is a description of how engineering methodology generally\npresents the issue how to solve design problems. From that somewhat\nhigher perspective there is room for all kinds of normative questions\nthat are not addressed here, such as whether the functional\nrequirements defining a design problem can be seen as an adequate\nrepresentation of the values of the prospective users of an artifact\nor a technology, or by which methods values such as safety and\nsustainability can best be elicited and represented in the design\nprocess. These issues will be taken up in\n Section 3. \nUnderstanding the process of designing artifacts is the theme in\nphilosophy of technology that most directly touches on the interests\nof engineering practice. This is hardly true for another issue of\ncentral concern to analytic philosophy of technology, which is the\nstatus and the character of artifacts. This is perhaps not unlike the\nsituation in the philosophy of science, where working scientists seem\nalso to be much less interested in investigating the status and\ncharacter of models and theories than philosophers are. \nArtifacts are man-made objects: they have an author (see Hilpinen 1992\nand Hilpinen’s article\n artifact\n in this encyclopedia). The artifacts that are of relevance to\ntechnology are, in particular, made to serve a purpose. This excludes,\nwithin the set of all man-made objects, on the one hand byproducts and\nwaste products and on the other hand works of art. Byproducts and\nwaste products result from an intentional act to make something but\njust not precisely, although the author at work may be well aware of\ntheir creation. Works of art result from an intention directed at\ntheir creation (although in exceptional cases of conceptual art, this\ndirectedness may involve many intermediate steps) but it is contested\nwhether artists include in their intentions concerning their work an\nintention that the work serves some purpose. A further discussion of\nthis aspect belongs to the philosophy of art. An interesting general\naccount has been presented by Dipert (1993). \nTechnical artifacts, then, are made to serve some purpose, generally\nto be used for something or to act as a component in a larger\nartifact, which in its turn is either something to be used or again a\ncomponent. Whether end product or component, an artifact is ‘for\nsomething’, and what it is for is called the artifact’s\nfunction. Several researchers have emphasized that an\nadequate description of artifacts must refer both to their status as\ntangible physical objects and to the intentions of the people engaged\nwith them. Kroes and Meijers (2006) have dubbed this view “the\ndual nature of technical artifacts”; its most mature formulation\nis Kroes 2012. They suggest that the two aspects are ‘tied\nup’, so to speak, in the notion of artifact function. This gives\nrise to several problems. One, which will be passed over quickly\nbecause little philosophical work seems to have been done concerning\nit, is that structure and function mutually constrain each other, but\nthe constraining is only partial. It is unclear whether a general\naccount of this relation is possible and what problems need to be\nsolved to arrive there. There may be interesting connections with the\nissue of multiple realizability in the philosophy of mind and with\naccounts of reduction in science; an example where this is explored is\nMahner and Bunge 2001. \nIt is equally problematic whether a unified account of the notion of\nfunction as such is possible, but this issue has received considerably\nmore philosophical attention. The notion of function is of paramount\nimportance for characterizing artifacts, but the notion is used much\nmore widely. The notion of an artifact’s function seems to refer\nnecessarily to human intentions. Function is also a key concept in\nbiology, however, where no intentionality plays a role, and it is a\nkey concept in cognitive science and the philosophy of mind, where it\nis crucial in grounding intentionality in non-intentional, structural\nand physical properties. Up till now there is no accepted general\naccount of function that covers both the intentionality-based notion\nof artifact function and the non-intentional notion of biological\nfunction—not to speak of other areas where the concept plays a\nrole, such as the social sciences. The most comprehensive theory, that\nhas the ambition to account for the biological notion, cognitive\nnotion and the intentional notion, is Ruth Millikan’s 1984; for\ncriticisms and replies, see B. Preston 1998, 2003; Millikan 1999;\nVermaas & Houkes 2003; and Houkes & Vermaas 2010. The\ncollection of essays edited by Ariew, Cummins and Perlman (2002)\npresents a recent introduction to the general topic of defining the\nnotion of function in general, although the emphasis is, as is\ngenerally the case in the literature on function, on biological\nfunctions. \nAgainst the view that, at least in the case of artifacts, the notion\nof function refers necessarily to intentionality, it could be argued\nthat in discussing the functions of the components of a larger device,\nand the interrelations between these functions, the intentional\n‘side’ of these functions is of secondary importance only.\nThis, however, would be to ignore the possibility of the\nmalfunctioning of such components. This notion seems to be\ndefinable only in terms of a mismatch between actual behavior and\nintended behavior. The notion of malfunction also sharpens an\nambiguity in the general reference to intentions when characterizing\ntechnical artifacts. These artifacts usually engage many people, and\nthe intentions of these people may not all pull in the same direction.\nA major distinction can be drawn between the intentions of the actual\nuser of an artifact for a particular purpose and the intentions of the\nartifact’s designer. Since an artifact may be used for a purpose\ndifferent from the one for which its designer intended it to be used,\nand since people may also use natural objects for some purpose or\nother, one is invited to allow that artifacts can have multiple\nfunctions, or to enforce a hierarchy among all relevant intentions in\ndetermining the function of an artifact, or to introduce a\nclassification of functions in terms of the sorts of determining\nintentions. In the latter case, which is a sort of middle way between\nthe two other options, one commonly distinguishes between the\nproper function of an artifact as the one intended by its\ndesigner and the accidental function of the artifact as the\none given to it by some user on private considerations. Accidental use\ncan become so common, however, that the original function drops out of\nmemory. \nClosely related to this issue to what extent use and design determine\nthe function of an artifact is the problem of characterizing artifact\nkinds. It may seem that we use functions to classify artifacts: an\nobject is a knife because it has the function of cutting, or more\nprecisely, of enabling us to cut. On closer inspection, however, the\nlink between function and kind-membership seems much less\nstraightforward. The basic kinds in technology are, for example,\n‘knife’, ‘aircraft’ and ‘piston’.\nThe members of these kinds have been designed in order to be used to\ncut something with, to transport something through the air and to\ngenerate mechanical movement through thermodynamic expansion. However,\none cannot create a particular kind of artifact just by designing\nsomething with the intention that it be used for some particular\npurpose: a member of the kind so created must actually be useful for\nthat purpose. Despite innumerable design attempts and claims, the\nperpetual motion machine is not a kind of artifact. A kind like\n‘knife’ is defined, therefore, not only by the intentions\nof the designers of its members that they each be useful for cutting\nbut also by a shared operational principle known to these designers,\nand on which they based their design. This is, in a different setting,\nalso defended by Thomasson, who in her characterization of what she in\ngeneral calls an artifactual kind says that such a kind is\ndefined by the designer’s intention to make something of that\nkind, by a substantive idea that the designer has of how this can be\nachieved, and by his or her largely successful achievement of it\n(Thomasson 2003, 2007). Qua sorts of kinds in which artifacts can be\ngrouped, a distinction must therefore be made between a kind like\n‘knife’ and a corresponding but different kind\n‘cutter’. A ‘knife’ indicates a particular way\na ‘cutter’ can be made. One can also cut, however, with a\nthread or line, a welding torch, a water jet, and undoubtedly by other\nsorts of means that have not yet been thought of. A\n‘cutter’ would then refer to a truly functional kind. As\nsuch, it is subject to the conflict between use and design: one could\nmean by ‘cutter’ anything than can be used for cutting or\nanything that has been designed to be used for cutting, by the\napplication of whatever operational principle, presently known or\nunknown. \nThis distinction between artifact kinds and functional kinds is\nrelevant for the status of such kinds in comparison to other notions\nof kinds. Philosophy of science has emphasized that the concept of\nnatural kind, such as exemplified by ‘water’ or\n‘atom’, lies at the basis of science. On the other hand it\nis generally taken for granted that there are no regularities that all\nknives or airplanes or pistons answer to. This, however, is loosely\nbased on considerations of multiple realizability that fully apply\nonly to functional kinds, not to artifact kinds. Artifact kinds share\nan operational principle that gives them some commonality in physical\nfeatures, and this commonality becomes stronger once a particular\nartifact kind is subdivided into narrower kinds. Since these kinds are\nspecified in terms of physical and geometrical parameters, they are\nmuch closer to the natural kinds of science, in that they support\nlaw-like regularities; see for a defense of this position (Soavi\n2009). A recent\ncollection of essays that discuss the metaphysics of artifacts and\nartifact kinds is Franssen, Kroes, Reydon and Vermaas 2014. \nThere is at least one additional technology-related topic that ought\nto be mentioned because it has created a good deal of analytic\nphilosophical literature, namely Artificial Intelligence and related\nareas. A full discussion of this vast field is beyond the scope of\nthis entry, however. Information is to be found in the entries on\n Turing machines,\n the Church-Turing thesis,\n computability and complexity,\n the Turing test,\n the Chinese room argument,\n the computational theory of mind,\n functionalism,\n multiple realizability, and\n the philosophy of computer science. \nIt was not until the twentieth century that the development of the\nethics of technology as a systematic and more or less independent\nsubdiscipline of philosophy started. This late development may seem\nsurprising given the large impact that technology has had on society,\nespecially since the industrial revolution. \nA plausible reason for this late development of ethics of technology\nis the instrumental perspective on technology that was mentioned in\n Section 2.2.\n This perspective implies, basically, a positive ethical assessment of\ntechnology: technology increases the possibilities and capabilities of\nhumans, which seems in general desirable. Of course, since antiquity,\nit has been recognized that the new capabilities may be put to bad use\nor lead to human hubris. Often, however, these undesirable\nconsequences are attributed to the users of technology, rather than\nthe technology itself, or its developers. This vision is known as the\ninstrumental vision of technology resulting in the so-called\nneutrality thesis. The neutrality thesis holds that technology is a\nneutral instrument that can be put to good or bad use by its users.\nDuring the twentieth century, this neutrality thesis met with severe\ncritique, most prominently by Heidegger and Ellul, who have been\nmentioned in this context in\n Section 2,\nbut also by philosophers from the Frankfurt School, such as Horkheimer\nand Adorno (1947 [2002]), Marcuse (1964), and Habermas (1968\n[1970]). \nThe scope and the agenda for ethics of technology to a large extent\ndepend on how technology is conceptualized. The second half of the\ntwentieth century has witnessed a richer variety of conceptualizations\nof technology that move beyond the conceptualization of technology as\na neutral tool, as a world view or as a historical necessity. This\nincludes conceptualizations of technology as a political phenomenon\n(Winner, Feenberg, Sclove), as a social activity (Latour, Callon, Bijker and others in the\narea of science and technology studies), as a cultural phenomenon\n(Ihde, Borgmann), as a professional activity (engineering ethics,\ne.g., Davis), and as a cognitive activity (Bunge, Vincenti). Despite\nthis diversity, the development in the second half of the twentieth\ncentury is characterized by two general trends. One is a move away\nfrom technological determinism and the assumption that technology is a\ngiven self-contained phenomenon which develops autonomously to an\nemphasis on technological development being the result of choices\n(although not necessarily the intended result). The other is a move\naway from ethical reflection on technology as such to ethical\nreflection of specific technologies and to specific phases in the\ndevelopment of technology. Both trends together have resulted in an\nenormous increase in the number and scope of ethical questions that\nare asked about technology. The developments also imply that ethics of\ntechnology is to be adequately empirically informed, not only about\nthe exact consequences of specific technologies but also about the\nactions of engineers and the process of technological development.\nThis has also opened the way to the involvement of other disciplines\nin ethical reflections on technology, such as Science and Technology\nStudies (STS) and Technology Assessment (TA). \nNot only is the ethics of technology characterized by a diversity of\napproaches, it might even be doubted whether something like a\nsubdiscipline of ethics of technology, in the sense of a community of\nscholars working on a common set of problems, exists. The scholars\nstudying ethical issues in technology have diverse backgrounds (e.g.,\nphilosophy, STS, TA, law, political science) and they do not always\nconsider themselves (primarily) ethicists of technology. To give the\nreader an overview of the field, three basic approaches or strands\nthat might be distinguished in the ethics of technology will be\ndiscussed. \nBoth cultural and political approaches build on the traditional\nphilosophy and ethics of technology of the first half of the twentieth\ncentury. Whereas cultural approaches conceive of technology as a\ncultural phenomenon that influences our perception of the world,\npolitical approaches conceive of technology as a political phenomenon,\ni.e., as a phenomenon that is ruled by and embodies institutional\npower relations between people. \nCultural approaches are often phenomenological in nature or at least\nposition themselves in relation to phenomenology as\npost-phenomenology. Examples of philosophers in this tradition are Don\nIhde, Albert Borgmann, Peter-Paul Verbeek and Evan Selinger (e.g.,\nBorgmann 1984; Ihde 1990; Verbeek 2000 [2005], 2011). The approaches are\nusually influenced by developments in STS, especially the idea that\ntechnologies contain a script that influences not only people’s\nperception of the world but also human behavior, and the idea of the\nabsence of a fundamental distinction between humans and non-humans,\nincluding technological artifacts (Akrich 1992; Latour 1992, 1993;\nIhde & Selinger 2003). The combination of both ideas has led some\nto claim that technology has (moral) agency, a claim that is discussed\nbelow in\n Section 3.3.1. \nPolitical approaches to technology mostly go back to Marx, who assumed\nthat the material structure of production in society, in which\ntechnology is obviously a major factor, determined the economic and\nsocial structure of that society. Similarly, Langdon Winner has argued\nthat technologies can embody specific forms of power and authority\n(Winner 1980). According to him, some technologies are inherently\nnormative in the sense that they require or are strongly compatible\nwith certain social and political relations. Railroads, for example,\nseem to require a certain authoritative management structure. In other\ncases, technologies may be political due to the particular way they\nhave been designed. Some political approaches to technology are\ninspired by (American) pragmatism and, to a lesser extent, discourse\nethics. A number of philosophers, for example, have pleaded for a\ndemocratization of technological development and the inclusion of\nordinary people in the shaping of technology (Winner 1983; Sclove\n1995; Feenberg 1999). \nAlthough political approaches have obviously ethical ramifications,\nmany philosophers who have adopted such approaches do not engage in\nexplicit ethical reflection on technology. An interesting recent\nexception, and an attempt to consolidate a number of recent\ndevelopments and to articulate them into a more general account of\nwhat an ethics of technology should look like, is the volume\nPragmatist Ethics for a Technological Culture (Keulartz et\nal. 2002). In this volume, the authors plead for a revival of the\npragmatist tradition in moral philosophy because it is better fit to\ndeal with a number of moral issues in technology. Instead of focusing\non how to reach and justify normative judgments about technology, a\npragmatist ethics focuses on how to recognize and trace moral problems\nin the first place. Moreover, the process of dealing with these\nproblems is considered more important than the outcome. \nEngineering ethics is a relatively new field of education and\nresearch. It started off in the 1980s in the United States, merely as\nan educational effort. Engineering ethics is concerned with “the\nactions and decisions made by persons, individually or collectively,\nwho belong to the profession of engineering” (Baum 1980: 1).\nAccording to this approach, engineering is a profession, in the same\nway as medicine is a profession. \nAlthough there is no agreement on how a profession exactly should be\ndefined, the following characteristics are often mentioned: \nTypical ethical issues that are discussed in engineering ethics are\nprofessional obligations of engineers as exemplified in, for example,\ncodes of ethics of engineers, the role of engineers versus managers,\ncompetence, honesty, whistle-blowing, concern for safety and conflicts\nof interest (Davis 1998, 2005; Martin & Schinzinger 2005; Harris,\nPritchard, & Rabins 2008). \nRecently, a number of authors have pleaded for broadening the\ntraditional scope of engineering ethics (e.g., Herkert 2001;, van de\nPoel & Royakkers 2011). This call for a broader approach derives\nfrom two concerns. One concern is that the traditional micro-ethical\napproach in engineering ethics tends to take the contexts in which\nengineers have to work for given, while major ethical issues pertain\nto how this context is ‘organized’. Another concern is\nthat the traditional micro-ethical focus tends to neglect issues\nrelating to the impact of technology on society or issues relating to\ndecisions about technology. Broadening the scope of engineering ethics\nwould then, among others, imply more attention for such issues as\nsustainability and social justice. \nThe last decades have witnessed an increase in ethical inquiries into\nspecific technologies. This may now be the largest of the three\nstrands discussed, especially given the rapid growth in\ntechnology-specific ethical inquiries in the last two decades. One of\nthe most visible new fields is probably computer ethics (e.g., Moor\n1985; Floridi 2010; Johnson 2009; Weckert 2007; van den Hoven &\nWeckert 2008), with more recently a focus on robotics, artificial\nintelligence, machine ethics, and the ethics of algorithms (Lin,\nAbney, & Jenkins 2017; Nucci & Santoni de Sio 2016;\nMittelstadt et al. 2016; Bostrom & Yudkowsky 2014; Wallach &\nAllen 2009). But biotechnology has spurred dedicated ethical\ninvestigations as well (e.g., Sherlock & Morrey 2002; P. Thompson\n2007). More traditional fields like architecture and urban planning\nhave also attracted specific ethical attention (Fox 2000). More\nrecently, nanotechnology and so-called converging technologies have\nled to the establishment of what is called nanoethics (Allhoff et al.\n2007). Other examples are the ethics of nuclear deterrence (Finnis et\nal. 1988), nuclear energy (Taebi & Roeser 2015) and geoengineering\n(C. Preston 2016). \nObviously the establishment of such new fields of ethical reflection\nis a response to social and technological developments. Still, the\nquestion can be asked whether the social demand is best met by\nestablishing new fields of applied ethics. This issue is in fact\nregularly discussed as new fields emerge. Several authors have for\nexample argued that there is no need for nanoethics because\nnanotechnology does not raise any really new ethical issues (e.g.,\nMcGinn 2010). The alleged absence of newness here is supported by the\nclaim that the ethical issues raised by nanotechnology are a variation\non, and sometimes an intensification of, existing ethical issues, but\nhardly really new, and by the claim that these issues can be dealt\nwith the existing theories and concepts from moral philosophy. For an\nearlier, similar discussion concerning the supposed new character of\nethical issues in computer engineering, see Tavani 2002. \nThe new fields of ethical reflection are often characterized as\napplied ethics, that is, as applications of theories, normative\nstandards, concepts and methods developed in moral philosophy. For\neach of these elements, however, application is usually not\nstraightforward but requires a further specification or revision. This\nis the case because general moral standards, concepts and methods are\noften not specific enough to be applicable in any direct sense to\nspecific moral problems. ‘Application’ therefore often\nleads to new insights which might well result in the reformulation or\nat least refinement of existing normative standards, concepts and\nmethods. In some cases, ethical issues in a specific field might\nrequire new standards, concepts or methods. Beauchamp and Childress\nfor example have proposed a number of general ethical principles for\nbiomedical ethics (Beauchamp & Childress 2001). These principles\nare more specific than general normative standards, but still so\ngeneral and abstract that they apply to different issues in biomedical\nethics. In computer ethics, existing moral concepts relating to for\nexample privacy and ownership has been redefined and adapted to deal\nwith issues which are typical for the computer age (Johnson 2003). New\nfields of ethical application might also require new methods for, for\nexample, discerning ethical issues that take into account relevant\nempirical facts about these fields, like the fact that technological\nresearch and development usually takes place in networks of people\nrather than by individuals (Zwart et al. 2006). Another more general\nissue that applies to many new technologies is how to deal with the\nuncertainties about (potential) social and ethical impacts that\ntypically surround new emerging technologies. Brey’s (2012)\nproposal for an anticipatory ethics may be seen as a reply to this\nchallenge. The issue of anticipation is also one of the central\nconcerns in the more recent interdisciplinary field of responsible\ninnovation (e.g., Owen et al. 2013). \nAlthough different fields of ethical reflection on specific\ntechnologies might well raise their own philosophical and ethical\nissues, it can be questioned whether this justifies the development of\nseparate subfields or even subdisciplines. One obvious argument might\nbe that in order to say something ethically meaningful about new\ntechnologies, one needs specialized and detailed knowledge of a\nspecific technology. Moreover such subfields allow interaction with\nrelevant non-philosophical experts in for example law, psychology,\neconomy, science and technology studies (STS) or technology assessment\n(TA). On the other side, it could also be argued that a lot can be\nlearned from interaction and discussion between ethicists specializing\nin different technologies, and a fruitful interaction with the two\nother strands discussed above (cultural and political approaches and\nengineering ethics). Currently, such interaction in many cases seems\nabsent, although there are of course exceptions. \nWe now turn to the description of some themes in the ethics of\ntechnology. We focus on a number of general themes that provide an\nillustration of general issues in the ethics of technology and the way\nthese are treated. \nOne important general theme in the ethics of technology is the\nquestion whether technology is value-laden. Some authors have\nmaintained that technology is value-neutral, in the sense that\ntechnology is just a neutral means to an end, and accordingly can be\nput to good or bad use (e.g., Pitt 2000). This view might have some\nplausibility in as far as technology is considered to be just a bare\nphysical structure. Most philosophers of technology, however, agree\nthat technological development is a goal-oriented process and that\ntechnological artifacts by definition have certain functions, so that\nthey can be used for certain goals but not, or far more difficulty or\nless effectively, for other goals. This conceptual connection between\ntechnological artifacts, functions and goals makes it hard to maintain\nthat technology is value-neutral. Even if this point is granted, the\nvalue-ladenness of technology can be construed in a host of different\nways. Some authors have maintained that technology can have moral\nagency. This claim suggests that technologies can autonomously and\nfreely ‘act’ in a moral sense and can be held morally\nresponsible for their actions. \nThe debate whether technologies can have moral agency started off in\ncomputer ethics (Bechtel 1985; Snapper 1985; Dennett 1997; Floridi\n& Sanders 2004) but has since broadened. Typically, the authors\nwho claim that technologies (can) have moral agency often redefine the\nnotion of agency or its connection to human will and freedom (e.g.,\nLatour 1993; Floridi & Sanders 2004, Verbeek 2011). A disadvantage\nof this strategy is that it tends to blur the morally relevant\ndistinctions between people and technological artifacts. More\ngenerally, the claim that technologies have moral agency sometimes\nseems to have become shorthand for claiming that technology is morally\nrelevant. This, however, overlooks the fact technologies can be\nvalue-laden in other ways than by having moral agency (see, e.g.,\nJohnson 2006; Radder 2009; Illies & Meijers 2009; Peterson &\nSpahn 2011). One might, for example, claim that technology enables (or\neven invites) and constrains (or even inhibits) certain human actions\nand the attainment of certain human goals and therefore is to some\nextent value-laden, without claiming moral agency for technological\nartifacts. A good overview of the debate can be found in Kroes and\nVerbeek 2014. \nThe debate about moral agency and technology is now particularly\nsalient with respect to the design of intelligent artificial agents.\nJames Moor (2006) has distinguished between four ways in which\nartificial agents may be or become moral agents: \nIt might perhaps never be possible to technologically design full\nethical agents, and if it were to become possible it might be\nquestionable whether it is morally desirable to do so (Bostrom &\nYudkowsky 2014). As Wallach and Allen (2009) have pointed out, the\nmain problem might not be to design artificial agents that can\nfunction autonomously and that can adapt themselves in interaction\nwith the environment, but rather to build enough, and the right kind\nof, ethical sensitivity into such machines. \nResponsibility has always been a central theme in the ethics of\ntechnology. The traditional philosophy and ethics of technology,\nhowever, tended to discuss responsibility in rather general terms and\nwere rather pessimistic about the possibility of engineers to assume\nresponsibility for the technologies they developed. Ellul, for\nexample, has characterized engineers as the high priests of\ntechnology, who cherish technology but cannot steer it. Hans Jonas\n(1979 [1984]) has argued that technology requires an ethics in which\nresponsibility is the central imperative because for the first time in\nhistory we are able to destroy the earth and humanity. \nIn engineering ethics, the responsibility of engineers is often\ndiscussed in relation to code of ethics that articulate specific\nresponsibilities of engineers. Such codes of ethics stress three types\nof responsibilities of engineers: (1) conducting the profession with\nintegrity and honesty and in a competent way, (2) responsibilities\ntowards employers and clients and (3) responsibility towards the\npublic and society. With respect to the latter, most US codes of\nethics maintain that engineers ‘should hold paramount the\nsafety, health and welfare of the public’. \nAs has been pointed out by several authors (Nissenbaum 1996; Johnson\n& Powers 2005; Swierstra & Jelsma 2006), it may be hard to\npinpoint individual responsibility in engineering. The reason is that\nthe conditions for the proper attribution of individual responsibility\nthat have been discussed in the philosophical literature (like freedom\nto act, knowledge, and causality) are often not met by individual\nengineers. For example, engineers may feel compelled to act in a\ncertain way due to hierarchical or market constraints, and negative\nconsequences may be very hard or impossible to predict beforehand. The\ncausality condition is often difficult to meet as well due to the long\nchain from research and development of a technology till its use and\nthe many people involved in this chain. Davis (2012) nevertheless\nmaintains that despite such difficulties individual engineers can and\ndo take responsibility. \nOne issue that is at stake in this debate is the notion of\nresponsibility. Davis (2012), and also for example Ladd (1991), argue\nfor a notion of responsibility that focuses less on blame and stresses\nthe forward-looking or virtuous character of assuming responsibility.\nBut many others focus on backward-looking notions of responsibility\nthat stress accountability, blameworthiness or liability. Zandvoort\n(2000), for example has pleaded for a notion of responsibility in\nengineering that is more like the legal notion of strict liability, in\nwhich the knowledge condition for responsibility is seriously\nweakened. Doorn (2012) compares three perspectives on responsibility\nascription in engineering—a merit-based, a right-based and a\nconsequentialist perspective—and argues that the\nconsequentialist perspective, which applies a forward-looking notion\nof responsibility, is most powerful in influencing engineering\npractice. \nThe difficulty of attributing individual responsibility may lead to\nthe Problem of Many Hands (PMH). The term was first coined by Dennis\nThompson (1980) in an article about the responsibility of public\nofficials. The term is used to describe problems with the ascription\nof individual responsibility in collective settings. Doorn (2010) has\nproposed a procedurals approach, based on Rawls’ reflective\nequilibrium model, to deal with the PMH; other ways of dealing with\nthe PMH include the design of institutions that help to avoid it or an\nemphasis on virtuous behavior in organizations (van de Poel, Royakers,\n& Zwart 2015). \nIn the last decades, increasingly attention is paid not only to\nethical issues that arise during the use of a technology, but also\nduring the design phase. An important consideration behind this\ndevelopment is the thought that during the design phase technologies,\nand their social consequences, are still malleable whereas during the\nuse phase technologies are more or less given and negative social\nconsequences may be harder to avoid or positive effects harder to\nachieve. \nIn computer ethics, an approach known as Value Sensitive Design (VSD)\nhas been developed to explicitly address the ethical nature of design.\nVSD aims at integrating values of ethical importance in engineering\ndesign in a systematic way (Friedman & Kahn 2003). The approach\ncombines conceptual, empirical and technical investigations. There is\nalso a range of other approaches aimed at including values in design.\n‘Design for X’ approaches in engineering aim at including\ninstrumental values (like maintainability, reliability and costs) but\nthey also include design for sustainability, inclusive design, and\naffective design (Holt & Barnes 2010). Inclusive design aims at\nmaking designs accessible to the whole population including, for\nexample, handicapped people and the elderly (Erlandson 2008).\nAffective design aims at designs that evoke positive emotions with the\nusers and so contributes to human well-being. Van de Hoven, Vermaas,\nand van de Poel 2015 gives a good overview of the state-of-the art of\nvalue sensitive design for various values and application domains. \nIf one tries to integrate values into design one may run into the\nproblem of a conflict of values. The safest car is, due to its weight,\nnot likely to be the most sustainability. Here safety and\nsustainability conflict in the design of cars. Traditional methods in\nwhich engineers deal with such conflicts and make trade-off between\ndifferent requirements for design include cost-benefit analysis and\nmultiple criteria analysis. Such methods are, however, beset with\nmethodological problems like those discussed in\n Section 2.4\n (Franssen 2005; Hansson 2007). Van de Poel (2009) discusses various\nalternatives for dealing with value conflicts in design including the\nsetting of thresholds (satisficing), reasoning about values,\ninnovation and diversity. \nThe risks of technology are one of the traditional ethical concerns in\nthe ethics of technology. Risks raise not only ethical issues but\nother philosophical issues, such as epistemological and\ndecision-theoretical issues as well (Roeser et al. 2012). \nRisk is usually defined as the product of the probability of an\nundesirable event and the effect of that event, although there are\nalso other definitions around (Hansson 2004b). In general it seems\ndesirable to keep technological risks as small as possible. The larger\nthe risk, the larger either the likeliness or the impact of an\nundesirable event is. Risk reduction therefore is an important goal in\ntechnological development and engineering codes of ethics often\nattribute a responsibility to engineers in reducing risks and\ndesigning safe products. Still, risk reduction is not always feasible\nor desirable. It is sometimes not feasible, because there are no\nabsolutely safe products and technologies. But even if risk reduction\nis feasible it may not be acceptable from a moral point of view.\nReducing risk often comes at a cost. Safer products may be more\ndifficult to use, more expensive or less sustainable. So sooner or\nlater, one is confronted with the question: what is safe enough? What\nmakes a risk (un)acceptable? \nThe process of dealing with risks is often divided into three stages:\nrisk assessment, risk evaluation and risk management. Of these, the\nsecond is most obviously ethically relevant. However, risk assessment\nalready involves value judgments, for example about which risks should\nbe assessed in the first place (Shrader-Frechette 1991). An important,\nand morally relevant, issue is also the degree of evidence that is\nneeded to establish a risk. In establishing a risk on the basis of a\nbody of empirical data one might make two kinds of mistakes. One can\nestablish a risk when there is actually none (type I error) or one can\nmistakenly conclude that there is no risk while there actually is a\nrisk (type II error). Science traditionally aims at avoiding type I\nerrors. Several authors have argued that in the specific context of\nrisk assessment it is often more important to avoid type II errors\n(Cranor 1990; Shrader-Frechette 1991). The reason for this is that\nrisk assessment not just aims at establishing scientific truth but has\na practical aim, i.e., to provide the knowledge on basis of which\ndecisions can be made about whether it is desirable to reduce or avoid\ncertain technological risks in order to protect users or the\npublic. \nRisk evaluation is carried out in a number of ways (see, e.g.,\nShrader-Frechette 1985). One possible approach is to judge the\nacceptability of risks by comparing them to other risks or to certain\nstandards. One could, for example, compare technological risks with\nnaturally occurring risks. This approach, however, runs the danger of\ncommitting a naturalistic fallacy: naturally occurring risks may\n(sometimes) be unavoidable but that does not necessarily make them\nmorally acceptable. More generally, it is often dubious to judge the\nacceptability of the risk of technology A by comparing it to the risk\nof technology B if A and B are not alternatives in a decision (for\nthis and other fallacies in reasoning about risks, see Hansson\n2004a). \nA second approach to risk evaluation is risk-cost benefit analysis,\nwhich is based on weighing the risks against the benefits of an\nactivity. Different decision criteria can be applied if a (risk) cost\nbenefit analysis is carried out (Kneese, Ben-David, and Schulze 1983).\nAccording to Hansson (2003: 306), usually the following criterion is\napplied:  \n… a risk is acceptable if and only if the total benefits that\nthe exposure gives rise to outweigh the total risks, measured as the\nprobability-weighted disutility of outcomes. \nA third approach is to base risk acceptance on the consent of people\nwho suffer the risks after they have been informed about these risks\n(informed consent). A problem of this approach is that technological\nrisks usually affect a large number of people at once. Informed\nconsent may therefore lead to a “society of stalemates”\n(Hansson 2003: 300). \nSeveral authors have proposed alternatives to the traditional\napproaches of risk evaluation on the basis of philosophical and\nethical arguments. Shrader-Frechette (1991) has proposed a number of\nreforms in risk assessment and evaluation procedures on the basis of a\nphilosophical critique of current practices. Roeser (2012) argues for\na role of emotions in judging the acceptability of risks. Hansson has\nproposed the following alternative principle for risk evaluation:  \nExposure of a person to a risk is acceptable if and only if this\nexposure is part of an equitable social system of risk-taking that\nworks to her advantage. (Hansson 2003: 305)  \nHansson’s proposal introduces a number of moral considerations\nin risk evaluation that are traditionally not addressed or only\nmarginally addressed. These are the consideration whether individuals\nprofit from a risky activity and the consideration whether the\ndistribution of risks and benefits is fair. \nSome authors have criticized the focus on risks in the ethics of\ntechnology. One strand of criticism argues that we often lack the\nknowledge to reliably assess the risks of a new technology before it\nhas come into use. We often do not know the probability that something\nmight go wrong, and sometimes we even do not know, or at least not\nfully, what might go wrong and what possible negative consequences may\nbe. To deal with this, some authors have proposed to conceive of the\nintroduction of new technology in society as a social experiment and\nhave urged to think about the conditions under which such experiments\nare morally acceptable (Martin & Schinzinger 2005; van de Poel\n2016). Another strand of criticism states that the focus on risks has\nled to a reduction of the impacts of technology that are considered\n(Swierstra & te Molder 2012). Only impacts related to safety and\nhealth, which can be calculated as risks, are considered, whereas\n‘soft’ impacts, for example of a social or psychological\nnature, are neglected, thereby impoverishing the moral evaluation of\nnew technologies.","contact.mail":"I.R.vandepoel@tudelft.nl","contact.domain":"tudelft.nl"}]
