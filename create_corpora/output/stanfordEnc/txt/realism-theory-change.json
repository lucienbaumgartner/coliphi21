[{"date.published":"2018-05-03","url":"https://plato.stanford.edu/entries/realism-theory-change/","author1":"Stathis Psillos","author1.info":"http://users.uoa.gr/~psillos/","entry":"realism-theory-change","body.text":"\n\n\nScientific theories seem to have an expiry date. If we look at the\nhistory of science, a number of theories that once were dominant and\nwidely accepted are currently taught in history of science courses.\nWill this be the fate of current scientific theories? Is there a\npattern of radical theory-change as science grows? Are theories\nabandoned en bloc? Or are there patterns of retention in\ntheory-change? That is, are some parts of theories more likely to\nsurvive than other parts? And what are the implications of all this\nfor the scientific image of the world?\n\n\nThese kinds of question have played a major role in the scientific\nrealism debate. The challenge to scientific realism is supposed to\ncome directly from the history of science. The history of science, it\nis claimed, is at odds with scientific realism’s epistemic\noptimism. It is full of theories which were shown to be false and\nabandoned, despite their empirical successes. Hence, it is claimed,\nrealists cannot be warrantedly optimistic about the (approximate)\ntruth of currently empirically successful theories. If we take the\nhistorical evidence seriously, it is claimed, current theories too\nwill, sooner or later, be abandoned and take their place in future\nhistory-of-science courses. This anti-realist line of argument has\nbecome known as ‘the pessimistic induction’ (aka\npessimistic meta-induction)—henceforth PI. Without\ndenying that theories change over time, scientific realists have tried\nto block this line of argument by showing that there is substantive\ncontinuity in theory-change which warrants the realist’s\noptimism that current science is on the right track.\n\n\nThis entry discusses the origin and current state of the historical\nchallenge to realism and the various realist reactions to it. The\nfirst part focuses on the first enactment of arguments based on\nhistorical pessimism, as these appeared in the so-called\n‘bankruptcy of science controversy’ in the end of the\nnineteenth century.\n\n\nThe second part deals with the historical challenge to scientific\nrealism as this is currently formulated and the various lines of\ndefense of the claim that scientific knowledge grows despite\ntheory-change.\n\n\n\n\nThe issue of theory-change in science was debated in the context of\nthe ‘bankruptcy of science’ controversy that was raging in\nParis in the last decade of the nineteenth century and the first\ndecade of the twentieth. A claim of growing popular reputation among\nvarious public intellectuals, spearheaded by Ferdinand\nBrunetière and Leo Tolstoy, was that scientific theories are\nephemeral; and this was supposed to prove that science has at best no\nmore than predictive value with no legitimate claim to showing what\nthe world is like—especially in its unobservable aspects. In\nlight of a growing interest in the history of science among scientists\nand philosophers, it was pointed out that science has had a poor track\nrecord: it has gone through many radical theory-changes in the past;\nhence, there is reason to believe that what is currently\naccepted will be overturned in the future. \nIn his essay “The Non-Acting”, published in French in\nAugust 1893, the Russian novelist Tolstoy (1828–1910) noted: \nLastly, does not each year produce its new scientific discoveries,\nwhich after astonishing the boobies of the whole world and bringing\nfame and fortune to the inventors, are eventually admitted to be\nridiculous mistakes even by those who promulgated them? (…)\nUnless then our century forms an exception (which is a supposition we\nhave no right to make), it needs no great boldness to conclude by\nanalogy that among the kinds of knowledge occupying the attention of\nour learned men and called science, there must necessarily be some\nwhich will be regarded by our descendants much as we now regard the\nrhetoric of the ancients and the scholasticism of the Middle Ages.\n(1904: 105) \nA few years earlier, in 1889, Ferdinand Brunetière\n(1849–1906), Professor at the École Normale\nSupérieure and editor of the prestigious journal Revue\ndes Deux Mondes, noted in his review of Paul Bourget’s play\n‘Le Disciple’: \nWe differ from animals in recognizing that humans have to be first\n(i.e., they have value). The laws of nature, the ‘struggle for\nlife’ or ‘natural selection’, do not show what we\nhave in common. Are these the only laws? Do we know whether perhaps\ntomorrow they will not join in the depths of oblivion the Cartesian\nvortices or the ‘quiddities’ of scholasticism? (1889:\n222, author’s translation) \nThis history-fed pessimism about science, which seemed to capture the\npublic mood, led to a spirited reaction by the scientific community.\nIn an anonymous article that appeared in Revue Scientifique,\na prestigious semi-popular scientific journal, in August 17 1889, the\nfollowing questions were raised: Is the history of science the history\nof human error? Will what theories affirm today be affirmed in a\ncentury or two? The reply was:  \nWe will say to savants, philosophers and physicists, physicians,\nchemists, astronomers or geologists: Go forward boldly, without\nlooking behind you, without caring for the consequences, reasonable or\nabsurd, that can be drawn from your work. Seek the truth, without the\nworry of its applications. (Anonymous 1889: 215, author’s\ntranslation) \nA few years later, in 1895, Brunetière strikes back with an\narticle titled ‘Après Une Visite Au\nVatican’, published in Revue des Deux Mondes, by\nclaiming that science is bankrupt:  \nScience has failed to deliver on its promise to change ‘the face\nof the world’. (...) Even if this is not a total bankruptcy, it is\ncertainly a partial bankruptcy, enough to shake off the credit from\nscience. (1895: 98, 103) \nThe eminent scientist Charles Richet (1850–1935), Professor of\nPhysiology at the Collège de France, Editor of Revue\nScientifique and Nobel Laureate for Medicine in 1913, replied\nwith an article titled ‘La Science a-t-elle fait\nbanqueroute?’ (Revue Scientifique, 12 January\n1895), which appeared in the section: Histoire des Sciences.\nIn this, he did three things. Firstly, he noted that science can never\nunderstand the ‘why’ (‘le pourquoi’)\nof things, especially when it comes to the infinitely small and the\ninfinitely large. Science “attends only to the phenomena. The\nintimate nature of things escapes from us” (1895: 34). Secondly,\nhe stressed that “science has not promised anything”, let\nalone the discovery of the essence of things. Thirdly, he added that\ndespite the fact that science has made no promises, it has changed the\nworld, citing various scientific, industrial and technological\nsuccesses (from the invention of printing and the microscope to the\nrailways, the electric battery, the composition of the air, and the\nnature of fermentation). \nTurning Brunetière’s argument on its head, Richet\nformulated what might be called an ‘optimistic induction’\nbased on the then recent history of scientific successes. To those who\nclaim that science has failed in the past, his reply is that history\nshows that it is unreasonable to claim for any scientific question\nthat we will always fail to answer it. Far from warranting epistemic\npessimism, the history of science is a source of cognitive optimism.\nRichet referred to a few remarkable cases, the most striking of which\nis the case of Jean Louis Prevost and Jean Baptiste Dumas, who had\nwritten in 1823:  \nThe pointlessness of our attempts to isolate the colouring matter of\nthe blood gives us almost the certainty that one will never be able to\nfind it. (1823, 246, author’s translation)  \nForty years after their bold statement, Richet exclaimed, this coloured matter\n(haemoglobin) had been isolated, analysed and studied. \nRichet’s reply to the historical challenge suggested lowering\nthe epistemic bar for science: science describes the phenomena and\ndoes not go beyond them to their (unobservable) causes. This attitude\nwas echoed in the reply to the ‘bankruptcy charge’ issued\nby the eminent chemist and politician of the French Third Republic,\nMarcelin Berthelot (1827–1907) in his pamphlet Science et\nMorale in 1897. He was firm in his claim that the alleged\nbankruptcy of science is an illusion of the non-scientific mind. Like\nRichet, he also argued that science has not pretended to have\npenetrated into the essence of things: “under the words\n‘essence’, ‘the nature of things’, we hide the\nidols of our own imagination” (1897: 18, author’s\ntranslation).  Science, he noted, has as its starting point the study\nof facts and aims to establish general relations, that is,\n‘scientific laws’, on their basis. If science does not aim\nfor more, we cannot claim that it is bankrupt; we cannot accuse it for\n“affirmations it did not make, or hopes it has not\naroused”.[1] \nBerthelot, who objected to atomism, captured a broad positivist trend\nin French science at the end of the nineteenth century, according to\nwhich science cannot offer knowledge of anything other than the\nphenomena. In light of this view, the history-fed pessimism is\nmisguided precisely because there has been substantial continuity at\nthe level of the description of the phenomena, even if explanatory\ntheories have come and gone. \nThis kind of attitude was captured by Pierre Duhem’s (1906)\ndistinction between two parts of a scientific theory: the\nrepresentative part, which classifies a set of experimental laws; and\nthe explanatory part, which “takes hold of the reality\nunderlying the phenomena” (1906 [1954: 32]). Duhem understood\nthe representative part of a theory as comprising the empirical laws\nand the mathematical formalism, which is used to represent,\nsystematize and correlate these laws, while he thought that the\nexplanatory part relates to the construction of physical (and in\nparticular, mechanical) models and explanatory hypotheses about the\nnature of physical processes which purport to reveal underlying\nunobservable causes of the phenomena. For him, the explanatory part is\nparasitic on the representative. To support this view, he turned to\nthe history of science, especially the history of optical theories and\nof mechanics. He argued that when a theory is abandoned because it\nfails to cover new experimental facts and laws, its representative\npart is retained, partially or fully, in its successor theory, while\nthe attempted explanations offered by the theory get abandoned. He\nspoke of the “constant breaking-out of explanations which arise\nto be quelled” (1906 [1954: 33]). \nThough Duhem embedded this claim for continuity in theory-change in an\ninstrumentalist account of scientific theories, he also took it that\nscience aims at a natural classification of the phenomena, where a\nclassification (that is the representation of the phenomena within a\nmathematical system) is natural if the relations it\nestablishes among the phenomena gathered by experiments\n“correspond to real relations among things” (1906 [1954:\n26–27]). Hence, scientific knowledge does go beyond the\nphenomena but in doing so, that is, in tending to be a natural\nclassification, it can extend only up to relations among “hidden\nrealities whose essence cannot be grasped” (1906 [1954: 297]). A\nclear mark of the naturalness of a classification is when it issues in\nnovel predictions (1906 [1954: 28]). Hence, successful novel\npredictions issued by a theory are a mark for the theory getting some\naspects of reality right, viz. real relations among unobservable\n entities.[2] \nThis kind of relationism became a popular middle way between\npositivism and what may be called full-blown realism. Duhem himself,\njustly, traced it back to his contemporary Henri Poincaré. He\nnoted with approval that Poincaré “felt a sort of\nrevolt” against the proposition that “theoretical physics\nis a mere collection of recipes” and he “loudly proclaimed\nthat a physical theory gives us something else than the mere knowledge\nof the facts, that it makes us discover the real relations among\nthings ([1906] 2007: 446; improved translation from the French\noriginal by Marie Guegeun and the author). \nIn his address to the 1900 International Congress of Physics in Paris,\nPoincaré made a definitive intervention in the\nbankruptcy-of-science debate and its history-fed pessimism. He\ndescribed the challenge thus: \nThe people of world [les gens du monde] are struck to see how\nephemeral scientific theories are. After some years of prosperity,\nthey see them successively abandoned; they see ruins accumulated on\nruins; they predict that the theories in fashion today will quickly\nsuccumb in their turn, and they conclude that they are absolutely\nfutile. This is what they call the bankruptcy of science\n(1900: 14, author’s translation). \nThe view of ‘the people of the world’ is not right:  \nTheir scepticism is superficial; they understand none of the aim and\nthe role of scientific theories; otherwise they would understand that\nruins can still be good for something.  \nBut unlike the positivist trend around him, Poincaré took it\nthat scientific theories offer knowledge of the relational structure\nof the world behind the phenomena. In the Introduction to La\nScience et l’Hypothése in 1902, he made clear what\nhe took to be the right answer to the historical challenge: \nWithout doubt, at first, the theories seem to us fragile, and the\nhistory of science proves to us how ephemeral they are; yet they do\nnot entirely perish, and of each of them something remains. It is this\nsomething we must seek to unravel, since there and there alone is the\ntrue reality. (1902: 26, author’s translation) \nPoincaré argued that what survives in theory-change are\nrelations among physical magnitudes, expressed by mathematical\nequations within theories. His prime example was the reproduction of\nFresnel’s laws concerning the relations of amplitudes of\nreflected rays vis-à-vis the amplitude of incident rays in the\ninterface of two media within Maxwell’s theory of\nelectromagnetism, although in this transition, the interpretation of\nthese laws changed dramatically, from an ether-based account to an\nelectromagnetic-field-based account. For Poincaré \nThese equations express relations, and if the equations remain true it\nis because these relations preserve their reality. They teach us,\nbefore and after, that there is such and such a relation between some\nthing and some other thing; only this something we used to\ncall motion, we now call it electric current. But\nthese names were only images substituted for the real objects which\nnature will eternally hide from us. The true relations between these\nreal objects are the only reality we can attain to, and the only\ncondition is that the same relations exist between these objects as\nbetween the images by which we are forced to replace them. If these\nrelations are known, what does it matter if we deem it convenient to\nreplace one image by another?  (1900: 15, author’s translation. \nIn recent literature, Poincaré’s line of thought has come\nto be known as structural realism, though it may be best if\nwe describe it as ‘relationism’. In the Introduction to\nLa Science et l’Hypothése, he noted that  \nthe things themselves are not what it [science] can reach, as the\nnaive dogmatists think, but only the relations between things. Apart\nfrom these relations there is no knowable reality. (1902: 25, author’s\ntranslation)\n \nIt should be stressed that Poincaré does not deny that there is\nreality outside relations; but he does deny that this reality is\nknowable. Note also that Poincaré does not use the\nexpression ‘things in themselves’ (choses en soi)\nbut the expression ‘things themselves’ (chose\nelles-memes). Elsewhere he talks about the “nature of\nthings” or “real objects”. It is quite clear that he\nwanted to draw a distinction between how things are—what their\nnature is—and how they are related to each other (and\nto us qua knowers). A plausible way to draw this distinction\nis to differentiate between the intrinsic and perhaps fully\nqualitative properties of things—what he plausibly calls\n‘nature’ of things—and their relations. The former\nare unknowable, whereas the latter are\n knowable.[3] \nSo, Poincaré and Duhem initiated a strategy for dealing with\ntheory-change in science which pointed to substantial continuities\namong successive theories. For them, the continuity is, by and large,\nrelational (and in this sense mathematical). Hence,\nmathematically-convergent scientific theories reveal the relational\nstructure of the world. \nThis relational answer to historical pessimism was motivated, at least\npartly, by the widespread scepticism towards the atomic theory of\nmatter. Atomism posited the existence of unobservable\nentities—the atoms—to account for a host of observable\nphenomena (from chemical bonding to Brownian motion). A trend among\nscientists opposed to the explanation of the visible in terms of the\ninvisible was what Ludwig Boltzmann called\n“phenomenologists” (which included the early Max Planck),\naccording to whom the aim of science was to “write down for\nevery group of phenomena the equations by means of which their\nbehavior could be quantitatively calculated” (Boltzmann 1901:\n249). The theoretical hypotheses from which the equations might have\nbeen deduced were taken to be the scaffolding that was discarded after\nthe equations were arrived at. For phenomenologists, then, hypotheses\nare not unnecessary or useless—rather they have only a\nheuristic value: they lead to stable (differential) equations\nand that’s it. \nAccording to Boltzmann, a motivation for this phenomenological\nattitude was the “historical principle”, viz., that\nhypotheses are essentially insecure because they tend to be abandoned\nand replaced by others, “totally different” ones. As he\nput it: \nfrequently opinions which are held in the highest esteem have been\nsupplanted within a very short space of time by totally different\ntheories; nay, even as St. Remigius the heathens, so now they [the\nphenomenologists] exhorted the theoretical physicists to consign to\nthe flames the idols that but a moment previously they had worshipped\n(1901: 252–253). \nLike Poincaré, Boltzmann’s answer to historical pessimism\nwas that despite the presence of “revolutions” in science,\nthere is enough continuity in theory change to warrant the claim that\nsome “achievements may possibly remain the possession of science\nfor all time” (1901: 253). But unlike Poincaré, Boltzmann\ndid not restrict the criterion of invariance-in-theory-change to\nrelations only: The answer to the historical challenge is to look\nfor patterns of continuity in theory change. In fact, as\nBoltzmann noted, if the historical principle is correct at all, it\ncuts also against the equations of the phenomenologists. For unless\nthese very equations remain invariant through theory-change, there\nshould be no warrant for taking them to be accurate descriptions of\nworldly relations (cf. 1901: 253). Besides, Boltzmann noted, the very\nconstruction of the differential equations of the phenomenologists\nrequires commitment to substantive atomistic assumptions. Hence, the\nphenomenologists are not merely disingenuous when they jettison the\natomistic assumptions after the relevant differential equations have\nbeen arrived at. Their move is self-undermining. In light of the\nhistorical principle, the success of the mathematical equations would\nlead to their defeat, since the very theory that led to this success\nwould fall foul of the historical principle: it would have to be\nabandoned. \nThe history-based pessimism (and the relevant debate) came to an end\nby the triumph of atomism in the first decade of the twentieth\ncentury. Due to the work of Albert Einstein and the French physicist\nJean Perrin on the atomic explanation of Brownian motion, one after\nthe other of major scientists who were initially sceptical about the\natomic conception of matter came to accept\n atomism.[4]\n The French philosopher André Lalande captured this point in\nhis 1913 (pp. 366–367) thus: \nM. Perrin, professor of physics at the Sorbonne, has described in\nLes Atomes, with his usual lucidity and vigour, the recent\nexperiments (in which he has taken so considerable a part) which prove\nconclusively that the atoms are physical realities and not symbolical\nconceptions as people have for a long time been fond of calling them.\nBy giving precise and concordant measures for their weights and\ndimensions, it is proved that bodies actually exist which, though\ninvisible, are analogous at all points to those which we see and\ntouch. An old philosophical question thus receives a positive\nsolution. \nBe that as it may, what this brief account of the history of the\nhistorical challenge to realism reveals are the two major lines of\ndefense of realism at play. Both lines of defense are based on the\npresence of substantial continuity in theory-change in the history of\nscience. This continuity suggests that the disruption of the\nscientific image of the world, as theories change, is less radical\nthan is assumed by the historical challenge to realism. But the two\nlines of defense (the Poincaré-Duhem and the Boltzmann one)\ndisagree over what is retained when theories change. The\nPoincaré-Duhem line of defense focuses on mathematical\nequations (which express relations) and claims that only relations\namong unobservable things are knowable, whereas the Boltzmann line of\ndefense focuses on whatever theoretical elements (including entities\nlike atoms) are retained while theories change; hence, it does not\nlimit scientific knowledge to the knowledge of relations only. Both\nlines have resurfaced in the current debate. \nCapitalizing on the work of Richard Boyd, the early Hilary Putnam took\nscientific realism to involve three theses: \nPutnam argued that the failure of the third thesis would lead to a\ndisastrous “meta-induction”: \njust as no term used in the science of more than fifty (or\nwhatever) years ago referred, so it will turn out that no term\nused now (except maybe observation terms, if there are such)\nrefers (1978: 25) (emphasis in the original).  \nAn answer to this ‘disastrous’ history-fed argument was\nthe development of a causal theory of reference, which allows for\nreferential continuity in theory-change. This theory was first\nsuggested by Saul Kripke (1972) as an alternative to the then dominant\ndescriptive theories of reference of proper names and was extended by\nPutnam (1973, 1975) to cover natural kind terms and theoretical terms.\nAccording to the causal theory, the reference of a theoretical term\nt is fixed during an introducing event in which an entity or a\nphysical magnitude is posited as the cause of various observable\nphenomena. The term t, then, refers to the posited entity.\nThough some kind of descriptions of the posited entity will be\nassociated with t, they do not play a role in reference fixing.\nThe referent has been fixed existentially: it is the entity\ncausally responsible for certain effects. \nThe causal theory of reference makes it possible that the same term\nfeaturing in different theories refers to the same worldly entity. If,\nfor instance, the referent of the term ‘electricity’ is\nfixed existentially, all different theories of electricity refer to,\nand dispute over, the same ‘existentially given’\nmagnitude, viz. electricity; better, the causal agent of\nsalient electrical effects. Hence, the causal theory makes available a\nway to compare past and present theories and to claim that the\nsuccessor theory is more truthlike than its predecessors since it says\ntruer things of the same entities. It turns out, however, that the\ncausal theory faces a number of conceptual problems, most notable of\nwhich is that it makes referential success inevitable insofar as the\nphenomena which lead to the introduction of a new theoretical term do\nhave a cause (see Psillos 1999: chapter 11 for a discussion).\nPhilosophers of science have tried to put forward a causal-descriptive\ntheory of reference which makes referential continuity possible whilst\nallowing room for causal descriptions in fixing the reference of a\ntheoretical\n term.[5] \nAn analogous history-fed pessimistic argument can be based on the\nso-called “principle of no privilege”, which was advanced\nby Mary Hesse in her 1976. According to this principle: \nour own scientific theories are held to be as much subject to radical\nconceptual change as past theories are seen to be. (1976: 266) \nThis principle can be used for the derivation of the strong conclusion\nthat all theories are false. As Hesse put it: \nEvery scientific system implies a conceptual classification of the\nworld into an ontology of fundamental entities and properties—it\nis an attempt to answer the question “What is the world really\nmade of?” But it is exactly these ontologies that are most\nsubject to radical change throughout the history of science. Therefore\nin the spirit of the principle of no privilege, it seems that we must\nsay either that all these ontologies are true, ie: we must give a\nrealistic interpretation of all of them or we must say they are all\nfalse. But they cannot all be true in the same world, because they\ncontain conflicting answers to the question “What is the world\nmade of?” Therefore they must all be false. (1976: 266) \nThis argument engages the history of theory-change in science in a\nsubstantial way. As Hesse admitted, the Principle of No Privilege\narises “from accepting the induction from the history of\nscience” (1976: 271). Hesse’s argument starts with the\nhistorical premise that, as science grows over time, there has been a\nrecognizable pattern of change in the ‘ontology of fundamental\nentities and properties’ posited by scientific theories.\nAssuming, then, the Principle of No Privilege, it is argued that\ncurrent theories too will be subjected to a radical change in the\nontology of the entities and properties they posit. Hence, current\ntheories are as false as the past ones. \nThe problem with this kind of argument is that the historical premise\nshould be borne out by the actual history of theory-change in science.\nIt’s not enough to say that scientific theories change over\ntime; these changes should be such that the newer theories are\nincompatible with the past ones. Or, to use Hesse’s idiom, it\nshould be shown that past and current scientific\n‘ontologies’ are incompatible with each other. Showing\nincompatibility between the claims made by current theory T and\na past theory T′ requires a theory of reference of\ntheoretical terms which does not allow that terms featuring\nin different theories can nonetheless refer to the same entity in the\nworld. Hence, it is question-begging to adopt a theory of reference\nwhich makes it inevitable that there is radical-reference variance in\n theory-change. \nReferential stability, as noted already, makes possible the claim that\npast and present ontologies are compatible, even if there have been\nchanges in what current theories say of the posited entities. The\n“revolutionary induction from the history of science about\ntheory change” (Hesse 1976: 268) can be blocked by pointing to a\npattern of substantial continuity in theory change. \nCan a history-fed argument be used in defence of realism?\nWilliam Newton-Smith (1981) was perhaps the first in the recent debate\nto answer positively this question. Scientific realism is committed to\nthe two following theses: \nAccording to Newton-Smith, (2) is under threat “if we reflect on\nthe fact that all physical theories in the past have had their heyday\nand have eventually been rejected as false”. And he added: \nIndeed, there is inductive support for a pessimistic induction: any\ntheory will be discovered to be false within, say 200 years of being\npropounded. We may think of some of our current theories as being\ntrue. But modesty requires us to assume that they are not so. For what\nis so special about the present? We have good inductive grounds for\nconcluding that current theories—even our most favourite\nones—will come to be seen to be false. Indeed the evidence might\neven be held to support the conclusion that no theory that will ever\nbe discovered by the human race is strictly speaking true. So how can\nit be rational to pursue that which we have evidence for thinking can\nnever be reached? (1981: 14) \nThe key answer to this question is that even if truth cannot be\nreached, it is enough for the defense of realism to posit “an\ninterim goal for the scientific enterprise”, viz., “the\ngoal of getting nearer the truth”. If this is the goal, the\n“sting” of the preceding induction “is\nremoved”. Accepting PI “is compatible with maintaining\nthat current theories, while strictly speaking false, are getting\nnearer the truth” (1981: 14). \nBut aren’t all false theories equally false? The standard\nrealist answer is based on what Newton-Smith called “the animal\nfarm move” (1981: 184), viz., that though all theories are\nfalse, some are truer than others. Hence, what was needed to be\ndefended was the thesis that if a theory \\(T_2\\) has greater\nverisimilitude than a theory \\(T_1\\), \\(T_2\\) is likely to have\ngreater observational success than \\(T_1\\). The key argument was based\non the “undeniable fact” that newer theories have yielded\nbetter predictions about the world than older ones (cf. Newton-Smith\n1981: 196). But if the ‘greater verisimilitude’ thesis is\ncorrect (that is, if theories “are increasing in truth-content\nwithout increasing in falsity-content”), then the increase in\npredictive power would be explained and rendered expectable. This\nincrease in predictive power “would be totally mystifying\n(…) if it were not for the fact that theories are capturing\nmore and more truth about the world” (1981: 196). \nThe key point, then, is that the defense of realism against the\nhistorical induction requires showing that there is, indeed, a\nprivilege that current theories enjoy over past ones, which is strong\nenough to block transferring, on inductive grounds, features of past\ntheories to current ones. For most realists, the privilege current\ntheories enjoy over past ones is not that they are true while the past\ntheories are false. Rather, the privilege is that they are more\ntruthlike than past theories because they have had more predictive\npower than past theories. The privilege is underpinned by an\nexplanatory argument: the increasing truthlikeness of current theories\nbest explains their increasing predictive and empirical\nsuccess. \nBut there is a way to see the historical challenge to realism which\nmakes it have as its target precisely to undercut the explanatory link\nbetween empirical success and truthlikeness. This was brought\nunder sharp relief in the subsequent debates. \nThe most famous history-based argument against realism, issued by\nLarry Laudan (1981), was meant to show how the explanatory link between\nsuccess and truthlikeness is undermined by taking seriously the\nhistory of science. This argument may be put thus: \nLaudan substantiated (L) by means of what he has called “the\nhistorical gambit”: the following list—which “could\nbe extended ad nauseam”—gives theories which were\nonce empirically successful and fruitful, yet just false. \nList of successful-yet-false theories \nThis is a list of a dozen of cases, but Laudan boldly noted the famous\n6 to 1 ratio:  \nI daresay that for every highly successful theory in the past of\nscience which we now believe to be a genuinely referring theory, one\ncould find half a dozen once successful theories which we now regard\nas substantially non-referring. (1981: 35) \nIf we are to take seriously this “plethora” of theories\nthat were both successful and false, it appears that (L) is meant to\nbe a genuinely inductive argument. \nAn argument such as (I) has obvious flaws. Two are the most important.\nThe first is that the basis for induction is hard to assess. This does\nnot just concern the 6:1 ratio, of which one may ask: where does\nit come from? It also concerns the issue of how we individuate\nand count theories as well as how we judge success and referential\nfailure. Unless we are clear on all these issues in advance of the\ninductive argument, we cannot even start putting together the\ninductive evidence for its conclusion (cf.\nMizrahi 2013). \nThe second flaw of (I) is that the conclusion is too strong.\nIt is supposed to be that there is rational warrant for the judgment\nthat current theories are not truthlike. The flaw with this\nkind of sweeping generalization is precisely that it totally\ndisregards the fresh strong evidence there is for current\ntheories—it renders current evidence totally irrelevant to the\nissue of their probability of being true. Surely this is unwarranted.\nNot only because it disregards potentially important differences in\nthe quality and quantity of evidence there is for current theories\n(differences that would justify treating current theories as more\nsupported by available evidence than past theories were by the then\navailable evidence); but also because it makes a mockery of looking\nfor evidence for scientific theories! If I know that X is more\nlikely than Y and that this relation cannot change by doing\nZ, there is no point in doing Z. \nIf we think of the pessimistic argument not as inductive but as a\nwarrant-remover argument and if we also think that the fate\nof (past) theories should have a bearing on what we are warranted in\naccepting now, we should think of its structure differently. It has\nbeen argued by Psillos (1999: chapter 5) that we should think of\nthe pessimistic argument as a kind of reductio. Argument (L)\nabove aimed to “discredit the claim that there is an explanatory\nconnection between empirical success and truth-likeness” which\nwould warrant the realist view that current successful theories are\ntruthlike. If we view the historical challenge\nthis way, viz., as a potential warrant-remover argument, the past\nrecord of science does play a role in it, since it is meant to offer\nthis warrant-remover. \nPsillos’s (1996) reconstruction of Laudan’s argument was\nas follows: \nPremise (B) of argument (P) is critical. It is meant to capture\nradical discontinuity in theory-change, which was put thus (stated in\nthe material mode):  \nPast theories are deemed not to have been truth-like because the\nentities they posited are no longer believed to exist and/or because\nthe laws and mechanisms they postulated are not part of our current\ntheoretical description of the world. (Psillos 1999: 97). \nIn this setting, the ‘historical gambit’ (C) makes perfect\nsense. Unless there are past successful theories which are\nwarrantedly deemed not to be truthlike, premise (B) cannot be\nsustained and the warrant-removing reductio of (A) fails. If (C) can\nbe substantiated, success cannot be used to warrant the claim that\ncurrent theories are true. The realists’ explanatory link\nbetween truthlikeness and empirical success is undercut. (C) can be\nsubstantiated only by examining past successful theories and their\nfate. History of science is thereby essentially engaged. \nThe realist response has come to be known as the divide et\nimpera strategy to refute the pessimistic argument. The focus of\nthis strategy was on rebutting the claim that the truth of current\ntheories implies that past theories cannot be deemed truthlike. To\ndefend realism, realists needed to be selective in their\ncommitments. This selectivity was developed by Kitcher (1993) and\n(independently) by Psillos (1994). \nOne way to be selective is to draw a distinction between working\nposits of a theory (viz., those theoretical posits that occur\nsubstantially in the explanatory schemata of the theory) and\npresuppositional posits (putative entities that apparently\nhave to exist if the instances of the explanatory schemata of the\ntheory are to be true) (cf. Kitcher 1993: 149). Another way is to draw\na distinction between the theoretical claims that essentially or\nineliminably contribute to the generation of successes of a theory and\nthose claims that are ‘idle’ components that have had no\ncontribution to the theory’s success (cf. Psillos 1994, 1996).\nThe underlying thought is that the empirical successes of a theory do\nnot indiscriminably support all theoretical claims of the theory, but\nrather the empirical support is differentially distributed among the\nvarious claims of the theory according to the contribution they make\nto the generation of the successes. Generally, Kitcher (1993) and\nPsillos (1996, 1999) have argued that there are ways to distinguish\nbetween the ‘good’ and the ‘bad’\nparts of past abandoned theories and to show that the\n‘good’ parts—those that enjoyed evidential support,\nwere not idle components and the like—were retained in\nsubsequent theories. \nIt is worth-noting that, methodologically, the divide et\nimpera strategy recommended that the historical challenge to\nrealism can only be met by looking at the actual successes of past\nsuccessful theories and by showing that those parts of past theories\n(e.g., the caloric theory of heat or the optical ether theories) that\nwere fuelling theory successes were retained in subsequent theories\nand those theoretical terms which were central in the relevant past\ntheories were referential. \nThe divide et impera response suggests that there has been\nenough theoretical continuity in theory-change to warrant the realist\nclaim that science is ‘on the right track’. \nThe realist move from substantive continuity in theory-change to\ntruthlikeness has been challenged on grounds that there is no\nentitlement to move from whatever preservation in theoretical\nconstituents there is in theory-change to these constituents’\nbeing truthlike (Chang 2003: 910–12; Stanford 2006). Against\nthis point it has been argued that the realist strategy proceeds in\ntwo steps (cf. Psillos 2009: 72). The first is to make the claim of\ncontinuity (or convergence) plausible, viz., to show that there is\ncontinuity in theory-change: substantive theoretical claims that\nfeatured in past theories and played a key role in their successes\n(especially novel predictions) have been incorporated in subsequent\ntheories and continue to play an important role in making them\nempirically successful. But this first step does not establish that\nthe convergence is to the truth. For this claim to be made\nplausible a second argument is needed, viz., that the emergence of\nthis evolving-but-convergent network of theoretical assertions is best\nexplained by the assumption that it is, by and large, truthlike. So\nthere is, after all, entitlement to move from convergence to\ntruthlikeness, insofar as truthlikeness is the best explanation of\nthis convergence. \nAnother critical point was that the divide et impera strategy\ncannot offer independent support to realism since it is tailor-made to\nsuit realism: it is the fact that the very same present theory is used\nboth to identify which parts of past theories were\nempirically successful and which parts were (approximately)\ntrue that accounts for the realists’ wrong impression that these\nparts coincide (Stanford 2006). He says: \nWith this strategy of analysis, an impressive retrospective\nconvergence between our judgements of the sources of a past\ntheory’s success and the things it ‘got right’ about\nthe world is virtually guaranteed: it is the very fact that some\nfeatures of a past theory survive in our present account of nature\nthat leads the realist both to regard them as true\nand to believe that they were the sources of the rejected\ntheory’s success or effectiveness. So the apparent convergence\nof truth and the sources of success in past theories is easily\nexplained by the simple fact that both kinds of retrospective\njudgements have a common source in our present beliefs about nature.\n(2006: 166) \nIt has been claimed by Psillos (2009) that the foregoing objection is\nmisguided. The problem is this. There are the theories scientists\ncurrently endorse and there are the theories that had been endorsed in\nthe past. Some (but not all) of them were empirically successful\n(perhaps for long periods of time). They were empirically successful\nirrespective of the fact that, subsequently, they came to be replaced\nby others. This replacement was a contingent matter that had to do\nwith the fact that the world did not fully co-operate with the then\nextant theories: some of their predictions failed; or the theories\nbecame overly ad hoc or complicated in their attempt to accommodate\nanomalies, or what have you. The replacement of theories by others\ndoes not cancel out the fact that the replaced theories were\nempirically successful. Even if scientists had somehow failed to come\nup with new theories, the old theories would not have ceased to be\nsuccessful. So success is one thing, replacement is another. \nHence, it is one thing to inquire into what features of some past\ntheories accounted for their success and quite another to ask whether\nthese features were such that they were retained in\nsubsequent theories of the same domain. These are two independent\nissues and they can be dealt with (both conceptually and historically)\nindependently. One should start with some past theories\nand—bracketing the question of their replacement—try to\nidentify, on independent grounds, the sources of their empirical\nsuccess; that is, to identify those theoretical constituents of the\ntheories that fuelled their successes. When a past theory has been, as\nit were, anatomised, we can then ask the independent question\nof whether there is any sense in which the sources of success of a\npast theory that the anatomy has identified are present in our current\ntheories. It’s not, then, the case that the current theory is\nthe common source for the identification of the successful parts of a\npast theory and of its truthlike parts. \nThe transition from Newton’s theory of gravity to\nEinstein’s illustrates this point. Einstein took it for granted\nthat Newton’s theory of gravity (aided by perturbation theory)\ncould account for 531 arc-second per century of the perturbation of\nMercury’s perihelion. Not only were the empirical successes of\nNewton’s theory identified independently of the successor\ntheory, but also some key theoretical components of Newton’s\ntheory—the law of attraction and the claim that the\ngravitational effects from the planets on each other were a\nsignificant cause of the deviations from their predicted\norbits—were taken to be broadly correct and explanatory (of at\nleast part) of the successes. Einstein could clearly identify the\nsources of successes of Newton’s theory independently of his own\nalternative theory and it is precisely for this reason that he\ninsisted that he had to recover Newton’s law of attraction (a\nkey source of the Newtonian success) as a limiting case of his own\nGTR. He could then show that his new theory could do both: it could\nrecover the (independently identified) sources of successes of\nNewton’s theory (in the form of the law of attraction)\nand account for its failures by identifying further causal\nfactors (the curvature of space-time) that account for the\ndiscrepancies between the predicted orbits of planets (by\nNewton’s theory of gravity) and the observed\n trajectories.[6] \nA refinement of the divide et impera move against the PI has\nbeen suggested by Peter Vickers (2017). He argues that the onus of\nproof lies with the antirealist: the antirealist has to reconstruct\nthe derivation of a prediction, identify the assumptions that merit\nrealist commitments and then show that at least one of them is not\ntruthlike by our current lights. But then, Vickers adds, all the\nrealists need to show is that the specific assumptions identified by\nthe anti-realist do not merit realist commitments. It should be noted\nthat this is exactly the strategy recommended by Psillos in his 1994,\nwhere he aimed to show, using specific cases, that various assumptions\nsuch as that heat is a material substance in the case of the caloric\ntheory of heat, do not merit realist commitment, because there are\nweaker assumptions that fuel the derivation of successful predictions.\nVickers generalizes this strategy by arguing as follows. Take a\nhypothesis H that is taken to be employed in the derivation of\nP and does not merit realist commitment. Identify an H* which\nis entailed by H and show that H* is enough for the\nderivation of P and does merit realist commitment.\n [7] \nAn instance of the divide et impera strategy is structural\nrealism. This view has been associated with John Worrall (1989), who\nrevived the relationist account of theory-change that emerged in the\nbeginning of the twentieth century. In opposition to scientific\nrealism, structural realism restricts the cognitive content of\nscientific theories to their mathematical structure together with\ntheir empirical consequences. But, in opposition to instrumentalism,\nstructural realism suggests that the mathematical structure of a\ntheory represents the structure of the world (real relations between\nthings). Against PI, structural realism contends that there is\ncontinuity in theory-change, but this continuity is (again) at the\nlevel of mathematical structure. Hence, the ‘carried over’\nmathematical structure of the theory correctly represents the\nstructure of the world and this best explains the predictive success\nof a\n theory.[8] \nStructural realism was independently developed in the 1970s by Grover\nMaxwell (1970a, 1970b) in an attempt to show that the Ramsey-sentence\napproach to theories need not lead to instrumentalism.\nRamsey-sentences go back to a seminal idea by Frank Ramsey (1929).  To\nget the Ramsey-sentence \\(^{R}T\\) of a (finitely axiomatisable)\ntheory T we conjoin the axioms of T in a single\nsentence, replace all theoretical predicates with distinct variables\n\\(u_i\\), and bind these variables by placing an equal number of\nexistential quantifiers \\(\\exists u_i\\) in front of the resulting\nformula. Suppose that the theory T is represented as T\n(\\(t_1\\),…, \\(t_n\\); \\(o_1\\),…, \\(o_m\\)), where T\nis a purely logical \\(m+n\\)-predicate. The Ramsey-sentence \\(^{R}T\\)\nof T is:  \nThe Ramsey-sentence \\(^{R}T\\) that replaces theory T has\nexactly the same observational consequences as T; it can play\nthe same role as T in reasoning; it is truth-evaluable if there\nare entities that satisfy it; but since it dispenses altogether with\ntheoretical vocabulary and refers to whatever entities satisfy it only\nby means of quantifiers, it was taken to remove the issue of the\nreference of theoretical terms/predicates. ‘Structural\nrealism’ was suggested to be the view that: i) scientific\ntheories issue in existential commitments to unobservable entities and\nii) all non-observational knowledge of unobservables is structural\nknowledge, i.e., knowledge not of their first-order (or\nintrinsic) properties, but rather of their higher-order (or\nstructural) properties. The key idea here was that a Ramsey-sentence\nsatisfies both conditions (i) and (ii). So we might say that, if true,\nthe Ramsey-sentence \\(^{R}T\\) gives us knowledge of the structure of\nthe world: there is a certain structure which satisfies the\nRamsey-sentence and the structure of the world (or of the relevant\nworldly domain) is isomorphic to this structure. \nThough initially Worrall’s version of structural realism was\ndifferent from Maxwell’s, being focused on—and motivated\nby—Poincaré’s argument for structural\ncontinuity in theory-change, in later work Worrall came to adopt the\nRamsey-sentence version of structural realism (see appendix IV of\nZahar 2001). \nA key problem with Ramsey-sentence realism is that though a\nRamsey-sentence of a theory may be empirically inadequate, and hence\nfalse, if it is empirically adequate (if, that is, the\nstructure of observable phenomena is embedded in one of its models),\nthen it is bound to be true. For, as Max Newman (1928) first noted in\nrelation to Russell’s (1927) structuralism, given some\ncardinality constraints, it is guaranteed that there is an\ninterpretation of the variables of \\(^{R}T\\) in the theory’s\nintended\n domain.[9] \nMore recently, David Papineau (2010) has argued that if we identify\nthe theory with its Ramsey-sentence, it can be argued that past\ntheories are approximately true if there are entities which satisfy,\nor nearly satisfy, their Ramsey-sentences. The advantage of this move,\naccording to Papineau, is that the issue of referential failure is\nbypassed when assessing theories for approximate truth, since the\nRamsey sentence replaces the theoretical terms with existentially\nbound variables. But as Papineau (2010: 381) admits, the force of the\nhistorical challenge to realism is not thereby thwarted. For it may\nwell be the case that the Ramsey-sentences of most past theories are\nnot satisfied (not even nearly\n so).[10] \nIn the more recent literature, there has been considerable debate as\nto how exactly we should understand PI. There are those, like Anjan\nChakravartty who take it that PI is an Induction. He says: \nPI can … be described as a two-step worry. First, there is an\nassertion to the effect that the history of science contains an\nimpressive graveyard of theories that were previously believed [to be\ntrue], but subsequently judged to be false … Second, there is\nan induction on the basis of this assertion, whose conclusion is that\ncurrent theories are likely future occupants of the same graveyard.\n(2008: 152) \nYet, it is plausible to think that qua an inductive argument,\nhistory-based pessimism is bound to fail. The key point here is that\nthe sampling of theories which constitute the inductive evidence is\nneither random nor otherwise representative of theories in\ngeneral. \nIt has been argued that, seen as an inductive argument, PI is\nfallacious: it commits the base-rate fallacy (cf. Lewis 2001). If in\nthe past there have been many more false theories than true ones, (if,\nin other words, truth has been rare), it cannot be concluded that\nthere is no connection between success and truth. Take S to\nstand for Success and not-S to stand for failure. Analogously,\ntake T to stand for truth of theory T and not-T\nfor falsity of theory T. Assume also that the probability that\na theory is unsuccessful given that it is true is zero\n\\((\\textrm{Prob}({\\textrm{not-}S}/T)=0)\\) and that the probability\nthat a theory is successful given that it is false is \\(0.05\n(\\textrm{Prob}(S/{\\textrm{not-}T})=0.05)\\). Assume that is, that there\nis a very high True Positives (successful but true) rate and a small\nFalse Positives (successful but false theories) rate. We may then ask\nthe question: How likely is it that a theory is true, given that it is\nsuccessful? That is, what is the posterior probability\n\\(\\textrm{Prob}(T/S)\\)? \nThis answer is indeterminate if we don’t take into account the\nbase-rate of truth, viz., the incidence rate of truth in the\npopulation of theories. If the base rate is very low (let’s\nassume that only 1 in 50 theories have been true), then it is\nunlikely that T is true given success.\n\\(\\textrm{Prob}(T/S)\\) would be around 0.3. But this does not imply\nsomething about the connection between success and truth. It is still\nthe case that the false positives are low and that the true positives\nhigh. The low probability is due to the fact that truth is rare (or\nthat falsity is much more frequent). For \\(\\textrm{prob}(T/S)\\) to be\nhigh, it must be the case that \\(\\textrm{prob}(T)\\) is not too small.\nBut if \\(\\textrm{prob}(T)\\) is low, it can dominate over a high\nlikelihood of true positives and lead to a very low posterior\nprobability \\(\\textrm{prob}(T/S)\\). Similarly, the probability that a\ntheory is false given that it is successful (i.e.,\n\\(\\textrm{prob}({\\textrm{not-}T}/S))\\) may be high simply because\nthere are a lot more false theories than true ones. As Peter Lewis put\nit: \nAt a given time in the past, it may well be that false theories vastly\noutnumber true theories. In that case, even if only a small proportion\nof false theories are successful, and even if a large proportion of\ntrue theories are successful, the successful false theories may\noutnumber the successful true theories. So the fact that successful\nfalse theories outnumber successful true theories at some time does\nnothing to undermine the reliability of success as a test for truth at\nthat time, let alone at other times (2001: 376–7). \nSeen in this light, PI does not discredit the reliability of success\nas a test for truth of a theory; it merely points to the fact that\ntruth is scarce among past\n theories.[11] \nChallenging the inductive credentials of PI has acquired a life of its\nown. A standard objection (cf. Mizrahi 2013) is that theories are not\nuniform enough to allow an inductive generalization of the form\n“seen one, seen them all”. That is, theories are diverse\nenough over time, structure and content not to allow us to take a few\nof them—not picked randomly—as representative of all and\nto project the characteristics shared by those picked to all theories\nin general. In particular, the list that Laudan produced is not a\nrandom sample of theories. They are all before the twentieth century\nand all have been chosen solely on the basis that they had had some\nsuccesses (irrespective of how robust these successes were). An\nargument of the form: \nX % of past successful theories are false \nTherefore, X % of all successful theories are false \nwould be a weak inductive argument because  \nit fails to provide grounds for projecting the property of the\nobserved members of the reference class to unobserved members of the\nreference class. (Mizrahi 2013: 3219) \nThings would be different, if we had a random sampling of theories.\nMizrahi (2013: 3221–3222) collected 124 instances of\n‘theory’ from various sources and picked at random 40 of\nthem. These 40 were then divided into three groups: accepted theories,\nabandoned theories and debated theories. Of those 40 theories, 15%\nwere abandoned and 12% debated. Mizrahi then notes that these randomly\nselected data cannot justify an inductively drawn conclusion that most\nsuccessful theories are false. On the contrary, an optimistic\ninduction would be more warranted: \n72% of sampled theories are accepted theories (i.e., considered\ntrue). \nTherefore, 72% of all theories are accepted theories (i.e., considered\ntrue). \nMizrahi has come back to the issue of random sampling and has\nattempted to show that the empirical evidence is against PI: \nIf the history of science were a graveyard of dead theories and\nabandoned posits, then random samples of scientific theories and\ntheoretical posits would contain significantly more dead theories and\nabandoned posits than live theories and accepted posits. \nIt is not the case that random samples of scientific theories and\ntheoretical posits contain significantly more dead theories and\nabandoned posits than live theories and accepted posits. \nTherefore, It is not the case that the history of science is a\ngraveyard of dead theories and abandoned posits. (2016: 267) \nA similar argument has been defended by Park (2011). We may call it,\nthe explosion argument: Most key theoretical terms of successful\ntheories of the twentieth century refer “in the light of current\ntheories”. But then, “most central terms of successful\npast theories refer”, the reason being that there are far more\ntwentieth century theories than theories in total. This is because\n“the body of scientific knowledge exploded in the twentieth\ncentury with far more human and technological resources” (2011:\n79). \nLet’s call this broad way to challenge the inductive credentials\nof the pessimistic argument ‘the Privilege-for-current-theories\nstrategy’. This has been adopted by Michael Devitt (2007) too,\nthough restricted to entities. Devitt, who takes realism to be a\nposition concerning the existence of unobservables, noted that the\nright question to ask is this: ‘What is the “success\nratio” of past theories?’, where the “success\nratio” is “the ratio of the determinately existents to the\ndeterminately nonexistents + indeterminates”. Asserting a\nprivilege for current science, he claims that “we are now\nmuch better at finding out about unobservables”.\nAccording to him, then, it is “fairly indubitable” that\nthe historical record shows “improvement over time in our\nsuccess ratio for unobservables’. \nIn a similar fashion but focusing on current theories, Doppelt (2007)\nclaims that realists should confine their commitment to the\napproximate truth of current best theories, where best theories are\nthose that are both most successful and well established. The\nasymmetry between current best theories and past ones is such that the\nsuccess of current theories is of a different kind than the\nsuccess of past theories. The difference, Doppelt assumes, is so big\nthat the success of current theories can only be explained by assuming\nthat they are approximately true, whereas the explanation of the\nsuccess of past theories does not require this commitment. \nIf this is right, there is sufficient qualitative distance between\npast theories and current best ones to block  \nany pessimistic induction from the successful-but-false superseded\ntheories to the likelihood that our most successful and\nwell-established current theories are also probably false. (Doppelt\n2007: 110).  \nThe key difference, Doppelt argues, is that  \nour best current theories enjoy a singular degree of empirical\nconfirmation impossible for their predecessors, given their ignorance\nof so many kinds of phenomena and dimensions of nature discovered by\nour best current theories.  \nThis singular degree of empirical confirmation amounts to raising the\nstandards of empirical success to a level unreachable by past theories\n(cf. 2007: 112). \nThe advocate of PI can argue that past ‘best theories’\nalso raised and met the standards of empirical success, which\ninductively supports the conclusion that current best theories will be\nsuperseded by others which will meet even higher standards of success.\nDoppelt’s reply is that this new version of PI “should not\nbe given a free pass as though it were on a par with the original\npessimistic induction” the reason being that “in the\nhistory of the sciences, there is greater continuity in standards of\nempirical success than in the theories taken to realize them”.\nHence, the standards of empirical success change slower than theories.\nHence, it is not very likely that current standards of empirical\nsuccess will change any time soon. \nIt has been argued, however, that Doppelt cannot explain the novel\npredictive success of past theories without arguing that they had\ntruthlike constituents (cf. Alai 2017). Besides, as Alai puts it,\n“current best theories explain the (empirical) success of\ndiscarded ones only to the extent that they show that the latter were\npartly true” (2017: 3282). \nThe ‘Privilege-for-current-theories strategy’ has been\nsupported by Ludwig Fahrbach (2011). The key point of this strategy is\nthat the history of science does not offer a representative sample of\nthe totality of theories that should be used to feed the historical\npessimism of PI. In order to substantiate this, Fahrbach suggested,\nbased on extensive bibliometric data, that over the last three\ncenturies the number of papers published by scientists as well as the\nnumber of scientists themselves have grown exponentially,\nwith a doubling rate of 15–20 years. Hence, he claims, the past\ntheories that feed the historical premise of PI were produced during\nthe time of the first 5% of all scientific work ever done by\nscientists. As such the sample is totally unrepresentative of theories\nin total; and hence the pessimistic conclusion, viz., that current\ntheories are likely to be false and abandoned in due course, is\ninductively unwarranted. Moreover, Fahrbach argues, the vast majority\nof theories enunciated in the last 50–80 years, (which\nconstitute the vast majority of scientific work ever produced) are\nstill with us. Hence, as he puts it,  \n(t)he anti-realist will have a hard time finding even one or two\nconvincing examples of similarly successful theories that were\naccepted in the last 50–80 years for some time, but later\nabandoned. (2011: 152)  \nSince there have been practically no changes “among our best\n(i.e., most successful) theories”, Fahrbach suggests  \nan optimistic meta-induction to the effect that they will remain\nstable in the future, i.e., all their empirical consequences which\nscientists will ever have occasion to compare with results from\nobservation at any time in the future are true. (2011: 153)  \nThe conclusion is that the PI is unsound: “its conclusion that\nmany of our current best scientific theories will fail empirically in\nthe future cannot be drawn” (2011: 153). \nA key assumption of the foregoing argument is that there is a strong\nconnection between the amount of scientific work (as measured by the\nnumber of journal articles) and the degree of success of the best\nscientific theories. But this can be contested on the grounds that\nit’s a lot easier to publish currently than it was in the\nseventeenth century and that current research is more tightly\nconnected to the defense of a single theoretical paradigm than before.\nThis might well be a sign of maturity of current science but, as it\nstands, it does not show that the present theoretical paradigm is not\nsubject to radical change. Florian Müller (2015) put the point in\nterms of decreasing marginal revenues. The correlation between\nincreased scientific work and scientific progress, which is assumed by\nFahrbach may not be strong enough:  \nIt seems more plausible to expect decreasing marginal revenues of\nscientific work since it usually takes much less time to establish\nvery basic results than to make progress in a very advanced state of\nscience. (Müller 2015: 404) \nThe ‘Privilege-for-current-theories strategy’ can be\nfurther challenged on the grounds that it requires some\n“fundamental difference between the theories we currently\naccept, and the once successful theories we have since rejected”\n(Wray 2013: 4325). As Brad Wray (2013) has argued Fahrbach’s\nstrategy is doomed to fail because the argument from exponential\ngrowth could be repeated at former periods too, thereby undermining\nitself. Imagine that we are back in 1950 and we look at the period\nbetween 1890 and 1950. We could then argue, along Farhbach’s\nlines, that the pre-1890 theories (which were false and abandoned)\nwere an unrepresentative sample of all theories and that the recent\ntheories (1890–1950) are by far the most theories until 1950 and\nthat, since most of them have not been abandoned (by 1950), they are\nlikely to remain impervious to theory-change. Or imagine that we are\nfurther back in 1890 and look at the theories of the period\n1830–1890. We could run the same argument about those theories,\nviz, that they are likely to survive theory change. But if we look at\nthe historical pattern, they did not survive; nor did the\ntheories between 1890–1950. By the same token, we should not\nexpect current theories to survive theory-change. \nIs there room for defending an epistemic privilege for current\nscience? Two points are worth making. The first is that it’s\nhard to defend some kind of epistemic privilege of current science if\nthe realist argument against PI stays only at a level of statistics\n(even assuming that there can be statistics over theories). If there\nis an epistemic privilege of current science in relation to past\nscience, it is not a matter of quantity but of quality. The\nissue is not specifying how likely it is that an arbitrary current\ntheory T be true, given the evidence of the past record of\nscience. The issue, instead, is how a specific scientific\ntheory—a real theory that describes and explains certain\nwell-founded worldly phenomena—is supported by the evidence\nthere is for it. If we look at the matter from this perspective, we\nshould look at case-histories and not at the history of science at\nlarge. The evidence there is for specific theory T (e.g., the\nDarwinian synthesis or GTR etc.) need not be affected by past failures\nin the theoretical understanding of the world in general. The reason\nis that there is local epistemic privilege, that is, privilege over\npast relevant theories concerning first-order evidence and specific\nmethods. \nThe second point is this. Wray’s argument against Fahrbach is,\nin effect, that there can be a temporal meta-(meta-)induction which\nundermines at each time t (or period Dt) the privilege\nthat scientific theories at t or Dt are supposed to\nhave. So Wray’s point is this: at each time \\(t_{i}\\) (or period\n\\(Dt_{i}\\)), scientists claim that their theories are not subject to\nradical change at subsequent times; but if we look at the pattern of\ntheory change over time, the history of science shows that there have\nbeen subsequent times \\(t_{i}+1\\) (or periods \\({Dt}_{i}+1\\)) such\nthat the theories accepted at \\(t_{i}\\) were considered false and\nabandoned. Hence, he takes it that at no time \\(t_{i}\\) are scientists\njustified in accepting their theories as not being subject to radical\nchange in the future. But this kind of argument is open to the\nfollowing criticism. It assumes, as it were, unit-homogeneity, viz.,\nthat science at all times \\(t_{i}\\) (and all periods \\({Dt}_{i}\\)) is\nthe same when it comes to how far it is from the truth. Only on\nthis assumption can it be argued that at no time can\nscientists claim that their theories are not subject to radical\nchange. For if there are senses in which subsequent theories are\ncloser to the truth than their predecessors, it is not equally likely\nthat they will be overturned as their predecessors were. \nThe point, then, is that though at each time \\(t_{i}\\) (or period\n\\({Dt}_{i}\\)) scientists might well claim that their theories are not\nsubject to radical change at subsequent times, they are not equally\njustified in making this claim! There might well be times\n\\(t_{i}\\) (or periods \\({Dt}_{i}\\)) in which scientists are more\njustified in making the claims that their theories are not subject to\nradical change at subsequent times simply because they have reasons to\nbelieve that their theories are truer than their predecessors. To give\nan example: if Wray’s argument is right then Einstein’s\nGTR is as likely to be overthrown at future time \\(t_{2100}\\) as was\nAristotle’s crystalline spheres theory in past time\n\\(t_{-300}\\). But this is odd. It totally ignores the fact that all\navailable evidence renders GTR closer to the truth than the simply\nfalse Aristotelian theory. In other words, that GTR has substantial\ntruth-content makes it less likely to be radically revised in the\nfuture. \nAn analogous point was made by Park (2016). He defined what he called\nProportional Pessimism as the view that “as theories\nare discarded, the inductive rationale for concluding that the next\ntheories will be discarded grows stronger” (2016: 835). This\nview entails that the more theories have been discarded before\nT is discarded, the more justified we are in thinking that\nT is likely to be discarded. However, it is also the case that\nbased on their greater success, we are more justified to take newer\ntheories to be more likely to be truthlike than older ones. We then\nreach a paradoxical situation: we are justified to take newer theories\nto be both more probable than older ones and more likely to be\nabandoned than older ones. \nIf an inductive rendering of historical pessimism fails, would a\ndeductive rendering fare better? Could PI be considered at least as a\nvalid deductive argument? Wray (2015: 65) interprets the\noriginal argument by Laudan as being deductive. And he notes  \nas far as Laudan is concerned, a single successful theory that is\nfalse would falsify the realist claim that (all) successful theories\nare true; and a single successful theory that refers to a non-existent\ntype of entity would falsify the realist claim that (all) successful\ntheories have genuinely referring theoretical terms.  \nBut if this is the intent of the argument, history plays no\nrole in it. All that is needed is a single counterexample, past\nor present. This, it should be noted, is an endemic problem with all\nattempts to render PI as a deductive argument. Müller, for\ninstance, notes that the fundamental problem raised by PI is\n“simply that successful theories can be false”. He adds:\n \nEven just one counterexample (as long as it is not explained away)\nundermines the claim that truth is the best explanation for the\nsuccess of theories as it calls into question the explanatory\nconnection in general. (2015: 399)  \nThus put, the history of past failures plays no role in PI. Any\ncounterexample, even one concerning a current theory, will do. \nHow is it best to understand the realist theses that the history of\nscience is supposed to undermine? Mizrahi (2013: 3224) notes that the\nrealist claim is not meant to be a universal statement. As he puts\nit: \nSuccess may be a reliable indicator of (approximate) truth, but this\nis compatible with some instances of successful theories that turn out\nnot to be approximately true. In other words, that a theory is\nsuccessful is a reason to believe that it is approximately true, but\nit is not a conclusive proof that the theory is approximately\ntrue. \nThe relation between success and (approximate) truth, in this sense,\nis more like the relation between flying and being a bird: flying\ncharacterizes birds even if kiwis do not fly. If this is so, then\nthere is need for more than one counter-example for the realist thesis\nto be undermined. \nA recent attempt to render PI as a deductive argument is by Timothy\nLyons. He (2016b) takes realism to issue in the following\nmeta-hypothesis: “our successful scientific theories\nare (approximately) true”. He then reconstructs PI thus: \nThis is supposed to be a deductive argument against the ‘meta\nhypothesis’. But in his argument the history of science plays no\nrole. All that is needed for the argument above to be sound is a\nsingle instance of a successful theory that is not true. A\nsingle non-white swan is enough to falsify the hypothesis ‘All\nswans are white’—there is no point in arguing here: the\nmore, the merrier! In a similar fashion, it doesn’t add much to\nargument (D) to claim  \nthe quest to empirically increase the quantity of instances (…)\nis rather to secure the soundness of the modus tollens, to\nsecure the truth of the pivotal second premise, the claim that there\nare counterinstances to the realist meta-hypothesis. (Lyons 2016b:\n566) \nIn any case, a critical question is: can some\nfalse-but-rigorously-empirically-successful theories justifiably be\ndeemed truthlike from the point of view of successor theories? This\nquestion is hard to answer without looking at actual cases in the\nhistory of science. The general point, made by Vickers (2017) is that\nit is not enough for the challenger of realism to identify some\ncomponents of past theories which were contributing to their successes\nsuch that they were not retained in subsequent theories. The\nchallenger of realism should show that false components “merit\nrealist commitment”. If they do not, “ (…) that is\nenough to answer the historical challenge”. \nMore generally, the search for a generic form of the pessimistic\nX-duction (In-duction or De-duction) has yielded the following\nproblem: If the argument is inductive, it is at best weak. If the\nargument is deductive, even if it is taken to be sound, it makes the\nrole of the history of science\n irrelevant.[12] \nStanford (2006) has aimed to replace PI with what he calls the\n‘new induction’ on the history of science, according to\nwhich past historical evidence of transient underdetermination of\ntheories by evidence makes it likely that current theories will be\nsupplanted by hitherto unknown (unconceived) ones, which nonetheless,\nare such that when they are formulated, they will be at least as well\nconfirmed by the evidence as the current ones. But the new induction\nis effective, if at all, only in tandem with PI. For if there is\ncontinuity in our scientific image of the world, the hitherto\nunconceived theories that will replace the current ones won’t be\nthe radical rivals they are portrayed to\n be.[13] \nWhen it comes to the realist\ncommitment to theories, the proper philosophical task is to ignore\nneither the first order scientific evidence that there is for a given\ntheory nor the lessons that can be learned from the history of\nscience. Rather, the task is to balance the first-order and the second\norder of evidence. The first-order evidence is typically\nassociated with whatever scientists take into account when they form\nan epistemic attitude towards a theory. It can be broadly understood\nto include some of the theoretical virtues of the theory at\nhand—of the kind that typically go into plausibility judgments\nassociated with assignment of prior probability to theories. The\nsecond-order evidence comes from the past record of\nscientific theories and/or from meta-theoretical (philosophical)\nconsiderations that have to do with the reliability of scientific\nmethodology. It concerns not particular scientific theories, but\nscience as a whole. This second-order evidence feeds claims such as\nthose that motivate PI or the New Induction. Actually, this\nsecond-order evidence is multi-faceted—it is negative (showing\nlimitations and shortcomings) as well as positive (showing how\nlearning from experience can be improved).","contact.mail":"psillos@phs.uoa.gr","contact.domain":"phs.uoa.gr"}]
