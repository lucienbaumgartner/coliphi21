[{"date.published":"2002-01-14","date.changed":"2021-04-24","url":"https://plato.stanford.edu/entries/game-evolutionary/","author1":"J. McKenzie Alexander","entry":"game-evolutionary","body.text":"\n\n\nEvolutionary game theory originated as an application of the\nmathematical theory of games to biological contexts, arising from the\nrealization that frequency dependent fitness introduces a strategic\naspect to evolution. Recently, however, evolutionary game theory has\nbecome of increased interest to economists, sociologists, and\nanthropologists--and social scientists in general--as well as\nphilosophers. The interest among social scientists in a theory with\nexplicit biological roots derives from three facts. First, the\n‘evolution’ treated by evolutionary game theory need not\nbe biological evolution. ‘Evolution’ may, in this context,\noften be understood as cultural evolution, where this refers\nto changes in beliefs and norms over time. Second, the rationality\nassumptions underlying evolutionary game theory are, in many cases,\nmore appropriate for the modelling of social systems than those\nassumptions underlying the traditional theory of games. Third,\nevolutionary game theory, as an explicitly dynamic theory, provides an\nimportant element missing from the traditional theory. In the preface\nto Evolution and the Theory of Games, Maynard Smith notes\nthat “[p]aradoxically, it has turned out that game theory is\nmore readily applied to biology than to the field of economic\nbehaviour for which it was originally designed.” It is perhaps\ndoubly paradoxical, then, that the subsequent development of\nevolutionary game theory has produced a theory which holds\ngreat promise for social scientists, and is as readily applied to the\nfield of economic behaviour as that for which it was originally\ndesigned.\n\nEvolutionary game theory was first developed by R. A. Fisher [see\nThe Genetic Theory of Natural Selection (1930)] in his\nattempt to explain the approximate equality of the sex ratio in\nmammals. The puzzle Fisher faced was this: why is it that the sex\nratio is approximately equal in many species where the majority of\nmales never mate? (See, for example, the Northern elephant seal\nMirounga angustirostris.) In these species, the non-mating\nmales would seem to be excess baggage carried around by the rest of\nthe population, having no real use. Fisher realized that if we measure\nindividual fitness in terms of the expected number of\ngrandchildren, then individual fitness depends on the\ndistribution of males and females in the population. When there is a\ngreater number of females in the population, males have a higher\nindividual fitness; when there are more males in the population,\nfemales have a higher individual fitness. Fisher pointed out that, in\nsuch a situation, the evolutionary dynamics lead to the sex ratio\nbecoming fixed at equal numbers of males and females. The fact that\nindividual fitness depends upon the relative frequency of males and\nfemales in the population introduces a strategic element to\nevolution. \nFisher’s argument can be understood game theoretically, but he\ndid not state it in those terms. In 1961, R. C. Lewontin made the\nfirst explicit application of\n game theory\n to evolutionary biology in “Evolution and the Theory of\nGames” (not to be confused with the Maynard Smith work of the\nsame name). In 1972, Maynard Smith first introduced the concept of an\nevolutionarily stable strategy (hereafter ESS) in the chapter\n“Game Theory and the Evolution of Fighting.” However, it\nwas the publication of “The Logic of Animal Conflict,” by\nMaynard Smith and Price in 1973 that introduced the concept of an ESS\ninto widespread circulation. In 1982, Maynard Smith’s seminal\ntext Evolution and the Theory of Games appeared, followed\nshortly thereafter by Robert Axelrod’s famous work The\nEvolution of Cooperation in 1984. Since then, there has been a\nveritable explosion of interest by economists and social scientists in\nevolutionary game theory (see the bibliography below). \nInitially, it was thought that evolutionary game theory might provide\nan inroad into solving the equilibrium selection problem of\ntraditional game theory. Although the fundamental solution concept of\ntraditional game theory, the Nash equilibrium, had the\ndesirable property of always existing for any game with a finite\nnumber of players and strategies, provided that mixed strategies were\nallowed, it had several deficencies. A Nash equilibrium was not\nguaranteed to be unique (sometimes even with uncountable many Nash\nequilibria existing), did not always seem to correspond to a\nreasonable outcome (see Hargreaves Heap and Varoufakis, 2004), and\noccasionally conflicted with people’s intuitions as to what ought to\ncount as a rational outcome. In contrast, it could be shown\nthat a completely mixed evolutionarily stable strategy was unique,\nthat there were at most only a finite number of evolutionarily stable\nstrategies, and that several intuitive definitions of evolutionarily\nstability were equivalent to the original definition of Maynard Smith\nand Price. \nIt was soon realised that evolutionary game theory itself had problems\nstructurally similar to that of the equilibrium selection problem.\nSeveral competing definitions of evolutionary stability were put\nforward, each of which had certain intuitive merit. In addition, as\nthe connection between the static and dynamic\napproaches to evolutionary game theory were explored in detail, it was\nfound that there was, at best, an imperfect fit between static\nconcepts of evolutionary stability and that of dynamic stability.\nFurthermore, dynamical models of evolutionary game theory led to\noutcomes which were expressely irrational from the point of\nview of traditional game theory, such as the preservation of\nstrictly dominated strategies. \nThere are two approaches to evolutionary game theory. The first\napproach derives from the work of Maynard Smith and Price and employs\nthe concept of an evolutionarily stable strategy as the principal tool\nof analysis. The second approach constructs an explicit model of the\nprocess by which the frequency of strategies change in the population\nand studies properties of the evolutionary dynamics within that\nmodel. \nThe first approach can thus be thought of as providing a static\nconceptual analysis of evolutionary stability. “Static”\nbecause, although definitions of evolutionary stability are given, the\ndefinitions advanced do not typically refer to the underlying process\nby which behaviours (or strategies) change in the population. The\nsecond approach, in contrast, does not attempt to define a notion of\nevolutionary stability: once a model of the population dynamics has\nbeen specified, all of the standard stability concepts used in the\nanalysis of dynamical systems can be brought to bear. \nIn game theory, the main solution concept is the Nash\nequilibrium. A Nash equilibrium is a profile of strategies (that\nis, an assignment of strategies to each player) which is a mutual best\nresponse, meaning that no player has any incentive to deviate from\ntheir chosen strategy. \nTo see why the traditional game theoretic solution concept of a Nash\nequilibrium is too weak to capture the notion of evolutionary\nstability, consider the game of figure 1. There are two Nash\nequilibria in pure strategies: \\( (S_1, S_1) \\) and \\( (S_2, S_2) \\).\nSince a Nash equilibrium is a set of mutual best responses, no player\ncan improve their payoff by adopting a different strategy,\nbut a Nash equilibrium allows for the possibility that a player who\ndeviates from their equilibrium strategy receives the same\npayoff. This is the case for the \\((S_2, S_2)\\) equilibrium. And that\nis why a Nash equilibrium does not suffice for evolutionary stability:\nit allows for the possibility of drift away from the equilibrium,\neventually leading to the replacement of the incumbent strategy. \nTo see this, suppose that a population of individuals all followed the\nstrategy \\(S_2\\). If a mutant appeared who played the strategy\n\\(S_1\\), the payoff of the \\(S_1\\)-mutant would be the same as the\nrest of population, and hence there would be no selection pressure\nagainst the mutant. If a second mutant appeared, the payoff earned by\nan \\(S_1-S_1\\) interaction would yield a fitness to both greater than\nthe average fitness of the population. This would allow the \\(S_1\\)\nmutant to spread and eventually take over the rest of the\npopulation. \nFigure 1. A Nash equilibrium is\ninsufficient to capture the notion of a Nash equilibrium \nA strict Nash equilibrium is one where any unilateral\ndeviation from a player’s equilibrium strategy leaves that player\nworse off. Although a strict Nash equilibrium does intuitively capture\none sense of evolutionary stability (it can be thought of as a kind of\n“local optimum”), it can also be shown that a strict Nash equilibrium\nis too strong to capture the idea of evolutionary stability, in\ngeneral. \nTo see this, consider the Hawk-Dove game, analyzed by Maynard Smith\nand Price in their 1973 paper “The Logic of Animal\nConflict.” In this game, two individuals compete for a resource\nof a fixed value \\(V\\). (In biological contexts, the value \\(V\\) of\nthe resource corresponds to an increase in the Darwinian fitness of\nthe individual who obtains the resource; in a cultural context, the\nvalue \\(V\\) of the resource would need to be given an alternate\ninterpretation more appropriate to the specific model at hand.) Each\nindividual follows exactly one of two strategies described below: \nNow assume the following: \nGiven this, the fitness payoffs for the Hawk-Dove game are summarized\naccording to the following matrix: \nFigure 2. The Hawk-Dove Game. (It is\nassumed that \\( V\\lt C \\), as otherwise Hawk dominates Dove.) \nThe Hawk-Dove game has no Nash equilibria in pure strategies and\nexactly one Nash equilibrium in mixed strategies. The mixed strategy\nNash equilibrium has both individuals playing Hawk with probability\n\\(\\frac{V}{C}\\) and Dove with probability \\(1-\\frac{V}{C}\\). Denote\nthis strategy by \\(\\sigma\\). According to the fundamental theorem of\nmixed-strategy Nash equilibria (see Gintis, 2009), it is the case that\n\\[ \\pi(\\text{Hawk} \\mid \\sigma) = \\pi(\\text{Dove} \\mid \\sigma) =\n\\pi(\\sigma \\mid \\sigma), \\] where “\\(\\pi( x \\mid y)\\)” denotes the\npayoff obtained when playing strategy \\(x\\) against someone using the\nstrategy \\(y\\). From this, it follows that for any other\nmixed strategy \\(\\mu\\) it is the case that \\(\\pi( \\mu \\mid \\sigma ) =\n\\pi(\\sigma \\mid \\sigma)\\), and so the Nash equilibrium is not\nstrict. Yet a population where everyone follows the strategy\n\\(\\sigma\\) is still nevertheless able to resist invasion, for it can\nbe shown that \\(\\pi(\\sigma \\mid \\mu) > \\pi(\\mu \\mid \\mu)\\). That\nis, the incumbent strategy \\(\\sigma\\) receives a higher payoff when\nplayed against any mutant strategy \\(\\mu\\), than the mutant strategy\nreceives when played against itself. \nThese considerations lead Maynard Smith (1982) to propose the\nfollowing\n definition:[1] \nDefinition. A strategy \\(\\sigma\\) is an\nevolutionarily stable strategy (ESS) if and only if for all\nother strategies \\(\\mu \\neq \\sigma\\) it is the case that\neither \\(\\pi(\\sigma \\mid \\sigma) \\gt \\pi(\\mu \\mid \\sigma)\\)\nor that \\(\\pi(\\sigma \\mid \\sigma) = \\pi(\\mu \\mid \\sigma)\\)\nand \\(\\pi(\\sigma \\mid \\mu) \\gt \\pi(\\mu \\mid \\mu)\\). \nOr alternatively: \nDefinition. A strategy \\(\\sigma\\) is an\nevolutionarily stable strategy (ESS) if and only if, for all\nother strategies \\(\\mu \\neq \\sigma\\), \nThe second definition, while trivially logically equivalent to the\nfirst, has the advantage of making it clear that every evolutionarily\nstable strategy is also a Nash equilibrium. The first definition, from\nMaynard Smith, has the advantage of making it clear that every strict\nNash equilibrium is also evolutionarily stable. \nFrom this, we see that an evolutionarily stable strategy is a Nash\nequilibrium with an additional second-order stability criterion. It is\na proper strengthening of the Nash equilibrium concept, because\nwhereas every game with finitely many players and a finite number, of\nstrategies has at least one Nash equilibrium (if mixed strategies are\nallowed), not every game has an evolutionarily stable strategy. It is\neasily shown that the game of Rock-Scissors-Paper, shown in figure 3,\ndoes not have an evolutionarily stable strategy. (The only potential\ncandidate for being evolutionarily stable is the Nash equilibrium\nmixed strategy \\(\\sigma\\) which assigns equal probability to all three\npure strategies. But since \\( \\pi( \\sigma \\mid \\sigma) = \\pi(\n\\text{Rock} \\mid \\sigma) \\) and \\(\\pi(\\sigma \\mid \\text{Rock}) = \\pi(\n\\text{Rock} \\mid \\text{Rock})\\), it is not evolutionarily stable.) \nFigure 3. The game of\nRock-Scissors-Paper has no evolutionarily stable strategy. \nDespite the fact that evolutionarily stable strategies do not always\nexist, one advantage of the Maynard Smith and Price definition of an\nevolutionarily stable strategy is that it can be shown to be\nequivalent to two other concepts of evolutionary stability which were\ndefined. Since these three definitions of evolutionary stability are\nnot obviously the same, the fact that they turn out to be logically\nequivalent is interesting. We briefly discuss these other concepts\nbelow, before stating the equivalence result. \nTo begin, recall how in the discussion above regarding the game of\nfigure 1, we appealed informally to the idea of a mutant attempting to\ninvade a population, all of which follow a single incumbent strategy.\nTo make this idea precise, we need to introduce some notation: let\n\\(\\mu, \\sigma\\) be two strategies and let \\(0 \\lt \\epsilon \\lt 1\\).\nThen \\(\\epsilon \\mu + (1-\\epsilon) \\sigma\\) denotes the strategy which\nplays \\(\\mu\\) with probability \\(\\epsilon\\) and plays \\(\\sigma\\) with\nprobability \\(1-\\epsilon\\). However, we may also interpret this as\nrepresenting the strategy used by a player chosen at random\nfrom a large population in which the majority \\((1-\\epsilon)\\) follow\nthe strategy \\(\\sigma\\) and a minority (\\(\\epsilon\\)) follow the\nstrategy \\(\\mu\\) which is attempting to invade. Definition. A strategy \\(\\sigma\\) has\na uniform invasion barrier if there exists an \\(\\bar\\epsilon\n\\gt 0\\) such that for all \\(\\mu \\neq \\sigma\\) and \\(\\epsilon \\in\n(0,\\bar\\epsilon)\\), \\[ \\pi( \\sigma \\mid \\epsilon \\mu +\n(1-\\epsilon)\\sigma) \\gt \\pi( \\mu \\mid \\epsilon \\mu + (1-\\epsilon)\n\\sigma). \\] \nA uniform invasion barrier is a natural concept of evolutionary\nstability. It says that, when a population all follows the same\nstrategy \\(\\sigma\\), except for a small proportion of mutants, all of\nwhom follow a single strategy \\(\\mu\\), the incumbent strategy\n\\(\\sigma\\) has a strictly higher expected fitness in the mixed\npopulation than the invading strategy\n \\(\\mu\\).[2]\n Hence there would be selection against the invading strategy and, in\na sufficiently large population with a sufficiently small number of\nmutants, \\(\\sigma\\) would be evolutionarily stable. \nA second concept of evolutionary stability draws upon the intuition\nthat a stable strategy should rule out the possibility of drift. That\nis, after all, what created problems for the second Nash equilibria in\npure strategies for the game of figure 1. One way of characterising\nthis is as follows. \nDefinition. A strategy \\(\\sigma\\) is said to be\nlocally superior if there exists a neighbourhood \\(U\\) around\n\\(\\sigma\\) such that for all strategies \\(\\mu \\in U\\), where \\(\\mu\\neq\n\\sigma\\), it is the case that \\(\\pi( \\sigma \\mid \\mu) > \\pi (\\mu\n\\mid \\mu)\\). \nOne can then prove the following: \nTheorem (Hofbauer et al., 1979) The following are\nequivalent: \nIn the years following the original work of Maynard Smith and Price,\nalternate analytic solution concepts have been proposed for\nevolutionary game theory. One such alternative is the idea of an\nevolutionarily stable set (see Thomas 1984, 1985a,b).\nConsider, by way of motivation, the game shown in figure 4. In that\ngame, there are no evolutionarily stable strategies, since both\n\\(S_1\\) and \\(S_2\\) receive the same payoff when played against each\nother. However, any population containing a mix of \\(S_1\\)\nand \\(S_2\\) is, in a sense, stable: although drift will certainly\noccur regarding the exact proportion of \\(S_1\\) and \\(S_2\\),\nregardless of the specific mix the population will still tend to drive\nout any \\(S_3\\) or \\(S_4\\) mutants due to the differences in expected\nfitness. \nFigure 4. A game with no evolutionarily\nstable strategy, but one with an evolutionarily stable set. \nOther analytic solution concepts exist, as well. Swinkels (1992)\nintroduced the idea of an equilibrium evolutionarily stable\nset, providing a further refinement of the idea of an\nevolutionarily stable set. (Every evolutionarily stable set contained\nsome equilibrium evolutionarily stable set, but not every equilibrium\nevolutionarily stable set was an evolutionarily stable set.) Hence we\nsee that the search for static solution concepts which suffice for\ncapturing the idea of evolutionary stability encounters a problem\nstructurally similar to that of the equilibrium selection problem in\ntraditional game theory: there are multiple competing concepts of\nevolutionary stability, all of which have some intuitive claim to\nplausibility. \nAs an example of the second approach, consider the well-known\nPrisoner’s Dilemma. In this game, individuals choose one of two\nstrategies, typically called “Cooperate” and\n“Defect.” Here is the general form of the payoff matrix\nfor the prisoner’s dilemma: \nFigure 5. Payoff Matrix for the\nPrisoner’s Dilemma. \nIn figure 5, the payoffs are assumed to satisfy the ordering \\(T \\gt R\n\\gt P \\gt S\\) and \\(\\frac{T + S}2 \\lt R\\). The latter requirement,\nalthough often omitted in discussions of the Prisoner’s Dilemma,\nensures that in the context of an indefinitely repeated game there is\nno net overall advantage in the players alternating between\nCooperate-Defect and Defect-Cooperate. \nHow will a population of individuals that plays the Prisoner’s\nDilemma evolve over time? We cannot answer that question without\nintroducing a few assumptions concerning the nature of the population.\nFirst, assume that the population is quite large and that the\nprobability of interacting with a Cooperator or Defector equals the\nproportion of the population following that strategy. This allows us\nto represent the state of the population by simply keeping track of\nwhat proportion follow the strategies Cooperate and Defect. Let\n\\(p_C\\) and \\(p_D\\) denote these proportions. Denote the expected\nfitness of cooperators and defectors by \\(W_C\\) and \\(W_D\\),\nrespectively, and let \\(\\overline{W}\\) denote the average fitness of\nthe entire population. (These quantities may vary over time; the\ntime-dependency has been suppressed for clarity of notation.) Given\nthese assumptions, the values of \\(W_C, W_D\\), and \\(\\overline{W}\\)\ncan be expressed in terms of the population proportions and payoff\nvalues as follows, where \\(F_0\\) stands for the base fitness level of\nan individual prior to any interaction: \nSecond, assume that the proportion of the population following the\nstrategies Cooperate and Defect in the next generation is related to\nthe proportion of the population following the strategies Cooperate\nand Defect in the current generation according to the following\nrule: \nThe justification for these transition rules is as follows: if \\(W_C\n\\lt \\overline{W}\\), then the expected fitness of Cooperate is lower\nthan the average fitness of the population. That means it is more\nadvantageous to Defect than Cooperate, and so we would expect some\nproportion of the population to switch. The rate at which individuals\nswitch is proportional to how much worse Cooperate does than the\npopulation average. (We are being deliberately ambiguous in terms of\nwhether we are thinking in terms of biological or cultural evolution,\nbut at this level of abstraction it makes little difference.) Since\n\\(\\frac{W_C}{\\overline{W}} \\lt 1\\), it follows that \\(p_c' \\lt p_c\\),\nas would be expected. \nWe can rewrite these expressions in the following form: \nIf we assume that the change in the strategy frequency from one\ngeneration to the next are small, these difference equations may be\napproximated by the differential equations: \nThese equations were offered by Taylor and Jonker (1978) and Zeeman\n(1979) to provide continuous dynamics for evolutionary game theory and\nare known as the replicator dynamics. \nSince it is the case that, for any value of \\(p_c\\) and \\(p_d\\), \\(\nW_C \\lt \\overline{W}\\), future population states will always feature\nfewer cooperators than before. This is represented in the diagram of\nfigure 6. \nFigure 6: The Replicator Dynamical Model\nof the Prisoner’s Dilemma \nThis diagram is interpreted as follows: the leftmost point represents\nthe state of the population where everyone defects, the rightmost\npoint represents the state where everyone cooperates, and intermediate\npoints represent states containing a mix of both cooperators and\ndefectors. (One maps states of the population onto points in the\ndiagram by mapping the state when \\(N\\)% of the population defects\nonto the point of the line \\(N\\)% of the way to the leftmost point.)\nArrows on the line indicate the evolutionary trajectory followed by\nthe population over time. The open circle at the rightmost point\nrepresents the fact that the state where everybody cooperates is an\nunstable equilibrium, in that if a small portion (any amount\n\\(\\epsilon > 0\\)) of the population deviates from the strategy\nCooperate, then the evolutionary dynamics will lead the population\naway from the all-Cooperate state. The solid circle at the leftmost\npoint indicates that the state where everybody Defects is a stable\nequilibrium, in the sense that if some portion of the population\ndeviates from the strategy Defect, then the evolutionary dynamics will\ndrive the population back to the all-Defect state. \nAlthough the replicator dynamics were the first dynamics to be used in\nevolutionary game theory, many alternative dynamics have since been\nexplored. In what follows, we shall talk about evolutionary dynamics\nentirely from the perspective of cultural evolution, which\nmeans nothing more than change in belief (e.g., strategy) over\ntime. \nIn his comprehensive work Population Games and Evolutionary\n Dynamics,[3]\n William Sandholm provides a useful framework which allows us to\nrelate the particular learning rules used by individuals to the\nevolutionary dynamics at the population-level of description. This can\nbe thought of as providing the “microfoundations” of evolutionary game\ntheory, analogous to how the study of individual decision making\nprovides the microfoundations of macroeconomics. \nWe begin with a sketch of the framework for modelling learning rules,\nand then give several examples of individual learning rules and the\nevolutionary dynamics which result at the population-level. For\nsimplicity, all of the mathematical details are suppressed in the\nfollowing discussion; for elaboration, see Sandholm (2010).\n(Sandholm’s framework, in general, allows for games featuring multiple\nnonoverlapping populations. One of the simplifying assumptions made\nhere is that there is only one population.) Assume that we have a\nsymmetric game \\(G\\) with \\(n\\) strategies \\(S_1,\\dots, S_n\\). (A\nsymmetric game is one where the payoff for playing a particular\nstrategy depends only on the strategies used by the other players, not\non who is playing what strategy.) In addition, assume that the only\npossible information which could be taken into account by individuals\nis the following: (1) the current state of the population, represented\nas a distribution over the pure strategies of the game, and (2) the\nexpected payoffs of each strategy, given the current state of the\npopulation.  \nAn individual learning rule (or, to use Sandholm’s terminology, a\nrevision protocol) can be represented as a function which\ntakes these two pieces of information as arguments, mapping them to a\nmatrix of the conditional switch rates between strategies.\nThat is, the \\(ij^{th}\\)-entry of the matrix contains the rate at\nwhich followers of strategy \\(S_i\\) will switch to the strategy\n\\(S_j\\). It is called the conditional switch rate because the\n\\(S_i\\to S_j\\) switch rate will typically depend on — that is,\nbe conditional on — both the state of the population and the\nvector of expected payoffs. Note that the function may not actually\nuse all of the information contained in its arguments: some learning\nrules may be more sophisticated than others. \nFrom this, the population-level evolutionary dynamics can be derived\nelegantly: the instantaneous rate of change for the proportion of the\npopulation following strategy \\(S_i\\) simply equals the total rate at\nwhich players following other strategies switch to\n\\(S_i\\), minus the total rate at which current followers of\n\\(S_i\\) switch to some other strategy. Substituting an\nindividual learning rule into the following equation schema, and then\nsolving the resulting system of equations, gives the population-level\ndynamics. \nThis general framework allows one to investigate\nthe relationship between particular learning rules, at the individual\nlevel, and the evolutionary dynamics, at the population level. Here\nare three examples. \nThe Replicator Dynamics. Suppose each player selects\nsomeone else from the population at random (with all individuals\nequally likely to be selected), and compares their payoff in the last\nround of play with the payoff earned by the person selected. If the\nperson selected received a higher payoff, then the player adopts the\nstrategy used by the person selected with a probability proportional\nto the payoff difference. Schlag (1998) showed that this learning rule\nyields the replicator dynamics. \nThe Brown-Nash-von Neumann dynamics. One key\nassumption made by the learning rule which yields the replicator\ndynamics is that imitation is a reliable guide to future payoffs. This\ncan be problematic for two reasons. First, the fact that a strategy\nhas an expected payoff greater than the average payoff of the\npopulation may simply be indicative of peculiarities of the current\npopulation composition, and not of any particular strategic merit the\nstrategy possesses. Such a learning rule may end up with much of the\npopulation shifting to adopt a strategy, only for its transient\nfitness benefits to disappear. Second, if a strategy is not\npresent in the population at all, it has no chance of being\nadopted by imitation. \nAs an alternative, one might consider a learning rule where the rate\nplayers switch to strategy \\(S_i\\) only depends on whether the\nexpected payoff of \\(S_i\\) exceeds the average payoff of the\npopulation at the current time. Notice that such a learning rule\nattributes a higher degree of rationality to the individual players\nthan the learning rule which generates the replicator dynamics. Why?\nThis learning rule requires people to know the entire set of possible\nstrategies, as well as the associated payoff matrix, so that they can\ndetermine if a strategy presently absent from the population would be\nworth adopting. When this learning rule is plugged into the above\nschema, one obtains the Brown-Nash-von Neumann (BNN) dynamic (see\nBrown and von Neumann, 1950). Unlike the replicator dynamics, the BNN\ndynamic can introduce new strategies into the population\nwhich are not represented. When the population is started in the\nstates where everyone plays Rock, Paper or Scissors, the BNN dynamic\nwill eventually end up in a state where all strategies are\nrepresented. \nThe Smith Dynamics. One unusual feature of the\nlearning rule which generates the BNN dynamic is that it compares the\nexpected payoff of alternative possible strategies with the average\npayoff of the population. One might wonder why performing better than\nthe population average is a sensible point of comparison, since the\naverage payoff of the population is often not actually achievable by\nany particular strategy available to the players. Instead,\nconsider the learning rule which compares the expected payoff of one’s\ncurrent strategy, in the present population state, with the\nexpected payoff of other possible strategies, in the present\npopulation state, but where only those alternative strategies which\nhave a higher expected payoff have a nonzero probability of being\nadopted. When plugged into the Sandholm framework above, this learning\nrule generates an evolutionary dynamic first studied by Smith (1984)\nand is thus known as the Smith dynamic. \nGiven the number of different types of evolutionary dynamics, as seen\nin section 2.2, and the number of different concepts of evolutionary\nstability, as seen in section 2.1, a first question to ask is what\nrelationships exist between the two? A second question to ask is what\nrelationships exist between the various families of evolutionary\ndynamics and what one might consider to be the “rational” outcome of a\ngame? Answers to these questions turn out to be more subtle and\ncomplex than one might first anticipate. \nOne complication, at the outset, is that an ESS is a\nstrategy, possibly mixed, which satisfies certain properties.\nIn contrast, all of the evolutionary dynamics described above model\npopulations where individuals employ only pure strategies. How, then,\nare we to relate the two concepts? \nOne natural suggestion is to interpret the probabilities which appear\nin an evolutionarily stable strategy as population\nfrequencies. When the probabilities are understood in this way,\none speaks of an evolutionarily stable state, in order to\nstress the difference in interpretation. One can then ask under what\nconditions a particular evolutionary dynamic will converge to an\nevolutionarily stable state. \nIn the case of the replicator dynamics, it is immediately apparent\nthat the replicator dynamics need not converge to an evolutionarily\nstable state. This is because, as noted previously, the replicator\ndynamics cannot introduce strategies into the population if they are\ninitially absent. Hence, if an evolutionarily stable state requires\ncertain pure strategies to be present, and those pure strategies do\nnot appear in the initial population state, then the replicator\ndynamics will not converge to the evolutionarily stable state. \nThis can be seen in a particularly stark form in figure 6, above. In\nthe case of the Prisoner’s Dilemma, if the population begins in the\nstate where everyone cooperates, the replicator dynamics will\nremain in that state forever, because the strategy of Defect cannot be\nintroduced. This shows that, under the replicator dynamics, there can\nbe instances where even strictly dominated strategies will\npersist. \nThat said, it can also be seen in figure 6 that whenever there is a\nnonzero proportion of Defectors present in the population, that they\nwill increase in number and eventually drive Cooperate to extinction.\n(In the limit, because another property of the replicator dynamics is\nthat no strategy which appears can ever go extinct in a finite amount\nof time.) This motivates the following result: \nTheorem (Akin, 1980) Let \\(G\\) be a symmetric,\ntwo-player game, and let \\(\\vec p(0)\\) be an initial population state\nin which all pure strategies are represented (that is, appear with a\nfrequency greater than zero). Then, under the replicator dynamics\nbeginning at the initial state \\(\\vec p(0)\\), all strictly dominated\nstrategies will disappear in the limit. \nWhat the above theorem shows is that, although there are cases where\nstrictly dominated strategies may persist under the replicator\ndynamics, such cases are rare. As long as all strategies are initially\npresent, no matter to how small an extent, the replicator dynamics\neliminates strictly dominated strategies. \nHowever, the same does not hold for weakly dominated\nstrategies. A strategy \\(A\\) is said to weakly dominate the strategy\n\\(B\\) if \\(A\\) does at least as well as \\(B\\) against all possible\ncompetitors, and there is at least one case where \\(A\\) does strictly\nbetter. When this happens, the strategy \\(B\\) is known as a weakly\ndominated strategy. Weakly dominated strategies can can\nappear in a Nash equilibrium, as figure 7 shows below. When both\nplayers adopt \\(S_2\\), it is in neither player’s interest to switch\nbecause they continue to receive a payoff of 100 — and so we\nhave a Nash equilibrium when both players adopt \\(S_2\\). Yet it is\nalso the case that \\(S_1\\) weakly dominates \\(S_2\\). \nFigure 7. A game in which a weakly\ndominated strategy \\((S_2)\\) appears in a Nash equilibrium. \nIt can be shown (see Weibull, 1995) that a weakly dominated strategy\ncan never be an ESS. In the case of the game shown in figure\n7, this is surprising because the equilibrium generated by the weakly\ndominated strategy is Pareto optimal and has a much higher expected\npayoff than any other Nash equilibrium. However, it is also the case\n(see Skyrms, 1996) that the replicator dynamics need not eliminate\nweakly dominated strategies. In fact, in chapter 2 of Evolution of\nthe Social Contract, Brian Skyrms shows that there are some games\nin which the replicator dynamics almost always yields outcomes\ncontaining a weakly dominated strategy! This shows that there can be\nconsiderable disagreement between the evolutionary outcomes of the\nreplicator dynamics and what the static approach identifies as an\nevolutionarily stable strategy. \nThe potential disagreement between the outcomes of an evolutionary\ndynamic and what ordinary game theory would consider to be a\n“rational” outcome of play is not just limited to the replicator\ndynamics. For example, consider the BNN dynamic and the Smith dynamic,\ndescribed in section 2.2. In both cases, the underlying learning rule\nwhich generates those dynamics has some intuitive plausibility. In\nparticular, each of those learning rules can be seen as employing\nslightly more rational approaches to the problem of strategy revision\nthan the imitative learning rule which generated the replicator\ndynamics. Yet Hofbauer and Sandholm (2011) show that both the BNN\ndynamic and the Smith dynamic are not guaranteed to eliminate strictly\ndominated strategies! \nConsider the game of figure 8 below. This is known as the game of\n“Rock-Paper-Scissors with a feeble twin”. In this game, the Twin\nstrategy is identical to Paper with the exception that all of its\npayoffs are decreased uniformly by some small amount \\(\\varepsilon\n> 0\\) (hence, the “feeble twin”). This means that the Twin strategy\nis strictly dominated by Paper, since there are absolutely no\ninstances in which it is rationally preferable to play Twin instead of\nPaper. Yet, under the Smith dynamic, there are a nontrivial number of\ninitial conditions which end up trapped in cycles where the Twin\nstrategy is played by a nontrivial portion of the population. \nFigure 8. Rock-Scissors-Paper with a\nfeeble twin. \nThe connection between ESSs and stable states under an evolutionary\ndynamical model is weakened further if we do not model the dynamics\nusing a continuous population model. For example, suppose we use a\nlocal interaction model in which each individual plays the\nprisoner’s dilemma with his or her neighbors. Nowak and May\n(1992, 1993), using a spatial model in which local interactions occur\nbetween individuals occupying neighboring nodes on a square lattice,\nshow that stable population states for the prisoner’s dilemma\ndepend upon the specific form of the payoff matrix.\n [4]\n (What is interesting about this finding is that, in all cases, it\nremains true that Defect strictly dominates Cooperate, so the\nfundamental underlying strategic problem described by the game has not\nchanged.) \nWhen the payoff matrix for the population has the values \\(T = 2.8\\),\n\\(R = 1.1\\), \\(P = 0.1\\), and \\(S = 0\\), the evolutionary dynamics of\nthe local interaction model agree with those of the replicator\ndynamics, and lead to a state where each individual follows the\nstrategy Defect—which is, as noted before, the only\nevolutionarily stable strategy in the prisoner’s dilemma. The\nfigure below illustrates how rapidly one such population converges to\na state where everyone defects. \nFigure 9: Prisoner’s Dilemma: All\nDefect\n[View a movie of this model]\n  \nHowever, when the payoff matrix has values of \\(T = 1.2, R = 1.1, P =\n0\\).1, and \\(S = 0\\), the evolutionary dynamics carry the population\nto a stable cycle oscillating between two states. In this cycle\ncooperators and defectors coexist, with some regions containing\n“blinkers” oscillating between defectors and cooperators\n(as seen in generation 19 and 20). \nFigure 10: Prisoner’s Dilemma:\nCooperate\n[View a movie of this model]\n  \nNotice that with these particular settings of payoff values, the\nevolutionary dynamics of the local interaction model differ\nsignificantly from those of the replicator dynamics. Under these\npayoffs, the stable states have no corresponding analogue in either\nthe replicator dynamics nor in the analysis of evolutionarily stable\nstrategies. \nA phenomenon of greater interest occurs when we choose payoff values\nof \\(T = 1.61, R = 1.01, P = 0\\).01, and \\(S = 0\\). Here, the dynamics\nof local interaction lead to a world constantly in flux: under these\nvalues regions occupied predominantly by Cooperators may be\nsuccessfully invaded by Defectors, and regions occupied predominantly\nby Defectors may be successfully invaded by Cooperators. In this\nmodel, there is no “stable strategy” in the traditional\ndynamical sense.\n [5] \nFigure 11: Prisoner’s Dilemma:\nChaotic\n[view a movie of this model]\n  \nThese results demonstrate that, although there are cases where both\nthe static and dynamic approaches to evolutionary game theory agree\nabout the expected outcome of an evolutionary game, there are enough\ndifferences in the outcomes of the two modes of analysis to justify\nthe development of each program independently. \nThe concept of a Nash equilibrium (see the entry on\n game theory)\n has been the most used solution concept in game theory since its\nintroduction by John Nash (1950). A selection of strategies by a group\nof agents is said to be in a Nash equilibrium if each agent’s\nstrategy is a best-response to the strategies chosen by the other\nplayers. By best-response, we mean that no individual can improve her\npayoff by switching strategies unless at least one other individual\nswitches strategies as well. This need not mean that the payoffs to\neach individual are optimal in a Nash equilibrium: indeed, one of the\ndisturbing facts of the prisoner’s dilemma is that the only Nash\nequilbrium of the game—when both agents defect—is\n suboptimal.[6] \nYet a difficulty arises with the use of Nash equilibrium as a solution\nconcept for games: if we restrict players to using pure strategies,\nnot every game has a Nash equilbrium. The game “Matching\nPennies” illustrates this problem. \nFigure 12: Payoff matrix for the game of\nMatching Pennies. (Row wins if the two coins do not match, whereas\nColumn wins if the two coins match). \nWhile it is true that every noncooperative game in which players may\nuse mixed strategies has a Nash equilibrium, some have questioned the\nsignificance of this for real agents. If it seems appropriate to\nrequire rational agents to adopt only pure strategies (perhaps because\nthe cost of implementing a mixed strategy runs too high), then the\ngame theorist must admit that certain games lack solutions. \nA more significant problem with invoking the Nash equilibrium as the\nappropriate solution concept arises because some games have\nmultiple Nash equilibria (see the section on\n Solution Concepts and Equilibria,\n in the entry on game theory). When there are several different Nash\nequilibria, how is a rational agent to decide which of the several\nequilibria is the “right one” to settle\n upon?[7]\n Attempts to resolve this problem have produced a number of possible\nrefinements to the concept of a Nash equilibrium, each refinement\nhaving some intuitive purchase. Unfortunately, so many refinements of\nthe notion of a Nash equilibrium have been developed that, in many\ngames which have multiple Nash equilibria, each equilibrium could be\njustified by some refinement present in the literature. The problem\nhas thus shifted from choosing among multiple Nash equilibria to\nchoosing among the various refinements. \nSamuelson (1997), in his work Evolutionary Games and Equilibrium\nSelection) expressed hope that further development of\nevolutionary game theory could be of service in addressing the\nequilibrium selection problem. At present, this hope does not seem to\nhave been realised. As section 2.1 showed, there are multiple\ncompeting concepts of evolutionary stability in play. Furthermore, as\nsection 3 showed, there is an imperfect agreement between what is\nevolutionary stable, in the dynamic setting, and what is evolutionary\nstable, in the static setting. \nNumerous results from experimental economics have shown that these\nstrong rationality assumptions do not describe the behavior of real\nhuman subjects. Humans are rarely (if ever) the hyperrational agents\ndescribed by traditional game theory. For example, it is not uncommon\nfor people, in experimental situations, to indicate that they prefer\n\\(A\\) to \\(B, B\\) to \\(C\\), and \\(C\\) to \\(A\\). These “failures\nof the transitivity of preference” would not occur if people had\na well-defined consistent set of preferences. Furthermore, experiments\nwith a class of games known as a “beauty pageant” show,\nquite dramatically, the failure of common knowledge assumptions\ntypically invoked to solve\n games.[8]\n Since evolutionary game theory successfully explains the predominance\nof certain behaviors of insects and animals, where strong rationality\nassumptions clearly fail, this suggests that rationality is not as\ncentral to game theoretic analyses as previously thought. The hope,\nthen, is that evolutionary game theory may meet with greater success\nin describing and predicting the choices of human subjects, since it\nis better equipped to handle the appropriate weaker rationality\nassumptions. Indeed, one of the great strengths of the framework\nintroduced by Sandholm (2010) is that it provides a general method for\nlinking the learning rules used by individuals, at the micro level,\nwith the dynamics describing changes in the population, at the macro\nlevel. \nAt the end of the first chapter of Theory of Games and Economic\nBehavior, von Neumann and Morgenstern write: \nThe theory of evolution is a dynamical theory, and the second approach\nto evolutionary game theory sketched above explicitly models the\ndynamics present in interactions among individuals in the population.\nSince the traditional theory of games lacks an explicit treatment of\nthe dynamics of rational deliberation, evolutionary game theory can be\nseen, in part, as filling an important lacuna of traditional game\ntheory. \nOne may seek to capture some of the dynamics of the decision-making\nprocess in traditional game theory by modeling the game in its\nextensive form, rather than its normal form. However, for most games\nof reasonable complexity (and hence interest), the extensive form of\nthe game quickly becomes unmanageable. Moreover, even in the extensive\nform of a game, traditional game theory represents an\nindividual’s strategy as a specification of what choice that\nindividual would make at each information set in the game. A selection\nof strategy, then, corresponds to a selection, prior to game play, of\nwhat that individual will do at any possible stage of the game. This\nrepresentation of strategy selection clearly presupposes hyperrational\nplayers and fails to represent the process by which one player\nobserves his opponent’s behavior, learns from these\nobservations, and makes the best move in response to what he has\nlearned (as one might expect, for there is no need to model learning\nin hyperrational individuals). The inability to model the dynamical\nelement of game play in traditional game theory, and the extent to\nwhich evolutionary game theory naturally incorporates dynamical\nconsiderations, reveals an important virtue of evolutionary game\ntheory. \nEvolutionary game theory has been used to explain a number of aspects\nof human behavior. A small sampling of topics which have been analysed\nfrom the evolutionary perspective include: altruism\n(Fletcher and Zwick, 2007; Gintis et al., 2003;\nSánchez and Cuesta, 2005; Trivers, 1971), behavior in\npublic goods game (Clemens and Riechmann, 2006; Hauert, 2006;\nHauert et al., 2002, 2006; Huberman and Glance, 1995),\nempathy (Page and Nowak, 2002; Fishman, 2006),\nhuman culture (Enquist and Ghirlanda, 2007; Enquist\net al., 2008), moral behaviour (Alexander,\n2007; Boehm, 1982; Harms and Skyrms, 2008; Skyrms 1996, 2004),\nprivate property (Gintis, 2007), signaling\nsystems and other proto-linguistic behaviour (Barrett, 2007;\nHausken and Hirshleirfer, 2008; Hurd, 1995; Jäger, 2008; Nowak\net al., 1999; Pawlowitsch, 2007, 2008; Skyrms, 2010; Zollman,\n2005), social learning (Kameda and Nakanishi, 2003;\nNakahashi, 2007; Rogers, 1988; Wakano and Aoki, 2006; Wakano et\nal., 2004), and social norms (Axelrod, 1986;\nBicchieri, 2006; Binmore and Samuelson, 1994; Chalub et al.,\n2006; Kendal et al., 2006; Ostrum, 2000). \nThe following subsections provide a brief illustration of the use of\nevolutionary game theoretic models to explain two areas of human\nbehavior. The first concerns the tendency of people to share equally\nin perfectly symmetric situations. The second shows how populations of\npre-linguistic individuals may coordinate on the use of a simple\nsignaling system even though they lack the ability to communicate.\nThese two models have been pointed to as preliminary explanations of\nour sense of fairness and language, respectively. They were selected\nfor inclusion here for three reasons: (1) the relative simplicity of\nthe model, (2) the apparent success at explaining the phenomenon in\nquestion, and (3) the importance of the phenomenon to be\nexplained. \nOne natural game to use for investigating the evolution of fairness is\ndivide-the-cake (this is the simplest version of the Nash\nbargaining game). In chapter 1 of Evolution of the Social\nContract, Skyrms presents the problem as follows: \nMore formally, suppose that two individuals are presented with a\nresource of size \\(C\\) by a third party. A strategy for a\nplayer, in this game, consists of an amount of cake that he would\nlike. The set of possible strategies for a player is thus any amount\nbetween 0 and \\(C\\). If the sum of strategies for each player is less\nthan or equal to \\(C\\), each player receives the amount he asked for.\nHowever, if the sum of strategies exceeds \\(C\\), no player receives\nanything. Figure 13 illustrates the feasible set for this game. \nFigure 13: The feasible set for the game\nof Divide-the-Cake. In this figure, the cake is of size \\(C=10\\) but\nall strategies between 0 and 10 inclusive are permitted for either\nplayer (including fractional demands). \nWe have a clear intuition that the “obvious” strategy for\neach player to select is C/2; the philosophical problem lies\nin explaining why agents would choose that strategy rather\nthan some other one. Even in the perfectly symmetric situation,\nanswering this question is more difficult than it first appears. To\nsee this, first notice that there are an infinite number of Nash\nequilibria for this game. If player 1 asks for \\(p\\) of the cake,\nwhere \\(0 \\le p \\le C\\), and player 2 asks for \\(C - p\\), then this\nstrategy profile is a Nash equilibrium for any value of \\(p \\in\n[0,C]\\). (Each player’s strategy is a best response given what\nthe other has chosen, in the sense that neither player can increase\nher payoff by changing her strategy.) Thus the equal split is only one\nof infinitely many Nash equilibria. \nOne might propose that both players should choose that strategy which\nmaximizes their expected payoff on the assumption they are uncertain\nas to whether they will be assigned the role of Player 1 or Player 2.\nThis proposal, Skyrms notes, is essentially that of Harsanyi (1953).\nThe problem with this is that if players only care about their\nexpected payoff, and they think that it is equally likely that they\nwill be assigned the role of Player 1 or Player 2, then this, too,\nfails to select uniquely the equal split. Consider the strategy\nprofile \\(\\langle p, C - p\\rangle\\) which assigns Player 1 \\(p\\)\nslices and Player 2 \\(C - p\\) slices. If a player thinks it is equally\nlikely that he will be assigned the role of Player 1 or Player 2, then\nhis expected utility is \\(\\frac{1}{2} p + \\frac{1}{2}(C - p) =\n\\frac{C}{2}\\), for all values \\(p \\in[0, C]\\). \nNow consider the following evolutionary model: suppose we have a\npopulation of individuals who pair up and repeatedly play the game of\ndivide-the-cake, modifying their strategies over time in a way which\nis described by the replicator dynamics. For convenience, let us\nassume that the cake is divided into 10 equally sized slices and that\neach player’s strategy conforms to one of the following 11\npossible types: Demand 0 slices, Demand 1 slice, … , Demand 10\nslices. For the replicator dynamics, the state of the population is\nrepresented by a vector \\(\\langle p_0, p_1 , \\ldots ,p_{10}\\rangle\\)\nwhere each \\(p_i\\) denotes the frequency of the strategy “Demand\n\\(i\\) slices” in the population. \nThe replicator dynamics allows us to model how the distribution of\nstrategies in the population changes over time, beginning from a\nparticular initial condition. Figure 14 below shows two evolutionary\noutcomes under the continuous replicator dynamics. Notice that\nalthough fair division can evolve, as in Figure 14(a), it is not the\nonly evolutionary stable outcome, as Figure 14(b) illustrates. \nFigure 14: Two evolutionary outcomes\nunder the continuous replicator dynamics for the game of\ndivide-the-cake. Of the eleven strategies present, only three are\ncolour-coded so as to be identifiable in the plot, as noted in the\nlegend.   \nRecall that the task at hand was to explain why we think the\n“obvious” strategy choice in a perfectly symmetric\nresource allocation problem is for both players to ask for half of the\nresource. What the above shows is that, in a population of boundedly\nrational agents who modify their behaviours in a manner described by\nthe replicator dynamics, fair division is one, although not the only,\nevolutionary outcome. The tendency of fair division to emerge,\nassuming that any initial condition is equally likely, can be measured\nby determining the size of the\n basin of attraction\n of the state where everyone in the population uses the strategy\nDemand 5 slices. Skyrms (1996) measures the size of the basin of\nattraction of fair division using\n Monte Carlo methods,\n finding that fair division evolves roughly 62% of the time. \nHowever, it is important to realise that the replicator dynamics\nassumes any pairwise interaction between individuals is equally\nlikely. In reality, quite often interactions between individuals are\ncorrelated to some extent. Correlated interaction can occur\nas a result of spatial location (as shown above for the case of the\nspatial prisoner’s dilemma), the structuring effect of social\nrelations, or ingroup/outgroup membership effects, to list a few\ncauses. \nWhen correlation is introduced, the frequency with which fair division\nemerges changes drastically. The amount of correlation in the model is\nrepresented by the correlation coefficient \\(\\varepsilon\\),\nwhich can range between 0 and 1. When \\(\\varepsilon=0\\), there is no\ncorrelation at all and the likelihood of pairwise interactions is\ndetermined simply by the proportion of agents in the population\nfollowing a particular strategy. When \\(\\varepsilon=1\\), correlation\nis perfect and agents following a particular strategy only interact\nwith their own kind. Intermediate levels of correlation introduce some\ntendency for agents to interact with their own kind, where the\ntendency increases with the value of \\(\\varepsilon\\). Figure 15\nillustrates how the basin of attraction of All Demand 5 changes as the\ncorrelation coefficient \\(\\varepsilon\\) increases from 0 to\n 0.2.[9]\n Once the amount of correlation present in the interactions reaches\n\\(\\varepsilon = 0.2\\), fair division is virtually an evolutionary\ncertainty. Note that this does not depend on there only being three\nstrategies present: allowing for some correlation between interactions\nincreases the probability of fair division evolving even if the\ninitial conditions contain individuals using any of the eleven\npossible strategies. \nFigure 15: Three diagrams showing how,\nas the amount of correlation among interactions increases, fair\ndivision is more likely to evolve. In figures 15(a) and 15(b), there\nis an unstable fixed point in the interior of space where all three\nstrategies are present in the population. (This is the point where the\nevolutionary trajectories appear to intersect.) This fixed point is\nwhat is known as a saddle point in dynamical systems theory:\nthe smallest perturbation will cause the population to evolve away\nfrom that point to one of the other two attractors. \nWhat, then, can we conclude from this model regarding the evolution of\nfair division? It all depends, of course, on how accurately the\nreplicator dynamics models the primary evolutionary forces (cultural\nor biological) acting on human populations. Although the replicator\ndynamics are a “simple” mathematical model, it does\nsuffice for modelling both a type of biological evolution (see Taylor\nand Jonker, 1978) and a type of cultural evolution (see Börgers\nand Sarin, 1996; Weibull, 1995). As Skyrms (1996) notes: \nThis claim, of course, has not gone without comment. For a selection\nof some discussion see, in particular, D’Arms (1996, 2000);\nD’Arms et al., 1998; Danielson (1998); Bicchieri\n(1999); Kitcher (1999); Gintis (2000); Harms (2000); Krebs (2000);\nAlexander and Skyrms (1999); and Alexander (2000, 2007). \nIn his seminal work Convention, David Lewis developed the\nidea of sender-receiver games. Such games have been used to explain\nhow language, and semantic content, can emerge in a community which\noriginally did not possess any language\n whatsoever.[10]\n His original definition is as follows (with portions of extraneous\ncommentary deleted for concision and points enumerated for clarity and\nlater reference): \nWhenever \\(Fc\\) and \\(Fa\\) combine […] to give the preferred\ndependence of the audience’s response upon the state of affairs,\nwe call \\(\\langle Fc, Fa\\rangle\\) a signaling system. (Lewis,\n1969, pp. 130–132) \nSince the publication of Convention, it is more common to\nrefer to the communicator as the sender and the members of\nthe audience as receivers. The basic idea behind\nsender-receiver games is the following: Nature selects which state of\nthe world obtains. The person in the role of Sender observes this\nstate of the world (correctly identifying it), and sends a signal to\nthe person in the role of Receiver. The Receiver, upon receipt of this\nsignal, performs a response. If what the Receiver does is the correct\nresponse, given the state of the world, then both players receive a\npayoff of 1; if the Receiver performed an incorrect response, then\nboth players receive a payoff of 0. Notice that, in this simplified\nmodel, no chance of error exists at any stage. The Sender always\nobserves the true state of the world and always sends the signal he\nintended to send. Likewise, the Receiver always receives the signal\nsent by the Sender (i.e., the channel is not noisy), and the Receiver\nalways performs the response he intended to. \nWhereas Lewis allowed the “audience” to consist of more\nthan one person, it is more common to consider sender-receiver games\nplayed between two people, so that there is only a single receiver\n(or, in Lewisian terms, a single member of the\n audience).[11]\n For simplicity, in the following we will consider a two-player,\nsender-receiver game with two states of the world \\(\\{S_1, S_2\\}\\),\ntwo signals \\(\\{\\sigma_1, \\sigma_2\\}\\), and two responses \\(\\{r_1,\nr_2\\}\\). (We shall see later why larger sender-receiver games are\nincreasingly difficult to analyse.) \nNotice that, in point (2) of his definition of sender-receiver games,\nLewis requires two things: that there be a unique best response to the\nstate of the world (this is what requiring \\(F\\) to be one-to-one\namounts to) and that everyone in the audience agrees that this is the\ncase. Since we are considering the case where there is only a single\nresponder, the second requirement is otiose. For the case of two\nstates of the world and two responses, there are only two ways of\nassigning responses to states of the world which satisfy Lewis’s\nrequirement. These are as follows (where \\(X \\Rightarrow Y\\) denotes\n“in state of the world \\(X\\), the best response is to do\n\\(Y\\)”): \nIt makes no real difference for the model which one of these we\nchoose, so pick the intuitive one: in state of the world \\(S_i\\), the\nbest response is \\(r_i\\) (i.e., function 1). \nA strategy for the sender (what Lewis called a\n“communicator’s contingency plan”) consists of a\nfunction specifying what signal he sends given the state of the world.\nIt is, as Lewis notes, a function from the set of states of the world\ninto the set of signals. This means that it is possible that\na sender may send the same signal in two different states of\nthe world. Such a strategy makes no sense, from a rational point of\nview, because the receiver would not get enough information to be able\nto identify the correct response for the state of the world. However,\nwe do not exclude these strategies from consideration because they are\nlogically possible strategies. \nHow many sender strategies are there? Because we allow for the\npossibility of the same signal to be sent for multiple states of the\nworld, there are two choices for which signal to send given state\n\\(S_1\\) and two choices for which signal to send given state \\(S_2\\).\nThis means there are four possible sender strategies. These strategies\nare as follows (where \\(\\mathrm{`}X \\rightarrow Y\\text{'}\\) means that\nwhen the state of the world is \\(X\\) the sender will send signal\n\\(Y)\\): \nWhat is a strategy for a receiver? Here, it proves useful to deviate\nfrom Lewis’s original definition of the “audience’s\ncontingency plan”. Instead, let us take a receiver’s\nstrategy to be a function from the set of signals into the set of\nresponses. As in the case of the sender, we allow the receiver to\nperform the same response for more than one signal. By symmetry, this\nmeans there are \\(\\mathbf{4}\\) possible receiver strategies. These\nreceiver strategies are: \nIf the roles of Sender and Receiver are permanently assigned to\nindividuals — as Lewis envisaged — then there are only two\nsignaling systems: \\(\\langle\\)Sender 2, Receiver 2\\(\\rangle\\) and\n\\(\\langle\\)Sender 3, Receiver 3\\(\\rangle\\). All other possible\ncombinations of strategies result in the players failing to\ncoordinate. The coordination failure occurs because the Sender and\nReceiver only pair the appropriate action with the state of the world\nin one instance, as with \\(\\langle\\)Sender 1, Receiver 1\\(\\rangle\\),\nor not at all, as with \\(\\langle\\)Sender 2, Receiver 3\\(\\rangle\\). \nWhat if the roles of Sender and Receiver are not permanently assigned\nto individuals? That is, what if nature flips a coin and assigns one\nplayer to the role of Sender and the other player to the role of\nReceiver, and then has them play the game? In this case, a\nplayer’s strategy needs to specify what he will do when assigned\nthe role of Sender, as well as what he will do when assigned the role\nof Receiver. Since there are four possible strategies to use as Sender\nand four possible strategies to use as Receiver, this means that there\nare a total of \\(\\mathbf{16}\\) possible strategies for the\nsender-receiver game when roles are not permanently assigned to\nindividuals. Here, a player’s strategy consists of an ordered\npair (Sender \\(X\\), Receiver \\(Y)\\), where \\(X, Y \\in \\{1, 2, 3,\n4\\}\\). \nIt makes a difference whether one considers the roles of Sender and\nReceiver to be permanently assigned or not. If the roles are assigned\nat random, there are four signaling systems amongst two\n players[12]: \nSignaling systems 3 and 4 are curious. System 3 is a case where, for\nexample, I speak in French but listen in German, and you speak German\nbut listen in French. (System 4 swaps French and German for both you\nand me.) Notice that in systems 3 and 4 the players are able to\ncorrectly coordinate the response with the state of the world\nregardless of who gets assigned the role of Sender or\nReceiver. \nThe problem, of course, with signaling systems 3 and 4 is that neither\nPlayer 1 nor Player 2 would do well when pitted against a clone of\nhimself. They are cases where the signaling system would not work in a\npopulation of players who are pairwise randomly assigned to play the\nsender-receiver game. In fact, it is straightforward to show that the\nstrategies (Sender 2, Receiver 2) and (Sender 3, Receiver 3) are the\nonly evolutionarily stable strategies (see Skyrms 1996,\n89–90). \nAs a first approach to the dynamics of sender-receiver games, let us\nrestrict attention to the four strategies (Sender 1, Receiver 1),\n(Sender 2, Receiver 2), (Sender 3, Receiver 3), and (Sender 4,\nReceiver 4). Figure 16 illustrates the state space under the\ncontinuous replicator dynamics for the sender-receiver game consisting\nof two states of the world, two signals, and two responses, where\nplayers are restricted to using one of the previous four strategies.\nOne can see that evolution leads the population in almost all\n cases[13]\n to converge to one of the two signaling\n systems.[14] \nFigure 16: The evolution of signaling\nsystems. \nFigure 17 illustrates the outcome of one run of the replicator\ndynamics (for a single population model) where all sixteen possible\nstrategies are represented. We see that eventually the population, for\nthis particular set of initial conditions, converges to one of the\npure Lewisian signalling systems identified above. \nFigure 17: The evolution of a signalling\nsystem under the replicator dynamics. \nWhen the number of states of the world, the number of signals, and the\nnumber of actions increase from 2, the situation rapidly becomes much\nmore complex. If there are \\(N\\) states of the world, \\(N\\) signals,\nand \\(N\\) actions, the total number of possible strategies equals\n\\(N^{2N}\\). For \\(N=2\\), this means there are 16 possible strategies,\nas we have seen. For \\(N=3\\), there are 729 possible strategies, and a\nsignalling problem where \\(N=4\\) has 65,536 possible strategies. Given\nthis, one might think that it would prove difficult for evolution to\nsettle upon an optimal signalling system. \nSuch an intuition is correct. Hofbauer and Hutteger (2008) show that,\nquite often, the replicator dynamics will converge to a suboptimal\noutcome in signalling games. In these suboptimal outcomes, a\npooling or partial pooling equilibrium will emerge.\nA pooling equilibrium occurs when the Sender uses the same signal\nregardless of the state of the world. A partial pooling equilibrium\noccurs when the Sender is capable of differentiating between some\nstates of the world but not others. As an example of a partial pooling\nequilibrium, consider the following strategies for the case where\n\\(N=3\\): Suppose that the Sender sends signal 1 in state of the world\n1, and signal 2 in states of the world 2 and 3. Furthermore, suppose\nthat the Receiver performs action 1 upon receipt of signal 1, and\naction 2 upon receipt of signals 2 and 3. If all states of the world\nare equiprobable, this is a partial pooling equilibrium. Given that\nthe Sender does not differentiate states of the world 2 and 3, the\nReceiver cannot improve his payoffs by responding differently to\nsignal 2. Given the particular response behaviour of the Receiver, the\nSender cannot improve her payoffs by attempting to differentiate\nstates of the world 2 and 3. \nAs noted previously, evolutionary game theoretic models may often be\ngiven both a biological and a cultural evolutionary interpretation. In\nthe biological interpretation, the numeric quantities which play a\nrole analogous to “utility” in traditional game theory\ncorrespond to the fitness (typically Darwinian fitness) of\n individuals.[15]\n How does one interpret “fitness” in the cultural\nevolutionary interpretation? \nIn many cases, fitness in cultural evolutionary interpretations of\nevolutionary game theoretic models directly measures some objective\nquantity of which it can be safely assumed that (1) individuals always\nwant more rather than less and (2) interpersonal comparisons are\nmeaningful. Depending on the particular problem modeled, money, slices\nof cake, or amount of land would be appropriate cultural evolutionary\ninterpretations of fitness. Requiring that fitness in cultural\nevolutionary game theoretic models conform to this interpretative\nconstraint severely limits the kinds of problems that one can address.\nA more useful cultural evolutionary framework would provide a more\ngeneral theory which did not require that individual fitness be a\nlinear (or strictly increasing) function of the amount of some real\nquantity, like amount of food. \nIn traditional game theory, a strategy’s fitness was measured by\nthe expected utility it had for the individual in question. Yet\nevolutionary game theory seeks to describe individuals of limited\nrationality (commonly known as “boundedly rational”\nindividuals), and the utility theory employed in traditional game\ntheory assumes highly rational individuals. Consequently, the utility\ntheory used in traditional game theory cannot simply be carried over\nto evolutionary game theory. One must develop an alternate theory of\nutility/fitness, one compatible with the bounded rationality of\nindividuals, that is sufficient to define a utility measure adequate\nfor the application of evolutionary game theory to cultural\nevolution. \nAnother question facing evolutionary game theoretic explanations of\nsocial phenomena concerns the kind of explanation it seeks to give.\nDepending on the type of explanation it seeks to provide, are\nevolutionary game theoretic explanations of social phenomena\nirrelevant or mere vehicles for the promulgation of pre-existing\nvalues and biases? To understand this question, recognize that one\nmust ask whether evolutionary game theoretic explanations target the\netiology of the phenomenon in question, the persistence of the\nphenomenon, or various aspects of the normativity attached to the\nphenomenon. The latter two questions seem deeply connected, for\npopulation members typically enforce social behaviors and rules having\nnormative force by sanctions placed on those failing to comply with\nthe relevant norm; and the presence of sanctions, if suitably strong,\nexplains the persistence of the norm. The question regarding a\nphenomenon’s etiology, on the other hand, can be considered\nindependent of the latter questions. \nIf one wishes to explain how some currently existing social phenomenon\ncame to be, it is unclear why approaching it from the point of view of\nevolutionary game theory would be particularily illuminating. The\netiology of any phenomenon is a unique historical event and, as such,\ncan only be discovered empirically, relying on the work of\nsociologists, anthropologists, archaeologists, and the like. Although\nan evolutionary game theoretic model may exclude certain historical\nsequences as possible histories (since one may be able to show that\nthe cultural evolutionary dynamics preclude one sequence from\ngenerating the phenomenon in question), it seems unlikely that an\nevolutionary game theoretic model would indicate a unique historical\nsequence suffices to bring about the phenomenon. An empirical inquiry\nwould then still need to be conducted to rule out the extraneous\nhistorical sequences admitted by the model, which raises the question\nof what, if anything, was gained by the construction of an\nevolutionary game theoretic model in the intermediate stage. Moreover,\neven if an evolutionary game theoretic model indicated that a single\nhistorical sequence was capable of producing a given social\nphenomenon, there remains the important question of why we ought to\ntake this result seriously. One may point out that since nearly any\nresult can be produced by a model by suitable adjusting of the\ndynamics and initial conditions, all that the evolutionary game\ntheorist has done is provide one such model. Additional work needs to\nbe done to show that the underlying assumptions of the model (both the\ncultural evolutionary dynamics and the initial conditions) are\nempirically supported. Again, one may wonder what has been gained by\nthe evolutionary model—would it not have been just as easy to\ndetermine the cultural dynamics and initial conditions beforehand,\nconstructing the model afterwards? If so, it would seem that the\ncontributions made by evolutionary game theory in this context simply\nare a proper part of the parent social science—sociology,\nanthropology, economics, and so on. If so, then there is nothing\nparticular about evolutionary game theory employed in the\nexplanation, and this means that, contrary to appearances,\nevolutionary game theory is really irrelevant to the given\nexplanation. \nIf evolutionary game theoretic models do not explain the etiology of a\nsocial phenomenon, presumably they explain the persistence of the\nphenomenon or the normativity attached to it. Yet we rarely need an\nevolutionary game theoretic model to identify a particular social\nphenomenon as stable or persistent as that can be done by observation\nof present conditions and examination of the historical records; hence\nthe charge of irrelevancy is raised again. Moreover, most of the\nevolutionary game theoretic models developed to date have provided the\ncrudest approximations of the real cultural dynamics driving the\nsocial phenomenon in question. One may well wonder why, in these\ncases, we should take seriously the stability analysis given by the\nmodel; answering this question would require one engage in an\nempirical study as previously discussed, ultimately leading to the\ncharge of irrelevance again. \nIt is sometimes argued that evolutionary game theoretic models answer\n“how possibly” questions. That is, an evolutionary game\ntheoretic model shows how some phenomenon could possibly be generated\nby an underlying dynamical process of interacting, boundedly rational\nagents. Although this is certainly the case, one might wonder whether\nthis subtly shifts the explanatory target. Answering a “how\npossibly” question is most interesting when we do not know\nwhether something is possible at all. The challenge faced by some\nevolutionary game theoretic accounts of social phenomena is that they\nanswer a “how possibly” question regarding something which\nwe already knew was possible, because the phenomenon actually exists.\nWhat we would like to know is how the answer to the “how\npossibly” question connects to the actual real-world processes\ngenerating the phenomenon. This suggests that evolutionary game\ntheoretic explanations of social phenomena are, even in the best\ncases, incomplete.  \nIf one seeks to use an evolutionary game theoretic model to explain\nthe normativity attached to a social rule, one must explain how such\nan approach avoids committing the so-called “naturalistic\nfallacy” of inferring an ought-statement from a conjunction of\n is-statements.[16]\n Assuming that the explanation does not commit such a fallacy, one\nargument charges that it must then be the case that the evolutionary\ngame theoretic explanation merely repackages certain key value claims\ntacitly assumed in the construction of the model. After all, since any\nargument whose conclusion is a normative statement must have at least\none normative statement in the premises, any evolutionary game\ntheoretic argument purporting to show how certain norms acquire\nnormative force must contain—at least implicitly—a\nnormative statement in the premises. Consequently, this application of\nevolutionary game theory does not provide a neutral analysis of the\nnorm in question, but merely acts as a vehicle for advancing\nparticular values, namely those smuggled in the premises. \nThis criticism seems less serious than the charge of irrelevancy.\nCultural evolutionary game theoretic explanations of norms need not\n“smuggle in” normative claims in order to draw normative\nconclusions. The theory already contains, in its core, a proper\nsubtheory having normative content—namely a theory of rational\nchoice in which boundedly rational agents act in order to maximize, as\nbest as they can, their own self-interest. One may challenge the\nsuitability of this as a foundation for the normative content of\ncertain claims, but this is a different criticism from the above\ncharge. Although cultural evolutionary game theoretic models do act as\nvehicles for promulgating certain values, they wear those minimal\nvalue commitments on their sleeve. Evolutionary explanations of social\nnorms have the virtue of making their value commitments explicit and\nalso of showing how other normative commitments (such as fair division\nin certain bargaining situations, or cooperation in the\nprisoner’s dilemma) may be derived from the principled action of\nboundedly rational, self-interested agents.","contact.mail":"jalex@lse.ac.uk","contact.domain":"lse.ac.uk"}]
