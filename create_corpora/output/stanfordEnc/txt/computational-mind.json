[{"date.published":"2015-10-16","date.changed":"2020-02-21","url":"https://plato.stanford.edu/entries/computational-mind/","author1":"Michael Rescorla","entry":"computational-mind","body.text":"\n\nCould a machine think? Could the mind itself be a thinking machine?\nThe computer revolution transformed discussion of these questions,\noffering our best prospects yet for machines that emulate reasoning,\ndecision-making, problem solving, perception, linguistic\ncomprehension, and other mental processes. Advances in\ncomputing raise the prospect that the mind itself is a computational\nsystem—a position known as the computational theory of\nmind (CTM). Computationalists are researchers who\nendorse CTM, at least as applied to certain important mental\nprocesses. CTM played a central role within cognitive science during\nthe 1960s and 1970s. For many years, it enjoyed orthodox status. More\nrecently, it has come under pressure from various rival paradigms. A\nkey task facing computationalists is to explain what one means when\none says that the mind “computes”. A second task is to\nargue that the mind “computes” in the relevant sense. A\nthird task is to elucidate how computational description relates to\nother common types of description, especially neurophysiological\ndescription (which cites neurophysiological properties of the\norganism’s brain or body) and intentional description\n(which cites representational properties of mental states).\n\nThe intuitive notions of computation\nand algorithm are central to mathematics. Roughly speaking,\nan algorithm is an explicit, step-by-step procedure for answering some\nquestion or solving some problem. An algorithm provides routine\nmechanical instructions dictating how to proceed at each\nstep. Obeying the instructions requires no special ingenuity or\ncreativity. For example, the familiar grade-school algorithms describe\nhow to compute addition, multiplication, and division. Until the early\ntwentieth century, mathematicians relied upon informal notions of\ncomputation and algorithm without attempting anything like a formal\nanalysis. Developments in the foundations of mathematics eventually\nimpelled logicians to pursue a more systematic treatment. Alan\nTuring’s landmark paper “On Computable Numbers, With\nan Application to the Entscheidungsproblem” (Turing 1936)\noffered the analysis that has proved most influential. A Turing machine is an abstract model of an idealized\ncomputing device with unlimited time and storage space at its\ndisposal. The device manipulates symbols, much as a human\ncomputing agent manipulates pencil marks on paper during arithmetical\ncomputation. Turing says very little about the nature of symbols. He\nassumes that primitive symbols are drawn from a finite alphabet. He\nalso assumes that symbols can be inscribed or erased at “memory\nlocations”. Turing’s model works as follows: Turing translates this informal description into a rigorous\nmathematical model. For more details, see the entry\non Turing machines. Turing motivates his approach by reflecting on idealized human\ncomputing agents. Citing finitary limits on our perceptual and\ncognitive apparatus, he argues that any symbolic algorithm executed by\na human can be replicated by a suitable Turing machine. He concludes\nthat the Turing machine formalism, despite its extreme simplicity, is\npowerful enough to capture all humanly executable mechanical\nprocedures over symbolic configurations. Subsequent discussants have\nalmost universally agreed. Turing computation is often described as digital rather\nthan analog. What this means is not always so clear, but the\nbasic idea is usually that computation operates over discrete\nconfigurations.  By comparison, many historically important algorithms\noperate over continuously variable configurations. For example,\nEuclidean geometry assigns a large role to ruler-and-compass\nconstructions, which manipulate geometric shapes. For any shape,\none can find another that differs to an arbitrarily small\nextent. Symbolic configurations manipulated by a Turing machine do not\ndiffer to arbitrarily small extent. Turing machines operate over\ndiscrete strings of elements (digits) drawn from a finite\nalphabet. One recurring controversy concerns whether the digital\nparadigm is well-suited to model mental activity or whether an analog\nparadigm would instead be more fitting (MacLennan 2012; Piccinini and\nBahar 2013). Besides introducing Turing machines, Turing (1936) proved\nseveral seminal mathematical results involving them. In particular, he\nproved the existence of a universal Turing machine (UTM).\nRoughly speaking, a UTM is a Turing machine that can mimic any other\nTuring machine. One provides the UTM with a symbolic input that codes\nthe machine table for Turing machine M. The UTM\nreplicates M’s behavior, executing instructions\nenshrined by M’s machine table. In that sense, the UTM\nis a programmable general purpose computer. To a first\napproximation, all personal computers are also general purpose: they\ncan mimic any Turing machine, when suitably programmed. The main\ncaveat is that physical computers have finite memory, whereas a Turing\nmachine has unlimited memory. More accurately, then, a personal\ncomputer can mimic any Turing machine until it exhausts its\nlimited memory supply. Turing’s discussion helped lay the foundations\nfor computer science, which seeks to design, build, and\nunderstand computing systems. As we know, computer scientists can now\nbuild extremely sophisticated computing machines. All these machines\nimplement something resembling Turing computation, although the\ndetails differ from Turing’s simplified model. Rapid progress in computer science prompted many, including Turing,\nto contemplate whether we could build a computer capable of\nthought.  Artificial Intelligence (AI) aims to construct\n“thinking machinery”. More precisely, it aims to construct\ncomputing machines that execute core mental tasks such as reasoning,\ndecision-making, problem solving, and so on. During the 1950s and\n1960s, this goal came to seem increasingly realistic (Haugeland\n1985). Early AI research emphasized logic. Researchers sought to\n“mechanize” deductive reasoning. A famous example was\nthe Logic Theorist computer program (Newell and Simon 1956),\nwhich proved 38 of the first 52 theorems from Principia\nMathematica (Whitehead and Russell 1925). In one case, it\ndiscovered a simpler proof than Principia’s. Early success of this kind stimulated enormous interest inside and\noutside the academy. Many researchers predicted that intelligent\nmachines were only a few years away. Obviously, these predictions have\nnot been fulfilled. Intelligent robots do not yet walk among us. Even\nrelatively low-level mental processes such as perception vastly exceed\nthe capacities of current computer programs. When confident\npredictions of thinking machines proved too optimistic, many observers\nlost interest or concluded that AI was a fool’s\nerrand. Nevertheless, the decades have witnessed gradual progress. One\nstriking success was IBM’s Deep Blue, which defeated chess\nchampion Gary Kasparov in 1997. Another major success was the\ndriverless car Stanley (Thrun, Montemerlo, Dahlkamp, et al. 2006),\nwhich completed a 132-mile course in the Mojave Desert, winning the\n2005 Defense Advanced Research Projects Agency (DARPA) Grand\nChallenge. A less flashy success story is the vast improvement in\nspeech recognition algorithms. One problem that dogged early work in AI is uncertainty.\nNearly all reasoning and decision-making operates under conditions of\nuncertainty. For example, you may need to decide whether to go on a\npicnic while being uncertain whether it will rain. Bayesian\ndecision theory is the standard mathematical model of\ninference and decision-making under uncertainty. Uncertainty is codified\nthrough probability. Precise rules dictate how to update\nprobabilities in light of new evidence and how to select actions in\nlight of probabilities and utilities. (See the\nentries Bayes’s theorem\nand normative theories of rational choice: expected utility\n for details.)  In the 1980s and\n1990s, technological and conceptual developments enabled efficient\ncomputer programs that implement or approximate Bayesian inference in\nrealistic scenarios. An explosion of Bayesian AI ensued (Thrun,\nBurgard, and Fox 2006), including the aforementioned advances in\nspeech recognition and driverless vehicles. Tractable algorithms that\nhandle uncertainty are a major achievement of contemporary AI (Murphy 2012), and\npossibly a harbinger of more impressive future progress. Some philosophers insist that computers, no matter how\nsophisticated they become, will at best mimic rather\nthan replicate thought. A computer simulation of the weather\ndoes not really rain. A computer simulation of flight does not really\nfly. Even if a computing system could simulate mental activity, why\nsuspect that it would constitute the genuine article? Turing (1950) anticipated these worries and tried to defuse\nthem. He proposed a scenario, now called the Turing Test,\nwhere one evaluates whether an unseen interlocutor is a computer or a\nhuman. A computer passes the Turing test if one cannot\ndetermine that it is a computer. Turing proposed that we abandon the\nquestion “Could a computer think?” as hopelessly vague,\nreplacing it with the question “Could a computer pass the Turing\ntest?”.  Turing’s discussion has received considerable\nattention, proving especially influential within AI. Ned Block (1981)\noffers an influential critique. He argues that certain possible\nmachines pass the Turing test even though these machines do not come\nclose to genuine thought or intelligence. See the\nentry the Turing test for discussion of\nBlock’s objection and other issues surrounding the Turing\nTest. For more on AI, see the entry\n logic and artificial intelligence.\n For much more detail, see Russell and\nNorvig (2010). Warren McCulloch and Walter Pitts (1943) first suggested that\nsomething resembling the Turing machine might provide a good model for\nthe mind.  In the 1960s, Turing computation became central to the\nemerging interdisciplinary initiative cognitive science,\nwhich studies the mind by drawing upon psychology, computer science\n(especially AI), linguistics, philosophy, economics (especially game\ntheory and behavioral economics), anthropology, and neuroscience. The\nlabel classical computational theory of mind (which we will\nabbreviate as CCTM) is now fairly standard. According to CCTM, the\nmind is a computational system similar in important respects to a\nTuring machine, and core mental processes (e.g., reasoning,\ndecision-making, and problem solving) are computations similar in\nimportant respects to computations executed by a Turing machine. These\nformulations are imprecise. CCTM is best seen as a family of views,\nrather than a single well-defined\nview.[1] It is common to describe CCTM as embodying “the computer\nmetaphor”. This description is doubly misleading. First, CCTM is better formulated by describing the mind as a\n“computing system” or a “computational system”\nrather than a “computer”. As David Chalmers (2011) notes,\ndescribing a system as a “computer” strongly suggests that\nthe system is programmable. As Chalmers also notes, one need\nnot claim that the mind is programmable simply because one regards it\nas a Turing-style computational system. (Most Turing machines are not\nprogrammable.) Thus, the phrase “computer metaphor”\nstrongly suggests theoretical commitments that are inessential to\nCCTM.  The point here is not just terminological. Critics of CCTM\noften object that the mind is not a programmable general purpose\ncomputer (Churchland, Koch, and Sejnowski 1990). Since classical\ncomputationalists need not claim (and usually do not claim) that the\nmind is a programmable general purpose computer, the objection is\nmisdirected. Second, CCTM is not intended metaphorically. CCTM does not simply\nhold that the mind is like a computing system. CCTM holds\nthat the mind literally is a computing system. Of course, the\nmost familiar artificial computing systems are made from silicon chips\nor similar materials, whereas the human body is made from flesh and\nblood. But CCTM holds that this difference disguises a more\nfundamental similarity, which we can capture through a Turing-style\ncomputational model. In offering such a model, we prescind from\nphysical details. We attain an abstract computational description that\ncould be physically implemented in diverse ways (e.g., through silicon\nchips, or neurons, or pulleys and levers). CCTM holds that a suitable\nabstract computational model offers a literally true description of\ncore mental processes. It is common to summarize CCTM through the slogan “the mind\nis a Turing machine”. This slogan is also somewhat misleading,\nbecause no one regards Turing’s precise formalism as a plausible\nmodel of mental activity. The formalism seems too restrictive in\nseveral ways: CCTM claims that mental activity is “Turing-style\ncomputation”, allowing these and other departures from\nTuring’s own formalism.  Hilary Putnam (1967) introduced CCTM into philosophy. He\ncontrasted his position with logical behaviorism\nand type-identity theory. Each position purports to\nreveal the nature of mental states, including propositional attitudes\n(e.g., beliefs), sensations (e.g., pains), and emotions (e.g.,\nfear). According to logical behaviorism, mental states are behavioral\ndispositions.  According to type-identity theory, mental states are\nbrain states.  Putnam advances an opposing functionalist\nview, on which mental states are functional states. According to\nfunctionalism, a system has a mind when the system has a \nsuitable functional organization. Mental states are states\nthat play appropriate roles in the system’s functional\norganization. Each mental state is individuated by its interactions\nwith sensory input, motor output, and other mental states. Functionalism offers notable advantages over logical behaviorism\nand type-identity theory: Putnam defends a brand of functionalism now called machine\nfunctionalism. He emphasizes probabilistic automata,\nwhich are similar to Turing machines except that transitions between\ncomputational states are stochastic. He proposes that mental activity\nimplements a probabilistic automaton and that particular mental states\nare machine states of the automaton’s central processor. The\nmachine table specifies an appropriate functional organization, and it\nalso specifies the role that individual mental states play within that\nfunctional organization. In this way, Putnam combines functionalism\nwith CCTM. Machine functionalism faces several problems. One problem,\nhighlighted by Ned Block and Jerry Fodor (1972), concerns\nthe productivity of thought. A normal human can entertain a\npotential infinity of propositions. Machine functionalism identifies\nmental states with machine states of a probabilistic automaton. Since\nthere are only finitely many machine states, there are not enough\nmachine states to pair one-one with possible mental states of a normal\nhuman. Of course, an actual human will only ever entertain finitely\nmany propositions. However, Block and Fodor contend that this\nlimitation reflects limits on lifespan and memory, rather than (say)\nsome psychological law that restricts the class of humanly\nentertainable propositions. A probabilistic automaton is endowed with\nunlimited time and memory capacity yet even still has only finitely\nmany machine states. Apparently, then, machine functionalism\nmislocates the finitary limits upon human cognition. Another problem for machine functionalism, also highlighted by\nBlock and Fodor (1972), concerns the systematicity of\nthought. An ability to entertain one proposition is correlated\nwith an ability to think other propositions. For example, someone who\ncan entertain the thought that John loves Mary can also\nentertain the thought that Mary loves John. Thus, there seem\nto be systematic relations between mental states. A good theory should\nreflect those systematic relations. Yet machine functionalism\nidentifies mental states with unstructured machines states, which lack\nthe requisite systematic relations to another. For that reason,\nmachine functionalism does not explain systematicity. In response to\nthis objection, machine functionalists might deny that they are\nobligated to explain systematicity. Nevertheless, the objection\nsuggests that machine functionalism neglects essential features of\nhuman mentality. A better theory would explain those features in a\nprincipled way. While the productivity and systematicity objections to machine\nfunctionalism are perhaps not decisive, they provide strong impetus to\npursue an improved version of CCTM. See Block (1978) for additional\nproblems facing machine functionalism and functionalism more\ngenerally. Fodor (1975, 1981, 1987, 1990, 1994, 2008) advocates a version of\nCCTM that accommodates systematicity and productivity much more\nsatisfactorily. He shifts attention to the symbols\nmanipulated during Turing-style computation. An old view, stretching back at least to William of\nOckham’s Summa Logicae, holds that thinking occurs in\na language of thought (sometimes\ncalled Mentalese). Fodor revives this view. He postulates a\nsystem of mental representations, including both primitive\nrepresentations and complex representations formed from primitive\nrepresentations. For example, the primitive Mentalese words JOHN,\nMARY, and LOVES can combine to form the Mentalese sentence JOHN LOVES\nMARY. Mentalese is compositional: the meaning of a complex\nMentalese expression is a function of the meanings of its parts and\nthe way those parts are combined. Propositional attitudes are\nrelations to Mentalese symbols. Fodor calls this view the\nrepresentational theory of mind (RTM). Combining RTM\nwith CCTM, he argues that mental activity involves Turing-style\ncomputation over the language of thought. Mental computation stores\nMentalese symbols in memory locations, manipulating those symbols in\naccord with mechanical rules. A prime virtue of RTM is how readily it accommodates productivity\nand systematicity: Productivity: RTM postulates a finite set of primitive\nMentalese expressions, combinable into a potential infinity of complex\nMentalese expressions. A thinker with access to primitive Mentalese\nvocabulary and Mentalese compounding devices has the potential to\nentertain an infinity of Mentalese expressions. She therefore has the\npotential to instantiate infinitely many propositional attitudes\n(neglecting limits on time and memory). Systematicity: According to RTM, there are systematic\nrelations between which propositional attitudes a thinker can\nentertain. For example, suppose I can think that John loves Mary.\nAccording to RTM, my doing so involves my standing in some\nrelation R to a Mentalese sentence JOHN LOVES MARY, composed\nof Mentalese words JOHN, LOVES, and MARY combined in the right way. If\nI have this capacity, then I also have the capacity to stand in\nrelation R to the distinct Mentalese sentence MARY LOVES\nJOHN, thereby thinking that Mary loves John. So the capacity to think\nthat John loves Mary is systematically related to the capacity to\nthink that Mary loves John. By treating propositional attitudes as relations to complex mental\nsymbols, RTM explains both productivity and systematicity. CCTM+RTM differs from machine functionalism in several other\nrespects.  First, machine functionalism is a theory of mental\nstates in general, while RTM is only a theory of\npropositional attitudes.  Second, proponents of CCTM+RTM need not say\nthat propositional attitudes are individuated functionally. As Fodor\n(2000: 105, fn. 4) notes, we must\ndistinguish computationalism (mental processes are\ncomputational) from functionalism (mental states are\nfunctional states). Machine functionalism endorses both doctrines.\nCCTM+RTM endorses only the first. Unfortunately, many philosophers\nstill mistakenly assume that computationalism entails a functionalist\napproach to propositional attitudes (see Piccinini 2004 for\ndiscussion). Philosophical discussion of RTM tends to focus mainly\non high-level human thought, especially belief and\ndesire. However, CCTM+RTM is applicable to a much wider range of\nmental states and processes. Many cognitive scientists apply it to\nnon-human animals. For example, Gallistel and King (2009) apply it to\ncertain invertebrate phenomena (e.g., honeybee navigation). Even\nconfining attention to humans, one can apply CCTM+RTM\nto subpersonal processing. Fodor (1983) argues that\nperception involves a subpersonal “module” that converts\nretinal input into Mentalese symbols and then performs computations\nover those symbols. Thus, talk about a language of thought is\npotentially misleading, since it suggests a non-existent restriction\nto higher-level mental activity. Also potentially misleading is the description of Mentalese as\na language, which suggests that all Mentalese symbols\nresemble expressions in a natural language. Many philosophers,\nincluding Fodor, sometimes seem to endorse that position. However,\nthere are possible non-propositional formats for Mentalese\nsymbols. Proponents of CCTM+RTM can adopt a pluralistic line, allowing\nmental computation to operate over items akin to images, maps,\ndiagrams, or other non-propositional representations (Johnson-Laird\n2004: 187; McDermott 2001: 69; Pinker 2005: 7; Sloman 1978:\n144–176). The pluralistic line seems especially plausible as\napplied to subpersonal processes (such as perception) and non-human\nanimals. Michael Rescorla (2009a,b) surveys research on cognitive\nmaps (Tolman 1948; O’Keefe and Nadel 1978; Gallistel 1990),\nsuggesting that some animals may navigate by computing over mental\nrepresentations more similar to maps than sentences. Elisabeth Camp\n(2009), citing research\non baboon social interaction (Cheney and Seyfarth\n2007), argues that\nbaboons may encode social dominance relations through non-sentential\ntree-structured representations. CCTM+RTM is schematic. To fill in the schema, one must provide\ndetailed computational models of specific mental processes. A complete\nmodel will: By providing a detailed computational model, we decompose a complex\nmental process into a series of elementary operations governed by\nprecise, routine instructions. CCTM+RTM remains neutral in the traditional debate between\nphysicalism and substance dualism. A Turing-style model proceeds at a\nvery abstract level, not saying whether mental computations are\nimplemented by physical stuff or Cartesian soul-stuff (Block 1983:\n522). In practice, all\nproponents of CCTM+RTM embrace a broadly physicalist outlook. They\nhold that mental computations are implemented not by soul-stuff but\nrather by the brain. On this view, Mentalese symbols are realized by\nneural states, and computational operations over Mentalese symbols are\nrealized by neural processes. Ultimately, physicalist proponents of\nCCTM+RTM must produce empirically well-confirmed theories that explain\nhow exactly neural activity implements Turing-style computation. As\nGallistel and King (2009) emphasize, we do not currently have such\ntheories—though see Zylberberg, Dehaene, Roelfsema, and Sigman\n(2011) for some speculations. Fodor (1975) advances CCTM+RTM as a foundation for cognitive\nscience.  He discusses mental phenomena such as decision-making,\nperception, and linguistic processing. In each case, he maintains, our\nbest scientific theories postulate Turing-style computation over\nmental representations. In fact, he argues that our only\nviable theories have this form. He concludes that CCTM+RTM is\n“the only game in town”. Many cognitive scientists argue\nalong similar lines. C.R. Gallistel and Adam King (2009), Philip\nJohnson-Laird (1988), Allen Newell and Herbert Simon (1976), and Zenon\nPylyshyn (1984) all recommend Turing-style computation over mental\nsymbols as the best foundation for scientific theorizing about the\nmind.  In the 1980s, connectionism emerged as a prominent rival to\nclassical computationalism. Connectionists draw inspiration from\nneurophysiology rather than logic and computer science. They employ\ncomputational models, neural networks, that differ\nsignificantly from Turing-style models. A neural network is a\ncollection of interconnected nodes. Nodes fall into three\ncategories: input nodes, output nodes,\nand hidden nodes (which mediate between input and output\nnodes). Nodes have activation values, given by real numbers. One node\ncan bear a weighted connection to another node, also given by\na real number. Activations of input nodes are determined exogenously:\nthese are the inputs to computation.  Total input activation\nof a hidden or output node is a weighted sum of the activations of\nnodes feeding into it. Activation of a hidden or output node is a\nfunction of its total input activation; the particular function varies\nwith the network. During neural network computation, waves of\nactivation propagate from input nodes to output nodes, as determined\nby weighted connections between nodes. In a feedforward network, weighted connections flow only\nin one direction. Recurrent networks have feedback loops, in\nwhich connections emanating from hidden units circle back to hidden\nunits. Recurrent networks are less mathematically tractable than\nfeedforward networks. However, they figure crucially in psychological\nmodeling of various phenomena, such as phenomena that involve some\nkind of memory (Elman 1990). Weights in a neural network are typically mutable, evolving in\naccord with a learning algorithm. The literature offers\nvarious learning algorithms, but the basic idea is usually to adjust\nweights so that actual outputs gradually move closer to\nthe target outputs one would expect for the relevant\ninputs. The backpropagation algorithm is a widely used\nalgorithm of this kind (Rumelhart, Hinton, and Williams 1986). Connectionism traces back to McCulloch and Pitts (1943), who\nstudied networks of interconnected logic gates (e.g.,\nAND-gates and OR-gates). One can view a network of logic gates as a\nneural network, with activations confined to two values (0 and 1) and\nactivation functions given by the usual truth-functions. McCulloch and\nPitts advanced logic gates as idealized models of individual\nneurons. Their discussion exerted a profound influence on computer\nscience (von Neumann 1945). Modern digital computers are simply\nnetworks of logic gates. Within cognitive science, however,\nresearchers usually focus upon networks whose elements are more\n“neuron-like” than logic gates. In particular, modern-day\nconnectionists typically emphasize analog neural networks whose nodes\ntake continuous rather than discrete activation values. Some authors\neven use the phrase “neural network” so that it\nexclusively denotes such networks. Neural networks received relatively scant attention from cognitive\nscientists during the 1960s and 1970s, when Turing-style models\ndominated. The 1980s witnessed a huge resurgence of interest in neural\nnetworks, especially analog neural networks, with the\ntwo-volume Parallel Distributed Processing (Rumelhart, McClelland, and the PDP research group, 1986; McClelland, Rumelhart, and the PDP research group, 1987) serving as a\nmanifesto.  Researchers constructed connectionist models of diverse\nphenomena: object recognition, speech perception, sentence\ncomprehension, cognitive development, and so on. Impressed by\nconnectionism, many researchers concluded that CCTM+RTM was no longer\n“the only game in town”. \nIn the 2010s, a class of computational models known as deep neural\nnetworks became quite popular (Krizhevsky, Sutskever, and Hinton\n2012; LeCun, Bengio, and Hinton 2015). These models are neural\nnetworks with multiple layers of hidden nodes (sometimes hundreds of\nsuch layers). Deep neural networks—trained on large data sets\nthrough one or another learning algorithm (usually\nbackpropagation)—have achieved great success in many areas of\nAI, including object recognition and strategic game-playing. Deep\nneural networks are now widely deployed in commercial applications,\nand they are the focus of extensive ongoing investigation within both\nacademia and industry. Researchers have also begun using them to model\nthe mind (e.g. Marblestone, Wayne, and Kording 2016; Kriegeskorte\n2015). \nFor a detailed overview of neural networks, see Haykin (2008). For a\nuser-friendly introduction, with an emphasis on psychological\napplications, see Marcus (2001). For a philosophically oriented\nintroduction to deep neural networks, see Buckner (2019). Neural networks have a very different “feel” than\nclassical (i.e., Turing-style) models. Yet classical computation and\nneural network computation are not mutually exclusive: Although some researchers suggest a fundamental opposition between\nclassical computation and neural network computation, it seems more\naccurate to identify two modeling traditions that overlap in certain\ncases but not others (cf. Boden 1991; Piccinini 2008b). In this\nconnection, it is also worth noting that classical computationalism\nand connectionist computationalism have their common origin in the\nwork of McCulloch and Pitts. Philosophers often say that classical computation involves\n“rule-governed symbol manipulation” while neural network\ncomputation is non-symbolic. The intuitive picture is that\n“information” in neural networks is globally distributed\nacross the weights and activations, rather than concentrated in\nlocalized symbols. However, the notion of “symbol” itself\nrequires explication, so it is often unclear what theorists mean by\ndescribing computation as symbolic versus non-symbolic. As mentioned\nin §1, the Turing formalism places very few\nconditions on “symbols”. Regarding primitive symbols,\nTuring assumes just that there are finitely many of them and that they\ncan be inscribed in read/write memory locations. Neural networks can\nalso manipulate symbols satisfying these two conditions: as just\nnoted, one can implement a Turing-style model in a neural network. Many discussions of the symbolic/non-symbolic dichotomy employ a\nmore robust notion of “symbol”. On the more robust\napproach, a symbol is the sort of thing that represents a subject\nmatter. Thus, something is a symbol only if it has semantic or\nrepresentational properties. If we employ this more robust notion of\nsymbol, then the symbolic/non-symbolic distinction cross-cuts the\ndistinction between Turing-style computation and neural network\ncomputation. A Turing machine need not employ symbols in the more\nrobust sense. As far as the Turing formalism goes, symbols manipulated\nduring Turing computation need not have representational properties\n(Chalmers 2011). Conversely, a neural network can manipulate symbols\nwith representational properties. Indeed, an analog neural network can\nmanipulate symbols that have a combinatorial syntax and semantics\n(Horgan and Tienson 1996; Marcus 2001). Following Steven Pinker and Alan Prince (1988), we may distinguish\nbetween eliminative connectionism and implementationist\nconnectionism. Eliminative connectionists advance connectionism as a rival to\nclassical computationalism. They argue that the Turing formalism is\nirrelevant to psychological explanation. Often, though not always,\nthey seek to revive the associationist tradition in\npsychology, a tradition that CCTM had forcefully challenged. Often,\nthough not always, they attack the mentalist, nativist linguistics\npioneered by Noam Chomsky (1965). Often, though not always, they\nmanifest overt hostility to the very notion of mental\nrepresentation. But the defining feature of eliminative connectionism\nis that it uses neural networks as replacements for\nTuring-style models. Eliminative connectionists view the mind as a\ncomputing system of a radically different kind than the Turing\nmachine. A few authors explicitly espouse eliminative connectionism\n(Churchland 1989; Rumelhart and McClelland\n1986; Horgan and Tienson\n1996), and many others incline towards it. Implementationist connectionism is a more ecumenical position. It\nallows a potentially valuable role for both Turing-style\nmodels and neural networks, operating harmoniously at\ndifferent levels of description (Marcus 2001; Smolensky 1988). A\nTuring-style model is higher-level, whereas a neural network model is\nlower-level.  The neural network illuminates how the brain implements\nthe Turing-style model, just as a description in terms of logic gates\nilluminates how a personal computer executes a program in a high-level\nprogramming language. Connectionism excites many researchers because of the analogy\nbetween neural networks and the brain. Nodes resemble neurons, while\nconnections between nodes resemble synapses. Connectionist modeling\ntherefore seems more “biologically plausible” than\nclassical modeling. A connectionist model of a psychological\nphenomenon apparently captures (in an idealized way) how\ninterconnected neurons might generate the phenomenon. \nWhen evaluating the argument from biological plausibility, one should\nrecognize that neural networks vary widely in how closely they match\nactual brain activity. Many networks that figure prominently in\nconnectionist writings are not so biologically plausible (Bechtel and\nAbrahamsen 2002: 341–343; Bermúdez 2010: 237–239;\nClark 2014: 87–89; Harnish 2002: 359–362). A few\nexamples: \nOn the other hand, some neural networks are more biologically\nrealistic (Buckner and Garson 2019; Illing, Gerstner, and Brea\n2019). For instance, there are neural networks that replace\nbackpropagation with more realistic learning algorithms, such as a\nreinforcement learning algorithm (Pozzi, Bohté, and Roelfsema\n2019, Other Internet Resources) or an unsupervised learning algorithm\n(Krotov and Hopfield 2019). There are also neural networks whose nodes\noutput discrete spikes roughly akin to those emitted by real neurons\nin the brain (Maass 1996; Buesing, Bill, Nessler, and Maass 2011). Even when a neural network is not biologically plausible, it may\nstill be more biologically plausible than classical\nmodels. Neural networks certainly seem closer than Turing-style\nmodels, in both details and spirit, to neurophysiological\ndescription. Many cognitive scientists worry that CCTM reflects a\nmisguided attempt at imposing the architecture of digital computers\nonto the brain. Some doubt that the brain implements anything\nresembling digital computation, i.e., computation over discrete\nconfigurations of digits (Piccinini and Bahar 2013). Others doubt that\nbrains display clean Turing-style separation between central processor\nand read/write memory (Dayan 2009). Neural networks fare better on\nboth scores: they do not require computation over discrete\nconfigurations of digits, and they do not postulate a clean separation\nbetween central processor and read/write memory. Classical computationalists typically reply that it is premature to\ndraw firm conclusions based upon biological plausibility, given how\nlittle we understand about the relation between neural, computational,\nand cognitive levels of description (Gallistel and King 2009; Marcus\n2001). Using measurement techniques such as cell recordings and\nfunctional magnetic resonance imaging (fMRI), and drawing upon\ndisciplines as diverse as physics, biology, AI, information theory,\nstatistics, graph theory, and dynamical systems theory,\nneuroscientists have accumulated substantial knowledge about the brain\nat varying levels of granularity (Zednik 2019). We now know quite a\nlot about individual neurons, about how neurons interact within neural\npopulations, about the localization of mental activity in cortical\nregions (e.g. the visual cortex), and about interactions among\ncortical regions. Yet we still have a tremendous amount to learn about\nhow neural tissue accomplishes the tasks that it surely accomplishes:\nperception, reasoning, decision-making, language acquisition, and so\non. Given our present state of relative ignorance, it would be rash to\ninsist that the brain does not implement anything resembling Turing\ncomputation. Connectionists offer numerous further arguments that we should\nemploy connectionist models instead of, or in addition to, classical\nmodels.  See the entry connectionism\nfor an overview. For purposes of this entry, we mention two additional\narguments. The first argument emphasizes learning (Bechtel and\nAbrahamsen 2002: 51). A vast range of cognitive phenomena involve\nlearning from experience. Many connectionist models are explicitly\ndesigned to model learning, through backpropagation or some other\nalgorithm that modifies the weights between nodes. By contrast,\nconnectionists often complain that there are no good classical models\nof learning. Classical computationalists can respond by citing\nperceived defects of connectionist learning algorithms (e.g., the\nheavy reliance of backpropagation upon supervised training). Classical\ncomputationalists can also cite Bayesian decision theory, which models\nlearning as probabilistic updating. More specifically, classical\ncomputationalists can cite the achievements of Bayesian cognitive\nscience, which uses Bayesian decision theory to construct\nmathematical models of mental activity (Ma 2019). Over the past few\ndecades, Bayesian cognitive science has accrued many explanatory\nsuccesses. This impressive track record suggests that some mental\nprocesses are Bayesian or approximately Bayesian (Rescorla\n2020). Moreover, the advances mentioned\nin §2 show how classical computing systems\ncan execute or at least approximately execute Bayesian updating in\nvarious realistic scenarios. These developments provide hope that\nclassical computation can model many important cases of learning. The second argument emphasizes speed of\ncomputation. Neurons are much slower than silicon-based\ncomponents of digital computers. For this reason, neurons could not\nexecute serial computation quickly enough to match rapid human\nperformance in perception, linguistic comprehension, decision-making,\netc. Connectionists maintain that the only viable solution is to\nreplace serial computation with a “massively parallel”\ncomputational architecture—precisely what neural networks\nprovide (Feldman and Ballard 1982; Rumelhart 1989). However, this\nargument is only effective against classical computationalists who\ninsist upon serial processing. As noted\nin §3, some Turing-style models\ninvolve parallel processing. Many classical computationalists are\nhappy to allow “massively parallel” mental computation,\nand the argument gains no traction against these researchers. That\nbeing said, the argument highlights an important question that any\ncomputationalist—whether classical, connectionist, or\notherwise—must address: How does a brain built from relatively\nslow neurons execute sophisticated computations so quickly? Neither\nclassical nor connectionist computationalists have answered this\nquestion satisfactorily (Gallistel and King 2009: 174 and 265). Fodor and Pylyshyn (1988) offer a widely discussed critique of\neliminativist connectionism. They argue that systematicity and\nproductivity fail in connectionist models, except when the\nconnectionist model implements a classical model. Hence, connectionism\ndoes not furnish a viable alternative to CCTM. At best, it supplies a\nlow-level description that helps bridge the gap between Turing-style\ncomputation and neuroscientific description. This argument has elicited numerous replies and\ncounter-replies. Some argue that neural networks can exhibit\nsystematicity without implementing anything like classical\ncomputational architecture (Horgan and Tienson 1996; Chalmers 1990;\nSmolensky 1991; van Gelder 1990). Some argue that Fodor and Pylyshyn\nvastly exaggerate systematicity (Johnson 2004) or productivity\n(Rumelhart and McClelland 1986), especially for non-human animals\n(Dennett 1991).  These issues, and many others raised by Fodor and\nPylyshyn’s argument, have been thoroughly investigated in the\nliterature.  For further discussion, see Bechtel and Abrahamsen (2002:\n156–199), Bermúdez (2005: 244–278), Chalmers\n(1993), Clark (2014: 84–86), and the encyclopedia entries on\n the language of thought hypothesis\n and on \n connectionism. Gallistel and King (2009) advance a related but distinct\nproductivity argument. They emphasize productivity of mental\ncomputation, as opposed to productivity of mental\nstates. Through detailed empirical case studies, they argue that\nmany non-human animals can extract, store, and retrieve detailed\nrecords of the surrounding environment. For example, the Western scrub\njay records where it cached food, what kind of food it cached in each\nlocation, when it cached the food, and whether it has depleted a given\ncache (Clayton, Emery, and Dickinson 2006). The jay can access these\nrecords and exploit them in diverse computations: computing whether a\nfood item stored in some cache is likely to have decayed; computing a\nroute from one location to another; and so on. The number of possible\ncomputations a jay can execute is, for all practical purposes,\ninfinite. CCTM explains the productivity of mental computation by positing a\ncentral processor that stores and retrieves symbols in addressable\nread/write memory. When needed, the central processor can retrieve\narbitrary, unpredicted combinations of symbols from memory. In\ncontrast, Gallistel and King argue, connectionism has difficulty\naccommodating the productivity of mental computation. Although\nGallistel and King do not carefully distinguish between eliminativist\nand implementationist connectionism, we may summarize their argument\nas follows: Gallistel and King conclude that CCTM is much better suited than\neither eliminativist or implementationist connectionism to explain a\nvast range of cognitive phenomena. Critics attack this new productivity argument from various angles,\nfocusing mainly on the empirical case studies adduced by Gallistel and\nKing. Peter Dayan (2009), John Donahoe (2010), and Christopher Mole\n(2014) argue that biologically plausible neural network models can\naccommodate at least some of the case studies. Dayan and Donahoe argue\nthat empirically adequate neural network models can dispense with\nanything resembling read/write memory. Mole argues that, in certain\ncases, empirically adequate neural network models\ncan implement the read/write memory mechanisms posited by\nGallistel and King. Debate on these fundamental issues seems poised to\ncontinue well into the future.  Computational neuroscience describes the nervous system\nthrough computational models. Although this research program is\ngrounded in mathematical modeling of individual neurons, the\ndistinctive focus of computational neuroscience is systems of\ninterconnected neurons. Computational neuroscience usually models\nthese systems as neural networks. In that sense, it is a variant,\noff-shoot, or descendant of connectionism. However, most computational\nneuroscientists do not self-identify as connectionists. There are\nseveral differences between connectionism and computational\nneuroscience: One might say that computational neuroscience is concerned mainly\nwith neural computation (computation by systems of neurons),\nwhereas connectionism is concerned mainly with abstract computational\nmodels inspired by neural computation. But the boundaries\nbetween connectionism and computational neuroscience are admittedly\nsomewhat porous. For an overview of computational neuroscience, see\nTrappenberg (2010) or Miller (2018). Serious philosophical engagement with neuroscience dates back at\nleast to Patricia Churchland’s Neurophilosophy\n(1986). As computational neuroscience matured, Churchland became one\nof its main philosophical champions (Churchland, Koch, and Sejnowski\n1990; Churchland and Sejnowski 1992). She was joined by Paul\nChurchland (1995, 2007) and others (Eliasmith 2013; Eliasmith and\nAnderson 2003; Piccinini and Bahar 2013; Piccinini and Shagrir\n2014). All these authors hold that theorizing about mental computation\nshould begin with the brain, not with Turing machines or other\ninappropriate tools drawn from logic and computer science. They also\nhold that neural network modeling should strive for greater biological\nrealism than connectionist models typically attain. Chris Eliasmith\n(2013) develops this neurocomputational viewpoint through\nthe Neural Engineering Framework, which supplements\ncomputational neuroscience with tools drawn from control theory\n(Brogan 1990). He aims to “reverse engineer” the brain,\nbuilding large-scale, biologically plausible neural network models of\ncognitive phenomena. Computational neuroscience differs in a crucial respect from CCTM\nand connectionism: it abandons multiply realizability. Computational\nneuroscientists cite specific neurophysiological properties and\nprocesses, so their models do not apply equally well to (say) a\nsufficiently different silicon-based creature. Thus, computational\nneuroscience sacrifices a key feature that originally attracted\nphilosophers to CTM. Computational neuroscientists will respond that\nthis sacrifice is worth the resultant insight into neurophysiological\nunderpinnings. But many computationalists worry that, by focusing too\nmuch on neural underpinnings, we risk losing sight of the cognitive\nforest for the neuronal trees. Neurophysiological details are\nimportant, but don’t we also need an additional abstract level\nof computational description that prescinds from such details?\nGallistel and King (2009) argue that a myopic fixation upon what we\ncurrently know about the brain has led computational neuroscience to\nshortchange core cognitive phenomena such as navigation, spatial and\ntemporal learning, and so on. Similarly, Edelman (2014) complains that\nthe Neural Engineering Framework substitutes a blizzard of\nneurophysiological details for satisfying psychological\nexplanations. \nPartly in response to such worries, some researchers propose an\nintegrated cognitive computational neuroscience that connects\npsychological theories with neural implementation mechanisms\n(Naselaris et al. 2018; Kriegeskorte and Douglas 2018). The basic idea\nis to use neural network models to illuminate how mental processes are\ninstantiated in the brain, thereby grounding multiply realizable\ncognitive description in the neurophysiological. A good example is\nrecent work on neural implementation of Bayesian inference (e.g.,\nPouget et al. 2013; Orhan and Ma 2017; Aitchison and Lengyel\n2016). Researchers articulate (multiply realizable) Bayesian models of\nvarious mental processes; they construct biologically plausible neural\nnetworks that execute or approximately execute the posited Bayesian\ncomputations; and they evaluate how well these neural network models\nfit with neurophysiological data. Despite the differences between connectionism and computational\nneuroscience, these two movements raise many similar issues. In\nparticular, the dialectic from §4.4\nregarding systematicity and productivity arises in similar form. Philosophers and cognitive scientists use the term\n“representation” in diverse ways. Within philosophy, the\nmost dominant usage ties representation to intentionality, i.e., the\n“aboutness” of mental states. Contemporary philosophers\nusually elucidate intentionality by invoking representational\ncontent. A representational mental state has a content that\nrepresents the world as being a certain way, so we can ask whether the\nworld is indeed that way. Thus, representationally contentful mental\nstates are semantically evaluable with respect to properties\nsuch as truth, accuracy, fulfillment, and so on. To illustrate: Beliefs have truth-conditions (conditions under which they are\ntrue), perceptual states have accuracy-conditions (conditions under\nwhich they are accurate), and desires have fulfillment-conditions\n(conditions under which they are fulfilled). In ordinary life, we frequently predict and explain behavior by\ninvoking beliefs, desires, and other representationally contentful\nmental states. We identify these states through their representational\nproperties. When we say “Frank believes that Emmanuel Macron is\nFrench”, we specify the condition under which Frank’s\nbelief is true (namely, that Emmanuel Macron is French). When we say\n“Frank wants to eat chocolate”, we specify the condition\nunder which Frank’s desire is fulfilled (namely, that Frank eats\nchocolate). So folk psychology assigns a central role\nto intentional descriptions, i.e., descriptions that identify\nmental states through their representational properties. Whether\nscientific psychology should likewise employ intentional descriptions\nis a contested issue within contemporary philosophy of mind. Intentional realism is realism regarding\nrepresentation. At a minimum, this position holds that\nrepresentational properties are genuine aspects of mentality. Usually,\nit is also taken to hold that scientific psychology should freely\nemploy intentional descriptions when appropriate. Intentional realism\nis a popular position, advocated by Tyler Burge (2010a), Jerry Fodor\n(1987), Christopher Peacocke (1992, 1994), and many others. One\nprominent argument for intentional realism cites cognitive science\npractice. The argument maintains that intentional description\nfigures centrally in many core areas of cognitive science, such as\nperceptual psychology and linguistics. For example,\nperceptual psychology describes how perceptual activity transforms\nsensory inputs (e.g., retinal stimulations) into representations of\nthe distal environment (e.g., perceptual representations of distal\nshapes, sizes, and colors). The science identifies perceptual states\nby citing representational properties (e.g., representational\nrelations to specific distal shapes, sizes, colors). Assuming a\nbroadly scientific realist perspective, the explanatory achievements\nof perceptual psychology support a realist posture towards\nintentionality. Eliminativism is a strong form of anti-realism about\nintentionality. Eliminativists dismiss intentional description as\nvague, context-sensitive, interest-relative, explanatorily\nsuperficial, or otherwise problematic. They recommend that scientific\npsychology jettison representational content. An early example is W.V.\nQuine’s Word and Object (1960), which seeks to replace\nintentional psychology with behaviorist stimulus-response psychology.\nPaul Churchland (1981), another prominent eliminativist, wants to\nreplace intentional psychology with neuroscience. Between intentional realism and eliminativism lie various\nintermediate positions. Daniel Dennett (1971, 1987) acknowledges that\nintentional discourse is predictively useful, but he questions whether\nmental states really have representational properties.\nAccording to Dennett, theorists who employ intentional descriptions\nare not literally asserting that mental states have\nrepresentational properties. They are merely adopting the\n“intentional stance”. Donald Davidson (1980) espouses a\nneighboring interpretivist position. He emphasizes the\ncentral role that intentional ascription plays within ordinary\ninterpretive practice, i.e., our practice of interpreting one\nanother’s mental states and speech acts. At the same time, he\nquestions whether intentional psychology will find a place within\nmature scientific theorizing. Davidson and Dennett both profess\nrealism about intentional mental states. Nevertheless, both\nphilosophers are customarily read as intentional anti-realists. (In\nparticular, Dennett is frequently read as a kind\nof instrumentalist about intentionality.) One source of this\ncustomary reading involves indeterminacy of\ninterpretation. Suppose that behavioral evidence allows two\nconflicting interpretations of a thinker’s mental states.\nFollowing Quine, Davidson and Dennett both say there is then “no\nfact of the matter” regarding which interpretation is correct.\nThis diagnosis indicates a less than fully realist attitude towards\nintentionality. Debates over intentionality figure prominently in philosophical\ndiscussion of CTM. Let us survey some highlights. Classical computationalists typically assume what one might\ncall the formal-syntactic conception of computation\n(FSC). The intuitive idea is that computation manipulates symbols in\nvirtue of their formal syntactic properties rather than their semantic\nproperties. FSC stems from innovations in mathematical logic during the late\n19th and early 20th centuries, especially\nseminal contributions by George Boole and Gottlob Frege. In\nhis Begriffsschrift (1879/1967), Frege effected a\nthoroughgoing formalization of deductive reasoning. To\nformalize, we specify a formal language whose component\nlinguistic expressions are individuated non-semantically (e.g., by\ntheir geometric shapes). We may have some intended interpretation in\nmind, but elements of the formal language are purely syntactic\nentities that we can discuss without invoking semantic properties such\nas reference or truth-conditions. In particular, we can\nspecify inference rules in formal syntactic terms. If we\nchoose our inference rules wisely, then they will cohere with our\nintended interpretation: they will carry true premises to true\nconclusions. Through formalization, Frege invested logic with\nunprecedented rigor. He thereby laid the groundwork for numerous\nsubsequent mathematical and philosophical developments. Formalization plays a significant foundational role within computer\nscience. We can program a Turing-style computer that manipulates\nlinguistic expressions drawn from a formal language. If we program the\ncomputer wisely, then its syntactic machinations will cohere with our\nintended semantic interpretation. For example, we can program the\ncomputer so that it carries true premises only to true conclusions, or\nso that it updates probabilities as dictated by Bayesian decision\ntheory. FSC holds that all computation manipulates formal\nsyntactic items, without regard to any semantic properties those items\nmay have.  Precise formulations of FSC vary. Computation is said to be\n“sensitive” to syntax but not semantics, or to have\n“access” only to syntactic properties, or to operate\n“in virtue” of syntactic rather than semantic properties,\nor to be impacted by semantic properties only as\n“mediated” by syntactic properties. It is not always so\nclear what these formulations mean or whether they are equivalent to\none another. But the intuitive picture is that syntactic properties\nhave causal/explanatory primacy over semantic properties in driving\ncomputation forward. Fodor’s article “Methodological Solipsism Considered as\na Research Strategy in Cognitive Psychology” (1980) offers an\nearly statement. Fodor combines FSC with CCTM+RTM. He analogizes\nMentalese to formal languages studied by logicians: it contains simple\nand complex items individuated non-semantically, just as typical\nformal languages contain simple and complex expressions individuated\nby their shapes. Mentalese symbols have a semantic interpretation, but\nthis interpretation does not (directly) impact mental computation. A\nsymbol’s formal properties, rather than its semantic properties,\ndetermine how computation manipulates the symbol. In that sense, the\nmind is a “syntactic engine”. Virtually all classical\ncomputationalists follow Fodor in endorsing FSC. Connectionists often deny that neural networks manipulate\nsyntactically structured items. For that reason, many connectionists\nwould hesitate to accept FSC. Nevertheless, most connectionists\nendorse a generalized formality thesis: computation is\ninsensitive to semantic properties. The generalized formality thesis\nraises many of the same philosophical issues raised by FSC. We focus\nhere on FSC, which has received the most philosophical discussion. Fodor combines CCTM+RTM+FSC with intentional realism. He holds that\nCCTM+RTM+FSC vindicates folk psychology by helping us convert common\nsense intentional discourse into rigorous science. He motivates his\nposition with a famous abductive argument for CCTM+RTM+FSC (1987:\n18–20). Strikingly, mental activity tracks semantic properties\nin a coherent way. For example, deductive inference carries premises\nto conclusions that are true if the premises are true. How can we\nexplain this crucial aspect of mental activity? Formalization shows\nthat syntactic manipulations can track semantic properties, and\ncomputer science shows how to build physical machines that execute\ndesired syntactic manipulations. If we treat the mind as a\nsyntax-driven machine, then we can explain why mental activity tracks\nsemantic properties in a coherent way. Moreover, our explanation does\nnot posit causal mechanisms radically different from those posited\nwithin the physical sciences. We thereby answer the pivotal\nquestion: How is rationality mechanically possible? Stephen Stich (1983) and Hartry Field (2001) combine CCTM+FSC with\neliminativism. They recommend that cognitive science model the mind in\nformal syntactic terms, eschewing intentionality altogether. They\ngrant that mental states have representational properties, but they\nask what explanatory value scientific psychology gains by invoking\nthose properties. Why supplement formal syntactic description with\nintentional description? If the mind is a syntax-driven machine, then\ndoesn’t representational content drop out as explanatorily\nirrelevant? At one point in his career, Putnam (1983: 139–154) combined\nCCTM+FSC with a Davidson-tinged interpretivism. Cognitive\nscience should proceed along the lines suggested by Stich and Field,\ndelineating purely formal syntactic computational models. Formal\nsyntactic modeling co-exists with ordinary interpretive practice, in\nwhich we ascribe intentional contents to one another’s mental\nstates and speech acts. Interpretive practice is governed by holistic\nand heuristic constraints, which stymie attempts at converting\nintentional discourse into rigorous science. For Putnam, as for Field\nand Stich, the scientific action occurs at the formal syntactic level\nrather than the intentional level. CTM+FSC comes under attack from various directions. One criticism\ntargets the causal relevance of representational content\n(Block 1990; Figdor 2009; Kazez 1995). Intuitively speaking, the\ncontents of mental states are causally relevant to mental activity and\nbehavior. For example, my desire to drink water rather than orange\njuice causes me to walk to the sink rather than the refrigerator. The\ncontent of my desire (that I drink water) seems to play an\nimportant causal role in shaping my behavior. According to Fodor\n(1990: 137–159), CCTM+RTM+FSC accommodates such\nintuitions. Formal syntactic activity implements intentional\nmental activity, thereby ensuring that intentional mental states\ncausally interact in accord with their contents. However, it is not so\nclear that this analysis secures the causal relevance of content. FSC\nsays that computation is “sensitive” to syntax but not\nsemantics.  Depending on how one glosses the key term\n“sensitive”, it can look like representational content is\ncausally irrelevant, with formal syntax doing all the causal\nwork. Here is an analogy to illustrate the worry. When a car drives\nalong a road, there are stable patterns involving the car’s\nshadow. Nevertheless, shadow position at one time does not influence\nshadow position at a later time. Similarly, CCTM+RTM+FSC may explain\nhow mental activity instantiates stable patterns described in\nintentional terms, but this is not enough to ensure the causal\nrelevance of content. If the mind is a syntax-driven machine, then\ncausal efficacy seems to reside at the syntactic rather the semantic\nlevel. Semantics is just “along for the ride”. Apparently,\nthen, CTM+FSC encourages the conclusion that representational\nproperties are causally inert. The conclusion may not trouble\neliminativists, but intentional realists usually want to avoid it.  A second criticism dismisses the formal-syntactic picture as\nspeculation ungrounded in scientific practice. Tyler Burge (2010a,b,\n2013: 479–480) contends that formal syntactic description of\nmental activity plays no significant role within large areas of\ncognitive science, including the study of theoretical reasoning,\npractical reasoning, and perception. In each case, Burge argues, the\nscience employs intentional description rather than formal\nsyntactic description. For example, perceptual psychology individuates\nperceptual states not through formal syntactic properties but through\nrepresentational relations to distal shapes, sizes, colors, and so\non. To understand this criticism, we must distinguish formal\nsyntactic description and neurophysiological\ndescription. Everyone agrees that a complete scientific\npsychology will assign prime importance to neurophysiological\ndescription. However, neurophysiological description is distinct from\nformal syntactic description, because formal syntactic description is\nsupposed to be multiply realizable in the neurophysiological. The\nissue here is whether scientific psychology should\nsupplement intentional descriptions\nand neurophysiological descriptions with multiply\nrealizable, non-intentional formal syntactic descriptions.  Putnam’s landmark article “The Meaning of\n‘Meaning’” (1975: 215–271) introduced\nthe Twin Earth thought experiment, which postulates a world\njust like our own except that H2O is replaced by a\nqualitatively similar substance XYZ with different chemical\ncomposition. Putnam argues that XYZ is not water and that speakers on\nTwin Earth use the word “water” to refer to XYZ rather\nthan to water. Burge (1982) extends this conclusion\nfrom linguistic reference to mental content. He\nargues that Twin Earthlings instantiate mental states with different\ncontents. For example, if Oscar on Earth thinks that water is\nthirst-quenching, then his duplicate on Twin Earth thinks a\nthought with a different content, which we might gloss as that\ntwater is thirst-quenching. Burge concludes that mental content\ndoes not supervene upon internal neurophysiology. Mental content is\nindividuated partly by factors outside the thinker’s skin,\nincluding causal relations to the environment. This position\nis externalism about mental content. Formal syntactic properties of mental states are widely taken to\nsupervene upon internal neurophysiology. For example, Oscar and Twin\nOscar instantiate the same formal syntactic manipulations. Assuming\ncontent externalism, it follows that there is a huge gulf between\nordinary intentional description and formal syntactic description. Content externalism raises serious questions about the explanatory\nutility of representational content for scientific psychology: Argument from Causation (Fodor 1987, 1991): How can mental\ncontent exert any causal influence except as manifested within\ninternal neurophysiology? There is no “psychological action at a\ndistance”. Differences in the physical environment impact\nbehavior only by inducing differences in local brain states. So the\nonly causally relevant factors are those that supervene upon internal\nneurophysiology. Externally individuated content is causally\nirrelevant. Argument from Explanation (Stich 1983): Rigorous\nscientific explanation should not take into account factors outside\nthe subject’s skin. Folk psychology may taxonomize mental states\nthrough relations to the external environment, but scientific\npsychology should taxonomize mental states entirely through factors\nthat supervene upon internal neurophysiology. It should treat Oscar\nand Twin Oscar as psychological\nduplicates.[3] Some authors pursue the two arguments in conjunction with one\nanother. Both arguments reach the same conclusion: externally\nindividuated mental content finds no legitimate place within causal\nexplanations provided by scientific psychology. Stich (1983) argues\nalong these lines to motivate his formal-syntactic eliminativism. Many philosophers respond to such worries by promoting content\ninternalism. Whereas content externalists favor wide\ncontent (content that does not supervene upon internal\nneurophysiology), content internalists favor narrow content\n(content that does so supervene). Narrow content is what remains of\nmental content when one factors out all external elements. At one\npoint in his career, Fodor (1981, 1987) pursued internalism as a\nstrategy for integrating intentional psychology with\nCCTM+RTM+FSC. While conceding that wide content should not figure in\nscientific psychology, he maintained that narrow content should play a\ncentral explanatory role. Radical internalists insist that all content is narrow. A\ntypical analysis holds that Oscar is thinking not about water but\nabout some more general category of substance that subsumes XYZ, so\nthat Oscar and Twin Oscar entertain mental states with the same\ncontents.  Tim Crane (1991) and Gabriel Segal (2000) endorse such an\nanalysis.  They hold that folk psychology always individuates\npropositional attitudes narrowly. A less radical internalism\nrecommends that we recognize narrow content in addition to\nwide content. Folk psychology may sometimes individuate propositional\nattitudes widely, but we can also delineate a viable notion of narrow\ncontent that advances important philosophical or scientific\ngoals. Internalists have proposed various candidate notions of narrow\ncontent (Block 1986; Chalmers 2002; Cummins 1989; Fodor 1987; Lewis\n1994; Loar 1988; Mendola 2008). See the\nentry narrow mental content for an\noverview of prominent candidates. Externalists complain that existing theories of narrow content are\nsketchy, implausible, useless for psychological explanation, or\notherwise objectionable (Burge 2007; Sawyer 2000; Stalnaker\n1999). Externalists also question internalist arguments that\nscientific psychology requires narrow content: Argument from Causation: Externalists insist that wide\ncontent can be causally relevant. The details vary among externalists,\nand discussion often becomes intertwined with complex issues\nsurrounding causation, counterfactuals, and the metaphysics of mind.\nSee the entry mental causation for\nan introductory overview, and see Burge (2007), Rescorla (2014a), and\nYablo (1997, 2003) for representative externalist discussion. Argument from Explanation: Externalists claim that\npsychological explanation can legitimately taxonomize mental states\nthrough factors that outstrip internal neurophysiology (Peacocke\n1993; Shea, 2018). Burge observes that non-psychological sciences often\nindividuate explanatory kinds relationally, i.e., through\nrelations to external factors. For example, whether an entity counts\nas a heart depends (roughly) upon whether its biological function in\nits normal environment is to pump blood. So physiology individuates\norgan kinds relationally. Why can’t psychology likewise\nindividuate mental states relationally? For a notable exchange on\nthese issues, see Burge (1986, 1989, 1995) and Fodor (1987, 1991). Externalists doubt that we have any good reason to replace or\nsupplement wide content with narrow content. They dismiss the search\nfor narrow content as a wild goose chase. Burge (2007, 2010a) defends externalism by analyzing current\ncognitive science. He argues that many branches of scientific\npsychology (especially perceptual psychology) individuate mental\ncontent through causal relations to the external environment. He\nconcludes that scientific practice embodies an externalist\nperspective.  By contrast, he maintains, narrow content is a\nphilosophical fantasy ungrounded in current science. Suppose we abandon the search for narrow content. What are the\nprospects for combining CTM+FSC with externalist intentional\npsychology? The most promising option emphasizes levels of\nexplanation. We can say that intentional psychology occupies one\nlevel of explanation, while formal-syntactic computational psychology\noccupies a different level. Fodor advocates this approach in his later\nwork (1994, 2008). He comes to reject narrow content as otiose. He\nsuggests that formal syntactic mechanisms implement externalist\npsychological laws. Mental computation manipulates Mentalese\nexpressions in accord with their formal syntactic properties, and\nthese formal syntactic manipulations ensure that mental activity\ninstantiates appropriate law-like patterns defined over wide\ncontents. In light of the internalism/externalism distinction, let us revisit\nthe eliminativist challenge raised in §5.1:\nwhat explanatory value does intentional description add to\nformal-syntactic description?  Internalists can respond that suitable\nformal syntactic manipulations determine and maybe even constitute\nnarrow contents, so that internalist intentional description is\nalready implicit in suitable formal syntactic description (cf. Field\n2001: 75). Perhaps this response vindicates intentional realism,\nperhaps not. Crucially, though, no such response is available to\ncontent externalists.  Externalist intentional description is not\nimplicit in formal syntactic description, because one can hold formal\nsyntax fixed while varying wide content. Thus, content externalists\nwho espouse CTM+FSC must say what we gain by supplementing\nformal-syntactic explanations with intentional explanations. Once we\naccept that mental computation is sensitive to syntax but not\nsemantics, it is far from clear that any useful explanatory work\nremains for wide content. Fodor addresses this challenge at various\npoints, offering his most systematic treatment in The Elm and the\nExpert (1994). See Arjo (1996), Aydede (1998), Aydede and Robbins\n(2001), Wakefield (2002); Perry (1998), and Wakefield (2002) for\ncriticism. See Rupert (2008) and Schneider (2005) for positions close\nto Fodor’s. Dretske (1993) and Shea (2018, pp. 197–226) pursue\nalternative strategies for vindicating the explanatory relevance of\nwide content.  The perceived gulf between computational description and\nintentional description animates many writings on CTM. A few\nphilosophers try to bridge the gulf using computational descriptions\nthat individuate computational states in representational terms. These\ndescriptions are content-involving, to use Christopher\nPeacocke’s (1994) terminology. On the content-involving\napproach, there is no rigid demarcation between computational and\nintentional description. In particular, certain scientifically\nvaluable descriptions of mental activity are both computational and\nintentional. Call this position content-involving\ncomputationalism. Content-involving computationalists need not say that all\ncomputational description is intentional. To illustrate, suppose we\ndescribe a simple Turing machine that manipulates symbols individuated\nby their geometric shapes. Then the resulting computational\ndescription is not plausibly content-involving. Accordingly,\ncontent-involving computationalists do not usually advance\ncontent-involving computation as a general theory of computation. They\nclaim only that some important computational descriptions are\ncontent-involving. One can develop content-involving computationalism in an\ninternalist or externalist direction. Internalist\ncontent-involving computationalists hold that some computational\ndescriptions identify mental states partly through\ntheir narrow contents.  Murat Aydede (2005) recommends a\nposition along these lines.  Externalist content-involving\ncomputationalism holds that certain computational descriptions\nidentify mental states partly through their wide\ncontents. Tyler Burge (2010a: 95–101), Christopher Peacocke\n(1994, 1999), Michael Rescorla (2012), and Mark Sprevak (2010) espouse\nthis position. Oron Shagrir (2001, forthcoming) advocates a\ncontent-involving computationalism that is neutral between internalism\nand externalism. Externalist content-involving computationalists typically cite\ncognitive science practice as a motivating factor. For example,\nperceptual psychology describes the perceptual system as computing an\nestimate of some object’s size from retinal stimulations and\nfrom an estimate of the object’s depth. Perceptual\n“estimates” are identified representationally, as\nrepresentations of specific distal sizes and depths. Quite plausibly,\nrepresentational relations to specific distal sizes and depths do not\nsupervene on internal neurophysiology. Quite plausibly, then,\nperceptual psychology type-identifies perceptual computations through\nwide contents. So externalist content-involving computationalism seems\nto harmonize well with current cognitive science. A major challenge facing content-involving computationalism\nconcerns the interface with standard computationalism formalisms, such\nas the Turing machine. How exactly do content-involving descriptions\nrelate to the computational models found in logic and computer\nscience?  Philosophers usually assume that these models offer\nnon-intentional descriptions. If so, that would be a major and perhaps\ndecisive blow to content-involving computationalism. Arguably, though, many familiar computational formalisms allow a\ncontent-involving rather than formal syntactic construal. To\nillustrate, consider the Turing machine. One can individuate\nthe “symbols” comprising the Turing machine alphabet\nnon-semantically, through factors akin to geometric shape. But does\nTuring’s formalism require a non-semantic individuative\nscheme? Arguably, the formalism allows us\nto individuate symbols partly through their contents. Of course, the\nmachine table for a Turing machine does not explicitly cite semantic\nproperties of symbols (e.g., denotations or truth-conditions).\nNevertheless, the machine table can encode mechanical rules that\ndescribe how to manipulate symbols, where those symbols are\ntype-identified in content-involving terms. In this way, the machine\ntable dictates transitions among content-involving states without\nexplicitly mentioning semantic properties. Aydede (2005) suggests an\ninternalist version of this view, with symbols type-identified through\ntheir narrow\n contents.[4]\n Rescorla (2017a) develops the view in\nan externalist direction, with symbols type-identified through their\nwide contents. He argues that some Turing-style models describe\ncomputational operations over externalistically individuated Mentalese\nsymbols.[5] In principle, one might embrace both externalist content-involving\ncomputational description and formal syntactic description.\nOne might say that these two kinds of description occupy distinct\nlevels of explanation. Peacocke suggests such a view. Other\ncontent-involving computationalists regard formal syntactic\ndescriptions of the mind more skeptically. For example, Burge\nquestions what explanatory value formal syntactic description\ncontributes to certain areas of scientific psychology (such as\nperceptual psychology). From this viewpoint, the eliminativist\nchallenge posed in §5.1 has matters\nbackwards. We should not assume that formal syntactic descriptions are\nexplanatorily valuable and then ask what value intentional\ndescriptions contribute.  We should instead embrace the externalist\nintentional descriptions offered by current cognitive science and then\nask what value formal syntactic description contributes. Proponents of formal syntactic description respond by\nciting implementation mechanisms. Externalist description of\nmental activity presupposes that suitable causal-historical relations\nbetween the mind and the external physical environment are in\nplace. But surely we want a “local” description that\nignores external causal-historical relations, a description that\nreveals underlying causal mechanisms. Fodor (1987, 1994) argues in\nthis way to motivate the formal syntactic picture. For possible\nexternalist responses to the argument from implementation mechanisms,\nsee Burge (2010b), Rescorla (2017b), Shea (2013), and Sprevak\n(2010). Debate over this argument, and more generally over the\nrelation between computation and representation, seems likely to\ncontinue into the indefinite future.  The literature offers several alternative conceptions, usually\nadvanced as foundations for CTM. In many cases, these conceptions\noverlap with one another or with the conceptions considered above. It is common for cognitive scientists to describe computation as\n“information-processing”. It is less common for proponents\nto clarify what they mean by “information” or\n“processing”. Lacking clarification, the description is\nlittle more than an empty slogan. Claude Shannon introduced a scientifically important notion of\n“information” in his 1948 article “A Mathematical\nTheory of Communication”. The intuitive idea is that information\nmeasures reduction in uncertainty, where reduced uncertainty\nmanifests as an altered probability distribution over possible states.\nShannon codified this idea within a rigorous mathematical framework,\nlaying the foundation for information theory (Cover and\nThomas 2006). Shannon information is fundamental to modern\nengineering. It finds fruitful application within cognitive science,\nespecially cognitive neuroscience. Does it support a convincing\nanalysis of computation as “information-processing”?\nConsider an old-fashioned tape machine that records messages received\nover a wireless radio. Using Shannon’s framework, one can\nmeasure how much information is carried by some recorded\nmessage. There is a sense in which the tape machine\n“processes” Shannon information whenever we replay a\nrecorded message. Still, the machine does not seem to implement a\nnon-trivial computational \nmodel.[6]\n Certainly, neither the Turing machine\nformalism nor the neural network formalism offers much insight into\nthe machine’s operations. Arguably, then, a system can process\nShannon information without executing computations in any interesting\nsense. Confronted with such examples, one might try to isolate a more\ndemanding notion of “processing”, so that the tape machine\ndoes not “process” Shannon information. Alternatively, one\nmight insist that the tape machine executes non-trivial computations.\nPiccinini and Scarantino (2010) advance a highly general notion of\ncomputation—which they dub generic\ncomputation—with that consequence.  A second prominent notion of information derives from Paul\nGrice’s (1989) influential discussion of natural\nmeaning. Natural meaning involves reliable,\ncounterfactual-supporting correlations. For example, tree rings\ncorrelate with the age of the tree, and pox correlate with\nchickenpox. We colloquially describe tree rings as carrying\ninformation about tree age, pox as carrying information about\nchickenpox, and so on. Such descriptions suggest a conception that\nties information to reliable, counterfactual-supporting\ncorrelations. Fred Dretske (1981) develops this conception into a\nsystematic theory, as do various subsequent philosophers. Does\nDretske-style information subserve a plausible analysis of computation\nas “information-processing”? Consider an\nold-fashioned bimetallic strip thermostat. Two metals are\njoined together into a strip. Differential expansion of the metals\ncauses the strip to bend, thereby activating or deactivating a heating\nunit. Strip state reliably correlates with current ambient\ntemperature, and the thermostat “processes” this\ninformation-bearing state when activating or deactivating the\nheater. Yet the thermostat does not seem to implement any non-trivial\ncomputational model. One would not ordinarily regard the thermostat as\ncomputing. Arguably, then, a system can process Dretske-style\ninformation without executing computations in any interesting\nsense. Of course, one might try to handle such examples through\nmaneuvers parallel to those from the previous paragraph. A third prominent notion of information is semantic\ninformation, i.e., representational\ncontent.[7] Some\nphilosophers hold that a physical system computes only if the\nsystem’s states have representational properties (Dietrich 1989;\nFodor 1998: 10; Ladyman 2009; Shagrir 2006; Sprevak 2010). In that\nsense, information-processing is necessary for\ncomputation. As Fodor memorably puts it, “no computation without\nrepresentation” (1975: 34). However, this position is\ndebatable. Chalmers (2011) and Piccinini (2008a) contend that a Turing\nmachine might execute computations even though symbols manipulated by\nthe machine have no semantic interpretation. The machine’s\ncomputations are purely syntactic in nature, lacking anything like\nsemantic properties. On this view, representational content is not\nnecessary for a physical system to count as computational. It remains unclear whether the slogan “computation is\ninformation-processing” provides much insight. Nevertheless, the\nslogan seems unlikely to disappear from the literature anytime soon.\nFor further discussion of possible connections between computation and\ninformation, see Gallistel and King (2009: 1–26), Lizier,\nFlecker, and Williams (2013), Milkowski (2013), Piccinini and\nScarantino (2010), and Sprevak (forthcoming).  In a widely cited passage, the perceptual psychologist David Marr\n(1982) distinguishes three levels at which one can describe an\n“information-processing device”:  Computational theory: “[t]he device is\ncharacterized as a mapping from one kind of information to another,\nthe abstract properties of this mapping are defined precisely, and its\nappropriateness and adequacy for the task as hand are\ndemonstrated” (p. 24).  Representation and algorithm: “the choice of\nrepresentation for the input and output and the algorithm to be used\nto transform one into the other” (pp. 24–25).  Hardware implementation: “the details of how the\nalgorithm and representation are realized physically” (p.\n25). Marr’s three levels have attracted intense philosophical\nscrutiny. For our purposes, the key point is that Marr’s\n“computational level” describes a mapping from inputs to\noutputs, without describing intermediate steps. Marr illustrates his\napproach by providing “computational level” theories of\nvarious perceptual processes, such as edge detection. Marr’s discussion suggests a functional conception of\ncomputation, on which computation is a matter of transforming\ninputs into appropriate outputs. Frances Egan elaborates the\nfunctional conception over a series of articles (1991, 1992, 1999,\n2003, 2010, 2014, 2019). Like Marr, she treats computational\ndescription as description of input-output relations. She also claims\nthat computational models characterize a purely mathematical\nfunction: that is, a mapping from mathematical inputs to mathematical\noutputs. She illustrates by considering a visual mechanism (called\n“Visua”) that computes an object’s depth from\nretinal disparity. She imagines a neurophysiological duplicate\n(“Twin Visua”) embedded so differently in the physical\nenvironment that it does not represent depth. Visua and Twin Visua\ninstantiate perceptual states with different representational\nproperties. Nevertheless, Egan says, vision science treats Visua and\nTwin Visua as computational duplicates. Visua and Twin Visua\ncompute the same mathematical function, even though the computations\nhave different representational import in the two cases. Egan\nconcludes that computational modeling of the mind yields an\n“abstract mathematical description” consistent with many\nalternative possible representational descriptions.  Intentional\nattribution is just a heuristic gloss upon underlying computational\ndescription. Chalmers (2012) argues that the functional conception neglects\nimportant features of computation. As he notes, computational models\nusually describe more than just input-output relations. They describe\nintermediate steps through which inputs are transformed into outputs.\nThese intermediate steps, which Marr consigns to the\n“algorithmic” level, figure prominently in computational\nmodels offered by logicians and computer scientists. Restricting the\nterm “computation” to input-output description does not\ncapture standard computational practice. An additional worry faces functional theories, such as\nEgan’s, that exclusively emphasize mathematical inputs\nand outputs.  Critics complain that Egan mistakenly elevates\nmathematical functions, at the expense of intentional explanations\nroutinely offered by cognitive science (Burge\n2005; Rescorla\n2015; Silverberg 2006; Sprevak 2010). To illustrate, suppose\nperceptual psychology describes the perceptual system as estimating\nthat some object’s depth is 5 meters. The perceptual\ndepth-estimate has a representational content: it is accurate only if\nthe object’s depth is 5 meters. We cite the number 5 to identify\nthe depth-estimate.  But our choice of this number depends upon our\narbitrary choice of measurement units. Critics contend that the\ncontent of the depth-estimate, not the arbitrarily chosen number\nthrough which we theorists specify that content, is what matters for\npsychological explanation. Egan’s theory places the number\nrather than the content at explanatory center stage. According to\nEgan, computational explanation should describe the visual system as\ncomputing a particular mathematical function that\ncarries particular mathematical inputs into particular\nmathematical outputs. Those particular mathematical inputs and\noutputs depend upon our arbitrary choice of measurement units, so they\narguably lack the explanatory significance that Egan assigns to\nthem. We should distinguish the functional approach, as pursued by Marr\nand Egan, from the functional programming paradigm in\ncomputer science. The functional programming paradigm models\nevaluation of a complex function as successive evaluation of simpler\nfunctions. To take a simple example, one might evaluate \\(f(x,y) =\n(x^{2}+y)\\) by first evaluating the squaring function and then\nevaluating the addition function. Functional programming differs from\nthe “computational level” descriptions emphasized by Marr,\nbecause it specifies intermediate computational stages. The functional\nprogramming paradigm stretches back to Alonzo Church’s\n(1936) lambda calculus, continuing with programming languages\nsuch as PCF and LISP. It plays an important role in AI and theoretical\ncomputer science. Some authors suggest that it offers special insight\ninto mental computation (Klein 2012; Piantadosi, Tenenbaum, and\nGoodman 2012). However, many computational formalisms do not conform\nto the functional paradigm: Turing machines; imperative programming\nlanguages, such as C; logic programming languages, such as Prolog; and\nso on. Even though the functional paradigm describes numerous\nimportant computations (possibly including mental computations), it\ndoes not plausibly capture computation in general. Many philosophical discussions embody a structuralist\nconception of computation: a computational model describes an\nabstract causal structure, without taking into account particular\nphysical states that instantiate the structure. This conception traces\nback at least to Putnam’s original treatment (1967).\nChalmers (1995, 1996a, 2011, 2012) develops it in detail. He\nintroduces the combinatorial-state automaton (CSA) formalism,\nwhich subsumes most familiar models of computation (including Turing\nmachines and neural networks). A CSA provides an abstract description\nof a physical system’s causal topology: the pattern of\ncausal interaction among the system’s parts, independent of the\nnature of those parts or the causal mechanisms through which they\ninteract.  Computational description specifies a causal topology. Chalmers deploys structuralism to delineate a very general version\nof CTM. He assumes the functionalist view that psychological states\nare individuated by their roles in a pattern of causal organization.\nPsychological description specifies causal roles, abstracted away from\nphysical states that realize those roles. So psychological properties\nare organizationally invariant, in that they supervene upon\ncausal topology. Since computational description characterizes a\ncausal topology, satisfying a suitable computational description\nsuffices for instantiating appropriate mental properties. It also\nfollows that psychological description is a species of computational\ndescription, so that computational description should play a central\nrole within psychological explanation. Thus, structuralist computation\nprovides a solid foundation for cognitive science. Mentality is\ngrounded in causal patterns, which are precisely what computational\nmodels articulate. Structuralism comes packaged with an attractive account of\nthe implementation relation between abstract computational\nmodels and physical systems. Under what conditions does a physical\nsystem implement a computational model? Structuralists say that a\nphysical system implements a model just in case the model’s\ncausal structure is “isomorphic” to the model’s\nformal structure. A computational model describes a physical system by\narticulating a formal structure that mirrors some relevant causal\ntopology. Chalmers elaborates this intuitive idea, providing detailed\nnecessary and sufficient conditions for physical realization of CSAs.\nFew if any alternative conceptions of computation can provide so\nsubstantive an account of the implementation relation. We may instructively compare structuralist computationalism with\nsome other theories discussed above: Machine functionalism. Structuralist computationalism\nembraces the core idea behind machine functionalism: mental states are\nfunctional states describable through a suitable computational\nformalism. Putnam advances CTM as an empirical hypothesis, and he\ndefends functionalism on that basis. In contrast, Chalmers follows\nDavid Lewis (1972) by grounding functionalism in the conceptual\nanalysis of mentalistic discourse. Whereas Putnam defends\nfunctionalism by defending computationalism, Chalmers defends\ncomputationalism by assuming functionalism. Classical computationalism, connectionism, and computational\nneuroscience. Structuralist computationalism emphasizes\norganizationally invariant descriptions, which are multiply\nrealizable.  In that respect, it diverges from computational\nneuroscience.  Structuralism is compatible with both classical and\nconnectionist computationalism, but it differs in spirit from those\nviews.  Classicists and connectionists present their rival positions\nas bold, substantive hypotheses. Chalmers advances structuralist\ncomputationalism as a relatively minimalist position unlikely to be\ndisconfirmed. Intentional realism and eliminativism. Structuralist\ncomputationalism is compatible with both positions. CSA description\ndoes not explicitly mention semantic properties such as reference,\ntruth-conditions, representational content, and so on. Structuralist\ncomputationalists need not assign representational content any\nimportant role within scientific psychology. On the other hand,\nstructuralist computationalism does not preclude an important role for\nrepresentational content. The formal-syntactic conception of computation. Wide\ncontent depends on causal-historical relations to the external\nenvironment, relations that outstrip causal topology. Thus, CSA\ndescription leaves wide content underdetermined. Narrow content\npresumably supervenes upon causal topology, but CSA description does\nnot explicitly mention narrow contents. Overall, then, structuralist\ncomputationalism prioritizes a level of formal, non-semantic\ncomputational description. In that respect, it resembles FSC. On the\nother hand, structuralist computationalists need not say that\ncomputation is “insensitive” to semantic properties, so\nthey need not endorse all aspects of FSC. Although structuralist computationalism is distinct from CTM+FSC,\nit raises some similar issues. For example, Rescorla (2012) denies\nthat causal topology plays the central explanatory role within\ncognitive science that structuralist computationalism dictates. He\nsuggests that externalist intentional description rather than\norganizationally invariant description enjoys explanatory\nprimacy. Coming from a different direction, computational\nneuroscientists will recommend that we forego organizationally\ninvariant descriptions and instead employ more neurally specific\ncomputational models. In response to such objections, Chalmers (2012)\nargues that organizationally invariant computational description\nyields explanatory benefits that neither intentional description nor\nneurophysiological description replicate: it reveals the underlying\nmechanisms of cognition (unlike intentional description); and it\nabstracts away from neural implementation details that are irrelevant\nfor many explanatory purposes. The mechanistic nature of computation is a recurring theme in\nlogic, philosophy, and cognitive science. Gualtiero Piccinini (2007,\n2012, 2015) and Marcin Milkowski (2013) develop this theme into a\nmechanistic theory of computing systems. A functional\nmechanism is a system of interconnected components, where each\ncomponent performs some function within the overall\nsystem. Mechanistic explanation proceeds by decomposing the\nsystem into parts, describing how the parts are organized into the\nlarger system, and isolating the function performed by each part. A\ncomputing system is a functional mechanism of a particular kind. On\nPiccinini’s account, a computing system is a mechanism whose\ncomponents are functionally organized to process vehicles in accord\nwith rules. Echoing Putnam’s discussion of multiple\nrealizability, Piccinini demands that the rules\nbe medium-independent, in that they abstract away from the\nspecific physical implementations of the vehicles. Computational\nexplanation decomposes the system into parts and describes how each\npart helps the system process the relevant vehicles. If the system\nprocesses discretely structured vehicles, then the computation is\ndigital. If the system processes continuous vehicles, then the\ncomputation is analog. Milkowski’s version of the mechanistic\napproach is similar. He differs from Piccinini by pursuing an\n“information-processing” gloss, so that computational\nmechanisms operate over information-bearing states. Milkowski and\nPiccinini deploy their respective mechanistic theories to defend\ncomputationalism. Mechanistic computationalists typically individuate computational\nstates non-semantically. They therefore encounter worries about the\nexplanatory role of representational content, similar to worries\nencountered by FSC and structuralism. In this spirit, Shagrir (2014)\ncomplains that mechanistic computationalism does not accommodate\ncognitive science explanations that are simultaneously computational\nand representational. The perceived force of this criticism will\ndepend upon one’s sympathy for content-involving\ncomputationalism. We have surveyed various contrasting and sometimes overlapping\nconceptions of computation: classical computation, connectionist\ncomputation, neural computation, formal-syntactic computation,\ncontent-involving computation, information-processing computation,\nfunctional computation, structuralist computation, and mechanistic\ncomputation. Each conception yields a different form of\ncomputationalism. Each conception has its own strengths and\nweaknesses.  One might adopt a pluralistic stance that\nrecognizes distinct legitimate conceptions. Rather than elevate one\nconception above the others, pluralists happily employ whichever\nconception seems useful in a given explanatory context. Edelman (2008)\ntakes a pluralistic line, as does Chalmers (2012) in his most recent\ndiscussion. The pluralistic line raises some natural questions. Can we provide\na general analysis that encompasses all or most types of computation?\nDo all computations share certain characteristic marks with one\nanother?  Are they perhaps instead united by something like family\nresemblance?  Deeper understanding of computation requires us to\ngrapple with these questions. CTM has attracted numerous objections. In many cases, the\nobjections apply only to specific versions of CTM (such as classical\ncomputationalism or connectionist computationalism). Here are a few\nprominent objections. See also the\nentry the Chinese room argument for a\nwidely discussed objection to classical computationalism advanced by\nJohn Searle (1980).  A recurring worry is that CTM is trivial, because we can\ndescribe almost any physical system as executing computations. Searle\n(1990) claims that a wall implements any computer program,\nsince we can discern some pattern of molecular movements in the wall\nthat is isomorphic to the formal structure of the program. Putnam\n(1988: 121–125) defends a less extreme but still very strong\ntriviality thesis along the same lines. Triviality arguments play a\nlarge role in the philosophical literature. Anti-computationalists\ndeploy triviality arguments against computationalism, while\ncomputationalists seek to avoid triviality. Computationalists usually rebut triviality arguments by insisting\nthat the arguments overlook constraints upon computational\nimplementation, constraints that bar trivializing implementations. The\nconstraints may be counterfactual, causal, semantic, or otherwise,\ndepending on one’s favored theory of computation. For example,\nDavid Chalmers (1995, 1996a) and B. Jack Copeland (1996) hold that\nPutnam’s triviality argument ignores counterfactual conditionals\nthat a physical system must satisfy in order to implement a\ncomputational model. Other philosophers say that a physical system\nmust have representational properties to implement a computational\nmodel (Fodor 1998: 11–12; Ladyman 2009; Sprevak 2010) or at\nleast to implement a content-involving computational model (Rescorla\n2013, 2014b). The details here vary considerably, and\ncomputationalists debate amongst themselves exactly which types of\ncomputation can avoid which triviality arguments. But most\ncomputationalists agree that we can avoid any devastating triviality\nworries through a sufficiently robust theory of the implementation\nrelation between computational models and physical systems. Pancomputationalism holds that every physical system\nimplements a computational model. This thesis is plausible, since any\nphysical system arguably implements a sufficiently trivial\ncomputational model (e.g., a one-state finite state automaton). As\nChalmers (2011) notes, pancomputationalism does not seem worrisome for\ncomputationalism. What would be worrisome is the much stronger\ntriviality thesis that almost every physical system implements almost\nevery computational model. For further discussion of triviality arguments and computational\nimplementation, see Sprevak (2019) and the\nentry computation in physical systems.  According to some authors, Gödel’s incompleteness\ntheorems show that human mathematical capacities outstrip the\ncapacities of any Turing machine (Nagel and Newman 1958). J.R. Lucas\n(1961) develops this position into a famous critique of CCTM. Roger Penrose\npursues the critique in The Emperor’s New Mind (1989)\nand subsequent writings. Various philosophers and logicians have\nanswered the critique, arguing that existing formulations suffer from\nfallacies, question-begging assumptions, and even outright\nmathematical errors (Bowie 1982; Chalmers 1996b; Feferman 1996; Lewis 1969, 1979; Putnam\n1975: 365–366, 1994; Shapiro 2003). There is a wide consensus\nthat this criticism of CCTM lacks any force. It may turn out that\ncertain human mental capacities outstrip Turing-computability, but\nGödel’s incompleteness theorems provide no reason to\nanticipate that outcome.  Could a computer compose the Eroica symphony? Or discover\ngeneral relativity? Or even replicate a child’s effortless\nability to perceive the environment, tie her shoelaces, and discern\nthe emotions of others? Intuitive, creative, or skillful human\nactivity may seem to resist formalization by a computer program\n(Dreyfus 1972, 1992). More generally, one might worry that crucial\naspects of human cognition elude computational modeling, especially\nclassical computational modeling. Ironically, Fodor promulgates a forceful version of this\ncritique. Even in his earliest statements of CCTM, Fodor (1975:\n197–205) expresses considerable skepticism that CCTM can handle\nall important cognitive phenomena. The pessimism becomes more\npronounced in his later writings (1983, 2000), which focus especially\non abductive reasoning as a mental phenomenon that\npotentially eludes computational modeling. His core argument may be\nsummarized as follows: Some critics deny (1), arguing that suitable Turing-style\ncomputations can be sensitive to “nonlocal” properties\n(Schneider 2011; Wilson 2005). Some challenge (2), arguing that\ntypical abductive inferences are sensitive only to “local”\nproperties (Carruthers 2003; Ludwig and Schneider 2008; Sperber\n2002). Some concede step (3) but dispute step (4), insisting that we\nhave promising non-Turing-style models of the relevant mental\nprocesses (Pinker 2005). Partly spurred by such criticisms, Fodor\nelaborates his argument in considerable detail. To defend (2), he\ncritiques theories that model abduction by deploying\n“local” heuristic algorithms (2005: 41–46; 2008:\n115–126) or by positing a profusion of domain-specific cognitive\nmodules (2005: 56–100). To defend (4), he critiques various\ntheories that handle abduction through non-Turing-style models (2000:\n46–53; 2008), such as connectionist networks. The scope and limits of computational modeling remain\ncontroversial. We may expect this topic to remain an active focus of\ninquiry, pursued jointly with AI.  Mental activity unfolds in time. Moreover, the mind accomplishes\nsophisticated tasks (e.g., perceptual estimation) very quickly. Many\ncritics worry that computationalism, especially classical\ncomputationalism, does not adequately accommodate temporal aspects of\ncognition. A Turing-style model makes no explicit mention of the time\nscale over which computation occurs. One could physically implement\nthe same abstract Turing machine with a silicon-based device, or a\nslower vacuum-tube device, or an even slower pulley-and-lever\ndevice. Critics recommend that we reject CCTM in favor of some\nalternative framework that more directly incorporates temporal\nconsiderations. van Gelder and Port (1995) use this argument to\npromote a non-computational dynamical systems framework for\nmodeling mental activity. Eliasmith (2003, 2013: 12–13) uses it\nto support his Neural Engineering Framework. Computationalists respond that we can supplement an\nabstract computational model with temporal considerations (Piccinini\n2010; Weiskopf 2004). For example, a Turing machine model presupposes\ndiscrete “stages of computation”, without describing how\nthe stages relate to physical time. But we can supplement our model by\ndescribing how long each stage lasts, thereby converting our\nnon-temporal Turing machine model into a theory that yields detailed\ntemporal predictions. Many advocates of CTM employ supplementation\nalong these lines to study temporal properties of cognition (Newell\n1990). Similar supplementation figures prominently in computer\nscience, whose practitioners are quite concerned to build machines\nwith appropriate temporal properties. Computationalists conclude that\na suitably supplemented version of CTM can adequately capture how\ncognition unfolds in time.  A second temporal objection highlights the contrast\nbetween discrete and continuous temporal evolution\n(van Gelder and Port 1995). Computation by a Turing machine unfolds in\ndiscrete stages, while mental activity unfolds in a continuous time.\nThus, there is a fundamental mismatch between the temporal properties\nof Turing-style computation and those of actual mental activity. We\nneed a psychological theory that describes continuous temporal\nevolution. Computationalists respond that this objection assumes what is to be\nshown: that cognitive activity does not fall into explanatory\nsignificant discrete stages (Weiskopf 2004). Assuming that physical\ntime is continuous, it follows that mental activity unfolds in\ncontinuous time. It does not follow that cognitive models\nmust have continuous temporal structure. A personal computer operates\nin continuous time, and its physical state evolves continuously. A\ncomplete physical theory will reflect all those physical changes. But\nour computational model does not reflect every physical\nchange to the computer. Our computational model has discrete temporal\nstructure. Why assume that a good cognitive-level model of the mind\nmust reflect every physical change to the brain? Even if there is a\ncontinuum of evolving physical states, why assume a continuum\nof evolving cognitive states? The mere fact of continuous\ntemporal evolution does not militate against computational models with\ndiscrete temporal structure. Embodied cognition is a research program that draws inspiration\nfrom the continental philosopher Maurice Merleau-Ponty, the perceptual\npsychologist J.J. Gibson, and other assorted influences. It is a\nfairly heterogeneous movement, but the basic strategy is to emphasize\nlinks between cognition, bodily action, and the surrounding\nenvironment. See Varela, Thompson, and Rosch (1991) for an influential\nearly statement. In many cases, proponents deploy tools of dynamical\nsystems theory. Proponents typically present their approach as a\nradical alternative to computationalism (Chemero 2009; Kelso 1995;\nThelen and Smith 1994). CTM, they complain, treats mental activity as\nstatic symbol manipulation detached from the embedding environment. It\nneglects myriad complex ways that the environment causally or\nconstitutively shapes mental activity. We should replace CTM with a\nnew picture that emphasizes continuous links between mind, body, and\nenvironment. Agent-environment dynamics, not internal mental\ncomputation, holds the key to understanding cognition. Often, a\nbroadly eliminativist attitude towards intentionality propels this\ncritique. Computationalists respond that CTM allows due recognition of\ncognition’s embodiment. Computational models can take into\naccount how mind, body, and environment continuously interact. After\nall, computational models can incorporate sensory inputs and motor\noutputs. There is no obvious reason why an emphasis upon\nagent-environment dynamics precludes a dual emphasis upon internal\nmental computation (Clark 2014: 140–165; Rupert 2009).\nComputationalists maintain that CTM can incorporate any legitimate\ninsights offered by the embodied cognition movement. They also insist\nthat CTM remains our best overall framework for explaining numerous\ncore psychological phenomena.","contact.mail":"rescorla@ucla.edu","contact.domain":"ucla.edu"}]
