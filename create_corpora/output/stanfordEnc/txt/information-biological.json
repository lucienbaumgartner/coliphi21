[{"date.published":"2007-10-04","date.changed":"2016-03-23","url":"https://plato.stanford.edu/entries/information-biological/","author1":"Peter Godfrey-Smith","author1.info":"http://www.petergodfreysmith.com/","author2.info":"http://www.victoria.ac.nz/hppi/about/staff/kim-sterelny","entry":"information-biological","body.text":"\n\nSince the 1950s, the concept of information has acquired a\nstrikingly prominent role in many parts of biology. This enthusiasm\nextends far beyond domains where the concept might seem to have an\nobvious application, such as the biological study of perception,\ncognition, and language, and now reaches into the most basic parts of\nbiological theory. Hormones and other cellular products through which\nphysiological systems are regulated are typically described as\nsignals.  Descriptions of how genes play their causal role in\nmetabolic processes and development are routinely given in terms of\n“transcription”, “translation”, and\n“editing”. The most general term used for the processes by\nwhich genes exert their effects is “gene expression”. The\nfates of cells in a developing organism are explained in terms of\ntheir processing of “positional information” given to them\nfrom surrounding cells and other factors. Many biologists think of the\ndevelopmental processes by which organisms progress from egg to adult\nin terms of the execution of a “developmental\nprogram”. Other biologists have argued for a pivotal role for\ninformation in evolution rather than development: John Maynard Smith\nand Eors Szathmáry (for example) suggest that major transitions in\nevolution depend on expansions in the amount and accuracy with which\ninformation is transmitted across generations. And some have argued\nthat we can only understand the evolutionary role of genes by\nrecognizing an informational “domain” that exists\nalongside the domain of matter and energy. \n\nBoth philosophers and biologists have contributed to an ongoing\nfoundational discussion of the status of this mode of description in\nbiology. It is generally agreed that the sense of information isolated\nby Claude Shannon and used in mathematical information theory is\nlegitimate, useful, and relevant in many parts of biology. In this\nsense, anything is a source of information if it has a range\nof possible states, and one variable carries information\nabout another to the extent that their states are physically\ncorrelated. But it is also agreed that many uses of informational\nlanguage in biology seem to make use of a richer and more problematic\nconcept than Shannon’s. Some have drawn on\nthe teleosemantic tradition in philosophy of mind to make\nsense of this richer concept.  Other theorists have countered that Shannon’s correlational conception of information is richer than it looks.\n\nA minority tradition has argued that the enthusiasm for information\nin biology has been a serious theoretical wrong turn, and that it\nfosters naive genetic determinism, other distortions of our\nunderstanding of the roles of interacting causes, or an implicitly\ndualist ontology. However, this sceptical response is fading, with key\nsceptics coming to accept a modest but genuine role for informational\nconcepts in the life sciences. Others have taken the critique seriously but\ntried to distinguish legitimate appeals to information from misleading\nor erroneous ones.\n\nBiology is concerned with living organisms—with their\nstructure, activities, distribution in space and time, and\nparticipation in evolutionary and developmental histories. Many of\nthese organisms engage in activities that seem best understood in\nterms of information processing or representation. These include\nperception, cognition, signalling, and language use. We will not much\nbe concerned with the role of concepts of information and\nrepresentation in these cognitive contexts. For over the second half\nof the 20th century, biology came to apply informational\nconcepts (and their relatives) far more broadly than this. For many\nbiologists, the most basic processes characteristic of living\norganisms should now be understood in terms of the expression of\ninformation: the response to signals, the execution of programs, and\nthe interpretation of codes. So although contemporary mainstream\nbiology is an overtly materialist field, it has come to employ\nconcepts that seem intentional or semantic. These come with a long\nhistory of causing foundational problems for materialists (and, to\nsome extent, for everyone else). The embrace of informational and other semantic concepts has been\nespecially marked within genetics, and other fields—evolutionary\ntheory and developmental biology—with strong ties to\ngenetics. But informational language is not only found there: for\nexample, hormones are routinely seen as long\ndistance signals that enable one organ system to coordinate with\nothers (Levy 2011). That said, the usage that has generated the most\ndiscussion is found in the description of the relations between genes\nand the various structures and processes to which genes\ncontribute. For many biologists, the causal role of genes should be\nunderstood in terms of their carrying information about their\nvarious products; and perhaps as well about the environments in which\nthese products enhance fitness (Lorenz 1965; Shea 2013). The\nexpression of that information might depend on the presence of various\nenvironmental factors, but the same can be said of other kinds of\nmessage. Breaking things down further, we can recognize two causal\nroles for genes, and hence two potential explanatory roles for genetic\ninformation, within biology. Genes are crucial to both explaining the\ndevelopment of individual organisms, and to explaining the inheritance\nof characteristics across generations. Information has been invoked in\nboth explanatory contexts. One important use of informational language is relatively\nuncontroversial, through being anchored in striking and well\nestablished facts about the role of DNA and RNA in the manufacture of\nprotein molecules in cells, a set of facts summarized in the familiar\nchart representing the “genetic code”, that maps triplets\nof the DNA bases (C, A, T, G) to individual amino acids, which are the\nbuilding blocks of protein molecules. Not even this use\nof “coding” language is completely uncontroversial (Sarkar\n1996). More importantly, enthusiasm for the use of informational\nlanguage in biology both predates the discovery of the DNA-RNA-amino\nacid mapping (Schrödinger 1944), and goes far beyond it, so this\nmapping is only a partial explanation of the uptake of informational\nnotions in biology. Current applications of informational concepts in\nbiology include: There is no consensus about the status of these ideas. Indeed, the\nuse of informational notions is controversial even when giving\naccounts of animal communication, with some theorists denying that\nsuch communication is the flow of information from one animal to\nanother (Krebs and Dawkins 1984; Owren et al. 2010). The result has\nbeen a growing foundational discussion within biology and the\nphilosophy of biology. Some have hailed the employment of\ninformational concepts as a crucial advance (Williams 1992). Others\nhave seen almost every biological application of informational\nconcepts as a serious error, one that distorts our understanding and\ncontributes to lingering genetic determinism (Francis 2003). Most of\nthe possible options between these extreme views have also been\ndefended. Perhaps most commentators within philosophy have seen their\nproject as one of sorting through the various kinds of informational\ndescription that have become current, distinguishing legitimate ones\nfrom illegitimate ones (Godfrey-Smith 2000; Griffiths 2001;\nGodfrey-Smith 2007; Shea 2013; Lean 2014). Philosophers have also\ntried to give a reductive or naturalistic explanation for the\nlegitimate ones. This entry proceeds as follows. In the next section we discuss the\nmost unproblematic technical use of information in biology, which\ndraws on Shannon and the mathematical theory of information. Against\nthat background, some of the more contentious uses are both motivated\nand introduced. In section 3, we discuss\nrecent attempts to build a richer concept out of Shannon’s\ntechnical notion. We then discuss the status of the “genetic\ncode” in its original sense (section 4),\nand signalling systems inside and outside of genetics\n(section 5). We then look at ways of rejecting\ninformational forms of description (section\n6). The key sceptical idea is the so-called “parity\nthesis”: the idea that while genes play an immensely important\nrole in evolution and development, so do other factors, and so it is a\nmistake to think of genes and only genes as instructing or controlling\nthese processes. The final two sections discuss the idea that\nbiological development in individuals proceeds by the execution of a\nprogram (section 7), and the use of information\nin understanding the role of inheritance systems in the “major\ntransitions” in evolution (section 8). It is common to begin the analysis of information in biology with\nan uncontroversial but minimal notion: a causal or correlational\nconception. Smoke in the air and fire in the forest are correlated\nwith one another. If you see a forest burning at night, you can\npredict the air will be smoky. If you see a plume of smoke rising from\nabove the trees, you can predict that below there is a fire. The\ncorrelation does not have to be perfect to be informative. Putting the\npoint generally, a signal carries information about a source, in this\nsense, if we can predict the state of the source from the signal. This\nsense of information is associated with Claude Shannon (1948) who\nshowed how the concept of information could be used\nto quantify facts about contingency and correlation in a\nuseful way, initially for use in communication technology. For\nShannon, anything is a source of information if it has a\nnumber of alternative states that might be realized on a particular\noccasion. Any other variable contains some information about\nthat source, or carries information about it, if its state is correlated with the state of the source. This is a matter of degree; a signal carries more information\nabout a source if its state is a better predictor of the source, less\ninformation if it is a worse predictor.  In this sense, a signal can carry information about a source\nwithout there being any biological system designed to produce that\nsignal, nor any to use it once produced. When a biologist employs this\nsense of information in a description of gene action or of other\nprocesses, he or she is just adopting a quantitative framework for\ndescribing ordinary correlations or causal connections within those\nsystems. So it is one thing for there to be a signal carrying\ninformation about a source; quite another to explain biological\nprocesses as the result of signalling. The familiar example of tree\nrings is helpful here. When a tree lays down rings, it establishes a\nstructure that can be used by us to make inferences about the\npast. The number and size of the rings carry information, in\nShannon’s sense, about the history of the tree and its\ncircumstances (the technique, “dendrochronology”, is\nscientifically important). Despite the usefulness of the informational\ndescription, there is no sense in which we are explaining how\nthe tree does what it does in informational terms. The only reader or\nuser of the information in the tree rings is the human observer. The\ntree itself does not exploit the information in its rings to control\nits growth or flowering. Similarly, we might note ways in which the\ndistribution of different DNA sequences within and between biological\npopulations “carries information about” the historical\nrelationships between those populations, and the histories of the\nindividual populations themselves. Using this information has given\nevolutionary biologists a much richer and more reliable picture of the\nhistory of life (for a good introduction to these uses see Bromham\n2008). It has been argued, for example, that the greater diversity in\nmitochondrial DNA in African populations, in comparison to other human\npopulations, is an indicator of a comparatively recent human dispersal\nfrom an African origin. In making these inferences, informational\nconcepts can be useful tools. But this is just a more complicated\nversion of what is going on in the case of tree rings. The appeal to\ninformation has an inferential use that is no\nway explanatory. A large proportion of the informational\ndescriptions found in biology have this character. The border between these two categories, however, can be hard to\ndiscern. Steven Frank (2012) has developed a detailed mapping between\nequations describing change due to natural selection and equations\nused in information theory. Change across generations can be seen as\nthe accumulation of information by a population about its\nenvironment. So far, there is no claim that populations use this\ninformation in any way, and the situation is akin to that found in\ntree rings: a tree accumulates, in its rings, information about\nclimatic conditions in its past, though the tree does nothing with\nthat information. Frank may view the role of information acquired by\nevolution in a more substantive way: “selection causes\npopulations to accumulate information about the fit of characters to\nthe environment” (2012: 2391). But it is doubtful that anything more\nthan an inferential role has yet been shown for this kind of\ninformation. Consequently, philosophers have sometimes set the discussion up by\nsaying that there is one kind of “information” appealed to\nin biology, the kind originally described by Shannon, that is\nunproblematic and does not require much philosophical attention. The\nterm “causal” information is also sometimes used to refer\nto this concept, and this is essentially Grice’s “natural\nmeaning”, (see entry on Grice) from his\nwork in the philosophy of language (Grice 1957). Information in this\nsense exists whenever there is contingency and correlation. So we can\nsay that genes contain information about the proteins they make, and\nalso that genes contain information about the whole-organism\nphenotype. But when we say that, we are saying no more than what we\nare saying when we say that there is an informational connection\nbetween smoke and fire, or between tree rings and a tree’s\nage. The more contentious question becomes whether or not biology\nneeds another, richer concept of information as\nwell. Information in this richer sense is sometimes called\n“semantic” or “intentional” information. Why might we think that biology needs to employ a richer concept?\nOne thought is that genes play a special, instructional role in\ndevelopment, telling the embryo how to grow. It is true that genes\ncarry Shannon-information about phenotypes: the genome of a fertilized\negg predicts much of the resulting phenotype. In mammals, for example,\nchromosome structure predicts the sex of the adult animal. But if an\ninformational relationship between gene and phenotype is supposed to\ninvolve a distinctive instruction-like mode of causation, then this\ncannot be information in Shannon’s sense.  For environmental\nfactors, not just genes, “carry information” about\nphenotypes, in Shannon’s sense. With respect to Shannon\ninformation, there is a “parity” between the roles of\nenvironmental and genetic causes (Griffiths and Gray 1994). Moreover,\nif we are using Shannon’s concept, then the informational\nrelationship between genotype and phenotype is symmetrical. For\nexample, once a reader knows that both authors of this article are\nmale, they can predict that we both carry a Y-chromosome.  Some talk\nabout information in biology is consistent with these features of\nShannon information, but some is not. In particular, it is usually\nthought that at least some applications of informational language to\ngenes ascribe to them a property that is not ascribed to\nenvironmental conditions, even when the environment is predictively\nimportant. In addition, a message that carries “semantic\ninformation”, it is often thought, has the capacity\nto mis-represent, as well as accurately represent, what it is\nabout. There is a capacity for error. Shannon information does not\nhave that feature; we cannot say that one variable “carried\nfalse information” about another, if we are using Shannon’s\nsense of the term. But biologists do apparently want to use language\nof that kind when talking about genes. Genes carry a message that\nis supposed to be expressed, whether or not it actually is\nexpressed. Thus the developmental disorders caused by thalidomide in\nthe 1960s were due to an inability to use genetic signals\nthat were supposed to control limb development. These genes\nwere present and active; thalidomide did not do its harm by causing\nmutations. These are the usual “marks” that are taken in the\nliterature to show that a richer sense of information than\nShannon’s has been introduced to biology. But the most crucial\ndifference between the less and more contentious applications of\ninformational concepts is that, in the richer cases, information use\nis supposed to help explain how biological systems do what\nthey do—how cells work, how an egg can develop into an adult,\nhow genetic inheritance mechanisms make the evolution of complex\nphenotypes possible. At this point, there are a number of options on the table. One is\nto deny that genes, cells and other biological structures literally\ntraffic in information in ways that explain their behaviour, but to\nargue nevertheless that this is a useful analogy or model. The idea is\nthat there are useful similarities between paradigm cases of information and\nrepresentation using systems—cognitively sophisticated agents,\nthinking and communicating with one another—and biological\nsystems. Hormones, for example, are usefully thought of as messages\nbecause they are small, stable, energetically inexpensive items that\ncan travel long distances (relative to their own size) without\ndecaying, to specialized locations where they have\npredictable, specific, and important effects on arrival. But while\nthis is a helpful way of thinking, perhaps we should not take talk of\nmessages too seriously. For example, we should not treat a question\nabout whether prolactin is a report of pregnancy or an instruction to\nthe mammary glands as if it had a literally correct answer. Arnon Levy\nhas developed the most sophisticated version of this view of\nbiological information (Levy 2011).  A second option is to argue that genes and other biological\nstructures literally carry semantic information, and their\ninformational character explains the distinctive role of these\nstructures in biological processes. If we think of genes or cells as\nliterally carrying semantic information, our problem changes; who or\nwhat could count as composing or reading these messages? Paradigm\ncases of structures with semantic information—pictures,\nsentences, programs—are built by the thought and action of\nintelligent agents. So we need to show how genes and\ncells—neither intelligent systems themselves nor the products of\nintelligence—can carry semantic information, and how the\ninformation they carry explains their biological role. We need some\nkind of reductive explanation of semantic information (arguably, we\nneed this to understand cognition, too). One place we might look for\nsuch an analysis is naturalistic philosophy of mind. A third option is to argue that causal information itself can\nexplain biological phenomena, and no additional concept of information\nis needed. Biological systems, on this view, can be adapted to send or\nreceive signals that carry causal information. Elevated prolactin, for\nexample, does covary with pregnancy, and that is no\naccident. Likewise, the production of milk is a designed response to\nthe registration of these elevated levels at the mammary glands. While\nit is true that lactation carries as much causal information about\nprolactin levels as those levels carry about milk flow, there is a\nphysical asymmetry, hence directionality, between source and\nreceiver. (After birth, suckling stimulates local production of\nprolactin, so the physical asymmetry is the signal of pregnancy from\nthe pituitary gland to the breasts, tuning them for lactation). Brian\nSkyrms’ work has been very important in returning causal\ninformation to centre stage, in part because his work shows that\nsender-receiver signalling systems need not be cognitively\nsophisticated agents (Skyrms 2010). Sending and receiving causal\ninformation can emerge and stabilise amongst simple systems; certainly\namongst systems no more complex than a cell. As noted above, several philosophers and biologists have argued\nthat much informational talk about genes uses a richer concept than\nShannon’s, but this concept can be given a naturalistic\nanalysis.  The aim has been to make sense of the idea that\ngenes semantically specify their normal products, in a sense\nsimilar to that seen in some paradigm cases of symbolic phenomena. If genes are seen as “carrying a message” in this\nsense, the message apparently has a prescriptive or imperative\ncontent, as opposed to a descriptive or indicative one. Their\n“direction of fit” to their effects is such that if the\ngenes and the eventual structure produced (the phenotype) do not\nmatch, what we have is a case of unfulfilled instructions rather than\ninaccurate descriptions.  Alternatively, perhaps it is possible to\nthink that the genes are telling the developing phenotype the\nenvironment it can expect. For the gene pool from which those genes\nhave been drawn has been filtered by selection. In arid regions of\nAustralia, genes which contributed to the development of leaves whose\nshape and surface restricted water loss have become common. Hence\nperhaps we can see these genes as telling the trees: conditions will\nbe dry (for this line of thought, see Shea 2011, 2013). In making sense of these ideas, the usual way to proceed has been\nto make use of a rich concept of biological function, in\nwhich the function of an entity derives from a history of natural\nselection (see Teleological Notions in\nBiology).  This move is familiar from the philosophy of mind,\nwhere similar problems arise in the explanation of the semantic\nproperties of mental states (Millikan 1984). When an entity has been\nsubject to and shaped by a history of natural selection, this can\nprovide the grounding for a kind of purposive or normative description\nof the causal capacities of that entity. To use the standard example\n(Wright 1973), the function of a heart is to pump blood, not to make\nthumping sounds, because it is the former effect that has led to\nhearts being favored by natural selection. The hope is that a similar\n“teleofunctional” strategy might help make sense of the\nsemantic properties of genes, and perhaps other biological structures\nwith semantic properties. The idea of a teleosemantic approach to genetic information was\ndeveloped in an early form by Sterelny et al. (1996); see also\nMaclaurin (1998). The eminent biologist John Maynard Smith took a\nsimilar approach, when he tried to make sense of his own enthusiasm\nfor informational concepts in biology (Maynard Smith 2000; and see\nalso the commentaries that follow Maynard Smith’s article). Eva\nJablonka has also defended a version of this idea (Jablonka 2002). Her\ntreatment is more unorthodox, as she seeks to treat environmental\nsignals as having semantic information, along with genes, if they are\nused by the organism in an appropriate way. Nick Shea has developed\nthe most sophisticated teleosemantic treatment of genes to date (see\nShea 2007a,b, 2011, 2013). One way of developing these ideas is to focus on the evolved\nfunctions of the genetic machinery as a whole (Godfrey-Smith\n1999). Carl Bergstrom and Martin Rosvall take this approach,\nemphasising the adapted, impressively engineered features of\nintergenerational gene flow in developing their “transmission\nsense of information”. Bergstron and Rosvall point out that\nintergeneration gene flow is structured so as to make possible the\ntransmission of arbitrary sequences (so the message is relatively\nunconstrained by the medium); that information is stored compactly and\nstably; that the bandwidth is large and indefinitely extendable. DNA\nsequences are reliably replicable with very high fidelity, so\ntransmission is accurate, yet the mapping between DNA and amino acid sequence seems\noptimised to reduce the impact of those errors that do occur. Many\npoint mutations are mapped onto the same or a chemically similar amino\nacid. Bergstrom and Rosvall conclude both that DNA replication is an\nexquisitely designed information channel, and that we can tell this\nfrom the characteristics of the DNA-amino acid system itself. We do\nnot need to know what signals from the parental generation say to the\noffspring generation, in order to know that the characteristics of DNA\nreplication are explained by its information-carrying capacities\n(Bergstrom and Rosvall 2011). The more usual route, and the one taken both by Sterelny et\nal. (1996) and by Maynard Smith (2000) is to focus on the natural\nselection of particular genetic elements. That view faces an\nimmediate problem, for the fact that specific genetic elements (or the\ngenetic system as a whole) have an evolved function is clearly not\nsufficient for genes to carry semantic information.  Legs are for\nwalking, but they do not represent walking. Enzymes are for catalyzing\nreactions, but they do not instruct this activity. There are things\nthat legs and enzymes are supposed to do, but this does not\nmake them into information-carriers, in a rich beyond-Shannon\nsense. Why should it do so for genes? Sterelny, Smith and Dickison\n(1996), suggested that the differences between genes and legs is that\ngenes have been selected to play a causal role\nin developmental processes. They add, however, that any\nnon-genetic factors that have a similar developmental role, and have\nbeen selected to play that role, also have semantic properties. So\nSterelny, Smith and Dickison want to ascribe very rich semantic\nproperties to genes, but not only to genes. Some non-genetic factors\nhave the same status. Even so, many quite plausible cases turn out not\nto be informational on this view: prolactin, like most hormones, does\nnot have a specifically developmental function, so it would not count\nas carrying information to the mammary glands. This is one reason why\nLevy thinks of talk of information as merely metaphorical (Levy\n2011). In contrast to the views of Sterelny, Smith and Dickison, in\nhis 2000 paper, John Maynard Smith argued that only genes carry\nsemantic information about phenotypes. He suggested that in contrast\nto other developmental resources, the relationship between adapted\ngene and phenotypic outcome is arbitrary; the gene-trait relationship\nis like the word-object relationship. This idea is intriguing but hard\nto make precise. One problem is that any causal relation can look\n“arbitrary” if it operates via many intervening links, for\nthere are many possible interventions on those links which would\nchange the product of the causal chain. The problem of arbitrariness\nis discussed further in section 4. To distinguish between ordinary biological functions and\nrepresentational functions, Nicholas Shea draws upon the more\nelaborate machinery of Ruth Millikan’s teleosemantic theory. For\nMillikan, any object that has semantic properties plays a role that\ninvolves a kind of mediation between two “cooperating\ndevices”, a producer and a consumer device. In the case of an\nindicative signal or representation, the representation is supposed to\naffect the activities of the consumer in a way that will only further\nthe performance of the consumer’s biological functions if some\nstate of affairs obtains. In the case of an imperative representation,\nthe representation is supposed to affect the activities of the\nconsumer by inducing them to bring some state of affairs about. Shea\ntreats genetic messages as having both indicative and imperative\ncontent, and depending on both producers and consumers (Shea 2007b,\n2011, 2013). Where Shea follows Millikan in emphasizing the roles of\nboth producers and consumers, Jablonka (2002) tries to achieve as much\nas possible with an emphasis on consumer mechanisms alone. It seems\nclear that the attention to producer and consumer mechanisms is a step\nforward in this discussion, but we will see\nin section 5 that it also poses problems. In\nthinking about both inheritance and development, it turns out to be\nunclear whether there are independently identifiable mechanisms which\ncount as senders and receivers, producers and consumers. The overall picture envisaged by the teleosemantic approach has\nundeniably appealing structural features. If this program succeeds, we\nwould have an uncontroversial sense of information, via Shannon, that\napplies to all sorts of physical correlations. This picture can then\nbe developed by identifying a subset of cases in which these signals\nhave been co-opted or produced to drive biological processes. In\naddition, perhaps we can appeal to rich semantic properties in cases\nwhere we have the right kind of history of natural selection to\nexplain the distinctive role of genes, and perhaps other factors, in\ndevelopment.  Genes and a handful of non-genetic factors would have\nthese properties; most environmental features that have a causal role\nin biological development would not. There remain many problems of\ndetail, but the appeal of the overall picture provides, at least for\nsome, good reason to persevere with some account along these\nlines. So far we have mostly discussed the concept of information; there\nhas not been much talk of “coding”. With the exception of\nour discussion of Bergstrom and Rosvall’s “transmission\nsense” of information, the discussion so far has not emphasised\nthe distinctive features of the cell-level processes in which genes\nfigure, such as the language-like combinatorial structure of the\n“genetic code”. These structural features of DNA and its\nrelation to amino acids are not central to some of the ideas about\ninformation in biology, even when the focus is on development and\ninheritance. As noted above, the enthusiasm for semantic\ncharacterization of biological structures extends back before the\ngenetic code was discovered (see Kay 2000 for a detailed historical\ntreatment). But one line of thought in the literature, overlapping\nwith the ideas above, has focused on the special features of genetic\nmechanisms, and on the idea of “genetic coding” as a\ncontingent feature of these mechanisms. Both Peter Godfrey-Smith and Paul Griffiths have argued that there\nis one highly restricted use of a fairly rich semantic notion within\ngenetics that is justified (Godfrey-Smith 2000; Griffiths 2001).  This\nis the idea that genes “code for” the amino acid sequence\nof protein molecules, in virtue of the peculiar features of the\n“transcription and translation” mechanisms found within\ncells. Genes specify amino acid sequence via a templating process that\ninvolves a regular mapping rule between two quite different kinds of\nmolecules (nucleic acid bases and amino acids). This mapping rule\nis combinatorial, and apparently arbitrary (in a\nsense that is hard to make precise, though see Stegmann 2004 for\ndiscussion of different versions of this idea). Figure 1 below has the standard genetic code\nsummarized. The three-letter abbreviations such as “Phe”\nand “Leu” are types of amino acid molecules. Figure 1. The Standard Genetic\nCode This very narrow understanding of the informational properties of\ngenes is basically in accordance with the influential early proposal\nof Francis Crick (1958). The argument is that these low-level\nmechanistic features make gene expression into a causal process that\nhas significant analogies to paradigmatic symbolic phenomena. Some have argued that this analogy becomes questionable once we\nmove from the genetics of simple prokaryotic organisms (bacteria and\narchaea), to those in eukaryotic cells (Sarkar 1996). Mainstream\nbiology tends to regard the complications that arise in the case of\neukaryotes as mere details that do not compromise the basic picture we\nhave of how gene expression works (for an extensive discussion of\nthese complexities, by those who think they really matter, see\n(Griffiths and Stotz 2013)). An example is the editing and\n“splicing” of mRNA transcripts. The initial stage in gene\nexpression is the use of DNA in a template process to construct an\nintermediate molecule, mRNA or “messenger RNA”, that is\nthen used as a template in the manufacture of a protein. The protein\nis made by stringing a number of amino acid molecules together. In\neukaryotes the mRNA is often extensively modified\n(“edited”) prior to its use. Moreover, much of the DNA in\na eukaryotic organism is not transcribed and translated at all. Some\nof this “non-coding” DNA (note the informational language\nagain) is certainly functional, serving as binding sites for proteins\nthat bind to the DNA thus regulating the timing and rate of\ntranscription. The extent of wholly nonfunctional DNA remains\nunclear. These facts make eukaryotic DNA a much less straightforward\npredictor of the protein’s amino acid sequence than it is in\nbacteria, but it can be argued that this does not much affect the\ncrucial features of gene expression mechanisms that motivate the\nintroduction of a symbolic or semantic mode of description. So the argument in Godfrey-Smith (2000) and Griffiths (2001) is\nthat there is one kind of informational or semantic property that\ngenes and only genes have: coding for the amino acid sequences of\nprotein molecules. But this relation “reaches” only as far\nas the amino acid sequence. It does not vindicate the idea that genes\ncode for whole-organism phenotypes, let alone provide a basis for the\nwholesale use of informational or semantic language in biology.  Genes\ncan have a reliable causal role in the production of a whole-organism\nphenotype, of course. But if this causal relation is to be described\nin informational terms, then it is a matter of ordinary Shannon\ninformation, which applies to environmental factors as well. That\nsaid, it is possible to argue that the specificity of gene action, and\nthe existence of an array of actual and possible alternatives at a\ngiven site on a chromosome, means that genes exert fine-grained causal\ncontrol over phenotypes, and that few other developmental resources\nexert this form of causal control (Waters\n2007; Maclaurin 1998;\nStegmann 2014). In contrast, Griffiths and Stotz\n(2013) say that these\nother factors are often a source of “Crick information”,\nas they contribute to specifying the linear sequence of a gene\nproduct.  We return to this issue in section\n6. One of the most appealing, but potentially problematic, features of\nthe idea that genes code for amino acid sequences concerns the alleged\n“arbitrariness” of the genetic code. The notion of\narbitrariness figures in other discussions of genetic information as\nwell (Maynard Smith 2000; Sarkar 2003; Stegmann 2004). It is common to\nsay that the standard genetic code has arbitrary features, as many\nother mappings between DNA base triplets and amino acids would be\nbiologically possible, if there were compensating changes in the\nmachinery by which “translation” of the genetic message is\nachieved. Francis Crick suggested that the structure of the genetic\ncode should be seen as a “frozen accident”, one that was\ninitially highly contingent but is now very hard for evolution to\nchange (Crick 1958). But the very idea of arbitrariness, and the\nhypothesis of a frozen accident, have become problematic. For one\nthing, as noted in our discussion of Bergstrom and Rosvall (2011)\nabove, the code is not arbitrary in the sense that many others would\nwork as well. To the contrary, the existing code is near-optimal for\nminimising error costs. Conceptually, it seems that any causal\nrelation can look “arbitrary” if it operates via many\nintervening links. There is nothing “arbitrary” about the\nmechanisms by which each molecular binding event occurs. What makes\nthe code seem arbitrary is the fact that the mapping between base\ntriplets and amino acids is mediated by a causal chain with a number\nof intervening links (especially involving “transfer RNA”\nmolecules, and enzymes that bind amino acids to these intervening\nmolecules). Because we often focus on the “long-distance”\nconnection between DNA and protein, the causal relation appears\narbitrary. If we focused on steps in any other biological cascade that\nare separated by three or four intervening links, the causal relation\nwould look just as “arbitrary”. So the very idea of\narbitrariness is elusive. And empirically, the standard genetic code\nis turning out to have more systematic and non-accidental structure\nthan people had once supposed (Knight, Freeland, and Landweber\n1999). The notion of\narbitrariness has also been used in discussions of the links between\ngenes and phenotypes in a more general sense. Kirscher and Gerhart\n(2005) discuss a kind of arbitrariness that derives from the details\nof protein molecules and their relation to gene regulation. Proteins\nthat regulate gene action tend to have distinct binding sites, which\nevolution can change independently. To bind, a protein must be able to\nattach to a site, but that requires congruence only with a local\nfeature of the protein, not sharply constraining its overall shape\n(Planer 2014). This gives rise to a huge range of possible processes\nof gene regulation. So there is a perennial temptation to appeal to\nthe idea of arbitrariness when discussing the alleged informational\nnature of some biological causation. In section 2, we noted that Brian\nSkyrms’ work on signalling systems has made this framework a\nvery natural fit for a quite large range of biological phenomena. As\nwe remarked above, it is very intuitive to see hormones such as\ninsulin, testosterone, and growth hormone as signals, as they are\nproduced in one part of the body, and travel to other parts where they\ninteract with “receptors” in a way that modifies the\nactivities of various other structures. It is routine to describe\nhormones as “chemical messages”. The Skyrms framework fits\nthese designed cause and response systems within biological agents for\nthree reasons. First, as noted, this framework shows that signalling\ndoes not require intelligence or an intelligent grasp of signal\nmeaning. Second, the simplest cases for models of signalling are cases in which there is common interest.\nThe sender and receiver are advantaged or disadvantaged by the same\noutcomes. While complete common interest is atypical of\norganism-to-organism communication, the cells and other structures\nwithin an organism share a common fate (with complex exceptions). So\nin this respect the base models might fit organ-to-organ communication\nbetter than they fit between-organism phenomena. Third, in many of these biological systems, the abstract structure\nspecified as a signalling system—environmental source, sender,\nmessage, receiver, response—maps very naturally onto concrete\nbiological mechanisms. For example, Ron Planer has recently argued\nthat we should see gene expression as the operation of a signalling\nsystem (Planer 2014). His view is quite nuanced, for the identity of\nthe sender, receiver, and message vary somewhat from case to case. For\nexample, when a protein is a transcription factor, then the gene\ncounts as a sender. He treats other gene-protein relations\ndifferently. The details of his view need not concern us here. The\npoint is that there is machinery in the cell—genes, proteins\nmRNA transcripts, ribosomes and their associated tRNA—that can\nbe plausibly mapped onto sender-receiver systems. There is nothing\nforced about mapping the information-processing sender-receiver\nstructure onto the molecular machinery of the cell. However, while this framework very naturally fits within cell and\nbetween cell processes, it is much less clear how naturally other\nsuggestions mesh with this framework. For example, in the\nBergstrom-Rosvall picture of intergenerational transmission, who or\nwhat are the senders and receivers? Perhaps in the case of multicelled\norganisms, the receiver exists independently of and prior to the\nmessage. For an egg is a complex and highly structured system, before\ngene expression begins in the fertilised nucleus, and that structure\nplays an important role in guiding that gene expression (Sterelny\n2009; Gilbert 2013). But most organisms are single celled prokaryotes,\nand when they fission, it is not obvious that there is a daughter who exists prior to and\nindependently of the intergenerational genetic message she is to\nreceive. Likewise, Nicholas Shea’s Millikan-derived analysis does not\nseem to map naturally onto independently specifiable biological\nmechanisms. For him, the sender of genetic messages is natural\nselection operating on and filtering the gene pool of a population;\nmessages are read by the developmental system of the organism as a\nwhole (Shea 2013). But the less clearly a sender-receiver or\nproducer-consumer framework maps onto independently recognised\nbiological mechanisms, the more plausible a fictionalist or\nanalogical analysis of that case becomes. So we see an important\ndifference between a reader being the developmental system as a whole,\nand reader being a receptor on a cell membrane. The cell-level\nmachinery of transcription and translation (the ribosomal/tRNA\nmachinery, especially) really is a reader or consumer of nucleic acid\nsequences, with the function of creating protein products that will\nhave a variety of uses elsewhere in the cell. But this realization of\nthe causal schematism applies only at the cell level, at the\nlevel at which the transcription and translation apparatus shows up as\na definite part of the machinery. One of the most extraordinary\nfeatures of ontogeny is that it proceeds reliably and predictably\nwithout any central control of the development of the organism as a\nwhole. There is nothing, for example, that checks whether the\nleft-side limbs are the same size as the right side limbs, intervening\nto ensure symmetry. Similarly, there are DNA sequence-readers, and some\nintercellular sender-receiver systems with clear “readers”\n(such as neural structures), but there are no higher-level readers\nthat interpret a message specifying the structure of a whole limb-bud in the embryo. Some biologists and philosophers have argued that the introduction\nof informational and semantic concepts has had a bad effect on\nbiology, that it fosters various kinds of explanatory illusions and\ndistortions, perhaps along with ontological confusion. Here we will\nsurvey some of the more emphatic claims of this kind, but some degree\nof unease can be detected in many other discussions (see, for example,\nGriesemer 2005). The movement known as Developmental Systems Theory (DST) has often\nopposed the mainstream uses of informational concepts in biology,\nlargely because of the idea that these concepts distort our\nunderstanding of the causal processes in which genes are involved. For\na seminal discussion, see Oyama (1985), and also Lehrman (1970);\nGriffiths and Gray (1994); Griffiths and Neumann-Held (1999). These\ntheorists have two connected objections to the biological use of\ninformational notions.  One is the idea that informational models are\npreformationist. Preformationism, in its original form, in effect\nreduces development to growth: within the fertilized egg there exists\na miniature form of the adult to come. Preformationism does not\nexplain how an organized, differentiated adult develops from a much\nless organized and more homogeneous egg; it denies the phenomenon.\nDST’s defenders suspect that informational models of development\ndo the same. In supposing, for example, that instructions for a\n“language organ” are coded in the genome of a new-born\nbaby, you do not explain how linguistic abilities can develop in an\norganism that lacks them, and you foster the illusion that no such\nexplanation is necessary. (See Francis 2003 for a particularly\nvigorous version of the idea that the appeal to information leads to\npseudo-explanation in biology.) Second, DST theorists have often endorsed a “parity\nthesis”: genes play an indispensable role in development, but so\ndo other causal factors, and there is no reason to privilege\ngene’s contribution to development. This claim is often\nbuttressed by reference to Richard Lewontin’s arguments for the\ncomplexity and context sensitivity of developmental interaction, and\nhis consequent arguments that we cannot normally partition the causal\nresponsibility of the genetic and the environmental contributions to\nspecific phenotypic outcomes (Lewontin 1974, 2000). DST theorists\nthink that informational models of genes and gene action make it very\ntempting to neglect parity, and to attribute a kind of\ncausal primacy to these factors, even though they are just\none of a set of essential contributors to the process in\nquestion. Once one factor in a complex system is seen in informational\nterms, the other factors tend to be treated as mere background, as\nsupports rather than bona fide causal actors. It becomes\nnatural to think that the genes direct, control, or organise\ndevelopment; other factors provide essential resources. But, the\nargument goes, in biological systems the causal role of genes is in\nfact tightly interconnected with the roles of many other factors\n(often loosely lumped together as “environmental”).\nSometimes a gene will have a reliable effect against a wide range of\nenvironmental backgrounds; sometimes an environmental factor will have\na reliable effect against a wide range of genetic\nbackgrounds. Sometimes both genetic and environmental causes are\nhighly context-sensitive in their operation. Paul Griffiths has\nemphasised this issue, arguing that the informational mode of\ndescribing genes can foster the appearance of\ncontext-independence:\n Genes are instructions—they provide\ninformation—whilst other causal factors are merely\nmaterial…. A gay gene is an instruction to be gay even when\n[because of other factors] the person is straight. (Griffiths 2001:\n395–96) The inferential habits and associations that tend to go along with\nthe use of informational or semantic concepts are claimed to lead us to think of\ngenes as having an additional and subtle form of extra causal\nspecificity. These habits can have an effect even when people are\nwilling to overtly accept context-dependence of (most) causes in\ncomplex biological systems. So DST theorists suggest that it is\nmisleading to treat genes and only genes as carrying\n“messages” that are expressed in their effects. To say\nthis is almost inevitably to treat environmental factors as secondary\nplayers. The parity thesis has been the focus of considerable discussion and\nresponse. In a helpful paper, Ulrich Stegmann shows that the parity\nthesis is really a cluster of theses rather than a single thesis\n(Stegmann 2012). Some ways of interpreting parity make the idea quite\nuncontroversial, as no more than an insistence on the complex and\ninteractive character of development, or as pointing to the fact that\njust as genes come in slightly different versions, with slightly\ndifferent effects (holding other factors constant), the same is true\nof nongenetic factors. Epigenetic markers on genes, due to nutritional\nenvironments, litter position and birth order, may also come in\nslightly different variants, with slightly different effects. Other\nversions of the claim are much more controversial. One response to the parity thesis has been to accept the view that\ngenes are just one of a set of individually necessary and collectively\nsufficient developmental factors, but to argue nonetheless that genes\nplay both a distinctive and especially important role in development\n(Austin 2015; Lean 2014;\nPlaner 2014). As noted above, perhaps the most promising suggestion\nalong these lines is that genes exert a form of causal control over\ndevelopment that is universal, pervasive and fine-grained. Many\nfeatures of the phenotypes of every organism exist in an array of\nsomewhat different versions, as a result of allelic variation in\ncausally relevant genes. No other developmental factor exerts control\nthat is similarly universal, pervasive and fine-grained (Woodward\n2010; Stegmann 2014). Thus Stegmann illustrates Woodward’s idea\nthat genes exert specific control over phenotypes by contrasting the\neffects of intervening on, say, the quantity of polymerase on cell\nactivity with intervening on the DNA template itself. Polymerase is\ncritically causally important, but varying its concentration will\nmodify the rate of synthesis, but not the sequences produced. That is\nnot true of modifications of the DNA sequence itself, so the DNA\nsequence is more causally specific than polymerase. Shea takes a different approach, arguing that different causal\nfactors have different evolutionary histories. Some causal factors are\nsimply persisting features of the environment (gravity being one\nexample). Others are experienced by the developing organism as a\nresult of histories of selection. Burrows, for example, ensure that\neggs and nestlings develop in fairly constant temperature and\nhumidity. But burrows are not naturally selected inheritance\nmechanisms. They have not come into existence to ensure that a seabird\nchick resembles its parents. In contrast, some other developmental\nfeatures are present and act in development because of histories of\nselection in which the selective advantage is that these mechanisms\nhelp ensure parent-offspring similarity. Shea argues that genes,\nprobably epigenetic markings on genes, and perhaps a few other\ndevelopmental resources are shaped by this form of natural\nselection. So genes, and perhaps a few other developmental factors,\nplay a distinctive developmental role, even though many other factors\nare causally necessary (Shea 2011). In sum then, there are good reasons to be cautious about the use of\ninformational terminology in thinking about development. But it is\nalso possible to over-estimate the strength of the connection between\ninformational conceptions of development and the idea that genes play\na uniquely important role in development. There are ways of defending\nthe idea that genes play a special role while acknowledging the\ninteractive character of development. Moreover, an ambitious use of\ninformational concepts is not confined to those within mainstream\nbiological thinking. Eva Jablonka and Marion Lamb defend quite\nheterodox views of inheritance and evolution, while basing key parts\nof their work—including an advocacy of “Lamarckian”\nideas—around informational concepts (Jablonka and Lamb\n2005). They suggest that one of the useful features of informational\ndescriptions is that they allow us to generalize across different\nheredity systems, comparing their properties in a common currency. In\naddition, one of the present authors has used informational concepts\nto distinguish between the evolutionary role of genes from that of\nother inherited factors whilst demonstrating the evolutionary\nimportance of non-genetic inheritance (Sterelny 2004, 2011). So in\nvarious ways, an informational point of view may facilitate discussion\nof unorthodox theoretical options, including non-genetic mechanisms of\ninheritance. Talk of genetic “programs” is common both in popular\npresentations of biology, and in biology itself. Often, the idea is\njust a vivid (but perhaps misleading) way of drawing attention to the\norderly, well-controlled and highly structured character of\ndevelopment. In its overall results, development is astonishingly\nstable and predictable, despite the extraordinary complexity of\nintracellular and intercellular interactions, and despite the fact\nthat the physical context in which development takes place can never\nbe precisely controlled. So when biologists speak, for example, of\n“programmed cell death”, they could just as well say that\nin an important class of cases, cell death is predictable, organised,\nand adaptive. There are attempts to draw closer and more instructive parallels\nbetween computational systems and biological development. In\nparticular, Roger Sansom has made a sustained and detailed attempt to\ndevelop close and instructive parallels between biological development\nand connectionist computational models (Sansom 2008b,a, 2011). This\nview has the merit of recognising that there is no central control of\ndevelopment; organisms develop as a result of local interactions\nwithin and between cells. However, the most promising ideas about\nprogram-development parallels seem to us to be ones that point to an\napparently close analogy between processes within cells, and\nthe low-level operation of modern computers. One crucial kind\nof causal process within cells is cascades of up and down-regulation\nin genetic networks. One gene will make a product that binds to and\nhence down-regulates another gene, which is then prevented from making\na product that up-regulates another… and so on. What we have\nhere is a cascade of events that can often be described in terms of\nBoolean relationships between variables. One event might only follow\nfrom the conjunction of another two, or from a disjunction of\nthem. Down-regulation is a kind of negation, and there can be double\nand triple negations in a network. Gene regulation networks often have\na rich enough structure of this kind for it to be useful to think of\nthem as engaged in a kind of computation. Computer chip\n“and-gates”, neural “and-gates” and genetic\n“and-gates” have genuine similarities. While talking of signalling networks rather than programs, Brett\nCalcott has shown that positional information in the developing\nfruitfly embryo depends on this kind of Boolean structure, with limb\nbud development depending on the cells that differentiate into limb\nbuds integrating one positive with two negative signals, so the buds\ndevelop in a regular pattern on the anterior midline of the embryo.\nCalcott shows that thinking of development in terms of these\nsignalling networks with their Boolean structures has real explanatory\nvalue, for it enables us to explain how positional information, for\nexample, can easily be reused in evolution. Wing spots on fruitflies\ncan evolve very easily, for the networks that tell cells where they\nare on the wing already exist, so the evolution of the wingspot just\nneed a simple mutational change that links that positional information\nto pigment production (Calcott 2014). Ron Planer agrees that gene\nregulation has this Boolean structure, and that we can, in effect,\nrepresent each gene as instantiating a conditional instruction. The\n“if” part of the conditional specifies the molecular\nconditions which turn the gene on; the “then” part of the\nconditional specifies the amino acid sequence made from the gene. As\nwith Calcott, Planer goes on to point out that these conditional\ninstructions can be and often are, linked together to build complex\nnetworks of control. Even so, Planer argues that while these are\nsignalling networks, they should not be thought of as computer\nprograms. For example, the combinations of instructions have no intrinsic order; we\ncan represent each of the genes as a specific conditional instruction,\nbut there is nothing in the set of instructions itself that tells us\nwhere to begin and end (Planer 2014). Information has also become a focus of general discussion of\nevolutionary processes, especially as they relate to the mechanisms of\ninheritance. One strand of this discussion misconceives information\nand its role in biological processes. In particular, G.C.  Williams\nargues that, via reflection on the role of genes in evolution, we can\ninfer that there is an informational “domain” that exists\nalongside the physical domain of matter and energy (Williams\n1992). Richard Dawkins defends a similar view, arguing that the\nlong-term path of evolution is made up of gradual changes in inherited\ninformation—as a river that “flows through time, not\nspace” (Dawkins 1995: 4). This is an extension of a more common\nidea, that there exists such things as “informational\ngenes” that should be understood as distinct from the\n“material genes” that are made of DNA and localized in\nspace and time (Haig 1997). It is a mistake to think that there are\ntwo different things; that there is both a physical entity—a\nstring of bases—and an informational entity, a message. It is\ntrue that for evolutionary (and many other) purposes genes are often\nbest thought of in terms of their base sequence (the sequence of C, A,\nT and G), not in terms of their full set of material properties. This\nway of thinking is essentially a piece of abstraction (Griesemer\n2005). We rightly ignore some properties of DNA and focus on\nothers. But it is a mistake to treat this abstraction as an extra\nentity, with mysterious relations to the physical domain. Other ways of linking informational ideas to general issues in\nevolutionary theory seem more promising. As John Maynard Smith, Eors\nSzathmáry, Mark Ridley and Richard Dawkins have emphasized in\ndifferent ways, inheritance mechanisms that give rise to significant\nevolutionary outcomes must satisfy some rather special conditions\n(Dawkins 1995; Jablonka and Szathmáry 1995; Szathmáry and Maynard\nSmith 1997; Ridley 2000). Maynard Smith and Szathmáry claim, for\nexample, that the inheritance system must be unlimited or\n“indefinite” in its capacity to produce new combinations,\nbut must also maintain high fidelity of transmission. This fact about\nthe relationship between inheritance systems and biological structure\nis often thought to reveal one of the most pressing problems in\nexplaining the origins of life. If reproduction depends on the\nreplication of a crucial set of ingredients to kick-start the new\ngeneration (whether or not we think of those ingredients as\ninstructions), these ingredients must be replicated accurately. Yet\naccurate replication apparently depends on complex molecular and\nintracellular machinery that itself is the result of a long regime of\nadaptive evolution, and hence on deep lineages of living systems. So\nhow could reproduction have begun? (see Ridley 2000 for a thoughtful\ndiscussion). So life itself, the argument goes, depends on the evolution of\nmechanisms that support a high fidelity flow of information from one\ngeneration to the next. More ambitiously still, Maynard Smith and\nSzathmáry argue that many of the crucial steps in the last four\nbillion years of evolution—their “major transitions in\nevolution”—involve the creation of new ways of\ntransmitting information across generations—more reliable, more\nfine-grained, and more powerful ways of making possible the reliable\nre-creation of form across events of biological reproduction. The\ntransition to a DNA-based inheritance system (probably from a system\nbased on RNA) is one central example. But Maynard Smith and Szathmáry\nsuggest that the transition from great ape forms of social life to\nhuman social life is a major transition, in part because of the novel\nforms of large scale cooperation that typify human social life, but\nmostly because they see human language as a breakthrough informational\ntechnology, revolutionising the possibilities of high fidelity\nintergenerational cultural learning (MacArthur 1958; Maynard Smith and\nSzathmáry 1995, 1999). Maynard Smith and Szathmáry’s work on major transitions has been a\nlandmark in macroevolutionary thinking—in thinking about large\nscale patterns in the history of life. But the informational dimension\nof their work has not been taken up. A minor exception is Sterelny\n(2009). This paper argued that multicelled animal life depended not\njust on the transmission of more information with high fidelity, but\non the control of that information in development, suggesting that the\nevolution of the egg—a controlled, structured, information-rich\ndevelopmental environment—was critical to complex animal\nlife. But most focus has been on the other strand of their work on\nmajor transitions: on the solution of cooperation problems that then\nallow previously independent agents to combine into new, more complex\nagents. Thus in Calcott and Sterelny (2011), none of the papers\nfocused primarily on the expansion or control of intergenerational\ninformation flow. That may change. The evolutionarily crucial features\nof inheritance mechanisms are often now discussed in informational\nterms, and the combinatorial structure seen in both language and DNA\nprovides a powerful basis for analogical reasoning.","contact.mail":"pgodfreysmith@gmail.com","contact.domain":"gmail.com"},{"date.published":"2007-10-04","date.changed":"2016-03-23","url":"https://plato.stanford.edu/entries/information-biological/","author1":"Peter Godfrey-Smith","author1.info":"http://www.petergodfreysmith.com/","author2.info":"http://www.victoria.ac.nz/hppi/about/staff/kim-sterelny","entry":"information-biological","body.text":"\n\nSince the 1950s, the concept of information has acquired a\nstrikingly prominent role in many parts of biology. This enthusiasm\nextends far beyond domains where the concept might seem to have an\nobvious application, such as the biological study of perception,\ncognition, and language, and now reaches into the most basic parts of\nbiological theory. Hormones and other cellular products through which\nphysiological systems are regulated are typically described as\nsignals.  Descriptions of how genes play their causal role in\nmetabolic processes and development are routinely given in terms of\n“transcription”, “translation”, and\n“editing”. The most general term used for the processes by\nwhich genes exert their effects is “gene expression”. The\nfates of cells in a developing organism are explained in terms of\ntheir processing of “positional information” given to them\nfrom surrounding cells and other factors. Many biologists think of the\ndevelopmental processes by which organisms progress from egg to adult\nin terms of the execution of a “developmental\nprogram”. Other biologists have argued for a pivotal role for\ninformation in evolution rather than development: John Maynard Smith\nand Eors Szathmáry (for example) suggest that major transitions in\nevolution depend on expansions in the amount and accuracy with which\ninformation is transmitted across generations. And some have argued\nthat we can only understand the evolutionary role of genes by\nrecognizing an informational “domain” that exists\nalongside the domain of matter and energy. \n\nBoth philosophers and biologists have contributed to an ongoing\nfoundational discussion of the status of this mode of description in\nbiology. It is generally agreed that the sense of information isolated\nby Claude Shannon and used in mathematical information theory is\nlegitimate, useful, and relevant in many parts of biology. In this\nsense, anything is a source of information if it has a range\nof possible states, and one variable carries information\nabout another to the extent that their states are physically\ncorrelated. But it is also agreed that many uses of informational\nlanguage in biology seem to make use of a richer and more problematic\nconcept than Shannon’s. Some have drawn on\nthe teleosemantic tradition in philosophy of mind to make\nsense of this richer concept.  Other theorists have countered that Shannon’s correlational conception of information is richer than it looks.\n\nA minority tradition has argued that the enthusiasm for information\nin biology has been a serious theoretical wrong turn, and that it\nfosters naive genetic determinism, other distortions of our\nunderstanding of the roles of interacting causes, or an implicitly\ndualist ontology. However, this sceptical response is fading, with key\nsceptics coming to accept a modest but genuine role for informational\nconcepts in the life sciences. Others have taken the critique seriously but\ntried to distinguish legitimate appeals to information from misleading\nor erroneous ones.\n\nBiology is concerned with living organisms—with their\nstructure, activities, distribution in space and time, and\nparticipation in evolutionary and developmental histories. Many of\nthese organisms engage in activities that seem best understood in\nterms of information processing or representation. These include\nperception, cognition, signalling, and language use. We will not much\nbe concerned with the role of concepts of information and\nrepresentation in these cognitive contexts. For over the second half\nof the 20th century, biology came to apply informational\nconcepts (and their relatives) far more broadly than this. For many\nbiologists, the most basic processes characteristic of living\norganisms should now be understood in terms of the expression of\ninformation: the response to signals, the execution of programs, and\nthe interpretation of codes. So although contemporary mainstream\nbiology is an overtly materialist field, it has come to employ\nconcepts that seem intentional or semantic. These come with a long\nhistory of causing foundational problems for materialists (and, to\nsome extent, for everyone else). The embrace of informational and other semantic concepts has been\nespecially marked within genetics, and other fields—evolutionary\ntheory and developmental biology—with strong ties to\ngenetics. But informational language is not only found there: for\nexample, hormones are routinely seen as long\ndistance signals that enable one organ system to coordinate with\nothers (Levy 2011). That said, the usage that has generated the most\ndiscussion is found in the description of the relations between genes\nand the various structures and processes to which genes\ncontribute. For many biologists, the causal role of genes should be\nunderstood in terms of their carrying information about their\nvarious products; and perhaps as well about the environments in which\nthese products enhance fitness (Lorenz 1965; Shea 2013). The\nexpression of that information might depend on the presence of various\nenvironmental factors, but the same can be said of other kinds of\nmessage. Breaking things down further, we can recognize two causal\nroles for genes, and hence two potential explanatory roles for genetic\ninformation, within biology. Genes are crucial to both explaining the\ndevelopment of individual organisms, and to explaining the inheritance\nof characteristics across generations. Information has been invoked in\nboth explanatory contexts. One important use of informational language is relatively\nuncontroversial, through being anchored in striking and well\nestablished facts about the role of DNA and RNA in the manufacture of\nprotein molecules in cells, a set of facts summarized in the familiar\nchart representing the “genetic code”, that maps triplets\nof the DNA bases (C, A, T, G) to individual amino acids, which are the\nbuilding blocks of protein molecules. Not even this use\nof “coding” language is completely uncontroversial (Sarkar\n1996). More importantly, enthusiasm for the use of informational\nlanguage in biology both predates the discovery of the DNA-RNA-amino\nacid mapping (Schrödinger 1944), and goes far beyond it, so this\nmapping is only a partial explanation of the uptake of informational\nnotions in biology. Current applications of informational concepts in\nbiology include: There is no consensus about the status of these ideas. Indeed, the\nuse of informational notions is controversial even when giving\naccounts of animal communication, with some theorists denying that\nsuch communication is the flow of information from one animal to\nanother (Krebs and Dawkins 1984; Owren et al. 2010). The result has\nbeen a growing foundational discussion within biology and the\nphilosophy of biology. Some have hailed the employment of\ninformational concepts as a crucial advance (Williams 1992). Others\nhave seen almost every biological application of informational\nconcepts as a serious error, one that distorts our understanding and\ncontributes to lingering genetic determinism (Francis 2003). Most of\nthe possible options between these extreme views have also been\ndefended. Perhaps most commentators within philosophy have seen their\nproject as one of sorting through the various kinds of informational\ndescription that have become current, distinguishing legitimate ones\nfrom illegitimate ones (Godfrey-Smith 2000; Griffiths 2001;\nGodfrey-Smith 2007; Shea 2013; Lean 2014). Philosophers have also\ntried to give a reductive or naturalistic explanation for the\nlegitimate ones. This entry proceeds as follows. In the next section we discuss the\nmost unproblematic technical use of information in biology, which\ndraws on Shannon and the mathematical theory of information. Against\nthat background, some of the more contentious uses are both motivated\nand introduced. In section 3, we discuss\nrecent attempts to build a richer concept out of Shannon’s\ntechnical notion. We then discuss the status of the “genetic\ncode” in its original sense (section 4),\nand signalling systems inside and outside of genetics\n(section 5). We then look at ways of rejecting\ninformational forms of description (section\n6). The key sceptical idea is the so-called “parity\nthesis”: the idea that while genes play an immensely important\nrole in evolution and development, so do other factors, and so it is a\nmistake to think of genes and only genes as instructing or controlling\nthese processes. The final two sections discuss the idea that\nbiological development in individuals proceeds by the execution of a\nprogram (section 7), and the use of information\nin understanding the role of inheritance systems in the “major\ntransitions” in evolution (section 8). It is common to begin the analysis of information in biology with\nan uncontroversial but minimal notion: a causal or correlational\nconception. Smoke in the air and fire in the forest are correlated\nwith one another. If you see a forest burning at night, you can\npredict the air will be smoky. If you see a plume of smoke rising from\nabove the trees, you can predict that below there is a fire. The\ncorrelation does not have to be perfect to be informative. Putting the\npoint generally, a signal carries information about a source, in this\nsense, if we can predict the state of the source from the signal. This\nsense of information is associated with Claude Shannon (1948) who\nshowed how the concept of information could be used\nto quantify facts about contingency and correlation in a\nuseful way, initially for use in communication technology. For\nShannon, anything is a source of information if it has a\nnumber of alternative states that might be realized on a particular\noccasion. Any other variable contains some information about\nthat source, or carries information about it, if its state is correlated with the state of the source. This is a matter of degree; a signal carries more information\nabout a source if its state is a better predictor of the source, less\ninformation if it is a worse predictor.  In this sense, a signal can carry information about a source\nwithout there being any biological system designed to produce that\nsignal, nor any to use it once produced. When a biologist employs this\nsense of information in a description of gene action or of other\nprocesses, he or she is just adopting a quantitative framework for\ndescribing ordinary correlations or causal connections within those\nsystems. So it is one thing for there to be a signal carrying\ninformation about a source; quite another to explain biological\nprocesses as the result of signalling. The familiar example of tree\nrings is helpful here. When a tree lays down rings, it establishes a\nstructure that can be used by us to make inferences about the\npast. The number and size of the rings carry information, in\nShannon’s sense, about the history of the tree and its\ncircumstances (the technique, “dendrochronology”, is\nscientifically important). Despite the usefulness of the informational\ndescription, there is no sense in which we are explaining how\nthe tree does what it does in informational terms. The only reader or\nuser of the information in the tree rings is the human observer. The\ntree itself does not exploit the information in its rings to control\nits growth or flowering. Similarly, we might note ways in which the\ndistribution of different DNA sequences within and between biological\npopulations “carries information about” the historical\nrelationships between those populations, and the histories of the\nindividual populations themselves. Using this information has given\nevolutionary biologists a much richer and more reliable picture of the\nhistory of life (for a good introduction to these uses see Bromham\n2008). It has been argued, for example, that the greater diversity in\nmitochondrial DNA in African populations, in comparison to other human\npopulations, is an indicator of a comparatively recent human dispersal\nfrom an African origin. In making these inferences, informational\nconcepts can be useful tools. But this is just a more complicated\nversion of what is going on in the case of tree rings. The appeal to\ninformation has an inferential use that is no\nway explanatory. A large proportion of the informational\ndescriptions found in biology have this character. The border between these two categories, however, can be hard to\ndiscern. Steven Frank (2012) has developed a detailed mapping between\nequations describing change due to natural selection and equations\nused in information theory. Change across generations can be seen as\nthe accumulation of information by a population about its\nenvironment. So far, there is no claim that populations use this\ninformation in any way, and the situation is akin to that found in\ntree rings: a tree accumulates, in its rings, information about\nclimatic conditions in its past, though the tree does nothing with\nthat information. Frank may view the role of information acquired by\nevolution in a more substantive way: “selection causes\npopulations to accumulate information about the fit of characters to\nthe environment” (2012: 2391). But it is doubtful that anything more\nthan an inferential role has yet been shown for this kind of\ninformation. Consequently, philosophers have sometimes set the discussion up by\nsaying that there is one kind of “information” appealed to\nin biology, the kind originally described by Shannon, that is\nunproblematic and does not require much philosophical attention. The\nterm “causal” information is also sometimes used to refer\nto this concept, and this is essentially Grice’s “natural\nmeaning”, (see entry on Grice) from his\nwork in the philosophy of language (Grice 1957). Information in this\nsense exists whenever there is contingency and correlation. So we can\nsay that genes contain information about the proteins they make, and\nalso that genes contain information about the whole-organism\nphenotype. But when we say that, we are saying no more than what we\nare saying when we say that there is an informational connection\nbetween smoke and fire, or between tree rings and a tree’s\nage. The more contentious question becomes whether or not biology\nneeds another, richer concept of information as\nwell. Information in this richer sense is sometimes called\n“semantic” or “intentional” information. Why might we think that biology needs to employ a richer concept?\nOne thought is that genes play a special, instructional role in\ndevelopment, telling the embryo how to grow. It is true that genes\ncarry Shannon-information about phenotypes: the genome of a fertilized\negg predicts much of the resulting phenotype. In mammals, for example,\nchromosome structure predicts the sex of the adult animal. But if an\ninformational relationship between gene and phenotype is supposed to\ninvolve a distinctive instruction-like mode of causation, then this\ncannot be information in Shannon’s sense.  For environmental\nfactors, not just genes, “carry information” about\nphenotypes, in Shannon’s sense. With respect to Shannon\ninformation, there is a “parity” between the roles of\nenvironmental and genetic causes (Griffiths and Gray 1994). Moreover,\nif we are using Shannon’s concept, then the informational\nrelationship between genotype and phenotype is symmetrical. For\nexample, once a reader knows that both authors of this article are\nmale, they can predict that we both carry a Y-chromosome.  Some talk\nabout information in biology is consistent with these features of\nShannon information, but some is not. In particular, it is usually\nthought that at least some applications of informational language to\ngenes ascribe to them a property that is not ascribed to\nenvironmental conditions, even when the environment is predictively\nimportant. In addition, a message that carries “semantic\ninformation”, it is often thought, has the capacity\nto mis-represent, as well as accurately represent, what it is\nabout. There is a capacity for error. Shannon information does not\nhave that feature; we cannot say that one variable “carried\nfalse information” about another, if we are using Shannon’s\nsense of the term. But biologists do apparently want to use language\nof that kind when talking about genes. Genes carry a message that\nis supposed to be expressed, whether or not it actually is\nexpressed. Thus the developmental disorders caused by thalidomide in\nthe 1960s were due to an inability to use genetic signals\nthat were supposed to control limb development. These genes\nwere present and active; thalidomide did not do its harm by causing\nmutations. These are the usual “marks” that are taken in the\nliterature to show that a richer sense of information than\nShannon’s has been introduced to biology. But the most crucial\ndifference between the less and more contentious applications of\ninformational concepts is that, in the richer cases, information use\nis supposed to help explain how biological systems do what\nthey do—how cells work, how an egg can develop into an adult,\nhow genetic inheritance mechanisms make the evolution of complex\nphenotypes possible. At this point, there are a number of options on the table. One is\nto deny that genes, cells and other biological structures literally\ntraffic in information in ways that explain their behaviour, but to\nargue nevertheless that this is a useful analogy or model. The idea is\nthat there are useful similarities between paradigm cases of information and\nrepresentation using systems—cognitively sophisticated agents,\nthinking and communicating with one another—and biological\nsystems. Hormones, for example, are usefully thought of as messages\nbecause they are small, stable, energetically inexpensive items that\ncan travel long distances (relative to their own size) without\ndecaying, to specialized locations where they have\npredictable, specific, and important effects on arrival. But while\nthis is a helpful way of thinking, perhaps we should not take talk of\nmessages too seriously. For example, we should not treat a question\nabout whether prolactin is a report of pregnancy or an instruction to\nthe mammary glands as if it had a literally correct answer. Arnon Levy\nhas developed the most sophisticated version of this view of\nbiological information (Levy 2011).  A second option is to argue that genes and other biological\nstructures literally carry semantic information, and their\ninformational character explains the distinctive role of these\nstructures in biological processes. If we think of genes or cells as\nliterally carrying semantic information, our problem changes; who or\nwhat could count as composing or reading these messages? Paradigm\ncases of structures with semantic information—pictures,\nsentences, programs—are built by the thought and action of\nintelligent agents. So we need to show how genes and\ncells—neither intelligent systems themselves nor the products of\nintelligence—can carry semantic information, and how the\ninformation they carry explains their biological role. We need some\nkind of reductive explanation of semantic information (arguably, we\nneed this to understand cognition, too). One place we might look for\nsuch an analysis is naturalistic philosophy of mind. A third option is to argue that causal information itself can\nexplain biological phenomena, and no additional concept of information\nis needed. Biological systems, on this view, can be adapted to send or\nreceive signals that carry causal information. Elevated prolactin, for\nexample, does covary with pregnancy, and that is no\naccident. Likewise, the production of milk is a designed response to\nthe registration of these elevated levels at the mammary glands. While\nit is true that lactation carries as much causal information about\nprolactin levels as those levels carry about milk flow, there is a\nphysical asymmetry, hence directionality, between source and\nreceiver. (After birth, suckling stimulates local production of\nprolactin, so the physical asymmetry is the signal of pregnancy from\nthe pituitary gland to the breasts, tuning them for lactation). Brian\nSkyrms’ work has been very important in returning causal\ninformation to centre stage, in part because his work shows that\nsender-receiver signalling systems need not be cognitively\nsophisticated agents (Skyrms 2010). Sending and receiving causal\ninformation can emerge and stabilise amongst simple systems; certainly\namongst systems no more complex than a cell. As noted above, several philosophers and biologists have argued\nthat much informational talk about genes uses a richer concept than\nShannon’s, but this concept can be given a naturalistic\nanalysis.  The aim has been to make sense of the idea that\ngenes semantically specify their normal products, in a sense\nsimilar to that seen in some paradigm cases of symbolic phenomena. If genes are seen as “carrying a message” in this\nsense, the message apparently has a prescriptive or imperative\ncontent, as opposed to a descriptive or indicative one. Their\n“direction of fit” to their effects is such that if the\ngenes and the eventual structure produced (the phenotype) do not\nmatch, what we have is a case of unfulfilled instructions rather than\ninaccurate descriptions.  Alternatively, perhaps it is possible to\nthink that the genes are telling the developing phenotype the\nenvironment it can expect. For the gene pool from which those genes\nhave been drawn has been filtered by selection. In arid regions of\nAustralia, genes which contributed to the development of leaves whose\nshape and surface restricted water loss have become common. Hence\nperhaps we can see these genes as telling the trees: conditions will\nbe dry (for this line of thought, see Shea 2011, 2013). In making sense of these ideas, the usual way to proceed has been\nto make use of a rich concept of biological function, in\nwhich the function of an entity derives from a history of natural\nselection (see Teleological Notions in\nBiology).  This move is familiar from the philosophy of mind,\nwhere similar problems arise in the explanation of the semantic\nproperties of mental states (Millikan 1984). When an entity has been\nsubject to and shaped by a history of natural selection, this can\nprovide the grounding for a kind of purposive or normative description\nof the causal capacities of that entity. To use the standard example\n(Wright 1973), the function of a heart is to pump blood, not to make\nthumping sounds, because it is the former effect that has led to\nhearts being favored by natural selection. The hope is that a similar\n“teleofunctional” strategy might help make sense of the\nsemantic properties of genes, and perhaps other biological structures\nwith semantic properties. The idea of a teleosemantic approach to genetic information was\ndeveloped in an early form by Sterelny et al. (1996); see also\nMaclaurin (1998). The eminent biologist John Maynard Smith took a\nsimilar approach, when he tried to make sense of his own enthusiasm\nfor informational concepts in biology (Maynard Smith 2000; and see\nalso the commentaries that follow Maynard Smith’s article). Eva\nJablonka has also defended a version of this idea (Jablonka 2002). Her\ntreatment is more unorthodox, as she seeks to treat environmental\nsignals as having semantic information, along with genes, if they are\nused by the organism in an appropriate way. Nick Shea has developed\nthe most sophisticated teleosemantic treatment of genes to date (see\nShea 2007a,b, 2011, 2013). One way of developing these ideas is to focus on the evolved\nfunctions of the genetic machinery as a whole (Godfrey-Smith\n1999). Carl Bergstrom and Martin Rosvall take this approach,\nemphasising the adapted, impressively engineered features of\nintergenerational gene flow in developing their “transmission\nsense of information”. Bergstron and Rosvall point out that\nintergeneration gene flow is structured so as to make possible the\ntransmission of arbitrary sequences (so the message is relatively\nunconstrained by the medium); that information is stored compactly and\nstably; that the bandwidth is large and indefinitely extendable. DNA\nsequences are reliably replicable with very high fidelity, so\ntransmission is accurate, yet the mapping between DNA and amino acid sequence seems\noptimised to reduce the impact of those errors that do occur. Many\npoint mutations are mapped onto the same or a chemically similar amino\nacid. Bergstrom and Rosvall conclude both that DNA replication is an\nexquisitely designed information channel, and that we can tell this\nfrom the characteristics of the DNA-amino acid system itself. We do\nnot need to know what signals from the parental generation say to the\noffspring generation, in order to know that the characteristics of DNA\nreplication are explained by its information-carrying capacities\n(Bergstrom and Rosvall 2011). The more usual route, and the one taken both by Sterelny et\nal. (1996) and by Maynard Smith (2000) is to focus on the natural\nselection of particular genetic elements. That view faces an\nimmediate problem, for the fact that specific genetic elements (or the\ngenetic system as a whole) have an evolved function is clearly not\nsufficient for genes to carry semantic information.  Legs are for\nwalking, but they do not represent walking. Enzymes are for catalyzing\nreactions, but they do not instruct this activity. There are things\nthat legs and enzymes are supposed to do, but this does not\nmake them into information-carriers, in a rich beyond-Shannon\nsense. Why should it do so for genes? Sterelny, Smith and Dickison\n(1996), suggested that the differences between genes and legs is that\ngenes have been selected to play a causal role\nin developmental processes. They add, however, that any\nnon-genetic factors that have a similar developmental role, and have\nbeen selected to play that role, also have semantic properties. So\nSterelny, Smith and Dickison want to ascribe very rich semantic\nproperties to genes, but not only to genes. Some non-genetic factors\nhave the same status. Even so, many quite plausible cases turn out not\nto be informational on this view: prolactin, like most hormones, does\nnot have a specifically developmental function, so it would not count\nas carrying information to the mammary glands. This is one reason why\nLevy thinks of talk of information as merely metaphorical (Levy\n2011). In contrast to the views of Sterelny, Smith and Dickison, in\nhis 2000 paper, John Maynard Smith argued that only genes carry\nsemantic information about phenotypes. He suggested that in contrast\nto other developmental resources, the relationship between adapted\ngene and phenotypic outcome is arbitrary; the gene-trait relationship\nis like the word-object relationship. This idea is intriguing but hard\nto make precise. One problem is that any causal relation can look\n“arbitrary” if it operates via many intervening links, for\nthere are many possible interventions on those links which would\nchange the product of the causal chain. The problem of arbitrariness\nis discussed further in section 4. To distinguish between ordinary biological functions and\nrepresentational functions, Nicholas Shea draws upon the more\nelaborate machinery of Ruth Millikan’s teleosemantic theory. For\nMillikan, any object that has semantic properties plays a role that\ninvolves a kind of mediation between two “cooperating\ndevices”, a producer and a consumer device. In the case of an\nindicative signal or representation, the representation is supposed to\naffect the activities of the consumer in a way that will only further\nthe performance of the consumer’s biological functions if some\nstate of affairs obtains. In the case of an imperative representation,\nthe representation is supposed to affect the activities of the\nconsumer by inducing them to bring some state of affairs about. Shea\ntreats genetic messages as having both indicative and imperative\ncontent, and depending on both producers and consumers (Shea 2007b,\n2011, 2013). Where Shea follows Millikan in emphasizing the roles of\nboth producers and consumers, Jablonka (2002) tries to achieve as much\nas possible with an emphasis on consumer mechanisms alone. It seems\nclear that the attention to producer and consumer mechanisms is a step\nforward in this discussion, but we will see\nin section 5 that it also poses problems. In\nthinking about both inheritance and development, it turns out to be\nunclear whether there are independently identifiable mechanisms which\ncount as senders and receivers, producers and consumers. The overall picture envisaged by the teleosemantic approach has\nundeniably appealing structural features. If this program succeeds, we\nwould have an uncontroversial sense of information, via Shannon, that\napplies to all sorts of physical correlations. This picture can then\nbe developed by identifying a subset of cases in which these signals\nhave been co-opted or produced to drive biological processes. In\naddition, perhaps we can appeal to rich semantic properties in cases\nwhere we have the right kind of history of natural selection to\nexplain the distinctive role of genes, and perhaps other factors, in\ndevelopment.  Genes and a handful of non-genetic factors would have\nthese properties; most environmental features that have a causal role\nin biological development would not. There remain many problems of\ndetail, but the appeal of the overall picture provides, at least for\nsome, good reason to persevere with some account along these\nlines. So far we have mostly discussed the concept of information; there\nhas not been much talk of “coding”. With the exception of\nour discussion of Bergstrom and Rosvall’s “transmission\nsense” of information, the discussion so far has not emphasised\nthe distinctive features of the cell-level processes in which genes\nfigure, such as the language-like combinatorial structure of the\n“genetic code”. These structural features of DNA and its\nrelation to amino acids are not central to some of the ideas about\ninformation in biology, even when the focus is on development and\ninheritance. As noted above, the enthusiasm for semantic\ncharacterization of biological structures extends back before the\ngenetic code was discovered (see Kay 2000 for a detailed historical\ntreatment). But one line of thought in the literature, overlapping\nwith the ideas above, has focused on the special features of genetic\nmechanisms, and on the idea of “genetic coding” as a\ncontingent feature of these mechanisms. Both Peter Godfrey-Smith and Paul Griffiths have argued that there\nis one highly restricted use of a fairly rich semantic notion within\ngenetics that is justified (Godfrey-Smith 2000; Griffiths 2001).  This\nis the idea that genes “code for” the amino acid sequence\nof protein molecules, in virtue of the peculiar features of the\n“transcription and translation” mechanisms found within\ncells. Genes specify amino acid sequence via a templating process that\ninvolves a regular mapping rule between two quite different kinds of\nmolecules (nucleic acid bases and amino acids). This mapping rule\nis combinatorial, and apparently arbitrary (in a\nsense that is hard to make precise, though see Stegmann 2004 for\ndiscussion of different versions of this idea). Figure 1 below has the standard genetic code\nsummarized. The three-letter abbreviations such as “Phe”\nand “Leu” are types of amino acid molecules. Figure 1. The Standard Genetic\nCode This very narrow understanding of the informational properties of\ngenes is basically in accordance with the influential early proposal\nof Francis Crick (1958). The argument is that these low-level\nmechanistic features make gene expression into a causal process that\nhas significant analogies to paradigmatic symbolic phenomena. Some have argued that this analogy becomes questionable once we\nmove from the genetics of simple prokaryotic organisms (bacteria and\narchaea), to those in eukaryotic cells (Sarkar 1996). Mainstream\nbiology tends to regard the complications that arise in the case of\neukaryotes as mere details that do not compromise the basic picture we\nhave of how gene expression works (for an extensive discussion of\nthese complexities, by those who think they really matter, see\n(Griffiths and Stotz 2013)). An example is the editing and\n“splicing” of mRNA transcripts. The initial stage in gene\nexpression is the use of DNA in a template process to construct an\nintermediate molecule, mRNA or “messenger RNA”, that is\nthen used as a template in the manufacture of a protein. The protein\nis made by stringing a number of amino acid molecules together. In\neukaryotes the mRNA is often extensively modified\n(“edited”) prior to its use. Moreover, much of the DNA in\na eukaryotic organism is not transcribed and translated at all. Some\nof this “non-coding” DNA (note the informational language\nagain) is certainly functional, serving as binding sites for proteins\nthat bind to the DNA thus regulating the timing and rate of\ntranscription. The extent of wholly nonfunctional DNA remains\nunclear. These facts make eukaryotic DNA a much less straightforward\npredictor of the protein’s amino acid sequence than it is in\nbacteria, but it can be argued that this does not much affect the\ncrucial features of gene expression mechanisms that motivate the\nintroduction of a symbolic or semantic mode of description. So the argument in Godfrey-Smith (2000) and Griffiths (2001) is\nthat there is one kind of informational or semantic property that\ngenes and only genes have: coding for the amino acid sequences of\nprotein molecules. But this relation “reaches” only as far\nas the amino acid sequence. It does not vindicate the idea that genes\ncode for whole-organism phenotypes, let alone provide a basis for the\nwholesale use of informational or semantic language in biology.  Genes\ncan have a reliable causal role in the production of a whole-organism\nphenotype, of course. But if this causal relation is to be described\nin informational terms, then it is a matter of ordinary Shannon\ninformation, which applies to environmental factors as well. That\nsaid, it is possible to argue that the specificity of gene action, and\nthe existence of an array of actual and possible alternatives at a\ngiven site on a chromosome, means that genes exert fine-grained causal\ncontrol over phenotypes, and that few other developmental resources\nexert this form of causal control (Waters\n2007; Maclaurin 1998;\nStegmann 2014). In contrast, Griffiths and Stotz\n(2013) say that these\nother factors are often a source of “Crick information”,\nas they contribute to specifying the linear sequence of a gene\nproduct.  We return to this issue in section\n6. One of the most appealing, but potentially problematic, features of\nthe idea that genes code for amino acid sequences concerns the alleged\n“arbitrariness” of the genetic code. The notion of\narbitrariness figures in other discussions of genetic information as\nwell (Maynard Smith 2000; Sarkar 2003; Stegmann 2004). It is common to\nsay that the standard genetic code has arbitrary features, as many\nother mappings between DNA base triplets and amino acids would be\nbiologically possible, if there were compensating changes in the\nmachinery by which “translation” of the genetic message is\nachieved. Francis Crick suggested that the structure of the genetic\ncode should be seen as a “frozen accident”, one that was\ninitially highly contingent but is now very hard for evolution to\nchange (Crick 1958). But the very idea of arbitrariness, and the\nhypothesis of a frozen accident, have become problematic. For one\nthing, as noted in our discussion of Bergstrom and Rosvall (2011)\nabove, the code is not arbitrary in the sense that many others would\nwork as well. To the contrary, the existing code is near-optimal for\nminimising error costs. Conceptually, it seems that any causal\nrelation can look “arbitrary” if it operates via many\nintervening links. There is nothing “arbitrary” about the\nmechanisms by which each molecular binding event occurs. What makes\nthe code seem arbitrary is the fact that the mapping between base\ntriplets and amino acids is mediated by a causal chain with a number\nof intervening links (especially involving “transfer RNA”\nmolecules, and enzymes that bind amino acids to these intervening\nmolecules). Because we often focus on the “long-distance”\nconnection between DNA and protein, the causal relation appears\narbitrary. If we focused on steps in any other biological cascade that\nare separated by three or four intervening links, the causal relation\nwould look just as “arbitrary”. So the very idea of\narbitrariness is elusive. And empirically, the standard genetic code\nis turning out to have more systematic and non-accidental structure\nthan people had once supposed (Knight, Freeland, and Landweber\n1999). The notion of\narbitrariness has also been used in discussions of the links between\ngenes and phenotypes in a more general sense. Kirscher and Gerhart\n(2005) discuss a kind of arbitrariness that derives from the details\nof protein molecules and their relation to gene regulation. Proteins\nthat regulate gene action tend to have distinct binding sites, which\nevolution can change independently. To bind, a protein must be able to\nattach to a site, but that requires congruence only with a local\nfeature of the protein, not sharply constraining its overall shape\n(Planer 2014). This gives rise to a huge range of possible processes\nof gene regulation. So there is a perennial temptation to appeal to\nthe idea of arbitrariness when discussing the alleged informational\nnature of some biological causation. In section 2, we noted that Brian\nSkyrms’ work on signalling systems has made this framework a\nvery natural fit for a quite large range of biological phenomena. As\nwe remarked above, it is very intuitive to see hormones such as\ninsulin, testosterone, and growth hormone as signals, as they are\nproduced in one part of the body, and travel to other parts where they\ninteract with “receptors” in a way that modifies the\nactivities of various other structures. It is routine to describe\nhormones as “chemical messages”. The Skyrms framework fits\nthese designed cause and response systems within biological agents for\nthree reasons. First, as noted, this framework shows that signalling\ndoes not require intelligence or an intelligent grasp of signal\nmeaning. Second, the simplest cases for models of signalling are cases in which there is common interest.\nThe sender and receiver are advantaged or disadvantaged by the same\noutcomes. While complete common interest is atypical of\norganism-to-organism communication, the cells and other structures\nwithin an organism share a common fate (with complex exceptions). So\nin this respect the base models might fit organ-to-organ communication\nbetter than they fit between-organism phenomena. Third, in many of these biological systems, the abstract structure\nspecified as a signalling system—environmental source, sender,\nmessage, receiver, response—maps very naturally onto concrete\nbiological mechanisms. For example, Ron Planer has recently argued\nthat we should see gene expression as the operation of a signalling\nsystem (Planer 2014). His view is quite nuanced, for the identity of\nthe sender, receiver, and message vary somewhat from case to case. For\nexample, when a protein is a transcription factor, then the gene\ncounts as a sender. He treats other gene-protein relations\ndifferently. The details of his view need not concern us here. The\npoint is that there is machinery in the cell—genes, proteins\nmRNA transcripts, ribosomes and their associated tRNA—that can\nbe plausibly mapped onto sender-receiver systems. There is nothing\nforced about mapping the information-processing sender-receiver\nstructure onto the molecular machinery of the cell. However, while this framework very naturally fits within cell and\nbetween cell processes, it is much less clear how naturally other\nsuggestions mesh with this framework. For example, in the\nBergstrom-Rosvall picture of intergenerational transmission, who or\nwhat are the senders and receivers? Perhaps in the case of multicelled\norganisms, the receiver exists independently of and prior to the\nmessage. For an egg is a complex and highly structured system, before\ngene expression begins in the fertilised nucleus, and that structure\nplays an important role in guiding that gene expression (Sterelny\n2009; Gilbert 2013). But most organisms are single celled prokaryotes,\nand when they fission, it is not obvious that there is a daughter who exists prior to and\nindependently of the intergenerational genetic message she is to\nreceive. Likewise, Nicholas Shea’s Millikan-derived analysis does not\nseem to map naturally onto independently specifiable biological\nmechanisms. For him, the sender of genetic messages is natural\nselection operating on and filtering the gene pool of a population;\nmessages are read by the developmental system of the organism as a\nwhole (Shea 2013). But the less clearly a sender-receiver or\nproducer-consumer framework maps onto independently recognised\nbiological mechanisms, the more plausible a fictionalist or\nanalogical analysis of that case becomes. So we see an important\ndifference between a reader being the developmental system as a whole,\nand reader being a receptor on a cell membrane. The cell-level\nmachinery of transcription and translation (the ribosomal/tRNA\nmachinery, especially) really is a reader or consumer of nucleic acid\nsequences, with the function of creating protein products that will\nhave a variety of uses elsewhere in the cell. But this realization of\nthe causal schematism applies only at the cell level, at the\nlevel at which the transcription and translation apparatus shows up as\na definite part of the machinery. One of the most extraordinary\nfeatures of ontogeny is that it proceeds reliably and predictably\nwithout any central control of the development of the organism as a\nwhole. There is nothing, for example, that checks whether the\nleft-side limbs are the same size as the right side limbs, intervening\nto ensure symmetry. Similarly, there are DNA sequence-readers, and some\nintercellular sender-receiver systems with clear “readers”\n(such as neural structures), but there are no higher-level readers\nthat interpret a message specifying the structure of a whole limb-bud in the embryo. Some biologists and philosophers have argued that the introduction\nof informational and semantic concepts has had a bad effect on\nbiology, that it fosters various kinds of explanatory illusions and\ndistortions, perhaps along with ontological confusion. Here we will\nsurvey some of the more emphatic claims of this kind, but some degree\nof unease can be detected in many other discussions (see, for example,\nGriesemer 2005). The movement known as Developmental Systems Theory (DST) has often\nopposed the mainstream uses of informational concepts in biology,\nlargely because of the idea that these concepts distort our\nunderstanding of the causal processes in which genes are involved. For\na seminal discussion, see Oyama (1985), and also Lehrman (1970);\nGriffiths and Gray (1994); Griffiths and Neumann-Held (1999). These\ntheorists have two connected objections to the biological use of\ninformational notions.  One is the idea that informational models are\npreformationist. Preformationism, in its original form, in effect\nreduces development to growth: within the fertilized egg there exists\na miniature form of the adult to come. Preformationism does not\nexplain how an organized, differentiated adult develops from a much\nless organized and more homogeneous egg; it denies the phenomenon.\nDST’s defenders suspect that informational models of development\ndo the same. In supposing, for example, that instructions for a\n“language organ” are coded in the genome of a new-born\nbaby, you do not explain how linguistic abilities can develop in an\norganism that lacks them, and you foster the illusion that no such\nexplanation is necessary. (See Francis 2003 for a particularly\nvigorous version of the idea that the appeal to information leads to\npseudo-explanation in biology.) Second, DST theorists have often endorsed a “parity\nthesis”: genes play an indispensable role in development, but so\ndo other causal factors, and there is no reason to privilege\ngene’s contribution to development. This claim is often\nbuttressed by reference to Richard Lewontin’s arguments for the\ncomplexity and context sensitivity of developmental interaction, and\nhis consequent arguments that we cannot normally partition the causal\nresponsibility of the genetic and the environmental contributions to\nspecific phenotypic outcomes (Lewontin 1974, 2000). DST theorists\nthink that informational models of genes and gene action make it very\ntempting to neglect parity, and to attribute a kind of\ncausal primacy to these factors, even though they are just\none of a set of essential contributors to the process in\nquestion. Once one factor in a complex system is seen in informational\nterms, the other factors tend to be treated as mere background, as\nsupports rather than bona fide causal actors. It becomes\nnatural to think that the genes direct, control, or organise\ndevelopment; other factors provide essential resources. But, the\nargument goes, in biological systems the causal role of genes is in\nfact tightly interconnected with the roles of many other factors\n(often loosely lumped together as “environmental”).\nSometimes a gene will have a reliable effect against a wide range of\nenvironmental backgrounds; sometimes an environmental factor will have\na reliable effect against a wide range of genetic\nbackgrounds. Sometimes both genetic and environmental causes are\nhighly context-sensitive in their operation. Paul Griffiths has\nemphasised this issue, arguing that the informational mode of\ndescribing genes can foster the appearance of\ncontext-independence:\n Genes are instructions—they provide\ninformation—whilst other causal factors are merely\nmaterial…. A gay gene is an instruction to be gay even when\n[because of other factors] the person is straight. (Griffiths 2001:\n395–96) The inferential habits and associations that tend to go along with\nthe use of informational or semantic concepts are claimed to lead us to think of\ngenes as having an additional and subtle form of extra causal\nspecificity. These habits can have an effect even when people are\nwilling to overtly accept context-dependence of (most) causes in\ncomplex biological systems. So DST theorists suggest that it is\nmisleading to treat genes and only genes as carrying\n“messages” that are expressed in their effects. To say\nthis is almost inevitably to treat environmental factors as secondary\nplayers. The parity thesis has been the focus of considerable discussion and\nresponse. In a helpful paper, Ulrich Stegmann shows that the parity\nthesis is really a cluster of theses rather than a single thesis\n(Stegmann 2012). Some ways of interpreting parity make the idea quite\nuncontroversial, as no more than an insistence on the complex and\ninteractive character of development, or as pointing to the fact that\njust as genes come in slightly different versions, with slightly\ndifferent effects (holding other factors constant), the same is true\nof nongenetic factors. Epigenetic markers on genes, due to nutritional\nenvironments, litter position and birth order, may also come in\nslightly different variants, with slightly different effects. Other\nversions of the claim are much more controversial. One response to the parity thesis has been to accept the view that\ngenes are just one of a set of individually necessary and collectively\nsufficient developmental factors, but to argue nonetheless that genes\nplay both a distinctive and especially important role in development\n(Austin 2015; Lean 2014;\nPlaner 2014). As noted above, perhaps the most promising suggestion\nalong these lines is that genes exert a form of causal control over\ndevelopment that is universal, pervasive and fine-grained. Many\nfeatures of the phenotypes of every organism exist in an array of\nsomewhat different versions, as a result of allelic variation in\ncausally relevant genes. No other developmental factor exerts control\nthat is similarly universal, pervasive and fine-grained (Woodward\n2010; Stegmann 2014). Thus Stegmann illustrates Woodward’s idea\nthat genes exert specific control over phenotypes by contrasting the\neffects of intervening on, say, the quantity of polymerase on cell\nactivity with intervening on the DNA template itself. Polymerase is\ncritically causally important, but varying its concentration will\nmodify the rate of synthesis, but not the sequences produced. That is\nnot true of modifications of the DNA sequence itself, so the DNA\nsequence is more causally specific than polymerase. Shea takes a different approach, arguing that different causal\nfactors have different evolutionary histories. Some causal factors are\nsimply persisting features of the environment (gravity being one\nexample). Others are experienced by the developing organism as a\nresult of histories of selection. Burrows, for example, ensure that\neggs and nestlings develop in fairly constant temperature and\nhumidity. But burrows are not naturally selected inheritance\nmechanisms. They have not come into existence to ensure that a seabird\nchick resembles its parents. In contrast, some other developmental\nfeatures are present and act in development because of histories of\nselection in which the selective advantage is that these mechanisms\nhelp ensure parent-offspring similarity. Shea argues that genes,\nprobably epigenetic markings on genes, and perhaps a few other\ndevelopmental resources are shaped by this form of natural\nselection. So genes, and perhaps a few other developmental factors,\nplay a distinctive developmental role, even though many other factors\nare causally necessary (Shea 2011). In sum then, there are good reasons to be cautious about the use of\ninformational terminology in thinking about development. But it is\nalso possible to over-estimate the strength of the connection between\ninformational conceptions of development and the idea that genes play\na uniquely important role in development. There are ways of defending\nthe idea that genes play a special role while acknowledging the\ninteractive character of development. Moreover, an ambitious use of\ninformational concepts is not confined to those within mainstream\nbiological thinking. Eva Jablonka and Marion Lamb defend quite\nheterodox views of inheritance and evolution, while basing key parts\nof their work—including an advocacy of “Lamarckian”\nideas—around informational concepts (Jablonka and Lamb\n2005). They suggest that one of the useful features of informational\ndescriptions is that they allow us to generalize across different\nheredity systems, comparing their properties in a common currency. In\naddition, one of the present authors has used informational concepts\nto distinguish between the evolutionary role of genes from that of\nother inherited factors whilst demonstrating the evolutionary\nimportance of non-genetic inheritance (Sterelny 2004, 2011). So in\nvarious ways, an informational point of view may facilitate discussion\nof unorthodox theoretical options, including non-genetic mechanisms of\ninheritance. Talk of genetic “programs” is common both in popular\npresentations of biology, and in biology itself. Often, the idea is\njust a vivid (but perhaps misleading) way of drawing attention to the\norderly, well-controlled and highly structured character of\ndevelopment. In its overall results, development is astonishingly\nstable and predictable, despite the extraordinary complexity of\nintracellular and intercellular interactions, and despite the fact\nthat the physical context in which development takes place can never\nbe precisely controlled. So when biologists speak, for example, of\n“programmed cell death”, they could just as well say that\nin an important class of cases, cell death is predictable, organised,\nand adaptive. There are attempts to draw closer and more instructive parallels\nbetween computational systems and biological development. In\nparticular, Roger Sansom has made a sustained and detailed attempt to\ndevelop close and instructive parallels between biological development\nand connectionist computational models (Sansom 2008b,a, 2011). This\nview has the merit of recognising that there is no central control of\ndevelopment; organisms develop as a result of local interactions\nwithin and between cells. However, the most promising ideas about\nprogram-development parallels seem to us to be ones that point to an\napparently close analogy between processes within cells, and\nthe low-level operation of modern computers. One crucial kind\nof causal process within cells is cascades of up and down-regulation\nin genetic networks. One gene will make a product that binds to and\nhence down-regulates another gene, which is then prevented from making\na product that up-regulates another… and so on. What we have\nhere is a cascade of events that can often be described in terms of\nBoolean relationships between variables. One event might only follow\nfrom the conjunction of another two, or from a disjunction of\nthem. Down-regulation is a kind of negation, and there can be double\nand triple negations in a network. Gene regulation networks often have\na rich enough structure of this kind for it to be useful to think of\nthem as engaged in a kind of computation. Computer chip\n“and-gates”, neural “and-gates” and genetic\n“and-gates” have genuine similarities. While talking of signalling networks rather than programs, Brett\nCalcott has shown that positional information in the developing\nfruitfly embryo depends on this kind of Boolean structure, with limb\nbud development depending on the cells that differentiate into limb\nbuds integrating one positive with two negative signals, so the buds\ndevelop in a regular pattern on the anterior midline of the embryo.\nCalcott shows that thinking of development in terms of these\nsignalling networks with their Boolean structures has real explanatory\nvalue, for it enables us to explain how positional information, for\nexample, can easily be reused in evolution. Wing spots on fruitflies\ncan evolve very easily, for the networks that tell cells where they\nare on the wing already exist, so the evolution of the wingspot just\nneed a simple mutational change that links that positional information\nto pigment production (Calcott 2014). Ron Planer agrees that gene\nregulation has this Boolean structure, and that we can, in effect,\nrepresent each gene as instantiating a conditional instruction. The\n“if” part of the conditional specifies the molecular\nconditions which turn the gene on; the “then” part of the\nconditional specifies the amino acid sequence made from the gene. As\nwith Calcott, Planer goes on to point out that these conditional\ninstructions can be and often are, linked together to build complex\nnetworks of control. Even so, Planer argues that while these are\nsignalling networks, they should not be thought of as computer\nprograms. For example, the combinations of instructions have no intrinsic order; we\ncan represent each of the genes as a specific conditional instruction,\nbut there is nothing in the set of instructions itself that tells us\nwhere to begin and end (Planer 2014). Information has also become a focus of general discussion of\nevolutionary processes, especially as they relate to the mechanisms of\ninheritance. One strand of this discussion misconceives information\nand its role in biological processes. In particular, G.C.  Williams\nargues that, via reflection on the role of genes in evolution, we can\ninfer that there is an informational “domain” that exists\nalongside the physical domain of matter and energy (Williams\n1992). Richard Dawkins defends a similar view, arguing that the\nlong-term path of evolution is made up of gradual changes in inherited\ninformation—as a river that “flows through time, not\nspace” (Dawkins 1995: 4). This is an extension of a more common\nidea, that there exists such things as “informational\ngenes” that should be understood as distinct from the\n“material genes” that are made of DNA and localized in\nspace and time (Haig 1997). It is a mistake to think that there are\ntwo different things; that there is both a physical entity—a\nstring of bases—and an informational entity, a message. It is\ntrue that for evolutionary (and many other) purposes genes are often\nbest thought of in terms of their base sequence (the sequence of C, A,\nT and G), not in terms of their full set of material properties. This\nway of thinking is essentially a piece of abstraction (Griesemer\n2005). We rightly ignore some properties of DNA and focus on\nothers. But it is a mistake to treat this abstraction as an extra\nentity, with mysterious relations to the physical domain. Other ways of linking informational ideas to general issues in\nevolutionary theory seem more promising. As John Maynard Smith, Eors\nSzathmáry, Mark Ridley and Richard Dawkins have emphasized in\ndifferent ways, inheritance mechanisms that give rise to significant\nevolutionary outcomes must satisfy some rather special conditions\n(Dawkins 1995; Jablonka and Szathmáry 1995; Szathmáry and Maynard\nSmith 1997; Ridley 2000). Maynard Smith and Szathmáry claim, for\nexample, that the inheritance system must be unlimited or\n“indefinite” in its capacity to produce new combinations,\nbut must also maintain high fidelity of transmission. This fact about\nthe relationship between inheritance systems and biological structure\nis often thought to reveal one of the most pressing problems in\nexplaining the origins of life. If reproduction depends on the\nreplication of a crucial set of ingredients to kick-start the new\ngeneration (whether or not we think of those ingredients as\ninstructions), these ingredients must be replicated accurately. Yet\naccurate replication apparently depends on complex molecular and\nintracellular machinery that itself is the result of a long regime of\nadaptive evolution, and hence on deep lineages of living systems. So\nhow could reproduction have begun? (see Ridley 2000 for a thoughtful\ndiscussion). So life itself, the argument goes, depends on the evolution of\nmechanisms that support a high fidelity flow of information from one\ngeneration to the next. More ambitiously still, Maynard Smith and\nSzathmáry argue that many of the crucial steps in the last four\nbillion years of evolution—their “major transitions in\nevolution”—involve the creation of new ways of\ntransmitting information across generations—more reliable, more\nfine-grained, and more powerful ways of making possible the reliable\nre-creation of form across events of biological reproduction. The\ntransition to a DNA-based inheritance system (probably from a system\nbased on RNA) is one central example. But Maynard Smith and Szathmáry\nsuggest that the transition from great ape forms of social life to\nhuman social life is a major transition, in part because of the novel\nforms of large scale cooperation that typify human social life, but\nmostly because they see human language as a breakthrough informational\ntechnology, revolutionising the possibilities of high fidelity\nintergenerational cultural learning (MacArthur 1958; Maynard Smith and\nSzathmáry 1995, 1999). Maynard Smith and Szathmáry’s work on major transitions has been a\nlandmark in macroevolutionary thinking—in thinking about large\nscale patterns in the history of life. But the informational dimension\nof their work has not been taken up. A minor exception is Sterelny\n(2009). This paper argued that multicelled animal life depended not\njust on the transmission of more information with high fidelity, but\non the control of that information in development, suggesting that the\nevolution of the egg—a controlled, structured, information-rich\ndevelopmental environment—was critical to complex animal\nlife. But most focus has been on the other strand of their work on\nmajor transitions: on the solution of cooperation problems that then\nallow previously independent agents to combine into new, more complex\nagents. Thus in Calcott and Sterelny (2011), none of the papers\nfocused primarily on the expansion or control of intergenerational\ninformation flow. That may change. The evolutionarily crucial features\nof inheritance mechanisms are often now discussed in informational\nterms, and the combinatorial structure seen in both language and DNA\nprovides a powerful basis for analogical reasoning.","contact.mail":"kim.sterelny@vuw.ac.nz","contact.domain":"vuw.ac.nz"}]
