[{"date.published":"2017-10-27","url":"https://plato.stanford.edu/entries/epistemic-self-doubt/","author1":"Sherrilyn Roush","entry":"epistemic-self-doubt","body.text":"\n\n\n\n\nAnd if I claim to be a wise man,\n\nWell, it surely means that I don't know.\n—Kansas\n\n\n\nIt is possible to direct doubt at oneself over many things. One can\ndoubt one’s own motives, or one’s competence to drive a\ncar. One can doubt that one is up to the challenge of fighting a\nserious illness. Epistemic self-doubt is the special case where what\nwe doubt is our ability to achieve an epistemically favorable state,\nfor example, to achieve true beliefs. Given our obvious fallibility,\nepistemic self-doubt seems a natural thing to engage in, and there is\ndefinitely nothing logically problematic about doubting someone\nelse’s competence to judge. However when we turn such doubt on\nourselves, incoherence seems to threaten because one is using\none’s judgment to make a negative assessment of one’s\njudgment. Even if this kind of self-doubt can be seen as coherent,\nthere are philosophical challenges concerning how to resolve the inner\nconflict involved in such a judgment, whether one’s initial\njudgment or one’s doubt should win, and why.\n\n\nSome ways of doubting that we are in a favorable epistemic state are\neasy to understand and unproblematic. Socrates was confident that he\ndid not know the answers to his most important questions. He believed\nhe did not have the answers, or the right kind of grasp of answers,\nthat would be required for knowledge of what, for example, piety,\nvirtue, and justice are. This recognition led him to avoid endorsing\nor believing particular answers to his questions, and motivated him to\ngo around town asking others for their answers and making awkward\nobservations about their replies. Though the authorities prosecuted\nhim for this, his offense was not an epistemic irrationality; the\nbelief states he had—doubt about himself that he knew the\nanswers, and lack of confidence in particular answers—fit\ntogether sensibly. Moreover, as Socrates told his interlocutors, his\nacknowledgment that he did not know had the salutary effect of making\nit possible for him to find out. If he were sure that he already knew,\nthen he would not have motivation to look for the answer.\n\n\nNot all epistemic self-doubt is so evidently constructive. Socrates\ncould hope to find his answers in the future in part because his doubt\nwas not directed at his faculties for gaining knowledge, and the\nmatters on which he believed himself ignorant were specific and\nlimited. This left him confident of his tools, and still in possession\nof a lot of knowledge to work with in seeking his answers. For\nexample, it was possible for Socrates to be both sure he did not know\nwhat virtue was and yet confident that it was something beneficial to\nthe soul. In contrast, Descartes in his Meditations set out\nto rid himself of all beliefs in order to rebuild his edifice of\nbelief from scratch, so as to avoid all possibility of erroneous\nfoundations. He did so by finding reason to doubt the soundness of his\nfaculty of, for example, sense perception. Instead of casting doubt on\nhis empirical beliefs one by one, he would doubt the reliability of\ntheir source and that would blanket all of them with suspicion,\nloosening the hold that even basic perceptual beliefs had on his mind.\nDescartes’ epistemic self-doubt was extreme in undermining trust\nin a belief-forming faculty, and in the wide scope of beliefs that\nwere thereby called into question. As in the case of Socrates though,\nhis belief states fit together sensibly; as he convinced himself he\nmight be dreaming, thus undermining his trust that he was in a\nposition to know he had hands, he was also shaken out of his belief\nthat he had hands.\n\n\nThe cases of Socrates and Descartes illustrate that judgments about\none’s own epistemic state and capacity can provide reasons to\nadjust one’s beliefs about the way things are. Less dramatic\ncases abound in which rationality’s demand for some kind of fit\nbetween one’s beliefs (first-order beliefs) and one’s\nbeliefs about one’s beliefs (second-order beliefs) can be seen\nin the breach. Suppose I am a medical doctor who has just settled on a\ndiagnosis of embolism for a patient when someone points out to me that\nI haven’t slept in 36 hours (Christensen 2010a). On reflection I\nrealize that she is right, and if I am rational then I will feel some\npressure to believe that my judgment might be impaired, reduce\nsomewhat my confidence in the embolism diagnosis, and re-check my\nwork-up of the case or ask for a colleague’s opinion.\n\n\nThough it seems clear in this case that some reconsideration of the\nfirst-order matter is required, it is not immediately clear how strong\nthe authority of the second-order might be compared to the first order\nin coming to an updated belief about the diagnosis, and there are\nclear cases where the second order should not prevail. If someone\ntells me I have unwittingly ingested a hallucinogenic drug, then that\nimposes some prima facie demand for further thought on my part, but if\nI know the person is a practical joker and he has a smirk on his face,\nthen it seems permissible not to reconsider my first-order beliefs.\nThere are also cases where it is not obvious which order should\nprevail. Suppose I’m confident that the murderer is #3 in the\nline-up because I witnessed the murder at close range. Then I learn of\nthe empirical literature saying that eyewitnesses are generally\noverconfident, especially when they witnessed the event in a state of\nstress (Roush 2009: 252–3). It seems I should doubt my\nidentification, but how can it be justified to throw out my\nfirst-order evidence that came from directly seeing that person, in\nperson and close up? An adjudication of some sort between the first\norder and the higher order is required, but it is not obvious what the\ngeneral rules might be for determining the outcome of the conflict, or\nwhat exactly would justify them.\n\n\nQuestions about epistemic self-doubt can be organized into five\nover-arching questions: 1) Can the doubting itself, a state of having\na belief state and doubting that it is the right one to have, be\nrational? 2) What is the source of the authority of second-order\nbeliefs? 3) Are there general rules for deciding which level should\nwin the tug of war? If so, what is their justification? 4) What does\nthe matching relation this adjudication is aiming at consist in? 5) If\nmismatch between the levels can be rational when one first acquires\nreason to doubt, is it also rationally permitted to remain in a\nlevel-splitting state—also known as epistemic akrasia\n(Owens 2002)—in which the self-doubting conflict is\nmaintained?\n\n\nFor convenience, approaches to modeling doubt about one’s own\nability to judge and to the five questions above can be separated into\nfour types, which are overlapping and complementary rather than\ninconsistent. One approach is through seeing the self-doubting subject\nas believing epistemically unflattering categorical statements about\nthe relation of her beliefs to the world. Another is through\nconditional principles, asking what a subject’s credence in\nq should be given that she has a particular credence in\nq but thinks she may be epistemically inadequate or\ncompromised. A third approach is to construe doubt about one’s\njudgment as a matter of respecting evidence about oneself and\none’s evidence (higher-order evidence). A fourth approach ties\ntogether the first order and second order by using the idea that we\nshould match our confidence in p to our expected reliability.\nThat is, treating ourselves like measuring instruments we should aim\nto be calibrated.\n\nIt might seem that consistency and coherence are not strong enough to\ntell us what the relation should be between the first order and the\nsecond order in cases of epistemic self-doubt, in the same way that\nthey don’t seem sufficient for explaining what is wrong with\nMoore-paradoxical statements (see entry on\n epistemic paradoxes).\n In the latter I assert either “p and I don’t\nbelieve p”, or “p and I believe\nnot-p”. There is a lack of fit between my belief and my\nbelief about my belief in either case, but the beliefs I hold\nsimultaneously are not inconsistent in content. What I say of myself\nwould be consistent and quite sensible if said about me by someone\nelse, thus: “p, but she doesn’t believe\np”. Similarly, there is nothing inconsistent in the claim\n“There is a cat in the distance and she is severely\nnear-sighted”, though there does seem to be a problem with the\nfirst-person claim “There is a cat in the distance and I am\nseverely near-sighted” if my assertion about the cat is made on\nthe basis of vision and I give no cue that I mean the second clause as\na qualification of the first. My confidence about the cat should have\nbeen tempered by my awareness of the limitation of my vision. If there\nare general principles of rationality that govern self-doubt of our\nfaculties or expertise it seems that they will have to go beyond\nconsistency among beliefs. \nHowever, consistency and coherence do impose constraints on what a\nsubject may believe about the reliability of her beliefs if combined\nwith an assumption that the subject knows what her beliefs are. (This\nis also found for Moore’s paradox, and is used in\nShoemaker’s approach to that problem; Shoemaker 1994.) One way\nof formulating an extreme case of believing that one’s epistemic\nsystem doesn’t function well is as attributing to oneself what\nSorensen calls anti-expertise (Sorensen 1988: 392f.). In the simplest\ntype of case, S is an anti-expert about p if\nand only if \nAnti-expertise (A)\n\nEither S believes p and p is false or S\ndoes not believe p and p is true.  \nSorensen pointed out that if S is consistent and knows\nperfectly what her beliefs are, then she cannot believe she is an\nanti-expert. For if S believes p then, by perfect\nself-knowledge, she believes that she believes p, but her\nbeliefs that p and that she believes p are together\ninconsistent with both of the disjuncts of A. Similarly for the case\nwhere S does not believe p. This phenomenon generalizes\nfrom outright belief to degrees of belief, and from perfect knowledge\nof one’s beliefs to decent but imperfect knowledge of them (Egan\n& Elga 2005: 84ff.). \nBelieving you are an anti-expert is not compatible with coherence and\ndecent knowledge of your own beliefs. Denying that knowledge of our\nown beliefs is a requirement of rationality would not be helpful,\nsince usefully doubting that one’s beliefs are soundly formed\nwould seem to require a good idea of what they are. Egan and Elga\nfavor the view that your response to this fact about anti-expertise\nshould be to maintain coherence and decent self-knowledge of belief,\nand refrain from believing that you are an anti-expert. However, one\ncan imagine examples where the evidence that you are incompetent is so\noverwhelming that one might think you should believe you are an anti-expert even if\nit makes you incoherent (Conee 1987; Sorensen 1987, 1988; Richter\n1990; Christensen 2011). \nThe problem with self-attributing unreliability coherently\ndoesn’t go away if the degree of unreliability is more modest.\nConsider the following property: \nI’m not perfect (INP)\n\n\\(P((P(q) \\gt .99 \\amp -q )\\textrm{ or } (P(q) \\lt .01 \\amp q)) \\gt .05\\) \n \nThis says that you are at least 5% confident that you’re highly confident\nof q though it’s false or lack confidence in q\nthough it’s true. It is a softened version of anti-expertise,\nand you can’t coherently fulfill it, have \\(P(q) >.99\\), and\nhave perfect knowledge of your beliefs. For in that case \\(P(P(q) >\n.99) = 1\\), which means INP can only be true if \\(P(-q) > .05\\).\nBut \\(P(-q) > .05\\) implies \\(P(q) < .95\\) which contradicts\n\\(P(q) > .99\\). The point survives if you have imperfect but good\nknowledge of what your beliefs are. The self-doubt expressed through\nINP is quite modest, but is no more consistent than attributing to\noneself anti-expertise, and this will be so for any value of\nINP’s right hand side that is not equal to \\(P(-q)\\).  \nEgan and Elga think that the significance of evidence of\nanti-reliability is taken into account by seeing it as obligating a\nsubject to revise her first-order belief. However, their view implies\nthat to be rational such a revision must be done without attributing\nanti-expertise to oneself. One can revise whenever one wants of\ncourse, but any revision should have a reason or motivation. If one\ndoes not give any credence at all to the possibility one is an\nanti-expert, then what is one’s reason to revise one’s\nfirst-order belief? There would seem to be no other way to take on\nboard and acknowledge the evidence of your anti-expertise than to give\nsome credence to its possibility. Egan and Elga say that the belief\nthe evidence should lead a subject to is that she has been an\nanti-expert and that should lead her to revise (Egan & Elga 2005:\n86). But if she avoids\nincoherence by attributing anti-expertise only to a previous self,\nthen that belief can’t be what leads her to revise her current\nview. If she does not attribute anti-expertise to her current self\nthen she doesn’t give her current self any reason to revise. \nThe same problem can be seen with Egan and Elga’s treatment of\ncases of self-attributing less extreme unreliability (such as INP)\nthat they regard as unproblematic. Consider a person with growing\nevidence that his memory isn’t what it once was. What effect\nshould this have on his beliefs about the names of students? They\ncompare what happens to his confidence that a given student is named\n“Sarah” when he hears counterevidence—overhearing\nsomeone calling her “Kate” for example—in the case\nwhere he has and the case where he hasn’t taken the evidence of\nhis decline in memory into account. Via a Bayesian calculation they\nconclude that when he hasn’t taken into account the evidence\nabout his memory, the counterevidence to his particular belief that\nthe student is named Sarah does reduce his belief that she is Sarah,\nbut it does so much less than it would have if he had taken the\nevidence about his memory into account. \nBut this analysis represents taking into account the evidence about\none’s memory only implicitly, as an effect that that evidence\nalready had on one’s prior probability that the student is\nSarah. That effect is the difference between a .99 and a .90 prior\nprobability, or degree of belief. The distinction that is then derived\nbetween the effects that counterevidence can have on the self-doubter\nand the non-self-doubter is just the familiar point that\ncounterevidence will have a greater effect the lower one’s\ninitial probability.  \nThis doesn’t tell us how to assimilate news about one’s\ndecline in reliability to the first-order belief, but only how to\ntreat other evidence about the first-order matter once one has done\nso. The question was supposed to be how the evidence about memory\nshould affect our beliefs, and to answer that requires saying how and\nwhy that evidence about his memory should make our subject have a .90\nrather than .99 initial confidence that the student was Sarah. Surely\none must attribute reduced reliability to oneself if one is to have\nany reason to revise one’s first-order belief that the student\nwas Sarah on the basis of evidence of diminished reliability. Even a\nraw feel of redness of a flower must become an ascription of red to\nthe flower in order for one’s experience of it to affect other\nbeliefs such as that it would or wouldn’t be an appropriate\ngift. Even evidence suggesting a small amount of unreliability, as\nwith INP above, presents us with a trilemma: either we incoherently\nattribute unreliability to ourselves but revise and have a\njustification for doing so, or we coherently fail to attribute\nunreliability and revise without justification for doing so, or we\nremain coherent by failing to attribute unreliability and don’t\nrevise, ignoring evidence of our unreliability. It seems that it is\nnot possible for a rational subject to acknowledge evidence of her own\nunreliability and update her first-order belief on the basis of\nit. \nThis approach using consistency (or coherence) plus self-knowledge of\nbelief gives a way of representing what a state of self-doubt is. It\nsensibly implies that it is irrational to stay in such a state but it\nalso implies that it is irrational to be in it in the first place,\nmaking it unclear how self-doubt could be a reason to revise. The\napproach identifies a kind of matching that rationality demands: give\nno more credence to the possibility that one is a bad judge of\nq than one gives to not-q. However, it leaves other\nquestions unanswered. If the rational subject finds herself doubting\nher judgment, should she defer to her first-order evidence, or her\nsecond-order evidence about the reliability of her first-order\njudgment? What are the rules by which she should decide, and how can\nthey be justified? \nWe might do better at understanding the relations rationality requires\nbetween your beliefs and your beliefs about them by adding to the\nrequirements of consistency and coherence a bridge principle between\nthe two orders expressed via conditional (subjective) probability.\nConditional probabilities say what your degree of belief in one\nproposition is (should be) given another proposition, here the\nrelevant propositions being a first-order proposition q, and\nthe proposition that one has degree of belief x in q,\nrespectively. A first pass at how to represent a situation where my\nbeliefs at the two levels don’t match comes from its apparent\nconflict with the synchronic instance of the Reflection Principle (van\nFraassen 1984). \nReflection\n\n\\(P_0 (q\\mid P_1 (q) = x) = x \\)  \nReflection says that my current self’s degree of belief in\nq given that my future self will believe it to degree x\nshould be x. It is implied by the fact that her degrees of\nbelief are represented as probabilities that my future self is\ncoherent, but that alone does not rule out the possibility that her\njudgment is compromised in some other way—as for example when\nUlysses anticipated that he would be entranced by the Sirens—and\nthe principle can be questioned for such cases (Sobel 1987;\nChristensen 1991; van Fraassen 1995). However, the self-doubt we are\nimagining is one that the subject has about her current beliefs, and\nthe synchronic version of Reflection \nSynchronic Reflection (SR)\n\n\\(P_{0}(q\\mid P_{0}(q)=x) = x \\)  \nwhich says that my degree of belief in q now given that I now\nbelieve q to degree x should be x, seems less\nopen to question. Christensen (2007b) also calls this principle\nSelf-Respect (SR). This is not the tautology that if I believe\nq to degree x then I believe q to degree\nx, for in a logically equivalent form the principle is \nSynchronic Reflection/Self-Respect (SR)\n\n\\([P_{0}(q \\amp P_{0}(q)=x)\\mid P_{0}(P_{0}(q)=x)] = x \\)  \nwhich does not follow from either deductive logic or the probability\naxioms alone. But SR has been widely endorsed as unobjectionable, and\naccording to some even undeniable, as a requirement of rationality\n(van Fraassen 1984: 248; Vickers 2000: 160; Koons 1992:\n23—Skyrms 1980 sees\nas useful a version of it he calls Miller’s Principle, though he\nalso shows it is subject to counterexamples). \nWhile I can sensibly imagine my future self to be epistemically\ncompromised, unworthy of my deference, violating SR would require\nregarding my current self as epistemically compromised, as having a\ndegree of belief that should be other than it is. This appears to be\nsomething that doubt of my own judgment would call for, in which case\nwhether self-doubt can be rational depends on whether SR is a\nrequirement of rationality. \nSR can be defended as a rational ideal by Dutch Strategy arguments,\nthough not by the strongest kind of Dutch Book argument (Sobel 1987;\nChristensen 1991, 2007b: 328–330, 2010b; Briggs 2010—Roush\n2016 argues it can’t be defended as a requirement by a\nDutch book argument at all). It has been argued to be questionable if\nnot false on grounds that it conflicts with the Epistemic\nImpartiality Principle that says we should not in general take\nthe mere fact that we have a belief as a reason to have that belief\nany more so than we do with the mere fact that others have that belief\n(Christensen 2000: 363–4; Evnine 2008: 139–143; Roush\n 2016).[1] \nNevertheless the probabilist—one who thinks that rationality\nrequires probabilistic coherence—will have a difficult time\nresisting SR since, analogously to what we saw above with\nanti-expertise, SR follows from coherence if it is supplemented with\nthe further assumption that the subject has perfect knowledge of her\nown beliefs. Still, this does little to explain intuitively why SR\nshould be binding; even someone who has perfect knowledge that he has\na belief should be able to sensibly wonder whether it is a belief he\nought to have. That perfect knowledge of our beliefs is a requirement\nof rationality can be doubted in a variety of ways (Williamson 2000;\nChristensen 2007b: 327–328; Roush 2016). However, as\nabove so for the discussion here, denying that rationality requires\nknowledge of our own beliefs does not overcome the problem.\nSelf-correction that will be of any use requires some degree of\naccuracy about one’s beliefs, and even if a subject\ndoesn’t have perfect self-knowledge coherence still makes\nreflective demands; Christensen (2007b: 332) has noted that the closer\na coherent subject comes to perfect knowledge of his beliefs the more\nnearly he will satisfy SR. \nSR has something to recommend it, but it seems to be a rule a\nself-doubter will violate. Consider our underslept doctor. It looks as\nif once it is pointed out to her how long it has been since she slept,\nshe should regard her current confidence in q, her diagnosis of\nembolism, as higher than it ought to be. I.e., she would instantiate a\nprinciple we could call Refraction: \nRefraction\n\n\\(P_{0}(q\\mid P_{0}(q)=x) < x \\)  \nApparently, her degree of belief that it’s an embolism given\nthat she has degree of belief x that it’s an embolism\nshould be less than x, contradicting SR. Or imagine that the\nperson who tells me a hallucinogenic drug has been slipped into my\ncoffee is a trusted friend who is neither in the habit of joking nor\ncurrently smirking. I seem to have an obligation to\nregard some of my current degrees of belief as higher than they should\nbe. \nRefraction is a way of representing a state of self-doubt, one in\nwhich I don’t regard the degree of belief I (think I) have as\nthe right one to have. But despite the fact that it doesn’t have\nthe subject attributing unreliability to herself categorically as we\nhad in the last section, Refraction is not compatible with the\ncombination of coherence and knowledge of one’s beliefs, since\nthe latter two together imply SR. In this representation of what\nself-doubt is, it is not rational according to the probabilistic\nstandard. \nOne might defend this verdict by saying that the exception proves the\nrule: if I really think that my degree of belief in q should be\ndifferent than it is, say because I realize I am severely underslept,\nthen surely I should change it accordingly until I come to a credence\nI do approve of, at which point I will satisfy SR. However even if it\nis ideal to be in the state of self-respect that SR describes, it\nseems wrong to say that a state of disapproving of one’s\nfirst-order belief when faced with evidence of one’s impaired\njudgment is irrational. In such a case it would seem to be irrational\nnot to be in a state of self-doubt. Moreover, it is unclear\nhow a revision from a state violating SR to one in conformity with SR\ncan be rational. According to many probabilists the rational way to\nrevise beliefs is via conditionalization, where one’s new\ndegrees of belief come from what one’s previous function said\nthey should be given the new belief that prompts the revision (see\nentries on\n interpretations of probability\n and\n Bayes’ Theorem).\n That is, all change of belief is determined by the conditional\nprobabilities of the function one is changing from. Thus change of\nbelief on the basis of a belief about what my belief is will depend on\nthe value of \\(P_i(q\\mid P_i(q) = x)\\). If the value of \\(P_i(q\\mid\nP_i(q)=x)\\) isn’t already x then a conditionalization\nusing that conditional probability will not necessarily make\n\\(P_f(q\\mid P_f(q)=x) = x\\), as required by SR, and it is hard to see\nhow it could. In this approach, similarly to the previous, the whole\ncycle of epistemic self-doubt and resolution appears to be unavailable\nto a probabilistically rational subject. \nIf we represent epistemic self-doubt as a violation of Synchronic\nReflection (Self-Respect), then it is not rational for a coherent\nperson who knows what her beliefs are. This is a general rule giving\nthe same verdict in all cases, that the orders must match, and it\ngives the form of that matching in terms of a conditional probability.\nThe second-order is in the driving seat since the condition in\nSR’s conditional probability that determines a value for the\nfirst-order proposition q is itself a statement of probability,\nbut SR can’t lead to change of first-order belief unless one\ndoes not know what one’s belief in q is. As in the\napproach via categorical statements above, this simple approach via\nconditional probability does not represent the cycle of self-doubt and\nresolution as available to a rational subject. \nAnother way of representing self-doubt using conditional probability\nfollows the intuitive thought that I should tailor my first-order\ndegree of belief to the confidence I think the maximally rational\nsubject would have if she were in my situation (Christensen 2010b:\n121). This would be a sensible explanation of the authority of higher\norder beliefs, of why taking them into account would be justified. It\ngives half of an answer to the question when the first-order should\nand shouldn’t defer to the second order by identifying a class\nof second-order statements to which the first order must always defer.\nHowever it pushes back the question of which individual statements\nthose are to the question of which probability function is the\nmaximally rational. \nA conditional principle that would capture the idea of deferring to\nthe view of an ideal agent who was in one’s shoes is: \n(Christensen 2010b) which says that one’s credence in q\ngiven that the maximally rational subject in one’s situation has\ncredence x in q, should be x. The maximally\nrational subject obeys the probability axioms and possibly has further\nrationality properties that one might not possess oneself, though she\nis assumed to be in your situation, having no more evidence than you\ndo. If one obeys the probability axioms oneself, then this principle\nbecomes: \nRatRef\n\n\\(P(q\\mid P_{M}(q) = x) = x \\)  \nThis says that your credence in q should be whatever you take\nthe maximally rational credence to be for your situation, an idea that\nseems hard to argue with. It is a variant of a principle used by Haim\nGaifman (1988) to construct a theory of higher-order probability.\nThere the role of \\(P_{M}\\) was given to what has become known as an\nexpert function corresponding, in his use, to the\nprobabilities of a subject who has maximal knowledge. \nRatRef gives a sensible account of cases like the underslept doctor.\nIt would say the reason that the realization she is sleep-deprived\nshould make her less confident of her diagnosis is that a maximally\nrational person in her situation would have a lower confidence.\nMoreover this provides us with a way of representing the state of\nself-doubt coherently, even with perfect knowledge of what one’s\nbelief states are. One can have degree of belief y in q,\nand one can even believe that one has degree of belief y, that\nis, believe that \\(P(q) = y\\), consistently with believing that the\nmaximally rational agent has degree of belief x, i.e.,\n\\(P_{M}(q) =x\\), because these are two different probability\nfunctions. \nBecause self-doubt is not defined as a violation of the conditional\nprobability RatRef, as it was with SR, we can also see a revision that\ntakes you from self-doubt to having your confidence match that of the\nmaximally rational subject as rational according to\nconditionalization. You may have degree of belief y in\nq, discover that the maximally rational subject has degree of\nbelief x, and because you have the conditional probability\nRatRef put yourself in line with that ideal subject. Note that it is\nnot necessary to have an explicit belief about what your own degree of\nbelief in q is for this revision to occur or to be\nrational. \nRatRef has problems that are easiest to see by considering a\ngeneralization of it: \nRational Reflection (RR)\n\n\\(P(q\\mid P' \\textrm{ is ideal}) = P'(q) \\)  \nRational Reflection (Elga 2013) maintains the idea that my degree of\nbelief in q should be in line with that I think the maximally\nrational subject would have in my situation, but also highlights the\nfact that my determining what this value is depends on my identifying\nwhich probability function is the maximally rational one to have. I\ncan be coherent while being uncertain about that, and there are cases\nwhere that seems like the most rational option. This by itself is not\na problem because RR is consistent with using an expected value for\nthe ideal subject, a weighted average of the values for q of\nthe subjects I think might be the maximally rational subject. But it\nis not only I who may be uncertain about who the maximally rational\nsubject is. Arguably the maximally rational subject herself may be\nuncertain that she is—after all, this is a contingent fact, and\none might think that anyone’s confidence in it should depend on\nempirical evidence (Elga 2013). \nThe possibility of this combination of things leads to a problem for\nRR, for if the subject who is actually the maximally rational one is\nunsure that she is, then if she follows RR she won’t fully trust\nher own first-order verdict on q but will as I do correct it to\na weighted average of the verdicts of those subjects she thinks might\nbe the maximally rational one. In this case my degree of belief in\nq given that she is the maximally rational subject\nshould not be her degree of belief in q. It should be the one\nshe would have in case she were certain that she were the maximally\nrational subject: \nNew Rational Reflection (NRR)\n\n\\(P(q\\mid P' \\textrm{ is ideal}) = P'(q\\mid P' \\textrm{ is ideal}) \\)\n \nThis principle (Elga 2013) also faces problems, which are developed\nbelow through the approach to self-doubt via higher-order\nevidence. \nThe approach asking what the maximally rational subject would do gives\na motivation for the idea that second-order evidence has authority\nwith respect to first-order beliefs, and like the SR approach puts the\nsecond order in the driver’s seat by having a statement of\nprobability in the condition of the conditional probability. This may\nappear to give unconditional authority to second-order evidence, but\nsecond-order evidence won’t change the subject’s\nfirst-order verdict if she believes the latter is already what the\nmaximally rational subject would think. The approach represents\nself-doubt as a coherent state that one can also coherently revise via\nconditionalization. It identifies a state of matching between the\norders—matching one’s confidence to one’s best guess\nof the maximally rational subject’s confidence. This gives a\ngeneral rule, and demands that same matching for all cases, though\ngives no explicit guidance about how to determine which is the\nmaximally rational subject or degree of belief. \nQuestions about the rationality (or reasonableness or justifiedness)\nand import of epistemic self-doubt can be developed as questions about\nwhether and how to respect evidence about one’s evidence.\nHigher-order evidence is evidence about what evidence one possesses or\nwhat conclusions one’s evidence supports (see entry on\n evidence).\n This issue about the upshot of higher-order evidence does not in the\nfirst instance depend on whether we take such evidence as necessary\nfor justification of first-order beliefs. The question is how our\nbeliefs should relate to our beliefs about our beliefs when we do\nhappen to have evidence about our evidence, as we often do (Feldman\n2005; Christensen 2010a; Kelly 2005, 2010). \nSelf-doubt is a special case of responding to higher-order evidence.\nNot all evidence about our evidence arises from self-doubt because not\nall such evidence is about oneself, as we will see below. Also,\nrepresenting self-doubting situations as responding to evidence about\nmy evidence takes information about my capacities to be important just\ninsofar as it provides evidence that I have either incorrectly\nidentified my evidence or incorrectly evaluated the support relation\nbetween my evidence and my conclusion. For example, in the case of the\ndoctor above who receives evidence that she is severely underslept,\nthe reason she should reconsider her diagnosis is because this is\nevidence that she might be wrong either in reading the lab tests or in\nthinking that the evidence of lab tests and symptoms supports her\ndiagnosis. By contrast the fourth approach below via calibration does\nnot see the implications of self-doubt as necessarily proceeding via\nevidence about our evidence or evidential support. \nLike the approach via the maximally rational agent, the evidential\napproach has the virtue of identifying a justification for responding\nto the second-order beliefs that self-doubt brings. Their authority\ncomes from the facts that they are evidence relevant to whether one\nhas good evidence for one’s first-order belief, and that one\nshould respect one’s evidence. This raises the hope that what we\nalready know about evidence can help settle when negative second-order\nevidence should override a first order belief and when not. Many\nauthors have thought that in either kind of case rationality demands\nthat the two orders eventually match, in some sense, but we will see\nbelow that more recent thinking about evidence has led some to defend\nthe rationality of having the first- and second-order beliefs in\ntension, for some cases. Another virtue of the evidential approach is\nthat merely knowing what your beliefs are doesn’t automatically\nimply that a state of self-doubt is inconsistent or incoherent as it\ndid in first two approaches above, via categorical beliefs and\nconditional principles. There is no obvious contradiction in believing\nboth q and that one’s evidence doesn’t support\nq, even if one also has a correct belief that one believes\nq, so what may be irrational about the state must be based on\nfurther considerations. \nThe higher-order evidence approach can be usefully developed through\nthe example of hypoxia, a condition of impaired judgment that is\ncaused by lack of sufficient oxygen, and that is rarely recognized by\nthe sufferer at its initial onset. Hypoxia is a risk at altitudes of\n10,000 feet and higher (Christensen 2010b: 126–127). Suppose you\nare a pilot who does a recalculation while flying, to conclude that\nyou have more than enough fuel to get to an airport fifty miles\nfurther than the one in your initial plan. Suppose you then glance at\nthe altimeter to see that you’re at 10,500 feet and remember the\nphenomenon of hypoxia and its insidious onset. You now have evidence\nthat you might have hypoxia and therefore might have misidentified the\nsupport relations between your evidence and your conclusion. Are you\nnow justified in believing that you can get to the more distant\nairport? Are you justified in believing that your evidence supports\nthat claim? \nLetting F be the proposition that you have sufficient fuel to\nget to the more distant airport, the following four answers are\npossible: \n4) doesn’t seem plausible; even if you can’t actually\nbring yourself to believe F, being justified in believing your\nevidence supports F prima facie justifies you in believing\nF. \nHowever none of the other answers seem to be entirely adequate either.\nIt may seem, as in 1, that you could still be justified in believing\nF—in case your calculation was actually right—but\nno longer have strong enough reason to believe that the calculation\nwas right. However, this would also mean that you could justifiably\nbelieve “F, but my overall evidence doesn’t support\nF”. Feldman (2005: 110–111) argues that it is\nimpossible for this belief to be both true and reasonable since the\nsecond conjunct undermines the reasonableness of the first conjunct\n(cf. Bergmann 2005: 243; Gibbons 2006: 32; Adler 2002). And if you\nwere aware of having this belief then you would be believing something\nthat you know is unreasonable if true. You would be, in the view of\nFeldman and others, disrespecting the evidence. The state in which you\nbelieve “F and my evidence does not support F” is a case\nof “level-splitting”, also called epistemic\nakrasia, because you believe you ought not to have a particular\nbelief state but you have it anyway. \nThe second reply—you are justified in believing F and\njustified in believing that your evidence supports\nF—might seem reasonable in some cases, for example if the\nevidence about one’s evidence comes in the form of skeptical\nphilosophical arguments, which one may think are too\nrecherché to command revisions in our everyday\nbeliefs. But this attitude hardly seems acceptable in general since it\nwould mean never giving ground on a first-order belief when presented\nwith evidence that you may be wrong about what your evidence implies.\nWhen flying airplanes this kind of rigidity could even be hazardous.\nHowever, Feldman counts the second reply as a possible way of\nrespecting the evidence; it might be fitting not only when faced with\nradical skeptical arguments but also in cases where one’s\ninitial view of what the first-order evidence supports is actually\ncorrect. \nThe third reply, that after noting the altimeter evidence one\nisn’t justified in believing that one’s evidence supports\nF and also isn’t justified in believing F has the\nvirtue of caution but also the consequence that the altimeter evidence\ndeprives you of justification for believing F even if you do\nnot suffer from hypoxia, which Feldman takes to be problematic.\nHowever, this response, unlike the first answer, respects the\nhigher-order evidence; the altimeter evidence gives you some reason to\nbelieve that you might suffer from hypoxia, which gives you some\nreason to believe your evidence does not support F. The\nmisfortune of being deprived of your knowledge even if you don’t\nactually have hypoxia is an instance of the familiar misfortune of\nmisleading evidence in general. However, as we will see shortly,\nmisleading higher-order self-doubting evidence is distinct from other\nhigher-order evidence, and some recent authors have been led by this\nto the view that option 1 above—akrasia—can be more\nrational than option 3 in some cases. \nNotably, in both of the replies that Feldman counts as possible ways\nof respecting the evidence, 2) and 3), the first-order and\nhigher-order attitudes match; one either is justified in believing\nF and justified in believing one’s evidence supports\nF, or isn’t justified in believing F and also\nisn’t justified in believing one’s evidence supports\nF. On getting evidence suggesting one’s evidence does not\nsupport one’s conclusion, one should either maintain that it\ndoes so support and maintain the first-order belief—be\n“steadfast”—or grant that it might not support\none’s first-order belief and give up the latter—be\n“conciliatory”. If one thinks that which of these\nattitudes is the right response varies with the case, then the\n“total evidence” view will be attractive. On this view\nwhether the first order should concede to the second depends on the\nrelative strength of the evidence at each level. (Kelly 2010) \nIn the conciliatory cases, self-doubting higher-order evidence acts as\na defeater of justification for belief, which raises the question of\nits similarities to and differences from other defeaters. In John\nPollock’s (1989) terminology, some defeaters of justification\nfor a conclusion are rebutters, that is, are simply evidence against\nthe conclusion, while other defeaters are undercutters; they undermine\nthe relation between the evidence and the conclusion. (These are also\nreferred to as Type I and Type II defeaters.) The pilot we imagined\nwould be getting a rebutting defeater of her justification for\nbelieving that she had enough fuel for an extra 50 miles if she looked\nout her window and saw fuel leaking out of her tank. However, if the\naltimeter reading is a defeater, then as evidence about whether she\ndrew the right conclusion from her evidence it is definitely of the\nundercutting type. \nAll undercutters are evidence that has implications about the relation\nbetween evidence and conclusion, and to that extent are higher-order\nevidence. But the higher-order evidence that leads to self-doubt is\ndistinct from other undercutting-type evidence. In the classic Type II\ndefeater case one’s justification for believing a cloth is red\nis that it looks red and then one then learns that the cloth is\nilluminated with red light. This evidence undermines your\njustification for believing that the cloth’s looking red is\nsufficient evidence that it is red, by giving information about a\nfeature of the lighting that gives an alternative explanation of the\ncloth’s looking red. This is higher-order evidence because it is\nevidence about the cause of your evidence, and thereby evidence about\nthe support relation between it and the conclusion, but higher-order\nevidence in the cases of the doctor and the pilot are not about how\nthe evidence was caused, and not directly about how matters in the\nworld relevant to one’s conclusion are related to each other.\n \nSelf-doubting defeaters are about agents and they are in addition\nagent-specific (Christensen 2010a: 202). They are based on information\nabout you, the person who came to the conclusion about that support\nrelation, and have direct negative implications only for your\nconclusion. In the case of the cloth, anyone with the same evidence\nwould have their justification undercut by the evidence of the red\nlight. The evidence that the doctor is underslept, however, would not\naffect the justification possessed by some other doctor who had\nreasoned from the same evidence to the same conclusion using the same\nbackground knowledge. The evidence that the pilot is at risk of\nhypoxia would not be a reason for a person on the ground, who had\nreasoned from the same instrument readings to the same conclusion, to\ngive up the belief that the plane had enough fuel for fifty more\nmiles. \nChristensen argues that the agent-specificity of self-doubting\nhigher-order evidence requires the subject to “bracket”\nher first-order evidence in a way that other defeating evidence does\nnot. He thinks this means that in no longer using the evidence to draw\nthe conclusion, she will not be able to give her evidence its due\n(Christensen 2010a: 194–196). In contrast, in the case of the\nred light and other cases not involving self-doubt, once the redness\nof the light is added to the evidence, discounting the appearance of\nthe cloth doesn’t count as failing to respect that evidence\nbecause one is justified in believing it is no longer due respect as\nevidence of redness. However, arguably, the difference is not that the\nself-doubter must fail to give the evidence its due. In the\nhigher-order self-doubt cases we have seen the undercutting evidence\ndoes not give the subject reason to believe the evidential relation\nshe supposed was there is not there. It gives reason to think that she\ndoesn’t know whether the evidential relation is there, even if\nit is. If it isn’t then in bracketing her first-order evidence\nshe isn’t failing to give it its due; it isn’t due any\nrespect. Because self-doubting defeating evidence concerns the\nsubject’s knowledge of the evidential support relation and not\nthe relation itself it appears weaker than typical defeating evidence.\nHowever it is potentially more corrosive because it doesn’t give\nthe means to settle whether the evidential relation she endorsed is\nthere and so whether the first-order evidence deserves respect. \nIf the pilot gives up the belief in F and she had been right\nabout the evidential relation, then she will have been the victim of a\nmisleading defeater. Misleading defeaters present well-known\ndifficulties for a theory of justification based on the idea of defeat\nbecause Type II defeaters may be subject to further defeaters\nindefinitely. For example, if one had learned that the light\nilluminating the cloth was red via testimony, the defeat of\none’s justification for believing the cloth was red would be\ndefeated by good evidence that one’s source was a pathological\nliar. If we say that justified belief requires that there be no\ndefeaters then that leads us to disqualify any case where a misleading\ndefeater exists, and a subject will lose justification she might have\nhad, even if the misleading defeaters are distant facts the\nshe isn’t aware of. But if we refine the view to say that\nonly defeaters for which no defeater exists will undermine\njustification then a subject will count as justified even if she\nignores evidence that looks like a defeater for all she knows, because\nof the existence of a defeater defeater she doesn’t know about.\n In general we will face the question how many and which of the\nexisting defeater defeaters matter to whether we have a justified\nbelief (Harman 1973; Lycan 1977). \nIf despite being at an altitude of 10,500 feet our pilot did do the\ncalculation correctly, then her first-order evidence deserved her\nbelief and her evidence from her altitude and the phenomenon of\nhypoxia was a misleading defeater. It was good reason to worry that\nher blood oxygen was low, but it might not have been low, and it would\nbe possible in principle for her to get further evidence that would\nsupport this view, such as from the reading of a finger pulse\noximeter. Misleading defeaters are not new, but few would be tempted\nto say in the case where one gets evidence that the light is red that\nit would be rational for the subject to believe both “my\nevidence doesn’t support the claim that the cloth is red”\nand “the cloth is red”. However, for self-doubting type II\ndefeaters several authors have claimed that such level-splitting can\nbe rational. \nFor example, Williamson (2011) has argued that it is possible for the\nevidential probability of a proposition to be quite high, while it is\nalso highly probable that the evidential probability is low.\nFor instance one’s evidence about oneself might indicate that\none has made a mistake evaluating one’s evidence, a kind of\nmistake that would lead one to believe an unsupported conclusion,\nF. One evaluates the evidential probability of F as high\nbecause of one’s view of its evidence, but thinks F might\nwell be true without one’s belief in it being knowledge.  \nAnother way of arguing that it can be rationally required to respond\nto a real support relation—that is, for the pilot, to believe\nF—even when one has evidence it might not exist, and so,\nshould also believe one’s evidence does not (or might not)\nsupport that belief, is with the thought that a rational norm does not\ncease to apply just because a subject has evidence she hasn’t\nfollowed it (Weatherson 2008, 2010 (Other Internet Resources); Coates\n2012). This rationale would not sanction akrasia for one who learned\nthe light was red, because the point is restricted to cases where the\ndefeating evidence concerns the subject; we saw above that that makes\nthe defeating evidence weaker, and it is weaker in just the right way\nto support this approach. \nAnother way of arguing that akrasia can be rational is to take the\nexistence of a support relation as sufficient for justification of\nbelief in a proposition whether the subject has correct beliefs about\nthat support relation or not (Wedgwood 2011). This is motivated by\nexternalism about justification (see entry on\n internalist and externalist conceptions of epistemic justification),\n which might be more plausible for justifications subject to\nself-doubting higher order evidence because it is weaker than other\nundercutting evidence. In a different tack, it has been argued that a\ngeneral rule that takes negative self-doubting higher-order evidence\nto always exert some defeating force on first-order beliefs will be\nvery hard to come by. Because the subject is being asked to behave\nrationally in the face of evidence that she has not behaved\nrationally, she is subject to norms that give contradictory advice,\nand fully general rules for adjudicating between such rules are\nsubject to paradoxes (Lasonen-Aarnio 2014). \nPossibly the only thing harder than defending a fully general rule\nrequiring the first-order and second-order to match is accepting the\nintuitive consequences of level-splitting or akrasia. In this\nsituation one believes a certain belief state is (or might be)\nirrational but persists in it anyway. Horowitz (2014) has defended the\nNon-Akrasia Constraint (also sometimes referred to as an Enkratic\nPrinciple) which forbids being highly confident in both\n“q” and “my evidence does not support\nq”, in part by arguing that allowing akrasia\ndelivers highly counter-intuitive follow-on consequences in paradigm\ncases of higher-order evidence. For example, if our pilot maintains\nconfidence that F, she has enough fuel, how should she explain\nhow she got to a belief in F that she thinks is true when she\nalso thinks her evidence does not support F? It would seem that\nshe can only tell herself that she must have gotten lucky. \nShe could further tell herself that if the reason she persisted in\nbelieving F despite the altimeter reading was that she did in\nfact have low blood oxygen, then it was really lucky she had that\nhypoxia! Otherwise in evaluating her total evidence correctly she\nwould have come to a false belief that not-F. Reasoning so, the\npilot would be using her confidence in F as a reason to believe\nthe altimeter reading was a misleading defeater, which does not seem\nto be a good way of finding that out. Moreover, if she did this\nargument a number of times she could use the track record so formed to\nbootstrap her way to judging herself reliable after all (Christensen\n2007a,b; White 2009; Horowitz 2014—for general discussion of\nwhat is wrong with bootstrapping, see Vogel 2000 and Cohen 2002).\nAkrasia also sanctions correspondingly odd betting behavior. \nTo the extent that New Rational Reflection (of the previous section)\ncalls for a match between first-order and second-order beliefs it\ncounts as a Non-Akrasia principle. However, this particular matching\nrequirement is subject to several problems about evidence that are\nbrought out by Lasonen-Aarnio (2015). It requires substantive\nassumptions about evidence and updating our beliefs that are not\nobvious, and does not appear to respect the internalism about\nrationality that apparently motivates it, namely that one’s\nopinions about what states it is rational to be in match what states\none is actually in. Moreover it does not seem that New Rational\nReflection could embody the attractive idea that in general a subject\nmay rationally always be uncertain whether she is rational, i.e., even\nthe ideal agent can doubt that she is the ideal agent—an idea\nthat RatRef failed to conform with and that led to the formulation of\nthis new principle. This is because New Rational Reflection must\nassume that some things, such as conditionalization, cannot be doubted\nto be rational, i.e., to be what the ideal agent would do. It is not\nclear that we should have expected that everything can be doubted at\nonce (Vickers 2000; Roush et al. 2012), but this is an ongoing area of\nresearch (Sliwa & Horowitz 2015). \nAnother problem some have seen with any version of Rational Reflection\nis that it ultimately doesn’t allow the subject to remain unsure\nwhat degree of belief it is rational for her to have. It forces her to\nmatch her first-order degree of belief to a specific value, namely, a\nweighted average of the degrees of belief that she thinks it might be\nrational to have. It collapses her uncertainty about what is rational\nto a certainty about the average of the possibilities and forces her\nto embrace that precise value. This doesn’t allow a kind of\nmismatch or akrasia that might be the right way to respond to some\nhigher-order evidence, where one is confident that q and also\nthinks it likely that the evidence supports a lower confidence than\none has but is unsure what that lower confidence should be. Maybe one\nshould not defer to the higher-order evidence in this case, because\none is not sure what its verdict is (Sliwa & Horowitz 2015). See\nthe higher-order calibration approach below for a way of representing\nthis uncertainty that can give a justification for thinking that\nmatching to averages is rational. \nThe evidential approach locates the authority that second-order\ninformation about our judgment has over us in the idea that it is\nevidence and we should respect our evidence. A state of self-doubt on\nthis view is confidence both that q and that one’s\nevidence may well not support q. This state does not make the\nsubject inconsistent, but is a state of level-splitting or akrasia.\nMatching on this view is constituted by agreement between one’s\nlevel of confidence in q and how far one thinks one’s\nevidence supports q, and doesn’t allow akrasia, but\ntaking an evidential approach does not by itself settle whether\nrationality requires matching, or under what circumstances first- or\nsecond-order evidence should determine one’s first order\nconfidence. General rules about how to adjudicate in self-doubting\ncases between the claims of the two orders of evidence may be hard to\nget due to paradoxes, and to the need in every instance to hold some\nfeatures of rationality as undoubtable in order to initiate and\nresolve one’s doubt. \nAnother approach to self-doubt explains the authority that\nsecond-order evidence sometimes has over one’s first-order\nbeliefs by means of the idea that such evidence provides information\nabout the relation of one’s first-order beliefs to the way the\nworld is which one is obligated to take into account. That is,\nevidence like the altitude reading, sleep-deprivation, and empirical\nstudies of the unreliability of eyewitness testimony, provides\ninformation about whether your beliefs are reliable indicators of the\ntruth. We take the reading of a thermometer no more seriously than we\nregard the instrument as reliable. Our beliefs can be viewed as\nreadings of the world and treated the same way (Roush 2009; White\n2009; Sliwa & Horowitz 2015). \nOne way of formulating a constraint that says we should be no more\nconfident than we are reliable is by requiring guess\ncalibration (GC): \nIf I draw the conclusion that q on the basis of evidence\ne, my credence in q should equal my prior expected\nreliability with respect to q. (White 2009; Sliwa &\nHorowitz 2015) \nYour expected reliability with respect to q is understood as\nthe probability—chance or propensity—that your guess of\nq is true. You may not be sure what your reliability is so you\nwill use an expected value, a weighted average of the values you think\nare possible, and this should be a prior probability, evaluated\nindependently of your current belief that p. \nSelf-doubt on this picture would be a state in which you’ve\ndrawn conclusion q, say because your confidence in q\nexceeded a given threshold, but also have reason to believe your\nreliability with respect to q is not as high as that\nconfidence, and such a state would be a violation of GC. Whether this\nself-doubting state can be coherent when a subject knows her own\nbeliefs depends very much on how the reliability aspect is formulated.\nIf chances and propensities that your guesses are true are determined\nby frequencies of ordered pairs of guesses of q and truth or\nfalsity of q then self-doubt will make one incoherent here in\nthe way that representing oneself as an anti-expert did above, because\ncoherence and expected reliability will be logically\n equivalent.[2]\n In any case, GC requires matching between the orders in all cases, and\ntells us that the match is between your confidence and your\nreliability. \nGC makes sense of the intuition in some cases of self-doubt that the\nsubject should drop her confidence. The pilot on looking at the\naltimeter reading should cease to be so sure she has enough gas for\nfifty more miles because it gives her reason to think she is in a\nstate where her calculations will not reliably turn out truths.\nSimilarly, the doctor realizing she is severely underslept acquires\nreason to think she is in a state where her way of coming to beliefs\ndoes not reliably lead to true conclusions. The main dissatisfaction\nwith GC has been that it apparently cedes all authority to\nsecond-order evidence. In fact in GC’s formulation the\nconfidence you should finally have in q does not depend on how\nfar the evidence e supports q or how far you think\ne supports q, but only on what you think is your\npropensity for or frequency of getting it right about q,\nwhether you used evidence or not. \nThere may be cases where second-order evidence is worrying enough that\none’s first order conclusion should be mistrusted entirely, even\nif it was in fact soundly made—maybe the pilot and doctor are\nsuch cases since the stakes are high. But as we saw above it\ndoesn’t seem right across the board to take first-order evidence\nto count for nothing when second-order evidence is around. Here we can\nsee that by supposing that two people, Anton and Ana, reason to\ndifferent conclusions, q and not-q, on the basis of the\nsame evidence, Anton evaluating the evidence correctly, Ana not.\nSuppose both are given the same undermining evidence, say that people\nin their conditions only get it right 60% of the time. According to\nGC, rationality requires both of them to become 60% confident in their\nconclusions. Anton, who reasoned correctly from the evidence, is no\nmore rational than Ana, and has no right to a higher confidence in his\nconclusion q than Ana who reasoned badly has for her conclusion\nnot-q (Sliwa & Horowitz 2015). \nIt seems wrong that second-order evidence should always swamp the\nfirst-order verdict completely, so the calibration idea has been\nre-formulated so as to incorporate dependence on first-order evidence\nexplicitly, in the evidential calibration constraint\n(EC): \nWhen one’s evidence favors q over not-q,\none’s credence in q should equal the [prior] expected\nreliability of one’s educated guess that q. (Sliwa &\nHorowitz 2015) \nYour educated guess corresponds to the answer that you have the\nhighest credence in. Reliability of such a guess is defined as the\nprobability that you would assign the highest credence to the true\nanswer if you had to choose, and as above this probability is\nunderstood as your propensity to guess correctly. What is used in EC,\nas in GC, is an expected rather than actual reliability so it is\nweighted by how likely you think each possible reliability level is.\nThe difference between GC and EC is that in the latter the calibration\nrequirement depends explicitly on which conclusion the first-order\nevidence actually supports. On this principle, Anton, who reasoned\ncorrectly with the first-order evidence, is rational to be .6\nconfident in q rather than not-q because q is the\nconclusion the first-order evidence actually supports. The\ncontribution of the second-order evidence is to reduce his confidence\nin that conclusion from a high value to .6. \nAccording to Sliwa and Horowitz EC implies that Ana is not rational to\nhave a .6 confidence in not-q because not-q is not the\nconclusion the evidence actually favors. It would be rational for her\nto have .6 confidence that q, the same as Anton. This claim\nhighlights ambiguities in the phrase “one’s educated guess\nthat q”. Expected reliability is the probability that you\nwould assign the highest credence to the true answer, and the\nundermining evidence both Anton and Ana were given said that in the\nconditions they were in they had a 60% chance of their guess being the\nright\n answer.[3]\n If so, then neither of them have enough information to know the\nexpected reliability of a guess that q. If the\nprobabilities it appeared they were given, for one’s guess about\nq whatever it is, are to be usable, then the phrase in EC would\nneed to be interpreted “one’s educated guess that q\nor not-q”. \nThe fact that Ana did not actually guess that q makes for a\ndifficulty on either interpretation of the higher-order evidence and\nEC. Suppose the 60%-probability evidence they were given was indeed\nonly about guesses that q, and that “educated guess that\nq” in EC refers only to guesses that q. If the\nword “one’s” in the phrase “one’s\neducated guess that q” refers narrowly to the individual\nEC is being applied to, then EC does not imply anything for Ana, since\nshe did not guess that q. If “one’s” refers\nbroadly to anyone who guesses that q in the conditions Anton\nand Ana were in, then it does follow that what is rational for Ana is\nto believe q with 60% confidence. However, whether they are\ngiven general evidence about guesses that q or not-q, or\nseparate statistics on the success of q-guesses and of\nnot-q guesses, that higher-order evidence would have given Ana\nno means to correct herself. Because she got it wrong in the first\nstep by incorrectly concluding the first-order evidence supported\nnot-q she will also lack the means to correct herself, that is,\nto know whether she should be 60% confident in q or in\nnot-q. What EC says it is rational for her to do in the\nsituation is not something she has the ability to do. It might be\npossible to avoid these difficulties in a reformulation, but they are\nconsequences of the move in EC to add deference to the actual\nfirst-order evidential support relation. \nEC rules out many cases of bootstrapping that level-splitting views\nallow. For example, a bootstrapping doctor with evidence that he is\nunreliable assembles a sterling track record of success in his\nfirst-order decisions by judging the correctness of his conclusions by\nhis confidence in those conclusions. He thinks the evidence of his\nunreliability that he started out with has now been outweighed, so he\nconcludes that he is reliable after all. EC doesn’t allow this\nto be rational because it doesn’t allow him to assemble the\ntrack record in the first place, since he is obligated in every\ninstance to take into account the expected (un)reliability that he has\nevidence for. It is unclear, however, that EC similarly rules out\nbootstrapping for a subject who begins with no evidence at all about\nher reliability. \nThe EC reformulation of GC takes a different view of whether\nrationality requires us to get it right about the first-order support\nrelation or merely to get it right by our own lights, but this\nquestion is of course not specific to the topic of the relation of\nfirst-order and second-order evidence. For example, in a probabilistic\naccount evidential support relations are dictated completely by\nconditional probabilities. In a subjective Bayesian version of this\npicture rationality requires one to have the confidence dictated by\nthe subjective conditional probabilities that follow from one’s\nconfidences in other propositions. In an objective version rationality\nwould obligate one to have a confidence that is in line with the\nobjective conditional probabilities. There are other ways of making\nout subjective vs. objective views of the relevant evidential support\nrelations, and whether we should favor one or the other depends on\nmore general considerations that could provide independent reason to\nfavor one or the other view in the current debate about order\nrelations. \nThough this distinction is not specific to the current context it\nappears to have played a role in some authors’ intuitions about\nlevel-splitting above. For example, when Weatherson and Coates say\nthat the subject ought to believe what the first-order evidence\nactually supports because a norm does not cease to apply just because\none has evidence one hasn’t followed it, they assume that the\nrelevant norm and evidential support relation are objective.\nWedgwood’s appeal to externalism about justification also takes\nits bearings from what the first-order evidence actually supports\nrather than what to one’s own point of view it seems to support.\nA challenge for these approaches that achieve some additional\nauthority for first order evidence over the second order by requiring\ndeference to the actual evidential relation at the first order is to\nexplain why this is an obligation at the first order but a subject\nneed only take into account the expected reliability at the\nsecond order. \nAnother approach that sees rationality constraints between the two\norders as based on taking evidence of one’s expected reliability\ninto account derives the constraints top-down from general, widely\nheld, subjective Bayesian assumptions about evidential support, and\nexplicit representation of second-order reliability claims in\nhigher-order objective probability (Roush 2009). Like approach 2 above\nit uses subjective conditional probability to express the match that\nis required between the two orders, but it avoids the consequence we\nsaw in most of those approaches—and in the categorical approach\nand the other calibration approaches just discussed—that a state\nof self-doubt combined with knowledge of one’s beliefs is\nincoherent. Unlike the first two calibration approaches it gives an\nexplanation why calibration is part of rationality; it does this by\nderiving the constraint from another widely accepted assumption, the\nPrincipal Principle. \nWe can write a description of the relation of the subject’s\nbelief in q to the way the world is—her\nreliability—as an objective conditional probability: \nCalibration Curve\n\n\\(\\PR(q\\mid P(q)=x) = y \\)  \nThe objective probability of q given that the subject believes\nq to degree x is y. This is a curve, a function\nthat allows reliability y to vary with the independent variable\nof confidence, x, with different variables used in order to\nallow for the possibility that the subject’s degree of belief\ntends not to match the objective probability, and that the level and\ndirection of mismatch can vary with the level of confidence. The curve\nis specific to proposition q and to the subject whose\nprobability function is P. A subject is calibrated on q,\non this definition, if his calibration curve is the line \\(x =\n y\\).[4] \nCalibration curves are widely studied by empirical psychologists who\nfind that human beings’ reliability tends on average to vary\nsystematically and uniformly with confidence, with for example high\nconfidence tending to overconfidence, as in eyewitness testimony.\nDespite the averages found when subjects take tests in controlled\nsettings, the curves also vary with sub-group, individual traits,\nprofessional skills, and particular circumstances. All manner of\nhigher-order evidence about a subject’s belief-forming\nprocesses, methods, circumstances, track-record, and competences are\nrelevant to estimating this function. In real life no one could get\nenough evidence in one lifetime to warrant certainty about an\nindividual’s calibration curve for q in a set of\ncircumstances, but if one is a Bayesian one can form a confidence\nabout what a person’s calibration curve is, or what value it has\nfor some argument x, that is proportional to the strength of\none’s evidence about this, and one can have such a confidence\nabout one’s own calibration curve. \nOn this approach epistemic self-doubt is a state where one is\nconfident and more or less correct that one believes q to\ndegree x, that is, \\(P(q) = x\\), but also has an uncomfortably\nhigh level of confidence, say \\(≥ .5\\), that one is unreliable\nabout q at that confidence. That is, one has confidence \\(≥\n.5\\) that the objective probability of q when one has\nx-level of confidence in q is different from x,\nwhich we would write \\(P(\\PR(q\\mid P(q)=x) \\ne x) ≥ .5\\). Let us\nsay that the different value is y, so \\(P(\\PR(q\\mid P(q)=x) =\ny) ≥ .5\\), \\(y \\ne x\\). Whether or not the reason for this\nunreliability is that one tends to mistake evidential support\nrelations and whether one does or does not think a given evidential\nsupport relation obtains, make no general difference to this\nevaluation which is simply about whether one tends to get things right\nwhen doing the sort of thing one did in coming to be confident in\nq to level\n x;[5]\n it is about the relation between one’s confidence and the way\nthings are. \nOn this view, a state of self-doubt involves a combination of states\nof the following sort: \nYou actually believe q to degree x, you are confident\n(say at .99) that you so believe, and you have an uncomfortably high\nlevel of confidence that you are not calibrated for q at\nx, that the objective probability of q when you are\nx confident of q is y. This state escapes\nincoherence for two reasons. One is that one’s confidence either\nabout one’s degree of belief or one’s reliability is not\n1, and unlike some conditional probability formulations of self-doubt\nabove, the slightest uncertainty is enough to make it coherent to\nattribute a large discrepancy between your believed confidence and\nyour believed reliability. \nThis is made possible by the second factor, that (un)reliability is\nexpressed here as an objective conditional probability, and coherence\nalone does not dictate how subjective and objective probabilities must\nrelate. This is analogous to the reason that the approach via the\nmaximally rational subject above was able to represent a state of\nself-doubt as coherent, namely, that in evaluating my own P I\ncompare it to a different probability function. However, in this case\nthe second function is not an expert function that declares\nunconditionally what value the maximally rational subject’s\nvalue for q would be, but a calibration function, a conditional\nprobability that tells one what objective probability is indicated by\none’s subjective probability. One difference between the two\napproaches is that there are obvious ways to investigate calibration\ncurves empirically, whereas it would be hard to recruit enough\nmaximally rational subjects for a statistically significant study so\nwe tend to be left appealing to intuitions about what seems\nrational. \nOnce the defeating information about the relation of a subject’s\ncredences to the world is expressed in objective probability it can be\nrepresented explicitly as a consideration the subject takes on board\nin assessing the quality of the degree of belief she takes herself to\nhave in q and resolving the question what her degree of belief\nshould be, thus: \nThis asks for the degree of belief the subject should have in q\non condition that she actually has degree of belief x in\nq and the objective probability of q given that she has\ndegree of belief x in q is y. This expression is\nthe left-hand side of Self-Respect/Synchronic Reflection with a\nfurther conjunct added to its condition. SR doesn’t specify what\nto do when there is another conjunct and so is not suited to\nexplicitly represent the question of self-doubt, which means that the\nself-doubting examples above are not counterexamples to it (Roush\n2009). However, some in the past have endorsed variants on an\nunrestricted version of SR (Koons 1992; Gaifman 1988) where the value\nof this expression is x regardless of what other conjunct might\nbe present: \nUnrestricted Self-Respect\n (USR)[6]\n\\(P(q\\mid P(q)=x \\amp r) = x\\), for r any proposition  \nDutch book arguments that might give support to SR do not do the same\nfor USR, leaving us with a need to find other ways of evaluating it\nwhen r is the statement of a calibration curve. \nIt is not incoherent but it is baldly counterintuitive to suppose that\nthe subject should have degree of belief x when she believes\nthat her so believing is an indicator that the objective probability\nof q is not x, and a principled argument can also be\nmade to this effect (Roush 2009). Unpacking the condition \\(P(q)=x\n\\amp \\PR(q\\mid P(q)=x) = y\\), it seems to say that my credence is\nx and when my credence is x the objective probability is\ny, inviting us to discharge and infer that the objective\nprobability is y. If\n so,[7]\n then the expression would reduce to: \nwhich is the left-hand side of a generalization of the Principal\nPrinciple (see entry on\n David Lewis) \nPrincipal Principle\n (PP)[8]\n\\(P(q\\mid Ch(q)=y) = y\\)  \nfrom chance to any type of objective probability. PP says that your\ncredences in propositions should conform to what you take to be their\nchances of being true, and, admissibility issues notwithstanding, it\nis hard to deny that there exists a domain in which the Principal\nPrinciple is compelling, and surely one where the generalization to\nany type of objective probability is too. If so then the answer to the\nquestion what the subject’s credence in q ought to be in\nlight of her consideration of information about her reliability\nis: \nCal\n\n\\(P(q\\mid (P(q)=x \\amp \\PR(q\\mid P(q)=x)=y)) = y \\)  \nCal says that your credence in q given that your credence in\nq is x and the objective probability of q given\nthat your credence in q is x is y, should be\ny. \nCal is a synchronic constraint, but if we revise our credences by\nconditionalization then it implies a diachronic constraint: \nRe-Cal\n\n\\(P_{n+1}(q) = P_{n}(q\\mid (P_{n}(q)=x \\amp \\PR(q\\mid P_{n}(q)=x)=y))\n= y \\)  \nThis calibration approach tells the subject how to respond to\ninformation about her cognitive impairment in every case. It uses the\ninformation about herself to correct her belief about the world.\nIntuitively it is a graded generalization of the thought that if you\nknew of someone (or yourself) that he invariably had false beliefs,\nthen you could gain a true belief by negating everything he said. \nCal and Re-Cal give an explicit characterization of self-doubt and\njustification of a unique and determinate response to it on the basis\nof deeper principles that are compelling independently of the current\ncontext. Cal follows from only two assumptions, first that\nprobabilistic coherence is a requirement of rationality, and second\nthat rationality requires one’s credences to align with what\naccording to one’s evidence are the objective probabilities.\nRe-Cal comes from further assuming that updating our beliefs should\noccur by conditionalization. \nAlthough self-doubt under the current definition of it is not an\nincoherent state, Cal implies that rationality always requires a\nresolution of the doubt that brings matching between the two levels,\nand tells us that the matching consists in the alignment of subjective\nand perceived objective probabilities. High confidences in\n“q”, “I have confidence x in\nq”, and “the objective probability of q when\nI have confidence x in q is low” are not\nincoherent, but they do violate the Principal Principle. Re-Cal tells\nus how to get back in line with PP. \nThough Re-Cal has us conditioning on second-order evidence, the\nadjustment it recommends depends on both first- and second-order\nevidence and does not always favor one level or the other. How much\nauthority the second-order claim about the reliability/calibration\ncurve has depends very much on the quality of one’s evidence\nabout it. This can be seen by imagining being uncertain about, e.g.,\none’s calibration curve, i.e., \\(P(\\PR(q\\mid P(q) = x) = y) <\n1\\), and doing a Jeffrey conditionalization version of Re-Cal (Roush\n2017, Other Internet Resources). But even in case one has perfect\nknowledge of one’s calibration curve, the role of the\nfirst-order evidence in determining one’s first-order belief is\nineliminable. The verdict, the level of confidence, that the first\norder gave you for q is the index for determining which point\non the calibration curve is relevant to potentially correcting your\ndegree of belief. To understand why this is far from trivial, recall\nthat the curve can in principle and does often in fact have different\nmagnitudes and directions of distortion at different confidences. The\nverdict’s dependence on the first-order evidential support\nrelation is different from that of EC in another way, since it uses\nnot the objective support relation at the first order but the\nconsequences of the subject’s take on it. Thus, Ana above would\nnot be left not knowing how to make herself rational. \nThe fact that the update proceeds by conditionalization means that all\nof the kinds of evaluation of evidence that conditionalization imposes\ncome along with that. Misleading self-doubting defeaters troubled some\nauthors above and led them to level-splitting views, but they are\nhandled by Re-Cal as conditionalization always handles them.\nSelf-doubting defeaters are processed at face value as relevant to the\ncalibration curve in proportion to their quality as evidence.\nConvergence theorems tell us that if the world isn’t\nsystematically deceptive then the misleading defeaters will be washed\nout, that is, defeated by some other evidence, in the long run. In\nsome cases that will happen only long after we’re all dead, but\nif one views that as inadequate then that is a dissatisfaction with\nsubjective Bayesianism, and is not specific to its usage here. \nThe approach to epistemic self-doubt in terms of higher-order\nprobability allows the state of self-doubt to be rational (coherent),\nand to be rationally resolved. Cal expresses a requirement of matching\nbetween the two orders in all cases, though it does not imply that\nattributing a mismatch to oneself is incoherent. Neither\norder is always dominant; both orders always make a contribution to\ndetermining the resolution at the first order of conflicts between\norders, and their relative contribution depends on the quality of the\nevidence at each order. Cal and Re-Cal explain why one should revise\nin light of higher-order evidence, when one should, by reference only\nto probabilistic coherence, the Principal Principle, and\nconditionalization. Cal and Re-Cal are general and make available all\nof the resources of the Bayesian framework for analysis of\nhigher-order evidence. A further notable fact about the framework is\nthat Re-Cal allows for cases where news about one’s reliability\nshould increase one’s confidence, which would be appropriate for\nexample in cases, easy to imagine, where one acquired evidence that\none was systematically underconfident. Thus, it is possible\nfor second-order evidence to make it rational to be not only steadfast\nor conciliatory, but even emboldened.","contact.mail":"sherri.roush@gmail.com","contact.domain":"gmail.com"}]
