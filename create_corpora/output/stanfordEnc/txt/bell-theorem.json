[{"date.published":"2004-07-21","date.changed":"2019-03-13","url":"https://plato.stanford.edu/entries/bell-theorem/","author1":"Wayne Myrvold","author1.info":"http://publish.uwo.ca/~wmyrvold/","author2.info":"https://www.inrim.it/ugov/persone/72","entry":"bell-theorem","body.text":"\n\n\nBell’s Theorem is the collective name for a family of\nresults, all of which involve the derivation, from a condition on\nprobability distributions inspired by considerations of local\ncausality, together with auxiliary assumptions usually thought of as\nmild side-assumptions, of probabilistic predictions about the results\nof spatially separated experiments that conflict, for appropriate\nchoices of quantum states and experiments, with quantum mechanical\npredictions. These probabilistic predictions take the form of\ninequalities that must be satisfied by correlations derived from any\ntheory satisfying the conditions of the proof, but which are violated,\nunder certain circumstances, by correlations calculated from quantum\nmechanics. Inequalities of this type are known as Bell\ninequalities, or sometimes, Bell-type inequalities.\nBell’s theorem shows that no theory that satisfies the\nconditions imposed can reproduce the probabilistic predictions of\nquantum mechanics under all circumstances. \n\n\nThe principal condition used to derive Bell inequalities is a\ncondition that may be called Bell locality, or\nfactorizability. It is, roughly, the condition that any\ncorrelations between distant events be explicable in local terms, as\ndue to states of affairs at the common source of the\nparticles upon which the experiments are performed. See section 3.1\nfor a more careful statement.\n\n\nThe incompatibility of theories satisfying the conditions that entail\nBell inequalities with the predictions of quantum mechanics permits an\nexperimental adjudication between the class of theories satisfying\nthose conditions and the class, which includes quantum mechanics, of\ntheories that violate those conditions. At the time that Bell\nformulated his theorem, it was an open question whether, under the\ncircumstances considered, the Bell inequality-violating correlations\npredicted by quantum mechanics were realized in nature. Beginning in\nthe 1970s, there has been a series of experiments of increasing\nsophistication to test whether the Bell inequalities are satisfied.\nWith few exceptions, the results of these experiments have confirmed\nthe quantum mechanical predictions, violating the relevant Bell\nInequalities. Until recently, however, each of these experiments was\nvulnerable to at least one of two loopholes, referred to as the\ncommunication, or locality loophole, and the\ndetection loophole (see section 5). Finally, in 2015,\nexperiments were performed that demonstrated violation of Bell\ninequalities with these loopholes blocked. This has consequences for\nour physical worldview; the conditions that entail Bell inequalities\nare, arguably, an integral part of the physical worldview that was\naccepted prior to the advent of quantum mechanics. If one accepts the\nlessons of the experimental results, then some one or other of these\nconditions must be rejected.\n\n\nFor much of the interval between the original publication of\nBell’s theorem and the experiments of Aspect and his collaborators, interest in Bell’s theorem was confined to a handful of\nphysicists and philosophers. During that period, much of the\ndiscussions on the foundations of physics occurred in a mimeographed\npublication entitled Epistemological Letters. In the wake of\nthe Aspect experiments (Aspect, Grangier, and Roger, 1982; Aspect,\nDalibard, and Roger 1982), there was considerable philosophical\ndiscussion of the implications of Bell’s theorem; see Cushing\nand McMullin, eds. (1989), for a snapshot of the philosophical\ndiscussions of the time. Interest was also stimulated by the\npublication of a collection of Bell’s papers on the foundations\nof quantum mechanics (Bell 1987b). The rise of quantum information\ntheory, which, among other things, explores the ways in which quantum\nentanglement can be used to perform tasks that would not be feasible\nclassically, also contributed to raising awareness of the significance\nof Bell’s theorem, which throws into sharp relief the difference\nbetween quantum entanglement-based correlations and classical\ncorrelations. The year 2014 was the 50th anniversary of the original\npublication of Bell’s theorem, and was marked by a special issue\nof Physical Review A (47, number 42, 24\nOctober 2014), a collection of essays (Bell and Gao, eds., 2016), and\na large conference comprising over 400 attendees (see Bertlmann and\nZeilinger, eds., 2017). The interested reader is urged to consult\nthese collections for an overview of current discussions on topics\nsurrounding Bell’s theorem.\n\nIn 1964 John S. Bell, a native of Northern Ireland and a staff member\nof CERN (European Organisation for Nuclear Research) whose primary\nresearch concerned theoretical high energy physics, published a paper\n(Bell 1964) in the short-lived journal Physics, which\neventually transformed the study of the foundation of quantum\nmechanics.  \nThe paper showed, under conditions that were relaxed in later work by\nBell (1971, 1976) himself and by his followers (Clauser, Horne, Shimony, and Holt 1969, Clauser and Horne 1974, Aspect 1983, Mermin 1986),\nthat, on the assumption of certain auxiliary conditions, no physical\ntheory that satisfies a certain locality condition, which may be\ncalled Bell locality, can fully reproduce the quantum\nprobabilities for outcomes of experiments. Since that time, variants\non the theorem, with family resemblances, have been formulated.\n“Bell’s Theorem” is the collective name for the\nentire family. \nThe theorem has roots in Bell’s investigations into the status\nof the hidden-variables program, and on earlier work concerning\nquantum entanglement.  \nBell presented several formulations of the theorem over the years\n(Bell 1964, 1971, 1976, 1990), and variants of it have been presented\nby others. The original derivation (1964) relied on a set-up involving\nperfect anticorrelation of the results of spin experiments on pairs of\nspin-1/2 particles prepared in the singlet state. Under this\ncondition, the Bell locality condition entails that outcomes of\nexperiments are predetermined by the complete specification of state,\na condition which we will call outcome determinism (OD).\nClauser, Horne, Shimony and Holt (1969) derived an inequality, the\nCHSH inequality, that does not require this assumption. Though in\ntheir proof they employed the condition OD, this condition is\nunnecessary for the derivation of the inequality, as shown by Bell\n(1971), who provided a proof of the CHSH inequality that relies on\nneither the assumption of perfect anticorrelation nor an assumption of\noutcome determinism.  \nOne line of investigation in the prehistory of Bell’s Theorem is\nBell’s examination of the hidden-variables program. This program\ninvolves supplementation of the quantum mechanical state of a system\nby further “elements of reality”, or “hidden\nvariables”, the incompleteness of the quantum state being the\nexplanation for the statistical character of quantum mechanical\npredictions concerning the system. A pioneering version of a hidden\nvariables theory was proposed by Louis de Broglie in 1926–7 (de\nBroglie 1927, 1928), and revived by David Bohm in 1952 (Bohm 1952; see\nalso the entry on\n Bohmian mechanics). \nIn a paper (Bell 1966) that was written before the one in which\nBell’s theorem first appeared, but, due to an editorial mishap,\nwas published later, Bell raises the question of the viability of\na hidden-variables theory that reproduces the statistical predictions\nof quantum mechanics via averaging over better defined states that\nuniquely determine the result of any experiment that could be\nperformed. In this paper he examines several theorems that had been\npresented as no-go theorems for theories of this sort, and\nsupplements them with one of his own, a theorem that was\nindependently formulated by Specker (1960), and published by Kochen\nand Specker (1967), and has come to be known as the Kochen-Specker\nTheorem or Bell-Kochen Specker Theorem (see\n entry on the Kochen-Specker Theorem\n for more details). In each case Bell argues that the proof contains\npremises that are physically unwarranted. \nThe Bell-Kochen-Specker theorem is a corollary of Gleason’s\ntheorem (Gleason 1957), though Bell and Kochen-Specker obtain it\ndirectly, and not via Gleason’s theorem, whose proof is\nconsiderably more intricate. The question addressed by Gleason has to\ndo with assignments of probabilities to closed subspaces of a Hilbert\nspace (or, equivalently, to projection operators onto such subspaces),\nsuch that the probabilities assigned to orthogonal projections are\nadditive. Gleason proved that, in a Hilbert space of dimension 3 or\ngreater, any such assignment of probabilities can be represented by a\ndensity operator. The BKS theorem deals with the special case in which\nthe assignments are confined to the values 1 or 0. \nThe assumption that a definite value (1 or 0) is to be assigned to\neach projector on the system’s Hilbert space, with the condition\nthat values assigned to commuting projectors be additive, is a\nweakening of the assumption of the von Neumann no-go theorem (von\nNeumann 1932), which assumes that the quantum mechanical additivity of\nexpectation values of all observables, whether represented by\ncommuting operators or not, extends to the hypothetical\ndispersion-free states (see section 2 of entry on\n the Kochen-Specker Theorem).\n Despite the prima facie plausibility of this assumption,\nBell regards it, too, as physically unmotivated, and therefore, unlike\nKochen and Specker, does not regard the Bell-Kochen-Specker theorem as\na no-go theorem for hidden-variables theories. The reason for this is\nthat the assumption embodies a condition that was later to be known as\n noncontextuality.[1]\n In a Hilbert space of dimension greater than two, any projection\noperator will be a member of more than one complete set of commuting\nprojections. For each of these complete sets, there will be an\nexperiment whose outcomes correspond to the projections in the set.\nThe assumption of noncontextuality amounts to the assumption that a\nvalue can be assigned to a projection operator that is independent of\nwhich of these experiments is to be performed, or as Bell puts it,\nthat “measurement of an observable must yield the same value\nindependently of what other measurements may be made\nsimultaneously.” The assumption need not hold; “[t]he\nresult of an observation may reasonably depend not only on the state\nof the system (including hidden variables) but also on the complete\ndisposition of the apparatus” (Bell 1966, 451; 1987b and 2004,\n9 ). \nNoncontextual hidden-variables theories that reproduce the predictions\nof quantum mechanics are ruled out by the Bell-Kochen-Specker theorem.\nA natural question arises as to the possibility of a contextual\nhidden-variables theory on which the unavoidable contextuality is\nrestricted to local dependencies. Is it possible to have a \ntheory on which the outcome of an experiment performed in some spatial\nregion \\(A\\) is determined by the complete state of a system in a\nway that does not depend on the disposition of experimental apparatus\nat a distance from \\(A\\)? \nBell’s article ends with a brief exposition of the Bohm theory,\nnoting in particular the feature that “in this theory an\nexplicit causal mechanism exists whereby the disposition of one piece\nof apparatus affects the results obtained with a distant piece”\n(Bell 1966, 452; 1987b and 2004, 11). The article ends with the\nremark, \nTo the second of the above-quoted sentences is attached a note:\n“Since the completion of this paper such a proof has been\nfound.” The potential drama of the announcement was spoiled by\nthe fact that the follow-up paper containing the proof (Bell 1964) had\nalready been published (see Jammer 1974, 303, for an account of the\ncircumstances that led to the publication delay). \nThe fact that Bell’s theorem has roots into investigations on\nhidden-variables theories has led to a misconception that the theorem\nis a no-go theorem for hidden-variables theories tout court.\nThere could be no such theorem, since, as Bell himself repeatedly\nemphasized, there is a functioning hidden-variables theory, the de\nBroglie-Bohm theory. \nAnother line of investigation leading to Bell’s Theorem was the\ninvestigation of quantum mechanical entangled states, that is, quantum\nstates of a composite system that cannot be expressed either as\nproducts of quantum states of the individual components, or as\nmixtures of product states. That quantum mechanics admits of such\nentangled states was discovered by Erwin Schrödinger (1926) in\none of his pioneering papers, but the significance of this discovery\nwas not emphasized until the paper of Einstein, Podolsky, and Rosen\n(1935). They examined correlations between the positions and the\nlinear momenta of two well separated spinless particles and concluded\nthat in order to avoid an appeal to nonlocality these correlations\ncould only be explained by “elements of physical reality”\nin each particle — specifically, both definite position and\ndefinite momentum — and since this description is richer than\npermitted by the uncertainty principle of quantum mechanics their\nconclusion is effectively an argument for a hidden variables\ninterpretation.\n [2]\n See also the entry on the\n Einstein-Podolsky-Rosen paradox. \nIn the present section the pattern of Bell’s 1964 paper will be\nfollowed: formulation of a framework, derivation of an inequality,\ndemonstration of a discrepancy between certain quantum mechanical\nexpectation values and this inequality. As already mentioned,\nBell’s 1964 derivation assumed an experiment involving perfect\nanticorrelation of the results of aligned Stern-Gerlach experiments on\na pair of entangled spin-\\(\\frac{1}{2}\\) particles. Experimental tests, in\nwhich perfect anticorrelation (or correlation) may be approximated but\ncannot be assumed to hold exactly, require this assumption to be\nrelaxed. Papers which took the steps from Bell’s 1964\ndemonstration to the one given here are Clauser, Horne, Shimony and\nHolt (1969), Bell (1971), Clauser and Horne (1974), Aspect (1983) and\nMermin\n (1986).[3]\n Other strategies for deriving Bell-type theorems will be mentioned in\n Section 6. \nThis conceptual framework first of all postulates an ensemble of pairs\nof systems, the individual systems in each pair being labeled as 1 and\n2. Each pair of systems is characterized by a “complete\nstate” \\(\\lambda\\) which contains the entirety of the\nproperties of the pair at the moment of generation. No assumption\nwhatsoever is made about the nature of the state \\(\\lambda\\).\nThe state space \\(\\Lambda\\), which is the totality of all\npossible complete states \\(\\lambda\\), could be a set of quantum\nstates and nothing more, or a set whose elements are quantum states\nsupplemented by additional variables, or something more exotic,\nperhaps some state space as yet unthought of. \nWe make the assumption, which remains tacit in most expositions, that\nwe have an appropriate choice of subsets of \\(\\Lambda\\) to be\nregarded as the measurable subsets, forming a measurable space to\nwhich probabilistic considerations may be applied. It is assumed that\nthe mode of generation of the pairs establishes a probability\ndistribution \\(\\rho\\) that is independent of the adventures of\neach of the two systems after they separate. This does not preclude\ntemporal evolution of the properties of the two systems after\nseparation. What is assumed is that the state \\(\\lambda\\)\nprescribes probabilities for subsequent events (including any temporal\nevolution), and thereby probabilities for outcomes of experiments to\nbe performed on the systems. \nDifferent experiments may be performed on each system. We will use\n\\(a, a'\\) as variables ranging over possible\nexperiments on 1, and \\(b, b'\\) as variables that\nrange over experiments on 2. It is not assumed that these parameters\ncapture the complete state of the experimental apparatus, which might\nhave a range of microstates corresponding to each experimental\nsetting. \nThe result of an experiment with setting \\(a\\) on system 1 is\nlabeled by a real parameter \\(s\\), which can take on values from\na discrete set \\(S_a\\) of real numbers in the interval\n[\\(-1, 1\\)]. Likewise, the result of an experiment on 2 is labeled\nby a parameter \\(t\\), which can take on any of a discrete set of\nreal numbers \\(T_b\\) in \\([-1, 1]\\). As suggested by\nthe subscripts, the sets of potential outcomes may depend on the\nexperimental settings. The restriction of the values of the outcome\nlabels to lie in the interval \\([-1, 1]\\) is of no physical significance, and\nis a choice made only for convenience. Indeed, the use of numbers to\nlabel outcomes is merely a matter of convenience, and, can, if\ndesired, be dispensed with, in favour of relations expressed solely in\nterms of the relevant probabilities, as in the CH inequality,\ninequality (25), below. Bell’s own version of his theorem assumed\nexperiments with two possible outcomes, labelled \\(\\pm 1\\). Other\nvariants of the theorem involve larger sets of potential outcomes. \nWe assume that, for each pair of settings \\(a, b\\), and every\n\\(\\lambda\\) in \\(\\Lambda\\), there is a probability function\n\\(p_{a,b}(s,t \\mid \\lambda)\\), which takes on values in the interval\n[0, 1] and sums to unity when summed over all \\(s\\) in \\(S_a\\) and\n\\(t\\) in \\(T_b\\). These response functions may include implicit\naveraging over possible states of the experimental\napparatus.[4] We\ncan use these probability functions — which we will call\nresponse probabilities — to define marginal\nprobabilities: \nHere, and in what follows, it is to be understood that the sums be\ntaken over all \\(s \\in S_a\\) and \\(t \\in T_b\\). Define\n\\(A_{\\lambda}(a, b)\\), \\(B_{\\lambda}(a, b)\\) as the expectation\nvalues, for complete state \\(\\lambda\\), of the outcomes of experiments\non system 1 and system 2, respectively, when the settings are \\(a,\nb\\).  \nNow define the expectation value of the product \\(st\\)\nof outcomes: \nBell-type inequalities follow from a condition, inspired by locality\nconsiderations, which has been called Factorizability, or\nBell locality. \nThis is the condition formulated explicitly by Bell in his later\nexpositions of Bell’s theorem (Bell 1976, 1990). It should be\nnoted that Bell himself regarded this condition “not as the\nformulation of ‘local causality’, but as a\nconsequence thereof” (Bell 1990, 109; 2004, 243). Bell\nrefers to the factorizability condition (F) as the condition that the\ncorrelations be locally explicable (Bell 1981, C2–55;\n1987b and 2004, 152; see also 1990, 109; 2004, 243). This has two\ncomponents: the condition that the correlations be explained, and not\ntaken as primitive, and the condition that the explanation be local.\nThis will be discussed further in section 3.1.  \nAs we have seen, Bell’s investigations were stimulated, in part,\nby the question of the prospects for theories in which the complete\nstate uniquely determines the outcome of any experiment, and quantum\nuncertainty relations reflect incompleteness of the usual\nspecification of state. For such a theory, the response probabilities\n\\(p_{ab}(s,t|\\lambda)\\)\ntake on the extremal values 0 or 1. Let us call this condition OD,\nfor outcome determinism. It is also sometimes referred to,\nmisleadingly, as realism (see discussion in\n section 3.3,\n below). \nSuppes and Zanotti (1976) showed that, for the special case of perfect\ncorrelations between outcomes of the two experiments, in which an\noutcome of an experiment on one system makes possible prediction with\nprobability one of the outcome of an experiment on the other, OD must\nbe satisfied if the factorizability condition (F) is. This applies to\nthe case considered by Bell 1964. In Bell 1971 and subsequent\nexpositions Bell provided a generalization that does not presume\nperfect correlation and does not require OD, either as a supposition\nor as a consequence of other\n suppositions.[5] \nIf the factorizability condition (F) is satisfied, then \nThe above definitions are valid for experiments with any number of\ndiscrete outcomes. An important special case is that in which each\nexperiment has only two distinct outcomes, which we may label by\n\\(\\pm 1\\). For the case of bivalent experiments, (4) is equivalent to\ncondition (F). \nNow consider the quantities \nLet S\\(_{\\varrho}\\) denote the corresponding relation\nbetween the expectation values of the \\(E_{\\lambda}s\\),\nwith respect to the preparation distribution \\(\\rho\\). \nSince the absolute value of the average of any random variable cannot\nbe greater than the average of its absolute value, is clear that \nWe now have the materials in hand required to state and prove a\nBell-type theorem. The first step consists of showing that, if the\nfactorizability condition (F) is satisfied, then \nThe second step consists of showing that there are quantum states and\nexperimental set-ups that are such that the quantum-mechanical\nexpectation values violate the inequality (8). This shows that no\ntheory satisfying the factorizability condition can reproduce the\nstatistical predictions of quantum mechanics in all situations.\nMoreover, the inequality furnishes a bound on how close the\npredictions of such a theory can come to reproducing quantum\nmechanical predictions. The inequality (8), due to Clauser, Horne,\nShimony, and Holt (1969), is known as the CHSH inequality.\n \nWe now prove the first part of the theorem, namely, that the CHSH\ninequality follows from the factorizability condition (F). If F is\nsatisfied, then, using (4), and the fact that \\(A_{\\lambda}(a)\\) and\n\\(A_{\\lambda}(a')\\) lie in the interval \\([-1,1]\\), \nIt is easy to check that \nSince \\(B_{\\lambda}(b)\\) and \\(B_{\\lambda}(b')\\) also lie in the\ninterval \\([-1,1]\\), from (9) and (10) we conclude that, for every\n\\(\\lambda\\),  \nSince this bound holds for every value of \\(\\lambda\\), it must\nalso hold for the expectation value of\n\\(S_{\\lambda}\\). \nThis, together with (7), yields the CHSH inequality (8). \nThe final step of the proof our Bell-type theorem is to exhibit a\nsystem, a quantum mechanical state, and a set of quantities for which\nthe statistical predictions violate inequality (8). The example used\nby Bell stems from Bohm’s variant of the EPR thought-experiment\n(Bohm 1951, Bohm and Aharonov 1957). A pair of spin-\\(\\frac{1}{2}\\) particles\nis produced in the singlet state, \nwhere \\(\\mathbf{n}\\) is an arbitrarily chosen direction, and\n\\(|\\mathbf{n}+\\rangle , |\\mathbf{n}-\\rangle\\) are\nspin-up and spin-down eigenstates of spin in the \\(\\mathbf{n}\\)\ndirection. The state is rotationally invariant, and hence the\nexpression (13) represents the state for any direction\n\\(\\mathbf{n}\\). If Stern-Gerlach experiments are performed on the\ntwo particles, then, regardless of the direction of the axes of the\ndevices, there is equal probability of both results on each side. If\nexperiments are done with the axes of the two devices aligned, the\nresults are guaranteed to be opposite; spin-up on one will be obtained\nin one experiment if and only if spin-down is obtained on the other.\nIf the axes are at right angles, the results are probabilistically\nindependent. In the general case, with device axes in directions given\nby unit vectors \\(\\mathbf{a}, \\mathbf{b}\\), respectively,\nwith results labelled \\(\\pm 1\\), then the expectation value of the\nproduct of the outcomes is given by \nwhere \\(\\theta_{\\mathbf{a}\\mathbf{b}} = \\theta_{\\mathbf{a}} -\n\\theta_{\\mathbf{b}}\\) is the angle between the vectors\n\\(\\mathbf{a},\\mathbf{b}\\). \nThough the example of spin-\\(\\frac{1}{2}\\) particles in the singlet state is\nubiquitous in the literature as an illustrative example,\npolarization-entangled photons have been more significant for\nexperimental tests of Bell inequalities. Consider a pair of photons 1\nand 2 propagating in the \\(z\\)-direction. Let \\(|x\\rangle_j\\) and \\(|y\\rangle_j\\)\nrepresent states in which photon \\(j \\; (j =1, 2)\\) is\nlinearly polarized in the \\(x\\)- and \\(y\\)- directions,\nrespectively. Consider the following state vector, \nwhich is invariant under rotation of the \\(x\\) and \\(y\\)\naxes in the plane perpendicular to \\(z\\). The total quantum state\nof the pair of photons 1 and 2 is invariant under the exchange of the\ntwo photons, as required by the fact that photons are integral spin\nparticles. Suppose now that photons 1 and 2 impinge respectively on\nthe faces of birefringent crystal polarization analyzers I and II,\nwith the entrance face of each analyzer perpendicular to \\(z\\).\nEach analyzer has the property of separating light incident upon its\nface into two outgoing non-parallel rays, the ordinary ray\nand the extraordinary ray. The transmission axis of the\nanalyzer is a direction with the property that a photon polarized\nalong it will emerge in the ordinary ray (with certainty if the\ncrystals are assumed to be ideal), while a photon polarized in a\ndirection perpendicular to \\(z\\) and to the transmission axis\nwill emerge in the extraordinary ray. See Figure 1: Figure 1\n\n(reprinted with permission) \nPhoton pairs are emitted from the source, each pair quantum\nmechanically described by \\(\\ket{\\Phi}\\) of Eq. (15). I and II are\npolarization analyzers, with outcome \\(s=1\\) and \\(t=1\\)\ndesignating emergence in the ordinary ray, while \\(s = -1\\)\nand \\(t = -1\\) designate emergence in the extraordinary\nray. The crystals are also idealized by assuming that no incident\nphoton is absorbed, but each emerges in either the ordinary or the\nextraordinary ray.  \nThe expectation value, in state \\(\\ket{\\Phi}\\), of the product of\n\\(s\\) and \\(t\\), is \nNote that this displays the same sort of sinusoidal dependence on\nangle exhibited by (14), with \\(2\\theta\\) replacing\n\\(\\theta\\). \nOften, in popular writings, the case of aligned devices is the only\none mentioned, and the perfect anticorrelation (for spin-\\(\\frac{1}{2}\\)\nparticles in the singlet state) or correlation (for photons in state\n\\(\\ket{\\Phi}\\)) of results in this case is offered as evidence of\n“spooky action at a distance.” In fact, as Bell (1964,\n1966) demonstrated by means of simple toy models, this behaviour can\nbe reproduced by entirely local means. An important insight of\nBell’s was to consider the less-than perfect correlations\nobtained when the device axes are not aligned. In Bell’s toy\nmodel, correlations fall off linearly with the angle between the\ndevice axes, whereas the quantum correlations (14, 16) fall off\nsinusoidally; the decrease in correlations away from the case of\nperfect alignments is less steep than in the toy model. Bell’s\ntheorem shows that this sort of behaviour is not a peculiarity of his\nmodel; no model satisfying the condition F can reproduce the quantum\ncorrelations for all angles. This can be seen by considering the\nquantum predictions (14, 16) and plugging them into the expression for\n\\(S\\). For a pair of spin-\\(\\frac{1}{2}\\) particles in the singlet state,\nwe have, \nChoose coplanar unit vectors \\(\\mathbf{a},\\) \\(\\mathbf{a}',\\)\n\\(\\mathbf{b},\\) \\(\\mathbf{b}'\\) such that \\(\\theta_{\\mathbf{b}'} -\n\\theta_{\\mathbf{a}}\\, =\\) \\(\\theta_{\\mathbf{a}} - \\theta_{\\mathbf{b}}\\, =\\)\n\\(\\theta_{\\mathbf{b}} - \\theta_{\\mathbf{a}'} = \\phi\\), and therefore,\n\\(\\theta_{\\mathbf{b}'} - \\theta_{\\mathbf{a}'} = 3\\phi\\). This choice\nyields  \nThis exceeds the CHSH bound (8) when \\(0 \\lt |\\phi | \\lt\\)\n\\(\\arccos\\left((\\sqrt{3} - 1)/2 \\right) \\approx 1.95\\) radians, or 68°, with\nmaximum violation at \\(\\phi = \\pm {\\pi}/{4}\\), or 45°. For\nthese angles, we have \nFor the case of polarization-entangled photons, we have, \nThis takes on its maximum at \\(\\phi = {\\pi}/{8}\\), or 22.5°. \nThis value \\(2\\sqrt{2}\\) appearing in equations (19) and (21) is the\nmaximum violation of the CHSH inequality for any quantum state, as\nshown by Tsirelson (Cirel’son 1980). This bound on quantum\nviolations of the CHSH inequality is called the Tsirelson\nbound. As Gisin (1991) and Popescu and Rohrlich (1992)\nindependently demonstrated, for any pure entangled quantum state of a\npair of systems observables can be found yielding a violation of the\nCHSH inequality. Popescu and Rohrlich (1992) also show that the\nmaximum amount of violation is achieved with a quantum state of\nmaximum degree of entanglement. Incidentally, this is not true for\nmixed states; there are entangled mixed states not violating any Bell\ninequality (Werner 1989). \nThe distinctive condition giving rise to the Bell Inequality\nassumption is the factorizability condition (F). This condition is\nmotivated by considerations concerning locality and causality.\nConsiderations of this sort have been the focus of the discussion of\nthe implications of Bell’s theorem. However, in order to a\nderive a conflict between the predictions for theories that satisfy\nthis assumption, other assumptions—some of which are the sort\nusually accepted without question in scientific\nexperimentation—are needed. The analysis of Bell’s theorem\nhas provoked careful scrutiny of the reasoning required to reach the\nconclusion that F is to be rejected. As a result, some assumptions\nthat in another context would have been left implicit have been made\nexplicit, and each has been challenged by some authors. In this\nsection we outline a set of assumptions sufficient to ensure\nsatisfaction of Bell inequalities, which, therefore, constitute a set\nof assumptions that cannot all be satisfied by any theory that yields,\nin agreement with experiment, violations of Bell inequalities. There\nare other paths to Bell inequalities; see, in particular, Wiseman and\nCavalcanti (2017), who offer an analysis similar to that found here,\nas well as other analyses. \nAs mentioned above, the article (Bell 1964) in which Bell’s\ntheorem first appeared is a follow-up to Bell (1966), which explores\nthe prospects for hidden-variables theories in which the outcome of an\nexperiment performed is predetermined by the complete state of the\nsystem. The introduction to Bell (1964) begins with mention of\ntheories with additional variables that are to restore causality and\nlocality, and says that “In this note that idea will be\nformulated mathematically and shown to be incompatible with the\nstatistical predictions of quantum mechanics.” The locality\nassumption is glossed as the requirement “that the result of a\nmeasurement on one system be unaffected by operations on a distant\nsystem with which it has interacted in the past.” Applied to the\ncase at hand, of Stern-Gerlach experiments performed on an entangled\npair of spin-1/2 particles, this is “the hypothesis ... that if\nthe two measurements are made at places remote from one another the\norientation of one magnet does not influence the result obtained with\nthe other.” Bell follows this with, \nThis suggests that OD is not being assumed, but, rather, derived, via\nan EPR-type argument, from a locality assumption and the perfect\nanticorrelations predicted by the considered quantum state. This is\nhow Bell explained the reasoning in later publications (see Bell 1981,\nfn\n 10).[6] \nIn Bell (1976) and (1990), Bell derives the factorizability condition\nfrom a condition he calls the Principle of Local Causality\n(see Norsen 2011 for discussion). In (1990) he begins his analysis\nwith a rehearsal of the reason that relativity should be taken to\nprohibit superluminal causation. On the usual notion of causation, the\ncause-effect relation is taken to be temporally asymmetric, with\ncauses temporally preceding their effects. In a relativistic\nspacetime, events at spacelike separation are taken to have no\ntemporal order. Any system of coordinates will assign time coordinates\nto each of any pair of events, but, if the events are spacelike\nseparated, the time coordinates assigned to a pair of events at\nspacelike separation will differ in their ordering, depending on which\nreference frame is being employed. If we take all of these reference\nframes to be physically on a par, it must be concluded that there is\nno temporal order between the events, as relations that are not\nrelativistically invariant have no physical significance. \nOn the basis of these considerations, that is, Lorentz invariance and\nthe assumption that causes temporally precede their effects, Bell\nintroduces what he calls the principle of local causality.\n \nThis, says Bell, is “not yet sufficiently sharp and clean for\nmathematics.” For this reason, he introduces what he presents as\na sharpened version of the principle (refer to Figure 2). Figure 2 \nIn Bell’s terminology, a beable is any element of a\nphysical theory that is taken to correspond to something physically\nreal, and local beables pertaining to a space-time region are\nthose contained within that region. \nThe transition from what we have called PLC-1 to PLC-2 should,\naccording to Bell, “be viewed with the utmost suspicion”\nas “it is precisely in cleaning up intuitive ideas for\nmathematics that one is likely to throw the baby out with the\nbathwater” (1990, 106; 2004, 239). The relation between them\nis not further discussed in that article, but remarks in other papers\nshed light on the transition between them. \nIn (1976), Bell motivates the formulation of the local causality\ncondition with the remark, \nHe then precedes to formulate the condition of local causality, which\nis that a full specification of beables in the overlap of the backward\nlight cones of the spacetime regions 1 and 2 should screen off\ncorrelations between them. Implicit in this is that correlations\nbetween two variables be susceptible to causal explanation, either via\na causal connection between the variables, or via a common cause. This\nassumption is made explicit in a later article (Bell 1981), in which\nhe says that “the scientific attitude is that correlations cry\nout for explanation” (Bell 1981, C2–55; 1987b and 2004, 152). \nThe assumption that correlations between two variables that are not in\na cause-effect relation to each other be explained by some common\ncause was named the principle of the common cause by\nReichenbach (1956, § 19), and for this reason is often referred\nto as Reichenbach’s Common Cause Principle, though\nReichenbach made no pretense of originating the principle, and\nregarded it as a codification of a mode of inference common in both\nscience and everyday life. See the entry on\n Reichenbach’s common cause principle\n for more details. A variable \\(C\\) is a Reichenbachian common\ncause of a correlation between two variables \\(A\\) and \\(B\\)\nif the variables \\(A\\) and \\(B\\) are uncorrelated,\nconditional on specification of the value of \\(C\\).\nReichenbach’s Common Cause Principle says that, if two\ncorrelated variables are not in a cause-effect relation with each\nother, there is a Reichenbachian common cause of their correlations.\nThe condition we have called PLC-1 does not, by itself, entail PLC-2,\nbut it does follow from the conjunction of PLC-1 and\nReichenbach’s Common Cause Principle. \nWhat Bell calls the Principle of Local Causality, PLC-2, can be\nthought of as a conjunction of (1) a causal locality condition along\nthe lines of PLC-1, restricting causes of an event to that\nevent’s past light cone, and (2) Reichenbach’s Common\nCause Principle. The former condition can, itself, be regarded as\nfollowing from relativistic invariance and the principle that the\ncause of an event lie in its temporal past. Bell’s Principle of\nLocal Causality thus follows from the conjunction of three\nassumptions, all of which, in various writings, were explicitly\nformulated by Bell: \nThe condition F of factorizability is the application, to the\nparticular set-up of Bell-type experiments, of Bell’s Principle\nof Local Causality. As we have seen, it can be thought of as the\nconjunction of the condition of causal locality, and the common cause\nprinciple. In this section we apply those conditions to the set-up of\nBell-type experiments. \nOn the assumption that the experimental settings can be treated as\nfree variables, whose values are determined exogenously, if the choice\nof setting on one wing is made at spacelike separation from the\nexperiment on the other, a dependence of the probability of the\noutcome of one experiment on the setting of the other would seem\nstraightforwardly to be an instance of a nonlocal causal influence.\nThe condition that this not occur can be formulated as follows.  \nThis is the condition that has come to be known as parameter\nindependence, following Shimony (1986, 1990). \nFor fixed values of the experimental settings, Bell’s Principle\nof Local Causality entails that the outcomes of the experiments on the\ntwo systems be independent, conditional on the specification\n\\(\\lambda\\) of the complete state of the system at the source.\nThis is the condition \nThis is the condition that has come to be known as outcome\nindependence, following Shimony (1986, 1990). As demonstrated by\nJarrett (1983, 1984), the factorizability condition (F) is the conjunction\nof parameter independence and outcome independence. The two conditions\nbear different relations to the locality and causality conditions\ndiscussed in the previous subsection. PI is a consequence of the\ncausal locality condition PLC-1 alone, whereas OI requires in addition\nthe assumption of the common cause principle. \nThe parsing of the Bell locality condition as a conjunction of PI and\nOI is due to Jarrett (1983, 1984), who referred to the conditions as\nlocality and completeness. Jarrett (1983, 1984, 1989)\nargued that a violation of PI would inevitably permit superluminal\nsignalling. The conclusion requires an additional assumption, that the\nstate of the system be\n controllable.[7]\n It would not hold for a theory on which there are principled\nlimitations on the ability of would-be signallers to control the\nstates of the systems they are dealing with. Nonetheless, in some of\nthe literature PI has been treated as equivalent to no-signalling.\nThis is predicated on regarding any limitations on control that might\nprevent a violation of PI from being exploited for signalling involves\nonly practical limitations irrelevant to foundational concerns (see\ne.g. Ballentine and Jarrett 1987, fn. 6; Jarrett 1989, 70; Shimony 1993, 139),\ndisregarding the possibility of theories involving in-principle\nlimitations on control.  \nThough it might seem that this goes without saying, the entire\nanalysis is predicated on the assumption that, of the potential\noutcomes of a given experiment, one and only one occurs, and hence\nthat it makes sense to speak of the outcome of an experiment.\nThe reason that this assumption is worth mentioning is that there is a\nfamily of approaches to the interpretation of quantum mechanics,\nnamely, Everettian, or “many-worlds” approaches, and some\nvariants of the relational approach, hold that all outcomes occur in\nwhat are effectively distinct worlds. See entries on\n many-worlds interpretation of quantum mechanics,\n Everett’s relative-state formulation of quantum mechanics, and\n relational quantum mechanics\n  \nBell’s original analysis (1964, 1971) tacitly assumed that the\ncomplete state \\(\\lambda\\) is sampled from the same probability\ndistribution \\(\\rho\\), no matter what choice of experiment is\nmade, and that, for this reason, the subset of experiments\ncorresponding to any given choice of settings is a fair sample of the\ndistribution of \\(\\lambda\\). Clauser and Horne (1974, fn. 13)\nmade this assumption explicit. At the end of Bell (1976), in which\nBell provides an exposition of a Bell-type theorem, we find a remark\nregarding the independence of \\(\\lambda\\) and the experimental\nsettings. \nThis assumption was not, however, explicitly invoked in the\nderivation, and the role this assumption is meant to be play was not\nmade sufficiently clear in that article.  \nThat an assumption of this sort is required was emphasized by Shimony,\nHorne and Clauser (1976), who illustrate the point via a conspiracy\ninvolving manufacturers of experimental apparatus and\nphysicists’ assistants. The director of the conspiracy concocts\na set of correlation experiment data, consisting of a sequence of\npairs of experimental settings and results obtained. The director\ninstructs the manufacturer to preprogram the apparatus to produce the\ndesired outcomes, and the assistants of the physicists performing the\nexperiment to orchestrate the apparatus settings to match those\nspecified by the predetermined list. Clearly, the conspirators may\nutilize any set of correlation data for their nefarious schemes;\nhence, any set of correlation data can be obtained as the outcomes of\nthis sort of process, without any violation of any sort of locality\ncondition. Nonetheless, for the sorts of experiments envisaged in\ntests of Bell inequalities, Shimony, Horne, and Clauser consider the\nassumption of independence of settings and the state of the particle\npairs to be justified, even though relativistic causality does not\nmandate this independence. \nIn response, Bell (1977) acknowledged that his formulation in (1976)\nhad been inadequate, and explained his reasoning as “primarily\nan analysis of certain kinds of physical theory.” \nHe makes it clear, however, that no metaphysical hypothesis of\nexperimenters exempt from the laws of physics need be invoked. What is\nneeded is something considerably weaker than the condition that the\nvariables not be determined in the overlap of the backward light cones\nof the experiments. What is needed is that they be “at least\neffectively free for the purpose at hand.”  Bell argues that a deterministic randomizer that is extraordinarily sensitive to initial\nconditions would suffice to provide the requisite independence, and that variables of this type may be treated as if they have\nimplications only for events in their future light cones. The upshot\nof the exchange was substantial agreement between Bell and Shimony,\nHorne, and Clauser. \nWe will call the assumption that experimental settings may be regarded\nas “effectively free for the purpose at hand” and treated\nas statistically independent of the variable \\(\\lambda\\), the\nassumption of measurement independence. It has also been\ncalled the free will assumption, the freedom of choice\nassumption and the no-conspiracies assumption. \nIn experimental tests of Bell locality, care is taken that the\nexperiments on the two systems, from choice of experimental setting to\nregistration of results, take place at spacelike separation. It is\nassumed that experiments have unique results. The question arises as\nto when the unique result emerges. It is typically assumed\nthat the result is definite once a detector has been triggered or the\nresult is recorded in a computer memory. However, as Kent (2005) has\npointed out, proposals have been made according to which the quantum\nstate of the apparatus would remain in a superposition of terms\ncorresponding to distinct outcomes for a greater length of time. One\nsuch proposal is the suggestion that state reduction takes place only\nwhen the uncollapsed state involves a superposition of sufficiently\ndistinct gravitational fields (Diósi 1987, Penrose 1989,\n1996). Another is Wigner’s suggestion that conscious awareness\nof the result is required to induce collapse (Wigner 1961). This gives\nrise to what Kent calls the collapse locality loophole. One\ncan consider theories—Kent calls the family of such theories\ncausal quantum theories—on which collapses are\nlocalized events, and the probability of a collapse is independent\nof events, including other collapses, at spacelike separation from it.\nA theory of that sort would differ in its predictions from standard\nquantum theory, but a test to discriminate between such a theory and\nstandard quantum mechanics would require a set-up in which the entire\nexperiment on one system, from arrival to satisfaction of the collapse\ncondition, takes place at spacelike separation from the experiment on\nthe other. If the experiments are taken to end, not when the detector\nis triggered, but when the difference between outcomes amounts to\ndifferences in mass configurations large enough to correspond to\nsignificantly distinct gravitational fields, then, as Kent argued,\nexperiments extant at the time of writing (2005) were subject to this\nloophole. The experiment of Salart et al. (2008) closed the\nloophole for the particular proposals of Penrose and Diósi,\nthough, as Kent (2018) points out, altering the Penrose-Diósi\nthreshold by a few orders of magnitude would render them compatible\nwith the results of this experiment. No experiment to date has addressed\nthe collapse locality loophole if the collapse condition is taken to\nbe awareness of the result by a conscious observer. See Kent (2018)\nfor proposals of ways in which causal quantum theory could be\nsubjected to more stringent tests. \nIt has become commonplace to say that (provided that the supplementary\nassumptions are accepted), the class of theories ruled out by\nexperimental violations of Bell inequalities is the class of local\nrealistic theories, and that the worldview to be abandoned is\nlocal realism. The ubiquity of the use of this terminology\ntends to obscure the fact that not all who use it use it in the same\nsense; further, it is not always clear what is meant when the phrase\nis used. \nThe terminology of “local realistic theories” as the\ntargets of experimental tests of Bell inequalities was introduced by\nClauser and Shimony (1978), intended as a synonym for what Clauser and\nHorne (1974) called “objective local theories.” The\nterminology was adopted by d’Espagnat (1979) and Mermin (1980).\nFor Clauser and Shimony realism is “a philosophical view\naccording to which external reality is assumed to exist and have\ndefinite properties, whether or not they are observed by\nsomeone” (1978, 1883). In a similar vein, d’Espagnat\n(1979) says that realism is “the doctrine that regularities in\nobserved phenomena are caused by some physical reality whose existence\nis independent of human observers” (158). Mermin, on the other\nhand, takes realism to involve the condition that we have called\noutcome determinism (OD): “As I shall use the term\nhere, local realism holds that one can assign a definite value to the\nresult of an impending measurement of any component of the spin of\neither of the two correlated particles, whether or not that\nmeasurement is actually performed” (Mermin 1980, 356). This is\nnot a commitment of realism in the sense of Clauser and Shimony, who\nexplicitly consider stochastic local realistic theories . \nIt is Mermin’s sense that seems to be most widely used in the\ncurrent literature. In this sense, local realism, applied to\nthe set-up of the Bell experiments, amounts to the conjunction of\nParameter Independence (PI) and outcome determinism (OD). Now, it is\ntrue that, if PI and OD hold, so does factorizability (F), and hence\nthe Bell inequalities. But the condition OD is stronger than what is\nrequired, as the conjunction of PI and the strictly weaker condition\nOI also suffice. Thus, to say that violations of Bell inequalities\nrule out local realistic theories, with “realism”\nidentified as outcome determinism, is true but misleading, as it may\nsuggest that one can retain locality by rejecting\n“realism” in the sense of outcome determinism. However, if\none accepts the supplementary assumptions, one is obliged to reject\nnot merely the conjunction of OD and PI, but the weaker condition of\nfactorizability, which contains no assumption regarding predetermined\noutcomes of experiments. \nFurther confusion arises if the two senses are conflated. This can\nlead to the notion that the condition OD is equivalent to the\nmetaphysical thesis that physical reality exists and possess\nproperties independent of their cognizance by human or other agents.\nThis would be an error, as stochastic theories, on which the outcome\nof an experiment is not uniquely determined by the physical state of\nthe world prior to the experiment, but is a matter of chance, are\nperfectly compatible with the metaphysical thesis. One occasionally\nfinds traces of a conflation of this sort in the literature; see,\ne.g., d’Espagnat (1979) and Mermin (1981). \nFor other authors, rejection of realism seems to amount primarily to\nan avowal of operationalism. If all one asks of a theory is that it\nproduce the correct probabilities for outcomes of experiments,\neschewing all questions about what sort of physical reality gives rise\nto these outcomes, then this undercuts the motivation of the analysis\nthat leads to Bell’s theorem. In this sense of\n“realism”, it is not an assumption of the theorem but a\nmotivation for formulating it.  \nSeveral authors (see, in particular, Norsen 2007; Maudlin 2014) have\nargued that no clear sense of “realism” has been\nidentified such that realism, in that sense, is a particular\npresupposition of the derivation of Bell inequalities (as\ndistinguished from a presupposition of all physics). These authors\nurge rejection of the currently prevalent practice of saying that\n“local realist” theories are the targets of experimental\ntests of Bell inequalities. Other authors maintain that there is,\nindeed, a sense of “realism” on which realism is an\nassumption of the derivation of Bell inequalities; see Żukowski\nand Brukner (2014), Werner (2014), Żukowski (2017), Clauser\n(2017). \nA first proposal to test Bell inequality was made by Clauser, Horne,\nShimony, and Holt (1969), henceforth CHSH, who suggested that the\npairs 1 and 2 be photons produced in an atomic cascade from an initial\natomic state with total angular momentum \\(J = 0\\) to an\nintermediate atomic state with \\(J = 1\\) to a final atomic state\n\\(J = 0\\), as in an experiment performed with calcium vapor for\nother purposes by Kocher and Commins (1967). The proposed test was\nfirst performed by Freedman and Clauser (1972). The result obtained by\nFreedman and Clauser was 6.5 standard deviations from the limit\nallowed by the CHSH inequality but in good agreement with the quantum\nmechanical prediction. This was a difficult experiment, requiring 200\nhours of running time, much longer than in most later tests of\nBell’s Inequality, which were able to use lasers for exciting\nthe sources of photon pairs.  \nSince then, several dozen experiments have been performed to test\nBell’s Inequalities. References will now be given to some of the\nmost noteworthy of these, along with references to survey articles\nwhich provide information about others. A discussion of more recent\nexperiments addressed to close two serious loopholes in the early Bell\nexperiments, the “detection loophole” and the\n“communication loophole”, will be reserved for\n Section 5. \nHolt and Pipkin completed in 1973 (Holt 1973) an experiment very much\nlike that of Freedman and Clauser, but examining photon pairs produced\nin the \\(9^1 P_1 \\rightarrow 7^3 S_1\\rightarrow 6^3 P_0\\) cascade in the zero nuclear-spin isotope of mercury-198 after\nusing electron bombardment to pump the atoms to the first state in\nthis cascade. The result of Holt and Pipkin was in fairly good\nagreement with the CHSH Inequality, and in disagreement with the\nquantum mechanical prediction by nearly 4 standard\ndeviations—contrary to the results of Freedman and Clauser.\nBecause of the discrepancy between these two early experiments, Clauser\n(1976) repeated the Holt-Pipkin experiment, using the same cascade and\nexcitation method but a different spin-0 isotope of mercury, and his\nresults agreed well with the quantum mechanical predictions but\nviolated Bell’s Inequality. Clauser also suggested a possible\nexplanation for the anomalous result of Holt-Pipkin: that the glass of\nthe Pyrex bulb containing the mercury vapor was under stress and hence\nwas optically active, thereby giving rise to erroneous determinations\nof the polarizations of the cascade photons. \nFry and Thompson (1976) also performed a variant of the Holt-Pipkin\nexperiment, using a different isotope of mercury and a different\ncascade and exciting the atoms by radiation from a narrow-bandwidth\ntunable dye laser. Their results also agreed well with the quantum\nmechanical predictions and disagreed sharply with Bell’s\nInequality. They gathered data in only 80 minutes, as a result of the\nhigh excitation rate achieved by the laser. \nFour experiments in the 1970s — by Kasday-Ullman-Wu,\nFaraci-Gutkowski-Notarigo-Pennisi, Wilson-Lowe-Butt, and\nBruno-d’Agostino-Maroni — used photon pairs produced in\npositronium annihilation instead of cascade photons. Of these, all but\nthat of Faraci et al. gave results in good agreement with the\nquantum mechanical predictions and in disagreement with Bell’s\nInequalities. A discussion of these experiments is given in the review\narticle by Clauser and Shimony (1978), who regard them as less\nconvincing than those using cascade photons, because they rely upon\nstronger auxiliary assumptions. \nThe first experiment using polarization analyzers with two exit\nchannels, thus realizing the theoretical scheme envisaged in\n Section 2,\n was performed in the early 1980s with cascade photons from\nlaser-excited calcium atoms by Aspect, Grangier, and Roger (1982). The\noutcome confirmed the predictions of quantum mechanics over those\nsatisfying the Bell inequalities more dramatically than any of its\npredecessors, with the experimental result deviating from the upper\nlimit in a Bell’s Inequality by 40 standard deviations. An\nexperiment soon afterwards by Aspect, Dalibard, and Roger (1982),\nwhich aimed at closing the communication loophole, will be discussed\nin\n Section 5.\n The historical article by Aspect (1992) reviews these experiments and\nalso surveys experiments performed by Shih and Alley, by Ou and\nMandel, by Rarity and Tapster, and by others, using photon pairs with\ncorrelated linear momenta produced by down-conversion in non-linear\ncrystals. Discussion of more recent Bell tests can be found in review papers\n(Zeilinger 1999, Genovese 2005, 2016). \nPairs of photons have been the most common physical systems in Bell\ntests because they are relatively easy to produce and analyze, but\nthere have been experiments using other systems. Lamehi-Rachti and\nMittig (1976) measured spin correlations in proton pairs prepared by\nlow-energy scattering. Their results agreed well with the quantum\nmechanical prediction and violated Bell’s Inequality, but, as in the positronium experiments, strong\nauxiliary assumptions had to be made.  \nThe outcomes of the Bell tests provide dramatic confirmations of the\nprima facie entanglement of many quantum states of systems\nconsisting of 2 or more constituents. Actually, the first confirmation\nof entanglement antedated Bell’s work, since Bohm and Aharonov\n(1957) demonstrated that the results of Wu and Shaknov (1950), Compton\nscattering of the photon pairs produced in positronium annihilation,\nalready showed the entanglement of the photon pairs. \nThe derivations of all the variants of Bell’s Inequality depend\nupon independence conditions inspired by relativistic causality. In\nthe early tests of Bell’s Inequalities it was plausible that\nthese conditions were satisfied just because the 1 and the 2 arms of\nexperiment were spatially well separated in the laboratory frame of\nreference. This satisfaction, however, is a mere contingency not\nguaranteed by any law of physics, and hence it is physically possible\nthat the setting of the analyzer of 1 and its detection or\nnon-detection could influence the outcome of analysis and the\ndetection or non-detection of 2, and conversely. This is the\ncommunication loophole, to which the early Bell tests were\nsusceptible. It is addressed by ensuring that the experiments on the\ntwo systems take place at spacelike separation.  \nAspect, Dalibard, and Roger (1982) published the results of an\nexperiment in which the choices of the orientations of the analyzers\nof photons 1 and 2 were performed so rapidly that they were events\nwith space-like separation. No physical modification was made of the\nanalyzers themselves. Instead, switches consisting of vials of water\nin which standing waves were excited ultrasonically were placed in the\npaths of the photons 1 and 2. When the wave is switched off, the\nphoton propagates in the zeroth order of diffraction to polarization\nanalyzers respectively oriented at angles \\(a\\) and \\(b\\),\nand when it is switched on the photons propagate in the first order of\ndiffraction to polarization analyzers respectively oriented at angles\n\\(a'\\) and \\(b'\\). The complete choices of\norientation require time intervals 6.7 ns and 13.37 ns respectively,\nmuch smaller than the 43 ns required for a signal to travel between\nthe switches in obedience to special relativity theory. Prima\nfacie it is reasonable that the independence conditions are\nsatisfied, and therefore that the coincidence counting rates agreeing\nwith the quantum mechanical predictions constitute a refutation of the\nBell inequality and hence of the family of theories that entail it.\nThere are, however, several imperfections in the experiment. First of\nall, the choices of orientations of the analyzers are not random, but\nare governed by quasiperiodic establishment and removal of the\nstanding acoustical waves in each switch. A scenario can be invented\naccording to which clever hidden variables of each analyzer can\ninductively infer the choice made by the switch controlling the other\nanalyzer and adjust accordingly its decision to transmit or to block\nan incident photon. Also, coincident count technology is employed for\ndetecting joint transmission of 1 and 2 through their respective\nanalyzers, and this technology establishes an electronic link which\ncould influence detection rates. And because of the finite size of the\napertures of the switches there is a spread of the angles of incidence\nabout the Bragg angles, resulting in a loss of control of the\ndirections of a non-negligible percentage of the outgoing photons. \nThe experiment of Tittel, Brendel, Zbinden, and Gisin (1998) did not\ndirectly address the communication loophole but threw some light\nindirectly on this question and also provided dramatic evidence\nconcerning the maintenance of entanglement between particles of a pair\nthat are well separated. Pairs of photons were generated in Geneva and\ntransmitted via cables with very small probability per unit length of\nlosing the photons to two analyzing stations in suburbs of Geneva,\nlocated 10.9 kilometers apart on a great circle. The counting rates\nagreed well with the predictions of quantum mechanics and violated the\nCHSH inequality. No precautions were taken to ensure that the choices\nof orientations of the two analyzers were events with space-like\nseparation. The great distance between the two analyzing stations\nmakes it difficult to conceive a plausible scenario for a conspiracy\nthat would violate Bell’s independence conditions. Furthermore\n— and this is the feature which seems most to have captured the\nimagination of physicists — this experiment achieved much\ngreater separation of the analyzers than ever before, thereby\nproviding a test of a conjecture by Schrödinger (1935) that\nentanglement is a property that may dwindle with spatial separation.\nMore recently, Bell\ninequality violation was demonstrated even at 144 km distance (Scheidl\net al., 2010) and, in 2017, from satellite transmission with\na 1200 km distance (Yin et al., 2017).  \nAn experiment that came closer to closing the communication loophole\nis that of Weihs, Jennewein, Simon, Weinfurter, and Zeilinger (1998).\nThe pairs of systems used to test a Bell’s Inequality are photon\npairs in the entangled polarization state \nwhere the ket \\(\\ket{H}\\) represents horizontal polarization and\n\\(\\ket{V}\\) represents vertical polarization. Each photon pair\nis produced from a photon of a laser beam by the down-conversion\nprocess in a nonlinear crystal. The momenta, and therefore the\ndirections, of the daughter photons are strictly correlated, which\nensures that a non-negligible proportion of the pairs jointly enter\nthe apertures (very small) of two optical fibers, as was also achieved\nin the experiment of Tittel et al.. The two stations to which\nthe photon pairs are delivered are 400 m apart, a distance which light\nin vacuo traverses in \\(1.3 \\mu\\)s. Each photon emerging from an optical\nfiber enters a fixed two-channel polarizer (i.e., its exit channels\nare the ordinary ray and the extraordinary ray). Upstream from each\npolarizer is an electro-optic modulator, which causes a rotation of\nthe polarization of a traversing photon by an angle proportional to\nthe voltage applied to the modulator. Each modulator is controlled by\namplification from a very rapid generator, which randomly causes one\nof two rotations of the polarization of the traversing photon. An\nessential feature of the experimental arrangement is that the\ngenerators applied to photons 1 and 2 are electronically independent.\nThe rotations of the polarizations of 1 and 2 are effectively the same\nas randomly and rapidly rotating the polarizer entered by 1 between\ntwo possible orientations \\(a\\) and \\(a'\\) and the\npolarizer entered by 2 between two possible orientations \\(b\\)\nand \\(b'\\). The output from each of the two exit channels\nof each polarizer goes to a separate detector, and a “time\ntag” is attached to each detected photon by means of an atomic\nclock. Coincidence counting is done after all the detections are\ncollected by comparing the time tags and retaining for the\nexperimental statistics only those pairs whose tags are sufficiently\nclose to each other to indicate a common origin in a single\ndown-conversion process. Accidental coincidences will also enter, but\nthese are calculated to be relatively infrequent. This procedure of\ncoincidence counting eliminates the electronic connection between the\ndetector of 1 and the detector of 2 while detection is taking place,\nwhich conceivably could cause an error-generating transfer of\ninformation between the two stations. The total time for all the\nelectronic and optical processes in the path of each photon, including\nthe random generator, the electro-optic modulator, and the detector,\nis conservatively calculated to be smaller than 100 ns, which is much\nless than the \\(1.3 \\mu\\)s required for a light signal between the two\nstations. \nThe experimental result in the experiment of Weihs et al. is\n\\(2.73 \\pm 0.02,\\) in good agreement with the quantum mechanical\nprediction, and it is 30 standard deviations away from the upper limit\nof the CHSH inequality inequality (8). Aspect, who designed the first\nexperimental test of a Bell Inequality with rapidly switched analyzers\n(Aspect, Dalibard, Roger 1982) appreciatively summarized the import of\nthis result: \nEven if some small imperfection prevented the experiment of Weihs\net al. from completely blocking the detection loophole, these\nproblems were overcome in subsequent experiments. \nThe CHSH inequality (8) is a relation between expectation values. An\nexperimental test, therefore, requires empirical estimation of the\nprobabilities of the outcomes of experiments. This estimation involves\ncomputing a ratio of event-counts: the number of pair-production\nevents with a certain outcome to the total number of pair-production\nevents. Typically, in experiments involving photons, most of the pairs\nproduced fail to enter the analyzers. Furthermore, some photons that\nenter the analyzers will fail to be detected; in addition, the\ndetector will occasionally register a detection even when no photon is\ndetected (the rate of occurrence of this is known as the\n“dark-count”). \nThree strategies for addressing this issue have been pursued.  \nOne is to employ an auxiliary assumption to yield an estimate of the\nnormalization factor required to infer relative frequencies from\nevent-counts, as required by a test of the CHSH inequality. CHSH\n(1969) proposed the assumption that, if a photon passes through an\nanalyser, its probability of detection is independent of the\nanalyser’s orientation. Though physically plausible, this is not\na condition required by local causality. \nThe fact that an assumption of this sort is needed for the analysis of\nexperiments of this type was made clear by toy models constructed by\nPearle (1970) and Clauser and Horne (1974). In these models, the rates\nat which the photon pairs pass through the polarization analyzers with\nvarious orientations are consistent with an inequality of Bell’s\ntype, but the hidden variables provide instructions to the photons and\nthe apparatus not only regarding passage through the analyzers but\nalso regarding detection, thereby violating the fair sampling\nassumption. Detection or non-detection is selective in the model in\nsuch a way that the detection rates violate the Bell-type inequality\nand agree with the quantum mechanical predictions. Other models were\nconstructed later by Fine (1982a) and corrected by Maudlin (1994) (the\n“Prism Model”) and by C.H. Thompson (1996) (the\n“Chaotic Ball model”). Although all these models are\nad hoc and lack physical plausibility, they constitute\nexistence proofs that theories satisfying the local causality\ncondition can be consistent with the quantum mechanical predictions\nprovided that the detectors are properly selective. \nA second strategy involves construction of an experimental set-up in\nwhich the production of each particle-pair may be registered. Clauser\nand Shimony (1978) referred to apparatus achieving this as\n“event-ready” detectors; some recent literature has\nreferred to a process of this sort as “heralding.” \nA third strategy involves employment of an inequality that can be\nshown to be violated without knowledge of the absolute value of the\nprobabilities involved. This eliminates the need for untestable\nauxiliary assumptions. An inequality suitable for this purpose was\nfirst derived by Clauser and Horne (1974) (henceforth CH). The set-up\nis as before, with the exception that each analyzer will have only one\noutput channel, and the eventualities to be considered are detection\nand non-detection. We want an inequality expressed in terms of\nprobabilities of detection alone. The same sort of reasoning that\nleads to the CHSH inequality yields the CH Inequality: \nThe probabilities appearing in (25) can be estimated by dividing\nevent-counts registered in a run of an experiment by the total number\nof pairs produced. If we assume that the production rate at the source\nis independent of the analyzer settings, we can take the normalization\nfactor to be the same for each term, and hence, the magnitude of this\nfactor need not be known in order to demonstrate a violation of the\nupper bound of (25). Another useful observation was made by Eberhard\n(1993), who demonstrated that the minimal detection efficiency for a\ndetection loophole free experiment can be reduced (from 82% to 67%)\nfor non-maximally entangled states (i.e. a bipartite entangled state\nwith different weight for the two components). This involves starting\nwith a specified efficiency level, and then choosing a state and a set\nof observables that maximize violation of the CH inequality at that\nefficiency level. \nFor the maximally entangled states we have been considering, in the\nidealized case of perfect detection efficiency, inequality (25) is\nmaximally violated by the quantum predictions for the same settings\nconsidered above for violation of the CHSH inequality. However, for\nnon-ideal experiments, the quantum predictions satisfy the inequality\nunless detector efficiency is high, considerably higher than that of\nany experiment that had been performed up until the time that CH were\nwriting. For that reason, CH introduced a new auxiliary assumption,\ncalled the no-enhancement assumption: for any value of\n\\(\\lambda\\), and any setting of an analyzer, the probability of\ndetection with the analyzer present is no higher than the probability\nof detection with the analyzer removed. Let \\(p^1_{\\infty}\\) and\n\\(p^2_{\\infty}\\) be the probabilities of detection of particles 1 and\n2 when their respective analyzers have been removed. This assumption\ngives rise to what may be called the second CH\ninequality: \nAs CH note, this is violated by the results of the Freedman and\nClauser experiment, and hence that experiment rules out theories\nsatisfying the factorizability condition (F) and the no-enhancement\nassumption, though it does not rule out the toy model constructed by\nCH. \nHistorically, the efforts toward a detection loophole-free experiment\nfollowed two main paths, though a few other possibilities were also\nexplored. One of these other possibilities involved K,B mesons\n(Selleri 1983, Go 2004), where the detection loophole reappears in\nanother form (Genovese, Novero, and Predazzi 2001); and another\ninvolved solid state systems (Ansmann et al. 2009). \nOne of the main avenues of approach employed entangled ions. The use\nof ions looked very promising, since for such experiments detection\nefficiency is very high. The experiment of Rowe et al. (2001)\nemployed beryllium ions, observing a CHSH inequality violation \\(S =\n2.25 \\pm 0.03\\) with a total detection efficiency of about 98%.\nNevertheless, in this set-up the measurements on two ions not only\nwere not space-like separated; there was a common measurement on the\ntwo ions. More recently, the distance between ions was increased. For\ninstance, Matsukevich et al. (2008) entangled two ytterbium\nions via interference and joint detection of two emitted photons, with\nthe distance between the ions set to 1 meter. However, a conclusive\nexperiment of this sort that eliminated also the communication\nloophole would require a separation of kilometers. \nThe other main avenue of approach, which paved the way to a conclusive\ntest of Bell inequalities, involved innovations in tests using\nphotons. First, efficient sources of photon entangled states were\nrealized by exploiting Parametric Down Conversion, a non-linear\noptical phenomenon in which a photon of higher energy converts into\ntwo lower frequency photons inside a non-linear medium in such a way\nthat energy and momentum are conserved. This allows a high collection\nefficiency due to wave vector correlation of the emitted photons.\nNext, high efficiency single photon Transition Edge Sensors were\nproduced. These advances led to detection loophole-free experiments\nwith photons (Giustina et al. 2013, Christensen et\nal. 2013) and finally to the conclusive tests discussed in the\nnext section. \nIn 2015 three papers appeared claiming a conclusive test of Bell\ninequalities. The first (Hensen et al., 2015) achieved a\nviolation of the CHSH inequality via an event-ready scheme. This\nexperiment is based on using electronic spin associated with the\nnitrogen-vacancy (NV) defect in two diamond chips located in distant\nlaboratories. In the experiment, each of these two spins is entangled\nwith the emission time of a single photon. Then the two,\nindistinguishable, photons are transmitted to a remote beam splitter.\nA measurement is made on the photons after the beam splitter. An\nappropriate result of the measurement of the photons projects the\nspins in the two diamond chips onto a maximally entangled state, on\nwhich a Bell inequality test is realized. The high efficiency in spin\nmeasurement and the distance between the laboratories allows closure\nof the detection and communication loophole at the same time. However,\nthe experiment utilized only a small number, 245, of trials, and thus\nthe statistical significance (2 standard deviations) of the result\n\\(S = 2.42 \\pm 0.20\\) is limited. \nThe other two experiments, published in the same issue of Physical\nReview Letters (Giustina et al. 2015, Shalm et\nal. 2015), are based on transmitting two polarization-entangled\nphotons, produced by Parametric Down Conversion, to two remote\nlaboratories, where they are measured by high detection efficiency\nTransition Edge Sensors. These experiments use states that are not\nmaximally entangled, but are optimized, in accordance with the\nanalysis of Eberhard (1993), to produce a maximal violation of the CH\ninequality, given the detection efficiency of the experiments. In both\nof these experiments a violation of the CH inequality was obtained, at\na high degree of statistical significance. Shalm et al.\nreport a \\(p\\)-value of \\(2.3 \\times 10^{-7}\\), whereas\nGiustina et al. report a \\(p\\)-value of \\(3.4 \\times\n10^{-31}\\), corresponding to an 11.5 standard deviation effect. A very\ncareful analysis of data (including spacelike separation of detection\nevents), of statistical significance, and of all possible loopholes\nleaves really no space for doubts about their conclusiveness. Besides\nthe detection and communication loophole, these two experiments\naddress also the following issues: \nFurthermore, independent random number generators based on laser phase\ndiffusion guarantee the elimination of freedom-of-choice loophole\n(except in presence of superdeterminism or other hypotheses that, by\ndefinition, do not allow a test through Bell inequalities). \nIn summary, these experiments, having satisfied carefully all the\nconditions required for a conclusive test, unequivocally tested Bell\ninequalities without any additional hypothesis. \nThis section will discuss in some detail two variants of Bell’s\nTheorem which depart in some respect from the conceptual framework\npresented in\n Section 2.\n Both are less general than the version in\n Section 2,\n because they rely on perfect correlations, which, together with the\nfactorizability condition (F), entail outcome determinism (OD). At the\nend of the section two other variants will be mentioned briefly but\nnot summarized in detail. \nThe first variant is due independently to\n Kochen,[8]\n Stairs (1978, 1983), and Heywood and Redhead (1983). Its ensemble of\ninterest consists of pairs of spin-1 particles in the entangled\nstate \nwhere \\(\\ket{z,i}_1\\), with \\(i = -1\\) or 0 or 1 is the spin state of\nparticle 1 with component of spin \\(i\\) along the axis \\(z\\), and\n\\(\\ket{z,i}_2\\) has an analogous meaning for particle 2. If \\(x,y,z\\)\nis a triple of orthogonal axes in 3-space then the components \\(s_x,\ns_y, s_z\\) of the spin operator along these axes do not pairwise\ncommute. However, the squares of these operators — \\(s_x^2,\ns_y^2, s_z^2\\) — do commute, and therefore, in view of the\nconsiderations of\n Section 1,\n any two of them can constitute a context in the measurement of the\nthird. If the operator of interest is\n\\(s_z^2\\), the axes \\(x\\) and\n\\(y\\) can be any pair of orthogonal axes in the plane\nperpendicular to \\(z\\), thus offering an infinite family of\ncontexts for the measurement of\n\\(s_{z}^2\\). As noted in\n Section 1\n Bell exhibited the possibility of a contextual hidden\nvariables theory for a quantum system whose Hilbert space has\ndimension 3 or greater even though the Bell-Kochen-Specker theorem\nshowed the impossibility of a non-contextual hidden variables\ntheory for such a system. The strategy of the argument is to use the\nentangled state of Eq. (27) to predict the outcome of measuring\n\\(s_{z}^2\\) for particle 2 (for any choice of \\(z)\\) by measuring its\ncounterpart on particle 1. A specific complete state \\(\\lambda\\) would\ndetermine whether \\(s_{z}^2\\) of 1, measured together with a context\nin 1, is 0 or 1. Agreement with the quantum mechanical prediction of\nthe entangled state of Eq. (27) implies that \\(s_z^2\\) of 2 has the\nsame value 0 or 1. But if the factorizability condition (F) is\nassumed, then the result of measuring \\(s_{z}^2\\) on 2 must be\nindependent of the remote context, that is, independent of the choice\nof \\(s_{x}^2\\) and \\(s_y^2\\) of 1, hence of 2 because of correlation,\nfor any pair of orthogonal directions \\(x\\) and \\(y\\) in the plane\nperpendicular to \\(z\\). It follows that the hypothetical theory which\nsupplies the complete state \\(\\lambda\\) is not contextual after all,\nbut maps the set of operators \\(s_z^2\\) of 2, for any direction\n\\(z\\), noncontextually into the pair of values (0, 1). But\nthat is impossible in view of the Bell-Kochen-Specker theorem. The\nconclusion is that no theory satisfying the factorizability condition\n(F) is consistent with the quantum mechanical predictions of the\nentangled state (29). \nA simpler proof of Bell’s Theorem, also relying upon\ncounterfactual reasoning and based upon a deterministic local theory,\nis that of Hardy (1993), here presented in Laloë’s (2001)\nformulation. Consider an ensemble of pairs 1 and 2 of\nspin\\(-\\frac{1}{2}\\) particles, the spin of 1 measured along\ndirections in the \\(xz\\)-plane making angles \\(a=\\theta /2\\) and\n\\(a'=0\\) with the \\(z\\)-axis, and angles \\(b\\) and \\(b'\\) having\nanalogous significance for 2. The quantum states for particle 1 with\nspins \\(+\\frac{1}{2}\\) and \\(-\\frac{1}{2}\\) relative to direction\n\\(a'\\) are respectively \\(\\ket{a',+}_1\\) and \\(\\ket{a',-}_1\\), and\nrelative to direction \\(a\\) are respectively \nthe spin states for 2 are analogous. The ensemble of interest is\nprepared in the entangled quantum state \n(unnormalized, because normalization is not needed for the following\nargument). Then for the specified \\(a, a',\nb\\), and \\(b'\\) the following quantum mechanical\npredictions hold: \nand for almost all values of the \\(\\theta\\) of Eq. (31) \nwith the maximum occurring around \\(\\theta = 9\\degr\\). Inequality\n(33) asserts that for the specified angles there is a non-empty\nsubensemble \\(E'\\) of pairs for which the results for a\nspin measurement along \\(a\\) for 1 and along \\(b\\) for 2 are\nboth +. Eq. (30) implies the counterfactual proposition that if the\nspin of a 2 in \\(E'\\) had been measured along\n\\(b'\\) then with certainty the result would have been\n\\(-\\); and likewise Eq. (31) implies the counterfactual proposition\nthat if the spin of a 1 in \\(E'\\) had been measured along\n\\(a'\\) then with certainty the result would have been\n\\(-\\). It is in this step that counterfactual reasoning is used in\nthe argument. Since the subensemble \\(E'\\) is non-empty, we\nhave reached a contradiction with Eq. (32), which asserts that if the\nspin of 1 is measured along \\(a'\\) and that of 2 is\nmeasured along \\(b'\\) then it is impossible that both\nresults are \\(-\\). The incompatibility of a deterministic local\ntheory with quantum mechanics is thereby demonstrated. \nAn attempt was made by Stapp (1997) to demonstrate a strengthened\nversion of Bell’s theorem which dispenses with the conceptual\nframework outlined above, and to use instead the logic of\ncounterfactual conditionals. His intricate argument has been the\nsubject of a criticism by Shimony and Stein (2001, 2003), who are\ncritical of certain counterfactual conditionals that are asserted by\nStapp by means of a “possible worlds” analysis without a\ngrounding on a local deterministic theory, and a response by Stapp (2001)\nhimself, who defends his argument with some modifications. \nThe three variants of Bell’s Theorem considered so far in this\nsection concern ensembles of pairs of particles. An entirely new\ndomain of variants is opened by studying ensembles of\n\\(n\\)-tuples of particles with \\(n \\ge 3\\). The prototype\nof this kind of theorem was demonstrated by Greenberger, Horne, and\nZeilinger (1989) (GHZ) for \\(n=4\\) and modified to \\(n=3\\)\nby Mermin (1990) and by Greenberger, Horne, Shimony, and Zeilinger\n(1990) (GHSZ). In the theorem of GHZ an entangled quantum state was\nwritten for four spin-\\(\\frac{1}{2}\\) particles and the expectation value of\nthe product of certain binary measurements performed on the individual\nparticles was calculated. They then showed that the attempt to\nduplicate this expectation value subject to the factorizability\nconstraint (F) produces a contradiction. A similar result was obtained\nby Mermin for a state of 3 spin-\\(\\frac{1}{2}\\) particles and by GHSZ for a\nstate of 3 photons entangled with respect to their direction of\npropagation. Because of the length of these arguments and limitations\nof space in the present article the details will not be summarized\nhere, it is however worth mentioning that experimental tests were\nrealized (Pan et al. 2000, Genovese 2005) (also in this case\nwith additional hypotheses). \nThe set-up envisaged in the proof of Bell’s theorem highlights a\nstriking prediction of quantum theory, namely, long-distance\nentanglement, and experimental tests of the Bell inequalities provide\nconvincing evidence that it is a feature of reality. Moreover,\nBell’s theorem reveals that the entanglement-based correlations\npredicted by quantum mechanics are strikingly different from the sort\nof locally explicable correlations familiar in a classical\ncontext. \nInvestigations into entanglement and the ways in which it can be\nexploited to perform tasks that would not be feasible with only\nclassical resources forms a key part of the discipline of quantum\ninformation theory (see Benatti, et al. eds. (2010), and the\nentries on\n quantum computing\n and\n quantum entanglement and information).\n In drawing attention to the import of entanglement and the ways in\nwhich it is different from anything in classical physics, Bell’s\ntheorem and the experimental work derived from it provided, at least\nindirectly, some of the impetus to the development of quantum\ninformation theory. \nBell’s theorem has played a direct role in the development of\ndevice independent quantum cryptography. One can exploit quantum\ncorrelations to devise a quantum key distribution protocol that is\nprovably secure on the assumption that, whatever the underlying\nphysics is, it does not permit superluminal signalling. The basic idea\nis that, if one has Bell inequality-violating correlations at\nspacelike separation, any predictability of the results beyond that\nafforded by the quantum probabilities could be exploited for\nsuperluminal signalling; the contrapositive of this is that impossibility of\nsignalling entails “absolute randomness” — absolute\nin the sense of being independent of the details of the underlying\nphysics beyond the prohibition on superluminal\n signalling.[9] \nThe experiments demonstrating loophole-free violations of Bell\ninequalities take on particular significance in this context. The toy\nmodels demonstrating the reality of the detector inefficiency loophole\nlack physical plausibility, and, in the absence of conspiracies aimed\nat deceiving the experimenters, may be disregarded on the assumption\nthat nature, though subtle, is not malicious. Cryptography, on the\nother hand, by its very nature must take into account the possibility\nof a conspiracy aimed at deceiving the users of a cryptographic key,\nand so, in this context, it is essential to demonstrate security in\nthe presence of such a conspiracy.  \nA result due to Colbeck and Renner (2011, 2016), building on work of\nBranciard et al. (2008), shows that this cannot be done if PI\nis satisfied. This result has significance both at the operational and\nthe fundamental level. It can be applied at the fundamental level to\nconclude that any theory with sharper probabilities than the quantum\npredictions must violate PI. In addition, even if some deterministic\ntheory (such as the de Broglie-Bohm theory) applies at the fundamental\nlevel, the Colbeck-Renner theorem can be applied at the operational\nlevel, where the probabilities involved may indicate limitations on\naccessible information about the physical state. A violation of PI at\nthe operational level would permit signalling. Thus, the theorem shows\nthat, as long as the no-signalling condition is satisfied, a would-be\neavesdropper attempting to subvert the privacy of a key distribution\nscheme by intercepting the particle pairs and substituting ones that\nwill yield results that she has some information about, cannot do so\nwithout disrupting the correlations between the particle pairs. See\nLeegwater (2016) for a clear exposition of the theorem. \nBell inequalities follow from a number of assumptions that have\nintuitive plausibility and which, arguably, are rooted in the sort of\nworld-view that results from reflection on classical physics and\nrelativity theory. If one accepts that the experimental evidence gives\nus strong reason to believe that Bell inequality-violating\ncorrelations are features of physical reality, then one or more of\nthese assumptions must be given up. Some of these assumptions are of\nthe sort that have traditionally been regarded as metaphysical\nassumptions. The fact that a conjunction of theses of the sort usually\nregarded as metaphysical has consequences that can be subjected to\nexperimental test has led Shimony to speak\nof the enterprise of experimental testing for violations of Bell\ninequalities as “experimental metaphysics” (Shimony 1984a, 35; 1993, 115). As may be\nexpected, the conclusions of experimental metaphysics are not\nunambiguous. Some prima facie plausible options are excluded,\nleaving a number of options open. In this section these are briefly\noutlined, with no attempt made to adjudicate between them. \nSuppose that one accepts that the experimental evidence indicates that\nBell-inequality violating correlations are a real feature of the\nworld, even when the experiments are conducted at spacelike\nseparation, on the most stringent of conditions that advocates of a\ncollapse locality loophole might wish to impose (see section 3.2.3).\nAcceptance of the reality of such correlations requires rejection of\nthe conjunction of any set of assumptions sufficient to entail a Bell\ninequality. The analysis of the assumptions of the proof outlined in\nsection 3 affords a taxonomy of positions that one might adopt in\nlight of their experimental violation, as at least one of the\nassumptions must be rejected. The distinctive assumption is the\nPrinciple of Local Causality, which, when applied to the setup of\nBell-type experiments, is embodied in the factorizability condition.\nThis condition can be maintained only if one of the supplementary\nassumptions is rejected. We begin with options rejecting supplementary\nassumptions. \nAs we are considering implications of accepting Bell\ninequality violations as a feature of reality, even when the\nexperiments are performed at spacelike separation, we set aside the\ncollapse locality loophole. This leaves us with two options. \nReject unique outcomes. This is the option taken by\nEverettian, or Many-Worlds theories, and related interpretations of\nquantum mechanics. The question arises whether locality can be\nmaintained on this option. Posing the question of whether Bell’s\ntheorem has implications for locality on Everettian or relative-state\ninterpretations requires that one first consider how to formulate\nlocality conditions in such a context, as the conditions formulated in\nsection 3.1, presuppose unique experimental outcomes. See Vaidman\n(1994), Bacciagaluppi (2002), Chapter 8 of Wallace (2012), Tipler\n(2014), Vaidman (2016), Brown and Timpson (2016), and Myrvold (2016)\nfor discussions of locality in Everettian interpretations, and Smerlak\nand Rovelli (2007) for a discussion in the context of relational\ninterpretations. \nReject the measurement independence assumption. There are\nessentially two ways to do this. One is to suppose a common cause in\nthe past that determines both experimental settings and experimental\noutcomes, as in the fanciful conspiracy story concocted by Shimony,\nHolt, and Clauser (1976). This sort of account has been called\nsuperdeterminism. Another avenue was suggested by Costa de\nBeauregard (1977), in a comment on the interchange between Bell and\nShimony, Holt, and Clauser. Costa de Beauregard objected that the\ndiscussants were disregarding the possibility of retrocausality, a\npossibility that he had advanced earlier (1976). If causal influence\nfrom future to past is admitted, then, even if the settings are\nregarded as free variables, they could influence the state of the\nsystem at the moment of preparation, contravening the assumption that\nthe preparation probability distribution be independent of\nexperimental settings. \nSuperdeterminism has gained some advocates, most notably Gerard\n’t Hooft, who makes it a feature of his Cellular Automaton\nInterpretation of quantum mechanics (’t Hooft 2016). It should\nbe noted that superdeterminism is a condition that is considerably\nstronger than mere determinism. It is uncontroversial that the sort of\ndevices that Bell speaks of, which have the effect of making the\nchoice setting of an experimental parameter depend on initial\nconditions in a way that is highly sensitive to small perturbations\nand which would ordinarily be accepted as effectively randomizing the\nchoice, do exist. Superdeterminism requires these settings to be\nnonetheless determined by the conditions at some past time, in such a\nway that the settings are distributed in just the right way to produce\nthe quantum statistics in Bell-type experiments, despite the\nunderlying physics being local, no matter what randomization\nmethod is employed. For example, in the experiment of Shalm\net al. (2015), the measurement decisions were determined by\napplying an XOR (exclusive or) operation to three bits from three\nindependent sources, one of which was a pseudorandom sequence\nconstructed from digits of \\(\\pi\\) and binary strings derived\nfrom various popular culture sources, including episodes of the movies\nMonty Python and the Holy Grail, the Back to the\nFuture trilogy, and episodes of Doctor Who, Saved by\nthe Bell, and Star Trek. A hypothetical cause that\nachieved the observed statistics via correlation between states of the\nphotons studied and the choice of measurements would have had to\nprecisely orchestrate the creative processes leading up to the\ndigitized versions of these cultural artifacts in such a way that,\nwhen processed in conjunction with the outputs of the random number\ngenerators, produced just the right sequence of experimental\nchoices. \nNo experiment can completely rule out the logical possibility of\nconspiracies of this sort. One can, however, conduct experiments that\nguarantee independence of the settings and the state of the systems at\nthe source on the basis of plausible physical assumptions. The\nexperiment of Scheidl et al. (2010) was the first to employ\nrandom-number generators operating at spacelike separation from the\ngeneration of the particle pairs at the source. Following a suggestion\nof Clauser, other tests have employed measurements on photons from\nMilky Way stars (Handsteiner et al. 2017) and from distant\nquasars (Rauch et al. 2018). In addition, the “Big Bell\nTest” ran experiments utilizing inputs provided by approximately\n100,000 volunteers (Abellán et al. 2018). \nIf the auxiliary assumptions are accepted, then the lesson to be\ndrawn from the experimental violation of Bell inequalities is that the\ncondition that Bell called the Principle of Local Causality\nmust be rejected. As we have seen, this condition is a conjunction of\ntwo conditions: a causal locality condition (PLC-1, above), and the\nPrinciple of the Common Cause. The causal locality condition itself\ncan be regarded as stemming from the assumption that causes temporally\nprecede their effects and Lorentz invariance of the relation of\ntemporal precedence. If the relation of temporal precedence is to be Lorentz\ninvariant, then either it is the trivial relation that holds between\nany two spacetime points, or else the past of a spacetime point is its\npast light cone (Stein 1991, 2009). \nIn this context, it is useful to recall that the factorizability\ncondition can be thought of as a conjunction of outcome independence\n(OI) and parameter independence (PI). PI is a consequence of causal\nlocality (PLC-1), applied to the set-up of the Bell experiments,\nwhereas OI is a consequence of causal locality and the common cause\nprinciple. This leaves us with a dilemma. A rejection of\nfactorizability involves a rejection of PI or OI. A rejection of PI\ninvolves a rejection of causal locality. If causal locality is to be\nmaintained, then OI, and hence the Common Cause Principle, must be\nrejected. \nAny deterministic theory must satisfy OI, and hence, a deterministic\ntheory that rejects factorizability must reject causal locality.  \nThese considerations yield a taxonomy of options for accepting the\nsupplementary assumptions while also accepting Bell-inequality\nviolating correlations.  \nReject PLC-1. One option is to reject the assumption PLC-1 of\ncausal locality, and accept that there are causal relations between\nevents that are outside of each others’ light-cones. There are\ntwo ways to do this. \nReject the Principle of Common Cause. A stochastic theory,\nsuch as a dynamical collapse theory, that reproduces quantum\nprobabilities for Bell experiments, will involve correlated events at\nspacelike separation. It need not, however, involve any events in the\ncommon past of these events that screen off the correlations; these\ncorrelations will be built-in to the laws of the theory yielding\nprobabilities of events. Whether one is willing to extend talk of\ncause-effect relations to refer to the relation between such events is\nmerely a matter of terminology; it should be noted, however, that\nthere is nothing of the usual asymmetry between cause and effect in\nthe relation between these events. To accept this relation as a new\nsort of symmetric cause-effect relation removes any reason there is to\nthink that cause-effect relations between spacelike separated events\nare incompatible with relativistic spacetime structure. \nA more common view is that the lesson of Bell’s theorem is that\nthere may be correlations that are not explicable in terms of\ncause-effect relations. This involves rejection of the Common Cause\nPrinciple (see, e.g., van Fraassen 1982, Fine 1989,\nButterfield 1992, Elby 1992). Others have maintained that, though the\nCommon Cause Principle as formulated by Reichenbach is to be rejected,\nthe principle has been formulated too narrowly, and  needs to\nbe reformulated in light of quantum phenomena. Some have suggested\nthat despite not satisfying Reichenbach’s principle,\ncorrelations violating Bell inequalities are due to a common cause in\ntheir past, namely, the process that created entanglement (Unruh 2002,\n136). Hofer-Szabo, Rédei, and Szabo (1999, 2002) have suggested\na modification of the common cause principle, as have Leifer and\nSpekkens (Leifer 2006, Leifer and Spekkens 2011). See Cavalcanti and\nLal (2014) for discussion and a critique of these proposals. \nDoes Bell’s theorem show that quantum theory is incompatible\nwith relativity?  \nThe answer, of course, depends on what one takes relativity theory to\nrequire. It can be shown (Eberhard 1978, Ghirardi, Rimini & Weber\n1980, Page 1982) that, in the absence of nonlocal interaction terms in\nthe Hamiltonian, quantum correlations cannot be exploited to send\nsignals superluminally. There has been a tendency in some of the\nliterature to take this by itself to indicate compatibility with\nrelativity. That this is insufficient can be seen from the fact that\nthere can be theories, such as the de Broglie-Bohm theory, that\nrequire nonrelativistic spacetime structure for the formulation of\ntheir dynamical laws, while not permitting signalling (at least, as\nlong as the usual distribution postulate for particle positions is\nsatisfied).  \nTake a relativistic spacetime structure to be a structure\nthat includes a relation of temporal precedence on which the past of a\nspacetime point is the set of points on or within its past lightcone,\nand its future, the set of points on or within its future lightcone,\nand other points are temporally unrelated to it, neither past nor\nfuture. One can ask whether a given account of the goings-on in the\nworld is compatible with a relativistic spacetime structure. If a\ntheory requires events outside of its lightcone to be partitioned into\nthose that are past, future, and simultaneous with a point, as does\nthe de-Broglie Bohm theory, it is not compatible with a relativistic\nspacetime structure. Compatibility of a theory with relativistic\nspacetime structure, in this sense, is a distinct issue from Lorentz\ncovariance of the equations expressing the theory’s dynamical\nlaws. One can construct theories on which nonrelativistic\nspacetime structure is introduced dynamically, as is done by\nDürr, Goldstein, Münch-Berndl and Zanghì (1999), who\nformulate a Bohmian theory against a background of Minkowski spacetime\nby introducing an auxiliary field that picks out a distinguished\nfoliation that is then used to formulate the dynamics of the theory\n(see Dürr et al. 2014 for discussion). Berndl,\nDürr, Goldstein, and Zanghì (1996) consider the class of\ntrajectory theories—that is, theories that, like the de\nBroglie-Bohm theory, assign definite trajectories to\nparticles—and prove that such theories could not satisfy the\nBorn-rule distribution postulate along arbitrary spacelike\nhyperplanes. The argument generalizes to any theories, such as modal\ninterpretations, that assign definite values to variables other\nthan position (Dickson and Clifton 1998, Arntzenius 1998, Myrvold\n2002). Theories of this sort, therefore, must invoke a distinguished\nfoliation. \nA deterministic theory must satisfy the condition of outcome\nindependence, and hence if one accepts \\((a)\\) that a violation\nof PI, that is, a situation in which the choice of an experiment on\none side changes the probability of an outcome on the other, is an\ninstance of causation, and \\((b)\\) that a cause must temporally\nprecede its effect, and then it follows that a deterministic theory\nthat satisfies the supplementary conditions and reproduces the quantum\ncorrelations is incompatible with relativistic spacetime structure.\nSuch a theory may, however, be Lorentz invariant at the phenomenal\nlevel, if (as in the de Broglie-Bohm theory) the distinguished\nfoliation is unobservable. \nA stochastic theory, such as a dynamical collapse theory, must involve\ncorrelations between spacelike separated events. A relation like that,\nhowever, is symmetric, and does not require that one of the events be\nin the past of the other. There is no prima facie need for\nnonrelativistic spacetime structure. Indeed, it is possible to\nformulate a fully relativistic dynamical collapse theory. A\nrelativistic generalization of the Ghirardi-Rimini-Weber (GRW) theory\nwas constructed by Dove (1996) and, independently, by Tumulka (2006).\nRelativistic versions of the Continuous Spontaneous Localization (CSL)\ntheory have been constructed by Bedingham (2011) and Pearle (2015).\nFor discussions of ontology for such theories, required to extract\nfrom them a sensible account of a world that includes macroscopic\nobjects, see Pearle (1997), Bedingham et al. (2014), and\nMyrvold (2019).  \nShimony, in several of his writings (Shimony 1978, 1983, 1984a,b, 1986,\n1988, 1989, 1990, 1991) spoke of “peaceful coexistence”\nbetween special relativity and quantum theory. The meaning of this\nvaried. In its initial formulation (1978), “peaceful\ncoexistence” had to do with regarding experimental outcomes as\ntransitions from potentiality to actuality, something that, Shimony\nsays, requires further investigation to be understood. In later\nwritings, peaceful coexistence is “suggested” by the fact\nthat quantum correlations cannot be exploited for superluminal\nsignalling (Shimony 1983), though still associated with the notion\nthat quantum mechanics motivates a change in our conception of an\nevent (Shimony 1984a), and involves the requirement of a coherent\nmeshing of events as described with respect to different reference\nframes (1986). In other works peaceful coexistence seems to be simply\nidentified with the impossibility of exploiting quantum correlations\nfor signalling (Shimony 1984b, 1990, 1991). Convinced by Bell (1990)\nthat anthropocentric considerations such as manipulability have no\nplace in considerations of fundamental physics, Shimony became\ndissatisfied with this avenue of approach to reconciling relativity\nand quantum theory (Shimony 2009, 489).  \nBell’s own attitude towards the question of whether Bell’s\ntheorem indicates a fundamental incompatibility between quantum theory\nand relativity seems to have varied with time. At the end of the 1964\narticle, he wrote  \nAt this point he is claiming incompatibility with relativity only for\ndeterministic hidden-variables theories. Later, however, he spoke of\n“the apparently essential conflict between any sharp formulation\n[of quantum theory] and fundamental relativity. That is to say, we\nhave an apparent incompatibility, at the deepest level, between the\ntwo fundamental pillars of contemporary theory…” (Bell\n1987b and 2004, 172; these are remarks from a meeting held in 1984).\nNote that this is hedged, and he speaks of “apparently\nessential” conflict only. In the same year, he wrote, “I\nam unable to prove, or even formulate clearly, the proposition that a\nsharp formulation of quantum field theory, such as that set out here,\nmust disrespect serious Lorentz invariance. But it seems to me that\nthis is probably so” (1984, 7; 1987b and 2004, 180). \nA shift in attitude was occasioned by the publication of the GRW\ndynamical collapse theory (Ghirardi, Rimini, and Weber, 1986). In a\ncommentary on this theory, Bell wrote, \nIn a lecture delivered in Trieste in the last year of his life, Bell\ndiscussed the prospects for a genuinely relativistic version of a\ndynamical collapse theory, and concluded that the difficulties\nencountered by Ghirardi, Grassi, and Pearle in producing a genuinely\nrelativistic version of the Continuous Spontaneous Localization theory\n(CSL), a theory that would be “Lorentz invariant, not just for\nall practical purposes but deeply, in the sense of Einstein,\neliminating entirely any privileged reference system from the\ntheory” (2007, 2931), were “Second-Class\nDifficulties,” technical difficulties, and not deep conceptual\nones. This seems to have been borne out by the construction of the\nfully relativistic collapse theories already mentioned.","contact.mail":"wmyrvold@uwo.ca","contact.domain":"uwo.ca"},{"date.published":"2004-07-21","date.changed":"2019-03-13","url":"https://plato.stanford.edu/entries/bell-theorem/","author1":"Wayne Myrvold","author1.info":"http://publish.uwo.ca/~wmyrvold/","author2.info":"https://www.inrim.it/ugov/persone/72","entry":"bell-theorem","body.text":"\n\n\nBell’s Theorem is the collective name for a family of\nresults, all of which involve the derivation, from a condition on\nprobability distributions inspired by considerations of local\ncausality, together with auxiliary assumptions usually thought of as\nmild side-assumptions, of probabilistic predictions about the results\nof spatially separated experiments that conflict, for appropriate\nchoices of quantum states and experiments, with quantum mechanical\npredictions. These probabilistic predictions take the form of\ninequalities that must be satisfied by correlations derived from any\ntheory satisfying the conditions of the proof, but which are violated,\nunder certain circumstances, by correlations calculated from quantum\nmechanics. Inequalities of this type are known as Bell\ninequalities, or sometimes, Bell-type inequalities.\nBell’s theorem shows that no theory that satisfies the\nconditions imposed can reproduce the probabilistic predictions of\nquantum mechanics under all circumstances. \n\n\nThe principal condition used to derive Bell inequalities is a\ncondition that may be called Bell locality, or\nfactorizability. It is, roughly, the condition that any\ncorrelations between distant events be explicable in local terms, as\ndue to states of affairs at the common source of the\nparticles upon which the experiments are performed. See section 3.1\nfor a more careful statement.\n\n\nThe incompatibility of theories satisfying the conditions that entail\nBell inequalities with the predictions of quantum mechanics permits an\nexperimental adjudication between the class of theories satisfying\nthose conditions and the class, which includes quantum mechanics, of\ntheories that violate those conditions. At the time that Bell\nformulated his theorem, it was an open question whether, under the\ncircumstances considered, the Bell inequality-violating correlations\npredicted by quantum mechanics were realized in nature. Beginning in\nthe 1970s, there has been a series of experiments of increasing\nsophistication to test whether the Bell inequalities are satisfied.\nWith few exceptions, the results of these experiments have confirmed\nthe quantum mechanical predictions, violating the relevant Bell\nInequalities. Until recently, however, each of these experiments was\nvulnerable to at least one of two loopholes, referred to as the\ncommunication, or locality loophole, and the\ndetection loophole (see section 5). Finally, in 2015,\nexperiments were performed that demonstrated violation of Bell\ninequalities with these loopholes blocked. This has consequences for\nour physical worldview; the conditions that entail Bell inequalities\nare, arguably, an integral part of the physical worldview that was\naccepted prior to the advent of quantum mechanics. If one accepts the\nlessons of the experimental results, then some one or other of these\nconditions must be rejected.\n\n\nFor much of the interval between the original publication of\nBell’s theorem and the experiments of Aspect and his collaborators, interest in Bell’s theorem was confined to a handful of\nphysicists and philosophers. During that period, much of the\ndiscussions on the foundations of physics occurred in a mimeographed\npublication entitled Epistemological Letters. In the wake of\nthe Aspect experiments (Aspect, Grangier, and Roger, 1982; Aspect,\nDalibard, and Roger 1982), there was considerable philosophical\ndiscussion of the implications of Bell’s theorem; see Cushing\nand McMullin, eds. (1989), for a snapshot of the philosophical\ndiscussions of the time. Interest was also stimulated by the\npublication of a collection of Bell’s papers on the foundations\nof quantum mechanics (Bell 1987b). The rise of quantum information\ntheory, which, among other things, explores the ways in which quantum\nentanglement can be used to perform tasks that would not be feasible\nclassically, also contributed to raising awareness of the significance\nof Bell’s theorem, which throws into sharp relief the difference\nbetween quantum entanglement-based correlations and classical\ncorrelations. The year 2014 was the 50th anniversary of the original\npublication of Bell’s theorem, and was marked by a special issue\nof Physical Review A (47, number 42, 24\nOctober 2014), a collection of essays (Bell and Gao, eds., 2016), and\na large conference comprising over 400 attendees (see Bertlmann and\nZeilinger, eds., 2017). The interested reader is urged to consult\nthese collections for an overview of current discussions on topics\nsurrounding Bell’s theorem.\n\nIn 1964 John S. Bell, a native of Northern Ireland and a staff member\nof CERN (European Organisation for Nuclear Research) whose primary\nresearch concerned theoretical high energy physics, published a paper\n(Bell 1964) in the short-lived journal Physics, which\neventually transformed the study of the foundation of quantum\nmechanics.  \nThe paper showed, under conditions that were relaxed in later work by\nBell (1971, 1976) himself and by his followers (Clauser, Horne, Shimony, and Holt 1969, Clauser and Horne 1974, Aspect 1983, Mermin 1986),\nthat, on the assumption of certain auxiliary conditions, no physical\ntheory that satisfies a certain locality condition, which may be\ncalled Bell locality, can fully reproduce the quantum\nprobabilities for outcomes of experiments. Since that time, variants\non the theorem, with family resemblances, have been formulated.\n“Bell’s Theorem” is the collective name for the\nentire family. \nThe theorem has roots in Bell’s investigations into the status\nof the hidden-variables program, and on earlier work concerning\nquantum entanglement.  \nBell presented several formulations of the theorem over the years\n(Bell 1964, 1971, 1976, 1990), and variants of it have been presented\nby others. The original derivation (1964) relied on a set-up involving\nperfect anticorrelation of the results of spin experiments on pairs of\nspin-1/2 particles prepared in the singlet state. Under this\ncondition, the Bell locality condition entails that outcomes of\nexperiments are predetermined by the complete specification of state,\na condition which we will call outcome determinism (OD).\nClauser, Horne, Shimony and Holt (1969) derived an inequality, the\nCHSH inequality, that does not require this assumption. Though in\ntheir proof they employed the condition OD, this condition is\nunnecessary for the derivation of the inequality, as shown by Bell\n(1971), who provided a proof of the CHSH inequality that relies on\nneither the assumption of perfect anticorrelation nor an assumption of\noutcome determinism.  \nOne line of investigation in the prehistory of Bell’s Theorem is\nBell’s examination of the hidden-variables program. This program\ninvolves supplementation of the quantum mechanical state of a system\nby further “elements of reality”, or “hidden\nvariables”, the incompleteness of the quantum state being the\nexplanation for the statistical character of quantum mechanical\npredictions concerning the system. A pioneering version of a hidden\nvariables theory was proposed by Louis de Broglie in 1926–7 (de\nBroglie 1927, 1928), and revived by David Bohm in 1952 (Bohm 1952; see\nalso the entry on\n Bohmian mechanics). \nIn a paper (Bell 1966) that was written before the one in which\nBell’s theorem first appeared, but, due to an editorial mishap,\nwas published later, Bell raises the question of the viability of\na hidden-variables theory that reproduces the statistical predictions\nof quantum mechanics via averaging over better defined states that\nuniquely determine the result of any experiment that could be\nperformed. In this paper he examines several theorems that had been\npresented as no-go theorems for theories of this sort, and\nsupplements them with one of his own, a theorem that was\nindependently formulated by Specker (1960), and published by Kochen\nand Specker (1967), and has come to be known as the Kochen-Specker\nTheorem or Bell-Kochen Specker Theorem (see\n entry on the Kochen-Specker Theorem\n for more details). In each case Bell argues that the proof contains\npremises that are physically unwarranted. \nThe Bell-Kochen-Specker theorem is a corollary of Gleason’s\ntheorem (Gleason 1957), though Bell and Kochen-Specker obtain it\ndirectly, and not via Gleason’s theorem, whose proof is\nconsiderably more intricate. The question addressed by Gleason has to\ndo with assignments of probabilities to closed subspaces of a Hilbert\nspace (or, equivalently, to projection operators onto such subspaces),\nsuch that the probabilities assigned to orthogonal projections are\nadditive. Gleason proved that, in a Hilbert space of dimension 3 or\ngreater, any such assignment of probabilities can be represented by a\ndensity operator. The BKS theorem deals with the special case in which\nthe assignments are confined to the values 1 or 0. \nThe assumption that a definite value (1 or 0) is to be assigned to\neach projector on the system’s Hilbert space, with the condition\nthat values assigned to commuting projectors be additive, is a\nweakening of the assumption of the von Neumann no-go theorem (von\nNeumann 1932), which assumes that the quantum mechanical additivity of\nexpectation values of all observables, whether represented by\ncommuting operators or not, extends to the hypothetical\ndispersion-free states (see section 2 of entry on\n the Kochen-Specker Theorem).\n Despite the prima facie plausibility of this assumption,\nBell regards it, too, as physically unmotivated, and therefore, unlike\nKochen and Specker, does not regard the Bell-Kochen-Specker theorem as\na no-go theorem for hidden-variables theories. The reason for this is\nthat the assumption embodies a condition that was later to be known as\n noncontextuality.[1]\n In a Hilbert space of dimension greater than two, any projection\noperator will be a member of more than one complete set of commuting\nprojections. For each of these complete sets, there will be an\nexperiment whose outcomes correspond to the projections in the set.\nThe assumption of noncontextuality amounts to the assumption that a\nvalue can be assigned to a projection operator that is independent of\nwhich of these experiments is to be performed, or as Bell puts it,\nthat “measurement of an observable must yield the same value\nindependently of what other measurements may be made\nsimultaneously.” The assumption need not hold; “[t]he\nresult of an observation may reasonably depend not only on the state\nof the system (including hidden variables) but also on the complete\ndisposition of the apparatus” (Bell 1966, 451; 1987b and 2004,\n9 ). \nNoncontextual hidden-variables theories that reproduce the predictions\nof quantum mechanics are ruled out by the Bell-Kochen-Specker theorem.\nA natural question arises as to the possibility of a contextual\nhidden-variables theory on which the unavoidable contextuality is\nrestricted to local dependencies. Is it possible to have a \ntheory on which the outcome of an experiment performed in some spatial\nregion \\(A\\) is determined by the complete state of a system in a\nway that does not depend on the disposition of experimental apparatus\nat a distance from \\(A\\)? \nBell’s article ends with a brief exposition of the Bohm theory,\nnoting in particular the feature that “in this theory an\nexplicit causal mechanism exists whereby the disposition of one piece\nof apparatus affects the results obtained with a distant piece”\n(Bell 1966, 452; 1987b and 2004, 11). The article ends with the\nremark, \nTo the second of the above-quoted sentences is attached a note:\n“Since the completion of this paper such a proof has been\nfound.” The potential drama of the announcement was spoiled by\nthe fact that the follow-up paper containing the proof (Bell 1964) had\nalready been published (see Jammer 1974, 303, for an account of the\ncircumstances that led to the publication delay). \nThe fact that Bell’s theorem has roots into investigations on\nhidden-variables theories has led to a misconception that the theorem\nis a no-go theorem for hidden-variables theories tout court.\nThere could be no such theorem, since, as Bell himself repeatedly\nemphasized, there is a functioning hidden-variables theory, the de\nBroglie-Bohm theory. \nAnother line of investigation leading to Bell’s Theorem was the\ninvestigation of quantum mechanical entangled states, that is, quantum\nstates of a composite system that cannot be expressed either as\nproducts of quantum states of the individual components, or as\nmixtures of product states. That quantum mechanics admits of such\nentangled states was discovered by Erwin Schrödinger (1926) in\none of his pioneering papers, but the significance of this discovery\nwas not emphasized until the paper of Einstein, Podolsky, and Rosen\n(1935). They examined correlations between the positions and the\nlinear momenta of two well separated spinless particles and concluded\nthat in order to avoid an appeal to nonlocality these correlations\ncould only be explained by “elements of physical reality”\nin each particle — specifically, both definite position and\ndefinite momentum — and since this description is richer than\npermitted by the uncertainty principle of quantum mechanics their\nconclusion is effectively an argument for a hidden variables\ninterpretation.\n [2]\n See also the entry on the\n Einstein-Podolsky-Rosen paradox. \nIn the present section the pattern of Bell’s 1964 paper will be\nfollowed: formulation of a framework, derivation of an inequality,\ndemonstration of a discrepancy between certain quantum mechanical\nexpectation values and this inequality. As already mentioned,\nBell’s 1964 derivation assumed an experiment involving perfect\nanticorrelation of the results of aligned Stern-Gerlach experiments on\na pair of entangled spin-\\(\\frac{1}{2}\\) particles. Experimental tests, in\nwhich perfect anticorrelation (or correlation) may be approximated but\ncannot be assumed to hold exactly, require this assumption to be\nrelaxed. Papers which took the steps from Bell’s 1964\ndemonstration to the one given here are Clauser, Horne, Shimony and\nHolt (1969), Bell (1971), Clauser and Horne (1974), Aspect (1983) and\nMermin\n (1986).[3]\n Other strategies for deriving Bell-type theorems will be mentioned in\n Section 6. \nThis conceptual framework first of all postulates an ensemble of pairs\nof systems, the individual systems in each pair being labeled as 1 and\n2. Each pair of systems is characterized by a “complete\nstate” \\(\\lambda\\) which contains the entirety of the\nproperties of the pair at the moment of generation. No assumption\nwhatsoever is made about the nature of the state \\(\\lambda\\).\nThe state space \\(\\Lambda\\), which is the totality of all\npossible complete states \\(\\lambda\\), could be a set of quantum\nstates and nothing more, or a set whose elements are quantum states\nsupplemented by additional variables, or something more exotic,\nperhaps some state space as yet unthought of. \nWe make the assumption, which remains tacit in most expositions, that\nwe have an appropriate choice of subsets of \\(\\Lambda\\) to be\nregarded as the measurable subsets, forming a measurable space to\nwhich probabilistic considerations may be applied. It is assumed that\nthe mode of generation of the pairs establishes a probability\ndistribution \\(\\rho\\) that is independent of the adventures of\neach of the two systems after they separate. This does not preclude\ntemporal evolution of the properties of the two systems after\nseparation. What is assumed is that the state \\(\\lambda\\)\nprescribes probabilities for subsequent events (including any temporal\nevolution), and thereby probabilities for outcomes of experiments to\nbe performed on the systems. \nDifferent experiments may be performed on each system. We will use\n\\(a, a'\\) as variables ranging over possible\nexperiments on 1, and \\(b, b'\\) as variables that\nrange over experiments on 2. It is not assumed that these parameters\ncapture the complete state of the experimental apparatus, which might\nhave a range of microstates corresponding to each experimental\nsetting. \nThe result of an experiment with setting \\(a\\) on system 1 is\nlabeled by a real parameter \\(s\\), which can take on values from\na discrete set \\(S_a\\) of real numbers in the interval\n[\\(-1, 1\\)]. Likewise, the result of an experiment on 2 is labeled\nby a parameter \\(t\\), which can take on any of a discrete set of\nreal numbers \\(T_b\\) in \\([-1, 1]\\). As suggested by\nthe subscripts, the sets of potential outcomes may depend on the\nexperimental settings. The restriction of the values of the outcome\nlabels to lie in the interval \\([-1, 1]\\) is of no physical significance, and\nis a choice made only for convenience. Indeed, the use of numbers to\nlabel outcomes is merely a matter of convenience, and, can, if\ndesired, be dispensed with, in favour of relations expressed solely in\nterms of the relevant probabilities, as in the CH inequality,\ninequality (25), below. Bell’s own version of his theorem assumed\nexperiments with two possible outcomes, labelled \\(\\pm 1\\). Other\nvariants of the theorem involve larger sets of potential outcomes. \nWe assume that, for each pair of settings \\(a, b\\), and every\n\\(\\lambda\\) in \\(\\Lambda\\), there is a probability function\n\\(p_{a,b}(s,t \\mid \\lambda)\\), which takes on values in the interval\n[0, 1] and sums to unity when summed over all \\(s\\) in \\(S_a\\) and\n\\(t\\) in \\(T_b\\). These response functions may include implicit\naveraging over possible states of the experimental\napparatus.[4] We\ncan use these probability functions — which we will call\nresponse probabilities — to define marginal\nprobabilities: \nHere, and in what follows, it is to be understood that the sums be\ntaken over all \\(s \\in S_a\\) and \\(t \\in T_b\\). Define\n\\(A_{\\lambda}(a, b)\\), \\(B_{\\lambda}(a, b)\\) as the expectation\nvalues, for complete state \\(\\lambda\\), of the outcomes of experiments\non system 1 and system 2, respectively, when the settings are \\(a,\nb\\).  \nNow define the expectation value of the product \\(st\\)\nof outcomes: \nBell-type inequalities follow from a condition, inspired by locality\nconsiderations, which has been called Factorizability, or\nBell locality. \nThis is the condition formulated explicitly by Bell in his later\nexpositions of Bell’s theorem (Bell 1976, 1990). It should be\nnoted that Bell himself regarded this condition “not as the\nformulation of ‘local causality’, but as a\nconsequence thereof” (Bell 1990, 109; 2004, 243). Bell\nrefers to the factorizability condition (F) as the condition that the\ncorrelations be locally explicable (Bell 1981, C2–55;\n1987b and 2004, 152; see also 1990, 109; 2004, 243). This has two\ncomponents: the condition that the correlations be explained, and not\ntaken as primitive, and the condition that the explanation be local.\nThis will be discussed further in section 3.1.  \nAs we have seen, Bell’s investigations were stimulated, in part,\nby the question of the prospects for theories in which the complete\nstate uniquely determines the outcome of any experiment, and quantum\nuncertainty relations reflect incompleteness of the usual\nspecification of state. For such a theory, the response probabilities\n\\(p_{ab}(s,t|\\lambda)\\)\ntake on the extremal values 0 or 1. Let us call this condition OD,\nfor outcome determinism. It is also sometimes referred to,\nmisleadingly, as realism (see discussion in\n section 3.3,\n below). \nSuppes and Zanotti (1976) showed that, for the special case of perfect\ncorrelations between outcomes of the two experiments, in which an\noutcome of an experiment on one system makes possible prediction with\nprobability one of the outcome of an experiment on the other, OD must\nbe satisfied if the factorizability condition (F) is. This applies to\nthe case considered by Bell 1964. In Bell 1971 and subsequent\nexpositions Bell provided a generalization that does not presume\nperfect correlation and does not require OD, either as a supposition\nor as a consequence of other\n suppositions.[5] \nIf the factorizability condition (F) is satisfied, then \nThe above definitions are valid for experiments with any number of\ndiscrete outcomes. An important special case is that in which each\nexperiment has only two distinct outcomes, which we may label by\n\\(\\pm 1\\). For the case of bivalent experiments, (4) is equivalent to\ncondition (F). \nNow consider the quantities \nLet S\\(_{\\varrho}\\) denote the corresponding relation\nbetween the expectation values of the \\(E_{\\lambda}s\\),\nwith respect to the preparation distribution \\(\\rho\\). \nSince the absolute value of the average of any random variable cannot\nbe greater than the average of its absolute value, is clear that \nWe now have the materials in hand required to state and prove a\nBell-type theorem. The first step consists of showing that, if the\nfactorizability condition (F) is satisfied, then \nThe second step consists of showing that there are quantum states and\nexperimental set-ups that are such that the quantum-mechanical\nexpectation values violate the inequality (8). This shows that no\ntheory satisfying the factorizability condition can reproduce the\nstatistical predictions of quantum mechanics in all situations.\nMoreover, the inequality furnishes a bound on how close the\npredictions of such a theory can come to reproducing quantum\nmechanical predictions. The inequality (8), due to Clauser, Horne,\nShimony, and Holt (1969), is known as the CHSH inequality.\n \nWe now prove the first part of the theorem, namely, that the CHSH\ninequality follows from the factorizability condition (F). If F is\nsatisfied, then, using (4), and the fact that \\(A_{\\lambda}(a)\\) and\n\\(A_{\\lambda}(a')\\) lie in the interval \\([-1,1]\\), \nIt is easy to check that \nSince \\(B_{\\lambda}(b)\\) and \\(B_{\\lambda}(b')\\) also lie in the\ninterval \\([-1,1]\\), from (9) and (10) we conclude that, for every\n\\(\\lambda\\),  \nSince this bound holds for every value of \\(\\lambda\\), it must\nalso hold for the expectation value of\n\\(S_{\\lambda}\\). \nThis, together with (7), yields the CHSH inequality (8). \nThe final step of the proof our Bell-type theorem is to exhibit a\nsystem, a quantum mechanical state, and a set of quantities for which\nthe statistical predictions violate inequality (8). The example used\nby Bell stems from Bohm’s variant of the EPR thought-experiment\n(Bohm 1951, Bohm and Aharonov 1957). A pair of spin-\\(\\frac{1}{2}\\) particles\nis produced in the singlet state, \nwhere \\(\\mathbf{n}\\) is an arbitrarily chosen direction, and\n\\(|\\mathbf{n}+\\rangle , |\\mathbf{n}-\\rangle\\) are\nspin-up and spin-down eigenstates of spin in the \\(\\mathbf{n}\\)\ndirection. The state is rotationally invariant, and hence the\nexpression (13) represents the state for any direction\n\\(\\mathbf{n}\\). If Stern-Gerlach experiments are performed on the\ntwo particles, then, regardless of the direction of the axes of the\ndevices, there is equal probability of both results on each side. If\nexperiments are done with the axes of the two devices aligned, the\nresults are guaranteed to be opposite; spin-up on one will be obtained\nin one experiment if and only if spin-down is obtained on the other.\nIf the axes are at right angles, the results are probabilistically\nindependent. In the general case, with device axes in directions given\nby unit vectors \\(\\mathbf{a}, \\mathbf{b}\\), respectively,\nwith results labelled \\(\\pm 1\\), then the expectation value of the\nproduct of the outcomes is given by \nwhere \\(\\theta_{\\mathbf{a}\\mathbf{b}} = \\theta_{\\mathbf{a}} -\n\\theta_{\\mathbf{b}}\\) is the angle between the vectors\n\\(\\mathbf{a},\\mathbf{b}\\). \nThough the example of spin-\\(\\frac{1}{2}\\) particles in the singlet state is\nubiquitous in the literature as an illustrative example,\npolarization-entangled photons have been more significant for\nexperimental tests of Bell inequalities. Consider a pair of photons 1\nand 2 propagating in the \\(z\\)-direction. Let \\(|x\\rangle_j\\) and \\(|y\\rangle_j\\)\nrepresent states in which photon \\(j \\; (j =1, 2)\\) is\nlinearly polarized in the \\(x\\)- and \\(y\\)- directions,\nrespectively. Consider the following state vector, \nwhich is invariant under rotation of the \\(x\\) and \\(y\\)\naxes in the plane perpendicular to \\(z\\). The total quantum state\nof the pair of photons 1 and 2 is invariant under the exchange of the\ntwo photons, as required by the fact that photons are integral spin\nparticles. Suppose now that photons 1 and 2 impinge respectively on\nthe faces of birefringent crystal polarization analyzers I and II,\nwith the entrance face of each analyzer perpendicular to \\(z\\).\nEach analyzer has the property of separating light incident upon its\nface into two outgoing non-parallel rays, the ordinary ray\nand the extraordinary ray. The transmission axis of the\nanalyzer is a direction with the property that a photon polarized\nalong it will emerge in the ordinary ray (with certainty if the\ncrystals are assumed to be ideal), while a photon polarized in a\ndirection perpendicular to \\(z\\) and to the transmission axis\nwill emerge in the extraordinary ray. See Figure 1: Figure 1\n\n(reprinted with permission) \nPhoton pairs are emitted from the source, each pair quantum\nmechanically described by \\(\\ket{\\Phi}\\) of Eq. (15). I and II are\npolarization analyzers, with outcome \\(s=1\\) and \\(t=1\\)\ndesignating emergence in the ordinary ray, while \\(s = -1\\)\nand \\(t = -1\\) designate emergence in the extraordinary\nray. The crystals are also idealized by assuming that no incident\nphoton is absorbed, but each emerges in either the ordinary or the\nextraordinary ray.  \nThe expectation value, in state \\(\\ket{\\Phi}\\), of the product of\n\\(s\\) and \\(t\\), is \nNote that this displays the same sort of sinusoidal dependence on\nangle exhibited by (14), with \\(2\\theta\\) replacing\n\\(\\theta\\). \nOften, in popular writings, the case of aligned devices is the only\none mentioned, and the perfect anticorrelation (for spin-\\(\\frac{1}{2}\\)\nparticles in the singlet state) or correlation (for photons in state\n\\(\\ket{\\Phi}\\)) of results in this case is offered as evidence of\n“spooky action at a distance.” In fact, as Bell (1964,\n1966) demonstrated by means of simple toy models, this behaviour can\nbe reproduced by entirely local means. An important insight of\nBell’s was to consider the less-than perfect correlations\nobtained when the device axes are not aligned. In Bell’s toy\nmodel, correlations fall off linearly with the angle between the\ndevice axes, whereas the quantum correlations (14, 16) fall off\nsinusoidally; the decrease in correlations away from the case of\nperfect alignments is less steep than in the toy model. Bell’s\ntheorem shows that this sort of behaviour is not a peculiarity of his\nmodel; no model satisfying the condition F can reproduce the quantum\ncorrelations for all angles. This can be seen by considering the\nquantum predictions (14, 16) and plugging them into the expression for\n\\(S\\). For a pair of spin-\\(\\frac{1}{2}\\) particles in the singlet state,\nwe have, \nChoose coplanar unit vectors \\(\\mathbf{a},\\) \\(\\mathbf{a}',\\)\n\\(\\mathbf{b},\\) \\(\\mathbf{b}'\\) such that \\(\\theta_{\\mathbf{b}'} -\n\\theta_{\\mathbf{a}}\\, =\\) \\(\\theta_{\\mathbf{a}} - \\theta_{\\mathbf{b}}\\, =\\)\n\\(\\theta_{\\mathbf{b}} - \\theta_{\\mathbf{a}'} = \\phi\\), and therefore,\n\\(\\theta_{\\mathbf{b}'} - \\theta_{\\mathbf{a}'} = 3\\phi\\). This choice\nyields  \nThis exceeds the CHSH bound (8) when \\(0 \\lt |\\phi | \\lt\\)\n\\(\\arccos\\left((\\sqrt{3} - 1)/2 \\right) \\approx 1.95\\) radians, or 68°, with\nmaximum violation at \\(\\phi = \\pm {\\pi}/{4}\\), or 45°. For\nthese angles, we have \nFor the case of polarization-entangled photons, we have, \nThis takes on its maximum at \\(\\phi = {\\pi}/{8}\\), or 22.5°. \nThis value \\(2\\sqrt{2}\\) appearing in equations (19) and (21) is the\nmaximum violation of the CHSH inequality for any quantum state, as\nshown by Tsirelson (Cirel’son 1980). This bound on quantum\nviolations of the CHSH inequality is called the Tsirelson\nbound. As Gisin (1991) and Popescu and Rohrlich (1992)\nindependently demonstrated, for any pure entangled quantum state of a\npair of systems observables can be found yielding a violation of the\nCHSH inequality. Popescu and Rohrlich (1992) also show that the\nmaximum amount of violation is achieved with a quantum state of\nmaximum degree of entanglement. Incidentally, this is not true for\nmixed states; there are entangled mixed states not violating any Bell\ninequality (Werner 1989). \nThe distinctive condition giving rise to the Bell Inequality\nassumption is the factorizability condition (F). This condition is\nmotivated by considerations concerning locality and causality.\nConsiderations of this sort have been the focus of the discussion of\nthe implications of Bell’s theorem. However, in order to a\nderive a conflict between the predictions for theories that satisfy\nthis assumption, other assumptions—some of which are the sort\nusually accepted without question in scientific\nexperimentation—are needed. The analysis of Bell’s theorem\nhas provoked careful scrutiny of the reasoning required to reach the\nconclusion that F is to be rejected. As a result, some assumptions\nthat in another context would have been left implicit have been made\nexplicit, and each has been challenged by some authors. In this\nsection we outline a set of assumptions sufficient to ensure\nsatisfaction of Bell inequalities, which, therefore, constitute a set\nof assumptions that cannot all be satisfied by any theory that yields,\nin agreement with experiment, violations of Bell inequalities. There\nare other paths to Bell inequalities; see, in particular, Wiseman and\nCavalcanti (2017), who offer an analysis similar to that found here,\nas well as other analyses. \nAs mentioned above, the article (Bell 1964) in which Bell’s\ntheorem first appeared is a follow-up to Bell (1966), which explores\nthe prospects for hidden-variables theories in which the outcome of an\nexperiment performed is predetermined by the complete state of the\nsystem. The introduction to Bell (1964) begins with mention of\ntheories with additional variables that are to restore causality and\nlocality, and says that “In this note that idea will be\nformulated mathematically and shown to be incompatible with the\nstatistical predictions of quantum mechanics.” The locality\nassumption is glossed as the requirement “that the result of a\nmeasurement on one system be unaffected by operations on a distant\nsystem with which it has interacted in the past.” Applied to the\ncase at hand, of Stern-Gerlach experiments performed on an entangled\npair of spin-1/2 particles, this is “the hypothesis ... that if\nthe two measurements are made at places remote from one another the\norientation of one magnet does not influence the result obtained with\nthe other.” Bell follows this with, \nThis suggests that OD is not being assumed, but, rather, derived, via\nan EPR-type argument, from a locality assumption and the perfect\nanticorrelations predicted by the considered quantum state. This is\nhow Bell explained the reasoning in later publications (see Bell 1981,\nfn\n 10).[6] \nIn Bell (1976) and (1990), Bell derives the factorizability condition\nfrom a condition he calls the Principle of Local Causality\n(see Norsen 2011 for discussion). In (1990) he begins his analysis\nwith a rehearsal of the reason that relativity should be taken to\nprohibit superluminal causation. On the usual notion of causation, the\ncause-effect relation is taken to be temporally asymmetric, with\ncauses temporally preceding their effects. In a relativistic\nspacetime, events at spacelike separation are taken to have no\ntemporal order. Any system of coordinates will assign time coordinates\nto each of any pair of events, but, if the events are spacelike\nseparated, the time coordinates assigned to a pair of events at\nspacelike separation will differ in their ordering, depending on which\nreference frame is being employed. If we take all of these reference\nframes to be physically on a par, it must be concluded that there is\nno temporal order between the events, as relations that are not\nrelativistically invariant have no physical significance. \nOn the basis of these considerations, that is, Lorentz invariance and\nthe assumption that causes temporally precede their effects, Bell\nintroduces what he calls the principle of local causality.\n \nThis, says Bell, is “not yet sufficiently sharp and clean for\nmathematics.” For this reason, he introduces what he presents as\na sharpened version of the principle (refer to Figure 2). Figure 2 \nIn Bell’s terminology, a beable is any element of a\nphysical theory that is taken to correspond to something physically\nreal, and local beables pertaining to a space-time region are\nthose contained within that region. \nThe transition from what we have called PLC-1 to PLC-2 should,\naccording to Bell, “be viewed with the utmost suspicion”\nas “it is precisely in cleaning up intuitive ideas for\nmathematics that one is likely to throw the baby out with the\nbathwater” (1990, 106; 2004, 239). The relation between them\nis not further discussed in that article, but remarks in other papers\nshed light on the transition between them. \nIn (1976), Bell motivates the formulation of the local causality\ncondition with the remark, \nHe then precedes to formulate the condition of local causality, which\nis that a full specification of beables in the overlap of the backward\nlight cones of the spacetime regions 1 and 2 should screen off\ncorrelations between them. Implicit in this is that correlations\nbetween two variables be susceptible to causal explanation, either via\na causal connection between the variables, or via a common cause. This\nassumption is made explicit in a later article (Bell 1981), in which\nhe says that “the scientific attitude is that correlations cry\nout for explanation” (Bell 1981, C2–55; 1987b and 2004, 152). \nThe assumption that correlations between two variables that are not in\na cause-effect relation to each other be explained by some common\ncause was named the principle of the common cause by\nReichenbach (1956, § 19), and for this reason is often referred\nto as Reichenbach’s Common Cause Principle, though\nReichenbach made no pretense of originating the principle, and\nregarded it as a codification of a mode of inference common in both\nscience and everyday life. See the entry on\n Reichenbach’s common cause principle\n for more details. A variable \\(C\\) is a Reichenbachian common\ncause of a correlation between two variables \\(A\\) and \\(B\\)\nif the variables \\(A\\) and \\(B\\) are uncorrelated,\nconditional on specification of the value of \\(C\\).\nReichenbach’s Common Cause Principle says that, if two\ncorrelated variables are not in a cause-effect relation with each\nother, there is a Reichenbachian common cause of their correlations.\nThe condition we have called PLC-1 does not, by itself, entail PLC-2,\nbut it does follow from the conjunction of PLC-1 and\nReichenbach’s Common Cause Principle. \nWhat Bell calls the Principle of Local Causality, PLC-2, can be\nthought of as a conjunction of (1) a causal locality condition along\nthe lines of PLC-1, restricting causes of an event to that\nevent’s past light cone, and (2) Reichenbach’s Common\nCause Principle. The former condition can, itself, be regarded as\nfollowing from relativistic invariance and the principle that the\ncause of an event lie in its temporal past. Bell’s Principle of\nLocal Causality thus follows from the conjunction of three\nassumptions, all of which, in various writings, were explicitly\nformulated by Bell: \nThe condition F of factorizability is the application, to the\nparticular set-up of Bell-type experiments, of Bell’s Principle\nof Local Causality. As we have seen, it can be thought of as the\nconjunction of the condition of causal locality, and the common cause\nprinciple. In this section we apply those conditions to the set-up of\nBell-type experiments. \nOn the assumption that the experimental settings can be treated as\nfree variables, whose values are determined exogenously, if the choice\nof setting on one wing is made at spacelike separation from the\nexperiment on the other, a dependence of the probability of the\noutcome of one experiment on the setting of the other would seem\nstraightforwardly to be an instance of a nonlocal causal influence.\nThe condition that this not occur can be formulated as follows.  \nThis is the condition that has come to be known as parameter\nindependence, following Shimony (1986, 1990). \nFor fixed values of the experimental settings, Bell’s Principle\nof Local Causality entails that the outcomes of the experiments on the\ntwo systems be independent, conditional on the specification\n\\(\\lambda\\) of the complete state of the system at the source.\nThis is the condition \nThis is the condition that has come to be known as outcome\nindependence, following Shimony (1986, 1990). As demonstrated by\nJarrett (1983, 1984), the factorizability condition (F) is the conjunction\nof parameter independence and outcome independence. The two conditions\nbear different relations to the locality and causality conditions\ndiscussed in the previous subsection. PI is a consequence of the\ncausal locality condition PLC-1 alone, whereas OI requires in addition\nthe assumption of the common cause principle. \nThe parsing of the Bell locality condition as a conjunction of PI and\nOI is due to Jarrett (1983, 1984), who referred to the conditions as\nlocality and completeness. Jarrett (1983, 1984, 1989)\nargued that a violation of PI would inevitably permit superluminal\nsignalling. The conclusion requires an additional assumption, that the\nstate of the system be\n controllable.[7]\n It would not hold for a theory on which there are principled\nlimitations on the ability of would-be signallers to control the\nstates of the systems they are dealing with. Nonetheless, in some of\nthe literature PI has been treated as equivalent to no-signalling.\nThis is predicated on regarding any limitations on control that might\nprevent a violation of PI from being exploited for signalling involves\nonly practical limitations irrelevant to foundational concerns (see\ne.g. Ballentine and Jarrett 1987, fn. 6; Jarrett 1989, 70; Shimony 1993, 139),\ndisregarding the possibility of theories involving in-principle\nlimitations on control.  \nThough it might seem that this goes without saying, the entire\nanalysis is predicated on the assumption that, of the potential\noutcomes of a given experiment, one and only one occurs, and hence\nthat it makes sense to speak of the outcome of an experiment.\nThe reason that this assumption is worth mentioning is that there is a\nfamily of approaches to the interpretation of quantum mechanics,\nnamely, Everettian, or “many-worlds” approaches, and some\nvariants of the relational approach, hold that all outcomes occur in\nwhat are effectively distinct worlds. See entries on\n many-worlds interpretation of quantum mechanics,\n Everett’s relative-state formulation of quantum mechanics, and\n relational quantum mechanics\n  \nBell’s original analysis (1964, 1971) tacitly assumed that the\ncomplete state \\(\\lambda\\) is sampled from the same probability\ndistribution \\(\\rho\\), no matter what choice of experiment is\nmade, and that, for this reason, the subset of experiments\ncorresponding to any given choice of settings is a fair sample of the\ndistribution of \\(\\lambda\\). Clauser and Horne (1974, fn. 13)\nmade this assumption explicit. At the end of Bell (1976), in which\nBell provides an exposition of a Bell-type theorem, we find a remark\nregarding the independence of \\(\\lambda\\) and the experimental\nsettings. \nThis assumption was not, however, explicitly invoked in the\nderivation, and the role this assumption is meant to be play was not\nmade sufficiently clear in that article.  \nThat an assumption of this sort is required was emphasized by Shimony,\nHorne and Clauser (1976), who illustrate the point via a conspiracy\ninvolving manufacturers of experimental apparatus and\nphysicists’ assistants. The director of the conspiracy concocts\na set of correlation experiment data, consisting of a sequence of\npairs of experimental settings and results obtained. The director\ninstructs the manufacturer to preprogram the apparatus to produce the\ndesired outcomes, and the assistants of the physicists performing the\nexperiment to orchestrate the apparatus settings to match those\nspecified by the predetermined list. Clearly, the conspirators may\nutilize any set of correlation data for their nefarious schemes;\nhence, any set of correlation data can be obtained as the outcomes of\nthis sort of process, without any violation of any sort of locality\ncondition. Nonetheless, for the sorts of experiments envisaged in\ntests of Bell inequalities, Shimony, Horne, and Clauser consider the\nassumption of independence of settings and the state of the particle\npairs to be justified, even though relativistic causality does not\nmandate this independence. \nIn response, Bell (1977) acknowledged that his formulation in (1976)\nhad been inadequate, and explained his reasoning as “primarily\nan analysis of certain kinds of physical theory.” \nHe makes it clear, however, that no metaphysical hypothesis of\nexperimenters exempt from the laws of physics need be invoked. What is\nneeded is something considerably weaker than the condition that the\nvariables not be determined in the overlap of the backward light cones\nof the experiments. What is needed is that they be “at least\neffectively free for the purpose at hand.”  Bell argues that a deterministic randomizer that is extraordinarily sensitive to initial\nconditions would suffice to provide the requisite independence, and that variables of this type may be treated as if they have\nimplications only for events in their future light cones. The upshot\nof the exchange was substantial agreement between Bell and Shimony,\nHorne, and Clauser. \nWe will call the assumption that experimental settings may be regarded\nas “effectively free for the purpose at hand” and treated\nas statistically independent of the variable \\(\\lambda\\), the\nassumption of measurement independence. It has also been\ncalled the free will assumption, the freedom of choice\nassumption and the no-conspiracies assumption. \nIn experimental tests of Bell locality, care is taken that the\nexperiments on the two systems, from choice of experimental setting to\nregistration of results, take place at spacelike separation. It is\nassumed that experiments have unique results. The question arises as\nto when the unique result emerges. It is typically assumed\nthat the result is definite once a detector has been triggered or the\nresult is recorded in a computer memory. However, as Kent (2005) has\npointed out, proposals have been made according to which the quantum\nstate of the apparatus would remain in a superposition of terms\ncorresponding to distinct outcomes for a greater length of time. One\nsuch proposal is the suggestion that state reduction takes place only\nwhen the uncollapsed state involves a superposition of sufficiently\ndistinct gravitational fields (Diósi 1987, Penrose 1989,\n1996). Another is Wigner’s suggestion that conscious awareness\nof the result is required to induce collapse (Wigner 1961). This gives\nrise to what Kent calls the collapse locality loophole. One\ncan consider theories—Kent calls the family of such theories\ncausal quantum theories—on which collapses are\nlocalized events, and the probability of a collapse is independent\nof events, including other collapses, at spacelike separation from it.\nA theory of that sort would differ in its predictions from standard\nquantum theory, but a test to discriminate between such a theory and\nstandard quantum mechanics would require a set-up in which the entire\nexperiment on one system, from arrival to satisfaction of the collapse\ncondition, takes place at spacelike separation from the experiment on\nthe other. If the experiments are taken to end, not when the detector\nis triggered, but when the difference between outcomes amounts to\ndifferences in mass configurations large enough to correspond to\nsignificantly distinct gravitational fields, then, as Kent argued,\nexperiments extant at the time of writing (2005) were subject to this\nloophole. The experiment of Salart et al. (2008) closed the\nloophole for the particular proposals of Penrose and Diósi,\nthough, as Kent (2018) points out, altering the Penrose-Diósi\nthreshold by a few orders of magnitude would render them compatible\nwith the results of this experiment. No experiment to date has addressed\nthe collapse locality loophole if the collapse condition is taken to\nbe awareness of the result by a conscious observer. See Kent (2018)\nfor proposals of ways in which causal quantum theory could be\nsubjected to more stringent tests. \nIt has become commonplace to say that (provided that the supplementary\nassumptions are accepted), the class of theories ruled out by\nexperimental violations of Bell inequalities is the class of local\nrealistic theories, and that the worldview to be abandoned is\nlocal realism. The ubiquity of the use of this terminology\ntends to obscure the fact that not all who use it use it in the same\nsense; further, it is not always clear what is meant when the phrase\nis used. \nThe terminology of “local realistic theories” as the\ntargets of experimental tests of Bell inequalities was introduced by\nClauser and Shimony (1978), intended as a synonym for what Clauser and\nHorne (1974) called “objective local theories.” The\nterminology was adopted by d’Espagnat (1979) and Mermin (1980).\nFor Clauser and Shimony realism is “a philosophical view\naccording to which external reality is assumed to exist and have\ndefinite properties, whether or not they are observed by\nsomeone” (1978, 1883). In a similar vein, d’Espagnat\n(1979) says that realism is “the doctrine that regularities in\nobserved phenomena are caused by some physical reality whose existence\nis independent of human observers” (158). Mermin, on the other\nhand, takes realism to involve the condition that we have called\noutcome determinism (OD): “As I shall use the term\nhere, local realism holds that one can assign a definite value to the\nresult of an impending measurement of any component of the spin of\neither of the two correlated particles, whether or not that\nmeasurement is actually performed” (Mermin 1980, 356). This is\nnot a commitment of realism in the sense of Clauser and Shimony, who\nexplicitly consider stochastic local realistic theories . \nIt is Mermin’s sense that seems to be most widely used in the\ncurrent literature. In this sense, local realism, applied to\nthe set-up of the Bell experiments, amounts to the conjunction of\nParameter Independence (PI) and outcome determinism (OD). Now, it is\ntrue that, if PI and OD hold, so does factorizability (F), and hence\nthe Bell inequalities. But the condition OD is stronger than what is\nrequired, as the conjunction of PI and the strictly weaker condition\nOI also suffice. Thus, to say that violations of Bell inequalities\nrule out local realistic theories, with “realism”\nidentified as outcome determinism, is true but misleading, as it may\nsuggest that one can retain locality by rejecting\n“realism” in the sense of outcome determinism. However, if\none accepts the supplementary assumptions, one is obliged to reject\nnot merely the conjunction of OD and PI, but the weaker condition of\nfactorizability, which contains no assumption regarding predetermined\noutcomes of experiments. \nFurther confusion arises if the two senses are conflated. This can\nlead to the notion that the condition OD is equivalent to the\nmetaphysical thesis that physical reality exists and possess\nproperties independent of their cognizance by human or other agents.\nThis would be an error, as stochastic theories, on which the outcome\nof an experiment is not uniquely determined by the physical state of\nthe world prior to the experiment, but is a matter of chance, are\nperfectly compatible with the metaphysical thesis. One occasionally\nfinds traces of a conflation of this sort in the literature; see,\ne.g., d’Espagnat (1979) and Mermin (1981). \nFor other authors, rejection of realism seems to amount primarily to\nan avowal of operationalism. If all one asks of a theory is that it\nproduce the correct probabilities for outcomes of experiments,\neschewing all questions about what sort of physical reality gives rise\nto these outcomes, then this undercuts the motivation of the analysis\nthat leads to Bell’s theorem. In this sense of\n“realism”, it is not an assumption of the theorem but a\nmotivation for formulating it.  \nSeveral authors (see, in particular, Norsen 2007; Maudlin 2014) have\nargued that no clear sense of “realism” has been\nidentified such that realism, in that sense, is a particular\npresupposition of the derivation of Bell inequalities (as\ndistinguished from a presupposition of all physics). These authors\nurge rejection of the currently prevalent practice of saying that\n“local realist” theories are the targets of experimental\ntests of Bell inequalities. Other authors maintain that there is,\nindeed, a sense of “realism” on which realism is an\nassumption of the derivation of Bell inequalities; see Żukowski\nand Brukner (2014), Werner (2014), Żukowski (2017), Clauser\n(2017). \nA first proposal to test Bell inequality was made by Clauser, Horne,\nShimony, and Holt (1969), henceforth CHSH, who suggested that the\npairs 1 and 2 be photons produced in an atomic cascade from an initial\natomic state with total angular momentum \\(J = 0\\) to an\nintermediate atomic state with \\(J = 1\\) to a final atomic state\n\\(J = 0\\), as in an experiment performed with calcium vapor for\nother purposes by Kocher and Commins (1967). The proposed test was\nfirst performed by Freedman and Clauser (1972). The result obtained by\nFreedman and Clauser was 6.5 standard deviations from the limit\nallowed by the CHSH inequality but in good agreement with the quantum\nmechanical prediction. This was a difficult experiment, requiring 200\nhours of running time, much longer than in most later tests of\nBell’s Inequality, which were able to use lasers for exciting\nthe sources of photon pairs.  \nSince then, several dozen experiments have been performed to test\nBell’s Inequalities. References will now be given to some of the\nmost noteworthy of these, along with references to survey articles\nwhich provide information about others. A discussion of more recent\nexperiments addressed to close two serious loopholes in the early Bell\nexperiments, the “detection loophole” and the\n“communication loophole”, will be reserved for\n Section 5. \nHolt and Pipkin completed in 1973 (Holt 1973) an experiment very much\nlike that of Freedman and Clauser, but examining photon pairs produced\nin the \\(9^1 P_1 \\rightarrow 7^3 S_1\\rightarrow 6^3 P_0\\) cascade in the zero nuclear-spin isotope of mercury-198 after\nusing electron bombardment to pump the atoms to the first state in\nthis cascade. The result of Holt and Pipkin was in fairly good\nagreement with the CHSH Inequality, and in disagreement with the\nquantum mechanical prediction by nearly 4 standard\ndeviations—contrary to the results of Freedman and Clauser.\nBecause of the discrepancy between these two early experiments, Clauser\n(1976) repeated the Holt-Pipkin experiment, using the same cascade and\nexcitation method but a different spin-0 isotope of mercury, and his\nresults agreed well with the quantum mechanical predictions but\nviolated Bell’s Inequality. Clauser also suggested a possible\nexplanation for the anomalous result of Holt-Pipkin: that the glass of\nthe Pyrex bulb containing the mercury vapor was under stress and hence\nwas optically active, thereby giving rise to erroneous determinations\nof the polarizations of the cascade photons. \nFry and Thompson (1976) also performed a variant of the Holt-Pipkin\nexperiment, using a different isotope of mercury and a different\ncascade and exciting the atoms by radiation from a narrow-bandwidth\ntunable dye laser. Their results also agreed well with the quantum\nmechanical predictions and disagreed sharply with Bell’s\nInequality. They gathered data in only 80 minutes, as a result of the\nhigh excitation rate achieved by the laser. \nFour experiments in the 1970s — by Kasday-Ullman-Wu,\nFaraci-Gutkowski-Notarigo-Pennisi, Wilson-Lowe-Butt, and\nBruno-d’Agostino-Maroni — used photon pairs produced in\npositronium annihilation instead of cascade photons. Of these, all but\nthat of Faraci et al. gave results in good agreement with the\nquantum mechanical predictions and in disagreement with Bell’s\nInequalities. A discussion of these experiments is given in the review\narticle by Clauser and Shimony (1978), who regard them as less\nconvincing than those using cascade photons, because they rely upon\nstronger auxiliary assumptions. \nThe first experiment using polarization analyzers with two exit\nchannels, thus realizing the theoretical scheme envisaged in\n Section 2,\n was performed in the early 1980s with cascade photons from\nlaser-excited calcium atoms by Aspect, Grangier, and Roger (1982). The\noutcome confirmed the predictions of quantum mechanics over those\nsatisfying the Bell inequalities more dramatically than any of its\npredecessors, with the experimental result deviating from the upper\nlimit in a Bell’s Inequality by 40 standard deviations. An\nexperiment soon afterwards by Aspect, Dalibard, and Roger (1982),\nwhich aimed at closing the communication loophole, will be discussed\nin\n Section 5.\n The historical article by Aspect (1992) reviews these experiments and\nalso surveys experiments performed by Shih and Alley, by Ou and\nMandel, by Rarity and Tapster, and by others, using photon pairs with\ncorrelated linear momenta produced by down-conversion in non-linear\ncrystals. Discussion of more recent Bell tests can be found in review papers\n(Zeilinger 1999, Genovese 2005, 2016). \nPairs of photons have been the most common physical systems in Bell\ntests because they are relatively easy to produce and analyze, but\nthere have been experiments using other systems. Lamehi-Rachti and\nMittig (1976) measured spin correlations in proton pairs prepared by\nlow-energy scattering. Their results agreed well with the quantum\nmechanical prediction and violated Bell’s Inequality, but, as in the positronium experiments, strong\nauxiliary assumptions had to be made.  \nThe outcomes of the Bell tests provide dramatic confirmations of the\nprima facie entanglement of many quantum states of systems\nconsisting of 2 or more constituents. Actually, the first confirmation\nof entanglement antedated Bell’s work, since Bohm and Aharonov\n(1957) demonstrated that the results of Wu and Shaknov (1950), Compton\nscattering of the photon pairs produced in positronium annihilation,\nalready showed the entanglement of the photon pairs. \nThe derivations of all the variants of Bell’s Inequality depend\nupon independence conditions inspired by relativistic causality. In\nthe early tests of Bell’s Inequalities it was plausible that\nthese conditions were satisfied just because the 1 and the 2 arms of\nexperiment were spatially well separated in the laboratory frame of\nreference. This satisfaction, however, is a mere contingency not\nguaranteed by any law of physics, and hence it is physically possible\nthat the setting of the analyzer of 1 and its detection or\nnon-detection could influence the outcome of analysis and the\ndetection or non-detection of 2, and conversely. This is the\ncommunication loophole, to which the early Bell tests were\nsusceptible. It is addressed by ensuring that the experiments on the\ntwo systems take place at spacelike separation.  \nAspect, Dalibard, and Roger (1982) published the results of an\nexperiment in which the choices of the orientations of the analyzers\nof photons 1 and 2 were performed so rapidly that they were events\nwith space-like separation. No physical modification was made of the\nanalyzers themselves. Instead, switches consisting of vials of water\nin which standing waves were excited ultrasonically were placed in the\npaths of the photons 1 and 2. When the wave is switched off, the\nphoton propagates in the zeroth order of diffraction to polarization\nanalyzers respectively oriented at angles \\(a\\) and \\(b\\),\nand when it is switched on the photons propagate in the first order of\ndiffraction to polarization analyzers respectively oriented at angles\n\\(a'\\) and \\(b'\\). The complete choices of\norientation require time intervals 6.7 ns and 13.37 ns respectively,\nmuch smaller than the 43 ns required for a signal to travel between\nthe switches in obedience to special relativity theory. Prima\nfacie it is reasonable that the independence conditions are\nsatisfied, and therefore that the coincidence counting rates agreeing\nwith the quantum mechanical predictions constitute a refutation of the\nBell inequality and hence of the family of theories that entail it.\nThere are, however, several imperfections in the experiment. First of\nall, the choices of orientations of the analyzers are not random, but\nare governed by quasiperiodic establishment and removal of the\nstanding acoustical waves in each switch. A scenario can be invented\naccording to which clever hidden variables of each analyzer can\ninductively infer the choice made by the switch controlling the other\nanalyzer and adjust accordingly its decision to transmit or to block\nan incident photon. Also, coincident count technology is employed for\ndetecting joint transmission of 1 and 2 through their respective\nanalyzers, and this technology establishes an electronic link which\ncould influence detection rates. And because of the finite size of the\napertures of the switches there is a spread of the angles of incidence\nabout the Bragg angles, resulting in a loss of control of the\ndirections of a non-negligible percentage of the outgoing photons. \nThe experiment of Tittel, Brendel, Zbinden, and Gisin (1998) did not\ndirectly address the communication loophole but threw some light\nindirectly on this question and also provided dramatic evidence\nconcerning the maintenance of entanglement between particles of a pair\nthat are well separated. Pairs of photons were generated in Geneva and\ntransmitted via cables with very small probability per unit length of\nlosing the photons to two analyzing stations in suburbs of Geneva,\nlocated 10.9 kilometers apart on a great circle. The counting rates\nagreed well with the predictions of quantum mechanics and violated the\nCHSH inequality. No precautions were taken to ensure that the choices\nof orientations of the two analyzers were events with space-like\nseparation. The great distance between the two analyzing stations\nmakes it difficult to conceive a plausible scenario for a conspiracy\nthat would violate Bell’s independence conditions. Furthermore\n— and this is the feature which seems most to have captured the\nimagination of physicists — this experiment achieved much\ngreater separation of the analyzers than ever before, thereby\nproviding a test of a conjecture by Schrödinger (1935) that\nentanglement is a property that may dwindle with spatial separation.\nMore recently, Bell\ninequality violation was demonstrated even at 144 km distance (Scheidl\net al., 2010) and, in 2017, from satellite transmission with\na 1200 km distance (Yin et al., 2017).  \nAn experiment that came closer to closing the communication loophole\nis that of Weihs, Jennewein, Simon, Weinfurter, and Zeilinger (1998).\nThe pairs of systems used to test a Bell’s Inequality are photon\npairs in the entangled polarization state \nwhere the ket \\(\\ket{H}\\) represents horizontal polarization and\n\\(\\ket{V}\\) represents vertical polarization. Each photon pair\nis produced from a photon of a laser beam by the down-conversion\nprocess in a nonlinear crystal. The momenta, and therefore the\ndirections, of the daughter photons are strictly correlated, which\nensures that a non-negligible proportion of the pairs jointly enter\nthe apertures (very small) of two optical fibers, as was also achieved\nin the experiment of Tittel et al.. The two stations to which\nthe photon pairs are delivered are 400 m apart, a distance which light\nin vacuo traverses in \\(1.3 \\mu\\)s. Each photon emerging from an optical\nfiber enters a fixed two-channel polarizer (i.e., its exit channels\nare the ordinary ray and the extraordinary ray). Upstream from each\npolarizer is an electro-optic modulator, which causes a rotation of\nthe polarization of a traversing photon by an angle proportional to\nthe voltage applied to the modulator. Each modulator is controlled by\namplification from a very rapid generator, which randomly causes one\nof two rotations of the polarization of the traversing photon. An\nessential feature of the experimental arrangement is that the\ngenerators applied to photons 1 and 2 are electronically independent.\nThe rotations of the polarizations of 1 and 2 are effectively the same\nas randomly and rapidly rotating the polarizer entered by 1 between\ntwo possible orientations \\(a\\) and \\(a'\\) and the\npolarizer entered by 2 between two possible orientations \\(b\\)\nand \\(b'\\). The output from each of the two exit channels\nof each polarizer goes to a separate detector, and a “time\ntag” is attached to each detected photon by means of an atomic\nclock. Coincidence counting is done after all the detections are\ncollected by comparing the time tags and retaining for the\nexperimental statistics only those pairs whose tags are sufficiently\nclose to each other to indicate a common origin in a single\ndown-conversion process. Accidental coincidences will also enter, but\nthese are calculated to be relatively infrequent. This procedure of\ncoincidence counting eliminates the electronic connection between the\ndetector of 1 and the detector of 2 while detection is taking place,\nwhich conceivably could cause an error-generating transfer of\ninformation between the two stations. The total time for all the\nelectronic and optical processes in the path of each photon, including\nthe random generator, the electro-optic modulator, and the detector,\nis conservatively calculated to be smaller than 100 ns, which is much\nless than the \\(1.3 \\mu\\)s required for a light signal between the two\nstations. \nThe experimental result in the experiment of Weihs et al. is\n\\(2.73 \\pm 0.02,\\) in good agreement with the quantum mechanical\nprediction, and it is 30 standard deviations away from the upper limit\nof the CHSH inequality inequality (8). Aspect, who designed the first\nexperimental test of a Bell Inequality with rapidly switched analyzers\n(Aspect, Dalibard, Roger 1982) appreciatively summarized the import of\nthis result: \nEven if some small imperfection prevented the experiment of Weihs\net al. from completely blocking the detection loophole, these\nproblems were overcome in subsequent experiments. \nThe CHSH inequality (8) is a relation between expectation values. An\nexperimental test, therefore, requires empirical estimation of the\nprobabilities of the outcomes of experiments. This estimation involves\ncomputing a ratio of event-counts: the number of pair-production\nevents with a certain outcome to the total number of pair-production\nevents. Typically, in experiments involving photons, most of the pairs\nproduced fail to enter the analyzers. Furthermore, some photons that\nenter the analyzers will fail to be detected; in addition, the\ndetector will occasionally register a detection even when no photon is\ndetected (the rate of occurrence of this is known as the\n“dark-count”). \nThree strategies for addressing this issue have been pursued.  \nOne is to employ an auxiliary assumption to yield an estimate of the\nnormalization factor required to infer relative frequencies from\nevent-counts, as required by a test of the CHSH inequality. CHSH\n(1969) proposed the assumption that, if a photon passes through an\nanalyser, its probability of detection is independent of the\nanalyser’s orientation. Though physically plausible, this is not\na condition required by local causality. \nThe fact that an assumption of this sort is needed for the analysis of\nexperiments of this type was made clear by toy models constructed by\nPearle (1970) and Clauser and Horne (1974). In these models, the rates\nat which the photon pairs pass through the polarization analyzers with\nvarious orientations are consistent with an inequality of Bell’s\ntype, but the hidden variables provide instructions to the photons and\nthe apparatus not only regarding passage through the analyzers but\nalso regarding detection, thereby violating the fair sampling\nassumption. Detection or non-detection is selective in the model in\nsuch a way that the detection rates violate the Bell-type inequality\nand agree with the quantum mechanical predictions. Other models were\nconstructed later by Fine (1982a) and corrected by Maudlin (1994) (the\n“Prism Model”) and by C.H. Thompson (1996) (the\n“Chaotic Ball model”). Although all these models are\nad hoc and lack physical plausibility, they constitute\nexistence proofs that theories satisfying the local causality\ncondition can be consistent with the quantum mechanical predictions\nprovided that the detectors are properly selective. \nA second strategy involves construction of an experimental set-up in\nwhich the production of each particle-pair may be registered. Clauser\nand Shimony (1978) referred to apparatus achieving this as\n“event-ready” detectors; some recent literature has\nreferred to a process of this sort as “heralding.” \nA third strategy involves employment of an inequality that can be\nshown to be violated without knowledge of the absolute value of the\nprobabilities involved. This eliminates the need for untestable\nauxiliary assumptions. An inequality suitable for this purpose was\nfirst derived by Clauser and Horne (1974) (henceforth CH). The set-up\nis as before, with the exception that each analyzer will have only one\noutput channel, and the eventualities to be considered are detection\nand non-detection. We want an inequality expressed in terms of\nprobabilities of detection alone. The same sort of reasoning that\nleads to the CHSH inequality yields the CH Inequality: \nThe probabilities appearing in (25) can be estimated by dividing\nevent-counts registered in a run of an experiment by the total number\nof pairs produced. If we assume that the production rate at the source\nis independent of the analyzer settings, we can take the normalization\nfactor to be the same for each term, and hence, the magnitude of this\nfactor need not be known in order to demonstrate a violation of the\nupper bound of (25). Another useful observation was made by Eberhard\n(1993), who demonstrated that the minimal detection efficiency for a\ndetection loophole free experiment can be reduced (from 82% to 67%)\nfor non-maximally entangled states (i.e. a bipartite entangled state\nwith different weight for the two components). This involves starting\nwith a specified efficiency level, and then choosing a state and a set\nof observables that maximize violation of the CH inequality at that\nefficiency level. \nFor the maximally entangled states we have been considering, in the\nidealized case of perfect detection efficiency, inequality (25) is\nmaximally violated by the quantum predictions for the same settings\nconsidered above for violation of the CHSH inequality. However, for\nnon-ideal experiments, the quantum predictions satisfy the inequality\nunless detector efficiency is high, considerably higher than that of\nany experiment that had been performed up until the time that CH were\nwriting. For that reason, CH introduced a new auxiliary assumption,\ncalled the no-enhancement assumption: for any value of\n\\(\\lambda\\), and any setting of an analyzer, the probability of\ndetection with the analyzer present is no higher than the probability\nof detection with the analyzer removed. Let \\(p^1_{\\infty}\\) and\n\\(p^2_{\\infty}\\) be the probabilities of detection of particles 1 and\n2 when their respective analyzers have been removed. This assumption\ngives rise to what may be called the second CH\ninequality: \nAs CH note, this is violated by the results of the Freedman and\nClauser experiment, and hence that experiment rules out theories\nsatisfying the factorizability condition (F) and the no-enhancement\nassumption, though it does not rule out the toy model constructed by\nCH. \nHistorically, the efforts toward a detection loophole-free experiment\nfollowed two main paths, though a few other possibilities were also\nexplored. One of these other possibilities involved K,B mesons\n(Selleri 1983, Go 2004), where the detection loophole reappears in\nanother form (Genovese, Novero, and Predazzi 2001); and another\ninvolved solid state systems (Ansmann et al. 2009). \nOne of the main avenues of approach employed entangled ions. The use\nof ions looked very promising, since for such experiments detection\nefficiency is very high. The experiment of Rowe et al. (2001)\nemployed beryllium ions, observing a CHSH inequality violation \\(S =\n2.25 \\pm 0.03\\) with a total detection efficiency of about 98%.\nNevertheless, in this set-up the measurements on two ions not only\nwere not space-like separated; there was a common measurement on the\ntwo ions. More recently, the distance between ions was increased. For\ninstance, Matsukevich et al. (2008) entangled two ytterbium\nions via interference and joint detection of two emitted photons, with\nthe distance between the ions set to 1 meter. However, a conclusive\nexperiment of this sort that eliminated also the communication\nloophole would require a separation of kilometers. \nThe other main avenue of approach, which paved the way to a conclusive\ntest of Bell inequalities, involved innovations in tests using\nphotons. First, efficient sources of photon entangled states were\nrealized by exploiting Parametric Down Conversion, a non-linear\noptical phenomenon in which a photon of higher energy converts into\ntwo lower frequency photons inside a non-linear medium in such a way\nthat energy and momentum are conserved. This allows a high collection\nefficiency due to wave vector correlation of the emitted photons.\nNext, high efficiency single photon Transition Edge Sensors were\nproduced. These advances led to detection loophole-free experiments\nwith photons (Giustina et al. 2013, Christensen et\nal. 2013) and finally to the conclusive tests discussed in the\nnext section. \nIn 2015 three papers appeared claiming a conclusive test of Bell\ninequalities. The first (Hensen et al., 2015) achieved a\nviolation of the CHSH inequality via an event-ready scheme. This\nexperiment is based on using electronic spin associated with the\nnitrogen-vacancy (NV) defect in two diamond chips located in distant\nlaboratories. In the experiment, each of these two spins is entangled\nwith the emission time of a single photon. Then the two,\nindistinguishable, photons are transmitted to a remote beam splitter.\nA measurement is made on the photons after the beam splitter. An\nappropriate result of the measurement of the photons projects the\nspins in the two diamond chips onto a maximally entangled state, on\nwhich a Bell inequality test is realized. The high efficiency in spin\nmeasurement and the distance between the laboratories allows closure\nof the detection and communication loophole at the same time. However,\nthe experiment utilized only a small number, 245, of trials, and thus\nthe statistical significance (2 standard deviations) of the result\n\\(S = 2.42 \\pm 0.20\\) is limited. \nThe other two experiments, published in the same issue of Physical\nReview Letters (Giustina et al. 2015, Shalm et\nal. 2015), are based on transmitting two polarization-entangled\nphotons, produced by Parametric Down Conversion, to two remote\nlaboratories, where they are measured by high detection efficiency\nTransition Edge Sensors. These experiments use states that are not\nmaximally entangled, but are optimized, in accordance with the\nanalysis of Eberhard (1993), to produce a maximal violation of the CH\ninequality, given the detection efficiency of the experiments. In both\nof these experiments a violation of the CH inequality was obtained, at\na high degree of statistical significance. Shalm et al.\nreport a \\(p\\)-value of \\(2.3 \\times 10^{-7}\\), whereas\nGiustina et al. report a \\(p\\)-value of \\(3.4 \\times\n10^{-31}\\), corresponding to an 11.5 standard deviation effect. A very\ncareful analysis of data (including spacelike separation of detection\nevents), of statistical significance, and of all possible loopholes\nleaves really no space for doubts about their conclusiveness. Besides\nthe detection and communication loophole, these two experiments\naddress also the following issues: \nFurthermore, independent random number generators based on laser phase\ndiffusion guarantee the elimination of freedom-of-choice loophole\n(except in presence of superdeterminism or other hypotheses that, by\ndefinition, do not allow a test through Bell inequalities). \nIn summary, these experiments, having satisfied carefully all the\nconditions required for a conclusive test, unequivocally tested Bell\ninequalities without any additional hypothesis. \nThis section will discuss in some detail two variants of Bell’s\nTheorem which depart in some respect from the conceptual framework\npresented in\n Section 2.\n Both are less general than the version in\n Section 2,\n because they rely on perfect correlations, which, together with the\nfactorizability condition (F), entail outcome determinism (OD). At the\nend of the section two other variants will be mentioned briefly but\nnot summarized in detail. \nThe first variant is due independently to\n Kochen,[8]\n Stairs (1978, 1983), and Heywood and Redhead (1983). Its ensemble of\ninterest consists of pairs of spin-1 particles in the entangled\nstate \nwhere \\(\\ket{z,i}_1\\), with \\(i = -1\\) or 0 or 1 is the spin state of\nparticle 1 with component of spin \\(i\\) along the axis \\(z\\), and\n\\(\\ket{z,i}_2\\) has an analogous meaning for particle 2. If \\(x,y,z\\)\nis a triple of orthogonal axes in 3-space then the components \\(s_x,\ns_y, s_z\\) of the spin operator along these axes do not pairwise\ncommute. However, the squares of these operators — \\(s_x^2,\ns_y^2, s_z^2\\) — do commute, and therefore, in view of the\nconsiderations of\n Section 1,\n any two of them can constitute a context in the measurement of the\nthird. If the operator of interest is\n\\(s_z^2\\), the axes \\(x\\) and\n\\(y\\) can be any pair of orthogonal axes in the plane\nperpendicular to \\(z\\), thus offering an infinite family of\ncontexts for the measurement of\n\\(s_{z}^2\\). As noted in\n Section 1\n Bell exhibited the possibility of a contextual hidden\nvariables theory for a quantum system whose Hilbert space has\ndimension 3 or greater even though the Bell-Kochen-Specker theorem\nshowed the impossibility of a non-contextual hidden variables\ntheory for such a system. The strategy of the argument is to use the\nentangled state of Eq. (27) to predict the outcome of measuring\n\\(s_{z}^2\\) for particle 2 (for any choice of \\(z)\\) by measuring its\ncounterpart on particle 1. A specific complete state \\(\\lambda\\) would\ndetermine whether \\(s_{z}^2\\) of 1, measured together with a context\nin 1, is 0 or 1. Agreement with the quantum mechanical prediction of\nthe entangled state of Eq. (27) implies that \\(s_z^2\\) of 2 has the\nsame value 0 or 1. But if the factorizability condition (F) is\nassumed, then the result of measuring \\(s_{z}^2\\) on 2 must be\nindependent of the remote context, that is, independent of the choice\nof \\(s_{x}^2\\) and \\(s_y^2\\) of 1, hence of 2 because of correlation,\nfor any pair of orthogonal directions \\(x\\) and \\(y\\) in the plane\nperpendicular to \\(z\\). It follows that the hypothetical theory which\nsupplies the complete state \\(\\lambda\\) is not contextual after all,\nbut maps the set of operators \\(s_z^2\\) of 2, for any direction\n\\(z\\), noncontextually into the pair of values (0, 1). But\nthat is impossible in view of the Bell-Kochen-Specker theorem. The\nconclusion is that no theory satisfying the factorizability condition\n(F) is consistent with the quantum mechanical predictions of the\nentangled state (29). \nA simpler proof of Bell’s Theorem, also relying upon\ncounterfactual reasoning and based upon a deterministic local theory,\nis that of Hardy (1993), here presented in Laloë’s (2001)\nformulation. Consider an ensemble of pairs 1 and 2 of\nspin\\(-\\frac{1}{2}\\) particles, the spin of 1 measured along\ndirections in the \\(xz\\)-plane making angles \\(a=\\theta /2\\) and\n\\(a'=0\\) with the \\(z\\)-axis, and angles \\(b\\) and \\(b'\\) having\nanalogous significance for 2. The quantum states for particle 1 with\nspins \\(+\\frac{1}{2}\\) and \\(-\\frac{1}{2}\\) relative to direction\n\\(a'\\) are respectively \\(\\ket{a',+}_1\\) and \\(\\ket{a',-}_1\\), and\nrelative to direction \\(a\\) are respectively \nthe spin states for 2 are analogous. The ensemble of interest is\nprepared in the entangled quantum state \n(unnormalized, because normalization is not needed for the following\nargument). Then for the specified \\(a, a',\nb\\), and \\(b'\\) the following quantum mechanical\npredictions hold: \nand for almost all values of the \\(\\theta\\) of Eq. (31) \nwith the maximum occurring around \\(\\theta = 9\\degr\\). Inequality\n(33) asserts that for the specified angles there is a non-empty\nsubensemble \\(E'\\) of pairs for which the results for a\nspin measurement along \\(a\\) for 1 and along \\(b\\) for 2 are\nboth +. Eq. (30) implies the counterfactual proposition that if the\nspin of a 2 in \\(E'\\) had been measured along\n\\(b'\\) then with certainty the result would have been\n\\(-\\); and likewise Eq. (31) implies the counterfactual proposition\nthat if the spin of a 1 in \\(E'\\) had been measured along\n\\(a'\\) then with certainty the result would have been\n\\(-\\). It is in this step that counterfactual reasoning is used in\nthe argument. Since the subensemble \\(E'\\) is non-empty, we\nhave reached a contradiction with Eq. (32), which asserts that if the\nspin of 1 is measured along \\(a'\\) and that of 2 is\nmeasured along \\(b'\\) then it is impossible that both\nresults are \\(-\\). The incompatibility of a deterministic local\ntheory with quantum mechanics is thereby demonstrated. \nAn attempt was made by Stapp (1997) to demonstrate a strengthened\nversion of Bell’s theorem which dispenses with the conceptual\nframework outlined above, and to use instead the logic of\ncounterfactual conditionals. His intricate argument has been the\nsubject of a criticism by Shimony and Stein (2001, 2003), who are\ncritical of certain counterfactual conditionals that are asserted by\nStapp by means of a “possible worlds” analysis without a\ngrounding on a local deterministic theory, and a response by Stapp (2001)\nhimself, who defends his argument with some modifications. \nThe three variants of Bell’s Theorem considered so far in this\nsection concern ensembles of pairs of particles. An entirely new\ndomain of variants is opened by studying ensembles of\n\\(n\\)-tuples of particles with \\(n \\ge 3\\). The prototype\nof this kind of theorem was demonstrated by Greenberger, Horne, and\nZeilinger (1989) (GHZ) for \\(n=4\\) and modified to \\(n=3\\)\nby Mermin (1990) and by Greenberger, Horne, Shimony, and Zeilinger\n(1990) (GHSZ). In the theorem of GHZ an entangled quantum state was\nwritten for four spin-\\(\\frac{1}{2}\\) particles and the expectation value of\nthe product of certain binary measurements performed on the individual\nparticles was calculated. They then showed that the attempt to\nduplicate this expectation value subject to the factorizability\nconstraint (F) produces a contradiction. A similar result was obtained\nby Mermin for a state of 3 spin-\\(\\frac{1}{2}\\) particles and by GHSZ for a\nstate of 3 photons entangled with respect to their direction of\npropagation. Because of the length of these arguments and limitations\nof space in the present article the details will not be summarized\nhere, it is however worth mentioning that experimental tests were\nrealized (Pan et al. 2000, Genovese 2005) (also in this case\nwith additional hypotheses). \nThe set-up envisaged in the proof of Bell’s theorem highlights a\nstriking prediction of quantum theory, namely, long-distance\nentanglement, and experimental tests of the Bell inequalities provide\nconvincing evidence that it is a feature of reality. Moreover,\nBell’s theorem reveals that the entanglement-based correlations\npredicted by quantum mechanics are strikingly different from the sort\nof locally explicable correlations familiar in a classical\ncontext. \nInvestigations into entanglement and the ways in which it can be\nexploited to perform tasks that would not be feasible with only\nclassical resources forms a key part of the discipline of quantum\ninformation theory (see Benatti, et al. eds. (2010), and the\nentries on\n quantum computing\n and\n quantum entanglement and information).\n In drawing attention to the import of entanglement and the ways in\nwhich it is different from anything in classical physics, Bell’s\ntheorem and the experimental work derived from it provided, at least\nindirectly, some of the impetus to the development of quantum\ninformation theory. \nBell’s theorem has played a direct role in the development of\ndevice independent quantum cryptography. One can exploit quantum\ncorrelations to devise a quantum key distribution protocol that is\nprovably secure on the assumption that, whatever the underlying\nphysics is, it does not permit superluminal signalling. The basic idea\nis that, if one has Bell inequality-violating correlations at\nspacelike separation, any predictability of the results beyond that\nafforded by the quantum probabilities could be exploited for\nsuperluminal signalling; the contrapositive of this is that impossibility of\nsignalling entails “absolute randomness” — absolute\nin the sense of being independent of the details of the underlying\nphysics beyond the prohibition on superluminal\n signalling.[9] \nThe experiments demonstrating loophole-free violations of Bell\ninequalities take on particular significance in this context. The toy\nmodels demonstrating the reality of the detector inefficiency loophole\nlack physical plausibility, and, in the absence of conspiracies aimed\nat deceiving the experimenters, may be disregarded on the assumption\nthat nature, though subtle, is not malicious. Cryptography, on the\nother hand, by its very nature must take into account the possibility\nof a conspiracy aimed at deceiving the users of a cryptographic key,\nand so, in this context, it is essential to demonstrate security in\nthe presence of such a conspiracy.  \nA result due to Colbeck and Renner (2011, 2016), building on work of\nBranciard et al. (2008), shows that this cannot be done if PI\nis satisfied. This result has significance both at the operational and\nthe fundamental level. It can be applied at the fundamental level to\nconclude that any theory with sharper probabilities than the quantum\npredictions must violate PI. In addition, even if some deterministic\ntheory (such as the de Broglie-Bohm theory) applies at the fundamental\nlevel, the Colbeck-Renner theorem can be applied at the operational\nlevel, where the probabilities involved may indicate limitations on\naccessible information about the physical state. A violation of PI at\nthe operational level would permit signalling. Thus, the theorem shows\nthat, as long as the no-signalling condition is satisfied, a would-be\neavesdropper attempting to subvert the privacy of a key distribution\nscheme by intercepting the particle pairs and substituting ones that\nwill yield results that she has some information about, cannot do so\nwithout disrupting the correlations between the particle pairs. See\nLeegwater (2016) for a clear exposition of the theorem. \nBell inequalities follow from a number of assumptions that have\nintuitive plausibility and which, arguably, are rooted in the sort of\nworld-view that results from reflection on classical physics and\nrelativity theory. If one accepts that the experimental evidence gives\nus strong reason to believe that Bell inequality-violating\ncorrelations are features of physical reality, then one or more of\nthese assumptions must be given up. Some of these assumptions are of\nthe sort that have traditionally been regarded as metaphysical\nassumptions. The fact that a conjunction of theses of the sort usually\nregarded as metaphysical has consequences that can be subjected to\nexperimental test has led Shimony to speak\nof the enterprise of experimental testing for violations of Bell\ninequalities as “experimental metaphysics” (Shimony 1984a, 35; 1993, 115). As may be\nexpected, the conclusions of experimental metaphysics are not\nunambiguous. Some prima facie plausible options are excluded,\nleaving a number of options open. In this section these are briefly\noutlined, with no attempt made to adjudicate between them. \nSuppose that one accepts that the experimental evidence indicates that\nBell-inequality violating correlations are a real feature of the\nworld, even when the experiments are conducted at spacelike\nseparation, on the most stringent of conditions that advocates of a\ncollapse locality loophole might wish to impose (see section 3.2.3).\nAcceptance of the reality of such correlations requires rejection of\nthe conjunction of any set of assumptions sufficient to entail a Bell\ninequality. The analysis of the assumptions of the proof outlined in\nsection 3 affords a taxonomy of positions that one might adopt in\nlight of their experimental violation, as at least one of the\nassumptions must be rejected. The distinctive assumption is the\nPrinciple of Local Causality, which, when applied to the setup of\nBell-type experiments, is embodied in the factorizability condition.\nThis condition can be maintained only if one of the supplementary\nassumptions is rejected. We begin with options rejecting supplementary\nassumptions. \nAs we are considering implications of accepting Bell\ninequality violations as a feature of reality, even when the\nexperiments are performed at spacelike separation, we set aside the\ncollapse locality loophole. This leaves us with two options. \nReject unique outcomes. This is the option taken by\nEverettian, or Many-Worlds theories, and related interpretations of\nquantum mechanics. The question arises whether locality can be\nmaintained on this option. Posing the question of whether Bell’s\ntheorem has implications for locality on Everettian or relative-state\ninterpretations requires that one first consider how to formulate\nlocality conditions in such a context, as the conditions formulated in\nsection 3.1, presuppose unique experimental outcomes. See Vaidman\n(1994), Bacciagaluppi (2002), Chapter 8 of Wallace (2012), Tipler\n(2014), Vaidman (2016), Brown and Timpson (2016), and Myrvold (2016)\nfor discussions of locality in Everettian interpretations, and Smerlak\nand Rovelli (2007) for a discussion in the context of relational\ninterpretations. \nReject the measurement independence assumption. There are\nessentially two ways to do this. One is to suppose a common cause in\nthe past that determines both experimental settings and experimental\noutcomes, as in the fanciful conspiracy story concocted by Shimony,\nHolt, and Clauser (1976). This sort of account has been called\nsuperdeterminism. Another avenue was suggested by Costa de\nBeauregard (1977), in a comment on the interchange between Bell and\nShimony, Holt, and Clauser. Costa de Beauregard objected that the\ndiscussants were disregarding the possibility of retrocausality, a\npossibility that he had advanced earlier (1976). If causal influence\nfrom future to past is admitted, then, even if the settings are\nregarded as free variables, they could influence the state of the\nsystem at the moment of preparation, contravening the assumption that\nthe preparation probability distribution be independent of\nexperimental settings. \nSuperdeterminism has gained some advocates, most notably Gerard\n’t Hooft, who makes it a feature of his Cellular Automaton\nInterpretation of quantum mechanics (’t Hooft 2016). It should\nbe noted that superdeterminism is a condition that is considerably\nstronger than mere determinism. It is uncontroversial that the sort of\ndevices that Bell speaks of, which have the effect of making the\nchoice setting of an experimental parameter depend on initial\nconditions in a way that is highly sensitive to small perturbations\nand which would ordinarily be accepted as effectively randomizing the\nchoice, do exist. Superdeterminism requires these settings to be\nnonetheless determined by the conditions at some past time, in such a\nway that the settings are distributed in just the right way to produce\nthe quantum statistics in Bell-type experiments, despite the\nunderlying physics being local, no matter what randomization\nmethod is employed. For example, in the experiment of Shalm\net al. (2015), the measurement decisions were determined by\napplying an XOR (exclusive or) operation to three bits from three\nindependent sources, one of which was a pseudorandom sequence\nconstructed from digits of \\(\\pi\\) and binary strings derived\nfrom various popular culture sources, including episodes of the movies\nMonty Python and the Holy Grail, the Back to the\nFuture trilogy, and episodes of Doctor Who, Saved by\nthe Bell, and Star Trek. A hypothetical cause that\nachieved the observed statistics via correlation between states of the\nphotons studied and the choice of measurements would have had to\nprecisely orchestrate the creative processes leading up to the\ndigitized versions of these cultural artifacts in such a way that,\nwhen processed in conjunction with the outputs of the random number\ngenerators, produced just the right sequence of experimental\nchoices. \nNo experiment can completely rule out the logical possibility of\nconspiracies of this sort. One can, however, conduct experiments that\nguarantee independence of the settings and the state of the systems at\nthe source on the basis of plausible physical assumptions. The\nexperiment of Scheidl et al. (2010) was the first to employ\nrandom-number generators operating at spacelike separation from the\ngeneration of the particle pairs at the source. Following a suggestion\nof Clauser, other tests have employed measurements on photons from\nMilky Way stars (Handsteiner et al. 2017) and from distant\nquasars (Rauch et al. 2018). In addition, the “Big Bell\nTest” ran experiments utilizing inputs provided by approximately\n100,000 volunteers (Abellán et al. 2018). \nIf the auxiliary assumptions are accepted, then the lesson to be\ndrawn from the experimental violation of Bell inequalities is that the\ncondition that Bell called the Principle of Local Causality\nmust be rejected. As we have seen, this condition is a conjunction of\ntwo conditions: a causal locality condition (PLC-1, above), and the\nPrinciple of the Common Cause. The causal locality condition itself\ncan be regarded as stemming from the assumption that causes temporally\nprecede their effects and Lorentz invariance of the relation of\ntemporal precedence. If the relation of temporal precedence is to be Lorentz\ninvariant, then either it is the trivial relation that holds between\nany two spacetime points, or else the past of a spacetime point is its\npast light cone (Stein 1991, 2009). \nIn this context, it is useful to recall that the factorizability\ncondition can be thought of as a conjunction of outcome independence\n(OI) and parameter independence (PI). PI is a consequence of causal\nlocality (PLC-1), applied to the set-up of the Bell experiments,\nwhereas OI is a consequence of causal locality and the common cause\nprinciple. This leaves us with a dilemma. A rejection of\nfactorizability involves a rejection of PI or OI. A rejection of PI\ninvolves a rejection of causal locality. If causal locality is to be\nmaintained, then OI, and hence the Common Cause Principle, must be\nrejected. \nAny deterministic theory must satisfy OI, and hence, a deterministic\ntheory that rejects factorizability must reject causal locality.  \nThese considerations yield a taxonomy of options for accepting the\nsupplementary assumptions while also accepting Bell-inequality\nviolating correlations.  \nReject PLC-1. One option is to reject the assumption PLC-1 of\ncausal locality, and accept that there are causal relations between\nevents that are outside of each others’ light-cones. There are\ntwo ways to do this. \nReject the Principle of Common Cause. A stochastic theory,\nsuch as a dynamical collapse theory, that reproduces quantum\nprobabilities for Bell experiments, will involve correlated events at\nspacelike separation. It need not, however, involve any events in the\ncommon past of these events that screen off the correlations; these\ncorrelations will be built-in to the laws of the theory yielding\nprobabilities of events. Whether one is willing to extend talk of\ncause-effect relations to refer to the relation between such events is\nmerely a matter of terminology; it should be noted, however, that\nthere is nothing of the usual asymmetry between cause and effect in\nthe relation between these events. To accept this relation as a new\nsort of symmetric cause-effect relation removes any reason there is to\nthink that cause-effect relations between spacelike separated events\nare incompatible with relativistic spacetime structure. \nA more common view is that the lesson of Bell’s theorem is that\nthere may be correlations that are not explicable in terms of\ncause-effect relations. This involves rejection of the Common Cause\nPrinciple (see, e.g., van Fraassen 1982, Fine 1989,\nButterfield 1992, Elby 1992). Others have maintained that, though the\nCommon Cause Principle as formulated by Reichenbach is to be rejected,\nthe principle has been formulated too narrowly, and  needs to\nbe reformulated in light of quantum phenomena. Some have suggested\nthat despite not satisfying Reichenbach’s principle,\ncorrelations violating Bell inequalities are due to a common cause in\ntheir past, namely, the process that created entanglement (Unruh 2002,\n136). Hofer-Szabo, Rédei, and Szabo (1999, 2002) have suggested\na modification of the common cause principle, as have Leifer and\nSpekkens (Leifer 2006, Leifer and Spekkens 2011). See Cavalcanti and\nLal (2014) for discussion and a critique of these proposals. \nDoes Bell’s theorem show that quantum theory is incompatible\nwith relativity?  \nThe answer, of course, depends on what one takes relativity theory to\nrequire. It can be shown (Eberhard 1978, Ghirardi, Rimini & Weber\n1980, Page 1982) that, in the absence of nonlocal interaction terms in\nthe Hamiltonian, quantum correlations cannot be exploited to send\nsignals superluminally. There has been a tendency in some of the\nliterature to take this by itself to indicate compatibility with\nrelativity. That this is insufficient can be seen from the fact that\nthere can be theories, such as the de Broglie-Bohm theory, that\nrequire nonrelativistic spacetime structure for the formulation of\ntheir dynamical laws, while not permitting signalling (at least, as\nlong as the usual distribution postulate for particle positions is\nsatisfied).  \nTake a relativistic spacetime structure to be a structure\nthat includes a relation of temporal precedence on which the past of a\nspacetime point is the set of points on or within its past lightcone,\nand its future, the set of points on or within its future lightcone,\nand other points are temporally unrelated to it, neither past nor\nfuture. One can ask whether a given account of the goings-on in the\nworld is compatible with a relativistic spacetime structure. If a\ntheory requires events outside of its lightcone to be partitioned into\nthose that are past, future, and simultaneous with a point, as does\nthe de-Broglie Bohm theory, it is not compatible with a relativistic\nspacetime structure. Compatibility of a theory with relativistic\nspacetime structure, in this sense, is a distinct issue from Lorentz\ncovariance of the equations expressing the theory’s dynamical\nlaws. One can construct theories on which nonrelativistic\nspacetime structure is introduced dynamically, as is done by\nDürr, Goldstein, Münch-Berndl and Zanghì (1999), who\nformulate a Bohmian theory against a background of Minkowski spacetime\nby introducing an auxiliary field that picks out a distinguished\nfoliation that is then used to formulate the dynamics of the theory\n(see Dürr et al. 2014 for discussion). Berndl,\nDürr, Goldstein, and Zanghì (1996) consider the class of\ntrajectory theories—that is, theories that, like the de\nBroglie-Bohm theory, assign definite trajectories to\nparticles—and prove that such theories could not satisfy the\nBorn-rule distribution postulate along arbitrary spacelike\nhyperplanes. The argument generalizes to any theories, such as modal\ninterpretations, that assign definite values to variables other\nthan position (Dickson and Clifton 1998, Arntzenius 1998, Myrvold\n2002). Theories of this sort, therefore, must invoke a distinguished\nfoliation. \nA deterministic theory must satisfy the condition of outcome\nindependence, and hence if one accepts \\((a)\\) that a violation\nof PI, that is, a situation in which the choice of an experiment on\none side changes the probability of an outcome on the other, is an\ninstance of causation, and \\((b)\\) that a cause must temporally\nprecede its effect, and then it follows that a deterministic theory\nthat satisfies the supplementary conditions and reproduces the quantum\ncorrelations is incompatible with relativistic spacetime structure.\nSuch a theory may, however, be Lorentz invariant at the phenomenal\nlevel, if (as in the de Broglie-Bohm theory) the distinguished\nfoliation is unobservable. \nA stochastic theory, such as a dynamical collapse theory, must involve\ncorrelations between spacelike separated events. A relation like that,\nhowever, is symmetric, and does not require that one of the events be\nin the past of the other. There is no prima facie need for\nnonrelativistic spacetime structure. Indeed, it is possible to\nformulate a fully relativistic dynamical collapse theory. A\nrelativistic generalization of the Ghirardi-Rimini-Weber (GRW) theory\nwas constructed by Dove (1996) and, independently, by Tumulka (2006).\nRelativistic versions of the Continuous Spontaneous Localization (CSL)\ntheory have been constructed by Bedingham (2011) and Pearle (2015).\nFor discussions of ontology for such theories, required to extract\nfrom them a sensible account of a world that includes macroscopic\nobjects, see Pearle (1997), Bedingham et al. (2014), and\nMyrvold (2019).  \nShimony, in several of his writings (Shimony 1978, 1983, 1984a,b, 1986,\n1988, 1989, 1990, 1991) spoke of “peaceful coexistence”\nbetween special relativity and quantum theory. The meaning of this\nvaried. In its initial formulation (1978), “peaceful\ncoexistence” had to do with regarding experimental outcomes as\ntransitions from potentiality to actuality, something that, Shimony\nsays, requires further investigation to be understood. In later\nwritings, peaceful coexistence is “suggested” by the fact\nthat quantum correlations cannot be exploited for superluminal\nsignalling (Shimony 1983), though still associated with the notion\nthat quantum mechanics motivates a change in our conception of an\nevent (Shimony 1984a), and involves the requirement of a coherent\nmeshing of events as described with respect to different reference\nframes (1986). In other works peaceful coexistence seems to be simply\nidentified with the impossibility of exploiting quantum correlations\nfor signalling (Shimony 1984b, 1990, 1991). Convinced by Bell (1990)\nthat anthropocentric considerations such as manipulability have no\nplace in considerations of fundamental physics, Shimony became\ndissatisfied with this avenue of approach to reconciling relativity\nand quantum theory (Shimony 2009, 489).  \nBell’s own attitude towards the question of whether Bell’s\ntheorem indicates a fundamental incompatibility between quantum theory\nand relativity seems to have varied with time. At the end of the 1964\narticle, he wrote  \nAt this point he is claiming incompatibility with relativity only for\ndeterministic hidden-variables theories. Later, however, he spoke of\n“the apparently essential conflict between any sharp formulation\n[of quantum theory] and fundamental relativity. That is to say, we\nhave an apparent incompatibility, at the deepest level, between the\ntwo fundamental pillars of contemporary theory…” (Bell\n1987b and 2004, 172; these are remarks from a meeting held in 1984).\nNote that this is hedged, and he speaks of “apparently\nessential” conflict only. In the same year, he wrote, “I\nam unable to prove, or even formulate clearly, the proposition that a\nsharp formulation of quantum field theory, such as that set out here,\nmust disrespect serious Lorentz invariance. But it seems to me that\nthis is probably so” (1984, 7; 1987b and 2004, 180). \nA shift in attitude was occasioned by the publication of the GRW\ndynamical collapse theory (Ghirardi, Rimini, and Weber, 1986). In a\ncommentary on this theory, Bell wrote, \nIn a lecture delivered in Trieste in the last year of his life, Bell\ndiscussed the prospects for a genuinely relativistic version of a\ndynamical collapse theory, and concluded that the difficulties\nencountered by Ghirardi, Grassi, and Pearle in producing a genuinely\nrelativistic version of the Continuous Spontaneous Localization theory\n(CSL), a theory that would be “Lorentz invariant, not just for\nall practical purposes but deeply, in the sense of Einstein,\neliminating entirely any privileged reference system from the\ntheory” (2007, 2931), were “Second-Class\nDifficulties,” technical difficulties, and not deep conceptual\nones. This seems to have been borne out by the construction of the\nfully relativistic collapse theories already mentioned.","contact.mail":"m.genovese@inrim.it","contact.domain":"inrim.it"}]
