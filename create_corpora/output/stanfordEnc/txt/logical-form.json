[{"date.published":"1999-10-19","date.changed":"2015-11-30","url":"https://plato.stanford.edu/entries/logical-form/","author1":"Paul Pietroski","author1.info":"http://www.wam.umd.edu/~pietro/","entry":"logical-form","body.text":"\n\nLogical FormFirst published Tue Oct 19, 1999; substantive revision Mon Nov 30, 2015\n\n\n\nSome inferences are impeccable. Examples like (1–3)\nillustrate reasoning that cannot lead from true premises to false\nconclusions. \n\n\n  (1)\n        John danced if Mary sang, and Mary sang; so\nJohn\ndanced.\n      (2)\n        Every politician is deceitful, and every\nsenator is a\npolitician; so every senator is deceitful.\n      (3)\n        The detective is in the garden; so someone is\nin the\ngarden.\n      \n\nIn such cases, a thinker takes no epistemic risk by endorsing the\nconditional claim that the conclusion is true if the\npremises are true.  The conclusion follows from the premises, without\nany further assumptions that might turn out to be false. Any risk of\nerror lies entirely with the premises, as opposed to the reasoning. By\ncontrast, examples like (4–6) illustrate reasoning that involves\nat least some risk of going wrong—from correct premises to a\nmistaken conclusion.\n\n  (4)\n        John danced if Mary sang, and John danced; so\nMary\nsang.\n      (5)\n        Every feathered biped is a bird, and Tweety is\na\nfeathered biped; so Tweety can fly.\n      (6)\n        Every human born before 1879 died; so every\nhuman will\ndie.\n      \n\n Inference (4) is not secure. John might dance whenever Mary sings,\nbut also sometimes when Mary doesn't sing. Similarly, with regard to\n(5), Tweety might turn out to be a bird that cannot fly. Even (6)\nfalls short of the demonstrative character exhibited by (1–3).\nWhile laws of nature may preclude immortality, the conclusion of (6)\ngoes beyond its premise, even if it is foolish to resist the\ninference.\n\n\nAppeals to logical form arose in the context of attempts to say more\nabout this intuitive distinction between impeccable inferences, which\ninvite metaphors of security, and inferences that involve some risk of\nslipping from truth to falsity. The idea is that some inferences, like\n(1-3), are structured in a way that confines any risk of\nerror to the premises. The motivations for developing this idea were\nboth practical and theoretical. Experience teaches us that an\ninference can initially seem more secure than it is; and if we knew\nwhich forms of inference are risk-free, that might help us\navoid errors. As we'll see, claims about inference are also intimately\nconnected with claims about the nature of thought and its relation to\nlanguage.\n\n\nMany philosophers have been especially interested in the\npossibility that grammar masks the underlying structure of\nthought, perhaps in ways that invite mistaken views about how ordinary\nlanguage is related to cognition and the world we talk about.  For\nexample, similarities across sentences like ‘Odysseus\narrived’, ‘Nobody arrived’, and ‘The king\narrived’ initially suggest that the corresponding thoughts\nexhibit a common subject-predicate form. But even if\n‘Odysseus’ indicates an entity that can be the subject of\na thought that is true if and only if the entity in question arrived,\nother considerations suggest that ‘Nobody’ and ‘The\nking’ do not indicate subjects of thoughts in this sense. This\nraises further questions about inference—e.g., why ‘The\nking arrived’ implies an arrival, while ‘Nobody\narrived’ does not—and more general questions about how\nlogic is related to grammar. Do thoughts and sentences exhibit\ndifferent kinds of structure? Do sentences exhibit grammatical\nstructures that are not obvious? And if the logical structure of a\nthought can diverge from the grammatical structure of a sentence that\nis used to express the thought, how should we construe proposals about\nthe logical forms of inferences like (1-6)? Are such proposals\nnormative claims about how we ought to think/talk, or empirical\nhypotheses about aspects of psychological/linguistic reality?\n\nProposed answers to these questions are usually interwoven with\nclaims about why various inferences seem compelling. So it would be\nnice to know which inferences really are secure, and in virtue of what\nthese inferences are special.  The most common suggestion has\nbeen that certain inferences are secure by virtue of their logical\nform. Though unsurprisingly, conceptions of form have evolved along\nwith conceptions of logic and language.\n\nOne ancient idea is that impeccable inferences exhibit patterns\nthat can be characterized schematically by abstracting away from the\nspecific contents of particular premises and conclusions, thereby\nrevealing a general form common to many other impeccable inferences.\nSuch forms, along with the inferences that exemplify them, are said to\nbe valid.  Given a valid inference, there is a sense in which the premises\ncontain the conclusion, which is correspondingly extractable from the\npremises. With regard to (1) and (7), it seems especially clear that the conclusion is part of the first\npremise, and that the second premise is another part of the first. We\ncan express this point by saying that these inferences are instances\nof the following form: B if A, and\nA; so B. The Stoics discussed\nseveral patterns of this kind, using ordinal numbers (instead of\nletters) to capture abstract forms like the ones shown\nbelow.  If the first then the\nsecond, and the first; so the\nsecond. \nIf the first then the second, but\nnot the second; so not the\nfirst. \nEither the first or the second, but\nnot the second; so the first. Not both the first and the\nsecond, but the first; so not the\nsecond. These schematic formulations require variables. And let us\nintroduce ‘proposition’ as a term of art for whatever the\nvariables above, indicated in bold, range over. Propositions are\npotential premises/conclusions. They can be endorsed or rejected, and\nthey exhibit containment relations of some kind. So presumably,\npropositions are abstract things that can be evaluated for truth or\nfalsity. This leaves it open what propositions are: sentences,\nstatements, states of affairs, or whatever. But let's assume that\ndeclarative sentences can be used to express propositions. (For\ndiscussion, see Cartwright (1962) and the essay on\n structured propositions.)  A significant complication is that in ordinary conversation, the\ncontext matters with regard to which proposition is expressed with a\ngiven sentence. For example, ‘Pat is asleep’ can be used\nat one time to express a true premise, and at another time to express\na false premise. A given speaker might use ‘I am tired’ to\nexpress a false proposition, while another speaker uses the same\nsentence at the same time to express a true proposition. What counts\nas being tired can also vary across conversations. Context\nsensitivity, of various kinds, is ubiquitous in ordinary discourse.\nMoreover, even given a context, a sentence like ‘He is\nbald’ may not express a unique proposition. (There may be no\nreferent for the pronoun; and even if there is, the\n vagueness of ‘bald’ may yield a\nrange of candidate propositions, with no fact of the matter as to\nwhich one is the proposition expressed.) Nonetheless, we can\noften use sentences like ‘Every circle is an ellipse’ and\n‘Thirteen is a prime number’ to express premises of valid\narguments. To be sure, ordinary conversation differs from theoretical\ndiscourse in mathematics. But the distinction between impeccable and\nrisky inferences is not limited to special contexts in which we try to\nthink especially clearly about especially abstract matters. So when\nfocusing on the phenomenon of valid inference, we can try to simplify\nthe initial discussion by abstracting away from the context\nsensitivity of language use.   \nAnother complication is that in speaking of an inference, one might be\ntalking about (i) a process in which a thinker draws a\nconclusion from some premises, or (ii) some propositions, one\nof which is designated as an alleged consequence of the others; see,\ne.g., Harman (1973). But we can describe a risky thought process as\none in which a thinker who accepts certain propositions—perhaps\ntentatively or hypothetically—comes to accept, on that basis, a\nproposition that does not follow from the initial premises. And it\nwill be simpler to focus on premises/conclusions, as opposed to\nepisodes of reasoning. \nWith regard to (1), the inference seems secure in part\nbecause its first premise has the form\n‘B if A’.  If the first premise didn't have this form, the inference wouldn't\nbe an instance of ‘B if A, and\nA; so B’. It isn't obvious\nthat all impeccable inferences are instances of a more\ngeneral valid form, much less inferences whose impeccability is due to\nthe forms of the relevant propositions. But this thought has served as\nan ideal for the study of valid inference, at least since Aristotle's\ntreatment of examples like (2).  Again, the first premise seems to have several parts, each of\nwhich is a part of the second premise or the conclusion. (In English,\nthe indefinite article in ‘Every senator is a\npolitician’ cannot be omitted; likewise for ‘Every\npolitician is a liar’. But at least for now, let's assume that\nin examples like these, ‘a’ does not itself indicate a\npropositional constituent.) Aristotle, predating the Stoics, noted\nthat conditional claims like the following are sure to be true: if\n(the property of) being a politician belongs to every senator, and\nbeing deceitful belongs to every politician, then being deceitful\nbelongs to every senator. Correspondingly, the inference pattern\nbelow is valid. And inference (2) seems to be valid because its parts exhibit this\npattern. Aristotle discussed many such forms of inference, called\nsyllogisms, involving propositions that can be expressed with\nquantificational words like ‘every’ and\n‘some’. For example, the syllogistic patterns below are\nalso valid. Every S is P, and some S is\nD; so some P is D. Some S is P, and every P\nis D; so some S is D. Some S is not P, every D is P;\nso some S is not D. We can rewrite the last two, so that each of the valid syllogisms\nabove is represented as having a first premise of the form\n‘Every S is P’. Every S is P, and some D is\nS; so some D is P. Every S is P, and some D is not\nP; so some D is not S. But however the inferences are represented, the important point is\nthat the variables—represented here in italics—range over\ncertain parts of propositions. Intuitively, common nouns like\n‘politician’ and adjectives like ‘deceitful’\nare general terms, since they can apply to more than one\nindividual. And many propositions apparently contain correspondingly\ngeneral elements. For example, the proposition that every senator is\ndeceitful contains two such elements, both relevant to the validity of\ninferences involving this proposition. Propositions thus seem to have structure that bears on the validity\nof inferences, even ignoring premises/conclusions with propositional\nparts. That is, even simple propositions have logical form. And as\nAristotle noted, pairs of such propositions can be related in\ninteresting ways. If every S is P, then some\nS is P. (For these purposes, assume there is at\nleast one S.) If no S is P, then some\nS is not P. It is certain that either every\nS is P or some S is not P; and\nwhichever of these propositions is true, the other is\nfalse. Similarly, the following propositions cannot both be true:\nevery S is P; and no S is P. But\nit isn't certain that either every S is P, or no\nS is P. Perhaps some S is P, and\nsome S is not P. This network of logical relations\nstrongly suggests that the propositions in question contain a\nquantificational element and two general elements—and in some\ncases, an element of negation. This raises the question of whether\nother propositions have a similar structure. \nConsider the proposition that Vega is a star, which can figure in\ninferences like (8). \nAristotle's logic focused on quantificational propositions; and as we\nshall see, this was prescient. But on his view, propositions like the\nconclusion of (8) still exemplify a subject-predicate structure that\nis shared by at least many of the sentences we used to express\npropositions. And one can easily formulate the schema ‘every\nS is P, and n is S; so n\nis P’, where the new lower-case variable is intended to\nrange over proposition-parts of the sort indicated by names. (On some\nviews, discussed below, a name like ‘Vega’ is a complex\nquantificational expression; though unsurprisingly, such views are\ntendentious.)  Typically, a declarative sentence can be divided into a subject\nand a predicate: ‘Every star / is purple’, ‘Vega /\nis a star’, ‘Some politician / lied’, ‘The\nbrightest planet / is visible tonight’, etc. Until quite\nrecently, it was widely held that this grammatical division reflects a\ncorresponding kind of logical structure: the subject of a proposition\n(i.e., what the proposition is about) is a target for predication. On\nthis view, both ‘Every star’ and ‘Vega’\nindicate subjects of propositions in (8), while ‘is’\nintroduces predicates. Aristotle would have said that in the premises\nof (8), being purple is predicated of every star, and being a star is\npredicated of Vega. Later theorists emphasized the contrast between\ngeneral terms like ‘star’ and singular terms like\n‘Vega’, while also distinguishing terms from\nsyncategorematic expressions (e.g., ‘every’ and\n‘is’) that can combine with terms to form complex subjects\nand predicates, including ‘will lie’, ‘can\nlie’, and ‘may have lied’. But despite the\ncomplications, it seemed clear that many propositions have the\nfollowing canonical form: Subject-copula-Predicate; where a copula\nlinks a subject, which may consist of a quantifier and a general term,\nto a general term. Sentences like ‘Every star twinkles’\ncan be paraphrased with sentences like ‘Every star is a thing\nthat does some twinkling’. This invites the suggestion that\n‘twinkles’ is somehow an abbreviation for ‘is a\nthing that does some twinkling’, perhaps in the way that\n‘bachelor’ is arguably short for ‘unmarried\nmarriageable man’.  The proposition that not only Vega twinkles, which seems to contain\nthe proposition that Vega twinkles, presumably includes elements that\nare indicated with ‘only’ and ‘not’. Such\nexamples invite the hypothesis that all propositions are composed of\nterms along with a relatively small number of syncategorematic\nelements, and that complex propositions can be reduced to canonical\npropositions that are governed by Aristotelian logic. This is not to\nsay that all propositions were, or could be, successfully analyzed in\nthis manner. But via this strategy, medieval logicians were able to\ndescribe many impeccable infererences as instances of valid forms. And\nthis informed their discussions of how logic is related to\ngrammar. Many viewed their project as an attempt to uncover principles of a\nmental language common to all thinkers. Aristotle had said, similarly,\nthat spoken sounds symbolize “affections of the soul.”\nFrom this perspective, one expects a few differences between\npropositions and overt sentences. If ‘Every star twinkles’\nexpresses a proposition that contains a copula, then spoken languages\nmask certain aspects of logical structure. Ockham also held that a\nmental language would have no need for Latin's declensions, and that\nlogicians could ignore such aspects of spoken language. The ancient\nGreeks were aware of sophisms like the following: that dog is a\nfather, and that dog is yours; so that dog is your father. This bad\ninference cannot share its form with the superficially parallel but\nimpeccable variant: that dog is a mutt, and that mutt is yours; so\nthat dog is your mutt. (See Plato, Euthydemus 298 d-e.) So the\nsuperficial features of sentences are not infallible guides to the\nlogical forms of propositions. Still, the divergence was held to be\nrelatively minor. Spoken sentences have structure; they are composed,\nin systematic ways, of words. And the assumption was that sentences\nreflect the major aspects of propositional form, including a\nsubject-predicate division. So while there is a distinction between\nthe study of valid inference and the study of sentences used in spoken\nlanguage, the connection between logic and grammar was thought to run\ndeep. This suggested that the logical form of a proposition just is\nthe grammatical form of some (perhaps mental) sentence. Towards the end of the eighteenth century, Kant could say (without\nmuch exaggeration) that logic had followed a single path since its\ninception, and that “since Aristotle it has not had to retrace a\nsingle step.” He also said that syllogistic logic was “to\nall appearance complete and perfect.” But this was exuberance.\nIndeed, some of the real successes highlighted known problems. Some valid schemata are reducible to others, in that any inference\nof the reducible form can be revealed as valid (with a little work)\ngiven other schemata. Consider (9). \nAssume that ‘Al did not run’ negates ‘Al ran’,\nwhile ‘Bob did not swim’ negates ‘Bob\nswam’. Then (9) is an instance of the following valid form: if\nA then either not-A or\nnot-B, and A; so\nnot-B. But we can treat this as a derived form, by\nshowing that any instance of this form is valid given two (intuitively\nmore basic) Stoic inference forms: if the first then\nthe second, and the first, so\nthe second; either not the first or\nnot the second, and the first; so\nnot the second. For suppose we are given the\nfollowing premises: A; and if A,\nthen either not-A or not-B. We can\nsafely infer that either not-A or\nnot-B; and since we were given that\nA, we can safely infer that not-B. Similarly, the\nsyllogistic schema (10) can be treated as a derived form. \nIf some S is not P, and every D is\nP, then it isn't true that every S is\nD. For if every S is D, and every\nD is P, then every S is P. But\nif some S is not P, then as we saw above,\nnot every S is P. So given the premises of (10),\nadding ‘every S is D’ would lead to\ncontradiction: every S is P, and not every\nS is P.  So the premises imply the negation of\n‘every S is D’. This reasoning shows how\n(10) can be reduced to inferential patterns that seem more\nbasic—raising the question of how much reduction is possible.\nEuclid's geometry had provided a model for how to present a body of\nknowledge as a network of propositions that follow from a few basic\naxioms. Aristotle himself indicated how to reduce all the valid\nsyllogistic schemata to four basic patterns, given a few principles\nthat govern how the basic patterns can be used to derive others; see\nParsons (2014) for discussion. And further reduction is possible given\ninsights from the medieval period. Consider the following pair of valid inferences: Fido is a brown\ndog, so Fido is a dog; Fido is not a dog, so Fido is not a brown dog.\nAs illustrated with the first example, replacing a predicate (or\ngeneral term) like ‘brown dog’ with a less\nrestrictive predicate like ‘dog’ is often valid. But\nsometimes—paradigmatically, in cases involving\nnegation—replacing a predicate like ‘dog’ with a\nmore restrictive predicate like ‘brown dog’ is\nvalid. Plausibly, the first pattern reflects the default direction of\nvalid replacement: removing a restriction preserves truth, except in\nspecial cases like those involving negation. Suppose we take it as\ngiven that poodles are dogs of a particular sort, and hence that every\npoodle is a dog. Then replacing‘poodle’ with ‘dog'\nin ‘Fido is P’ is valid, regardless of what\n‘Fido’ names. This can be viewed as a special case of\n‘n is P, and every P is D;\nso n is D’. But the validity of this inference\nform can also be viewed as symptom of a basic principle that came be\ncalled dictum de omni: whatever is true of every P\nis true of any P. Or as Aristotle might have put it, if the\nproperty of being a dog belongs to every poodle, then it belongs to\nany poodle. In which case, Fido is a dog if Fido is a poodle. And\nsince the property of being a dog surely belongs to every brown dog,\nany brown dog is a dog. The flip side of this point is that negation\ninverts the default direction of inference. Anything that isn't a dog\nisn't a brown dog; and similarly, if Fido isn't a dog, Fido isn't a\npoodle. So in special cases, adding a restriction to a general term\nlike ‘dog’ can preserve truth. From this perspective, the Aristotelian quantifier\n‘Some’ is a default-style quantifier that validates\nremoving restrictions. If some brown dog is a clever mutt, it\nfollows that some dog is a clever mutt, and hence that some dog is a\nmutt. By contrast, ‘No’ is an inverted-style quantifier\nthat validates adding restrictions.  If no dog is a mutt, it\nfollows that no dog is a clever mutt, and hence that no brown dog is a\nclever mutt. The corresponding principle, dictum de nullo,\nencodes this pattern: whatever is true of no P is not true of\nany P; so if the property of being a mutt belongs to no dog,\nit belongs to no poodle. (And as Aristotle noted, instances of\n‘No S is P’ can be analyzed as the\npropositional negations of corresponding instances of ‘Some\nS isn't P’. Interestingly, ‘Every’ is like ‘No’ in one\nrespect, and like ‘Some’ in another respect. If every dog\nis clever, it follows that every brown dog is clever; but if every dog\nis a clever mutt, it follows that every dog is a mutt. So when the\nuniversal quantifier combines with a general term S to form a\nsubject, S is governed by the inverted rule of\nreplacement.  But when a universally quantified subject combines with\na second general term to form a proposition, this second term is\ngoverned by the default rule of replacement. Given that\n‘Every’ has this mixed logical character, the valid\nsyllogisms can be derived from two basic patterns (noted above), both\nof which reflect dictum de omni: whatever is true of every\nP is true of any P.\n Every S is P, and every P is\nD; so every S is D. \nEvery S is P, and some D is S; so\nsome D is P. The first principle reflects the sense in which universal\nquantification is transitive. The second principle captures the idea\nthat a universal premise can licence replacement of\n‘S’ with ‘P’ in a premise\nabout a specific individual. In this sense, classical logic exhibits a\nstriking unity and simplicity, at least with regard to inferences\ninvolving the Aristotelian quantifiers and predication; see Sommers\n(1984) and Ludlow (2005), drawing on Sanchez (1991), for further\ndiscussion. Alas, matters become more complicated once we consider\nrelations. Sentences like ‘Juliet kissed Romeo’ do not seem to\nhave Subject-copula-Predicate form. One might suggest ‘Juliet\nwas a kisser of Romeo’ as a paraphrase. But ‘kisser of\nRomeo’ differs, in ways that matter to inference, from general\nterms like ‘politician’. If Juliet (or anyone) was a\nkisser of Romeo, it follows that someone was kissed; whereas if Juliet\nwas a politician, there is no corresponding logical consequence to the\neffect that someone was __-ed. Put another way, the proposition that\nJuliet kissed someone exhibits interesting logical structure, even if\nwe can express this proposition via the sentence ‘Juliet was a\nkisser of someone’. A quantifier can be part of a complex\npredicate. But classical logic did not capture the validity of\ninferences involving predicates that have quantificational\nconstituents.  Consider (11).\n If ‘respects every doctor’ and ‘respects some\nliar’ indicate nonrelational proposition-parts, much like\n‘is sick’ or ‘is happy’, then inference (11)\nhas the following form ‘Some P is S, and some\nD is L; so some P is H’. But\nthis schema, which fails to reflect the quantificational structure\nwithin the predicates is not valid. Its instances include bad\ninferences like the following: some patient is sick, and some doctor\nis a liar; so some patient is happy. This dramatizes the point that\n‘respects every doctor’ and ‘respects some\nliar’ are—unlike ‘is sick’ and ‘is\ntall’—logically related in a way that matters given the\nmiddle premise of (11). \nOne can adopt the view that many propositions have relational parts,\nintroducing a variable ‘R’ intended to range over\nrelations; see the entries on\n medieval relations, and\n medieval terms. One can also formulate\nthe following schema: some P R every D, and\nsome D is L; so some P R some\nL.  But the problem remains. Quantifiers can appear in\ncomplex predicates that figure in valid inferences like (12). \nBut if ‘patient who respects every doctor’ and\n‘patient who saw every lawyer’ are nonrelational, much\nlike ‘old patient’ or ‘young patient’, then\n(12) has the following form: every O is S, and some\nY R every D; so some Y is\nS. And many inferences of this form are invalid. For example:\nevery otter is sick, and some yak respects every doctor; so some yak\nis sick. Again, one can abstract a valid schema that covers (12),\nletting parentheses indicate a relative clause that restricts the\nadjacent predicate. But no matter how complex the schema, the relevant predicates can\nexhibit further quantificational structure. (Consider the proposition\nthat every patient who met some doctor who saw no lawyer\nrespects some lawyer who saw no patient who met every\ndoctor.) Moreover, schemata like the one above are poor\ncandidates for basic inference patterns. As medieval logicians knew, propositions expressed with relative\nclauses also pose other difficulties; see the entry on\n medieval syllogism.  If every doctor\nis healthy, it follows that every young doctor is healthy. By itself,\nthis is expected, since a universally quantified subject is governed\nby the non-default (de nullo) inference rule that licenses\nreplacement of ‘doctor’ with the more restrictive\n‘young doctor’. But consider (13) and (14).\n Here, the direction of valid inference is from ‘young\ndoctor’ to ‘doctor’, as if the inference is governed\nby the default (de omni) inferential rule. One can say that\nthe default direction of implication, from more restrictive to less\nrestrictive predicates, has been inverted twice—once by\n‘No’, and once by ‘every’. But one wants a\nsystematic account of propositional structure that explains the net\neffect; see Ludlow (2002) for further discussion. Sommers (1982)\noffers a strategy for recoding and extending classical logic, in part\nby exploiting an idea suggested by Leibniz (and arguably Panini): a\nrelational sentence like ‘Juliet loved Romeo’ somehow\ncombines an active-voice sentence with a passive-voice sentence,\nperhaps along the lines of ‘Juliet loved, and thereby\nRomeo was loved’; cp. section nine\nbelow.  But if impeccability is to be revealed as a matter of form,\nthen one way or another, quantifiers need to characterized in a way\nthat captures their general logical role—and not just their role\nas potential subjects of Aristotelian propsitions. Quantifiers are not\nsimply devices for creating schemata like ‘Every S is\nP’, into which general terms like\n‘politician’ and ‘deceitful’ can be\ninserted. Instances of ‘S’ and\n‘P’ can themselves have quantificational\nstructure and relational constituents.  Frege showed how to resolve these difficulties for classical logic\nin one fell swoop. His system of logic, published in 1879 and still in\nuse (with notational modifications), was arguably the single greatest\ncontribution to the subject. So it is significant that on Frege's\nview, propositions do not have subject-predicate form. His account\nrequired a substantial distinction between logical form and\ngrammatical form as traditionally conceived. It is hard to\noveremphasize the impact of this point on subsequent discussions of\nthought and its relation to language.  Frege's leading idea was that propositions have\n“function-argument” structure. Though for Frege, functions\nare not abstract objects. In particular, while a function maps each\nentity in some domain onto exactly one entity in some range, Frege\n(1891) does not identify functions with sets of ordered pairs. On the\ncontrary, he says that a function “by itself must be called\nincomplete, in need of supplementation, or unsaturated. And in this\nrespect functions differ fundamentally from numbers (p. 133).”\nFor example, we can represent the successor function as follows, with\nthe integers as the relevant domain for the variable ‘x’:\nS(x) = x + 1. This function maps zero onto one, one onto two, and so\non. We can specify a corresponding object—e.g., the set\n{〈x, y〉: y = x + 1}—as the “value-range”\nof the successor function. But according to Frege, any particular\nargument (e.g., the number one) “goes together with the function\nto make up a complete whole” (e.g., the number two); and a\nnumber does not go together with a set in this fashion. Put another\nway, while each number is an object, a mapping from numbers to numbers\nis not an additional object in Frege’s sense. As Frege noted,\nthe word ‘function’ is often used to talk about what he\nwould call the value-range of a function. But he maintained that the\nnotion of an unsaturated function, which may be applied to endlessly\nmany arguments, is “logically prior” to any notion of a\nset with endlessly many arguments that are specified functionally as\nin {〈x, y〉: y = x + 1}; see p.135, note E. \nFunctions need not be unary. For example, arithmetic division can be\nrepresented as a function from ordered pairs of numbers onto\nquotients: Q(x, y) = x/y. Mappings can also be conditional. Consider\nthe function that maps every even integer onto itself, and every odd\ninteger onto its successor: C(x) = x if x is even, and x + 1\notherwise; C(1) = 2, C(2) = 2, C(3) = 4, etc. Frege held that\npropositions have parts that indicate functions, and in particular,\nconditional functions that map arguments onto special values that\nreflect the truth or falsity of propositions/sentences. (As discussed\nbelow, Frege [1892] also distinguished these “truth\nvalues” from what he called Thoughts [Gedanken] or the\n“senses” [Sinnen] of propositions; where each of these\nsentential senses “presents” a truth value in certain\nway—i.e., as the value of a certain indicated function given a\ncertain indicated argument.).  Variable letters, such as ‘x’ and ‘y’ in\n‘Q(x, y) = x/y’, are typographically convenient for\nrepresenting functions that take more than one argument. But we could\nalso index argument places, as shown below. Or we could replace the subscripts above with lines that connect\neach pair of round brackets on the left of ‘=’ to a\ncorresponding pair of brackets on the right. But the idea, however we\nencode it, is that a proposition has at least one constituent that is\nsaturated by the requisite number of arguments. (If it helps, think of\nan unsaturated proposition-part as the result of abstracting away from\none or more arguments in a complete proposition.) Frege was here\ninfluenced by Kant's discussion of judgment, and the ancient\nobservation that merely combining two things does not make the\ncombination truth-evaluable. So in saying that propositions have\n“function-argument” structure, Frege was not only\nrejecting the traditional idea that logical from reflects the\n“subject-predicate” structure of ordinary sentences, he\nwas suggesting that propositions exhibit a special kind of unity:\nunlike a mere concatenation of objects, a potential premise/conclusion\nis formed by saturating an unsaturated mapping with a suitable\nargument. \n\nOn Frege's view, the proposition that Mary sang has a\nfunctional component indicated by ‘sang’ and an argument\nindicated by ‘Mary’, even if the English sentence\n‘Mary sang’ has ‘Mary’ as its subject and\n‘sang’ as its predicate. The proposition can be\nrepresented as follows: Sang(Mary). Frege thought of the relevant\nfunction as a conditional mapping from individuals to truth values:\nSang(x) = T if x sang, and F\notherwise; where ‘T’ and\n‘F’ stand for special entities such that\nfor each individual x, Sang(x) = T if and only if x\nsang, and Sang(x) = F if and only if x did not\nsing. According to Frege, the proposition that John admires Mary\ncombines an ordered pair of arguments with a functional component\nindicated by the transitive verb: Admires(John, Mary); where for any\nindividual x, and any individual y, Admires(x, y) = T\nif x admires y, and F otherwise.  From this\nperspective, the structure and constituents are the same in the\nproposition that Mary is admired by John, even though\n‘Mary’ is the grammatical subject of the passive\nsentence. Likewise, Frege did not distinguish the proposition that\nthree precedes four from the proposition that four is preceded by\nthree. More importantly, Frege's treatment of quantified propositions\ndeparts radically from the traditional idea that the grammatical\nstructure of sentence reflects the logical structure of the indicated\nproposition.\n If S is the function indicated by ‘sang’, then Mary\nsang iff—i.e., if and only if—S(Mary) =\nT.  Likewise, someone sang iff: S maps some\nindividual onto T; that is, for some individual x,\nS(x) = T. Or using a modern variant of Frege's\noriginal notation, someone sang iff ∃x[S(x)]. The quantifier\n‘∃x’ is said to bind the variable ‘x’,\nwhich ranges over individual things in a domain of discourse. (For\nnow, assume that the domain contains only people.) If every individual\nin the domain sang, then S maps every individual onto the truth value\nT; or using formal notation, ∀x[S(x)]. A\nquantifier binds each occurrence of its variable, as in\n‘∃x[P(x) & D(x)]’, which reflects the logical\nform of ‘Someone is both a politician and deceitful’. In\nthis last example, the quantifier combines with a complex predicate\nthat formed by conjoining two simpler predicates.  With regard to the proposition that some politician is deceitful,\ntraditional grammar suggests the division ‘Some politician / is\ndeceitful’, with the noun ‘politician’ forming a\nconstituent with the quantificational word. But on a Fregean view,\ngrammar masks the logical division between the existential quantifier\nand the rest: ∃x[P(x) & D(x)]. With regard to the\nproposition that every politician is deceitful, Frege also stresses\nthe logical division between the quantifier and its scope:\n∀x[P(x) → D(x)]; every individual is deceitful if a\npolitician. Here too, the quantifier combines with a complex\npredicate, albeit a conditional rather than conjunctive\npredicate. (The formal sentence ‘∀x[P(x) &\nD(x)]’ implies, unconditionally, that every individual is a\npolitician.) As Frege (1879) defined his analogs of the relevant\nmodern symbols used here, ‘P(x) → D(x)’ is equivalent\nto ‘¬P(x) ∨ D(x)’, and ‘∀x’ is\nequivalent to ‘¬∃x¬’. So\n‘∀x[P(x) → D(x)]’ is equivalent to\n‘¬∃x¬[¬P(x) ∨ D(x)]’; and given de\nMorgan's Laws (concerning the relations between negation, disjunction,\nand conjunction), ¬∃x¬[¬P(x) ∨ D(x)] iff\n¬∃x[P(x) & ¬D(x)]. Hence, ∀x[P(x) →\nD(x)] iff ¬∃x[P(x) & ¬D(x)]. This captures the idea\nthat every politician is deceitful iff no individual is both a\npolitician and not deceitful. If this conception of logical form is correct, then grammar is\nmisleading in several respects.  First, grammar leads us to think that\n‘some politician’ indicates a constituent of the\nproposition that some politician is deceitful. Second, grammar masks a\ndifference between existential and universally quantified\npropositions; predicates are related conjunctively in the former, and\nconditionally in the latter. (Though as discussed\nin section seven, one can—and Frege [1884]\ndid—adopt a different view that allows for relational/restricted\nquantifiers as in ‘∀x:P(x)[D(x)]’.) More importantly, Frege's account was designed to\napply equally well to propositions involving relations and multiple\nquantifiers. And with regard to these propositions, there seems to be\na big difference between logical structure and grammatical\nstructure. On Frege's view, a single quantifier can bind an unsaturated\nposition that is associated with a function that takes a single\nargument. But it is equally true that two quantifiers can bind two\nunsaturated positions associated with a function that takes a pair of\narguments. For example, the proposition that everyone likes everyone\ncan be represented with the formal sentence\n‘∀x∀y[L(x, y)]’. Assuming that\n‘Romeo’ and ‘Juliet’ indicate arguments, it\nfollows that Romeo likes everyone, and that everyone likes\nJuliet—∀y[L(r, y)] and ∀x[L(x, j)]. And it follows\nfrom all three propositions that Romeo likes Juliet: L(r, j). The\nrules of inference for Frege's logic capture this general feature of\nthe universal quantifier. A variable bound by a universal quantifier\ncan be replaced with a name for some individual in the\ndomain. Correlatively, a name can be replaced with a variable bound by\nan existential quantifier.  Given that Romeo likes Juliet, it follows\nthat someone likes Juliet, and Romeo likes someone. Frege's formalism\ncan capture this as well: L(r, j); so ∃x[L(x, j)] &\n∃x[L(r, x)]. And given either conjunct in the conclusion, it\nfollows that someone likes someone: ∃x∃y[L(x, y)]. A\nsingle quantifier can also bind multiple argument positions, as in\n‘∃x[L(x, x)]’, which is true iff someone likes\nherself. Putting these points schematically:\n∀x(…x…), so …n…; and\n…n…, so ∃x(…x…). Mixed quantification introduces an interesting wrinkle. The\npropositions expressed with ‘∃x∀y[L(x,y)]’\nand ‘∀y∃x[L(x,y)]’ differ. We can paraphrase\nthe first as ‘there is someone who likes everyone’ and the\nsecond as ‘everyone is liked by someone or other’. The\nsecond follows from the first, but not vice versa. This suggests that\n‘someone likes everyone’ is ambiguous, in that this string\nof English words can be used to express two different\npropositions. This in turn raises difficult questions about what\nnatural language expressions are, and how they can be used to express\npropositions; see section eight. But for Frege,\nthe important point concerned the distinction between the propositions\n(Gedanken). Similar remarks apply to ‘∀x∃y[L(x,\ny)]’ and ‘∃y∀x[L(x, y)]’. A related phenomenon is exhibited by ‘John danced if Mary\nsang and Chris slept’. Is the intended proposition of the form\n‘(A if B) and\nC’ or ‘A if\n(B and C)’?  Indeed, it seems\nthat the relation between word-strings and propositions expressed is\noften one-to-many. Is someone who says ‘The artist drew a\nclub’ talking about a sketch or a card game? One can use\n‘is’ to express identity, as in ‘Hesperus is the\nplanet Venus’; but in ‘Hesperus is bright’,\n‘is’ indicates predication. In ‘Hesperus is a\nplanet’, ‘a’ seems to be logically inert; yet in\n‘John saw a planet’, ‘a’ seems to indicate\nexistential quantification: ∃x[P(x) & S(j,x)]. (One can\nrender ‘Hesperus is a planet’ as ‘∃x[P(x)\n& h = x]’. But this treats ‘is a planet’ as\nimportantly different than ‘is bright’; and this leads to\nother difficulties.) According to Frege, such ambiguities provide\nfurther evidence that natural language is not suited to the task of\nrepresenting propositions and inferential relations perspicuously. And\nhe wanted a language that was suited for this task. (Leibniz and\nothers had envisioned a “Characteristica Universalis”, but\nwithout detailed proposals for how to proceed beyond syllogistic logic\nin creating one.) This is not to deny that natural language is well\nsuited for other purposes, perhaps including efficient human\ncommunication.  And Frege held that we often do use natural language\nto express propositions. But he suggested that natural language is\nlike the eye, whereas a good formal language is like a microscope that\nreveals structure not otherwise observable. On this view, the logical\nform of a proposition is made manifest by the structure of a sentence\nin an ideal formal language—what Frege called a Begriffsschrift\n(concept-script); where the sentences of such a language exhibit\nfunction-argument structures that differ in kind from the grammatical\nstructures exhibited by the sentences we use in ordinary\ncommunication. The real power of Frege's strategy for representing propositional\nstructure is most evident in his discussions of proofs by induction,\nthe Dedekind-Peano axioms for arithemetic, and how the proposition\nthat every number has a successor is logically related to more basic\ntruths of arithmetic; see the entry on\n Frege's theorem and foundations for arithmetic.\n But without getting into these details, one can get a\nsense of Frege's improvement on previous logic by considering\n(15–16) and Fregean analyses of the corresponding\npropositions. \nSuppose that every individual has the following conditional property:\nif hex is a patient, then some individual is such\nthat shey is both a doctor and respected by\nhimx. Then it follows—intuitively and given\nthe rules of Frege's logic—that every\nindividualx has the following conditional property:\nif hex is both old and a patient, then some\nindividualy is such that shey is\nboth a doctor and respected by himx. So the\nproposition expressed with (16) follows from the one expressed with\n(15). More interestingly, we can also account for why the proposition\nexpressed with (14) follows from the one expressed with (13). \nFor suppose it is false that some individual has the following\nconjunctive property: hex is a patient; and\nhex saw every young doctor (i.e., every\nindividualy is such that if shey\nis a young doctor, then hex was seen by\nhery); and hex is healthy. Then\nintuitively, and also given the rules of Frege's logic, it is false\nthat some individual has the following conjunctive property:\nhex is a patient; and hex saw\nevery doctor; and hex is healthy. This explains why\nthe direction of valid inference is from the more restrictive\n‘young doctor’ in (13) to the less restrictive\n‘patient’ in (14), despite the fact that in simpler cases,\nreplacing ‘every doctor’ with ‘every young\ndoctor’ is valid. More generally, Frege's logic handles a wide\nrange of inferences that had puzzled medieval logicians. But the\nFregean logical forms seem to differ dramatically from the grammatical\nforms of sentences like (13–16). Frege concluded that we need a\nBegriffsschrift, distinct from the languages we naturally speak, in\norder to depict (and help us discern) the structures of the\npropositions we express by using natural languages. \nFrege also made a different kind of contribution, which would prove\nimportant, to the study of propositions. In early work, he spoke as\nthough propositional constituents were the relevant functions and\n(ordered n-tuples of) entities that such functions map to\ntruth-values.  But he later refined this view in light of his\ndistinction between Sinn and Bedeutung (see the entry on\n Gottlob Frege).  The Sinn of an expression was\nsaid to be a “way of presenting” the corresponding\nBedeutung, which might be an entity, a truth-value, or a function from\n(ordered n-tuples of) entities to truth-values. The basic idea is that\ntwo names, like ‘Hesperus’ and ‘Phosphorus’,\ncan present the same Bedeutung in different ways; in which case, the\nSinn of the first name differs from the Sinn of the second. Given this\ndistinction, we can think of ‘Hesperus’ as an expression\nthat presents the evening star (a.k.a. Venus) as such, while\n‘Phosphorus’ presents the morning star (also a.k.a. Venus)\nin a different way. Likewise, we can think of ‘is bright’\nas an expression that presents a certain function in a certain way,\nand ‘Hesperus is bright’ as a sentence that presents its\ntruth-value in a certain way—i.e., as the value of the function\nin question given the argument in question. From this perspective,\npropositions are sentential ways of presenting truth-values, and\nproposition-parts are subsentential ways of presenting functions and\narguments. Frege could thus distinguish the proposition that Hesperus\nis bright from the proposition that Phosphorus is bright, even though\nthe two propositions are alike with regard to the relevant function\nand argument. Likewise, he could distinguish the trivial proposition\nHesperus is Hesperus from the (apparently nontrivial) proposition\nHesperus is Phosphorus. This is an attractive view. For intuitively,\nancient astronomers were correct not to regard the inference Hesperus\nis Hesperus, so Hesperus is Phosphorus as an instance of the following\nvalid schema: A, so A. But this\nraised questions about what the Sinn of an expression really is, what\n“presentation” could amount to, and what to say about a\nname with no Bedeutung. Frege did not distinguish (or at least did not emphasize any\ndistinction between) names like ‘John’ and descriptions\nlike ‘the boy’ or ‘the tall boy from Canada’.\nInitially, both kinds of expression seem to indicate arguments, as\nopposed to functions. So one might think that the logical form of\n‘The boy sang’ is simply ‘S(b)’, where\n‘b’ is an unstructured symbol that stands for the boy in\nquestion (and presents him in a certain way). But this makes the\nelements of a description logically irrelevant. And this seems\nwrong. If the tall boy from Canada sang, then some boy from Canada\nsang. Moreover, ‘the’ implies uniqueness in a way\nthat ‘some’ does not. Of course, one can say ‘The\nboy sang’ without denying that universe contains more than one\nboy. But likewise, in ordinary conversation, one can say\n‘Everything is in the trunk’ without denying that the\nuniverse contains some things not in the trunk. And intuitively, a\nspeaker who uses ‘the’ does imply that the adjacent\npredicate is satisfied by exactly one contextually relevant thing. \nBertrand Russell held that these\nimplications reflect the logical form of a proposition expressed (in a\ngiven context) with a definite description.  On his view, ‘The\nboy sang’ has the following logical form: ∃x{Boy(x) &\n∀y[Boy(y) → y = x] & S(x)}; some\nindividualx is such that hex is a\nboy, and every (relevant) individualy is such that\nif hey is a boy, then hey is\nidentical with himx, and hex\nsang. The awkward middle conjunct was Russell's way of expressing\nuniqueness with Fregean tools; cf.\n section seven. But rewriting the middle conjunct would not affect\nRussell's technical point, which is that ‘the boy’ does\nnot correspond to any constituent of the formalism.  This in turn\nreflects Russell's central claim—viz., that while a speaker may\nrefer to a certain boy in saying ‘The boy sang’, the boy\nin question is not a constituent of the proposition\nindicated. According to Russell, the proposition has the form of an\nexistential quantification with a bound variable. It does not\nhave the form of a function saturated by (an argument that is) the boy\nreferred to. The proposition is general rather than singular. In this\nrespect, ‘the boy’ is like ‘some boy’ and\n‘every boy’; though on Russell's view, not even\n‘the’ indicates a constituent of the proposition\nexpressed. \nThis extended Frege's idea that natural language misleads us about the\nstructure of the propositions we assert. Russell went on to apply this\nhypothesis to what became a famous puzzle. Even though France is\ncurrently kingless, ‘The present king of France is bald’\ncan be used to express a proposition. The sentence is not meaningless;\nit has implications. So if the proposition consists of the function\nindicated with ‘Bald( )’ and an argument indicated\nwith ‘The present king of France’, there must be an\nargument so indicated. But appeal to nonexistent kings is, to say the\nleast, dubious. Russell concluded that ‘The present king of\nFrance is bald’ expresses a quantificational proposition:\n∃x{K(x) & ∀y[K(y) → y = x] & B(x)}; where\nK(x) = T iff x is a present king of France, and B(x)\n= T iff x is bald. (For present purposes, set aside\nworries about the vagueness of ‘bald’.)  And as Russell\nnoted, the following contrary reasoning is spurious: every proposition\nis true or false; so the present king of France is bald or not; so\nthere is a king of France, and he is either bald or not. For let\nP be the proposition that the king of France is\nbald. Russell held that P is indeed true or false. On\nhis view, it is false. Given that ¬∃x[K(x)], it follows that\n¬∃x{K(x) & ∀y[K(y) → y = x] & B(x)}. But\nit does not follow that there is a present king of France who is\neither bald or not. Given that ¬∃x[K(x)], it hardly follows\nthat ∃x{K(x) & [B(x) v ¬B(x)]}. So we must not confuse\nthe negation of P with the following false\nproposition: ∃x{K(x) & ∀y[K(y) → y = x] &\n¬B(x)}. The ambiguity of natural language may foster such\nconfusion, given examples like ‘The present king of France is\nbald or not’. But according to Russell, puzzles about\n“nonexistence” can be resolved without special\nmetaphysical theses, given the right views about logical form and\nnatural language. \nThis invited the thought that other philosophical puzzles might\ndissolve if we properly understood the logical forms of our\nclaims. Wittgenstein argued, in his influential Tractatus\nLogico-Philosophicus, that: (i) the very possibility of\nmeaningful sentences, which can be true or false depending on how the\nworld is, requires propositions with structures of the sort Frege and\nRussell were getting at; (ii) all propositions are logical compounds\nof—and thus analyzable into—atomic propositions that are\ninferentially independent of one another; though (iii) even simple\nnatural language sentences may indicate very complex propositions; and\n(iv) the right analyses would, given a little reflection, reveal all\nphilosophical puzzles as confusions about how language is related to\nthe world. Russell never endorsed (iv). And Wittgenstein later noted\nthat claims like ‘This is red’ and ‘This is\nyellow’ presented difficulties for his earlier view. If the\nexpressed propositions are unanalyzable, and thus logically\nindependent, each should be compatible with the other. But at least so\nfar, no one has provided a plausible analysis that accounts for the\napparent impeccabilty of ‘This is red, so this is not\nyellow’. (This raises questions about whether all\ninferential security is due to logical form.) Though for reasons\nrelated to epistemological puzzles, Russell did say that (a) we are\ndirectly acquainted with the constituents of those\npropositions into which every proposition (that we can grasp) can be\nanalyzed; (b) at least typically, we are not directly acquainted with\nthe mind-independent bearers of proper names; and so (c) the things we\ntypically refer to with names are not constituents of basic\npropositions. \nThis led Russell to say that natural language names are disguised\ndescriptions. On this view, ‘Hesperus’ is semantically\nassociated with a complex predicate—say, for illustration, a\npredicate of the form ‘E(x) & S(x)’, suggesting\n‘evening star’. In which case, ‘Hesperus is\nbright’ expresses a proposition of the form\n‘∃x{[E(x) & S(x)] & ∀y{[E(y) & S(y)]\n→ y = x]} & B(x)}’. It also follows that Hesperus\nexists iff ∃x[E(x) & S(x)]; and this would be challenged by\nKripke (1980); see the entries on\n rigid-designators and\n names. But by analyzing names as\ndescriptions—quantificational expressions, as opposed to logical\nconstants (like ‘b’) that indicate\nindividuals—Russell offered an attractive account of why the\nproposition that Hesperus is bright differs from the proposition that\nPhosphorus is bright. Instead of saying that propositional\nconstituents are Fregean senses, Russell could say that\n‘Phosphorus is bright’ expresses a proposition of the form\n‘∃x{[M(x) & S(x)] & ∀y{[M(y) & S(y)]\n→ y = x]} & B(x)’; where ‘E(x)’ and\n‘M(x)’ indicate different functions, specified\n(respectively) in terms of evenings and mornings. This leaves room for\nthe discovery that the complex predicates ‘E(x) &\nS(x)’ and ‘M(x) & S(x)’ both indicate functions\nthat map Venus and nothing else to the truth-value\nT. The hypothesis was that the propositions expressed\nwith ‘Hesperus is bright’ and ‘Phosphorus is\nbright’ have different (fundamental) constituents, even though\nHesperus is Phosphorus, but not because propositional constituents are\n“ways of presenting” Bedeutungen. Similarly, the idea was\nthat the propositions expressed with ‘Hesperus is\nHesperus’ and ‘Hesperus is Phosphorus’ differ,\nbecause only the latter has predicational/unsaturated constituents\ncorresponding to ‘Phosphorus’. Positing unexpected logical\nforms seemed to have explanatory payoffs. \nQuestions about names and descriptions are also related to\npsychological reports, like ‘Mary thinks Venus is bright’,\nwhich present puzzles of their own; see the entry on\n propositional attitude reports.\n Such reports seem to indicate propositions that are\nneither atomic nor logical compounds of simpler propositions. For as\nFrege noted, replacing one name with another name for the same object\ncan apprarently affect the truth of a psychological report. If Mary\nfails to know that Hesperus is Venus, she might think Venus is a\nplanet without thinking Hesperus is a planet; though cp. Soames (1987,\n1995, 2002) and see the entry on\n singular propositions. Any\nfunction that has the value T given Venus as argument\nhas the value T given Hesperus as argument. So Frege,\nRussell, and Wittgenstein all held—in varying ways—that\npsychological reports are also misleading with respect to the logical\nforms of the indicated propositions. \nWithin the analytic tradition inspired by these philosophers, it\nbecame a commonplace that logical form and grammatical form typically\ndiverge, often in dramatic ways. This invited attempts to provide\nanalyses of propositions, and accounts of natural language, with the\naim of saying how relatively simple sentences (with subject-predicate\nstructures) could be used to express propositions (with\nfunction-argument structures). \nThe logical positivists explored the idea that the meaning of a\nsentence is a procedure for determining the truth or falsity of that\nsentence. From this perspective, studies of linguistic meaning and\npropositional structure still dovetail, even if natural language\nemploys “conventions” that make it possible to indicate\ncomplex propositions with grammatically simple sentences; see the\nentry on analysis. But to cut short a long\nand interesting story, there was little success in formulating\n“semantic rules” that were plausible both as (i)\ndescriptions of how ordinary speakers understand sentences of natural\nlanguage, and (ii) analyses that revealed logical structure of the\nsort envisioned. (And until Montague [1970], discussed briefly in the\nnext section, there was no real progress in showing how to\nsystematically associate quantificational constructions of natural\nlanguage with Fregean logical forms.) \n Rudolf Carnap, one of the\nleading positivists, responded to difficulties facing his earlier\nviews by developing a sophisticated position according to which\nphilosophers could (and should) articulate alternative sets of\nconventions for associating sentences of a language with\npropositions. Within each such language, the conventions would\ndetermine what follows from what. But one would have to decide, on\nbroadly pragmatic grounds, which interpreted language was best for\ncertain purposes (like conducting scientific inquiry). On this view,\nquestions about “the” logical form of an ordinary sentence\nare in part questions about which conventions one should adopt. The\nidea was that “internal” to any logically perspicuous\nlinguistic scheme, there would be an answer to the question of how two\nsentences are inferentially related. But “external”\nquestions, about which conventions we should adopt, would not be\nsettled by descriptive facts about how we understand languages that we\nalready use. \nThis was, in many ways, an attractive development of Frege's vision.\nBut it also raised a skeptical worry. Perhaps the structural\nmismatches between sentences of a natural language and sentences of a\nFregean Begriffsschrift are so severe that one cannot formulate\ngeneral rules for associating the sentences we ordinarily use with\npropositions. Later theorists would combine this view with the idea\nthat propositions are sentences of a mental language that is\nrelevantly like Frege's invented language and relevantly unlike the\nspoken languages humans use to communicate; see Fodor (1975,\n1978). But given the rise of\n behaviorism, both in philosophy and\npsychology, this variant on a medieval idea was initially ignored or\nridiculed. (And it does face difficulties; see\n section 8.) \nWillard Van Orman Quine combined behaviorist\npsychology with a normative conception of logical form similar to\nCarnap's. The result was an influential view according to which there\nis no fact of the matter about which proposition a speaker/thinker\nexpresses with a sentence of natural language, because talk of\npropositions is (at best) a way of talking about how we should\nregiment our verbal behavior for certain purposes—and in\nparticular, for purposes of scientific inquiry. On this view, claims\nabout logical form are evaluative, and such claims are underdetermined\nby the totality of facts concerning speakers' dispositions to use\nlanguage. From this perspective, mismatches between logical and\ngrammatical form are to be expected, and we should not conclude that\nordinary speakers have mental representations that are isomorphic with\nsentences of a Fregean Begriffsschrift. \nAccording to Quine, speakers' behavioral dispositions constrain what\ncan be plausibly said about how to best regiment their language. He\nalso allowed for some general constraints on interpretability that an\nidealized “field linguist” might impose in coming up with\na regimented interpretation scheme.  \n(Donald Davidson developed a similar line\n of thought in a less\nbehavioristic idiom, speaking in terms of constraints on a\n“Radical Interpreter,” who seeks “charitable”\nconstruals of alien speech.) But unsurprisingly, this left ample room\nfor “slack” with respect to which logical forms should be\nassociated with a given sentential utterance. \nQuine also held that decisions about how to make such associations\nshould be made holistically. As he sometimes put it, the\n“unit of translation” is an entire language, not a\nparticular sentence. On this view, one can translate a sentence\nS of a natural language NL with a structurally mismatching\nsentence\nµ\nof a formal language FL, even if it seems (locally) implausible\nthat S is used to express the proposition associated with\nµ, so long as the following condition is met: the association\nbetween S and µ is part of a general account of NL and\nFL that figures in an overall theory—which includes an account\nof language, logic, and the language-independent world—that is\namong the best overall theories available. This holistic conception of\nhow to evaluate proposed regimentations of natural language was part\nand parcel of Quine's criticism of the early positivists' \nanalytic-synthetic distinction, and\nhis more radical suggestion that there is no\nsuch distinction. \nThe suggestion was that even apparently tautologous sentences, like\n‘Bachelors are unmarried’ and ‘Caesar died if Brutus\nkilled him’, have empirical content. These may be among the last\nsentences we would dissent from, faced with recalcitrant experience;\nwe may prefer to say that Caesar didn't really die, or that Brutus\ndidn't really kill him, if the next best alternative is to deny the\nconditional claim. But for Quine, every meaningful claim is a claim\nthat could turn out to be false—and so a claim we must be\nprepared, at least in principle, to reject. Correlatively, no\nsentences are known to be true simply by knowing what they mean (and\nknowing a priori that sentences with such meanings must be\ntrue). \nFor present purposes, we can abstract away from the details of debates\nabout whether Quine's overall view was plausible. Here, the important\npoint is that claims about logical form were said to be (at least\npartly) claims about the kind of regimented language we\nshould use, not claims about the propositions actually\nexpressed with sentences of natural language. And one aspect of\nQuine's view, about the kind of regimented language we should\nuse, turned out to be especially important for subsequent discussions\nof logical form.  For even among those who rejected the behavioristic\nassumptions that animated Quine's conception of language, it was often\nheld that logical forms are expressions of a first-order predicate\ncalculus. \nFrege's Begriffsschrift, recall, was designed to capture the\nDedekind-Peano axioms for arithmetic, including the axiom of\ninduction; see the entry on\n Frege's theorem and foundations for arithmetic.\n This required\nquantification into positions occupiable by predicates, as well as\npositions occupiable by names. Using modern notation, Frege allowed\nfor formulae like ‘(Fa & Fb) → ∃X(Xa &\nXb)’ and ‘∀x∀y[x = y ↔ ∀X(Xx\n↔ Xy)]’. And he took second-order quantification to be\nquantification over functions. This is to say, for example, that\n‘∃X(Xa & Xb)’ is true iff: there is a function,\nX, that maps both the individual called ‘a’ and the\nindividual called ‘b’ onto the truth-value\nT. Frege also took it to be a truth of logic that for\nany predicate P, there is a function such that for each\nindividual x, that function maps x to T iff x\nsatisfies (or “falls under”) P.  In which case,\nfor each predicate, there is the set of all and only the things that\nsatisfy the predicate. The axioms for Frege's logic thus generated\n Russell's paradox, given predicates\nlike ‘is not a member of itself’. This invited attempts to\nweaken the axioms, while preserving second-order\nquantification. But for various reasons, Quine and others advocated a\nrestriction to a first-order fragment of Frege's logic, disallowing\nquantification into positions occupied by predicates. (Godel had\nproved the completeness of first-order predicate calculus, thus\nproviding a purely formal criterion for what followed from what in\nthat language. Quine also held that second-order quantification\nillicitly treated predicates as names for sets, thereby spoiling\nFrege's conception of propositions as unified by virtue of having\nunsaturated predicational constituents that are satisfied by things\ndenoted by names.) On Quine's view, we should replace ‘(Fa &\nFb) → ∃X(Xa & Xb)’ with explicit first-order\nquantification over sets, as in ‘(Fa & Fb) →\n∃s(a∈s & b∈s)’; where ‘∈’\nstands for ‘is an element of’, and this second conditional\nis not a logical truth, but rather a hypothesis (to be evaluated\nholistically) concerning sets. \nThe preference for first-order regimentations has come to seem\nunwarranted, or at least highly tendentious; see Boolos (1998). But it\nfueled the idea that logical form can diverge wildly from grammatical\nform. For as students quickly learn, first-order regimentations of\nnatural sentences often turn out to be highly artificial. (And in some\ncases, such regimentations seem to be unavailable.) This was, however,\ntaken to show that natural languages are far from ideal for purposes\nof indicating logical structure. \nA different strand of thought in analytic philosophy—pressed by\nWittgenstein in Philosophical Investigations and developed by\nothers, including Strawson and Austin—also suggested that a\nsingle sentence could be used (on different occasions) to express\ndifferent kinds of propositions. Strawson (1950) argued that\npace Russell, a speaker could use an instance of ‘The\nF is G’ to express a singular proposition\nabout a specific individual: namely, the F in the context at\nhand.  According to Strawson, sentences themselves do not have truth\nconditions, since sentences (as opposed to speakers) do not express\npropositions; and speakers can use ‘The boy is tall’ to\nexpress a proposition with the contextually relevant boy as a\nconstituent. Donnellan (1966) went on to argue that a speaker could\neven use an instance of ‘The F is G’ to\nexpress a singular proposition about an individual that isn't an\nF; see the entry on reference.\nSuch considerations, which have received a great deal of attention in\nrecent discussions of context dependence, suggested that relations\nbetween natural language sentences and propositions are (at best) very\ncomplex and mediated by speakers' intentions. All of which made it\nseem that such relations are far more tenuous than the pre-Fregean\ntradition suggested. This bolstered the Quine/Carnap idea that\nquestions about the structure of premises and conclusions are really\nquestions about how we should talk (when trying to describe\nthe world), much as logic itself seems to be more concerned with how\nwe should infer than with how we do infer. From this perspective, the\nconnections between logic and grammar seemed rather shallow.  On the other hand, more recent work on quantifiers suggests that\nthe divergence had been exaggerated, in part because of how Frege's\nidea of variable-binding was originally implemented. Consider again\nthe proposition that some boy sang, and the proposed logical division\ninto the quantifier and the rest: ∃x[Boy(x) & Sang(x)];\nsomething is both a boy and an individual that sang.  This is one way\nto regiment the English sentence. But one can also offer a logical\nparaphrase that more closely parallels the grammatical division\nbetween ‘some boy’ and ‘sang’: for some\nindividual x such that x is a boy, x sang. One can formalize this\nparaphrase with restricted quantifiers, which incorporate a\nrestriction on the domain over which the variable in question\nranges. For example, ‘∃x:B(x)’ can be an existential\nquantifier that binds a variable ranging over the boys in the relevant\ndomain, with ‘∃x:B(x)[S(x)]’ being true iff some boy\nsang. Since ‘∃x:B(x)[S(x)]’ and ‘∃x[B(x)\n& S(x)]’ are logically equivalent, logic provides no reason\nfor preferring the latter regimentation of the English sentence. And\nchoosing the latter does not show that the proposition expressed with\n‘Some boy sang’ has a structure that differs from\ngrammatical structure of the sentence. Universal quantifiers can also be restricted, as in\n‘∀x:B(x)[S(x)]’, interpreted as follows: for every\nindividual x such that x is a boy, x sang.  Restrictors can also be\nlogically complex, as in ‘Some boy from Canada sang’ or\n‘Some boy who respects Mary sang’, rendered as\n‘∃x:B(x) & F(x, c)[S(x)]’ and\n‘∃x:B(x) & R(x, m)[S(x)]’. Given these\nrepresentations, the inferential difference between ‘some boy\nsang’ and ‘every boy sang’ lies with the\npropositional contributions of ‘some’ and\n‘every’ after all, and not partly with the contribution of\nconnectives like ‘&’ and ‘→’.  Words like ‘someone’, and the grammatical requirement\nthat ‘every’ be followed by a noun (or noun phrase),\nreflect the fact that natural language employs restricted quantifiers.\nPhrases like ‘every boy’ are composed of a determiner and\na noun. Correspondingly, one can think of determiners as expressions\nthat can combine with an ordered pair of predicates to form a\nsentence, much as one can think of transitive verbs as expressions\nthat can combine with an ordered pair of names to form a sentence. And\nthis grammatical analogy, between determiners and transitive verbs,\nhas a semantic correlate. \nSince ‘x’ and ‘y’ are variables ranging over\nindividuals, one can say that the function indicated by the transitive\nverb ‘likes’ yields the value T given the\nordered pair 〈x,y〉 as argument if and only if x likes y. In\nthis notational scheme, ‘y’ corresponds to the direct\nobject (or internal argument), which combines with the verb to form a\nphrase; ‘x’ corresponds to the grammatical subject (or\nexternal argument) of the verb. If we think about ‘every boy\nsang’ analogously, ‘boy’ is the internal argument of\n‘every’, since ‘every boy’ is a phrase. By\ncontrast, ‘boy’ and ‘sang’ do not form a\nphrase in ‘every boy sang’. So let us introduce\n‘X’ and ‘Y’ as second-order variables ranging\nover functions, from individuals to truth values, stipulating that the\nextension of such a function is the set of things that the function\nmaps onto the truth value T. Then one can say that\nthe function indicated by ‘every’ yields the value\nT given the ordered pair 〈X, Y〉 as argument\niff the extension of X includes the extension of Y.  Similarly, one\ncan say that the function indicated by ‘some’ maps the\nordered pair 〈X, Y〉 onto T iff the\nextension of X intersects with the extension of Y. \nJust as we can describe ‘likes’ as a predicate satisfied\nby ordered pairs 〈x, y〉 such that x likes y, so we can think\nabout ‘every’ as a predicate satisfied by ordered pairs\n〈X, Y〉 such that the extension of X includes the extension\nof Y. (This is compatible with thinking about ‘every boy’\nas a restricted quantifier that combines with a predicate to form a\nsentence that is true iff every boy satisfies that predicate.) One\nvirtue of this notational scheme is that it lets us represent\nrelations between predicates that cannot be captured with\n‘∀’, ‘∃’, and the sentential\nconnectives; see Rescher (1962), Wiggins (1980). For example, most\nboys sang iff the boys who sang outnumber the boys who did not\nsing. So we can say that ‘most’ indicates a function that\nmaps 〈X, Y〉 to T iff the number of things\nthat both Y and X map to T exceeds the number of\nthings that Y but not X maps to T. Using restricted quantifiers, and thinking about determiners as\ndevices for indicating relations between functions, also suggests an\nalternative to Russell's treatment of ‘the’. The formula\n‘∃x{B(x) & ∀y[B(y) → x = y] &\nS(x)}’ can be rewritten as ‘∃x:B(x)[S(x)] & |B|\n= 1’, interpreted as follows: for some individual x such that x\na boy, x sang, and the number of (relevant) boys is exactly one. On\nthis view, ‘the boy’ still does not correspond to a\nconstituent of the formalism; nor does ‘the’. But one can\ndepart farther from Russell's notation, while emphasizing his idea\nthat ‘the’ is relevantly like ‘some’ and\n‘every’. For one can analyze ‘the boy sang’ as\n‘!x:Boy(x)[Sang(x)]’, specifying the propositional\ncontribution of ‘!’—on a par with as\n‘∃’ and ‘∀’—as follows: \nThis way of encoding Russell's theory preserves his central claim.\nWhile there may be a certain boy that a speaker refers to in saying\n‘The boy sang’, that boy is not a constituent of the\nquantificational proposition expressed with\n‘!x:Boy(x)[Sang(x)]’; see Neale (1990) for discussion. But\nfar from showing that the logical form of ‘The boy sang’\ndiverges dramatically from its grammatical form, the\nrestricted quantifier notation suggests that the logical form closely\nparallels the grammatical form. For ‘the boy’ and\n‘the’ do correspond to constituents of\n‘!x:B(x)[S(x)]’, at least if we allow for logical forms\nthat represent quantificational propositions in terms of second-order\nrelations; see Montague (1970). It is worth noting, briefly, an implication of this point for the\ninference ‘The boy sang, so some boy sang’. If the logical\nform of ‘The boy sang’ is ‘∃x:B(x)[S(x)] &\n|B|=1’, then the inference is an instance of the schema\n‘A & B, so\nA’. But if the logical form of ‘The boy\nsang’ is simply ‘!(x):B(x)[S(x)]’, the premise and\nconclusion have the same form, differing only by substitution of\n‘!’ for ‘∃’. In which case, the\nimpeccability of the inference depends on the specific contributions\nof ‘the/!’ and ‘some/∃’.  Only when\nthese contributions are “spelled out,” perhaps in terms of\nset-intersection, would the validity of the inference be manifest; see King (2002). So\neven if grammar and logic do not diverge in this case, one might say\nthat grammatical structure does not reveal the logical\nstructure. From this perspective, further analysis of\n‘the’ is required. Those who are skeptical of an\nanalytic/synthetic distinction can say that it remains more a decision\nthan a discovery to say that ‘Some boy sang’ follows from\n‘The boy sang’. In general, and especially with regard to\naspects of propositional form indicated with individual words, issues\nabout logical form are connected with issues about the\n analytic-synthetic distinction.  Even given restricted quantifiers (and acceptance of second-order\nlogical forms), the subject/predicate structure of ‘Juliet /\nlikes every doctor’ diverges from the corresponding formula\nbelow.  We can rewrite ‘Likes(Juliet, y)’ as\n‘[Likes(y)](Juliet)’, to reflect the fact that\n‘likes’ combines with a direct object to form a phrase,\nwhich in turn combines with a subject. But this does not affect the\nmain point; ‘every’ seems to be a grammatical constituent\nof the verb phrase ‘likes every doctor’, and yet the main\nquantifier of the expressed proposition. In natural language,\n‘likes’ and ‘every doctor’ form a phrase. But\nwith respect to logical form, ‘likes’ evidently combines\nwith ‘Juliet’ and a variable to form a complex predicate\nthat is in turn an external argument of the higher-order predicate\n‘every’. Similar remarks apply to ‘Some boy likes\nevery doctor’ and\n‘[∃x:Boy(x)][∀y:Doctor(y)]{Likes(x, y)]’. So\nit seems that mismatches remain in the very places that troubled\nmedieval logicians—viz., quantificational direct objects and\nother examples of complex predicates with quantificational\nconstituents. Montague (1970, 1974) showed that these mismatches do not preclude\nsystematic connections of natural language sentences with the\ncorresponding propositional structures. Abstracting from the technical\ndetails, one can specify an algorithm that pairs each natural language\nsentence that contains one or more quantificational expressions like\n‘every doctor’ with one or more Fregean logical\nforms. This was a significant advance. Together with subsequent\ndevelopments, Montague's work showed that Frege's logic was compatible\nwith the idea that quantificational constructions in natural language\nhave a systematic semantics. Indeed, one can use Frege's formal\napparatus to study such constructions. Montague himself maintained\nthat the syntax of natural language was misleading for purposes of\n(what he took to be) real semantics. On this view, the study of valid\ninference still suggests that natural language grammar disguises the\nstructure of human thought. But in thinking about the relation of\nlogic to grammar, one should not assume a naive conception of the\nlatter. For example, the grammatical form of a sentence need not be\ndetermined by the linear order of its words. Using brackets to\ndisambiguate, we can distinguish the sentence ‘Mary [saw [the\n[boy [with binoculars]]]]’—whose direct object is\n‘the boy with binoculars’—from the homophonous\nsentence ‘Mary [[saw [the boy]] [with binoculars]]’, in\nwhich ‘saw the boy’ is modified by an adverbial\nphrase. The first implies that the boy had binoculars, while the\nsecond implies that Mary used binoculars to see the boy. This\ndistinction may not be audibly marked.  Nonetheless, there is a\ndifference between modifying a noun (like ‘boy’) with a\nprepositional phrase and modifying a verb phrase (‘saw the\nboy’). More generally, grammatical structure need not be\nobvious. Just as it may take work to discover the kind(s) of structure\nthat propositions exhibit, so it may take work to discover the kind(s)\nof structure that sentences exhibit. And many studies of natural\nlanguage suggest a rich conception of grammatical form that diverges\nfrom traditional views; see especially Chomsky (1957, 1965, 1981,\n1986, 1995). So we need to ask how logical forms are related to actual\ngrammatical forms, which linguists try to discover, since these may\ndiffer importantly from any hypothesized grammatical forms that may be\nsuggested by casual reflection on spoken language.  Appearances may be\nmisleading with respect to both grammatical and logical form, leaving\nroom for the possibility that these notions of structure are not so\ndifferent after all. A leading idea of modern linguistics is that at least some\ngrammatical structures are transformations of others. Put another way,\nlinguistic expressions often appear to be displaced from the positions\ncanonically associated with certain grammatical relations that the\nexpressions exhibit. For example, the word ‘who’ in (17)\nis apparently associated with the internal (direct object) argument\nposition of the verb ‘saw’.  Correspondingly, (17) can be glossed as ‘Mary wondered which\nperson is such that John saw that person’. This invites the\nhypothesis that (17) reflects a transformation of the “Deep\nStructure” (17D) into the “Surface Structure”\n(17S), \nwith indices indicating a grammatical relation between the\nindexed positions. In (17D), the embedded clause has the same form as\n‘John saw Bill’. But in (17S), ‘who’ has been\ndisplaced from the indexed argument position. Similar remarks apply to\nthe question ‘Who did John see’ and other question-words\nlike ‘why’, ‘what’, ‘when’, and\n‘how’. \nOne might also explain the synonymy of (18) and (19) by positing a\ncommon deep structure, (18D).  If every English sentence needs a grammatical subject, (18D) must\nbe modified: either by displacing ‘John’, as in (18S); or\nby inserting a pleonastic subject, as in (19). Note that in (19),\n‘It’ does not indicate an argument; compare\n‘There’ in ‘There is something in the garden’.\nAppeal to displacement also lets one distinguish the superficially\nparallel sentences (20) and (21).  If (20) is true, John is easily pleased. In which case, it is easy\n(for someone) to please John; where ‘it’ is\npleonastic. But if (21) is true, John is eager that he please someone\nor other. This asymmetry is effaced by representations like\n‘Easy-to-please(John)’ and\n‘Eager-to-please(John)’. The contrast is made manifest,\nhowever, with (20S) and (21S);  where ‘e’ indicates an unpronounced argument\nposition. It may be that in (21S), which does not mean that it is\neager for John to please someone, ‘John’ is grammatically\nlinked but not actually displaced from the coindexed position. But\nwhatever the details, the “surface subject” of a sentence\ncan be the object of a verb embedded within the main predicate, as in\n(20S). Of course, such hypotheses about grammatical structure require\ndefense. But Chomsky and others have long argued that such hypotheses\nare needed to account for various facts concerning human linguistic\ncapacities; see, e.g., Berwick et.al. (2014). As an illustration of\nthe kind of data that is relevant, note that (22–24) are\nperfectly fine expressions of English, while (25) is not.  This suggests that the auxiliary verb ‘was’ can be\ndisplaced from some positions but not others. That is, while (22S) is\na permissible transformation of (22D), (24S) is not a permissible\ntransformation of (24D). \nThe ill-formedness of (25) is striking, since one can sensibly ask\nwhether or not the boy who was happy sang. One can also ask whether or\nnot (26) is true. But (27) is not the yes/no question corresponding to\n(26).  Rather, (27) is the yes/no question corresponding to ‘The\nboy who lost was kept crying’, which has an unexpected\nmeaning. So we want some account of why (27) cannot have the\ninterpretation corresponding to (26). But the “negative\nfact” concerning (27) is precisely what one would expect if\n‘was’ cannot be displaced from its position in (26).  By contrast, if we merely specify an algorithm that associates\n(27) with its actual meaning—or if we merely hypothesize that\n(27) is the English translation of a certain mental sentence—we\nhave not yet explained why (27) cannot also be used to ask whether or\nnot (26) is true. Explanations of such facts appeal to nonobvious\ngrammatical structure, and constraints on natural language\ntransformations. (For example, an auxiliary verb in a relative clause\ncannot be “fronted;” though of course, theorists try to\nfind deeper explanations for such constraints.) The idea was that a sentence has both a deep structure (DS), which\nreflects semantically relevant relations between verbs and their\narguments, and a surface structure (SS) that may include displaced (or\npleonastic) elements. In some cases, pronunciation might depend on\nfurther transformations of SS, resulting in a distinct\n“phonological form” (PF). Linguists posited various\nconstraints on these levels of grammatical structure, and the\ntransformations that relate them. But as the theory was elaborated and\nrefined under empirical pressure, various facts that apparently called\nfor explanation in these terms still went unexplained. This suggested\nanother level of grammatical structure, perhaps obtained by a\ndifferent kind of transformation on SS. The hypothesized level was\ncalled ‘LF’ (intimating ‘logical form’); and\nthe hypothesized transformation—called quantifier raising\nbecause it targeted the kinds of expressions that indicate\n(restricted) quantifiers—mapped structures like (28S) onto\nstructures like (28L). \nClearly, (28L) does not reflect the pronounced word order in\nEnglish. But the idea was that (PF) determines pronunciation, while LF\nwas said to be the level at which the scope of a natural language\nquantifier is determined; see May (1985). If we think about\n‘every’ as a kind of second-order transitive predicate,\nwhich can combine with two predictes like ‘doctor’ and\n‘Juliet likes ( _ )i’ to form a\ncomplete sentence, we should expect that at some level of analysis,\nthe sentence ‘Juliet likes every doctor’ has the structure\nindicated in (28L). And mapping (28L) to the logical form\n‘[∀x:Doctor(x)]{Likes(Juliet, x)}’ is\ntrivial. Similarly, if the surface structure (29S) can be mapped onto\n(29L) or (29L'), then (29S) can be mapped onto the logical forms\n‘[∃x:Boy(x)][∀y:Doctor(y)]{Likes(x, y)}’ and\n‘[∀y:Doctor(y)][∃x:Boy(x)]{Likes(x,\ny)}’. This assimilates quantifier scope ambiguity to the\nstructural ambiguity of examples like ‘Juliet saw the boy with\nbinoculars’. More generally, many apparent examples of\ngrammar/logic mismatches were rediagnosed as mismatches between\ndifferent aspects of grammatical structure—between those aspects\nthat determine pronunication, and those that determine interpretation.\nIn one sense, this is fully in keeping with the idea that in natural\nlanguage, “surface appearances” are often misleading with\nregard to propositional structure. But it also makes room for the idea\nthat grammatical structure and logical structure converge, in ways\nthat can be discovered through investigation, once we move beyond\ntraditional subject-predicate conceptions of structure with regard to\nboth logic and grammar.  There is independent evidence for “covert”\ntransformations—displacement of expressions from their audible\npositions, as in (28L); see Huang (1995), Hornstein (1995). Consider,\nfor example, the French translation of ‘Who did John see’:\nJean a vu qui. If we assume that qui (‘who’) is displaced\nat  LF, then we can explain why the question-word is understood\nin both French and English like a quantifier binding a variable: which\nperson x is such that John saw x? Similarly, example (30) from Chinese\nis transliterated as in (31). \nBut (30) is ambiguous, between the interrogative (31a) and the\ncomplex declarative (31b). \nThis suggests covert displacement of the quantificational\nquestion-word in Chinese; see Huang (1982, 1995). Chomsky (1981) also\nargued that the constraints on such displacement can help explain\ncontrasts like the one illustrated with (32) and (33). \nIn (32), the pronoun ‘he’ can have a bound-variable\nreading: which person x is such that x said that x has the best\nsmile. This suggests that the following grammatical structure is\npossible: Whoi {[(  )i said\n[hei has the best smile]]}. But (33) cannot be used\nto ask this question, suggesting that some linguistic constraint rules\nout the following structure: And there cannot be constraints on transformations without\ntransformations. So if English overtly displaces question-words that\nare covertly displaced in other languages, we should not be surprised\nif English covertly displaces other quantificational expressions like\n‘every doctor’. Likewise, (34) has the reading indicated in\n(34a) but not the\nreading indicated in (34b).  This suggests that ‘every doctor’ gets displaced, but\nonly so far. Similarly, (13) cannot mean that every doctor is such\nthat no patient who saw that doctor is healthy. As we have already seen, English seems to abhor fronting certain\nelements from within an embedded relative clause. This invites the\nhypothesis that quantifier raising is subject to a similar constraint,\nand hence, that there is quantifier-raising in English. This\nhypothesis is controversial; see, e.g., Jacobson (1999). But many\nlinguists (following Chomsky [1995, 2000]) would now posit only two\nlevels of grammatical structure, corresponding to PF and LF—the\nthought being that constraints on DS and SS can be eschewed in favor\nof a simpler theory that only posits constraints on how expressions\ncan be combined in the course of constructing complex expressions that\ncan be pronounced and interpreted. If this development of earlier\ntheories proves correct, then the only semantically relevant level of\ngrammatical structure often reflects covert displacement of audible\nexpressions; see, e.g., Hornstein (1995). In any case, there is a\nlarge body of work suggesting that many logical properties of\nquantifiers, names, and pronouns are reflected in properties of\nLF. \nFor example, if (35) is true, it follows that some doctor treated some\ndoctor; whereas (36) does not have this consequence: The truth conditions of (35–36) seem to be as indicated in\n(35a) and (36a).  This suggests that ‘himself’ is behaving like a\nvariable bound by ‘the doctor’, while ‘every\nboy’ can bind ‘him’. And there are independent\ngrammatical reasons for saying that ‘himself’ must be\nlinked to ‘the doctor’, while ‘him’ must not\nbe so linked. Note that in ‘Pat thinks Chris treated\nhimself/him’, the antecedent of ‘himself’ must be\nthe subject of ‘treated’, while the antecedent of\n‘him’ must not be. We still need to enforce the conceptual distinction between LF and\nthe traditional notion of logical form. There is no guarantee that\nstructural features of natural language sentences will mirror the\nlogical features of propositions; cp. Stanley (2000), King (2007). But\nthis leaves room for the empirical hypothesis that LF reflects at\nleast a great deal of propositional structure; see Harman (1972),\nHigginbotham (1986), Segal (1989), Larson and Ludlow (1993), and the\nessay on structured propositions.\n  Moreover, even if the LF of a sentence S\nunderdetermines the logical form of the proposition a speaker\nexpresses with S (on a given occasion of use), the LF may provide a\n“scaffolding” that can be elaborated in particular\ncontexts, with little or no mismatch between grammatical and\npropositional architecture. If some such view is correct, it might\navoid certain (unpleasant) questions prompted by earlier Fregean\nviews: how can a sentence indicate a proposition with a different\nstructure; and if grammar is deeply misleading, why think that our\nintuitions concerning impeccability provide reliable evidence about\nwhich propositions follow from which?  These are, however, issues that\nremain unsettled. If propositions are the “things” that really have\nlogical form, and sentences of English are not themselves\npropositions, then sentences of English “have” logical\nforms only by association with propositions. But if the meaning of a\nsentence is some proposition—or perhaps a function from contexts\nto propositions—then one might say that the logical form\n“of” a sentence is its semantic structure (i.e., the\nstructure of that sentence's meaning). Alternatively, one might\nsuspect that in the end, talk of propositions is just convenient\nshorthand for talking about the semantic properties of sentences:\nperhaps sentences of a Begriffsschrift, or sentences of mentalese, or\nsentences of natural languages (abstracting away from their\nlogically/semantically irrelevant properties). In any case, the notion\nof logical form has played a significant role in recent work on\ntheories of meaning for natural languages. So an introductory\ndiscussion of logical form would not be complete without some hint of\nwhy such work is relevant, especially since attending to details of\nnatural languages (as opposed to languages invented to study the\nfoundations of arithmetic) led to renewed discussion of how to\nrepresent propositions that involve relations. Prima facie, ‘Every old patient respects some doctor’\nand ‘Some young politician likes every liar’ exhibit\ncommon modes of linguistic combination. So a natural hypothesis is\nthat the meaning of each sentence is fixed by these modes of\ncombination, given the relevant word meanings. It may be hard to see\nhow this hypothesis could be true if there are widespread mismatches\nbetween logical and grammatical form. But it is also hard to see how\nthe hypothesis could be false. Children, who have finite cognitive\nresources, typically acquire the capacity to understand the endlessly\nmany expressions of the languages spoken around them. A great deal of\nrecent work has focussed on these issues, concering the connections\nbetween logical form and the senses in which natural languages are\nsemantically compositional. It was implicit in Frege that each of the endlessly many sentences\nof an ideal language would have a compositionally determined\ntruth-condition. Frege did not actually specify an algorithm that\nwould associate each sentence of his Begriffsschrift with its\ntruth-condition. But Tarski (1933) showed how\nto do this for the first-order predicate calculus, focussing on\ninteresting cases of multiple quantification like\n‘∀x[Number(x) → ∃y[SuccessorOf(y, x) &\n∀z[SuccessorOf(z, x) → (z = y)]]]’. This made it\npossible to capture, with precision, the idea that an inference is\nvalid in the predicate calculus iff: every interpretation that makes\nthe premises true also makes the conclusion true, holding fixed the\ninterpretations of logical elements like ‘if’ and\n‘every’. Davidson (1967a) conjectured that one could do\nfor English what Tarski did for the predicate calculus; and Montague,\nsimilarly inspired by Tarski, showed how one could start dealing with\npredicates that have quantificational constituents. Still, many\napparent objections to the conjecture remained. As noted at the end of\nsection four, sentences like ‘Pat thinks\nthat Hesperus is Phosphorus’ present difficulties; though\nDavidson (1968) offered an influential suggestion. Davidson's (1967b)\nproposal concerning examples like (37–40) also proved enormously\nfruitful.  If (37) is true, so are (38–40); and if (38) or (39) is\ntrue, so is (40). The inferences seem impeccable. But the\nfunction-argument structures are not obvious. If we represent\n‘kissed quickly at midnight’ as an unstructured predicate\nthat takes two arguments, like ‘kissed’ or\n‘kicked’, we will represent the inference from (37) to\n(40) as having the form: K*(x, y); so K(x, y). But this form is\nexemplified by the bad inference ‘Juliet kicked Romeo; so Juliet\nkissed Romeo’. Put another way, if ‘kissed quickly at\nmidnight’ is a logically unstructured binary predicate, then the\nfollowing conditional is a nonlogical assumption: if Juliet kissed\nRomeo in a certain manner at a certain time, then Juliet kissed\nRomeo. But this conditional seems like a tautology, not an assumption\nthat introduces any epistemic risk. Davidson concluded that the\nsurface appearances of sentences like (37–40) mask relevant\nsemantic structure. In particular, he proposed that such sentences are\nunderstood in terms of quantification over events. According to Davdison, who echoed Ramsey (1927), the meaning of\n(40) is reflected in the paraphrase ‘There was a kissing of\nRomeo by Juliet’. One can formalize this proposal in various\nways: ∃e[KissingOf(e, Romeo) & KissingBy(e, Juliet)]; or\n∃e[Kiss(e, Juliet, Romeo)], with the verb ‘kiss’\nindicating a function that takes three arguments; or as in (40a),  with Juliet and Romeo explicitly represented as players of certain\nroles in an event. But given any such representation, adverbs like\n‘quickly’ and ‘at midnight’ can be analyzed as\nadditional predicates of events, as shown in (37a-39a).  If this is correct, then the inference from (37) to (40) is an\ninstance of the following valid form: ∃e[...e... & Q(e)\n& A(e))]; hence, ∃e[...e...]. The other impeccable\ninferences involving (37–40) can likewise be viewed as instances\nof conjunction reduction. If the grammatical form of (40) is simply\n‘{Juliet [kissed Romeo]}’, then the mapping from\ngrammatical to logical form is not transparent; and natural language\nis misleading, in that no word corresponds to the event\nquantifier. But this does not posit a significant structural mismatch\nbetween grammatical and logical form. On the contrary, each word in\n(40) corresponds to a conjunct in (40a). This suggests a strategy for\nthinking about how the meaning of a sentence like (40) might be\ncomposed from the meanings of the constituent words. A growing body of\nliterature, in philosophy and linguistics, suggests that Davidson's\nproposal captures an important feature of natural language semantics,\nand that “event analyses” provide a useful framework for\nfuture discussions of logical form. In one sense, it is an ancient idea that action reports like (40)\nrepresent individuals as participating in events; see Gillon's (2007)\ndiscussion of Panini's grammar of Sanskrit. But if (40) can be glossed\nas ‘Juliet did some kissing, and Romeo was thereby\nkissed’, perhaps the ancient idea can be deployed in developing\nLeibniz' suggestion that relational sentences like (40) somehow\ncontain simpler active-voice and passive-voice sentences; cp. Kratzer\n(1996). And perhaps appeals to quantifier raising can help in\ndefending the idea that ‘Juliet kissed some/the/every\nboy’ is, after all, a sentence that exhibits\nSubject-copula-Predicate form: ‘[some/the/every\nboy]i is P’, with\n‘P’ as a complex predicate akin to ‘[some\nevent]e was both a kissing done by Juliet and one in\nwhich hei was kissed’. With this in mind, let's return to the idea that each complex\nexpression of natural language has semantic properties that are\ndetermined by (i) the semantic properties of its constituents, and\n(ii) the ways in which these constituents are grammatically\narranged. If this is correct, then following Davidson, one might say\nthat the logical forms of expressions (of some natural language) just\nare the structures that determine the corresponding meanings given the\nrelevant word meanings; see Lepore and Ludwig (2002). In which case,\nthe phenomenon of valid inference may be largely a by-product of\nsemantic compositionality. If principles governing the meanings of\n(37–40) have the consequence that (40) is true iff an\nexistential claim like (40a) is true, perhaps this is illustrative of\nthe general case. Given a sentence of some natural language NL, the\ntask of specifying its logical form may be inseparable from the task\nof providing a compositional specification of what the sentences of NL\nmean. At this point, many issues become relevant to further discussions\nof logical form. Most obviously, there are questions concerning\nparticular examples. Given just about any sentence of natural\nlanguage, one can ask interesting questions (that remain unsettled)\nabout its logical form. There are also very abstract questions about\nthe relation of semantics to logic. Should we follow Davidson and\nMontague, among others, in characterizing theories of meaning for\nnatural languages as theories of truth (that perhaps satisfy certain\nconditions on learnability)? Is an algorithm that correctly associates\nsentences with truth-conditions (relative to contexts) necessary\nand/or sufficient for being an adequate theory of meaning? What should\nwe say about the paradoxes apparently engendered by sentences like\n‘This sentence is false’? If we allow for second-order\nlogical forms, how should we understand second-order quantification,\ngiven Russell's Paradox?  Are claims about the “semantic\nstructure” of a sentence fundamentally descriptive claims about\nspeakers (or their communities, or their languages)? Or is there an\nimportant sense in which claims about semantic structure are normative\nclaims about how we should use language? Are facts about the\nacquisition of language germane to hypotheses about logical form? And\nof course, the history of the subject reveals that the answers to the\ncentral questions are by no means obvious: what is logical structure,\nwhat is grammatical structure, and how are they related? Or put\nanother way, what kinds of structures do propositions and sentences\nexhibit, and how do thinkers/speakers relate them?","contact.mail":"pietro@umd.edu","contact.domain":"umd.edu"}]
