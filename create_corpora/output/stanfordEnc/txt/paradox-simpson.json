[{"date.published":"2021-03-24","url":"https://plato.stanford.edu/entries/paradox-simpson/","author1":"Jan Sprenger","author1.info":"http://www.laeuferpaar.de","author2.info":"https://sites.google.com/site/naftaliweinberger/","entry":"paradox-simpson","body.text":"\n\n\nSimpson’s Paradox is a statistical phenomenon where an\nassociation between two variables in a population emerges, disappears\nor reverses when the population is divided into subpopulations. For\ninstance, two variables may be positively associated in a population,\nbut be independent or even negatively associated in all\nsubpopulations. Cases exhibiting the paradox are unproblematic from\nthe perspective of mathematics and probability theory, but\nnevertheless strike many people as surprising. Additionally, the\nparadox has implications for a range of areas that rely on\nprobabilities, including decision theory, causal inference, and\nevolutionary biology. Finally, there are many instances of the\nparadox, including in epidemiology and in studies of discrimination,\nwhere understanding the paradox is essential for drawing the correct\nconclusions from the data.\n\n\nThe following article provides a mathematical analysis of the paradox,\nexplains its role in causal reasoning and inference, compares theories\nof what makes the paradox seem paradoxical, and surveys its\napplications in different domains.\n\nWe begin with an illustration of the paradox with concrete data. The\nnumbers in\n Table 1\n summarize the effect of a medical treatment for the overall\npopulation (N = 52), and separately for men and women: \nTable 1: Simpson's Paradox: the type of\nassociation at the population level (positive, negative, independent)\nchanges at the level of subpopulations. Numbers taken from\nSimpson’s original example (1951). \nFor matters of exposition, we assume that these frequencies are\nunbiased estimates of the underlying probabilities. The treatment\nlooks ineffective at the level of the overall population, but it leads\nto higher success percentages than the control both for men and for\nwomen (61% vs. 57% for men and 44% vs. 40% for women). Writing these\nproportions as conditional probabilities, with \\(\\r{T}\\)=treatment,\n\\(\\r{S}\\)=success/recovery, and \\(\\r{M}\\)=male subpopulation, we\nobtain  \nbut at the same time,  \nShould we use the treatment or not? When we know the gender of the\npatient, we would presumably administer the treatment, whereas it does\nnot look like the right thing to do when we don’t know the\npatient’s gender—although we know that the patient is\neither male or female! \nThis phenomenon was first pointed out in papers by Karl G. Pearson\n(1899) and George U. Yule (1903), but it was Simpson’s short\npaper “The interpretation of interaction in contingency\ntables” (1951), discussing the interpretation of such\nassociation reversals, that led to the phenomenon being labeled as\n“Simpson’s Paradox”. The phenomenon is, however,\nbroader than independence in the overall population and positive\nassociation in the subpopulations; for example, the associations may\nalso be reversed. Nagel and Cohen (1934: ch. 16) provide an example of\nsuch a reversal as part of a exercise for logic students. \nUnderstanding the paradox is essential for drawing the proper\nconclusions from statistical data. To give a recent example involving\nthe paradox (Kügelgen, Gresele, & Schölkopf [see\n Other Internet Resources]),\n early data revealed that the case fatality rate for Covid-19 was\nhigher in Italy than in China overall. Yet within every age group the\nfatality rate was higher in China than in Italy. One thus appears to\nget opposite conclusions about the comparative severity of the virus\nin the countries depending on whether one compares the whole\npopulations or the age-partitioned populations. Having a proper\nanalysis of what is going on is such cases is thus crucial for using\nstatistics to inform policy. \nIn what follows,\n Section 2\n explains different varieties of the paradox, clarifies the logical\nrelationships between them, and identifies precise conditions for when\nthe paradox can occur. While that section focuses on the mathematical\ncharacterization of the paradox,\n Section 3\n focuses on its role in causal inference, its implications for\nprobabilistic theories of causality, and its analysis by means of\ncausal models based on directed acyclic graphs (DAGs: Spirtes,\nGlymour, & Scheines 2000; Pearl 2000 [2009]). \nBased on these different approaches,\n Section 4\n discusses different analyses of what makes Simpson’s Paradox\nlook paradoxical, and what kind of error it reveals in human\nreasoning. This section also reports empirical findings on the\nprevalence of the paradox in reasoning and inference.\n Section 5\n surveys the occurrence and interpretation of the paradox in applied\nstatistics (regression models), philosophy of biology, decision theory\nand public policy. For example, Simpson’s Paradox is relevant\nwhen analyzing data to test for race or gender discrimination (Bickel,\nHammel, & O’Connell 1975).\n Section 6\n wraps up our findings and concludes. \nThis section shows how Simpson’s Paradox can be characterized\nmathematically, under which conditions it occurs, and how it can be\navoided. We begin by further considering the concrete example from the\nintroduction in order to build intuitions that will guide us through\nthe more technical results. \nThe data in\n Table 1\n can be translated into success or recovery rates, showing that\ntreated men have a higher recovery rate than untreated men (roughly\n61% vs. 57%), and the same for women (44% vs. 40%). Two observations\nare key to understanding why this positive association vanishes in the\naggregate data. First, the recovery rate of untreated men is still\nhigher than the recovery rate of women who receive treatment (57% vs.\n44%), suggesting that not only treatment, but also gender is a\nrelevant predictor of recovery. Second, while the treatment group is\nmajority female (27 vs. 13), the control group is majority male (7 vs.\n5). Speaking informally, the lack of population-level correlation\nbetween treatment and recovery results from men being both (i) more\nlikely to recover from the treatment, and (ii) less likely to be in\nthe treatment group. \nThis becomes evident when we use conditional probabilities to\nrepresent recovery rates given treatment and/or subpopulation. The\noverall recovery rates given treatment and control can, by the Law of\nTotal Probability, be written as the weighted average of recovery\nrates in the subpopulations:  \nPlugging in the numbers from\n Table 1\n to calculate the overall recovery rates via these equations, we see\nthat the first line is a weighted average of success rates for treated\nmen and women (61% and 44%) while the second line is a weighted\naverage of success rates of the two control groups (57% and 40%).\nThese averages are weighted by the percentage of males and females in\neach group, and in the present case the gender disparity between the\ngroups results in both averages being 50%. Since these weights can be\ndifferent, the treatment may raise the probability of success among males and females\nwithout doing so in the combined population. \nLater we will show that the positive association in the subpopulations\ncannot vanish if the correlation of treatment with gender is broken\n(e.g., by balancing gender rates in both conditions). The weights in\neach line are then identical—\\(p(\\r{M}\\mid \\r{T}) =\np(\\r{M}\\mid \\neg \\r{T})\\)—and associations in subpopulations\nare preserved for the aggregate data\n (Theorem 1 in Section 2.2).\n In fact, the absence of such a correlation rules out Simpson’s\nParadox. \nSimpson’s Paradox can occur for various types of data, but\nclassically, it is formulated with respect to \\(2\\times2\\) contingency\ntables. Let \\(D_i = (a_i, b_i, c_i, d_i)\\) be a four-dimensional\nvector of real numbers representing the \\(2\\times2\\) contingency table\nfor treatment and success in the i-th subpopulation, and let\n \nbe the aggregate data set over \\(N\\) subpopulations. These data should\nbe read as shown in\n Table 2. \nTable 2: Abstract representation of a\n\\(2 \\times 2\\) contingency table with subpopulations \\(D_1\\) and\n\\(D_2\\). \nLet \\(\\alpha (D_i)\\) be a measure the strength of the probabilistic\nassociation between \\(T\\) and \\(S\\) in population\n \\(D_i\\).[1]\n By convention, \\(\\alpha (D_i) = 0\\) corresponds to no association\nbetween the variables, \\(\\alpha (D_i) \\gt 0\\) indicates a positive\nassociation, and \\(\\alpha (D_i) < 0\\) a negative one. This can best\nbe translated into the condition  \nThe condition \\(a_i \\, d_i > b_i \\, c_i\\) is equivalent to saying\nthat the success rate in the first row (“treatment\ncondition”) is higher than the success rate in the second row\n(“control condition”):  \nApplying all this to our dataset in\n Table 1,\n we see that \\(\\alpha(D) = 0\\) although \\(\\alpha(D_1) > 0\\) and\n\\(\\alpha(D_2) > 0\\). This is a special case of what Samuels (1993)\ncalls Association Reversal (AR). Association reversal\noccurs if and only if there is a population such that the association\nin all partitioned subpopulations is either (i) positive (ii)\nnegative, or (iii) zero, and the type of association in the population\ndoes not match that of the subpopulations. Writing this out\nmathematically, this means for a dataset \\(D = \\sum_{i=1}^N D_i\\) that\none of the following two conditions holds,  \nwhere at least one of the inequalities has to be strict. Association\nreversal is the standard variety of Simpson’s Paradox\n(Bandyopadhyay et al. 2011; Blyth 1972, 1973) and also the one that is\nmost frequently investigated in the psychology of reasoning, or by\nphilosophers analyzing the paradox (e.g., Cartwright 1979; Eells 1991;\nMalinas 2001). \nAn important special case of AR occurs when there is no association in\nthe subpopulations, but an association emerges in the overall dataset:\n \nReferring to the pioneering work of the statistician George U. Yule\n(1903: 132–134), Mittal (1991) calls this Yule’s\nAssociation Paradox (YAP). It is typical of spurious\ncorrelations between variables with a common cause, that is, variables\nthat are dependent unconditionally (\\(\\alpha(D) \\ne 0\\)) but\nindependent given the values of the common cause (\\(\\alpha(D_i) =\n0\\)). For example, sleeping in one’s clothes is correlated with\nhaving a headache the next morning. However, once we stratify the data\naccording to the levels of alcohol intake on the previous night, the\nassociation vanishes: given the same level of drunkenness, people who\nundress before going to bed will have the same headache, ceteris\nparibus, as those who kept their clothes on. \nFinally, the most general version of Simpson’s Paradox is the\nAmalgamation Paradox (AMP) identified by Good and\nMittal (1987). This paradox occurs when the overall degree of\nassociation is bigger (or smaller) than each degree of association in\nthe subpopulations, or mathematically,  \nAMP challenges the intuition that the degree of association in the\ngeneral population, in virtue of being “the sum” of the\nindividual subpopulations, has to fall in between the minimal and the\nmaximal degree of association observed on that level. The logical\nstrength of the paradoxes is inversely related to their generality and\nfrequency of occurrence: \\(\\text{YAP} \\Rightarrow \\text{AR}\n\\Rightarrow \\text{AMP}\\). Variations of the paradox for\nnon-categorical data (e.g., bivariate real-valued data) will be\ndiscussed in\n Section 5.1. \nWe proceed to characterizing the mathematical conditions under which\nSimpson’s Paradox occurs. We have already suggested that the\nparadox arises in the medical example due to correlations between the\ntreatment variable and the partitioning variable, and we can now make\nthis more precise: \nTheorem 1 (Lindley & Novick 1981; Mittal\n1991): If \\(\\alpha(D) > 0\\) and association reversal occurs\nfor the subpopulations characterized by attribute \\(\\r{M}\\) and\n\\(\\neg\\r{M}\\), (i.e., \\(\\alpha(D_1), \\alpha(D_2) \\le 0\\)), then\neither \nAs Theorem 1 makes clear, the lack of correlation between \\(\\r{M}\\)\nand \\(\\r{T}\\) is sufficient to rule out association reversals (and\nthus YAP as well). Does it also rule out the more general amalgamation\nparadox? The answer to this depends on which measure of\nassociation one chooses for \\(\\alpha\\). Discussions of\nSimpson’s Paradox commonly treat association as the\ndifference in the success rate between the treated and the\nuntreated, but this is only one of many possibilities (Fitelson 1999).\nWhile the lack of association between \\(M\\) and \\(T\\) is sufficient to\nrule out AMP for most measures (including the difference measure) it\ndoes not rule it out for all measures, as we will now explain. Readers\nnot interested the specific details may skip to the following\nsection. \nHere are some widely used association measures for a dataset \\((a, b,\nc, d)\\):  \nSome of these measures can be formulated probabilistically and have\nbeen suggested as measures of causal strength and outcome measures for\nclinical trials (Edwards 1963; Eells 1991; Fitelson & Hitchcock\n2011; Greenland 1987; Peirce 1884; Sprenger 2018; Sprenger &\nStegenga 2017). For example, \\(\\pi_{D} = p(\\r{S}\\mid \\r{T}) -\np(\\r{S}\\mid \\neg \\r{T})\\) represents the difference and \\(\\pi_R =\np(\\r{S}\\mid \\r{T}) / p(\\r{S}\\mid \\neg \\r{T})\\) the ratio of success\nrates in treatment and control conditions. \\(\\pi_W\\) can be\ninterpreted as the prognostic weight of evidence that treatment\nprovides for success (i.e., as the log-Bayes factor), \\(\\pi_{Y}\\) is\nYule’s (1903) measure of association, \\(\\pi_{O}\\) is the\nlog-odds ratio familiar from epidemiological data analysis, and\n\\(\\pi_C\\) I.J. Good’s (1960) measure of causal strength. \nWe now consider the extent to which AMP for different measures is\nruled out by different experimental designs. Suppose that individuals\nare uniformly assigned to the treatment and control condition across\nsubpopulations. In such a case, where the ratio of persons assigned to\nthe treatment and control condition is equal for each subpopulation,\nthe experimental design is called row-uniform.\nSpecifically, there has to be a \\(\\lambda > 0\\) such that for any\nsubpopulation i  \nIn particular, row uniformity holds approximately if our sample is\nlarge and we sample at random from the population. \nRow-uniform design of a trial ensures independence between a potential\nconfounder \\(M\\) and the treatment variable \\(T\\). Accordingly, by\n Theorem 1,\n it rules out association reversals. Additionally, row-uniform design\nis sufficient to rule out the AMP for a wide class of association\nmeasures: \nTheorem 2 (Good & Mittal 1987): If a\ndataset \\(D = \\sum D_{i}\\) satisfies row uniformity, then the\nAmalgamation Paradox is avoided for the measures \\(\\pi_{D}\\),\n\\(\\pi_{R}\\), \\(\\pi_{Y}\\) and \\(\\pi_{W}\\) and \\(\\pi_{C}\\). It is\nnot avoided for the log-odds ratio \\(\\pi_{O}\\). \nSome studies also exhibit column-uniform design where\nthe proportion of successes and failures is constant across all\nsubpopulations:  \nAlso then \\(\\r{M}\\) is independent of \\(\\r{S}\\). Column uniformity can\noccur in case-control studies with various subpopulations (e.g.,\ndifferent hospitals) where one does not match the number of persons\nwith the explanatory attribute, like in an RCT. Instead, for each\nperson with a certain attribute (e.g., a specific form of cancer), one\nselects a number of persons that does not have this attribute.\nColumn-uniform design avoids AR as well, but among the presented\nassociation measures, it suffices to rule out AMP only for\n\\(\\pi_Y\\). \nTable 3: An overview of how row- and\ncolumn-uniform design avoid the amalgamation paradox for various\nassociation measures. \n\n Table 3\n summarizes the properties of all association measures with respect to\nthe AMP and the different forms of experimental design. The behavior\nof the log-odds measure \\(\\pi_O\\), where neither row- nor\ncolumn-uniform design suffices to rule out the AMP, will be discussed\nin\n Section 5.2. \nWe now identify one last fundamental condition for when data exhibit\nassociation reversal. Have a look at\n Figure 1\n which displays the success proportions for treatment and control\ngraphically. \nFigure 1: A geometrical representation\nof a necessary condition for the occurrence of Association Reversal.\nThe paradox can occur if the proportions are ordered like in the left\ngraph; it cannot occur if they are ordered like in the right graph.\n[An\n extended description of figure 1\n is in the supplement.] \nIn both examples, the treatment success rate is for both\nsubpopulations greater than the control success rate. When will this\norder be preserved at the overall level? We know that the overall\nsuccess rate for each condition (treatment/control) is\nconstrained by the success rates in the subpopulations: \nFact 1: Suppose \\(a_i, b_i > 0\\) for all\n\\(1 \\le i \\le N\\). Then also  \nThis fact follows directly from the Law of Total Probability (proof\nomitted) and it gives us a simple necessary condition for the\noccurrence of Association Reversal (AR): turning to\n Figure 1\n again, it implies that the overall success rate per condition has to\nbe on the solid lines. Thus AR cannot occur in the right part\nof\n Figure 1,\n but it can occur if the proportions are ordered as in the left part\nof\n Figure 1.\n Generally, AR is avoided when the following condition holds:  \nAny dataset that satisfies\n (RH)\n will be called row-homogenous. By contrast, for any\ngiven set of proportions violating condition\n (RH),\n we can find datasets exhibiting these very same proportions such that\nAR indeed occurs (by fiddling with the size of the subpopulations;\nLemma 3.1 in Mittal 1991). However, neither row homogeneity, nor the\nanalogous condition of column homogeneity, nor their conjunction is\nsufficient for avoiding the amalgamation paradox AMP. \nFinally, one might be interested in how frequently the paradox arises.\nSimulations by Pavlides and Perlman (2009) suggest that it should not\noccur frequently: the confidence interval for the probability of AR is\na subset of the interval \\([0;0.03]\\) for both the uniform prior and\nthe (objective) Jeffreys prior. Of course, the practical value of this\ndiagnosis depends on whether the sampling assumptions are sensible,\nand whether the entire approach makes sense for real-life datasets\nwhere researchers can group the data into subpopulations along\nnumerous dimensions. \nWithin the philosophical literature, Simpson’s Paradox received\nsustained attention due to its implications for accounts of causality\nthat posit systematic connections between causal relationships and\nprobability-raising. Specifically, the paradox reveals that facts\nabout probability-raising will not necessarily be preserved when one\npartitions a population into subpopulations. This poses a number of\nimportant challenges to philosophical accounts of causal inference\nbased on the concept of probability: \nStrategies for treating the paradox and answering these questions have\ncontributed substantially to the development of theories of\nprobabilistic causality (Cartwright 1979; Eells 1991). A different set\nof answers is provided by more recent work on the paradox in the\nframework of graphical causal models (e.g., Pearl 1988, 2000 [2009];\nSpirtes et al. 2000), and we will discuss both accounts in turn. In\nparticular, we will explain how Simpson’s Paradox can be\nanalyzed through the notions of confounding and the identifiability of\na causal effect. \nEarly accounts of probabilistic causation (e.g., Reichenbach 1956;\nSuppes 1970) sought to explicate causal claims purely in terms of\nprobabilistic and temporal facts. On Suppes’ (1970) account,\nevent \\(\\r{C}\\) is a prima facie cause of \\(\\r{E}\\) if and\nonly if (i) \\(\\r{C}\\) occurs before \\(\\r{E}\\) and (ii) \\(\\r{C}\\)\nraises the probability of\n \\(\\r{E}\\).[2]\n As we have already seen in\n Section 2.1,\n not all prima facie causes are genuine causes. If I drink a\nstrong blond Belgian beer now, I will probably be happy during the\nday, but also have a headache tomorrow. However, being happy would not\nthereby by the cause of the headache: the correlation is explained by\nthe common cause—the beer drinking. The variable for drinking\nthe beer screens off the probabilistic relationship between\nits effects, meaning that the effects will be uncorrelated when one\nconditions on it. The crux of Suppes’ account is that a\nprima facie causal relationship between \\(\\r{C}\\) and\n\\(\\r{E}\\) is a genuine causal relationship iff there is no factor F\nprior to C that screens off \\(\\r{C}\\) from\n \\(\\r{E}\\).[3] \nLater theorists such as Cartwright (1979) and Eells (1991) developed\nthis condition by making causal claims relative to a causally\nhomogenous background context, which is specified by a set of\nvariables \\(\\b{K}\\). Consider the following example of association\nreversal presented by Cartwright. Supposing that smoking \\((\\r{S})\\)\nis a cause of heart disease \\((\\r{H})\\), one might expect that smoking\nwould raise the probability of heart disease. Yet this might not be\nthe case. Suppose that in a population there is a strong correlation\nbetween smoking and exercising (X), and that exercise lowers the\nprobability of heart disease by more than smoking raises its\nprobability. In such a case, smoking might lower the probability of\nheart disease although conditional on either \\(X\\) or \\(\\neg X\\),\n\\(\\r{S}\\) raises \\(\\r{H}\\)’s probability. \nCartwright interprets this case as follows: causes always raise the\nprobability of their effects, but this can be “concealed”\nby the correlation between the cause and some other variable (here,\n\\(X\\)). In order to isolate the genuine probabilistic relationship\nbetween \\(\\r{C}\\) and \\(\\r{E}\\), one needs to consider it in a context\nwhere such correlations cannot occur: \nProbabilistic Causality (Cartwright) Let\n\\(\\b{K}\\) denote all and only the causes of \\(\\r{E}\\) other than\n\\(\\r{C}\\) and effects of \\(\\r{C}\\). Then \\(\\r{C}\\) causes\n\\(\\r{E}\\) if and only if relative to all combinations of values\nvariables in \\(\\b{K}\\), \\(\\r{C}\\) raises the probability of \\(\\r{E}\\):\n\\(p(\\r{C}\\mid \\r{E},\\b{K}) > p(\\r{C}\\mid\n\\neg{\\r{E},\\b{K}})\\). \nWhile Suppes defends a reductive account of probabilistic\ncausality, where the elements of \\(\\b{K}\\) are determined without\nappeal to causal assumptions, Cartwright presents a\nnon-reductive account where \\(\\b{K}\\) must include all and\nonly the causes of \\(\\r{E}\\), excluding \\(C\\) itself and any variables\nthat are causally intermediate between \\(\\r{C}\\) and \\(\\r{E}\\). The\ncurrent consensus is that it is impossible to give a probabilistic\naccount of causation without relying and causal concepts, and thus\nthat no non-reductive account is feasible (though see Spohn 2012 for\na dissenting view). \nAlthough non-reductive accounts could not be used to explain causation\nto someone with no prior causal knowledge, they can nevertheless\nclarify how causal claims are tested, and illuminate the relationship\nbetween causation and probability (see also Woodward 2003:\n20–22). Moreover, Cartwright argues that her general criterion\nfor inclusion of background factors in \\(\\b{K}\\) avoids the\nreference class problem for purely statistical\naccounts of causal explanation (i.e., by specifying the relevant\npopulations for evaluating causal claims), thereby eliminating a\nthreat to the objectivity of causal explanation. More detail is\nprovided in the\n entry on probabilistic causality. \nCartwright’s innovations for probabilistic accounts of causality\nhave triggered various debates related to Simpson’s Paradox. We\nhighlight three of them here: \nCartwright claims that causes raise the probabilities of their effects\nacross all background\n contexts,[4]\n but many purported causes only raise the probabilities of their\neffects in some contexts. In the latter cases, causes\ninteract with background factors in producing their effects.\nTo give Cartwright’s own example (1979: 428), ingesting an acid\npoison generally causes death, except in contexts where one also\ningests an alkali poison (in which case the two cancel one another\nout). The problem of such interactive causes for probabilistic\naccounts is that they threaten Cartwright’s picture on which the\neffect of probability-raising causes is “concealed” by a\nstronger negative cause which “dominates” them. These\nmetaphors suggest that the probability-raising relationship between a\ncause and its effect reflects an intrinsic relationship between the\nvariables that exists even when not manifested, an idea further\ndeveloped in (Cartwright 1989). Interaction means that causes do not\noperate in a vacuum, but rather only in the presence of background\nfactors (for further discussion, see Otte 1985; Eells 1986; Hardcastle\n1991). \nSimpson’s Paradox should not be conflated with causal\ninteraction, however. What is distinctive of the paradox is not that\nthe probabilistic relationship reverses upon partitioning, but rather\nthat it reverses in all of the resulting subpopulations. \nCartwright requires \\(\\b{K}\\) to include all causes of \\(\\r{E}\\), and\nthus to evaluate effects relative to homogenous background contexts.\nThe account thus does not allow for average effects. For example,\nsuppose that a particular treatment \\((\\r{T})\\) raises the probability\nof heart disease \\((\\r{H})\\) in individuals who were born prematurely\n\\((\\r{P})\\) but not individuals who were not, and that \\(\\r{P}\\) is\nnot correlated with \\(\\r{T}\\). In the whole population, the amount by\nwhich \\(\\r{T}\\) lowers or raises the probability of \\(H\\) will be an\naverage of the effects in the \\(\\r{P}\\) and \\(\\neg\\r{P}\\) populations,\nweighted by their size. Dupré (1984) argues for abandoning the\nrequirement that \\(\\b{K}\\) include all causes of \\(E\\), and thus for\nallowing average effects. \nA tempting lesson to draw from our opening example is that\nSimpson’s Paradox arises as a result of averaging over the\npopulations of males and females, and that the only way to eliminate\nit is by ruling out average effects. However, causal heterogeneity\ndoes not by itself lead to the\n paradox.[5]\n Cases with heterogeneous background factors only produce association\nreversal if the factors are correlated with the causal\nvariable—as demonstrated by\n Theorem 1\n in\n Section 2.2. \nAccording to Cartwright, the set \\(\\b{K}\\) should not include\nvariables that are causally intermediate between \\(C\\) and \\(E\\). Such\nvariables are called mediators. To see why, imagine a\ndrug reduces the risk of heart disease by producing a chemical,\nrepresented by variable \\(Z\\), in the blood stream, and via no other\nfactors. If \\(C\\) and \\(E\\) have no common causes, they will be\nprobabilistically independent conditional on \\(Z\\). Intuitively, one\nshould not hold the blood chemical fixed in evaluating the effect,\nsince it is the means by which the effect is brought about. \nWhen there are multiple paths between cause and\neffect, the question becomes more complex. Hesslow (1976)\nprovides an example where taking birth control pills promotes a\nblood-clotting condition called thrombosis via a chemical in the\nblood, but inhibits it via preventing pregnancy, which itself is a\ncause of thrombosis. As a result, taking birth control intuitively\ninfluences thrombosis both positively and negatively. If one is\ninterested in the net effect of \\(C\\) on \\(E\\)—as opposed to the\neffects via particular paths (Hitchcock 2001)—then one should\nnot condition on mediators. However, conditioning is necessary for\ncalculating path-specific effects (e.g., Pearl 2001; Weinberger\n2019). \nDistinguishing mediators from common causes is crucial for analyses of\nSimpson’s Paradox. For example, the causal models \\(C\\to Z \\to\nE\\) and \\(C\\leftarrow Z \\to E\\) exhibit the same conditional\nindependencies: \\(C\\) and \\(E\\) will be associated unconditionally,\nbut independent conditional on \\(Z\\). Only causal knowledge enables us\nto decide how we shall deal with the association reversal, and whether\nwe need to condition upon \\(Z\\) when estimating the causal effect of\n\\(C\\) on \\(E\\) (we do in the second model, but not in the first). See\nalso\n Section 3.4. \nIn recent years, the formal analysis of causation has been\nsignificantly enhanced by the development of graphical methods for\nrepresenting causal hypotheses and for choosing among candidate\nhypotheses given one’s evidence, in particular those using directed\nacyclic graphs (=DAGs: Pearl 1988, 2000 [2009]; Spirtes et\nal. 2000). A DAG contains a set of nodes connected by a set of\ndirected edges or arrows such that there are no cycles (one cannot get\nfrom a node back to itself via a set of directed arrows). In the\ncausal context, the nodes in a DAG are random variables and the arrows\ncorrespond to direct causal relationships. It is common to assume that\nthe set of variables in a DAG is causally sufficient, meaning\nthat it includes all common causes of variables in the set. \nDAGs enable one to systematically map the relationship between causal\nhypotheses and joint probability distributions. They overlap with and\nbuild on techniques in the literature on probabilistic causality, but\nprovide significantly stronger tools and results. See the entries on\n causal models,\n causation and manipulability and\n counterfactual theories of causation\n for detailed introductions to causal inference with DAGs. \nFigure 2: The relationship between the\nvariables Treatment, Gender, and Success\nrepresented in a DAG, without and with an intervention on\nTreatment. \n\n Figure 2\n (left part) presents a plausible DAG for our running example,\nincluding the variables Treatment, Gender, and\nSuccess. There are two ways in which Treatment\nprovides information about Success. One is that people who\ntake the treatment may be more (or less) likely to recover as a result\nof having taken it. The other is that learning that someone took the\ntreatment provides information about whether they are likely to be\nmale or female, and this information is relevant to determining\nwhether they will recover regardless of whether they took the\ntreatment. \nThe graphs can, however, also be interpreted causally, and here, the\nnotion of an ideal intervention is crucial:\n\n \nFor an intervention on a variable \\(V\\) to be ideal\nis for it to determine \\(V\\)’s value such that it no longer\ndepends on its other causes in the DAG. Graphically, we can represent\nan intervention by adding an additional cause I that\n“breaks” all of the arrows that would otherwise go into\n\\(V\\). So, in Figure 2 \nIntervention is an ideal intervention on Treatment.\nIntervening on Treatment disrupts the evidential relationship\nwith Gender—for example, by controlling for the\nproportion of male and female patients in each sample—so that\nany remaining probabilistic relationship between treatment and\nrecovery can only be explained by having taken the treatment. Such an\nexperimental design, where Treatment and Gender are\nmade probabilistically independent, suffices to rule out association\nreversal (cf.\n Section 2.2). \nUsing the notion of an ideal intervention, one can explicate causation\nas follows (Pearl 2000 [2009]; Woodward 2003). \\(C\\) causes \\(E\\) if\nand only if it is possible to change the value or probability of \\(E\\)\nvia some ideal intervention on \\(C\\). Such interventions distinguish\nbetween causal and merely probabilistic dependencies by eliminating\nany probabilistic relationship between \\(C\\) or \\(E\\) that can be\ntraced to the influence of a common cause. This does not mean,\nhowever, that one can only get causal knowledge in cases where one can\nexperimentally intervene. One of the key contributions of graphical\ncausal models is that they enable one to systematically determine when\none’s prior causal knowledge licenses one to interpret a\nparticular probabilistic relationship causally. \nThe difference between the probability distributions resulting from\nconditioning and from intervening is formally represented by\nsupplementing the probability calculus with the\ndo-operator (\\(\\do(X)\\)) where\napplying the operator to a variable formally represents intervening\nupon it. Taking \\(T\\), \\(S\\), and \\(M\\) to denote\nTreatment, Success, and Gender, and given\nthe graph in\n Figure 2,\n \nthe observational probability distribution of \\(S\\) given \\(T\\) is not equal to\nthe probability distribution of \\(S\\) given an intervention on \\(T\\):  \nThe difference between these two quantities is due to the impact of\n\\(M\\) on the distribution of \\(T\\). In contrast, the following two\nexpressions are equivalent given the DAG: \nHere one can infer the effect of \\(T\\) on \\(S\\) from the observational\ndistribution by conditioning on \\(M\\). In such a case, we say that\n (4) \nidentifies the causal effect of \\(T\\) on \\(S\\). More\ngenerally, identifiability is a relationship between\na DAG G, probability distribution \\(P\\) and a causal quantity\n\\(\\r{Q}\\), such that \\(\\r{Q}\\) is identifiable if and only if it is\nuniquely determined by \\(P\\) given \\(G\\). By contrast, when there are\nunmeasured common cause(s) of \\(S\\) and \\(T\\), the probability\ndistribution is compatible with any possible distribution for\n\\(p(\\r{S}\\mid \\do (\\r{T}))\\). \nThe concept of identifiability is crucial for understanding\nconfounding, and the analysis of Simpson’s Paradox through\ngraphical causal models. The relationship between \\(X\\) and \\(Y\\) is\nconfounded relative to variable set \\(\\b{Z}\\) just in\ncase \\(P(Y\\mid X,\\b{Z}) \\ne P(Y\\mid \\do(X),\\b{Z})\\) (i.e., the\nrelationship is not identified). A confounding set of\nvariables is one that biases the effect measurement.\nFor instance, an unmeasured common cause is a confounder because it\nmakes it impossible to differentiate the probabilistic dependence\nbetween the variables resulting from the common cause from that\nresulting from a causal relationship between them. Simpson’s\nParadox emerges on this account due to confounding by the third\nvariable. This notion of confounding can diverge from a common\ncolloquial understanding of confounders as alternative explanations of\nan observed outcome other than the treatment. \nA useful sufficient condition for identifiability is the\nback-door criterion (Pearl 1993, 2000 [2009: 79]).\nFirst we need to introduce some graphical terminology. A path\nbetween \\(X\\) and \\(Y\\) be a set of connected edges between \\(X\\) and\n\\(Y\\) going in any direction. \\(Y\\) is a descendant of \\(X\\)\nif there is a path from \\(X\\) to \\(Y\\) in which all the arrows go in\nthe same direction. When \\(X\\) and \\(Y\\) are connected via a single\npath including a common cause such as \\(X \\leftarrow Z \\rightarrow\nY\\), \\(X\\) and \\(Y\\) will\n typically[6]\n be unconditionally probabilistically dependent, but will be\nindependent conditional on \\(Z\\). For such a path, we say that \\(Z\\)\nblocks the path between \\(X\\) and \\(Y\\). In contrast, when\n\\(X\\) and \\(Y\\) are connected by a path including a common\neffect, such as \\(X \\rightarrow Z \\leftarrow Y\\), then the\npath will be blocked provided that one does not condition on\n\\(Z\\) or a descendant of \\(Z\\). This reflects the fact that\nindependent causes of a common effect will typically be dependent\nconditional on a common effect. An effect of \\(X\\) on \\(Y\\) is\nidentifiable if there are no unblocked “back-door paths”\nbetween \\(X\\) and \\(Y\\): all paths that pass through common causes are\nblocked, and all other paths excepting those by which the cause\ninfluences its effect are open. \nBack-door Criterion (Pearl 1993) Given a\nvariable pair \\(\\{X,Y\\}\\) in a DAG G, the effect of \\(X\\) on \\(Y\\) is\nidentifiable if there exists a variable set \\(\\b{Z}\\) in G\nsatisfying the following conditions: \nIn this case, the effect of \\(X\\) on \\(Y\\) is identified by the\nformula  \n\n Equation (5)\n reveals that can be possible to derive a causal effect in a\npopulation by averaging over the effects in subpopulations partitioned\nby \\(Z\\). This is what we already saw in\n Section 2.2:\n if there is no dependence between being treated and being a part of a\nsubpopulation, associations cannot reverse at the general population\nlevel. Yet such a derivation is only licensed by causal assumptions\nabout the relationships between the variables. The reader can verify\nthat given the DAG in\n Figure 2,\n the variables satisfy the back-door criterion (with\n \\(\\b{Z}=\\{\\textit{Gender}\\}\\)).[7] \nIn our original example, the treatment increased the probability of\nrecovery in each subpopulation, but not it in the population as a\nwhole. Should one approve the drug or not? The causal approach makes\nit easy to see why one should. The probabilistic relationship between\nTreatment and Success in the population is an\nevidential rather than a causal one. Learning that someone took the\ndrug provides evidence about their gender, and this information is\nrelevant to predicting whether they will recover. But this does not\ntell one about whether the drug is causally efficacious. To learn\nthis, one needs to know how the chances of recovery for individuals in\nthe population would change given an intervention on\ntreatment. This can be determined by conditioning on gender\nwhich enables one both to learn the gender-specific effects of the\ndrug, and to derive the average effect in the whole population (using\nthe back-door criterion). \nFigure 3: The DAG for the variables\nTreatment, Gender and Success (third\nvariable = confounding factor), contrasted with the DAG for the\nvariables Birth Control, Pregnancy, and\nThrombosis (third variable = mediator). \nThus, whether one should partition the population based on a factor in\norder to identify a particular causal relationship does not depend\nonly on the statistical distribution, but crucially on one’s\ncausal background assumptions. Suppose that one was considering an\nintermediate variable such as Pregnancy in Hesslow’s\n(1976) example. Recall that in the example birth control influences\nthrombosis both positively via a blood chemical and negatively by\nreducing one’s chance of getting pregnant. This case is shown in\n Figure 3\n and contrasted with our running example where the third variable is a\nconfounding factor. In order to identify the effect of birth control\non thrombosis, it is crucial that one does not condition on\npregnancy. If there are no unmeasured common causes of birth control\nand thrombosis, then a probability-raising relationship between birth\ncontrol and thrombosis in the population as a whole would reliably\nindicate that taking birth control pills promotes thrombosis. \nIt is worth emphasizing that there is no basis for distinguishing the\ntwo causal structures in\n Figure 3\n using statistics alone. Any data generated by the model on the left\ncould also have been generated by a model with the causal structure of\nthat on the right. Accordingly the judgment that one should partition\nthe population in one case but not the other cannot be based on the\nprobabilities alone, but requires the additional information supplied\nby the causal model. \nCoherent with\n Theorem 1,\n Pearl proves a causal version of Savage’s (1954) sure-thing\nprinciple (see also\n Section 5.3): \nCausal sure-thing principle (Pearl 2016) An\naction \\(\\r{C}\\) that increases the probability of an event \\(\\r{E}\\)\nin each subpopulation must also increase the probability of \\(\\r{E}\\)\nin the population as a whole, provided that the action does not change\nthe distribution of the\n subpopulations.[8] \nFor example, if one assumes that Gender is not an effect of\nTreatment, it cannot be the case that the drug raises the\nprobability of recovery in both males and females, but has no effect\non recovery in the general population. This result provides an error\ntheory for why people often find Simpson’s Paradox to be\nparadoxical in the first place. Specifically, Pearl (2000 [2009],\n2014) claims that people conflate observational claims that \\(\\r{X}\\)\nraises the probability of \\(\\r{Y}\\) with causal claims that\ndoing \\(\\r{X}\\) (versus \\(\\neg\\r{X}\\)) would raise the\nprobability of \\(\\r{Y}\\). And assuming that the partitioning variable\nis not an effect of \\(X\\), it is impossible for doing\n\\(\\r{X}\\) to raise the probability of \\(\\r{Y}\\) in all subpopulations,\nbut not in the population as a whole. So Pearl’s explanation of\nthe paradox is that people conflate causal and non-causal expressions,\nand if the conditional probabilities in the examples are interpreted\ncausally, Simpson’s reversals are impossible. \nWhether Pearl provides the correct causal explanation of\nSimpson’s Paradox remains a topic of continued debate (Armistead\n2014 see also\n Section 4).\n What should not, however, be controversial is that recent causal\nmodeling techniques enable one to systematically distinguish between\ncausal and probabilistic claims in a much more general and precise way\nthan had previously been possible. While Cartwright required that all\ncauses of \\(E\\) be included in the background context, for the sake of\neliminating confounding it is only necessary to hold fixed\ncommon causes (and other variables needed to block back-door\npaths). Theorists of probabilistic causality were to some extent aware\nthat one did not need to hold fixed all causes of the effect in order\nto eliminate confounding, but they lacked a general account of which\nvariable sets are sufficient for identifying the effect.\nSimpson’s Paradox was especially threatening, since there was no\nway to provide general conditions under which an apparent positive\ncausal relationship in a population would disappear entirely upon\npartitioning. Using Pearl’s framework, it is trivial to show\nthat as long as one does not condition on mediators, if a\nprobabilistic expression identifies an average positive effect between\n\\(X\\) and \\(Y\\) in a population, intervening on \\(X\\) must raise\n\\(Y\\)’s effect in at least some subpopulations\n(Weinberger 2015). \nTurning back to the debate about average effects in the probabilistic\nframework, this fact vindicates Dupré’s (1984) liberal\nattitude toward average effects against critics such as Eells and\nSober (1983: 54) who dismiss it as a “sorry excuse for a causal\nconcept” (though see Hitchcock 2003: 13–15, and Hausman\n2010: 56, for further nuances). Of course, a positive average effect\nis compatible with the cause lowering the probability of the effect\nsignificantly in many subpopulations. This reflects the fact that the\npartitioning variable(s) could interact with the cause of interest.\nBut such possible interactions do not make the effect any less genuine\nas an average effect for the whole population. \nThis brings us to the issue of whether Simpson’s Paradox\nthreatens the objectivity of causal relationships. Properly\nunderstood, it does not. It is certainly true that a cause can raise\nthe probability of its effect in one population and lower it in\nanother, or that it can have a positive effect in a whole population,\nbut not in some of its subpopulations. But it is not as if only some\nof these causal relationships are genuine and that philosophers must\ntherefore find a privileged background context within which the true\nrelationship is revealed. It is simply a fact about causation that\ndifferent populations can have different sets of interactive\nbackground factors, and thus the average effects will genuinely differ\nacross the populations. \nSimpson’s Paradox is not a paradox in the sense of presenting an\ninconsistent set of plausible propositions of which at least one must\nbe rejected. As shown in\n Section 2.2,\n mathematics does not rule out associations to be reversed at the\nlevel of subpopulations. Bandyopadhyay et al. (2011) helpfully\ndistinguish between three questions one could ask about\nSimpson’s Paradox: \nQuestion (i) is essentially a question about the psychology of\nreasoning: one must offer an account of why the (mathematically\ninnocent) association reversals seem paradoxical to many. Such\naccounts help to identify valid forms of inference that leads\nindividuals to mistakenly rule out association reversals, and thereby\nprovide answers to question (ii). Such analyses can differentiate\namong subtly different forms of reasoning, and open the door to\nempirical work testing whether humans systematically fail to attend to\nparticular differences. \n\n Section 3.4\n already presented one analysis of the paradox. On Pearl’s\ncausal analysis, the appearance of a paradox results\nfrom a conflation between causal and probabilistic reasoning.\nIf one interprets the claim that taking the drug raises the\nprobability of recovery as the causal statement that intervening to\ngive the drug will make patients more likely to recover, and plausibly\nassumes that taking the drug has no influence on gender, then the drug\ncannot lower the probability of recovery both among males and among\nfemales. But, of course, if one is considering ordinary conditional\nprobabilities without any do-operators, such reversals can occur.\nAccordingly, the appearance of paradox results from conflating\nordinary conditional probabilities with conditional probabilities\nrepresenting the results of interventions. \nPearl’s answer to (ii) has immediate implications for (iii). In\nevaluating the relationships between two variables \\(X\\) and \\(Y\\) and\ndetermining whether one should partition based on some variable (or\nvariable set) \\(Z\\), one should partition based on \\(Z\\) only if doing\nso will enable one to identify the causal relationship between\n\\(X\\) and \\(Y\\). This answer presupposes that the aim of\npartitioning the population is to identify causal relationships.\nQuestions about how to proceed in light of the paradox only make sense\ngiven a context and given the kind of inference one wishes to\ndraw. \nPearl (2014) presents several reasons supporting his analysis of the\nparadox. First, he argues that were the surprise resulting from the\nparadox to be the result of a mere mathematical error, this could\nneither account for why the paradox “has captured the\nfascination of statisticians, mathematicians, and philosophers for\nover a century” (2014: 9) nor for the difficulty that reasoners\nhave in avoiding the error even once they’ve been made aware of\nit. Only by means of a causal semantics can one demonstrate that\nSimpson’s reversals cannot occur when the conditional\nprobabilities are interpreted causally. Second, he points to\nSimpson’s (1951) observation that judgments about whether the\naggregated or non-aggregated population is relevant for evaluating the\ncorrelations depends on the story behind the what the\nfrequencies represent. Pearl accounts for this story-relativity by\nshowing that whether one should partition a population is decided not\nby the probabilities but rather by the causal model generating the\nprobabilities. These causal models\n\ncannot be distinguished by conditional probabilities alone. \nBandyopadhyay et al. (2011) reject Pearl’s causal analysis of\nthe paradox, and defend an alternative mathematical\nexplanation. They note that there can be instances of the\nparadox that do not seem to invoke any causal notions. For example,\nsuppose we take the proportions in Table 1 not to refer to the\nproportions of recovering/non-recovering patients among the\ntreatment/non-treatment groups in male and female populations, but\nrather to the proportions of red and blue marbles among big or small\nmarbles in two bags. Suppose that in either bag the big marbles have a\nhigher red-to-blue ratio than the small marbles. Bandyopadhyay et\nal. plausibly claim that in this case, it would be surprising to\ndiscover that, were we to pour the bags into a single box, the small\nmarbles have a higher red-to-blue ratio than the big ones. If there\nare cases of the paradox that still exhibit surprise despite having\nnothing to do with causality, then the general explanation of the\nparadox cannot be causal.[9] \nBandyopadhyay et al. rephrase the paradox as being about ratios and\nproportions: when it is the case that  \n—to be read as success proportions for treatment and control in\nthe subpopulations, compare\n Table 2—many\n people expect that these equalities are preserved in the overall\npopulation:  \nAs we know from\n Section 2,\n this need not be the case. Bandyopadhyay et al. conducted a survey\nwith university students on this matter: only 12% give the correct\nanswer that equations\n (6),\n by themselves, do not constrain the truth value of equation\n (7). \nGiven the widespread literature revealing how seemingly error-prone\nhumans can be when reasoning about probabilities (e.g., Kahneman,\nSlovic, & Tversky 1982), the proposal that Simpson’s Paradox\ncan be explained by appeal to an error in probabilistic reasoning is\nplausible. Yet Bandyopadhyay et al. do not specify what this error is.\nOr, more specifically, they do not propose a valid form of reasoning\nthat reasoners are mistakenly appealing to when falling prey to the\nparadox. The fact that people expect that the ratios in subpopulations\nto be preserved in the combined population just shows that people are\ntricked by the paradox. It does not illuminate the underlying mistake\nthat they are making when they are tricked. In this sense,\nBandyopadhyay et al. do not answer their second question. They also,\nby their own admission, do not provide a general answer to (iii). They\nview this as a virtue of their account, since they believe that\ndiscussions of (iii) ought to be divorced from discussions of (i) and\n(ii). \nRecently, Fitelson (2017) has proposed a\nconfirmation-theoretic explanation of Simpson’s\nParadox. His analysis relies on identifying confirmation with\nincreasing the (subjective) probability of a proposition. Statements\nof the form “evidence \\(\\r{E}\\) confirms hypothesis\n\\(\\r{H}\\)” are, however, usually evaluated with respect to\nbackground knowledge K, and this can lead to ambiguities. In\nparticular, Fitelson distinguishes between the suppositional\nand conjunctive readings of a confirmation statement. In our\nrunning example, these statements would be as follows: \nSuppositional (\\(\\bf \\r{E}\\) raises the probability of\n\\(\\bf\\r{H}\\) given \\(\\b{K}\\)): If one is female, then\nreceiving treatment increases one’s chance of recovery. \nConjunctive (\\(\\bf \\r{E}\\wedge\\b{K}\\) raises the probability\nof \\(\\bf \\r{H}\\)): Being a female treatment-receiver\nincreases one’s chance of recovery. \nWhile the suppositional and conjunctive reading coincide for some\naccounts of confirmation (e.g., Carnap’s account of degree of\nconfirmation as conditional probability), they can produce different\noutcomes for confirmation as probability-raising. For our\ndata in\n Table 1,\n the suppositional reading is true: if one is in the female\nsubpopulation, receiving treatment rather than being in the control\ngroup increases one’s chances of recovery. On the conjunctive\nreading, however, the statement is false: female treatment-receivers\nare less-likely to recover (12/27) compared to the set of individuals\nwho are either male or did not receive the treatment (16/25). More\nimportantly, while the suppositional reading allows for association\nreversals, on the conjunctive reading it cannot be the case both that\nbeing a female treatment-receiver and being a male treatment-receiver\nraises the probability of recovery, but being a treatment receiver\nsimpliciter does not (Fitelson 2017: 300–302). \nFitelson’s confirmation-theoretic explanation of Simpson’s\nParadox is that reasoners are not attentive to the difference between\nthe suppositional and conjunctive readings of confirmation statements\nwhen considering the evidential relevance of learning an\nindividual’s gender. In the conjunctive reading there cannot be\nassociation reversals, and because the suppositional and conjunctive\nreading do not differ for many accounts of confirmation, people\nmistakenly assume that there cannot be such reversals, even when they\nare relying on the suppositional reading. \nBoth Bandyopadhyay et al. and Fitelson claim that because the\nformulation of Simpson’s paradox does not itself appeal to\ncausal considerations, it is a preferable to find a non-causal\nexplanation for the paradox. Ultimately, it is an empirical question\nwhether the paradox can be accounted for exclusively by errors in\nprobabilistic reasoning, or, as Pearl suggests, due to a conflation of\ncausal and probabilistic reasoning. One conceptual barrier to\ndisentangling these hypotheses is that there are systematic\nrelationships between causal and probabilistic claims For example,\nwhen the third variable \\(\\r{M}\\) is uncorrelated with treatment \\(T\\)\n(i.e., \\(p(\\r{T}\\mid \\r{M}) = p(\\r{T})\\)), there can be no reversals\n(see also the theorems in\n Section 2.2).\n Does it follow that Simpson’s Paradox has a purely\nprobabilistic explanation? Not necessarily. An alternative hypothesis\nis that the epistemic agent does not have knowledge of the relevant\nconditional probabilities, but does know that \\(M\\) is not a cause of\n\\(T\\) \\((p(\\r{T}\\mid \\do(\\r{M})) = p(\\r{T})\\)), preempting the\noccurrence of association reversals. The question of whether the\nsource of the paradox is causal cannot be resolved purely by appeal to\nthe mathematical conditions under which the it arises. Rather, it\ndepends on substantive psychological hypotheses about the role of\ncausal and probabilistic assumptions in human\n reasoning.[10] \nThe empirical evidence on the paradox shows that reasoners find\ntrivariate reasoning (i.e., with a causally relevant third variable)\ngenerally hard and fail to take its role properly into account, even\nif salient cues to its relevance are provided (Fiedler, Walther,\nFreytag, & Nickel 2003). Other studies point to the facilitative\neffect of causal model, statistical training and high motivation\n(Schaller 1992; Waldmann & Hagmayer 1995), but the significant\ndifficulties that reasoners encounter in Simpson-like tasks make it\nunlikely that the question of the right analysis of the paradox will\nsoon be decided empirically. \nTable 4: Verbal SAT score data for\nAmerican high schools, taken from Rinott & Tam (2003). \nSimpson’s Paradox is not limited to categorical data: it can\noccur for cardinal data as well and show up in standard models for\nquantitative analysis. A famous example is the analysis of SAT\nscores—the results of college admission tests—in the\nUnited States as a function of the high school grade point average\n(GPA) of students. The data are given in\n Table 4:\n the overall SAT average rises from 1992 to 2002, but for each GPA\ngroup (A+/A/…), SAT averages are falling. This phenomenon is,\nhowever, very natural. As soon as there is a bit of grade inflation at\nhigh schools, each group loses their best students to the next higher\ngroup, lowering the SAT average per group. But this is of course\nconsistent with the overall SAT average remaining equal, or even\nrising from 501 to 516, like in our dataset. A conclusion from the\nstratified data that “students are getting more stupid”\nwould be mistaken. Since societal developments such as grade inflation\naffect both the grade distribution and the SAT scores, one should\nnot condition on the GPA of a student when studying SAT\nscores over time (compare the back-door criterion from\n Section 3.4).[11] \nFigure 4: A linear regression model that\nillustrates Simpson’s Paradox for bivariate cardinal data. Each\ncluster of values corresponds to a single person (repeated\nmeasurement). \nA similar example is presented in\n Figure 4,\n adapted from Kievit, Frankenhuis, Waldorp, and Borsboom (2013). The\nfigure shows the results of coffee intake on performance at an IQ test. Suppose\nthat coffee actually decreases performance slightly because it\nmakes drinkers more nervous and less focused. At the same time, coffee\nintake co-varies with education level (construction workers are too\nbusy for drinking coffee all the time!) and education level co-varies\nwith test performance. When we measure performance repeatedly for\ndifferent individuals, we see that their performance is slightly\nnegatively affected by their coffee intake. However, the\n(unconditional) regression model of performance as a function of\ncoffee intake suggests misleadingly\nthat coffee consumption strongly improves performance! The reason\nfor the confounding is the causal impact of the hidden covariate,\neducation level, on both coffee consumption and performance. Similar\nto the results from\n section 2,\n Simpson’s Paradox in linear models can be characterized\nformally by means of inequalities among regression coefficients (e.g.,\nPearl 2013), and its occurrence depends on the nature of the causal\ninteraction between the involved variables. \nSimpson’s Paradox in its various forms has attracted a lot of\nattention in the epidemiological literature since it is relevant for\ndetermining and estimating the effect size of medical treatments, and\nthe effect of exposure to risk factors (e.g., smoking, alcohol) on\nmedical hazards. \nOne of the aims behind the methodology of randomized controlled trials\n(RCTs) is to eliminate the effect of potential confounders on whether\na person is treated or not. This was described in\n Section 2.2\n as row-uniform design (for experiments with categorical data). For\nexample, if we ensure the same proportion of both genders in the\ntreatment and control group, the same prevalence of different age\ngroups, etc., we know that association reversal (AR) cannot occur with\nrespect to those third variables, and also the amalgamation paradox\n(AMP) is ruled out for many measures. \nHowever, the (log-)odds ratio, a popular measure of effect size in\nepidemiological research, shows a deviant behavior. Uniformly\nassigning individuals to treatment and control condition reliably\nproduces the AMP for the odds ratio whenever the third\nvariable (=the subpopulation attribute) influences the success rate,\ngiven the treatment level (Theorem 2.4, Samuels 1993). The odds ratio\nis thus a particularly tricky association measure. Greenland (1987)\ngives the instructive example of an odds ratio that is equal in all\nsubpopulations with row-uniform design, but halved when data are\npooled. \nMeta-analytic problems, such as pooling various\nstudies for determining the overall effect size of an intervention or\nrisk factor give a particularly interesting twist to Simpson’s\nParadox. How should such studies be aggregated? Naïvely, somebody\nmay suggest to pool the data from all studies and to treat them as a\nsingle big study. This may work out if the study populations are very\nsimilar and the data are from RCTs, where the treatment/control ratio\nis typically 50:50. If this is indeed the case, then the overall\ndataset is row-uniform and AR (and for most measures, AMP) is avoided,\nas shown in\n Section 2.2.\n But for non-experimental data, there is no reason to assume that\ntreatment/control proportions are equal across studies. Thus, the\ndirection of the direction of the effect can be reversed when pooling\n(for examples, see Hanley & Thériault 2000; Reintjes, Boer,\nPelt, & Mintjes-de Groot 2000; Rücker & Schumacher\n2008). \nAnother reason for not pooling the data is that study populations are\noften heterogenous and that calculating the strength of association\n(i.e., the effect size) on the basis of the data may bias the estimate\nin the direction of the study with the largest sample size, while the\ncharacteristics of patients in that study need not be representative\nof the target group as a whole. In particular, while at the level of\nstudies patients are usually assigned randomly to the treatment or\ncontrol group, this cannot be said about the aggregate data (Cates\n2002). Proper meta-analysis therefore proceeds on the basis of\nweighting the effects rather than pooling the data, either by a fixed\neffects model or (e.g., if the study populations are heterogenous) by\nintroducing a random effect of the study in the statistical model. The\nquestion of how to conduct a meta-analysis of epidemiological studies\nis also entangled with the choice of an association or effect size\nmeasure (Altman & Deeks 2002; Cates 2002; Greenland 1987), a\nquestion discussed in\n Section 2.2. \nBlyth (1972) argued that Simpson’s Paradox also constitutes a\ncounterexample to the sure-thing principle of decision theory, or at\nleast restricts its scope substantially. That principle is supposed to\nguide rational decisions under uncertainty, and has been stated by\nSavage as follows: \nSure-Thing Principle (STP) “If you\nwould definitely prefer \\(g\\) to \\(f\\) either knowing that the event\n\\(\\r{B}\\) obtained, or knowing that the event B did not obtain, then\nyou definitely prefer \\(g\\) to \\(f\\)”. (Savage 1954:\n21–22) \nIn his purported counterexample, Blyth treats \\(\\r{B}\\) and\n\\(\\neg\\r{B}\\) as indicating the two subpopulations (e.g., two\ndifferent hospitals). Suppose that treatment \\(\\r{T}\\) is positively\nassociated with recovery \\(\\r{R}\\) for each subpopulation. In that\ncase, assuming equal odds, we would rather bet on the recovery of a\npatient in the treatment group (action \\(g\\)) than on the recovery on\nthe patient in a control group (action \\(f\\))—regardless of\nwhether that person is in group \\(\\r{B}\\) or group \\(\\neg\\r{B}\\).\nThus, since we prefer \\(g\\) to \\(f\\) in either subpopulation, and\nsince all patients are either in group \\(\\r{B}\\) or in group\n\\(\\neg\\r{B}\\), we can infer, by the Sure-Thing Principle, that \\(g\\)\nis preferable to \\(f\\) also when we don’t know whether a patient\nis in group \\(\\r{B}\\) or group \\(\\neg\\r{B}\\). But this inference is\nmistaken if association reversal occurs: it is perfectly compatible\nwith the above scenario that the overall frequency of recovery is\nhigher for non-treated than for treated patients! Blyth (1972: 366)\nconcludes that  \nthe Sure-Thing Principle […] seems not applicable to situations\nin which any action taken within \\(f\\) or \\(g\\) […] is allowed\nto be based sequentially on events dependent with [\\(B\\)].  \nSee also Malinas (2001) for discussion. \nTo the extent that (conditional) degrees of belief just represent\n(conditional) dispositions to bet, Blyth’s reasoning is\ncompelling. Association reversal means that  \nalthough  \nand  \nand thus preference for a conditional bet on \\(\\r{T}\\) (given the\nvarious levels of \\(B\\)) does not imply preference for the\nunconditional bet on \\(\\r{T}\\) (see\n Section 2).\n However, Savage certainly did not intend the sure-thing principle to\nbe a theorem of probability. To evaluate it as a principle that guides\nproper decision-making, we must consider cases where the predictor\nvariable (here: treatment/control) stands for a proper act\nthat affects the outcome via multiple paths. \nJeffrey (1982) recalls Savage’s (1954: 21) example of a\nbusinessman who believes that it is advantageous to buy a property\nregardless of whether the Democratic or the Republican candidate will\nwin the upcoming mayor elections. Jeffrey’s twist is that the\nbusinessman’s utility depends not only on the property deal, but\nalso on the election outcome. Specifically, buying the property raises\nthe chances that the Democratic candidate, whom he dislikes, will win.\nIn that case he would certainly buy the property after the\nelection, regardless of the outcome, but he may refrain from buying it\nbefore the election. \nIn response to this challenge, Jeffrey (1982: 720) restricts the\nsure-thing principle to the case where  \nchoice of one act or another is thought to have no tendency to\nfacilitate or impede the coming about of any of the possible states of\nnature, and […] this is reflected in a probabilistic\nindependence of states from acts.  \nThat is, buying the property should not change our rational degree of\nbelief in who wins the election. Pearl (2016) considers this response\nan “overkill” and notes that probabilistic associations\nare not a good means of expressing causal tendencies. Therefore he\nproposes a causal sure-thing principle that we have encountered in\n Section 3.4:\n If one is considering two acts \\(f\\) and \\(g\\), and the\nprobability distribution of \\(\\r{B}\\) does not change depending on\nwhether one intervenes to choose \\(f\\) or \\(g\\), then if one\nprefers \\(f\\) to \\(g\\) whether or not \\(B\\) occurs, one prefers \\(f\\)\nunconditionally. The italicized condition ensures that the\npartitioning variable is not an effect of the intervention, and thus\nrules out Simpson’s reversals (see\n section 3.4).\n Note that Pearl’s formulation, but not Jeffrey’s, allows\nto apply the (causal) sure-thing principle to observational data,\nwhere states and acts may be statistically dependent without\nindicating genuine causation (e.g., because of self-selection\neffects). \nThroughout this entry we have assumed knowledge of the causal facts\npertinent to a situation. Scenarios in which an agent lacks such\nknowledge raise additional complications for decision theory. An agent\ntypically cannot ensure that all confounders have been accounted for,\nand thus the possibility of repeated reversals raises questions about\nwhen one should adopt a promising policy that has not been\nexperimentally tested (Peters, Janzing, & Schölkopf 2017:\n174–175). A distinct concern is that an agent may not be sure\nwhether her action counts as an intervention (e.g., in Newcomb\nscenarios), since it might not be clear whether she can manipulate a\nvariable to render it independent of its prior causes (Stern 2019).\nWhether Simpson’s Paradox raises novel difficulties in such\ndecision-making contexts has not yet been explored. See the entries on\n decision theory\n and\n causal decision theory\n for further discussion. \nWithin the philosophy of biology, the units of\nselection debate (Sober 2000 [2018: ch. 4], 2014; Williams\n1966) concerns whether natural selection operates only at the level of\nthe individual or also on groups (where the individual is typically\nconceived either as the organism or the gene). This debate is\nespecially important for understanding the evolution of altruism\n(Sober & Wilson 1999). Since altruistic individuals harm their own\nchances of survival and reproduction, they are less fit, and it is\nthus unclear how altruism could evolve as a result of natural\nselection. If, however, groups with more altruists are fitter than\ngroups with fewer, and selection can act on groups, this could\npotentially explain how altruism could still evolve. Within the units\nof selection debate, Simpson reversals have played an\nimportant role in explaining the possibility of group-level\nselection. \nConsider the following naive argument against the conceptual\npossibility of group-level\n selection.[12]\n Suppose that we define the fitness of a group as the average fitness\nof its individuals. In this context, altruistic individuals are, by\ndefinition, those with traits that reduce their individual fitness\nwhile improving the fitness of other group members. For instance,\ncrows that issue warning cries when a predator approaches benefit the\ngroup while increasing the chances of being harmed themselves. Natural\nselection explains the evolution of traits on the basis that\nindividuals with the trait are fitter than those without it (all else\nbeing equal). Since selfish individuals are by definition fitter than\naltruistic ones, it follows that groups with more altruistic\nindividuals cannot be fitter. Or so one might argue. \nBy now it should be clear what is wrong with this type of\nargument—it does not follow from the fact that altruistic\nindividuals are less fit than selfish ones in every population that\npopulations which average over selfish and altruistic individuals\ncannot be fitter than populations with just selfish individuals. It\ncould be that being an altruist is correlated with being in a\npopulation with more altruists, and that populations with more\naltruists are fitter. This dispenses with the naive argument. Note,\nhowever, that within every single group selfish individuals are\nfitter, so if the groups change membership only through reproduction\n(as opposed to migration and mutation) then over enough generations\nevery group will end up consisting only of selfish individuals. So\nwhether groups selection can occur depends on additional facts about\npopulation structure and dynamics. Hamilton’s (1964) Kin\nSelection theory explains how altruism can evolve in cases where\naltruists are more likely to associate with other altruists (possibly\nbecause it runs in the family). \nThe group selection hypothesis remains controversial among biologists.\nThe present discussion reveals how the phenomenon of Simpson’s\nParadox is relevant to theorizing how it might be possible, and more\nbroadly reveals how philosophical work on causation and probability\ncan aid in clarifying scientific debates. \nRecently, Simpson’s Paradox has been invoked in an ongoing\ndebate regarding whether natural selection should be understood as\ncausal or statistical. Walsh (2010), a prominent defender of the\nstatistical view, points to cases of Simpson’s Paradox as\nshowing that selection cannot be understood causally, and Otsuka,\nTurner, Allen, & Lloyd (2011) rebut this claim. An important point\nthat emerges from this debate is that the term\n“population” is used differently in discussions of\nSimpson’s Paradox than it is in biology (cf. Weinberger 2018).\nWalsh presents an example in which a correlation in a population\ndisappears when one splits the population into two parts. As Otsuka et\nal. point out, within population genetics, population size can be\ncausally relevant to the fitness of its individuals. Note that\nWalsh’s example of dividing a population in half is not what we\nhave been talking about in the context of Simpson’s Paradox. In\nthe prior discussion, dividing the population was not a matter of\nchanging its size, but rather of partitioning its probability\ndistribution based on a variable. \nBickel et al. (1975) present a classic example of Simpson’s\nParadox involving a study of gender discrimination at Berkeley. The\ndata revealed that men were more likely than women to be accepted to\nthe university’s graduate programs, but the authors were unable\nto detect a bias towards men in any individual department. The authors\nuse the paradox to explain why the higher university-wide acceptance\nrate for men does not show that any department discriminated against\nwomen. Specifically, women were more likely to apply to departments\nwith lower acceptance rates. This leads to a probabilistic association\nbetween gender and the partitioning variable (department), which we\nhave seen can lead to Simpson’s reversals. \nWhile the probabilistic structure of the Berkeley case is similar to\nother instances of the paradox, it raises an additional question. On a\nnatural way to understand the case, the applicant’s gender is a\ncause of his or her applying to a more or less selective departments.\nExactly what it means for demographic variables such as gender or race\nto be a cause is a longer story for another day (Glymour & Glymour\n2014; Sen & Wasow 2016). But assuming that gender is a cause here,\nthen the department variable is a mediator, and one should\nnot condition on mediators in evaluating the mediated causal\nrelationship. So what is the justification for conditioning on\ndepartment? \nThe answer is that in evaluating discrimination, what often matters\nare path-specific effects, rather than the net effect\nalong all paths (Pearl 2000 [2009: 4.5.3]; Zhang & Bareinboim\n2018). To give a different example (Pearl 2001), consider whether a\nhypothetical black job candidate was discriminated against based on\nher race. It is possible that as a result of prior racial\ndiscrimination, the candidate was denied opportunities to develop\njob-relevant qualifications, and as a result of lacking these\nqualifications was denied the job. This indirect effect of race hiring\nwould not be relevant for determining whether an employer\ndiscriminated against the candidate. Rather, what matters is whether\nthe employee would have been more likely to get the position had she\nbeen white, but had the same qualifications that she does as a result\nof being black. This is called the natural direct\neffect (Pearl 2001; Weinberger 2019). In determining whether\nthe employer discriminated, what matters is not whether being black\nmade any difference in the person’s being hired, but\nrather whether their being black had a direct influence not via their\njob-relevant qualifications. \nThe common explanation for the Berkeley data, on which the paradox\nresults from women applying to more selective department, points to a\nlarger class of cases in which it is important to account for\ndifferences in the difficulty-level across tasks. In baseball, for\ninstance, it appears that over time batters have been striking out\nmore frequently, despite their improving in their ability to hit more\ndifficult pitches while remaining as good at hitting less difficult\nones (Watt 2016 [see Other Internet Resources]). \nThis could be accounted for by the fact that\npitchers have been throwing a higher proportion of difficult-to-hit\npitches. This highlights the way that statistics about success rates\nin performing a task can be misleading in cases where the\ntask-difficulty changes over time. \nSimpson’s Paradox is not only a surprising mathematical fact; it\nserves as a lens through which to understand the role of probabilities\nin data analysis, causal inference, and decision-making. In this\narticle, we have characterized its mathematical properties, given\nnecessary and sufficient conditions for its occurrence, discussed its\ninfluence on theories of causality, evaluated competing theories of\nthe nature of the paradox, and surveyed its applications in a range of\nempirical domains. \nAlthough Simpson’s Paradox has been known for over a century and\nhas a straightforward probabilistic analysis, we predict that it will\nremain a source of fruitful philosophical discussion. Pearl’s\ncausal analysis of the paradox is relatively recent, and it is only\nnow that graphical causal models are starting to play a central role\nin philosophical discussions of the paradox. Despite the continuity\nbetween graphical accounts and earlier probabilistic theories of\ncausality, here we have highlighted ways in which the newer methods\nlead one to draw substantially different implications from the\nparadox. Pearl’s account renders certain debates from the\nearlier literature moot, while opening up new debates about the proper\ninterpretation of the paradox. The responses to Pearl considered in\n section 4\n are only the first steps in a broader discussion about the\nrelationships between causation, probability theory, and the\npsychology of reasoning. There remains room to clarify what it means\nto explain the paradox, and what counts as empirical support for a\nparticular explanation. Such work would open the door to empirical\ntesting, which has thus far been limited. \nFinally, we would like to highlight connections between\nSimpson’s Paradox and other reasoning fallacies in the\nliterature. First, the base rate fallacy is related to\nSimpson’s Paradox since the illusion that association reversal\nis impossible may be based on a neglect of the different base rates\nfor treated and untreated people, given the third variable (Bar-Hillel\n1990). Second, the fallacy of mistaking correlation for\ncausation may contribute to the appearance of paradoxicality\nsince association reversal implies two contradicting causal claims,\nwhen combined with this fallacy. Third, in both Simpson’s\nParadox and the Monty Hall fallacy reasoners fail to see the\nprobabilistic relevance of causal information. While in\nSimpson’s Paradox, reasoners ignore the relevance of a back-door\npath for an observed association, in the Monty Hall problem, reasoners\nfail to take into account how Monty’s action depends on his\nknowledge of what is in back of the doors. Fourth, and last, the\ncapacity of reasoners to detect the causes of association reversal\nalso depends on the extent of the confirmation bias to which\nthey are exposed (e.g., whether or not they find a discrimination\nmechanism plausible). We are unaware of systematic research into the\nconnection between Simpson’s Paradox and these reasoning\nfallacies, but this could be a fruitful field for future research.\nThere is perhaps nothing paradoxical about Simpson’s Paradox,\nbut since we often struggle to understand it, our reasoning about\nassociation reversals may be entangled with various forms of reasoning\nthat are susceptible to bias and error.","contact.mail":"jan.sprenger@unito.it","contact.domain":"unito.it"},{"date.published":"2021-03-24","url":"https://plato.stanford.edu/entries/paradox-simpson/","author1":"Jan Sprenger","author1.info":"http://www.laeuferpaar.de","author2.info":"https://sites.google.com/site/naftaliweinberger/","entry":"paradox-simpson","body.text":"\n\n\nSimpson’s Paradox is a statistical phenomenon where an\nassociation between two variables in a population emerges, disappears\nor reverses when the population is divided into subpopulations. For\ninstance, two variables may be positively associated in a population,\nbut be independent or even negatively associated in all\nsubpopulations. Cases exhibiting the paradox are unproblematic from\nthe perspective of mathematics and probability theory, but\nnevertheless strike many people as surprising. Additionally, the\nparadox has implications for a range of areas that rely on\nprobabilities, including decision theory, causal inference, and\nevolutionary biology. Finally, there are many instances of the\nparadox, including in epidemiology and in studies of discrimination,\nwhere understanding the paradox is essential for drawing the correct\nconclusions from the data.\n\n\nThe following article provides a mathematical analysis of the paradox,\nexplains its role in causal reasoning and inference, compares theories\nof what makes the paradox seem paradoxical, and surveys its\napplications in different domains.\n\nWe begin with an illustration of the paradox with concrete data. The\nnumbers in\n Table 1\n summarize the effect of a medical treatment for the overall\npopulation (N = 52), and separately for men and women: \nTable 1: Simpson's Paradox: the type of\nassociation at the population level (positive, negative, independent)\nchanges at the level of subpopulations. Numbers taken from\nSimpson’s original example (1951). \nFor matters of exposition, we assume that these frequencies are\nunbiased estimates of the underlying probabilities. The treatment\nlooks ineffective at the level of the overall population, but it leads\nto higher success percentages than the control both for men and for\nwomen (61% vs. 57% for men and 44% vs. 40% for women). Writing these\nproportions as conditional probabilities, with \\(\\r{T}\\)=treatment,\n\\(\\r{S}\\)=success/recovery, and \\(\\r{M}\\)=male subpopulation, we\nobtain  \nbut at the same time,  \nShould we use the treatment or not? When we know the gender of the\npatient, we would presumably administer the treatment, whereas it does\nnot look like the right thing to do when we don’t know the\npatient’s gender—although we know that the patient is\neither male or female! \nThis phenomenon was first pointed out in papers by Karl G. Pearson\n(1899) and George U. Yule (1903), but it was Simpson’s short\npaper “The interpretation of interaction in contingency\ntables” (1951), discussing the interpretation of such\nassociation reversals, that led to the phenomenon being labeled as\n“Simpson’s Paradox”. The phenomenon is, however,\nbroader than independence in the overall population and positive\nassociation in the subpopulations; for example, the associations may\nalso be reversed. Nagel and Cohen (1934: ch. 16) provide an example of\nsuch a reversal as part of a exercise for logic students. \nUnderstanding the paradox is essential for drawing the proper\nconclusions from statistical data. To give a recent example involving\nthe paradox (Kügelgen, Gresele, & Schölkopf [see\n Other Internet Resources]),\n early data revealed that the case fatality rate for Covid-19 was\nhigher in Italy than in China overall. Yet within every age group the\nfatality rate was higher in China than in Italy. One thus appears to\nget opposite conclusions about the comparative severity of the virus\nin the countries depending on whether one compares the whole\npopulations or the age-partitioned populations. Having a proper\nanalysis of what is going on is such cases is thus crucial for using\nstatistics to inform policy. \nIn what follows,\n Section 2\n explains different varieties of the paradox, clarifies the logical\nrelationships between them, and identifies precise conditions for when\nthe paradox can occur. While that section focuses on the mathematical\ncharacterization of the paradox,\n Section 3\n focuses on its role in causal inference, its implications for\nprobabilistic theories of causality, and its analysis by means of\ncausal models based on directed acyclic graphs (DAGs: Spirtes,\nGlymour, & Scheines 2000; Pearl 2000 [2009]). \nBased on these different approaches,\n Section 4\n discusses different analyses of what makes Simpson’s Paradox\nlook paradoxical, and what kind of error it reveals in human\nreasoning. This section also reports empirical findings on the\nprevalence of the paradox in reasoning and inference.\n Section 5\n surveys the occurrence and interpretation of the paradox in applied\nstatistics (regression models), philosophy of biology, decision theory\nand public policy. For example, Simpson’s Paradox is relevant\nwhen analyzing data to test for race or gender discrimination (Bickel,\nHammel, & O’Connell 1975).\n Section 6\n wraps up our findings and concludes. \nThis section shows how Simpson’s Paradox can be characterized\nmathematically, under which conditions it occurs, and how it can be\navoided. We begin by further considering the concrete example from the\nintroduction in order to build intuitions that will guide us through\nthe more technical results. \nThe data in\n Table 1\n can be translated into success or recovery rates, showing that\ntreated men have a higher recovery rate than untreated men (roughly\n61% vs. 57%), and the same for women (44% vs. 40%). Two observations\nare key to understanding why this positive association vanishes in the\naggregate data. First, the recovery rate of untreated men is still\nhigher than the recovery rate of women who receive treatment (57% vs.\n44%), suggesting that not only treatment, but also gender is a\nrelevant predictor of recovery. Second, while the treatment group is\nmajority female (27 vs. 13), the control group is majority male (7 vs.\n5). Speaking informally, the lack of population-level correlation\nbetween treatment and recovery results from men being both (i) more\nlikely to recover from the treatment, and (ii) less likely to be in\nthe treatment group. \nThis becomes evident when we use conditional probabilities to\nrepresent recovery rates given treatment and/or subpopulation. The\noverall recovery rates given treatment and control can, by the Law of\nTotal Probability, be written as the weighted average of recovery\nrates in the subpopulations:  \nPlugging in the numbers from\n Table 1\n to calculate the overall recovery rates via these equations, we see\nthat the first line is a weighted average of success rates for treated\nmen and women (61% and 44%) while the second line is a weighted\naverage of success rates of the two control groups (57% and 40%).\nThese averages are weighted by the percentage of males and females in\neach group, and in the present case the gender disparity between the\ngroups results in both averages being 50%. Since these weights can be\ndifferent, the treatment may raise the probability of success among males and females\nwithout doing so in the combined population. \nLater we will show that the positive association in the subpopulations\ncannot vanish if the correlation of treatment with gender is broken\n(e.g., by balancing gender rates in both conditions). The weights in\neach line are then identical—\\(p(\\r{M}\\mid \\r{T}) =\np(\\r{M}\\mid \\neg \\r{T})\\)—and associations in subpopulations\nare preserved for the aggregate data\n (Theorem 1 in Section 2.2).\n In fact, the absence of such a correlation rules out Simpson’s\nParadox. \nSimpson’s Paradox can occur for various types of data, but\nclassically, it is formulated with respect to \\(2\\times2\\) contingency\ntables. Let \\(D_i = (a_i, b_i, c_i, d_i)\\) be a four-dimensional\nvector of real numbers representing the \\(2\\times2\\) contingency table\nfor treatment and success in the i-th subpopulation, and let\n \nbe the aggregate data set over \\(N\\) subpopulations. These data should\nbe read as shown in\n Table 2. \nTable 2: Abstract representation of a\n\\(2 \\times 2\\) contingency table with subpopulations \\(D_1\\) and\n\\(D_2\\). \nLet \\(\\alpha (D_i)\\) be a measure the strength of the probabilistic\nassociation between \\(T\\) and \\(S\\) in population\n \\(D_i\\).[1]\n By convention, \\(\\alpha (D_i) = 0\\) corresponds to no association\nbetween the variables, \\(\\alpha (D_i) \\gt 0\\) indicates a positive\nassociation, and \\(\\alpha (D_i) < 0\\) a negative one. This can best\nbe translated into the condition  \nThe condition \\(a_i \\, d_i > b_i \\, c_i\\) is equivalent to saying\nthat the success rate in the first row (“treatment\ncondition”) is higher than the success rate in the second row\n(“control condition”):  \nApplying all this to our dataset in\n Table 1,\n we see that \\(\\alpha(D) = 0\\) although \\(\\alpha(D_1) > 0\\) and\n\\(\\alpha(D_2) > 0\\). This is a special case of what Samuels (1993)\ncalls Association Reversal (AR). Association reversal\noccurs if and only if there is a population such that the association\nin all partitioned subpopulations is either (i) positive (ii)\nnegative, or (iii) zero, and the type of association in the population\ndoes not match that of the subpopulations. Writing this out\nmathematically, this means for a dataset \\(D = \\sum_{i=1}^N D_i\\) that\none of the following two conditions holds,  \nwhere at least one of the inequalities has to be strict. Association\nreversal is the standard variety of Simpson’s Paradox\n(Bandyopadhyay et al. 2011; Blyth 1972, 1973) and also the one that is\nmost frequently investigated in the psychology of reasoning, or by\nphilosophers analyzing the paradox (e.g., Cartwright 1979; Eells 1991;\nMalinas 2001). \nAn important special case of AR occurs when there is no association in\nthe subpopulations, but an association emerges in the overall dataset:\n \nReferring to the pioneering work of the statistician George U. Yule\n(1903: 132–134), Mittal (1991) calls this Yule’s\nAssociation Paradox (YAP). It is typical of spurious\ncorrelations between variables with a common cause, that is, variables\nthat are dependent unconditionally (\\(\\alpha(D) \\ne 0\\)) but\nindependent given the values of the common cause (\\(\\alpha(D_i) =\n0\\)). For example, sleeping in one’s clothes is correlated with\nhaving a headache the next morning. However, once we stratify the data\naccording to the levels of alcohol intake on the previous night, the\nassociation vanishes: given the same level of drunkenness, people who\nundress before going to bed will have the same headache, ceteris\nparibus, as those who kept their clothes on. \nFinally, the most general version of Simpson’s Paradox is the\nAmalgamation Paradox (AMP) identified by Good and\nMittal (1987). This paradox occurs when the overall degree of\nassociation is bigger (or smaller) than each degree of association in\nthe subpopulations, or mathematically,  \nAMP challenges the intuition that the degree of association in the\ngeneral population, in virtue of being “the sum” of the\nindividual subpopulations, has to fall in between the minimal and the\nmaximal degree of association observed on that level. The logical\nstrength of the paradoxes is inversely related to their generality and\nfrequency of occurrence: \\(\\text{YAP} \\Rightarrow \\text{AR}\n\\Rightarrow \\text{AMP}\\). Variations of the paradox for\nnon-categorical data (e.g., bivariate real-valued data) will be\ndiscussed in\n Section 5.1. \nWe proceed to characterizing the mathematical conditions under which\nSimpson’s Paradox occurs. We have already suggested that the\nparadox arises in the medical example due to correlations between the\ntreatment variable and the partitioning variable, and we can now make\nthis more precise: \nTheorem 1 (Lindley & Novick 1981; Mittal\n1991): If \\(\\alpha(D) > 0\\) and association reversal occurs\nfor the subpopulations characterized by attribute \\(\\r{M}\\) and\n\\(\\neg\\r{M}\\), (i.e., \\(\\alpha(D_1), \\alpha(D_2) \\le 0\\)), then\neither \nAs Theorem 1 makes clear, the lack of correlation between \\(\\r{M}\\)\nand \\(\\r{T}\\) is sufficient to rule out association reversals (and\nthus YAP as well). Does it also rule out the more general amalgamation\nparadox? The answer to this depends on which measure of\nassociation one chooses for \\(\\alpha\\). Discussions of\nSimpson’s Paradox commonly treat association as the\ndifference in the success rate between the treated and the\nuntreated, but this is only one of many possibilities (Fitelson 1999).\nWhile the lack of association between \\(M\\) and \\(T\\) is sufficient to\nrule out AMP for most measures (including the difference measure) it\ndoes not rule it out for all measures, as we will now explain. Readers\nnot interested the specific details may skip to the following\nsection. \nHere are some widely used association measures for a dataset \\((a, b,\nc, d)\\):  \nSome of these measures can be formulated probabilistically and have\nbeen suggested as measures of causal strength and outcome measures for\nclinical trials (Edwards 1963; Eells 1991; Fitelson & Hitchcock\n2011; Greenland 1987; Peirce 1884; Sprenger 2018; Sprenger &\nStegenga 2017). For example, \\(\\pi_{D} = p(\\r{S}\\mid \\r{T}) -\np(\\r{S}\\mid \\neg \\r{T})\\) represents the difference and \\(\\pi_R =\np(\\r{S}\\mid \\r{T}) / p(\\r{S}\\mid \\neg \\r{T})\\) the ratio of success\nrates in treatment and control conditions. \\(\\pi_W\\) can be\ninterpreted as the prognostic weight of evidence that treatment\nprovides for success (i.e., as the log-Bayes factor), \\(\\pi_{Y}\\) is\nYule’s (1903) measure of association, \\(\\pi_{O}\\) is the\nlog-odds ratio familiar from epidemiological data analysis, and\n\\(\\pi_C\\) I.J. Good’s (1960) measure of causal strength. \nWe now consider the extent to which AMP for different measures is\nruled out by different experimental designs. Suppose that individuals\nare uniformly assigned to the treatment and control condition across\nsubpopulations. In such a case, where the ratio of persons assigned to\nthe treatment and control condition is equal for each subpopulation,\nthe experimental design is called row-uniform.\nSpecifically, there has to be a \\(\\lambda > 0\\) such that for any\nsubpopulation i  \nIn particular, row uniformity holds approximately if our sample is\nlarge and we sample at random from the population. \nRow-uniform design of a trial ensures independence between a potential\nconfounder \\(M\\) and the treatment variable \\(T\\). Accordingly, by\n Theorem 1,\n it rules out association reversals. Additionally, row-uniform design\nis sufficient to rule out the AMP for a wide class of association\nmeasures: \nTheorem 2 (Good & Mittal 1987): If a\ndataset \\(D = \\sum D_{i}\\) satisfies row uniformity, then the\nAmalgamation Paradox is avoided for the measures \\(\\pi_{D}\\),\n\\(\\pi_{R}\\), \\(\\pi_{Y}\\) and \\(\\pi_{W}\\) and \\(\\pi_{C}\\). It is\nnot avoided for the log-odds ratio \\(\\pi_{O}\\). \nSome studies also exhibit column-uniform design where\nthe proportion of successes and failures is constant across all\nsubpopulations:  \nAlso then \\(\\r{M}\\) is independent of \\(\\r{S}\\). Column uniformity can\noccur in case-control studies with various subpopulations (e.g.,\ndifferent hospitals) where one does not match the number of persons\nwith the explanatory attribute, like in an RCT. Instead, for each\nperson with a certain attribute (e.g., a specific form of cancer), one\nselects a number of persons that does not have this attribute.\nColumn-uniform design avoids AR as well, but among the presented\nassociation measures, it suffices to rule out AMP only for\n\\(\\pi_Y\\). \nTable 3: An overview of how row- and\ncolumn-uniform design avoid the amalgamation paradox for various\nassociation measures. \n\n Table 3\n summarizes the properties of all association measures with respect to\nthe AMP and the different forms of experimental design. The behavior\nof the log-odds measure \\(\\pi_O\\), where neither row- nor\ncolumn-uniform design suffices to rule out the AMP, will be discussed\nin\n Section 5.2. \nWe now identify one last fundamental condition for when data exhibit\nassociation reversal. Have a look at\n Figure 1\n which displays the success proportions for treatment and control\ngraphically. \nFigure 1: A geometrical representation\nof a necessary condition for the occurrence of Association Reversal.\nThe paradox can occur if the proportions are ordered like in the left\ngraph; it cannot occur if they are ordered like in the right graph.\n[An\n extended description of figure 1\n is in the supplement.] \nIn both examples, the treatment success rate is for both\nsubpopulations greater than the control success rate. When will this\norder be preserved at the overall level? We know that the overall\nsuccess rate for each condition (treatment/control) is\nconstrained by the success rates in the subpopulations: \nFact 1: Suppose \\(a_i, b_i > 0\\) for all\n\\(1 \\le i \\le N\\). Then also  \nThis fact follows directly from the Law of Total Probability (proof\nomitted) and it gives us a simple necessary condition for the\noccurrence of Association Reversal (AR): turning to\n Figure 1\n again, it implies that the overall success rate per condition has to\nbe on the solid lines. Thus AR cannot occur in the right part\nof\n Figure 1,\n but it can occur if the proportions are ordered as in the left part\nof\n Figure 1.\n Generally, AR is avoided when the following condition holds:  \nAny dataset that satisfies\n (RH)\n will be called row-homogenous. By contrast, for any\ngiven set of proportions violating condition\n (RH),\n we can find datasets exhibiting these very same proportions such that\nAR indeed occurs (by fiddling with the size of the subpopulations;\nLemma 3.1 in Mittal 1991). However, neither row homogeneity, nor the\nanalogous condition of column homogeneity, nor their conjunction is\nsufficient for avoiding the amalgamation paradox AMP. \nFinally, one might be interested in how frequently the paradox arises.\nSimulations by Pavlides and Perlman (2009) suggest that it should not\noccur frequently: the confidence interval for the probability of AR is\na subset of the interval \\([0;0.03]\\) for both the uniform prior and\nthe (objective) Jeffreys prior. Of course, the practical value of this\ndiagnosis depends on whether the sampling assumptions are sensible,\nand whether the entire approach makes sense for real-life datasets\nwhere researchers can group the data into subpopulations along\nnumerous dimensions. \nWithin the philosophical literature, Simpson’s Paradox received\nsustained attention due to its implications for accounts of causality\nthat posit systematic connections between causal relationships and\nprobability-raising. Specifically, the paradox reveals that facts\nabout probability-raising will not necessarily be preserved when one\npartitions a population into subpopulations. This poses a number of\nimportant challenges to philosophical accounts of causal inference\nbased on the concept of probability: \nStrategies for treating the paradox and answering these questions have\ncontributed substantially to the development of theories of\nprobabilistic causality (Cartwright 1979; Eells 1991). A different set\nof answers is provided by more recent work on the paradox in the\nframework of graphical causal models (e.g., Pearl 1988, 2000 [2009];\nSpirtes et al. 2000), and we will discuss both accounts in turn. In\nparticular, we will explain how Simpson’s Paradox can be\nanalyzed through the notions of confounding and the identifiability of\na causal effect. \nEarly accounts of probabilistic causation (e.g., Reichenbach 1956;\nSuppes 1970) sought to explicate causal claims purely in terms of\nprobabilistic and temporal facts. On Suppes’ (1970) account,\nevent \\(\\r{C}\\) is a prima facie cause of \\(\\r{E}\\) if and\nonly if (i) \\(\\r{C}\\) occurs before \\(\\r{E}\\) and (ii) \\(\\r{C}\\)\nraises the probability of\n \\(\\r{E}\\).[2]\n As we have already seen in\n Section 2.1,\n not all prima facie causes are genuine causes. If I drink a\nstrong blond Belgian beer now, I will probably be happy during the\nday, but also have a headache tomorrow. However, being happy would not\nthereby by the cause of the headache: the correlation is explained by\nthe common cause—the beer drinking. The variable for drinking\nthe beer screens off the probabilistic relationship between\nits effects, meaning that the effects will be uncorrelated when one\nconditions on it. The crux of Suppes’ account is that a\nprima facie causal relationship between \\(\\r{C}\\) and\n\\(\\r{E}\\) is a genuine causal relationship iff there is no factor F\nprior to C that screens off \\(\\r{C}\\) from\n \\(\\r{E}\\).[3] \nLater theorists such as Cartwright (1979) and Eells (1991) developed\nthis condition by making causal claims relative to a causally\nhomogenous background context, which is specified by a set of\nvariables \\(\\b{K}\\). Consider the following example of association\nreversal presented by Cartwright. Supposing that smoking \\((\\r{S})\\)\nis a cause of heart disease \\((\\r{H})\\), one might expect that smoking\nwould raise the probability of heart disease. Yet this might not be\nthe case. Suppose that in a population there is a strong correlation\nbetween smoking and exercising (X), and that exercise lowers the\nprobability of heart disease by more than smoking raises its\nprobability. In such a case, smoking might lower the probability of\nheart disease although conditional on either \\(X\\) or \\(\\neg X\\),\n\\(\\r{S}\\) raises \\(\\r{H}\\)’s probability. \nCartwright interprets this case as follows: causes always raise the\nprobability of their effects, but this can be “concealed”\nby the correlation between the cause and some other variable (here,\n\\(X\\)). In order to isolate the genuine probabilistic relationship\nbetween \\(\\r{C}\\) and \\(\\r{E}\\), one needs to consider it in a context\nwhere such correlations cannot occur: \nProbabilistic Causality (Cartwright) Let\n\\(\\b{K}\\) denote all and only the causes of \\(\\r{E}\\) other than\n\\(\\r{C}\\) and effects of \\(\\r{C}\\). Then \\(\\r{C}\\) causes\n\\(\\r{E}\\) if and only if relative to all combinations of values\nvariables in \\(\\b{K}\\), \\(\\r{C}\\) raises the probability of \\(\\r{E}\\):\n\\(p(\\r{C}\\mid \\r{E},\\b{K}) > p(\\r{C}\\mid\n\\neg{\\r{E},\\b{K}})\\). \nWhile Suppes defends a reductive account of probabilistic\ncausality, where the elements of \\(\\b{K}\\) are determined without\nappeal to causal assumptions, Cartwright presents a\nnon-reductive account where \\(\\b{K}\\) must include all and\nonly the causes of \\(\\r{E}\\), excluding \\(C\\) itself and any variables\nthat are causally intermediate between \\(\\r{C}\\) and \\(\\r{E}\\). The\ncurrent consensus is that it is impossible to give a probabilistic\naccount of causation without relying and causal concepts, and thus\nthat no non-reductive account is feasible (though see Spohn 2012 for\na dissenting view). \nAlthough non-reductive accounts could not be used to explain causation\nto someone with no prior causal knowledge, they can nevertheless\nclarify how causal claims are tested, and illuminate the relationship\nbetween causation and probability (see also Woodward 2003:\n20–22). Moreover, Cartwright argues that her general criterion\nfor inclusion of background factors in \\(\\b{K}\\) avoids the\nreference class problem for purely statistical\naccounts of causal explanation (i.e., by specifying the relevant\npopulations for evaluating causal claims), thereby eliminating a\nthreat to the objectivity of causal explanation. More detail is\nprovided in the\n entry on probabilistic causality. \nCartwright’s innovations for probabilistic accounts of causality\nhave triggered various debates related to Simpson’s Paradox. We\nhighlight three of them here: \nCartwright claims that causes raise the probabilities of their effects\nacross all background\n contexts,[4]\n but many purported causes only raise the probabilities of their\neffects in some contexts. In the latter cases, causes\ninteract with background factors in producing their effects.\nTo give Cartwright’s own example (1979: 428), ingesting an acid\npoison generally causes death, except in contexts where one also\ningests an alkali poison (in which case the two cancel one another\nout). The problem of such interactive causes for probabilistic\naccounts is that they threaten Cartwright’s picture on which the\neffect of probability-raising causes is “concealed” by a\nstronger negative cause which “dominates” them. These\nmetaphors suggest that the probability-raising relationship between a\ncause and its effect reflects an intrinsic relationship between the\nvariables that exists even when not manifested, an idea further\ndeveloped in (Cartwright 1989). Interaction means that causes do not\noperate in a vacuum, but rather only in the presence of background\nfactors (for further discussion, see Otte 1985; Eells 1986; Hardcastle\n1991). \nSimpson’s Paradox should not be conflated with causal\ninteraction, however. What is distinctive of the paradox is not that\nthe probabilistic relationship reverses upon partitioning, but rather\nthat it reverses in all of the resulting subpopulations. \nCartwright requires \\(\\b{K}\\) to include all causes of \\(\\r{E}\\), and\nthus to evaluate effects relative to homogenous background contexts.\nThe account thus does not allow for average effects. For example,\nsuppose that a particular treatment \\((\\r{T})\\) raises the probability\nof heart disease \\((\\r{H})\\) in individuals who were born prematurely\n\\((\\r{P})\\) but not individuals who were not, and that \\(\\r{P}\\) is\nnot correlated with \\(\\r{T}\\). In the whole population, the amount by\nwhich \\(\\r{T}\\) lowers or raises the probability of \\(H\\) will be an\naverage of the effects in the \\(\\r{P}\\) and \\(\\neg\\r{P}\\) populations,\nweighted by their size. Dupré (1984) argues for abandoning the\nrequirement that \\(\\b{K}\\) include all causes of \\(E\\), and thus for\nallowing average effects. \nA tempting lesson to draw from our opening example is that\nSimpson’s Paradox arises as a result of averaging over the\npopulations of males and females, and that the only way to eliminate\nit is by ruling out average effects. However, causal heterogeneity\ndoes not by itself lead to the\n paradox.[5]\n Cases with heterogeneous background factors only produce association\nreversal if the factors are correlated with the causal\nvariable—as demonstrated by\n Theorem 1\n in\n Section 2.2. \nAccording to Cartwright, the set \\(\\b{K}\\) should not include\nvariables that are causally intermediate between \\(C\\) and \\(E\\). Such\nvariables are called mediators. To see why, imagine a\ndrug reduces the risk of heart disease by producing a chemical,\nrepresented by variable \\(Z\\), in the blood stream, and via no other\nfactors. If \\(C\\) and \\(E\\) have no common causes, they will be\nprobabilistically independent conditional on \\(Z\\). Intuitively, one\nshould not hold the blood chemical fixed in evaluating the effect,\nsince it is the means by which the effect is brought about. \nWhen there are multiple paths between cause and\neffect, the question becomes more complex. Hesslow (1976)\nprovides an example where taking birth control pills promotes a\nblood-clotting condition called thrombosis via a chemical in the\nblood, but inhibits it via preventing pregnancy, which itself is a\ncause of thrombosis. As a result, taking birth control intuitively\ninfluences thrombosis both positively and negatively. If one is\ninterested in the net effect of \\(C\\) on \\(E\\)—as opposed to the\neffects via particular paths (Hitchcock 2001)—then one should\nnot condition on mediators. However, conditioning is necessary for\ncalculating path-specific effects (e.g., Pearl 2001; Weinberger\n2019). \nDistinguishing mediators from common causes is crucial for analyses of\nSimpson’s Paradox. For example, the causal models \\(C\\to Z \\to\nE\\) and \\(C\\leftarrow Z \\to E\\) exhibit the same conditional\nindependencies: \\(C\\) and \\(E\\) will be associated unconditionally,\nbut independent conditional on \\(Z\\). Only causal knowledge enables us\nto decide how we shall deal with the association reversal, and whether\nwe need to condition upon \\(Z\\) when estimating the causal effect of\n\\(C\\) on \\(E\\) (we do in the second model, but not in the first). See\nalso\n Section 3.4. \nIn recent years, the formal analysis of causation has been\nsignificantly enhanced by the development of graphical methods for\nrepresenting causal hypotheses and for choosing among candidate\nhypotheses given one’s evidence, in particular those using directed\nacyclic graphs (=DAGs: Pearl 1988, 2000 [2009]; Spirtes et\nal. 2000). A DAG contains a set of nodes connected by a set of\ndirected edges or arrows such that there are no cycles (one cannot get\nfrom a node back to itself via a set of directed arrows). In the\ncausal context, the nodes in a DAG are random variables and the arrows\ncorrespond to direct causal relationships. It is common to assume that\nthe set of variables in a DAG is causally sufficient, meaning\nthat it includes all common causes of variables in the set. \nDAGs enable one to systematically map the relationship between causal\nhypotheses and joint probability distributions. They overlap with and\nbuild on techniques in the literature on probabilistic causality, but\nprovide significantly stronger tools and results. See the entries on\n causal models,\n causation and manipulability and\n counterfactual theories of causation\n for detailed introductions to causal inference with DAGs. \nFigure 2: The relationship between the\nvariables Treatment, Gender, and Success\nrepresented in a DAG, without and with an intervention on\nTreatment. \n\n Figure 2\n (left part) presents a plausible DAG for our running example,\nincluding the variables Treatment, Gender, and\nSuccess. There are two ways in which Treatment\nprovides information about Success. One is that people who\ntake the treatment may be more (or less) likely to recover as a result\nof having taken it. The other is that learning that someone took the\ntreatment provides information about whether they are likely to be\nmale or female, and this information is relevant to determining\nwhether they will recover regardless of whether they took the\ntreatment. \nThe graphs can, however, also be interpreted causally, and here, the\nnotion of an ideal intervention is crucial:\n\n \nFor an intervention on a variable \\(V\\) to be ideal\nis for it to determine \\(V\\)’s value such that it no longer\ndepends on its other causes in the DAG. Graphically, we can represent\nan intervention by adding an additional cause I that\n“breaks” all of the arrows that would otherwise go into\n\\(V\\). So, in Figure 2 \nIntervention is an ideal intervention on Treatment.\nIntervening on Treatment disrupts the evidential relationship\nwith Gender—for example, by controlling for the\nproportion of male and female patients in each sample—so that\nany remaining probabilistic relationship between treatment and\nrecovery can only be explained by having taken the treatment. Such an\nexperimental design, where Treatment and Gender are\nmade probabilistically independent, suffices to rule out association\nreversal (cf.\n Section 2.2). \nUsing the notion of an ideal intervention, one can explicate causation\nas follows (Pearl 2000 [2009]; Woodward 2003). \\(C\\) causes \\(E\\) if\nand only if it is possible to change the value or probability of \\(E\\)\nvia some ideal intervention on \\(C\\). Such interventions distinguish\nbetween causal and merely probabilistic dependencies by eliminating\nany probabilistic relationship between \\(C\\) or \\(E\\) that can be\ntraced to the influence of a common cause. This does not mean,\nhowever, that one can only get causal knowledge in cases where one can\nexperimentally intervene. One of the key contributions of graphical\ncausal models is that they enable one to systematically determine when\none’s prior causal knowledge licenses one to interpret a\nparticular probabilistic relationship causally. \nThe difference between the probability distributions resulting from\nconditioning and from intervening is formally represented by\nsupplementing the probability calculus with the\ndo-operator (\\(\\do(X)\\)) where\napplying the operator to a variable formally represents intervening\nupon it. Taking \\(T\\), \\(S\\), and \\(M\\) to denote\nTreatment, Success, and Gender, and given\nthe graph in\n Figure 2,\n \nthe observational probability distribution of \\(S\\) given \\(T\\) is not equal to\nthe probability distribution of \\(S\\) given an intervention on \\(T\\):  \nThe difference between these two quantities is due to the impact of\n\\(M\\) on the distribution of \\(T\\). In contrast, the following two\nexpressions are equivalent given the DAG: \nHere one can infer the effect of \\(T\\) on \\(S\\) from the observational\ndistribution by conditioning on \\(M\\). In such a case, we say that\n (4) \nidentifies the causal effect of \\(T\\) on \\(S\\). More\ngenerally, identifiability is a relationship between\na DAG G, probability distribution \\(P\\) and a causal quantity\n\\(\\r{Q}\\), such that \\(\\r{Q}\\) is identifiable if and only if it is\nuniquely determined by \\(P\\) given \\(G\\). By contrast, when there are\nunmeasured common cause(s) of \\(S\\) and \\(T\\), the probability\ndistribution is compatible with any possible distribution for\n\\(p(\\r{S}\\mid \\do (\\r{T}))\\). \nThe concept of identifiability is crucial for understanding\nconfounding, and the analysis of Simpson’s Paradox through\ngraphical causal models. The relationship between \\(X\\) and \\(Y\\) is\nconfounded relative to variable set \\(\\b{Z}\\) just in\ncase \\(P(Y\\mid X,\\b{Z}) \\ne P(Y\\mid \\do(X),\\b{Z})\\) (i.e., the\nrelationship is not identified). A confounding set of\nvariables is one that biases the effect measurement.\nFor instance, an unmeasured common cause is a confounder because it\nmakes it impossible to differentiate the probabilistic dependence\nbetween the variables resulting from the common cause from that\nresulting from a causal relationship between them. Simpson’s\nParadox emerges on this account due to confounding by the third\nvariable. This notion of confounding can diverge from a common\ncolloquial understanding of confounders as alternative explanations of\nan observed outcome other than the treatment. \nA useful sufficient condition for identifiability is the\nback-door criterion (Pearl 1993, 2000 [2009: 79]).\nFirst we need to introduce some graphical terminology. A path\nbetween \\(X\\) and \\(Y\\) be a set of connected edges between \\(X\\) and\n\\(Y\\) going in any direction. \\(Y\\) is a descendant of \\(X\\)\nif there is a path from \\(X\\) to \\(Y\\) in which all the arrows go in\nthe same direction. When \\(X\\) and \\(Y\\) are connected via a single\npath including a common cause such as \\(X \\leftarrow Z \\rightarrow\nY\\), \\(X\\) and \\(Y\\) will\n typically[6]\n be unconditionally probabilistically dependent, but will be\nindependent conditional on \\(Z\\). For such a path, we say that \\(Z\\)\nblocks the path between \\(X\\) and \\(Y\\). In contrast, when\n\\(X\\) and \\(Y\\) are connected by a path including a common\neffect, such as \\(X \\rightarrow Z \\leftarrow Y\\), then the\npath will be blocked provided that one does not condition on\n\\(Z\\) or a descendant of \\(Z\\). This reflects the fact that\nindependent causes of a common effect will typically be dependent\nconditional on a common effect. An effect of \\(X\\) on \\(Y\\) is\nidentifiable if there are no unblocked “back-door paths”\nbetween \\(X\\) and \\(Y\\): all paths that pass through common causes are\nblocked, and all other paths excepting those by which the cause\ninfluences its effect are open. \nBack-door Criterion (Pearl 1993) Given a\nvariable pair \\(\\{X,Y\\}\\) in a DAG G, the effect of \\(X\\) on \\(Y\\) is\nidentifiable if there exists a variable set \\(\\b{Z}\\) in G\nsatisfying the following conditions: \nIn this case, the effect of \\(X\\) on \\(Y\\) is identified by the\nformula  \n\n Equation (5)\n reveals that can be possible to derive a causal effect in a\npopulation by averaging over the effects in subpopulations partitioned\nby \\(Z\\). This is what we already saw in\n Section 2.2:\n if there is no dependence between being treated and being a part of a\nsubpopulation, associations cannot reverse at the general population\nlevel. Yet such a derivation is only licensed by causal assumptions\nabout the relationships between the variables. The reader can verify\nthat given the DAG in\n Figure 2,\n the variables satisfy the back-door criterion (with\n \\(\\b{Z}=\\{\\textit{Gender}\\}\\)).[7] \nIn our original example, the treatment increased the probability of\nrecovery in each subpopulation, but not it in the population as a\nwhole. Should one approve the drug or not? The causal approach makes\nit easy to see why one should. The probabilistic relationship between\nTreatment and Success in the population is an\nevidential rather than a causal one. Learning that someone took the\ndrug provides evidence about their gender, and this information is\nrelevant to predicting whether they will recover. But this does not\ntell one about whether the drug is causally efficacious. To learn\nthis, one needs to know how the chances of recovery for individuals in\nthe population would change given an intervention on\ntreatment. This can be determined by conditioning on gender\nwhich enables one both to learn the gender-specific effects of the\ndrug, and to derive the average effect in the whole population (using\nthe back-door criterion). \nFigure 3: The DAG for the variables\nTreatment, Gender and Success (third\nvariable = confounding factor), contrasted with the DAG for the\nvariables Birth Control, Pregnancy, and\nThrombosis (third variable = mediator). \nThus, whether one should partition the population based on a factor in\norder to identify a particular causal relationship does not depend\nonly on the statistical distribution, but crucially on one’s\ncausal background assumptions. Suppose that one was considering an\nintermediate variable such as Pregnancy in Hesslow’s\n(1976) example. Recall that in the example birth control influences\nthrombosis both positively via a blood chemical and negatively by\nreducing one’s chance of getting pregnant. This case is shown in\n Figure 3\n and contrasted with our running example where the third variable is a\nconfounding factor. In order to identify the effect of birth control\non thrombosis, it is crucial that one does not condition on\npregnancy. If there are no unmeasured common causes of birth control\nand thrombosis, then a probability-raising relationship between birth\ncontrol and thrombosis in the population as a whole would reliably\nindicate that taking birth control pills promotes thrombosis. \nIt is worth emphasizing that there is no basis for distinguishing the\ntwo causal structures in\n Figure 3\n using statistics alone. Any data generated by the model on the left\ncould also have been generated by a model with the causal structure of\nthat on the right. Accordingly the judgment that one should partition\nthe population in one case but not the other cannot be based on the\nprobabilities alone, but requires the additional information supplied\nby the causal model. \nCoherent with\n Theorem 1,\n Pearl proves a causal version of Savage’s (1954) sure-thing\nprinciple (see also\n Section 5.3): \nCausal sure-thing principle (Pearl 2016) An\naction \\(\\r{C}\\) that increases the probability of an event \\(\\r{E}\\)\nin each subpopulation must also increase the probability of \\(\\r{E}\\)\nin the population as a whole, provided that the action does not change\nthe distribution of the\n subpopulations.[8] \nFor example, if one assumes that Gender is not an effect of\nTreatment, it cannot be the case that the drug raises the\nprobability of recovery in both males and females, but has no effect\non recovery in the general population. This result provides an error\ntheory for why people often find Simpson’s Paradox to be\nparadoxical in the first place. Specifically, Pearl (2000 [2009],\n2014) claims that people conflate observational claims that \\(\\r{X}\\)\nraises the probability of \\(\\r{Y}\\) with causal claims that\ndoing \\(\\r{X}\\) (versus \\(\\neg\\r{X}\\)) would raise the\nprobability of \\(\\r{Y}\\). And assuming that the partitioning variable\nis not an effect of \\(X\\), it is impossible for doing\n\\(\\r{X}\\) to raise the probability of \\(\\r{Y}\\) in all subpopulations,\nbut not in the population as a whole. So Pearl’s explanation of\nthe paradox is that people conflate causal and non-causal expressions,\nand if the conditional probabilities in the examples are interpreted\ncausally, Simpson’s reversals are impossible. \nWhether Pearl provides the correct causal explanation of\nSimpson’s Paradox remains a topic of continued debate (Armistead\n2014 see also\n Section 4).\n What should not, however, be controversial is that recent causal\nmodeling techniques enable one to systematically distinguish between\ncausal and probabilistic claims in a much more general and precise way\nthan had previously been possible. While Cartwright required that all\ncauses of \\(E\\) be included in the background context, for the sake of\neliminating confounding it is only necessary to hold fixed\ncommon causes (and other variables needed to block back-door\npaths). Theorists of probabilistic causality were to some extent aware\nthat one did not need to hold fixed all causes of the effect in order\nto eliminate confounding, but they lacked a general account of which\nvariable sets are sufficient for identifying the effect.\nSimpson’s Paradox was especially threatening, since there was no\nway to provide general conditions under which an apparent positive\ncausal relationship in a population would disappear entirely upon\npartitioning. Using Pearl’s framework, it is trivial to show\nthat as long as one does not condition on mediators, if a\nprobabilistic expression identifies an average positive effect between\n\\(X\\) and \\(Y\\) in a population, intervening on \\(X\\) must raise\n\\(Y\\)’s effect in at least some subpopulations\n(Weinberger 2015). \nTurning back to the debate about average effects in the probabilistic\nframework, this fact vindicates Dupré’s (1984) liberal\nattitude toward average effects against critics such as Eells and\nSober (1983: 54) who dismiss it as a “sorry excuse for a causal\nconcept” (though see Hitchcock 2003: 13–15, and Hausman\n2010: 56, for further nuances). Of course, a positive average effect\nis compatible with the cause lowering the probability of the effect\nsignificantly in many subpopulations. This reflects the fact that the\npartitioning variable(s) could interact with the cause of interest.\nBut such possible interactions do not make the effect any less genuine\nas an average effect for the whole population. \nThis brings us to the issue of whether Simpson’s Paradox\nthreatens the objectivity of causal relationships. Properly\nunderstood, it does not. It is certainly true that a cause can raise\nthe probability of its effect in one population and lower it in\nanother, or that it can have a positive effect in a whole population,\nbut not in some of its subpopulations. But it is not as if only some\nof these causal relationships are genuine and that philosophers must\ntherefore find a privileged background context within which the true\nrelationship is revealed. It is simply a fact about causation that\ndifferent populations can have different sets of interactive\nbackground factors, and thus the average effects will genuinely differ\nacross the populations. \nSimpson’s Paradox is not a paradox in the sense of presenting an\ninconsistent set of plausible propositions of which at least one must\nbe rejected. As shown in\n Section 2.2,\n mathematics does not rule out associations to be reversed at the\nlevel of subpopulations. Bandyopadhyay et al. (2011) helpfully\ndistinguish between three questions one could ask about\nSimpson’s Paradox: \nQuestion (i) is essentially a question about the psychology of\nreasoning: one must offer an account of why the (mathematically\ninnocent) association reversals seem paradoxical to many. Such\naccounts help to identify valid forms of inference that leads\nindividuals to mistakenly rule out association reversals, and thereby\nprovide answers to question (ii). Such analyses can differentiate\namong subtly different forms of reasoning, and open the door to\nempirical work testing whether humans systematically fail to attend to\nparticular differences. \n\n Section 3.4\n already presented one analysis of the paradox. On Pearl’s\ncausal analysis, the appearance of a paradox results\nfrom a conflation between causal and probabilistic reasoning.\nIf one interprets the claim that taking the drug raises the\nprobability of recovery as the causal statement that intervening to\ngive the drug will make patients more likely to recover, and plausibly\nassumes that taking the drug has no influence on gender, then the drug\ncannot lower the probability of recovery both among males and among\nfemales. But, of course, if one is considering ordinary conditional\nprobabilities without any do-operators, such reversals can occur.\nAccordingly, the appearance of paradox results from conflating\nordinary conditional probabilities with conditional probabilities\nrepresenting the results of interventions. \nPearl’s answer to (ii) has immediate implications for (iii). In\nevaluating the relationships between two variables \\(X\\) and \\(Y\\) and\ndetermining whether one should partition based on some variable (or\nvariable set) \\(Z\\), one should partition based on \\(Z\\) only if doing\nso will enable one to identify the causal relationship between\n\\(X\\) and \\(Y\\). This answer presupposes that the aim of\npartitioning the population is to identify causal relationships.\nQuestions about how to proceed in light of the paradox only make sense\ngiven a context and given the kind of inference one wishes to\ndraw. \nPearl (2014) presents several reasons supporting his analysis of the\nparadox. First, he argues that were the surprise resulting from the\nparadox to be the result of a mere mathematical error, this could\nneither account for why the paradox “has captured the\nfascination of statisticians, mathematicians, and philosophers for\nover a century” (2014: 9) nor for the difficulty that reasoners\nhave in avoiding the error even once they’ve been made aware of\nit. Only by means of a causal semantics can one demonstrate that\nSimpson’s reversals cannot occur when the conditional\nprobabilities are interpreted causally. Second, he points to\nSimpson’s (1951) observation that judgments about whether the\naggregated or non-aggregated population is relevant for evaluating the\ncorrelations depends on the story behind the what the\nfrequencies represent. Pearl accounts for this story-relativity by\nshowing that whether one should partition a population is decided not\nby the probabilities but rather by the causal model generating the\nprobabilities. These causal models\n\ncannot be distinguished by conditional probabilities alone. \nBandyopadhyay et al. (2011) reject Pearl’s causal analysis of\nthe paradox, and defend an alternative mathematical\nexplanation. They note that there can be instances of the\nparadox that do not seem to invoke any causal notions. For example,\nsuppose we take the proportions in Table 1 not to refer to the\nproportions of recovering/non-recovering patients among the\ntreatment/non-treatment groups in male and female populations, but\nrather to the proportions of red and blue marbles among big or small\nmarbles in two bags. Suppose that in either bag the big marbles have a\nhigher red-to-blue ratio than the small marbles. Bandyopadhyay et\nal. plausibly claim that in this case, it would be surprising to\ndiscover that, were we to pour the bags into a single box, the small\nmarbles have a higher red-to-blue ratio than the big ones. If there\nare cases of the paradox that still exhibit surprise despite having\nnothing to do with causality, then the general explanation of the\nparadox cannot be causal.[9] \nBandyopadhyay et al. rephrase the paradox as being about ratios and\nproportions: when it is the case that  \n—to be read as success proportions for treatment and control in\nthe subpopulations, compare\n Table 2—many\n people expect that these equalities are preserved in the overall\npopulation:  \nAs we know from\n Section 2,\n this need not be the case. Bandyopadhyay et al. conducted a survey\nwith university students on this matter: only 12% give the correct\nanswer that equations\n (6),\n by themselves, do not constrain the truth value of equation\n (7). \nGiven the widespread literature revealing how seemingly error-prone\nhumans can be when reasoning about probabilities (e.g., Kahneman,\nSlovic, & Tversky 1982), the proposal that Simpson’s Paradox\ncan be explained by appeal to an error in probabilistic reasoning is\nplausible. Yet Bandyopadhyay et al. do not specify what this error is.\nOr, more specifically, they do not propose a valid form of reasoning\nthat reasoners are mistakenly appealing to when falling prey to the\nparadox. The fact that people expect that the ratios in subpopulations\nto be preserved in the combined population just shows that people are\ntricked by the paradox. It does not illuminate the underlying mistake\nthat they are making when they are tricked. In this sense,\nBandyopadhyay et al. do not answer their second question. They also,\nby their own admission, do not provide a general answer to (iii). They\nview this as a virtue of their account, since they believe that\ndiscussions of (iii) ought to be divorced from discussions of (i) and\n(ii). \nRecently, Fitelson (2017) has proposed a\nconfirmation-theoretic explanation of Simpson’s\nParadox. His analysis relies on identifying confirmation with\nincreasing the (subjective) probability of a proposition. Statements\nof the form “evidence \\(\\r{E}\\) confirms hypothesis\n\\(\\r{H}\\)” are, however, usually evaluated with respect to\nbackground knowledge K, and this can lead to ambiguities. In\nparticular, Fitelson distinguishes between the suppositional\nand conjunctive readings of a confirmation statement. In our\nrunning example, these statements would be as follows: \nSuppositional (\\(\\bf \\r{E}\\) raises the probability of\n\\(\\bf\\r{H}\\) given \\(\\b{K}\\)): If one is female, then\nreceiving treatment increases one’s chance of recovery. \nConjunctive (\\(\\bf \\r{E}\\wedge\\b{K}\\) raises the probability\nof \\(\\bf \\r{H}\\)): Being a female treatment-receiver\nincreases one’s chance of recovery. \nWhile the suppositional and conjunctive reading coincide for some\naccounts of confirmation (e.g., Carnap’s account of degree of\nconfirmation as conditional probability), they can produce different\noutcomes for confirmation as probability-raising. For our\ndata in\n Table 1,\n the suppositional reading is true: if one is in the female\nsubpopulation, receiving treatment rather than being in the control\ngroup increases one’s chances of recovery. On the conjunctive\nreading, however, the statement is false: female treatment-receivers\nare less-likely to recover (12/27) compared to the set of individuals\nwho are either male or did not receive the treatment (16/25). More\nimportantly, while the suppositional reading allows for association\nreversals, on the conjunctive reading it cannot be the case both that\nbeing a female treatment-receiver and being a male treatment-receiver\nraises the probability of recovery, but being a treatment receiver\nsimpliciter does not (Fitelson 2017: 300–302). \nFitelson’s confirmation-theoretic explanation of Simpson’s\nParadox is that reasoners are not attentive to the difference between\nthe suppositional and conjunctive readings of confirmation statements\nwhen considering the evidential relevance of learning an\nindividual’s gender. In the conjunctive reading there cannot be\nassociation reversals, and because the suppositional and conjunctive\nreading do not differ for many accounts of confirmation, people\nmistakenly assume that there cannot be such reversals, even when they\nare relying on the suppositional reading. \nBoth Bandyopadhyay et al. and Fitelson claim that because the\nformulation of Simpson’s paradox does not itself appeal to\ncausal considerations, it is a preferable to find a non-causal\nexplanation for the paradox. Ultimately, it is an empirical question\nwhether the paradox can be accounted for exclusively by errors in\nprobabilistic reasoning, or, as Pearl suggests, due to a conflation of\ncausal and probabilistic reasoning. One conceptual barrier to\ndisentangling these hypotheses is that there are systematic\nrelationships between causal and probabilistic claims For example,\nwhen the third variable \\(\\r{M}\\) is uncorrelated with treatment \\(T\\)\n(i.e., \\(p(\\r{T}\\mid \\r{M}) = p(\\r{T})\\)), there can be no reversals\n(see also the theorems in\n Section 2.2).\n Does it follow that Simpson’s Paradox has a purely\nprobabilistic explanation? Not necessarily. An alternative hypothesis\nis that the epistemic agent does not have knowledge of the relevant\nconditional probabilities, but does know that \\(M\\) is not a cause of\n\\(T\\) \\((p(\\r{T}\\mid \\do(\\r{M})) = p(\\r{T})\\)), preempting the\noccurrence of association reversals. The question of whether the\nsource of the paradox is causal cannot be resolved purely by appeal to\nthe mathematical conditions under which the it arises. Rather, it\ndepends on substantive psychological hypotheses about the role of\ncausal and probabilistic assumptions in human\n reasoning.[10] \nThe empirical evidence on the paradox shows that reasoners find\ntrivariate reasoning (i.e., with a causally relevant third variable)\ngenerally hard and fail to take its role properly into account, even\nif salient cues to its relevance are provided (Fiedler, Walther,\nFreytag, & Nickel 2003). Other studies point to the facilitative\neffect of causal model, statistical training and high motivation\n(Schaller 1992; Waldmann & Hagmayer 1995), but the significant\ndifficulties that reasoners encounter in Simpson-like tasks make it\nunlikely that the question of the right analysis of the paradox will\nsoon be decided empirically. \nTable 4: Verbal SAT score data for\nAmerican high schools, taken from Rinott & Tam (2003). \nSimpson’s Paradox is not limited to categorical data: it can\noccur for cardinal data as well and show up in standard models for\nquantitative analysis. A famous example is the analysis of SAT\nscores—the results of college admission tests—in the\nUnited States as a function of the high school grade point average\n(GPA) of students. The data are given in\n Table 4:\n the overall SAT average rises from 1992 to 2002, but for each GPA\ngroup (A+/A/…), SAT averages are falling. This phenomenon is,\nhowever, very natural. As soon as there is a bit of grade inflation at\nhigh schools, each group loses their best students to the next higher\ngroup, lowering the SAT average per group. But this is of course\nconsistent with the overall SAT average remaining equal, or even\nrising from 501 to 516, like in our dataset. A conclusion from the\nstratified data that “students are getting more stupid”\nwould be mistaken. Since societal developments such as grade inflation\naffect both the grade distribution and the SAT scores, one should\nnot condition on the GPA of a student when studying SAT\nscores over time (compare the back-door criterion from\n Section 3.4).[11] \nFigure 4: A linear regression model that\nillustrates Simpson’s Paradox for bivariate cardinal data. Each\ncluster of values corresponds to a single person (repeated\nmeasurement). \nA similar example is presented in\n Figure 4,\n adapted from Kievit, Frankenhuis, Waldorp, and Borsboom (2013). The\nfigure shows the results of coffee intake on performance at an IQ test. Suppose\nthat coffee actually decreases performance slightly because it\nmakes drinkers more nervous and less focused. At the same time, coffee\nintake co-varies with education level (construction workers are too\nbusy for drinking coffee all the time!) and education level co-varies\nwith test performance. When we measure performance repeatedly for\ndifferent individuals, we see that their performance is slightly\nnegatively affected by their coffee intake. However, the\n(unconditional) regression model of performance as a function of\ncoffee intake suggests misleadingly\nthat coffee consumption strongly improves performance! The reason\nfor the confounding is the causal impact of the hidden covariate,\neducation level, on both coffee consumption and performance. Similar\nto the results from\n section 2,\n Simpson’s Paradox in linear models can be characterized\nformally by means of inequalities among regression coefficients (e.g.,\nPearl 2013), and its occurrence depends on the nature of the causal\ninteraction between the involved variables. \nSimpson’s Paradox in its various forms has attracted a lot of\nattention in the epidemiological literature since it is relevant for\ndetermining and estimating the effect size of medical treatments, and\nthe effect of exposure to risk factors (e.g., smoking, alcohol) on\nmedical hazards. \nOne of the aims behind the methodology of randomized controlled trials\n(RCTs) is to eliminate the effect of potential confounders on whether\na person is treated or not. This was described in\n Section 2.2\n as row-uniform design (for experiments with categorical data). For\nexample, if we ensure the same proportion of both genders in the\ntreatment and control group, the same prevalence of different age\ngroups, etc., we know that association reversal (AR) cannot occur with\nrespect to those third variables, and also the amalgamation paradox\n(AMP) is ruled out for many measures. \nHowever, the (log-)odds ratio, a popular measure of effect size in\nepidemiological research, shows a deviant behavior. Uniformly\nassigning individuals to treatment and control condition reliably\nproduces the AMP for the odds ratio whenever the third\nvariable (=the subpopulation attribute) influences the success rate,\ngiven the treatment level (Theorem 2.4, Samuels 1993). The odds ratio\nis thus a particularly tricky association measure. Greenland (1987)\ngives the instructive example of an odds ratio that is equal in all\nsubpopulations with row-uniform design, but halved when data are\npooled. \nMeta-analytic problems, such as pooling various\nstudies for determining the overall effect size of an intervention or\nrisk factor give a particularly interesting twist to Simpson’s\nParadox. How should such studies be aggregated? Naïvely, somebody\nmay suggest to pool the data from all studies and to treat them as a\nsingle big study. This may work out if the study populations are very\nsimilar and the data are from RCTs, where the treatment/control ratio\nis typically 50:50. If this is indeed the case, then the overall\ndataset is row-uniform and AR (and for most measures, AMP) is avoided,\nas shown in\n Section 2.2.\n But for non-experimental data, there is no reason to assume that\ntreatment/control proportions are equal across studies. Thus, the\ndirection of the direction of the effect can be reversed when pooling\n(for examples, see Hanley & Thériault 2000; Reintjes, Boer,\nPelt, & Mintjes-de Groot 2000; Rücker & Schumacher\n2008). \nAnother reason for not pooling the data is that study populations are\noften heterogenous and that calculating the strength of association\n(i.e., the effect size) on the basis of the data may bias the estimate\nin the direction of the study with the largest sample size, while the\ncharacteristics of patients in that study need not be representative\nof the target group as a whole. In particular, while at the level of\nstudies patients are usually assigned randomly to the treatment or\ncontrol group, this cannot be said about the aggregate data (Cates\n2002). Proper meta-analysis therefore proceeds on the basis of\nweighting the effects rather than pooling the data, either by a fixed\neffects model or (e.g., if the study populations are heterogenous) by\nintroducing a random effect of the study in the statistical model. The\nquestion of how to conduct a meta-analysis of epidemiological studies\nis also entangled with the choice of an association or effect size\nmeasure (Altman & Deeks 2002; Cates 2002; Greenland 1987), a\nquestion discussed in\n Section 2.2. \nBlyth (1972) argued that Simpson’s Paradox also constitutes a\ncounterexample to the sure-thing principle of decision theory, or at\nleast restricts its scope substantially. That principle is supposed to\nguide rational decisions under uncertainty, and has been stated by\nSavage as follows: \nSure-Thing Principle (STP) “If you\nwould definitely prefer \\(g\\) to \\(f\\) either knowing that the event\n\\(\\r{B}\\) obtained, or knowing that the event B did not obtain, then\nyou definitely prefer \\(g\\) to \\(f\\)”. (Savage 1954:\n21–22) \nIn his purported counterexample, Blyth treats \\(\\r{B}\\) and\n\\(\\neg\\r{B}\\) as indicating the two subpopulations (e.g., two\ndifferent hospitals). Suppose that treatment \\(\\r{T}\\) is positively\nassociated with recovery \\(\\r{R}\\) for each subpopulation. In that\ncase, assuming equal odds, we would rather bet on the recovery of a\npatient in the treatment group (action \\(g\\)) than on the recovery on\nthe patient in a control group (action \\(f\\))—regardless of\nwhether that person is in group \\(\\r{B}\\) or group \\(\\neg\\r{B}\\).\nThus, since we prefer \\(g\\) to \\(f\\) in either subpopulation, and\nsince all patients are either in group \\(\\r{B}\\) or in group\n\\(\\neg\\r{B}\\), we can infer, by the Sure-Thing Principle, that \\(g\\)\nis preferable to \\(f\\) also when we don’t know whether a patient\nis in group \\(\\r{B}\\) or group \\(\\neg\\r{B}\\). But this inference is\nmistaken if association reversal occurs: it is perfectly compatible\nwith the above scenario that the overall frequency of recovery is\nhigher for non-treated than for treated patients! Blyth (1972: 366)\nconcludes that  \nthe Sure-Thing Principle […] seems not applicable to situations\nin which any action taken within \\(f\\) or \\(g\\) […] is allowed\nto be based sequentially on events dependent with [\\(B\\)].  \nSee also Malinas (2001) for discussion. \nTo the extent that (conditional) degrees of belief just represent\n(conditional) dispositions to bet, Blyth’s reasoning is\ncompelling. Association reversal means that  \nalthough  \nand  \nand thus preference for a conditional bet on \\(\\r{T}\\) (given the\nvarious levels of \\(B\\)) does not imply preference for the\nunconditional bet on \\(\\r{T}\\) (see\n Section 2).\n However, Savage certainly did not intend the sure-thing principle to\nbe a theorem of probability. To evaluate it as a principle that guides\nproper decision-making, we must consider cases where the predictor\nvariable (here: treatment/control) stands for a proper act\nthat affects the outcome via multiple paths. \nJeffrey (1982) recalls Savage’s (1954: 21) example of a\nbusinessman who believes that it is advantageous to buy a property\nregardless of whether the Democratic or the Republican candidate will\nwin the upcoming mayor elections. Jeffrey’s twist is that the\nbusinessman’s utility depends not only on the property deal, but\nalso on the election outcome. Specifically, buying the property raises\nthe chances that the Democratic candidate, whom he dislikes, will win.\nIn that case he would certainly buy the property after the\nelection, regardless of the outcome, but he may refrain from buying it\nbefore the election. \nIn response to this challenge, Jeffrey (1982: 720) restricts the\nsure-thing principle to the case where  \nchoice of one act or another is thought to have no tendency to\nfacilitate or impede the coming about of any of the possible states of\nnature, and […] this is reflected in a probabilistic\nindependence of states from acts.  \nThat is, buying the property should not change our rational degree of\nbelief in who wins the election. Pearl (2016) considers this response\nan “overkill” and notes that probabilistic associations\nare not a good means of expressing causal tendencies. Therefore he\nproposes a causal sure-thing principle that we have encountered in\n Section 3.4:\n If one is considering two acts \\(f\\) and \\(g\\), and the\nprobability distribution of \\(\\r{B}\\) does not change depending on\nwhether one intervenes to choose \\(f\\) or \\(g\\), then if one\nprefers \\(f\\) to \\(g\\) whether or not \\(B\\) occurs, one prefers \\(f\\)\nunconditionally. The italicized condition ensures that the\npartitioning variable is not an effect of the intervention, and thus\nrules out Simpson’s reversals (see\n section 3.4).\n Note that Pearl’s formulation, but not Jeffrey’s, allows\nto apply the (causal) sure-thing principle to observational data,\nwhere states and acts may be statistically dependent without\nindicating genuine causation (e.g., because of self-selection\neffects). \nThroughout this entry we have assumed knowledge of the causal facts\npertinent to a situation. Scenarios in which an agent lacks such\nknowledge raise additional complications for decision theory. An agent\ntypically cannot ensure that all confounders have been accounted for,\nand thus the possibility of repeated reversals raises questions about\nwhen one should adopt a promising policy that has not been\nexperimentally tested (Peters, Janzing, & Schölkopf 2017:\n174–175). A distinct concern is that an agent may not be sure\nwhether her action counts as an intervention (e.g., in Newcomb\nscenarios), since it might not be clear whether she can manipulate a\nvariable to render it independent of its prior causes (Stern 2019).\nWhether Simpson’s Paradox raises novel difficulties in such\ndecision-making contexts has not yet been explored. See the entries on\n decision theory\n and\n causal decision theory\n for further discussion. \nWithin the philosophy of biology, the units of\nselection debate (Sober 2000 [2018: ch. 4], 2014; Williams\n1966) concerns whether natural selection operates only at the level of\nthe individual or also on groups (where the individual is typically\nconceived either as the organism or the gene). This debate is\nespecially important for understanding the evolution of altruism\n(Sober & Wilson 1999). Since altruistic individuals harm their own\nchances of survival and reproduction, they are less fit, and it is\nthus unclear how altruism could evolve as a result of natural\nselection. If, however, groups with more altruists are fitter than\ngroups with fewer, and selection can act on groups, this could\npotentially explain how altruism could still evolve. Within the units\nof selection debate, Simpson reversals have played an\nimportant role in explaining the possibility of group-level\nselection. \nConsider the following naive argument against the conceptual\npossibility of group-level\n selection.[12]\n Suppose that we define the fitness of a group as the average fitness\nof its individuals. In this context, altruistic individuals are, by\ndefinition, those with traits that reduce their individual fitness\nwhile improving the fitness of other group members. For instance,\ncrows that issue warning cries when a predator approaches benefit the\ngroup while increasing the chances of being harmed themselves. Natural\nselection explains the evolution of traits on the basis that\nindividuals with the trait are fitter than those without it (all else\nbeing equal). Since selfish individuals are by definition fitter than\naltruistic ones, it follows that groups with more altruistic\nindividuals cannot be fitter. Or so one might argue. \nBy now it should be clear what is wrong with this type of\nargument—it does not follow from the fact that altruistic\nindividuals are less fit than selfish ones in every population that\npopulations which average over selfish and altruistic individuals\ncannot be fitter than populations with just selfish individuals. It\ncould be that being an altruist is correlated with being in a\npopulation with more altruists, and that populations with more\naltruists are fitter. This dispenses with the naive argument. Note,\nhowever, that within every single group selfish individuals are\nfitter, so if the groups change membership only through reproduction\n(as opposed to migration and mutation) then over enough generations\nevery group will end up consisting only of selfish individuals. So\nwhether groups selection can occur depends on additional facts about\npopulation structure and dynamics. Hamilton’s (1964) Kin\nSelection theory explains how altruism can evolve in cases where\naltruists are more likely to associate with other altruists (possibly\nbecause it runs in the family). \nThe group selection hypothesis remains controversial among biologists.\nThe present discussion reveals how the phenomenon of Simpson’s\nParadox is relevant to theorizing how it might be possible, and more\nbroadly reveals how philosophical work on causation and probability\ncan aid in clarifying scientific debates. \nRecently, Simpson’s Paradox has been invoked in an ongoing\ndebate regarding whether natural selection should be understood as\ncausal or statistical. Walsh (2010), a prominent defender of the\nstatistical view, points to cases of Simpson’s Paradox as\nshowing that selection cannot be understood causally, and Otsuka,\nTurner, Allen, & Lloyd (2011) rebut this claim. An important point\nthat emerges from this debate is that the term\n“population” is used differently in discussions of\nSimpson’s Paradox than it is in biology (cf. Weinberger 2018).\nWalsh presents an example in which a correlation in a population\ndisappears when one splits the population into two parts. As Otsuka et\nal. point out, within population genetics, population size can be\ncausally relevant to the fitness of its individuals. Note that\nWalsh’s example of dividing a population in half is not what we\nhave been talking about in the context of Simpson’s Paradox. In\nthe prior discussion, dividing the population was not a matter of\nchanging its size, but rather of partitioning its probability\ndistribution based on a variable. \nBickel et al. (1975) present a classic example of Simpson’s\nParadox involving a study of gender discrimination at Berkeley. The\ndata revealed that men were more likely than women to be accepted to\nthe university’s graduate programs, but the authors were unable\nto detect a bias towards men in any individual department. The authors\nuse the paradox to explain why the higher university-wide acceptance\nrate for men does not show that any department discriminated against\nwomen. Specifically, women were more likely to apply to departments\nwith lower acceptance rates. This leads to a probabilistic association\nbetween gender and the partitioning variable (department), which we\nhave seen can lead to Simpson’s reversals. \nWhile the probabilistic structure of the Berkeley case is similar to\nother instances of the paradox, it raises an additional question. On a\nnatural way to understand the case, the applicant’s gender is a\ncause of his or her applying to a more or less selective departments.\nExactly what it means for demographic variables such as gender or race\nto be a cause is a longer story for another day (Glymour & Glymour\n2014; Sen & Wasow 2016). But assuming that gender is a cause here,\nthen the department variable is a mediator, and one should\nnot condition on mediators in evaluating the mediated causal\nrelationship. So what is the justification for conditioning on\ndepartment? \nThe answer is that in evaluating discrimination, what often matters\nare path-specific effects, rather than the net effect\nalong all paths (Pearl 2000 [2009: 4.5.3]; Zhang & Bareinboim\n2018). To give a different example (Pearl 2001), consider whether a\nhypothetical black job candidate was discriminated against based on\nher race. It is possible that as a result of prior racial\ndiscrimination, the candidate was denied opportunities to develop\njob-relevant qualifications, and as a result of lacking these\nqualifications was denied the job. This indirect effect of race hiring\nwould not be relevant for determining whether an employer\ndiscriminated against the candidate. Rather, what matters is whether\nthe employee would have been more likely to get the position had she\nbeen white, but had the same qualifications that she does as a result\nof being black. This is called the natural direct\neffect (Pearl 2001; Weinberger 2019). In determining whether\nthe employer discriminated, what matters is not whether being black\nmade any difference in the person’s being hired, but\nrather whether their being black had a direct influence not via their\njob-relevant qualifications. \nThe common explanation for the Berkeley data, on which the paradox\nresults from women applying to more selective department, points to a\nlarger class of cases in which it is important to account for\ndifferences in the difficulty-level across tasks. In baseball, for\ninstance, it appears that over time batters have been striking out\nmore frequently, despite their improving in their ability to hit more\ndifficult pitches while remaining as good at hitting less difficult\nones (Watt 2016 [see Other Internet Resources]). \nThis could be accounted for by the fact that\npitchers have been throwing a higher proportion of difficult-to-hit\npitches. This highlights the way that statistics about success rates\nin performing a task can be misleading in cases where the\ntask-difficulty changes over time. \nSimpson’s Paradox is not only a surprising mathematical fact; it\nserves as a lens through which to understand the role of probabilities\nin data analysis, causal inference, and decision-making. In this\narticle, we have characterized its mathematical properties, given\nnecessary and sufficient conditions for its occurrence, discussed its\ninfluence on theories of causality, evaluated competing theories of\nthe nature of the paradox, and surveyed its applications in a range of\nempirical domains. \nAlthough Simpson’s Paradox has been known for over a century and\nhas a straightforward probabilistic analysis, we predict that it will\nremain a source of fruitful philosophical discussion. Pearl’s\ncausal analysis of the paradox is relatively recent, and it is only\nnow that graphical causal models are starting to play a central role\nin philosophical discussions of the paradox. Despite the continuity\nbetween graphical accounts and earlier probabilistic theories of\ncausality, here we have highlighted ways in which the newer methods\nlead one to draw substantially different implications from the\nparadox. Pearl’s account renders certain debates from the\nearlier literature moot, while opening up new debates about the proper\ninterpretation of the paradox. The responses to Pearl considered in\n section 4\n are only the first steps in a broader discussion about the\nrelationships between causation, probability theory, and the\npsychology of reasoning. There remains room to clarify what it means\nto explain the paradox, and what counts as empirical support for a\nparticular explanation. Such work would open the door to empirical\ntesting, which has thus far been limited. \nFinally, we would like to highlight connections between\nSimpson’s Paradox and other reasoning fallacies in the\nliterature. First, the base rate fallacy is related to\nSimpson’s Paradox since the illusion that association reversal\nis impossible may be based on a neglect of the different base rates\nfor treated and untreated people, given the third variable (Bar-Hillel\n1990). Second, the fallacy of mistaking correlation for\ncausation may contribute to the appearance of paradoxicality\nsince association reversal implies two contradicting causal claims,\nwhen combined with this fallacy. Third, in both Simpson’s\nParadox and the Monty Hall fallacy reasoners fail to see the\nprobabilistic relevance of causal information. While in\nSimpson’s Paradox, reasoners ignore the relevance of a back-door\npath for an observed association, in the Monty Hall problem, reasoners\nfail to take into account how Monty’s action depends on his\nknowledge of what is in back of the doors. Fourth, and last, the\ncapacity of reasoners to detect the causes of association reversal\nalso depends on the extent of the confirmation bias to which\nthey are exposed (e.g., whether or not they find a discrimination\nmechanism plausible). We are unaware of systematic research into the\nconnection between Simpson’s Paradox and these reasoning\nfallacies, but this could be a fruitful field for future research.\nThere is perhaps nothing paradoxical about Simpson’s Paradox,\nbut since we often struggle to understand it, our reasoning about\nassociation reversals may be entangled with various forms of reasoning\nthat are susceptible to bias and error.","contact.mail":"naftali.weinberger@gmail.com","contact.domain":"gmail.com"}]
