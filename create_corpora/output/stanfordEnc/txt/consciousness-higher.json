[{"date.published":"2001-04-03","date.changed":"2020-09-02","url":"https://plato.stanford.edu/entries/consciousness-higher/","author1":"Peter Carruthers","author2":"Rocco Gennaro","author1.info":"http://www.philosophy.umd.edu/Faculty/pcarruthers/","entry":"consciousness-higher","body.text":"\n\n\nHigher-order theories of consciousness try to explain the difference between unconscious and conscious mental states in terms of a relation obtaining\nbetween the conscious state in question and a higher-order\nrepresentation of some sort (either a higher-order perception of that\nstate, or a higher-order thought about it). The most\nchallenging properties to explain are those involved in\nphenomenal consciousness—the sort of state that has a\nsubjective dimension, that has ‘feel’, or that it\nis like something to undergo. These properties will\nform the focus of this article. \n\nOne of the advances made in the last few decades has been to\ndistinguish between different questions concerning consciousness (see\nparticularly: Rosenthal 1986; Dretske 1993; Block 1995; Lycan 1996).\nNot everyone agrees on quite which distinctions need to be\ndrawn. But all are agreed that we should distinguish creature\nconsciousness from mental-state consciousness. It is one\nthing to say of an individual person or organism that it is\nconscious (either in general or of something in particular); and it is\nquite another thing to say of one of the mental states of a\ncreature that it is conscious. \nIt is also agreed that within creature-consciousness itself we should\ndistinguish between intransitive and transitive\nvariants. To say of an organism that it is conscious\nsimpliciter (intransitive) is to say just that it is awake,\nas opposed to asleep or comatose. There don’t appear to be any deep philosophical difficulties lurking here (or at least, they aren’t difficulties specific to the topic of\nconsciousness, as opposed to mentality in general). But to say of an\norganism that it is conscious of such-and-such (transitive)\nis normally to say at least that it is perceiving\nsuch-and-such, or aware of such-and-such. So we say of the\nmouse that it is conscious of the cat outside its hole, in explaining\nwhy it doesn’t come out; meaning that it perceives the cat’s\npresence. To provide an account of transitive creature-consciousness\nwould thus be to attempt a theory of perception. \nThere is a choice to be made concerning transitive\ncreature-consciousness, failure to notice which may be a potential\nsource of confusion. For we have to decide whether the perceptual\nstate in virtue of which an organism may be said to be\ntransitively-conscious of something must itself be a conscious one\n(state-conscious—see below). If we say ‘Yes’ then we\nshall need to know more about the mouse than merely that it perceives\nthe cat if we are to be assured that it is conscious of the\ncat—we shall need to establish that its percept of the cat is\nitself conscious. If we say ‘No’, on the other hand, then\nthe mouse’s perception of the cat will be sufficient for the mouse to\ncount as conscious of the cat; but we may have to say that although it\nis conscious of the cat, the mental state in virtue of which it is so\nconscious is not itself a conscious one! It may be best to by-pass any\ndanger of confusion here by avoiding the language of\ntransitive-creature-consciousness altogether. Nothing of importance\nwould be lost to us by doing this.  \nTurning now to the notion of mental-state consciousness, the\nmajor distinction here is between phenomenal consciousness,\non the one hand—which is a property of states that it is\nlike something to be in, that have a distinctive\n‘feel’ (Nagel 1974)—and various\nfunctionally-definable forms of access consciousness, on the\nother (Block 1995). Most theorists believe that there are mental\nstates—such as occurrent thoughts or judgments—that are\naccess-conscious (in whatever is the correct functionally-definable\nsense), but that are not phenomenally conscious. In contrast, there is\nconsiderable dispute as to whether mental states can be\nphenomenally-conscious without also being conscious in the\nfunctionally-definable sense—and even more dispute about whether\nphenomenal consciousness can be reductively explained in\nfunctional and/or representational terms. \nIt is plain that there is nothing deeply problematic about\nfunctionally-definable notions of mental-state consciousness, from a\nnaturalistic perspective. For mental functions and mental\nrepresentations are the staple fare of naturalistic accounts of the\nmind. But this leaves plenty of room for dispute about the form that\nthe correct functional account should take. Some claim that for a\nstate to be conscious in the relevant sense is for it to be poised to\nhave an impact on the organism’s decision-making processes (Kirk 1994;\nDretske 1995; Tye 1995, 2000), perhaps also with the additional requirement\nthat those processes should be distinctively rational ones\n(Block 1995). Others think that the relevant requirement for\naccess-consciousness is that the state should be suitably related to\nhigher-order representations—experiences and/or thoughts—of\nthat very state (Armstrong 1968, 1984; Rosenthal 1986, 1993, 2005;\nDennett 1978a, 1991; Carruthers 1996, 2000, 2005; Lycan 1987,\n1996; Gennaro 2012). \nWhat is often thought to be naturalistically problematic, in\ncontrast, is phenomenal consciousness (Nagel 1974, 1984; Jackson 1982,\n1986; McGinn 1991; Block 1995; Chalmers 1996). And what is really and\ndeeply controversial is whether phenomenal consciousness can be\nexplained in terms of some or other functionally-definable\nnotion. Cognitive (or representational) theories\nmaintain that it can. Higher-order cognitive theories\nmaintain that phenomenal consciousness can be reductively explained in\nterms of representations (either experiences or thoughts) that are\nhigher-order. It is such theories that concern us here. \nHigher-order theories, like cognitive/representational theories in\ngeneral, assume that the right level at which to seek an\nexplanation of phenomenal consciousness is a cognitive one, providing\nan explanation in terms of some combination of causal role\nand intentional content. All such theories claim that\nphenomenal consciousness consists in a certain kind of intentional or\nrepresentational content (analog or\n‘fine-grained’ in comparison with any concepts we possess)\nfiguring in a certain distinctive position in the causal architecture\nof the mind. They must therefore maintain that these latter sorts of\nmental property don’t already implicate or presuppose phenomenal\nconsciousness. In fact, all cognitive accounts are united in rejecting\nthe thesis that the very properties of mind or\nmentality already presuppose phenomenal consciousness, as\nproposed by Searle (1992, 1997) for example. The higher-order approach does not attempt to reduce consciousness directly to neurophysiology but rather its reduction is in mentalistic terms, that is, by using such notions as thoughts and awareness. \nThe major divide amongst representational theories of phenomenal\nconsciousness in general, is between accounts that are provided in\npurely first-order terms and those that implicate higher-order\nrepresentations of one sort or another (see below). These higher-order\ntheorists will allow that first-order accounts—of the sort\ndefended by Dretske (1995) and Tye (1995), for example—can\nalready make some progress with the problem of consciousness.\nAccording to first-order views, phenomenal consciousness consists in\nanalog or fine-grained contents that are available to the first-order\nprocesses that guide thought and action. So a phenomenally-conscious\npercept of red, for example, consists in a state with the analog\ncontent red which is tokened in such a way as to feed into\nthoughts about red, or into actions that are in one way or another\nguided by redness. Now, the point to note in favor of such an account\nis that it can explain the natural temptation to think that phenomenal\nconsciousness is in some sense ineffable, or\nindescribable. This will be because such states have\nfine-grained contents that can slip through the mesh of any conceptual\nnet. We can always distinguish many more shades of red than we have\nconcepts for, or could describe in language (other than\nindexically—e.g., ‘That shade’). \nThe main motivation behind higher-order theories of consciousness, in\ncontrast, derives from the belief that all (or at least most)\nmental-state types admit of both conscious and unconscious varieties.\nAlmost everyone now accepts, for example, (post-Freud) that beliefs\nand desires can be activated unconsciously. (Think, here, of the way\nin which problems can apparently become resolved during sleep, or\nwhile one’s attention is directed to other tasks. Notice, too, that\nappeals to unconscious intentional states are now routine in cognitive\nscience.) And then if we ask what makes the difference between a\nconscious and an unconscious mental state, one natural answer is that\nconscious states are states that we are aware of. And if\nawareness is thought to be a form of creature-consciousness\n(see section 1 above), then this will translate into the view that\nconscious states are states of which the subject is aware, or\nstates of which the subject is creature-conscious. That is to say,\nthese are states that are the objects of some sort of higher-order\nrepresentation—whether a higher-order perception or experience,\nor a higher-order thought. This is similar to the widely referenced Transitivity Principle (TP) which says that a conscious state is a state whose subject is, in some way, aware of being in it. On the other hand, a mental state of which a subject is completely unaware is clearly an unconscious state. (See also Lycan’s (2001b)‘related simple argument’ for a higher-order representation account of\nconsciousness.) \nOne crucial question, then, is whether perceptual states as well as\nintentional states admit of both conscious and unconscious varieties. Can there\nbe, for example, such a thing as an unconscious visual perceptual\nstate? Higher-order theorists are united in thinking that there can.\nArmstrong (1968) uses the example of absent-minded driving to make the\npoint. Most of us at some time have had the rather unnerving\nexperience of ‘coming to’ after having been driving on\n‘automatic pilot’ while our attention was directed\nelsewhere—perhaps having been day-dreaming or engaged in intense\nconversation with a passenger. We were apparently not consciously\naware of much of the route we have recently taken, nor of any of the\nobstacles we avoided on the way. Yet we must surely have been\nseeing, or we would have crashed the car. Others have used\nthe example of blindsight (Carruthers 1989, 1996). This is a condition\nin which subjects have had a portion of their primary visual cortex\ndestroyed, and apparently become blind in a region of their visual\nfield as a result. But it has now been known for some time that if\nsubjects are asked to guess at the properties of their\n‘blind’ field (e.g. whether it contains a horizontal or\nvertical grating, or whether it contains an ‘X’ or an\n‘O’), they prove remarkably accurate. Subjects can also\nreach out and grasp objects in their ‘blind’ field with\nsomething like 80% or more of normal accuracy, and can catch a ball\nthrown from their ‘blind’ side, all without conscious\nawareness. (See Weiskrantz 1986, 1997, for details and\ndiscussion.) \nA powerful case for the existence of unconscious visual\nexperience has also been generated by the two-systems theory of\nvision proposed and defended by Milner and Goodale (1995; see also\nJacob and Jeannerod 2003; Glover 2004). They review a wide variety of\nkinds of neurological and neuro-psychological evidence for the\nsubstantial independence of two distinct visual systems, instantiated\nin the temporal and parietal lobes respectively. They conclude that\nthe parietal lobes provide a set of specialized semi-independent\nmodules for the on-line visual control of action; whereas the temporal\nlobes are primarily concerned with more off-line functions such as\nvisual learning and object recognition. And only the perceptions\ngenerated by the temporal-lobe system are phenomenally conscious, on\ntheir account. (Note that this isn’t the familiar distinction between what\nand where visual systems, but is rather a successor to it.\nFor the temporal-lobe system is supposed to have access both to\nproperty information and to spatial information. Instead, it is a\ndistinction between a combined what-where system located in\nthe temporal lobes and a how-to or action-guiding system\nlocated in the parietal lobes.) \nTo get the flavor of Milner and Goodale’s hypothesis, consider just\none strand from the wealth of evidence that they provide. This is a\nneurological syndrome called visual form agnosia, which\nresults from damage localized to both temporal lobes, leaving primary\nvisual cortex and the parietal lobes intact. (Visual form agnosia is\nnormally caused by carbon monoxide poisoning, for reasons that are\nlittle understood.) Such patients cannot recognize objects or shapes,\nand may be capable of little conscious visual experience; but their\nsensorimotor abilities remain largely intact. \nOne particular patient—D.F.—has now been examined in\nconsiderable detail. While D.F. is severely agnosic, she isn’t\ncompletely lacking in conscious visual experience. Her capacities to\nperceive colors and textures are almost completely preserved. (Why\njust these sub-modules in her temporal cortex should have been spared\nisn’t known.) As a result, she can sometimes guess the identity of a\npresented object—recognizing a banana, say, from its yellow\ncolor and the distinctive texture of its surface. But she is unable to\nperceive the shape of the banana (whether straight or curved, say);\nnor its orientation (upright or horizontal; pointing towards her or\nacross). Yet many of her sensorimotor abilities are close to\nnormal—she would be able to reach out and grasp the banana,\norienting her hand and wrist appropriately for its position and\norientation, and using a normal and appropriate finger grip. Under\nexperimental conditions it turns out that although D.F. is at chance\nwhen identifying the orientation of a broad line or letter-box, she is\nalmost normal when posting a letter through a similarly-shaped slot\noriented at random angles. In the same way, although she is at chance\nwhen trying to discriminate between rectangular blocks of very\ndifferent sizes, her reaching and grasping behaviors when asked to\npick up such a block are virtually indistinguishable from those of\nnormal controls. It is very hard to make sense of these data without\nsupposing that the sensorimotor perceptual system is functionally and\nanatomically distinct from the object-recognition/conscious\nsystem. \nBut what implications does this have for phenomenal consciousness?\nMust these unconscious percepts also be lacking in phenomenal\nproperties? Most people think so. While it may be possible to get\noneself to believe that the perceptions of the absent-minded car\ndriver can remain phenomenally conscious (perhaps lying outside of the\nfocus of attention, or being instantly forgotten), it is very hard to\nbelieve that either blindsight percepts or D.F.’s sensorimotor\nperceptual states might be phenomenally conscious ones. For these\nperceptions are ones to which the subjects of those states are\nblind, and of which they cannot be aware. And the\nquestion, then, is what makes the relevant difference? What is it\nabout a conscious perception that renders it phenomenal, that\na blindsight perceptual state would correspondingly lack? Higher-order\ntheorists are united in thinking that the relevant difference consists\nin the presence of something higher-order in the first case\nthat is absent in the second. The same would go for the difference between unconscious and conscious desires, emotions, pains, and so on. The core intuition, again, is that a\nphenomenally conscious state will be a state of which the subject\nis aware. \nWhat options does a first-order theorist have to resist this\nconclusion? One is to deny that the data are as problematic as they\nappear (as does Dretske 1995). It can be said that the unconscious\nstates in question lack the kind of fineness of grain and richness of\ncontent necessary to count as genuinely perceptual states. On\nthis view, the contrast discussed above isn’t really a difference\nbetween conscious and unconscious perceptions, but rather between\nconscious perceptions, on the one hand, and unconscious belief-like\nstates, on the other. Another option is to accept the distinction\nbetween conscious and unconscious perceptions, and then to explain\nthat distinction in first-order terms. It might be said, for example,\nthat conscious perceptions are those that are available to\nbelief and thought, whereas unconscious ones are\nthose that are available to guide movement (Kirk 1994). A\nfinal option is to bite the bullet, and insist that blindsight and\nsensorimotor perceptual states are indeed phenomenally conscious while\nnot being access-conscious. (See Block 1995; Tye 1995; and\nNelkin 1996; all of whom defend versions of this view.) On this\naccount, blindsight percepts are phenomenally conscious states to\nwhich the subjects of those states are blind. Higher-order theorists\nwill argue, of course, that none of these alternatives is acceptable\n(see, e.g., Carruthers 2000; Rosenthal 2005). \nFurther, Lau and Rosenthal (2011) survey the empirical\nevidence pertaining to the difference between higher-order theories\nand first-order ones. While much is equivocal, and many questions are\nleft unanswered, they point to a pair of studies that support a\nhigher-order account. One is Lau and Passingham (2006), who are able\nto demonstrate using carefully controlled stimuli that there are\ncircumstances in which people’s subjective reports of visual\nexperience are impaired while their first-order discrimination\nabilities remain fully intact. They also find that visual\nconsciousness in these conditions is specifically associated with\nactivity in a region of dorsolateral prefrontal cortex. Then in a\nfollow-up study Rounis et al. (2010) find that transcranial magnetic\nstimulation directed at this region of cortex, thereby disrupting its\nactivity, also has a significant impact on people’s meta-visual\nawareness, but again without impairing first-order task\nperformance. The degree to which the prefrontal cortex is required for having a conscious state and the view that the prefrontal cortex is the likely site of all higher-order thoughts are also the subject of vigorous continuing debate (Block 1995; Gennaro 2012, chapter nine; Kozuch 2014; Odegaard, Knight, and Lau 2017).  \nMost generally, then, higher-order theories of phenomenal consciousness\nclaim the following: \nHigher Order Theory (In General):\n\nA phenomenally conscious mental state is a mental state (of a certain\nsort—see below) that either is, or is disposed to be, the object\nof a higher-order representation of a certain sort (see below).\n \nHigher-order theorists do agree that one must normally become aware of the lower-order state non-inferentially since mental states can sometimes become targets of higher-order representation via conscious inference without being phenomenally conscious. For example, if I become aware of my unconscious desire to kill my boss because I have consciously inferred it from a session with my psychiatrist, then the characteristic phenomenal feel of such a conscious desire may be absent.  \nStill, there are then two main dimensions along which higher-order theorists\ndisagree amongst themselves. One concerns whether the higher-order\nstates in question are perception-like, on the one hand, or\nthought-like, on the other. A thought is composed of or constituted by concepts. Those taking the former option are higher-order perception (often called\n‘inner-sense’) theorists, and those taking the latter option are higher-order thought theorists. The two theories are therefore often abbreviated as HOP (higher-order perception) and HOT (higher-order thought) theory. The other general disagreement is internal to higher-order thought approaches, and concerns whether the relevant relation between the first-order state\nand the higher-order thought is one of availability or not.\nThat is, the question is whether a state is conscious by virtue of\nbeing disposed to give rise to a higher-order thought, or\nrather by virtue of being the actual target of such a\nthought. These are the three main options that will now concern us. (A\nfourth will be considered in section 6.) \nAccording to this view, humans not only have first-order\nnon-conceptual and/or analog perceptions of states of their\nenvironments and bodies, they also have second-order non-conceptual\nand/or analog perceptions of their first-order states of perception.\nAnd the most popular version of higher-order perception (HOP) theory holds,\nin addition, that humans (and perhaps other animals) not only have\nsense-organs that scan the environment/body to produce fine-grained\nrepresentations, but they also have inner senses which scan the first-order senses (i.e. perceptual experiences) to produce equally fine-grained, but higher-order,\nrepresentations of those outputs. A version of this view was first proposed by the British\nEmpiricist philosopher John Locke (1690). In our own time it has been defended especially by Armstrong (1968, 1984) and by Lycan (1996, 2004). \nA terminological point: ‘inner-sense theory’\nshould more strictly be called ‘higher-order-sense\ntheory’, since we of course have senses that are physically\n‘inner’, such as pain-perception and internal touch-perception, that aren’t intended to fall under its scope. For\nthese are first-order senses on a par with vision and hearing,\ndiffering only in that their purpose is to detect properties of the\nbody, rather than of the external world (Hill 2004). According to the\nsort of higher-order theory that is presently under discussion, these\nsenses, too, will need to have their outputs scanned to produce\nhigher-order analog contents in order for those outputs to become\nphenomenally conscious. In what follows, however, the term\n‘inner sense’ will be used to mean, more strictly, ‘higher-order sense’. \nWe therefore have the following proposal to consider: \nInner-Sense Theory:\n\nA phenomenally conscious mental state is a state with\nanalog/non-conceptual intentional content, which is in turn the target\nof a higher-order analog/non-conceptual intentional state, via the\noperations of a faculty of ‘inner sense’.\n \nOn this account, the difference between a phenomenally conscious\npercept of red and the sort of unconscious percepts of red that guide\nthe guesses of a blindsighter and the activity of the sensorimotor\nsystem, is as follows. The former is scanned by our inner senses to\nproduce a higher-order analog state with the content experience of\nred or seems red, whereas the latter states\naren’t—they remain merely first-order states with the\nanalog content red; and in so remaining, they lack any\ndimension of seeming or subjectivity. According to\ninner-sense theory, it is the higher-order perceptual contents\nproduced by the operations of our inner-senses that make some mental\nstates with analog contents, but not others, available to their\nsubjects.  \nOne of the main advantages of inner-sense theory is that it can\nexplain how it is possible for us to acquire purely recognitional\nconcepts of experience. For if we possess higher-order perceptual\ncontents, then it should be possible for us to learn to recognize the\noccurrence of our own perceptual states immediately grounded in those higher-order analog\ncontents. (Compare the way in which first-order perceptual contents\nrepresenting color and sound enable us the acquire first-order\nrecognitional concepts for colors and sounds.) And this should be\npossible without those recognitional concepts thereby having any\nconceptual connections with our beliefs about the content of\nthe states recognized, nor with any of our surrounding mental\nconcepts. This is then how inner-sense theory will claim to explain\nthe familiar philosophical thought-experiments concerning one’s own\nexperiences, which are supposed to cause such problems for\nphysicalist/naturalistic accounts of the mind (Kripke 1972; Chalmers\n1996). (For discussion of this ‘phenomenal concept\nstrategy’ see Carruthers and Veillet 2007.) \nFor example, I can think, ‘R [an experience as of red]\nmight have occurred in me, or might normally occur in others, in the\nabsence of any of its actual causes and effects.’ So on any view\nof intentional content that sees content as tied to normal causes\n(i.e. to information carried) and/or to normal effects (i.e. to\nteleological or inferential role), experience of type R might\noccur without representing red. Likewise I can think,\n‘R might occur in someone without occupying the role of\nexperience, but rather (say) of belief.’ In the same\nsort of way, I shall be able to think, ‘P [an\nexperience of pain] might have occurred in me, or might occur in\nothers, in the absence of any of the usual causes and effects of pain.\nThere could be someone in whom P experiences occur but who\nisn’t bothered by them, and where those experiences are never caused\nby tissue damage or other forms of bodily insult. And conversely,\nthere could be someone who behaves and acts just as I do when in pain,\nand in response to the same physical causes, but who is never subject\nto P types of experience.’ If we possess purely\nrecognitional concepts of experience such as R and\nP, then the thinkability of such thoughts is unthreatening to a naturalistic approach to the mind. \nInner sense theorists are thus well placed to respond to\nthose who claim that there is an unbridgeable explanatory gap between\nall physical, functional, and intentional facts, on the one hand, and\nthe facts of phenomenal consciousness, on the other (Levine 1983;\nChalmers 1996). And likewise they can explain the conceivability of\nzombies without becoming committed to the existence of any\nnon-physical properties of experience (contra Chalmers 1996).\nIt is the conceptual isolation of our higher-order\nrecognitional concepts of experience that explains how there can be no\na priori entailment between physical, functional, and intentional\nfacts and the occurrence of states of type R or P\n(where R and P express purely recognitional\nconcepts).  \nInner-sense theory does face a number of difficulties, however. One\nobjection is as follows (see Dretske 1995; Güzeldere 1995). If\ninner-sense theory were true, then how is it that there isn’t any\nphenomenology distinctive of inner sense, in the way that there is a\nphenomenology associated with each outer sense? Since each of the\nouter senses gives rise to a distinctive set of phenomenological\nproperties, one might expect that if there were such a thing\nas inner sense, then there would also be a phenomenology distinctive\nof its operation. But there doesn’t appear to be any. \nThis point turns on the so-called ‘transparency’ of our perceptual experience (Harman 1990). Concentrate as hard as you like on your ‘outer’ (first-order) experiences—you won’t\nfind any further phenomenological properties arising out of\nthe attention you pay to them, beyond those already belonging to the\ncontents of the experiences themselves. Paying close attention to your\nexperience of the color of the red rose, for example, just produces\nattention to the redness—a property of the rose. Put like this, however, the objection just seems to beg the question\nin favor of first-order theories of phenomenal consciousness. It\nassumes that first-order—‘outer’—perceptions\nalready have a phenomenology independently of their targeting by inner\nsense. But this is just what an inner-sense theorist will deny. And\nthen in order to explain the absence of any kind of higher-order\nphenomenology, an inner-sense theorist only needs to maintain that our\nhigher-order perceptions are never themselves targeted by an\ninner-sense-organ which might produce third-order analog\nrepresentations of them in turn. \nAnother objection to inner-sense theory is as follows (see Sturgeon\n2000). If there really were an organ of inner sense, then it ought to\nbe possible for it to malfunction, just as our first-order senses\nsometimes do. And in that case, it ought to be possible for someone to\nhave a first-order percept with the analog content red\ncausing a higher-order percept with the analog content\nseems-orange. Someone in this situation would be disposed to\njudge, ‘It’s red’, immediately and non-inferentially (i.e.\nnot influenced by beliefs about the object’s normal color or their own\nphysical state). But at the same time they would be disposed to judge,\n‘It seems orange’. Not only does this sort of\nthing never apparently occur, but the idea that it might do so\nconflicts with a powerful intuition. This is that our awareness of our\nown experiences is immediate, in such a way that to\nthink that you are undergoing an experience of a certain\nsort is to be undergoing an experience of that sort. But if\ninner-sense theory is correct, then it ought to be possible for\nsomeone to believe that they are in a state of seeming-orange\nwhen they are actually in a state of seeming-red. (The problem of misrepresentation will addressed further below in sections 6 and 7.) \nA different sort of objection to inner-sense theory is developed by\nCarruthers (2000). It starts from the fact that the internal monitors\npostulated by such theories would need to have considerable\ncomputational complexity in order to generate the requisite\nhigher-order experiences. In order to perceive an experience, the\norganism would need to have mechanisms to generate a set of internal\nrepresentations with an analog or non-conceptual content representing\nthe content of that experience, in all its richness and fine-grained\ndetail. And notice that any inner scanner would have to be a physical\ndevice (just as the visual system itself is) which depends upon the\ndetection of those physical events in the brain that are the\noutputs of the various sensory systems (just as the visual system is a\nphysical device that depends upon detection of physical properties of\nsurfaces via the reflection of light). For it is hard to see how any\ninner scanner could detect the presence of an experience qua\nexperience. Rather, it would have to detect the physical\nrealizations of experiences in the brain, and construct the\nrequisite higher-order representation of the experiences that those\nphysical events realize, on the basis of that physical-information\ninput. This makes it seem inevitable that the scanning device that\nsupposedly generates higher-order experiences of our first-order\nvisual experience would have to be almost as sophisticated and complex\nas the visual system itself. \nGiven this complexity in the operations of our organs of inner sense, there should be some plausible story to tell about the evolutionary pressures that led to\ntheir construction (Pinker 1994, 1997). But there would seem to be no such stories on the\nmarket. The most plausible suggestion is that inner-sense might have\nevolved to subserve our capacity to think about the mental states of\nconspecifics, thus enabling us to predict their actions and manipulate\ntheir responses. (This is the so-called ‘Machiavellian\nhypothesis’ to explain the evolution of intelligence in the\ngreat-ape lineage. See Byrne and Whiten 1988, 1998; and see Goldman\n2006, for a view of inner sense of this sort.) But this suggestion\npresupposes that the organism must already have some capacity\nfor higher-order thought, since it is such thoughts that\ninner sense is supposed to subserve. And yet as we shall see shortly\n(in section 5), some higher-order theories can claim all of\nthe advantages of inner-sense theory as an explanation of phenomenal\nconsciousness, but without the need to postulate any ‘inner\nscanners’.  \nActualist higher-order thought (HOT) theory is a proposal about the\nnature of state-consciousness in general, of which phenomenal\nconsciousness is but one species. Its main proponent has been\nRosenthal (1986, 1993, 2005). The proposal is this: a conscious mental\nstate M, of mine, is a state that is actually causing an\nactivated thought (generally a non-conscious one) that I have\nM, and causing it non-inferentially. (The qualification\nconcerning non-inferential causation will be discussed in a moment.)\nAn account of phenomenal consciousness can then be generated by\nstipulating that the mental state M should have some causal role\nand/or content of a certain distinctive sort in order to count as an\nexperience (e.g., with an analog content, perhaps), and that when\nM is an experience (or a mental image, bodily sensation, or\nemotional feeling), it will be phenomenally conscious when (and only\nwhen) suitably targeted. The HOT is typically of the form: ‘I am in mental state M.’ \nWe therefore have the following proposal to consider: \nActualist Higher-Order Thought Theory:\n\nA phenomenally conscious mental state is a state of a certain sort\n(e.g. with analog/non-conceptual intentional content, perhaps) which\nis the object of a higher-order thought, and which causes that thought\nnon-inferentially.\n \nAs noted earlier, Rosenthal interprets the non-inferential requirement as ruling out only conscious inferences in the generation of a consciousness-making higher-order\nthought. This enables him to avoid having to say that my unconscious\nmotives become conscious when I learn of them under psychoanalysis, or\nthat my jealousy is conscious when I learn of it by noticing and\ninterpreting my own behavior. But Rosenthal (2005) thinks that\nunconscious self-interpretation is acceptable as a source of\nthe conscious status of the states thereby attributed. So if I arrive\nat the thought that I am feeling cheerful by unconsciously noticing the\nspring in my own step and the smile on my own face and drawing an\nunconscious inference, my cheerfulness will thereby have been rendered\nconscious. This aspect of Rosenthal’s actualist form of\nHOT theory would appear to be optional for a HOT theorist,\nhowever. \nIn addition, and more controversially, Rosenthal (2005) thinks that the occurrence of a suitably caused HOT is sufficient for consciousness, even in the\nabsence of any targeted first-order state (usually called ‘targetless’ or ‘empty’ HOTs). So I am undergoing a conscious experience of red provided that I think that I am\nundergoing an experience of red, even if I am actually in no\nfirst-order perceptual state whatever. This aspect of Rosenthal’s\nview, too, appears optional for an actualist HOT theorist. Such a\ntheorist can—and perhaps should—insist that phenomenally conscious experience occurs when and only when a first-order\nperceptual state causes a higher-order thought in the existence of that\nstate in a way that doesn’t depend upon self-interpretation. In recent years, the twin problems of misrepresentation between HOTs and their first-order targets as well as targetless HOT cases has led to significant disagreement among HOT theorists (see section 7 below).   \nThe actualist HOT account avoids some of the difficulties inherent in\ninner-sense theory, while retaining the latter’s ability to explain\nthe distinction between conscious and unconscious perceptions.\n(Conscious perceptions will be analog states that are targeted by a\nHOT, whereas perceptions such as those involved in blindsight or subliminal perceptions will be unconscious by virtue of not being so targeted.) In particular, it is easy to see a function for HOTs, in general, and to tell a story about their likely evolution. A capacity to entertain HOTs about\nexperiences would enable a creature to negotiate the is/seems\ndistinction, perhaps learning not to trust its own experiences in\ncertain circumstances, and also to induce appearances in others, by\ndeceit. And a capacity to entertain HOTs about mental states\n(such as beliefs and desires) would enable a creature to reflect on,\nand to alter, its own beliefs and patterns of reasoning, as well as to\npredict and manipulate the thoughts and behaviors of others. Indeed,\nit can plausibly be claimed that it is our capacity to target\nhigher-order thoughts on our own mental states that underlies our\nstatus as rational agents (Burge 1996; Sperber 1996; Rolls 2004).  A common initial objection to HOT theory (or even HOP) is that they lead to an infinite regress.  It might seem that an infinite regress results because a conscious mental state (M) must be accompanied by a HOT, which, in turn must be accompanied by another HOT and so on. However, the standard and widely accepted reply or explanation is that when M is conscious, the HOT is not itself conscious (Rosenthal 1986, 2005).  M is a first-order world-directed conscious state, such as a desire or perception, accompanied by an unconscious HOT. But when the HOT is itself conscious, there is a yet another higher-order (or third-order) thought directed at the conscious HOT. This would be a case of introspection according to HOT theory such that one’s attention is directed inward at M (such as introspecting my desire). When this crucial distinction is overlooked, it can lead to some misguided objections such as supposing that, according to HOT theory, having any conscious state (even for animals and infants) requires the ability to introspect. \nOne objection to HOT theory is due to Dretske (1993). We are asked to imagine a case in which we carefully examine two line-drawings, say (or in Dretske’s example, two\npatterns of differently-sized spots). These drawings are similar in\nalmost all respects, but differ in just one aspect—in Dretske’s\nexample, one of the pictures contains a black spot that the other\nlacks. It is surely plausible that, in the course of examining these\ntwo pictures, one will have enjoyed a conscious visual experience of\nthe respect in which they differ—e.g. of the offending spot.\nBut, as is familiar, one can be in this position while not knowing\nthat the two pictures are different, or in what way\nthey are different. In which case, since one can have a conscious\nexperience (e.g. of the spot) without being aware that one is having\nit, consciousness cannot require higher-order awareness. \nReplies to this objection have been made by Seager (1994), Byrne\n(1997), and Rosenthal (2005), among others. They point out that it is\none thing to have a conscious experience of the aspect that\ndifferentiates the two pictures, and quite another to consciously\nexperience that the two pictures are differentiated by that aspect.\nThat is, consciously seeing the extra spot in one picture needn’t mean\nseeing that this is the difference between the two pictures. So while\nscanning the two pictures one will enjoy conscious experience of the\nextra spot. A HOT theorist will say that this means\nundergoing a percept with the content spot here that\nforms the target of a HOT that one is undergoing a\nperception with that content. But this can perfectly well be true\nwithout one undergoing a percept with the content spot here in\nthis picture but absent here in that one. And it can also be true\nwithout one forming any HOT to the effect that one is\nundergoing a perception with the content spot here\nwhen looking at a given picture but not when looking at the other. In\nwhich case the purported counter-example isn’t really a\ncounter-example. \nAnother objection to actualist HOT theory is epistemological, and is\ndue to Goldman (2000). It turns crucially on the fact that the\nconsciousness-making higher-order thoughts postulated by the theory\nare, themselves, characteristically unconscious. The\nobjection goes like this. When I undergo a conscious mental state\nM, I generally know, or have good reason to believe, that\nM is conscious. But how can this be, if what\nmakes M conscious is the existence of an\nunconscious HOT targeted on M?\nSince I don’t know that this thought exists, it seems that I shouldn’t\nbe able to know that M is conscious, either. As Goldman himself acknowledges, however, this argument can only really work on the assumption that actualist HOT theory is supposed to be some sort of analytic or logical truth. Rosenthal has always made clear,\nhowever, that the theory isn’t intended to be a piece of conceptual\nanalysis, but is rather an account of the properties that\nconstitute the property of being conscious (see Rosenthal\n1986, as well as his 2005). And the epistemological argument gets no\ntraction against this sort of view. \nA different sort of problem with the actualist version of higher-order\nthought theory relates to the huge number of thoughts that would have\nto be caused by any given phenomenally conscious experience. (This is\nthe analogue of the ‘computational complexity’ objection\nto inner-sense theory, sketched in section 3 above). Consider just how\nrich and detailed a conscious experience can be. It would seem that\nthere can be an immense amount of which we can be consciously aware at\nany one time. Imagine looking down on a city from a window high up in\na tower-block, for example. In such a case you can have phenomenally\nconscious percepts of a complex distribution of trees, roads, and\nbuildings; colors on the ground and in the sky above; moving cars and\npedestrians; and so on. And you can—it seems—be conscious of all of this simultaneously. According to actualist HOT theory, then, it seems you would need to have a distinct activated\nHOT for each distinct aspect of your experience—either that, or just a few such thoughts with immensely complex contents. Either way, the objection is the same. For it seems implausible that all of this higher-order activity should be\ntaking place (albeit non-consciously) every time someone is the\nsubject of a complex conscious experience. What would be the\npoint? And think of the amount of cognitive/neural space that these\nthoughts would take up! (In contrast, we know that neural tissue and\nactivity are expensive; see Aiello and Wheeler 1995; and we also know\nthat as a result of such constraints, the wiring diagram for the brain\nis about as efficient as it is possible for it to be; see Cherniak\net al. 2004.) \nThis objection to actualist forms of HOT theory is\nconsidered at some length in Carruthers (2000), where a variety of\npossible replies are discussed and evaluated. Perhaps the most\nplausible and challenging such reply would be to deny the main premise\nlying behind the objection, concerning the rich nature\nof phenomenally conscious experience. The theory could align\nitself with Dennett’s (1991) conception of consciousness as highly\nfragmented, with multiple streams of perceptual content being\nprocessed in parallel in different regions of the brain, and with no\nstage at which all of these contents are routinely integrated into a\nphenomenally conscious perceptual manifold. Rather, contents become\nconscious on a piecemeal basis, as a result of internal or external\nprobing that gives rise to a HOT about the\ncontent in question. This serves to convey to us the mere illusion of riches,\nbecause wherever we direct our attention, there we find a conscious\nperceptual content. (For a related reply, see Gennaro 2012, chapter six).  \nIt is difficult to know whether this sort of ‘fragmentist’ account can really explain the phenomenology of our experience, however. For it still faces the objection that the objects of attention can be\nimmensely rich and varied at any given moment, hence requiring there\nto be an equally rich and varied repertoire of HOTs\ntokened at the same time. Think of immersing yourself in the colors\nand textures of a Van Gogh painting, for example, or the scene as you\nlook out at your garden—it would seem that one can be\nphenomenally conscious of a highly complex set of properties,\nwhich one could not even begin to describe or conceptualize in any\ndetail. However, since the issues here are large and controversial, it\ncannot yet be concluded that actualist forms of HOT\ntheory have been refuted. This is particularly the case when one considers such phenomena as change and inattentional blindness where subjects often do not even notice somewhat significant changes occurring in an image or video even within one’s focal visual field (Simons 2000; Simons and Chabris 1999). \nAnother difficulty for actualist forms of HOT theory takes the form of\na puzzle: how can the targeting of a perceptual state by HOT make the former ‘light up’, and acquire the properties of ‘feel’ or what it is like-ness?\nSuppose, for example, that I am undergoing an unconscious perception\nof red. How could such a percept then acquire the properties\ndistinctive of phenomenal consciousness merely by virtue of me coming\nto think (in non-inferential fashion) that I am undergoing an\nexperience of red? \nRosenthal (2005) replies to this objection by pointing to cases in\nwhich (he says) the acquisition and application of novel higher-order\nconcepts to our experience transforms the phenomenal properties of the\nlatter. Thus a course in wine-tasting can lead me to have experiences\nof the wine that are phenomenally quite distinct from any that I\nenjoyed previously (see also Siegel 2010; Gennaro 2012, chapter six). And a course in classical music appreciation might lead to changes in my experience of the sound of the orchestra, perhaps distinguishing between the sounds of the oboes and the\nclarinets for the first time. Since changes in higher-order concepts\ncan lead to changes in phenomenal consciousness, Rosenthal thinks, it\nis plausible that it is the presence of higher-order thoughts\ntargeting our perceptual states that is responsible for the latter’s\nphenomenal properties tout court. \nIn response, an opponent of the theory might observe that some of the concepts\nthat one acquires in such cases do not appear to be higher-order ones at\nall. Thus the concepts oaky and tanniny that one\nacquires when wine-tasting pick out secondary qualities of the\nwine (which are first-order), not higher-order properties of our\nexperience of the wine. And likewise the concept oboe when\napplied in an experience is a first-order concept of a sound type, not\na higher-order concept of one’s experience of sound. The phenomenon\nhere is quite general: acquiring and applying new concepts in one’s\nperception can transform the similarity spaces and organization of\none’s perceptual states. (Think here of the familiar duck/rabbit.) But\nit appears to be a first-order phenomenon, not a higher-order one. At\nany rate, there is considerable work for a HOT theorist to do here in\nmaking out the case to the contrary. \nAccording to the dispositionalist HOT theory, the conscious status of an perceptual state consists in its availability to higher-order thought (Dennett 1978a; Carruthers 1996, 2000, 2005). As with the non-dispositionalist version\nof the theory, in its simplest form we have here a quite general\nproposal concerning the conscious status of any type of occurrent\nmental state, which becomes an account of phenomenal consciousness\nwhen the states in question are experiences (or images, emotions,\netc.) with analog content. The proposal is this: a conscious mental\nevent M, of mine, is one that is disposed to cause an\nactivated thought (generally a non-conscious one) that I have\nM, and to cause it non-inferentially. \nThe proposal before us is therefore as follows: \nDispositionalist Higher-Order Thought Theory:\n\nA phenomenally conscious mental state is a state of a certain sort\n(perhaps with analog/non-conceptual intentional content, and perhaps\nheld in a special-purpose short-term memory store) which is available\nto cause (non-inferentially) higher-order thoughts about itself (or\nperhaps about any of the contents of the memory store).\n \nIn contrast with the actualist form of theory, the higher-order\nthoughts that render a percept conscious are not necessarily actual,\nbut potential. So the objection now disappears, that\nan unbelievable amount of cognitive space would have to be taken up\nwith every conscious experience. (There need not actually be\nany HOT occurring, in order for a given\nperceptual state to count as phenomenally conscious, on this view.) So\nwe might be able to retain our belief in the rich and integrated nature of\nphenomenally conscious experience—we just have to suppose that\nall of the contents in question are simultaneously available\nto higher-order thought. (Such availability might be realized by the\n‘global broadcast’ of perceptual representations to a wide range of conceptual systems in the brain, for drawing inferences, for forming memories, and for planning, as well as for forming\nhigher-order beliefs. See Baars 1988, 1997, 2002.) Nor will there be\nany problem in explaining why our faculty of higher-order thought\nshould have evolved, nor why it should have access to perceptual\ncontents in the first place—this can be the standard sort of\nstory in terms of Machiavellian intelligence. \nIt might well be wondered how their mere availability to\nhigher-order thoughts could confer on our perceptual states the\npositive properties distinctive of phenomenal consciousness—that\nis, of states having a subjective dimension, or a distinctive\nsubjective feel. The answer may lie in the theory of content.\nSuppose that one agrees with Millikan (1984) that the representational\ncontent of a state depends, in part, upon the powers of the systems\nthat consume that state. That is, suppose one thinks that\nwhat a state represents will depend, in part, on the kinds of\ninferences that the cognitive system is prepared to make in the\npresence of that state, or on the kinds of behavioral control that it\ncan exert. In that case the presence of first-order perceptual\nrepresentations to a consumer-system that can deploy a ‘theory\nof mind’, and that is capable of recognitional applications of\ntheoretically-embedded concepts of experience, may be sufficient to\nrender those representations at the same time as higher-order\nones. This would be what confers on our phenomenally conscious\nexperiences the dimension of subjectivity. Each experience would at\nthe same time (while also representing some state of the world, or of\nour own bodies) be a representation that we are undergoing just such\nan experience, by virtue of the powers of the ‘theory of\nmind’ system. Each percept of green, for example, would\nat one and the same time be an analog representation of green\nand an analog (non-conceptual) representation of seems green\nor experience of green. (Consumer semantics embraces not only a number of different varieties of teleosemantics, but also various forms of inferential role semantics. For the former, see Millikan 1984, 1986, 1989; and Papineau 1987, 1993. For the latter, see Loar 1981, 1982; McGinn\n1982, 1989; Block 1986; and Peacocke 1986, 1992).  \nAs an independent illustration of how consumer systems can transform\nperceptual contents, consider prosthetic vision (Bach-y-Rita 1995;\nBach-y-Rita and Kercel 2003). Blind subjects can be fitted with a\ndevice that transduces the output from a hand-held or head-mounted\nvideo-camera into patterns of electrically-induced tactile stimulation\nacross the subject’s back or tongue. Initially, of course, the\nsubjects just feel patterns of gentle tickling sensations spreading\nover the area in question, while the camera scans what is in front of\nthem. But provided that they are allowed to control the movements of\nthe camera themselves, their experiences after a time acquire\nthree-dimensional distal intentional contents, representing the\npositions and movements of objects in space.(Note that the patterns of tactile simulations themselves become imbued with spatial content. The subjects in question say that it has\ncome to seem to them that there is a spherical object moving\ntowards them, for example.) Here everything on the input side remains\nthe same as it was when subjects first began to wear the device; but\nthe planning and action-controlling systems have learned to interpret\nthose states differently. And as a result, the subjects’\nfirst-order intentional perceptual contents have become quite\ndifferent. Likewise, according to dispositional HOT theory, when the\n‘theory of mind’ system has learned to interpret the subject’s perceptual states as perceptual states: they all\nacquire a dimension of seeming or subjectivity. \nProponents of this account hold that it achieves all of the benefits\nof inner-sense theory, but without the associated costs. (Some\npotential draw-backs will be noted in a moment.) In particular, we can\nendorse the claim that phenomenal consciousness consists in a set of\nhigher-order perceptions. This enables us to explain, not only the\ndifference between conscious and unconscious perception, but also how\nanalog states come to acquire a subjective dimension or\n‘feel’. And we can also explain how it can be possible for us to acquire some purely recognitional concepts of experience (thus\nexplaining the standard philosophical thought-experiments concerning\nzombies and such-like). But we don’t have to appeal to the existence\nof any ‘inner scanners’ or organs of inner sense (together with their associated problems) in order to do this. Moreover, it\nshould also be obvious why there can be no question of our\nhigher-order contents misrepresenting their first-order\ncounterparts, in such a way that one might be disposed to make\nrecognitional judgments of red and seems orange at\nthe same time. This is because the content of the higher-order\nexperience is parasitic on the content of the first-order one. Carruthers, therefore, also refers to this view as dual content theory. \nOn the downside, the account isn’t neutral on questions of semantic\ntheory. On the contrary, it requires us to reject any form of pure\ninput semantics, in favor of some sort of consumer semantics. We\ncannot then accept that intentional content reduces to informational\ncontent, nor that it can be explicated purely in terms of causal\nco-variance relations to the environment. So anyone who finds such\nviews attractive will think that the account is a hard one to swallow.\n(For discussion of various different versions of input semantics, see\nDretske 1981, 1986; Fodor 1987, 1990; and Loewer and Rey 1991.) \nMoreover, Rosenthal (2005) has objected that dispositional HOT theory\ncan’t account for our actual awareness of our conscious mental\nstates, since mere dispositions to entertain thoughts doesn’t make us\naware of anything. Two replies can be made (see Carruthers 2000,\n2005). One is that, in virtue of our disposition to entertain\nhigher-order thoughts about it, a perceptual state will\nalready possess an analog higher-order content. It is this\ncontent that makes us aware of the experience in question. But the\nsecond reply is that there does, in any case, seem to be a perfectly\ngood dispositional sense of ‘know’ and\n‘aware’. As Dennett pointed out long ago (1978b), I can be\nsaid to know, or to be aware, that zebras in the wild don’t wear\novercoats, even though I have never actually considered the matter,\nbecause I am disposed to assent to that proposition in light\nof what I occurrently know. \nIn addition, Rowlands (2001) and Jehle and Kriegel (2006) have\nobjected that dispositional HOT theory can’t explain the sense in\nwhich the phenomenal properties of experience are\ncategorical. For the higher-order analog intentional contents\nthat our conscious perceptual states possess—and that are\nidentified with the ‘feel’ of experience—are said to\nbe constituted by the dispositional property that such states have, of\ngiving rise to HOTs about themselves. This objection,\nhowever, appears to beg the question in favor of irreducible and\nintrinsic qualia as an account of the distinctive properties of\nphenomenally conscious states. In any case it doesn’t seem to be an\nobjection against dispositional HOT theory as such, since it will\ncount equally against any representationalist theory of consciousness.\n(For example, Tye 1995, explains consciousness in terms of the\npoisedness of perceptual states to have an impact on belief\nand reasoning, which is a dispositional notion.) Any theory that\nproposes to reductively explain phenomenal consciousness in terms of\nsome combination of intentional content and causal role will be\nexplaining consciousness in terms that are at least partly\ndispositional.  \nA well-known objection to dispositionalist higher-order thought\ntheory, however, is that it may have to deny phenomenal consciousness\nto most species of non-human animal. This objection will be discussed,\namong others, in section 7, since it can be raised\nagainst any form of higher-order theory. \nCarruthers no longer holds dispositional HOT theory or, for that matter, any form of higher-order theory and actually defends a version of first-order representationalism instead (Carruthers 2017). He responds to his own previous two main lines of argument against first-order representationalism and then finds it unnecessary to propose a higher-order theory in order to explain the difference between unconscious and conscious states. Still, Carruthers thinks that dispositional HOT theory is preferable to actualist HOT theory. \nThe two most familiar forms of higher-order theory postulate the\nexistence of a pair of distinct mental states: a first-order\nperceptual or quasi-perceptual state with a given content, and a\nHOT or HOP representing the presence of that first-order state, thereby rendering it conscious. Either one of these states can occur without the other, although there may be a reliable causal relation between them, such that certain types of first-order\nperception (e.g. attended outputs of the temporal-lobe visual system)\nregularly cause higher-order representations of themselves to be\nformed. In recent years, however, a cluster of different proposals\nhave been made that would reject this independent-state assumption.\nRather, the relationship between the conscious state in question and\nthe higher-order state is said to be constitutive, or\ninternal. To some extent, this view is inspired by Brentano (1874/1973) and the phenomenological tradition, including Sartre (1956). (See Kriegel 2006, 2018; Kriegel and Williford 2006; Zahavi 2004; Miguens et al. 2016.) We can refer to these as ‘self-representational’ higher-order theories. (Kriegel initially coined the term ‘same-order monitoring theory’ but this was potentially misleading). \nWe therefore have the following proposal to consider: \nSelf-Representational Theory:\n\nA phenomenally conscious mental state is a state of a certain sort\n(perhaps with analog/non-conceptual intentional content) which also,\nat the same time, possesses an intentional content,\nthereby in some sense representing itself to the person who\nis the subject of that state.\n \nThere are two basic types of self-representational theory, depending\non whether the constitutive relation between the conscious state and\nthe higher-order state is one of identity, on the one hand,\nor part-whole, on the other. According to the former type of\naccount, it is one and the same perceptual state that is both\nfirst-order (representing the world to us) and higher-order\n(presenting itself to us). (Caston 2002, argues that Aristotle had a\ntheory of conscious perception of this sort.) Kriegel (2006) claims\nthat such accounts are rather mysterious from a naturalistic perspective, but Carruthers (2000, 2005) and perhaps also Van Gulick (2001, 2004) purport to provide naturalistic\nexplanations of just this sort of view. According to Carruthers, a\nfirst-order perceptual state with analog content acquires, at the same\ntime, a higher-order analog content by virtue of its availability to a\n‘theory of mind’ faculty, together with the truth of some suitable form of consumer semantics (as explained in section 5 above). Van Gulick can be interpreted as defending a similar view, which likewise\nrelies on a form of consumer semantics/functional role semantics,\nwhich he labels a ‘Higher-Order Global State (HOGS)\ntheory’. On this account, globally broadcast first-order\nperceptual states acquire at the same time a higher-order\nseeming dimension though their availability to, and\nincorporation into, higher-order models of the self and its relation\nto the perceived environment. (What isn’t entirely clear is whether\nVan Gulick thinks that the resulting perceptual state is the\nHOG state, or is rather a component part of the HOG\nstate—in which case he would be advocating a kind of part-whole\nself-representational account.) Kriegel’s (2009) eventual view emphasizes and argues for the claim that there is a ubiquitous conscious (but inattentive or peripheral) self-awareness which accompanies all first-order (attentive and outer-directed) conscious states. Gennaro (2012, chapter five) rejects this view by, among other things, arguing that it is difficult to make sense of such alleged pervasive peripheral self-awareness especially when one is focused on outer-directed tasks. It is at least not as clearly present as, say, outer-directed peripheral vision in normal visual perception. At minimum, it is notoriously difficult to settle these sorts of disagreements between competing phenomenological claims. \nSome varieties of part-whole self-representational theory take the\nsame general form as actualist kinds of HOT theory, in which a\nfirst-order perceptual state with the content analog-red (as\nit might be) gives rise to a higher-order thought that one is\nexperiencing red. But rather than claiming that it is the first-order\nperception that becomes phenomenally conscious because of the presence\nof the higher-order thought, what is said that the complex state made\nup of both the first-order perception and the\nhigher-order thought becomes conscious. Gennaro (1996, 2008, 2012) defends such a view which he calls the wide intrinsicality view such that the HOT is better thought of as belonging to the same overall complex state as its target. It is, however, not always clear how this theory could offer any substantive benefits not already obtainable from actualist HOT theory. Rather, the claim is merely that a conscious state is one that contains two parts, one of which is an awareness of the other. Kriegel himself (2003, 2006, 2009) and (as Kriegel interprets him) Van\nGulick (2001, 2004) emphasize that the first-order perception and the\nhigher-order judgment need to be integrated with one another\nin order for the resulting complex state to be phenomenally conscious.\nKriegel argues that there needs to be a kind of integration resulting\nfrom a psychologically real process (as opposed to a theorist’s\ndefinition) in order for the resulting state to have causal powers\nthat differ from those of the first-order state/higher-order state\npair. \nKriegel and Van Gulick do not give fully developed accounts of just\nwhy the integration of first-order perceptions with\nhigher-order judgments should give rise to the properties that are\ndistinctive of phenomenal consciousness. But one plausible\nreconstruction is as follows, modeled on the way that the\nconceptualization of analog (non-conceptual) first-order perceptual\ncontent can transform the latter’s properties. Consider, for example,\nthe familiar duck/rabbit. When someone sees this figure for the first\ntime she may just experience a complex of curved lines, representing\nnothing. But when she comes to see it as a rabbit, those\nlines take on a certain distinctive organization (the figure now has\nboth a front and a back, for example), thereby transforming the\nrepresented properties of the figure. Arguably what happens in such\ncases is that the conceptual systems succeed in deploying a\nrecognitional template for the concept rabbit, finding a\n‘best match’ with the incoming non-conceptual\nrepresentations. Indeed, there is reason to think that just such a\nprocess routinely takes place in perception, with conceptual systems\nseeking matches against incoming data, and with the resulting states\npossessing contents that integrate both conceptual and non-conceptual\n(analog) representations (Kosslyn 1994; Carruthers 2000). The result\nis a single perceptual state that represents both a\nparticular analog shape and a rabbit. Now suppose that when\nsuch states are globally broadcast and are made available to the\nsystems responsible for higher-order thought, a similar process takes\nplace. Those systems bring to bear the concept experience or\nthe concept seeing to produce a further integrated perceptual\nstate. This single state will not only have first-order contents\nrepresenting the lines on the page, and representing a rabbit, they\nwill also have a higher-order content representing that one is\nexperiencing something rabbit-like. Hence the perceptual\nstate in question becomes ‘self-presenting’, and acquires,\nas part of its content, a dimension of seeming or\nsubjectivity. (See also Gennaro 2005, 2012, for a related line of argument in response to this sort of challenge.) \nPicciuto (2011) points out, however, that Kriegel’s form of\nself-representational theory still permits a mismatch between the\nfirst-order and higher-order components of the integrated state. For\nthere seems to be nothing in the structure of the account to rule out\nthe possibility of a first-order analog content green\nbecoming integrated with the higher-order judgment I am\nexperiencing yellow, for example. In order to avoid this difficulty, Picciuto (2011) proposes an alternative form of part-whole self-representational theory. (See also\nColeman, 2015; Timpe, 2015.) He does so by appropriating, and\ndeploying for a novel purpose, the idea of a quotational\nphenomenal concept, originally introduced by Papineau (2002) and Balog\n(2009) as part of their defense of physicalism against the arguments\nof Chalmers (1996) and others. Picciuto’s idea is that the relevant\nsort of complex self-representational state will consist of a\nfirst-order perceptual content combined with a higher-order concept\nlike experience that embeds, or ‘quotes’ that very perceptual content. Given this structure, it will be impossible\nthat there should be a mismatch between the two. For the higher-order\ncomponent of the complex state is not a judgment about the\nexperience component (which would permit them to be mismatched) but\nrather a concept that quotes the experience component. \nAll part-whole self-representational accounts differ from the\ndual-content theory of Carruthers (2000, 2005) in the following way,\nhowever: on Carruthers’ account, the end-product can be entirely\nnon-conceptual. And in particular, the higher-order content possessed\nby a conscious percept is a non-conceptual one, representing a\nseeming of the first-order content of the state by virtue of\nits availability to higher-order consumer systems. On all of the\npart-whole accounts sketched above, in contrast, a conscious\nperception is always partially conceptual, containing the higher-order\nconcept experience (or something similar) as part of its\ncontent. There are probably multiple dimensions along which these two\nsorts of theory could be compared, and each may have its own\nadvantages.  \nThere have, of course, been a whole host of objections raised against\nhigher-order theories of phenomenal consciousness over the years.\n(See, e.g., Aquila 1990; Jamieson and Bekoff 1992; Dretske 1993, 1995;\nGoldman 1993, 2000; Güzeldere 1995; Tye 1995; Chalmers 1996;\nByrne 1997; Siewert 1998; Levine 2001; Rowlands 2001; Seager 2004;\nBlock, 2011.) Many of these objections, although perhaps intended as\nobjections to higher-order theories as such, are often framed in terms\nof one or another particular version of such a theory. A general moral\nto be taken away from the present discussion should then be this: the\ndifferent versions of a higher-order theory of phenomenal\nconsciousness need to be kept distinct from one another, and critics\nshould take care to state which version of the approach is under\nattack, or to frame objections that turn merely on the\nhigher-order character of all of these approaches. I shall discuss a few\n‘local’ objections first, before discussing some generic\nones. \nA good many objections against specific versions of higher-order\ntheory have already been discussed above. Thus in section 3 we\ndiscussed Dretske’s (1995) ‘lack of any higher-order\nphenomenology’ objection to inner sense theory (which\nonly targets inner sense theory). And in section 4 we\ndiscussed Dretske’s (1993) ‘spot’ objection to actualist\nhigher-order thought theory, as well as Goldman’s (2000)\nepistemological objection, each of which appears to apply only to HOT\ntheories. Of course some of the objections discussed above target more\nthan one version of higher-order theory, while still not being fully\ngeneral in scope. Thus the cognitive/computational complexity\nobjections discussed in sections 3 and 4 apply to inner sense theories\nand to actualist HOT theories, but not to dispositionalist HOT or to\nsome self-representational theories. \nAnother ‘local’ objection (which is actually a\ngeneralization of a variant of the misrepresentation problem discussed\nin connection with inner sense theory in section 3 above) is the\ntargetless higher-order representation problem (Byrne 1997; Neander\n1998; Levine 2001). This is confronted by both inner sense theory and\nactualist HOT theory (but not by either dispositionalist HOT or\nself-representational theories, according to which the relevant\nhigher-order state can’t exist in the absence of the targeted state).\nFor in each case it seems that a higher-order experience of a\nperception of red, say, or a HOT about a perception\nof red, might exist in the absence of any such perception occurring.\nSo it would seem to the subject that she is experiencing red,\nor she might think that she is experiencing red, in the\nabsence of any such experience. (Note that the point isn’t just that\nshe might undergo such a seeming in the\nabsence of anything really red. Rather, the point is that she might\nnot really be undergoing any sort of visual experience as of\nred at all.) In which case, does the subject have a phenomenally\nconscious experience as of red, or not? \nBoth Lycan (1996) and Rosenthal (2005) are sanguine in the face of\nthis objection. Each allows that targetless higher-order\nrepresentations are a possibility (albeit rare, perhaps), and each\nopts to say that the subject in such a case is phenomenally conscious.\nBut each denies that this is a problem for their account. Lycan, for\nexample, insists that it is surely possible that it might\nseem to someone that she is feeling pain when really no\nrelevant first-order representation of pain is present. (He suggests\nthat the effects of morphine, which leaves patients saying that their\npain feels just as it was, but that they no longer care, might\nconstitute such a case.) And surely such a person would have a\nphenomenally conscious experience as of pain. Rosenthal,\nlikewise, uses pain as an example. He points to cases of dental\npatients who initially experience pain in the dentist’s chair despite\nthe fact that the relevant nerves are completely destroyed. It seems\nthat their fear, combined with the noise and vibration of the drill,\ncauses them to mistakenly think that they are feeling pain. (When\nthe drilling is stopped, and their dead nerves are explained to them,\nthey thereafter experience only the sound and the vibration.) So this\nwould be a case in which a HOT about experiencing\npain is alone sufficient to induce a phenomenally conscious experience\nas of being in pain. A critic, however, might respond that\nthe illusion is caused, instead, by a vivid imagining of\npain, rather than by a HOT about feeling pain.\nAlternatively, if a HOT is causally involved it might\nbe that a top-down expectation of pain causes a first-order\nexperience of pain, as opposed to being constitutive of the\nfeeling of pain. It might be that introspective anticipation of pain causes the pain in the first-place. (Note that this seems perfectly possible, since it is\nthe opposite of well-known placebo effects of expectation in reducing\npain.)  The targetless HOT problem has recently become a very significant topic of debate among HOT theorists as well as some critics (Block 2011; Rosenthal 2005, 2011; Weisberg 2011; Gennaro 2012, chapter four; Wilberg 2010; Berger 2014, 2017; Brown 2015; Lau and Brown 2019) which has also led some to advocate for other variants of HOT theory or to clarify their own theories. Gennaro (2012) argues, for example, that since the HOTs in question are typically themselves unconscious, it makes little sense to suppose that these HOTs are phenomenally conscious in the context of HOT theory, especially since a conscious HOT would be an introspective state instead. Maintaining that an unconscious HOT would yield the same subjective experience without any target state seems to run counter to a central initial motivation of HOT theory, namely, to explain what makes a first-order state conscious. Thus, the first-order state must exist first in order to be rendered conscious by an appropriate and accompanying HOT with some sort of corresponding conceptual content.  If both aren’t present, then no relevant conscious state occurs (see also Wilberg 2010). Berger (2014), however, argues that consciousness is not a property of states at all; instead, it is a property of individual persons (that is, how my mental states appear to me). And Brown (2015) challenges the very assumption that HOT theory is best interpreted as a relational theory at all; instead, it is better construed as a HOROR theory, that is, higher-order representation of a representation, regardless of whether or not the target representation exists. In this sense, HOT theory is better understood as non-relational which also seems to be Rosenthal’s considered view in recent decades. The debate continues (Rosenthal 2018).  \nIn response specifically to Block (2011), Rosenthal (2011) and Weisberg (2011) stress that the mere seeming of, say, being in pain (provided by the\nHOT that one is in pain in the absence of first-order\npain) is sufficient for phenomenally conscious pain. Consciousness is about mental appearance. It is not clear\nthat this fully addresses Block’s point, which is that one would not expect\nthe mere thoughts that one feels pain to matter to us in all\nthe ways that pain matters. Block develops this point with respect to\nthe awfulness of pain. It would be remarkable (indeed,\nmysterious) if a higher-order thought should have all of the causal\npowers of the mental state that the thought is about. And in\nparticular, there is no reason to expect that a HOT that one is in pain should possess the negative valence and high-arousal properties of pain itself. But the latter are surely\ncrucial components of phenomenally conscious pain. If so, then a\nHOT that one feels pain in the absence of first-order\npain will not be sufficient for the conscious feeling of\npain. It is also again important not to conflate introspection (conscious HOTs) with mere unconscious HOTs. (See also Shepherd 2013.) \nYet another ‘local’ objection is targeted against\nhigher-order thought theories in particular (whether actualist or\ndispositionalist). It presents such theories with a dilemma: either\nthey are attempts to explicate the concept of consciousness,\nin which case they are circular; or they are attempts to provide a\nreductive explanation of the property of being conscious, in\nwhich case they generate a vicious regress (Rowlands 2001). The first\nhorn can be swiftly dismissed. For as Rosenthal (2005) and many others\nhave made clear, higher-order theories aren’t in the business of\nconceptual analysis. Rather, their goal is to provide a reductive\nexplanation of what it is for a state to be phenomenally\nconscious. Our discussion will therefore focus upon the second\nhorn. \nRowlands thinks that HOT theories face a vicious regress because they\nexplain state-consciousness in terms of HOT, and\nbecause (Rowlands claims) only conscious thoughts make us\naware of the things that those thoughts concern. He gives the example\nof coming to believe that his dog is seriously ill. If he (Rowlands)\nthinks and behaves in ways that are best explained by attributing to\nhim the thought that his dog is ill, but if that thoughts isn’t\nentertained consciously, then surely this won’t be a case in which he\nis aware that his dog is ill. So if we\nare to become aware of our conscious states by entertaining\nhigher-order thoughts about them, then these thoughts will have to be\nconscious ones, requiring us to be aware of them, in turn, via further\nhigher-order thoughts that are also conscious; and so on. \nHOT theorists might respond in several ways: One is to challenge the intuition that only conscious thoughts make us aware of things. Thus it seems that Rowlands, when\nreflecting back on his dog-nurturing behavior of recent days, could\nsurely conclude something along the lines of, ‘It seems that I\nhave been aware of my dog’s illness all along; that is why I have been\nbehaving as I have.’ Another response would be to allow that\nthere is a way of understanding the concept of awareness such\nthat a person only counts as aware of something if the mental state in\nvirtue of which they are aware of that thing is itself a conscious\none, but to deny that this is the relevant sense of\n‘awareness’ which is put to work in HOT theories. A third\noption would be to stress the distinction between phenomenal\nconsciousness and state consciousness more generally, claiming that\nthere need be no regress involved in explaining the former in terms of\nthe latter, provided that some separate account can be provided for\nthe latter. \nOne generic objection, which can probably be recast in such a way as\nto apply to any higher-order theory (although it is most easily\nexpressed against inner sense theory or actualist HOT theory), is the\nso-called ‘rock’ objection (Goldman 1993; Stubenberg 1998). We don’t think that when we become aware of a rock (either\nperceiving it, or entertaining a thought about it) that the rock\nthereby becomes conscious. So why should our higher-order awareness of\na mental state render that mental state conscious? Thinking about a rock doesn’t make the rock‘light up’ and become phenomenally conscious. So why should thinking about my perception of the rock make the latter\nphenomenally conscious, either? \nAn initial reply to this objection involves pointing out that my\nperception of the rock is a mental state, whereas the rock\nitself isn’t (Lycan 1996). Since phenomenal consciousness is a\nproperty that (some) mental states possess, we can then say that the\nreason why the rock isn’t rendered phenomenally conscious by my\nawareness of it is that it isn’t the right sort of thing to\nbe phenomenally conscious, whereas my perception of the rock\nis. This reply may be apt to strike the objector as trite. But perhaps\nmore can be said from the perspective of inner sense theory, at least.\nNotice that my perception of the rock does, in one sense, confer on the\nlatter a subjective aspect. For example, the rock is represented from\none particular spatial perspective, and only some of its properties\n(e.g. color) and not others (e.g. mass) are represented. Likewise,\nthen, with my perception of the rock.  \nSimilar replies to the rock objection are given by Van Gulick (2001)\nand Gennaro (2005). Both point out that a rock isn’t the kind of thing that can\nbe incorporated into a complex mental state that involves\nhigher-order representations, in the sort of way required by a\nself-representational or HOT theory. In contrast, whether actualist HOT\ntheory can reply adequately to the rock objection will depend on\nwhether or not there is an adequate reply to the problem considered in\nsection 4, which challenges the actualist HOT theorist to say why\ntargeting a mental state with a HOT about that state should cause the\nlatter to ‘light up’ and acquire a subjective dimension or\nfeel. \nAnother generic objection is that higher-order theories, when combined\nwith plausible empirical claims about the mental abilities of\nnon-human animals, will conflict with our common-sense intuition that\nsuch animals enjoy phenomenally conscious experience (Jamieson and\nBekoff 1992; Dretske 1995; Tye 1995; Seager 2004). This objection can\nbe pressed most forcefully against higher-order thought\ntheories, of either variety, and against self-representational\ntheories; but it is also faced by inner-sense theory (depending on\nwhat account can be offered of the evolutionary function of organs of\ninner sense). Are cats and dogs really capable of having such apparently complex HOTs which presumably contain mental state concepts? Since there has been onsiderable dispute as to whether even chimpanzees (and other primates) have the kind of sophisticated ‘theory of mind’ to enable them to entertain thoughts about experiential states as such (Byrne and Whiten 1988, 1998; Povinelli 2000), it seems implausible that many other species of mammal (let alone\nreptiles, birds, and fish) would qualify as phenomenally conscious, on\nthese accounts (Carruthers 2000, 2005). Yet the intuition that such creatures enjoy\nphenomenally conscious experiences is a powerful one,\nfor many people. (Witness Nagel’s classic 1974 paper, which argues\nthat there must be something that it is like to be a bat.) \nMany higher-order theorists have attempted to resist the claim that\ntheir theory has any such entailment (e.g.\nGennaro 1996, 2004; Van Gulick 2006). In each case, a common strategy is to\nclaim that the relevant higher-order representations are somehow\nsimpler than those tested for by those who do comparative\n‘theory of mind’ research, hence leaving it open that\nthese simpler representations might be widespread in the animal\nkingdom. Gennaro (1996), for example, suggests that although animals\nmight lack the concept experience, they can nevertheless be\ncapable of higher-order indexical thoughts of the form\n‘this is different from that’ (where\n‘this’ and ‘that’ might refer to experiences\nof red and of green, respectively). The trouble here, however, is to\nexplain what makes these indexicals higher-order in content without\nattributing concepts like experience of green to the animal.\n \nGennaro (2004) takes a somewhat different tack. While allowing that\nanimals lack the concept experience of green, he thinks that\nthey might nevertheless possess the (simpler) concept seeing\ngreen. But here he faces a dilemma. There is, indeed, a simpler\nconcept of seeing, grounded in the capacity to track eye-direction and\nline of sight. But this isn’t necessarily a higher-order concept. To say, in this\nsense, that someone sees green in just to say that there is some green\nin the line in which their eyes are pointed—no mental state\nneeds to be attributed. In contrast, it appears that any concept of\nseeing that is genuinely higher-order will be one that it would be\nless plausible to attribute to most species of animal (given the\ncomparative evidence). Perhaps a first-order explanation of experimental observations of animals is virtually always possible (see, for example, Carruthers 2008). But Gennaro (2012, chapter eight) ultimately argues that there is plenty of evidence that many animals are capable of metacognition (thinking about their own mental states) as well as mindreading (thinking about other minds).  For example, in the case of mindreading, rhesus monkeys seem to attribute visual and auditory perceptions to others in more competitive paradigms (Flombaum and Santos 2005) and crows and scrub jays return alone to caches seen by other animals and recache them in new places (Emery and Clayton 2001). Any evidence of deception or empathy in animals would also seem to indicate some kind of mindreading ability. In addition, many animals seem capable of metacognition (including possessing self-concepts) as evidenced by the presence of episodic memory, for example (Dere et al. 2006; see also the essays in Terrace and Metcalf 2005; Hurley and Nudds 2006).  A related debate takes place with respect to infant consciousness and the capacity of infants to have metacognitive and mindreading abilities (see Gennaro 2012, chapter seven, for some discussion).   \nVan Gulick (2006), in contrast, suggests that all of the higher-order\nrepresenting sufficient to render an experience phenomenally conscious\ncan be left merely implicit in the way that the experience\nenters into relationships with other mental states and the control of\nbehavior. So animals that lack the sorts of explicit higher-order\nconcepts tested for in comparative ‘theory of mind’\nresearch can nevertheless be phenomenally conscious. The difficulty\nhere, however, is to flesh out the relevant notion of implicitness in\nsuch a way that not every mental state, possessed by every creature\n(no matter how simple), will count as phenomenally conscious. For\nsince mental states can’t occur singly, but are always part of a\nnetwork of other related states, mental states will always carry\ninformation about others, thus implicitly representing them. It is\nimplicit in the behavior of any creature that drinks, for example,\nthat it is thirsty; so the drinking behavior implicitly represents the\noccurrence of the mental state of thirst.  \nOf course, the basis for the common-sense intuition that animals possess\nphenomenally conscious states can even be challenged. (How, after\nall, are we supposed to know whether it is like something to\nbe a bat?) And that intuition can perhaps be explained away as a mere\nby-product of imaginative identification with the animal. (Since our\nimages of their experiences are phenomenally conscious, we\nmay naturally assume that the experiences imaged are\nsimilarly conscious (Carruthers 1999, 2000). But there is no doubt\nthat one major source of resistance to higher-order theories will lie here,\nfor many people, especially given various moral considerations about animal pain and suffering. (For one set of attempts to defuse this resistance,\narguing that a higher-order account need have few if any implications\nfor our moral practices or for comparative psychology, see Carruthers\n2005, chapter nine; 2019, chapter eight.)  Of course, some will point out that there are also enough neurophysiological similarities between (at least some parts of) human and animal brains to justify attributions of, say, pains, desires, and basic perceptual states. It is worth emphasizing here that HOT theory does not say that having conscious states requires having introspective states, that is, having conscious HOTs. Conflating introspection with having mere unconscious HOTs (and therefore simply having first-order conscious states) may lead some to put forth a misguided straw man argument against HOT theory. \nA third generic objection is that higher-order approaches cannot\nreally explain the distinctive properties of phenomenal\nconsciousness (Chalmers 1996; Siewert 1998; Levine 2006). Whereas the\nargument from animals is that higher-order representations aren’t\nnecessary for phenomenal consciousness, the argument here is\nthat such representations aren’t sufficient. It is claimed,\nfor example, that we can easily conceive of creatures who enjoy the\npostulated kinds of higher-order representation, related in the right\nsort of way to their first-order perceptual states, but where those\ncreatures are wholly lacking in phenomenal consciousness. \nIn response to this objection, higher-order theorists will join forces\nwith first-order theorists and others in claiming that these objectors\npitch the standards for explaining phenomenal consciousness too high\n(Block and Stalnaker 1999; Tye 1999; Carruthers 2000, 2005; Lycan\n2001). They will insist that a reductive explanation of\nsomething—and of phenomenal consciousness in\nparticular—doesn’t have to be such that we cannot conceive of\nthe explanandum (that which is being explained) in the\nabsence of the explanans (that which does the explaining).\n(Indeed, we can also explain why no such explanation can be\nforthcoming, in terms of our possession of purely recognitional\nconcepts of experience.) Rather, we just need to have good reason to\nthink that the explained properties are constituted\nby the explaining ones, in such a way that nothing\nelse needed to be added to the world once the explaining\nproperties were present, in order for the world to contain the target\nphenomenon. But this is hotly contested territory. And it is on this\nground that the battle for phenomenal consciousness may ultimately be\nwon or lost. \nBefore we close, it is worth considering a variant of the third\ngeneric objection that we have just been discussing, which need\ninvolve no commitment to the latter’s demanding standards of\nexplanation. For it might be said that self-representing mental states\n(or indeed any of the theoretically-relevant kinds of pairing of\nfirst-order with higher-order representations) might occur within the\nunconscious mind, without (of course) thereby becoming conscious (Rey,\n2008). Suppose that some version of Freudian theory is true, for\nexample. Might there not be higher-order thoughts about the subject’s\nexperiences occurring within the unconscious mind, formed while the\nlatter tries to figure out how to get its messages past the\n‘censor’ and expressed in speech? So here again, just as\nwith the third generic objection, the claim is that the occurrence of\nthe sorts of representations postulated by higher-order theories isn’t\nsufficient for phenomenal consciousness. \nOne sort of response to this objection would be to deny that such\npurely unconscious higher-order cognition ever actually\noccurs. Indeed, one might deny that it is even possible, given the\nconstraints provided by the evolution of cognitively demanding mental\nfunctions (Carruthers 2000). But note that this reply will be\nunavailable to any higher-order theorist who has opted to downplay the\ncognitive demands of the capacity for higher-order representation in\nresponse to the problem of animal consciousness. For if higher-order\nrepresentation is easily evolved, and is rife within the animal\nkingdom, then there doesn’t appear to be any reason why it shouldn’t\nevolve within unconscious sub-systems of the mind as well. And in any\ncase it is doubtful whether the mere natural impossibility of\nhigher-order representing within the unconscious mind would be enough\nto rebut the objection. Since higher-order theories claim that\nphenomenal consciousness is to be identified with, or is\nconstituted by, the relevant sorts of higher-order\nrepresenting, we would need to show that the imagined occurrence of\nthe latter within the unconscious mind is metaphysically\nimpossible, not just that it is naturally so. \nOther responses to the objection remain (Carruthers 2000, 2005). One\nwould be to allow that unconscious phenomenal consciousness is\npossible, and to appeal to the distinction between phenomenal\nconsciousness and access consciousness to explain away the\nseeming contradiction involved. Remember, phenomenally conscious\nstates are those that it is like something to be in, and that\npossess a subjective feel; whereas access-conscious states\nare those that are available to interact with some specified cognitive\nprocesses (for example, they might be those that are reportable in\nspeech). So all we would be saying is that states with feel\ncan occur in ways that aren’t (for example) reportable by the subject.\nThere is no contradiction here. An alternative possible response would\nbe to extend the higher-order theory in question to include the\nrelevant sort of access-consciousness as a further component. A\ndispositional HOT theorist, for example, might say that a phenomenally\nconscious state is one that both possesses the right sort of\ndual content and that is reportable by the subject. I shall\nnot attempt to adjudicate between these possibilities here. \n(Objections have also been raised as to how (or if) HOT theory and self-representational theories can account for various pathologies of self-awareness or ‘depersonalization disorders,’ such as somatoparaphrenia and thought insertion in schizophrenia. See the essays in Gennaro 2015 for some discussion.)","contact.mail":"rjgennaro@usi.edu","contact.domain":"usi.edu"}]
