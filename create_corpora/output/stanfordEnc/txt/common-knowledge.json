[{"date.published":"2001-08-28","date.changed":"2013-07-23","url":"https://plato.stanford.edu/entries/common-knowledge/","author1":"Peter Vanderschraaf","author2":"Giacomo Sillari","author1.info":"http://docenti.luiss.it/sillari/","entry":"common-knowledge","body.text":"\n\n\n\nA proposition A is mutual knowledge among a set of\nagents if each agent knows that A. Mutual knowledge by itself\nimplies nothing about what, if any, knowledge anyone attributes to\nanyone else. Suppose each student arrives for a class meeting knowing\nthat the instructor will be late. That the instructor will be late is\nmutual knowledge, but each student might think only she knows the\ninstructor will be late. However, if one of the students says openly\n“Peter told me he will be late again,” then each student knows that\neach student knows that the instructor will be late, each student knows\nthat each student knows that each student knows that the instructor\nwill be late, and so on, ad infinitum. The announcement made\nthe mutually known fact common knowledge among the students. \n\n\n\n\nCommon knowledge is a phenomenon which underwrites much of social\nlife. In order to communicate or otherwise coordinate their behavior\nsuccessfully, individuals typically require mutual or common\nunderstandings or background knowledge. Indeed, if a particular\ninteraction results in “failure”, the usual explanation for this is\nthat the agents involved did not have the common knowledge that would\nhave resulted in success. If a married couple are separated in a\ndepartment store, they stand a good chance of finding one another\nbecause their common knowledge of each others' tastes and experiences\nleads them each to look for the other in a part of the store both know\nthat both would tend to frequent. Since the spouses both love\ncappuccino, each expects the other to go to the coffee bar, and they\nfind one another. But in a less happy case, if a pedestrian causes a\nminor traffic jam by crossing against a red light, she explains her\nmistake as the result of her not noticing, and therefore not knowing,\nthe status of the traffic signal that all the motorists knew. The\nspouses coordinate successfully given their common knowledge, while the\npedestrian and the motorists miscoordinate as the result of a breakdown\nin common knowledge.\n\n\n\nGiven the importance of common knowledge in social interactions, it is\nremarkable that only quite recently have philosophers and social\nscientists attempted to analyze the concept. David Hume (1740) was\nperhaps the first to make explicit reference to the role of mutual\nknowledge in coordination. In his account of convention in A\nTreatise of Human Nature, Hume argued that a necessary condition\nfor coordinated activity was that agents all know what behavior to\nexpect from one another. Without the requisite mutual knowledge, Hume\nmaintained, mutually beneficial social conventions would disappear.\nMuch later, J. E. Littlewood (1953) presented some examples of\ncommon-knowledge-type reasoning, and Thomas Schelling (1960) and John\nHarsanyi (1967–1968) argued that something like common knowledge is\nneeded to explain certain inferences people make about each other. The\nphilosopher Robert Nozick describes, but does not develop, the notion\nin his doctoral dissertation (Nozick 1963), while the first\nmathematical analysis and application of the notion of common\nknowledge is found in the technical report by Friedell (1967), then\npublished as (Friedell\n 1969).[1]\nThe first full-fledged philosophical\nanalysis of common knowledge was offered by David Lewis (1969) in the\nmonograph Convention. Stephen Schiffer (1972), Robert Aumann\n(1976), and Gilbert Harman (1977) independently gave alternate\ndefinitions of common knowledge. Jon Barwise (1988, 1989) gave a\nprecise formulation of Harman's intuitive account. Throughout the\n1980's a number of epistemic logicians, both from philosophy and from\ncomputer science, studied the logical structure of common knowledge,\nand the interested reader should consult the relevant portions of the\ntwo important monographs (Fagin et al. 1995) and (Meyer and Van der\nHoek 1995). Margaret Gilbert (1989) proposed a somewhat different\naccount of common knowledge which she argues is preferable to the\nstandard account. Others have developed accounts of mutual\nknowledge, approximate common knowledge, and common\nbelief which require less stringent assumptions than the standard\naccount, and which serve as more plausible models of what agents know\nin cases where strict common knowledge seems impossible (Brandenburger\nand Dekel 1987, Stinchcombe 1988, Monderer and Samet 1989, Rubinstein\n1992). The analysis and applications of common knowledge and related\nmulti-agent knowledge concepts has become a lively field of\nresearch.\n\n\n\nThe purpose of this essay is to overview of some of the most\nimportant results stemming from this contemporary research. The topics\nreviewed in each section of this essay are as follows: Section 1 gives\nmotivating examples which illustrate a variety of ways in which the\nactions of agents depend crucially upon their having, or lacking,\ncertain common knowledge. Section 2 discusses alternative analyses of\ncommon knowledge. Section 3 reviews applications of multi-agent\nknowledge concepts, particularly to game theory (von Neumann\nand Morgenstern 1944), in which common knowledge assumptions have been\nfound to have great importance in justifying solution concepts\nfor mathematical games. Section 4 discusses skeptical doubts about the\nattainability of common knowledge. Finally, Section 5 discusses the\ncommon belief concept which result from weakening the\nassumptions of Lewis' account of common knowledge.\n\n\n\nMost of the examples in this section are familiar in the common\nknowledge literature, although some of the details and interpretations\npresented here are new. Readers may want to ask themselves what, if\nany, distinctive aspects of mutual and common knowledge reasoning each\nexample illustrates.  \n\nA waiter serving dinner slips, and spills gravy on a guest's white silk\nevening gown. The guest glares at the waiter, and the waiter declares\n“I'm sorry. It was my fault.” Why did the waiter say that he was at\nfault? He knew that he was at fault, and he knew from the guest's angry\nexpression that she knew he was at fault. However, the sorry waiter\nwanted assurance that the guest knew that he knew he was at\nfault. By saying openly that he was at fault, the waiter knew that the\nguest knew what he wanted her to know, namely, that he knew he was at\nfault. Note that the waiter's declaration established at least three\nlevels of nested knowledge.  \n\nCertain assumptions are implicit in the preceding story.  In\nparticular, the waiter must know that the guest knows he has spoken\nthe truth, and that she can draw the desired conclusion from what he\nsays in this context. More fundamentally, the waiter must know that if\nhe announces “It was my fault” to the guest, she will\ninterpret his intended meaning correctly and will infer what his\nmaking this announcement ordinarily implies in this context. This in\nturn implies that the guest must know that if the waiter announces\n“It was my fault” in this context, then the waiter indeed\nknows he is at fault. Then on account of his announcement, the waiter\nknows that the guest knows that he knows he was at fault. The waiter's\nannouncement was meant to generate higher-order levels of\nknowledge of a fact each already knew. \n\nJust a slight strengthening of the stated assumptions results in\neven higher levels of nested knowledge. Suppose the waiter and the\nguest each know that the other can infer what he infers from the\nwaiter's announcement. Can the guest now believe that the waiter does\nnot know that she knows that he knows he is at fault? If the guest\nconsiders this question, she reasons that if the waiter falsely\nbelieves it is possible that she does not know that he knows he is at\nfault, then the waiter must believe it to be possible that she cannot\ninfer that he knows he is at fault from his own declaration. Since she\nknows she can infer that the waiter knows he is at fault from\nhis declaration, she knows that the waiter knows she can infer this, as\nwell. Hence the waiter's announcement establishes the fourth-order\nknowledge claim: The guest knows that the waiter knows that she knows\nthat he knows he is at fault. By similar, albeit lengthier, arguments,\nthe agents can verify that corresponding knowledge claims of even\nhigher-order must also obtain under these assumptions. \n\nThis is a variation of an example first published by Littlewood\n(1953), although he notes that his version of the example was already\nwell-known at the\n time.[3]N individuals enjoy a picnic supper\ntogether which includes barbecued spareribs. At the end of the meal,\nk ≥ 1 of these diners have barbecue sauce on their\nfaces. Since no one can see her own face, none of the messy diners\nknows whether he or she is messy. Then the cook who served the\nspareribs returns with a carton of ice cream. Amused by what he sees,\nthe cook rings the dinner bell and makes the following announcement:\n“At least one of you has barbecue sauce on her face. I will ring\nthe dinner bell over and over, until anyone who is messy has wiped her\nface. Then I will serve dessert.” For the first k\n− 1 rings, no one does anything. Then, at\nthe kth ring, each of the messy individuals\nsuddenly reaches for a napkin, and soon afterwards, the diners are all\nenjoying their ice cream. \n\nHow did the messy diners finally realize that their faces needed\ncleaning? The k = 1 case is easy, since in this case, the lone\nmessy individual will realize he is messy immediately, since he sees\nthat everyone else is clean. Consider the k = 2 case next. At\nthe first ring, messy individual i1 knows that one\nother person, i2, is messy, but does not yet know\nabout himself. At the second ring, i1 realizes that\nhe must be messy, since had i2 been the only messy\none, i2 would have known this after the first ring\nwhen the cook made his announcement, and would have cleaned her face\nthen. By a symmetric argument, messy diner i2 also\nconcludes that she is messy at the second ring, and both pick up a\nnapkin at that time. \n\nThe general case follows by induction. Suppose that if k =\nj, then each of the j messy diners can determine that\nhe is messy after j rings. Then if k = j +\n1, then at the j + 1st ring, each of the j\n+ 1 individuals will realize that he is messy. For if he were not\nmessy, then the other j messy ones would have all realized\ntheir messiness at the jth ring and cleaned\nthemselves then. Since no one cleaned herself after the\njth ring, at the j + 1st ring\neach messy person will conclude that someone besides the other\nj messy people must also be messy, namely, himself. \n\nThe “paradox” of this argument is that for k > 1, like\nthe case of the clumsy waiter of Example 1.1, the cook's announcement\ntold the diners something that each already knew. Yet apparently the\ncook's announcement also gave the diners useful information. How could\nthis be? By announcing a fact already known to every diner, the cook\nmade this fact common knowledge among them, enabling each of\nthem to eventually deduce the condition of his own face after\nsufficiently many rings of the bell.[4] \n\nDoes meeting one's obligations to others serve one's self-interest?\nPlato and his successors recognized that in certain cases, the answer\nseems to be “No.” Hobbes (1651, pp. 101–102) considers the challenge of\na “Foole”, who claims that it is irrational to honor an agreement made\nwith another who has already fulfilled his part of the agreement.\nNoting that in this situation one has gained all the benefit of the\nother's compliance, the Foole contends that it would now be best for\nhim to break the agreement, thereby saving himself the costs of\ncompliance. Of course, if the Foole's analysis of the situation is\ncorrect, then would the other party to the agreement not anticipate the\nFoole's response to agreements honored, and act accordingly? \n\nHume (1740, pp. 520–521) takes up this question, using an example:\nTwo neighboring farmers each expect a bumper crop of corn. Each will\nrequire his neighbor's help in harvesting his corn when it ripens, or\nelse a substantial portion will rot in the field. Since their corn will\nripen at different times, the two farmers can ensure full harvests for\nthemselves by helping each other when their crops ripen, and both know\nthis. Yet the farmers do not help each other. For the farmer whose corn\nripens later reasons that if she were to help the other farmer, then\nwhen her corn ripens he would be in the position of Hobbes' Foole,\nhaving already benefited from her help. He would no longer have\nanything to gain from her, so he would not help her, sparing himself\nthe hard labor of a second harvest. Since she cannot expect the other\nfarmer to return her aid when the time comes, she will not help when\nhis corn ripens first, and of course the other farmer does not help her\nwhen her corn ripens later. \n\nThe structure of Hume's Farmers' Dilemma problem can be\nsummarized using the following tree diagram: \n\nThis tree is an example of a game in extensive form. At each\nstage i, the agent who moves can either choose\nCi, which corresponds to helping or\ncooperating, or Di, which corresponds to\nnot helping or defecting. The relative preferences of the two\nagents over the various outcomes are reflected by the ordered pairs of\npayoffs each receives at any particular outcome. If, for\ninstance, Fiona chooses Ci and Alan chooses\nDi, then Fiona's payoff is 0, her worst payoff, and\nAlan's is 4, his best payoff. In a game such as the Figure 1.1.a game,\nagents are (Bayesian) rational if each chooses an act that\nmaximizes her expected payoff, given what she knows.  \n\nIn the Farmers' Dilemma game, following the\nC1,C2-path is strictly better\nfor both farmers than following the\nD1,D2-path. However, Fiona\nchooses D1, as the result of the following simple\nargument: “If I were to choose C1, then Alan, who\nis rational and who knows the payoff structure of the game, would\nchoose D2. I am also rational and know the payoff\nstructure of the game. So I should choose D1.”\nSince Fiona knows that Alan is rational and knows the game's payoffs,\nshe concludes that she need only analyze the reduced game in\nthe following figure: \n\nIn this reduced game, Fiona is certain to gain a strictly higher\npayoff by choosing D1 than if she chooses\nC1, so D1 is her unique best\nchoice. Of course, when Fiona chooses D1, Alan,\nbeing rational, responds by choosing D2. If Fiona\nand Alan know: (i) that they are both rational, (ii) that they both\nknow the payoff structure of the game, and (iii) that they both know\n(i) and (ii), then they both can predict what the other will do at\nevery node of the Figure 1.1.a game, and conclude that they can rule\nout the D1,C2-branch of the\nFigure 1.1.b game and analyze just the reduced game of the following\nfigure: \n\nOn account of this mutual knowledge, both know that Fiona\nwill choose D1, and that Alan will respond with\nD2. Hence, the\nD1,D2-outcome results if the\nFarmers' Dilemma game is played by agents having this mutual knowledge,\nthough it is suboptimal since both agents would fare better at the\n C1,C2-branch.[5]\n This\nargument, which in its essentials is Hume's argument, is an example of\na standard technique for solving sequential games known as\nbackwards\n induction.[6]\n The basic idea behind backwards induction is\nthat the agents engaged in a sequential game deduce how each will act\nthroughout the entire game by ruling out the acts that are not\npayoff-maximizing for the agents who would move last, then ruling out\nthe acts that are not payoff-maximizing for the agents who would move\nnext-to-last, and so on. Clearly, backwards induction arguments rely\ncrucially upon what, if any, mutual knowledge the agents have regarding\ntheir situation, and they typically require the agents to evaluate the\ntruth values of certain subjunctive conditionals, such as “If I (Fiona)\nwere to choose C1, then Alan would choose\nD2”. \n\nThe mutual knowledge assumptions required to construct a backwards\ninduction solution to a game become more complex as the number of\nstages in the game increases. To see this, consider the sequential\nCentipede game depicted in the following figure: \n\nAt each stage i, the agent who moves can either choose\nRi, which in the first three stages gives\nthe other agent an opportunity to move, or\nLi, which ends the game.  \n\nLike the Farmers' Dilemma, this game is a commitment problem for the\nagents. If each agent could trust the other to choose\nRi at each stage, then they would each\nexpect to receive a payoff of 3. However, Alan chooses\nL1, leaving each with a payoff of only 1, as the\nresult of the following backwards induction argument: “If node\nn4 were to be reached, then Fiona, (being rational)\nwould choose L4. I, knowing this, would (being\nrational) choose L3 if node n3\nwere to be reached. Fiona, knowing this, would (being\nrational) choose L2 if node n2\nwere to be reached. Hence, I (being rational) should choose\nL1.” To carry out this backwards induction\nargument, Alan implicitly assumes that: (i) he knows that Fiona knows\nhe is rational, and (ii) he knows that Fiona knows that he knows she is\nrational. Put another way, for Alan to carry out the backwards\ninduction argument, at node n1 he must know what\nFiona must know at node n2 to make\nL2 her best response should n2\nbe reached. While in the Farmer's Dilemma Fiona needed only\nfirst-order knowledge of Alan's rationality and\nsecond-order knowledge of Alan's knowledge of the game to\nderive the backwards induction solution, in the Figure 1.2 game, for\nAlan to be able to derive the backwards induction solution, the agents\nmust have third-order mutual knowledge of the game and\nsecond-order mutual knowledge of rationality, and Alan must\nhave fourth-order knowledge of this mutual knowledge of the\ngame and third-order knowledge of their mutual knowledge of\nrationality. This argument also involves several counterfactuals, since\nto construct it the agents must be able to evaluate conditionals of the\nform, “If node ni were to be reached, Alan\n(Fiona) would choose Li\n(Ri)”, which for i > 1 are\ncounterfactual, since third-order mutual knowledge of rationality\nimplies that nodes n2, n3, and\nn4 are never reached. \n\nThe method of backwards induction can be applied to any sequential\ngame of perfect information, in which the agents can observe\neach others' moves in turn and can recall the entire history of play.\nHowever, as the number of potential stages of play increases, the\nbackwards induction argument evidently becomes harder to construct.\nThis raises certain questions: (1) What precisely are the mutual or\ncommon knowledge assumptions that are required to justify the backwards\ninduction argument for a particular sequential game? (2) As a\nsequential game increases in complexity, would we expect the mutual\nknowledge that is required for backwards induction to start to\nfail? \n\nSchelling's department store problem is an example of a pure\ncoordination problem, that is, an interaction problem in which the\ninterests of the agents coincide perfectly. Schelling (1960) and Lewis\n(1969), who were the first to make explicit the role common knowledge\nplays in social coordination, were also among the first to argue that\ncoordination problems can be modeled using the analytic vocabulary of\ngame theory. A very simple example of such a coordination problem is\ngiven in the next figure: \n\nThe matrix of Figure 1.3 is an example of a game in strategic\nform. At each outcome of the game, which corresponds to a cell in\nthe matrix, the row (column) agent receives as payoff the first\n(second) element of the ordered pair in the corresponding cell.\nHowever, in strategic form games, each agent chooses without first\nbeing able to observe the choices of any other agent, so that all must\nchoose as if they were choosing simultaneously. The Figure 1.3 game is\na game of pure coordination (Lewis 1969), that is, a game in\nwhich at each outcome, each agent receives exactly the same payoff. One\ninterpretation of this game is that Schelling's spouses, Liz and\nRobert, are searching for each other in the department store with four\nfloors, and they find each other if they go to the same floor. Four\noutcomes at which the spouses coordinate correspond to the strategy\nprofiles\n(sj, sj),\n1 ≤ j ≤ 4, of the Figure 1.3 game. These four profiles\nare strict Nash equilibria (Nash 1950, 1951) of the game, that\nis, each agent has a decisive reason to follow her end of one of these\nstrategy profiles provided that the other also follows this\n profile.[7] \n\nThe difficulty the agents face is trying to select an equilibrium to\nfollow. For suppose that Robert hopes to coordinate with Liz on a\nparticular equilibrium of the game, say\n(s2, s2). Robert reasons\nas follows: “Since there are several strict equilibria we might follow,\nI should follow my end of\n(s2, s2) if, and only if,\nI have sufficiently high expectations that Liz will follow her end of\n(s2, s2). But I can only\nhave sufficiently high expectations that Liz will follow\n(s2, s2 ) if she has\nsufficiently high expectations that I will follow\n(s2, s2). For her to have\nsuch expectations, Liz must have sufficiently high (second-order)\nexpectations that I have sufficiently high expectations that she will\nfollow (s2, s2), for if\nLiz doesn't have these (second-order) expectations, then she will\nbelieve I don't have sufficient reason to follow\n(s2, s2) and may therefore\ndeviate from (s2, s2) herself.\nSo I need to have sufficiently high (third-order) expectations that Liz\nhas sufficiently high (second-order) expectations that I have\nsufficiently high expectations that she will follow\n(s2, s2 ), which involves\nher in fourth-order expectations regarding me, which involves me in\nfifth-order expectations regarding Liz, and so on.” What would\nsuffice for Robert, and Liz, to have decisive reason to follow\n(s2, s2) is that they\neach know that the other knows that … that\nthe other will follow (s2,\ns2) for any number of levels of knowledge, which is\nto say that between Liz and Robert it is common knowledge that they\nwill follow (s2, s2). If agents\nfollow a strict equilibrium in a pure coordination game as a\nconsequence of their having common knowledge of the game, their\nrationality and their intentions to follow this equilibrium, and no\nother, then the agents are said to be following a\nLewis-convention (Lewis 1969). \n\nLewis' theory of convention applies to a more general class of games\nthan pure coordination games, but pure coordination games already model\na variety of important social interactions. In particular, Lewis models\nconventions of language as equilibrium points of a pure coordination\ngame. The role common knowledge plays in games of pure coordination\nsketched above of course raises further questions: (1) Can people ever\nattain the common knowledge which characterizes a Lewis-convention? (2)\nWould less stringent epistemic assumptions suffice to justify Nash\nequilibrium behavior in a coordination problem? \n\nInformally, a proposition A is mutually known among a\nset of agents if each agent knows that A. Mutual knowledge by\nitself implies nothing about what, if any, knowledge anyone attributes\nto anyone else. Suppose each student arrives for a class meeting\nknowing that the instructor will be late. That the instructor will be\nlate is mutual knowledge, but each student might think only she knows\nthe instructor will be late. However, if one of the students says\nopenly “Peter told me he will be late again,” then the mutally known\nfact is now commonly known. Each student now knows that the\ninstructor will be late, and so on, ad infinitum. The agents\nhave common knowledge in the sense articulated informally by Schelling\n(1960), and more precisely by Lewis (1969) and Schiffer (1972).\nSchiffer uses the formal vocabulary of epistemic logic\n(Hintikka 1962) to state his definition of common knowledge. Schiffer's\ngeneral approach was to augment a system of sentential logic with a set\nof knowledge operators corresponding to a set of agents, and then to\ndefine common knowledge as a hierarchy of propositions in the augmented\nsystem. Bacharach (1992) and Bicchieri (1993) adopt this approach, and\ndevelop logical theories of common knowledge which include soundness\nand completeness theorems in the style of (Fagin et al. 1995). One can\nalso develop formal accounts of common knowledge in set-theoretic\nterms, as it was done in the early Friedell (1969) and in the economic\nliterature after Aumann (1976). Such an approach, easily proven to be\nequivalent to the ones cast in epistemic logic, is taken also in this\narticle.[8] \n\nMonderer and Samet (1988) and Binmore and Brandenburger (1989) give a\nparticularly elegant set-theoretic definition of common knowledge. I\nwill review this definition here, and then show that it is logically\nequivalent to the ‘i knows that j knows that\n… k knows that A’ hierarchy that Lewis (1969)\nand Schiffer (1972) argue characterizes common\n knowledge.[9] \n\nSome preliminary notions must be stated first. Following C. I. Lewis\n(1943–1944) and Carnap (1947), propositions are formally subsets of a\nset Ω of state descriptions or possible\nworlds. One can think of the elements of Ω as representing\nLeibniz's possible worlds or Wittgenstein's possible states of\naffairs. Some results in the common knowledge literature presuppose\nthat Ω is of finite cardinality. If this admittedly unrealistic\nassumption is needed in any context, this will be explicitly stated in\nthis essay, and otherwise one may assume that Ω may be either a\nfinite or an infinite set. A distinguished actual world\nωα is an element of Ω. A proposition\nA ⊆ Ω obtains (or is true) if the actual world\nωα ∈ A. In general, we say that\nA obtains at a world ω ∈ Ω if\nω ∈ A. What an agent i knows about the\npossible worlds is stated formally in terms of a knowledge\noperator Ki. Given a\nproposition A ⊆ Ω,\nKi(A) denotes a new\nproposition, corresponding to the set of possible worlds at which\nagent i knows that A obtains.\nKi(A) is read as\n‘i knows (that) A (is the case)’. The\nknowledge operator Ki satisfies\ncertain axioms, including: \n K1:   Ki(A) ⊆\nA  \n\nK2:   Ω ⊆\nKi(Ω) \n\nK3:   Ki(∩kAk)   =   ∩kKi(Ak) \n\nK4:   Ki(A)\n⊆\nKiKi(A)[10] \n\nK5:   −Ki(A)\n⊆\nKi−Ki(A) \n In words, K1 says that if i knows A, then\nA must be the case. K2 says that i knows that some\npossible world in Ω occurs no matter which possible world\nω occurs. K3 says that i knows a conjunction if, and\nonly if, i knows each conjunct. K4 is a reflection\naxiom, sometimes also presented as the axiom of\ntransparency (or of positive introspection), which says\nthat if i knows A, then i knows that she\nknows A. Finally, K5 says that if the agent does not\nknow an event, then she knows that she does not know. This axiom is\npresented as the axiom of negative introspection, or as\nthe axiom of wisdom (since the agents possess Socratic\nwisdom, knowing that they do not know.) Note that by K3, if A\n⊆ B then\nKi(A) ⊆\nKi(B), by K1 and K2,\nKi(Ω) = Ω, and by K1\nand K4, Ki(A) =\nKiKi(A). Any\nsystem of knowledge satisfying K1 – K5 corresponds to the modal\nsystem S5, while any system satisying K1 – K4 corresponds to S4\n(Kripke 1963). If one drops the K1 axiom and retains the others, the\nresulting system would give a formal account of what an agent\nbelieves, but does not necessarily know.  \n\nA useful notion in the formal analysis of knowledge is that of a\npossibility set. An agent i's possibility set at a state of\nthe world Ω is the smallest set of possible worlds that\ni thinks could be the case if ω is the actual world.\nMore precisely, \n\nThe collection of sets  \n\nis i's private information system. \n\nSince in words,\n Hi(ω) is the\nintersection of all propositions which i knows at\n ω,\n Hi(ω) is the smallest\nproposition in Ω that i knows at ω. Put another\nway,\n Hi(ω) is the most\nspecific information that i has about the possible world\nω. The intuition behind assigning agents private information\nsystems is that while an agent i may not be able to perceive\nor comprehend every last detail of the world in which i lives,\ni does know certain facts about that world. The elements of\ni's information system represent what i knows\nimmediately at a possible world. We also have the following: \n\nIn many formal analyses of knowledge in the literature, possibility\nsets are taken as primitive and Proposition 2.2 is given as the\ndefinition of knowledge. If one adopts this viewpoint, then the axioms\nK1 – K5 follow as consequences of the definition of knowledge. In many\napplications, the agents' possibility sets are assumed to\n partition[11]\n the set, in which case\n Hi is called\ni's private information partition. Notice that if axioms K1 –\nK5 hold, then the possibility sets of each agent always partition the\nstate set, and vice versa. \n\nTo illustrate the idea of possibility sets, let us return to the\nBarbecue Problem described in Example 1.2. Suppose there are three\ndiners: Cathy, Jennifer and Mark. Then there are 8 relevant states of\nthe world, summarized by Table 2.1: \n\nEach diner knows the condition of the other diners' faces, but not\nher own. Suppose the cook makes no announcement, after all. Then none\nof the diners knows the true state of the world whatever ω ∈\nΩ the actual world turns out to be, but they do know a\npriori that certain propositions are true at various states of the\nworld. For instance, Cathy's information system before any announcement\nis made is depicted in Figure 2.1a: \n\nIn this case, Cathy's information system is a partition\n H1 of Ω defined\nby \n\nwhere \n\nHCM = {ω4,\nω6} (i.e., Jennifer is clean and Mark is messy) \n\nHMC = {ω3,\nω5} (i.e., Jennifer is messy and Mark is clean) \n\nHMM = {ω7,\nω8} (i.e., Jennifer and Mark are both messy) \n\nCathy knows immediately which cell\n H1(ω) in her partition is the\ncase at any state of the world, but does not know which is the true\nstate at any ω ∈ Ω. \n\nIf we add in the assumption stated in Example 1.2 that if there is\nat least one messy diner, then the cook announces the fact, then\nCathy's information partition is depicted by Figure 2.1b: \n\nIn this case, Cathy's information system is a partition\n H1 of Ω defined\nby \n\nwhere \n\nIn this case, Cathy's information partition is a refinement\nof the partition she has when there is no announcement, for in this\ncase, then Cathy knows a priori that if ω1 is\nthe case there will be no announcement and will know immediately that\nshe is clean, and Cathy knows a priori that if\nω2 is the case, then she will know immediately from\nthe cook's announcement that she is messy. \n\nSimilarly, if the cook makes an announcement only if he sees at least\ntwo messy diners, Cathy's possibility set is the one represented in\nfig. 2.1c: \n\nCathy's information partition is now defined by \n\nwhere \n\nIn this case, Cathy knows a priori that if\nω3 obtains there will be no announcement, and\nsimilarly for ω4. Thus, she will be able to\ndistinguish these states from ω5 and\nω6, respectively.  \n As mentioned earlier in this subsection, the assumption that agents'\npossibility sets partition the state space depends on the modeler's\nchoice of specific axioms for the knowledge operators. For example, if\nwe drop axiom K5 (preserving the validity of K1 – K4) the agent's\npossibility sets need not partition the space set (follow the link for\nan\n example.\n For more details and applications, cf. Samet 1990.) It was\nconjectured (cf. Geanakoplos 1989) that lack of negative introspection\n(i.e. systems without K5) would allow to incorporate unforeseen\ncontingencies in the epistemic model, by representing the agents'\nunawareness of certain events (i.e. the case in which the\nagent does not know that an event occurs and also does not know that\nshe does not know that.) It was later shown by Dekel et al. (1998)\nthat standard models are not suitable to represent agents'\nunawareness. An original non-standard model to represent unawareness\nis provided in Heifetz et al. (2006). For a comprehensive\nbibliography on modeling unawareness and applications of the notion,\ncf. the external links at the end on this entry.  \n\nWe can now define mutual and common knowledge as follows: \n\n1. The proposition that A is (first level or\nfirst order) mutual knowledge for the agents of N,\nK1N(A), is the\nset defined by \n\n2. The proposition that A is mthlevel (or mthorder) mutual\nknowledge among the agents of N,\nKmN(A),\nis defined recursively as the set \n\n3. The proposition that A is common knowledge\namong the agents of N,\nK*N(A), is\ndefined as the set[12] \n\nCommon knowledge of a proposition E implies common\nknowledge of all that E implies, as is shown in the\nfollowing: \n\n Proof. \n\nNote that\n(KmN(E))m≥1 is a decreasing sequence of events, in the sense\nthat\nKm+1N(E)\n⊆\nKmN(E),\nfor all m ≥ 1. It is also easy to check that if everyone\nknows E, then E must be true, that is,\nK1N(E) ⊆\nE. If Ω is assumed to be finite, then if E is\ncommon knowledge at ω, this implies that there must be a finite\nm such that \n\nThe following result relates the set-theoretic definition of common\nknowledge to the hierarchy of ‘i knows that j\nknows that … knows A’ statements. Proposition 2.5\n\n ω ∈\nKmN(A)\niff (1) For all agents i1,\ni2, … , im\n∈ N, ω ∈\nKi1Ki2\n …\n Kim(A)\n Hence, ω ∈\nK*N(A) iff (1)\nis the case for each m ≥ 1.  \n\n Proof. \n\nThe condition that ω ∈\n Ki1Ki2\n …\n Kim(A)\nfor all m ≥ 1 and all i1,\ni2, … , im\n∈ N is Schiffer's definition of common knowledge, and is\noften used as the definition of common knowledge in the literature. \n\nLewis is credited with the idea of characterizing common knowledge as\na hierarchy of ‘i knows that j knows that\n… knows that A’ propositions. However, Lewis is\naware of the difficulties that such an infinitary definition raises. A\nfirst problem is whether it is possible to reduce the infinity\ninherent in the hierarchical account into a workable finite\ndefinition. A second problem is the issue that finite agents cannot\nentertain the infinite amount of epistemic states which is necessary\nfor common knowledge to obtain. Lewis tackles both problems, but his\npresentation is informal.  Aumann is often credited with presenting\nthe first finitary method of generating the common knowledge hierarchy\n(Aumann 1976), even though (Friedell 1969) in fact predates both\nAumann's and Lewis's work. Recently, Cubitt and Sugden (2003) have\nargued that Aumann's and Lewis' accounts of common knowledge are\nradically different and irreconcilable. \n\nAlthough Lewis introduced the technical term ‘common\nknowledge,’ his analysis is about belief, rather than\nknowledge. Indeed, Lewis offers his solution to the second problem\nmentioned above by introducing a distinction between actual\nbelief and reason to believe. Reasons to believe are\ninterpreted as potential beliefs of agents, so that the infinite\nhierarchy of epistemic states becomes harmless, consisting in an\ninfinite number of states of potential belief. The solution to the\nfirst problem is given by providing a finite set of conditions that,\nif met, generate the infinite series of reasons to believe. Such\nconditions taken together represent Lewis' official definition of\ncommon knowledge. Notice that it would be more appropriate to speak of\n‘common reason to believe,’ or, at least, of ‘common\nbelief.’ Lewis himself later acknowledges that “[t]hat\nterm [common knowledge] was unfortunate, since there is no assurance\nthat it will be knowledge, or even that it will be true.”\nCf. (Lewis 1978, p.  44, n.13) Disregarding the distinction between\nreasons to believe and actual belief, we follow (Vanderschraaf 1998)\nto give the details of a formal account of Lewis's definition here,\nand show that Lewis' analysis does result in the common knowledge\nhierarchy following from a finite set of axioms. It is however\ndebatable whether a possible worlds approach can properly render the\nsubtelties of Lewis' characterization. Cubitt and Sugden (2003), for\nexample, abandon the possible worlds framework altogether and propose\na different formal interpretation of Lewis in which, among other\nelements, the distinction between reasons to believe and actual belief\nis taken into account. An attempt to reconcile the two positions can\nbe found in (Sillari 2005), where Lewis' characterization is\nformalized in a richer possible worlds semantic framework where the\ndistinction between reasons to believe and actual believe is\nrepresented. \n\nLewis presents his account of common knowledge on pp. 52–57 of\nConvention. Lewis does not specify what account of knowledge\nis needed for common knowledge. As it turns out, Lewis' account is\nsatisfactory for any formal account of knowledge in which the knowledge\noperators Ki, i ∈\nN, satisfy K1, K2, and K3. A crucial assumption in Lewis'\nanalysis of common knowledge is that agents know they share the same\n“rationality, inductive standards and background\ninformation” (Lewis 1969, p. 53) with respect to a state of\naffairs A′, that is, if an agent can draw any\nconclusion from A′, she knows that all can do\nlikewise. This idea is made precise in the following: \n\nThe definiens says that for each agent i, if i can\ninfer from A′ that E is the case and that\neveryone knows that A′ is the case, then i can\nalso infer that everyone knows that E is the case. \n\nL2:   Ki(A*)\n⊆ Ki(∩j∈NKj(A*)) \n\nL3:   Ki(A*)\n⊆ Ki(E) \n\nA* is a basis for the agents' common knowledge.\nL*N (E) denotes the\nproposition defined by L1 – L3 for a set N of\nA*-symmetric reasoners, so we can say that E is\nLewis-common knowledge for the agents of N iff\nω ∈ L*N(E). \n\nIn words, L1 says that i knows A* at ω. L2\nsays that if i knows that A* obtains, then i\nknows that everyone knows that A* obtains. This axiom is meant\nto capture the idea that common knowledge is based upon a proposition\nA* that is publicly known, as is the case when agents\nhear a public announcement. If the agents' knowledge is represented by\npartitions, then a typical basis for the agents' common knowledge would\nbe an element\n M(ω)\n in\nthe\n meet[14]\n of their partitions. L3 says that\ni can infer from A* that E. Lewis'\ndefinition implies the entire common knowledge hierarchy, as is shown\nin the following result. \n\n Proof. \n\nAs mentioned above, it has recently come into question whether a\nformal rendition of Lewis' definition as the one given above\nadeguately represents all facets of Lewis' approach. Cubitt and Sugden\n(2003) argue that it does not, their critique hinging on a feature of\nLewis' analysis that is lost in the possible worlds framework, namely\nthe 3-place relation of indication used by Lewis. The\ndefinition of indication can be found at pp. 52–53\nof Convention: \n\nThe wording of Lewis' definition and the use he makes of the\nindication relation in the definitory clauses for common knowledge,\nsuggest that Lewis is careful to distinguish indication and material\nimplication. Cubitt and Sugden (2003) incorporate such distinction in\ntheir formal reconstruction. Paired with their interpretation of\n“i has reason to believe x” as\n“x is yielded by some logic of reasoning\nthat i endorses,” we have that, if A\nindi x, then i's reason to believe A\nprovides i with reason to believe x as well. Given\nthat Lewis does want to endow agents with deductive reasoning, (Cubitt\nand Sugden 2003) list the following axioms, claiming that they capture\nthe desired properties of indication. For all agents i, j,\nwith RiA standing for “agent i has reason to believe A”, we have \n\nCS2:   (A entails B) → A\nindiB \n\nCS3:   (A indix\n∧ A indiy) →\nA indi (x ∧\ny) \n\nCS4:   (A indi B ∧ B\nindi x) → A indi x \n\nCS5:   ((A indi\nRj B) ∧\nRi(B indj x))\n→ A indi Rj\nx \n\nThe first axioms captures the intuition behind indication. It says\nthat if an agent has reason to believe that A holds, then, if\nA indicates x to her, she has reason to believe\nx as well. CS2 says that indication extends material\nimplication. CS3 says that if two propositions x and\ny are indicated to an agent by a proposition A, then\nA indicates to her also the conjunction of x and\ny. The next axiom states that indication is transitive. CS5\nsays that if a proposition A indicates to i that\nagent j has reason to believe B, and i has\nreason to believe that B indicates x to j,\nthen A indicates to i also that j has reason\nto believe x. \n\nArmed with these axioms, it is possible to give the following\ndefinition. \n\nRCI1:   A → Ri\nA \n\nRCI2:   A indiRj\nA \n\nRCI3:   A indi x \n\nRCI4:   A indjy →\nRi(A indj\ny) \n\nClauses RCI1-RCI3 above render L1-L3 of definition 2.7 above in the\nformal language that underlies axioms CS1-CS5; while RCI4 affirms (cf.\ndefinition 2.6 above) that agents are symmetric reasoners, i.e. that if\na proposition indicates another proposition to a certain agent, then it\ndoes so to all agents in the population. \n\nThe following proposition shows that RCI1-RCI4 are sufficient\nconditions for ‘common reason to believe’ to arise: \n\n Proof. \n\nA group of (ideal) faultless reasoners who have common reason\nto believe that p, will achieve common belief\nin p. \n\nIs it possible to take formally in account the insights of Lewis'\ndefinition of common knowledge without abandoning the possible world\nframework? (Sillari 2005) puts forth an attempt to give a postive\nanswer to that question by articulating in a possible world semantics\nthe distinction between actual belief and reason to believe. As in\n(Cubitt and Sugden 2003), the basic epistemic operator represents\nreasons to believe. The idea is then to impose an awareness\nstructure over possible worlds, adopting the framework first\nintroduced by Fagin and Halpern (1988). Simply put, an awareness\nstructure associates to each agent, for every possible world, a set of\nevents of which the agent is said to be aware. An agent entertains an\nactual belief that a certain event occurs if and only if she has\nreason to believe that the event occurs and such event is in\nher awareness set at the world under consideration. \n\nAumann (1976) gives a different characterization of common knowledge\nwhich gives another simple algorithm for determining what information\nis commonly known. Aumann's original account assumes that the each\nagent's possibility set forms a private information partition of the\nspace Ω of possible worlds. Aumann shows that a proposition C is\ncommon knowledge if, and only if, C contains a cell of the meet of the\nagents' partitions. One way to compute the meet\n M\n of the partitions\n Hi, i ∈\nN is to use the idea of “reachability”. \n\nIn words, ω′ is reachable from ω if there exists a\nsequence or “chain” of states from ω to ω′ such that\ntwo consecutive states are in the same cell of some agent's information\npartition. To illustrate the idea of reachability, let us return to the\nmodified Barbecue Problem in which Cathy, Jennifer and Mark receive no\nannouncement. Their information partitions are all depicted in Figure\n2.1d: \n\nOne can understand the importance of the notion of reachability in\nthe following way: If ω′ is reachable from ω, then if\nω obtains then some agent can reason that some other agent thinks\nthat ω′ is possible. Looking at Figure 2.1d, if ω =\nω1 occurs, then Cathy (who knows only that\n{ω1, ω2} has occurred) knows that\nJennifer thinks that ω5 might have occurred (even\nthough Cathy knows that ω5 did not occur). So Cathy\ncannot rule out the possibility that Jennifer thinks that Mark thinks\nthat that ω8 might have occurred. And Cathy cannot\nrule out the possibility that Jennifer thinks that Mark thinks that\nCathy believes that ω7 is possible. In this sense,\nω7 is reachable from ω1. The chain of\nstates which establishes this is ω1, ω\n2, ω5, ω8,\nω7, since\n H1(ω1) =\n H1(ω2),\n H2(ω2)\n=\n H2(ω5),\n H3(ω5)\n=\n H3(ω8), and\n H1(ω8)\n=\n H1(ω7). Note that one\ncan show similarly that in this example any state is reachable from any\nother state. This example also illustrates the following immediate\nresult: \n\nOne can read (1) as: ‘At ω, i1\nthinks that i2 thinks that … ,\nim thinks that ω′ is\npossible.’ \n\nWe now have: \n\n Proof. \n\nand  \n\n Proof. \n\nand  \n\n Proof. \n\nIf E =\nK1N(E), then\nE is a public event (Milgrom 1981) or a common\ntruism (Binmore and Brandenburger 1989). Clearly, a common truism\nis common knowledge whenever it occurs, since in this case E =\nK1N(E) =\nK2N(E) =\n… , so E =\nK*N(E). The proof of\nProposition 2.17 shows that the common truisms are precisely the\nelements of\n M\n and unions of\nelements of\n M,\n so any\ncommonly known event is the consequence of a common truism. \n\nBarwise (1988) proposes another definition of common knowledge that\navoids explicit reference to the hierarchy of ‘i knows\nthat j knows that … knows that A’\npropositions. Barwise's analysis builds upon an informal proposal by\nHarman (1977). Consider the situation of the guest and clumsy waiter in\nExample 1 when he announces that he was at fault. They are now in a\nsetting where they have heard the waiter's announcement and know that\nthey are in the setting. Harman adopts the circularity in this\ncharacterization of the setting as fundamental, and propses a\ndefinition of common knowledge in terms of this circularity. Barwise's\nformal analysis gives a precise formulation of Harman's intuitive\nanalysis of common knowledge as a fixed point. Given a\nfunction f, A is a fixed point of f if\nf(A)=A. Now note that \n\nSo we have established that\nK*N (E) is a\nfixed point of the function fE defined by\nfE(X) =\nK1N (E ∩\nX). fE has other fixed points. For\ninstance, any contradiction B ∩ Bc =\nø is a fixed point of\n fE.[15]\n Note\nalso that if A ⊆ B, then E ∩\nA ⊆ E ∩ B and so  \n\nthat is, fE is monotone. (We saw\nthat K1N is also\nmonotone in the proof of Proposition 2.4.) Barwise's analysis of common\nknowledge can be developed using the following result from set theory:  \n\nThis proposition establishes that fE has a greatest\nfixed point, which characterizes common knowledge in Barwise's account.\nAs Barwise himself observes, the fixed point analysis of common\nknowledge is closely related to Aumann's partition account. This is\neasy to see when one compares the fixed point analysis to the notion of\ncommon truisms that Aumann's account generates. Some authors regard the\nfixed point analysis as an alternate formulation of Aumann's analysis.\nBarwise's fixed point analysis of common knowledge is favored by those\nwho are especially interested in the applications of common knowledge\nto problems in logic, while the hierarchical and the partition accounts\nare favored by those who wish to apply common knowledge in social\nphilosophy and social science. When knowledge operators satisfy the\naxioms (K1)-(K5), the Barwise account of common knowledge is equivalent\nto the hierarchical account.  \n\n Proof. \n\nBarwise argues that in fact the fixed point analysis is more flexible\nand consequently more general than the hierachical account.  This may\nsurprise readers in light of Proposition 2.18, which shows that\nBarwise's fixed point definition is equivalent to the\nhierarchical account. Indeed, while Barwise (1988, 1989) proves a\nresult showing that the fixed point account implies the hierarchical\naccount and gives examples that satisfy the common knowledge hierarchy\nbut fail to be fixed points, a number of authors who have written\nafter Barwise have given various proofs of the equivalence of the two\ndefinitions, as was shown in Proposition 2.18. In fact, as (Heifetz\n1999) shows, the hierarchical and fixed-point accounts are equivalent\nfor all finite levels of iteration, while fixed-point common knowledge\nimplies the conjunction of mutual knowledge up to any transfinite\norder, but it is never implied by any such conjunction. \n\nGilbert (1989, Chapter 3) presents an alternative account of common\nknowledge, which is meant to be more intuitively plausible than Lewis'\nand Aumann's accounts. Gilbert gives a highly detailed description of\nthe circumstances under which agents have common knowledge. \n\nGilbert's definition appears to contain some redundancy, since\npresumably an agent would not perceive A unless A is the case. Gilbert\nis evidently trying to give a more explicit account of single agent\nknowledge than Lewis and Aumann give. For Gilbert, agent i\nknows that a proposition E is the case if, and only if,\nω ∈ E, that is, E is true, and either\ni perceives that the state of affairs E describes\nobtains or i can infer E as a consequence of other\npropositions i knows, given sufficient inferential\ncapacity. \n\nLike Lewis, Gilbert recognizes that human agents do not in fact have\nunlimited inferential capacity. To generate the infinite hierarchy of\nmutual knowledge, Gilbert introduces the device of an agent's\nsmooth-reasoner counterpart. The smooth-reasoner counterpart\ni′ of an agent i is an agent that draws every\nlogical conclusion from every fact that i knows. Gilbert\nstipulates that i′ does not have any of the constrains\non time, memory, or reasoning ability that i might have, so\ni′ can literally think through the infinitely many\nlevels of a common knowledge hierarchy. \n\nFrom this definition we get the following immediate consequence: \n\nConsequently,\nKmN′(A)\n for any m ∈\n . \n\nGilbert argues that, given\n S′N′(A),\nthe smooth-reasoner counterparts of the agents of N actually\nsatisfy a much stronger condition, namely mutual knowledge\nKαN′(A)\nto the level of any ordinal number α, finite or infinite. When\nthis stronger condition is satisfied, the proposition A is\nsaid to be open* to the agents of N. With the concept\nof open*-ness, Gilbert gives her definition of common knowledge. \n\nOne might think that an immediate corollary to Gilbert's definition\nis that Gilbert-common knowledge implies the hierarchical common\nknowledge of Proposition 2.5. However, this claim follows only on the\nassumption that an agent knows all of the propositions that her\nsmooth-reasoner counterpart reasons through. Gilbert does not\nexplicitly endorse this position, although she correctly observes that\nLewis and Aumann are committed to something like\n it.[17]\n Gilbert\nmaintains that her account of common knowledge expresses our intuitions\nwith respect to common knowledge better than Lewis' and Aumann's\naccounts, since the notion of open*-ness presumably makes explicit that\nwhen a proposition is common knowledge, it is “out in the open”, so to\nspeak. \n\nReaders primarily interested in philosophical applications of common\nknowledge may want to focus on the No Disagreement Theorem and\nConvention subsections. Readers interested in applications of common\nknowledge in game theory may continue with the Strategic Form Games,\nand Games of Perfect Information subsections.  \n\nAumann (1976) originally used his definition of common knowledge to\nprove a celebrated result that says that in a certain sense, agents\ncannot “agree to disagree” about their beliefs, formalized as\nprobability distributions, if they start with common prior beliefs.\nSince agents in a community often hold different opinions and know they\ndo so, one might attribute such differences to the agents' having\ndifferent private information. Aumann's surprising result is that even\nif agents condition their beliefs on private information, mere common\nknowledge of their conditioned beliefs and a common prior probability\ndistribution implies that their beliefs cannot be different, after\nall! \n\nThen qi(E) =\nqj(E).  \n\n Proof.\n\n[Note that in the proof of this proposition, and in the sequel,\nμ(·|B) denotes conditional\nprobability; that is, given μ(B)>0,\nμ(A|B) =\nμ(A∩B)/μ(B).] \n\nIn a later article, Aumann (1987) argues that the assumptions that\nΩ is finite and that μ(ω) > 0 for each\nω ∈ Ω reflect the idea that agents only regard as\n“really” possible a finite collection of salient worlds to which they\nassign positive probability, so that one can drop the states with\nprobability 0 from the description of the state space. Aumann also\nnotes that this result implicitly assumes that the agents have common\nknowledge of their partitions, since a description of each possible\nworld includes a description of the agents' possibility sets. And of\ncourse, this result depends crucially upon (i), which is known as the\ncommon prior assumption (CPA). \n\nAumann's “no disagreement” theorem has been generalized in\na number of ways in the literature (McKelvey and Page 1986, Monderer\nand Samet 1989, Geanakoplos 1994). However, all of these “no\ndisagreement” results raise the same philosophical puzzle raised\nby Aumann's original result: How are we to explain differences in\nbelief? Aumann's result leaves us with two options: (1) admit that at\nsome level, common knowledge of the agents' beliefs or how they form\ntheir beliefs fails, or (2) deny the CPA. Thus, even if agents do\nassign precise posterior probabilities to an event, Aumann shows that\nif they have merely first-order mutual knowledge of the posteriors,\nthey can “agree to\n disagree”.[18]\nAnother way Aumann's result might fail is if agents do not have common\nknowledge that they update their beliefs by Bayesian\nconditionalization. Then clearly, agents can explain divergent\nopinions as the result of others having modified their beliefs in the\n“wrong” way. However, there are cases in which neither\nexplanation will seem convincing and denying the requisite common\nknowledge seems a rather ad hoc move. Why should one think\nthat such failures of common knowledge provide a general explanation\nfor divergent beliefs? \n\nWhat of the second option, that is, denying the \nCPA?[19]The main\nargument put forward in favor of the CPA is that any differences in\nagents' probabilities should be the result of their having different\ninformation only, that is, there is no reason to think that the\ndifferent beliefs that agents have regarding the same event are the\nresult of anything other than their having different information.\nHowever, one can reply that this argument amounts simply to a\nrestatement of the Harsanyi Doctrine.[20] \n\nSchelling's Department Store problem of Example 1.5 is a very simple\nexample in which the agents “solve” their coordination problem\nappropriately by establishing a convention. (see also the entry on\n convention in this encyclopedia.) Using the\nvocabulary of game theory, Lewis (1969) defines a convention as a\nstrict coordination equilibrium of a game which agents follow\non account of their common knowledge that they all prefer to follow\nthis coordination equilibrium in a recurrent coordination problem. A\ncoordination equilibrium of a game is a strategy combination such that\nno agent is better off if any agent unilaterally deviates from this\ncombination. As with equilibria in general, a coordination equilibrium\nis strict if any agent who deviates unilaterally from the\nequilibrium is strictly worse off. The strategic form game of Figure\n1.3 summarizes Liz's and Robert's situation. The Department Store game\nhas four Nash equilibrium outcomes in pure strategies:\n(s1, s1),\n(s2, s2),\n(s3, s3), and\n(s4,\n s4).[21] \nThese four equilibria are all strict coordination equilibria. If the\nagents follow either of these equilibria, then they coordinate\nsuccessfully. For agents to be following a Lewis-convention in this\nsituation, they must follow one of the game's coordination\nequilibria. However, for Lewis to follow a coordination equilibrium is\nnot a sufficient condition for agents to be following a\nconvention. For suppose that Liz and Robert fail to analyze their\npredicament properly at all, but Liz chooses\ns2 and Robert chooses s2, so\nthat they coordinate at (s2,\ns2) by sheer luck. Lewis does not count accidental\ncoordination of this sort as a convention. \n\nSuppose next that both agents are Bayesian rational, and that part\nof what each agent knows is the payoff structure of the Intersection\ngame. If the agents expect each other to follow\n(s2, s2) and they consequently\ncoordinate successfully, are they then following a convention? Not\nnecessarily, contends Lewis, in a subtle argument on p. 59 of\nConvention. For while each knows the game and that she is\nrational, she might not attribute like knowledge to the other\nagent. If each agent believes that the other agent will follow her end\nof the (s2, s2) equilibrium\nmindlessly, then her best response is to follow her end of\n(s2, s2). But in this case the\nagents coordinated as the result of their each falsely believing that\nthe other acts like an automaton, and Lewis thinks that any proper\naccount of convention must require that agents have correct\nbeliefs about one another. In particular, Lewis requires that each\nagent involved in a convention must have mutual expectations that each\nis acting with the aim of coordinating with the other. The argument\ncan be carried further on. What if both agents believe that they will\nfollow (s2, s2), and believe\nthat each other will do so thinking that the other will\nchoose s2 rationally and not midlessly? Then, say,\nLiz would coordinate as the result of her false second-order belief\nthat Robert believes that Liz acts mindlessly. Similarly for\nthird-order beliefs and so on for any higher order of knowledge. \n\nLewis concludes that a necessary condition for agents to be following\na convention is that their preferences to follow the corresponding\ncoordination equilibrium be common knowledge (the issue whether\nconventions need to be common knowledge has been debated recently,\ncf. Cubitt and Sugden 2003, Binmore 2008, Sillari 2008, and, for an\nexperimental approach, see Devetag et al. 2013). So on Lewis' account,\na convention for a set of agents is a coordination equilibrium which\nthe agents follow on account of their common knowledge of their\nrationality, the payoff structure of the relevant game and that each\nagent follows her part of the equilibrium. \n\nwhere R′ is some possible regularity in the behavior of\nmembers of P in S, such that no one in any instance\nof S among members of P could conform both to\nR′ and to R.\n\n (Lewis 1969, p.\n 76)[22] \n\nLewis includes the requirement that there be an alternate\ncoordination equilibrium R′ besides the equilibrium\nR that all follow in order to capture the fundamental\nintuition that how the agents who follow a convention behave depends\ncrucially upon how they expect the others to behave. \n\nSugden (1986) and Vanderschraaf (1998) argue that it is not crucial\nto the notion of convention that the corresponding equilibrium be a\ncoordination equilibrium. Lewis' key insight is that a convention is a\npattern of mutually beneficial behavior which depends on the agents'\ncommon knowledge that all follow this pattern, and no other.\nVanderschraaf gives a more general definition of convention as a\nstrict equilibrium together with common knowledge that all\nfollow this equilibrium and that all would have followed a different\nequilibrium had their beliefs about each other been different. An\nexample of this more general kind of convention is given below in the\ndiscussion of the Figure 3.1 example. \n\nLewis formulated the notion of common knowledge as part of his\ngeneral account of conventions. In the years following the publication\nof Convention, game theorists have recognized that any\nexplanation of a particular pattern of play in a game depends crucially\non mutual and common knowledge assumptions. More specifically,\nsolution concepts in game theory are both motivated and\njustified in large part by the mutual or common knowledge the agents in\nthe game have regarding their situation. \n\nTo establish the notation that will be used in the discussion that\nfollows, the usual definitions of a game in strategic form, expected\nutility and agents' distributions over their opponents' strategies, are\ngiven here: \n\nwhere Ik(x)\nprojects x ∈\n n onto its\nkth component. \n\nThe subscript ‘-k’ indicates the result of\nremoving the kth component of an n-tuple\nor an n-fold Cartesian product. For instance, \n\ndenotes the pure strategy combinations that agent k's\nopponents may play. \n\nNow let us formally introduce a system of the agents' beliefs into\nthis framework.\nΔk(S-k) denotes\nthe set of probability distributions over the measurable space\n(S-k,\n k), where\n k denotes the Boolean algebra\ngenerated by the strategy combinations\nS-k. Each agent k has a\nprobability distribution μk ∈\nΔk(S-k), and\nthis distribution determines the (Savage) expected utilities\nfor each of k's possible acts: \n\nIf i is an opponent of k, then i's\nindividual strategy si j may\nbe characterized as a union of strategy combinations\n ∪{s−k | sij ∈ s−k}\n ∈\n k, and so\nk's marginal probability for i's strategy\nsi j may be calculated as\nfollows: \n\nμk(· | A) denotes k's\nconditional probability distribution given a set A, and\nE(· | A)\ndenotes k's conditional expectation given\nμk(· | A).  \n\nSuppose first that the agents have common knowledge of the full\npayoff structure of the game they are engaged in and that they are all\nrational, and that no other information is common knowledge. In other\nwords, each agent knows that her opponents are expected utility\nmaximizers, but does not in general know exactly which strategies they\nwill choose or what their probabilities for her acts are. These common\nknowledge assumptions are the motivational basis for the solution\nconcept for noncooperative games known as rationalizability,\nintroduced independently by Bernheim (1984) and Pearce (1984). Roughly\nspeaking, a rationalizable strategy is any strategy an agent\nmay choose without violating common knowledge of Bayesian rationality.\nBernheim and Pearce argue that when only the structure of the game and\nthe agents' Bayesian rationality are common knowledge, the game should\nbe considered “solved” if every agent plays a rationalizable strategy.\nFor instance, in the “Chicken” game with payoff structure defined by\nFigure 3.1, \n\nif Joanna and Lizzi have common knowledge of all of the payoffs at\nevery strategy combination, and they have common knowledge that both\nare Bayesian rational, then any of the four pure strategy profiles is\nrationalizable. For if their beliefs about each other are defined by\nthe probabilities  \n\nthen  \n\nand  \n\nso each agent maximizes her expected utility by playing\ns1 if αi + 2\n≥ 4αi or\nαi ≤ 2/3 and maximizes her\nexpected utility by playing s2 if\nαi ≥ 2/3. If it so happens that\nαi > 2/3 for both agents, then\nboth conform with Bayesian rationality by playing their respective ends\nof the strategy combination\n(s2,s2) given their\nbeliefs, even though each would want to defect from this strategy\ncombination were she to discover that the other is in fact going to\nplay s2. Note that the game's pure strategy Nash\nequilibria, (s1, s2) and\n(s2, s1), are rationalizable,\nsince it is rational for Lizzi and Joanna to conform with either\nequilibrium given appropriate distributions. In general, the set of a\ngame's rationalizable strategy combinations contains the set of the\ngame's pure strategy Nash \nequilibria.[23] \n\nRationalizability can be defined formally in several ways. A\nvariation of Bernheim's original (1984) definition is given here. \n\nis Bayes concordant if and only if,  \n\nand (3.i) is common knowledge. A pure strategy combination\ns =\n (s1j1,\n… ,\n snjn)\n∈ S is rationalizable if and only if the agents\nhave a Bayes concordant system μ of beliefs and,\nfor each agent k ∈ N,  \n\nThe following result shows that the common knowledge restriction on\nthe distributions in Definition 3.1 formalizes the assumption that the\nagents have common knowledge of Bayesian rationality. \n\n Proof. \n\nWhen agents have common knowledge of the game and their Bayesian\nrationality only, one can predict that they will follow a\nrationalizable strategy profile. However, rationalizability becomes an\nunstable solution concept if the agents come to know more about one\nanother. For instance, in the Chicken example above with\nαi > 2/3, i = 1, 2, if either\nagent were to discover the other agent's beliefs about her, she would\nhave good reason not to follow the\n(s2,s2) profile and to revise\nher own beliefs regarding the other agent. If, in the other hand, it so\nhappens that α1 = 1 and α2 = 0, so\nthat the agents maximize expected payoff by following the\n(s2, s1) profile, then should\nthe agents discover their beliefs about each other, they will still\nfollow (s2, s1). Indeed, if\ntheir beliefs are common knowledge, then one can predict with certainty\nthat they will follow (s2,s1).\nThe Nash equilibrium (s2,s1) is\ncharacterized by the belief distributions defined by\nα1 = 1 and α2 = 0. \n\nThe Nash equilibrium is a special case of correlated equilibrium\nconcepts, which are defined in terms of the belief distributions\nof the agents in a game. In general, a correlated\nequilibrium-in-beliefs is a system of agents' probability distributions\nwhich remains stable given common knowledge of the game, rationality\nand the beliefs themselves. We will review two alternative\ncorrelated equilibrium concepts (Aumann 1974, 1987; Vanderschraaf\n1995, 2001), and show how each generalizes the Nash equilibrium concept. \n\nis an endogenous correlated equilibrium if, and only\nif, \n\nIf μ* is an endogenous correlated equilibrium a\npure strategy combination s* =\n(s1*, … ,sn* )\n∈ S is an endogenous correlated equilibrium strategy\ncombination given μ* if, and only if, for each\nagent k ∈ N, \n\nHence, the endogenous correlated equilibrium μ*\nrestricts the set of strategies that the agents might follow, as do the\nBayes concordant beliefs of rationalizability. However, the endogenous\ncorrelated equilibrium concept is a proper refinement of\nrationalizability, because the latter does not presuppose that\ncondition (3.iii) holds with respect to the beliefs one's opponents\nactually have. If exactly one pure strategy combination\ns* satisfies (3.iv) given\nμ*, then μ* is a strict\nequilibrium, and in this case one can predict with certainty what\nthe agents will do given common knowledge of the game, rationality and\ntheir beliefs. Note that Definition 3.5 says nothing about whether or\nnot the agents regard their opponents' strategy combinations as\nprobabilistically independent. Also, this definition does not require\nthat the agents' probabilities are consistent, in the sense\nthat agents' probabilities for a mutual opponent's acts agree. A simple\nrefinement of the endogenous correlated equilibrium concept\ncharacterizes the Nash equilibrium concept. \n\nIn other words, an endogenous correlated equilibrium is a Nash\nequilibrium-in-beliefs when each agent regards the moves of his\nopponents as probabilistically independent and the agents'\nprobabilities are consistent. Note that in the 2-agent case, conditions\n(b) and (c) of the Definition 3.6 are always satisfied, so for 2-agent\ngames the endogenous correlated equilibrium concept reduces to the Nash\nequilibrium concept. Conditions (b) and (c) are traditionally assumed\nin game theory, but Skyrms (1991) and Vanderschraaf (1995, 2001) argue that\nthere may be good reasons to relax these assumptions in games with 3 or\nmore agents. \n\nBrandenburger and Dekel (1988) show that in 2-agent games, if the\nbeliefs of the agents are common knowledge, condition (3.iii)\ncharacterizes a Nash equilibrium-in-beliefs. As they note, condition\n(3.iii) characterizes a Nash equilibrium in beliefs for the\nn-agent case if the probability distributions are consistent\nand satisfy probabilistic independence. Proposition 3.7 extends\nBrandenburger and Dekel's result to the endogenous correlated\nequilibrium concept by relaxing the consistency and probabilistic\nindependence assumptions. \n\nare common knowledge. Then common knowledge of Bayesian rationality is\nsatisfied if, and only if, μ is an endogenous\ncorrelated equilibrium.  \n\n Proof. \n\nIn addition, we have:  \n\nare common knowledge. Then common knowledge of Bayesian rationality\nis satisfied if, and only if, μ is a Nash\nequilibrium. \n\nProof.\n\nThe endogenous correlated equilibrium concept reduces to the Nash\nequilibrium concept in the 2-agent case, so the corollary follows by\nProposition 3.7. \n\nIf μ* is a strict equilibrium, then one can\npredict which pure strategy profile the agents in a game will follow\ngiven common knowledge of the game, rationality and\nμ*. But if μ* is such that\nseveral distinct pure strategy profiles satisfy (3.iv) with respect to\nμ*, then one can no longer predict with certainty\nwhat the agents will do. For instance, in the Chicken game of Figure\n3.1, the belief distributions defined by α1 =\nα2 = 2/3 together are a Nash equilibrium-in-beliefs.\nGiven common knowledge of this equilibrium, either pure strategy is a\nbest reply for each agent, in the sense that either pure strategy\nmaximizes expected utility. Indeed, if agents can also adopt randomized\nor mixed strategies at which they follow one of several pure\nstrategies according to the outcome of a chance experiment, then any of\nthe infinitely mixed strategies an agent might adopt in Chicken is a\nbest reply given\n μ*.[25]\n So the endogenous\ncorrelated equilibrium concept does not determine the exact outcome of\na game in all cases, even if one assumes probabilistic consistency and\nindependence so that the equilibrium is a Nash equilibrium. \n\nAnother correlated equilibrium concept formalized by Aumann (1974,\n1987) does give a determinate prediction of what agents will do in a\ngame given appropriate common knowledge. To illustrate Aumann's\ncorrelated equilibrium concept, let us consider the Figure 3.1 game\nonce more. If Joanna and Lizzi can tie their strategies to their\nknowledge of the possible worlds in a certain way, they can follow a\nsystem of correlated strategies which will yield a payoff vector they\nboth prefer to that of the mixed Nash equilibrium and which is itself\nan equilibrium. One way they can achieve this is to have their friend\nRon play a variation of the familiar shell game by hiding a pea under\none of three walnut shells, numbered 1, 2 and 3. Joanna and Lizzi both\nthink that each of the three relevant possible worlds corresponding to\nωk = {the pea lies under shell k} is\nequally likely. Ron then gives Lizzi and Joanna each a private\nrecommendation, based upon the outcome of the game, which defines a\nsystem of strategy combinations f as follows \n\nf is a correlated strategy system because the\nagents tie their strategies, by following their recommendations, to the\nsame set of states of the world Ω. f is also a strict\nAumann correlated equilibrium, for if each agent knows how Ron\nmakes his recommendations, but knows only the recommendation he gives\nher, either would do strictly worse were she to deviate from her\n recommendation.[26]\n Since there are several strict equilibria of\nChicken, f corresponds to a convention as defined in\nVanderschraaf (1998). The overall expected payoff vector of f\nis (3,3), which lies outside the convex hull of the payoffs for the\ngame's Nash equilibria and which Pareto-dominates the expected payoff\nvector (4/3, 4/3), of the mixed Nash equilibrium defined by\nα1 = 2/3, i = 1,\n 2.[27]\n The correlated equilibrium\nf is characterized by the probability distribution of the agents' play\nover the strategy profiles, given in Figure 3.3: \n\nAumann (1987) proves a result relating his correlated equilibrium\nconcept to common knowledge. To review this result, we must give the\nformal definition of Aumann correlated equilibrium. \n\nfor each k ∈ N and for any function\ngk that is a function of\nfi. \n\nThe agents are at Aumann correlated equilibrium if at each possible\nworld ω ∈ Ω, no agent will want to deviate from his\nrecommended strategy, given that the others follow their recommended\nstrategies. Hence, Aumann correlated equilibrium uniquely specifies the\nstrategy of each agent, by explicitly introducing a space of possible\nworlds to which agents can correlate their acts. The deviations\ngi are required to be functions of\nfi, that is, compositions of some other\nfunction with fi, because i is\ninformed of fi(ω) only, and so can\nonly distinguish between the possible worlds of Ω that are\ndistinguished by fi. As noted already, the\nprimary difference between Aumann's notion of correlated equilibrium\nand the endogenous correlated equilibrium is that in Aumann's\ncorrelated equilibrium, the agents correlate their strategies to some\nevent ω ∈ Ω that is external to the game. One way to\nview this difference is that agents who correlate their strategies\nexogenously can calculate their expected utilities conditional on their\nown strategies. \n\nIn Aumann's model, a description of each possible world ω\nincludes descriptions of the following: the game Γ, the agent's\nprivate information partitions, and the actions chosen by each agent at\nω, and each agent's prior probability distribution\nμk(·) over\nΩ. The basic idea is that conditional on ω, everyone knows\neverything that can be the object of uncertainty on the part of any\nagent, but in general, no agent necessarily knows which world ω\nis the actual world. The agents can use their priors to calculate the\nprobabilities that the various act combinations\ns ∈ S are played. If the\nagents' priors are such that for all i, j ∈\nN, μi(ω) = 0 iff\nμj(ω) = 0, then the agents' priors are\nmutually absolutely continuous. If the agents' priors all\nagree, that is, μ1(ω) = … =\nμn(ω) = μ(ω) for each ω ∈\nΩ, then it is said that the common prior assumption, or\nCPA, is satisfied. If agents are following an Aumann correlated\nequilibrium f and the CPA is satisfied, then f is an\nobjective Aumann correlated equilibrium. An Aumann correlated\nequilibrium is a Nash equilibrium if the CPA is satisfied and the\nagents' distributions satisfy probabilistic\n independence.[28] \n\nLet si(ω) denote the strategy\nchosen by agent i at possible world ω. Then\ns: Ω → S defined by\ns(ω) =\n(s1(ω),…,sn(ω))\nis a correlated n-tuple. Given that\n Hi is a partition of\n Ω,[29]\nthe function si: Ω →\nsi defined by s is\n Hi-measurable if for\neach\n Hij ∈\n Hi,\nsi(ω′) is constant for each\nω′ ∈\n Hij.\n Hi-measurability is a formal\nway of saying that i knows what she will do at each possible\nworld, given her information. \n\nfor any\n Hi-measurable function\nvi : Ω →\nsi. \n\nNote that Aumann's definition of ω-Bayesian rationality implies\nthat\nμi(Hi(ω))\n> 0, so that the conditional expectations are defined. Aumann's\nmain result, given next, implicitly assumes that\nμi(Hi(ω))\n> 0 for every agent i ∈ N and every possible\nworld ω ∈ Ω. This poses no technical difficulties if\nthe CPA is satisfied, or even if the priors are only mutually\nabsolutely continuous, since if this is the case then one can simply\ndrop any ω with zero prior from consideration. \n\n Proof. \n\nPart of the uncertainty the agents might have about their situation\nis whether or not all agents are rational. But if it is assumed that\nall agents are ω-Bayesian rational at each ω ∈\nΩ, then a description of this fact forms part of the description\nof each possible ω and thus lies in the meet of the agents'\npartitions. As noted already, descriptions of the agents' priors, their\npartitions and the game also form part of the description of each\npossible world, so propositions corresponding to these facts also lie\nin the meet of the agents' partitions. So another way of stating\nAumann's main result is as follows: Common knowledge of\nω-Bayesian rationality at each possible world implies that\nthe agents follow an Aumann correlated equilibrium. \n\nPropositions 3.7 and 3.11 are powerful results. They say that common\nknowledge of rationality and of agents beliefs about each other,\nquantified as their probability distributions over the strategy\nprofiles they might follow, implies that the agents' beliefs\ncharacterize an equilibrium of the game. Then if the agents' beliefs\nare unconditional, Proposition 3.7 says that the agents are rational to\nfollow a strategy profile consistent with the corresponding endogenous\ncorrelated equilibrium. If their beliefs are conditional on their\nprivate information partitions, then Proposition 3.11 says they are\nrational to follow the strategies the corresponding Aumann correlated\nequilibrium recommends. However, we must not overestimate the\nimportance of these results, for they say nothing about the\norigins of the common knowledge of rationality and beliefs.\nFor instance, in the Chicken game of Figure 3.1, we considered an\nexample of a correlated equilibrium in which it was assumed\nthat Lizzi and Joanna had common knowledge of the system of recommended\nstrategies defined by\n (). Given this\ncommon knowledge, Joanna and Lizzi indeed have decisive reason to\nfollow the Aumann correlated equilibrium f. But where did this common\nknowledge come from? How, in general, do agents come to have the common\nknowledge which justifies their conforming to an equilibrium?\nPhilosophers and social scientists have made only limited progress in\naddressing this question. \n\nIn extensive form games, the agents move in sequence. At each stage,\nthe agent who is to move must base her decisions upon what she knows\nabout the preceding moves. This part of the agent's knowledge is\ncharacterized by an information set, which is the set of\nalternative moves that an agent knows her predecessor might have\nchosen. For instance, consider the extensive form game of Figure\n3.4: \n\nWhen Joanna moves she is at her information set I22\n= {C1, D1}, that is, she moves\nknowing that Lizzi might have chosen either C1 or\nD1, so this game is an extensive form\nrepresentation of the Chicken game of Figure 3.1.  \n\nIn a game of perfect information, each information set consists of a\nsingle node in the game tree, since by definition at each state the\nagent who is to move knows exactly how her predecessors have moved. In\nExample 1.4 it was noted that the method of backwards induction can be\napplied to any game of perfect\n information.[30]\n The backwards induction\nsolution is the unique Nash equilibrium of a game of perfect\ninformation. The following result gives sufficient conditions to\njustify backwards induction play in a game of perfect information: \n\nProposition 3.12 says that far less than common knowledge of the\ngame and of rationality suffices for the backwards induction solution\nto obtain in a game of perfect information. All that is needed is for\neach agent at each of her information sets to be rational, to know the\ngame and to know what the next agent to move knows! For instance, in\nthe Figure 1.2 game, if R1 (R2)\nstands for “Alan (Fiona) is rational” and\nKi(Γ ) stands for\n“i knows the game Γ”, then the backwards induction\nsolution is implied by the following: \n\nOne might think that a corollary to Proposition 3.11 is that in a game\nof perfect information, common knowledge of the game and of rationality\nimplies the backwards induction solution. This is the classical\nargument for the backwards induction solution. Many game theorists\ncontinue to accept the classical argument, but in recent years, the\nargument has come under strong challenge, led by the work of Reny\n(1987, 1992), Binmore (1987) and Bicchieri (1989, 1993). The basic idea\nunderlying their criticisms of backwards induction can be illustrated\nwith the Figure 1.2 game. According to the classical argument, if Alan\nand Fiona have common knowledge of rationality and the game, then each\nwill predict that the other will follow her end of the backwards\ninduction solution, to which his end of the backwards induction\nsolution is his unique best response. However, what if Fiona\nreconsiders what to do if she finds herself at the information set\nI22? If the information set I22\nis reached, then Alan has of course not followed the backwards\ninduction solution. If we assume that at I22, Fiona\nknows only what is stated in (iii), then she can explain her being at\nI22 as a failure of either\nK1K2K\n1(R2) or\nK1K2K\n1K2(Γ) at\nI11. In this case, Fiona's thinking that either\n∼K1K2\nK1(R2) or\n∼K1K2\nK1K2(Γ) at\nI11 is compatible with what Alan in fact does know\nat I11, so Fiona should not necessarily be\nsurprised to find herself at I22, and given that\nwhat she knows there is characterized by (iii), following the backwards\ninduction solution is her best strategy. But if rationality and the\ngame are common knowledge, or even if Fiona and Alan both have just\nhave mutual knowledge of the statements characterized by (iii) and\n(iv), then at I22, Fiona knows that\nK1K2K\n1(R2) or\nK1K2K\n1K2(Γ) at\nI11. Hence given this much mutual knowledge, Fiona\nno longer can explain why Alan has deviated from the backwards\ninduction solution, since this deviation contradicts part of what is\ntheir mutual knowledge. So if she finds herself at\nI22, Fiona does not necessarily have good reason to\nthink that Alan will follow the backwards induction solution of the\nsubgame beginning at I22, and hence she might not\nhave good reason to follow the backwards induction solution, either.\nBicchieri (1993), who along with Binmore (1987) and Reny (1987, 1992)\nextends this argument to games of perfect information with arbitrary\nlength, draws a startling conclusion: If agents have strictly too few\nor strictly too many levels of mutual knowledge of rationality\nand the game relative to the number of potential moves, one cannot\npredict that they will follow the backwards induction solution. This\nwould undermine the central role backwards induction has played in the\nanalysis of extensive form games. For why should the number of levels\nof mutual knowledge the agents have depend upon the length of the game? \n\nThe classical argument for backwards induction implicitly assumes\nthat at each stage of the game, the agents discount the preceding moves\nas strategically irrelevant. Defenders of the classical argument can\nargue that this assumption makes sense, since by definition at any\nagents' decision node, the previous moves that led to this node are now\nfixed. Critics of the classical argument question this assumption,\ncontending that when reasoning about how to move at any of his\ninformation sets, including those not on the backwards induction\nequilibrium path, part of what an agent must consider is what\nconditions might have led to his being at that information set. In\nother words, agents should incorporate reasoning about the reasoning of\nthe previous movers, or forward induction reasoning, into\ntheir deliberations over how to move at a given information set.\nBinmore (1987) and Bicchieri (1993) contend that a backwards induction\nsolution to a game should be consistent with the solution a\ncorresponding forward induction argument recommends. As we have seen,\ngiven common knowledge of the game and of rationality, forward\ninduction reasoning can lead the agents to an apparent contradiction:\nThe classical argument for backwards induction is predicated on what\nagents predict they would do at nodes in the tree that are never\nreached. They make these predictions based upon their common knowledge\nof the game and of rationality. But forward induction reasoning seems\nto imply that if any off-equilibrium node had been reached, common\nknowledge of rationality and the game must have failed, so how could\nthe agents have predicted what would happen at these nodes? \n\nSituations in which a member of a population P is willing\nto engage in a certain course of action provided that a large enough\nportion of P engages in some appropriate behavior are typical\nproblems of collective action. Consider the case of an agent\nwho is debating whether to join a revolt. Her decision to join or not\nto join will depend on the number of other agents whom she expects to\njoin the revolt. If such a number is too low, she will prefer not to\nrevolt, while if the number is sufficiently large, she will prefer to\nrevolt. Michael Chwe proposes a model where such a situation is modeled\ngame-theoretically. Players' knowledge about other players' intentions\ndepends on a social network in which players are located. The\nindividual ‘thresholds’ for each player (the number of\nother agents that are needed for that specific player to revolt) are\nonly known by the immediate neighbors in the network. Besides the\nintrinsic value of the results obtained by Chwe's analysis regarding\nthe subject of collective action, his model also provides insights\nabout both the relation between social networks and common knowledge\nand about the role of common knowledge in collective action. For\nexample, in some situations, first-order knowledge of other agents'\npersonal thresholds is not sufficient to motivate an agent to take\naction, whereas higher-order knowledge or, in the limit, common\nknowledge is. \n\nWe present Chwe's model following (Chwe 1999) and (Chwe 2000).\nSuppose there is a group P of n people, and each\nagent has two strategies: r (revolt, that is participating in\nthe collective action) and s (stay home and not participate).\nEach agent has her own individual threshold θ ∈ (1,\n2,..., n+1) and she prefers r over s if and\nonly if the total number of players who revolt is greater that or equal\nto her threshold. An agent with threshold 1 always revolts; an agent\nwith threshold 2 revolts only if another agent does; an agent with\nthreshold n revolts only if all agents do; an agent with\nthreshold n+1 never revolts, etc. The agents are located in a\nsocial network, represented by a binary relation → over\nP. The intended meaning of i → j is that agent\ni ‘talks’ to agent j, that is to say,\nagent i knows the threshold of agent j. If we define\nB(i) to be the set {j ∈ P : j → i}, we can\ninterpret B(i) as i's ‘neighborhood’ and\nsay that, in general, i knows the thresholds of all agents in\nher neighborood. A further assumption is that, for all j,k ∈\nB(i), i knows whetehr j → k or not, that\nis, every agent knows whether her neighbors are communicating with each\nother. The relation → is taken to be reflexive (one knows her own\nthreshold). \n\nPlayers' knowledge is represented as usual in a possible worlds\nframework. Consider for example the case in which there are two\nagents, both with one of thresholds 1, 2 or 3. There are nine possible\nworlds represented by ordered pairs of numbers, representing the first\nand second player's individual thresholds respectively: 11, 12,\n13,..., 32, 33. If the players do not communicate, each knows her own\nthreshold only. Player 1's information partition reflects her\nignorance about player's 2 threshold and it consists of the sets {11,\n12, 13}, {21, 22, 23}, {31, 32, 33}; whereas, similarly, player 2's\npartition consists of the sets {11, 21, 31}, {12, 22, 32}, {13, 23,\n33}. If player 1's threshold is 1, she revolts no matter what player\n2's threshold is. Hence, player 1 revolts in {11, 12, 13}. If player\n1's threshold is 3, she never revolts.  Hence, she plays s in\n{31, 32, 33}. If her threshold is 2, she revolts only if the other\nplayer revolts as well. Since in this example we are assuming that\nthere is no communication between the agents, player 1 cannot be sure\nof player's 2 action, and chooses the non-risky\ns in {21, 22, 23} as well. Similarly, player 2 plays\nr in {11, 21, 31} and s otherwise. Consider now the\ncase in which 1 → 2 and 2 → 1. Both players have now the\nfinest information partitions. Thresholds of 1 and 3 yield r\nand s, respectively, for both players again. However, in\nplayer 1's cells {21} and {22}, she knows that player 2 will revolt,\nand, having threshold 2, she revolts as well. Similarly for player 2 in\nhis cells {12} and {22}. Note, that the case in which both players have\nthreshold 2, yields both the equilibrium in which both players revolt\nand the equilibrium in which each player stays home. It is assumed that\nin the case of multiple equilibria, the one which results in the most\nrevolt will obtain. \n\nThe analysis of the example above applies to general networks with\nn agents. Consider for example the three person network 1\n→ 2, 2 → 1, 2 → 3, represented in figure 3.5a (notice\nthat symmetric links are represented by a line without arrowheads) and\nassume that each player has threshold 2. The network between players 1\nand 2 is the same as the one above, hence if they have threshold 2,\nthey both revolt regardless of the threshold of player 3. Player 3, on\nthe other hand, knows her own threshold and player 2's. Hence, if they\nall have threshold 2, she cannot distinguish between the possibilities\nin the set {122, 222, 322, 422}. At 422, in particular, neither player\n1 nor player 2 revolt, hence player 3 cannot take the risk and does\nnot revolt, even if, in fact, she has a neighbor who\nrevolts. Adding the link 1 → 3 to the network (cf. figure 3.5b)\nwe provide player 3 with knowledge about player 1's action, hence in\nthis case, if they all have threshold 2, they all revolt. Notice that\nif we break the link between players 1 and 2 (so that the network is 1\n→ 3 and 2 → 3), player 3 knows that 1 and 2 cannot\ncommunicate and hence do not revolt at 222, therefore she\nchooses s as well. Knowledge of what other players know about\nother players is crucial. \n\nThe next example reveals that in some cases not even first-order\nknowledge is sufficient to trigger action, and higher levels of\nknowledge are necessary. Consider four players, each with threshold 3,\nin the two different networks represented in figure 3.6\n(‘square’, in figure 3.6a, and ‘kite’, in\nfigure 3.6b.) In the square network, player 1 knows that both\n2 and 4 have threshold 3. However, she does not know about player 3's\nthreshold. If player 3 has threshold 5, then player 2 will never\nrevolt, since he does not know about player 4's threshold and it is\nthen possible for him that player 4 has threshold 5 as well. Player 1's\nuncertainty about player 3 together with player 1's knowledge of player\n2's uncertainty about player 4 force her not to revolt, although she\nhas threshold 3 and two neighbors with threshold 3 as well. Similar\nreasoning applies to all other players, hence in the square no one\nrevolts. Consider now the kite network. Player 4 ignores\nplayer 1's and player 2's thresholds, hence he does not revolt.\nHowever, player 1 knows that players 2 and 3 have threshold 3, that\nthey know that they do, and that they know that player 1 knows that\nthey do. This is enough to trigger action r for the three of\nthem, and indeed if players 1, 2 and 3 all revolt in all states in\n{3331, 3332, 3333, 3334, 3335}, this is an equilibrium since in all\nstates at least three people revolt each with threshold three. \n\nThe difference between the square and the kite networks is that,\nalthough in the square enough agents are willing to revolt for a revolt\nto actually take place, and they all individually know this, no agent\nknows that others know it. In the kite, on the other hand, agents in\nthe triangle not only know that there are three agents with threshold\n3, but they also know that they all know it, know that they all know\nthat they all know it, and so on. There is common knwoledge of such\nfact among them. It is interesting to notice that in Chwe's model,\ncommon knowledge obtains without there been a publicly known\nfact (cf. section 2.2). The proposition “players 1, 2 and 3 all have\nthreshold 3” (semantically: the event {3331, 3332, 3333, 3334, 3335})\nis known by players 1, 2 and 3 because of the network structure, and\nbecomes common knolwedge because the network structure is known by the\nplayers. To be sure, the network structure is not just simply known,\nbut it is actually commonly known by the players. Player 1, for\nexample, does not only know that players 2 and 3 communicate with each\nother. She also knows that players 2 and 3 know that she knows that\nthey communicate with each other, and so on. \n\nIn complete networks (networks in which all players\ncommunicate with everyone else, as within the triangle in the kite network)\nthe information partitions of the players coincide, and they are the\nfinest partitions of the set of possible worlds. Hence, if players have\nsufficiently low thresholds, such fact is commonly known and there is\nan equilibrium in which all players revolt. \n\nFor a game in which all players have sufficiently low thresholds, the\ncomplete network is clearly sufficient. Is the complete network\nnecessary to obtain an equilibrium in which all players revolt? It\nturns out that it is not. A crucial role is played by structures of\nthe same kind as the ‘triangle’ group in the kite network,\ncalled cliques. In such structures, ‘local’\ncommon knowledge (that is, limited to the players part of the\nstructure) arises naturally. In a complete network (that is, a network\nin which there is sufficient but not superfluous communication for it\nto fully revolt) in which cliques cover the entire population, if one\nclique speaks to another then every member of that clique speaks to\nevery member of the other clique. Moreover, for every two cliques such\nthat one is talking to the other, there exists a ‘chain’\nof cliques with a starting element. In other words, every pair of\ncliques in the relation are part of a chain (of length at least 2)\nwith a starting element (a leading clique.) Revolt propagates\nin the network moving from ‘leading adopters’ to\n‘followers’, according to the social role\nhierarchy defined by the cliques and their relation. Consider the\nfollowing example, in which cliques are represented by circles and\nnumbers represent the thresholds of individual players: \n\nHere the threshold 3 clique is the leading clique, igniting revolt\nin the threshold 5 follower clique. In turn, the clique of a single\nthreshold 3 element follows. Notice that although she does not need to\nknow that the leading clique actually revolts to be willing to revolt,\nthat information is needed to ensure that the threshold 5 clique does\nrevolt, and hence that it is safe for her to join the revolt. While in\neach clique information about thresholds and hence willingness to\nrevolt is common knowledge, in a chain of cliques information is\n‘linear’; each clique knows about the clique of which it is\na follower, but does not know about earlier cliques. \n\nAnalyzing Chwe's models for collective action under the respect of\nweak versus strong links (cf. both Chwe 1999 and Chwe 2000) provides\nfurther insights about the interaction between communication networks\nand common knolwedge. A strong link, roughly speaking, joins close\nfriends, whereas a weak link joins acquaintances. Strong links tend to\nincrease more slowly than weak ones, since people have common close\nfriends more often than they share acquaintances. In terms of spreading\ninformation and connecting society, then, weak links do a better job\nthan strong links, since they traverse society more quickly and have\ntherefore larger reach. What role do strong and weak links play in\ncollective action? In Chwe's dynamic analysis, strong links fare\nbetter when thresholds are low, whereas weak links are better when\nplayers' thresholds are higher. Intuitively, one sees that strong links\ntend to form small cliques right away (because of the symmetry\nintrinsic in them: my friends' friends tend to be my friends as well);\ncommon knowledge arises quickly at the local level and, if thresholds\nare low, there is a better chance that a group tied by a strong link\nbecomes a leading clique initiating revolt. If, on the other hand,\nthresholds are high, local common knowledge in small cliques is\nfruitless, and weak links, reaching further distances more quickly, speed up\ncommunication and building of the large cliques needed to sparkle\ncollective action. Such considerations shed some light on the relation\nbetween social networks and common knowledge. While it is true that\nknowledge spreads faster in networks in which weak links predominate,\nhigher-order knowledge (and, hence, common knowledge) tends to arise\nmore slowly in this kind of networks. Networks with a larger number of\nstrong links, on the other hand, facilitate the formation of common\nknowledge at the local level. \n\nLewis formulated an account of common knowledge which generates the\nhierarchy of‘i knows that j knows that …\nk knows that A’ propositions in order to ensure\nthat in his account of convention, agents have correct beliefs about\neach other. But since human agents obviously cannot reason their way\nthrough such an infinite hierarchy, it is natural to wonder whether any\ngroup of people can have full common knowledge of any proposition. More\nbroadly, the analyses of common knowledge reviewed in §3 would be\nof little worth to social scientists and philosophers if this common\nknowledge lies beyond the reach of human agents. \n\nFortunately for Lewis' program, there are strong arguments that common\nknowledge is indeed attainable. Lewis (1969) argues that the common\nknowledge hierarchy should be viewed as a chain of implications, and\nnot as steps in anyone's actual reasoning. He gives informal arguments\nthat the common knowledge hierarchy is generated from a finite set of\naxioms. We saw in §2 that it is possible to formulate Lewis'\naxioms precisely and to derive the common knowledge hierarchy from\nthese axioms and a public event functioning as a basis for\ncommon knowledge. Again, the basic idea behind Lewis' argument is that\nfor a set of agents, if a proposition A is publicly known among them\nand each agent knows that everyone can draw the same\nconclusion p from A that she can, then p is common\nknowledge. These conditions are obviously context dependent, just as\nan individual's knowing or not knowing a proposition is context\ndependent.  Yet there are many cases where it is natural to assume\nthat a public event generates common knowledge, because it is properly\nbroadcast, agents in the group are in ideal conditions to perceive it,\nthe inference from the public event to the object of common knowledge\nis immediate, etc. Common knowledge could fail if some of the people\nfailed to perceive the public event, or if some of them believed that\nsome of the others could not understand the announcement, or hear it,\nor could not draw the necessary inferences, and so on. Skeptical doubt\nabout common knowledge is certainly possible, but such doubt relies\nupon ad hoc assumptions similar to those that are needed to\nexplain failure of individual knowledge, not with the\nattainability of common knowledge in principle. Nevertheless, care\nmust be taken in ascribing common knowledge to a group of human\nagents. Common knowledge is a phenomenon highly sensitive to the\nagents' circumstances. The following section gives an example that\nshows that in order for A to be a common truism for a set of\nagents, they ordinarily must perceive an event which\nimplies A simultaneously and publicly. \n\nIn certain contexts, agents might not be able to achieve common\nknowledge. Might they achieve something “close”? One weakening of\ncommon knowledge is of course mth level mutual\nknowledge. For a high value of m,\nKmN(A)\nmight seem a good approximation of\nK*N(A).\nHowever, the following example, due to Rubinstein (1989, 1992), shows\nthat simply truncating the common knowledge hierarchy at any finite\nlevel can lead agents to behave as if they had no mutual knowledge at\n all.[32] \n\nLizzi and Joanna are faced with the coordination problem summarized in\nthe following figure:  \n\nIn Figure 5.1, the payoffs are dependent upon a pair of possible\nworlds. World ω1 occurs with probability\nμ(ω1) = .51, while ω2 occurs with\nprobability μ(ω2) = .49. Hence, they coordinate\nwith complete success by both choosing A (B) only if\nthe state of the world is ω1 (ω2).  \n\nSuppose that Lizzi can observe the state of the world, but Joanna\ncannot. We can interpret this game as follows: Joanna and Lizzi would\nlike to have a dinner together prepared by Aldo, their favorite chef.\nAldo alternates between A and B, the two branches of\nSorriso, their favorite restaurant. State ωi\nis Aldo's location that day. At state ω1\n(ω2), Aldo is at A (B). Lizzi, who\nis on Sorriso's special mailing list, receives notice of\nωi. Lizzi's and Joanna's best outcome occurs\nwhen they meet where Aldo is working, so they can have their planned\ndinner. If they meet but miss Aldo, they are disappointed and do not\nhave dinner after all. If either goes to A and finds herself\nalone, then she is again disappointed and does not have dinner. But\nwhat each really wants to avoid is going to B if the other\ngoes to A. If either of them arrives at B alone, she\nnot only misses dinner but must pay the exorbitant parking fee of the\nhotel which houses B, since the headwaiter of B\nrefuses to validate the parking ticket of anyone who asks for a table\nfor two and then sits alone. This is what Harsanyi (1967) terms a game\nof incomplete information, since the game's payoffs depend\nupon states which not all the agents know. \n\nA is a “play-it-safe” strategy for both Joanna and\n Lizzi.[33]\n By choosing A whatever the state\nof the world happens to be, the agents run the risk that they will fail\nto get the positive payoff of meeting where Aldo is, but each is also\nsure to avoid the really bad consequence of choosing B if the\nother chooses A. And since only Lizzi knows the state of the\nworld, neither can use information regarding the state of the world to\nimprove their prospects for coordination. For Joanna has no such\ninformation, and since Lizzi knows this, she knows that Joanna has to\nchoose accordingly, so Lizzi must choose her best response to the move\nshe anticipates Joanna to make regardless of the state of the world\nLizzi observes. Apparently Lizzi and Joanna cannot achieve expected\npayoffs greater than 1.02 for each, their expected payoffs if they\nchoose (A, A) at either state of the world. \n\nIf the state ω were common knowledge, then the conditional\nstrategy profile (A, A) if ω =\nω1 and (B, B), if ω =\nω2 would be a strict Nash equilibrium at which each\nwould achieve a payoff of 2. So the obvious remedy to their predicament\nwould be for Lizzi to tell Joanna Aldo's location in a face-to-face or\ntelephone conversation and for them to agree to go where Aldo is, which\nwould make the state ω and their intentions to coordinate on the\nbest outcome given ω common knowledge between them. Suppose for\nsome reason they cannot talk to each other, but they prearrange that\nLizzi will send Joanna an e-mail message if, and only if,\nω2 occurs. Suppose further that Joanna's and Lizzi's\ne-mail systems are set up to send a reply message automatically to the\nsender of any message received and viewed, and that due to technical\nproblems there is a small probability, ε > 0, that any\nmessage can fail to arrive at its destination. Then if Lizzi sends\nJoanna a message, and receives an automatic confirmation, then Lizzi\nknows that Joanna knows that ω2 has occurred. If\nJoanna receives an automatic confirmation of Lizzi's automatic\nconfirmation, then Joanna knows that Lizzi knows that Joanna knows that\nω2 occurred, and so on. That ω2 has\noccurred would become common knowledge if each agent received\ninfinitely many automatic confirmations, assuming that all the\nconfirmations could be sent and received in a finite amount of\n time.[34]\n However, because of the probability\nε of transmission failure at every stage of communication, the\nsequence of confirmations stops after finitely many stages with\nprobability one. With probability one, therefore, the agents fail to\nachieve full common knowledge. But they do at least achieve something\n“close” to common knowledge. Does this imply that they have good\nprospects of settling upon (B, B)? \n\nRubinstein shows by induction that if the number of automatically\nexchanged confirmation messages is finite, then A is the only\nchoice that maximizes expected utility for each agent, given what she\nknows about what they both know. \n\nSo even if agents have “almost” common knowledge, in the sense that\nthe number of levels of knowledge in “Joanna knows that Lizzi knows\nthat … that Joanna knows that ω2 occurred” is\nvery large, their behavior is quite different from their behavior given\ncommon knowledge that ω2 has occurred. Indeed, as\nRubinstein points out, given merely “almost” common knowledge, the\nagents choose as if no communication had occurred at all! Rubinstein\nalso notes that this result violates our intuitions about what we would\nexpect the agents to do in this case. (See Rubinstein 1992, p. 324.) If\nTi = 17, wouldn't we expect agent\ni to choose B? Indeed, in many actual situations we\nmight think it plausible that the agents would each expect the other to\nchoose B even if T1 =\nT2 = 2, which is all that is needed for Lizzi to\nknow that Joanna has received her original message and for Joanna to\nknow that Lizzi knows this! Binmore and Samelson (2001) in fact show\nthat if Joanna and Lizzi incur a cost when paying attention to the\nmessages they exchange, or if sending a message is costly, then longer\nstreams of messages are not paid attention to or do not occur,\nrespectively. \n\nThe example in Section 5.1 hints that mutual knowledge is not the only\nweakening of common knowledge that is relevant to coordination.\nBrandenburger and Dekel (1987), Stinchcombe (1988) and Monderer and\nSamet (1989) explore another option, which is to weaken the properties\nof the K*N operator.\nMonderer and Samet motivate this approach by noting that even if a\nmutual knowledge hierarchy stops at a certain level, agents might still\nhave higher level mutual beliefs about the proposition in\nquestion. So they replace the knowledge operator\nKi with a belief operator\nBpi:  \nBpi(A)\nis to be read ‘i believes A (given i's\nprivate information) with probability at least p at\nω’, or ‘i   p-believes\nA’. The belief operator\nBpi satisfies\naxioms K2, K3, and K4 of the knowledge operator.\nBpi does not\nsatisfy K1, but does satisfy the weaker property \n\nthat is, if one believes A with probability at least\np, then the probability of A is indeed at least\np. \n\nOne can define mutual and common p-beliefs\nrecursively in a manner similar to the definition of mutual and common\nknowledge: \n\n(1) The proposition that A is (first level or first\norder) mutual p-belief for the agents of N,\nBpN1(A),\nis the set defined by \n\n(2) The proposition that A is mthlevel (or mthorder)\nmutual p-belief among the agents of N,\nBpNm(A),\n is defined recursively as the set  \n\n(3) The proposition that A is common p-belief among\nthe agents of N,\nBpN*(A),\nis defined as the set  \n\nIf A is common (or mth level mutual)\nknowledge at world ω, then A is common\n(mth level) p-belief at ω for every\nvalue of p. So mutual and common p-beliefs formally\ngeneralize the mutual and common knowledge concepts. However, note that\nB1N*(A) is not\nnecessarily the same proposition as\nK*N(A), that\nis, even if A is common 1-belief, A can fail to be\ncommon knowledge. \n\nCommon p-belief forms a hierarchy similar to a common\nknowledge hierarchy: Proposition 5.3\n\n ω ∈\nBpNm(A) iff (1) For all agents i1,\ni2, … , im\n∈ N, ω ∈\nBpi1Bpi2 …\nBpim(A) Hence, ω ∈\nBpN*(A)\niff (1) is the case for each m ≥ 1.  \n\nProof. Similar to the\n Proof of Proposition 2.5. \n\nOne can draw several morals from the e-mail game of Example 5.1.\nRubinstein (1987) argues that his conclusion seems paradoxical for the\nsame reason the backwards induction solution of Alan's and Fiona's\nperfect information game might seem paradoxical: Mathematical induction\ndoes not appear to be part of our “everyday” reasoning. This game also\nshows that in order for A to be a common truism for a set of agents,\nthey ordinarily must perceive an event which implies A\nsimultaneously in each others' presence. A third moral is that\nin some cases, it may make sense for the agents to employ some solution\nconcept weaker than Nash or correlated equilibrium. In their analysis\nof the e-mail game, Monderer and Samet (1989) introduce the notions of\nex ante and ex post ε-equilibrium. An ex\nante equilibrium h is a system of strategy profiles such\nthat no agent i expects to gain more than ε-utiles if\ni deviates from h. An ex post equilibrium\nh′ is a system of strategy profiles such that no agent\ni expects to gain more than ε-utiles by deviating from\nh′ given i's private information. When\nε = 0, these concepts coincide, and h is a Nash\nequilibrium. Monderer and Samet show that, while the agents in the\ne-mail game can never achieve common knowledge of the world ω, if\nthey have common p-belief of ω for sufficiently high\np, then there is an ex ante equilibrium at which they\nfollow (A,A) if ω = ω1 and\n(B,B), if ω = ω2. This\nequilibrium turns out not to be ex post. However, if the\nsituation is changed so that there are no replies, then Lizzi and\nJoanna could have at most first order mutual knowledge that ω =\nω2. Monderer and Samet show that in this situation,\ngiven sufficiently high common p-belief that ω =\nω2, there is an ex post equilibrium at which\nJoanna and Lizzi choose (B,B) if ω =\nω2! So another way one might view this third moral of\nthe e-mail game is that agents' prospects for coordination can\nsometimes improve dramatically if they rely on their common beliefs as\nwell as their mutual knowledge.","contact.mail":"gsillari@luiss.it","contact.domain":"luiss.it"}]
