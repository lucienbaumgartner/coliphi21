[{"date.published":"2005-02-19","date.changed":"2019-06-27","url":"https://plato.stanford.edu/entries/molecular-biology/","author1":"James Tabery","author1.info":"https://faculty.utah.edu/u0578517-JAMES_TABERY/biography/index.hml","author2.info":"http://www.albany.edu/philosophy/faculty.shtml#piotrowska","entry":"molecular-biology","body.text":"\n\n\nThe field of molecular biology studies macromolecules and the\nmacromolecular mechanisms found in living things, such as the\nmolecular nature of the gene and its mechanisms of gene replication,\nmutation, and expression. Given the fundamental importance of these\nmacromolecular mechanisms throughout the history of molecular biology,\na philosophical focus on the concept of a mechanism generates the\nclearest picture of molecular biology’s history, concepts, and\ncase studies utilized by philosophers of science.\n\nDespite its prominence in the contemporary life sciences, molecular\nbiology is a relatively young discipline, originating in the 1930s and\n1940s, and becoming institutionalized in the 1950s and 1960s. It\nshould not be surprising, then, that many of the philosophical issues\nin molecular biology are closely intertwined with this recent history.\nThis section sketches four facets of molecular biology’s\ndevelopment: its origins, its classical period, its subsequent\nmigration into other biological domains, and its more recent turn to\ngenomics and post-genomics. The rich historiography of molecular\nbiology can only be briefly utilized in this shortened history (see,\nfor example, Abir-Am 1985, 1987, 1994, 2006; Burian 1993a; Canguillhem\n1989; de Chadarevian 2002, 2003; de Chadarevian and Gaudilliere 1996;\nde Chadarevian and Strasser 2002; Deichmann 2002; Fisher 2010;\nHausmann 2002; Holmes 2001; Judson 1980, 1996; Kay 1993; Marcum 2002;\nMorange 1997a, 1998; Olby 1979, 1990, 1994, 2003; Powell et al. 2007;\nRheinberger 1997; Sapp 1992; Sarkar 1996a; Stegenga 2011; van Holde\nand Zlatanova 2018; Witkowski 2005; Zallen 1996. Also see\nautobiographical accounts by biologists, such as Brenner 2001; Cohen\n1984; Crick 1988; Echols 2001; Jacob 1988; Kornberg 1989; Luria 1984;\nWatson 1968, 2002, 2007; Wilkins 2003). \nThe field of molecular biology arose from the convergence of work by\ngeneticists, physicists, and structural chemists on a common problem:\nthe nature of inheritance. In the early twentieth century, although\nthe nascent field of genetics was guided by Mendel’s laws of\nsegregation and independent assortment, the actual mechanisms of gene\nreproduction, mutation and expression remained unknown. Thomas Hunt\nMorgan and his colleagues utilized the fruit fly, Drosophila\nmelanogaster, as a model organism to study the relationship\nbetween the gene and the chromosomes in the hereditary process (Morgan\n1926; discussed in Darden 1991; Darden and Maull 1977; Kohler 1994;\nRoll-Hanson 1978; Wimsatt 1992). A former student of Morgan’s,\nHermann J. Muller, recognized the “gene as a basis of\nlife”, and so set out to investigate its structure (Muller\n1926). Muller discovered the mutagenic effect of x-rays on\nDrosophila, and utilized this phenomenon as a tool to explore\nthe size and nature of the gene (Carlson 1966, 1971, 1981, 2011; Crow\n1992; Muller 1927). But despite the power of mutagenesis, Muller\nrecognized that, as a geneticist, he was limited in the extent to\nwhich he could explicate the more fundamental properties of genes and\ntheir actions. He concluded a 1936 essay: \nThe geneticist himself is helpless to analyse these properties\nfurther. Here the physicist, as well as the chemist, must step in. Who\nwill volunteer to do so? (Muller 1936: 214) \nMuller’s request did not go unanswered. The next decade saw\nseveral famous physicists turn their attention to the nature of\ninheritance (Keller 1990; Kendrew 1967). In What is Life, the\nphysicist Erwin Schroedinger (1944) proposed ways in which the\nprinciples of quantum physics might account for the stability, yet\nmutability, of the gene (see the entry on\n life)\n (Elitzur 1995; Moore 1989; Olby 1994; Sarkar 1991; for a\nreinterpretation see Kay 2000). Max Delbrueck also became interested\nin the physical basis of heredity after hearing a lecture by his\nteacher, quantum physicist Niels Bohr (1933), which expounded a\nprinciple of complementarity between physics and biology (McKaughan\n2005; Roll-Hansen 2000). In contrast to Schroedinger, Bohr (and\nsubsequently Delbrueck) did not seek to reduce biology to physics;\ninstead, the goal was to understand how each discipline complemented\nthe other (Delbrueck 1949; Sloan and Fogel 2011). To investigate the\nself-reproductive characteristic of life, Delbrueck used\nbacteriophage, viruses that infect bacteria and then multiply very\nrapidly. The establishment of “The Phage Group” in the\nearly 1940s by Delbrueck and another physicist-turned-biologist\nSalvador Luria marked a critical point in the rise of molecular\nbiology (Brock 1990; Cairns et al. 1966; Fischer and Lipson 1988;\nFleming 1968; Lewontin 1968; Luria 1984; Morange 1998: Ch. 4; Stent\n1968). Delbrueck’s colleague at Cal Tech, Linus Pauling,\nutilized his knowledge of structural chemistry to study macromolecular\nstructure. Pauling contributed both theoretical work on the nature of\nchemical bonds and experimental work using x-ray crystallography to\ndiscover the physical structure of macromolecular compounds (Pauling\n1939, 1970; Olby 1979; Hager 1995; Crick 1996; Sarkar 1998). \nAs suggested in the brief history above, experimentation figured\nprominently in the rise of molecular biology (see the entry on\n experiment in biology).\n X-ray crystallography allowed molecular biologists to investigate the\nstructure of macromolecules.\n Alfred Hershey and Martha Chase (1952) used phage viruses to confirm\nthat the genetic material transmitted from generation to generation\nwas DNA and not proteins (see Hershey-Chase Experiment in\n Other Internet Resources).\n Muller (1927) used x-rays to intervene on and alter gene function,\nthus revealing the application of methods from physics to a biological\ndomain (see Elof Carlson on Muller’s Research in\n Other Internet Resources).\n  \nRecognizing quite early the importance of these new physical and\nstructural chemical approaches to biology, Warren Weaver, then the\ndirector of the Natural Sciences section of the Rockefeller\nFoundation, introduced the term “molecular biology” in a\n1938 report to the Foundation. Weaver wrote, \nAnd gradually there is coming into being a new branch of\nscience—molecular biology—which is beginning to uncover\nmany secrets concerning the ultimate units of the living\ncell….in which delicate modern techniques are being used to\ninvestigate ever more minute details of certain life processes (quoted\nin Olby 1994: 442). \nBut perhaps a more telling account of the term’s origin came\nfrom Francis Crick, who said he started calling himself a molecular\nbiologist because: \nwhen inquiring clergymen asked me what I did, I got tired of\nexplaining that I was a mixture of crystallographer, biophysicist,\nbiochemist, and geneticist, an explanation which in any case they\nfound too hard to grasp. (quoted in Stent 1969: 36) \nThis brief recapitulation of the origins of molecular biology reflects\nthemes addressed by philosophers, such as reduction (see\n Section 3.1),\n the concept of the gene (see\n Section 2.3),\n and experimentation (see\n Section 3.4).\n For Schroedinger, biology was to be reduced to the more fundamental\nprinciples of physics, while Delbrueck instead resisted such a\nreduction and sought what made biology unique. Muller’s shift\nfrom Mendelian genetics to the study of gene structure raises the\nquestion of the relation between the gene concepts found in those\nseparate fields of genetics. And the import of experimental methods\nfrom physics to biology raised the question of the relation between\nthose disciplines. \nMolecular biology’s classical period began in 1953, with James\nWatson and Francis Crick’s discovery of the double helical\nstructure of DNA (Watson and Crick 1953a,b). Watson and Crick’s\nscientific relationship unified the various disciplinary approaches\ndiscussed above: Watson, a student of Luria and the phage group,\nrecognized the need to utilize crystallography to elucidate the\nstructure of DNA; Crick, a physicist enticed by Schroedinger’s\nWhat is Life? to turn to biology, became trained in, and\ncontributed to the theory of, x-ray crystallography. At Cambridge\nUniversity, Watson and Crick found that they shared an interest in\ngenes and the structure of DNA (see the entry on\n scientific revolutions). \nWatson and Crick collaborated to build a model of the double helical\nstructure of DNA, with its two helical strands held together by\nhydrogen-bonded base pairs (Olby 1994). They made extensive use of\ndata from x-ray crystallography work on DNA by Maurice Wilkins and\nRosalind Franklin at King’s College, London, appallingly without\nFranklin’s permission or even knowledge (Maddox 2002),\nCrick’s theoretical work on crystallography (Crick 1988), and\nthe model building techniques pioneered by Pauling (de Chadarevian\n2002; Judson 1996; Olby 1970, 1994, 2009). \nWith the structure of DNA in hand, molecular biology shifted its focus\nto how the double helical structure aided elucidation of the\nmechanisms of genetic replication and function, the keys to\nunderstanding the role of genes in heredity (see the entries on\n replication and reproduction\n and\n inheritance systems).\n This subsequent research was guided by the notion that the gene was\nan informational molecule. According to Lily Kay, \nUp until around 1950 molecular biologists…described genetic\nmechanisms without ever using the term information. (Kay\n2000: 328) \n“Information” replaced earlier talk of biological\n“specificity”. Watson and Crick’s second paper of\n1953, which discussed the genetical implications of their recently\ndiscovered (Watson and Crick 1953a) double-helical structure of DNA,\nused both “code” and “information”: \n…it therefore seems likely that the precise sequence of the\nbases is the code which carries the genetical\ninformation…. (Watson and Crick 1953b: 244, emphasis\nadded) \nIn 1958, Francis Crick used and characterized the concept of\ninformation in the context of stating the “central\ndogma” of molecular biology. Crick characterized the central\ndogma as follows: \nThis states that once “information” has passed into\nprotein it cannot get out again. In more detail, the transfer\nof information from nucleic acid to nucleic acid, or from nucleic acid\nto protein may be possible, but transfer from protein to protein, or\nfrom protein to nucleic acid is impossible. Information means here the\nprecise determination of sequence, either of bases in the nucleic acid\nor of amino acid residues in the protein. (Crick 1958: 152–153,\nemphasis in original) \nIt is important not to confuse the genetic code and genetic\ninformation. The genetic code refers to the relation between three\nbases of DNA, called a “codon”, and one amino acid. Tables\navailable in molecular biology textbooks (e.g., Watson et al. 1988:\nfrontispiece) show the relation between 64 codons and 20 amino acids.\nFor example, CAC codes for histidine. Only a few exceptions for these\ncoding relations have been found, in a few anomalous cases (see the\nlist in a small table in Alberts et al. 2002: 814). In contrast,\ngenetic information refers to the linear sequence of codons along the\nDNA, which (in the simplest case) are transcribed to messenger RNA,\nwhich are translated to linearly order the amino acids in a\nprotein. \nWith the genetic code elucidated and the relationship between genes\nand their molecular products traced, it seemed in the late 1960s that\nthe concept of the gene was secure in its connection between gene\nstructure and gene function. The machinery of protein synthesis\ntranslated the coded information in the linear order of nucleic acid\nbases into the linear order of amino acids in a protein. However, such\n“colinear” simplicity did not persist. In the late 1970s,\na series of discoveries by molecular biologists complicated the\nstraightforward relationship between a single, continuous DNA sequence\nand its protein product. Overlapping genes were discovered (Barrell et\nal. 1976); such genes were considered “overlapping”\nbecause two different amino acid chains might be read from the same\nstretch of nucleic acids by starting from different points on the DNA\nsequence. And split genes were found (Berget et al. 1977; Chow et al.\n1977). In contrast to the colinearity hypothesis that a continuous\nnucleic acid sequence generated an amino acid chain, it became\napparent that stretches of DNA were often split between coding regions\n(exons) and non-coding regions (introns). Moreover, the exons might be\nseparated by vast portions of this non-coding, supposedly “junk\nDNA”. The distinction between exons and introns became even more\ncomplicated when\n alternative splicing\n was discovered the following year (Berk and Sharp 1978). A series of\nexons could be spliced together in a variety of ways, thus generating\na variety of molecular products. Discoveries such as overlapping\ngenes, split genes, and alternative splicing forced molecular\nbiologists to rethink their understanding of what actually made a\ngene…a gene (Portin 1993; for a survey of such complications\nsee Gerstein et al. 2007: Table 1). \nThese developments in molecular biology have received philosophical\nscrutiny. Molecular biologists sought to discover mechanisms\n(see\n Section 2.1),\n drawing the attention of philosophers to this concept. Also,\nconceptualizing DNA as an informational molecule (see\n Section 2.2)\n was a move that philosophers have subjected to critical scrutiny.\nFinally, the concept of the gene (see\n Section 2.3)\n itself has intrigued philosophers. Complex molecular mechanisms, such\nas alternative splicing, have obligated philosophers to consider to\nwhat the term “gene” actually refers. Experimentation also\nfigured prominently in the classical period (see\n Section 3.4);\n Matthew Meselson and Frank Stahl utilized bacteria grown with\ndifferent weights combined with centrifugation to determine how DNA,\nas modeled by Watson and Crick, was replicated (Meselson and Stahl\n1958; see also The Semi-Conservative Replication of DNA in\n Other Internet Resources).\n  \nIn a 1963 letter to Max Perutz, molecular biologist Sydney Brenner\nforeshadowed what would be molecular biology’s next intellectual\nmigration: \nIt is now widely realized that nearly all the “classical”\nproblems of molecular biology have either been solved or will be\nsolved in the next decade…. Because of this, I have long felt\nthat the future of molecular biology lies in the extension of research\nto other fields of biology, notably development and the nervous\nsystem. (Brenner, letter to Perutz, 1963) \nAlong with Brenner, in the late 1960s and early 1970s, many of the\nleading molecular biologists from the classical period redirected\ntheir research agendas, utilizing the newly developed molecular\ntechniques to investigate unsolved problems in other fields. Francois\nJacob, Jacques Monod and their colleagues used the bacteria\nEscherichia coli to investigate how environmental conditions\nimpact gene expression and regulation (Jacob and Monod 1961; discussed\nin Craver and Darden 2013; Morange 1998: Ch. 14; Schaffner 1974a;\nWeber 2005; see also the entry on the\n developmental biology).\n The study of behavior and the nervous system also lured some\nmolecular biologists. Finding appropriate model organisms that could\nbe subjected to molecular genetic analyses proved challenging.\nReturning to the fruit flies used in Mendelian genetics, Seymour\nBenzer induced behavioral mutations in Drosophila as a\n“genetic scalpel” to investigate the pathways from genes\nto behavior (Benzer 1968; Weiner 1999). And at Cambridge, Sydney\nBrenner developed the nematode worm, Caenorhabditis elegans,\nto study the nervous system, as well as the genetics of behavior\n(Brenner 1973, 2001; Ankeny 2000; Brown 2003). In subsequent decades,\nthe study of cells was transformed from descriptive cytology into\nmolecular cell biology (Alberts et al. 1983; Alberts et al. 2002;\nBechtel 2006). Molecular evolution developed as a phylogenetic method\nfor the comparison of DNA sequences and whole genomes; molecular\nsystematics sought to research the evolution of the genetic code as\nwell as the rates of that evolutionary process by comparing\nsimilarities and differences between molecules (Dietrich 1998; see\nalso the entries on\n evolution,\n heritability, and\n adaptationism).\n The immunological relationship between antibodies and antigens was\nrecharacterized at the molecular level (Podolsky and Tauber 1997;\nSchaffner 1993; see also the entry on the\n philosophy of immunology).\n And the study of oncogenes in cancer research as well as the\nmolecular bases of mental illness were examples of advances in\nmolecular medicine (Morange 1997b; see also the entry on\n philosophy of psychiatry). \nThis process of “going molecular” thus generally amounted\nto using experimental methods from molecular biology to examine\ncomplex phenomena (be it gene regulation, behavior, or evolution) at\nthe molecular level. The molecularization of many fields introduced a\nrange of issues of interest to philosophers. Inferences made about\nresearch on model organisms such as worms and flies raised questions\nabout extrapolation (see\n Section 3.3).\n And the reductive techniques of molecular biology raised questions\nabout whether scientific investigations should always strive to reduce\nto lower and lower levels (see\n Section 3.1). \nIn the 1970s, as many of the leading molecular biologists were\nmigrating into other fields, molecular biology itself was going\ngenomic (see the entry on\n genomics and postgenomics).\n The genome is a collection of nucleic acid base pairs within an\norganism’s cells (adenine (A) pairs with thymine (T) and\ncytosine (C) with guanine (G)). The number of base pairs varies widely\namong species. For example, the infection-causing Haemophilus\ninfluenzae (the first bacterial genome to be sequenced) has\nroughly 1.9 million base pairs in its genome (Fleischmann et al.\n1995), while the infection-catching Homo sapiens carries more\nthan 3 billion base pairs in its genome (International Human Genome\nSequencing Consortium 2001, Venter et al. 2001). The history of\ngenomics is the history of the development and use of new experimental\nand computational methods for producing, storing, and interpreting\nsuch sequence data (Ankeny 2003; Stevens 2013). \nFrederick Sanger played a seminal role in initiating such\ndevelopments, creating influential DNA sequencing techniques in the\n1950s and 1960s (Saiki et al. 1985; for historical treatments see\nSanger 1988; Judson 1992; Culp 1995; Rabinow 1996; Morange 1998; de\nChadarevian 2002; Little 2003; Garcia-Sancho 2012; Sanger Method of\nDNA Sequencing in\n Other Internet Resources).\n Equally important was Edwin Southern’s development of a method\nto detect specific sequences of DNA in DNA samples (Southern 1975).\nThe Southern Blot, as it came to be known, starts by digesting a\nstrand of DNA into many small DNA fragments; those fragments are then\nseparated (in a process called gel electrophoresis) based on size,\nplaced on filter paper which “blots” the DNA fragments on\nto a new medium, and then chemically labeled with DNA probes; the\nprobes then allow for identification and visualization of the DNA\nfragments (see also The Southern Blot in\n Other Internet Resources).\n Playing off the “southern” homonym, subsequent blotting\ntechniques that detect RNA and proteins came to be called Northern\nblotting and Western blotting.  \nIn the mid 1980s, after the development of sequencing techniques, the\nUnited States Department of Energy (DoE) originated a project to\nsequence the human genome (initially as part of a larger plan to\ndetermine the impact of radiation on the human genome induced by the\nHiroshima and Nagasaki bombings). The resulting Human Genome Project\n(HGP) managed jointly by the DoE and the United States National\nInstitutes of Health (NIH), utilized both existent sequencing\nmethodologies and introduced new ones (Kevles and Hood 1992, see also\nthe entry on\n the human genome project).\n While the human genome project received most of the public attention,\nhundreds of genomes have been sequenced to date, including the cat\n(Pontius et al. 2007), the mouse (Waterson et al. 2002), rice (Goff et\nal. 2002) and a flock of bird genomes (Zhang et al. 2014). One of the\nmost shocking results of those sequencing projects was the total\nnumber of genes (defined in this context as stretches of DNA that code\nfor a protein product) found in the genomes. The human genome contains\n20,000 to 25,000 genes, the cat contains 20,285 genes, the mouse\n24,174, and rice 32,000 to 50,000. So in contrast to early assumptions\nstemming from the classical period of molecular biology about how\ngenes produced proteins which in turn produced organisms, it turned\nout that neither organismal complexity nor even position on the food\nchain was predictive of gene-number (see the entry on\n genomics and postgenomics). \nThe increased attention to sequencing genomes encouraged a number of\ndisciplines to “go genomic”, including behavioral genetics\n(Plomin et al. 2003), developmental biology (Srinivasan and Sommer\n2002), cell biology (Taniguchi et al. 2002), and evolutionary biology\n(Ohta and Kuroiwa 2002). What’s more, genomics has been\ninstitutionalized with textbooks (Cantor and Smith 1999) and journals,\nsuch as Genomics and Genome Research. And the human\ngenome project itself has turned its attention from a standardized\nhuman genome to variation between genomes in the form of the Human\nGenome Diversity Initiative (Gannett 2003) and the HapMap Project\n(International HapMap Consortium 2003). \nBut just as a number of disciplines “went molecular” while\nmolecular biology itself was wrestling with the complexities posed by\nsplit genes and overlapping genes, so too are fields going genomic\nwhile genomics itself is wrestling with the complexities posed by how\na mere 20,000 genes can construct a human while a grain of rice\nrequires 50,000 genes (Baedke 2018; Brigandt, Green, and\nO’Malley 2017; Green 2017). A related challenge was making sense\nof the genetic similarity claims. For example, how to interpret the\nfinding that human and pumpkin genomes are 75% similar? Does this\nfinding tell us anything substantive about our overall similarity to\npumpkins (Piotrowska 2009)? To help answer such questions, genomics is\nnow supplemented by post-genomics. There is ongoing debate about what\nactually constitutes post-genomics (Morange 2006), but the general\ntrend is a focus beyond the mere sequence of As, Cs, Ts, and Gs and\ninstead on the complex, cellular mechanisms involved in generating\nsuch a variety of protein products from a relatively small number of\nprotein-coding regions in the genome. Post-genomics utilizes the\nsequence information provided by genomics but then situates it in an\nanalysis of all the other entities and activities involved in the\nmechanisms of transcription (transcriptomics), regulation\n(regulomics), metabolism (metabolomics), and expression (proteomics).\n(See ENCODE Project Consortium 2012; Germain et al. 2014; (see also\nthe entry on\n philosophy of systems and synthetic biology). \nDevelopments in genomics and post-genomics have sparked a number of\nphilosophical questions about molecular biology. Since the genome\nrequires a vast array of other mechanisms to facilitate the generation\nof a protein product, can DNA really be causally prioritized (see\n Section 2.3)?\n Similarly, in the face of such interdependent mechanisms involved in\ntranscription, regulation, and expression, can DNA alone be privileged\nas the bearer of hereditary information, or is information distributed\nacross all such entities and activities (see\n Section 2.2)?\n And is it appropriate to extrapolate from information about other\nspecies’ genomes to how the human genome operates (see\n Section 3.3)? \nThe concepts of mechanism, information, and\ngene all figured quite prominently in the history of\nmolecular biology. Philosophers, in turn, have focused a great deal of\nattention on these concepts in order to understand how they have been,\nare, and should be used. \nMolecular biologists discover and explain by identifying and\nelucidating mechanisms, such as DNA replication, protein synthesis,\nand the myriad mechanisms of gene expression. The phrase “theory\nof molecular biology” was not used above and for good reason;\ngeneral knowledge in the field is represented by diagrams of\nmechanisms (Machamer, Darden, and Craver 2000; Darden 2006a, 2006b;\nCraver and Darden 2013; Baetu 2017). Discovering the mechanism that\nproduces a phenomenon is an important accomplishment for several\nreasons. First, knowledge of a mechanism shows how something works:\nelucidated mechanisms provide understanding. Second, knowing how a\nmechanism works allows predictions to be made based upon the\nregularity in mechanisms. For example, knowing how the mechanism of\nDNA base pairing works in one species allows one to make predictions\nabout how it works in other species, even if conditions or inputs are\nchanged. Third, knowledge of mechanisms potentially allows one to\nintervene to change what the mechanism produces, to manipulate its\nparts to construct experimental tools, or to repair a broken, diseased\nmechanism. In short, knowledge of elucidated mechanisms provides\nunderstanding, prediction, and control. Given the general importance\nof mechanisms and the fact that mechanisms play such a central role in\nthe field of molecular biology, it is not surprising that philosophers\nof biology pioneered analyzing the concept of mechanism (see the entry\non mechanisms in science). \nStarting in the 1990s, a number of philosophers focused squarely on\nhow the concept of a mechanism functions in science generally and\nmolecular biology specifically (Glennan and Illari 2017; see also the\nentry on\n mechanisms in science).\n A number of characterizations of what a mechanism is have emerged\nover the years (Bechtel and Abrahamsen 2005; Glennan 2002; Machamer,\nDarden, and Craver 2000). Phyllis McKay Illari and Jon Williamson have\nmore recently offered a characterization that draws on the essential\nfeatures of all the earlier contributions: \nA mechanism for a phenomenon consists of entities and activities\norganized in such a way that they are responsible for the phenomenon.\n(Illari and Williamson 2012: 120) \nAs an example, consider the phenomenon of DNA replication. As Watson\nand Crick (1953a) famously noted upon discovery of the structure of\nDNA, the macromolecule’s structure pointed to the mechanism of\nDNA replication: \nIt has not escaped our notice that the specific pairing we have\npostulated immediately suggests a possible copying mechanism for the\ngenetic material. \nIn short, the double helix of DNA (an entity with an organization)\nunwinds (an activity) and new component parts (entities) bond (an\nactivity) to both parts of the unwound DNA helix. DNA is a nucleic\nacid composed of several subparts: a sugar-phosphate backbone and\nnucleic acid bases. When DNA unwinds, the bases exhibit weak charges,\nproperties that result from slight asymmetries in the molecules. These\nweak charges allow a DNA base and its complement to engage in the\nactivity of forming hydrogen (weak polar) chemical bonds; the\nspecificity of this activity is due to the topological arrangements of\nthe weak polar charges in the subparts of the base. Ultimately,\nentities with polar charges enable the activity of hydrogen bond\nformation. After the complementary bases align, then the backbone\nforms via stronger covalent bonding. The mechanism proceeds with\nunwinding and bonding together (activities) new parts, to produce two\nhelices (newly formed entities) that are (more or less faithfully)\ncopies of the parent helix. (This process of “semi-conservative\nreplication” and the Meselson-Stahl experiment that confirmed it\nare discussed in more detail in\n Section 3.4.) \nScientists rarely depict all the particular details when describing a\nmechanism; representations are usually schematic, often depicted in\ndiagrams (see the entry on\n models in science).\n Such representations may be called a “model of a\nmechanism”, or “mechanism schema”. A mechanism\nschema is a truncated abstract description of a mechanism that can be\ninstantiated by filling it with more specific descriptions of\ncomponent entities and activities. An example is James Watson’s\n(1965) diagram of his version of the central dogma of molecular\nbiology: \nDNA → RNA → protein. \nThis is a schematic representation (with a high degree of abstraction)\nof the mechanism of protein synthesis, which can be instantiated with\ndetails of DNA base sequence, complementary RNA sequence, and the\ncorresponding order of amino acids in the protein produced by the more\nspecific mechanism. Molecular biology textbooks are replete with\ndiagrams of mechanism schemas. A mechanism schema can be instantiated\nto yield a description of a particular mechanism. In contrast, a\nmechanism sketch cannot (yet) be instantiated; components are (as yet)\nunknown. Sketches have black boxes for missing components or grey\nboxes whose function is known but whose entities and activities that\ncarry out that function are not yet elucidated. Such sketches guide\nnew research to fill in the details (Craver and Darden 2013). \nThe language of information appears often in molecular biology. Genes\nas linear DNA sequences of bases are said to carry\n“information” for the production of proteins. During\nprotein synthesis, the information is “transcribed” from\nDNA to messenger RNA and then “translated” from RNA to\nprotein. With respect to inheritance, it is often said that what is\npassed from one generation to the next is the\n“information” in the genes, namely the linear ordering of\nbases along complementary DNA strands. Historians of biology have\ntracked the entrenchment of information-talk in molecular biology (Kay\n2000) since its introduction. \nThe question for philosophers of biology is whether an analysis of the\nconcept of information can capture the various ways in which the\nconcept is used in molecular biology (e.g., Maynard Smith 2000). The\nusage of “information” in the mathematical theory of\ncommunication is too impoverished to capture the molecular biological\nusage, since the coded sequences in the DNA are more than just a\nsignal with some number of bits that may or may not be accurately\ntransmitted (Sarkar 1996b,c; Sterelny and Griffiths 1999; Shannon and\nWeaver 1949). Conversely, the usage in cognitive neuroscience, with\nits talk of “representations” (e.g., Crick 1988) may be\nsaid to be too rich, since the coded sequences in the DNA are also not\nsaid to have within them a representation of the structure of the\nprotein (Darden 2006b). No definition of “information” as\nit is used in molecular biology has yet received wide support among\nphilosophers of biology. \nStephen Downes (2006) helpfully distinguishes three positions on the\nrelation between information and the natural world: \nThese options may be read either ontologically or heuristically. A\nheuristic reading of (1), for instance, views the talk of information\nin molecular biology as useful in providing a way of talking and in\nguiding research. And so the heuristic benefit of the information\nconcept can be defended without making any commitment to the\nontological status (Sarkar 2000). Indeed, one might argue that a vague\nand open-ended use of information is valuable for heuristic purposes,\nespecially during early discovery phases in the development of a\nfield. \nPhilosophers’ discussions of the concept of information in\nbiology have also focused on its ontological reading. Three different\nphilosophical accounts of information serve as exemplars of\nDownes’ three categories. Ulrich Stegmann (2005) provides an\nexample of Downes’ first category with his analysis of\ntemplate-directed synthesis. (Stegmann does explicitly allow that\ncomponents other than nucleotide sequences might contain what he calls\ninstructional information. However, his only example is a thought\nexperiment involving enzymes linearly ordered along a membrane;\nnothing of the sort is known to actually exist or even seems very\nlikely to exist.) Stegmann calls this the sequentialization view.\nStegmann’s instructional account of genetic information requires\nthat the component carrying the information satisfy the following\nconditions: an advance specification of the kind and order of steps\nthat yield a determinate outcome if the steps are carried out. On his\naccount, DNA qualifies as an instructional information carrier for\nreplication, transcription and translation. The sequence of bases\nprovides the order. The hydrogen bonding between specific bases and\nthe genetic code provide the specific kinds of steps. And the\nmechanisms of replication, transcription, and translation yield\ncertain outcomes: a copy of the DNA double helix, an mRNA, and a\nlinear order of amino acids. Also, because DNA carries information for\na specific outcome, an error can occur as the mechanism operates to\nproduce that outcome; hence Stegmann’s account allows for errors\nand error-correcting mechanisms (such as proof reading mechanisms that\ncorrect DNA mutations). For more on this topic, see the entry on\n biological information. \nEva Jablonka (2002) is an example of Downes’ second category.\nShe argues that information is ubiquitous. She defines information as\nfollows: a source becomes an informational input when an interpreting\nreceiver can react to the form of the source (and variations in this\nform) in a functional manner. She claims a broad applicability of this\ndefinition. The definition, she says, accommodates information\nstemming from environmental cues as well as from evolved signals, and\ncalls for a comparison between information-transmission in different\ntypes of inheritance systems — the genetic, the epigenetic, the\nbehavioral, and the cultural-symbolic. On this view, genes have no\ntheoretically privileged informational status (Jablonka 2002:\n583). \nIn line with Downes’ third category, C. Kenneth Waters argues\nthat information is a useful term in rhetorical contexts, such as\nseeking funding for DNA sequencing by claiming that DNA carries\ninformation. However, from an ontological perspective, Waters claims\nthat explication of DNA’s causal role has no need for the\nconcept of information. Genes, he argues, should not be viewed as\n“immaterial units of information” (Waters 2000: 541). As\ndiscussed in\n Section 2.3\n below, Waters’ focus is on stretches of DNA whose causal roles\nare as actual specific difference makers in genetic mechanisms (Waters\n2007). Talk of information is not needed; causal role function talk is\nsufficient. (For more on Waters’ view see his entry on\n molecular genetics;\n for others who make similar points, see Sustar 2007; Weber 2005,\n2006.) \nThe question of whether classical, Mendelian genetics could be (or\nalready has been) reduced to molecular biology (to be taken up in\n Section 3.1\n below) motivated philosophers to consider the connectability of the\nterm they shared: the gene. Investigations of reduction and scientific\nchange raised the question of how the concept of the gene evolved over\ntime, figuring prominently in C. Kenneth Waters’ (1990, 1994,\n2007, see entry on\n molecular genetics),\n Philip Kitcher’s (1982, 1984) and Raphael Falk’s (1986)\nwork. Over time, however, philosophical discussions of the gene\nconcept took on a life of their own, as philosophers raised questions\nindependent of the reduction debate: What is a gene? And, is there\nanything causally distinct about DNA? (see the entry on the\n gene) \nFalk (1986) explicitly asked philosophers and historians of biology,\n“What is a Gene?” Discoveries such as overlapping genes,\nsplit genes, and alternative splicing (discussed in\n Section 1.2)\n made it clear that simply equating a gene with an uninterrupted\nstretch of DNA would no longer capture the complicated\nmolecular-developmental details of mechanisms such as gene expression\n(Downes 2004; Luc-Germain, Ratti and Boem 2015). In an effort to\nanswer Falk’s question, two general trends have emerged in the\nphilosophical literature: first, distinguish multiple gene concepts to\ncapture the complex structural and functional features separately, or\nsecond, rethink a unified gene concept to incorporate such complexity.\n(For a survey of gene concepts defended by philosophers, see Griffiths\nand Stotz 2007, 2013.; Rheinberger and Muller-Wille 2018) \nA paradigmatic example of the first line came from Lenny Moss’s\ndistinction between Gene-P and Gene-D (Moss 2001, 2002). Gene-P\nembraced an instrumental preformationism (providing the\n“P”); it was defined by its relationship to a phenotype.\nIn contrast, Gene-D referred to a developmental resource (providing\nthe “D”); it was defined by its molecular sequence. An\nexample will help to distinguish the two: When one talked about the\ngene for cystic fibrosis, the most common genetic disease affecting\npopulations of Western European descent, the Gene-P concept was being\nutilized; the concept referred to the ability to track the\ntransmission of this gene from generation to generation as an\ninstrumental predictor of cystic fibrosis, without being contingent on\nknowing the causal pathway between the particular sequence of DNA and\nthe ultimate phenotypic disease. The Gene-D concept, in contrast,\nreferred instead to just one developmental resource (i.e., the\nmolecular sequence) involved in the complex development of the\ndisease, which interacted with a host of other such resources\n(proteins, RNA, a variety of enzymes, etc.); Gene-D was indeterminate\nwith regards to the ultimate phenotypic disease. Moreover, in cases of\nother diseases where there are different disease alleles at the same\nlocus, a Gene-D perspective would treat these alleles as individual\ngenes, while a Gene-P perspective treats them collectively as\n“the gene for” the disease. (For other examples of\ngene-concept dividers, see Keller’s distinction between the gene\nas a structural entity and the gene as a functional entity as well as\nBaetu’s distinction between the gene as a syntax-based concept\nand the gene as a mapping concept (Baetu 2011; Keller 2000).) \nA second philosophical approach for conceptualizing the gene involved\nrethinking a single, unified gene concept that captured the\nmolecular-developmental complexities. For example, Eva Neumann-Held\n(Neumann-Held 1999, 2001; Griffiths and Neumann-Held 1999) claimed\nthat a “process molecular gene concept” (PMG) embraced the\ncomplicated developmental intricacies. On her unified view, the term\n“gene” referred to “the recurring process that leads\nto the temporally and spatially regulated expression of a particular\npolypeptide product” (Neumann-Held 1999). Returning to the case\nof cystic fibrosis, a PMG for an individual without the disease\nreferred to one of a variety of transmembrane ion-channel templates\nalong with all the epigenetic factors, i.e., nongenetic influences on\ngene expression, involved in the generation of the normal polypeptide\nproduct. And so cystic fibrosis arose when a particular stretch of the\nDNA sequence was missing from this process. (For another example of a\ngene-concept unifier, see Falk’s discussion of the gene as a DNA\nsequence that corresponded to a single norm of reaction for various\nmolecular products based on varying epigenetic conditions (Falk\n2001).) \nRelatedly, philosophers have also debated the causal distinctiveness\nof DNA. Consider again the case of cystic fibrosis. A stretch of DNA\non chromosome 7 is involved in the process of gene expression, which\ngenerates (or fails to generate) the functional product that\ntransports chloride ions. But obviously that final product results\nfrom that stretch of DNA as well as all the other developmental\nresources involved in gene expression, be it in the expression of the\nfunctional protein or the dysfunctional one. Thus, a number of authors\nhave argued for a causal parity thesis, wherein all developmental\nresources involved in the generation of a phenotype such as cystic\nfibrosis are treated as being on par (Griffiths and Knight 1998;\nRobert 2004; Stotz 2006). \nWaters (2007, see also his entry on\n molecular genetics),\n in reply, has argued that there is something causally distinctive\nabout DNA. Causes are often conceived of as being difference makers,\nin that a variable (i.e., an entity or activity in a mechanism) can be\ndeemed causal when a change in the value of that variable would\ncounterfactually have led to a different outcome (see the entry on\n scientific explanation).\n According to Waters, there are a number of potential\ndifference makers in the mechanisms involved in developing or not\ndeveloping cystic fibrosis; that is, an individual with two normal\ncopies of the gene could still display signs of cystic fibrosis if a\nmanipulation was done to the individual’s RNA polymerase (the\nprotein responsible for transcribing DNA to RNA), thereby undermining\nthe functional reading of the stretch of DNA. So RNA polymerase is a\ndifference maker in the development or lack of development of cystic\nfibrosis, but only a potential difference maker, since variation in\nRNA polymerase does not play a role in the development or lack of\ndevelopment of cystic fibrosis in natural populations. The stretch of\nDNA on chromosome 7, however, is an actual difference maker.\nThat is, there are actual differences in natural human populations on\nthis stretch of DNA, which lead to actual differences in developing or\nnot developing cystic fibrosis; DNA is causally distinctive, according\nto Waters, because it is an actual difference maker. Advocates of the\nparity thesis are thus challenged to identify the other resources (in\naddition to DNA) that are actual difference makers. \nRecently, Paul Griffiths and Karola Stotz (2013) have responded to\nthis challenge by offering examples in which, depending on context,\nregulatory mechanisms can either contribute additional information to\nthe gene products or create gene products for which there is no\nunderlying sequence. Thus, according to Griffiths and Stotz, to assign\na causally distinctive role to DNA, as Waters does, is to ignore key\naspects of how the gene makes its product. \nIn addition to analyzing key concepts in the field, philosophers have\nemployed case studies from molecular biology to address more general\nissues in the philosophy of science, such as reduction, explanation,\nextrapolation, and experimentation. For each of these philosophical\nissues, evidence from molecular biology directs philosophical\nattention toward understanding the concept of a mechanism for\naddressing the topic. \nReduction may be understood in multiple ways depending on what it is\nthat is being reduced (see the entry on\n scientific reduction).\n Theory reduction pertains to whether or not theories from one\nscientific field can be reduced to theories from another scientific\nfield. In contrast, explanatory reduction (often united with\nmethodological reduction) pertains to whether or not explanations that\ncome from lower levels (often united with methodologies that\ninvestigate those lower levels) are better than explanations that come\nfrom higher levels. Philosophical attention to molecular biology has\ncontributed to debates about both of these senses of reduction (see\nthe entry on\n reductionism in biology). \nPhilosophy of biology first came to prominence as a sub-specialty of\nphilosophy of science in the 1970s when it offered an apparent case\nstudy by which to judge how theories from one field may reduce to\ntheories from another field. The specific question was: might classic,\nMendelian genetics reduce to molecular genetics (see the entry on\n \n molecular genetics)?\n Kenneth Schaffner used and developed Ernst Nagel’s (1961)\nanalysis of derivational theory reduction to argue for the reduction\nof classical Mendelian genetics (T2) to molecular\nbiology (T1) and refined it over many years\n(summarized in Schaffner 1993). The goal of formal reduction was to\nlogically deduce the laws of classical genetics (or its improved\nsuccessor, “modern transmission genetics”\nT2*) from the laws of molecular biology. Such a\nderivation required that all the terms of T2* not\nin T1 had to be connected to terms in\nT1 via correspondence rules. Hence, Schaffner\nendeavored to find molecular equivalents of such terms as\n“gene”, as well as predicate terms, such as “is\ndominant”. David Hull (1974) criticized formal reduction, argued\nagainst Schaffner’s claims, and suggested, instead, that perhaps\nmolecular biology replaced classical genetics. \nEven though Schaffner and Hull were engaged in a debate over theory\nreduction, they simultaneously admitted that the question of formal\ntheory reduction was rather peripheral to what scientists actually did\nand studied (Schaffner 1974b; Hull 1974). And indeed, while the theory\nreduction debate was playing out, a number of philosophers of biology\nswitched attention from scientific theories to the\nstuff in nature that scientists investigated. William Wimsatt\n(1976) argued for a shift in the reduction debate from talk of\nrelations between theories to talk of decompositional explanation via\nmechanisms. And Lindley Darden and Nancy Maull (1977) focused\nattention on the bridges between fields formed by part-whole\nrelations, structure-function relations, and cause-effect\nrelations. \nThis shift in attention was a precursor to understanding the\nphilosophy of science through the lens of mechanisms. Darden, building\non the work of Machamer, Darden, and Craver (2000), has more recently\nreturned to the question of how Mendelian and molecular genetics are\nrelated and viewed it through this lens (Darden 2005). Rather than\nunderstanding the relationship as one of reduction, she suggests they\ncan be understood as relating via a focus on different working\nentities (often at different size levels) that operate at different\ntimes. Thus, the relation was one of integration of sequentially\noperating chromosomal and molecular hereditary mechanisms rather than\nreduction. (For an alternative but still integrative reading of the\nrelationship between classical genetics and molecular biology that\nfocuses on their shared functional units, see Baetu 2010.) \nReduction can also be about explanation and methodology. That is,\nreduction can be about using reductive methodologies to dig down to\nlower levels because the thought is that this exercise leads to more\nreductive explanations and more reductive explanations are better than\nexplanations at higher levels. Alex Rosenberg (1997, 2006)\ncontroversially divided biology into molecular biology and everything\nelse, which he dubbed “functional biology”.\n“Reductionism”, Rosenberg argued, \nis the thesis that biological theories and explanations that employ\nthem do need to be grounded in molecular biology and ultimately\nphysical science, for it is only by doing so that they can be\nimproved, corrected, strengthened, and made more accurate and more\nadequate and completed. (Rosenberg 2006: 4) \nHence, the task of this explanatory reduction is to explain all\nfunctional biological phenomena via molecular biology. \nCritics and defenders of Rosenberg’s view have discussed\norganizational and contextual features not captured by molecular\nbiological principles. These included orientation of the embryo in the\nearth’s gravitational field and other spatial, regulatory, and\ndynamical properties of developing systems (e.g., see Delehanty 2005;\nFrost-Arnold 2004; Keller 1999; Laubichler and Wagner 2001; Love et\nal. 2008; Robert 2001, 2004). This particular debate can be understood\nas an instance of a more general debate occurring in biology and\nphilosophy of biology about whether investigations of lower-level\nmolecular biology are better than investigations of high-level systems\nbiology (Baetu 2012a; Bechtel and Abrahamsen 2010; De Backer, De\nWaele, and Van Speybroeck 2010; Huettemann and Love 2011; Marco 2012;\nMorange 2008; Pigliucci 2013; Powell and Dupre 2009; see also the\nentries on\n feminist philosophy of biology,\n philosophy of systems and synthetic biology, and\n multiple realizability). \nTraditionally, philosophers of science took successful scientific\nexplanations to result from derivation from laws of nature (see the\nentries on\n laws of nature\n and\n scientific explanation).\n On this deductive-nomological account (Hempel and Oppenheim 1948), an\nexplanation of particular observation statements was analyzed as\nsubsumption under universal (applying throughout the universe),\ngeneral (exceptionless), necessary (not contingent) laws of nature\nplus the initial conditions of the particular case. Philosophers of\nbiology have criticized this traditional analysis as inapplicable to\nbiology, and especially molecular biology. \nSince the 1960s, philosophers of biology have questioned the existence\nof biological laws of nature. J. J. C. Smart (1963) emphasized the\nearth-boundedness of the biological sciences (in conflict with the\nuniversality of natural laws). No purported “law” in\nbiology has been found to be exceptionless, even for life on earth (in\nconflict with the generality of laws). And John Beatty (1995) argued\nthat the purported “laws” of, for example, Mendelian\ngenetics, were contingent on evolution (in conflict with the necessity\nof natural laws). (For further discussion, see Brandon 1997; Lange\n2000; Mitchell 1997; Sober 1997; Waters 1998; Weber 2005.) Hence,\nphilosophers’ search for biological laws of nature,\ncharacterized as universal, necessary generalizations, has ceased. \nWithout traditional laws of nature from which to derive explanations,\nphilosophers of biology have been forced to rethink the nature of\nscientific explanation in biology and, in particular, molecular\nbiology. Two accounts of explanation emerged: the unificationist and\nthe causal-mechanical. Philip Kitcher (1989, 1993) developed a\nunificationist account of explanation, and he and Sylvia Culp\nexplicitly applied it to molecular biology (Culp and Kitcher 1989).\nAmong the premises of the “Watson-Crick” argument schema\nwere “transcription, post-transcriptional modification and\ntranslation for the alleles in question”, along with details of\ncell biology and embryology for the organisms in question (Kitcher\n1989). An explanation of a particular pattern of distribution of\nprogeny phenotypes in a genetic cross resulted from instantiating the\nappropriate deductive argument schema: the variables were filled with\nthe details from the particular case and the conclusion derived from\nthe premises. \nWorking in the causal-mechanical tradition pioneered by Wesley Salmon\n(1984, 1998), other philosophers turned to understanding mechanism\nelucidation as the avenue to scientific explanation in biology\n(Bechtel and Abrahamsen 2005; Bechtel and Richardson 1993; Craver\n2007; Darden 2006a; Glennan 2002; Machamer, Darden, and Craver 2000;\nSarkar 1998; Schaffner 1993; Woodward 2002, 2010). There are\ndifferences between the various accounts of a mechanism, but they hold\nin common the basic idea that a scientist provides a successful\nexplanation of a phenomenon by identifying and manipulating variables\nin the mechanisms thereby determining how those variables are situated\nin and make a difference in the mechanism; the ultimate explanation\namounts to the elucidation of how those mechanism components act and\ninteract to produce the phenomenon under investigation. As mentioned\nabove (see\n Section 2.1,\n see also entry on mechanisms in science), an elucidated mechanism\nallows for the explanatory features of understanding, prediction, and\ncontrol. \nThere are several virtues of the causal-mechanical approach to\nunderstanding scientific explanation in molecular biology. For one, it\nis truest to molecular biologists’ own language when engaging in\nbiological explanation. Molecular biologists rarely describe their\npractice and achievements as the development of new theories; rather,\nthey describe their practice and achievements as the elucidation of\nmolecular mechanisms (Baetu 2017; Craver 2001; Machamer, Darden,\nCraver 2000). Another virtue of the causal-mechanical approach is that\nit captures biological explanations of both regularity and variation.\nUnlike in physics, where a scientist assumes that an electron is an\nelectron is an electron, a biologist is often interested in precisely\nwhat makes one individual different from another, one population\ndifferent from another, or one species different from another.\nPhilosophers have extended the causal-mechanical account of\nexplanation to cover biological explanations of variation, be it\nacross evolutionary time (Calcott 2009) or across individuals in a\npopulation (Tabery 2009, 2014). Tabery (2009, 2014) characterized\nbiological explanations of variation across individuals in a\npopulation as the elucidation of “difference mechanisms”.\nDifference mechanisms are regular causal mechanisms made up of\ndifference-making variables, one or more of which are actual\ndifference makers (see\n Section 2.3\n for the discussion of Waters’ (2007) concept of an actual\ndifference maker). There is regularity in difference mechanisms;\ninterventions made on variables in the mechanisms that change the\nvalues of the variables lead to different outcomes in the phenomena\nunder investigation. There is also variation in difference mechanisms;\ninterventions need not be taken to find differences in outcomes\nbecause, with difference mechanisms, some variables are actual\ndifference makers which already take different values in the natural\nworld, resulting in natural variation in the outcomes. \nBut philosophers have also raised challenges to the causal-mechanical\napproach. One complaint has been that mechanistic explanations\ndon’t work well in systems biology, a field distinct from\nmolecular biology but that nonetheless draws on the work of molecular\nbiologists. While some argue that systems biology is best explained\nusing mechanisms (cf. Boogerd et al. 2007; Matthiessen 2017;\nRichardson and Stephan 2007; Bechtel 2011; Bechtel and Abrahamsen\n2013), others argue that it requires non-mechanistic explanation (cf.\nBraillard 2010; Kuhlmann 2011; Silberstein and Chemero 2013). \nAnother complaint has been that mechanistic explanations fail to\nprovide a satisfactory account of the complexity of living systems\nbecause they fail to devote serious attention to the “thoroughly\nprocessual character of living systems” (Dupre 2012: 19). Those\nwho prefer a “process-oriented” ontology (Bapteste and\nDupre 2013; Bickhard 2011; Campbell 2015; Dupre 2012; Jaeger and Monk\n2015; Jaeger et al. 2012; Meincke 2018; Nicholson and Dupre 2018)\nargue that mechanistic accounts mistakenly assume that parts composing\na mechanism can be identified independently of the activities or\nprocesses in which they are involved. Instead, as Anne Sophie Maincke\nexplains, if there are parts or substances, they must be\n“reconceptualised as stabilized higher-order processes that\npersist as long as stabilization can be maintained” (2018).\nProcesses are ontologically primary. Recent literature in molecular\nbiology on molecular pathways (cf. Boniolo and Campaner 2018; Brigandt\n2018; Ioannides and Psillos 2017; Ross 2018) seems to be another\ninstantiation of this shift from mechanistic to processual\nexplanations. But Christopher Austin (2016) has recently challenged\nthe idea that processual explanations can be sufficiently grounded\n“without the metaphysical underpinning of the very\nmechanisms which processes purport to replace” (639). \nAs discussed earlier in the historical sections, molecular biologists\nhave relied heavily on model organisms (see the entry on\n models in science).\n The model organisms that were used to lay down the foundation of\nmolecular biology served as “exemplary models” in contrast\nto what today are called “surrogate models”—the\ndistinction comes from Jessica Bolker (2009). According to Bolker,\nexemplary models are “representatives of a broader group”\nand the goal of using them is “to elucidate fundamental or\ngeneral biological patterns and mechanisms” (487). But making\ninferences from a single exemplary model to general biological\npatterns has been cause for worry. What grounds do biologists have for\nbelieving that what is true of a mere model is true of many different\norganisms? One answer, provided by Marcel Weber (2005), is that the\ngenerality of biological knowledge obtained from studying exemplary\nmodels can be established on evolutionary grounds. According to Weber,\nif a mechanism is found in a set of phylogenetically distant\norganisms, this provides evidence that it is also likely to be found\nin all organisms that share a common ancestor with the organisms being\ncompared. \nExemplary models still play an important role in molecular biology,\nbut a new type of model organism, the “surrogate model”,\nhas become increasingly popular. According to Bolker, surrogate models\n“act as proxies for specific targets” and are often used\nin biomedical research “where the objective is to understand the\nmechanisms and etiology of human disease, and ultimately to develop\ntreatments” (Bolker 2009: 489). Unlike the aim of exemplary\nmodels, the representative aim of a surrogate model is not necessarily\nto be broad. Instead, it’s to faithfully replicate a specific\ntarget. For example, biomedical researchers frequently expose\nsurrogate models to harmful chemicals with the aim of modeling human\ndisease. However, if a chemical proves to be carcinogenic in rats, for\nexample, there is no guarantee that it will also cause cancer in\nhumans. \nThis difficulty of justifying the inference from rats to humans or,\nmore broadly, of “transferring causal generalizations from one\ncontext to another when homogeneity cannot be presumed” (Steel\n2008: 3) is known as the problem of extrapolation. Although\nthis problem is not unique to surrogate models, it often arises when\nbiomedical researchers use them to replicate human disease at the\nmolecular level. Consequently, philosophers who write about the\nproblem of extrapolation in the context of molecular biology often\nfocus on such models (see, for example, Ankeny 2001; Baetu 2016;\nBechtel and Abrahamsen 2005; Bolker 1995; Burian 1993b; Darden 2007;\nLaFollette and Shanks 1996; Love 2009; Piotrowska 2013; Schaffner\n1986; Steel 2008; Weber 2005; Wimsatt 1998). \nWithin the context of surrogate models, any successful solution to the\nproblem of extrapolation must explain how inferences can be justified\ngiven causally relevant differences between models and their targets\n(Lafollette and Shanks 1996). It must also avoid what Daniel Steel\n(2008) calls the “extrapolator’s circle”, which\narises when attempting to determine whether the model and its target\nare similar enough in casually relevant respects. \nOne way to escape the extrapolator’s circle is to black box the\nmechanisms being compared and instead treat the problem of\nextrapolation as a statistical problem (cf. Cook and Campbell 1979).\nThis method avoids the circle because it eliminates the need to know\nif two mechanisms are similar. All that matters is that two outcomes\nare produced to a statistically significant degree, given the same\nintervention. For this reason, statistically significant outcomes in\nclinical trials are at the top of the evidence hierarchy in biomedical\nresearch (Sackett et al. 1996). One problem with relying merely on\nstatistics to solve the problem of extrapolation, however, is that it\ncannot show that an observed correlation between model and target is\nthe result of intervention and not a confounder.\n\n \nA different strategy for avoiding the extrapolator’s circle is\nto remove the black box and compare the two mechanisms but argue that\nthey do not have to be causally similar at every stage for\nextrapolation to be justified. This approach avoids the circle because\nthe suitability of a model can be established given only partial\ninformation about the target. For example, Steel argues that only the\nstages downstream from the point where the mechanisms in the model and\ntarget are likely to differ need to be compared, since the point where\ndifferences are likely will serve as a bottleneck through which the\neventual outcome must be produced. \nThough promising, criticisms have been raised against Steel’s\nmechanistic approach to extrapolation. One worry, raised by Jeremy\nHowick et al. (2013), is that in order to identify the bottlenecks and\ndownstream differences, we must know more about the target than Steel\nadmits. If what we know about the mechanism in the target exceeds what\nwe know about the mechanism in the model, the extrapolator’s\ncircle will not have been avoided. Another worry with Steel’s\napproach to extrapolation is that it doesn’t avoid the masking\nproblem. According to Julian Reiss (2010), Federica Russo (2010), and\nBrendan Clarke et al. (2014), even if we establish that X\ncauses Y through some mechanism, this doesn’t seem to\neliminate the possibility of there being several paths that link\nX to Y. For example, there may be an upstream difference\nthat affects the outcome but does not pass through the downstream\nstages of the mechanism. (This problem is taken up again below in\n Section 3.4.)\n A third worry with Steel’s approach, which comes from Tudor\nBaetu (2016), is that mechanistic accounts of the experimental model\nof interest “combine data from tens or even hundreds of distinct\nexperimental setups” (956). The resulting big picture account of\nthe experimental model is an aggregate of findings that do not\ndescribe a mechanism that actually exists in any cell or organism.\nInstead, as a number of authors have also pointed out (Huber and Keuck\n2013; Lemoine 2017; Nelson 2013), the mechanism of interest is often\nstipulated first and then verified piecemeal in many different\nexperimental organisms. The result is what Mael Lemoine (2017) has\ncalled a “theoretical chimera”, a hypothesis supported by\nheterogeneous partial models. On the chimera view of extrapolation,\nit’s possible that all one-to-one analogies work, and yet the\naggregate theoretical chimera model fails.  \nWhile theoretical chimeras seem to further complicate Steel’s\nmechanistic account of extrapolation, actual chimeras also raise\nquestions that Steel does not address. Consider, for example,\nhumanized mice that have been engineered to carry a “partial or\ncomplete human physiological system” (Macchiarini et al. 2005:\n1307). These genetically engineered rodents are supposed to make\nextrapolation more reliable by simulating a variety of human diseases,\ne.g., asthma, diabetes, cancer, etc. As Monika Piotrowska (2013)\npoints out, however, this raises a new problem. The question is no\nlonger how an inference from model to target can be justified given\nexisting differences between the two, but rather, in what way should\nthese mice be modified in order to justify extrapolation to\nhumans? Piotrowska has proposed three conditions that should be met in\nthe process of modification to ensure that extrapolation is justified.\nThe first two requirements demand that we keep track of parts and\ntheir boundaries during transfer, which presupposes a mechanistic view\nof human disease, but the third requirement—that the constraints\nthat might prevent the trait from being expressed be\neliminated—highlights the limits of using a mechanistic approach\nwhen making inferences from humanized mice to humans. As Piotrowska\nexplains, \nwithout the right context, even the complete lack of differences\nbetween two mechanisms cannot justify the inference that what is true\nof one mechanism will be true of another (Piotrowska 2013: 453). \nIf Piotrowska is right, Steel’s mechanistic solution to the\nproblem of extrapolation seems to have reached its limit. As our\nability to manipulate biological models advances, philosophers will\nneed to revisit the problem of extrapolation and seek out new\nsolutions. \nThe history of molecular biology is in part the history of\nexperimental techniques designed to probe the macromolecular\nmechanisms found in living things. Philosophers in turn have looked to\nmolecular biology as a case study for understanding how\nexperimentation works in science—how it contributes to\nscientific discovery, distinguishes correlation from causal and\nconstitutive relevance, and decides between competing hypotheses\n(Barwich and Baschir 2017). In all three cases, the concept of a\nmechanism is central to understanding the function of experimentation\nin molecular biology (also see the entry on experimentation in\nbiology). \nTake discovery. Throughout much of the twentieth century, philosophers\nof science treated scientific discovery as if it were off-limits to\nphilosophical analysis; philosophers could evaluate the rational\nprocess of confirmation, the argument went, but the psychological\nprocess of discovery (the “aha!” moment) fell into the\nrealm of creativity (Popper 1965; see the entry on\n scientific discovery).\n Darden has countered with a focus on the strategies that scientists\nemploy to construct, evaluate, and revise mechanical explanations of\nphenomena; on her view, discovery is a piecemeal, incremental, and\niterative process of mechanism elucidation. In the 1950s and 1960s,\nfor example, scientists from both molecular biology and biochemistry\nemployed their own experimental strategies to elucidate the mechanisms\nof protein synthesis that linked DNA to the production of proteins.\nMolecular biologists moved forward from DNA using experimental\ntechniques such as x-ray crystallography and model building to\nunderstand how the structure of DNA dictated what molecules it could\ninteract with; biochemists simultaneously moved backward from the\nprotein products using in vitro experimental systems to\nunderstand the chemical reactions and chemical bonding necessary to\nbuild a protein. They met in the middle at RNA, ultimately leading to\nWatson’s famous mechanism schema DNA → RNA → protein.\nFar from being philosophically inscrutable, Darden points out that the\nmolecular biologists were “forward chaining” while the\nbiochemists were “backward chaining”, using information\nabout the working entities and activities that they knew about to\ninfer what could come next or before in the mechanism of protein\nsynthesis (Darden 2006a: chapters 3, 12; Craver and Darden 2013:\nchapter 10). \nTudor Baetu builds on the contemporary philosophy of mechanism\nliterature as well to provide an account of how different experiments\nin molecular biology move from finding correlations, to establishing\ncausal relevance, to establishing constitutive relevance (Baetu\n2012b). Much recent philosophical attention has been given to the\ntransition from correlation to causal relevance. On a manipulationist\naccount of causal relevance, some factor X is determined to be\ncausally relevant to some outcome Y when interventions on\nX can be shown to produce the change in Y. Experiments\nfrom molecular biology often take this form, which Baetu calls\n“one-variable experiments” because they involve a single\nintervention made on X in the experiment to establish the\ncausal relevance to Y. But these one-variable experiments,\nBaetu cautions, do not necessarily provide information about the\ncausal mechanism that links X to Y. Is X causally\nrelevant to Y by way of mechanism A, mechanism B,\nor some other unknown mechanism? Baetu, drawing on his own\nexperimental research in molecular oncology, shows that scientists\nprobe the mechanical link by performing “two-variable\nexperiments”. In a two-variable experiment, two interventions\nare simultaneously made on the initial factor and some component\npostulated in the mechanical link, thereby establishing both causal\nand constitutive relevance.  \nExperiments from molecular biology have also figured into\nphilosophical discussions about the possibility of “crucial\nexperiments”. An experiment is taken to be a crucial experiment\nif it is devised so as to result in the confirmation of one hypothesis\nby way of refuting other competing hypotheses. But the very idea of a\ncrucial experiment, Pierre Duhem pointed out, assumes that the set of\nknown competing hypotheses contains all possible explanations of a\ngiven phenomenon such that the refutation of all but one of the\nhypotheses deductively ensures the confirmation of the hypothesis left\nstanding. However, if there are in fact unknown alternatives that\nweren’t considered in the set of competing hypotheses, Duhem\nwarned, then the refutation cannot guarantee the confirmation of the\nlast hypothesis standing (also see the entry on Pierre Duhem). (Duhem\nactually raised two problems for crucial experiments—the problem\nmentioned above, as well as the problem of auxiliary assumptions,\nwhich any hypothesis brings with it; for reasons of space, we will\nonly discuss the former here.) Marcel Weber (2009) has utilized a\nfamous experiment from molecular biology to offer a different vision\nof how crucial experiments work. After Watson and Crick discovered the\ndouble helical structure of DNA, molecular biologists turned their\nattention to how that macromolecule could be replicated (see\n Section 1.2\n above). The focus was in part on the fact that the DNA was twisted\ntogether in a helix, and so the challenge was figuring out what\nprocess could unwind and replicate that complexly wound molecule.\nThree competing hypotheses emerged, each with their own prediction\nabout the extent to which newly replicated DNA double helices\ncontained old DNA strands versus newly synthesized material:\nsemi-conservative replication, conservative replication, and\ndispersive replication. Matthew Meselson and Frank Stahl, at Cal Tech,\ndevised a method for testing among these competing hypotheses (see The\nSemi-Conservative Replication of DNA in\n Other Internet Resources).\n They grew E.coli on a medium that contained a unique,\nheavier form of nitrogen which could be taken up into the\nE.coli DNA; then they transferred that E.coli with\nheavy DNA to a medium with the more common, lighter form of nitrogen\nand let the E.coli replicate there. By then taking regular\nsamples of the replicating E.coli and examining the DNA of\nthe subsequent generations, they determined the extent to which the\nnew DNA was a mix of old DNA strands or newly synthesized material.\nThe result was a clear victory for semi-conservative replication, and\nthe Meselson-Stahl experiment became known as the “most\nbeautiful experiment in biology” (Meselson and Stahl 1958;\nHolmes 2001). Weber argues that we should understand the quick uptake\nof Meselson and Stahl’s experimental result as an instance of\ninference to the best explanation (as opposed to Duhem’s\ndeductive characterization). Meselson and Stahl, Weber claims, took\nthe physiological mechanism of DNA replication and then embedded it in\nan “experimental mechanism”; that experimental mechanism\nthen generated the data pattern of heavy-vs-light DNA in subsequent\nE.coli generations. Moreover, any hypothesis of DNA\nreplication had to satisfy mechanistic constraints imposed by what was\nalready known about the physiological mechanism—that DNA was a\ndouble helix, and that the sequence of nucleotides in the DNA needed\nto be preserved in subsequent generations. So Duhem’s concern\nabout unknown alternatives was alleviated because known mechanistic\nconstraints limited the set of possible hypotheses that could generate\nthe phenomenon. On Weber’s reading, the mechanistic constraints\nculled the set of possible hypotheses for DNA replication to\nsemi-conservative replication, conservative replication, and\ndispersive replication; then, among that set, Meselson and Stahl\ndevised an experimental mechanism such that semi-conservative\nreplication was the best explanation of the data pattern they found.\n(For a critique, see Baetu 2017.) \nAn overview of the history of molecular biology revealed the original\nconvergence of geneticists, physicists, and structural chemists on a\ncommon problem: the nature of inheritance. Conceptual and\nmethodological frameworks from each of these disciplinary strands\nunited in the ultimate determination of the double helical structure\nof DNA (conceived of as an informational molecule) along with the\nmechanisms of gene replication, mutation, and expression. With this\nrecent history in mind, philosophers of molecular biology have\nexamined the key concepts of the field: mechanism, information, and\ngene. Moreover, molecular biology has provided cases for addressing\nmore general issues in the philosophy of science, such as reduction,\nexplanation, extrapolation, and experimentation.","contact.mail":"james.tabery@utah.edu","contact.domain":"utah.edu"},{"date.published":"2005-02-19","date.changed":"2019-06-27","url":"https://plato.stanford.edu/entries/molecular-biology/","author1":"James Tabery","author1.info":"https://faculty.utah.edu/u0578517-JAMES_TABERY/biography/index.hml","author2.info":"http://www.albany.edu/philosophy/faculty.shtml#piotrowska","entry":"molecular-biology","body.text":"\n\n\nThe field of molecular biology studies macromolecules and the\nmacromolecular mechanisms found in living things, such as the\nmolecular nature of the gene and its mechanisms of gene replication,\nmutation, and expression. Given the fundamental importance of these\nmacromolecular mechanisms throughout the history of molecular biology,\na philosophical focus on the concept of a mechanism generates the\nclearest picture of molecular biology’s history, concepts, and\ncase studies utilized by philosophers of science.\n\nDespite its prominence in the contemporary life sciences, molecular\nbiology is a relatively young discipline, originating in the 1930s and\n1940s, and becoming institutionalized in the 1950s and 1960s. It\nshould not be surprising, then, that many of the philosophical issues\nin molecular biology are closely intertwined with this recent history.\nThis section sketches four facets of molecular biology’s\ndevelopment: its origins, its classical period, its subsequent\nmigration into other biological domains, and its more recent turn to\ngenomics and post-genomics. The rich historiography of molecular\nbiology can only be briefly utilized in this shortened history (see,\nfor example, Abir-Am 1985, 1987, 1994, 2006; Burian 1993a; Canguillhem\n1989; de Chadarevian 2002, 2003; de Chadarevian and Gaudilliere 1996;\nde Chadarevian and Strasser 2002; Deichmann 2002; Fisher 2010;\nHausmann 2002; Holmes 2001; Judson 1980, 1996; Kay 1993; Marcum 2002;\nMorange 1997a, 1998; Olby 1979, 1990, 1994, 2003; Powell et al. 2007;\nRheinberger 1997; Sapp 1992; Sarkar 1996a; Stegenga 2011; van Holde\nand Zlatanova 2018; Witkowski 2005; Zallen 1996. Also see\nautobiographical accounts by biologists, such as Brenner 2001; Cohen\n1984; Crick 1988; Echols 2001; Jacob 1988; Kornberg 1989; Luria 1984;\nWatson 1968, 2002, 2007; Wilkins 2003). \nThe field of molecular biology arose from the convergence of work by\ngeneticists, physicists, and structural chemists on a common problem:\nthe nature of inheritance. In the early twentieth century, although\nthe nascent field of genetics was guided by Mendel’s laws of\nsegregation and independent assortment, the actual mechanisms of gene\nreproduction, mutation and expression remained unknown. Thomas Hunt\nMorgan and his colleagues utilized the fruit fly, Drosophila\nmelanogaster, as a model organism to study the relationship\nbetween the gene and the chromosomes in the hereditary process (Morgan\n1926; discussed in Darden 1991; Darden and Maull 1977; Kohler 1994;\nRoll-Hanson 1978; Wimsatt 1992). A former student of Morgan’s,\nHermann J. Muller, recognized the “gene as a basis of\nlife”, and so set out to investigate its structure (Muller\n1926). Muller discovered the mutagenic effect of x-rays on\nDrosophila, and utilized this phenomenon as a tool to explore\nthe size and nature of the gene (Carlson 1966, 1971, 1981, 2011; Crow\n1992; Muller 1927). But despite the power of mutagenesis, Muller\nrecognized that, as a geneticist, he was limited in the extent to\nwhich he could explicate the more fundamental properties of genes and\ntheir actions. He concluded a 1936 essay: \nThe geneticist himself is helpless to analyse these properties\nfurther. Here the physicist, as well as the chemist, must step in. Who\nwill volunteer to do so? (Muller 1936: 214) \nMuller’s request did not go unanswered. The next decade saw\nseveral famous physicists turn their attention to the nature of\ninheritance (Keller 1990; Kendrew 1967). In What is Life, the\nphysicist Erwin Schroedinger (1944) proposed ways in which the\nprinciples of quantum physics might account for the stability, yet\nmutability, of the gene (see the entry on\n life)\n (Elitzur 1995; Moore 1989; Olby 1994; Sarkar 1991; for a\nreinterpretation see Kay 2000). Max Delbrueck also became interested\nin the physical basis of heredity after hearing a lecture by his\nteacher, quantum physicist Niels Bohr (1933), which expounded a\nprinciple of complementarity between physics and biology (McKaughan\n2005; Roll-Hansen 2000). In contrast to Schroedinger, Bohr (and\nsubsequently Delbrueck) did not seek to reduce biology to physics;\ninstead, the goal was to understand how each discipline complemented\nthe other (Delbrueck 1949; Sloan and Fogel 2011). To investigate the\nself-reproductive characteristic of life, Delbrueck used\nbacteriophage, viruses that infect bacteria and then multiply very\nrapidly. The establishment of “The Phage Group” in the\nearly 1940s by Delbrueck and another physicist-turned-biologist\nSalvador Luria marked a critical point in the rise of molecular\nbiology (Brock 1990; Cairns et al. 1966; Fischer and Lipson 1988;\nFleming 1968; Lewontin 1968; Luria 1984; Morange 1998: Ch. 4; Stent\n1968). Delbrueck’s colleague at Cal Tech, Linus Pauling,\nutilized his knowledge of structural chemistry to study macromolecular\nstructure. Pauling contributed both theoretical work on the nature of\nchemical bonds and experimental work using x-ray crystallography to\ndiscover the physical structure of macromolecular compounds (Pauling\n1939, 1970; Olby 1979; Hager 1995; Crick 1996; Sarkar 1998). \nAs suggested in the brief history above, experimentation figured\nprominently in the rise of molecular biology (see the entry on\n experiment in biology).\n X-ray crystallography allowed molecular biologists to investigate the\nstructure of macromolecules.\n Alfred Hershey and Martha Chase (1952) used phage viruses to confirm\nthat the genetic material transmitted from generation to generation\nwas DNA and not proteins (see Hershey-Chase Experiment in\n Other Internet Resources).\n Muller (1927) used x-rays to intervene on and alter gene function,\nthus revealing the application of methods from physics to a biological\ndomain (see Elof Carlson on Muller’s Research in\n Other Internet Resources).\n  \nRecognizing quite early the importance of these new physical and\nstructural chemical approaches to biology, Warren Weaver, then the\ndirector of the Natural Sciences section of the Rockefeller\nFoundation, introduced the term “molecular biology” in a\n1938 report to the Foundation. Weaver wrote, \nAnd gradually there is coming into being a new branch of\nscience—molecular biology—which is beginning to uncover\nmany secrets concerning the ultimate units of the living\ncell….in which delicate modern techniques are being used to\ninvestigate ever more minute details of certain life processes (quoted\nin Olby 1994: 442). \nBut perhaps a more telling account of the term’s origin came\nfrom Francis Crick, who said he started calling himself a molecular\nbiologist because: \nwhen inquiring clergymen asked me what I did, I got tired of\nexplaining that I was a mixture of crystallographer, biophysicist,\nbiochemist, and geneticist, an explanation which in any case they\nfound too hard to grasp. (quoted in Stent 1969: 36) \nThis brief recapitulation of the origins of molecular biology reflects\nthemes addressed by philosophers, such as reduction (see\n Section 3.1),\n the concept of the gene (see\n Section 2.3),\n and experimentation (see\n Section 3.4).\n For Schroedinger, biology was to be reduced to the more fundamental\nprinciples of physics, while Delbrueck instead resisted such a\nreduction and sought what made biology unique. Muller’s shift\nfrom Mendelian genetics to the study of gene structure raises the\nquestion of the relation between the gene concepts found in those\nseparate fields of genetics. And the import of experimental methods\nfrom physics to biology raised the question of the relation between\nthose disciplines. \nMolecular biology’s classical period began in 1953, with James\nWatson and Francis Crick’s discovery of the double helical\nstructure of DNA (Watson and Crick 1953a,b). Watson and Crick’s\nscientific relationship unified the various disciplinary approaches\ndiscussed above: Watson, a student of Luria and the phage group,\nrecognized the need to utilize crystallography to elucidate the\nstructure of DNA; Crick, a physicist enticed by Schroedinger’s\nWhat is Life? to turn to biology, became trained in, and\ncontributed to the theory of, x-ray crystallography. At Cambridge\nUniversity, Watson and Crick found that they shared an interest in\ngenes and the structure of DNA (see the entry on\n scientific revolutions). \nWatson and Crick collaborated to build a model of the double helical\nstructure of DNA, with its two helical strands held together by\nhydrogen-bonded base pairs (Olby 1994). They made extensive use of\ndata from x-ray crystallography work on DNA by Maurice Wilkins and\nRosalind Franklin at King’s College, London, appallingly without\nFranklin’s permission or even knowledge (Maddox 2002),\nCrick’s theoretical work on crystallography (Crick 1988), and\nthe model building techniques pioneered by Pauling (de Chadarevian\n2002; Judson 1996; Olby 1970, 1994, 2009). \nWith the structure of DNA in hand, molecular biology shifted its focus\nto how the double helical structure aided elucidation of the\nmechanisms of genetic replication and function, the keys to\nunderstanding the role of genes in heredity (see the entries on\n replication and reproduction\n and\n inheritance systems).\n This subsequent research was guided by the notion that the gene was\nan informational molecule. According to Lily Kay, \nUp until around 1950 molecular biologists…described genetic\nmechanisms without ever using the term information. (Kay\n2000: 328) \n“Information” replaced earlier talk of biological\n“specificity”. Watson and Crick’s second paper of\n1953, which discussed the genetical implications of their recently\ndiscovered (Watson and Crick 1953a) double-helical structure of DNA,\nused both “code” and “information”: \n…it therefore seems likely that the precise sequence of the\nbases is the code which carries the genetical\ninformation…. (Watson and Crick 1953b: 244, emphasis\nadded) \nIn 1958, Francis Crick used and characterized the concept of\ninformation in the context of stating the “central\ndogma” of molecular biology. Crick characterized the central\ndogma as follows: \nThis states that once “information” has passed into\nprotein it cannot get out again. In more detail, the transfer\nof information from nucleic acid to nucleic acid, or from nucleic acid\nto protein may be possible, but transfer from protein to protein, or\nfrom protein to nucleic acid is impossible. Information means here the\nprecise determination of sequence, either of bases in the nucleic acid\nor of amino acid residues in the protein. (Crick 1958: 152–153,\nemphasis in original) \nIt is important not to confuse the genetic code and genetic\ninformation. The genetic code refers to the relation between three\nbases of DNA, called a “codon”, and one amino acid. Tables\navailable in molecular biology textbooks (e.g., Watson et al. 1988:\nfrontispiece) show the relation between 64 codons and 20 amino acids.\nFor example, CAC codes for histidine. Only a few exceptions for these\ncoding relations have been found, in a few anomalous cases (see the\nlist in a small table in Alberts et al. 2002: 814). In contrast,\ngenetic information refers to the linear sequence of codons along the\nDNA, which (in the simplest case) are transcribed to messenger RNA,\nwhich are translated to linearly order the amino acids in a\nprotein. \nWith the genetic code elucidated and the relationship between genes\nand their molecular products traced, it seemed in the late 1960s that\nthe concept of the gene was secure in its connection between gene\nstructure and gene function. The machinery of protein synthesis\ntranslated the coded information in the linear order of nucleic acid\nbases into the linear order of amino acids in a protein. However, such\n“colinear” simplicity did not persist. In the late 1970s,\na series of discoveries by molecular biologists complicated the\nstraightforward relationship between a single, continuous DNA sequence\nand its protein product. Overlapping genes were discovered (Barrell et\nal. 1976); such genes were considered “overlapping”\nbecause two different amino acid chains might be read from the same\nstretch of nucleic acids by starting from different points on the DNA\nsequence. And split genes were found (Berget et al. 1977; Chow et al.\n1977). In contrast to the colinearity hypothesis that a continuous\nnucleic acid sequence generated an amino acid chain, it became\napparent that stretches of DNA were often split between coding regions\n(exons) and non-coding regions (introns). Moreover, the exons might be\nseparated by vast portions of this non-coding, supposedly “junk\nDNA”. The distinction between exons and introns became even more\ncomplicated when\n alternative splicing\n was discovered the following year (Berk and Sharp 1978). A series of\nexons could be spliced together in a variety of ways, thus generating\na variety of molecular products. Discoveries such as overlapping\ngenes, split genes, and alternative splicing forced molecular\nbiologists to rethink their understanding of what actually made a\ngene…a gene (Portin 1993; for a survey of such complications\nsee Gerstein et al. 2007: Table 1). \nThese developments in molecular biology have received philosophical\nscrutiny. Molecular biologists sought to discover mechanisms\n(see\n Section 2.1),\n drawing the attention of philosophers to this concept. Also,\nconceptualizing DNA as an informational molecule (see\n Section 2.2)\n was a move that philosophers have subjected to critical scrutiny.\nFinally, the concept of the gene (see\n Section 2.3)\n itself has intrigued philosophers. Complex molecular mechanisms, such\nas alternative splicing, have obligated philosophers to consider to\nwhat the term “gene” actually refers. Experimentation also\nfigured prominently in the classical period (see\n Section 3.4);\n Matthew Meselson and Frank Stahl utilized bacteria grown with\ndifferent weights combined with centrifugation to determine how DNA,\nas modeled by Watson and Crick, was replicated (Meselson and Stahl\n1958; see also The Semi-Conservative Replication of DNA in\n Other Internet Resources).\n  \nIn a 1963 letter to Max Perutz, molecular biologist Sydney Brenner\nforeshadowed what would be molecular biology’s next intellectual\nmigration: \nIt is now widely realized that nearly all the “classical”\nproblems of molecular biology have either been solved or will be\nsolved in the next decade…. Because of this, I have long felt\nthat the future of molecular biology lies in the extension of research\nto other fields of biology, notably development and the nervous\nsystem. (Brenner, letter to Perutz, 1963) \nAlong with Brenner, in the late 1960s and early 1970s, many of the\nleading molecular biologists from the classical period redirected\ntheir research agendas, utilizing the newly developed molecular\ntechniques to investigate unsolved problems in other fields. Francois\nJacob, Jacques Monod and their colleagues used the bacteria\nEscherichia coli to investigate how environmental conditions\nimpact gene expression and regulation (Jacob and Monod 1961; discussed\nin Craver and Darden 2013; Morange 1998: Ch. 14; Schaffner 1974a;\nWeber 2005; see also the entry on the\n developmental biology).\n The study of behavior and the nervous system also lured some\nmolecular biologists. Finding appropriate model organisms that could\nbe subjected to molecular genetic analyses proved challenging.\nReturning to the fruit flies used in Mendelian genetics, Seymour\nBenzer induced behavioral mutations in Drosophila as a\n“genetic scalpel” to investigate the pathways from genes\nto behavior (Benzer 1968; Weiner 1999). And at Cambridge, Sydney\nBrenner developed the nematode worm, Caenorhabditis elegans,\nto study the nervous system, as well as the genetics of behavior\n(Brenner 1973, 2001; Ankeny 2000; Brown 2003). In subsequent decades,\nthe study of cells was transformed from descriptive cytology into\nmolecular cell biology (Alberts et al. 1983; Alberts et al. 2002;\nBechtel 2006). Molecular evolution developed as a phylogenetic method\nfor the comparison of DNA sequences and whole genomes; molecular\nsystematics sought to research the evolution of the genetic code as\nwell as the rates of that evolutionary process by comparing\nsimilarities and differences between molecules (Dietrich 1998; see\nalso the entries on\n evolution,\n heritability, and\n adaptationism).\n The immunological relationship between antibodies and antigens was\nrecharacterized at the molecular level (Podolsky and Tauber 1997;\nSchaffner 1993; see also the entry on the\n philosophy of immunology).\n And the study of oncogenes in cancer research as well as the\nmolecular bases of mental illness were examples of advances in\nmolecular medicine (Morange 1997b; see also the entry on\n philosophy of psychiatry). \nThis process of “going molecular” thus generally amounted\nto using experimental methods from molecular biology to examine\ncomplex phenomena (be it gene regulation, behavior, or evolution) at\nthe molecular level. The molecularization of many fields introduced a\nrange of issues of interest to philosophers. Inferences made about\nresearch on model organisms such as worms and flies raised questions\nabout extrapolation (see\n Section 3.3).\n And the reductive techniques of molecular biology raised questions\nabout whether scientific investigations should always strive to reduce\nto lower and lower levels (see\n Section 3.1). \nIn the 1970s, as many of the leading molecular biologists were\nmigrating into other fields, molecular biology itself was going\ngenomic (see the entry on\n genomics and postgenomics).\n The genome is a collection of nucleic acid base pairs within an\norganism’s cells (adenine (A) pairs with thymine (T) and\ncytosine (C) with guanine (G)). The number of base pairs varies widely\namong species. For example, the infection-causing Haemophilus\ninfluenzae (the first bacterial genome to be sequenced) has\nroughly 1.9 million base pairs in its genome (Fleischmann et al.\n1995), while the infection-catching Homo sapiens carries more\nthan 3 billion base pairs in its genome (International Human Genome\nSequencing Consortium 2001, Venter et al. 2001). The history of\ngenomics is the history of the development and use of new experimental\nand computational methods for producing, storing, and interpreting\nsuch sequence data (Ankeny 2003; Stevens 2013). \nFrederick Sanger played a seminal role in initiating such\ndevelopments, creating influential DNA sequencing techniques in the\n1950s and 1960s (Saiki et al. 1985; for historical treatments see\nSanger 1988; Judson 1992; Culp 1995; Rabinow 1996; Morange 1998; de\nChadarevian 2002; Little 2003; Garcia-Sancho 2012; Sanger Method of\nDNA Sequencing in\n Other Internet Resources).\n Equally important was Edwin Southern’s development of a method\nto detect specific sequences of DNA in DNA samples (Southern 1975).\nThe Southern Blot, as it came to be known, starts by digesting a\nstrand of DNA into many small DNA fragments; those fragments are then\nseparated (in a process called gel electrophoresis) based on size,\nplaced on filter paper which “blots” the DNA fragments on\nto a new medium, and then chemically labeled with DNA probes; the\nprobes then allow for identification and visualization of the DNA\nfragments (see also The Southern Blot in\n Other Internet Resources).\n Playing off the “southern” homonym, subsequent blotting\ntechniques that detect RNA and proteins came to be called Northern\nblotting and Western blotting.  \nIn the mid 1980s, after the development of sequencing techniques, the\nUnited States Department of Energy (DoE) originated a project to\nsequence the human genome (initially as part of a larger plan to\ndetermine the impact of radiation on the human genome induced by the\nHiroshima and Nagasaki bombings). The resulting Human Genome Project\n(HGP) managed jointly by the DoE and the United States National\nInstitutes of Health (NIH), utilized both existent sequencing\nmethodologies and introduced new ones (Kevles and Hood 1992, see also\nthe entry on\n the human genome project).\n While the human genome project received most of the public attention,\nhundreds of genomes have been sequenced to date, including the cat\n(Pontius et al. 2007), the mouse (Waterson et al. 2002), rice (Goff et\nal. 2002) and a flock of bird genomes (Zhang et al. 2014). One of the\nmost shocking results of those sequencing projects was the total\nnumber of genes (defined in this context as stretches of DNA that code\nfor a protein product) found in the genomes. The human genome contains\n20,000 to 25,000 genes, the cat contains 20,285 genes, the mouse\n24,174, and rice 32,000 to 50,000. So in contrast to early assumptions\nstemming from the classical period of molecular biology about how\ngenes produced proteins which in turn produced organisms, it turned\nout that neither organismal complexity nor even position on the food\nchain was predictive of gene-number (see the entry on\n genomics and postgenomics). \nThe increased attention to sequencing genomes encouraged a number of\ndisciplines to “go genomic”, including behavioral genetics\n(Plomin et al. 2003), developmental biology (Srinivasan and Sommer\n2002), cell biology (Taniguchi et al. 2002), and evolutionary biology\n(Ohta and Kuroiwa 2002). What’s more, genomics has been\ninstitutionalized with textbooks (Cantor and Smith 1999) and journals,\nsuch as Genomics and Genome Research. And the human\ngenome project itself has turned its attention from a standardized\nhuman genome to variation between genomes in the form of the Human\nGenome Diversity Initiative (Gannett 2003) and the HapMap Project\n(International HapMap Consortium 2003). \nBut just as a number of disciplines “went molecular” while\nmolecular biology itself was wrestling with the complexities posed by\nsplit genes and overlapping genes, so too are fields going genomic\nwhile genomics itself is wrestling with the complexities posed by how\na mere 20,000 genes can construct a human while a grain of rice\nrequires 50,000 genes (Baedke 2018; Brigandt, Green, and\nO’Malley 2017; Green 2017). A related challenge was making sense\nof the genetic similarity claims. For example, how to interpret the\nfinding that human and pumpkin genomes are 75% similar? Does this\nfinding tell us anything substantive about our overall similarity to\npumpkins (Piotrowska 2009)? To help answer such questions, genomics is\nnow supplemented by post-genomics. There is ongoing debate about what\nactually constitutes post-genomics (Morange 2006), but the general\ntrend is a focus beyond the mere sequence of As, Cs, Ts, and Gs and\ninstead on the complex, cellular mechanisms involved in generating\nsuch a variety of protein products from a relatively small number of\nprotein-coding regions in the genome. Post-genomics utilizes the\nsequence information provided by genomics but then situates it in an\nanalysis of all the other entities and activities involved in the\nmechanisms of transcription (transcriptomics), regulation\n(regulomics), metabolism (metabolomics), and expression (proteomics).\n(See ENCODE Project Consortium 2012; Germain et al. 2014; (see also\nthe entry on\n philosophy of systems and synthetic biology). \nDevelopments in genomics and post-genomics have sparked a number of\nphilosophical questions about molecular biology. Since the genome\nrequires a vast array of other mechanisms to facilitate the generation\nof a protein product, can DNA really be causally prioritized (see\n Section 2.3)?\n Similarly, in the face of such interdependent mechanisms involved in\ntranscription, regulation, and expression, can DNA alone be privileged\nas the bearer of hereditary information, or is information distributed\nacross all such entities and activities (see\n Section 2.2)?\n And is it appropriate to extrapolate from information about other\nspecies’ genomes to how the human genome operates (see\n Section 3.3)? \nThe concepts of mechanism, information, and\ngene all figured quite prominently in the history of\nmolecular biology. Philosophers, in turn, have focused a great deal of\nattention on these concepts in order to understand how they have been,\nare, and should be used. \nMolecular biologists discover and explain by identifying and\nelucidating mechanisms, such as DNA replication, protein synthesis,\nand the myriad mechanisms of gene expression. The phrase “theory\nof molecular biology” was not used above and for good reason;\ngeneral knowledge in the field is represented by diagrams of\nmechanisms (Machamer, Darden, and Craver 2000; Darden 2006a, 2006b;\nCraver and Darden 2013; Baetu 2017). Discovering the mechanism that\nproduces a phenomenon is an important accomplishment for several\nreasons. First, knowledge of a mechanism shows how something works:\nelucidated mechanisms provide understanding. Second, knowing how a\nmechanism works allows predictions to be made based upon the\nregularity in mechanisms. For example, knowing how the mechanism of\nDNA base pairing works in one species allows one to make predictions\nabout how it works in other species, even if conditions or inputs are\nchanged. Third, knowledge of mechanisms potentially allows one to\nintervene to change what the mechanism produces, to manipulate its\nparts to construct experimental tools, or to repair a broken, diseased\nmechanism. In short, knowledge of elucidated mechanisms provides\nunderstanding, prediction, and control. Given the general importance\nof mechanisms and the fact that mechanisms play such a central role in\nthe field of molecular biology, it is not surprising that philosophers\nof biology pioneered analyzing the concept of mechanism (see the entry\non mechanisms in science). \nStarting in the 1990s, a number of philosophers focused squarely on\nhow the concept of a mechanism functions in science generally and\nmolecular biology specifically (Glennan and Illari 2017; see also the\nentry on\n mechanisms in science).\n A number of characterizations of what a mechanism is have emerged\nover the years (Bechtel and Abrahamsen 2005; Glennan 2002; Machamer,\nDarden, and Craver 2000). Phyllis McKay Illari and Jon Williamson have\nmore recently offered a characterization that draws on the essential\nfeatures of all the earlier contributions: \nA mechanism for a phenomenon consists of entities and activities\norganized in such a way that they are responsible for the phenomenon.\n(Illari and Williamson 2012: 120) \nAs an example, consider the phenomenon of DNA replication. As Watson\nand Crick (1953a) famously noted upon discovery of the structure of\nDNA, the macromolecule’s structure pointed to the mechanism of\nDNA replication: \nIt has not escaped our notice that the specific pairing we have\npostulated immediately suggests a possible copying mechanism for the\ngenetic material. \nIn short, the double helix of DNA (an entity with an organization)\nunwinds (an activity) and new component parts (entities) bond (an\nactivity) to both parts of the unwound DNA helix. DNA is a nucleic\nacid composed of several subparts: a sugar-phosphate backbone and\nnucleic acid bases. When DNA unwinds, the bases exhibit weak charges,\nproperties that result from slight asymmetries in the molecules. These\nweak charges allow a DNA base and its complement to engage in the\nactivity of forming hydrogen (weak polar) chemical bonds; the\nspecificity of this activity is due to the topological arrangements of\nthe weak polar charges in the subparts of the base. Ultimately,\nentities with polar charges enable the activity of hydrogen bond\nformation. After the complementary bases align, then the backbone\nforms via stronger covalent bonding. The mechanism proceeds with\nunwinding and bonding together (activities) new parts, to produce two\nhelices (newly formed entities) that are (more or less faithfully)\ncopies of the parent helix. (This process of “semi-conservative\nreplication” and the Meselson-Stahl experiment that confirmed it\nare discussed in more detail in\n Section 3.4.) \nScientists rarely depict all the particular details when describing a\nmechanism; representations are usually schematic, often depicted in\ndiagrams (see the entry on\n models in science).\n Such representations may be called a “model of a\nmechanism”, or “mechanism schema”. A mechanism\nschema is a truncated abstract description of a mechanism that can be\ninstantiated by filling it with more specific descriptions of\ncomponent entities and activities. An example is James Watson’s\n(1965) diagram of his version of the central dogma of molecular\nbiology: \nDNA → RNA → protein. \nThis is a schematic representation (with a high degree of abstraction)\nof the mechanism of protein synthesis, which can be instantiated with\ndetails of DNA base sequence, complementary RNA sequence, and the\ncorresponding order of amino acids in the protein produced by the more\nspecific mechanism. Molecular biology textbooks are replete with\ndiagrams of mechanism schemas. A mechanism schema can be instantiated\nto yield a description of a particular mechanism. In contrast, a\nmechanism sketch cannot (yet) be instantiated; components are (as yet)\nunknown. Sketches have black boxes for missing components or grey\nboxes whose function is known but whose entities and activities that\ncarry out that function are not yet elucidated. Such sketches guide\nnew research to fill in the details (Craver and Darden 2013). \nThe language of information appears often in molecular biology. Genes\nas linear DNA sequences of bases are said to carry\n“information” for the production of proteins. During\nprotein synthesis, the information is “transcribed” from\nDNA to messenger RNA and then “translated” from RNA to\nprotein. With respect to inheritance, it is often said that what is\npassed from one generation to the next is the\n“information” in the genes, namely the linear ordering of\nbases along complementary DNA strands. Historians of biology have\ntracked the entrenchment of information-talk in molecular biology (Kay\n2000) since its introduction. \nThe question for philosophers of biology is whether an analysis of the\nconcept of information can capture the various ways in which the\nconcept is used in molecular biology (e.g., Maynard Smith 2000). The\nusage of “information” in the mathematical theory of\ncommunication is too impoverished to capture the molecular biological\nusage, since the coded sequences in the DNA are more than just a\nsignal with some number of bits that may or may not be accurately\ntransmitted (Sarkar 1996b,c; Sterelny and Griffiths 1999; Shannon and\nWeaver 1949). Conversely, the usage in cognitive neuroscience, with\nits talk of “representations” (e.g., Crick 1988) may be\nsaid to be too rich, since the coded sequences in the DNA are also not\nsaid to have within them a representation of the structure of the\nprotein (Darden 2006b). No definition of “information” as\nit is used in molecular biology has yet received wide support among\nphilosophers of biology. \nStephen Downes (2006) helpfully distinguishes three positions on the\nrelation between information and the natural world: \nThese options may be read either ontologically or heuristically. A\nheuristic reading of (1), for instance, views the talk of information\nin molecular biology as useful in providing a way of talking and in\nguiding research. And so the heuristic benefit of the information\nconcept can be defended without making any commitment to the\nontological status (Sarkar 2000). Indeed, one might argue that a vague\nand open-ended use of information is valuable for heuristic purposes,\nespecially during early discovery phases in the development of a\nfield. \nPhilosophers’ discussions of the concept of information in\nbiology have also focused on its ontological reading. Three different\nphilosophical accounts of information serve as exemplars of\nDownes’ three categories. Ulrich Stegmann (2005) provides an\nexample of Downes’ first category with his analysis of\ntemplate-directed synthesis. (Stegmann does explicitly allow that\ncomponents other than nucleotide sequences might contain what he calls\ninstructional information. However, his only example is a thought\nexperiment involving enzymes linearly ordered along a membrane;\nnothing of the sort is known to actually exist or even seems very\nlikely to exist.) Stegmann calls this the sequentialization view.\nStegmann’s instructional account of genetic information requires\nthat the component carrying the information satisfy the following\nconditions: an advance specification of the kind and order of steps\nthat yield a determinate outcome if the steps are carried out. On his\naccount, DNA qualifies as an instructional information carrier for\nreplication, transcription and translation. The sequence of bases\nprovides the order. The hydrogen bonding between specific bases and\nthe genetic code provide the specific kinds of steps. And the\nmechanisms of replication, transcription, and translation yield\ncertain outcomes: a copy of the DNA double helix, an mRNA, and a\nlinear order of amino acids. Also, because DNA carries information for\na specific outcome, an error can occur as the mechanism operates to\nproduce that outcome; hence Stegmann’s account allows for errors\nand error-correcting mechanisms (such as proof reading mechanisms that\ncorrect DNA mutations). For more on this topic, see the entry on\n biological information. \nEva Jablonka (2002) is an example of Downes’ second category.\nShe argues that information is ubiquitous. She defines information as\nfollows: a source becomes an informational input when an interpreting\nreceiver can react to the form of the source (and variations in this\nform) in a functional manner. She claims a broad applicability of this\ndefinition. The definition, she says, accommodates information\nstemming from environmental cues as well as from evolved signals, and\ncalls for a comparison between information-transmission in different\ntypes of inheritance systems — the genetic, the epigenetic, the\nbehavioral, and the cultural-symbolic. On this view, genes have no\ntheoretically privileged informational status (Jablonka 2002:\n583). \nIn line with Downes’ third category, C. Kenneth Waters argues\nthat information is a useful term in rhetorical contexts, such as\nseeking funding for DNA sequencing by claiming that DNA carries\ninformation. However, from an ontological perspective, Waters claims\nthat explication of DNA’s causal role has no need for the\nconcept of information. Genes, he argues, should not be viewed as\n“immaterial units of information” (Waters 2000: 541). As\ndiscussed in\n Section 2.3\n below, Waters’ focus is on stretches of DNA whose causal roles\nare as actual specific difference makers in genetic mechanisms (Waters\n2007). Talk of information is not needed; causal role function talk is\nsufficient. (For more on Waters’ view see his entry on\n molecular genetics;\n for others who make similar points, see Sustar 2007; Weber 2005,\n2006.) \nThe question of whether classical, Mendelian genetics could be (or\nalready has been) reduced to molecular biology (to be taken up in\n Section 3.1\n below) motivated philosophers to consider the connectability of the\nterm they shared: the gene. Investigations of reduction and scientific\nchange raised the question of how the concept of the gene evolved over\ntime, figuring prominently in C. Kenneth Waters’ (1990, 1994,\n2007, see entry on\n molecular genetics),\n Philip Kitcher’s (1982, 1984) and Raphael Falk’s (1986)\nwork. Over time, however, philosophical discussions of the gene\nconcept took on a life of their own, as philosophers raised questions\nindependent of the reduction debate: What is a gene? And, is there\nanything causally distinct about DNA? (see the entry on the\n gene) \nFalk (1986) explicitly asked philosophers and historians of biology,\n“What is a Gene?” Discoveries such as overlapping genes,\nsplit genes, and alternative splicing (discussed in\n Section 1.2)\n made it clear that simply equating a gene with an uninterrupted\nstretch of DNA would no longer capture the complicated\nmolecular-developmental details of mechanisms such as gene expression\n(Downes 2004; Luc-Germain, Ratti and Boem 2015). In an effort to\nanswer Falk’s question, two general trends have emerged in the\nphilosophical literature: first, distinguish multiple gene concepts to\ncapture the complex structural and functional features separately, or\nsecond, rethink a unified gene concept to incorporate such complexity.\n(For a survey of gene concepts defended by philosophers, see Griffiths\nand Stotz 2007, 2013.; Rheinberger and Muller-Wille 2018) \nA paradigmatic example of the first line came from Lenny Moss’s\ndistinction between Gene-P and Gene-D (Moss 2001, 2002). Gene-P\nembraced an instrumental preformationism (providing the\n“P”); it was defined by its relationship to a phenotype.\nIn contrast, Gene-D referred to a developmental resource (providing\nthe “D”); it was defined by its molecular sequence. An\nexample will help to distinguish the two: When one talked about the\ngene for cystic fibrosis, the most common genetic disease affecting\npopulations of Western European descent, the Gene-P concept was being\nutilized; the concept referred to the ability to track the\ntransmission of this gene from generation to generation as an\ninstrumental predictor of cystic fibrosis, without being contingent on\nknowing the causal pathway between the particular sequence of DNA and\nthe ultimate phenotypic disease. The Gene-D concept, in contrast,\nreferred instead to just one developmental resource (i.e., the\nmolecular sequence) involved in the complex development of the\ndisease, which interacted with a host of other such resources\n(proteins, RNA, a variety of enzymes, etc.); Gene-D was indeterminate\nwith regards to the ultimate phenotypic disease. Moreover, in cases of\nother diseases where there are different disease alleles at the same\nlocus, a Gene-D perspective would treat these alleles as individual\ngenes, while a Gene-P perspective treats them collectively as\n“the gene for” the disease. (For other examples of\ngene-concept dividers, see Keller’s distinction between the gene\nas a structural entity and the gene as a functional entity as well as\nBaetu’s distinction between the gene as a syntax-based concept\nand the gene as a mapping concept (Baetu 2011; Keller 2000).) \nA second philosophical approach for conceptualizing the gene involved\nrethinking a single, unified gene concept that captured the\nmolecular-developmental complexities. For example, Eva Neumann-Held\n(Neumann-Held 1999, 2001; Griffiths and Neumann-Held 1999) claimed\nthat a “process molecular gene concept” (PMG) embraced the\ncomplicated developmental intricacies. On her unified view, the term\n“gene” referred to “the recurring process that leads\nto the temporally and spatially regulated expression of a particular\npolypeptide product” (Neumann-Held 1999). Returning to the case\nof cystic fibrosis, a PMG for an individual without the disease\nreferred to one of a variety of transmembrane ion-channel templates\nalong with all the epigenetic factors, i.e., nongenetic influences on\ngene expression, involved in the generation of the normal polypeptide\nproduct. And so cystic fibrosis arose when a particular stretch of the\nDNA sequence was missing from this process. (For another example of a\ngene-concept unifier, see Falk’s discussion of the gene as a DNA\nsequence that corresponded to a single norm of reaction for various\nmolecular products based on varying epigenetic conditions (Falk\n2001).) \nRelatedly, philosophers have also debated the causal distinctiveness\nof DNA. Consider again the case of cystic fibrosis. A stretch of DNA\non chromosome 7 is involved in the process of gene expression, which\ngenerates (or fails to generate) the functional product that\ntransports chloride ions. But obviously that final product results\nfrom that stretch of DNA as well as all the other developmental\nresources involved in gene expression, be it in the expression of the\nfunctional protein or the dysfunctional one. Thus, a number of authors\nhave argued for a causal parity thesis, wherein all developmental\nresources involved in the generation of a phenotype such as cystic\nfibrosis are treated as being on par (Griffiths and Knight 1998;\nRobert 2004; Stotz 2006). \nWaters (2007, see also his entry on\n molecular genetics),\n in reply, has argued that there is something causally distinctive\nabout DNA. Causes are often conceived of as being difference makers,\nin that a variable (i.e., an entity or activity in a mechanism) can be\ndeemed causal when a change in the value of that variable would\ncounterfactually have led to a different outcome (see the entry on\n scientific explanation).\n According to Waters, there are a number of potential\ndifference makers in the mechanisms involved in developing or not\ndeveloping cystic fibrosis; that is, an individual with two normal\ncopies of the gene could still display signs of cystic fibrosis if a\nmanipulation was done to the individual’s RNA polymerase (the\nprotein responsible for transcribing DNA to RNA), thereby undermining\nthe functional reading of the stretch of DNA. So RNA polymerase is a\ndifference maker in the development or lack of development of cystic\nfibrosis, but only a potential difference maker, since variation in\nRNA polymerase does not play a role in the development or lack of\ndevelopment of cystic fibrosis in natural populations. The stretch of\nDNA on chromosome 7, however, is an actual difference maker.\nThat is, there are actual differences in natural human populations on\nthis stretch of DNA, which lead to actual differences in developing or\nnot developing cystic fibrosis; DNA is causally distinctive, according\nto Waters, because it is an actual difference maker. Advocates of the\nparity thesis are thus challenged to identify the other resources (in\naddition to DNA) that are actual difference makers. \nRecently, Paul Griffiths and Karola Stotz (2013) have responded to\nthis challenge by offering examples in which, depending on context,\nregulatory mechanisms can either contribute additional information to\nthe gene products or create gene products for which there is no\nunderlying sequence. Thus, according to Griffiths and Stotz, to assign\na causally distinctive role to DNA, as Waters does, is to ignore key\naspects of how the gene makes its product. \nIn addition to analyzing key concepts in the field, philosophers have\nemployed case studies from molecular biology to address more general\nissues in the philosophy of science, such as reduction, explanation,\nextrapolation, and experimentation. For each of these philosophical\nissues, evidence from molecular biology directs philosophical\nattention toward understanding the concept of a mechanism for\naddressing the topic. \nReduction may be understood in multiple ways depending on what it is\nthat is being reduced (see the entry on\n scientific reduction).\n Theory reduction pertains to whether or not theories from one\nscientific field can be reduced to theories from another scientific\nfield. In contrast, explanatory reduction (often united with\nmethodological reduction) pertains to whether or not explanations that\ncome from lower levels (often united with methodologies that\ninvestigate those lower levels) are better than explanations that come\nfrom higher levels. Philosophical attention to molecular biology has\ncontributed to debates about both of these senses of reduction (see\nthe entry on\n reductionism in biology). \nPhilosophy of biology first came to prominence as a sub-specialty of\nphilosophy of science in the 1970s when it offered an apparent case\nstudy by which to judge how theories from one field may reduce to\ntheories from another field. The specific question was: might classic,\nMendelian genetics reduce to molecular genetics (see the entry on\n \n molecular genetics)?\n Kenneth Schaffner used and developed Ernst Nagel’s (1961)\nanalysis of derivational theory reduction to argue for the reduction\nof classical Mendelian genetics (T2) to molecular\nbiology (T1) and refined it over many years\n(summarized in Schaffner 1993). The goal of formal reduction was to\nlogically deduce the laws of classical genetics (or its improved\nsuccessor, “modern transmission genetics”\nT2*) from the laws of molecular biology. Such a\nderivation required that all the terms of T2* not\nin T1 had to be connected to terms in\nT1 via correspondence rules. Hence, Schaffner\nendeavored to find molecular equivalents of such terms as\n“gene”, as well as predicate terms, such as “is\ndominant”. David Hull (1974) criticized formal reduction, argued\nagainst Schaffner’s claims, and suggested, instead, that perhaps\nmolecular biology replaced classical genetics. \nEven though Schaffner and Hull were engaged in a debate over theory\nreduction, they simultaneously admitted that the question of formal\ntheory reduction was rather peripheral to what scientists actually did\nand studied (Schaffner 1974b; Hull 1974). And indeed, while the theory\nreduction debate was playing out, a number of philosophers of biology\nswitched attention from scientific theories to the\nstuff in nature that scientists investigated. William Wimsatt\n(1976) argued for a shift in the reduction debate from talk of\nrelations between theories to talk of decompositional explanation via\nmechanisms. And Lindley Darden and Nancy Maull (1977) focused\nattention on the bridges between fields formed by part-whole\nrelations, structure-function relations, and cause-effect\nrelations. \nThis shift in attention was a precursor to understanding the\nphilosophy of science through the lens of mechanisms. Darden, building\non the work of Machamer, Darden, and Craver (2000), has more recently\nreturned to the question of how Mendelian and molecular genetics are\nrelated and viewed it through this lens (Darden 2005). Rather than\nunderstanding the relationship as one of reduction, she suggests they\ncan be understood as relating via a focus on different working\nentities (often at different size levels) that operate at different\ntimes. Thus, the relation was one of integration of sequentially\noperating chromosomal and molecular hereditary mechanisms rather than\nreduction. (For an alternative but still integrative reading of the\nrelationship between classical genetics and molecular biology that\nfocuses on their shared functional units, see Baetu 2010.) \nReduction can also be about explanation and methodology. That is,\nreduction can be about using reductive methodologies to dig down to\nlower levels because the thought is that this exercise leads to more\nreductive explanations and more reductive explanations are better than\nexplanations at higher levels. Alex Rosenberg (1997, 2006)\ncontroversially divided biology into molecular biology and everything\nelse, which he dubbed “functional biology”.\n“Reductionism”, Rosenberg argued, \nis the thesis that biological theories and explanations that employ\nthem do need to be grounded in molecular biology and ultimately\nphysical science, for it is only by doing so that they can be\nimproved, corrected, strengthened, and made more accurate and more\nadequate and completed. (Rosenberg 2006: 4) \nHence, the task of this explanatory reduction is to explain all\nfunctional biological phenomena via molecular biology. \nCritics and defenders of Rosenberg’s view have discussed\norganizational and contextual features not captured by molecular\nbiological principles. These included orientation of the embryo in the\nearth’s gravitational field and other spatial, regulatory, and\ndynamical properties of developing systems (e.g., see Delehanty 2005;\nFrost-Arnold 2004; Keller 1999; Laubichler and Wagner 2001; Love et\nal. 2008; Robert 2001, 2004). This particular debate can be understood\nas an instance of a more general debate occurring in biology and\nphilosophy of biology about whether investigations of lower-level\nmolecular biology are better than investigations of high-level systems\nbiology (Baetu 2012a; Bechtel and Abrahamsen 2010; De Backer, De\nWaele, and Van Speybroeck 2010; Huettemann and Love 2011; Marco 2012;\nMorange 2008; Pigliucci 2013; Powell and Dupre 2009; see also the\nentries on\n feminist philosophy of biology,\n philosophy of systems and synthetic biology, and\n multiple realizability). \nTraditionally, philosophers of science took successful scientific\nexplanations to result from derivation from laws of nature (see the\nentries on\n laws of nature\n and\n scientific explanation).\n On this deductive-nomological account (Hempel and Oppenheim 1948), an\nexplanation of particular observation statements was analyzed as\nsubsumption under universal (applying throughout the universe),\ngeneral (exceptionless), necessary (not contingent) laws of nature\nplus the initial conditions of the particular case. Philosophers of\nbiology have criticized this traditional analysis as inapplicable to\nbiology, and especially molecular biology. \nSince the 1960s, philosophers of biology have questioned the existence\nof biological laws of nature. J. J. C. Smart (1963) emphasized the\nearth-boundedness of the biological sciences (in conflict with the\nuniversality of natural laws). No purported “law” in\nbiology has been found to be exceptionless, even for life on earth (in\nconflict with the generality of laws). And John Beatty (1995) argued\nthat the purported “laws” of, for example, Mendelian\ngenetics, were contingent on evolution (in conflict with the necessity\nof natural laws). (For further discussion, see Brandon 1997; Lange\n2000; Mitchell 1997; Sober 1997; Waters 1998; Weber 2005.) Hence,\nphilosophers’ search for biological laws of nature,\ncharacterized as universal, necessary generalizations, has ceased. \nWithout traditional laws of nature from which to derive explanations,\nphilosophers of biology have been forced to rethink the nature of\nscientific explanation in biology and, in particular, molecular\nbiology. Two accounts of explanation emerged: the unificationist and\nthe causal-mechanical. Philip Kitcher (1989, 1993) developed a\nunificationist account of explanation, and he and Sylvia Culp\nexplicitly applied it to molecular biology (Culp and Kitcher 1989).\nAmong the premises of the “Watson-Crick” argument schema\nwere “transcription, post-transcriptional modification and\ntranslation for the alleles in question”, along with details of\ncell biology and embryology for the organisms in question (Kitcher\n1989). An explanation of a particular pattern of distribution of\nprogeny phenotypes in a genetic cross resulted from instantiating the\nappropriate deductive argument schema: the variables were filled with\nthe details from the particular case and the conclusion derived from\nthe premises. \nWorking in the causal-mechanical tradition pioneered by Wesley Salmon\n(1984, 1998), other philosophers turned to understanding mechanism\nelucidation as the avenue to scientific explanation in biology\n(Bechtel and Abrahamsen 2005; Bechtel and Richardson 1993; Craver\n2007; Darden 2006a; Glennan 2002; Machamer, Darden, and Craver 2000;\nSarkar 1998; Schaffner 1993; Woodward 2002, 2010). There are\ndifferences between the various accounts of a mechanism, but they hold\nin common the basic idea that a scientist provides a successful\nexplanation of a phenomenon by identifying and manipulating variables\nin the mechanisms thereby determining how those variables are situated\nin and make a difference in the mechanism; the ultimate explanation\namounts to the elucidation of how those mechanism components act and\ninteract to produce the phenomenon under investigation. As mentioned\nabove (see\n Section 2.1,\n see also entry on mechanisms in science), an elucidated mechanism\nallows for the explanatory features of understanding, prediction, and\ncontrol. \nThere are several virtues of the causal-mechanical approach to\nunderstanding scientific explanation in molecular biology. For one, it\nis truest to molecular biologists’ own language when engaging in\nbiological explanation. Molecular biologists rarely describe their\npractice and achievements as the development of new theories; rather,\nthey describe their practice and achievements as the elucidation of\nmolecular mechanisms (Baetu 2017; Craver 2001; Machamer, Darden,\nCraver 2000). Another virtue of the causal-mechanical approach is that\nit captures biological explanations of both regularity and variation.\nUnlike in physics, where a scientist assumes that an electron is an\nelectron is an electron, a biologist is often interested in precisely\nwhat makes one individual different from another, one population\ndifferent from another, or one species different from another.\nPhilosophers have extended the causal-mechanical account of\nexplanation to cover biological explanations of variation, be it\nacross evolutionary time (Calcott 2009) or across individuals in a\npopulation (Tabery 2009, 2014). Tabery (2009, 2014) characterized\nbiological explanations of variation across individuals in a\npopulation as the elucidation of “difference mechanisms”.\nDifference mechanisms are regular causal mechanisms made up of\ndifference-making variables, one or more of which are actual\ndifference makers (see\n Section 2.3\n for the discussion of Waters’ (2007) concept of an actual\ndifference maker). There is regularity in difference mechanisms;\ninterventions made on variables in the mechanisms that change the\nvalues of the variables lead to different outcomes in the phenomena\nunder investigation. There is also variation in difference mechanisms;\ninterventions need not be taken to find differences in outcomes\nbecause, with difference mechanisms, some variables are actual\ndifference makers which already take different values in the natural\nworld, resulting in natural variation in the outcomes. \nBut philosophers have also raised challenges to the causal-mechanical\napproach. One complaint has been that mechanistic explanations\ndon’t work well in systems biology, a field distinct from\nmolecular biology but that nonetheless draws on the work of molecular\nbiologists. While some argue that systems biology is best explained\nusing mechanisms (cf. Boogerd et al. 2007; Matthiessen 2017;\nRichardson and Stephan 2007; Bechtel 2011; Bechtel and Abrahamsen\n2013), others argue that it requires non-mechanistic explanation (cf.\nBraillard 2010; Kuhlmann 2011; Silberstein and Chemero 2013). \nAnother complaint has been that mechanistic explanations fail to\nprovide a satisfactory account of the complexity of living systems\nbecause they fail to devote serious attention to the “thoroughly\nprocessual character of living systems” (Dupre 2012: 19). Those\nwho prefer a “process-oriented” ontology (Bapteste and\nDupre 2013; Bickhard 2011; Campbell 2015; Dupre 2012; Jaeger and Monk\n2015; Jaeger et al. 2012; Meincke 2018; Nicholson and Dupre 2018)\nargue that mechanistic accounts mistakenly assume that parts composing\na mechanism can be identified independently of the activities or\nprocesses in which they are involved. Instead, as Anne Sophie Maincke\nexplains, if there are parts or substances, they must be\n“reconceptualised as stabilized higher-order processes that\npersist as long as stabilization can be maintained” (2018).\nProcesses are ontologically primary. Recent literature in molecular\nbiology on molecular pathways (cf. Boniolo and Campaner 2018; Brigandt\n2018; Ioannides and Psillos 2017; Ross 2018) seems to be another\ninstantiation of this shift from mechanistic to processual\nexplanations. But Christopher Austin (2016) has recently challenged\nthe idea that processual explanations can be sufficiently grounded\n“without the metaphysical underpinning of the very\nmechanisms which processes purport to replace” (639). \nAs discussed earlier in the historical sections, molecular biologists\nhave relied heavily on model organisms (see the entry on\n models in science).\n The model organisms that were used to lay down the foundation of\nmolecular biology served as “exemplary models” in contrast\nto what today are called “surrogate models”—the\ndistinction comes from Jessica Bolker (2009). According to Bolker,\nexemplary models are “representatives of a broader group”\nand the goal of using them is “to elucidate fundamental or\ngeneral biological patterns and mechanisms” (487). But making\ninferences from a single exemplary model to general biological\npatterns has been cause for worry. What grounds do biologists have for\nbelieving that what is true of a mere model is true of many different\norganisms? One answer, provided by Marcel Weber (2005), is that the\ngenerality of biological knowledge obtained from studying exemplary\nmodels can be established on evolutionary grounds. According to Weber,\nif a mechanism is found in a set of phylogenetically distant\norganisms, this provides evidence that it is also likely to be found\nin all organisms that share a common ancestor with the organisms being\ncompared. \nExemplary models still play an important role in molecular biology,\nbut a new type of model organism, the “surrogate model”,\nhas become increasingly popular. According to Bolker, surrogate models\n“act as proxies for specific targets” and are often used\nin biomedical research “where the objective is to understand the\nmechanisms and etiology of human disease, and ultimately to develop\ntreatments” (Bolker 2009: 489). Unlike the aim of exemplary\nmodels, the representative aim of a surrogate model is not necessarily\nto be broad. Instead, it’s to faithfully replicate a specific\ntarget. For example, biomedical researchers frequently expose\nsurrogate models to harmful chemicals with the aim of modeling human\ndisease. However, if a chemical proves to be carcinogenic in rats, for\nexample, there is no guarantee that it will also cause cancer in\nhumans. \nThis difficulty of justifying the inference from rats to humans or,\nmore broadly, of “transferring causal generalizations from one\ncontext to another when homogeneity cannot be presumed” (Steel\n2008: 3) is known as the problem of extrapolation. Although\nthis problem is not unique to surrogate models, it often arises when\nbiomedical researchers use them to replicate human disease at the\nmolecular level. Consequently, philosophers who write about the\nproblem of extrapolation in the context of molecular biology often\nfocus on such models (see, for example, Ankeny 2001; Baetu 2016;\nBechtel and Abrahamsen 2005; Bolker 1995; Burian 1993b; Darden 2007;\nLaFollette and Shanks 1996; Love 2009; Piotrowska 2013; Schaffner\n1986; Steel 2008; Weber 2005; Wimsatt 1998). \nWithin the context of surrogate models, any successful solution to the\nproblem of extrapolation must explain how inferences can be justified\ngiven causally relevant differences between models and their targets\n(Lafollette and Shanks 1996). It must also avoid what Daniel Steel\n(2008) calls the “extrapolator’s circle”, which\narises when attempting to determine whether the model and its target\nare similar enough in casually relevant respects. \nOne way to escape the extrapolator’s circle is to black box the\nmechanisms being compared and instead treat the problem of\nextrapolation as a statistical problem (cf. Cook and Campbell 1979).\nThis method avoids the circle because it eliminates the need to know\nif two mechanisms are similar. All that matters is that two outcomes\nare produced to a statistically significant degree, given the same\nintervention. For this reason, statistically significant outcomes in\nclinical trials are at the top of the evidence hierarchy in biomedical\nresearch (Sackett et al. 1996). One problem with relying merely on\nstatistics to solve the problem of extrapolation, however, is that it\ncannot show that an observed correlation between model and target is\nthe result of intervention and not a confounder.\n\n \nA different strategy for avoiding the extrapolator’s circle is\nto remove the black box and compare the two mechanisms but argue that\nthey do not have to be causally similar at every stage for\nextrapolation to be justified. This approach avoids the circle because\nthe suitability of a model can be established given only partial\ninformation about the target. For example, Steel argues that only the\nstages downstream from the point where the mechanisms in the model and\ntarget are likely to differ need to be compared, since the point where\ndifferences are likely will serve as a bottleneck through which the\neventual outcome must be produced. \nThough promising, criticisms have been raised against Steel’s\nmechanistic approach to extrapolation. One worry, raised by Jeremy\nHowick et al. (2013), is that in order to identify the bottlenecks and\ndownstream differences, we must know more about the target than Steel\nadmits. If what we know about the mechanism in the target exceeds what\nwe know about the mechanism in the model, the extrapolator’s\ncircle will not have been avoided. Another worry with Steel’s\napproach to extrapolation is that it doesn’t avoid the masking\nproblem. According to Julian Reiss (2010), Federica Russo (2010), and\nBrendan Clarke et al. (2014), even if we establish that X\ncauses Y through some mechanism, this doesn’t seem to\neliminate the possibility of there being several paths that link\nX to Y. For example, there may be an upstream difference\nthat affects the outcome but does not pass through the downstream\nstages of the mechanism. (This problem is taken up again below in\n Section 3.4.)\n A third worry with Steel’s approach, which comes from Tudor\nBaetu (2016), is that mechanistic accounts of the experimental model\nof interest “combine data from tens or even hundreds of distinct\nexperimental setups” (956). The resulting big picture account of\nthe experimental model is an aggregate of findings that do not\ndescribe a mechanism that actually exists in any cell or organism.\nInstead, as a number of authors have also pointed out (Huber and Keuck\n2013; Lemoine 2017; Nelson 2013), the mechanism of interest is often\nstipulated first and then verified piecemeal in many different\nexperimental organisms. The result is what Mael Lemoine (2017) has\ncalled a “theoretical chimera”, a hypothesis supported by\nheterogeneous partial models. On the chimera view of extrapolation,\nit’s possible that all one-to-one analogies work, and yet the\naggregate theoretical chimera model fails.  \nWhile theoretical chimeras seem to further complicate Steel’s\nmechanistic account of extrapolation, actual chimeras also raise\nquestions that Steel does not address. Consider, for example,\nhumanized mice that have been engineered to carry a “partial or\ncomplete human physiological system” (Macchiarini et al. 2005:\n1307). These genetically engineered rodents are supposed to make\nextrapolation more reliable by simulating a variety of human diseases,\ne.g., asthma, diabetes, cancer, etc. As Monika Piotrowska (2013)\npoints out, however, this raises a new problem. The question is no\nlonger how an inference from model to target can be justified given\nexisting differences between the two, but rather, in what way should\nthese mice be modified in order to justify extrapolation to\nhumans? Piotrowska has proposed three conditions that should be met in\nthe process of modification to ensure that extrapolation is justified.\nThe first two requirements demand that we keep track of parts and\ntheir boundaries during transfer, which presupposes a mechanistic view\nof human disease, but the third requirement—that the constraints\nthat might prevent the trait from being expressed be\neliminated—highlights the limits of using a mechanistic approach\nwhen making inferences from humanized mice to humans. As Piotrowska\nexplains, \nwithout the right context, even the complete lack of differences\nbetween two mechanisms cannot justify the inference that what is true\nof one mechanism will be true of another (Piotrowska 2013: 453). \nIf Piotrowska is right, Steel’s mechanistic solution to the\nproblem of extrapolation seems to have reached its limit. As our\nability to manipulate biological models advances, philosophers will\nneed to revisit the problem of extrapolation and seek out new\nsolutions. \nThe history of molecular biology is in part the history of\nexperimental techniques designed to probe the macromolecular\nmechanisms found in living things. Philosophers in turn have looked to\nmolecular biology as a case study for understanding how\nexperimentation works in science—how it contributes to\nscientific discovery, distinguishes correlation from causal and\nconstitutive relevance, and decides between competing hypotheses\n(Barwich and Baschir 2017). In all three cases, the concept of a\nmechanism is central to understanding the function of experimentation\nin molecular biology (also see the entry on experimentation in\nbiology). \nTake discovery. Throughout much of the twentieth century, philosophers\nof science treated scientific discovery as if it were off-limits to\nphilosophical analysis; philosophers could evaluate the rational\nprocess of confirmation, the argument went, but the psychological\nprocess of discovery (the “aha!” moment) fell into the\nrealm of creativity (Popper 1965; see the entry on\n scientific discovery).\n Darden has countered with a focus on the strategies that scientists\nemploy to construct, evaluate, and revise mechanical explanations of\nphenomena; on her view, discovery is a piecemeal, incremental, and\niterative process of mechanism elucidation. In the 1950s and 1960s,\nfor example, scientists from both molecular biology and biochemistry\nemployed their own experimental strategies to elucidate the mechanisms\nof protein synthesis that linked DNA to the production of proteins.\nMolecular biologists moved forward from DNA using experimental\ntechniques such as x-ray crystallography and model building to\nunderstand how the structure of DNA dictated what molecules it could\ninteract with; biochemists simultaneously moved backward from the\nprotein products using in vitro experimental systems to\nunderstand the chemical reactions and chemical bonding necessary to\nbuild a protein. They met in the middle at RNA, ultimately leading to\nWatson’s famous mechanism schema DNA → RNA → protein.\nFar from being philosophically inscrutable, Darden points out that the\nmolecular biologists were “forward chaining” while the\nbiochemists were “backward chaining”, using information\nabout the working entities and activities that they knew about to\ninfer what could come next or before in the mechanism of protein\nsynthesis (Darden 2006a: chapters 3, 12; Craver and Darden 2013:\nchapter 10). \nTudor Baetu builds on the contemporary philosophy of mechanism\nliterature as well to provide an account of how different experiments\nin molecular biology move from finding correlations, to establishing\ncausal relevance, to establishing constitutive relevance (Baetu\n2012b). Much recent philosophical attention has been given to the\ntransition from correlation to causal relevance. On a manipulationist\naccount of causal relevance, some factor X is determined to be\ncausally relevant to some outcome Y when interventions on\nX can be shown to produce the change in Y. Experiments\nfrom molecular biology often take this form, which Baetu calls\n“one-variable experiments” because they involve a single\nintervention made on X in the experiment to establish the\ncausal relevance to Y. But these one-variable experiments,\nBaetu cautions, do not necessarily provide information about the\ncausal mechanism that links X to Y. Is X causally\nrelevant to Y by way of mechanism A, mechanism B,\nor some other unknown mechanism? Baetu, drawing on his own\nexperimental research in molecular oncology, shows that scientists\nprobe the mechanical link by performing “two-variable\nexperiments”. In a two-variable experiment, two interventions\nare simultaneously made on the initial factor and some component\npostulated in the mechanical link, thereby establishing both causal\nand constitutive relevance.  \nExperiments from molecular biology have also figured into\nphilosophical discussions about the possibility of “crucial\nexperiments”. An experiment is taken to be a crucial experiment\nif it is devised so as to result in the confirmation of one hypothesis\nby way of refuting other competing hypotheses. But the very idea of a\ncrucial experiment, Pierre Duhem pointed out, assumes that the set of\nknown competing hypotheses contains all possible explanations of a\ngiven phenomenon such that the refutation of all but one of the\nhypotheses deductively ensures the confirmation of the hypothesis left\nstanding. However, if there are in fact unknown alternatives that\nweren’t considered in the set of competing hypotheses, Duhem\nwarned, then the refutation cannot guarantee the confirmation of the\nlast hypothesis standing (also see the entry on Pierre Duhem). (Duhem\nactually raised two problems for crucial experiments—the problem\nmentioned above, as well as the problem of auxiliary assumptions,\nwhich any hypothesis brings with it; for reasons of space, we will\nonly discuss the former here.) Marcel Weber (2009) has utilized a\nfamous experiment from molecular biology to offer a different vision\nof how crucial experiments work. After Watson and Crick discovered the\ndouble helical structure of DNA, molecular biologists turned their\nattention to how that macromolecule could be replicated (see\n Section 1.2\n above). The focus was in part on the fact that the DNA was twisted\ntogether in a helix, and so the challenge was figuring out what\nprocess could unwind and replicate that complexly wound molecule.\nThree competing hypotheses emerged, each with their own prediction\nabout the extent to which newly replicated DNA double helices\ncontained old DNA strands versus newly synthesized material:\nsemi-conservative replication, conservative replication, and\ndispersive replication. Matthew Meselson and Frank Stahl, at Cal Tech,\ndevised a method for testing among these competing hypotheses (see The\nSemi-Conservative Replication of DNA in\n Other Internet Resources).\n They grew E.coli on a medium that contained a unique,\nheavier form of nitrogen which could be taken up into the\nE.coli DNA; then they transferred that E.coli with\nheavy DNA to a medium with the more common, lighter form of nitrogen\nand let the E.coli replicate there. By then taking regular\nsamples of the replicating E.coli and examining the DNA of\nthe subsequent generations, they determined the extent to which the\nnew DNA was a mix of old DNA strands or newly synthesized material.\nThe result was a clear victory for semi-conservative replication, and\nthe Meselson-Stahl experiment became known as the “most\nbeautiful experiment in biology” (Meselson and Stahl 1958;\nHolmes 2001). Weber argues that we should understand the quick uptake\nof Meselson and Stahl’s experimental result as an instance of\ninference to the best explanation (as opposed to Duhem’s\ndeductive characterization). Meselson and Stahl, Weber claims, took\nthe physiological mechanism of DNA replication and then embedded it in\nan “experimental mechanism”; that experimental mechanism\nthen generated the data pattern of heavy-vs-light DNA in subsequent\nE.coli generations. Moreover, any hypothesis of DNA\nreplication had to satisfy mechanistic constraints imposed by what was\nalready known about the physiological mechanism—that DNA was a\ndouble helix, and that the sequence of nucleotides in the DNA needed\nto be preserved in subsequent generations. So Duhem’s concern\nabout unknown alternatives was alleviated because known mechanistic\nconstraints limited the set of possible hypotheses that could generate\nthe phenomenon. On Weber’s reading, the mechanistic constraints\nculled the set of possible hypotheses for DNA replication to\nsemi-conservative replication, conservative replication, and\ndispersive replication; then, among that set, Meselson and Stahl\ndevised an experimental mechanism such that semi-conservative\nreplication was the best explanation of the data pattern they found.\n(For a critique, see Baetu 2017.) \nAn overview of the history of molecular biology revealed the original\nconvergence of geneticists, physicists, and structural chemists on a\ncommon problem: the nature of inheritance. Conceptual and\nmethodological frameworks from each of these disciplinary strands\nunited in the ultimate determination of the double helical structure\nof DNA (conceived of as an informational molecule) along with the\nmechanisms of gene replication, mutation, and expression. With this\nrecent history in mind, philosophers of molecular biology have\nexamined the key concepts of the field: mechanism, information, and\ngene. Moreover, molecular biology has provided cases for addressing\nmore general issues in the philosophy of science, such as reduction,\nexplanation, extrapolation, and experimentation.","contact.mail":"mpiotrowska@albany.edu","contact.domain":"albany.edu"}]
