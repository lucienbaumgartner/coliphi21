[{"date.published":"2015-06-15","date.changed":"2020-08-07","url":"https://plato.stanford.edu/entries/measurement-science/","author1":"Eran Tal","author1.info":"http://sites.google.com/site/erantalphilosophy/","entry":"measurement-science","body.text":"\n\n\nMeasurement is an integral part of modern science as well as of\nengineering, commerce, and daily life. Measurement is often considered\na hallmark of the scientific enterprise and a privileged source of\nknowledge relative to qualitative modes of\n inquiry.[1]\n Despite its ubiquity and importance, there is little consensus among\nphilosophers as to how to define measurement, what sorts of things are\nmeasurable, or which conditions make measurement possible. Most (but\nnot all) contemporary authors agree that measurement is an activity\nthat involves interaction with a concrete system with the aim of\nrepresenting aspects of that system in abstract terms (e.g., in terms\nof classes, numbers, vectors etc.) But this characterization also fits\nvarious kinds of perceptual and linguistic activities that are not\nusually considered measurements, and is therefore too broad to count\nas a definition of measurement. Moreover, if “concrete”\nimplies “real”, this characterization is also too narrow,\nas measurement often involves the representation of ideal systems such\nas the average household or an electron at complete rest.\n\n\nPhilosophers have written on a variety of conceptual, metaphysical,\nsemantic and epistemological issues related to measurement. This entry\nwill survey the central philosophical standpoints on the nature of\nmeasurement, the notion of measurable quantity and related\nepistemological issues. It will refrain from elaborating on the many\ndiscipline-specific problems associated with measurement and focus on\nissues that have a general character.\n\nModern philosophical discussions about measurement—spanning from\nthe late nineteenth century to the present day—may be divided\ninto several strands of scholarship. These strands reflect different\nperspectives on the nature of measurement and the conditions that make\nmeasurement possible and reliable. The main strands are mathematical\ntheories of measurement, operationalism, conventionalism, realism,\ninformation-theoretic accounts and model-based accounts. These strands\nof scholarship do not, for the most part, constitute directly\ncompeting views. Instead, they are best understood as highlighting\ndifferent and complementary aspects of measurement. The following is a\nvery rough overview of these perspectives: \nThese perspectives are in principle consistent with each other. While\nmathematical theories of measurement deal with the mathematical\nfoundations of measurement scales, operationalism and conventionalism\nare primarily concerned with the semantics of quantity terms, realism\nis concerned with the metaphysical status of measurable quantities,\nand information-theoretic and model-based accounts are concerned with\nthe epistemological aspects of measuring. Nonetheless, the subject\ndomain is not as neatly divided as the list above suggests. Issues\nconcerning the metaphysics, epistemology, semantics and mathematical\nfoundations of measurement are interconnected and often bear on one\nanother. Hence, for example, operationalists and conventionalists have\noften adopted anti-realist views, and proponents of model-based\naccounts have argued against the prevailing empiricist interpretation\nof mathematical theories of measurement. These subtleties will become\nclear in the following discussion. \nThe list of strands of scholarship is neither exclusive nor\nexhaustive. It reflects the historical trajectory of the philosophical\ndiscussion thus far, rather than any principled distinction among\ndifferent levels of analysis of measurement. Some philosophical works\non measurement belong to more than one strand, while many other works\ndo not squarely fit either. This is especially the case since the\nearly 2000s, when measurement returned to the forefront of\nphilosophical discussion after several decades of relative neglect.\nThis recent body of scholarship is sometimes called “the\nepistemology of measurement”, and includes a rich array of works\nthat cannot yet be classified into distinct schools of thought. The\nlast section of this entry will be dedicated to surveying some of\nthese developments. \nAlthough the philosophy of measurement formed as a distinct area of\ninquiry only during the second half of the nineteenth century,\nfundamental concepts of measurement such as magnitude and quantity\nhave been discussed since antiquity. According to Euclid’s\nElements, a magnitude—such as a line, a surface or a\nsolid—measures another when the latter is a whole multiple of\nthe former (Book V, def. 1 & 2). Two magnitudes have a common\nmeasure when they are both whole multiples of some magnitude, and are\nincommensurable otherwise (Book X, def. 1). The discovery of\nincommensurable magnitudes allowed Euclid and his contemporaries to\ndevelop the notion of a ratio of magnitudes. Ratios can be\neither rational or irrational, and therefore the concept of ratio is\nmore general than that of measure (Michell 2003, 2004a;\nGrattan-Guinness 1996). \nAristotle distinguished between quantities and qualities. Examples of\nquantities are numbers, lines, surfaces, bodies, time and place,\nwhereas examples of qualities are justice, health, hotness and\npaleness (Categories §6 and §8). According to\nAristotle, quantities admit of equality and inequality but not of\ndegrees, as “one thing is not more four-foot than another”\n(ibid. 6.6a19). Qualities, conversely, do not admit of equality or\ninequality but do admit of degrees, “for one thing is called\nmore pale or less pale than another” (ibid. 8.10b26). Aristotle\ndid not clearly specify whether degrees of qualities such as paleness\ncorrespond to distinct qualities, or whether the same quality,\npaleness, was capable of different intensities. This topic was at the\ncenter of an ongoing debate in the thirteenth and fourteenth centuries\n(Jung 2011). Duns Scotus supported the “addition theory”,\naccording to which a change in the degree of a quality can be\nexplained by the addition or subtraction of smaller degrees of that\nquality (2011: 553). This theory was later refined by Nicole Oresme,\nwho used geometrical figures to represent changes in the intensity of\nqualities such as velocity (Clagett 1968; Sylla 1971). Oresme’s\ngeometrical representations established a subset of qualities that\nwere amenable to quantitative treatment, thereby challenging the\nstrict Aristotelian dichotomy between quantities and qualities. These\ndevelopments made possible the formulation of quantitative laws of\nmotion during the sixteenth and seventeenth centuries (Grant\n1996). \nThe concept of qualitative intensity was further developed by Leibniz\nand Kant. Leibniz’s “principle of continuity” stated\nthat all natural change is produced by degrees. Leibniz argued that\nthis principle applies not only to changes in extended magnitudes such\nas length and duration, but also to intensities of representational\nstates of consciousness, such as sounds (Jorgensen 2009; Diehl 2012).\nKant is thought to have relied on Leibniz’s principle of\ncontinuity to formulate his distinction between extensive and\nintensive magnitudes. According to Kant, extensive magnitudes are\nthose “in which the representation of the parts makes possible\nthe representation of the whole” (1787: A162/B203). An example\nis length: a line can only be mentally represented by a successive\nsynthesis in which parts of the line join to form the whole. For Kant,\nthe possibility of such synthesis was grounded in the forms of\nintuition, namely space and time. Intensive magnitudes, like warmth or\ncolors, also come in continuous degrees, but their apprehension takes\nplace in an instant rather than through a successive synthesis of\nparts. The degrees of intensive magnitudes “can only be\nrepresented through approximation to negation” (1787: A\n168/B210), that is, by imagining their gradual diminution until their\ncomplete absence. \nScientific developments during the nineteenth century challenged the\ndistinction between extensive and intensive magnitudes. Thermodynamics\nand wave optics showed that differences in temperature and hue\ncorresponded to differences in spatio-temporal magnitudes such as\nvelocity and wavelength. Electrical magnitudes such as resistance and\nconductance were shown to be capable of addition and division despite\nnot being extensive in the Kantian sense, i.e., not synthesized from\nspatial or temporal parts. Moreover, early experiments in\npsychophysics suggested that intensities of sensation such as\nbrightness and loudness could be represented as sums of “just\nnoticeable differences” among stimuli, and could therefore be\nthought of as composed of parts (see\n Section 3.3).\n These findings, along with advances in the axiomatization of branches\nof mathematics, motivated some of the leading scientists of the late\nnineteenth century to attempt to clarify the mathematical foundations\nof measurement (Maxwell 1873; von Kries 1882; Helmholtz 1887; Mach\n1896; Poincaré 1898; Hölder 1901; for historical surveys\nsee Darrigol 2003; Michell 1993, 2003; Cantù and Schlaudt 2013;\nBiagioli 2016: Ch. 4, 2018). These works are viewed today as\nprecursors to the body of scholarship known as “measurement\ntheory”. \nMathematical theories of measurement (often referred to collectively\nas “measurement theory”) concern the conditions under\nwhich relations among numbers (and other mathematical entities) can be\nused to express relations among\n objects.[2]\n In order to appreciate the need for mathematical theories of\nmeasurement, consider the fact that relations exhibited by\nnumbers—such as equality, sum, difference and ratio—do not\nalways correspond to relations among the objects measured by those\nnumbers. For example, 60 is twice 30, but one would be mistaken in\nthinking that an object measured at 60 degrees Celsius is twice as hot\nas an object at 30 degrees Celsius. This is because the zero point of\nthe Celsius scale is arbitrary and does not correspond to an absence\nof\n temperature.[3]\n Similarly, numerical intervals do not always carry empirical\ninformation. When subjects are asked to rank on a scale from 1 to 7\nhow strongly they agree with a given statement, there is no prima\nfacie reason to think that the intervals between 5 and 6 and\nbetween 6 and 7 correspond to equal increments of strength of opinion.\nTo provide a third example, equality among numbers is transitive [if\n(a=b & b=c) then a=c] but empirical comparisons among physical\nmagnitudes reveal only approximate equality, which is not a transitive\nrelation. These examples suggest that not all of the mathematical\nrelations among numbers used in measurement are empirically\nsignificant, and that different kinds of measurement scale convey\ndifferent kinds of empirically significant information. \nThe study of measurement scales and the empirical information they\nconvey is the main concern of mathematical theories of measurement. In\nhis seminal 1887 essay, “Counting and Measuring”, Hermann\nvon Helmholtz phrased the key question of measurement theory as\nfollows:  \n[W]hat is the objective meaning of expressing through denominate\nnumbers the relations of real objects as magnitudes, and under what\nconditions can we do this? (1887: 4)  \nBroadly speaking, measurement theory sets out to (i) identify the\nassumptions underlying the use of various mathematical structures for\ndescribing aspects of the empirical world, and (ii) draw lessons about\nthe adequacy and limits of using these mathematical structures for\ndescribing aspects of the empirical world. Following Otto Hölder\n(1901), measurement theorists often tackle these goals through formal\nproofs, with the assumptions in (i) serving as axioms and the lessons\nin (ii) following as theorems. A key insight of measurement theory is\nthat the empirically significant aspects of a given mathematical\nstructure are those that mirror relevant relations among the\nobjects being measured. For example, the relation “bigger\nthan” among numbers is empirically significant for measuring\nlength insofar as it mirrors the relation “longer than”\namong objects. This mirroring, or mapping, of relations between\nobjects and mathematical entities constitutes a measurement scale. As\nwill be clarified below, measurement scales are usually thought of as\nisomorphisms or homomorphisms between objects and mathematical\nentities. \nOther than these broad goals and claims, measurement theory is a\nhighly heterogeneous body of scholarship. It includes works that span\nfrom the late nineteenth century to the present day and endorse a wide\narray of views on the ontology, epistemology and semantics of\nmeasurement. Two main differences among mathematical theories of\nmeasurement are especially worth mentioning. The first concerns the\nnature of the relata, or “objects”, whose\nrelations numbers are supposed to mirror. These relata may be\nunderstood in at least four different ways: as concrete individual\nobjects, as qualitative observations of concrete individual objects,\nas abstract representations of individual objects, or as universal\nproperties of objects. Which interpretation is adopted depends in\nlarge part on the author’s metaphysical and epistemic\ncommitments. This issue will be especially relevant to the discussion\nof realist accounts of measurement\n (Section 5).\n Second, different measurement theorists have taken different stands\non the kind of empirical evidence that is required to establish\nmappings between objects and numbers. As a result, measurement\ntheorists have come to disagree about the necessary conditions for\nestablishing the measurability of attributes, and specifically about\nwhether psychological attributes are measurable. Debates about\nmeasurability have been highly fruitful for the development of\nmeasurement theory, and the following subsections will introduce some\nof these debates and the central concepts developed therein. \nDuring the late nineteenth and early twentieth centuries several\nattempts were made to provide a universal definition of measurement.\nAlthough accounts of measurement varied, the consensus was that\nmeasurement is a method of assigning numbers to magnitudes.\nFor example, Helmholtz (1887: 17) defined measurement as the procedure\nby which one finds the denominate number that expresses the value of a\nmagnitude, where a “denominate number” is a number\ntogether with a unit, e.g., 5 meters, and a magnitude is a quality of\nobjects that is amenable to ordering from smaller to greater, e.g.,\nlength. Bertrand Russell similarly stated that measurement is  \nany method by which a unique and reciprocal correspondence is\nestablished between all or some of the magnitudes of a kind and all or\nsome of the numbers, integral, rational or real. (1903: 176)  \nNorman Campbell defined measurement simply as “the process of\nassigning numbers to represent qualities”, where a quality is a\nproperty that admits of non-arbitrary ordering (1920: 267). \nDefining measurement as numerical assignment raises the question:\nwhich assignments are adequate, and under what conditions? Early\nmeasurement theorists like Helmholtz (1887), Hölder (1901) and\nCampbell (1920) argued that numbers are adequate for expressing\nmagnitudes insofar as algebraic operations among numbers mirror\nempirical relations among magnitudes. For example, the qualitative\nrelation “longer than” among rigid rods is (roughly)\ntransitive and asymmetrical, and in this regard shares structural\nfeatures with the relation “larger than” among numbers.\nMoreover, the end-to-end concatenation of rigid rods shares structural\nfeatures—such as associativity and commutativity—with the\nmathematical operation of addition. A similar situation holds for the\nmeasurement of weight with an equal-arms balance. Here deflection of\nthe arms provides ordering among weights and the heaping of weights on\none pan constitutes concatenation. \nEarly measurement theorists formulated axioms that describe these\nqualitative empirical structures, and used these axioms to prove\ntheorems about the adequacy of assigning numbers to magnitudes that\nexhibit such structures. Specifically, they proved that ordering and\nconcatenation are together sufficient for the construction of an\nadditive numerical representation of the relevant magnitudes.\nAn additive representation is one in which addition is empirically\nmeaningful, and hence also multiplication, division etc. Campbell\ncalled measurement procedures that satisfy the conditions of\nadditivity “fundamental” because they do not involve the\nmeasurement of any other magnitude (1920: 277). Kinds of magnitudes\nfor which a fundamental measurement procedure has been\nfound—such as length, area, volume, duration, weight and\nelectrical resistance—Campbell called “fundamental\nmagnitudes”. A hallmark of such magnitudes is that it is\npossible to generate them by concatenating a standard sequence of\nequal units, as in the example of a series of equally spaced marks on\na ruler. \nAlthough they viewed additivity as the hallmark of measurement, most\nearly measurement theorists acknowledged that additivity is not\nnecessary for measuring. Other magnitudes exist that admit of ordering\nfrom smaller to greater, but whose ratios and/or differences cannot\ncurrently be determined except through their relations to other,\nfundamentally measurable magnitudes. Examples are temperature, which\nmay be measured by determining the volume of a mercury column, and\ndensity, which may be measured as the ratio of mass and volume. Such\nindirect determination came to be called “derived”\nmeasurement and the relevant magnitudes “derived\nmagnitudes” (Campbell 1920: 275–7). \nAt first glance, the distinction between fundamental and derived\nmeasurement may seem reminiscent of the distinction between extensive\nand intensive magnitudes, and indeed fundamental measurement is\nsometimes called “extensive”. Nonetheless, it is important\nto note that the two distinctions are based on significantly different\ncriteria of measurability. As discussed in\n Section 2,\n the extensive-intensive distinction focused on the intrinsic\nstructure of the quantity in question, i.e., whether or not it is\ncomposed of spatio-temporal parts. The fundamental-derived\ndistinction, by contrast, focuses on the properties of measurement\noperations. A fundamentally measurable magnitude is one for\nwhich a fundamental measurement operation has been found.\nConsequently, fundamentality is not an intrinsic property of a\nmagnitude: a derived magnitude can become fundamental with the\ndiscovery of new operations for its measurement. Moreover, in\nfundamental measurement the numerical assignment need not mirror the\nstructure of spatio-temporal parts. Electrical resistance, for\nexample, can be fundamentally measured by connecting resistors in a\nseries (Campbell 1920: 293). This is considered a fundamental\nmeasurement operation because it has a shared structure with numerical\naddition, even though objects with equal resistance are not generally\nequal in size. \nThe distinction between fundamental and derived measurement was\nrevised by subsequent authors. Brian Ellis (1966: Ch. 5–8)\ndistinguished among three types of measurement: fundamental,\nassociative and derived. Fundamental measurement requires ordering and\nconcatenation operations satisfying the same conditions specified by\nCampbell. Associative measurement procedures are based on a\ncorrelation of two ordering relationships, e.g., the correlation\nbetween the volume of a mercury column and its temperature. Derived\nmeasurement procedures consist in the determination of the value of a\nconstant in a physical law. The constant may be local, as in the\ndetermination of the specific density of water from mass and volume,\nor universal, as in the determination of the Newtonian gravitational\nconstant from force, mass and distance. Henry Kyburg (1984: Ch.\n5–7) proposed a somewhat different threefold distinction among\ndirect, indirect and systematic measurement, which does not completely\noverlap with that of\n Ellis.[4]\n A more radical revision of the distinction between fundamental and\nderived measurement was offered by R. Duncan Luce and John Tukey\n(1964) in their work on conjoint measurement, which will be discussed\nin\n Section 3.4. \nThe previous subsection discussed the axiomatization of empirical\nstructures, a line of inquiry that dates back to the early days of\nmeasurement theory. A complementary line of inquiry within measurement\ntheory concerns the classification of measurement scales. The\npsychophysicist S.S. Stevens (1946, 1951) distinguished among four\ntypes of scales: nominal, ordinal, interval and ratio. Nominal scales\nrepresent objects as belonging to classes that have no particular\norder, e.g., male and female. Ordinal scales represent order but no\nfurther algebraic structure. For example, the Mohs scale of mineral\nhardness represents minerals with numbers ranging from 1 (softest) to\n10 (hardest), but there is no empirical significance to equality among\nintervals or ratios of those\n numbers.[5]\n Celsius and Fahrenheit are examples of interval scales: they\nrepresent equality or inequality among intervals of temperature, but\nnot ratios of temperature, because their zero points are arbitrary.\nThe Kelvin scale, by contrast, is a ratio scale, as are the familiar\nscales representing mass in kilograms, length in meters and duration\nin seconds. Stevens later refined this classification and\ndistinguished between linear and logarithmic interval scales (1959:\n31–34) and between ratio scales with and without a natural unit\n(1959: 34). Ratio scales with a natural unit, such as those used for\ncounting discrete objects and for representing probabilities, were\nnamed “absolute” scales. \nAs Stevens notes, scale types are individuated by the families of\ntransformations they can undergo without loss of empirical\ninformation. Empirical relations represented on ratio scales, for\nexample, are invariant under multiplication by a positive number,\ne.g., multiplication by 2.54 converts from inches to centimeters.\nLinear interval scales allow both multiplication by a positive number\nand a constant shift, e.g., the conversion from Celsius to Fahrenheit\nin accordance with the formula °C × 9/5 + 32 = °F.\nOrdinal scales admit of any transformation function as long as it is\nmonotonic and increasing, and nominal scales admit of any one-to-one\nsubstitution. Absolute scales admit of no transformation other than\nidentity. Stevens’ classification of scales was later\ngeneralized by Louis Narens (1981, 1985: Ch. 2) and Luce et al. (1990:\nCh. 20) in terms of the homogeneity and uniqueness of the relevant\ntransformation groups. \nWhile Stevens’ classification of scales met with general\napproval in scientific and philosophical circles, its wider\nimplications for measurement theory became the topic of considerable\ndebate. Two issues were especially contested. The first was whether\nclassification and ordering operations deserve to be called\n“measurement” operations, and accordingly whether the\nrepresentation of magnitudes on nominal and ordinal scales should\ncount as measurement. Several physicists, including Campbell, argued\nthat classification and ordering operations did not provide a\nsufficiently rich structure to warrant the use of numbers, and hence\nshould not count as measurement operations. The second contested issue\nwas whether a concatenation operation had to be found for a magnitude\nbefore it could be fundamentally measured on a ratio scale. The debate\nbecame especially heated when it re-ignited a longer controversy\nsurrounding the measurability of intensities of sensation. It is to\nthis debate we now turn. \nOne of the main catalysts for the development of mathematical theories\nof measurement was an ongoing debate surrounding measurability in\npsychology. The debate is often traced back to Gustav Fechner’s\n(1860) Elements of Psychophysics, in which he described a\nmethod of measuring intensities of sensation. Fechner’s method\nwas based on the recording of “just noticeable\ndifferences” between sensations associated with pairs of\nstimuli, e.g., two sounds of different intensity. These differences\nwere assumed to be equal increments of intensity of sensation. As\nFechner showed, under this assumption a stable linear relationship is\nrevealed between the intensity of sensation and the logarithm of the\nintensity of the stimulus, a relation that came to be known as\n“Fechner’s law” (Heidelberger 1993a: 203; Luce and\nSuppes 2004: 11–2). This law in turn provides a method for\nindirectly measuring the intensity of sensation by measuring the\nintensity of the stimulus, and hence, Fechner argued, provides\njustification for measuring intensities of sensation on the real\nnumbers. \nFechner’s claims concerning the measurability of sensation\nbecame the subject of a series of debates that lasted nearly a century\nand proved extremely fruitful for the philosophy of measurement,\ninvolving key figures such as Mach, Helmholtz, Campbell and Stevens\n(Heidelberger 1993a: Ch. 6 and 1993b; Michell 1999: Ch. 6). Those\nobjecting to the measurability of sensation, such as Campbell,\nstressed the necessity of an empirical concatenation operation for\nfundamental measurement. Since intensities of sensation cannot be\nconcatenated to each other in the manner afforded by lengths and\nweights, there could be no fundamental measurement of sensation\nintensity. Moreover, Campbell claimed that none of the psychophysical\nregularities discovered thus far are sufficiently universal to count\nas laws in the sense required for derived measurement (Campbell in\nFerguson et al. 1940: 347). All that psychophysicists have shown is\nthat intensities of sensation can be consistently ordered, but order\nby itself does not yet warrant the use of numerical relations such as\nsums and ratios to express empirical results. \nThe central opponent of Campbell in this debate was Stevens, whose\ndistinction between types of measurement scale was discussed above.\nStevens defined measurement as the “assignment of numerals to\nobjects or events according to rules” (1951: 1) and claimed that\nany consistent and non-random assignment counts as measurement in the\nbroad sense (1975: 47). In useful cases of scientific inquiry, Stevens\nclaimed, measurement can be construed somewhat more narrowly as a\nnumerical assignment that is based on the results of matching\noperations, such as the coupling of temperature to mercury volume or\nthe matching of sensations to each other. Stevens argued against the\nview that relations among numbers need to mirror qualitative empirical\nstructures, claiming instead that measurement scales should be\nregarded as arbitrary formal schemas and adopted in accordance with\ntheir usefulness for describing empirical data. For example, adopting\na ratio scale for measuring the sensations of loudness, volume and\ndensity of sounds leads to the formulation of a simple linear relation\namong the reports of experimental subjects: loudness = volume ×\ndensity (1975: 57–8). Such assignment of numbers to sensations\ncounts as measurement because it is consistent and non-random, because\nit is based on the matching operations performed by experimental\nsubjects, and because it captures regularities in the experimental\nresults. According to Stevens, these conditions are together\nsufficient to justify the use of a ratio scale for measuring\nsensations, despite the fact that “sensations cannot be\nseparated into component parts, or laid end to end like measuring\nsticks” (1975: 38; see also Hempel 1952: 68–9). \nIn the mid-twentieth century the two main lines of inquiry in\nmeasurement theory, the one dedicated to the empirical conditions of\nquantification and the one concerning the classification of scales,\nconverged in the work of Patrick Suppes (1951; Scott and Suppes 1958;\nfor historical surveys see Savage and Ehrlich 1992; Diez 1997a,b).\nSuppes’ work laid the basis for the Representational Theory of\nMeasurement (RTM), which remains the most influential mathematical\ntheory of measurement to date (Krantz et al. 1971; Suppes et al. 1989;\nLuce et al. 1990). RTM defines measurement as the construction of\nmappings from empirical relational structures into numerical\nrelational structures (Krantz et al. 1971: 9). An empirical relational\nstructure consists of a set of empirical objects (e.g., rigid rods)\nalong with certain qualitative relations among them (e.g., ordering,\nconcatenation), while a numerical relational structure consists of a\nset of numbers (e.g., real numbers) and specific mathematical\nrelations among them (e.g., “equal to or bigger than”,\naddition). Simply put, a measurement scale is a many-to-one\nmapping—a homomorphism—from an empirical to a numerical\nrelational structure, and measurement is the construction of\n scales.[6]\n RTM goes into great detail in clarifying the assumptions underlying\nthe construction of different types of measurement scales. Each type\nof scale is associated with a set of assumptions about the qualitative\nrelations obtaining among objects represented on that type of scale.\nFrom these assumptions, or axioms, the authors of RTM derive the\nrepresentational adequacy of each scale type, as well as the family of\npermissible transformations making that type of scale unique. In this\nway RTM provides a conceptual link between the empirical basis of\nmeasurement and the typology of\n scales.[7] \nOn the issue of measurability, the Representational Theory takes a\nmiddle path between the liberal approach adopted by Stevens and the\nstrict emphasis on concatenation operations espoused by Campbell. Like\nCampbell, RTM accepts that rules of quantification must be grounded in\nknown empirical structures and should not be chosen arbitrarily to fit\nthe data. However, RTM rejects the idea that additive scales are\nadequate only when concatenation operations are available (Luce and\nSuppes 2004: 15). Instead, RTM argues for the existence of fundamental\nmeasurement operations that do not involve concatenation. The central\nexample of this type of operation is known as “additive conjoint\nmeasurement” (Luce and Tukey 1964; Krantz et al. 1971:\n17–21 and Ch. 6–7). Here, measurements of two or more\ndifferent types of attribute, such as the temperature and pressure of\na gas, are obtained by observing their joint effect, such as the\nvolume of the gas. Luce and Tukey showed that by establishing certain\nqualitative relations among volumes under variations of temperature\nand pressure, one can construct additive representations of\ntemperature and pressure, without invoking any antecedent method of\nmeasuring volume. This sort of procedure is generalizable to any\nsuitably related triplet of attributes, such as the loudness,\nintensity and frequency of pure tones, or the preference for a reward,\nit size and the delay in receiving it (Luce and Suppes 2004: 17). The\ndiscovery of additive conjoint measurement led the authors of RTM to\ndivide fundamental measurement into two kinds: traditional measurement\nprocedures based on concatenation operations, which they called\n“extensive measurement”, and conjoint or\n“nonextensive” fundamental measurement. Under this new\nconception of fundamentality, all the traditional physical attributes\ncan be measured fundamentally, as well as many psychological\nattributes (Krantz et al. 1971: 502–3). \nAbove we saw that mathematical theories of measurement are primarily\nconcerned with the mathematical properties of measurement scales and\nthe conditions of their application. A related but distinct strand of\nscholarship concerns the meaning and use of quantity terms. Scientific\ntheories and models are commonly expressed in terms of quantitative\nrelations among parameters, bearing names such as\n“length”, “unemployment rate” and\n“introversion”. A realist about one of these terms would\nargue that it refers to a set of properties or relations that exist\nindependently of being measured. An operationalist or conventionalist\nwould argue that the way such quantity-terms apply to concrete\nparticulars depends on nontrivial choices made by humans, and\nspecifically on choices that have to do with the way the relevant\nquantity is measured. Note that under this broad construal, realism is\ncompatible with operationalism and conventionalism. That is, it is\nconceivable that choices of measurement method regulate the use of a\nquantity-term and that, given the correct choice, this term\nsucceeds in referring to a mind-independent property or relation.\nNonetheless, many operationalists and conventionalists adopted\nstronger views, according to which there are no facts of the matter as\nto which of several and nontrivially different operations is correct\nfor applying a given quantity-term. These stronger variants are\ninconsistent with realism about measurement. This section will be\ndedicated to operationalism and conventionalism, and the next to\nrealism about measurement. \nOperationalism (or “operationism”) about measurement is\nthe view that the meaning of quantity-concepts is determined by the\nset of operations used for their measurement. The strongest expression\nof operationalism appears in the early work of Percy Bridgman (1927),\nwho argued that  \nwe mean by any concept nothing more than a set of operations; the\nconcept is synonymous with the corresponding set of operations. (1927:\n5)  \nLength, for example, would be defined as the result of the operation\nof concatenating rigid rods. According to this extreme version of\noperationalism, different operations measure different quantities.\nLength measured by using rulers and by timing electromagnetic pulses\nshould, strictly speaking, be distinguished into two distinct\nquantity-concepts labeled “length-1” and\n“length-2” respectively. This conclusion led Bridgman to\nclaim that currently accepted quantity concepts have\n“joints” where different operations overlap in their\ndomain of application. He warned against dogmatic faith in the unity\nof quantity concepts across these “joints”, urging instead\nthat unity be checked against experiments whenever the application of\na quantity-concept is to be extended into a new domain. Nevertheless,\nBridgman conceded that as long as the results of different operations\nagree within experimental error it is pragmatically justified to label\nthe corresponding quantities with the same name (1927:\n 16).[8] \nOperationalism became influential in psychology, where it was\nwell-received by behaviorists like Edwin Boring (1945) and B.F.\nSkinner (1945). Indeed, Skinner maintained that behaviorism is\n“nothing more than a thoroughgoing operational analysis of\ntraditional mentalistic concepts” (1945: 271). Stevens, who was\nBoring’s student, was a key promoter of operationalism in\npsychology, and argued that psychological concepts have empirical\nmeaning only if they stand for definite and concrete operations (1935:\n517; see also Isaac 2017). The idea that concepts are defined by\nmeasurement operations is consistent with Stevens’ liberal views\non measurability, which were discussed above\n (Section 3.3).\n As long as the assignment of numbers to objects is performed in\naccordance with concrete and consistent rules, Stevens maintained that\nsuch assignment has empirical meaning and does not need to satisfy any\nadditional constraints. Nonetheless, Stevens probably did not embrace\nan anti-realist view about psychological attributes. Instead, there\nare good reasons to think that he understood operationalism as a\nmethodological attitude that was valuable to the extent that it\nallowed psychologists to justify the conclusions they drew from\nexperiments (Feest 2005). For example, Stevens did not treat\noperational definitions as a priori but as amenable to\nimprovement in light of empirical discoveries, implying that he took\npsychological attributes to exist independently of such definitions\n(Stevens 1935: 527). This suggests that Stevens’ operationalism\nwas of a more moderate variety than that found in the early writings\nof\n Bridgman.[9] \nOperationalism met with initial enthusiasm by logical positivists, who\nviewed it as akin to verificationism. Nonetheless, it was soon\nrevealed that any attempt to base a theory of meaning on\noperationalist principles was riddled with problems. Among such\nproblems were the automatic reliability operationalism conferred on\nmeasurement operations, the ambiguities surrounding the notion of\noperation, the overly restrictive operational criterion of\nmeaningfulness, and the fact that many useful theoretical concepts\nlack clear operational definitions (Chang\n 2009).[10]\n In particular, Carl Hempel (1956, 1966) criticized operationalists\nfor being unable to define dispositional terms such as\n“solubility in water”, and for multiplying the number of\nscientific concepts in a manner that runs against the need for\nsystematic and simple theories. Accordingly, most writers on the\nsemantics of quantity-terms have avoided espousing an operational\n analysis.[11] \nA more widely advocated approach admitted a conventional element to\nthe use of quantity-terms, while resisting attempts to reduce the\nmeaning of quantity terms to measurement operations. These accounts\nare classified under the general heading\n“conventionalism”, though they differ in the particular\naspects of measurement they deem conventional and in the degree of\narbitrariness they ascribe to such\n conventions.[12]\n An early precursor of conventionalism was Ernst Mach, who examined\nthe notion of equality among temperature intervals (1896: 52). Mach\nnoted that different types of thermometric fluid expand at different\n(and nonlinearly related) rates when heated, raising the question:\nwhich fluid expands most uniformly with temperature? According to\nMach, there is no fact of the matter as to which fluid expands more\nuniformly, since the very notion of equality among temperature\nintervals has no determinate application prior to a conventional\nchoice of standard thermometric fluid. Mach coined the term\n“principle of coordination” for this sort of\nconventionally chosen principle for the application of a quantity\nconcept. The concepts of uniformity of time and space received similar\ntreatments by Henri Poincaré (1898, 1902: Part 2).\nPoincaré argued that procedures used to determine equality\namong durations stem from scientists’ unconscious preference for\ndescriptive simplicity, rather than from any fact about nature.\nSimilarly, scientists’ choice to represent space with either\nEuclidean or non-Euclidean geometries is not determined by experience\nbut by considerations of convenience. \nConventionalism with respect to measurement reached its most\nsophisticated expression in logical positivism. Logical positivists\nlike Hans Reichenbach and Rudolf Carnap proposed “coordinative\ndefinitions” or “correspondence rules” as the\nsemantic link between theoretical and observational terms. These a\npriori, definition-like statements were intended to regulate the\nuse of theoretical terms by connecting them with empirical procedures\n(Reichenbach 1927: 14–19; Carnap 1966: Ch. 24). An example of a\ncoordinative definition is the statement: “a measuring rod\nretains its length when transported”. According to Reichenbach,\nthis statement cannot be empirically verified, because a universal and\nexperimentally undetectable force could exist that equally distorts\nevery object’s length when it is transported. In accordance with\nverificationism, statements that are unverifiable are neither true nor\nfalse. Instead, Reichenbach took this statement to expresses an\narbitrary rule for regulating the use of the concept of equality of\nlength, namely, for determining whether particular instances of length\nare equal (Reichenbach 1927: 16). At the same time, coordinative\ndefinitions were not seen as replacements, but rather as necessary\nadditions, to the familiar sort of theoretical definitions of concepts\nin terms of other concepts (1927: 14). Under the conventionalist\nviewpoint, then, the specification of measurement operations did not\nexhaust the meaning of concepts such as length or length-equality,\nthereby avoiding many of the problems associated with\n operationalism.[13] \nRealists about measurement maintain that measurement is best\nunderstood as the empirical estimation of an objective property or\nrelation. A few clarificatory remarks are in order with respect to\nthis characterization of measurement. First, the term\n“objective” is not meant to exclude mental properties or\nrelations, which are the objects of psychological measurement. Rather,\nmeasurable properties or relations are taken to be objective inasmuch\nas they are independent of the beliefs and conventions of the humans\nperforming the measurement and of the methods used for measuring. For\nexample, a realist would argue that the ratio of the length of a given\nsolid rod to the standard meter has an objective value regardless of\nwhether and how it is measured. Second, the term\n“estimation” is used by realists to highlight the fact\nthat measurement results are mere approximations of true\nvalues (Trout 1998: 46). Third, according to realists, measurement is\naimed at obtaining knowledge about properties and relations, rather\nthan at assigning values directly to individual objects. This is\nsignificant because observable objects (e.g., levers, chemical\nsolutions, humans) often instantiate measurable properties and\nrelations that are not directly observable (e.g., amount of mechanical\nwork, more acidic than, intelligence). Knowledge claims about such\nproperties and relations must presuppose some background theory. By\nshifting the emphasis from objects to properties and relations,\nrealists highlight the theory-laden character of measurements. \nRealism about measurement should not be confused with realism about\nentities (e.g., electrons). Nor does realism about measurement\nnecessarily entail realism about properties (e.g., temperature), since\none could in principle accept only the reality of relations (e.g.,\nratios among quantities) without embracing the reality of underlying\nproperties. Nonetheless, most philosophers who have defended realism\nabout measurement have done so by arguing for some form of realism\nabout properties (Byerly and Lazara 1973; Swoyer 1987; Mundy 1987;\nTrout 1998, 2000). These realists argue that at least some measurable\nproperties exist independently of the beliefs and conventions of the\nhumans who measure them, and that the existence and structure of these\nproperties provides the best explanation for key features of\nmeasurement, including the usefulness of numbers in expressing\nmeasurement results and the reliability of measuring instruments. \nFor example, a typical realist about length measurement would argue\nthat the empirical regularities displayed by individual objects’\nlengths when they are ordered and concatenated are best explained by\nassuming that length is an objective property that has an extensive\nstructure (Swoyer 1987: 271–4). That is, relations among lengths\nsuch as “longer than” and “sum of” exist\nindependently of whether any objects happen to be ordered and\nconcatenated by humans, and indeed independently of whether objects of\nsome particular length happen to exist at all. The existence of an\nextensive property structure means that lengths share much of their\nstructure with the positive real numbers, and this explains the\nusefulness of the positive reals in representing lengths. Moreover, if\nmeasurable properties are analyzed in dispositional terms, it becomes\neasy to explain why some measuring instruments are reliable. For\nexample, if one assumes that a certain amount of electric current in a\nwire entails a disposition to deflect an ammeter needle by a certain\nangle, it follows that the ammeter’s indications\ncounterfactually depend on the amount of electric current in the wire,\nand therefore that the ammeter is reliable (Trout 1998: 65). \nA different argument for realism about measurement is due to Joel\nMichell (1994, 2005), who proposes a realist theory of number based on\nthe Euclidean concept of ratio. According to Michell, numbers are\nratios between quantities, and therefore exist in space and time.\nSpecifically, real numbers are ratios between pairs of\ninfinite standard sequences, e.g., the sequence of lengths normally\ndenoted by “1 meter”, “2 meters” etc. and the\nsequence of whole multiples of the length we are trying to measure.\nMeasurement is the discovery and estimation of such ratios. An\ninteresting consequence of this empirical realism about numbers is\nthat measurement is not a representational activity, but rather the\nactivity of approximating mind-independent numbers (Michell 1994:\n400). \nRealist accounts of measurement are largely formulated in opposition\nto strong versions of operationalism and conventionalism, which\ndominated philosophical discussions of measurement from the 1930s\nuntil the 1960s. In addition to the drawbacks of operationalism\nalready discussed in the previous section, realists point out that\nanti-realism about measurable quantities fails to make sense of\nscientific practice. If quantities had no real values independently of\none’s choice of measurement procedure, it would be difficult to\nexplain what scientists mean by “measurement accuracy” and\n“measurement error”, and why they try to increase accuracy\nand diminish error. By contrast, realists can easily make sense of the\nnotions of accuracy and error in terms of the distance between real\nand measured values (Byerly and Lazara 1973: 17–8; Swoyer 1987:\n239; Trout 1998: 57). A closely related point is the fact that newer\nmeasurement procedures tend to improve on the accuracy of older ones.\nIf choices of measurement procedure were merely conventional it would\nbe difficult to make sense of such progress. In addition, realism\nprovides an intuitive explanation for why different measurement\nprocedures often yield similar results, namely, because they are\nsensitive to the same facts (Swoyer 1987: 239; Trout 1998: 56).\nFinally, realists note that the construction of measurement apparatus\nand the analysis of measurement results are guided by theoretical\nassumptions concerning causal relationships among quantities. The\nability of such causal assumptions to guide measurement suggests that\nquantities are ontologically prior to the procedures that measure\n them.[14] \nWhile their stance towards operationalism and conventionalism is\nlargely critical, realists are more charitable in their assessment of\nmathematical theories of measurement. Brent Mundy (1987) and Chris\nSwoyer (1987) both accept the axiomatic treatment of measurement\nscales, but object to the empiricist interpretation given to the\naxioms by prominent measurement theorists like Campbell (1920) and\nErnest Nagel (1931; Cohen and Nagel 1934: Ch. 15). Rather than\ninterpreting the axioms as pertaining to concrete objects or to\nobservable relations among such objects, Mundy and Swoyer reinterpret\nthe axioms as pertaining to universal magnitudes, e.g., to the\nuniversal property of being 5 meter long rather than to the concrete\ninstantiations of that property. This construal preserves the\nintuition that statements like “the size of x is twice\nthe size of y” are first and foremost about two\nsizes, and only derivatively about the objects x and\ny themselves (Mundy 1987:\n 34).[15]\n Mundy and Swoyer argue that their interpretation is more general,\nbecause it logically entails all the first-order consequences of the\nempiricist interpretation along with additional, second-order claims\nabout universal magnitudes. Moreover, under their interpretation\nmeasurement theory becomes a genuine scientific theory, with\nexplanatory hypotheses and testable predictions. Building on this\nwork, Jo Wolff (2020a) has recently proposed a novel realist account\nof quantities that relies on the Representational Theory of\nMeasurement. According to Wolff’s structuralist theory of\nquantity, quantitative attributes are relational structures.\nSpecifically, an attribute is quantitative if its structure has\ntranslations that form an Archimedean ordered group. Wolff’s\nfocus on translations, rather than on specific relations such as\nconcatenation and ordering, means that quantitativeness can be\nrealized in multiple ways and is not restricted to extensive\nstructures. It also means that being a quantity does not have anything\nspecial to do with numbers, as both numerical and non-numerical\nstructures can be quantitative. \nInformation-theoretic accounts of measurement are based on an analogy\nbetween measuring systems and communication systems. In a simple\ncommunication system, a message (input) is encoded into a signal at\nthe transmitter’s end, sent to the receiver’s end, and\nthen decoded back (output). The accuracy of the transmission depends\non features of the communication system as well as on features of the\nenvironment, i.e., the level of background noise. Similarly, measuring\ninstruments can be thought of as “information machines”\n(Finkelstein 1977) that interact with an object in a given state\n(input), encode that state into an internal signal, and convert that\nsignal into a reading (output). The accuracy of a measurement\nsimilarly depends on the instrument as well as on the level of noise\nin its environment. Conceived as a special sort of information\ntransmission, measurement becomes analyzable in terms of the\nconceptual apparatus of information theory (Hartley 1928; Shannon\n1948; Shannon and Weaver 1949). For example, the information that\nreading \\(y_i\\) conveys about the occurrence of a state \\(x_k\\) of the\nobject can be quantified as \\(\\log \\left[\\frac{p(x_k \\mid\ny_i)}{p(x_k)}\\right]\\), namely as a function of the decrease of\nuncertainty about the object’s state (Finkelstein 1975: 222; for\nalternative formulations see Brillouin 1962: Ch. 15; Kirpatovskii\n1974; and Mari 1999: 185). \nLudwik Finkelstein (1975, 1977) and Luca Mari (1999) suggested the\npossibility of a synthesis between Shannon-Weaver information theory\nand measurement theory. As they argue, both theories centrally appeal\nto the idea of mapping: information theory concerns the mapping\nbetween symbols in the input and output messages, while measurement\ntheory concerns the mapping between objects and numbers. If\nmeasurement is taken to be analogous to symbol-manipulation, then\nShannon-Weaver theory could provide a formalization of the syntax of\nmeasurement while measurement theory could provide a formalization of\nits semantics. Nonetheless, Mari (1999: 185) also warns that the\nanalogy between communication and measurement systems is limited.\nWhereas a sender’s message can be known with arbitrary precision\nindependently of its transmission, the state of an object cannot be\nknown with arbitrary precision independently of its measurement. \nInformation-theoretic accounts of measurement were originally\ndeveloped by metrologists — experts in physical measurement and\nstandardization — with little involvement from philosophers.\nIndependently of developments in metrology, Bas van Fraassen (2008:\n141–185) has recently proposed a conception of measurement in\nwhich information plays a key role. He views measurement as composed\nof two levels: on the physical level, the measuring apparatus\ninteracts with an object and produces a reading, e.g., a pointer\n position.[16]\n On the abstract level, background theory represents the\nobject’s possible states on a parameter space. Measurement\nlocates an object on a sub-region of this abstract parameter space,\nthereby reducing the range of possible states (2008: 164 and 172).\nThis reduction of possibilities amounts to the collection of\ninformation about the measured object. Van Fraassen’s analysis\nof measurement differs from information-theoretic accounts developed\nin metrology in its explicit appeal to background theory, and in the\nfact that it does not invoke the symbolic conception of information\ndeveloped by Shannon and Weaver. \nSince the early 2000s a new wave of philosophical scholarship has\nemerged that emphasizes the relationships between measurement and\ntheoretical and statistical modeling (Morgan 2001; Boumans 2005a,\n2015; Mari 2005b; Mari and Giordani 2013; Tal 2016, 2017; Parker 2017;\nMiyake 2017). According to model-based accounts, measurement consists\nof two levels: (i) a concrete process involving interactions between\nan object of interest, an instrument, and the environment; and (ii) a\ntheoretical and/or statistical model of that process, where\n“model” denotes an abstract and local representation\nconstructed from simplifying assumptions. The central goal of\nmeasurement according to this view is to assign values to one or more\nparameters of interest in the model in a manner that satisfies certain\nepistemic desiderata, in particular coherence and consistency. \nModel-based accounts have been developed by studying measurement\npractices in the sciences, and particularly in metrology. Metrology,\nofficially defined as the “science of measurement and its\napplication” (JCGM 2012: 2.2), is a field of study concerned\nwith the design, maintenance and improvement of measuring instruments\nin the natural sciences and engineering. Metrologists typically work\nat standardization bureaus or at specialized laboratories that are\nresponsible for the calibration of measurement equipment, the\ncomparison of standards and the evaluation of measurement\nuncertainties, among other tasks. It is only recently that\nphilosophers have begun to engage with the rich conceptual issues\nunderlying metrological practice, and particularly with the inferences\ninvolved in evaluating and improving the accuracy of measurement\nstandards (Chang 2004; Boumans 2005a: Chap. 5, 2005b, 2007a; Frigerio\net al. 2010; Teller 2013, 2018; Riordan 2015; Schlaudt and Huber 2015;\nTal 2016a, 2018; Mitchell et al. 2017; Mößner and Nordmann\n2017; de Courtenay et al. 2019). \nA central motivation for the development of model-based accounts is\nthe attempt to clarify the epistemological principles underlying\naspects of measurement practice. For example, metrologists employ a\nvariety of methods for the calibration of measuring instruments, the\nstandardization and tracing of units and the evaluation of\nuncertainties (for a discussion of metrology, see the previous\nsection). Traditional philosophical accounts such as mathematical\ntheories of measurement do not elaborate on the assumptions, inference\npatterns, evidential grounds or success criteria associated with such\nmethods. As Frigerio et al. (2010) argue, measurement theory is\nill-suited for clarifying these aspects of measurement because it\nabstracts away from the process of measurement and focuses solely on\nthe mathematical properties of scales. By contrast, model-based\naccounts take scale construction to be merely one of several tasks\ninvolved in measurement, alongside the definition of measured\nparameters, instrument design and calibration, object sampling and\npreparation, error detection and uncertainty evaluation, among others\n(2010: 145–7). \nAccording to model-based accounts, measurement involves interaction\nbetween an object of interest (the “system under\nmeasurement”), an instrument (the “measurement\nsystem”) and an environment, which includes the measuring\nsubjects. Other, secondary interactions may also be relevant for the\ndetermination of a measurement outcome, such as the interaction\nbetween the measuring instrument and the reference standards used for\nits calibration, and the chain of comparisons that trace the reference\nstandard back to primary measurement standards (Mari 2003: 25).\nMeasurement proceeds by representing these interactions with a set of\nparameters, and assigning values to a subset of those parameters\n(known as “measurands”) based on the results of the\ninteractions. When measured parameters are numerical they are called\n“quantities”. Although measurands need not be quantities,\na quantitative measurement scenario will be supposed in what\nfollows. \nTwo sorts of measurement outputs are distinguished by model-based\naccounts [JCGM 2012: 2.9 & 4.1; Giordani and Mari 2012: 2146; Tal\n2013]: \nAs proponents of model-based accounts stress, inferences from\ninstrument indications to measurement outcomes are nontrivial and\ndepend on a host of theoretical and statistical assumptions about the\nobject being measured, the instrument, the environment and the\ncalibration process. Measurement outcomes are often obtained through\nstatistical analysis of multiple indications, thereby involving\nassumptions about the shape of the distribution of indications and the\nrandomness of environmental effects (Bogen and Woodward 1988:\n307–310). Measurement outcomes also incorporate corrections for\nsystematic effects, and such corrections are based on theoretical\nassumptions concerning the workings of the instrument and its\ninteractions with the object and environment. For example, length\nmeasurements need to be corrected for the change of the measuring\nrod’s length with temperature, a correction which is derived\nfrom a theoretical equation of thermal expansion. Systematic\ncorrections involve uncertainties of their own, for example in the\ndetermination of the values of constants, and these uncertainties are\nassessed through secondary experiments involving further theoretical\nand statistical assumptions. Moreover, the uncertainty associated with\na measurement outcome depends on the methods employed for the\ncalibration of the instrument. Calibration involves additional\nassumptions about the instrument, the calibrating apparatus, the\nquantity being measured and the properties of measurement standards\n(Rothbart and Slayden 1994; Franklin 1997; Baird 2004: Ch. 4; Soler et\nal. 2013). Another component of uncertainty originates from vagueness\nin the definition of the measurand, and is known as\n“definitional uncertainty” (Mari and Giordani 2013;\nGrégis 2015). Finally, measurement involves background\nassumptions about the scale type and unit system being used, and these\nassumptions are often tied to broader theoretical and technological\nconsiderations relating to the definition and realization of scales\nand units. \nThese various theoretical and statistical assumptions form the basis\nfor the construction of one or more models of the measurement process.\nUnlike mathematical theories of measurement, where the term\n“model” denotes a set-theoretical structure that\ninterprets a formal language, here the term “model”\ndenotes an abstract and local representation of a target system that\nis constructed from simplifying\n assumptions.[17]\n The relevant target system in this case is a measurement process,\nthat is, a system composed of a measuring instrument, objects or\nevents to be measured, the environment (including human operators),\nsecondary instruments and reference standards, the time-evolution of\nthese components, and their various interactions with each other.\nMeasurement is viewed as a set of procedures whose aim is to\ncoherently assign values to model parameters based on instrument\nindications. Models are therefore seen as necessary preconditions for\nthe possibility of inferring measurement outcomes from instrument\nindications, and as crucial for determining the content of measurement\noutcomes. As proponents of model-based accounts emphasize, the same\nindications produced by the same measurement process may be used to\nestablish different measurement outcomes depending on how the\nmeasurement process is modeled, e.g., depending on which environmental\ninfluences are taken into account, which statistical assumptions are\nused to analyze noise, and which approximations are used in applying\nbackground theory. As Luca Mari puts it,  \nany measurement result reports information that is meaningful only in\nthe context of a metrological model, such a model being required to\ninclude a specification for all the entities that explicitly or\nimplicitly appear in the expression of the measurement result. (2003:\n25)  \nSimilarly, models are said to provide the necessary context for\nevaluating various aspects of the goodness of measurement outcomes,\nincluding accuracy, precision, error and uncertainty (Boumans 2006,\n2007a, 2009, 2012b; Mari 2005b). \nModel-based accounts diverge from empiricist interpretations of\nmeasurement theory in that they do not require relations among\nmeasurement outcomes to be isomorphic or homomorphic to observable\nrelations among the items being measured (Mari 2000). Indeed,\naccording to model-based accounts relations among measured objects\nneed not be observable at all prior to their measurement (Frigerio et\nal. 2010: 125). Instead, the key normative requirement of model-based\naccounts is that values be assigned to model parameters in a coherent\nmanner. The coherence criterion may be viewed as a conjunction of two\nsub-criteria: (i) coherence of model assumptions with relevant\nbackground theories or other substantive presuppositions about the\nquantity being measured; and (ii) objectivity, i.e., the mutual\nconsistency of measurement outcomes across different measuring\ninstruments, environments and\n models[18]\n (Frigerio et al. 2010; Tal 2017a; Teller 2018). The first\nsub-criterion is meant to ensure that the intended quantity\nis being measured, while the second sub-criterion is meant to ensure\nthat measurement outcomes can be reasonably attributed to the measured\nobject rather than to some artifact of the measuring\ninstrument, environment or model. Taken together, these two\nrequirements ensure that measurement outcomes remain valid\nindependently of the specific assumptions involved in their\nproduction, and hence that the context-dependence of measurement\noutcomes does not threaten their general applicability. \nBesides their applicability to physical measurement, model-based\nanalyses also shed light on measurement in economics. Like physical\nquantities, values of economic variables often cannot be observed\ndirectly and must be inferred from observations based on abstract and\nidealized models. The nineteenth century economist William Jevons, for\nexample, measured changes in the value of gold by postulating certain\ncausal relationships between the value of gold, the supply of gold and\nthe general level of prices (Hoover and Dowell 2001: 155–159;\nMorgan 2001: 239). As Julian Reiss (2001) shows, Jevons’\nmeasurements were made possible by using two models: a\ncausal-theoretical model of the economy, which is based on the\nassumption that the quantity of gold has the capacity to raise or\nlower prices; and a statistical model of the data, which is based on\nthe assumption that local variations in prices are mutually\nindependent and therefore cancel each other out when averaged. Taken\ntogether, these models allowed Jevons to infer the change in the value\nof gold from data concerning the historical prices of various\n goods.[19] \nThe ways in which models function in economic measurement have led\nsome philosophers to view certain economic models as measuring\ninstruments in their own right, analogously to rulers and balances\n(Boumans 1999, 2005c, 2006, 2007a, 2009, 2012a, 2015; Morgan 2001).\nMarcel Boumans explains how macroeconomists are able to isolate a\nvariable of interest from external influences by tuning parameters in\na model of the macroeconomic system. This technique frees economists\nfrom the impossible task of controlling the actual system. As Boumans\nargues, macroeconomic models function as measuring instruments insofar\nas they produce invariant relations between inputs (indications) and\noutputs (outcomes), and insofar as this invariance can be tested by\ncalibration against known and stable facts. When such model-based\nprocedures are combined with expert judgment, they can produce\nreliable measurements of economic phenomena even outside controlled\nlaboratory settings (Boumans 2015: Chap. 5). \nAnother area where models play a central role in measurement is\npsychology. The measurement of most psychological attributes, such as\nintelligence, anxiety and depression, does not rely on homomorphic\nmappings of the sort espoused by the Representational Theory of\nMeasurement (Wilson 2013: 3766). Instead, psychometric theory relies\npredominantly on the development of abstract models that are meant to\npredict subjects’ performance in certain tasks. These models are\nconstructed from substantive and statistical assumptions about the\npsychological attribute being measured and its relation to each\nmeasurement task. For example, Item Response Theory, a popular\napproach to psychological measurement, employs a variety of models to\nevaluate the reliability and validity of questionnaires. Consider a\nquestionnaire that is meant to assess English language comprehension\n(the “ability”), by presenting subjects with a series of\nyes/no questions (the “items”). One of the simplest models\nused to calibrate such questionnaires is the Rasch model (Rasch 1960).\nThis model supposes a straightforward algebraic relation—known\nas the “log of the odds”—between the probability\nthat a subject will answer a given item correctly, the difficulty of\nthat particular item, and the subject’s ability. New\nquestionnaires are calibrated by testing the fit between their\nindications and the predictions of the Rasch model and assigning\ndifficulty levels to each item accordingly. The model is then used in\nconjunction with the questionnaire to infer levels of English language\ncomprehension (outcomes) from raw questionnaire scores (indications)\n(Wilson 2013; Mari and Wilson 2014). \nThe sort of statistical calibration (or “scaling”)\nprovided by Rasch models yields repeatable results, but it is often\nonly a first step towards full-fledged psychological measurement.\nPsychologists are typically interested in the results of a measure not\nfor its own sake, but for the sake of assessing some underlying and\nlatent psychological attribute, e.g., English language comprehension.\nA good fit between item responses and a statistical model does not yet\ndetermine what the questionnaire is measuring. The process of\nestablishing that a procedure measures the intended psychological\nattribute is known as “validation”. One way of validating\na psychometric instrument is to test whether different procedures that\nare intended to measure the same latent attribute provide consistent\nresults. Such testing belongs to a family of validation techniques\nknown as “construct validation”. A construct is an\nabstract representation of the latent attribute intended to be\nmeasured, and  \nreflects a hypothesis […] that a variety of behaviors will\ncorrelate with one another in studies of individual differences and/or\nwill be similarly affected by experimental manipulations. (Nunnally\n& Bernstein 1994: 85)  \nConstructs are denoted by variables in a model that predicts which\ncorrelations would be observed among the indications of different\nmeasures if they are indeed measures of the same attribute. Such\nmodels involve substantive assumptions about the attribute, including\nits internal structure and its relations to other attributes, and\nstatistical assumptions about the correlation among different measures\n(Campbell & Fiske 1959; Nunnally & Bernstein 1994: Ch. 3;\nAngner 2008). \nIn recent years, philosophers of science have become increasingly\ninterested in psychometrics and the concept of validity. One debate\nconcerns the ontological status of latent psychological attributes.\nDenny Borsboom has argued against operationalism about latent\nattributes, and in favour of defining validity in a manner that\nembraces realism: “a test is valid for measuring an attribute if\nand only if a) the attribute exists, and b) variations in the\nattribute causally produce variations in the outcomes of the\nmeasurement procedure” (2005: 150; see also Hood 2009, 2013;\nFeest 2020). Elina Vessonen has defended a moderate form of\noperationalism about psychological attributes, and argued that\nmoderate operationalism is compatible with a cautious type of realism\n(2019). Another recent discussion focuses on the justification for\nconstruct validation procedures. According to Anna Alexandrova,\nconstruct validation is in principle a justified methodology, insofar\nas it establishes coherence with theoretical assumptions and\nbackground knowledge about the latent attribute. However, Alexandrova\nnotes that in practice psychometricians who intend to measure\nhappiness and well-being often avoid theorizing about these\nconstructs, and instead appeal to respondents’ folk beliefs.\nThis defeats the purpose of construct validation and turns it into a\nnarrow, technical exercise (Alexandrova and Haybron 2016; Alexandrova\n2017; see also McClimans et al. 2017). \nA more fundamental criticism leveled against psychometrics is that it\ndogmatically presupposes that psychological attributes can be\nquantified. Michell (2000, 2004b) argues that psychometricians have\nnot made serious attempts to test whether the attributes they purport\nto measure have quantitative structure, and instead adopted an overly\nloose conception of measurement that disguises this neglect. In\nresponse, Borsboom and Mellenbergh (2004) argue that Item Response\nTheory provides probabilistic tests of the quantifiability of\nattributes. Psychometricians who construct a statistical model\ninitially hypothesize that an attribute is quantitative, and then\nsubject the model to empirical tests. When successful, such tests\nprovide indirect confirmation of the initial hypothesis, e.g. by\nshowing that the attribute has an additive conjoint structure (see\nalso Vessonen 2020). \nSeveral scholars have pointed out similarities between the ways models\nare used to standardize measurable quantities in the natural and\nsocial sciences. For example, Mark Wilson (2013) argues that\npsychometric models can be viewed as tools for constructing\nmeasurement standards in the same sense of “measurement\nstandard” used by metrologists. Others have raised doubts about\nthe feasibility and desirability of adopting the example of the\nnatural sciences when standardizing constructs in the social sciences.\nNancy Cartwright and Rosa Runhardt (2014) discuss\n“Ballung” concepts, a term they borrow from Otto Neurath\nto denote concepts with a fuzzy and context-dependent scope. Examples\nof Ballung concepts are race, poverty, social exclusion, and the\nquality of PhD programs. Such concepts are too multifaceted to be\nmeasured on a single metric without loss of meaning, and must be\nrepresented either by a matrix of indices or by several different\nmeasures depending on which goals and values are at play (see also\nBradburn, Cartwright, & Fuller 2016, Other Internet Resources).\nAlexandrova (2008) points out that ethical considerations bear on\nquestions about the validity of measures of well-being no less than\nconsiderations of reproducibility. Such ethical considerations are\ncontext sensitive, and can only be applied piecemeal. In a similar\nvein, Leah McClimans (2010) argues that uniformity is not always an\nappropriate goal for designing questionnaires, as the open-endedness\nof questions is often both unavoidable and desirable for obtaining\nrelevant information from\n subjects.[20]\n The intertwining of ethical and epistemic considerations is\nespecially clear when psychometric questionnaires are used in medical\ncontexts to evaluate patient well-being and mental health. In such\ncases, small changes to the design of a questionnaire or the analysis\nof its results may result in significant harms or benefits to patients\n(McClimans 2017; Stegenga 2018, Chap. 8). These insights highlight the\nvalue-laden and contextual nature of the measurement of mental and\nsocial phenomena. \nThe development of model-based accounts discussed in the previous\nsection is part of a larger, “epistemic turn” in the\nphilosophy of measurement that occurred in the early 2000s. Rather\nthan emphasizing the mathematical foundations, metaphysics or\nsemantics of measurement, philosophical work in recent years tends to\nfocus on the presuppositions and inferential patterns involved in\nconcrete practices of measurement, and on the historical, social and\nmaterial dimensions of measuring. The philosophical study of these\ntopics has been referred to as the “epistemology of\nmeasurement” (Mari 2003, 2005a; Leplège 2003; Tal 2017a).\nIn the broadest sense, the epistemology of measurement is the study of\nthe relationships between measurement and knowledge. Central topics\nthat fall under the purview of the epistemology of measurement include\nthe conditions under which measurement produces knowledge; the\ncontent, scope, justification and limits of such knowledge; the\nreasons why particular methodologies of measurement and\nstandardization succeed or fail in supporting particular knowledge\nclaims, and the relationships between measurement and other\nknowledge-producing activities such as observation, theorizing,\nexperimentation, modelling and calculation. In pursuing these\nobjectives, philosophers are drawing on the work of historians and\nsociologists of science, who have been investigating measurement\npractices for a longer period (Wise and Smith 1986; Latour 1987: Ch.\n6; Schaffer 1992; Porter 1995, 2007; Wise 1995; Alder 2002; Galison\n2003; Gooday 2004; Crease 2011), as well as on the history and\nphilosophy of scientific experimentation (Harré 1981; Hacking\n1983; Franklin 1986; Cartwright 1999). The following subsections\nsurvey some of the topics discussed in this burgeoning body of\nliterature. \nA topic that has attracted considerable philosophical attention in\nrecent years is the selection and improvement of measurement\nstandards. Generally speaking, to standardize a quantity concept is to\nprescribe a determinate way in which that concept is to be applied to\nconcrete\n particulars.[21]\n To standardize a measuring instrument is to assess how well the\noutcomes of measuring with that instrument fit the prescribed mode of\napplication of the relevant concept.\n [22]\n The term “measurement standard” accordingly has at least\ntwo meanings: on the one hand, it is commonly used to refer to\nabstract rules and definitions that regulate the use of quantity\nconcepts, such as the definition of the meter. On the other hand, the\nterm “measurement standard” is also commonly used to refer\nto the concrete artifacts and procedures that are deemed exemplary of\nthe application of a quantity concept, such as the metallic bar that\nserved as the standard meter until 1960. This duality in meaning\nreflects the dual nature of standardization, which involves both\nabstract and concrete aspects. \nIn\n Section 4\n it was noted that standardization involves choices among nontrivial\nalternatives, such as the choice among different thermometric fluids\nor among different ways of marking equal duration. These choices are\nnontrivial in the sense that they affect whether or not the same\ntemperature (or time) intervals are deemed equal, and hence affect\nwhether or not statements of natural law containing the term\n“temperature” (or “time”) come out true.\nAppealing to theory to decide which standard is more accurate would be\ncircular, since the theory cannot be determinately applied to\nparticulars prior to a choice of measurement standard. This\ncircularity has been variously called the “problem of\ncoordination” (van Fraassen 2008: Ch. 5) and the “problem\nof nomic measurement” (Chang 2004: Ch. 2). As already mentioned,\nconventionalists attempted to escape the circularity by positing a\npriori statements, known as “coordinative\ndefinitions”, which were supposed to link quantity-terms with\nspecific measurement operations. A drawback of this solution is that\nit supposes that choices of measurement standard are arbitrary and\nstatic, whereas in actual practice measurement standards tend to be\nchosen based on empirical considerations and are eventually improved\nor replaced with standards that are deemed more accurate. \nA new strand of writing on the problem of coordination has emerged in\nrecent years, consisting most notably of the works of Hasok Chang\n(2001, 2004, 2007; Barwich and Chang 2015) and Bas van Fraassen (2008:\nCh. 5; 2009, 2012; see also Padovani 2015, 2017; Michel 2019). These\nworks take a historical and coherentist approach to the problem.\nRather than attempting to avoid the problem of circularity completely,\nas their predecessors did, they set out to show that the circularity\nis not vicious. Chang argues that constructing a quantity-concept and\nstandardizing its measurement are co-dependent and iterative tasks.\nEach “epistemic iteration” in the history of\nstandardization respects existing traditions while at the same time\ncorrecting them (Chang 2004: Ch. 5). The pre-scientific concept of\ntemperature, for example, was associated with crude and ambiguous\nmethods of ordering objects from hot to cold. Thermoscopes, and\neventually thermometers, helped modify the original concept and made\nit more precise. With each such iteration the quantity concept was\nre-coordinated to a more stable set of standards, which in turn\nallowed theoretical predictions to be tested more precisely,\nfacilitating the subsequent development of theory and the construction\nof more stable standards, and so on. \nHow this process avoids vicious circularity becomes clear when we look\nat it either “from above”, i.e., in retrospect given our\ncurrent scientific knowledge, or “from within”, by looking\nat historical developments in their original context (van Fraassen\n2008: 122). From either vantage point, coordination succeeds because\nit increases coherence among elements of theory and instrumentation.\nThe questions “what counts as a measurement of quantity\nX?” and “what is quantity X?”,\nthough unanswerable independently of each other, are addressed\ntogether in a process of mutual refinement. It is only when one adopts\na foundationalist view and attempts to find a starting point for\ncoordination free of presupposition that this historical process\nerroneously appears to lack epistemic justification (2008: 137). \nThe new literature on coordination shifts the emphasis of the\ndiscussion from the definitions of quantity-terms to the\nrealizations of those definitions. In metrological jargon, a\n“realization” is a physical instrument or procedure that\napproximately satisfies a given definition (cf. JCGM 2012: 5.1).\nExamples of metrological realizations are the official prototypes of\nthe kilogram and the cesium fountain clocks used to standardize the\nsecond. Recent studies suggest that the methods used to design,\nmaintain and compare realizations have a direct bearing on the\npractical application of concepts of quantity, unit and scale, no less\nthan the definitions of those concepts (Riordan 2015; Tal 2016). The\nrelationship between the definition and realizations of a unit becomes\nespecially complex when the definition is stated in theoretical terms.\nSeveral of the base units of the International System (SI) —\nincluding the meter, kilogram, ampere, kelvin and mole — are no\nlonger defined by reference to any specific kind of physical system,\nbut by fixing the numerical value of a fundamental physical constant.\nThe kilogram, for example, was redefined in 2019 as the unit of mass\nsuch that the numerical value of the Planck constant is exactly\n6.62607015 × 10-34 kg m2 s-1\n(BIPM 2019:131). Realizing the kilogram under this definition is a\nhighly theory-laden task. The study of the practical realization of\nsuch units has shed new light on the evolving relationships between\nmeasurement and theory (Tal 2018; de Courtenay et al 2019; Wolff\n2020b). \nAs already discussed above (Sections\n 7\n and\n 8.1),\n theory and measurement are interdependent both historically and\nconceptually. On the historical side, the development of theory and\nmeasurement proceeds through iterative and mutual refinements. On the\nconceptual side, the specification of measurement procedures shapes\nthe empirical content of theoretical concepts, while theory provides a\nsystematic interpretation for the indications of measuring\ninstruments. This interdependence of measurement and theory may seem\nlike a threat to the evidential role that measurement is supposed to\nplay in the scientific enterprise. After all, measurement outcomes are\nthought to be able to test theoretical hypotheses, and this seems to\nrequire some degree of independence of measurement from theory. This\nthreat is especially clear when the theoretical hypothesis being\ntested is already presupposed as part of the model of the measuring\ninstrument. To cite an example from Franklin et al. (1989: 230):  \nThere would seem to be, at first glance, a vicious circularity if one\nwere to use a mercury thermometer to measure the temperature of\nobjects as part of an experiment to test whether or not objects expand\nas their temperature increases.  \nNonetheless, Franklin et al. conclude that the circularity is not\nvicious. The mercury thermometer could be calibrated against another\nthermometer whose principle of operation does not presuppose the law\nof thermal expansion, such as a constant-volume gas thermometer,\nthereby establishing the reliability of the mercury thermometer on\nindependent grounds. To put the point more generally, in the context\nof local hypothesis-testing the threat of circularity can usually be\navoided by appealing to other kinds of instruments and other parts of\ntheory. \nA different sort of worry about the evidential function of measurement\narises on the global scale, when the testing of entire theories is\nconcerned. As Thomas Kuhn (1961) argues, scientific theories are\nusually accepted long before quantitative methods for testing them\nbecome available. The reliability of newly introduced measurement\nmethods is typically tested against the predictions of the theory\nrather than the other way around. In Kuhn’s words, “The\nroad from scientific law to scientific measurement can rarely be\ntraveled in the reverse direction” (1961: 189). For example,\nDalton’s Law, which states that the weights of elements in a\nchemical compound are related to each other in whole-number\nproportions, initially conflicted with some of the best known\nmeasurements of such proportions. It is only by assuming\nDalton’s Law that subsequent experimental chemists were able to\ncorrect and improve their measurement techniques (1961: 173). Hence,\nKuhn argues, the function of measurement in the physical sciences is\nnot to test the theory but to apply it with increasing scope and\nprecision, and eventually to allow persistent anomalies to surface\nthat would precipitate the next crisis and scientific revolution. Note\nthat Kuhn is not claiming that measurement has no evidential role to\nplay in science. Instead, he argues that measurements cannot test a\ntheory in isolation, but only by comparison to some alternative theory\nthat is proposed in an attempt to account for the anomalies revealed\nby increasingly precise measurements (for an illuminating discussion\nof Kuhn’s thesis see Hacking 1983: 243–5). \nTraditional discussions of theory-ladenness, like those of Kuhn, were\nconducted against the background of the logical positivists’\ndistinction between theoretical and observational language. The\ntheory-ladenness of measurement was correctly perceived as a threat to\nthe possibility of a clear demarcation between the two languages.\nContemporary discussions, by contrast, no longer present\ntheory-ladenness as an epistemological threat but take for granted\nthat some level of theory-ladenness is a prerequisite for measurements\nto have any evidential power. Without some minimal substantive\nassumptions about the quantity being measured, such as its amenability\nto manipulation and its relations to other quantities, it would be\nimpossible to interpret the indications of measuring instruments and\nhence impossible to ascertain the evidential relevance of those\nindications. This point was already made by Pierre Duhem (1906:\n153–6; see also Carrier 1994: 9–19). Moreover,\ncontemporary authors emphasize that theoretical assumptions play\ncrucial roles in correcting for measurement errors and evaluating\nmeasurement uncertainties. Indeed, physical measurement procedures\nbecome more accurate when the model underlying them is\nde-idealized, a process which involves increasing the theoretical\nrichness of the model (Tal 2011). \nThe acknowledgment that theory is crucial for guaranteeing the\nevidential reliability of measurement draws attention to the\n“problem of observational grounding”, which is an inverse\nchallenge to the traditional threat of theory-ladenness (Tal 2016b).\nThe challenge is to specify what role observation plays in\nmeasurement, and particularly what sort of connection with observation\nis necessary and/or sufficient to allow measurement to play an\nevidential role in the sciences. This problem is especially clear when\none attempts to account for the increasing use of computational\nmethods for performing tasks that were traditionally accomplished by\nmeasuring instruments. As Margaret Morrison (2009) and Wendy Parker\n(2017) argue, there are cases where reliable quantitative information\nis gathered about a target system with the aid of a computer\nsimulation, but in a manner that satisfies some of the central\ndesiderata for measurement such as being empirically grounded and\nbackward-looking (see also Lusk 2016). Such information does not rely\non signals transmitted from the particular object of interest to the\ninstrument, but on the use of theoretical and statistical models to\nprocess empirical data about related objects. For example, data\nassimilation methods are customarily used to estimate past atmospheric\ntemperatures in regions where thermometer readings are not available.\nSome methods do this by fitting a computational model of the\natmosphere’s behavior to a combination of available data from\nnearby regions and a model-based forecast of conditions at the time of\nobservation (Parker 2017). These estimations are then used in various\nways, including as data for evaluating forward-looking climate models.\nRegardless of whether one calls these estimations\n“measurements”, they challenge the idea that producing\nreliable quantitative evidence about the state of an object requires\nobserving that object, however loosely one understands the term\n “observation”.[23] \nTwo key aspects of the reliability of measurement outcomes are\naccuracy and precision. Consider a series of repeated weight\nmeasurements performed on a particular object with an equal-arms\nbalance. From a realist, “error-based” perspective, the\noutcomes of these measurements are accurate if they are close\nto the true value of the quantity being measured—in our case,\nthe true ratio of the object’s weight to the chosen\nunit—and precise if they are close to each other. An\nanalogy often cited to clarify the error-based distinction is that of\narrows shot at a target, with accuracy analogous to the closeness of\nhits to the bull’s eye and precision analogous to the tightness\nof spread of hits (cf. JCGM 2012: 2.13 & 2.15, Teller 2013: 192).\nThough intuitive, the error-based way of carving the distinction\nraises an epistemological difficulty. It is commonly thought that the\nexact true values of most quantities of interest to science are\nunknowable, at least when those quantities are measured on continuous\nscales. If this assumption is granted, the accuracy with which such\nquantities are measured cannot be known with exactitude, but only\nestimated by comparing inaccurate measurements to each other. And yet\nit is unclear why convergence among inaccurate measurements should be\ntaken as an indication of truth. After all, the measurements could be\nplagued by a common bias that prevents their individual inaccuracies\nfrom cancelling each other out when averaged. In the absence of\ncognitive access to true values, how is the evaluation of measurement\naccuracy possible? \nIn answering this question, philosophers have benefited from studying\nthe various senses of the term “measurement accuracy” as\nused by practicing scientists. At least five different senses have\nbeen identified: metaphysical, epistemic, operational, comparative and\npragmatic (Tal 2011: 1084–5). In particular, the epistemic or\n“uncertainty-based” sense of the term is metaphysically\nneutral and does not presuppose the existence of true values. Instead,\nthe accuracy of a measurement outcome is taken to be the closeness of\nagreement among values reasonably attributed to a quantity given\navailable empirical data and background knowledge (cf. JCGM 2012: 2.13\nNote 3; Giordani & Mari 2012; de Courtenay and Grégis\n2017). Thus construed, measurement accuracy can be evaluated by\nestablishing robustness among the consequences of models representing\ndifferent measurement processes (Basso 2017; Tal 2017b; Bokulich 2020;\nStaley 2020). \nUnder the uncertainty-based conception, imprecision is a special type\nof inaccuracy. For example, the inaccuracy of weight measurements is\nthe breadth of spread of values that are reasonably attributed to the\nobject’s weight given the indications of the balance and\navailable background knowledge about the way the balance works and the\nstandard weights used. The imprecision of these measurements is the\ncomponent of inaccuracy arising from uncontrolled variations to the\nindications of the balance over repeated trials. Other sources of\ninaccuracy besides imprecision include imperfect corrections to\nsystematic errors, inaccurately known physical constants, and vague\nmeasurand definitions, among others (see\n Section 7.1). \nPaul Teller (2018) raises a different objection to the error-based\nconception of measurement accuracy. He argues against an assumption he\ncalls “measurement accuracy realism”, according to which\nmeasurable quantities have definite values in reality. Teller argues\nthat this assumption is false insofar as it concerns the quantities\nhabitually measured in physics, because any specification of definite\nvalues (or value ranges) for such quantities involves idealization and\nhence cannot refer to anything in reality. For example, the concept\nusually understood by the phrase “the velocity of sound in\nair” involves a host of implicit idealizations concerning the\nuniformity of the air’s chemical composition, temperature and\npressure as well as the stability of units of measurement. Removing\nthese idealizations completely would require adding infinite amount of\ndetail to each specification. As Teller argues, measurement accuracy\nshould itself be understood as a useful idealization, namely as a\nconcept that allows scientists to assess coherence and consistency\namong measurement outcomes as if the linguistic expression of\nthese outcomes latched onto anything in the world. Precision is\nsimilarly an idealized concept, which is based on an open-ended and\nindefinite specification of what counts as repetition of measurement\nunder “the same” circumstances (Teller 2013: 194).","contact.mail":"eran.tal@mcgill.ca","contact.domain":"mcgill.ca"}]
