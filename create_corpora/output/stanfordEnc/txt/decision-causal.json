[{"date.published":"2008-10-25","date.changed":"2020-11-06","url":"https://plato.stanford.edu/entries/decision-causal/","author1":"Paul Weirich","author1.info":"https://philosophy.missouri.edu/people/weirich","entry":"decision-causal","body.text":"\n\n\nCausal decision theory adopts principles of rational choice that\nattend to an act’s consequences. It maintains that an account of\nrational choice must use causality to identify the considerations that\nmake a choice rational.\n\n\nGiven a set of options constituting a decision problem, decision\ntheory recommends an option that maximizes utility, that is, an option\nwhose utility equals or exceeds the utility of every other option. It\nevaluates an option’s utility by calculating the option’s\nexpected utility. It uses probabilities and utilities of an\noption’s possible outcomes to define an option’s expected\nutility. The probabilities depend on the option. Causal decision\ntheory takes the dependence to be causal rather than merely\nevidential.\n\n\nThis essay explains causal decision theory, reviews its history,\ndescribes current research in causal decision theory, and surveys the\ntheory’s philosophical foundations. The literature on causal\ndecision theory is vast, and this essay covers only a portion of\nit.\n\nSuppose that a student is considering whether to study for an exam. He\nreasons that if he will pass the exam, then studying is wasted effort.\nAlso, if he will not pass the exam, then studying is wasted effort. He\nconcludes that because whatever will happen, studying is wasted\neffort, it is better not to study. This reasoning errs because\nstudying raises the probability of passing the exam. Deliberations\nshould take account of an act’s influence on the probability of\nits possible outcomes. An act’s expected utility is a probability-weighted average\nof its possible outcomes’ utilities. Possible states of the\nworld that are mutually exclusive and jointly exhaustive, and so form\na partition, generate an act’s possible outcomes. An act-state\npair specifies an outcome. In the example, the act of studying and the\nstate of passing form an outcome comprising the effort of studying and\nthe benefit of passing. The expected utility of studying is the\nprobability of passing if one studies times the utility of studying\nand passing plus the probability of not passing if one studies times\nthe utility of studying and not passing. In compact notation, \nEach product specifies the probability and utility of a possible\noutcome. The sum is a probability-weighted average of the possible\noutcomes’ utilities. \nHow should decision theory interpret the probability of a state \\(S\\)\nif one performs an act \\(A\\), that is, \\(P(S \\mbox{ if }A)\\)?\nProbability theory offers a handy suggestion. It has an account of\nconditional probabilities that decision theory may adopt. Decision\ntheory may take \\(P(S \\mbox{ if }A)\\) as the probability of the state\nconditional on the act. Then \\(P(S \\mbox{ if }A)\\) equals \\(P(S\\mid A)\\),\nwhich probability theory defines as \\(P(S \\amp A)/P(A)\\) when \\(P(A)\n\\ne 0\\). Some theorists call expected utility computed using\nconditional probabilities conditional expected utility. I call it\nexpected utility tout court because the formula using\nconditional probabilities generalizes a simpler formula for expected\nutility that uses nonconditional probabilities of states. Also, some\ntheorists call an act’s expected utility its utility tout\ncourt because an act’s expected utility appraises the act\nand yields the act’s utility in ideal cases. I call it expected\nutility because a person by mistake may attach more or less utility to\na bet than its expected utility warrants. The equality of an\nact’s utility and its expected utility is normative rather than\ndefinitional. Expected utilities obtained from conditional probabilities steer\nthe student’s deliberations in the right\ndirection. and \nBecause of studying’s effect on the probability of passing,\n\\(P(P\\mid S) \\gt P(P\\mid {\\sim}S)\\) and \\(P({\\sim}P\\mid S) \\lt\nP({\\sim}P\\mid {\\sim}S)\\). So \\(\\textit{EU} (S) \\gt \\textit{EU}\n({\\sim}S)\\), assuming that studying’s increase in the\nprobability of passing compensates for the effort of\nstudying. Maximization of expected utility recommends studying. \nThe handy interpretation of the probability of a state if one performs\nan act, however, is not completely satisfactory. Suppose that one\ntosses a coin with an unknown bias and obtains heads. This result is\nevidence that the next toss will yield heads, although it does not\ncausally influence the next toss’s result. An event’s\nprobability conditional on another event indicates the evidence that\nthe second event provides for the first. If the two events are\ncorrelated, the second may provide evidence for the first without\ncausally influencing it. Causation entails correlation, but\ncorrelation does not entail causation. Deliberations should attend to\nan act’s causal influence on a state rather than an act’s\nevidence for a state. A good decision aims to produce a good outcome\nrather than evidence of a good outcome. It aims for the good and not\njust signs of the good. Often efficacy and auspiciousness go hand in\nhand. When they come apart, an agent should perform an efficacious act\nrather than an auspicious act. \nConsider the Prisoner’s Dilemma, a stock example of game theory.\nTwo people isolated from each other may each act either cooperatively\nor uncooperatively. They each do better if they each act cooperatively\nthan if they each act uncooperatively. However, each does better if he\nacts uncooperatively, no matter what the other does. Acting\nuncooperatively dominates acting cooperatively. Suppose, in addition,\nthat the two players are psychological twins. Each thinks as the other\nthinks. Moreover, they know this fact about themselves. Then if one\nplayer acts cooperatively, he concludes that his counterpart also acts\ncooperatively. His acting cooperatively is good evidence that his\ncounterpart does the same. Nonetheless, his acting cooperatively does\nnot cause his counterpart to act cooperatively. He has no contact with\nhis counterpart. Because he is better off not acting cooperatively\nwhatever his counterpart does, not acting cooperatively is the better\ncourse. Acting cooperatively is auspicious but not efficacious. \nTo make expected utility track efficacy rather than auspiciousness,\ncausal decision theory interprets the probability of a state if one\nperforms an act as a type of causal probability rather than as a\nstandard conditional probability. In the Prisoner’s Dilemma with\ntwins, consider the probability of one player’s acting\ncooperatively given that the other player does. This conditional\nprobability is high. Next, consider the causal probability of one\nplayer’s acting cooperatively if the other player does. Because\nthe players are isolated, this probability equals the probability of\nthe first player’s acting cooperatively. It is low if that\nplayer follows dominance. Using conditional probabilities, the\nexpected utility of acting cooperatively exceeds the expected utility\nof acting uncooperatively. However, using causal probabilities, the\nexpected utility of acting uncooperatively exceeds the expected\nutility of acting cooperatively. Switching from conditional to causal\nprobabilities makes expected-utility maximization yield acting\nuncooperatively. \nThis section tours causal decision theory’s history and along\nthe way presents various formulations of the theory. \nRobert Nozick (1969) presented a dilemma for decision theory. He\nconstructed an example in which the standard principle of dominance\nconflicts with the standard principle of expected-utility\nmaximization. Nozick called the example Newcomb’s Problem after\nthe physicist, William Newcomb, who first formulated the problem. \nIn Newcomb’s Problem an agent may choose either to take an\nopaque box or to take both the opaque box and a transparent box. The\ntransparent box contains one thousand dollars that the agent plainly\nsees. The opaque box contains either nothing or one million dollars,\ndepending on a prediction already made. The prediction was about the\nagent’s choice. If the prediction was that the agent will take\nboth boxes, then the opaque box is empty. On the other hand, if the\nprediction was that the agent will take just the opaque box, then the\nopaque box contains a million dollars. The prediction is reliable. The\nagent knows all these features of his decision problem. \nFigure 1 displays the agent’s options and their outcomes. A row\nrepresents an option, a column a state of the world, and a cell an\noption’s outcome in a state of the world. \nFigure 1. Newcomb’s Problem \nBecause the outcome of two-boxing is better by \\(\\$T\\) than the\noutcome of one-boxing given each prediction, two-boxing dominates\none-boxing. Two-boxing is the rational choice according to the\nprinciple of dominance. Because the prediction is reliable, a\nprediction of one-boxing has a high probability given one-boxing.\nSimilarly, a prediction of two-boxing has a high probability given\ntwo-boxing. Hence, using conditional probabilities to compute expected\nutilities, one-boxing’s expected utility exceeds\ntwo-boxing’s expected utility. One-boxing is the rational choice\naccording to the principle of expected-utility maximization. \nDecision theory should address all possible decision problems and not\njust realistic decision problems. However, if Newcomb’s problem\nseems untroubling because unrealistic, realistic versions of the\nproblem are plentiful. The essential feature of Newcomb’s\nproblem is an inferior act’s correlation with a good state that\nit does not causally promote. In realistic, medical Newcomb problems,\na medical condition and a behavioral symptom have a common cause and\nare correlated although neither causes the other. If the behavior is\nattractive, dominance recommends it although expected utility\nmaximization prohibits it. Also, Allan Gibbard and William Harper\n(1978: Sec. 12) and David Lewis (1979) observe that a Prisoner’s\nDilemma with psychological twins poses a Newcomb problem for each\nplayer. For each player, the other player’s act is a state\naffecting the outcome. Acting cooperatively is a sign, but not a\ncause, of the other player’s acting cooperatively. Dominance\nrecommends acting uncooperatively, whereas expected utility computed\nwith conditional probabilities recommends acting cooperatively. In\nsome realistic instances of the Prisoner’s Dilemma, the\nplayers’ anticipated similarity of thought creates a conflict\nbetween the principle of dominance and the principle of\nexpected-utility maximization. \nArif Ahmed (2018) offers a collection of essays on Newcomb’s\nproblem. Kenny Easwaran (forthcoming) distinguishes Newcomb-like problems\naccording to opportunities for causal intervention. \nRobert Stalnaker (1968) presented truth conditions for subjunctive\nconditionals. A subjunctive conditional is true if and only if in the\nnearest antecedent-world, its consequent is true. (This analysis is\nunderstood so that a subjunctive conditional is true if its antecedent\nis true in no world.) Stalnaker used analysis of subjunctive\nconditionals to ground their role in decision theory and in a\nresolution of Newcomb’s problem. In a letter to Lewis, Stalnaker (1972) proposed a way of\nreconciling decision principles in Newcomb’s problem. He\nsuggested calculating an act’s expected utility using\nprobabilities of conditionals in place of conditional\nprobabilities. Accordingly, \nwhere \\(A \\gt S_i\\) stands for the conditional that if \\(A\\) were\nperformed then \\(S_i\\) would obtain. Thus, instead of using the\nprobability of a prediction of one-boxing given one-boxing, one should\nuse the probability of the conditional that if the agent were to pick\njust one box, then the prediction would have been one-boxing. Because\nthe agent’s act does not cause the prediction, the probability\nof the conditional equals the probability that the prediction is\none-boxing. Also, consider the conditional that if the agent were to\npick both boxes, then the prediction would have been one-boxing. Its\nprobability similarly equals the probability that the prediction is\none-boxing. The act the agent performs does not affect any\nprediction’s probability because the prediction occurs prior to\nthe act. Consequently, using probabilities of conditionals to compute\nexpected utility, two-boxing’s expected utility exceeds\none-boxing’s expected utility. Therefore, the principle of\nexpected-utility maximization makes the same recommendation as does\nthe principle of dominance. \nGibbard and Harper (1978) elaborated and made public Stalnaker’s\nresolution of Newcomb’s problem. They distinguished causal\ndecision theory, which uses probabilities of subjunctive conditionals,\nfrom evidential decision theory, which uses conditional probabilities.\nBecause in decision problems probabilities of subjunctive conditionals\ntrack causal relations, using them to calculate an option’s\nexpected utility makes decision theory causal. \nGibbard and Harper distinguished two types of expected utility. One\ntype they called value and represented with \\(V\\). It indicates\nnews-value or auspiciousness. The other type they called utility and\nrepresented with \\(U\\). It indicates efficacy in attainment of goals.\nA calculation of an act’s expected value uses conditional\nprobabilities, and a calculation of its expected utility uses\nprobabilities of conditionals. They argued that expected utility,\ncalculated with probabilities of conditionals, yields genuine expected\nutility. \nAs Gibbard and Harper introduce \\(V\\) and \\(U\\), both rest on an\nassessment \\(D\\) (for desirability) of maximally specific outcomes.\nInstead of adopting a formula for expected utility that uses an\nassessment of outcomes neutral with respect to evidential and causal\ndecision theory, this essay follows Stalnaker (1972) in adopting a\nformula that uses utility to evaluate outcomes. \nConsider a conditional asserting that if an option were adopted, then\na certain state would obtain. Gibbard and Harper assume, to illustrate\nthe main ideas of causal decision theory, that the conditional has a\ntruth-value, and that, given its falsity, if the option were adopted,\nthen the state would not obtain. This assumption may be unwarranted if\nthe option is flipping a coin, and the relevant state is obtaining\nheads. It may be false (or indeterminate) that if the agent were to\nflip the coin, he would obtain heads. Similarly, the corresponding\nconditional about obtaining tails may be false (or indeterminate).\nThen probabilities of conditionals are not suitable for calculating\nthe option’s expected utility. The relevant probabilities do not\nsum to one (or do not even exist). To circumvent such impasses, some\ntheorists calculate causally-sensitive expected utilities without\nprobabilities of subjunctive conditionals. Causal decision theory has\nmany formulations. Brian Skyrms (1980: Sec IIC; 1982) presented a version of causal\ndecision theory that dispenses with probabilities of subjunctive\nconditionals. His theory separates factors that the agent’s act\nmay influence from factors that the agent’s act may not\ninfluence. It lets \\(K_i\\) stand for a possible full specification of\nfactors that an agent may not influence and lets \\(C_j\\) stand for a\npossible (but not necessarily full) specification of factors that the\nagent may influence. The set of \\(K_i\\) forms a partition, and the set\nof \\(C_j\\) forms a partition. The formula for an act’s expected\nutility first calculates its expected utility using factors the agent\nmay influence, with respect to each possible combination of factors\noutside the agent’s influence. Then it computes a\nprobability-weighted average of those conditional expected\nutilities. An act’s expected utility calculated this way is the\nact’s \\(K\\)-expectation, \\(\\textit{EU}_k(A)\\). According to\nSkyrms’s definition, \nSkyrms holds that an agent should select an act that maximizes\n\\(K\\)-expectation. Lewis (1981) presented a version of causal decision theory that\ncalculates expected utility using probabilities of dependency\nhypotheses instead of probabilities of subjunctive conditionals. A\ndependency hypothesis for an agent at a time is a maximally specific\nproposition about how the things the agent cares about do and do not\ndepend causally on his present acts. An option’s expected\nutility is its probability-weighted average utility with respect to a\npartition of dependency hypotheses \\(K_i\\). Lewis defines the expected\nutility of an option \\(A\\) as \nand holds that to act rationally is to realize an option that\nmaximizes expected utility. His formula for an option’s expected\nutility is the same as Skyrms’s assuming that \\(U(K_i \\amp A)\\)\nmay be expanded with respect to a partition of factors the agent may\ninfluence, using the formula \nSkyrms’s and Lewis’s calculations of expected utility\ndispense with causal probabilities. They build causality into states\nof the world so that causal probabilities are unnecessary. In cases\nsuch as Newcomb’s problem, their calculations yield the same\nrecommendations as calculations of expected utility employing\nprobabilities of subjunctive conditionals. The various versions of\ncausal decision theory make equivalent recommendations when cases meet\ntheir background assumptions. Adam Bales (2016) compares versions in\nspecial cases that do not meet the background assumptions. \nDecision theory often introduces probability and utility with\nrepresentation theorems. These theorems show that if preferences among\nacts meet certain constraints, such as transitivity, then there exist\na probability function and a utility function (given a choice of\nscale) that generate expected utilities agreeing with preferences.\nDavid Krantz, R. Duncan Luce, Patrick Suppes, and Amos Tversky (1971)\noffer a good, general introduction to the purposes and methods of\nconstructing representations theorems. In\n Section 3.1,\n I discuss the theorems’ function in decision theory. \nRichard Jeffrey ([1965] 1983) presented a representation theorem for\nevidential decision theory, using its formula for expected utility.\nBrad Armendt (1986, 1988a) presented a representation theorem for\ncausal decision theory, using its formula for expected utility. James\nJoyce (1999) constructed a very general representation theorem that\nyields either causal or evidential decision theory depending on the\ninterpretation of probability that the formula for expected utility\nadopts. \nThe most common objection to causal decision theory is that it yields\nthe wrong choice in Newcomb’s problem. It yields two-boxing,\nwhereas one-boxing is correct. Terry Horgan (1981 [1985]), Paul\nHorwich (1987: Chap. 11), and Caspar Hare and Brian Hedden (2016) for\nexample, promote one-boxing. The main rationale for one-boxing is that\none-boxers fare better than do two-boxers. Causal decision theorists\nrespond that Newcomb’s problem is an unusual case that rewards\nirrationality. One-boxing is irrational even if one-boxers\nprosper. Bales (2018) rejects the argument that two-boxing is\nirrational \nSome theorists hold that one-boxing is plainly rational if the\nprediction is completely reliable. They maintain that if the\nprediction is certainly accurate, then choice reduces to taking\n\\(\\$M\\) or taking \\(\\$T\\). This view oversimplifies. If an agent\none-boxes, then that act is certain to yield \\(\\$M\\). However, the\nagent still would have done better by taking both boxes. Dominance\nstill recommends two-boxing. Making the prediction certain to be\naccurate does not change the character of the problem. Efficacy still\ntrumps auspiciousness, as Howard Sobel (1994: Chap. 5) argues. \nA way of reconciling the two sides of the debate about Newcomb’s\nproblem acknowledges that a rational person should prepare for the\nproblem by cultivating a disposition to one-box. Then whenever the\nproblem arises, the disposition will prompt a prediction of one-boxing\nand afterwards the act of one-boxing (still freely chosen). Causal\ndecision theory may acknowledge the value of this preparation. It may\nconclude that cultivating a disposition to one-box is rational\nalthough one-boxing itself is irrational. Hence, if in Newcomb’s\nproblem an agent two-boxes, causal decision theory may concede that\nthe agent did not rationally prepare for the problem. It nonetheless\nmaintains that two-boxing itself is rational. Although two-boxing is\nnot the act of a maximally rational agent, it is rational given the\ncircumstances of Newcomb’s problem. \nCausal decision theory may also explain that it advances a claim about\nthe evaluation of an act given the agent’s circumstances in\nNewcomb’s problem. It asserts two-boxing’s conditional\nrationality. Conditional and nonconditional rationality treat mistakes\ndifferently. In contrast with conditional rationality, nonconditional\nrationality does not grant past mistakes. It evaluates an act taking\naccount of the influence of past mistakes. However, conditional\nrationality accepts present circumstances as they are and does not\ndiscredit an act because it stems from past mistakes. Causal decision\ntheory maintains that two-boxing is rational, granting the\nagent’s circumstances and so ignoring any mistakes leading to\nthose circumstances, such as irrational preparation for\nNewcomb’s problem. \nAnother objection to causal decision theory concedes that two-boxing\nis the rational choice in Newcomb’s problem but rejects causal\nprinciples of choice that yield two-boxing. It seeks noncausal\nprinciples that yield two-boxing. Positivism is a source of aversion\nto decision principles incorporating causation. Some decision\ntheorists shun causation because no positivist account specifies its\nnature. Without a definition of causation in terms of observable\nphenomena, they prefer that decision theory avoid causation. Causal\ndecision theory’s response to this objection is both to\ndiscredit positivism and also to clarify causation so that puzzles\nconcerning it no longer give decision theory any reason to avoid\nit. \nEvidential decision theory has weaker metaphysical assumptions than\nhas causal decision theory, even if causation has impeccable\nmetaphysical credentials. Some decision theorists do not omit\ncausation because of metaphysical scruples but for conceptual economy.\nJeffrey ([1965] 1983, 2004), for the sake of parsimony, formulates\ndecision principles that do not rely on causal relations. \nEllery Eells (1981, 1982) contends that evidential decision theory\nyields causal decision theory’s recommendations but, more\neconomically, without reliance on causal apparatus. In particular,\nevidential decision theory yields two-boxing in Newcomb’s\nproblem. An agent’s reflection on his evidence makes conditional\nprobabilities support two-boxing. \nA noncontentious elaboration of Newcomb’s problem posits that\nthe agent’s choice and its prediction have a common cause. The\nagent’s choice is evidence of the common cause and evidence of\nthe choice’s prediction. Once an agent acquires the probability\nof the common cause, he may put aside the evidence his choice provides\nabout the prediction. That evidence is superfluous. Given the\nprobability of the common cause, the probability of a prediction of\none-boxing is constant with respect to his options. Similarly, the\nprobability of a prediction of two-boxing is constant with respect to\nhis options. Because the probability of a prediction is the same\nconditional on either option, the expected utility of two-boxing\nexceeds the expected utility of one-boxing according to evidential\ndecision theory. Horgan (1981 [1985]) and Huw Price (1986) make\nsimilar points. \nSuppose that an event \\(S\\) is a sign of a cause \\(C\\) that produces\nan effect \\(E\\). For the probability of \\(E\\), knowing whether \\(C\\)\nholds makes superfluous knowing whether \\(S\\) holds. Observation of\n\\(C\\) screens off the evidence that \\(S\\) provides for \\(E\\).\nThat is, \\(P(E\\mid C \\amp S) = P(E\\mid C)\\). In Newcomb’s problem,\nassuming that the agent is rational, his beliefs and desires are a\ncommon cause of his choice and the prediction. So his choice is a sign\nof the prediction’s content. For the probability of a prediction\nof one-boxing, knowing one’s beliefs and desires makes\nsuperfluous knowing the choice that they yield. Knowledge of the\ncommon cause screens off evidence that the choice provides about the\nprediction. Hence, the probability of a prediction of one-boxing is\nconstant with respect to one’s choice, and maximization of\nevidential expected-utility agrees with the principle of dominance.\nThis defense of evidential decision theory is called the tickle\ndefense because it assumes that an introspected condition screens off\nthe correlation between choice and prediction. \nEells’s defense of evidential decision theory assumes that an\nagent chooses according to beliefs and desires and knows his beliefs\nand desires. Some agents may not choose this way and may not have this\nknowledge. Decision theory should prescribe a rational choice for such\nagents, and evidential decision theory may not do that correctly, as\nLewis (1981: 10–11) and John Pollock (2010) argue. Armendt\n(1988b: 326–329) and David Papineau (2001: 252–255) concur\nthat the phenomenon of screening off does not in all cases make\nevidential decision theory yield the results of causal decision\ntheory. \nHorwich (1987: Chap. 11) rejects Eells’s argument because, even\nif an agent knows that her choice springs from her beliefs and\ndesires, she may be unaware of the mechanism by which her beliefs and\ndesires produce her choice. The agent may doubt that she chooses by\nmaximizing expected utility. Then in Newcomb’s problem her\nchoice may offer relevant evidence about the prediction. Eells (1984a)\nconstructs a dynamic version of the tickle defense to meet this\nobjection. Sobel (1994: Chap. 2) discusses that version of the\ndefense. He argues that it does not yield evidential decision\ntheory’s agreement with causal decision theory in all decision\nproblems in which an act furnishes evidence concerning the state of\nthe world. Moreover, it does not establish that an evidential theory\nof rational desire agrees with a causal theory of rational desire. He\nconcludes that even in cases where evidential decision theory yields\nthe right recommendation, it does not yield it for the right\nreasons. \nPrice (2012) proposes a blend of evidential and causal decision theory\nand motivates it with an analysis of cases in which an agent has\nforeknowledge of an event occurring by chance. Causal decision theory\non its own accommodates such cases, argues Bales (2016). \nAhmed (2014a) champions evidential decision theory and advances several\nobjections to causal decision theory. His objections assume some\ncontroversial points about rational choice, including a controversial\nprinciple for sequences of choices. \nA common view distinguishes principles for evaluating choices from\nprinciples for evaluating sequences of choices. The principle of\nutility maximization evaluates an agent’s choice as a resolution\nof a decision problem only if the agent has direct control of each\noption in the decision problem, that is, only if the agent can at will\nimmediately adopt any option in the decision problem. The principle\ndoes not evaluate an agent’s sequence of multiple choices\nbecause the agent does not have direct control of such a sequence. She\nrealizes a sequence of multiple choices only by making each choice in\nthe sequence at the time for it; she cannot at will immediately\nrealize the entire sequence. Rationality evaluates an option in an\nagent’s direct control by comparing it with alternatives but\nevaluates a sequence in an agent’s indirect control by\nevaluating the directly controlled options in the sequence; a sequence\nof choices is rational if the choices in the sequence are rational.\nAdopting this common method of evaluating sequences of choices fends\noff objections to causal decision theory that assume rival\nmethods. \nDecision theory is an active area of research. Current work addresses\na number of problems. Causal decision theory’s approach to those\nproblems arises from its nonpositivistic methodology and its attention\nto causation. This section mentions some topics on causal decision\ntheory’s agenda. \nPrinciples of causal decision theory use probabilities and utilities.\nThe interpretation of probabilities and utilities is a matter of\ndebate. One tradition defines them in terms of functions that\nrepresentation theorems introduce to depict preferences. The\nrepresentation theorems show that if preferences meet certain\nstructural axioms, then if they also meet certain normative axioms,\nthey are as if they follow expected utility. That is, preferences\nfollow expected utility calculated using probability and utility\nfunctions constructed so that preferences follow expected utility.\nExpected utility calculated this way differs from expected utility\ncalculated using probability and utility assignments grounded in\nattitudes toward possible outcomes. For example, a person confused\nabout bets concerning a coin toss may have preferences among those\nbets that are as if he assigns probability 60% to heads, when, in\nfact, the evidence of past tosses leads him to assign probability 40%\nto heads. Consequently, when preferences meet a representation\ntheorem’s structural axioms, the theorem’s normative\naxioms justify only conformity with expected utility fabricated to\nagree with preferences and do not justify conformity with expected\nutility in the traditional sense. Defining probability and utility\nusing the representation theorems thus weakens the traditional\nprinciple of expected utility. It becomes merely a principle of\ncoherence among preferences. \nInstead of using the representation theorems to define probabilities\nand utilities, decision theory may use them to establish\nprobabilities’ and utilities’ measurability when\npreferences meet structural and normative axioms. This employment of\nthe representation theorems allows decision theory to advance the\ntraditional principle of expected utility and thereby enrich its\ntreatment of rational decisions. Decision theory may justify that\ntraditional principle by deriving it from general principles of\nevaluation, as in Weirich (2001). \nA broad account of probabilities and utilities takes them to indicate\nattitudes toward propositions. They are rational degrees of belief and\nrational degrees of desire, respectively. This account of\nprobabilities and utilities recognizes their existence in cases where\nthey are not inferable from preferences or their other effects but\ninstead are inferable from their causes, such as an agent’s\ninformation about objective probabilities, or are not inferable at all\n(except perhaps by introspection). The account relies on arguments\nthat degrees of belief and degrees of desire, if rational, conform to\nstandard principles of probability and utility. Bolstering these\narguments is work for causal decision theory. \nBesides clarifying its general interpretation of probability and\nutility, causal decision theory searches for the particular\nprobabilities and utilities that yield the best version of its\nprinciple to maximize expected utility. The causal probabilities in\nits formula for expected utility may be probabilities of subjunctive\nconditionals or various substitutes. Versions that use probabilities\nof subjunctive conditionals must settle on an analysis of those\nconditionals. Lewis (1973: Chap. 1) modifies Stalnaker’s\nanalysis to count a subjunctive conditional true if and only if as\nantecedent worlds come closer and closer to the actual world, there is\na point beyond which the consequent is true in all the worlds at least\nthat close. Joyce (1999: 161–180) advances probability images,\nas Lewis (1976) introduces them, as substitutes for probabilities of\nsubjunctive conditionals. The probability image of a state \\(S\\) under\nsubjunctive supposition of an act \\(A\\) is the probability of \\(S\\)\naccording to an assignment that shifts the probability of\n\\({\\sim}A\\)-worlds to nearby \\(A\\)-worlds. Causal relations among an\nact and possible states guide probability’s reassignment. A common formula for an act’s expected utility takes the\nutility for an act-state pair, the utility of the act’s outcome\nin the state, to be the utility of the act’s and the\nstate’s conjunction: \nDoes causal decision theory need an alternative, more\ncausally-sensitive utility for an act-state pair?  Weirich (1980)\nargues that it does. A person contemplating a wager that the capital\nof Missouri is Jefferson City entertains the consequences if\nhe were to make the wager given that St. Louis is\nMissouri’s capital. A rational deliberator subjunctively\nsupposes an act attending to causal relations and indicatively\nsupposes a state attending to evidential relations, but can suppose an\nact’s and a state’s conjunction only one way. Furthermore,\nusing the utility of an act’s and a state’s conjunction\nprevents an act’s expected utility from being\npartition-invariant. The next subsection elaborates this point. \nAn act’s expected utility is partition invariant if and only if\nit is the same under all partitions of states. Partition invariance is\na vital property of an act’s expected utility. If acts’\nexpected utilities lack this property, then decision theory may use\nonly expected utilities computed from selected partitions. Expected\nutility’s partition invariance makes an act’s expected\nutility independent of selection of a partition of states and thereby\nincreases expected utility’s explanatory power. \nPartition invariance ensures that various representations of the same\ndecision problem yield solutions that agree. Take Newcomb’s\nproblem with Figure 2’s representation. \nFigure 2. New States for Newcomb’s\nProblem \nDominance does not apply to this representation. It nonetheless\nsettles the problem’s solution because it applies to a decision\nproblem if it applies to any accurate representation of the problem,\nsuch as Figure 1’s representation of the problem. If expected\nutilities are partition-sensitive, then acts that maximize expected\nutility may be partition-sensitive. The principle of expected utility\ndoes not yield a decision problem’s solution, however, if acts\nof maximum expected-utility change from one partition to another. In\nthat case an act is not a solution to a decision problem simply\nbecause it maximizes expected utility under some accurate\nrepresentation of the problem. Too many acts have the same\ncredential. \nThe expected utility principle, using probabilities of conditionals,\napplies to Figure 2’s representation of Newcomb’s problem.\nLetting \\(P1\\) stand for a prediction of one-boxing and \\(P2\\) stand\nfor a prediction of two-boxing, the acts’ expected utilities\nare: \nHence \\(\\textit{EU}(1) \\lt EU(2)\\). This result agrees with the\nverdict of causal decision theory given other accurate representations\nof the problem. Provided that causal decision theory uses a\npartition-invariant formula for expected utility, its recommendations\nare independent of a decision problem’s representation. Lewis (1981: 12–13) observes that the formula \nis not partition invariant. Its results depend on the partition of\nstates. If a state is a set of worlds with equal utilities, then with\nrespect to a partition of such states every act has the same expected\nutility. An element \\(S_i\\) of the partition obscures the effects of\n\\(A\\) that the utility of an outcome should evaluate. Lewis overcomes\nthis problem by using only partitions of dependency\nhypotheses. However, causal decision theory may craft a\npartition-invariant formula for expected utility by adopting a\nsubstitute for \\(U(A \\amp S_i)\\). Sobel (1994: Chap. 9) investigates partition invariance. Putting\nhis work in this essay’s notation, he proceeds as\nfollows. First, he takes a canonical computation of an option’s\nexpected utility to use worlds as states. His basic formula is \nA world \\(W_i\\) absorbs an act performed in it. Only the worlds in\nwhich \\(A\\) holds contribute positive probabilities and so affect the\nsum. Next, Sobel searches for other computations, using coarse-grained\nstates, that are equivalent to the canonical computation. A suitable\nspecification of utilities achieves partition invariance given his\nassumptions. According to a theorem he proves (1994: 185), \nfor any partition of states. Joyce (2000: S11) also articulates for causal decision theory a\npartition-invariant formula for an act’s expected utility. He\nachieves partition invariance, assuming that by stipulating that \\(U(A \\amp S_i)\\) equals \nwhere \\(W_j\\) is a world and \\(P^A\\) stands for the probability image\nof \\(A\\). Weirich (2001: Secs. 3.2, 4.2.2), as Sobel does, substitutes\n\\(U(A \\mbox{ given }S_i)\\) for \\(U(A \\amp S_i)\\) in the formula for\nexpected utility and interprets \\(U(A \\mbox{ given }S_i)\\) as the\nutility of the outcome that \\(A\\)’s realization would produce if\n\\(S\\) obtains. Accordingly, \\(U(A \\mbox{ given }S_i)\\) responds to\n\\(A\\)’s causal consequences in worlds where \\(S_i\\) holds. Then\nthe formula \nis invariant with respect to partitions in which states are\nprobabilistically independent of the act. A more complex formula, \nassuming a causal interpretation of its probabilities, relaxes all\nrestriction on partitions. \\(U(A \\mbox{ given }(S_i \\mbox{ if }A))\\)\nis the utility of the outcome if \\(A\\) were realized, given that it is\nthe case that \\(S_i\\) would obtain if \\(A\\) were realized. \nOne issue concerning outcomes is their comprehensiveness. Are an\nact’s outcomes possible worlds, temporal aftermaths, or causal\nconsequences? Gibbard and Harper ([1978] 1981: 166–168) mention\nthe possibility of narrowing outcomes to causal consequences, as\npractical applicability advocates. The narrowing must be judicious,\nhowever, because the expected-utility principle requires that outcomes\ninclude every relevant consideration. For example, if an agent is\naverse to risk, then each of a risky act’s possible outcomes\nmust include the risk the act generates. Its inclusion tends to lower\neach possible outcome’s utility. In Sobel’s canonical formula for expected utility, \nThe formula, from one perspective, omits states of the world because\nthe outcomes themselves form a partition. The distinction between\nstates and outcomes dissolves because worlds play the role of both\nstates and outcomes. States are dispensable means of generating\noutcomes that are exclusive and exhaustive. According to a basic\nprinciple, an act’s expected utility is a probability-weighted\naverage of possible outcomes that are exclusive and exhaustive, such\nas the worlds to which the act may lead. Suppose that a world’s utility comes from realization of\nbasic intrinsic desires and aversions. Granting that the utilities of\ntheir realizations are additive, the utility of a world is a sum of\nthe utilities of their realizations. Then besides being a\nprobability-weighted average of the utilities of worlds to which it\nmay lead, an option’s expected utility is also a\nprobability-weighted average of the realizations of basic intrinsic\ndesires and aversions. In this formula for its expected utility,\nstates play no explicit role: \nwhere \\(B_i\\) ranges over possible realizations of basic intrinsic\ndesires and aversions. The formula considers for each basic desire and\naversion the prospect of its realization if the act were performed. It\ntakes the act’s expected utility as the sum of the\nprospects’ utilities. The formula provides an economical\nrepresentation of an act’s expected utility. It eliminates\nstates and obtains expected utility directly from outcomes taken as\nrealizations of basic desires and aversions. \nTo illustrate calculation of an act’s expected utility using\nbasic intrinsic desires and aversions, suppose that an agent has no\nbasic intrinsic aversions and just two basic intrinsic desires, one\nfor health and the other for wisdom. The utility of health is 4, and\nthe utility of wisdom is 8. In the formula for expected utility, a\nworld covers only matters about which the agent cares. In the example,\na world is a proposition specifying whether the agent has health and\nwhether he has wisdom. Accordingly, there are four worlds:\n\n\\[\n\\begin{align}\nH  \\amp W, \\\\\nH   \\amp {\\sim}W, \\\\\n{\\sim}H   \\amp  W, \\\\\n{\\sim}H   \\amp {\\sim}W.\\\\\n\\end{align}\n\\] \n\n Suppose that \\(A\\) is equally likely to generate any\nworld. Using worlds, \n\n\\[\n\\begin{align}\n\\textit{EU} (A) & = P(A \\gt(H \\amp W))\\util (H \\amp W) \\\\\n&\\qquad + P(A \\gt(H \\amp{\\sim}W))\\util (H \\amp{\\sim}W) \\\\\n&\\qquad + P(A \\gt({\\sim}H \\amp W))\\util ({\\sim}H \\amp W) \\\\\n&\\qquad + P(A \\gt({\\sim}H \\amp{\\sim}W))\\util ({\\sim}H \\amp{\\sim}W) \\\\\n& = (0.25)(12) + (0.25)(4) + (0.25)(8) + (0.25)(0) \\\\\n& = 6.\\\\\n\\end{align}\n\n\\] \n\n Using basic intrinsic attitudes,\n\n\\[\n\\begin{align}\n\\textit{EU} (A) &= P(A \\gt H)\\util (H) + P(A \\gt W)\\util (W) \\\\\n& = (0.5)(4) + (0.5)(8) \\\\\n& = 6.\n\\end{align}\n\\]\n\n The two methods of computing an option’s utility\nare equivalent given that, under supposition of an act’s\nrealization, the probability of a basic intrinsic desire’s or\naversion’s realization is the sum of the probabilities of the\nworlds that realize it. \nIn deliberations, a first-person action proposition represents an act.\nThe proposition has a subject-predicate structure and refers directly\nto the agent, its subject, without the intermediary of a concept of\nthe agent. A centered world represents the proposition. Such a world\nnot only specifies individuals and their properties and relations, but\nalso specifies which individual is the agent and where and when his\ndecision problem arises. Realization of the act is realization of a\nworld with, at its center, the agent at the time and place of his\ndecision problem. \nIsaac Levi (2000) objects to any decision theory that attaches\nprobabilities to acts. He holds that deliberation crowds out\nprediction. While deliberating, an agent does not have beliefs or\ndegrees of belief about the act that she will perform. Levi holds that\nNewcomb’s problem, and evidential and causal decision theories\nthat address it, involve mistaken assignments of probabilities to an\nagent’s acts. He rejects both Jeffrey’s ([1965] 1983)\nevidential decision theory and Joyce’s (1999) causal decision\ntheory because they allow an agent to assign probabilities to her acts\nduring deliberation. \nIn opposition to Levi’s views, Joyce (2002) argues that (1)\ncausal decision theory need not accommodate an agent’s assigning\nprobabilities to her acts, but (2) a deliberating agent may\nlegitimately assign probabilities to her acts. Evidential decision\ntheory computes an act’s expected utility using the probability\nof a state given the act, \\(P(S\\mid A)\\), defined as \\(P(S \\amp A)/P(A)\\).\nThe fraction’s denominator assigns a probability to an act.\nCausal decision theory replaces \\(P(S\\mid A)\\) with \\(P(A \\gt S)\\) or a\nsimilar causal probability. It need not assign a probability to an\nact. \nMay an agent deliberating assign probabilities to her possible acts?\nYes, a deliberator may sensibly assign probabilities to any events,\nincluding her acts. Causal decision theory may accommodate such\nprobabilities by forgoing their measurement with betting quotients.\nAccording to that method of measurement, willingness to make bets\nindicates probabilities. Suppose that a person is willing to take\neither side of a bet in which the stake for the event is \\(x\\) and the\nstake against the event is \\(y\\). Then the probability the person\nassigns to the event is the betting quotient \\(x/(x + y)\\). This\nmethod of measurement may fail when the event is an agent’s own\nfuture act. A bet on an act’s realization may influence the\nact’s probability, as a thermometer’s temperature may\ninfluence the temperature of a liquid it measures. \nJoyce (2007: 552–561) considers whether Newcomb problems are\ngenuine decision problems despite strong correlations between states\nand acts. He concludes that, yes, despite those correlations, an agent\nmay view her decision as causing her act. An agent’s decision\nsupports a belief about her act independently of prior correlations\nbetween states and her act. According to a principle of evidential\nautonomy (2007: 557),  \nA deliberating agent who regards herself as free need not proportion\nher beliefs about her own acts to the antecedent evidence that she has\nfor thinking that she will perform them.  \nShe should proportion her beliefs to her total evidence, including her\nself-supporting beliefs about her own acts. Those beliefs provide new\nrelevant evidence about her acts. \nHow should an agent deliberating about an act understand the\nbackground for her act? She should not adopt a backtracking\nsupposition of her act. Standing on the edge of a cliff, she should\nnot suppose that if she were to jump, she would have a parachute to\nbreak her fall. Also, she should not imagine gratuitous changes in her\nbasic desires. She should not imagine that if she were to choose\nchocolate instead of vanilla, despite currently preferring vanilla,\nthat she would then prefer chocolate. She should imagine that her\nbasic desires are constant as she imagines the various acts she may\nperform, and, moreover, should adopt during deliberations the pretense\nthat her will generates her act independently of her basic desires and\naversions. \nChristopher Hitchcock (1996) holds that an agent should pretend that\nher act is free of causal influence. Doing this makes partitions of\nstates yielding probabilities for decision agree with partitions of\nstates yielding probabilities defining causal relevance. As a result,\nprobabilities in causal decision theory may form a foundation for\nprobabilities in the probabilistic theory of causation. Causal\ndecision theory, in particular, the version using dependency\nhypotheses, grounds theories of probabilistic causation. \nProblems such as Pascal’s Wager and the St. Petersburg paradox\nsuggest that decision theory needs a means of handling infinite\nutilities and expected utilities. Suppose that an option’s\npossible outcomes all have finite utilities. Nonetheless, if those\nutilities are infinitely many and unbounded, then the option’s\nexpected utility may be infinite. Alan Hájek and Harris Nover\n(2006) also show that the option may have no expected utility. The\norder of possible outcomes, which is arbitrary, may affect convergence\nof their utilities’ probability-weighted average and the value\nto which the average converges if it does converge. Causal decision\ntheory should generalize its principle of expected-utility\nmaximization to handle such cases. \nAlso, common principles of causal decision theory advance standards of\nrationality that are too demanding to apply to humans. They are\nstandards for ideal agents in ideal circumstances (a precise\nformulation of the idealizations may vary from theorist to theorist).\nMaking causal decision theory realistic requires relaxing\nidealizations that its principles assume. A generalization of the\nprinciple of expected-utility maximization, for example, may relax\nidealizations to accommodate limited cognitive abilities. Weirich\n(2004) and Pollock (2006) take steps in this direction. Appropriate\ngeneralizations distinguish taking maximization of expected utility as\na procedure for making a decision and taking it as a standard for\nevaluating a decision even after the decision has been made. \nGibbard and Harper (1978: Sec. 11) present a problem for causal\ndecision theory using an example drawn from literature. A man in\nDamascus knows that he has an appointment with Death at midnight. He\nwill escape Death if he manages at midnight not to be at the place of\nhis appointment. He can be in either Damascus or Aleppo at midnight.\nAs the man knows, Death is a good predictor of his whereabouts. If he\nstays in Damascus, he thereby has evidence that Death will look for\nhim in Damascus. However, if he goes to Aleppo he thereby has evidence\nthat Death will look for him in Aleppo. Wherever he decides to be at\nmidnight, he has evidence that he would be better off at the other\nplace. No decision is stable. Decision instability arises in cases in\nwhich a choice provides evidence for its outcome, and each choice\nprovides evidence that another choice would have been better. Reed\nRichter (1984, 1986) uses cases of decision instability to argue\nagainst causal decision theory. The theory needs a resolution of the\nproblem of decision instability. \nA common analysis of the problem classifies options as either\nself-ratifying or not self-ratifying. Jeffrey ([1965] 1983) introduced\nratification as a component of evidential decision theory. His version\nof the theory evaluates a decision according to the expected utility\nof the act it selects. The distinction between an act and a decision\nto perform the act grounds his definition of an option’s\nself-ratification and his principle to make self-ratifying, or\nratifiable, decisions. According to his definition ([1965] 1983: 16),\n \nA ratifiable decision is a decision to perform an act of maximum\nestimated desirability relative to the probability matrix the agent\nthinks he would have if he finally decided to perform that act.  \nEstimated desirability is expected utility. An agent’s\nprobability matrix is an array of rows and columns for acts and\nstates, respectively, with each cell formed by the intersection of an\nact’s row and a state’s column containing the probability\nof the state given that the agent is about to perform the act. Before\nperforming an act, an agent may assess the act in light of a decision\nto perform it. Information the decision carries may affect the\nact’s expected utility and its ranking with respect to other\nacts. \nJeffrey used ratification as a means of making evidential decision\ntheory yield the same recommendations as causal decision theory. In\nNewcomb’s problem, for instance, two-boxing is the only\nself-ratifying option. However, Jeffrey (2004: 113n) concedes that\nevidential decision theory’s reliance on ratification does not\nmake it agree with causal decision theory in all cases. Moreover,\nJoyce (2007) argues that the motivation for ratification appeals to\ncausal relations, so that even if it yields correct recommendations\nusing Jeffrey’s formula for expected-utility, it still does not\nyield a purely evidential decision theory. \nCausal decision theory’s account of self-ratification may put\naside Jeffrey’s method of evaluating a decision by evaluating\nthe act it selects. Because the decision and the act differ, they may\nhave different consequences. For example, a decision may fail to\ngenerate the act it selects. Hence, the decision’s expected\nutility may differ from the act’s expected utility. Driving\nthrough a flooded section of highway may have high expected utility\nbecause it minimizes travel time to one’s destination. However,\nthe decision to drive through the flooded section may have low\nexpected utility because for all one knows the water may be deep\nenough to swamp the car. Using an act’s expected utility to\nassess a decision to perform the act leads to faulty evaluations of\ndecisions. It is better to evaluate a decision by comparing its\nexpected utility to the expected utilities of rival decisions. A\ndecision’s expected utility depends on the probability of its\nexecution as well as the expected consequences of the act it\nselects. \nWeirich (1985) and Harper (1986) define ratification in terms of an\noption’s expected utility given its realization rather than\ngiven a decision to realize it. An option is self-ratifying if and\nonly if it maximizes expected utility given its realization. This\naccount of ratification accommodates cases in which an option and a\ndecision to realize it have different expected utilities. Weirich and\nHarper also assume causal decision theory’s formula for expected\nutility. In the case of Death in Damascus, causal decision theory\nconcludes that the threatened man lacks a self-ratifying option. A\nself-ratifying option emerges, however, if the man may flip a coin to\nmake his decision. Adopting the probability distribution for locations\nis called a mixed strategy, whereas choices of location are called\npure strategies. Assuming that Death cannot predict the coin\nflip’s outcome, the mixed strategy is self-ratifying. \nDuring deliberations to resolve a decision problem, an agent may\nrevise the probabilities she assigns to pure strategies in light of\ncomputations of their expected utilities using earlier probability\nassignments. The process of revision may culminate in a stable\nprobability assignment that represents a mixed strategy. Skyrms (1982,\n1990) and Eells (1984b) investigate these dynamics of deliberation.\nSome open issues are whether adoption of a mixed strategy resolves a\ndecision problem and whether a pure strategy arising from a mixed\nstrategy that constitutes an equilibrium of deliberations is rational\nif the pure strategy itself is not self-ratifying. \nAndy Egan (2007) argues that causal decision theory yields the wrong\nrecommendation in decision problems with an option that provides\nevidence concerning its outcome. He entertains the case of an assassin\nwho deliberates about pulling the trigger, knowing that the\noption’s realization provides evidence of a brain lesion that\nruins his aim. Egan maintains that causal decision theory mistakenly\nignores the evidence that the option provides. However, versions of\ncausal decision theory that incorporate ratification are innocent of\nthe charges. Ratification takes account of evidence an option provides\nconcerning its outcome. \nAny version of the expected utility principle, whether it uses\nconditional probabilities or probabilities of conditionals, must\nspecify the information that guides assignments of probabilities and\nutilities. Principles of nonconditional expected-utility maximization\nuse the same information for all options, and hence exclude\ninformation about an option’s realization. The principle of\nratification uses for each option information that includes the\noption’s realization. It is a principle of conditional\nexpected-utility maximization. Egan’s cases count against\nnonconditional expected-utility maximization, and not against causal\ndecision theory. Conditional expected-utility maximization using\ncausal decision theory’s formula for expected utility addresses\nthe cases he presents. \nEgan’s examples do not refute causal decision theory but present\na challenge for it. Suppose that in a decision problem no\nself-ratifying option exists, or multiple self-ratifying options\nexist. How should a rational agent proceed, granting that a decision\nprinciple should take account of information that an option provides?\nThis is an open problem in causal decision theory (and in any decision\ntheory acknowledging that an option’s realization may constitute\nevidence concerning its outcome). Ratification analyzes decision\ninstability but is not a complete response to it. \nIn response to Egan, Frank Arntzenius (2008) and Joyce (2012) argue\nthat in some decision problems an agent’s rational deliberations\nusing freely available information do not settle on a single option\nbut instead settle on a probability distribution over options. They\nacknowledge that the agent may regret the option issuing from these\ndeliberations but differ about the regret’s significance.\nArntzenius holds that the regret counts against the option’s\nrationality, whereas Joyce denies this. Ahmed (2012) and Ralph\nWedgwood (2013) reject Arntzenius’s and Joyce’s responses\nto Egan because they hold that deliberations should settle on an\noption. Wedgwood introduces a novel decision principle to accommodate\nEgan’s decision problems. Ahmed contends that Egan’s\nanalysis of these decision problems has a flaw because when it is\nextended to some other decision problems, it declares every option\nirrational. \nAhmed (2014b) criticizes causal decision theory in cases of\ndecision instability.  Also, in such cases, Jack Spencer and Ian Wells\n(2019, preprint 2017) criticize a principle of causal dominance\nattributed to causal decision theory. This decision principle\nprohibits adopting an option if another causally dominates it. Joyce\n(2018) defends causal decision theory against Ahmed’s and Spencer and\nWells’s charges using points about deliberational dynamics. Armendt\n(2019) and Bales (2020) also defend causal decision theory’s handling\nof decision instability. \nPoints about ratification in decision problems clarify points about\nequilibrium in game theory because in games of strategy a\nplayer’s choice often furnishes evidence about other\nplayers’ choices. Decision theory underlies game theory because\na game’s solution identifies rational choices in the decision\nproblems the game creates for the players. Solutions to games\ndistinguish correlation and causation, as do decision principles.\nBecause in simultaneous-move games two agent’s strategies may be\ncorrelated but not related as cause and effect, solutions to such\ngames do not have the same properties as solutions to sequential\ngames. Causal decision theory attends to distinctions on which\nsolutions to games depend. It supports game theory’s account of\ninteractive decisions. Joyce and Gibbard (2016) describe the role of\nratification in game theory, and Stalnaker (2018) describes causal\ndecision theory’s place in game theory. \nThe existence of self-ratifying mixed strategies in decision problems\nsuch as Death in Damascus suggests that ratification, as causal\ndecision theory explains it, supports participation in a Nash\nequilibrium of a game. Such an equilibrium assigns a strategy to each\nplayer so that each strategy in the assignment is a best response to\nthe others. Suppose that two people are playing Matching Pennies.\nSimultaneously, each displays a penny. One player tries to make the\nsides match, and the other player tries to prevent a match. If the\nfirst player succeeds, he gets both pennies. Otherwise, the second\nplayer gets both pennies. Suppose that each player is good at\npredicting the other player, and each player knows this. Then if the\nfirst player displays heads, he has reason to think that the second\nplayer displays tails. Also, if the first player displays tails, he\nhas reason to think that the second player displays heads. Because\nMatching Pennies is a simultaneous-move game, neither player’s\nstrategy influences the other player’s strategy, but each\nplayer’s strategy is evidence of the other player’s\nstrategy. Mixed strategies help resolve decision instability in this\ncase. If the first player flips his penny to settle the side to\ndisplay, then his mixed strategy is self-ratifying. The second\nplayer’s situation is similar, and she also reaches a\nself-ratifying strategy by flipping her penny. The combination of\nself-ratifying strategies is a Nash equilibrium of the game. \nWeirich (2004: Chap. 9) presents a method of selecting among multiple\nself-ratifying strategies, and hence a method by which a group of\nplayers may coordinate to realize a particular Nash equilibrium when\nseveral exist. Although decision instability is an open problem,\ncausal decision theory has resources for addressing it. The\ntheory’s eventual resolution of the problem will offer game\ntheory a justification for participation in a Nash equilibrium of a\ngame. \nCausal decision theory has foundations in various areas of philosophy.\nFor example, it relies on metaphysics for an account of causation. It\nalso relies on inductive logic for an account of inferences concerning\ncausation. A comprehensive causal decision theory treats not only\ncausal probabilities’ generation of options’ expected\nutilities, but also evidence’s generation of causal\nprobabilities. \nResearch concerning causation contributes to the metaphysical\nfoundations of causal decision theory. Nancy Cartwright (1979), for\nexample, draws on ideas about causation to flesh out details of causal\ndecision theory. Also, some accounts of causation distinguish types of\ncauses. Both oxygen and a flame are metaphysical causes of\ntinder’s combustion. However, only the flame is causally\nresponsible for, and so a normative cause of, the combustion. Causal\nresponsibility for an event accrues to just the salient metaphysical\ncauses of the event. Causal decision theory is interested not only in\nevents for which an act is causally responsible, but also in other\nevents for which an act is a metaphysical cause. Expected utilities\nthat guide decisions are comprehensive. \nJudea Pearl (2000) and also Peter Spirtes, Clark Glymour, and Richard\nScheines (2000) present methods of inferring causal relations from\nstatistical data. They use directed acyclic graphs and associated\nprobability distributions to construct causal models. In a decision\nproblem, a causal model yields a way of calculating an act’s\neffect. A causal graph and its probability distribution express a\ndependency hypothesis and yield each act’s causal influence\ngiven that hypothesis. They specify the causal probability of a state\nunder supposition of an act. An act’s expected utility is a\nprobability-weighted average of its expected utility according to the\ndependency hypotheses that candidate causal models represent, as\nWeirich (2015: 225–236) explains. \nA causal model’s directed graph and probability distribution\nindicate causal relations among event types. As Pearl (2000: 30) and\nSprites et al. (2000: 11) explain, a causal model meets the causal\nMarkov condition if and only if with respect to its probability\ndistribution each event type in its directed graph is independent of\nall the event type’s nondescendants, given its parents. Given a\nmodel meeting the condition, knowledge of all an event’s direct\ncauses makes other information statistically irrelevant to the\nevent’s occurrence, except for information about the event and\nits effects. Knowledge of an event’s direct causes screens off\nevidence from indirect causes and independent effects of its causes.\nGiven a typical causal model for Newcomb’s problem, knowledge of\nthe common cause of a decision and a prediction screens off the\ncorrelation between the decision and the prediction. \nDirected acyclic graphs present causal structure clearly, and so\nclarify in decision theory points that depend on causal structure. For\nexample, Eells (2000) observes that choice is not genuine unless a\ndecision screens off an act’s correlation with states. Joyce\n(2007: 546) uses a causal graph to depict how this may happen in a\nNewcomb problem that arises in a Prisoner’s Dilemma with a\npsychological twin. He shows that the Newcomb problem is a genuine\nchoice despite correlation of acts and states because a decision\nscreens off that correlation. Wolfgang Spohn (2012) constructs for\nNewcomb’s problem a causal model that distinguishes a decision\nand its execution and argues that given the model causal decision\ntheory recommends one-boxing. An act in a decision problem may\nconstitute an intervention in the causal model for the decision\nproblem, as Meek and Glamour (1994) explain. Hitchcock (2016) and Joyce and Gibbard (2016)\nmaintain that treating an act as an intervention enriches causal\ndecision theory. \nTimothy Williamson (2007: Chap. 5) studies the epistemology of\ncounterfactual, or subjunctive, conditionals. He points out their role\nin contingency planning and decision making. According to his account,\none learns a subjunctive conditional if one robustly obtains its\nconsequent when imagining its antecedent. Experience disciplines\nimagination. The experience leading to a judgment that a subjunctive\nconditional holds may be neither strictly enabling nor strictly\nevidential so that knowledge of the conditional is neither purely\na priori nor purely a posteriori. Williamson claims\nthat knowledge of subjunctive conditionals is foundational so that\ndecision theory appropriately grounds knowledge of an act’s\nchoiceworthiness in knowledge of such conditionals. \nMost texts on decision theory are consistent with causal decision\ntheory. Many do not treat the special cases, such as Newcomb’s\nproblem, that motivate a distinction between causal and evidential\ndecision theory. For example, Leonard Savage (1954) analyzes only\ndecision problems in which options do not affect probabilities of\nstates, as his account of utility makes clear (1954: 73). Causal and\nevidential decision theories reach the same recommendations in these\nproblems. Causal decision theory is the prevailing form of decision\ntheory among those who distinguish causal and evidential decision\ntheory.","contact.mail":"weirichp@missouri.edu","contact.domain":"missouri.edu"}]
