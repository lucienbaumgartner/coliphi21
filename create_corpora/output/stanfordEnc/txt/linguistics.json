[{"date.published":"2011-09-21","date.changed":"2015-01-01","url":"https://plato.stanford.edu/entries/linguistics/","author1":"Barbara C. Scholz","author2":"Francis Jeffry Pelletier","author1.info":"http://www.ualberta.ca/~francisp/","author2.info":"http://www.ling.ed.ac.uk/~gpullum/","entry":"linguistics","body.text":"\n\n\n\nPhilosophy of linguistics is the philosophy of science as applied to \nlinguistics. This differentiates it sharply from the philosophy of \nlanguage, traditionally concerned with matters of meaning and \nreference.\n\n\n\nAs with the philosophy of other special sciences, there are general \ntopics relating to matters like methodology and explanation (e.g., \nthe status of statistical explanations in psychology and sociology, \nor the physics-chemistry relation in philosophy of chemistry), and \nmore specific philosophical issues that come up in the special \nscience at issue (simultaneity for philosophy of physics; \nindividuation of species and ecosystems for the philosophy of \nbiology). General topics of the first type in the philosophy of \nlinguistics include:\n\nWhat the subject matter is,\n\nWhat the theoretical goals are,\n\nWhat form theories should take, and\n\nWhat counts as data.\n\n\nSpecific topics include issues in language learnability, language \nchange, the competence-performance distinction, and the expressive \npower of linguistic theories.\n\n\n\nThere are also topics that fall on the borderline between philosophy \nof language and philosophy of linguistics: of “linguistic \nrelativity” (see the supplement on the linguistic relativity hypothesis \n  in the Summer 2015 archived version of the entry on \n relativism),\n language vs. \n idiolect,\n \n speech acts\n (including the distinction between locutionary, illocutionary, and \nperlocutionary acts), the language of thought, implicature, and the \nsemantics of mental states (see the entries on \n analysis,\n\n  semantic compositionality,\n \n mental representation,\n \n pragmatics,\n and \n defaults in semantics and pragmatics).\n In these cases it is often the kind of answer given and not the \ninherent nature of the topic itself that determines the \nclassification. Topics that we consider to be more in the philosophy \nof language than the philosophy of linguistics include intensional \ncontexts, direct reference, and empty names (see the entries on \n propositional attitude reports,\n \n intensional logic,\n \n rigid designators,\n \n reference,\n and \n descriptions).\n \n\n\n\nThis entry does not aim to provide a general introduction to \nlinguistics for philosophers; readers seeking that should consult a \nsuitable textbook such as Akmajian et al. (2010) or Napoli (1996). \nFor a general history of Western linguistic thought, including recent\ntheoretical linguistics, see Seuren (1998). Newmeyer (1986) is useful\nadditional reading for post-1950 American linguistics. \nTomalin (2006) traces the philosophical, scientific, and \nlinguistic antecedents of Chomsky’s magnum opus (1955/1956; published\n1975), and Scholz and Pullum (2007) provide a critical review.\n\n\n\nThe issues we discuss have been debated with vigor and sometimes \nvenom. Some of the people involved have had famous exchanges in the \nlinguistics journals, in the popular press, and in public forums. To \nunderstand the sharp disagreements between advocates of the \napproaches it may be useful to have a sketch of the dramatis personae\nbefore us, even if it is undeniably an oversimplification. \n\nWe see three tendencies or foci, divided by what they take to be the \nsubject matter, the approach they advocate for studying it, and what \nthey count as an explanation. We characterize them roughly in Table \n1. Table 1. Three Approaches to the Study of Language \n\nA broad and varied range of distinct research projects can be pursued\nwithin any of these approaches; one advocate may be more motivated by\nsome parts of the overall project than others are. So the tendencies \nshould not be taken as sharply honed, well-developed research \nprograms or theories. Rather, they provide background biases for the \ndevelopment of specific research programs—biases which \nsometimes develop into ideological stances or polemical programs or \nlead to the branching off of new specialisms with separate journals. \nIn the judgment of Phillips (2010), “Dialog between adherents \nof different approaches is alarmingly rare.” \n\nThe names we have given these approaches are just mnemonic tags, not \ndescriptions. The Externalists, for example, might well have been \ncalled ‘structural descriptivists’ instead, since they \ntend to be especially concerned to develop models that can be used to\npredict the structure of natural language expressions. The \nExternalists have long been referred to by Essentialists as \n‘empiricists’ (and sometimes Externalists apply that term to \nthemselves), though this is misleading (see Scholz and Pullum 2006: \n60–63): the ‘empiricist’ tag comes with an \naccusation of denying the role of learning biases in language \nacquisition (see Matthews 1984, Laurence and Margolis 2001), but that\nis no part of the Externalists’ creed (see e.g. Elman 1993, Lappin \nand Shieber 2007). \n\nEmergentists are also sometimes referred to by Essentialists as \n‘empiricists’, but they either use the Emergentist label \nfor themselves (Bates et al. 1998, O’Grady 2008, MacWhinney 2005) or \ncall themselves ‘usage-based’ linguists (Barlow and \nKemmer 2002, Tomasello 2003) or ‘construction \ngrammarians’ (Goldberg 1995). Newmeyer (1991), like Tomasello, \nrefers to the Essentialists as ‘formalists’, because of \ntheir tendency to employ abstractions, and to use tools from \nmathematics and logic. \n\nDespite these terminological inconsistencies, we can look at what \ntypical members of each approach would say about their vision of \nlinguistic science, and what they say about the alternatives. Many of\nthe central differences between these approaches depend on what \nproponents consider to be the main project of linguistic theorizing, \nand what they count as a satisfying explanation. \n\nMany researchers—perhaps most—mix elements from each of \nthe three approaches. For example, if Emergentists are to explain the\nsyntactic structure of expressions by appeal to facts about the \nnature of the use of symbols in human communication, then they will \npresuppose a great deal of Externalist work in describing linguistic \npatterns, and those Externalists who work on computational parsing \nsystems frequently use (at least as a starting point) rule systems \nand ‘structural’ patterns worked out by Essentialists. \nCertainly, there are no logical impediments for a researcher with one\ntendency from simultaneously pursuing another; these approaches are \nonly general centers of emphasis. \n\nIf one assumes, with the Externalists, that the main goal of a \nlinguistic theory is to develop accurate models of the structural \nproperties of the speech sounds, words, phrases, and other linguistic\nitems, then the clearly privileged information will include corpora \n(written and oral)—bodies of attested and recorded language use\n(suitably idealized). The goal is to describe how this public record \nexhibits certain (perhaps non-phenomenal) patterns that are \nprojectable. \n\nAmerican structural linguistics of the 1920s to 1950s championed the \ndevelopment of techniques for using corpora as a basis for developing\nstructural descriptions of natural languages, although such work was \nreally not practically possible until the wide-spread availability of\ncheap, powerful, and fast computers. André Martinet (1960: 1) \nnotes that one of the basic assumptions of structuralist approaches \nto linguistics is that “nothing may be called \n‘linguistic’ that is not manifest or manifested one way \nor another between the mouth of the speaker and the ears of the \nlistener”. He is, however, quick to point out that “this \nassumption does not entail that linguists should restrict their field\nof research to the audible part of the communication \nprocess—speech can only be interpreted as such, and not as so \nmuch noise, because it stands for something else that is not \nspeech.” \n\nAmerican structuralists—Leonard Bloomfield in \nparticular—were attacked, sometimes legitimately and sometimes \nillegitimately, by certain factions in the Essentialist tradition. \nFor example, it was perhaps justifiable to criticize Bloomfield for \nadopting a nominalist ontology as popularized by the logical \nempiricists. But he was later attacked by Essentialists for holding \nanti-mentalist views about linguistics, when it is arguable that his \nactual view was that the science of linguistics should not commit \nitself to any particular psychological theory. (He had earlier been \nan enthusiast for the mentalist and introspectionist psychology of \nWilhelm Wundt; see Bloomfield 1914.) \n\nExternalism continues to thrive within computational linguistics, \nwhere the American structuralist vison of studying language through \nautomatic analysis of corpora has enjoyed a recrudescence, and very \nlarge, computationally searchable corpora are being used to test \nhypotheses about the structure of languages (see Sampson 2001, \nchapter 1, for discussion). \n\nEmergentists aim to explain the capacity for language in terms of \nnon-linguistic human capacities: thinking, communicating, and \ninteracting. Edward Sapir expressed a characteristic Emergentist \ntheme when he wrote: \n\nThe “pretty patterns” derided here are characteristic of \nstructuralist analyses. Sociolinguistics, which is much closer in \nspirit to Sapir’s project, studies the influence of social and \nlinguistic structure on each other. One particularly influential \nstudy, Labov (1966), examines the influence of social class on \nlanguage variation. Other sociolinguists examine the relation between\nstatus within a group on linguistic innovation (Eckert 1989). This \ninterest in variation within languages is characteristic of \nEmergentist approaches to the study of language. \n\nAnother kind of Emergentist, like Tomasello (2003), will stress the \nrole of theory of mind and the capacity to use symbols to change \nconspecifics’ mental states as uniquely human preadaptations for \nlanguage acquisition, use, and invention. MacWhinney (2005) aims to \nexplain linguistic phenomena (such as phrase structure and \nconstraints on long distance dependencies) in terms of the way \nconversation facilitates accurate information-tracking and \nperspective-switching. \n\nFunctionalist research programs generally fall within the broad \ntendency to approach the study of language as an Emergentist. \nAccording to one proponent: \n\nAnd according to Russ Tomlin, a linguist who takes a functionalist \napproach: \n\nThe idea that linguistic form is autonomous, and more specifically \nthat syntactic form (rather than, say, phonological form) is \nautonomous, is a characteristic theme of the Essentialists. And the \nclaims of Van Valin and Tomlin to the effect that syntax is \nnot independent of semantics and pragmatics might tempt some\nto think that Emergentism and Essentialism are logically \nincompatible. But this would be a mistake, since there are a large \nnumber of nonequivalent autonomy of form theses. \n\nEven in the context of trying to explain what the autonomy thesis is,\nNewmeyer (1991: 3) talks about five formulations of the thesis, each \nof which can be found in some Essentialists’ writings, without \n(apparently) realizing that they are non-equivalent. One is the \nrelatively strong claim that the central properties of linguistic \nform must not be defined with essential reference to \n“concepts outside the system”, which suggests that no \nprimitives in linguistics could be defined in psychological or \nbiological terms. Another takes autonomy of form to be a \nnormative claim: that linguistic concepts ought not\nto be defined or characterized in terms of non-linguistic concepts. \nThe third and fourth versions are ontological: one denies that \ncentral linguistic concepts should be ontologically reduced \nto non-linguistic ones, and the other denies that they can \nbe. And in the fifth version the autonomy of syntax is taken to deny \nthat syntactic patterning can be explained in terms of \nmeaning or discourse functions. \n\nFor each of these versions of autonomy, there are Essentialists who \nagree with it. Probably the paradigmatic Essentialist agrees with \nthem all. But Emergentists need not disagree with them all. \nParadigmatic functionalists like Tomlin, Van Valin and MacWhinney \ncould in principle hold that the explanation of syntactic form, for \nexample, will ultimately be in terms of discourse functions and \nsemantics, but still accept that syntactic categories cannot be \nreduced to non-linguistic ones. \n\nIf Leonard Bloomfield is the intellectual ancestor of Externalism, \nand Sapir the father of Emergentism, then Noam Chomsky is the \nintellectual ancestor of Essentialism. The researcher with \npredominantly Essentialist inclinations aims to identify the \nintrinsic properties of language that make it what it is. For a huge \nmajority of practitioners of this approach—researchers in the \ntradition of generative grammar associated with \nChomsky—this means postulating universals of human linguistic \nstructure, unlearned but tacitly known, that permit and assist \nchildren to acquire human languages. This generative Essentialism has\na preference for finding surprising characteristics of languages that\ncannot be inferred from the data of usage, and are not predictable \nfrom human cognition or the requirements of communication. \n\nRather than being impressed with language variation, as are \nEmergentists and many Externalists, the generative Essentialists are \nextremely impressed with the idea that very young children of almost \nany intelligence level, and just about any social upbringing, acquire\nlanguage to the same high degree of mastery. From this it is inferred\nthat there must be unlearned features shared by all languages that \nsomehow assist in language acquisition. \n\nA large number of contemporary Essentialists who follow Chomsky’s \nteaching on this matter claim that semantics and pragmatics are not a\ncentral part of the study of language.\nIn Chomsky’s view, “it is possible that natural language has \nonly syntax and pragmatics” (Chomsky 1995: 26); that is, only \n“internalist computations and performance systems that access \nthem”; semantic theories are merely “part of an interface\nlevel” or “a form of syntax” (Chomsky 1992: 223). \n\nThus, while Bloomfield understood it to be a sensible practical \ndecision to assign semantics to some field other than linguistics \nbecause of the underdeveloped state of semantic research, Chomsky \nappears to think that semantics as standardly understood is not part \nof the essence of the language faculty at all. (In broad outline, \nthis exclusion of semantics from linguistics comports with Sapir’s \nview that form is linguistic but content is cultural.) \n\nAlthough Chomsky is an Essentialist in his approach to the study of \nlanguage, excluding semantics as a central part of linguistic theory \nclearly does not follow from linguistic Essentialism (Katz 1980 \nprovides a detailed discussion of Chomsky’s views on semantics). \nToday there are many Essentialists who do hold that \nsemantics is a component of a full linguistic theory. \n\nFor example, many linguists today are interested in the \nsyntax-semantics interface—the relationship between the surface\nsyntactic structure of sentences and their semantic interpretation. \nThis area of interest is generally quite alien to philosophers who \nare primarily concerned with semantics only, and it falls outside of \nChomsky’s syntactocentric purview as well. Linguists who work in the \nkind of semantics initiated by Montague (1974) certainly focus on the\nessential features of language (most of their findings appear to be \nof universal import rather than limited to the semantic rules of \nspecific languages). Useful works to consult to get a sense of the \nmodern style of investigation of the syntax-semantics interface would\ninclude Partee (1975), Jacobson (1996), Szabolcsi (1997), Chierchia \n(1998), Steedman (2000). \n\nThe discussion so far has been at a rather high level of abstraction.\nIt may be useful to contrast the three tendencies by looking at how \nthey each would analyze a particular linguistic phenomenon. We have \nselected the syntax of double-object clauses like \nHand the guard your pass (also called \nditransitive clauses), in which the verb is \nimmediately followed by a sequence of two noun phrases, the first \ntypically denoting a recipient and the second something transferred. \nFor many such clauses there is an alternative way of expressing \nroughly the same thing: for Hand the guard your pass there is \nthe alternative Hand your pass to the guard, in which the verb\nis followed by a single object noun phrase and the recipient is \nexpressed after that by a preposition phrase with to. We will \ncall these recipient-PP clauses. \n\nLarson (1988) offers a generative Essentialist approach to the syntax\nof double-object clauses. In order to provide even a rough outline of\nhis proposals, it will be very useful to be able to use tree \ndiagrams of syntactic structure. A tree is a mathematical \nobject consisting of a set of points called nodes \nbetween which certain relations hold. The nodes correspond to \nsyntactic units; left-right order on the page corresponds to temporal\norder of utterance between them; and upward connecting lines \nrepresent the relation ‘is an immediate subpart of’. \nNodes are labeled to show categories of phrases and words, such as \nnoun phrase (NP); preposition phrase (PP); and verb phrase (VP). When\nthe internal structure of some subpart of a tree is basically \nunimportant to the topic under discussion, it is customary to mask \nthat part with an empty triangle. Consider a simple example: an \nactive transitive clause like (Ai) and its passive equivalent (Aii). \n\nA tree structure for (Ai) is shown in (T1). \n\nIn analyses of the sort Larson exemplifies, the structure of an \nexpression is given by a derivation, which consists \nof a sequence of successively modified trees. Larson calls the \nearliest ones underlying structures. The last (and \nleast abstract) in the derivation is the surface structure, which \ncaptures properties relevant to the way the expression is written and\npronounced. The underlying structures are posited in order to better \nidentify syntactic generalizations. They are related to surface \nstructures by a series of operations called \ntransformations (which generative Essentialists \ntypically regard as mentally real operations of the human language \nfaculty). \n\nOne of the fundamental operations that a transformation can effect is\nmovement, which involves shifting a part of the \nsyntactic structure of a tree to another location within it. For \nexample, it is often claimed that passive clauses have very much the \nsame kinds of underlying structures as the synonymous active clauses,\nand thus a passive clause like (Aii) would have an underlying \nstructure much like (T1). A movement transformation would shift \nthe guard toward the end of the clause (and add by), \nand another would shift my pass into the position before the \nverb. In other words, passive clauses look much more like their \nactive counterparts in underlying structure. \n\nIn a similar way, Larson proposes that a double-object clause like \n(B.ii) has the same underlying structure as (B.i). \n\nMoreover, he proposes that the transformational operation of deriving\nthe surface structure of (B.ii) from the underlying structure of \n(B.i) is essentially the same as the one that derives the surface \nstructure of (A.ii) from the underlying structure of (A.i). \n\nLarson adopts many assumptions from Chomsky (1981) and subsequent \nwork. One is that all NPs have to be assigned Case \nin the course of a derivation. (Case is an abstract syntactic \nproperty, only indirectly related to the morphological case forms \ndisplayed by nominative, accusative, and genitive pronouns. Objective\nCase is assumed to be assigned to any NP in direct object position, \ne.g., my pass in (T1), and Nominative Case is assigned to an \nNP in the subject position of a tensed clause, e.g., the guard\nin (T1).) \n\nHe also makes two specific assumptions about the derivation of \npassive clauses. First, Case assignment to the position immediately \nafter the verb is “suppressed”, which entails that the NP\nthere will not get Case unless it moves to some other position. (The \nsubject position is the obvious one, because there it will receive \nNominative Case.) Second, there is an unusual assignment of semantic \nrole to NPs: instead of the subject NP being identified as the agent \nof the action the clause describes, that role is assigned to an \nadjunct at the end of the VP (the by-phrase in (A.ii); an \nadjunct is a constituent with an optional modifying role in its \nclause rather than a grammatically obligatory one like subject or \nobject). \n\nLarson proposes that both of these points about passive clauses have \nanalogs in the structure of double-object VPs. First, Case assignment\nto the position immediately after the verb is suppressed; and since \nLarson takes the preposition to to be the marker of Case, this means \nin effect that to disappears. This entails that the NP after \nto will not get Case unless it moves to some other position. \nSecond, there is an unusual assignment of semantic role to NPs: \ninstead of the direct object NP being identified as the entity \naffected by the action the clause describes, that role is assigned to\nan adjunct at the end of the VP. \n\nLarson makes some innovative assumptions about VPs. First, he \nproposes that in the underlying structure of a double-object clause \nthe direct object precedes the verb, the tree diagram being \n(T2). \n\nThis does not match the surface order of words (showed my pass to \nthe guard), but it is not intended to: it is an underlying \nstructure. A transformation will move the verb to the left of my \npass to produce the surface order seen in (B.i). \n\nSecond, he assumes that there are two nodes labeled VP in a \ndouble-object clause, and two more labeled V′, though there is \nonly one word of the verb (V) category. (Only the smaller VP and \nV′ are shown in the partial structure (T2).) \n\nWhat is important here is that (T2) is the basis for the \ndouble-object surface structure as well. To produce that, the \npreposition to is erased and an additional NP position (for \nmy pass) is attached to the V′, thus: \n\nThe additional NP is assigned the affected-entity semantic role. The \nother NP (the guard) does not yet have Case; but Larson \nassumes that it moves into the NP position before the verb. The \nresult is shown in (T4), where ‘e’ marks the empty\nstring left where some words have been moved away: \n\nLarson assumes that in this position the guard can receive \nCase. What remains is for the verb to move into a higher V position \nfurther to its left, to obtain the surface order: \n\nThe complete sequence of transformations is taken to give a deep \ntheoretical explanation of many properties of (B.i) and (B.ii), \nincluding such things as what could be substituted for the two NPs, \nand the fact there is at least rough truth-conditional equivalence \nbetween the two clauses. \n\nThe reader with no previous experience of generative linguistics will\nhave many questions about the foregoing sketch (e.g., whether it is \nreally necessary to have the guard after showed in \n(T3), then the opposite order in (T4), and finally the same order \nagain in (T5)). We cannot hope to answer such questions here; \nLarson’s paper is extremely rich in further assumptions, links to the\nprevious literature, and additional classes of data that he aims to \nexplain. But the foregoing should suffice to convey some of the \nflavor of the analysis. \n\nThe key point to note is that Essentialists seek underlying \nsymmetries and parallels whose operation is not manifest in the data \nof language use. For Essentialists, there is positive explanatory \nvirtue in hypothesizing abstract structures that are very far from \nbeing inferrable from performance; and the posited operations on \nthose structures are justified in terms of elegance and formal \nparallelism with other analyses, not through observation of language \nuse in communicative situations. \n\nMany Emergentists are favorably disposed toward the kind of \nconstruction grammar expounded in Goldberg (1995). \nWe will use her work as an exemplar of the Emergentist approach. The \nfirst thing to note is that Goldberg does not take double-object \nclauses like (B.ii) to be derived alternants of recipient-PP \nstructures like (B.i), the way Larson does. So she is not looking for\na regular syntactic operation that can relate their derivations; \nindeed, she does not posit derivations at all. She is interested in \nexplaining correlations between syntactic, semantic, and pragmatic \naspects of clauses; for example, she asks this question: \n\nThus she aims to explain why some verbs occur in both the \ndouble-object and recipient-PP kinds of expression and some do not. \n\nThe fundamental notion in Goldberg’s linguistic theory is that of a \nconstruction. A construction can be defined very \nroughly as a way of structurally composing words or phrases—a \nsort of template—for expressing a certain class of meanings. \nLike Emergentists in general, Goldberg regards linguistic theory as \ncontinuous with a certain part of general cognitive psychological \ntheory; linguistics emerges from this more general theory, and \nlinguistic matters are rarely fully separate from cognitive matters. \nSo a construction for Goldberg has a mental reality: it corresponds \nto a generalized concept or scenario expressible in a language, \nannotated with a guide to the linguistic structure of the expression. \n\nMany words will be trivial examples of constructions: a single concept paired \nwith a way of pronouncing and some details about grammatical \nrestrictions (category, inflectional class, etc.); but constructions \ncan be much more abstract and internally complex. The double-object \nconstruction, which Goldberg calls the Ditransitive Construction, is \na moderately abstract and complex one; she diagrams it thus (p. 50): \n\nThis expresses a set of constraints on how to use English to \ncommunicate the idea of a particular kind of scenario. The scenario \ninvolves a ternary relation CAUSE-RECEIVE holding between an\nagent (agt), a recipient (rec), and\na patient (pat). PRED is a variable that is filled \nby the meaning of a particular verb when it is employed in this \nconstruction. \n\nThe solid vertical lines downward from agt and \npat indicate that for any verb integrated into this \nconstruction it is required that its subject NP should express the \nagent participant, and the direct object (OBJ2) should \nexpress the patient participant. The dashed vertical line downward \nfrom rec signals that the first object (OBJ) may express the \nrecipient but it does not have to—the necessity of there being \na recipient is a property of the construction itself, and not every \nverb demands that it be made explicit who the recipient is. But if \nthere are two objects, the first is obligatorily associated with the \nrecipient role: We sent the builder a carpenter can only \nexpress a claim about the sending of a carpenter over to the builder,\nnever the sending of the builder over to where a carpenter is. \n\nWhen a particular verb is used in this construction, it may have \nobligatory accompanying NPs denoting what Goldberg calls \n“profiled participants” so that the match between the \nparticipant roles (agt, rec, \npat) is one-to-one, as with the verb hand. \nWhen this verb is used, the agent (‘hander’), recipient \n(‘handee’), and item transferred (‘handed’) \nmust all be made explicit. Goldberg gives the following diagram of \nthe “composite structure” that results when hand \nis used in the construction: \n\nBecause of this requirement of explicit presence, Hand him your \npass is grammatical, but *Hand him is not, and \nneither is *Hand your pass. The verb send, \non the other hand, illustrates the optional syntactic expression of \nthe recipient role: we can say Send a text message, which is \nunderstood to involve some recipient but does not make the recipient \nexplicit. \n\nThe R notation relates to the fact that particular verbs may express \neither an instance of causing someone to receive something, \nas with hand, or a means of causing someone to \nreceive something, as with kick: what Joe kicked Bill the \nball means is that Joe caused Bill to receive the ball by means \nof a kicking action. \n\nGoldberg’s discussion covers many subtle ways in which the scenario \ncommunicated affects whether the use of a construction is grammatical\nand appropriate. For example, there is something odd about \n?Joe kicked Bill the ball he was trying to kick to \nSam: the Ditransitive Construction seems best suited to cases of \nvolitional transfer (rather than transfer as an unexpected side \neffect of a blunder). However, an exception is provided by a class of\ncases in which the transfer is not of a physical object but is only \nmetaphorical: That guy gives me the creeps does not imply any \nvolitional transfer of a physical object. \n\nMetaphorical cases are distinguished from physical transfers in other\nways as well. Goldberg notes sentences like The music lent the \nevent a festive air, where the music is subject of the \nverb lend despite the fact that music cannot literally lend anything \nto anyone. \n\nGoldberg discusses many topics such as metaphorical extension, \nshading, metonymy, cutting, role merging, and also presents various \ngeneral principles linking meanings and constructions. One of these \nprinciples, the No Synonymy Principle, says that no two syntactically\ndistinct constructions can be both semantically and pragmatically \nsynonymous. It might seem that if any two sentences are synonymous, \npairs like this are: \n\nYet the two constructions cannot be fully synonymous, both \nsemantically and pragmatically, if the No Synonymy Principle is \ncorrect. And to support the principle, Goldberg notes purported \ncontrasts such as this: \n\nThere is a causation-as-transfer metaphor here, and it seems to be \ncompatible with the double object construction but not with the \nrecipient-PP. So (in Goldberg’s view) the two are not fully \nsynonymous. \n\nIt is no part of our aim here to provide a full account of the \ncontent of Goldberg’s discussion of double-object clauses. But what \nwe want to highlight is that the focus is not on finding abstract \nelements or operations of a purely syntactic nature that are \ncandidates for being essential properties of language per se. The \nfocus for Emergentists is nearly always on the ways in which meaning \nis conveyed, the scenarios that particular constructions are used to \ncommunicate, and the aspects of language that connect up with \npsychological topics like cognition, perception, and \nconceptualization. \n\nOne kind of work that is representative of the Externalist tendency is\nnicely illustrated by Bresnan et al. (2007) and Bresnan and Ford\n(2010). Bresnan and her colleagues defend the use of\ncorpora—bodies of attested written and spoken texts. One of\ntheir findings is that a number of types of expressions that linguists\nhave often taken to be ungrammatical do in fact turn up in actual\nuse. Essentialists and Emergentists alike have often, purely on the\nbasis of intuition, asserted that sentences like John gave Mary a\nkiss are grammatical but sentences like John gave a kiss to\nMary are no, as we see above with Goldberg’s (D)(ii). Bresnan and\nher colleagues find numerous occurrences of the latter sort on the\nWorld Wide Web, and conclude that they are not ungrammatical or even\nunacceptable, but merely dispreferred. \n\nBresnan and colleagues used a three-million-word collection of \nrecorded and transcribed spontaneous telephone conversations known as\nthe Switchboard corpus to study the double-object and recipient-PP \nconstructions. They first annotated the utterances with indications \nof a number of factors that they thought might influence the choice \nbetween the double-object and recipient-PP constructions: \n\nThey also coded the verb meanings by assigning them to half a dozen \nsemantic categories: \n\nThey then constructed a statistical model of the corpus: a \nmathematical formula expressing, for each combination of the factors \nlisted above, the ratio of the probabilities of the double object and\nthe recipient-PP. (To be precise, they used the natural logarithm of \nthe ratio of p to 1 − p, where p is the \nprobability of a double-object or recipient-PP in the corpus being of\nthe double-object form.) They then used logistic regression to \npredict the probability of fit to the data. \n\nTo determine how well the model generalized to unseen data, they \ndivided the data randomly 100 times into a training set and a testing\nset, fit the model parameters on each training set, and scored its \npredictions on the unseen testing set. The average percent of correct\npredictions on unseen data was 92%. All components of the model \nexcept number of the recipient NP made a statistically significant \ndifference—almost all at the 0.001 level. \n\nWhat this means is that knowing only the presence or absence of the \nsort of factors listed above they were reliably able to predict \nwhether double-object or recipient-PP structures would be used in a \ngiven context, with a 92% score accuracy rate. \n\nThe implication is that the two kinds of structure are not \ninterchangeable: they are reliably differentiated by the presence of \nother factors in the texts in which they occur. \n\nThey then took the model they had generated for the telephone speech \ndata and applied it to a corpus of written material: the Wall \nStreet Journal corpus (WSJ), a collection of 1987–9 \nnewspaper copy, only roughly edited. The main relevant difference \nwith written language is that the language producer has more \nopportunity to reflect thoughtfully on how they are going to phrase \nthings. It was reasonable to think that a model based on speech data \nmight not transfer well. But instead the model had 93.5% accuracy. \nThe authors conclude is that “the model for spoken English \ntransfers beautifully to written”. The main difference between \nthe corpora was found to be a slightly higher probability of the \nrecipient-PP structure in written English. \n\nIn a very thorough subsequent study, Bresnan and Ford (2010) show \nthat the results also correlate with native speakers’ metalinguistic \njudgments of naturalness for sentence structures, and with lexical \ndecision latencies (speed of deciding whether the words in a text \nwere genuine English words or not), and with a sentence completion \ntask (choosing the most natural of a list of possible completions of \na partial sentence). The results of these experiments confirmed that \ntheir model predicted participants’ performance. \n\nAmong the things to note about this work is that it was all done on \ndirectly recorded performance data: transcripts of people speaking to\neach other spontaneously on the phone in the case of the Switchboard \ncorpus, stories as written by newspaper journalists in the case of \nWSJ, measured responses of volunteer subjects in a laboratory in the \ncase of the psycholinguistic experiments of Bresnan and Ford (2010). \nThe focus is on identifying the factors in linguistic performance \nthat permit accurate prediction of future performance, and the \nmethods of investigation have a replicability and checkability that \nis familiar in the natural sciences. \n\nHowever, we should make it clear that the work is not some kind of \nclose-to-the-ground collecting and classifying of instances. The \nmodels that Bresnan and her colleagues develop are sophisticated \nmathematical abstractions, very far removed from the records of \nutterance tokens. They claim that these models “allow \nlinguistic theory to solve more difficult problems than it has in the\npast, and to build convergent projects with psychology, computer \nscience, and allied fields of cognitive science” (Bresnan et \nal. 2007: 69). \n\nIt is important to see that the contrast we have drawn here is not \njust between three pieces of work that chose to look at different \naspects of the phenomena associated with double-object sentences. It \nis true that Larson focuses more on details of tree structure, \nGoldberg more on subtle differences in meaning, and Bresnan et al. on\nfrequencies of occurrence. But that is not what we are pointing to. \nWhat we want to stress is that we are illustrating three different \nbroad approaches to language that regard different facts as likely to\nbe relevant, and make different assumptions about what needs to be \naccounted for, and what might count as an explanation. \n\nLarson looks at contrasts between different kinds of clause with \ndifferent meanings and see evidence of abstract operations affecting \nsubtle details of tree structure, and parallelism between \nderivational operations formerly thought distinct. \n\nGoldberg looks at the same facts and sees evidence not for anything \nto do with derivations but for the reality of specific \nconstructions—roughly, packets of syntactic, semantic, and \npragmatic information tied together by constraints. \n\nBresnan and her colleagues see evidence that readily observable facts\nabout speaker behavior and frequency of word sequences correlate \nclosely with certain lexical, syntactic, and semantic properties of \nwords. \n\nNothing precludes defenders of any of the three approaches from \npaying attention to any of the phenomena that the other approaches \nattend to. There is ample opportunity for linguists to mix aspects of\nthe three approaches in particular projects. But in broad outline \nthere are three different tendencies exhibited here, with \nstereotypical views and assumptions roughly as we laid them out in \nTable 1. \n\nThe complex and multi-faceted character of linguistic phenomena means\nthat the discipline of linguistics has a whole complex of \ndistinguishable subject matters associated with different research \nquestions. Among the possible topics for investigation are these: \n\nThere is no reason for all of the discipline of linguistics to \nconverge on a single subject matter, or to think that the entire \nfield of linguistics cannot have a diverse range of subject matters. \nTo give a few examples: \n\nMost saliently of all, Harris’s student Chomsky reacted strongly \nagainst indifference toward the mind, and insisted that the principal\nsubject matter of linguistics was, and had to be, a narrow \npsychological version of (i), and an individual, non-social, and \ninternalized conception of (ii). \n\nIn the course of advancing his view, Chomsky introduced a number of \nnovel pairs of terms into the linguistics literature: competence vs. \nperformance (Chomsky 1965); ‘I-language’ vs. \n‘E-language’ (Chomsky 1986); the faculty of language in \nthe narrow sense vs. the and faculty of language in the broad sense \n(the ‘FLN’ and ‘FLB’ of Hauser et al. 2002). \nBecause Chomsky’s terminological innovations have been adopted so \nwidely in linguistics, the focus of sections 2.1–2.3 will be to\nexamine the use of these expressions as they were introduced into the\nlinguistics literature and consider their relation to (i)-(vii). \n\nEssentialists invariably distinguish between what Chomsky (1965) \ncalled competence and performance. \nCompetence is what knowing a language confers: a tacit grasp of the \nstructural properties of all the sentences of a language. Performance\ninvolves actual real-time use, and may diverge radically from the \nunderlying competence, for at least two reasons: (a) an attempt to \nproduce an utterance may be perturbed by non-linguistic factors like \nbeing distracted or interrupted, changing plans or losing attention, \nbeing drunk or having a brain injury; or (b) certain capacity limits \nof the mechanisms of perception or production may be overstepped. \n\nEmergentists tend to feel that the competence/performance distinction\nsidelines language use too much. Bybee and McClelland put\nit this way: \n\nAnd Externalists are often concerned to describe and explain not only\nlanguage structure, but also the workings of processing mechanisms \nand the etiology of performance errors. \n\nHowever, every linguist accepts that some idealization away from the \nspeech phenomena is necessary. Emergentists and Externalists are \nalmost always happy to idealize away from sporadic speech errors. \nWhat they are not so keen to do is to idealize away from limitations \non linguistic processing and the short-term memory on which it \nrelies. Acceptance of a thoroughgoing competence/performance \ndistinction thus tends to be a hallmark of Essentialist approaches, \nwhich take the nature of language to be entirely independent of other\nhuman cognitive processes (though of course capable of connecting to \nthem). \n\nThe Essentialists’ practice of idealizing away from even \npsycholinguistically relevant factors like limits on memory and \nprocessing plays a significant role in various important debates \nwithin linguistics. Perhaps the most salient and famous is the issue \nof whether English is a finite-state language. \n\nThe claim that English is not accepted by any finite-state automaton \ncan only be supported by showing that every grammar for English has \ncenter- embedding to an unbounded depth (see Levelt 2008: 20–23\nfor an exposition and proof of the relevant theorem, originally from \nChomsky 1959). But even depth-3 center-embedding of clauses (a clause\ninterrupting a clause that itself interrupts a clause) is in practice\nextraordinarily hard to process. Hardly anyone can readily understand\neven semantically plausible sentences like Vehicles that engineers\nwho car companies trust build crash every day. And such sentences\nvirtually never occur, even in writing. Karlsson (2007) undertakes an\nextensive examination of available textual material, and concludes \nthat depth-3 center-embeddings are vanishingly rare, and no genuine \ndepth-4 center-embedding has ever occurred at all in naturally \ncomposed text. He proposes that there is no reason to regard \ncenter-embedding as grammatical beyond depth 3 (and for spoken \nlanguage, depth 2). Karlsson is proposing a grammar that stays close \nto what performance data can confirm; the standard Essentialist view \nis that we should project massively from what is observed, and say \nthat depth-n center-embedding is fully grammatical for all \nn. \n\nChomsky (1986) introduced into the linguistics literature two \ntechnical notions of a language: ‘E-Language’ and \n‘I-Language’. He deprecates the former as either \nundeserving of study or as a fictional entity, and promotes the latter \nas the only scientifically respectable object of study for a serious \nlinguistics. \n\nChomsky’s notion ‘E-language’ is supposed to suggest by \nits initial ‘E’ both ‘extensional’ (concerned\nwith which sentences happen to satisfy a definition of a language \nrather than with what the definition says) and ‘external’\n(external to the mind, that is, non-mental). The dismissal of \nE-language as an object of study is aimed at critics of \nEssentialism—many but not all of those critics falling within \nour categories of Externalists and Emergentists. \n\nExtensional. First, there is an attempt to impugn the\nextensional notion of a language that is found in two radically\ndifferent strands of Externalist work. Some Externalist investigations\nare grounded in the details of attested utterances (as collected in\ncorpora), external to human minds. Others, with mathematical or\ncomputational interests, sometimes idealize languages as extensionally\ndefinable objects (typically infinite sets of strings) with a certain\nstructure, independently of whatever device might be employed to\ncharacterize them. A set of strings of words either is or is not\nregular (finite-state), either is or is not recursive (decidable),\netc., independently of forms of grammar statement. Chomsky (1986)\nbasically dismissed both corpus-based work and mathematical\nlinguistics simply on the grounds that they employ an extensional\nconception of language that is, a conception that removes the object\nof study from having an essential connection with the mental. \n\nExternal. Second, a distinct meaning based on \n‘external’ was folded into the neologism \n‘E-language’ to suggest criticism of any view that \nconceives of a natural language as a public, intersubjectively \naccessible system used by a community of people (often millions of \nthem spread across different countries). Here, the objection is that \nlanguages as thus conceived have no clear criteria of individuation \nin terms of necessary and sufficient conditions. On this conception, \nthe subject matter of interest is a historico-geographical entity \nthat changes as it is transmitted over generations, or over mountain \nranges. Famously, for example, there is a gradual valley-to-valley \nchange in the language spoken between southeastern France and \nnorthwestern Italy such that each valley’s speakers can understand \nthe next. But the far northwesterners clearly speak French and the \nfar southeasterners clearly speak Italian. It is the politically \ndefined geographical border, not the intrinsic properties of the \ndialects, that would encourage viewing this continuum as two \ndifferent languages. \n\nPerhaps the most famous quotation by any linguist is standardly \nattributed to Max Weinreich (1945): ‘A shprakh iz a dialekt mit\nan armey un flot’ (‘A language is a dialect with an army \nand navy’; he actually credits the remark to an unnamed \nstudent). The implication is that E-languages are defined in terms of\nnon-linguistic, non-essential properties. Essentialists object that a\nscientific linguistics cannot tolerate individuating French and \nItalian in a way that is subject to historical contingencies of wars \nand treaties (after all, the borders could have coincided with a \ndifferent hill or valley had some battle had a different outcome). \n\nConsiderations of intelligibility fare no better. Mutual \nintelligibility between languages is not a transitive relation, and \nsometimes the intelligibility relation is not even symmetric \n(smaller, more isolated, or less prestigious groups often understand \nthe dialects of larger, more central, or higher-prestige groups when \nthe converse does not hold). So these sociological facts cannot \nindividuate languages either. \n\nChomsky therefore concludes that languages cannot be defined or\nindividuated extensionally or mind-externally, and hence the only\nscientifically interesting conception of a ‘language’ is\nthe ‘I-language’ view (see for example Chomsky 1986: 25;\n1992; 1995 and elsewhere). Chomsky says of E-languages that “all\nscientific approaches have simply abandoned these elements of what is\ncalled ‘language’ in common usage” (Chomsky 1988,\n37); and “we can define E-language in one way or another or not\nat all, since the concept appears to play no role in the theory of\nlanguage” (Chomsky 1986: 26; in saying that it appears to play\nno role in the theory of language, here he means that it plays no role\nin the theory he favours). \n\nThis conclusion may be bewildering to non-linguists as well as \nnon-Essentialists. It is at odds with what a broad range of \nphilosophers have tacitly assumed or explicitly claimed about \nlanguage or languages: ‘[A language] is a practice in which \npeople engage…it is constituted by rules which it is part of \nsocial custom to follow’ (Dummett 1986: 473–473); \n‘Language is a set of rules existing at the level of common \nknowledge’ and these rules are ‘norms which govern \nintentional social behavior’ (Itkonen 1978: 122), and so on. \nGenerally speaking, those philosophers influenced by Wittgenstein \nalso take the view that a language is a social-historical entity. But\nthe opposite view has become a part of the conceptual underpinning of\nlinguistics for many Essentialists. \n\nFailing to have precise individuation conditions is surely not a \nsufficient reason to deny that an entity can be studied \nscientifically. ‘Language’ as a count noun in the \nextensional and socio-historical sense is vague, but this need not be\nany greater obstacle to theorizing about them than is the vagueness \nof other terms for historical entities without clear individuation \nconditions, like ‘species’ and ‘individual \norganism’ in biology. \n\nAt least some Emergentist linguists, and perhaps some Externalists,\nwould be content to say that languages are collections of social\nconventions, publicly shared, and some philosophers would agree (see\nMillikan 2003, for example, and Chomsky 2003 for a reply). Lewis\n(1969) explicitly defends the view that language can be understood in\nterms of public communications, functioning to solve coordination\nproblems within a group (although he acknowledges that the\ncoordination could be between different temporal stages of one\nindividual, so language use by an isolated person is also\nintelligible; see the appendix “Lewis’s Theory of Languages\nas Conventions” in the entry on\n idiolects,\n for further discussion of Lewis). What Chomsky calls E-languages, \nthen, would be perfectly amenable to linguistic or philosophical \nstudy. \n\nChomsky (1986) introduced the neologism ‘I-language’ in \npart to disambiguate the word ‘grammar’. In earlier \ngenerative Essentialist literature, ‘grammar’ was \n(deliberately) ambiguous between (i) the linguist’s generative theory\nand (ii) what a speaker knows when they know a language. \n‘I-language’ can be regarded as a replacement for Bever’s\nterm ‘psychogrammar’ (see also George 1989): it denotes a\nmental or psychological entity (not a grammarian’s description of a \nlanguage as externally manifested). \n\nI-language is first discussed under the sub-heading of \n‘internalized language’ to denote linguistic knowledge. \nLater discussion in Chomsky 1986 and 1995 makes it clear that the \n‘I’ of ‘I-language’ is supposed to suggest at\nleast three English words: ‘individual’, \n‘internal’, and ‘intensional’. And Chomsky \nemphasizes that the neologism also implies a kind of realism about \nspeakers’ knowledge of language. \n\nIndividual. A language is claimed to be strictly a \nproperty of individual human beings—not groups. The contrast is\nbetween the idiolect of a single individual, and a dialect or \nlanguage of a geographical, social, historical, or political group. \nI-languages are properties of the minds of individuals who know them. \n\nInternal. As generative Essentialists see it, your \nI-language is a state of your mind/brain. Meaning is \ninternal—indeed, on Chomsky’s conception, an I-language \n\nAnd he clarifies the sense in which an I-language is internal by \nappealing to an analogy with the way the study of vision is internal: \n\nThus, while the speaker’s I-language may be involved in performing \noperations over representations of distal \nstimuli—representations of other speaker’s \nutterances—I-languages can and should be studied in isolation \nfrom their external environments. \n\nAlthough Chomsky sometimes refers to this narrow individuation of \nI-languages as ‘individual’, he clearly claims that \nI-languages are individuated in isolation from both speech \ncommunities and other aspects of the broadly conceived natural \nenvironment: \n\nThis passage can also be seen as suggesting a radically \nintensionalist conception of language. \n\nIntensional. The way in which I-languages are \n‘intensional’ for Chomsky needs a little explication. The\nconcept of intension is familiar in logic and semantics, where \n‘intensional’ contrasts with ‘extensional’. \nThe extension of a predicate like blue is simply the set of all blue \nobjects; the intension is the function that picks out in a given \nworld the blue objects contained therein. In a similar way, the \nextension of a set can be distinguished from an intensional \ndescription of the set in terms of a function: the set of integer \nsquares is {1, 4, 9, 16, 25, 36, …}, and the intension\ncould be given in terms of the one-place function f such that \nf(n) = n × n. One difference between\nthe two accounts of squaring is that the intensional one could be\napplied to a different domain (any domain on which the\n‘×’ operation is defined: on the rationals rather\nthan the integers, for example, the extension of the identically\ndefined function is a different and larger set containing infinitely\nmany fractions). \n\nIn an analogous way, a language can be identified with the set of all\nand only its expressions (regardless of what sort of object an \nexpression is: a word sequence, a tree structure, a complete \nderivation, or whatever), which is the extensional view; but it can \nalso be identified intensionally by means of a recipe or formal \nspecification of some kind—what linguists call a grammar. \n\nIn natural language semantics, an intensional context is one where \nsubstitution of co-extensional terms fails to preserve truth value \n(Scott is Scott is true, and Scott is the author of \nWaverley is true, but the truth of George knows that Scott is \nScott doesn’t guarantee the truth of George knows that Scott \nis the author of Waverly, so knows that establishes an \nintensional context). \n\nChomsky claims that the truth of an I-language attribution is not \npreserved by substituting terms that have the same extension. That \nis, even when two human beings do not differ at all on what \nexpressions are grammatical, it may be false to say that they have \nthe same I-language. Where H is a human being and L is a language (in\nthe informal sense) and R is the relation of knowing (or having, or \nusing) that holds between a human being and a language, Chomsky \nholds, in effect, that R establishes an intensional context in \nstatements of the theory: \n\nThe idea is that two individuals can know (or have, or use) different\nI-languages that generate exactly the same strings of words, and even\ngive them exactly the same structures. \n\nThe generative Essentialist conception of an I-language is \nantithetical to Emergentist research programs. If the fundamental \nexplanandum of scientific linguistics is how actual linguistic \ncommunication takes place, one must start by looking at both internal\n(psychological) and external (public) practices and conventions in \nvirtue of which it occurs, and consider the effect of historical and \ngeographic contingencies on the relevant underlying processes. That \nwould not rule out ‘I-language’ as part of the explanans;\nbut some Emergentists seem to be fictionalists about \nI-languages, in an analogous sense to the way that Chomsky is a \nfictionalist about E-languages. Emergentists do not see a child as \nlearning a generative grammar, but as learning how to use a symbolic \nsystem for propositional communication. On this view grammars are \nmere artifacts that are developed by linguists to codify aspects of \nthe relevant systems, and positing an I-language amounts to \nprojecting the linguist’s codification illegitimately onto human \nminds (see, for example, Tomasello 2003). \n\nThe I-language concept brushes aside certain phenomena of interest to\nthe Externalists, who hold that the forms of actually attested \nexpressions (sentences, phrases, syllables, and systems of such \nunits) are of interest for linguistics. For example, computational \nlinguistics (work on speech recognition, machine translation, and \nnatural language interfaces to databases) must rely on a conception \nof language as public and extensional; so must any work on the \nutterances of young children, or the effects of word frequency on \nvowel reduction, or misunderstandings caused by road sign wordings. \nAt the very least, it might be said on behalf of this strain of \nExternalism (along the lines of Soames 1984) that linguistics will \nneed careful work on languages as intersubjectively accessible \nsystems before hypotheses about the I-language that purportedly \nproduces them can be investigated. \n\nIt is a highly biased claim that the E-language concept \n“appears to play no role in the theory of language” \n(Chomsky 1986: 26). Indeed, the terminological contrast seems to have\nbeen invented not to clarify a distinction between concepts but to \nnudge linguistic research in a particular direction. \n\nIn Hauser et al. (2002) (henceforth HCF) a further pair of \ncontrasting terms is introduced. They draw a distinction quite \nseparate from the competence/performance and \n‘I-language’/‘E-language’ distinctions: the \n“language faculty in the narrow sense” (FLN) is \ndistinguished from the “language faculty in the broad \nsense” (FLB). According to HCF, FLB “excludes other \norganism-internal systems that are necessary but not sufficient for \nlanguage (e.g., memory, respiration, digestion, circulation, \netc.)” but includes whatever is involved in language, and FLN \nis some limited part of FLB (p. 1571) This is all fairly vague, but \nit is clear that FLN and FLB are both internal rather than external, \nand individual rather than social. \n\nThe FLN/FLB distinction apparently aims to address the uniqueness of \none component of the human capacity for language rather than (say) \nthe content of human grammars. HCF say (p. 1573) that “Only FLN\nis uniquely human”; they “hypothesize that most, if not \nall, of FLB is based on mechanisms shared with nonhuman \nanimals”; and they say: \n\nThe components of FLB that HCF hypothesize are not part of FLN are \nthe “sensory-motor” and \n“conceptual-intentional” systems. The study of the \nconceptual-intentional system includes investigations of things like \nthe theory of mind; referential vocal signals; whether imitation is \ngoal directed; and the field of pragmatics. The study of the sensory \nmotor system, by contrast, includes “vocal tract length and \nformant dispersion in birds and primates”; learning of songs by\nsongbirds; analyses of vocal dialects in whales and spontaneous \nimitation of artificially created sounds in dolphins; “primate \nvocal production, including the role of mandibular \noscillations”; and “[c]ross-modal perception and sign \nlanguage in humans versus unimodal communication in animals”. \n\nIt is presented as an empirical hypothesis that a core property of \nthe FLN is “recursion”: \n\nHCF leave open exactly what the FLN includes in addition to \nrecursion. It is not ruled out that the FLN incorporates substantive \nuniversals as well as the formal property of “recursion”.\nBut whatever “recursion” is in this context, it is \napparently not domain-specific in the sense of earlier discussions by\ngenerative Essentialists, because it is not unique to human natural \nlanguage or defined over specifically linguistic inputs and outputs: \nit is the basis for humans’ grasp of the formal and arguably \nnon-natural language of arithmetic (counting, and the successor \nfunction), and perhaps also navigation and social relations. It might\nbe more appropriate to say that HCF identify recursion as a cognitive\nuniversal, not a linguistic one. And in that case it is difficult to \nsee how the so-called ‘language faculty’ deserves that \nname: it is more like a faculty for cognition and communication. \n\nThis abandonment of linguistic domain-specificity contrasts very \nsharply with the picture that was such a prominent characteristic of \nthe earlier work on linguistic nativism, popularized in different \nways by Fodor (1983), Barkow et al. (1992), and Pinker (1994). And \nyet the HCF discussion of FLN seems to incline to the view that human\nlanguage capacities have a unique human (though not uniquely \nlinguistic) essence. \n\nThe FLN/FLB distinction provides earlier generative Essentialism with\nan answer (at least in part) to the question of what the singularity \nof the human language faculty consists in, and it does so in a way \nthat subsumes many of the empirical discoveries of paleoanthropology,\nprimatology, and ethnography that have been part of highly \ninfluential in Emergentist approaches as well as neo-Darwinian \nEssentialist approaches. A neo-Darwinian Essentialist like Pinker \nwill accept that the language faculty involves recursion, but also \nwill also hold (with Emergentists) that human language capacities \noriginated, via natural selection, for the purpose of linguistic \ncommunication. \n\nThus, over the years, those Essentialists who follow Chomsky closely\nhave changed the term they use for their core subject matter from\n‘linguistic competence’ to ‘I-language’ to\n‘FLN’, and the concepts expressed by these terms are all\nslightly different.  In particular, what they are counterposed to\ndiffers in each case. \n\nThe challenge for the generative Essentialist adopting the FLN/FLB \ndistinction as characterized by HCF is to identify empirical data \nthat can support the hypothesis that the FLN “yields discrete \ninfinity”. That will mean answering the question: discrete \ninfinity of what? HCF write that FLN “takes a finite set of \nelements and yields a potentially infinite array of discrete \nexpressions” (p. 1571), which makes it clear that there must be\na recursive procedure in the mathematical sense, perhaps putting \natomic elements such as words together to make internally complex \nelements like sentences (“array” should probably be \nunderstood as a misnomer for ‘set’). But then they say, \nsomewhat mystifyingly: \n\nBut the sensory-motor and conceptual-intentional systems are concrete\nparts of the organism: muscles and nerves and articulatory organs and\nperceptual channels and neuronal activity. How can each one of a \n“potentially infinite array” be “passed to” \nsuch concrete systems without it taking a potentially infinite amount\nof time? HCF may mean that for any one of the expressions that FLN \ndefines as well-formed (by generating it) there is a possibility of \nits being used as the basis for a pairing of sound and meaning. This \nwould be closer to the classical generative Essentialist view that \nthe grammar generates an infinite set of structural descriptions; but\nit is not what HCF say. \n\nAt root, HCF is a polemical work intended to identify the view it \npromotes as valuable and all other approaches to linguistics as \notiose. \n\nIt is hard to see this as anything other than a claim that approaches\nto linguistics focusing on anything that could fall under the label \n‘E-language’ are to be dismissed as useless. \n\nSome Externalists and Emergentists actually reject the idea that the \nhuman capacity for language yields “a potentially infinite \narray of expressions”. It is often pointed out by empirically \ninclined computational linguists that in practice there will only \never be a finite number of sentences to be dealt with (though the \npeople saying this may underestimate the sheer vastness of the finite\nset involved). And naturally, for those who do not believe there are \ngenerative grammars in speakers’ heads at all, it holds a fortiori \nthat speakers do not have grammars in their heads generating infinite\nlanguages. Externalists and Emergentists tend to hold that the \n“discrete infinity” that HCF posits is more plausibly a \nproperty of the generative Essentialists’ model of linguistic \ncompetence, I-language, or FLN, than a part of the human mind/brain. \nThis does not mean that non-Essentialists deny that actual language \nuse is creative, or (of course) that they think there is a longest \nsentence of English. But they may reject the link between linguistic \nproductivity or creativity and the mathematical notion of recursion \n(see Pullum and Scholz 2010). \n\nHCF’s remarks about how FLN “yields” or \n“generates” a specific “array” assume that \nlanguages are clearly and sharply individuated by their generators. \nThey appear to be committed to the view that there is a fact of the \nmatter about exactly which generator is in a given speaker’s head. \nEmergentists tend not to individuate languages in this way, and may \nreject generative grammars entirely as inappropriately or \nunacceptably ‘formalist’. They are content with the \nnotion that the common-sense concept of a language is vague, and it \nis not the job of linguistic theory to explain what a language is, \nany more than it is the job of physicists to explain what material \nis, or of biologists to explain what life is. Emergentists, in \nparticular, are interested not so much in identifying generators, or \nindividuating languages, but in exploring the component capacities \nthat facilitate linguistic communication, and finding out how they \ninteract. \n\nSimilarly, Externalists are interested in the linguistic structure of\nexpressions, but have little use for the idea of a discrete infinity \nof them, a view that is not, and cannot be empirically supported, unless one\nthinks of simplicity and elegance of theory as empirical matters. \nThey focus on the outward manifestations of language, not on a set of\nexpressions regarded as a whole language—at least not in any \nway that would give a language a definite cardinality. Zellig Harris,\nan archetypal Externalist, is explicit that the reason for not \nregarding the set of utterances as finite concerns the elegance of \nthe resulting grammar: “If we were to insist on a finite \nlanguage, we would have to include in our grammar several highly \narbitrary and numerical conditions” (Harris 1957: 208). \nInfinitude, on his view is an unimportant side consequence of setting\nup a sentence-generating grammar in an uncluttered and maximally \nelegant way, not a discovered property of languages (see Pullum and \nScholz 2010 for further discussion). \n\nNot all Essentialists agree that linguistics studies aspects of what \nis in the mind or aspects of what is human. There are some who do not\nsee language as either mental or human, and certainly do not regard \nlinguists as working on a problem within cognitive psychology or \nneurophysiology. Montague (1974), for example, is deeply concerned \nwith using powerful higher-order quantified modal logics and possible\nworlds to formalize aspects of natural language semantics, but \neschews psychologism. His leanings are toward Frege, and his ontology\ninclines toward platonism rather than psychologism. \n\nKatz (1981) is an explicit defense of the Fregean view that natural \nlanguages are timeless, locationless, and necessarily existent. The \nprimary essential property that Katz finds in natural languages is \neffability, the property of providing semantic expression \nfor absolutely every Fregean proposition. On the platonist view the \nfact that non-spatiotemporally located languages are grasped and used\nby human beings raises major epistemological issues (see the section \ntitled ‘The Epistemological Argument Against Platonism’\n in the entry on Platonism in metaphysics).\n Katz (1998) attempts to address these issues. \n\nKatz’s own tripartite classification of linguistic theories, derived \nfrom medieval solutions to the problem of universals (and used as the\nstructure of his book of readings, Katz 1985), is orthogonal to our \nclassification. Katz sees three ontological conceptions of the \nsubject matter of linguistics: \n\nKatz took nominalism to have been refuted by Chomsky in his critiques\nof American structuralists in the 1960s. But, in Katz’s opinion, \nChomsky had failed to notice that conceptualism was infected with \nmany of the same faults as nominalism, because it too localized \nlanguage spatiotemporally (in contingently existing, finite, human \nbrains). Through an argument by elimination, Katz concluded that only\nplatonism remained, and must be the correct view to adopt. \n\nKatz’s argument by elimination should probably be taken as another \nexample of an effort not to separate and clarify concepts used in \ndifferent kinds of linguistic theorizing, but rather to dismiss and \nexclude certain types of research from the theory of language (see \nPullum and Scholz 1997 for detailed discussion). But regardless of \nthat, his typology of linguists should certainly not be thought to \nrelate directly to the distinctions between centers of interest in \nlinguistic theorizing around which this article is structured. No \nparticular metaphysical view unifies any of our three groupings. For \nexample, not all Externalists incline toward nominalism; numerous \nEmergentists as well as most Essentialists take linguistics to be \nabout mental phenomena; and our Essentialists include Katz’s \nplatonists alongside the Chomskyan ‘I-language’ \nadvocates. \n\nLinguists’ conception of the components of the study of language \ncontrast with philosophers’ conceptions (even those of philosophers \nof language) in at least three ways. First, linguists are often \nintensely interested in small details of linguistic form in their own\nright. Second, linguists take an interest in whole topic areas like \nthe internal structure of phrases, the physics of pronunciation, \nmorphological features such as conjugation classes, lexical \ninformation about particular words, and so on—topics in which \nthere is typically little philosophical payoff. And third, linguists \nare concerned with relations between the different subsystems of \nlanguages: the exact way the syntax meshes with the semantics, the \nrelationship between phonological and syntactic facts, and so on. \n\nWith regard to form, philosophers broadly follow Morris (1938), a \nfoundational work in semiotics, and to some extent Peirce (see SEP \nentry: Peirce, semiotics), in thinking of the theory of language as \nhaving three main components: \n\nLinguists, by contrast, following both Sapir (1921) and Bloomfield \n(1933), treat the syntactic component in a more detailed way than \nMorris or Peirce, and distinguish between at least three kinds of \nlinguistic form: the form of speech sounds (phonology), the form of \nwords (morphology), and the form of sentences. (If syntax is about \nthe form of expressions in general, then each of these would be an \nelement of Morris’s syntax.) \n\nEmergentists in general deny that there is a distinction between \nsemantics and pragmatics—a position that is familiar enough in \nphilosophy: Quine (1987: 211), for instance, holds that “the \nseparation between semantics and pragmatics is a pernicious \nerror.” And generally speaking, those theorists who, like the \nlater Wittgenstein, focus on meaning as use will deny that one can \nseparate semantics from pragmatics. Emergentists such as Paul Hopper \n& Sandra Thompson agree: \n\nSome Essentialists—notably Chomsky—also deny that \nsemantics can be separated from pragmatics, but unlike the \nEmergentists (who think that semantics-pragmatics is a starting point\nfor linguistic theory), Chomsky (as we noted briefly in section 1.3) \ndenies that semantics and pragmatics can have any role in \nlinguistics: It seems that other cognitive systems—in particular, our system\nof beliefs concerning things in the world and their\nbehavior—play an essential part in our judgments of meaning and\nreference, in an extremely intricate manner, and it is not at all\nclear that much will remain if we try to separate the purely\nlinguistic components of what in informal usage or even in technical\ndiscussion we call ‘the meaning of [a] linguistic expression.’\n(Chomsky 1979; 142)\n \nRegarding the theoretical account of the relation between words or\nphrases and what speakers take them to refer to, Chomsky says,\n“I think such theories should be regarded as a variety of\nsyntax” (Chomsky 1992: 223).\n \n\nNot every Essentialist agrees with Chomsky on this point. Many \nbelieve that every theory should incorporate a linguistic component \nthat yields meanings, in much the same way that many philosophers of \nlanguage believe there to be such a separate component. Often, \nalthough not always, this component amounts to a truth-theoretic \naccount of the values of syntactically-characterized sentences. This \ntypically involves a translation of the natural language sentence \ninto some representation that is “intermediate” between \nnatural language and a truth-theory—perhaps an augmented \nversion of first-order logic, or perhaps a higher-order intensional \nlanguage. The Essentialists who study semantics in such ways usually \nagree with Chomsky in seeing little role for pragmatics within \nlinguistic theory. But their separation of semantics from pragmatics \nallows them to accord semantics a legitimacy within linguistics \nitself, and not just in psychology or sociology. \n\nSuch Essentialists, as well as the Emergentists, differ in important \nways from classical philosophical logic in their attitudes towards \n“the syntactic-semantic interface”, however. Philosophers\nof language and logic who are not also heavily influenced by \nlinguistics tend to move directly—perhaps by means of a \n“semantic intuition” or perhaps from an intuitive \nunderstanding of the truth conditions involved—from a natural \nlanguage sentence to its “deep, logical” representation. \nFor example, they may move directly from (EX1) to (LF1): \n\nAnd from there perhaps to a model-theoretic description of its \ntruth-conditions. A linguist, on the other hand, would aim to \ndescribe how (EX1) and (LF1) are related. From the point of view of a\nsemantically-inclined Essentialist, the question is: how should the \nsyntactic component of linguistic theory be written so that the \nsemantic value (or, “logical form representation”) can be\nassigned? From some Emergentist points of view, the question is: how \ncan the semantic properties and communicative function of an \nexpression explain its syntactic properties? \n\nMatters are perhaps less clear with the Externalists—at least \nwith those who identify semantic value with distribution in terms of \nneighboring words (there is a tradition stemming from the \nstructuralists of equating synonymy with the possibility of \nsubstitution in all contexts without affecting acceptability). \n\nMatters are in general quite a bit more subtle and tricky than (EX1) \nmight suggest. Philosophers have taken the natural language sentence \n(EX2) to have two logical forms, (LF2a) and (LF2b): \n\nBut for the linguist interested in the syntax-semantics interface, \nthere needs to be some explanation of how (LF2a) and (LF2b) are \nassociated with (EX2). It could be a way in which rules can derive \n(LF2a) and (LF2b) from the syntactic representation of (EX2), as some\nsemantically-inclined Essentialists would propose, or a way to \nexplain the syntactic properties of (EX2) from facts about the \nmeanings represented by (LF2a) and (LF2b), as some Emergentists might\nwant. But that they should be connected up in some way is something \nthat linguists would typically count as non-negotiable. \n\nThe strengths and limitations of different data gathering methods \nbegan to play an important role in linguistics in the early to \nmid-20th century. Voegelin and Harris (1951: 323) discuss several \nmethods that had been used to distinguish Amerindian languages and \ndialects: \n\nThey note that the anthropological linguists Boas and Sapir (who we \ntake to be proto-Emergentists) used the ‘ask the \ninformant’ method of informal elicitation, addressing questions\n“to the informant’s perception rather than to the data \ndirectly” (1951: 324). Bloomfield (the proto-Externalist), on \nthe other hand, worked on Amerindian languages mostly by collecting \ncorpora, with occasional use of monolingual elicitation. \n\nThe preferred method of Essentialists today is informal elicitation, \nincluding elicitation from oneself. Although the techniques for \ngathering data about speakers and their language use have changed \ndramatically over the past 60 or more years, the general strategies \nhave not: data is still gathered by elicitation of metalinguistic \njudgments, collection of corpus material, or direct psychological \ntesting of speakers’ reactions and behaviors. Different linguists \nwill have different preferences among these techniques, but it is \nimportant to understand that data could be gathered in any of the \nthree ways by advocates of any tendency. Essentialists, Emergentists,\nand Externalists differ as much on how data is interpreted and used \nas on their views of how it should be gathered. \n\nA wide range of methodological issues about data collection have been\nraised in linguistics. Since gathering data by direct objective \nexperimental testing of informants is a familiar practice throughout \nthe social, psychological, medical, and biological sciences, we will \nsay little about it here, focusing instead on these five issues about\ndata: \n\nThe debate in linguistics over the use of linguistic intuitions \n(elicited metalinguistic judgments) as data, and how that data should\nbe collected has resulted in enduring, rancorous, often ideologically\ntinged disputes over the past 45 years. The disputes are remarkable, \nif only for their fairly consistent venomous tone. \n\nAt their most extreme, many Emergentists and some Externalists cast \nthe debate in terms of whether linguistic intuitions should ever \ncount as evidence for linguistic theorizing. And many Essentialists \ncast it in terms of whether anything but linguistic intuitions are \never really needed to support linguistic theorizing. \n\nThe debate focuses on the Essentialists’ notion of a mental grammar, \nsince linguistic intuitions are generally understood to be a \nconsequence of tacit knowledge of language. Emergentists who deny \nthat speakers have innate domain-specific grammars (competence, \nI-languages, or FLN) have raised a diverse range of objections to the\nuse of reports of intuitions as linguistic data. (But see Devitt 2006\nfor an understanding of linguistic intuitions that does not base them\non inferred tacit knowledge of competence grammars.) The following \npassages are representative Emergentist critiques of intuitions \n(elicited judgments): Generative linguists typically respond to calls for evidence for\nthe reality of their theoretical constructs by claiming that no\nevidence is needed over and above the theory’s ability to account for\npatterns of grammaticality judgments elicited from native\nspeakers. This response is unsatisfactory on two accounts. First, such\njudgments are inherently unreliable because of their unavoidable\nmeta-cognitive overtones… Second, the outcome of a judgment (or\nthe analysis of an elicited utterance) is invariably brought to bear\non some distinction between variants of the current generative theory,\nnever on its foundational assumptions. (Edelman and Christiansen 2003:\n60) \nThe data that are actually used toward this end in Generative Grammar\nanalyses are almost always disembodied sentences that analysts have\nmade up ad hoc, … rather than utterances produced by real people\nin real discourse situations… In diametric opposition to these\nmethodological assumptions and choices, cognitive-functional linguists\ntake as their object of study all aspects of natural language\nunderstanding and use… They (especially the more functionally\noriented analysts) take as an important part of their data not\ndisembodied sentences derived from introspection, but rather\nutterances or other longer sequences from naturally occurring\ndiscourse. (Tomasello 1998: xiii) \n[T]he journals are full of papers containing highly questionable data,\nas readers can verify simply by perusing the examples in nearly any\nsyntax article about a familiar language. (Wasow and Arnold 2005:\n1484) \n\nIt is a common Emergentist objection that linguistic intuitions \n(taken to be reports of elicited judgments of the acceptability of \nexpressions not their grammaticality) are bad data points because not\nonly are they not usage data, i.e., they are metalinguistic, but also\nbecause they are judgments about linguist’s invented example \nsentences. On neither count would they be clear and direct evidence \nof language use and human communicative capacities—the subject \nmatter of linguistics on the Emergentist view. A further objection is\nto their use by theorists to the exclusion of all other kinds of \nevidence. For example, \n\n“Formal” is Emergentist shorthand for referring to \ngenerative linguistics. And it should be noted that the practice by \nEssentialists of collapsing various kinds of acceptability judgments \nunder the single label ‘intuitions’ masks important \ndifferences. In principle there might be significant differences \nbetween the judgments of (i) linguists with a stake in what the \nevidence shows; (ii) linguists with experience in syntactic theory \nbut no stake in the issue at hand; (iii) non-linguist native speakers\nwho have been tutored in how to provide the kinds of judgments the\nlinguist is interested in; and (iv) linguistically naïve native \nspeakers. \n\nMany Emergentists object to all four kinds of reports of intuitions \non the grounds that they are not direct evidence language use. For \nexample, a common objection is based on the view that \n\nBut collections of linguists’ reports of their own judgments are also\ncriticized by Emergentists as “arm-chair data \ncollection,” or “data collection by introspection”.\nAll parties tend to call this kind of data collection \n“informal”—though they all rely on either formally \nor informally elicited judgments to some degree. \n\nOn the other side, Essentialists tend to deny that usage data is \nadequate evidence by itself: \n\nAnd Essentialists often seem to deny that they are guilty of what the\nEmergentist claims they are guilty of. For example, Chomsky appears \nto be claiming that acceptability judgments are performance data, \ni.e. evidence of use: \n\nChomsky means to deny that acceptability judgments are direct \nevidence of linguistic competence. But it does not follow \nthat elicited acceptability judgments are direct evidence of language\nuse. \n\nAnd as for the charge of “arm-chair” collection methods, \nsome Essentialists claim to have shown that such methods are as good \nas more controlled experimental methods. For example, Sprouse and \nAlmeida report: \n\n(When they say “formal results” they apparently mean \n“results obtained by controlled experiments”.) This can \nbe read as either defending Essentialists’ consulting of their own \nintuitions simpliciter, or their self-consultation of intuitions on \nuncontroversial textbook cases only. The former is much more \ncontroversial than the later. \n\nFinally, both parties of the debate engage in ad hominem attacks on \ntheir opponents. Here is one example of a classic ad hominem (tu \nquoque) attack on Emergentists in defense of constructed examples by \nEssentialists: \n\nClearly, the mere fact that some Emergentists may in practice have \nmade use of invented examples in testing their theories does not tell\nagainst any cogent general objections they may have offered to such \npractice. What is needed is a decision on the methodological point, \nnot just a cry of “You did it too!”. \n\nGiven the intolerance of each other’s views, and the crosstalk \npresent in these debates, it is tempting to think that Emergentism \nand Essentialism are fundamentally incompatible on what counts as \nlinguistic data, since their differences are based on their different\nviews of the subject matter of linguistics, and what the phenomena \nand goals of linguistic theorizing are. There is no doubt that the \nopposing sides think that their respective views are incompatible. \nBut this conclusion may well be too hasty. In what follows, we try to\npoint to a way that the dispute could be ameliorated, if not \nadjudicated. \n\nEssentialists who accept the competence/performance distinction of \nChomsky (1965) traditionally emphasize elicited acceptability \njudgment data (although they need not reject data that is gathered \nusing other methods). But as Cowart notes: \n\nThe grammaticality of an expression, on the standard generative \nEssentialist view, is the status conferred on it by the competence \nstate of an ideal speaker. But competence can never be exercised or \nused without potentially interfering performance factors like memory \nbeing exercised as well. This means that judgments about \ngrammaticality are never really directly available to the linguist \nthrough informant judgments: they have to be inferred from judgments \nof acceptability (along with any other relevant evidence). \nNevertheless, Essentialists do take acceptability judgments to \nprovide fairly good evidence concerning the character of linguistic \ncompetence. In fact the use of informally gathered acceptability \njudgment data is a hallmark of post-1965 Essentialist practice. \n\nIt would be a mistake, however, to suppose that only Essentialists \nmake use of such judgments. Many contemporary Externalists and \nEmergentists who reject the competence/performance distinction still \nuse informally gathered acceptability judgments in linguistic \ntheorizing, though perhaps not in theory testing. Emergentists tend \nto interpret experimentally gathered judgment data as performance \ndata reflecting the interactions between learned features of \ncommunication systems and general learning mechanisms as deployed in \ncommunication. And Externalists use judgment data for corpus cleaning\n(see below). \n\nIt should be noted that sociolinguists and anthropological linguists \n(and we regard them as tending toward Emergentist views) often \ninformally elicit informant judgments not only about acceptability \nbut also about social and regional style and variation, and meaning. \nThey may ask informants questions like, “Who would typically \nsay that?”, or “What does X mean in context XYZ?”, \nor “If you can say WXY, can you say WXZ?” (see Labov \n1996: 77). \n\nA generative grammar gives a finite specification of a set of \nexpressions. A psychogrammar, to the extent that it corresponds to a \ngenerative grammar, might be thought to equip a speaker to know (at \nleast in principle) absolutely whether a string is in the language. \nHowever, elicited metalinguistic judgments are uncontroversially a \nmatter of degree. A question arises concerning the scale on which \nthese degrees of acceptability should be measured. \n\nLinguists have implicitly worked with a scale of roughly half a dozen\nlevels and types of acceptability, annotating them with prefixed \nsymbols. The most familiar is the asterisk, originally used simply to\nmark strings of words as ungrammatical, i.e., as not belonging to the\nlanguage at all. Other prefixed marks have gradually become current: \n\nBut other annotations have been used to indicate a gradation in the \nextent to which some sentences are unacceptable. No scientifically \nvalidated or explicitly agreed meanings have been associated with \nthese marks, but a tradition has slowly grown up of assigning \nprefixes such as those in Table 2 to signify degrees of \nunacceptability: Table 2: Prefixes used to mark levels of acceptability \n\nSuch markings are often used in a way that suggests an \nordinal scale, i.e. a partial ordering that is \nsilent on anything other than equivalence in acceptability or ranking\nin degree of unacceptability. \n\nBy contrast, Bard et al. (1996: 39) point out, it is possible to use \ninterval scales, which additionally measure distance\nbetween ordinal positions. Interval scales of acceptability would \nmeasure relative distances between strings—how much \nmore or less acceptable one is than another. Magnitude \nestimation is a method developed in psychophysics to measure\nsubjects’ judgments of physical stimuli on an interval scale. Bard et\nal. (1996) adapted these methods to linguistic acceptability \njudgments, arguing that interval scales of measurement are required \nfor testing theoretical claims that rely on subtle judgments of \ncomparative acceptability. An ordinal scale of acceptability can \nrepresent one expression as being less acceptable than another, but \ncannot support quantitative questions about how much less. Many \ngenerative Essentialist theorists had been suggesting that violation \nof different universal principles led to different degrees of \nunacceptability. According to Bard et al. (34–35), because \nthere may be “disproportion between the fineness of judgments \npeople can make and the symbol set available for recording \nthem” it will not suffice to use some fixed scale such as this \none: \n  ? < ?? < ?* < * < **\n \n\nindicating absolute degrees of unacceptability. Degrees of relative \nunacceptability must be measured. This is done by asking the \ninformant how much less acceptable one string is than another. \n\nMagnitude estimation can be used with both informal and experimental \nmethods of data collection. And data that is measured using interval \nscales can be subjected to much more mathematically sophisticated \ntests and analyses than data measured solely by an ordinal scale, \nprovided that quantitative data are available. \n\nIt should be noted that the value of applying magnitude estimation to\nthe judgment of acceptability has been directly challenged in two \nrecent papers. Weskott and Fanselow (2011) and Sprouse (2011) both \npresent critiques of Bard et al. (1996). Weskott and Fanselow \ncompared magnitude estimation data to standard judgments on binary \nand 7-point scales, and claim that magnitude estimation does not \nyield more information than other judgment tasks, and moreover can \nproduce spurious variance. And Sprouse, on the basis of recent \nformalizations of magnitude estimation in the psychophysics \nliterature, presents experimental evidence that participants cannot \nmake ratio judgments of acceptability (for example, a judgment that \none sentence is precisely half as acceptable as another), which \nsuggests that the magnitude estimation task probably provides the \nsame interval-level data as other judgment tasks. \n\nPart of the dispute over the reliability of informal methods of \nacceptability judgment elicitation and collection is between \ndifferent groups of Essentialists. Experimentally trained \npsycholinguists advocate using and adapting various experimental \nmethods that have been developed in the cognitive and behavioral \nsciences to collect acceptability judgments. And while the debate is \noften cast in terms of which method is absolutely better, a more \nappropriate question might be when one method is to be preferred to \nthe others. Those inclined toward less experimentally controlled \nmethods point out that there are many clear and uncontroversial \nacceptability judgments that do not need to be shown to be reliable. \nAdvocates of experimental methods point out that many purportedly \nclear, uncontroversial judgments have turned out to be unreliable, \nand led to false empirical generalizations about languages. Both seem\nto be right in different cases. \n\nChomsky has frequently stated his view that the experimental data-gathering \ntechniques developed in the behavioral sciences are neither used nor \nneeded in linguistic theorizing.  For example: \n\nHe also expressed the opinion that using experimental behavioral data\ncollection methods in linguistics “would be a waste of time and\nenergy” (1969: 81). \n\nAlthough many Emergentists—the intellectual heirs of \nSapir—would accept ‘ask-the-informant’ data, we \nmight expect them to tend to accept experimental data-gathering \nmethods that have been developed in the social sciences. There is \nlittle doubt that strict followers of the methodology preferred by \nBloomfield in his later career would disapprove of ‘ask the \ninformant’ methods. Charles Hockett remarked: \n\nWe might expect Bloomfield, having abandoned his earlier Wundtian \npsychological leanings, to be suspicious of any method that could be \ncast as introspective. And we might expect many contemporary \nExternalists to prefer more experimentally controlled methods too. \n(We shall see below that to some extent they do.) \n\nDerwing (1973) was one early critic of Chomsky’s view (1969) that \nexperimentally controlled data collection is useless; but it was \nnearly 25 years before systematic research into possible confounding \nvariables in acceptability judgment data started being conducted on \nany significant scale. In the same year that Bard et al. (1996) \nappeared, Carson Schütze (1996) published a monograph with the \nfollowing goal statement: \n\nIn a similar vein, Wayne Cowart stated that he wanted to \n“describe a family of practical methods that yield demonstrably\nreliable data on patterns of sentence acceptability.” He \nobserves that the stability and reliability of acceptability judgment\ncollection is \n\nSchütze also expresses the importance of using experimental \nmethods developed in cognitive science: \n\nThe above can be read as sympathetic to the Essentialist preference \nfor elicited judgments. \n\nAmong the findings of Schütze and Cowart about informal judgment\ncollection methods are these: \n\nAlthough Schütze (1996) and Cowart (1997) are both critical of \ntraditional Essentialist informal elicitation methods, their primary \nconcern is to show how the claims of Essentialist linguistics can be \nmade less vulnerable to legitimate complaints about informal data \ncollection methods. Broadly speaking, they are friends of \nEssentialism. Critics of Essentialism have raised similar concerns in\nless friendly terms, but it is important to note that the debate over\nthe reliability of informal methods is a debate within Essentialist \nlinguistics as well. \n\nInformal methods of acceptability judgment data have often been \ndescribed as excessively casual. Ferreira described the informal \nmethod this way: \n\n(It would be appropriate to read ‘grammatical’ and \n‘grammaticality’ in Ferreira’s text as meaning \n‘acceptable’ and ‘acceptability’.) \n\nThis critical characterization exemplifies the kind of method that \nSchütze and Cowart aimed to improve on. More recently, Gibson \nand Fedorenko describe the traditional informal method this way: \n\nWhile some Essentialists have acknowledged these problems with the \nreliability of informal methods, others have, in effect, denied their\nrelevance. For example, Colin Phillips (2010) argues that \n“there is little evidence for the frequent claim that sloppy \ndata-collection practices have harmed the development of linguistic \ntheories”. He admits that not all is epistemologically well in \nsyntactic theory, but adds, “I just don’t think that the \nproblems will be solved by a few rating surveys.” He concludes: \n\nTo suggest that informal methods are as fully reliable as controlled \nexperimental ones would be a serious charge, implying that \nresearchers like Bard, Robinson, Sorace, Cowart, Schütze, \nGibson, Fedorenko, and others have been wasting their time. But \nPhillips actually seems to be making a different claim. He suggests \nfirst that informally gathered data has not actually harmed \nlinguistics, and second that linguists are in danger of being \n“fooled” by critics who invent stories about unreliable \ndata having harmed linguistics. \n\nThe harm that Phillips claims has not occurred relates to the charge \nthat “mainstream linguistics” (he means the current \ngenerative Essentialist framework called ‘Minimalism’) is\n“irrelevant” to broader interests in the cognitive \nsciences, and has lost “the initiative in language \nstudy”. Of course, Phillips is right in a sense: one cannot \ninsure that experimental judgment collection methods will address \nevery way in which Minimalist theorizing is irrelevant to particular \nendeavors (language description, language teaching, natural language \nprocessing, or broader questions in cognitive psychological \nresearch). But this claim does not bear on what Schütze (1996) \nand Cowart (1997) show about the unreliability of informal methods. \n\nPhillips does not fully accept the view of Chomsky (1969) that \nexperimental methods are useless for data gathering (he says, \n“I do not mean to argue that comprehensive data gathering \nstudies of acceptability are worthless”). But his defense of \ninformal methods of data collection rests on whether these methods \nhave damaged Essentialist theory testing: \n\nThe critiques I have read present no evidence of the supposed damage \nthat informal intuitions have caused, and among those who do provide \nspecific examples it is rare to provide clear evidence of the \nsupposed damage that informal intuitions have caused… \n\nWhat I am specifically questioning is whether informal (and \noccasionally careless) gathering of acceptability judgments has \nactually held back progress in linguistics, and whether more careful \ngathering of acceptability judgments will provide the key to future \nprogress. \n\nEither Phillips is fronting the surprising opinion that generative \ntheorizing has never been led down the wrong track by demonstrably \nunreliable data, or he is changing the subject. And unless clear \ncriteria are established for what counts as “damage” and \n“holding back,” Phillips is not offering any testable \nhypothesis about data collection methodology. For example, Phillips \ndiscounts the observation of Schütze (1996) that conflicting \njudgments of relative unacceptability of violations of two linguistic\nuniversals held back the development of Government and Binding (GB), \non the grounds that two sets of conflicting judgments and their \nanalyses “are now largely forgotten, supplanted by theories \nthat have little to say about such examples.” But the fact that\nthe proposed universals are discarded principles of UG is irrelevant \nto the effect that unreliable data once had on the (now largely \nabandoned) GB theory. A methodological concern cannot be dismissed on\nthe basis of a move to a new theory that abandons the old theory but \nnot its methods! \n\nMore recently, Bresnan (2007) claims that many theoretical claims \nhave arguably been supported by unreliable informally gathered \nsyntactic acceptability judgments. She observes: \n\nHer discussion supports the view that various highly abstract \ntheoretical hypotheses have been defended through the use of \ngeneralizations based on unreliable data. \n\nThe debate over the harm that the acceptance of informally collected \ndata has had on theory testing is somewhat difficult to understand \nfor Essentialist, Externalist, and Emergentist researchers who have \nbeen trained in the methods of the cognitive and behavioral sciences.\nWhy try to support one’s theories of universal grammar, or of the \ngrammars of particular languages, by using questionably reliable \ndata? \n\nOne clue might be found in Culicover and Jackendoff (2010), who \nwrite: \n\nThe worry is that use of experimental methods is so resource\nconsumptive that it would impede the formulation of linguistic\ntheories. But this changes the subject from the importance of using\nreliable data as evidence in theory testing to using only\nexperimentally gathered data in theory formulation. We are\nnot aware of anyone who has ever suggested that at the stage of\nhypothesis development or theory formulation the linguist should\neschew intuition. Certainly Bard et al., Schütze, Cowart, Gibson\n& Fedorenko, and Ferreira say no such thing. The relevant issue\nconcerns what data should be used to test theories, which is\na very different matter. \n\nWe noted earlier that there are clear and uncontroversial \nacceptability judgments, and that these judgments are reliable data. \nThe difficulty lies in distinguishing the clear, uncontroversial, and\nreliable data from what only appears to be clear, uncontroversial, \nand reliable to a research community at a time. William Labov, the \nfounder of modern quantitative sociolinguistics, who takes an \nEmergentist approach, proposed a set of working methodological \nprinciples in Labov (1975) for adjudicating when experimental methods\nshould be employed. \n\nThe Consensus Principle: If there is no reason to \nthink otherwise, assume that the judgments of any native speaker are \ncharacteristic of all speakers. \n\nThe Experimenter Principle: If there is any \ndisagreement on introspective judgments, the judgments of those who \nare familiar with the theoretical issues may not be counted as \nevidence. \n\nThe Clear Case Principle: Disputed judgments should \nbe shown to include at least one consistent pattern in the speech \ncommunity or be abandoned. If differing judgments are said to \nrepresent different dialects, enough investigation of each dialect \nshould be carried out to show that each judgment is a clear case in \nthat dialect. (Labov 1975, quoted in Schütze 1996: 200) \n\nIf we accept that ‘introspective judgments’ are \nacceptability judgments, then Labov’s rules of thumb are guides for \nwhen to deploy experimental methods, although they no doubt need \nrefinement. However, it seems vastly more likely that careful \ndevelopment of such methodological rules of thumb can serve to \nimprove the reliability of linguistic data and adjudicate these \nmethodological disputes that seem largely independent of any \nparticular approach to linguistics. \n\nIn linguistics, the goal of collecting corpus data is to identify and\norganize a representative sample of a written and/or spoken variety \nfrom which characteristics of the entire variety or genre can be \ninduced. Concordances of word usage in linguistic context have long \nbeen used to aid in the translation and interpretation of literary \nand sacred texts of particular authors (e.g. Plato, Aristotle, \nAquinas) and of particular texts (e.g. the Torah, the rest of the Old\nTestament, the Gospels, the Epistles). Formal textual criticism, the \nidentification of antecedently existing oral traditions that were \nlater redacted into Biblical texts, and author identification (e.g. \nfiguring out which of the Epistles were written by Paul and which \nwere probably not) began to develop in the late 19th century. \n\nThe development of computational methods for collecting, analyzing, \nand searching corpora have seen rapid development as computer memory \nhas become less expensive and search and analysis programs have \nbecome faster. The first computer searchable corpus of American \nEnglish, the Brown Corpus, developed in the 1960s, contained just \nover one million word tokens. The British National Corpus (BNC) is a \nbalanced corpus containing over 100 million words—a hundredfold\nsize increase—of which 90% is written prose published from 1991\nto 1994 and 10% is spoken English. Between 2005 and 2007, \nbillion-word corpora were released for British English (ukWaC), \nGerman (deWaC), and Italian (itWaC)—a thousand times bigger \nthan the Brown corpus. And the entire World Wide Web probably holds \nabout a thousand times as much as that—around a trillion words.\nThus corpus linguistics has gone from megabytes of data (∼ \n103kB) to terabytes of data (∼ 109kB) in \nfifty years. \n\nJust as a central issue concerning acceptability judgment data \nconcerns its reliability as evidence for empirical generalizations \nabout languages or idiolects, a central question concerning the \ncollection of corpus data concerns whether or not it is \nrepresentative of the language variety it purports to represent. Some\nlinguists make the criterion of representativeness definitional: they\ncall a collection of samples of language use a corpus only if it has \nbeen carefully balanced between different genres (conversation, \ninformal writing, journalism, literature, etc.), regional varieties, \nor whatever. \n\nBut corpora are of many different kinds. Some are just very large\ncompilations of text from individual sources such as newspapers of\nrecord or the World Wide Web—compilations large enough for the\ndiversity in the source to act as a surrogate for representativeness.\nFor example, a billion words of a newspaper, despite coming from a\nsingle source, will include not only journalists’ news reports and\nprepared editorials but also quoted speech, political rhetoric, humor\ncolumns, light features, theater and film reviews, readers’ letters,\nfiction items, and so on, and will thus provide examples of a much\nwider variety of styles than one might have thought. \n\nCorpora are cleaned up through automatic or manual removal of such \nelements as numerical tables, typographical slips, spelling mistakes,\nmarkup tags, accidental repetitions (the the), larger-scale \nduplications (e.g., copies on mirror sites), boilerplate text \n(Opinions expressed in this email do not necessarily \nreflect…), and so on (see Baroni et al. 2009 for a fuller \ndiscussion of corpus cleaning). \n\nThe entire web itself can be used as a corpus to some degree, despite\nits constantly changing content, its multilinguality, its many tables\nand images, and its total lack of quality control; but when it is, the\noutputs of searches are nearly always cleaned by disregarding unwanted\nresults. For example, Google searches are blind to punctuation,\ncapitalization, and sentence boundaries, so search results for to\nbe will unfortunately include irrelevant cases, such as where a\nsentence like Do you want to? happens to be followed by a\nsentence like Be careful. \n\nCorpora can be annotated in ways that permit certain kinds of \nanalysis and grammar testing. One basic kind of annotation is \npart-of-speech tagging, in which each word is labeled with its \nsyntactic category. Another is lemmatization, which classifies the \ndifferent morphologically inflected forms of a word as belonging \ntogether (goes, gone, going, and went \nbelong with go, for example). A more thoroughgoing kind of \nannotation involves adding markup that encodes trees representing \ntheir structure; an example like That road leads to the \nfreeway might be marked up as a Clause within which the first two\nwords make up a Noun Phrase (NP), the last four constitute a Verb \nPhrase (VP), and so on, giving a structural analysis represented \nthus: \n\nSuch a diagram is isomorphic to (and the one shown was computed \ndirectly from) a labeled bracketing like this: \n(.Clause. (.NP. (.D. ‘that’ )\n  (.N. ‘road’ ) ) (.VP. (.V. ‘leads’ )\n  (.PP. (.P. ‘to’ ) (.NP. (.D. ‘the’ )\n  (.N. ‘freeway’ ) ) ) ) )\n \n\nand this in turn could be represented in a markup language like XML \nas: \n\nA corpus annotated with tree structure is known as a \ntreebank. Clearly, such a corpus is not a raw record\nof attested utterances at all; it is a combination of a collection of\nattested utterances together with a systematic attempt at analysing\ntheir structure. Whether the analysis is added manually or\nsemi-automatically, it is ultimately based on native speaker\njudgments. (Treebanks are often developed by graduate student\nannotators tutored by computational linguists; naturally, consistency\nbetween annotators is an issue that needs regular attention. See\nArtstein and Poesio, 2008, for discussion of the methodological\nissues.). \n\nOne of the purposes of a treebank is to permit the further \ninvestigation of a language and the checking of further linguistic \nhypotheses by searching a large database of previously established \nanalyses. It can also be used to test grammars, natural language \nprocessing systems, or machine learning programs. \n\nGoing beyond syntactic parse trees, it is possible to annotate \ncorpora further, with information of a semantic and pragmatic nature.\nThere is ongoing computational linguistic research aimed at \ndiscovering whether, for example, semantic annotation that is \nsemi-automatically added might suffice for recognition of whether a \nproduct review is positive or negative (what computational linguists \ncall ‘sentiment analysis’). \n\nNotice, then, that using corpus data does not mean abandoning or \nescaping from the use of intuitions about acceptability or \ngrammatical structure: the results of a corpus search are generally \nfiltered through the judgments of an investigator who decides which \npieces of corpus data are to be taken at face value and which are \njust bad hits or irrelevant noise. \n\nDifficult methodological issues arise in connection with the \ncollection, annotation, and use of corpus data. For example, there is\nthe issue of extremely rare expression tokens. Are they accurately \nrecorded tokens of expression types that turn up only in consequence \nof sporadic errors and should be dismissed as irrelevant unless the \ntopic of interest is performance errors? Are they due to errors in \nthe compilation of the corpus itself, corresponding to neither \naccepted usage nor sporadic speech errors? Or are they perfectly \ngrammatical but (for some extraneous reason) very rare, at least in \nthat particular corpus? \n\nMany questions arise about what kind of corpus is best suited to the \nresearch questions under consideration, as well as what kind of \nannotation is most appropriate. For example, as Ferreira (2005: 375) \npoints out, some large corpora, insofar as they have not been \ncleaned of speech errors, provide relevant data for studying the \ndistribution of speech disfluencies. In addition, probabilistic \ninformation about the relation between a particular verb and its \narguments has been used to show that “verb-argument preferences\n[are] an essential part of the process of sentence \ninterpretation” (Roland and Jurafsky 2002: 325): acceptability \njudgments on individual expressions do not provide information about \nthe distribution of a verb and its arguments in various kinds of \nspeech and writing. Studying conveyed meaning in context and \nidentification of speech acts will require a kind of data that \ndecontextualized acceptability judgments do not provide but \nsemantically annotated corpora might. \n\nMany Essentialists have been skeptical of the reliability of \nuncleaned, unanalyzed corpus data as evidence to support linguistic \ntheorizing, because it is assumed to be replete with strings that any\nnative speaker would judge unacceptable. And many Emergentists and \nExternalists, as well as some Essentialists, have charged that \ninformally gathered acceptability judgments can be highly unreliable \ntoo. Both worries are apposite; but the former does not hold for \nadequately cleaned and analyzed corpora, and the latter does not hold\nfor judgment data that has been gathered using appropriately \ncontrolled methods. In certain contested cases of acceptability, it \nwill of course be important to use both corpus and controlled \nelicitation methods to cross-compare. \n\nNotice that we have not in any way suggested that our three broad \napproaches to linguistics should differ in the kinds of data they use\nfor theory testing: Essentialists are not limited to informal \nelicitation; nor are Emergentists and Externalists denied access to \nit. In matters of methodology, at least, there is in principle an \nopen market—even if many linguists seem to think otherwise. \n\nEmergentists tend to follow Edward Sapir in taking an interest in \ninterlinguistic and intralinguistic variation. Linguistic \nanthropologists have explicitly taken up the task of defending a \nfamous claim associated with Sapir that connects linguistic variation\nto differences in thinking and cognition more generally. The claim is\nvery often referred to as the Sapir-Whorf Hypothesis\n(though this is a largely infelicitous label, as we shall see). \n\nThis topic is closely related to various forms of \nrelativism—epistemological, ontological, conceptual, and \nmoral—and its general outlines are discussed elsewhere in this \nencyclopedia; see the section on language in the Summer 2015\narchived version of the entry on  \n relativism (§3.1). \nCultural versions of moral relativism \nsuggest that, given how much cultures differ, what is moral for you \nmight depend on the culture you were brought up in. A somewhat \nanalogous view would suggest that, given how much language structures\ndiffer, what is thinkable for you might depend on the language you \nuse. (This is actually a kind of conceptual relativism, but it is \ngenerally called linguistic relativism, and we will continue that \npractice.) \n\nEven a brief skim of the vast literature on the topic is not remotely\nplausible in this article; and the primary literature is in any case \nmore often polemical than enlightening. It certainly holds no general\nanswer to what science has discovered about the influences of \nlanguage on thought. Here we offer just a limited discussion of the \nalleged hypothesis and the rhetoric used in discussing it, the vapid \nand not so vapid forms it takes, and the prospects for actually \ndevising testable scientific hypotheses about the influence of \nlanguage on thought. \n\nWhorf himself did not offer a hypothesis. He presented his “new\nprinciple of linguistic relativity” (Whorf 1956: 214) as a fact\ndiscovered by linguistic analysis: \n\nLater, Whorf’s speculations about the “sensuously and \noperationally different” character of different snow types for \n“an Eskimo” (Whorf 1956: 216) developed into a familiar \njournalistic meme about the Inuit having dozens or scores or hundreds\nof words for snow; but few who repeat that urban legend recall \nWhorf’s emphasis on its being grammar, rather than lexicon, that cuts\nup and organizes nature for us. \n\nIn an article written in 1937, posthumously published in an academic \njournal (Whorf 1956: 87–101), Whorf clarifies what is most \nimportant about the effects of language on thought and world-view. He\ndistinguishes ‘phenotypes’, which are overt grammatical \ncategories typically indicated by morphemic markers, from what he \ncalled ‘cryptotypes’, which are covert grammatical \ncategories, marked only implicitly by distributional patterns in a \nlanguage that are not immediately apparent. In English, the past \ntense would be an example of a phenotype (it is marked by the \n-ed suffix in all regular verbs). Gender in personal names and\ncommon nouns would be an example of a cryptotype, not systematically \nmarked by anything. In a cryptotype, “class membership of the \nword is not apparent until there is a question of using it or \nreferring to it in one of these special types of sentence, and then \nwe find that this word belongs to a class requiring some sort of \ndistinctive treatment, which may even be the negative treatment of \nexcluding that type of sentence” (p. 89). \n\nWhorf’s point is the familiar one that linguistic structure is \ncomprised, in part, of distributional patterns in language use that \nare not explicitly marked. What follows from this, according to \nWhorf, is not that the existing lexemes in a language (like its words\nfor snow) comprise covert linguistic structure, but that patterns \nshared by word classes constitute linguistic structure. In \n‘Language, mind, and reality’ (1942; published \nposthumously in Theosophist, a magazine published in India \nfor the followers of the 19th-century spiritualist Helena Blavatsky) \nhe wrote: \n\nWhorf apparently thought that only personal and proper names have an \nexact meaning or reference (Whorf 1956: 259). \n\nFor Whorf, it was an unquestionable fact that language influences \nthought to some degree: \n\nHe seems to regard it as necessarily true that language affects \nthought, given \n\nHe also seems to presume that the only structure and logic that\nthought has is grammatical structure. These views are not the ones\nthat after Whorf’s death came to be known as ‘the Sapir-Whorf\nHypothesis’ (a sobriquet due to Hoijer 1954). Nor are they what\nwas called the ‘Whorf thesis’ by Brown and Lenneberg\n(1954) which was concerned with the relation of obligatory lexical\ndistinctions and thought. Brown and Lenneberg (1954) investigated this\nquestion by looking at the relation of color terminology in a language\nand the classificatory abilities of the speakers of that language. The\nissue of the relation between obligatory lexical distinctions and\nthought is at the heart of what is now called ‘the Sapir-Whorf\nHypothesis’ or ‘the Whorf Hypothesis’ or\n‘Whorfianism’. \n\nNo one is going to be impressed with a claim that some aspect of your\nlanguage may affect how you think in some way or other; that is \nneither a philosophical thesis nor a psychological hypothesis. So it \nis appropriate to set aside entirely the kind of so-called hypotheses\nthat Steven Pinker presents in The Stuff of Thought (2007: \n126–128) as “five banal versions of the Whorfian \nhypothesis”: \n\nThese are just truisms, unrelated to any serious issue about \nlinguistic relativism. \n\nWe should also set aside some methodological versions of linguistic \nrelativism discussed in anthropology. It may be excellent advice to a\nbudding anthropologist to be aware of linguistic diversity, and to be\non the lookout for ways in which your language may affect your \njudgment of other cultures; but such advice does not constitute a \nhypothesis. \n\nThe term “Sapir-Whorf Hypothesis” was coined by Harry \nHoijer in his contribution (Hoijer 1954) to a conference on the work \nof Benjamin Lee Whorf in 1953. But anyone looking in Hoijer’s paper \nfor a clear statement of the hypothesis will look in vain. Curiously,\ndespite his stated intent “to review and clarify the \nSapir-Whorf hypothesis” (1954: 93), Hoijer did not even attempt\nto state it. The closest he came was this: \n\nThe claim that “language functions…as a way of defining \nexperience” appears to be offered as a kind of vague \nmetaphysical insight rather than either a statement of linguistic \nrelativism or a testable hypothesis. \n\nAnd if Hoijer seriously meant that what qualitative experiences a \nspeaker can have are constituted by that speaker’s language,\nthen surely the claim is false. There is no reason to doubt that \nnon-linguistic sentient creatures like cats can experience (for \nexample) pain or heat or hunger, so having a language is not a \nnecessary condition for having experiences. And it is surely not \nsufficient either: a robot with a sophisticated natural language \nprocessing capacity could be designed without the capacity for \nconscious experience. \n\nIn short, it is a mystery what Hoijer meant by his “central \nidea”. \n\nVague remarks of the same loosely metaphysical sort have continued to\nbe a feature of the literature down to the present. The statements \nmade in some recent papers, even in respected refereed journals, \ncontain non-sequiturs echoing some of the remarks of Sapir, Whorf, \nand Hoijer. And they come from both sides of the debate. \n\nLila Gleitman is an Essentialist on the other side of the \ncontemporary debate: she is against linguistic relativism, and \nagainst the broadly Whorfian work of Stephen Levinson’s group at the \nMax Planck Institute for Psycholinguistics. In the context of \ncriticizing a particular research design, Li and Gleitman (2002) \nquote Whorf’s claim that “language is the factor that limits \nfree plasticity and rigidifies channels of development”. But in\nthe claim cited, Whorf seems to be talking about the psychological \ntopic that holds universally of human conceptual development, not \nclaiming that linguistic relativism is true. \n\nLi and Gleitman then claim (p. 266) that such (Whorfian) views \n“have diminished considerably in academic favor” in part \nbecause of “the universalist position of Chomskian linguistics,\nwith its potential for explaining the striking similarity of language\nlearning in children all over the world.” But there is no clear\nconflict or even a conceptual connection between Whorf’s views about \nlanguage placing limits on developmental plasticity, and Chomsky’s \nthesis of an innate universal architecture for syntax. In short, \nthere is no reason why Chomsky’s I-languages could not be innately \nconstrained, but (once acquired) cognitively and developmentally \nconstraining. \n\nFor example, the supposedly deep linguistic universal of \n‘recursion’ (Hauser et al. 2002) is surely quite \nindependent of whether the inventory of colour-name lexemes in your \nlanguage influences the speed with which you can discriminate between\ncolor chips. And conversely, universal tendencies in color naming \nacross languages (Kay and Regier 2006) do not show that \ncolor-naming differences among languages are without effect on \ncategorical perception (Thierry et al. 2009). \n\nOne of the first linguists to defend a general form of universalism \nagainst linguistic relativism, thus presupposing that they conflict, \nwas Julia Penn (1972). She was also an early popularizer of the \ndistinction between ‘strong’ and ‘weak’ \nformulations of the Sapir-Whorf Hypothesis (and an opponent of the \n‘strong’ version). \n\n‘Weak’ versions of Whorfianism state that language \ninfluences or defeasibly shapes thought. \n‘Strong’ versions state that language determines\nthought, or fixes it in some way. The weak versions are commonly \ndismissed as banal (because of course there must be some \ninfluence), and the stronger versions as implausible. \n\nThe weak versions are considered banal because they are not \nadequately formulated as testable hypotheses that could conflict with\nrelevant evidence about language and thought. \n\nWhy would the strong versions be thought implausible? For a language \nto make us think in a particular way, it might seem that it must at \nleast temporarily prevent us from thinking in other ways, and thus \nmake some thoughts not only inexpressible but unthinkable. If this \nwere true, then strong Whorfianism would conflict with the Katzian \neffability claim. There would be thoughts that a person couldn’t \nthink because of the language(s) they speak. \n\nSome are fascinated by the idea that there are inaccessible thoughts;\nand the notion that learning a new language gives access to entirely \nnew thoughts and concepts seems to be a staple of popular writing \nabout the virtues of learning languages. But many scientists and \nphilosophers intuitively rebel against violations of effability: \nthinking about concepts that no one has yet named is part of their \njob description. \n\nThe resolution lies in seeing that the language could affect \ncertain aspects of our cognitive functioning without making certain \nthoughts unthinkable for us. \n\nFor example, Greek has separate terms for what we call light blue and\ndark blue, and no word meaning what ‘blue’ means in \nEnglish: Greek forces a choice on this distinction. Experiments have \nshown (Thierry et al. 2009) that native speakers of Greek react \nfaster when categorizing light blue and dark blue color \nchips—apparently a genuine effect of language on thought. But \nthat does not make English speakers blind to the distinction, or \nimply that Greek speakers cannot grasp the idea of a hue falling \nsomewhere between green and violet in the spectrum. \n\nThere is no general or global ineffability problem. There is, though,\na peculiar aspect of strong Whorfian claims, giving them a local \nanalog of ineffability: the content of such a claim cannot be \nexpressed in any language it is true of. This does not make the \nclaims self-undermining (as with the standard objections to \nrelativism); it doesn’t even mean that they are untestable. They are \nsomewhat anomalous, but nothing follows concerning the speakers of \nthe language in question (except that they cannot state the \nhypothesis using the basic vocabulary and grammar that they \nordinarily use). \n\nIf there were a true hypothesis about the limits that basic English \nvocabulary and constructions puts on what English speakers can think,\nthe hypothesis would turn out to be inexpressible in English, using \nbasic vocabulary and the usual repertoire of constructions. That \nmight mean it would be hard for us to discuss it in an article in \nEnglish unless we used terminological innovations or syntactic \nworkarounds. But that doesn’t imply anything about English speakers’ \nability to grasp concepts, or to develop new ways of expressing them \nby coining new words or elaborated syntax. \n\nA number of considerations are relevant to formulating, testing, and \nevaluating Whorfian hypotheses. \n\nGenuine hypotheses about the effects of language on thought will \nalways have a duality: there will be a linguistic part and a \nnon-linguistic one. The linguistic part will involve a claim that \nsome feature is present in one language but absent in another. \n\nWhorf himself saw that it was only obligatory features of languages\nthat established “mental patterns” or “habitual\nthought” (Whorf 1956: 139), since if it were optional then the\nspeaker could optionally do it one way or do it the other way.  And so\nthis would not be a case of “constraining the conceptual\nstructure”.  So we will likewise restrict our attention to\nobligatory features here. \n\nExamples of relevant obligatory features would include lexical \ndistinctions like the light vs. dark blue forced choice in Greek, or \nthe forced choice between “in (fitting tightly)” vs. \n“in (fitting loosely)” in Korean. They also include \ngrammatical distinctions like the forced choice in Spanish 2nd-person\npronouns between informal/intimate and formal/distant (informal \ntú vs. formal usted in the singular; informal \nvosotros vs. formal ustedes in the plural), or the \nforced choice in Tamil 1st-person plural pronouns between inclusive \n(“we = me and you and perhaps others”) and exclusive \n(“we = me and others not including you”). \n\nThe non-linguistic part of a Whorfian hypothesis will contrast the \npsychological effects that habitually using the two languages has on \ntheir speakers. For example, one might conjecture that the habitual \nuse of Spanish induces its speakers to be sensitive to the formal and\ninformal character of the speaker’s relationship with their \ninterlocutor while habitually using English does not. \n\nSo testing Whorfian hypotheses requires testing two independent \nhypotheses with the appropriate kinds of data. In consequence, \nevaluating them requires the expertise of both linguistics and \npsychology, and is a multidisciplinary enterprise. Clearly, the \nlinguistic hypothesis may hold up where the psychological hypothesis \ndoes not, or conversely. \n\nIn addition, if linguists discovered that some linguistic feature was\noptional in two different languages, then even if psychological experiments \nshowed differences between the two populations of speakers, this \nwould not show linguistic determination or influence. The cognitive \ndifferences might depend on (say) cultural differences. \n\nA further important consideration concerns the strength of the \ninducement relationship that a Whorfian hypothesis posits between a \nspeaker’s language and their non-linguistic capacities. The claim \nthat your language shapes or influences your cognition is quite \ndifferent from the claim that your language makes certain kinds of \ncognition impossible (or obligatory) for you. The strength of any \nWhorfian hypothesis will vary depending on the kind of relationship \nbeing claimed, and the ease of revisability of that relation. \n\nA testable Whorfian hypothesis will have a schematic form something \nlike this: \n\nThe relation R might in principle be causation or \ndetermination, but it is important to see that it might merely be \ncorrelation, or slight favoring; and the non-linguistic cognitive \neffect C might be readily suppressible or revisable. \n\nDan Slobin (1996) presents a view that competes with Whorfian \nhypotheses as standardly understood. He hypothesizes that when \nthe speakers are using their cognitive abilities in the service of a \nlinguistic ability (speaking, writing, translating, etc.), the \nlanguage they are planning to use to express their thought will have \na temporary online effect on how they express their thought. The \nclaim is that as long as language users are thinking in order to \nframe their speech or writing or translation in some language, the \nmandatory features of that language will influence the way they \nthink. \n\nOn Slobin’s view, these effects quickly attenuate as soon as the \nactivity of thinking for speaking ends. For example, if a speaker is \nthinking for writing in Spanish, then Slobin’s hypothesis would \npredict that given the obligatory formal/informal 2nd-person pronoun \ndistinction they would pay greater attention to the formal/informal \ncharacter of their social relationships with their audience than if \nthey were writing in English. But this effect is not permanent. As \nsoon as they stop thinking for speaking, the effect of Spanish on \ntheir thought ends. \n\nSlobin’s non-Whorfian linguistic relativist hypothesis raises the \nimportance of psychological research on bilinguals or people who \ncurrently use two or more languages with a native or near-native \nfacility. This is because one clear way to test Slobin-like \nhypotheses relative to Whorfian hypotheses would be to find out \nwhether language correlated non-linguistic cognitive differences \nbetween speakers hold for bilinguals only when are thinking for \nspeaking in one language, but not when they are thinking for speaking\nin some other language. If the relevant cognitive differences \nappeared and disappeared depending on which language speakers were \nplanning to express themselves in, it would go some way to vindicate \nSlobin-like hypotheses over more traditional Whorfian Hypotheses. Of \ncourse, one could alternately accept a broadening of Whorfian \nhypotheses to include Slobin-like evanescent effects. Either way, \nattention must be paid to the persistence and revisability of the \nlinguistic effects. \n\nKousta et al. (2008) shows that “for bilinguals there is \nintraspeaker relativity in semantic representations and, therefore, \n[grammatical] gender does not have a conceptual, non-linguistic \neffect” (843). Grammatical gender is obligatory in the \nlanguages in which it occurs and has been claimed by Whorfians to \nhave persistent and enduring non-linguistic effects on \nrepresentations of objects (Boroditsky et al. 2003). However, Kousta \net al. supports the claim that bilinguals’ semantic representations \nvary depending on which language they are using, and thus have \ntransient effects. This suggests that although some semantic \nrepresentations of objects may vary from language to language, their \nnon-linguistic cognitive effects are transitory. \n\nSome advocates of Whorfianism have held that if Whorfian hypotheses \nwere true, then meaning would be globally and radically \nindeterminate. Thus, the truth of Whorfian hypotheses is equated with\nglobal linguistic relativism—a well known self-undermining form\nof relativism. But as we have seen, not all Whorfian hypotheses are \nglobal hypotheses: they are about what is induced by particular \nlinguistic features. And the associated non-linguistic perceptual and\ncognitive differences can be quite small, perhaps insignificant. For \nexample, Thierry et al. (2009) provides evidence that an obligatory \nlexical distinction between light and dark blue affects Greek \nspeakers’ color perception in the left hemisphere only. And the \nquestion of the degree to which this affects sensuous experience is \nnot addressed. \n\nThe fact that Whorfian hypotheses need not be global linguistic \nrelativist hypotheses means that they do not conflict with the claim \nthat there are language universals. Structuralists of the first half \nof the 20th century tended to disfavor the idea of universals: Martin\nJoos’s characterization of structuralist linguistics as claiming that\n“languages can differ without limit as to either extent or \ndirection” (Joos 1966, 228) has been much quoted in this \nconnection. If the claim that languages can vary without limit were \nconjoined with the claim that languages have significant and \npermanent effects on the concepts and worldview of their speakers, a \ntruly profound global linguistic relativism would result. But neither\nconjunct should be accepted. Joos’s remark is regarded by nearly all \nlinguists today as overstated (and merely a caricature of the \nstructuralists), and Whorfian hypotheses do not have to take a global\nor deterministic form. \n\nJohn Lucy, a conscientious and conservative researcher of Whorfian \nhypotheses, has remarked: \n\nAlthough further empirical studies on Whorfian hypotheses have been \ncompleted since Lucy published his 1996 review article, it is hard to\nfind any that have satisfied the criteria of: \n\nThere is much important work yet to be done on testing the range of \nWhorfian hypotheses and other forms of linguistic conceptual \nrelativism, and on understanding the significance of any Whorfian \nhypotheses that turn out to be well supported. \n\nThe three approaches to linguistic theorizing have at least something\nto say about how languages are acquired, or could in principle be \nacquired. Language acquisition has had a much higher profile since \ngenerative Essentialist work of the 1970s and 1980s gave it a central\nplace on the agenda for linguistic theory. \n\nResearch into language acquisition falls squarely within the \npsychology of language; see the entry on \n language and innateness.\n In this section we do not aim to deal in detail with any of the voluminous\nliterature on psychological or computational experiments bearing on \nlanguage acquisition, or with any of the empirical study of language \nacquisition by developmental linguists, or the ‘stimulus \npoverty’ argument for the existence of innate knowledge about \nlinguistic structure (Pullum and Scholz 2002). Our goals are merely \nto define the issue of linguistic nativism, set it \nin context, and draw morals for our three approaches from some of the\nmathematical work on inductive language learning. \n\nThe reader with prior acquaintance with the literature of linguistics\nwill notice that we have not made reference to any partitioning of \nlinguists into two camps called ‘empiricists’ and \n‘rationalists’ (see e.g. Matthews 1984, Cowie 1999). We \ndraw a different distinction relating to the psychological and \nbiological prerequisites for first language acquisition. It divides \nnearly all Emergentists and Externalists from most Essentialists. It \nhas often been confused with the classical empiricist/rationalist \nissue. \n\nGeneral nativists maintain that the prerequisites \nfor language acquisition are just general cognitive abilities and \nresources. Linguistic nativists, by contrast, claim \nthat human infants have access to at least some specifically \nlinguistic information that is not learned from linguistic \nexperience. Table 3 briefly sketches the differences between the two \nviews. Table 3:\nGeneral and linguistic nativism contrasted \n\nThere does not really seem to be anyone who is a complete \nnon-nativist: nobody really thinks that a creature with no unlearned \ncapacities at all could acquire a language. That was the point of the\nmuch-quoted remark by Quine (1972: 95–96) about how “the \nbehaviorist is knowingly and cheerfully up to his neck in innate \nmechanisms of learning-readiness”. Geoffrey Sampson (2001, \n2005) is about as extreme an opponent of linguistic nativism as one \ncan find, but even he would not take the failure of language \nacquisition in his cat to be unrelated to the cognitive and physical \ncapabilities of cats. \n\nThe issue on which empirical research can and should be done is \nwhether some of the unlearned prerequisites that humans enjoy have \nspecifically linguistic content. For a philosophically-oriented \ndiscussion of the matter, see chapters 4–6 of Stainton (2006). \nFor extensive debate about “the argument from poverty of the \nstimulus”, see Pullum and Scholz (2002) together with the six \ncritiques published in the same issue of The Linguistic \nReview and the responses to those critiques by Scholz and Pullum\n(2002). \n\nLinguists have given considerable attention to considerations of \nin-principle learnability—not so much the \ncourse of language acquisition as tracked empirically (the work of \ndevelopmental psycholinguists) but the question of how languages of \nthe human sort could possibly be learned by any kind of learner. The \ntopic was placed squarely on the agenda by Chomsky (1965); and a \nhugely influential mathematical linguistics paper by Gold (1967)has \ndominated much of the subsequent discussion. \n\nGold began by considering a reformulation of the standard \nphilosophical problem of induction. The trouble with the question \n‘Which hypothesis is correct given the totality of the \ndata?’ is of course the one that Hume saw: if the domain is \nunbounded, no finite amount of data can answer the question. Any \nfinite body of evidence will be consistent with arbitrarily many \nhypotheses that are not consistent with each other. But Gold proposed\nreplacing the question with a very different one: Which tentative\nhypothesis is the one to pick, given the data provided so far, \nassuming a finite number of wrong guesses can be forgiven? \n\nGold assumed that the hypotheses, in the case of language learning, \nwere generative grammars (or alternatively parsers; he proves results\nconcerning both, but for brevity we follow most of the literature and\nneglect the very similar results on parsers). The learner’s task is \nconceived of as responding to an unending input data stream \n(ultimately complete, in that every expression eventually turns up) \nby enunciating a sequence of guesses at grammars. \n\nAlthough Gold talks in developmental psycholinguistic terms about \nlanguage learners learning grammars by trial and error, his extremely\nabstract proofs actually make no reference to the linguistic content \nof languages or grammars at all. The set of all finite grammars \nformulable in any given metalanguage is computably enumerable, so \ngrammars can be systematically numbered. Inputs—grammatical \nexpressions from the target language—can also be numerically \nencoded. We end up being concerned simply with the existence or \nnon-existence of certain functions from natural number sequences to \nnatural numbers. \n\nA successful learner is one who uses a procedure that is guaranteed \nto eventually hit on a correct grammar. For single languages, this is\ntrivial: if the target language is L and it is generated by a \ngrammar G, then the procedure “Always guess \nG” does the job, and every language is learnable. What \nmakes the problem interesting is applying it to classes of \ngrammars. A successful learner for a class C is one who uses a\nprocedure that is guaranteed to succeed no matter what grammar from \nC is the target and no matter what the data stream is like (as\nlong as it is complete and contains no ungrammatical examples). \n\nGold’s work has interesting similarities with earlier philosophical \nwork on inductive learning by Hilary Putnam (1963; it is not clear \nwhether Gold was aware of this paper). Putnam gave an informal proof \nof a sort of incompleteness theorem for inductive regularity-learning\ndevices: no matter what algorithm is used in a machine for inducing \nregularities from experience, and thus becoming able to predict \nevents, there will always be some possible environmental regularities\nthat will defeat it. (As a simple example, imagine an environment \ngiving an unbroken sequence of presentations all having some property\na. If there is a positive integer n such that after \nn presentations the machine will predict that presentation \nnumber n + 1 will also have property a, then the machine will \nbe defeated by an environment consisting of n presentations of\na followed by one with the incompatible property \nb—the future need not always resemble the past. But if \non the other hand there is no such n, then an environment \nconsisting of an unending sequence of a presentations will \ndefeat it.) \n\nGold’s theorems are founded on certain specific idealizing \nassumptions about the language learning situation, some of which are \nintuitively very generous to the learner. The main ones are these: \n\nThe most celebrated of the theorems Gold proved (using some reasoning\nremarkably similar to that of Putnam 1963) showed that a language \nlearner could be similarly hostage to malign environments. Imagine a \nlearner being exposed to an endless and ultimately exhaustive \nsequence of presented expressions from some target \nlanguage—Gold calls such a sequence a ‘text’. \nSuppose the learner does not know in advance whether the language is \ninfinite, or is one of the infinitely many finite languages over the \nvocabulary V. Gold reasons roughly thus: \n\nLeaping too soon to the conclusion that the target language is \ninfinite will be disastrous, because there will be no way to \nretrench: no presented examples from a finite language \nLk will ever conflict with the hypothesis \nthat the target is some infinite superset of \nLk. \n\nThe relevance of all this to the philosophy of linguistics is that \nthe theorem just sketched has been interpreted by many linguists, \npsycholinguists, and philosophers as showing that humans could not \nlearn languages by inductive inference based on examples of language \nuse, because all of the well-known families of languages \ndefined by different types of generative grammar have the crucial \nproperty of allowing grammars for every finite language and for at \nleast some infinite supersets of them. But Gold’s paper has often \nbeen over-interpreted. A few examples of the resultant mistakes \nfollow. \n\nIt’s not about underdetermination. Gold’s negative \nresults are sometimes wrongly taken to be an unsurprising reflection \nof the underdetermination of theories by finite bodies of evidence \n(Hauser et al. 2002 seem to make this erroneous equation on p. 1577; \nso do Fodor and Crowther 2002, implicitly—see Scholz and Pullum\n2002, 204–206). But the failure of text-identifiability for \ncertain classes of languages is different from underdetermination in \na very important way, because there are infinite classes of infinite \nlanguages that are identifiable from text. The first chapter\nof Jain et al. (1999) discusses an illustrative example (basically, \nit is the class containing, for all n > 0, the set of all strings \nwith length greater than n). There are infinitely many others.\nFor example, Shinohara (1990) showed that for any positive integer \nn the class of all languages generated by a context-sensitive \ngrammar with not more than n rules is learnable from text. \n\nIt’s not about stimulus poverty. It has also \nsometimes been assumed that Gold is giving some kind of argument from\npoverty of the stimulus (there are signs of this in Cowie 1999, \n194ff; Hauser et al. 2002, 1577; and Prinz 2002, 210). This is very \nclearly a mistake (as both Laurence and Margolis 2001 and Matthews \n2007 note): in Gold’s text-learning scenario there is no stimulus \npoverty at all. Every expression in the language eventually turns up \nin the learner’s input. \n\nIt’s not all bad news. It is sometimes forgotten \nthat Gold established a number of optimistic results as well as the \npessimistic one about learning from text. Given what he called an \n‘informant’ environment rather than a text environment, \nwe see strikingly different results. An informant environment is an \ninfinite sequence of presentations sorted into two lists, positive \ninstances (expressions belonging to the target language) and negative\ninstances (not in the language). Almost all major language-theoretic \nclasses are identifiable in the limit from an informant environment \n(up to and including the class of all languages with a primitive \nrecursive characteristic function, which comes close to covering any \nlanguage that could conceivably be of linguistic interest), and all \ncomputably enumerable languages become learnable if texts are allowed\nto be sequenced in particular ways (see the results in Gold 1967 on \n‘anomalous text’). \n\nGold did not give a necessary condition for a class to be identifiable\nin the limit from text, but Angluin (1980) later provided one (in a\nresult almost but not quite obtained by Wexler and Hamburger\n1973). Angluin showed that a class C is text-identifiable iff\nevery language L in C has a finite “telltale” subset\nT such that if T is also proper subset of some other\nlanguage in C, that other language is not a proper subset of\nL.  This condition precludes guessing too large a language.\nOnce all the members of the telltale subset for L have been\nreceived as input, the learner can safely make L the current\nconjecture.  The language to be identified must be either L or\n(if subsequent inputs include new sentences not in L) some\nlarger language, but it can’t be a proper subset of L. \n\nJohnson (2004) provides a useful review of several other \nmisconceptions about Gold’s work; e.g., the notion that it might be \nthe absence of semantics from the input that makes identification \nfrom text impossible (this is not the case). \n\nSome generative Essentialists see a kind of paradox in Gold’s \nresults—a reductio on one or more of the assumptions he makes \nabout in-principle learnability. To put it very crudely, learning \ngenerative grammars from presented grammatical examples seems to have\nbeen proved impossible, yet children do learn their first languages, \nwhich for generative Essentialists means they internalize generative \npsychogrammars, and it is claimed to be an empirical fact that they \nget almost no explicit evidence about what is not in the \nlanguage (Brown and Hanlon 1970 is invariably cited to support this).\nContradiction. Gold himself suggested three escape routes from the \napparent paradox: \n\nAll three of these paths have been subsequently explored. Path (1) \nappealed to generative Essentialists. Chomsky (1981) suggested an \nextreme restriction: that universal grammar permitted only finitely \nmany grammars. This claim (for which Chomsky had little basis: see \nPullum 1983) would immediately guarantee that not all finite \nlanguages are humanly learnable (there are infinitely many finite \nlanguages, so for most of them there would be no permissible \ngrammar). Osherson and Weinstein (1984) even proved that under three \nfairly plausible assumptions about the conditions on learning, \nfiniteness of the class of languages is necessary—that is, a \nclass must be finite if it is to be identifiable from text. \nHowever, they also proved that this is not sufficient: there are very\nsmall finite classes of languages that are not identifiable \nfrom text, so it is logically possible for text-identification to be \nimpossible even given only a finite number of languages (grammars). \nThese two results show that Chomsky’s approach cannot be the whole \nanswer. \n\nPath (2) proposes investigation of children’s input with an eye to \nfinding covert sources of negative evidence. Various psycholinguists \nhave pursued this idea; see the entry on \n language and innateness\n in this encyclopedia, and (to cite one example) the results of \nChouinard and Clark (2003) on hitherto unnoticed sources of negative \nevidence in the infant’s linguistic environment, such as parental \ncorrections. \n\nPath (3) suggests investigating the nature of children’s linguistic \nenvironments more generally. Making evidence available to the learner\nin some fixed order can certainly alter the picture quite radically \n(Gold proved that if some primitive-recursive generator controls the \ntext it can in effect encode the identity of the target language so \nthat all computably enumerable languages become identifiable from \ntext). It is possible in principle that limitations on texts (or on \nlearners’ uptake) might have positive rather than negative effects on\nlearnability (see Newport 1988; Elman 1993; Rohde and Plaut 1999; and\nthe entry on \n language and innateness).\n  \n\nGold’s suggested strategy of restricting the pre-set class of \ngrammars has been interpreted by some as a defense of rationalist \nrather than empiricist theories of language acquisition. For example,\nWexler and Culicover state: \n\nWexler and Culicover claim that ‘empiricist’ learning \nmechanisms are both weak and general: not only are they ‘not \nrelated to the learning of any particular subject matter or cognitive\nability’ but they are not ‘limited to any particular \nspecies’. It is of course not surprising that empiricist \nlearning fails if it is defined in a way that precludes drawing a \ndistinction between the cognitive abilities of humans and fruit \nflies. \n\nEquating Gold’s idea of restricting the class of grammars with the \nidea of a ‘rationalist’ knowledge acquisition theory, \nWexler and Culicover try to draw out the consequences of Gold’s \nparadigm for the Essentialist linguistic theory of Chomsky (1965). \nThey show how a very tightly restricted class of transformational \ngrammars could be regarded as text-identifiable under extremely \nstrong assumptions (e.g., that all languages have the same innately \nknown deep structures). \n\nMatthews (1984) follows Wexler and Culicover’s lead and draws a more \nphilosophically oriented moral: \n\nThe actual relation of Gold’s results to the empiricism/rationalism \ncontroversy seems to us rather different. Gold’s paradigm looks a lot\nmore like a formalization of so-called ‘rationalism’. The\nfixed class of candidate hypotheses (grammars) corresponds to what is\ngiven by universal grammar—the innate definition of the \nessential properties of language. What Gold actually shows, \ntherefore, is not “the plausibility of rationalism” but \nrather the inadequacy of a huge range of rationalist theories: under \na wide range of different choices of universal grammar, language \nacquisition appears to remain impossible. \n\nMoreover, Matthews ignores (as most linguists have) the existence of \nlarge and interesting classes of languages that are \ntext-identifiable. \n\nGold’s result, like Putnam’s earlier one, does show that a certain \nkind of trial-and-error inductive learning is insufficient to permit \nlearning of arbitrary environmental regularities. There has to be \nsome kind of initial bias in the learning procedure or in the data. \nBut ‘empiricism’, the supposed opponent of \n‘rationalism’, is not to be equated with a denial of the \nexistence of learning biases. No one doubts that humans have \ninductive biases. To quote Quine again, “Innate biases and \ndispositions are the cornerstone of behaviorism, and have been \nstudied by behaviorists” (1972: 95–96). As Lappin and \nShieber (2007) stress, there cannot be such a thing as a learning \nprocedure (or processing mechanism) with no biases at all. \n\nThe biases posited in Emergentist theories of language acquisition \nare found, at least in part, in the non-linguistic social and \ncognitive bases of human communication. And the biases of Externalist\napproaches to language acquisition are to be found in the \ndistributional and stochastic structure of the learning input and the\nmultitude of mechanisms that process that input and their \ninteractions. All contemporary approaches to language acquistion have\nacknowledged Gold’s results, but those results do not by themselves \nvindicate any one of our three approaches to the study of language. \n\nGold’s explicit equation of acquiring a language with identifying a \ngenerative grammar that exactly generates it naturally makes his work\nseem relevant to generative Essentialists (though even for them, his \nresults do not provide anything like a sufficient reason for adopting\nthe linguistic nativist position). But another key assumption, that \nnothing about the statistical structure of the input plays a role in \nthe acquisition process, is being questioned by increasing numbers of\nExternalists, many of whom have used Bayesian modeling to show that \nthe absence of positive evidence can function as a powerful source of\n(indirect) negative evidence: learning can be driven by what is not \nfound as well as by what is (see e.g. Foraker et al. (2009)). \n\nMost Emergentists simply reject the assumption that what is learned \nis a generative grammar. They see the acquisition task as a matter of\nlearning the details of an array of constructions (roughly, \nmeaning-bearing ways of structurally composing words or phrases) and \nhow to use them to communicate. How such learning is accomplished \nneeds a great deal of further study, but Gold’s paper did not show it\nto be impossible. \n\nOver the past two decades a large amount of work has been done on \ntopics to which the term ‘language evolution’ is \nattached, but there are in fact four distinct such topics: \n\nEmergentists tend to regard any of the topics (a)–(d) as \npotentially relevant to the study of language evolution. \nEssentialists tend to focus solely on (c). Some Essentialists even \ndeny that (a) and (b) have any relevance to the study of (c); for \nexample: \n\nOther generative Essentialists, like Pinker and Bloom (1990) and \nPinker and Jackendoff (2005), seem open to the view that even the \nmost elemental aspects of topic (b) can be directly relevant to the \nstudy of (c). This division among Essentialists reflects a division \namong their views about the role of adaptive explanations in the \nemergence of (b) and especially (c). For example: \n\nThe view expressed here that all (or even most) interesting \nproperties of the language faculty are not adaptations conflicts with\nthe basic explanatory strategy of evolutionary psychology found in \nthe neo-Darwinian Essentialist views of Pinker and Bloom. \nPiattelli-Palmarini (1989), following Chomsky, adopts a fairly \nstandard Bauplan critique of adaptationism. On this view the language\nfaculty did not originate as an adaptation, but more plausibly \n“may have originally arisen for some purely architectural or \nstructural reason (perhaps overall brain size, or the sheer \nduplication of pre-existing modules), or as a by product of the \nevolutionary pressures” (p. 19), i.e., it is a kind of Gouldian\nspandrel. \n\nMore recently, some Essentialist-leaning authors have rejected the \nview that no analogies and homologies between animal and human \ncommunication are relevant to the study of language. For example, in \nthe context of commenting on Hauser et al. (2002), Tecumseh Fitch \n(2010) claims that “Although Language, writ large, is unique to\nour species (many probably most) of the mechanisms involved in \nlanguage have analogues or homologues in other animals.” \nHowever, the view that the investigation of animal communication can \nshed light on human language is still firmly rejected by some. For \nexample, Bickerton (2007: 512) asserts that “nothing resembling\nhuman language could have developed from prior animal call \nsystems.” \n\nBickerton fronts the following simple argument for his view: \n\nThus, the mere fact that language is unique to humans is sufficient \nto rule out monkey and primate call systems as preadapations for \nlanguage. But, contra Bickerton, a neo-Darwinian like Jackendoff \n(2002) appeals to the work of Dunbar (1998), Power (1998), Worden \n(1998) to provide a selectionist story which assumes that cooperation\nin hunting, defense (Pinker and Bloom 1990), and “ \n‘social grooming’ or deception” are selective \nforces that operated on human ancestors to drive increases in \nexpressive power that distinguishes non-human communication and human\nlinguistic capacities and systems. \n\nWhile generative Essentialists debate among themselves about the \nplausibility of adaptive explanations for the emergence of essential \nfeatures of a modular language capacity, Emergentists are perhaps \nbest characterized as seeking broad evolutionary explanations of the \nfeatures of languages (topic (c)) and communicative capacities \n(topics (b) and (c)) conceived in non-essentialist, non-modular ways.\nAnd as theorists who are committed to exploring non-modular views \nof linguistic capacities (topic (c)), the differences and \nsimilarities between (a) and (b) are potentially relevant to (c). \n\nPrimatologists like Cheney and Seyfarth, psychologists like \nTomasello, anthropologists like Terrence Deacon, and linguists like \nPhillip Lieberman share an interest in investigating the \ncommunicative, anatomical, and cognitive characteristics of non-human\nanimals to identify biological differences between humans, and \nmonkeys and primates. In the following paragraph we discuss Cheney \nand Seyfarth (2005) as an example, but we could easily have chosen \nany of a number of other theorists. \n\nCheney and Seyfarth (2005) emphasize that non-human primates have a \nsmall, stimulus specific repertoire of vocal productions that are not \n“entirely involuntary,” and this contrasts with their \n“almost openended ability to learn novel sound-meaning \npairs” (p. 149). They also emphasize that vocalizations in \nmonkeys and apes are used to communicate information about the \nvocalizer, not to provide information intended to “rectify \nfalse beliefs in others or instruct others” (p. 150). Non-human\nprimate communication consists in the mainly involuntary broadcasting\nof the vocalizer’s current affective state. Moreover, although Cheney\nand Seyfarth recognize that the vervet monkey’s celebrated call \nsystem (Cheney and Seyfarth 1990) is “functionally \nreferential” in context, their calls have no explicit meaning \nsince they lack “any propositional structure”. From this \nthey conclude: \n\nBy ‘lexical syntax’ Cheney and Seyfarth mean a kind of \nsemantic compositionality of characteristic vocalizations. If a \nvocalization (call) were to have lexical syntax, the semantic \nsignificance of the whole would depend on the relation of the \nstructure of parts of the call to the structure of what they signify.\nThe absence of ‘lexical syntax’ in call systems suggests \nthat it is illegitimate to think of them as having anything like \nsemantic structure at all. \n\nDespite the rudimentary character of animal communication systems \nwhen compared with human languages, Cheney and Seyfarth argue that \nmonkeys and apes exhibit at least five characteristics that are \npre-adaptations for human communication: \n\nIt is, of course, controversial to claim that monkeys have \nrule-governed propositional social knowledge systems as claimed in \n(iv) and (v). But Emergentists, Externalists, and Essentialists could\nall, in principle, agree that there are both unique characteristics \nof human communicative capacities and characteristics of such \ncapacities that are shared with non-humans. For example, by the age \nof one, human infants can use direction of gaze and focus of \nattention to infer the referent of a speaker’s utterance (Baldwin and\nMoses 1994). By contrast, this sort of social referencing capacity in\nmonkeys and apes is rudimentary. This suggests that a major component\nof humans’ capacity to infer a specific referent is lacking in \nnon-humans. \n\nDisagreements between the approaches might be due to the perceived \nsignificance of non-human communicative capacities and their relation\nto uniquely human ones. \n\nWe mentioned earlier that both early 20th-century linguistics \nmonographs and contemporary introductory textbooks include \ndiscussions of historical linguistics, i.e., that branch that studies\nthe history and prehistory of changes in particular languages, how \nthey are related to each other, and how and why they change. \n\nThe last decade has seen two kinds of innovations related to studying\nchanges in particular languages. One, which we will call \n‘linguistic phylogeny’, concerns the application of \nstochastic phylogenetic methods to investigate prehistoric population\nand language dispersion (Gray and Jordan 2000, Gray 2005, Atkinson \nand Gray 2006, Gray et al. 2009). These methods answer questions \nabout how members of a family of languages are related to each other \nand dispersed throughout a geographic area. The second, which we will\ncall the effects of transmission, examines how interpreted artificial\nlanguages (sets of signifier/signified pairs) change under a range of\ntransmission conditions (Kirby et al. 2008, Kirby 2001, Hurford \n2000), thus providing evidence about how the process of transmission \naffects the characteristics, especially the structure, of the \ntransmitted interpreted system. \n\nRussell Gray and his colleagues have taken powerful phylogenetic \nmethods that were developed by biologists to investigate molecular \nevolution, and applied them to linguistic data in order to answer \nquestions about the evolution of language families. For example, Gray\nand Jordan (2000) used a parsimony analysis of a large language data \nset to adjudicate between competing hypotheses about the speed of the\nspread of Austronesian languages through the Pacific. More recently, \nGreenhill et al. (2010) used a NeighbourNet analysis to evaluate the \nrelative rates of change in the typological and lexical features of \nAustronesian and Indo-European. These results bear on hypotheses \nabout the relative stability of language types over lexical features \nof those languages, and how far back in time that stability extends. \nIf there were highly conserved typological and lexical features, then\nit might be possible to identify relationships between languages that\ndate beyond the 8000 (plus or minus 2000) year limit that is imposed \nby lexical instability. \n\nThe computational and laboratory experiments of Kirby and his \ncollaborators have shown that under certain conditions of iterated \nlearning, any given set of signifier/signified pairs in which the \nmapping is initially arbitrary will change to exhibit a very general \nkind of compositional structure. Iterated learning has been studied \nin both computational and laboratory experiments by means of \ndiffusion chains, i.e., sequences of learners. A primary \ncharacteristic of such sequences of transmission is that what is \ntransmitted from learner to learner will change in an iterated \nlearning environment, in a way that depends on the conditions of \ntransmission. \n\nThe children’s game called ‘Telephone’ in the USA \n(‘Chinese Whispers’ in the UK), provides an example of \ndiffusion chains under which what is transmitted is not stable. In a \ndiffusion chain learning situation what a chain member has actually \nlearned from an earlier member of the chain is presented as the input\nto the next learner, and what that learner has actually learned \nprovides the input to the following learner. In cases where the \ninitial learning task is very simple: i.e., where what is transmitted\nis both simple, completely transmitted, and the transmission channel \nis not noisy, what is transmitted is stable over iterated \ntransmissions even in cases when the participants are young children \nand chimpanzees (Horner et al. 2006). That is, there is little change\nin what is transmitted over iterated transmissions. However, in cases\nwhere what is transmitted is only partially presented, very complex, \nor the channel is noisy, then there is a decrease in the fidelity of \nwhat is transmitted across iterations just like there is in the \nchildren’s game of Telephone. \n\nWhat Kirby and colleagues show is that when the initial input to a \ndiffusion chain is a reasonably complex set of arbitrary \nsignal/signifier pairs, e.g. one in which 27 complex signals of 6 \nletters are randomly assigned to 27 objects varying on dimensions of \ncolor, kind of motion, and shape, what is transmitted becomes more \nand more compositional over iterated transmission. Here, \n‘compositional’ is being used to refer to the high degree\nto which sub-strings of the signals come to be systematically paired \nwith specific phenomenal sub-features of what is signified. The \ntransmission conditions in these experiments were free of noise, and \nfor each iteration of the learning task only half of the possible 27 \nsignifier/signified pairs were presented to participants. Under this \nkind of transmission bottleneck a high degree of sign/signified \nstructure emerged. \n\nA plausible interpretation of these results is that the developing \nstructure of the collection of signs is a consequence of the repeated\nforced inference by participants from 14 signs and signifieds in the \ntraining set to the entire set of 27 pairs. A moral could be that \niterated forced prediction of the sign/signified pairs in the entire \nset, on the basis of exposure to only about half of them, induced the\ndevelopment of a systematic, compositional structure over the course \nof transmission. It is reasonable to conjecture that this resulting \nstructure reflects effects of human memory, not a domain-specific \nlanguage module—although further work would be required to rule\nout many other competing hypotheses. \n\nThus Kirby and his colleagues focus on something very different from \nthe prerequisites for language emergence. Linguistic nativists have \nbeen interested in how primates like us could have become capable of \nacquiring systems with the structural properties of natural \nlanguages. Kirby and his colleagues (while not denying that human \ncognitive evolution is of interest) are studying how languages \nevolve to be capable of being acquired by primates like us.","contact.mail":"francisp@ualberta.ca","contact.domain":"ualberta.ca"},{"date.published":"2011-09-21","date.changed":"2015-01-01","url":"https://plato.stanford.edu/entries/linguistics/","author1":"Barbara C. Scholz","author2":"Francis Jeffry Pelletier","author1.info":"http://www.ualberta.ca/~francisp/","author2.info":"http://www.ling.ed.ac.uk/~gpullum/","entry":"linguistics","body.text":"\n\n\n\nPhilosophy of linguistics is the philosophy of science as applied to \nlinguistics. This differentiates it sharply from the philosophy of \nlanguage, traditionally concerned with matters of meaning and \nreference.\n\n\n\nAs with the philosophy of other special sciences, there are general \ntopics relating to matters like methodology and explanation (e.g., \nthe status of statistical explanations in psychology and sociology, \nor the physics-chemistry relation in philosophy of chemistry), and \nmore specific philosophical issues that come up in the special \nscience at issue (simultaneity for philosophy of physics; \nindividuation of species and ecosystems for the philosophy of \nbiology). General topics of the first type in the philosophy of \nlinguistics include:\n\nWhat the subject matter is,\n\nWhat the theoretical goals are,\n\nWhat form theories should take, and\n\nWhat counts as data.\n\n\nSpecific topics include issues in language learnability, language \nchange, the competence-performance distinction, and the expressive \npower of linguistic theories.\n\n\n\nThere are also topics that fall on the borderline between philosophy \nof language and philosophy of linguistics: of “linguistic \nrelativity” (see the supplement on the linguistic relativity hypothesis \n  in the Summer 2015 archived version of the entry on \n relativism),\n language vs. \n idiolect,\n \n speech acts\n (including the distinction between locutionary, illocutionary, and \nperlocutionary acts), the language of thought, implicature, and the \nsemantics of mental states (see the entries on \n analysis,\n\n  semantic compositionality,\n \n mental representation,\n \n pragmatics,\n and \n defaults in semantics and pragmatics).\n In these cases it is often the kind of answer given and not the \ninherent nature of the topic itself that determines the \nclassification. Topics that we consider to be more in the philosophy \nof language than the philosophy of linguistics include intensional \ncontexts, direct reference, and empty names (see the entries on \n propositional attitude reports,\n \n intensional logic,\n \n rigid designators,\n \n reference,\n and \n descriptions).\n \n\n\n\nThis entry does not aim to provide a general introduction to \nlinguistics for philosophers; readers seeking that should consult a \nsuitable textbook such as Akmajian et al. (2010) or Napoli (1996). \nFor a general history of Western linguistic thought, including recent\ntheoretical linguistics, see Seuren (1998). Newmeyer (1986) is useful\nadditional reading for post-1950 American linguistics. \nTomalin (2006) traces the philosophical, scientific, and \nlinguistic antecedents of Chomsky’s magnum opus (1955/1956; published\n1975), and Scholz and Pullum (2007) provide a critical review.\n\n\n\nThe issues we discuss have been debated with vigor and sometimes \nvenom. Some of the people involved have had famous exchanges in the \nlinguistics journals, in the popular press, and in public forums. To \nunderstand the sharp disagreements between advocates of the \napproaches it may be useful to have a sketch of the dramatis personae\nbefore us, even if it is undeniably an oversimplification. \n\nWe see three tendencies or foci, divided by what they take to be the \nsubject matter, the approach they advocate for studying it, and what \nthey count as an explanation. We characterize them roughly in Table \n1. Table 1. Three Approaches to the Study of Language \n\nA broad and varied range of distinct research projects can be pursued\nwithin any of these approaches; one advocate may be more motivated by\nsome parts of the overall project than others are. So the tendencies \nshould not be taken as sharply honed, well-developed research \nprograms or theories. Rather, they provide background biases for the \ndevelopment of specific research programs—biases which \nsometimes develop into ideological stances or polemical programs or \nlead to the branching off of new specialisms with separate journals. \nIn the judgment of Phillips (2010), “Dialog between adherents \nof different approaches is alarmingly rare.” \n\nThe names we have given these approaches are just mnemonic tags, not \ndescriptions. The Externalists, for example, might well have been \ncalled ‘structural descriptivists’ instead, since they \ntend to be especially concerned to develop models that can be used to\npredict the structure of natural language expressions. The \nExternalists have long been referred to by Essentialists as \n‘empiricists’ (and sometimes Externalists apply that term to \nthemselves), though this is misleading (see Scholz and Pullum 2006: \n60–63): the ‘empiricist’ tag comes with an \naccusation of denying the role of learning biases in language \nacquisition (see Matthews 1984, Laurence and Margolis 2001), but that\nis no part of the Externalists’ creed (see e.g. Elman 1993, Lappin \nand Shieber 2007). \n\nEmergentists are also sometimes referred to by Essentialists as \n‘empiricists’, but they either use the Emergentist label \nfor themselves (Bates et al. 1998, O’Grady 2008, MacWhinney 2005) or \ncall themselves ‘usage-based’ linguists (Barlow and \nKemmer 2002, Tomasello 2003) or ‘construction \ngrammarians’ (Goldberg 1995). Newmeyer (1991), like Tomasello, \nrefers to the Essentialists as ‘formalists’, because of \ntheir tendency to employ abstractions, and to use tools from \nmathematics and logic. \n\nDespite these terminological inconsistencies, we can look at what \ntypical members of each approach would say about their vision of \nlinguistic science, and what they say about the alternatives. Many of\nthe central differences between these approaches depend on what \nproponents consider to be the main project of linguistic theorizing, \nand what they count as a satisfying explanation. \n\nMany researchers—perhaps most—mix elements from each of \nthe three approaches. For example, if Emergentists are to explain the\nsyntactic structure of expressions by appeal to facts about the \nnature of the use of symbols in human communication, then they will \npresuppose a great deal of Externalist work in describing linguistic \npatterns, and those Externalists who work on computational parsing \nsystems frequently use (at least as a starting point) rule systems \nand ‘structural’ patterns worked out by Essentialists. \nCertainly, there are no logical impediments for a researcher with one\ntendency from simultaneously pursuing another; these approaches are \nonly general centers of emphasis. \n\nIf one assumes, with the Externalists, that the main goal of a \nlinguistic theory is to develop accurate models of the structural \nproperties of the speech sounds, words, phrases, and other linguistic\nitems, then the clearly privileged information will include corpora \n(written and oral)—bodies of attested and recorded language use\n(suitably idealized). The goal is to describe how this public record \nexhibits certain (perhaps non-phenomenal) patterns that are \nprojectable. \n\nAmerican structural linguistics of the 1920s to 1950s championed the \ndevelopment of techniques for using corpora as a basis for developing\nstructural descriptions of natural languages, although such work was \nreally not practically possible until the wide-spread availability of\ncheap, powerful, and fast computers. André Martinet (1960: 1) \nnotes that one of the basic assumptions of structuralist approaches \nto linguistics is that “nothing may be called \n‘linguistic’ that is not manifest or manifested one way \nor another between the mouth of the speaker and the ears of the \nlistener”. He is, however, quick to point out that “this \nassumption does not entail that linguists should restrict their field\nof research to the audible part of the communication \nprocess—speech can only be interpreted as such, and not as so \nmuch noise, because it stands for something else that is not \nspeech.” \n\nAmerican structuralists—Leonard Bloomfield in \nparticular—were attacked, sometimes legitimately and sometimes \nillegitimately, by certain factions in the Essentialist tradition. \nFor example, it was perhaps justifiable to criticize Bloomfield for \nadopting a nominalist ontology as popularized by the logical \nempiricists. But he was later attacked by Essentialists for holding \nanti-mentalist views about linguistics, when it is arguable that his \nactual view was that the science of linguistics should not commit \nitself to any particular psychological theory. (He had earlier been \nan enthusiast for the mentalist and introspectionist psychology of \nWilhelm Wundt; see Bloomfield 1914.) \n\nExternalism continues to thrive within computational linguistics, \nwhere the American structuralist vison of studying language through \nautomatic analysis of corpora has enjoyed a recrudescence, and very \nlarge, computationally searchable corpora are being used to test \nhypotheses about the structure of languages (see Sampson 2001, \nchapter 1, for discussion). \n\nEmergentists aim to explain the capacity for language in terms of \nnon-linguistic human capacities: thinking, communicating, and \ninteracting. Edward Sapir expressed a characteristic Emergentist \ntheme when he wrote: \n\nThe “pretty patterns” derided here are characteristic of \nstructuralist analyses. Sociolinguistics, which is much closer in \nspirit to Sapir’s project, studies the influence of social and \nlinguistic structure on each other. One particularly influential \nstudy, Labov (1966), examines the influence of social class on \nlanguage variation. Other sociolinguists examine the relation between\nstatus within a group on linguistic innovation (Eckert 1989). This \ninterest in variation within languages is characteristic of \nEmergentist approaches to the study of language. \n\nAnother kind of Emergentist, like Tomasello (2003), will stress the \nrole of theory of mind and the capacity to use symbols to change \nconspecifics’ mental states as uniquely human preadaptations for \nlanguage acquisition, use, and invention. MacWhinney (2005) aims to \nexplain linguistic phenomena (such as phrase structure and \nconstraints on long distance dependencies) in terms of the way \nconversation facilitates accurate information-tracking and \nperspective-switching. \n\nFunctionalist research programs generally fall within the broad \ntendency to approach the study of language as an Emergentist. \nAccording to one proponent: \n\nAnd according to Russ Tomlin, a linguist who takes a functionalist \napproach: \n\nThe idea that linguistic form is autonomous, and more specifically \nthat syntactic form (rather than, say, phonological form) is \nautonomous, is a characteristic theme of the Essentialists. And the \nclaims of Van Valin and Tomlin to the effect that syntax is \nnot independent of semantics and pragmatics might tempt some\nto think that Emergentism and Essentialism are logically \nincompatible. But this would be a mistake, since there are a large \nnumber of nonequivalent autonomy of form theses. \n\nEven in the context of trying to explain what the autonomy thesis is,\nNewmeyer (1991: 3) talks about five formulations of the thesis, each \nof which can be found in some Essentialists’ writings, without \n(apparently) realizing that they are non-equivalent. One is the \nrelatively strong claim that the central properties of linguistic \nform must not be defined with essential reference to \n“concepts outside the system”, which suggests that no \nprimitives in linguistics could be defined in psychological or \nbiological terms. Another takes autonomy of form to be a \nnormative claim: that linguistic concepts ought not\nto be defined or characterized in terms of non-linguistic concepts. \nThe third and fourth versions are ontological: one denies that \ncentral linguistic concepts should be ontologically reduced \nto non-linguistic ones, and the other denies that they can \nbe. And in the fifth version the autonomy of syntax is taken to deny \nthat syntactic patterning can be explained in terms of \nmeaning or discourse functions. \n\nFor each of these versions of autonomy, there are Essentialists who \nagree with it. Probably the paradigmatic Essentialist agrees with \nthem all. But Emergentists need not disagree with them all. \nParadigmatic functionalists like Tomlin, Van Valin and MacWhinney \ncould in principle hold that the explanation of syntactic form, for \nexample, will ultimately be in terms of discourse functions and \nsemantics, but still accept that syntactic categories cannot be \nreduced to non-linguistic ones. \n\nIf Leonard Bloomfield is the intellectual ancestor of Externalism, \nand Sapir the father of Emergentism, then Noam Chomsky is the \nintellectual ancestor of Essentialism. The researcher with \npredominantly Essentialist inclinations aims to identify the \nintrinsic properties of language that make it what it is. For a huge \nmajority of practitioners of this approach—researchers in the \ntradition of generative grammar associated with \nChomsky—this means postulating universals of human linguistic \nstructure, unlearned but tacitly known, that permit and assist \nchildren to acquire human languages. This generative Essentialism has\na preference for finding surprising characteristics of languages that\ncannot be inferred from the data of usage, and are not predictable \nfrom human cognition or the requirements of communication. \n\nRather than being impressed with language variation, as are \nEmergentists and many Externalists, the generative Essentialists are \nextremely impressed with the idea that very young children of almost \nany intelligence level, and just about any social upbringing, acquire\nlanguage to the same high degree of mastery. From this it is inferred\nthat there must be unlearned features shared by all languages that \nsomehow assist in language acquisition. \n\nA large number of contemporary Essentialists who follow Chomsky’s \nteaching on this matter claim that semantics and pragmatics are not a\ncentral part of the study of language.\nIn Chomsky’s view, “it is possible that natural language has \nonly syntax and pragmatics” (Chomsky 1995: 26); that is, only \n“internalist computations and performance systems that access \nthem”; semantic theories are merely “part of an interface\nlevel” or “a form of syntax” (Chomsky 1992: 223). \n\nThus, while Bloomfield understood it to be a sensible practical \ndecision to assign semantics to some field other than linguistics \nbecause of the underdeveloped state of semantic research, Chomsky \nappears to think that semantics as standardly understood is not part \nof the essence of the language faculty at all. (In broad outline, \nthis exclusion of semantics from linguistics comports with Sapir’s \nview that form is linguistic but content is cultural.) \n\nAlthough Chomsky is an Essentialist in his approach to the study of \nlanguage, excluding semantics as a central part of linguistic theory \nclearly does not follow from linguistic Essentialism (Katz 1980 \nprovides a detailed discussion of Chomsky’s views on semantics). \nToday there are many Essentialists who do hold that \nsemantics is a component of a full linguistic theory. \n\nFor example, many linguists today are interested in the \nsyntax-semantics interface—the relationship between the surface\nsyntactic structure of sentences and their semantic interpretation. \nThis area of interest is generally quite alien to philosophers who \nare primarily concerned with semantics only, and it falls outside of \nChomsky’s syntactocentric purview as well. Linguists who work in the \nkind of semantics initiated by Montague (1974) certainly focus on the\nessential features of language (most of their findings appear to be \nof universal import rather than limited to the semantic rules of \nspecific languages). Useful works to consult to get a sense of the \nmodern style of investigation of the syntax-semantics interface would\ninclude Partee (1975), Jacobson (1996), Szabolcsi (1997), Chierchia \n(1998), Steedman (2000). \n\nThe discussion so far has been at a rather high level of abstraction.\nIt may be useful to contrast the three tendencies by looking at how \nthey each would analyze a particular linguistic phenomenon. We have \nselected the syntax of double-object clauses like \nHand the guard your pass (also called \nditransitive clauses), in which the verb is \nimmediately followed by a sequence of two noun phrases, the first \ntypically denoting a recipient and the second something transferred. \nFor many such clauses there is an alternative way of expressing \nroughly the same thing: for Hand the guard your pass there is \nthe alternative Hand your pass to the guard, in which the verb\nis followed by a single object noun phrase and the recipient is \nexpressed after that by a preposition phrase with to. We will \ncall these recipient-PP clauses. \n\nLarson (1988) offers a generative Essentialist approach to the syntax\nof double-object clauses. In order to provide even a rough outline of\nhis proposals, it will be very useful to be able to use tree \ndiagrams of syntactic structure. A tree is a mathematical \nobject consisting of a set of points called nodes \nbetween which certain relations hold. The nodes correspond to \nsyntactic units; left-right order on the page corresponds to temporal\norder of utterance between them; and upward connecting lines \nrepresent the relation ‘is an immediate subpart of’. \nNodes are labeled to show categories of phrases and words, such as \nnoun phrase (NP); preposition phrase (PP); and verb phrase (VP). When\nthe internal structure of some subpart of a tree is basically \nunimportant to the topic under discussion, it is customary to mask \nthat part with an empty triangle. Consider a simple example: an \nactive transitive clause like (Ai) and its passive equivalent (Aii). \n\nA tree structure for (Ai) is shown in (T1). \n\nIn analyses of the sort Larson exemplifies, the structure of an \nexpression is given by a derivation, which consists \nof a sequence of successively modified trees. Larson calls the \nearliest ones underlying structures. The last (and \nleast abstract) in the derivation is the surface structure, which \ncaptures properties relevant to the way the expression is written and\npronounced. The underlying structures are posited in order to better \nidentify syntactic generalizations. They are related to surface \nstructures by a series of operations called \ntransformations (which generative Essentialists \ntypically regard as mentally real operations of the human language \nfaculty). \n\nOne of the fundamental operations that a transformation can effect is\nmovement, which involves shifting a part of the \nsyntactic structure of a tree to another location within it. For \nexample, it is often claimed that passive clauses have very much the \nsame kinds of underlying structures as the synonymous active clauses,\nand thus a passive clause like (Aii) would have an underlying \nstructure much like (T1). A movement transformation would shift \nthe guard toward the end of the clause (and add by), \nand another would shift my pass into the position before the \nverb. In other words, passive clauses look much more like their \nactive counterparts in underlying structure. \n\nIn a similar way, Larson proposes that a double-object clause like \n(B.ii) has the same underlying structure as (B.i). \n\nMoreover, he proposes that the transformational operation of deriving\nthe surface structure of (B.ii) from the underlying structure of \n(B.i) is essentially the same as the one that derives the surface \nstructure of (A.ii) from the underlying structure of (A.i). \n\nLarson adopts many assumptions from Chomsky (1981) and subsequent \nwork. One is that all NPs have to be assigned Case \nin the course of a derivation. (Case is an abstract syntactic \nproperty, only indirectly related to the morphological case forms \ndisplayed by nominative, accusative, and genitive pronouns. Objective\nCase is assumed to be assigned to any NP in direct object position, \ne.g., my pass in (T1), and Nominative Case is assigned to an \nNP in the subject position of a tensed clause, e.g., the guard\nin (T1).) \n\nHe also makes two specific assumptions about the derivation of \npassive clauses. First, Case assignment to the position immediately \nafter the verb is “suppressed”, which entails that the NP\nthere will not get Case unless it moves to some other position. (The \nsubject position is the obvious one, because there it will receive \nNominative Case.) Second, there is an unusual assignment of semantic \nrole to NPs: instead of the subject NP being identified as the agent \nof the action the clause describes, that role is assigned to an \nadjunct at the end of the VP (the by-phrase in (A.ii); an \nadjunct is a constituent with an optional modifying role in its \nclause rather than a grammatically obligatory one like subject or \nobject). \n\nLarson proposes that both of these points about passive clauses have \nanalogs in the structure of double-object VPs. First, Case assignment\nto the position immediately after the verb is suppressed; and since \nLarson takes the preposition to to be the marker of Case, this means \nin effect that to disappears. This entails that the NP after \nto will not get Case unless it moves to some other position. \nSecond, there is an unusual assignment of semantic role to NPs: \ninstead of the direct object NP being identified as the entity \naffected by the action the clause describes, that role is assigned to\nan adjunct at the end of the VP. \n\nLarson makes some innovative assumptions about VPs. First, he \nproposes that in the underlying structure of a double-object clause \nthe direct object precedes the verb, the tree diagram being \n(T2). \n\nThis does not match the surface order of words (showed my pass to \nthe guard), but it is not intended to: it is an underlying \nstructure. A transformation will move the verb to the left of my \npass to produce the surface order seen in (B.i). \n\nSecond, he assumes that there are two nodes labeled VP in a \ndouble-object clause, and two more labeled V′, though there is \nonly one word of the verb (V) category. (Only the smaller VP and \nV′ are shown in the partial structure (T2).) \n\nWhat is important here is that (T2) is the basis for the \ndouble-object surface structure as well. To produce that, the \npreposition to is erased and an additional NP position (for \nmy pass) is attached to the V′, thus: \n\nThe additional NP is assigned the affected-entity semantic role. The \nother NP (the guard) does not yet have Case; but Larson \nassumes that it moves into the NP position before the verb. The \nresult is shown in (T4), where ‘e’ marks the empty\nstring left where some words have been moved away: \n\nLarson assumes that in this position the guard can receive \nCase. What remains is for the verb to move into a higher V position \nfurther to its left, to obtain the surface order: \n\nThe complete sequence of transformations is taken to give a deep \ntheoretical explanation of many properties of (B.i) and (B.ii), \nincluding such things as what could be substituted for the two NPs, \nand the fact there is at least rough truth-conditional equivalence \nbetween the two clauses. \n\nThe reader with no previous experience of generative linguistics will\nhave many questions about the foregoing sketch (e.g., whether it is \nreally necessary to have the guard after showed in \n(T3), then the opposite order in (T4), and finally the same order \nagain in (T5)). We cannot hope to answer such questions here; \nLarson’s paper is extremely rich in further assumptions, links to the\nprevious literature, and additional classes of data that he aims to \nexplain. But the foregoing should suffice to convey some of the \nflavor of the analysis. \n\nThe key point to note is that Essentialists seek underlying \nsymmetries and parallels whose operation is not manifest in the data \nof language use. For Essentialists, there is positive explanatory \nvirtue in hypothesizing abstract structures that are very far from \nbeing inferrable from performance; and the posited operations on \nthose structures are justified in terms of elegance and formal \nparallelism with other analyses, not through observation of language \nuse in communicative situations. \n\nMany Emergentists are favorably disposed toward the kind of \nconstruction grammar expounded in Goldberg (1995). \nWe will use her work as an exemplar of the Emergentist approach. The \nfirst thing to note is that Goldberg does not take double-object \nclauses like (B.ii) to be derived alternants of recipient-PP \nstructures like (B.i), the way Larson does. So she is not looking for\na regular syntactic operation that can relate their derivations; \nindeed, she does not posit derivations at all. She is interested in \nexplaining correlations between syntactic, semantic, and pragmatic \naspects of clauses; for example, she asks this question: \n\nThus she aims to explain why some verbs occur in both the \ndouble-object and recipient-PP kinds of expression and some do not. \n\nThe fundamental notion in Goldberg’s linguistic theory is that of a \nconstruction. A construction can be defined very \nroughly as a way of structurally composing words or phrases—a \nsort of template—for expressing a certain class of meanings. \nLike Emergentists in general, Goldberg regards linguistic theory as \ncontinuous with a certain part of general cognitive psychological \ntheory; linguistics emerges from this more general theory, and \nlinguistic matters are rarely fully separate from cognitive matters. \nSo a construction for Goldberg has a mental reality: it corresponds \nto a generalized concept or scenario expressible in a language, \nannotated with a guide to the linguistic structure of the expression. \n\nMany words will be trivial examples of constructions: a single concept paired \nwith a way of pronouncing and some details about grammatical \nrestrictions (category, inflectional class, etc.); but constructions \ncan be much more abstract and internally complex. The double-object \nconstruction, which Goldberg calls the Ditransitive Construction, is \na moderately abstract and complex one; she diagrams it thus (p. 50): \n\nThis expresses a set of constraints on how to use English to \ncommunicate the idea of a particular kind of scenario. The scenario \ninvolves a ternary relation CAUSE-RECEIVE holding between an\nagent (agt), a recipient (rec), and\na patient (pat). PRED is a variable that is filled \nby the meaning of a particular verb when it is employed in this \nconstruction. \n\nThe solid vertical lines downward from agt and \npat indicate that for any verb integrated into this \nconstruction it is required that its subject NP should express the \nagent participant, and the direct object (OBJ2) should \nexpress the patient participant. The dashed vertical line downward \nfrom rec signals that the first object (OBJ) may express the \nrecipient but it does not have to—the necessity of there being \na recipient is a property of the construction itself, and not every \nverb demands that it be made explicit who the recipient is. But if \nthere are two objects, the first is obligatorily associated with the \nrecipient role: We sent the builder a carpenter can only \nexpress a claim about the sending of a carpenter over to the builder,\nnever the sending of the builder over to where a carpenter is. \n\nWhen a particular verb is used in this construction, it may have \nobligatory accompanying NPs denoting what Goldberg calls \n“profiled participants” so that the match between the \nparticipant roles (agt, rec, \npat) is one-to-one, as with the verb hand. \nWhen this verb is used, the agent (‘hander’), recipient \n(‘handee’), and item transferred (‘handed’) \nmust all be made explicit. Goldberg gives the following diagram of \nthe “composite structure” that results when hand \nis used in the construction: \n\nBecause of this requirement of explicit presence, Hand him your \npass is grammatical, but *Hand him is not, and \nneither is *Hand your pass. The verb send, \non the other hand, illustrates the optional syntactic expression of \nthe recipient role: we can say Send a text message, which is \nunderstood to involve some recipient but does not make the recipient \nexplicit. \n\nThe R notation relates to the fact that particular verbs may express \neither an instance of causing someone to receive something, \nas with hand, or a means of causing someone to \nreceive something, as with kick: what Joe kicked Bill the \nball means is that Joe caused Bill to receive the ball by means \nof a kicking action. \n\nGoldberg’s discussion covers many subtle ways in which the scenario \ncommunicated affects whether the use of a construction is grammatical\nand appropriate. For example, there is something odd about \n?Joe kicked Bill the ball he was trying to kick to \nSam: the Ditransitive Construction seems best suited to cases of \nvolitional transfer (rather than transfer as an unexpected side \neffect of a blunder). However, an exception is provided by a class of\ncases in which the transfer is not of a physical object but is only \nmetaphorical: That guy gives me the creeps does not imply any \nvolitional transfer of a physical object. \n\nMetaphorical cases are distinguished from physical transfers in other\nways as well. Goldberg notes sentences like The music lent the \nevent a festive air, where the music is subject of the \nverb lend despite the fact that music cannot literally lend anything \nto anyone. \n\nGoldberg discusses many topics such as metaphorical extension, \nshading, metonymy, cutting, role merging, and also presents various \ngeneral principles linking meanings and constructions. One of these \nprinciples, the No Synonymy Principle, says that no two syntactically\ndistinct constructions can be both semantically and pragmatically \nsynonymous. It might seem that if any two sentences are synonymous, \npairs like this are: \n\nYet the two constructions cannot be fully synonymous, both \nsemantically and pragmatically, if the No Synonymy Principle is \ncorrect. And to support the principle, Goldberg notes purported \ncontrasts such as this: \n\nThere is a causation-as-transfer metaphor here, and it seems to be \ncompatible with the double object construction but not with the \nrecipient-PP. So (in Goldberg’s view) the two are not fully \nsynonymous. \n\nIt is no part of our aim here to provide a full account of the \ncontent of Goldberg’s discussion of double-object clauses. But what \nwe want to highlight is that the focus is not on finding abstract \nelements or operations of a purely syntactic nature that are \ncandidates for being essential properties of language per se. The \nfocus for Emergentists is nearly always on the ways in which meaning \nis conveyed, the scenarios that particular constructions are used to \ncommunicate, and the aspects of language that connect up with \npsychological topics like cognition, perception, and \nconceptualization. \n\nOne kind of work that is representative of the Externalist tendency is\nnicely illustrated by Bresnan et al. (2007) and Bresnan and Ford\n(2010). Bresnan and her colleagues defend the use of\ncorpora—bodies of attested written and spoken texts. One of\ntheir findings is that a number of types of expressions that linguists\nhave often taken to be ungrammatical do in fact turn up in actual\nuse. Essentialists and Emergentists alike have often, purely on the\nbasis of intuition, asserted that sentences like John gave Mary a\nkiss are grammatical but sentences like John gave a kiss to\nMary are no, as we see above with Goldberg’s (D)(ii). Bresnan and\nher colleagues find numerous occurrences of the latter sort on the\nWorld Wide Web, and conclude that they are not ungrammatical or even\nunacceptable, but merely dispreferred. \n\nBresnan and colleagues used a three-million-word collection of \nrecorded and transcribed spontaneous telephone conversations known as\nthe Switchboard corpus to study the double-object and recipient-PP \nconstructions. They first annotated the utterances with indications \nof a number of factors that they thought might influence the choice \nbetween the double-object and recipient-PP constructions: \n\nThey also coded the verb meanings by assigning them to half a dozen \nsemantic categories: \n\nThey then constructed a statistical model of the corpus: a \nmathematical formula expressing, for each combination of the factors \nlisted above, the ratio of the probabilities of the double object and\nthe recipient-PP. (To be precise, they used the natural logarithm of \nthe ratio of p to 1 − p, where p is the \nprobability of a double-object or recipient-PP in the corpus being of\nthe double-object form.) They then used logistic regression to \npredict the probability of fit to the data. \n\nTo determine how well the model generalized to unseen data, they \ndivided the data randomly 100 times into a training set and a testing\nset, fit the model parameters on each training set, and scored its \npredictions on the unseen testing set. The average percent of correct\npredictions on unseen data was 92%. All components of the model \nexcept number of the recipient NP made a statistically significant \ndifference—almost all at the 0.001 level. \n\nWhat this means is that knowing only the presence or absence of the \nsort of factors listed above they were reliably able to predict \nwhether double-object or recipient-PP structures would be used in a \ngiven context, with a 92% score accuracy rate. \n\nThe implication is that the two kinds of structure are not \ninterchangeable: they are reliably differentiated by the presence of \nother factors in the texts in which they occur. \n\nThey then took the model they had generated for the telephone speech \ndata and applied it to a corpus of written material: the Wall \nStreet Journal corpus (WSJ), a collection of 1987–9 \nnewspaper copy, only roughly edited. The main relevant difference \nwith written language is that the language producer has more \nopportunity to reflect thoughtfully on how they are going to phrase \nthings. It was reasonable to think that a model based on speech data \nmight not transfer well. But instead the model had 93.5% accuracy. \nThe authors conclude is that “the model for spoken English \ntransfers beautifully to written”. The main difference between \nthe corpora was found to be a slightly higher probability of the \nrecipient-PP structure in written English. \n\nIn a very thorough subsequent study, Bresnan and Ford (2010) show \nthat the results also correlate with native speakers’ metalinguistic \njudgments of naturalness for sentence structures, and with lexical \ndecision latencies (speed of deciding whether the words in a text \nwere genuine English words or not), and with a sentence completion \ntask (choosing the most natural of a list of possible completions of \na partial sentence). The results of these experiments confirmed that \ntheir model predicted participants’ performance. \n\nAmong the things to note about this work is that it was all done on \ndirectly recorded performance data: transcripts of people speaking to\neach other spontaneously on the phone in the case of the Switchboard \ncorpus, stories as written by newspaper journalists in the case of \nWSJ, measured responses of volunteer subjects in a laboratory in the \ncase of the psycholinguistic experiments of Bresnan and Ford (2010). \nThe focus is on identifying the factors in linguistic performance \nthat permit accurate prediction of future performance, and the \nmethods of investigation have a replicability and checkability that \nis familiar in the natural sciences. \n\nHowever, we should make it clear that the work is not some kind of \nclose-to-the-ground collecting and classifying of instances. The \nmodels that Bresnan and her colleagues develop are sophisticated \nmathematical abstractions, very far removed from the records of \nutterance tokens. They claim that these models “allow \nlinguistic theory to solve more difficult problems than it has in the\npast, and to build convergent projects with psychology, computer \nscience, and allied fields of cognitive science” (Bresnan et \nal. 2007: 69). \n\nIt is important to see that the contrast we have drawn here is not \njust between three pieces of work that chose to look at different \naspects of the phenomena associated with double-object sentences. It \nis true that Larson focuses more on details of tree structure, \nGoldberg more on subtle differences in meaning, and Bresnan et al. on\nfrequencies of occurrence. But that is not what we are pointing to. \nWhat we want to stress is that we are illustrating three different \nbroad approaches to language that regard different facts as likely to\nbe relevant, and make different assumptions about what needs to be \naccounted for, and what might count as an explanation. \n\nLarson looks at contrasts between different kinds of clause with \ndifferent meanings and see evidence of abstract operations affecting \nsubtle details of tree structure, and parallelism between \nderivational operations formerly thought distinct. \n\nGoldberg looks at the same facts and sees evidence not for anything \nto do with derivations but for the reality of specific \nconstructions—roughly, packets of syntactic, semantic, and \npragmatic information tied together by constraints. \n\nBresnan and her colleagues see evidence that readily observable facts\nabout speaker behavior and frequency of word sequences correlate \nclosely with certain lexical, syntactic, and semantic properties of \nwords. \n\nNothing precludes defenders of any of the three approaches from \npaying attention to any of the phenomena that the other approaches \nattend to. There is ample opportunity for linguists to mix aspects of\nthe three approaches in particular projects. But in broad outline \nthere are three different tendencies exhibited here, with \nstereotypical views and assumptions roughly as we laid them out in \nTable 1. \n\nThe complex and multi-faceted character of linguistic phenomena means\nthat the discipline of linguistics has a whole complex of \ndistinguishable subject matters associated with different research \nquestions. Among the possible topics for investigation are these: \n\nThere is no reason for all of the discipline of linguistics to \nconverge on a single subject matter, or to think that the entire \nfield of linguistics cannot have a diverse range of subject matters. \nTo give a few examples: \n\nMost saliently of all, Harris’s student Chomsky reacted strongly \nagainst indifference toward the mind, and insisted that the principal\nsubject matter of linguistics was, and had to be, a narrow \npsychological version of (i), and an individual, non-social, and \ninternalized conception of (ii). \n\nIn the course of advancing his view, Chomsky introduced a number of \nnovel pairs of terms into the linguistics literature: competence vs. \nperformance (Chomsky 1965); ‘I-language’ vs. \n‘E-language’ (Chomsky 1986); the faculty of language in \nthe narrow sense vs. the and faculty of language in the broad sense \n(the ‘FLN’ and ‘FLB’ of Hauser et al. 2002). \nBecause Chomsky’s terminological innovations have been adopted so \nwidely in linguistics, the focus of sections 2.1–2.3 will be to\nexamine the use of these expressions as they were introduced into the\nlinguistics literature and consider their relation to (i)-(vii). \n\nEssentialists invariably distinguish between what Chomsky (1965) \ncalled competence and performance. \nCompetence is what knowing a language confers: a tacit grasp of the \nstructural properties of all the sentences of a language. Performance\ninvolves actual real-time use, and may diverge radically from the \nunderlying competence, for at least two reasons: (a) an attempt to \nproduce an utterance may be perturbed by non-linguistic factors like \nbeing distracted or interrupted, changing plans or losing attention, \nbeing drunk or having a brain injury; or (b) certain capacity limits \nof the mechanisms of perception or production may be overstepped. \n\nEmergentists tend to feel that the competence/performance distinction\nsidelines language use too much. Bybee and McClelland put\nit this way: \n\nAnd Externalists are often concerned to describe and explain not only\nlanguage structure, but also the workings of processing mechanisms \nand the etiology of performance errors. \n\nHowever, every linguist accepts that some idealization away from the \nspeech phenomena is necessary. Emergentists and Externalists are \nalmost always happy to idealize away from sporadic speech errors. \nWhat they are not so keen to do is to idealize away from limitations \non linguistic processing and the short-term memory on which it \nrelies. Acceptance of a thoroughgoing competence/performance \ndistinction thus tends to be a hallmark of Essentialist approaches, \nwhich take the nature of language to be entirely independent of other\nhuman cognitive processes (though of course capable of connecting to \nthem). \n\nThe Essentialists’ practice of idealizing away from even \npsycholinguistically relevant factors like limits on memory and \nprocessing plays a significant role in various important debates \nwithin linguistics. Perhaps the most salient and famous is the issue \nof whether English is a finite-state language. \n\nThe claim that English is not accepted by any finite-state automaton \ncan only be supported by showing that every grammar for English has \ncenter- embedding to an unbounded depth (see Levelt 2008: 20–23\nfor an exposition and proof of the relevant theorem, originally from \nChomsky 1959). But even depth-3 center-embedding of clauses (a clause\ninterrupting a clause that itself interrupts a clause) is in practice\nextraordinarily hard to process. Hardly anyone can readily understand\neven semantically plausible sentences like Vehicles that engineers\nwho car companies trust build crash every day. And such sentences\nvirtually never occur, even in writing. Karlsson (2007) undertakes an\nextensive examination of available textual material, and concludes \nthat depth-3 center-embeddings are vanishingly rare, and no genuine \ndepth-4 center-embedding has ever occurred at all in naturally \ncomposed text. He proposes that there is no reason to regard \ncenter-embedding as grammatical beyond depth 3 (and for spoken \nlanguage, depth 2). Karlsson is proposing a grammar that stays close \nto what performance data can confirm; the standard Essentialist view \nis that we should project massively from what is observed, and say \nthat depth-n center-embedding is fully grammatical for all \nn. \n\nChomsky (1986) introduced into the linguistics literature two \ntechnical notions of a language: ‘E-Language’ and \n‘I-Language’. He deprecates the former as either \nundeserving of study or as a fictional entity, and promotes the latter \nas the only scientifically respectable object of study for a serious \nlinguistics. \n\nChomsky’s notion ‘E-language’ is supposed to suggest by \nits initial ‘E’ both ‘extensional’ (concerned\nwith which sentences happen to satisfy a definition of a language \nrather than with what the definition says) and ‘external’\n(external to the mind, that is, non-mental). The dismissal of \nE-language as an object of study is aimed at critics of \nEssentialism—many but not all of those critics falling within \nour categories of Externalists and Emergentists. \n\nExtensional. First, there is an attempt to impugn the\nextensional notion of a language that is found in two radically\ndifferent strands of Externalist work. Some Externalist investigations\nare grounded in the details of attested utterances (as collected in\ncorpora), external to human minds. Others, with mathematical or\ncomputational interests, sometimes idealize languages as extensionally\ndefinable objects (typically infinite sets of strings) with a certain\nstructure, independently of whatever device might be employed to\ncharacterize them. A set of strings of words either is or is not\nregular (finite-state), either is or is not recursive (decidable),\netc., independently of forms of grammar statement. Chomsky (1986)\nbasically dismissed both corpus-based work and mathematical\nlinguistics simply on the grounds that they employ an extensional\nconception of language that is, a conception that removes the object\nof study from having an essential connection with the mental. \n\nExternal. Second, a distinct meaning based on \n‘external’ was folded into the neologism \n‘E-language’ to suggest criticism of any view that \nconceives of a natural language as a public, intersubjectively \naccessible system used by a community of people (often millions of \nthem spread across different countries). Here, the objection is that \nlanguages as thus conceived have no clear criteria of individuation \nin terms of necessary and sufficient conditions. On this conception, \nthe subject matter of interest is a historico-geographical entity \nthat changes as it is transmitted over generations, or over mountain \nranges. Famously, for example, there is a gradual valley-to-valley \nchange in the language spoken between southeastern France and \nnorthwestern Italy such that each valley’s speakers can understand \nthe next. But the far northwesterners clearly speak French and the \nfar southeasterners clearly speak Italian. It is the politically \ndefined geographical border, not the intrinsic properties of the \ndialects, that would encourage viewing this continuum as two \ndifferent languages. \n\nPerhaps the most famous quotation by any linguist is standardly \nattributed to Max Weinreich (1945): ‘A shprakh iz a dialekt mit\nan armey un flot’ (‘A language is a dialect with an army \nand navy’; he actually credits the remark to an unnamed \nstudent). The implication is that E-languages are defined in terms of\nnon-linguistic, non-essential properties. Essentialists object that a\nscientific linguistics cannot tolerate individuating French and \nItalian in a way that is subject to historical contingencies of wars \nand treaties (after all, the borders could have coincided with a \ndifferent hill or valley had some battle had a different outcome). \n\nConsiderations of intelligibility fare no better. Mutual \nintelligibility between languages is not a transitive relation, and \nsometimes the intelligibility relation is not even symmetric \n(smaller, more isolated, or less prestigious groups often understand \nthe dialects of larger, more central, or higher-prestige groups when \nthe converse does not hold). So these sociological facts cannot \nindividuate languages either. \n\nChomsky therefore concludes that languages cannot be defined or\nindividuated extensionally or mind-externally, and hence the only\nscientifically interesting conception of a ‘language’ is\nthe ‘I-language’ view (see for example Chomsky 1986: 25;\n1992; 1995 and elsewhere). Chomsky says of E-languages that “all\nscientific approaches have simply abandoned these elements of what is\ncalled ‘language’ in common usage” (Chomsky 1988,\n37); and “we can define E-language in one way or another or not\nat all, since the concept appears to play no role in the theory of\nlanguage” (Chomsky 1986: 26; in saying that it appears to play\nno role in the theory of language, here he means that it plays no role\nin the theory he favours). \n\nThis conclusion may be bewildering to non-linguists as well as \nnon-Essentialists. It is at odds with what a broad range of \nphilosophers have tacitly assumed or explicitly claimed about \nlanguage or languages: ‘[A language] is a practice in which \npeople engage…it is constituted by rules which it is part of \nsocial custom to follow’ (Dummett 1986: 473–473); \n‘Language is a set of rules existing at the level of common \nknowledge’ and these rules are ‘norms which govern \nintentional social behavior’ (Itkonen 1978: 122), and so on. \nGenerally speaking, those philosophers influenced by Wittgenstein \nalso take the view that a language is a social-historical entity. But\nthe opposite view has become a part of the conceptual underpinning of\nlinguistics for many Essentialists. \n\nFailing to have precise individuation conditions is surely not a \nsufficient reason to deny that an entity can be studied \nscientifically. ‘Language’ as a count noun in the \nextensional and socio-historical sense is vague, but this need not be\nany greater obstacle to theorizing about them than is the vagueness \nof other terms for historical entities without clear individuation \nconditions, like ‘species’ and ‘individual \norganism’ in biology. \n\nAt least some Emergentist linguists, and perhaps some Externalists,\nwould be content to say that languages are collections of social\nconventions, publicly shared, and some philosophers would agree (see\nMillikan 2003, for example, and Chomsky 2003 for a reply). Lewis\n(1969) explicitly defends the view that language can be understood in\nterms of public communications, functioning to solve coordination\nproblems within a group (although he acknowledges that the\ncoordination could be between different temporal stages of one\nindividual, so language use by an isolated person is also\nintelligible; see the appendix “Lewis’s Theory of Languages\nas Conventions” in the entry on\n idiolects,\n for further discussion of Lewis). What Chomsky calls E-languages, \nthen, would be perfectly amenable to linguistic or philosophical \nstudy. \n\nChomsky (1986) introduced the neologism ‘I-language’ in \npart to disambiguate the word ‘grammar’. In earlier \ngenerative Essentialist literature, ‘grammar’ was \n(deliberately) ambiguous between (i) the linguist’s generative theory\nand (ii) what a speaker knows when they know a language. \n‘I-language’ can be regarded as a replacement for Bever’s\nterm ‘psychogrammar’ (see also George 1989): it denotes a\nmental or psychological entity (not a grammarian’s description of a \nlanguage as externally manifested). \n\nI-language is first discussed under the sub-heading of \n‘internalized language’ to denote linguistic knowledge. \nLater discussion in Chomsky 1986 and 1995 makes it clear that the \n‘I’ of ‘I-language’ is supposed to suggest at\nleast three English words: ‘individual’, \n‘internal’, and ‘intensional’. And Chomsky \nemphasizes that the neologism also implies a kind of realism about \nspeakers’ knowledge of language. \n\nIndividual. A language is claimed to be strictly a \nproperty of individual human beings—not groups. The contrast is\nbetween the idiolect of a single individual, and a dialect or \nlanguage of a geographical, social, historical, or political group. \nI-languages are properties of the minds of individuals who know them. \n\nInternal. As generative Essentialists see it, your \nI-language is a state of your mind/brain. Meaning is \ninternal—indeed, on Chomsky’s conception, an I-language \n\nAnd he clarifies the sense in which an I-language is internal by \nappealing to an analogy with the way the study of vision is internal: \n\nThus, while the speaker’s I-language may be involved in performing \noperations over representations of distal \nstimuli—representations of other speaker’s \nutterances—I-languages can and should be studied in isolation \nfrom their external environments. \n\nAlthough Chomsky sometimes refers to this narrow individuation of \nI-languages as ‘individual’, he clearly claims that \nI-languages are individuated in isolation from both speech \ncommunities and other aspects of the broadly conceived natural \nenvironment: \n\nThis passage can also be seen as suggesting a radically \nintensionalist conception of language. \n\nIntensional. The way in which I-languages are \n‘intensional’ for Chomsky needs a little explication. The\nconcept of intension is familiar in logic and semantics, where \n‘intensional’ contrasts with ‘extensional’. \nThe extension of a predicate like blue is simply the set of all blue \nobjects; the intension is the function that picks out in a given \nworld the blue objects contained therein. In a similar way, the \nextension of a set can be distinguished from an intensional \ndescription of the set in terms of a function: the set of integer \nsquares is {1, 4, 9, 16, 25, 36, …}, and the intension\ncould be given in terms of the one-place function f such that \nf(n) = n × n. One difference between\nthe two accounts of squaring is that the intensional one could be\napplied to a different domain (any domain on which the\n‘×’ operation is defined: on the rationals rather\nthan the integers, for example, the extension of the identically\ndefined function is a different and larger set containing infinitely\nmany fractions). \n\nIn an analogous way, a language can be identified with the set of all\nand only its expressions (regardless of what sort of object an \nexpression is: a word sequence, a tree structure, a complete \nderivation, or whatever), which is the extensional view; but it can \nalso be identified intensionally by means of a recipe or formal \nspecification of some kind—what linguists call a grammar. \n\nIn natural language semantics, an intensional context is one where \nsubstitution of co-extensional terms fails to preserve truth value \n(Scott is Scott is true, and Scott is the author of \nWaverley is true, but the truth of George knows that Scott is \nScott doesn’t guarantee the truth of George knows that Scott \nis the author of Waverly, so knows that establishes an \nintensional context). \n\nChomsky claims that the truth of an I-language attribution is not \npreserved by substituting terms that have the same extension. That \nis, even when two human beings do not differ at all on what \nexpressions are grammatical, it may be false to say that they have \nthe same I-language. Where H is a human being and L is a language (in\nthe informal sense) and R is the relation of knowing (or having, or \nusing) that holds between a human being and a language, Chomsky \nholds, in effect, that R establishes an intensional context in \nstatements of the theory: \n\nThe idea is that two individuals can know (or have, or use) different\nI-languages that generate exactly the same strings of words, and even\ngive them exactly the same structures. \n\nThe generative Essentialist conception of an I-language is \nantithetical to Emergentist research programs. If the fundamental \nexplanandum of scientific linguistics is how actual linguistic \ncommunication takes place, one must start by looking at both internal\n(psychological) and external (public) practices and conventions in \nvirtue of which it occurs, and consider the effect of historical and \ngeographic contingencies on the relevant underlying processes. That \nwould not rule out ‘I-language’ as part of the explanans;\nbut some Emergentists seem to be fictionalists about \nI-languages, in an analogous sense to the way that Chomsky is a \nfictionalist about E-languages. Emergentists do not see a child as \nlearning a generative grammar, but as learning how to use a symbolic \nsystem for propositional communication. On this view grammars are \nmere artifacts that are developed by linguists to codify aspects of \nthe relevant systems, and positing an I-language amounts to \nprojecting the linguist’s codification illegitimately onto human \nminds (see, for example, Tomasello 2003). \n\nThe I-language concept brushes aside certain phenomena of interest to\nthe Externalists, who hold that the forms of actually attested \nexpressions (sentences, phrases, syllables, and systems of such \nunits) are of interest for linguistics. For example, computational \nlinguistics (work on speech recognition, machine translation, and \nnatural language interfaces to databases) must rely on a conception \nof language as public and extensional; so must any work on the \nutterances of young children, or the effects of word frequency on \nvowel reduction, or misunderstandings caused by road sign wordings. \nAt the very least, it might be said on behalf of this strain of \nExternalism (along the lines of Soames 1984) that linguistics will \nneed careful work on languages as intersubjectively accessible \nsystems before hypotheses about the I-language that purportedly \nproduces them can be investigated. \n\nIt is a highly biased claim that the E-language concept \n“appears to play no role in the theory of language” \n(Chomsky 1986: 26). Indeed, the terminological contrast seems to have\nbeen invented not to clarify a distinction between concepts but to \nnudge linguistic research in a particular direction. \n\nIn Hauser et al. (2002) (henceforth HCF) a further pair of \ncontrasting terms is introduced. They draw a distinction quite \nseparate from the competence/performance and \n‘I-language’/‘E-language’ distinctions: the \n“language faculty in the narrow sense” (FLN) is \ndistinguished from the “language faculty in the broad \nsense” (FLB). According to HCF, FLB “excludes other \norganism-internal systems that are necessary but not sufficient for \nlanguage (e.g., memory, respiration, digestion, circulation, \netc.)” but includes whatever is involved in language, and FLN \nis some limited part of FLB (p. 1571) This is all fairly vague, but \nit is clear that FLN and FLB are both internal rather than external, \nand individual rather than social. \n\nThe FLN/FLB distinction apparently aims to address the uniqueness of \none component of the human capacity for language rather than (say) \nthe content of human grammars. HCF say (p. 1573) that “Only FLN\nis uniquely human”; they “hypothesize that most, if not \nall, of FLB is based on mechanisms shared with nonhuman \nanimals”; and they say: \n\nThe components of FLB that HCF hypothesize are not part of FLN are \nthe “sensory-motor” and \n“conceptual-intentional” systems. The study of the \nconceptual-intentional system includes investigations of things like \nthe theory of mind; referential vocal signals; whether imitation is \ngoal directed; and the field of pragmatics. The study of the sensory \nmotor system, by contrast, includes “vocal tract length and \nformant dispersion in birds and primates”; learning of songs by\nsongbirds; analyses of vocal dialects in whales and spontaneous \nimitation of artificially created sounds in dolphins; “primate \nvocal production, including the role of mandibular \noscillations”; and “[c]ross-modal perception and sign \nlanguage in humans versus unimodal communication in animals”. \n\nIt is presented as an empirical hypothesis that a core property of \nthe FLN is “recursion”: \n\nHCF leave open exactly what the FLN includes in addition to \nrecursion. It is not ruled out that the FLN incorporates substantive \nuniversals as well as the formal property of “recursion”.\nBut whatever “recursion” is in this context, it is \napparently not domain-specific in the sense of earlier discussions by\ngenerative Essentialists, because it is not unique to human natural \nlanguage or defined over specifically linguistic inputs and outputs: \nit is the basis for humans’ grasp of the formal and arguably \nnon-natural language of arithmetic (counting, and the successor \nfunction), and perhaps also navigation and social relations. It might\nbe more appropriate to say that HCF identify recursion as a cognitive\nuniversal, not a linguistic one. And in that case it is difficult to \nsee how the so-called ‘language faculty’ deserves that \nname: it is more like a faculty for cognition and communication. \n\nThis abandonment of linguistic domain-specificity contrasts very \nsharply with the picture that was such a prominent characteristic of \nthe earlier work on linguistic nativism, popularized in different \nways by Fodor (1983), Barkow et al. (1992), and Pinker (1994). And \nyet the HCF discussion of FLN seems to incline to the view that human\nlanguage capacities have a unique human (though not uniquely \nlinguistic) essence. \n\nThe FLN/FLB distinction provides earlier generative Essentialism with\nan answer (at least in part) to the question of what the singularity \nof the human language faculty consists in, and it does so in a way \nthat subsumes many of the empirical discoveries of paleoanthropology,\nprimatology, and ethnography that have been part of highly \ninfluential in Emergentist approaches as well as neo-Darwinian \nEssentialist approaches. A neo-Darwinian Essentialist like Pinker \nwill accept that the language faculty involves recursion, but also \nwill also hold (with Emergentists) that human language capacities \noriginated, via natural selection, for the purpose of linguistic \ncommunication. \n\nThus, over the years, those Essentialists who follow Chomsky closely\nhave changed the term they use for their core subject matter from\n‘linguistic competence’ to ‘I-language’ to\n‘FLN’, and the concepts expressed by these terms are all\nslightly different.  In particular, what they are counterposed to\ndiffers in each case. \n\nThe challenge for the generative Essentialist adopting the FLN/FLB \ndistinction as characterized by HCF is to identify empirical data \nthat can support the hypothesis that the FLN “yields discrete \ninfinity”. That will mean answering the question: discrete \ninfinity of what? HCF write that FLN “takes a finite set of \nelements and yields a potentially infinite array of discrete \nexpressions” (p. 1571), which makes it clear that there must be\na recursive procedure in the mathematical sense, perhaps putting \natomic elements such as words together to make internally complex \nelements like sentences (“array” should probably be \nunderstood as a misnomer for ‘set’). But then they say, \nsomewhat mystifyingly: \n\nBut the sensory-motor and conceptual-intentional systems are concrete\nparts of the organism: muscles and nerves and articulatory organs and\nperceptual channels and neuronal activity. How can each one of a \n“potentially infinite array” be “passed to” \nsuch concrete systems without it taking a potentially infinite amount\nof time? HCF may mean that for any one of the expressions that FLN \ndefines as well-formed (by generating it) there is a possibility of \nits being used as the basis for a pairing of sound and meaning. This \nwould be closer to the classical generative Essentialist view that \nthe grammar generates an infinite set of structural descriptions; but\nit is not what HCF say. \n\nAt root, HCF is a polemical work intended to identify the view it \npromotes as valuable and all other approaches to linguistics as \notiose. \n\nIt is hard to see this as anything other than a claim that approaches\nto linguistics focusing on anything that could fall under the label \n‘E-language’ are to be dismissed as useless. \n\nSome Externalists and Emergentists actually reject the idea that the \nhuman capacity for language yields “a potentially infinite \narray of expressions”. It is often pointed out by empirically \ninclined computational linguists that in practice there will only \never be a finite number of sentences to be dealt with (though the \npeople saying this may underestimate the sheer vastness of the finite\nset involved). And naturally, for those who do not believe there are \ngenerative grammars in speakers’ heads at all, it holds a fortiori \nthat speakers do not have grammars in their heads generating infinite\nlanguages. Externalists and Emergentists tend to hold that the \n“discrete infinity” that HCF posits is more plausibly a \nproperty of the generative Essentialists’ model of linguistic \ncompetence, I-language, or FLN, than a part of the human mind/brain. \nThis does not mean that non-Essentialists deny that actual language \nuse is creative, or (of course) that they think there is a longest \nsentence of English. But they may reject the link between linguistic \nproductivity or creativity and the mathematical notion of recursion \n(see Pullum and Scholz 2010). \n\nHCF’s remarks about how FLN “yields” or \n“generates” a specific “array” assume that \nlanguages are clearly and sharply individuated by their generators. \nThey appear to be committed to the view that there is a fact of the \nmatter about exactly which generator is in a given speaker’s head. \nEmergentists tend not to individuate languages in this way, and may \nreject generative grammars entirely as inappropriately or \nunacceptably ‘formalist’. They are content with the \nnotion that the common-sense concept of a language is vague, and it \nis not the job of linguistic theory to explain what a language is, \nany more than it is the job of physicists to explain what material \nis, or of biologists to explain what life is. Emergentists, in \nparticular, are interested not so much in identifying generators, or \nindividuating languages, but in exploring the component capacities \nthat facilitate linguistic communication, and finding out how they \ninteract. \n\nSimilarly, Externalists are interested in the linguistic structure of\nexpressions, but have little use for the idea of a discrete infinity \nof them, a view that is not, and cannot be empirically supported, unless one\nthinks of simplicity and elegance of theory as empirical matters. \nThey focus on the outward manifestations of language, not on a set of\nexpressions regarded as a whole language—at least not in any \nway that would give a language a definite cardinality. Zellig Harris,\nan archetypal Externalist, is explicit that the reason for not \nregarding the set of utterances as finite concerns the elegance of \nthe resulting grammar: “If we were to insist on a finite \nlanguage, we would have to include in our grammar several highly \narbitrary and numerical conditions” (Harris 1957: 208). \nInfinitude, on his view is an unimportant side consequence of setting\nup a sentence-generating grammar in an uncluttered and maximally \nelegant way, not a discovered property of languages (see Pullum and \nScholz 2010 for further discussion). \n\nNot all Essentialists agree that linguistics studies aspects of what \nis in the mind or aspects of what is human. There are some who do not\nsee language as either mental or human, and certainly do not regard \nlinguists as working on a problem within cognitive psychology or \nneurophysiology. Montague (1974), for example, is deeply concerned \nwith using powerful higher-order quantified modal logics and possible\nworlds to formalize aspects of natural language semantics, but \neschews psychologism. His leanings are toward Frege, and his ontology\ninclines toward platonism rather than psychologism. \n\nKatz (1981) is an explicit defense of the Fregean view that natural \nlanguages are timeless, locationless, and necessarily existent. The \nprimary essential property that Katz finds in natural languages is \neffability, the property of providing semantic expression \nfor absolutely every Fregean proposition. On the platonist view the \nfact that non-spatiotemporally located languages are grasped and used\nby human beings raises major epistemological issues (see the section \ntitled ‘The Epistemological Argument Against Platonism’\n in the entry on Platonism in metaphysics).\n Katz (1998) attempts to address these issues. \n\nKatz’s own tripartite classification of linguistic theories, derived \nfrom medieval solutions to the problem of universals (and used as the\nstructure of his book of readings, Katz 1985), is orthogonal to our \nclassification. Katz sees three ontological conceptions of the \nsubject matter of linguistics: \n\nKatz took nominalism to have been refuted by Chomsky in his critiques\nof American structuralists in the 1960s. But, in Katz’s opinion, \nChomsky had failed to notice that conceptualism was infected with \nmany of the same faults as nominalism, because it too localized \nlanguage spatiotemporally (in contingently existing, finite, human \nbrains). Through an argument by elimination, Katz concluded that only\nplatonism remained, and must be the correct view to adopt. \n\nKatz’s argument by elimination should probably be taken as another \nexample of an effort not to separate and clarify concepts used in \ndifferent kinds of linguistic theorizing, but rather to dismiss and \nexclude certain types of research from the theory of language (see \nPullum and Scholz 1997 for detailed discussion). But regardless of \nthat, his typology of linguists should certainly not be thought to \nrelate directly to the distinctions between centers of interest in \nlinguistic theorizing around which this article is structured. No \nparticular metaphysical view unifies any of our three groupings. For \nexample, not all Externalists incline toward nominalism; numerous \nEmergentists as well as most Essentialists take linguistics to be \nabout mental phenomena; and our Essentialists include Katz’s \nplatonists alongside the Chomskyan ‘I-language’ \nadvocates. \n\nLinguists’ conception of the components of the study of language \ncontrast with philosophers’ conceptions (even those of philosophers \nof language) in at least three ways. First, linguists are often \nintensely interested in small details of linguistic form in their own\nright. Second, linguists take an interest in whole topic areas like \nthe internal structure of phrases, the physics of pronunciation, \nmorphological features such as conjugation classes, lexical \ninformation about particular words, and so on—topics in which \nthere is typically little philosophical payoff. And third, linguists \nare concerned with relations between the different subsystems of \nlanguages: the exact way the syntax meshes with the semantics, the \nrelationship between phonological and syntactic facts, and so on. \n\nWith regard to form, philosophers broadly follow Morris (1938), a \nfoundational work in semiotics, and to some extent Peirce (see SEP \nentry: Peirce, semiotics), in thinking of the theory of language as \nhaving three main components: \n\nLinguists, by contrast, following both Sapir (1921) and Bloomfield \n(1933), treat the syntactic component in a more detailed way than \nMorris or Peirce, and distinguish between at least three kinds of \nlinguistic form: the form of speech sounds (phonology), the form of \nwords (morphology), and the form of sentences. (If syntax is about \nthe form of expressions in general, then each of these would be an \nelement of Morris’s syntax.) \n\nEmergentists in general deny that there is a distinction between \nsemantics and pragmatics—a position that is familiar enough in \nphilosophy: Quine (1987: 211), for instance, holds that “the \nseparation between semantics and pragmatics is a pernicious \nerror.” And generally speaking, those theorists who, like the \nlater Wittgenstein, focus on meaning as use will deny that one can \nseparate semantics from pragmatics. Emergentists such as Paul Hopper \n& Sandra Thompson agree: \n\nSome Essentialists—notably Chomsky—also deny that \nsemantics can be separated from pragmatics, but unlike the \nEmergentists (who think that semantics-pragmatics is a starting point\nfor linguistic theory), Chomsky (as we noted briefly in section 1.3) \ndenies that semantics and pragmatics can have any role in \nlinguistics: It seems that other cognitive systems—in particular, our system\nof beliefs concerning things in the world and their\nbehavior—play an essential part in our judgments of meaning and\nreference, in an extremely intricate manner, and it is not at all\nclear that much will remain if we try to separate the purely\nlinguistic components of what in informal usage or even in technical\ndiscussion we call ‘the meaning of [a] linguistic expression.’\n(Chomsky 1979; 142)\n \nRegarding the theoretical account of the relation between words or\nphrases and what speakers take them to refer to, Chomsky says,\n“I think such theories should be regarded as a variety of\nsyntax” (Chomsky 1992: 223).\n \n\nNot every Essentialist agrees with Chomsky on this point. Many \nbelieve that every theory should incorporate a linguistic component \nthat yields meanings, in much the same way that many philosophers of \nlanguage believe there to be such a separate component. Often, \nalthough not always, this component amounts to a truth-theoretic \naccount of the values of syntactically-characterized sentences. This \ntypically involves a translation of the natural language sentence \ninto some representation that is “intermediate” between \nnatural language and a truth-theory—perhaps an augmented \nversion of first-order logic, or perhaps a higher-order intensional \nlanguage. The Essentialists who study semantics in such ways usually \nagree with Chomsky in seeing little role for pragmatics within \nlinguistic theory. But their separation of semantics from pragmatics \nallows them to accord semantics a legitimacy within linguistics \nitself, and not just in psychology or sociology. \n\nSuch Essentialists, as well as the Emergentists, differ in important \nways from classical philosophical logic in their attitudes towards \n“the syntactic-semantic interface”, however. Philosophers\nof language and logic who are not also heavily influenced by \nlinguistics tend to move directly—perhaps by means of a \n“semantic intuition” or perhaps from an intuitive \nunderstanding of the truth conditions involved—from a natural \nlanguage sentence to its “deep, logical” representation. \nFor example, they may move directly from (EX1) to (LF1): \n\nAnd from there perhaps to a model-theoretic description of its \ntruth-conditions. A linguist, on the other hand, would aim to \ndescribe how (EX1) and (LF1) are related. From the point of view of a\nsemantically-inclined Essentialist, the question is: how should the \nsyntactic component of linguistic theory be written so that the \nsemantic value (or, “logical form representation”) can be\nassigned? From some Emergentist points of view, the question is: how \ncan the semantic properties and communicative function of an \nexpression explain its syntactic properties? \n\nMatters are perhaps less clear with the Externalists—at least \nwith those who identify semantic value with distribution in terms of \nneighboring words (there is a tradition stemming from the \nstructuralists of equating synonymy with the possibility of \nsubstitution in all contexts without affecting acceptability). \n\nMatters are in general quite a bit more subtle and tricky than (EX1) \nmight suggest. Philosophers have taken the natural language sentence \n(EX2) to have two logical forms, (LF2a) and (LF2b): \n\nBut for the linguist interested in the syntax-semantics interface, \nthere needs to be some explanation of how (LF2a) and (LF2b) are \nassociated with (EX2). It could be a way in which rules can derive \n(LF2a) and (LF2b) from the syntactic representation of (EX2), as some\nsemantically-inclined Essentialists would propose, or a way to \nexplain the syntactic properties of (EX2) from facts about the \nmeanings represented by (LF2a) and (LF2b), as some Emergentists might\nwant. But that they should be connected up in some way is something \nthat linguists would typically count as non-negotiable. \n\nThe strengths and limitations of different data gathering methods \nbegan to play an important role in linguistics in the early to \nmid-20th century. Voegelin and Harris (1951: 323) discuss several \nmethods that had been used to distinguish Amerindian languages and \ndialects: \n\nThey note that the anthropological linguists Boas and Sapir (who we \ntake to be proto-Emergentists) used the ‘ask the \ninformant’ method of informal elicitation, addressing questions\n“to the informant’s perception rather than to the data \ndirectly” (1951: 324). Bloomfield (the proto-Externalist), on \nthe other hand, worked on Amerindian languages mostly by collecting \ncorpora, with occasional use of monolingual elicitation. \n\nThe preferred method of Essentialists today is informal elicitation, \nincluding elicitation from oneself. Although the techniques for \ngathering data about speakers and their language use have changed \ndramatically over the past 60 or more years, the general strategies \nhave not: data is still gathered by elicitation of metalinguistic \njudgments, collection of corpus material, or direct psychological \ntesting of speakers’ reactions and behaviors. Different linguists \nwill have different preferences among these techniques, but it is \nimportant to understand that data could be gathered in any of the \nthree ways by advocates of any tendency. Essentialists, Emergentists,\nand Externalists differ as much on how data is interpreted and used \nas on their views of how it should be gathered. \n\nA wide range of methodological issues about data collection have been\nraised in linguistics. Since gathering data by direct objective \nexperimental testing of informants is a familiar practice throughout \nthe social, psychological, medical, and biological sciences, we will \nsay little about it here, focusing instead on these five issues about\ndata: \n\nThe debate in linguistics over the use of linguistic intuitions \n(elicited metalinguistic judgments) as data, and how that data should\nbe collected has resulted in enduring, rancorous, often ideologically\ntinged disputes over the past 45 years. The disputes are remarkable, \nif only for their fairly consistent venomous tone. \n\nAt their most extreme, many Emergentists and some Externalists cast \nthe debate in terms of whether linguistic intuitions should ever \ncount as evidence for linguistic theorizing. And many Essentialists \ncast it in terms of whether anything but linguistic intuitions are \never really needed to support linguistic theorizing. \n\nThe debate focuses on the Essentialists’ notion of a mental grammar, \nsince linguistic intuitions are generally understood to be a \nconsequence of tacit knowledge of language. Emergentists who deny \nthat speakers have innate domain-specific grammars (competence, \nI-languages, or FLN) have raised a diverse range of objections to the\nuse of reports of intuitions as linguistic data. (But see Devitt 2006\nfor an understanding of linguistic intuitions that does not base them\non inferred tacit knowledge of competence grammars.) The following \npassages are representative Emergentist critiques of intuitions \n(elicited judgments): Generative linguists typically respond to calls for evidence for\nthe reality of their theoretical constructs by claiming that no\nevidence is needed over and above the theory’s ability to account for\npatterns of grammaticality judgments elicited from native\nspeakers. This response is unsatisfactory on two accounts. First, such\njudgments are inherently unreliable because of their unavoidable\nmeta-cognitive overtones… Second, the outcome of a judgment (or\nthe analysis of an elicited utterance) is invariably brought to bear\non some distinction between variants of the current generative theory,\nnever on its foundational assumptions. (Edelman and Christiansen 2003:\n60) \nThe data that are actually used toward this end in Generative Grammar\nanalyses are almost always disembodied sentences that analysts have\nmade up ad hoc, … rather than utterances produced by real people\nin real discourse situations… In diametric opposition to these\nmethodological assumptions and choices, cognitive-functional linguists\ntake as their object of study all aspects of natural language\nunderstanding and use… They (especially the more functionally\noriented analysts) take as an important part of their data not\ndisembodied sentences derived from introspection, but rather\nutterances or other longer sequences from naturally occurring\ndiscourse. (Tomasello 1998: xiii) \n[T]he journals are full of papers containing highly questionable data,\nas readers can verify simply by perusing the examples in nearly any\nsyntax article about a familiar language. (Wasow and Arnold 2005:\n1484) \n\nIt is a common Emergentist objection that linguistic intuitions \n(taken to be reports of elicited judgments of the acceptability of \nexpressions not their grammaticality) are bad data points because not\nonly are they not usage data, i.e., they are metalinguistic, but also\nbecause they are judgments about linguist’s invented example \nsentences. On neither count would they be clear and direct evidence \nof language use and human communicative capacities—the subject \nmatter of linguistics on the Emergentist view. A further objection is\nto their use by theorists to the exclusion of all other kinds of \nevidence. For example, \n\n“Formal” is Emergentist shorthand for referring to \ngenerative linguistics. And it should be noted that the practice by \nEssentialists of collapsing various kinds of acceptability judgments \nunder the single label ‘intuitions’ masks important \ndifferences. In principle there might be significant differences \nbetween the judgments of (i) linguists with a stake in what the \nevidence shows; (ii) linguists with experience in syntactic theory \nbut no stake in the issue at hand; (iii) non-linguist native speakers\nwho have been tutored in how to provide the kinds of judgments the\nlinguist is interested in; and (iv) linguistically naïve native \nspeakers. \n\nMany Emergentists object to all four kinds of reports of intuitions \non the grounds that they are not direct evidence language use. For \nexample, a common objection is based on the view that \n\nBut collections of linguists’ reports of their own judgments are also\ncriticized by Emergentists as “arm-chair data \ncollection,” or “data collection by introspection”.\nAll parties tend to call this kind of data collection \n“informal”—though they all rely on either formally \nor informally elicited judgments to some degree. \n\nOn the other side, Essentialists tend to deny that usage data is \nadequate evidence by itself: \n\nAnd Essentialists often seem to deny that they are guilty of what the\nEmergentist claims they are guilty of. For example, Chomsky appears \nto be claiming that acceptability judgments are performance data, \ni.e. evidence of use: \n\nChomsky means to deny that acceptability judgments are direct \nevidence of linguistic competence. But it does not follow \nthat elicited acceptability judgments are direct evidence of language\nuse. \n\nAnd as for the charge of “arm-chair” collection methods, \nsome Essentialists claim to have shown that such methods are as good \nas more controlled experimental methods. For example, Sprouse and \nAlmeida report: \n\n(When they say “formal results” they apparently mean \n“results obtained by controlled experiments”.) This can \nbe read as either defending Essentialists’ consulting of their own \nintuitions simpliciter, or their self-consultation of intuitions on \nuncontroversial textbook cases only. The former is much more \ncontroversial than the later. \n\nFinally, both parties of the debate engage in ad hominem attacks on \ntheir opponents. Here is one example of a classic ad hominem (tu \nquoque) attack on Emergentists in defense of constructed examples by \nEssentialists: \n\nClearly, the mere fact that some Emergentists may in practice have \nmade use of invented examples in testing their theories does not tell\nagainst any cogent general objections they may have offered to such \npractice. What is needed is a decision on the methodological point, \nnot just a cry of “You did it too!”. \n\nGiven the intolerance of each other’s views, and the crosstalk \npresent in these debates, it is tempting to think that Emergentism \nand Essentialism are fundamentally incompatible on what counts as \nlinguistic data, since their differences are based on their different\nviews of the subject matter of linguistics, and what the phenomena \nand goals of linguistic theorizing are. There is no doubt that the \nopposing sides think that their respective views are incompatible. \nBut this conclusion may well be too hasty. In what follows, we try to\npoint to a way that the dispute could be ameliorated, if not \nadjudicated. \n\nEssentialists who accept the competence/performance distinction of \nChomsky (1965) traditionally emphasize elicited acceptability \njudgment data (although they need not reject data that is gathered \nusing other methods). But as Cowart notes: \n\nThe grammaticality of an expression, on the standard generative \nEssentialist view, is the status conferred on it by the competence \nstate of an ideal speaker. But competence can never be exercised or \nused without potentially interfering performance factors like memory \nbeing exercised as well. This means that judgments about \ngrammaticality are never really directly available to the linguist \nthrough informant judgments: they have to be inferred from judgments \nof acceptability (along with any other relevant evidence). \nNevertheless, Essentialists do take acceptability judgments to \nprovide fairly good evidence concerning the character of linguistic \ncompetence. In fact the use of informally gathered acceptability \njudgment data is a hallmark of post-1965 Essentialist practice. \n\nIt would be a mistake, however, to suppose that only Essentialists \nmake use of such judgments. Many contemporary Externalists and \nEmergentists who reject the competence/performance distinction still \nuse informally gathered acceptability judgments in linguistic \ntheorizing, though perhaps not in theory testing. Emergentists tend \nto interpret experimentally gathered judgment data as performance \ndata reflecting the interactions between learned features of \ncommunication systems and general learning mechanisms as deployed in \ncommunication. And Externalists use judgment data for corpus cleaning\n(see below). \n\nIt should be noted that sociolinguists and anthropological linguists \n(and we regard them as tending toward Emergentist views) often \ninformally elicit informant judgments not only about acceptability \nbut also about social and regional style and variation, and meaning. \nThey may ask informants questions like, “Who would typically \nsay that?”, or “What does X mean in context XYZ?”, \nor “If you can say WXY, can you say WXZ?” (see Labov \n1996: 77). \n\nA generative grammar gives a finite specification of a set of \nexpressions. A psychogrammar, to the extent that it corresponds to a \ngenerative grammar, might be thought to equip a speaker to know (at \nleast in principle) absolutely whether a string is in the language. \nHowever, elicited metalinguistic judgments are uncontroversially a \nmatter of degree. A question arises concerning the scale on which \nthese degrees of acceptability should be measured. \n\nLinguists have implicitly worked with a scale of roughly half a dozen\nlevels and types of acceptability, annotating them with prefixed \nsymbols. The most familiar is the asterisk, originally used simply to\nmark strings of words as ungrammatical, i.e., as not belonging to the\nlanguage at all. Other prefixed marks have gradually become current: \n\nBut other annotations have been used to indicate a gradation in the \nextent to which some sentences are unacceptable. No scientifically \nvalidated or explicitly agreed meanings have been associated with \nthese marks, but a tradition has slowly grown up of assigning \nprefixes such as those in Table 2 to signify degrees of \nunacceptability: Table 2: Prefixes used to mark levels of acceptability \n\nSuch markings are often used in a way that suggests an \nordinal scale, i.e. a partial ordering that is \nsilent on anything other than equivalence in acceptability or ranking\nin degree of unacceptability. \n\nBy contrast, Bard et al. (1996: 39) point out, it is possible to use \ninterval scales, which additionally measure distance\nbetween ordinal positions. Interval scales of acceptability would \nmeasure relative distances between strings—how much \nmore or less acceptable one is than another. Magnitude \nestimation is a method developed in psychophysics to measure\nsubjects’ judgments of physical stimuli on an interval scale. Bard et\nal. (1996) adapted these methods to linguistic acceptability \njudgments, arguing that interval scales of measurement are required \nfor testing theoretical claims that rely on subtle judgments of \ncomparative acceptability. An ordinal scale of acceptability can \nrepresent one expression as being less acceptable than another, but \ncannot support quantitative questions about how much less. Many \ngenerative Essentialist theorists had been suggesting that violation \nof different universal principles led to different degrees of \nunacceptability. According to Bard et al. (34–35), because \nthere may be “disproportion between the fineness of judgments \npeople can make and the symbol set available for recording \nthem” it will not suffice to use some fixed scale such as this \none: \n  ? < ?? < ?* < * < **\n \n\nindicating absolute degrees of unacceptability. Degrees of relative \nunacceptability must be measured. This is done by asking the \ninformant how much less acceptable one string is than another. \n\nMagnitude estimation can be used with both informal and experimental \nmethods of data collection. And data that is measured using interval \nscales can be subjected to much more mathematically sophisticated \ntests and analyses than data measured solely by an ordinal scale, \nprovided that quantitative data are available. \n\nIt should be noted that the value of applying magnitude estimation to\nthe judgment of acceptability has been directly challenged in two \nrecent papers. Weskott and Fanselow (2011) and Sprouse (2011) both \npresent critiques of Bard et al. (1996). Weskott and Fanselow \ncompared magnitude estimation data to standard judgments on binary \nand 7-point scales, and claim that magnitude estimation does not \nyield more information than other judgment tasks, and moreover can \nproduce spurious variance. And Sprouse, on the basis of recent \nformalizations of magnitude estimation in the psychophysics \nliterature, presents experimental evidence that participants cannot \nmake ratio judgments of acceptability (for example, a judgment that \none sentence is precisely half as acceptable as another), which \nsuggests that the magnitude estimation task probably provides the \nsame interval-level data as other judgment tasks. \n\nPart of the dispute over the reliability of informal methods of \nacceptability judgment elicitation and collection is between \ndifferent groups of Essentialists. Experimentally trained \npsycholinguists advocate using and adapting various experimental \nmethods that have been developed in the cognitive and behavioral \nsciences to collect acceptability judgments. And while the debate is \noften cast in terms of which method is absolutely better, a more \nappropriate question might be when one method is to be preferred to \nthe others. Those inclined toward less experimentally controlled \nmethods point out that there are many clear and uncontroversial \nacceptability judgments that do not need to be shown to be reliable. \nAdvocates of experimental methods point out that many purportedly \nclear, uncontroversial judgments have turned out to be unreliable, \nand led to false empirical generalizations about languages. Both seem\nto be right in different cases. \n\nChomsky has frequently stated his view that the experimental data-gathering \ntechniques developed in the behavioral sciences are neither used nor \nneeded in linguistic theorizing.  For example: \n\nHe also expressed the opinion that using experimental behavioral data\ncollection methods in linguistics “would be a waste of time and\nenergy” (1969: 81). \n\nAlthough many Emergentists—the intellectual heirs of \nSapir—would accept ‘ask-the-informant’ data, we \nmight expect them to tend to accept experimental data-gathering \nmethods that have been developed in the social sciences. There is \nlittle doubt that strict followers of the methodology preferred by \nBloomfield in his later career would disapprove of ‘ask the \ninformant’ methods. Charles Hockett remarked: \n\nWe might expect Bloomfield, having abandoned his earlier Wundtian \npsychological leanings, to be suspicious of any method that could be \ncast as introspective. And we might expect many contemporary \nExternalists to prefer more experimentally controlled methods too. \n(We shall see below that to some extent they do.) \n\nDerwing (1973) was one early critic of Chomsky’s view (1969) that \nexperimentally controlled data collection is useless; but it was \nnearly 25 years before systematic research into possible confounding \nvariables in acceptability judgment data started being conducted on \nany significant scale. In the same year that Bard et al. (1996) \nappeared, Carson Schütze (1996) published a monograph with the \nfollowing goal statement: \n\nIn a similar vein, Wayne Cowart stated that he wanted to \n“describe a family of practical methods that yield demonstrably\nreliable data on patterns of sentence acceptability.” He \nobserves that the stability and reliability of acceptability judgment\ncollection is \n\nSchütze also expresses the importance of using experimental \nmethods developed in cognitive science: \n\nThe above can be read as sympathetic to the Essentialist preference \nfor elicited judgments. \n\nAmong the findings of Schütze and Cowart about informal judgment\ncollection methods are these: \n\nAlthough Schütze (1996) and Cowart (1997) are both critical of \ntraditional Essentialist informal elicitation methods, their primary \nconcern is to show how the claims of Essentialist linguistics can be \nmade less vulnerable to legitimate complaints about informal data \ncollection methods. Broadly speaking, they are friends of \nEssentialism. Critics of Essentialism have raised similar concerns in\nless friendly terms, but it is important to note that the debate over\nthe reliability of informal methods is a debate within Essentialist \nlinguistics as well. \n\nInformal methods of acceptability judgment data have often been \ndescribed as excessively casual. Ferreira described the informal \nmethod this way: \n\n(It would be appropriate to read ‘grammatical’ and \n‘grammaticality’ in Ferreira’s text as meaning \n‘acceptable’ and ‘acceptability’.) \n\nThis critical characterization exemplifies the kind of method that \nSchütze and Cowart aimed to improve on. More recently, Gibson \nand Fedorenko describe the traditional informal method this way: \n\nWhile some Essentialists have acknowledged these problems with the \nreliability of informal methods, others have, in effect, denied their\nrelevance. For example, Colin Phillips (2010) argues that \n“there is little evidence for the frequent claim that sloppy \ndata-collection practices have harmed the development of linguistic \ntheories”. He admits that not all is epistemologically well in \nsyntactic theory, but adds, “I just don’t think that the \nproblems will be solved by a few rating surveys.” He concludes: \n\nTo suggest that informal methods are as fully reliable as controlled \nexperimental ones would be a serious charge, implying that \nresearchers like Bard, Robinson, Sorace, Cowart, Schütze, \nGibson, Fedorenko, and others have been wasting their time. But \nPhillips actually seems to be making a different claim. He suggests \nfirst that informally gathered data has not actually harmed \nlinguistics, and second that linguists are in danger of being \n“fooled” by critics who invent stories about unreliable \ndata having harmed linguistics. \n\nThe harm that Phillips claims has not occurred relates to the charge \nthat “mainstream linguistics” (he means the current \ngenerative Essentialist framework called ‘Minimalism’) is\n“irrelevant” to broader interests in the cognitive \nsciences, and has lost “the initiative in language \nstudy”. Of course, Phillips is right in a sense: one cannot \ninsure that experimental judgment collection methods will address \nevery way in which Minimalist theorizing is irrelevant to particular \nendeavors (language description, language teaching, natural language \nprocessing, or broader questions in cognitive psychological \nresearch). But this claim does not bear on what Schütze (1996) \nand Cowart (1997) show about the unreliability of informal methods. \n\nPhillips does not fully accept the view of Chomsky (1969) that \nexperimental methods are useless for data gathering (he says, \n“I do not mean to argue that comprehensive data gathering \nstudies of acceptability are worthless”). But his defense of \ninformal methods of data collection rests on whether these methods \nhave damaged Essentialist theory testing: \n\nThe critiques I have read present no evidence of the supposed damage \nthat informal intuitions have caused, and among those who do provide \nspecific examples it is rare to provide clear evidence of the \nsupposed damage that informal intuitions have caused… \n\nWhat I am specifically questioning is whether informal (and \noccasionally careless) gathering of acceptability judgments has \nactually held back progress in linguistics, and whether more careful \ngathering of acceptability judgments will provide the key to future \nprogress. \n\nEither Phillips is fronting the surprising opinion that generative \ntheorizing has never been led down the wrong track by demonstrably \nunreliable data, or he is changing the subject. And unless clear \ncriteria are established for what counts as “damage” and \n“holding back,” Phillips is not offering any testable \nhypothesis about data collection methodology. For example, Phillips \ndiscounts the observation of Schütze (1996) that conflicting \njudgments of relative unacceptability of violations of two linguistic\nuniversals held back the development of Government and Binding (GB), \non the grounds that two sets of conflicting judgments and their \nanalyses “are now largely forgotten, supplanted by theories \nthat have little to say about such examples.” But the fact that\nthe proposed universals are discarded principles of UG is irrelevant \nto the effect that unreliable data once had on the (now largely \nabandoned) GB theory. A methodological concern cannot be dismissed on\nthe basis of a move to a new theory that abandons the old theory but \nnot its methods! \n\nMore recently, Bresnan (2007) claims that many theoretical claims \nhave arguably been supported by unreliable informally gathered \nsyntactic acceptability judgments. She observes: \n\nHer discussion supports the view that various highly abstract \ntheoretical hypotheses have been defended through the use of \ngeneralizations based on unreliable data. \n\nThe debate over the harm that the acceptance of informally collected \ndata has had on theory testing is somewhat difficult to understand \nfor Essentialist, Externalist, and Emergentist researchers who have \nbeen trained in the methods of the cognitive and behavioral sciences.\nWhy try to support one’s theories of universal grammar, or of the \ngrammars of particular languages, by using questionably reliable \ndata? \n\nOne clue might be found in Culicover and Jackendoff (2010), who \nwrite: \n\nThe worry is that use of experimental methods is so resource\nconsumptive that it would impede the formulation of linguistic\ntheories. But this changes the subject from the importance of using\nreliable data as evidence in theory testing to using only\nexperimentally gathered data in theory formulation. We are\nnot aware of anyone who has ever suggested that at the stage of\nhypothesis development or theory formulation the linguist should\neschew intuition. Certainly Bard et al., Schütze, Cowart, Gibson\n& Fedorenko, and Ferreira say no such thing. The relevant issue\nconcerns what data should be used to test theories, which is\na very different matter. \n\nWe noted earlier that there are clear and uncontroversial \nacceptability judgments, and that these judgments are reliable data. \nThe difficulty lies in distinguishing the clear, uncontroversial, and\nreliable data from what only appears to be clear, uncontroversial, \nand reliable to a research community at a time. William Labov, the \nfounder of modern quantitative sociolinguistics, who takes an \nEmergentist approach, proposed a set of working methodological \nprinciples in Labov (1975) for adjudicating when experimental methods\nshould be employed. \n\nThe Consensus Principle: If there is no reason to \nthink otherwise, assume that the judgments of any native speaker are \ncharacteristic of all speakers. \n\nThe Experimenter Principle: If there is any \ndisagreement on introspective judgments, the judgments of those who \nare familiar with the theoretical issues may not be counted as \nevidence. \n\nThe Clear Case Principle: Disputed judgments should \nbe shown to include at least one consistent pattern in the speech \ncommunity or be abandoned. If differing judgments are said to \nrepresent different dialects, enough investigation of each dialect \nshould be carried out to show that each judgment is a clear case in \nthat dialect. (Labov 1975, quoted in Schütze 1996: 200) \n\nIf we accept that ‘introspective judgments’ are \nacceptability judgments, then Labov’s rules of thumb are guides for \nwhen to deploy experimental methods, although they no doubt need \nrefinement. However, it seems vastly more likely that careful \ndevelopment of such methodological rules of thumb can serve to \nimprove the reliability of linguistic data and adjudicate these \nmethodological disputes that seem largely independent of any \nparticular approach to linguistics. \n\nIn linguistics, the goal of collecting corpus data is to identify and\norganize a representative sample of a written and/or spoken variety \nfrom which characteristics of the entire variety or genre can be \ninduced. Concordances of word usage in linguistic context have long \nbeen used to aid in the translation and interpretation of literary \nand sacred texts of particular authors (e.g. Plato, Aristotle, \nAquinas) and of particular texts (e.g. the Torah, the rest of the Old\nTestament, the Gospels, the Epistles). Formal textual criticism, the \nidentification of antecedently existing oral traditions that were \nlater redacted into Biblical texts, and author identification (e.g. \nfiguring out which of the Epistles were written by Paul and which \nwere probably not) began to develop in the late 19th century. \n\nThe development of computational methods for collecting, analyzing, \nand searching corpora have seen rapid development as computer memory \nhas become less expensive and search and analysis programs have \nbecome faster. The first computer searchable corpus of American \nEnglish, the Brown Corpus, developed in the 1960s, contained just \nover one million word tokens. The British National Corpus (BNC) is a \nbalanced corpus containing over 100 million words—a hundredfold\nsize increase—of which 90% is written prose published from 1991\nto 1994 and 10% is spoken English. Between 2005 and 2007, \nbillion-word corpora were released for British English (ukWaC), \nGerman (deWaC), and Italian (itWaC)—a thousand times bigger \nthan the Brown corpus. And the entire World Wide Web probably holds \nabout a thousand times as much as that—around a trillion words.\nThus corpus linguistics has gone from megabytes of data (∼ \n103kB) to terabytes of data (∼ 109kB) in \nfifty years. \n\nJust as a central issue concerning acceptability judgment data \nconcerns its reliability as evidence for empirical generalizations \nabout languages or idiolects, a central question concerning the \ncollection of corpus data concerns whether or not it is \nrepresentative of the language variety it purports to represent. Some\nlinguists make the criterion of representativeness definitional: they\ncall a collection of samples of language use a corpus only if it has \nbeen carefully balanced between different genres (conversation, \ninformal writing, journalism, literature, etc.), regional varieties, \nor whatever. \n\nBut corpora are of many different kinds. Some are just very large\ncompilations of text from individual sources such as newspapers of\nrecord or the World Wide Web—compilations large enough for the\ndiversity in the source to act as a surrogate for representativeness.\nFor example, a billion words of a newspaper, despite coming from a\nsingle source, will include not only journalists’ news reports and\nprepared editorials but also quoted speech, political rhetoric, humor\ncolumns, light features, theater and film reviews, readers’ letters,\nfiction items, and so on, and will thus provide examples of a much\nwider variety of styles than one might have thought. \n\nCorpora are cleaned up through automatic or manual removal of such \nelements as numerical tables, typographical slips, spelling mistakes,\nmarkup tags, accidental repetitions (the the), larger-scale \nduplications (e.g., copies on mirror sites), boilerplate text \n(Opinions expressed in this email do not necessarily \nreflect…), and so on (see Baroni et al. 2009 for a fuller \ndiscussion of corpus cleaning). \n\nThe entire web itself can be used as a corpus to some degree, despite\nits constantly changing content, its multilinguality, its many tables\nand images, and its total lack of quality control; but when it is, the\noutputs of searches are nearly always cleaned by disregarding unwanted\nresults. For example, Google searches are blind to punctuation,\ncapitalization, and sentence boundaries, so search results for to\nbe will unfortunately include irrelevant cases, such as where a\nsentence like Do you want to? happens to be followed by a\nsentence like Be careful. \n\nCorpora can be annotated in ways that permit certain kinds of \nanalysis and grammar testing. One basic kind of annotation is \npart-of-speech tagging, in which each word is labeled with its \nsyntactic category. Another is lemmatization, which classifies the \ndifferent morphologically inflected forms of a word as belonging \ntogether (goes, gone, going, and went \nbelong with go, for example). A more thoroughgoing kind of \nannotation involves adding markup that encodes trees representing \ntheir structure; an example like That road leads to the \nfreeway might be marked up as a Clause within which the first two\nwords make up a Noun Phrase (NP), the last four constitute a Verb \nPhrase (VP), and so on, giving a structural analysis represented \nthus: \n\nSuch a diagram is isomorphic to (and the one shown was computed \ndirectly from) a labeled bracketing like this: \n(.Clause. (.NP. (.D. ‘that’ )\n  (.N. ‘road’ ) ) (.VP. (.V. ‘leads’ )\n  (.PP. (.P. ‘to’ ) (.NP. (.D. ‘the’ )\n  (.N. ‘freeway’ ) ) ) ) )\n \n\nand this in turn could be represented in a markup language like XML \nas: \n\nA corpus annotated with tree structure is known as a \ntreebank. Clearly, such a corpus is not a raw record\nof attested utterances at all; it is a combination of a collection of\nattested utterances together with a systematic attempt at analysing\ntheir structure. Whether the analysis is added manually or\nsemi-automatically, it is ultimately based on native speaker\njudgments. (Treebanks are often developed by graduate student\nannotators tutored by computational linguists; naturally, consistency\nbetween annotators is an issue that needs regular attention. See\nArtstein and Poesio, 2008, for discussion of the methodological\nissues.). \n\nOne of the purposes of a treebank is to permit the further \ninvestigation of a language and the checking of further linguistic \nhypotheses by searching a large database of previously established \nanalyses. It can also be used to test grammars, natural language \nprocessing systems, or machine learning programs. \n\nGoing beyond syntactic parse trees, it is possible to annotate \ncorpora further, with information of a semantic and pragmatic nature.\nThere is ongoing computational linguistic research aimed at \ndiscovering whether, for example, semantic annotation that is \nsemi-automatically added might suffice for recognition of whether a \nproduct review is positive or negative (what computational linguists \ncall ‘sentiment analysis’). \n\nNotice, then, that using corpus data does not mean abandoning or \nescaping from the use of intuitions about acceptability or \ngrammatical structure: the results of a corpus search are generally \nfiltered through the judgments of an investigator who decides which \npieces of corpus data are to be taken at face value and which are \njust bad hits or irrelevant noise. \n\nDifficult methodological issues arise in connection with the \ncollection, annotation, and use of corpus data. For example, there is\nthe issue of extremely rare expression tokens. Are they accurately \nrecorded tokens of expression types that turn up only in consequence \nof sporadic errors and should be dismissed as irrelevant unless the \ntopic of interest is performance errors? Are they due to errors in \nthe compilation of the corpus itself, corresponding to neither \naccepted usage nor sporadic speech errors? Or are they perfectly \ngrammatical but (for some extraneous reason) very rare, at least in \nthat particular corpus? \n\nMany questions arise about what kind of corpus is best suited to the \nresearch questions under consideration, as well as what kind of \nannotation is most appropriate. For example, as Ferreira (2005: 375) \npoints out, some large corpora, insofar as they have not been \ncleaned of speech errors, provide relevant data for studying the \ndistribution of speech disfluencies. In addition, probabilistic \ninformation about the relation between a particular verb and its \narguments has been used to show that “verb-argument preferences\n[are] an essential part of the process of sentence \ninterpretation” (Roland and Jurafsky 2002: 325): acceptability \njudgments on individual expressions do not provide information about \nthe distribution of a verb and its arguments in various kinds of \nspeech and writing. Studying conveyed meaning in context and \nidentification of speech acts will require a kind of data that \ndecontextualized acceptability judgments do not provide but \nsemantically annotated corpora might. \n\nMany Essentialists have been skeptical of the reliability of \nuncleaned, unanalyzed corpus data as evidence to support linguistic \ntheorizing, because it is assumed to be replete with strings that any\nnative speaker would judge unacceptable. And many Emergentists and \nExternalists, as well as some Essentialists, have charged that \ninformally gathered acceptability judgments can be highly unreliable \ntoo. Both worries are apposite; but the former does not hold for \nadequately cleaned and analyzed corpora, and the latter does not hold\nfor judgment data that has been gathered using appropriately \ncontrolled methods. In certain contested cases of acceptability, it \nwill of course be important to use both corpus and controlled \nelicitation methods to cross-compare. \n\nNotice that we have not in any way suggested that our three broad \napproaches to linguistics should differ in the kinds of data they use\nfor theory testing: Essentialists are not limited to informal \nelicitation; nor are Emergentists and Externalists denied access to \nit. In matters of methodology, at least, there is in principle an \nopen market—even if many linguists seem to think otherwise. \n\nEmergentists tend to follow Edward Sapir in taking an interest in \ninterlinguistic and intralinguistic variation. Linguistic \nanthropologists have explicitly taken up the task of defending a \nfamous claim associated with Sapir that connects linguistic variation\nto differences in thinking and cognition more generally. The claim is\nvery often referred to as the Sapir-Whorf Hypothesis\n(though this is a largely infelicitous label, as we shall see). \n\nThis topic is closely related to various forms of \nrelativism—epistemological, ontological, conceptual, and \nmoral—and its general outlines are discussed elsewhere in this \nencyclopedia; see the section on language in the Summer 2015\narchived version of the entry on  \n relativism (§3.1). \nCultural versions of moral relativism \nsuggest that, given how much cultures differ, what is moral for you \nmight depend on the culture you were brought up in. A somewhat \nanalogous view would suggest that, given how much language structures\ndiffer, what is thinkable for you might depend on the language you \nuse. (This is actually a kind of conceptual relativism, but it is \ngenerally called linguistic relativism, and we will continue that \npractice.) \n\nEven a brief skim of the vast literature on the topic is not remotely\nplausible in this article; and the primary literature is in any case \nmore often polemical than enlightening. It certainly holds no general\nanswer to what science has discovered about the influences of \nlanguage on thought. Here we offer just a limited discussion of the \nalleged hypothesis and the rhetoric used in discussing it, the vapid \nand not so vapid forms it takes, and the prospects for actually \ndevising testable scientific hypotheses about the influence of \nlanguage on thought. \n\nWhorf himself did not offer a hypothesis. He presented his “new\nprinciple of linguistic relativity” (Whorf 1956: 214) as a fact\ndiscovered by linguistic analysis: \n\nLater, Whorf’s speculations about the “sensuously and \noperationally different” character of different snow types for \n“an Eskimo” (Whorf 1956: 216) developed into a familiar \njournalistic meme about the Inuit having dozens or scores or hundreds\nof words for snow; but few who repeat that urban legend recall \nWhorf’s emphasis on its being grammar, rather than lexicon, that cuts\nup and organizes nature for us. \n\nIn an article written in 1937, posthumously published in an academic \njournal (Whorf 1956: 87–101), Whorf clarifies what is most \nimportant about the effects of language on thought and world-view. He\ndistinguishes ‘phenotypes’, which are overt grammatical \ncategories typically indicated by morphemic markers, from what he \ncalled ‘cryptotypes’, which are covert grammatical \ncategories, marked only implicitly by distributional patterns in a \nlanguage that are not immediately apparent. In English, the past \ntense would be an example of a phenotype (it is marked by the \n-ed suffix in all regular verbs). Gender in personal names and\ncommon nouns would be an example of a cryptotype, not systematically \nmarked by anything. In a cryptotype, “class membership of the \nword is not apparent until there is a question of using it or \nreferring to it in one of these special types of sentence, and then \nwe find that this word belongs to a class requiring some sort of \ndistinctive treatment, which may even be the negative treatment of \nexcluding that type of sentence” (p. 89). \n\nWhorf’s point is the familiar one that linguistic structure is \ncomprised, in part, of distributional patterns in language use that \nare not explicitly marked. What follows from this, according to \nWhorf, is not that the existing lexemes in a language (like its words\nfor snow) comprise covert linguistic structure, but that patterns \nshared by word classes constitute linguistic structure. In \n‘Language, mind, and reality’ (1942; published \nposthumously in Theosophist, a magazine published in India \nfor the followers of the 19th-century spiritualist Helena Blavatsky) \nhe wrote: \n\nWhorf apparently thought that only personal and proper names have an \nexact meaning or reference (Whorf 1956: 259). \n\nFor Whorf, it was an unquestionable fact that language influences \nthought to some degree: \n\nHe seems to regard it as necessarily true that language affects \nthought, given \n\nHe also seems to presume that the only structure and logic that\nthought has is grammatical structure. These views are not the ones\nthat after Whorf’s death came to be known as ‘the Sapir-Whorf\nHypothesis’ (a sobriquet due to Hoijer 1954). Nor are they what\nwas called the ‘Whorf thesis’ by Brown and Lenneberg\n(1954) which was concerned with the relation of obligatory lexical\ndistinctions and thought. Brown and Lenneberg (1954) investigated this\nquestion by looking at the relation of color terminology in a language\nand the classificatory abilities of the speakers of that language. The\nissue of the relation between obligatory lexical distinctions and\nthought is at the heart of what is now called ‘the Sapir-Whorf\nHypothesis’ or ‘the Whorf Hypothesis’ or\n‘Whorfianism’. \n\nNo one is going to be impressed with a claim that some aspect of your\nlanguage may affect how you think in some way or other; that is \nneither a philosophical thesis nor a psychological hypothesis. So it \nis appropriate to set aside entirely the kind of so-called hypotheses\nthat Steven Pinker presents in The Stuff of Thought (2007: \n126–128) as “five banal versions of the Whorfian \nhypothesis”: \n\nThese are just truisms, unrelated to any serious issue about \nlinguistic relativism. \n\nWe should also set aside some methodological versions of linguistic \nrelativism discussed in anthropology. It may be excellent advice to a\nbudding anthropologist to be aware of linguistic diversity, and to be\non the lookout for ways in which your language may affect your \njudgment of other cultures; but such advice does not constitute a \nhypothesis. \n\nThe term “Sapir-Whorf Hypothesis” was coined by Harry \nHoijer in his contribution (Hoijer 1954) to a conference on the work \nof Benjamin Lee Whorf in 1953. But anyone looking in Hoijer’s paper \nfor a clear statement of the hypothesis will look in vain. Curiously,\ndespite his stated intent “to review and clarify the \nSapir-Whorf hypothesis” (1954: 93), Hoijer did not even attempt\nto state it. The closest he came was this: \n\nThe claim that “language functions…as a way of defining \nexperience” appears to be offered as a kind of vague \nmetaphysical insight rather than either a statement of linguistic \nrelativism or a testable hypothesis. \n\nAnd if Hoijer seriously meant that what qualitative experiences a \nspeaker can have are constituted by that speaker’s language,\nthen surely the claim is false. There is no reason to doubt that \nnon-linguistic sentient creatures like cats can experience (for \nexample) pain or heat or hunger, so having a language is not a \nnecessary condition for having experiences. And it is surely not \nsufficient either: a robot with a sophisticated natural language \nprocessing capacity could be designed without the capacity for \nconscious experience. \n\nIn short, it is a mystery what Hoijer meant by his “central \nidea”. \n\nVague remarks of the same loosely metaphysical sort have continued to\nbe a feature of the literature down to the present. The statements \nmade in some recent papers, even in respected refereed journals, \ncontain non-sequiturs echoing some of the remarks of Sapir, Whorf, \nand Hoijer. And they come from both sides of the debate. \n\nLila Gleitman is an Essentialist on the other side of the \ncontemporary debate: she is against linguistic relativism, and \nagainst the broadly Whorfian work of Stephen Levinson’s group at the \nMax Planck Institute for Psycholinguistics. In the context of \ncriticizing a particular research design, Li and Gleitman (2002) \nquote Whorf’s claim that “language is the factor that limits \nfree plasticity and rigidifies channels of development”. But in\nthe claim cited, Whorf seems to be talking about the psychological \ntopic that holds universally of human conceptual development, not \nclaiming that linguistic relativism is true. \n\nLi and Gleitman then claim (p. 266) that such (Whorfian) views \n“have diminished considerably in academic favor” in part \nbecause of “the universalist position of Chomskian linguistics,\nwith its potential for explaining the striking similarity of language\nlearning in children all over the world.” But there is no clear\nconflict or even a conceptual connection between Whorf’s views about \nlanguage placing limits on developmental plasticity, and Chomsky’s \nthesis of an innate universal architecture for syntax. In short, \nthere is no reason why Chomsky’s I-languages could not be innately \nconstrained, but (once acquired) cognitively and developmentally \nconstraining. \n\nFor example, the supposedly deep linguistic universal of \n‘recursion’ (Hauser et al. 2002) is surely quite \nindependent of whether the inventory of colour-name lexemes in your \nlanguage influences the speed with which you can discriminate between\ncolor chips. And conversely, universal tendencies in color naming \nacross languages (Kay and Regier 2006) do not show that \ncolor-naming differences among languages are without effect on \ncategorical perception (Thierry et al. 2009). \n\nOne of the first linguists to defend a general form of universalism \nagainst linguistic relativism, thus presupposing that they conflict, \nwas Julia Penn (1972). She was also an early popularizer of the \ndistinction between ‘strong’ and ‘weak’ \nformulations of the Sapir-Whorf Hypothesis (and an opponent of the \n‘strong’ version). \n\n‘Weak’ versions of Whorfianism state that language \ninfluences or defeasibly shapes thought. \n‘Strong’ versions state that language determines\nthought, or fixes it in some way. The weak versions are commonly \ndismissed as banal (because of course there must be some \ninfluence), and the stronger versions as implausible. \n\nThe weak versions are considered banal because they are not \nadequately formulated as testable hypotheses that could conflict with\nrelevant evidence about language and thought. \n\nWhy would the strong versions be thought implausible? For a language \nto make us think in a particular way, it might seem that it must at \nleast temporarily prevent us from thinking in other ways, and thus \nmake some thoughts not only inexpressible but unthinkable. If this \nwere true, then strong Whorfianism would conflict with the Katzian \neffability claim. There would be thoughts that a person couldn’t \nthink because of the language(s) they speak. \n\nSome are fascinated by the idea that there are inaccessible thoughts;\nand the notion that learning a new language gives access to entirely \nnew thoughts and concepts seems to be a staple of popular writing \nabout the virtues of learning languages. But many scientists and \nphilosophers intuitively rebel against violations of effability: \nthinking about concepts that no one has yet named is part of their \njob description. \n\nThe resolution lies in seeing that the language could affect \ncertain aspects of our cognitive functioning without making certain \nthoughts unthinkable for us. \n\nFor example, Greek has separate terms for what we call light blue and\ndark blue, and no word meaning what ‘blue’ means in \nEnglish: Greek forces a choice on this distinction. Experiments have \nshown (Thierry et al. 2009) that native speakers of Greek react \nfaster when categorizing light blue and dark blue color \nchips—apparently a genuine effect of language on thought. But \nthat does not make English speakers blind to the distinction, or \nimply that Greek speakers cannot grasp the idea of a hue falling \nsomewhere between green and violet in the spectrum. \n\nThere is no general or global ineffability problem. There is, though,\na peculiar aspect of strong Whorfian claims, giving them a local \nanalog of ineffability: the content of such a claim cannot be \nexpressed in any language it is true of. This does not make the \nclaims self-undermining (as with the standard objections to \nrelativism); it doesn’t even mean that they are untestable. They are \nsomewhat anomalous, but nothing follows concerning the speakers of \nthe language in question (except that they cannot state the \nhypothesis using the basic vocabulary and grammar that they \nordinarily use). \n\nIf there were a true hypothesis about the limits that basic English \nvocabulary and constructions puts on what English speakers can think,\nthe hypothesis would turn out to be inexpressible in English, using \nbasic vocabulary and the usual repertoire of constructions. That \nmight mean it would be hard for us to discuss it in an article in \nEnglish unless we used terminological innovations or syntactic \nworkarounds. But that doesn’t imply anything about English speakers’ \nability to grasp concepts, or to develop new ways of expressing them \nby coining new words or elaborated syntax. \n\nA number of considerations are relevant to formulating, testing, and \nevaluating Whorfian hypotheses. \n\nGenuine hypotheses about the effects of language on thought will \nalways have a duality: there will be a linguistic part and a \nnon-linguistic one. The linguistic part will involve a claim that \nsome feature is present in one language but absent in another. \n\nWhorf himself saw that it was only obligatory features of languages\nthat established “mental patterns” or “habitual\nthought” (Whorf 1956: 139), since if it were optional then the\nspeaker could optionally do it one way or do it the other way.  And so\nthis would not be a case of “constraining the conceptual\nstructure”.  So we will likewise restrict our attention to\nobligatory features here. \n\nExamples of relevant obligatory features would include lexical \ndistinctions like the light vs. dark blue forced choice in Greek, or \nthe forced choice between “in (fitting tightly)” vs. \n“in (fitting loosely)” in Korean. They also include \ngrammatical distinctions like the forced choice in Spanish 2nd-person\npronouns between informal/intimate and formal/distant (informal \ntú vs. formal usted in the singular; informal \nvosotros vs. formal ustedes in the plural), or the \nforced choice in Tamil 1st-person plural pronouns between inclusive \n(“we = me and you and perhaps others”) and exclusive \n(“we = me and others not including you”). \n\nThe non-linguistic part of a Whorfian hypothesis will contrast the \npsychological effects that habitually using the two languages has on \ntheir speakers. For example, one might conjecture that the habitual \nuse of Spanish induces its speakers to be sensitive to the formal and\ninformal character of the speaker’s relationship with their \ninterlocutor while habitually using English does not. \n\nSo testing Whorfian hypotheses requires testing two independent \nhypotheses with the appropriate kinds of data. In consequence, \nevaluating them requires the expertise of both linguistics and \npsychology, and is a multidisciplinary enterprise. Clearly, the \nlinguistic hypothesis may hold up where the psychological hypothesis \ndoes not, or conversely. \n\nIn addition, if linguists discovered that some linguistic feature was\noptional in two different languages, then even if psychological experiments \nshowed differences between the two populations of speakers, this \nwould not show linguistic determination or influence. The cognitive \ndifferences might depend on (say) cultural differences. \n\nA further important consideration concerns the strength of the \ninducement relationship that a Whorfian hypothesis posits between a \nspeaker’s language and their non-linguistic capacities. The claim \nthat your language shapes or influences your cognition is quite \ndifferent from the claim that your language makes certain kinds of \ncognition impossible (or obligatory) for you. The strength of any \nWhorfian hypothesis will vary depending on the kind of relationship \nbeing claimed, and the ease of revisability of that relation. \n\nA testable Whorfian hypothesis will have a schematic form something \nlike this: \n\nThe relation R might in principle be causation or \ndetermination, but it is important to see that it might merely be \ncorrelation, or slight favoring; and the non-linguistic cognitive \neffect C might be readily suppressible or revisable. \n\nDan Slobin (1996) presents a view that competes with Whorfian \nhypotheses as standardly understood. He hypothesizes that when \nthe speakers are using their cognitive abilities in the service of a \nlinguistic ability (speaking, writing, translating, etc.), the \nlanguage they are planning to use to express their thought will have \na temporary online effect on how they express their thought. The \nclaim is that as long as language users are thinking in order to \nframe their speech or writing or translation in some language, the \nmandatory features of that language will influence the way they \nthink. \n\nOn Slobin’s view, these effects quickly attenuate as soon as the \nactivity of thinking for speaking ends. For example, if a speaker is \nthinking for writing in Spanish, then Slobin’s hypothesis would \npredict that given the obligatory formal/informal 2nd-person pronoun \ndistinction they would pay greater attention to the formal/informal \ncharacter of their social relationships with their audience than if \nthey were writing in English. But this effect is not permanent. As \nsoon as they stop thinking for speaking, the effect of Spanish on \ntheir thought ends. \n\nSlobin’s non-Whorfian linguistic relativist hypothesis raises the \nimportance of psychological research on bilinguals or people who \ncurrently use two or more languages with a native or near-native \nfacility. This is because one clear way to test Slobin-like \nhypotheses relative to Whorfian hypotheses would be to find out \nwhether language correlated non-linguistic cognitive differences \nbetween speakers hold for bilinguals only when are thinking for \nspeaking in one language, but not when they are thinking for speaking\nin some other language. If the relevant cognitive differences \nappeared and disappeared depending on which language speakers were \nplanning to express themselves in, it would go some way to vindicate \nSlobin-like hypotheses over more traditional Whorfian Hypotheses. Of \ncourse, one could alternately accept a broadening of Whorfian \nhypotheses to include Slobin-like evanescent effects. Either way, \nattention must be paid to the persistence and revisability of the \nlinguistic effects. \n\nKousta et al. (2008) shows that “for bilinguals there is \nintraspeaker relativity in semantic representations and, therefore, \n[grammatical] gender does not have a conceptual, non-linguistic \neffect” (843). Grammatical gender is obligatory in the \nlanguages in which it occurs and has been claimed by Whorfians to \nhave persistent and enduring non-linguistic effects on \nrepresentations of objects (Boroditsky et al. 2003). However, Kousta \net al. supports the claim that bilinguals’ semantic representations \nvary depending on which language they are using, and thus have \ntransient effects. This suggests that although some semantic \nrepresentations of objects may vary from language to language, their \nnon-linguistic cognitive effects are transitory. \n\nSome advocates of Whorfianism have held that if Whorfian hypotheses \nwere true, then meaning would be globally and radically \nindeterminate. Thus, the truth of Whorfian hypotheses is equated with\nglobal linguistic relativism—a well known self-undermining form\nof relativism. But as we have seen, not all Whorfian hypotheses are \nglobal hypotheses: they are about what is induced by particular \nlinguistic features. And the associated non-linguistic perceptual and\ncognitive differences can be quite small, perhaps insignificant. For \nexample, Thierry et al. (2009) provides evidence that an obligatory \nlexical distinction between light and dark blue affects Greek \nspeakers’ color perception in the left hemisphere only. And the \nquestion of the degree to which this affects sensuous experience is \nnot addressed. \n\nThe fact that Whorfian hypotheses need not be global linguistic \nrelativist hypotheses means that they do not conflict with the claim \nthat there are language universals. Structuralists of the first half \nof the 20th century tended to disfavor the idea of universals: Martin\nJoos’s characterization of structuralist linguistics as claiming that\n“languages can differ without limit as to either extent or \ndirection” (Joos 1966, 228) has been much quoted in this \nconnection. If the claim that languages can vary without limit were \nconjoined with the claim that languages have significant and \npermanent effects on the concepts and worldview of their speakers, a \ntruly profound global linguistic relativism would result. But neither\nconjunct should be accepted. Joos’s remark is regarded by nearly all \nlinguists today as overstated (and merely a caricature of the \nstructuralists), and Whorfian hypotheses do not have to take a global\nor deterministic form. \n\nJohn Lucy, a conscientious and conservative researcher of Whorfian \nhypotheses, has remarked: \n\nAlthough further empirical studies on Whorfian hypotheses have been \ncompleted since Lucy published his 1996 review article, it is hard to\nfind any that have satisfied the criteria of: \n\nThere is much important work yet to be done on testing the range of \nWhorfian hypotheses and other forms of linguistic conceptual \nrelativism, and on understanding the significance of any Whorfian \nhypotheses that turn out to be well supported. \n\nThe three approaches to linguistic theorizing have at least something\nto say about how languages are acquired, or could in principle be \nacquired. Language acquisition has had a much higher profile since \ngenerative Essentialist work of the 1970s and 1980s gave it a central\nplace on the agenda for linguistic theory. \n\nResearch into language acquisition falls squarely within the \npsychology of language; see the entry on \n language and innateness.\n In this section we do not aim to deal in detail with any of the voluminous\nliterature on psychological or computational experiments bearing on \nlanguage acquisition, or with any of the empirical study of language \nacquisition by developmental linguists, or the ‘stimulus \npoverty’ argument for the existence of innate knowledge about \nlinguistic structure (Pullum and Scholz 2002). Our goals are merely \nto define the issue of linguistic nativism, set it \nin context, and draw morals for our three approaches from some of the\nmathematical work on inductive language learning. \n\nThe reader with prior acquaintance with the literature of linguistics\nwill notice that we have not made reference to any partitioning of \nlinguists into two camps called ‘empiricists’ and \n‘rationalists’ (see e.g. Matthews 1984, Cowie 1999). We \ndraw a different distinction relating to the psychological and \nbiological prerequisites for first language acquisition. It divides \nnearly all Emergentists and Externalists from most Essentialists. It \nhas often been confused with the classical empiricist/rationalist \nissue. \n\nGeneral nativists maintain that the prerequisites \nfor language acquisition are just general cognitive abilities and \nresources. Linguistic nativists, by contrast, claim \nthat human infants have access to at least some specifically \nlinguistic information that is not learned from linguistic \nexperience. Table 3 briefly sketches the differences between the two \nviews. Table 3:\nGeneral and linguistic nativism contrasted \n\nThere does not really seem to be anyone who is a complete \nnon-nativist: nobody really thinks that a creature with no unlearned \ncapacities at all could acquire a language. That was the point of the\nmuch-quoted remark by Quine (1972: 95–96) about how “the \nbehaviorist is knowingly and cheerfully up to his neck in innate \nmechanisms of learning-readiness”. Geoffrey Sampson (2001, \n2005) is about as extreme an opponent of linguistic nativism as one \ncan find, but even he would not take the failure of language \nacquisition in his cat to be unrelated to the cognitive and physical \ncapabilities of cats. \n\nThe issue on which empirical research can and should be done is \nwhether some of the unlearned prerequisites that humans enjoy have \nspecifically linguistic content. For a philosophically-oriented \ndiscussion of the matter, see chapters 4–6 of Stainton (2006). \nFor extensive debate about “the argument from poverty of the \nstimulus”, see Pullum and Scholz (2002) together with the six \ncritiques published in the same issue of The Linguistic \nReview and the responses to those critiques by Scholz and Pullum\n(2002). \n\nLinguists have given considerable attention to considerations of \nin-principle learnability—not so much the \ncourse of language acquisition as tracked empirically (the work of \ndevelopmental psycholinguists) but the question of how languages of \nthe human sort could possibly be learned by any kind of learner. The \ntopic was placed squarely on the agenda by Chomsky (1965); and a \nhugely influential mathematical linguistics paper by Gold (1967)has \ndominated much of the subsequent discussion. \n\nGold began by considering a reformulation of the standard \nphilosophical problem of induction. The trouble with the question \n‘Which hypothesis is correct given the totality of the \ndata?’ is of course the one that Hume saw: if the domain is \nunbounded, no finite amount of data can answer the question. Any \nfinite body of evidence will be consistent with arbitrarily many \nhypotheses that are not consistent with each other. But Gold proposed\nreplacing the question with a very different one: Which tentative\nhypothesis is the one to pick, given the data provided so far, \nassuming a finite number of wrong guesses can be forgiven? \n\nGold assumed that the hypotheses, in the case of language learning, \nwere generative grammars (or alternatively parsers; he proves results\nconcerning both, but for brevity we follow most of the literature and\nneglect the very similar results on parsers). The learner’s task is \nconceived of as responding to an unending input data stream \n(ultimately complete, in that every expression eventually turns up) \nby enunciating a sequence of guesses at grammars. \n\nAlthough Gold talks in developmental psycholinguistic terms about \nlanguage learners learning grammars by trial and error, his extremely\nabstract proofs actually make no reference to the linguistic content \nof languages or grammars at all. The set of all finite grammars \nformulable in any given metalanguage is computably enumerable, so \ngrammars can be systematically numbered. Inputs—grammatical \nexpressions from the target language—can also be numerically \nencoded. We end up being concerned simply with the existence or \nnon-existence of certain functions from natural number sequences to \nnatural numbers. \n\nA successful learner is one who uses a procedure that is guaranteed \nto eventually hit on a correct grammar. For single languages, this is\ntrivial: if the target language is L and it is generated by a \ngrammar G, then the procedure “Always guess \nG” does the job, and every language is learnable. What \nmakes the problem interesting is applying it to classes of \ngrammars. A successful learner for a class C is one who uses a\nprocedure that is guaranteed to succeed no matter what grammar from \nC is the target and no matter what the data stream is like (as\nlong as it is complete and contains no ungrammatical examples). \n\nGold’s work has interesting similarities with earlier philosophical \nwork on inductive learning by Hilary Putnam (1963; it is not clear \nwhether Gold was aware of this paper). Putnam gave an informal proof \nof a sort of incompleteness theorem for inductive regularity-learning\ndevices: no matter what algorithm is used in a machine for inducing \nregularities from experience, and thus becoming able to predict \nevents, there will always be some possible environmental regularities\nthat will defeat it. (As a simple example, imagine an environment \ngiving an unbroken sequence of presentations all having some property\na. If there is a positive integer n such that after \nn presentations the machine will predict that presentation \nnumber n + 1 will also have property a, then the machine will \nbe defeated by an environment consisting of n presentations of\na followed by one with the incompatible property \nb—the future need not always resemble the past. But if \non the other hand there is no such n, then an environment \nconsisting of an unending sequence of a presentations will \ndefeat it.) \n\nGold’s theorems are founded on certain specific idealizing \nassumptions about the language learning situation, some of which are \nintuitively very generous to the learner. The main ones are these: \n\nThe most celebrated of the theorems Gold proved (using some reasoning\nremarkably similar to that of Putnam 1963) showed that a language \nlearner could be similarly hostage to malign environments. Imagine a \nlearner being exposed to an endless and ultimately exhaustive \nsequence of presented expressions from some target \nlanguage—Gold calls such a sequence a ‘text’. \nSuppose the learner does not know in advance whether the language is \ninfinite, or is one of the infinitely many finite languages over the \nvocabulary V. Gold reasons roughly thus: \n\nLeaping too soon to the conclusion that the target language is \ninfinite will be disastrous, because there will be no way to \nretrench: no presented examples from a finite language \nLk will ever conflict with the hypothesis \nthat the target is some infinite superset of \nLk. \n\nThe relevance of all this to the philosophy of linguistics is that \nthe theorem just sketched has been interpreted by many linguists, \npsycholinguists, and philosophers as showing that humans could not \nlearn languages by inductive inference based on examples of language \nuse, because all of the well-known families of languages \ndefined by different types of generative grammar have the crucial \nproperty of allowing grammars for every finite language and for at \nleast some infinite supersets of them. But Gold’s paper has often \nbeen over-interpreted. A few examples of the resultant mistakes \nfollow. \n\nIt’s not about underdetermination. Gold’s negative \nresults are sometimes wrongly taken to be an unsurprising reflection \nof the underdetermination of theories by finite bodies of evidence \n(Hauser et al. 2002 seem to make this erroneous equation on p. 1577; \nso do Fodor and Crowther 2002, implicitly—see Scholz and Pullum\n2002, 204–206). But the failure of text-identifiability for \ncertain classes of languages is different from underdetermination in \na very important way, because there are infinite classes of infinite \nlanguages that are identifiable from text. The first chapter\nof Jain et al. (1999) discusses an illustrative example (basically, \nit is the class containing, for all n > 0, the set of all strings \nwith length greater than n). There are infinitely many others.\nFor example, Shinohara (1990) showed that for any positive integer \nn the class of all languages generated by a context-sensitive \ngrammar with not more than n rules is learnable from text. \n\nIt’s not about stimulus poverty. It has also \nsometimes been assumed that Gold is giving some kind of argument from\npoverty of the stimulus (there are signs of this in Cowie 1999, \n194ff; Hauser et al. 2002, 1577; and Prinz 2002, 210). This is very \nclearly a mistake (as both Laurence and Margolis 2001 and Matthews \n2007 note): in Gold’s text-learning scenario there is no stimulus \npoverty at all. Every expression in the language eventually turns up \nin the learner’s input. \n\nIt’s not all bad news. It is sometimes forgotten \nthat Gold established a number of optimistic results as well as the \npessimistic one about learning from text. Given what he called an \n‘informant’ environment rather than a text environment, \nwe see strikingly different results. An informant environment is an \ninfinite sequence of presentations sorted into two lists, positive \ninstances (expressions belonging to the target language) and negative\ninstances (not in the language). Almost all major language-theoretic \nclasses are identifiable in the limit from an informant environment \n(up to and including the class of all languages with a primitive \nrecursive characteristic function, which comes close to covering any \nlanguage that could conceivably be of linguistic interest), and all \ncomputably enumerable languages become learnable if texts are allowed\nto be sequenced in particular ways (see the results in Gold 1967 on \n‘anomalous text’). \n\nGold did not give a necessary condition for a class to be identifiable\nin the limit from text, but Angluin (1980) later provided one (in a\nresult almost but not quite obtained by Wexler and Hamburger\n1973). Angluin showed that a class C is text-identifiable iff\nevery language L in C has a finite “telltale” subset\nT such that if T is also proper subset of some other\nlanguage in C, that other language is not a proper subset of\nL.  This condition precludes guessing too large a language.\nOnce all the members of the telltale subset for L have been\nreceived as input, the learner can safely make L the current\nconjecture.  The language to be identified must be either L or\n(if subsequent inputs include new sentences not in L) some\nlarger language, but it can’t be a proper subset of L. \n\nJohnson (2004) provides a useful review of several other \nmisconceptions about Gold’s work; e.g., the notion that it might be \nthe absence of semantics from the input that makes identification \nfrom text impossible (this is not the case). \n\nSome generative Essentialists see a kind of paradox in Gold’s \nresults—a reductio on one or more of the assumptions he makes \nabout in-principle learnability. To put it very crudely, learning \ngenerative grammars from presented grammatical examples seems to have\nbeen proved impossible, yet children do learn their first languages, \nwhich for generative Essentialists means they internalize generative \npsychogrammars, and it is claimed to be an empirical fact that they \nget almost no explicit evidence about what is not in the \nlanguage (Brown and Hanlon 1970 is invariably cited to support this).\nContradiction. Gold himself suggested three escape routes from the \napparent paradox: \n\nAll three of these paths have been subsequently explored. Path (1) \nappealed to generative Essentialists. Chomsky (1981) suggested an \nextreme restriction: that universal grammar permitted only finitely \nmany grammars. This claim (for which Chomsky had little basis: see \nPullum 1983) would immediately guarantee that not all finite \nlanguages are humanly learnable (there are infinitely many finite \nlanguages, so for most of them there would be no permissible \ngrammar). Osherson and Weinstein (1984) even proved that under three \nfairly plausible assumptions about the conditions on learning, \nfiniteness of the class of languages is necessary—that is, a \nclass must be finite if it is to be identifiable from text. \nHowever, they also proved that this is not sufficient: there are very\nsmall finite classes of languages that are not identifiable \nfrom text, so it is logically possible for text-identification to be \nimpossible even given only a finite number of languages (grammars). \nThese two results show that Chomsky’s approach cannot be the whole \nanswer. \n\nPath (2) proposes investigation of children’s input with an eye to \nfinding covert sources of negative evidence. Various psycholinguists \nhave pursued this idea; see the entry on \n language and innateness\n in this encyclopedia, and (to cite one example) the results of \nChouinard and Clark (2003) on hitherto unnoticed sources of negative \nevidence in the infant’s linguistic environment, such as parental \ncorrections. \n\nPath (3) suggests investigating the nature of children’s linguistic \nenvironments more generally. Making evidence available to the learner\nin some fixed order can certainly alter the picture quite radically \n(Gold proved that if some primitive-recursive generator controls the \ntext it can in effect encode the identity of the target language so \nthat all computably enumerable languages become identifiable from \ntext). It is possible in principle that limitations on texts (or on \nlearners’ uptake) might have positive rather than negative effects on\nlearnability (see Newport 1988; Elman 1993; Rohde and Plaut 1999; and\nthe entry on \n language and innateness).\n  \n\nGold’s suggested strategy of restricting the pre-set class of \ngrammars has been interpreted by some as a defense of rationalist \nrather than empiricist theories of language acquisition. For example,\nWexler and Culicover state: \n\nWexler and Culicover claim that ‘empiricist’ learning \nmechanisms are both weak and general: not only are they ‘not \nrelated to the learning of any particular subject matter or cognitive\nability’ but they are not ‘limited to any particular \nspecies’. It is of course not surprising that empiricist \nlearning fails if it is defined in a way that precludes drawing a \ndistinction between the cognitive abilities of humans and fruit \nflies. \n\nEquating Gold’s idea of restricting the class of grammars with the \nidea of a ‘rationalist’ knowledge acquisition theory, \nWexler and Culicover try to draw out the consequences of Gold’s \nparadigm for the Essentialist linguistic theory of Chomsky (1965). \nThey show how a very tightly restricted class of transformational \ngrammars could be regarded as text-identifiable under extremely \nstrong assumptions (e.g., that all languages have the same innately \nknown deep structures). \n\nMatthews (1984) follows Wexler and Culicover’s lead and draws a more \nphilosophically oriented moral: \n\nThe actual relation of Gold’s results to the empiricism/rationalism \ncontroversy seems to us rather different. Gold’s paradigm looks a lot\nmore like a formalization of so-called ‘rationalism’. The\nfixed class of candidate hypotheses (grammars) corresponds to what is\ngiven by universal grammar—the innate definition of the \nessential properties of language. What Gold actually shows, \ntherefore, is not “the plausibility of rationalism” but \nrather the inadequacy of a huge range of rationalist theories: under \na wide range of different choices of universal grammar, language \nacquisition appears to remain impossible. \n\nMoreover, Matthews ignores (as most linguists have) the existence of \nlarge and interesting classes of languages that are \ntext-identifiable. \n\nGold’s result, like Putnam’s earlier one, does show that a certain \nkind of trial-and-error inductive learning is insufficient to permit \nlearning of arbitrary environmental regularities. There has to be \nsome kind of initial bias in the learning procedure or in the data. \nBut ‘empiricism’, the supposed opponent of \n‘rationalism’, is not to be equated with a denial of the \nexistence of learning biases. No one doubts that humans have \ninductive biases. To quote Quine again, “Innate biases and \ndispositions are the cornerstone of behaviorism, and have been \nstudied by behaviorists” (1972: 95–96). As Lappin and \nShieber (2007) stress, there cannot be such a thing as a learning \nprocedure (or processing mechanism) with no biases at all. \n\nThe biases posited in Emergentist theories of language acquisition \nare found, at least in part, in the non-linguistic social and \ncognitive bases of human communication. And the biases of Externalist\napproaches to language acquisition are to be found in the \ndistributional and stochastic structure of the learning input and the\nmultitude of mechanisms that process that input and their \ninteractions. All contemporary approaches to language acquistion have\nacknowledged Gold’s results, but those results do not by themselves \nvindicate any one of our three approaches to the study of language. \n\nGold’s explicit equation of acquiring a language with identifying a \ngenerative grammar that exactly generates it naturally makes his work\nseem relevant to generative Essentialists (though even for them, his \nresults do not provide anything like a sufficient reason for adopting\nthe linguistic nativist position). But another key assumption, that \nnothing about the statistical structure of the input plays a role in \nthe acquisition process, is being questioned by increasing numbers of\nExternalists, many of whom have used Bayesian modeling to show that \nthe absence of positive evidence can function as a powerful source of\n(indirect) negative evidence: learning can be driven by what is not \nfound as well as by what is (see e.g. Foraker et al. (2009)). \n\nMost Emergentists simply reject the assumption that what is learned \nis a generative grammar. They see the acquisition task as a matter of\nlearning the details of an array of constructions (roughly, \nmeaning-bearing ways of structurally composing words or phrases) and \nhow to use them to communicate. How such learning is accomplished \nneeds a great deal of further study, but Gold’s paper did not show it\nto be impossible. \n\nOver the past two decades a large amount of work has been done on \ntopics to which the term ‘language evolution’ is \nattached, but there are in fact four distinct such topics: \n\nEmergentists tend to regard any of the topics (a)–(d) as \npotentially relevant to the study of language evolution. \nEssentialists tend to focus solely on (c). Some Essentialists even \ndeny that (a) and (b) have any relevance to the study of (c); for \nexample: \n\nOther generative Essentialists, like Pinker and Bloom (1990) and \nPinker and Jackendoff (2005), seem open to the view that even the \nmost elemental aspects of topic (b) can be directly relevant to the \nstudy of (c). This division among Essentialists reflects a division \namong their views about the role of adaptive explanations in the \nemergence of (b) and especially (c). For example: \n\nThe view expressed here that all (or even most) interesting \nproperties of the language faculty are not adaptations conflicts with\nthe basic explanatory strategy of evolutionary psychology found in \nthe neo-Darwinian Essentialist views of Pinker and Bloom. \nPiattelli-Palmarini (1989), following Chomsky, adopts a fairly \nstandard Bauplan critique of adaptationism. On this view the language\nfaculty did not originate as an adaptation, but more plausibly \n“may have originally arisen for some purely architectural or \nstructural reason (perhaps overall brain size, or the sheer \nduplication of pre-existing modules), or as a by product of the \nevolutionary pressures” (p. 19), i.e., it is a kind of Gouldian\nspandrel. \n\nMore recently, some Essentialist-leaning authors have rejected the \nview that no analogies and homologies between animal and human \ncommunication are relevant to the study of language. For example, in \nthe context of commenting on Hauser et al. (2002), Tecumseh Fitch \n(2010) claims that “Although Language, writ large, is unique to\nour species (many probably most) of the mechanisms involved in \nlanguage have analogues or homologues in other animals.” \nHowever, the view that the investigation of animal communication can \nshed light on human language is still firmly rejected by some. For \nexample, Bickerton (2007: 512) asserts that “nothing resembling\nhuman language could have developed from prior animal call \nsystems.” \n\nBickerton fronts the following simple argument for his view: \n\nThus, the mere fact that language is unique to humans is sufficient \nto rule out monkey and primate call systems as preadapations for \nlanguage. But, contra Bickerton, a neo-Darwinian like Jackendoff \n(2002) appeals to the work of Dunbar (1998), Power (1998), Worden \n(1998) to provide a selectionist story which assumes that cooperation\nin hunting, defense (Pinker and Bloom 1990), and “ \n‘social grooming’ or deception” are selective \nforces that operated on human ancestors to drive increases in \nexpressive power that distinguishes non-human communication and human\nlinguistic capacities and systems. \n\nWhile generative Essentialists debate among themselves about the \nplausibility of adaptive explanations for the emergence of essential \nfeatures of a modular language capacity, Emergentists are perhaps \nbest characterized as seeking broad evolutionary explanations of the \nfeatures of languages (topic (c)) and communicative capacities \n(topics (b) and (c)) conceived in non-essentialist, non-modular ways.\nAnd as theorists who are committed to exploring non-modular views \nof linguistic capacities (topic (c)), the differences and \nsimilarities between (a) and (b) are potentially relevant to (c). \n\nPrimatologists like Cheney and Seyfarth, psychologists like \nTomasello, anthropologists like Terrence Deacon, and linguists like \nPhillip Lieberman share an interest in investigating the \ncommunicative, anatomical, and cognitive characteristics of non-human\nanimals to identify biological differences between humans, and \nmonkeys and primates. In the following paragraph we discuss Cheney \nand Seyfarth (2005) as an example, but we could easily have chosen \nany of a number of other theorists. \n\nCheney and Seyfarth (2005) emphasize that non-human primates have a \nsmall, stimulus specific repertoire of vocal productions that are not \n“entirely involuntary,” and this contrasts with their \n“almost openended ability to learn novel sound-meaning \npairs” (p. 149). They also emphasize that vocalizations in \nmonkeys and apes are used to communicate information about the \nvocalizer, not to provide information intended to “rectify \nfalse beliefs in others or instruct others” (p. 150). Non-human\nprimate communication consists in the mainly involuntary broadcasting\nof the vocalizer’s current affective state. Moreover, although Cheney\nand Seyfarth recognize that the vervet monkey’s celebrated call \nsystem (Cheney and Seyfarth 1990) is “functionally \nreferential” in context, their calls have no explicit meaning \nsince they lack “any propositional structure”. From this \nthey conclude: \n\nBy ‘lexical syntax’ Cheney and Seyfarth mean a kind of \nsemantic compositionality of characteristic vocalizations. If a \nvocalization (call) were to have lexical syntax, the semantic \nsignificance of the whole would depend on the relation of the \nstructure of parts of the call to the structure of what they signify.\nThe absence of ‘lexical syntax’ in call systems suggests \nthat it is illegitimate to think of them as having anything like \nsemantic structure at all. \n\nDespite the rudimentary character of animal communication systems \nwhen compared with human languages, Cheney and Seyfarth argue that \nmonkeys and apes exhibit at least five characteristics that are \npre-adaptations for human communication: \n\nIt is, of course, controversial to claim that monkeys have \nrule-governed propositional social knowledge systems as claimed in \n(iv) and (v). But Emergentists, Externalists, and Essentialists could\nall, in principle, agree that there are both unique characteristics \nof human communicative capacities and characteristics of such \ncapacities that are shared with non-humans. For example, by the age \nof one, human infants can use direction of gaze and focus of \nattention to infer the referent of a speaker’s utterance (Baldwin and\nMoses 1994). By contrast, this sort of social referencing capacity in\nmonkeys and apes is rudimentary. This suggests that a major component\nof humans’ capacity to infer a specific referent is lacking in \nnon-humans. \n\nDisagreements between the approaches might be due to the perceived \nsignificance of non-human communicative capacities and their relation\nto uniquely human ones. \n\nWe mentioned earlier that both early 20th-century linguistics \nmonographs and contemporary introductory textbooks include \ndiscussions of historical linguistics, i.e., that branch that studies\nthe history and prehistory of changes in particular languages, how \nthey are related to each other, and how and why they change. \n\nThe last decade has seen two kinds of innovations related to studying\nchanges in particular languages. One, which we will call \n‘linguistic phylogeny’, concerns the application of \nstochastic phylogenetic methods to investigate prehistoric population\nand language dispersion (Gray and Jordan 2000, Gray 2005, Atkinson \nand Gray 2006, Gray et al. 2009). These methods answer questions \nabout how members of a family of languages are related to each other \nand dispersed throughout a geographic area. The second, which we will\ncall the effects of transmission, examines how interpreted artificial\nlanguages (sets of signifier/signified pairs) change under a range of\ntransmission conditions (Kirby et al. 2008, Kirby 2001, Hurford \n2000), thus providing evidence about how the process of transmission \naffects the characteristics, especially the structure, of the \ntransmitted interpreted system. \n\nRussell Gray and his colleagues have taken powerful phylogenetic \nmethods that were developed by biologists to investigate molecular \nevolution, and applied them to linguistic data in order to answer \nquestions about the evolution of language families. For example, Gray\nand Jordan (2000) used a parsimony analysis of a large language data \nset to adjudicate between competing hypotheses about the speed of the\nspread of Austronesian languages through the Pacific. More recently, \nGreenhill et al. (2010) used a NeighbourNet analysis to evaluate the \nrelative rates of change in the typological and lexical features of \nAustronesian and Indo-European. These results bear on hypotheses \nabout the relative stability of language types over lexical features \nof those languages, and how far back in time that stability extends. \nIf there were highly conserved typological and lexical features, then\nit might be possible to identify relationships between languages that\ndate beyond the 8000 (plus or minus 2000) year limit that is imposed \nby lexical instability. \n\nThe computational and laboratory experiments of Kirby and his \ncollaborators have shown that under certain conditions of iterated \nlearning, any given set of signifier/signified pairs in which the \nmapping is initially arbitrary will change to exhibit a very general \nkind of compositional structure. Iterated learning has been studied \nin both computational and laboratory experiments by means of \ndiffusion chains, i.e., sequences of learners. A primary \ncharacteristic of such sequences of transmission is that what is \ntransmitted from learner to learner will change in an iterated \nlearning environment, in a way that depends on the conditions of \ntransmission. \n\nThe children’s game called ‘Telephone’ in the USA \n(‘Chinese Whispers’ in the UK), provides an example of \ndiffusion chains under which what is transmitted is not stable. In a \ndiffusion chain learning situation what a chain member has actually \nlearned from an earlier member of the chain is presented as the input\nto the next learner, and what that learner has actually learned \nprovides the input to the following learner. In cases where the \ninitial learning task is very simple: i.e., where what is transmitted\nis both simple, completely transmitted, and the transmission channel \nis not noisy, what is transmitted is stable over iterated \ntransmissions even in cases when the participants are young children \nand chimpanzees (Horner et al. 2006). That is, there is little change\nin what is transmitted over iterated transmissions. However, in cases\nwhere what is transmitted is only partially presented, very complex, \nor the channel is noisy, then there is a decrease in the fidelity of \nwhat is transmitted across iterations just like there is in the \nchildren’s game of Telephone. \n\nWhat Kirby and colleagues show is that when the initial input to a \ndiffusion chain is a reasonably complex set of arbitrary \nsignal/signifier pairs, e.g. one in which 27 complex signals of 6 \nletters are randomly assigned to 27 objects varying on dimensions of \ncolor, kind of motion, and shape, what is transmitted becomes more \nand more compositional over iterated transmission. Here, \n‘compositional’ is being used to refer to the high degree\nto which sub-strings of the signals come to be systematically paired \nwith specific phenomenal sub-features of what is signified. The \ntransmission conditions in these experiments were free of noise, and \nfor each iteration of the learning task only half of the possible 27 \nsignifier/signified pairs were presented to participants. Under this \nkind of transmission bottleneck a high degree of sign/signified \nstructure emerged. \n\nA plausible interpretation of these results is that the developing \nstructure of the collection of signs is a consequence of the repeated\nforced inference by participants from 14 signs and signifieds in the \ntraining set to the entire set of 27 pairs. A moral could be that \niterated forced prediction of the sign/signified pairs in the entire \nset, on the basis of exposure to only about half of them, induced the\ndevelopment of a systematic, compositional structure over the course \nof transmission. It is reasonable to conjecture that this resulting \nstructure reflects effects of human memory, not a domain-specific \nlanguage module—although further work would be required to rule\nout many other competing hypotheses. \n\nThus Kirby and his colleagues focus on something very different from \nthe prerequisites for language emergence. Linguistic nativists have \nbeen interested in how primates like us could have become capable of \nacquiring systems with the structural properties of natural \nlanguages. Kirby and his colleagues (while not denying that human \ncognitive evolution is of interest) are studying how languages \nevolve to be capable of being acquired by primates like us.","contact.mail":"gpullum@ed.ac.uk","contact.domain":"ed.ac.uk"}]
