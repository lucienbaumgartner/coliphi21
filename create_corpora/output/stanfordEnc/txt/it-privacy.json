[{"date.published":"2014-11-20","date.changed":"2019-10-30","url":"https://plato.stanford.edu/entries/it-privacy/","author1":"Jeroen van den Hoven","author2":"Martijn Blaauw","author1.info":"http://homepage.tudelft.nl/e7x9k/","author2.info":"http://homepage.tudelft.nl/68x7e/","entry":"it-privacy","body.text":"\n\n\nHuman beings value their privacy and the protection of their personal\nsphere of life. They value some control over who knows what about\nthem. They certainly do not want their personal information to be\naccessible to just anyone at any time. But recent advances in\ninformation technology threaten privacy and have reduced the amount of\ncontrol over personal data and open up the possibility of a range of\nnegative consequences as a result of access to personal data. In the\nsecond half of the 20th century data protection regimes\nhave been put in place as a response to increasing levels of\nprocessing of personal data. The 21st century has become\nthe century of big data and advanced information technology (e.g.\nforms of deep learning), the rise of big tech companies and the\nplatform economy, which comes with the storage and processing of\nexabytes of data.\n\n\nThe revelations of Edward Snowden, and more recently the Cambridge\nAnalytica case (Cadwalladr & Graham-Harrison 2018) have\ndemonstrated that worries about negative consequences are real. The\ntechnical capabilities to collect, store and search large quantities\nof data concerning telephone conversations, internet searches and\nelectronic payment are now in place and are routinely used by\ngovernment agencies and corporate actors alike. The rise of China and\nthe large scale of use and spread of advanced digital technologies for\nsurveillance and control have only added to the concern of many. For\nbusiness firms, personal data about customers and potential customers\nare now also a key asset. The scope and purpose of the personal data\ncentred business models of Big Tech (Google, Amazon, Facebook,\nMicrosoft, Apple) has been described in detail by Shoshana Zuboff\n(2018) under the label “surveillance capitalism”. \n\n\nAt the same time, the meaning and value of privacy remains the subject\nof considerable controversy. The combination of increasing power of\nnew technology and the declining clarity and agreement on privacy give\nrise to problems concerning law, policy and ethics. Many of these\nconceptual debates and issues are situated in the context of\ninterpretation and analysis of the General Data Protection Regulation\n(GDPR) that was adopted by the EU in spring 2018 as the successor of\nthe EU 1995 Directives, with application far beyond the borders of the\nEuropean Union.\n\n\nThe focus of this article is on exploring the relationship between\ninformation technology and privacy. We will both illustrate the\nspecific threats that IT and innovations in IT pose for privacy and\nindicate how IT itself might be able to overcome these\nprivacy concerns by being developed in ways that can be termed\n“privacy-sensitive”, “privacy enhancing” or “privacy\nrespecting”. We will also discuss the role of emerging technologies\nin the debate, and account for the way in which moral debates are\nthemselves affected by IT.\n\nDiscussions about privacy are intertwined with the use of technology.\nThe publication that began the debate about privacy in the Western\nworld was occasioned by the introduction of the newspaper printing\npress and photography. Samuel D. Warren and Louis Brandeis wrote their\narticle on privacy in the Harvard Law Review (Warren & Brandeis\n1890) partly in protest against the intrusive activities of the\njournalists of those days. They argued that there is a “right to\nbe left alone” based on a principle of “inviolate\npersonality”. Since the publication of that article, the debate\nabout privacy has been fuelled by claims regarding the right of\nindividuals to determine the extent to which others have access to\nthem (Westin 1967) and claims regarding the right of society to know\nabout individuals. Information being a cornerstone of access to\nindividuals, the privacy debate has co-evolved with – and in\nresponse to – the development of information technology. It is\ntherefore difficult to conceive of the notions of privacy and\ndiscussions about data protection as separate from the way computers,\nthe Internet, mobile computing and the many applications of these\nbasic technologies have evolved. \nInspired by subsequent developments in U.S. law, a distinction can be\nmade between (1) constitutional (or decisional)\nprivacy and (2) tort (or informational)\nprivacy (DeCew 1997). The first refers to the freedom to make\none’s own decisions without interference by others in regard to\nmatters seen as intimate and personal, such as the decision to use\ncontraceptives or to have an abortion. The second is concerned with\nthe interest of individuals in exercising control over access to\ninformation about themselves and is most often referred to as\n“informational privacy”. Think here, for instance, about\ninformation disclosed on Facebook or other social media. All too\neasily, such information might be beyond the control of the\nindividual. \nStatements about privacy can be either descriptive or normative,\ndepending on whether they are used to describe the way people define\nsituations and conditions of privacy and the way they value them, or\nare used to indicate that there ought to be constraints on the use of\ninformation or information processing. These conditions or constraints\ntypically involve personal information regarding individuals, or ways\nof information processing that may affect individuals. Informational\nprivacy in a normative sense refers typically to a non-absolute moral\nright of persons to have direct or indirect control over access to (1)\ninformation about oneself, (2) situations in which others could\nacquire information about oneself, and (3) technology that can be used\nto generate, process or disseminate information about oneself. \nThe debates about privacy are almost always revolving around new\ntechnology, ranging from genetics and the extensive study of\nbio-markers, brain imaging, drones, wearable sensors and sensor\nnetworks, social media, smart phones, closed circuit television, to\ngovernment cybersecurity programs, direct marketing, RFID tags, Big\nData, head-mounted displays and search engines. There are basically\ntwo reactions to the flood of new technology and its impact on\npersonal information and privacy: the first reaction, held by many\npeople in IT industry and in R&D, is that we have zero privacy in\nthe digital age and that there is no way we can protect it, so we\nshould get used to the new world and get over it (Sprenger 1999). The other reaction\nis that our privacy is more important than ever and that we can and we\nmust attempt to protect it. \nIn the literature on privacy, there are many competing accounts of the\nnature and value of privacy (Negley 1966, Rössler 2005). On one end of\nthe spectrum, reductionist accounts argue that privacy claims\nare really about other values and other things that matter from a\nmoral point of view. According to these views the value of privacy is\nreducible to these other values or sources of value (Thomson 1975).\nProposals that have been defended along these lines mention property\nrights, security, autonomy, intimacy or friendship, democracy,\nliberty, dignity, or utility and economic value. Reductionist accounts\nhold that the importance of privacy should be explained and its\nmeaning clarified in terms of those other values and sources of value\n(Westin 1967). The opposing view holds that privacy is valuable in\nitself and its value and importance are not derived from other\nconsiderations (see for a discussion Rössler 2004). Views that\nconstrue privacy and the personal sphere of life as a human right\nwould be an example of this non-reductionist conception. \nMore recently a type of privacy account has been proposed in relation\nto new information technology, which acknowledges that there is a\ncluster of related moral claims underlying appeals to privacy, but\nmaintains that there is no single essential core of privacy\nconcerns. This approach is referred to as cluster accounts (DeCew\n1997; Solove 2006; van den Hoven 1999; Allen 2011; Nissenbaum\n2004). \nFrom a descriptive perspective, a recent further addition to the body of\nprivacy accounts are epistemic accounts, where the notion of privacy\nis analyzed primarily in terms of knowledge or other epistemic states.\nHaving privacy means that others don’t know certain private\npropositions; lacking privacy means that others do know certain\nprivate propositions (Blaauw 2013). An important aspect of this\nconception of having privacy is that it is seen as a relation (Rubel\n2011; Matheson 2007; Blaauw 2013) with three argument places: a\nsubject (S), a set of propositions (P) and a set of\nindividuals (I). Here S is the subject who has (a\ncertain degree of) privacy. P is composed of those propositions\nthe subject wants to keep private (call the propositions in this set\n‘personal propositions’), and I is composed of\nthose individuals with respect to whom S wants to keep the\npersonal propositions private. \nAnother distinction that is useful to make is the one between a\nEuropean and a US American approach. A bibliometric study suggests\nthat the two approaches are separate in the literature. The first\nconceptualizes issues of informational privacy in terms of ‘data\nprotection’, the second in terms of ‘privacy’\n(Heersmink et al. 2011). In discussing the relationship of privacy\nmatters with technology, the notion of data protection is most\nhelpful, since it leads to a relatively clear picture of what the\nobject of protection is and by which technical means the data can be\nprotected. At the same time it invites answers to the question why the\ndata ought to be protected, pointing to a number of distinctive moral\ngrounds on the basis of which technical, legal and institutional\nprotection of personal data can be justified. Informational privacy is\nthus recast in terms of the protection of personal data (van den Hoven\n2008). This account shows how Privacy, Technology and Data Protection\nare related, without conflating Privacy and Data Protection. \nPersonal information or data is information or data that is linked or\ncan be linked to individual persons. Examples include explicitly\nstated characteristics such as a person‘s date of birth, sexual\npreference, whereabouts, religion, but also the IP address of your\ncomputer or metadata pertaining to these kinds of information. In\naddition, personal data can also be more implicit in the form of\nbehavioural data, for example from social media, that can be linked to\nindividuals. Personal data can be contrasted with data that is\nconsidered sensitive, valuable or important for other reasons, such as\nsecret recipes, financial data, or military intelligence. Data used to\nsecure other information, such as passwords, are not considered\nhere. Although such security measures (passwords) may contribute to\nprivacy, their protection is only instrumental to the protection of\nother (more private) information, and the quality of such security\nmeasures is therefore out of the scope of our considerations here. \nA relevant distinction that has been made in philosophical semantics\nis that between the referential and the attributive use of descriptive\nlabels of persons (van den Hoven 2008). Personal data is defined in\nthe law as data that can be linked with a natural person. There are\ntwo ways in which this link can be made; a referential mode and a\nnon-referential mode. The law is primarily concerned with the\n‘referential use’ of descriptions or attributes, the type\nof use that is made on the basis of a (possible) acquaintance\nrelationship of the speaker with the object of his knowledge.\n“The murderer of Kennedy must be insane”, uttered while\npointing to him in court is an example of a referentially used\ndescription. This can be contrasted with descriptions that are used\nattributively as in “the murderer of Kennedy must be insane,\nwhoever he is”. In this case, the user of the description is\nnot – and may never be – acquainted with the person he is\ntalking about or intends to refer to. If the legal definition of\npersonal data is interpreted referentially, much of the data that\ncould at some point in time be brought to bear on persons would be\nunprotected; that is, the processing of this data would not be\nconstrained on moral grounds related to privacy or personal sphere of\nlife, since it does not “refer” to persons in a straightforward\nway and therefore does not constitute “personal data” in a strict\nsense.  \nThe following types of moral reasons for the protection of personal\ndata and for providing direct or indirect control over access to those\ndata by others can be distinguished (van den Hoven 2008): \nThese considerations all provide good moral reasons for limiting and\nconstraining access to personal data and providing individuals with\ncontrol over their data. \nAcknowledging that there are moral reasons for protecting personal\ndata, data protection laws are in force in almost all countries. The\nbasic moral principle underlying these laws is the requirement of\ninformed consent for processing by the data subject, providing the\nsubject (at least in principle) with control over potential negative\neffects as discussed above. Furthermore, processing of personal\ninformation requires that its purpose be specified, its use be\nlimited, individuals be notified and allowed to correct inaccuracies,\nand the holder of the data be accountable to oversight authorities\n(OECD 1980). Because it is impossible to guarantee compliance of all\ntypes of data processing in all these areas and applications with\nthese rules and laws in traditional ways, so-called\n“privacy-enhancing technologies” (PETs) and identity management\nsystems are expected to replace human oversight in many cases. The\nchallenge with respect to privacy in the twenty-first century is to\nassure that technology is designed in such a way that it incorporates\nprivacy requirements in the software, architecture, infrastructure,\nand work processes in a way that makes privacy violations unlikely to\noccur. New generations of privacy regulations (e.g. GDPR) now require\nstandardly a “privacy by design” approach. The data ecosystems and\nsocio-technical systems, supply chains, organisations, including\nincentive structures, business processes, and technical hardware and\nsoftware, training of personnel, should all be designed in such a way that\nthe likelihood of privacy violations is a low as possible. \nThe debates about privacy are almost always revolving around new\ntechnology, ranging from genetics and the extensive study of\nbio-markers, brain imaging, drones, wearable sensors and sensor\nnetworks, social media, smart phones, closed circuit television, to\ngovernment cybersecurity programs, direct marketing, surveillance,\nRFID tags, big data, head-mounted displays and search engines. The\nimpact of some of these new technologies, with a particular focus on\ninformation technology, is discussed in this section. \n“Information technology” refers to automated systems for\nstoring, processing, and distributing information. Typically, this\ninvolves the use of computers and communication networks. The amount\nof information that can be stored or processed in an information\nsystem depends on the technology used. The capacity of the technology\nhas increased rapidly over the past decades, in accordance with\nMoore’s law. This holds for storage capacity, processing capacity, and\ncommunication bandwidth. We are now capable of storing and processing\ndata on the exabyte level. For illustration, to store 100 exabytes of\ndata on 720 MB CD-ROM discs would require a stack of them that would\nalmost reach the moon. \nThese developments have fundamentally changed our practices of\ninformation provisioning. The rapid changes have increased the need\nfor careful consideration of the desirability of effects. Some even\nspeak of a digital revolution as a technological leap similar to the\nindustrial revolution, or a digital revolution as a revolution in\nunderstanding human nature and the world, similar to the revolutions\nof Copernicus, Darwin and Freud (Floridi 2008). In both the technical\nand the epistemic sense, emphasis has been put on connectivity and\ninteraction. Physical space has become less important, information is\nubiquitous, and social relations have adapted as well. \nAs we have described privacy in terms of moral reasons for imposing\nconstraints on access to and/or use of personal information, the\nincreased connectivity imposed by information technology poses many\nquestions. In a descriptive sense, access has increased, which, in a\nnormative sense, requires consideration of the desirability of this\ndevelopment, and evaluation of the potential for regulation by\ntechnology (Lessig 1999), institutions, and/or law. \nAs connectivity increases access to information, it also increases the\npossibility for agents to act based on the new sources of\ninformation. When these sources contain personal information, risks of\nharm, inequality, discrimination, and loss of autonomy easily emerge.\nFor example, your enemies may have less difficulty finding out where\nyou are, users may be tempted to give up privacy for perceived\nbenefits in online environments, and employers may use online\ninformation to avoid hiring certain groups of people. Furthermore,\nsystems rather than users may decide which information is displayed,\nthus confronting users only with news that matches their profiles. \nAlthough the technology operates on a device level, information\ntechnology consists of a complex system of socio-technical practices,\nand its context of use forms the basis for discussing its role in\nchanging possibilities for accessing information, and thereby\nimpacting privacy. We will discuss some specific developments and\ntheir impact in the following sections. \nThe Internet, originally conceived in the 1960s and developed in the\n1980s as a scientific network for exchanging information, was not\ndesigned for the purpose of separating information flows (Michener\n1999). The World Wide Web of today was not foreseen, and neither was\nthe possibility of misuse of the Internet. Social network sites\nemerged for use within a community of people who knew each other in\nreal life – at first, mostly in academic settings – rather\nthan being developed for a worldwide community of users (Ellison\n2007). It was assumed that sharing with close friends would not cause\nany harm, and privacy and security only appeared on the agenda when\nthe network grew larger. This means that privacy concerns often had to\nbe dealt with as add-ons rather than by-design. \nA major theme in the discussion of Internet privacy revolves around\nthe use of cookies (Palmer 2005). Cookies are small pieces of data\nthat web sites store on the user’s computer, in order to enable\npersonalization of the site. However, some cookies can be used to\ntrack the user across multiple web sites (tracking cookies), enabling\nfor example advertisements for a product the user has recently viewed\non a totally different site. Again, it is not always clear what the\ngenerated information is used for. Laws requiring user consent for the\nuse of cookies are not always successful in terms of increasing the\nlevel of control, as the consent requests interfere with task flows,\nand the user may simply click away any requests for consent (Leenes\n& Kosta 2015). Similarly, features of social network sites\nembedded in other sites (e.g. “like”-button) may allow\nthe social network site to identify the sites visited by the user\n(Krishnamurthy & Wills 2009). \nThe recent development of cloud computing increases the many privacy\nconcerns (Ruiter & Warnier 2011). Previously, whereas information\nwould be available from the web, user data and programs would still be\nstored locally, preventing program vendors from having access to the\ndata and usage statistics. In cloud computing, both data and programs\nare online (in the cloud), and it is not always clear what the\nuser-generated and system-generated data are used for. Moreover, as\ndata are located elsewhere in the world, it is not even always obvious\nwhich law is applicable, and which authorities can demand access to\nthe data. Data gathered by online services and apps such as search\nengines and games are of particular concern here. Which data are used\nand communicated by applications (browsing history, contact lists,\netc.) is not always clear, and even when it is, the only choice\navailable to the user may be not to use the application. \nSome special features of Internet privacy (social media and big data)\nare discussed in the following sections. \nSocial media pose additional challenges. The question is not merely\nabout the moral reasons for limiting access to information, it is also\nabout the moral reasons for limiting the invitations to users\nto submit all kinds of personal information. Social network sites\ninvite the user to generate more data, to increase the value of the\nsite (“your profile is …% complete”). Users are\ntempted to exchange their personal data for the benefits of\nusing services, and provide both this data and their attention as\npayment for the services. In addition, users may not even be aware of\nwhat information they are tempted to provide, as in the aforementioned\ncase of the “like”-button on other sites. Merely limiting\nthe access to personal information does not do justice to the issues\nhere, and the more fundamental question lies in steering the users’\nbehaviour of sharing. When the service is free, the data is needed as a form of payment. \nOne way of limiting the temptation of users to share is requiring\ndefault privacy settings to be strict. Even then, this limits access\nfor other users (“friends of friends”), but it does not\nlimit access for the service provider. Also, such restrictions limit\nthe value and usability of the social network sites themselves, and\nmay reduce positive effects of such services. A particular example of\nprivacy-friendly defaults is the opt-in as opposed to the opt-out\napproach. When the user has to take an explicit action to share data\nor to subscribe to a service or mailing list, the resulting effects\nmay be more acceptable to the user. However, much still depends on how\nthe choice is framed (Bellman, Johnson, & Lohse 2001). \nUsers generate loads of data when online. This is not only data\nexplicitly entered by the user, but also numerous statistics on user\nbehavior: sites visited, links clicked, search terms entered, etc. Data\nmining can be employed to extract patterns from such data, which can\nthen be used to make decisions about the user. These may only affect\nthe online experience (advertisements shown), but, depending on which\nparties have access to the information, they may also impact the user\nin completely different contexts. \nIn particular, big data may be used in profiling the user (Hildebrandt\n2008), creating patterns of typical combinations of user properties,\nwhich can then be used to predict interests and behavior. An innocent\napplication is “you may also like …”, but,\ndepending on the available data, more sensitive derivations may be\nmade, such as most probable religion or sexual preference. These\nderivations could then in turn lead to inequal treatment or\ndiscrimination.  When a user can be assigned to a particular group,\neven only probabilistically, this may influence the actions taken by\nothers (Taylor, Floridi, & Van der Sloot 2017). For example,\nprofiling could lead to refusal of insurance or a credit card, in\nwhich case profit is the main reason for discrimination. When such\ndecisions are based on profiling, it may be difficult to challenge\nthem or even find out the explanations behind them. Profiling could\nalso be used by organizations or possible future governments that have\ndiscrimination of particular groups on their political agenda, in\norder to find their targets and deny them access to services, or\nworse. \nBig data does not only emerge from Internet transactions. Similarly,\ndata may be collected when shopping, when being recorded by\nsurveillance cameras in public or private spaces, or when using\nsmartcard-based public transport payment systems. All these data could\nbe used to profile citizens, and base decisions upon such profiles.\nFor example, shopping data could be used to send information about\nhealthy food habits to particular individuals, but again also for\ndecisions on insurance. According to EU data protection law,\npermission is needed for processing personal data, and they can only\nbe processed for the purpose for which they were obtained. Specific\nchallenges, therefore, are (a) how to obtain permission when the user\ndoes not explicitly engage in a transaction (as in case of\nsurveillance), and (b) how to prevent “function creep”,\ni.e. data being used for different purposes after they are collected\n(as may happen for example with DNA databases (Dahl & Sætnan\n2009). \nOne particular concern could emerge from genetics and genomic data\n(Tavani 2004, Bruynseels & van den Hoven, 2015). Like other data, genomics can be used to make predictions, and in particular could predict risks of diseases.\nApart from others having access to detailed user profiles, a\nfundamental question here is whether the individual should know what\nis known about her. In general, users could be said to have a right to\naccess any information stored about them, but in this case, there may\nalso be a right not to know, in particular when knowledge of the data\n(e.g. risks of diseases) would reduce the well-being – by causing\nfear, for instance – without enabling treatment. With respect to\nprevious examples, one may not want to know the patterns in one’s own\nshopping behavior either. \nAs users increasingly own networked devices such as smart phones,\nmobile devices collect and send more and more data. These devices\ntypically contain a range of data-generating sensors, including GPS\n(location), movement sensors, and cameras, and may transmit the\nresulting data via the Internet or other networks. One particular\nexample concerns location data. Many mobile devices have a GPS sensor\nthat registers the user’s location, but even without a GPS sensor,\napproximate locations can be derived, for example by monitoring the\navailable wireless networks. As location data links the online world\nto the user’s physical environment, with the potential of physical\nharm (stalking, burglary during holidays, etc.), such data are often\nconsidered particularly sensitive. \nMany of these devices also contain cameras which, when applications\nhave access, can be used to take pictures. These can be considered\nsensors as well, and the data they generate may be particularly\nprivate. For sensors like cameras, it is assumed that the user is\naware when they are activated, and privacy depends on such knowledge.\nFor webcams, a light typically indicates whether the camera is on, but\nthis light may be manipulated by malicious software. In general,\n“reconfigurable technology” (Dechesne, Warnier, & van\nden Hoven 2011) that handles personal data raises the question of user\nknowledge of the configuration. \nDevices connected to the Internet are not limited to user-owned\ncomputing devices like smartphones. Many devices contain chips and/or\nare connected in the so-called Internet of Things. RFID (radio\nfrequency identification) chips can be read from a limited distance,\nsuch that you can hold them in front of a reader rather than inserting\nthem. EU and US passports have RFID chips with protected biometric\ndata, but information like the user’s nationality may easily leak when\nattempting to read such devices (see Richter, Mostowski & Poll\n2008, in Other Internet Resources). “Smart” RFIDs are also\nembedded in public transport payment systems. “Dumb”\nRFIDs, basically only containing a number, appear in many kinds of\nproducts as a replacement of the barcode, and for use in logistics.\nStill, such chips could be used to trace a person once it is known\nthat he carries an item containing a chip. \nIn the home, there are smart meters for automatically reading and\nsending electricity and water consumption, and thermostats and other devices\nthat can be remotely controlled by the owner. Such devices again\ngenerate statistics, and these can be used for mining and profiling.\nIn the future, more and more household appliances will be connected,\neach generating its own information. Ambient intelligence (Brey 2005),\nand ubiquitous computing, along with the Internet of Things\n(Friedewald & Raabe 2011), also enable automatic adaptation of the\nenvironment to the user, based on explicit preferences and implicit\nobservations, and user autonomy is a central theme in considering the\nprivacy implications of such devices. In general, the move towards a\nservice-oriented provisioning of goods, with suppliers being informed\nabout how the products are used through IT and associated\nconnectivity, requires consideration of the associated privacy and\ntransparency concerns (Pieters 2013). For example, users will need to\nbe informed when connected devices contain a microphone and how and\nwhen it is used. \nGovernment and public administration have undergone radical\ntransformations as a result of the availability of advanced IT systems\nas well. Examples of these changes are biometric passports, online\ne-government services, voting systems, a variety of online citizen\nparticipation tools and platforms or online access to recordings of\nsessions of parliament and government committee meetings.  \nConsider the case of voting in elections. Information technology may\nplay a role in different phases in the voting process, which may have\ndifferent impact on voter privacy. Most countries have a requirement\nthat elections are to be held by secret ballot, to prevent vote buying\nand coercion. In this case, the voter is supposed to keep her vote\nprivate, even if she would want to reveal it. For information\ntechnology used for casting votes, this is defined as the requirement\nof receipt-freeness or coercion-resistance (Delaune, Kremer & Ryan\n2006). In polling stations, the authorities see to it that the voter\nkeeps the vote private, but such surveillance is not possible when\nvoting by mail or online, and it cannot even be enforced by\ntechnological means, as someone can always watch while the voter\nvotes. In this case, privacy is not only a right but also a duty, and\ninformation technology developments play an important role in the\npossibilities of the voter to fulfill this duty, as well as the\npossibilities of the authorities to verify this. In a broader sense,\ne-democracy initiatives may change the way privacy is viewed in the\npolitical process. \nMore generally, privacy is important in democracy to prevent undue\ninfluence. While lack of privacy in the voting process could enable\nvote buying and coercion, there are more subtle ways of influencing\nthe democratic process, for example through targeted (mis)information\ncampaigns. Online (political) activities of citizens on for example\nsocial media facilitate such attempts because of the possibility of\ntargeting through behavioural profiling. Compared to offline political\nactivities, it is more difficult to hide preferences and activities,\nbreaches of confidentiality are more likely, and attempts to influence\nopinions become more scalable. \nInformation technology is used for all kinds of surveillance tasks. It\ncan be used to augment and extend traditional surveillance systems\nsuch as CCTV and other camera systems, for example to identify\nspecific individuals in crowds, using face recognition techniques, or\nto monitor specific places for unwanted behaviour. Such approaches\nbecome even more powerful when combined with other techniques, such as\nmonitoring of Internet-of-Things devices (Motlagh et al. 2017). \nBesides augmenting existing surveillance systems, ICT techniques are\nnowadays mainly used in the digital domain, typically grouped together\nunder the term “surveillance capitalism” (Zuboff 2019). Social\nmedia and other online systems are used to gather large amounts of\ndata about individuals – either “voluntary”, because users\nsubscribe to a specific service (Google, Facebook), or involuntary by\ngathering all kinds of user related data in a less transparent manner.\nData analysis and machine learning techniques are then used to\ngenerate prediction models of individual users that can be used, for\nexample, for targeted advertisement, but also for more malicious\nintents such as fraud or micro-targeting to influence elections\n(Albright 2016, Other Internet Resources) or referenda such as Brexit\n(Cadwalladr 2019, Other Internet Resources). \nIn addition to the private sector surveillance industry, governments\nform another traditional group that uses surveillance techniques at a\nlarge scale, either by intelligence services or law enforcement. These\ntypes of surveillance systems are typically justified with an appeal\nto the “greater good” and protecting citizens, but their\nuse is also controversial. For such systems, one would typically like\nto ensure that any negative effects on privacy are proportional to the\nbenefits achieved by the technology. Especially since these systems\nare typically shrouded in secrecy, it is difficult for outsiders to\nsee if such systems are used proportionally, or indeed useful for\ntheir tasks (Lawner 2002). This is particularly pressing when\ngovernments use private sector data or services for surveillance\npurposes.  The almost universal use of good encryption techniques\nin communication systems makes it also harder to gather effective\nsurveillance information, leading to more and more calls for “back\ndoors” that can exclusively be used by government in communication\nsystems. From a privacy standpoint this could be evaluated as\nunwanted, not only because it gives governments access to private\nconversations, but also because it lowers the overall security of\ncommunication systems that employ this technique (Abelson et al.\n2015). \nWhereas information technology is typically seen as the cause\nof privacy problems, there are also several ways in which information\ntechnology can help to solve these problems. There are rules,\nguidelines or best practices that can be used for designing\nprivacy-preserving systems. Such possibilities range from\nethically-informed design methodologies to using encryption to protect\npersonal information from unauthorized use. In particular, methods\nfrom the field of information security, aimed at protecting\ninformation against unauthorized access, can play a key role in the\nprotection of personal data. \nValue sensitive design provides a “theoretically grounded\napproach to the design of technology that accounts for human values in\na principled and comprehensive manner throughout the design\nprocess” (Friedman et al. 2006). It provides a set of rules and\nguidelines for designing a system with a certain value in mind. One\nsuch value can be ‘privacy’, and value sensitive design\ncan thus be used as a method to design privacy-friendly IT systems\n(Van den Hoven et al. 2015).  The ‘privacy by design’\napproach as advocated by Cavoukian (2009) and others can be regarded\nas one of the value sensitive design approaches that specifically\nfocuses on privacy (Warnier et al. 2015). More recently, approaches\nsuch as “privacy engineering” (Ceross & Simpson 2018)\nextend the privacy by design approach by aiming to provide a more\npractical, deployable set of methods by which to achieve system-wide\nprivacy. \nThe privacy by design approach provides high-level guidelines in the\nform of principles for designing privacy-preserving systems.\nThese principles have at their core that “data protection needs\nto be viewed in proactive rather than reactive terms, making privacy\nby design preventive and not simply remedial” (Cavoukian 2010).\nPrivacy by design’s main point is that data protection should be\ncentral in all phases of product life cycles, from initial design to\noperational use and disposal (see Colesky et al. 2016) for a critical\nanalysis of the privacy by design approach). The Privacy Impact\nAssessment approach proposed by Clarke (2009) makes a similar point.\nIt proposes “a systematic process for evaluating the potential\neffects on privacy of a project, initiative or proposed system or\nscheme” (Clarke 2009). Note that these approaches should not\nonly be seen as auditing approaches, but rather as a means to make\nprivacy awareness and compliance an integral part of the\norganizational and engineering culture. \nThere are also several industry guidelines that can be used to design\nprivacy preserving IT systems. The Payment Card Industry Data Security\nStandard (see PCI DSS v3.2, 2018, in the Other Internet Resources),\nfor example, gives very clear guidelines for privacy and security\nsensitive systems design in the domain of the credit card industry and\nits partners (retailers, banks). Various International Organization\nfor Standardization (ISO) standards (Hone & Eloff 2002) also serve\nas a source of best practices and guidelines, especially with respect\nto information security, for the design of privacy friendly systems.\nFurthermore, the principles that are formed by the EU Data Protection\nDirective, which are themselves based on the Fair Information\nPractices (Gellman 2014) from the early 70s – transparency,\npurpose, proportionality, access, transfer – are technologically\nneutral and as such can also be considered as high level ‘design\nprinciples’. Systems that are designed with these rules and\nguidelines in mind should thus – in principle – be in\ncompliance with EU privacy laws and respect the privacy of its\nusers. \nThe rules and principles described above give high-level guidance for\ndesigning privacy-preserving systems, but this does not mean that if\nthese methodologies are followed the resulting IT system will\n(automatically) be privacy friendly. Some design principles are rather\nvague and abstract. What does it mean to make a transparent design or\nto design for proportionality? The principles need to be interpreted\nand placed in a context when designing a specific system. But\ndifferent people will interpret the principles differently, which will\nlead to different design choices, with different effects on privacy.\nThere is also a difference between the design and the implementation\nof a computer system. During the implementation phase software bugs\nare introduced, some of which can be exploited to break the system and\nextract private information. How to implement bug-free computer\nsystems remains an open research question (Hoare 2003). In addition,\nimplementation is another phase wherein choices and interpretations\nare made: system designs can be implemented in infinitely many ways.\nMoreover, it is very hard to verify – for anything beyond\nnon-trivial systems – whether an implementation meets its\ndesign/specification (Loeckx, Sieber, & Stansifer 1985). This is\neven more difficult for non-functional requirements such as\n‘being privacy preserving’ or security properties in\ngeneral.  \nSome specific solutions to privacy problems aim at increasing the\nlevel of awareness and consent of the user. These solutions can be\nseen as an attempt to apply the notion of informed consent to privacy\nissues with technology (Custers et al. 2018). This is connected to the\nidea that privacy settings and policies should be explainable to users\n(Pieters 2011). For example, the Privacy Coach supports customers in\nmaking privacy decisions when confronted with RFID tags (Broenink et\nal. 2010). However, users have only a limited capability of dealing\nwith such choices, and providing too many choices may easily lead to\nthe problem of moral overload (van den Hoven, Lokhorst, & Van de\nPoel 2012). A technical solution is support for automatic matching of\na privacy policy set by the user against policies issued by web sites\nor apps. \nA growing number of software tools are available that provide some\nform of privacy (usually anonymity) for their users, such tools are\ncommonly known as privacy enhancing technologies (Danezis &\nGürses 2010, Other Internet Resources). Examples include\ncommunication-anonymizing tools such as Tor (Dingledine, Mathewson,\n& Syverson 2004) and Freenet (Clarke et al. 2001), and\nidentity-management systems for which many commercial software\npackages exist (see below). Communication anonymizing tools allow\nusers to anonymously browse the web (with Tor) or anonymously share\ncontent (Freenet). They employ a number of cryptographic techniques\nand security protocols in order to ensure their goal of anonymous\ncommunication. Both systems use the property that numerous users use\nthe system at the same time which provides k-anonymity (Sweeney\n2002): no individual can be uniquely distinguished from a group of\nsize k, for large values for k. Depending on the system,\nthe value of k can vary between a few hundred to hundreds of\nthousands. In Tor, messages are encrypted and routed along numerous\ndifferent computers, thereby obscuring the original sender of the\nmessage (and thus providing anonymity). Similarly, in Freenet content\nis stored in encrypted form from all users of the system. Since users\nthemselves do not have the necessary decryption keys, they do not know\nwhat kind of content is stored, by the system, on their own computer.\nThis provides plausible deniability and privacy. The system can at any\ntime retrieve the encrypted content and send it to different Freenet\nusers. \nPrivacy enhancing technologies also have their downsides. For example,\nTor, the tool that allows anonymized communication and browsing over\nthe Internet, is susceptible to an attack whereby, under certain\ncircumstances, the anonymity of the user is no longer guaranteed\n(Back, Möller, & Stiglic 2001; Evans, Dingledine, &\nGrothoff 2009). Freenet (and other tools) have similar problems\n(Douceur 2002). Note that for such attacks to work, an attacker needs\nto have access to large resources that in practice are only realistic\nfor intelligence agencies of countries. However, there are other\nrisks. Configuring such software tools correctly is difficult for the\naverage user, and when the tools are not correctly configured\nanonymity of the user is no longer guaranteed. And there is always the\nrisk that the computer on which the privacy-preserving software runs\nis infected by a Trojan horse (or other digital pest) that monitors\nall communication and knows the identity of the user. \nAnother option for providing anonymity is the anonymization of data\nthrough special software. Tools exist that remove patient names and\nreduce age information to intervals: the age 35 is then represented as\nfalling in the range 30–40. The idea behind such anonymization\nsoftware is that a record can no longer be linked to an individual,\nwhile the relevant parts of the data can still be used for scientific\nor other purposes. The problem here is that it is very hard to\nanonymize data in such a way that all links with an individual are\nremoved and the resulting anonymized data is still useful for research\npurposes. Researchers have shown that it is almost always possible to\nreconstruct links with individuals by using sophisticated statistical\nmethods (Danezis, Diaz, & Troncoso 2007) and by combining multiple\ndatabases (Anderson 2008) that contain personal information.\nTechniques such as k-anonymity might also help to generalize\nthe data enough to make it unfeasible to de-anonymize data (LeFevre et\nal. 2005). \nCryptography has long been used as a means to protect data, dating\nback to the Caesar cipher more than two thousand years ago. Modern\ncryptographic techniques are essential in any IT system that needs to\nstore (and thus protect) personal data, for example by providing\nsecure (confidential) connections for browsing (HTTPS) and networking\n(VPN). Note however that by itself cryptography does not provide any\nprotection against data breaching; only when applied correctly in a\nspecific context does it become a ‘fence’ around personal\ndata. In addition, cryptographic schemes that become outdated by\nfaster computers or new attacks may pose threats to (long-term)\nprivacy. Cryptography is a large\nfield, so any description here will be incomplete. The focus will be\ninstead on some newer cryptographic techniques, in particular\nhomomorphic encryption, that have the potential to become very\nimportant for processing and searching in personal data. \nVarious techniques exist for searching through encrypted data (Song et\nal. 2000, Wang et al. 2016), which provides a form of privacy\nprotection (the data is encrypted) and selective access to sensitive\ndata. One relatively new technique that can be used for designing\nprivacy-preserving systems is ‘homomorphic encryption’\n(Gentry 2009, Acar et al. 2018). Homomorphic encryption allows a data\nprocessor to process encrypted data, i.e. users could send personal\ndata in encrypted form and get back some useful results – for\nexample, recommendations of movies that online friends like – in\nencrypted form. The original user can then again decrypt the result\nand use it without revealing any personal data to the data processor.\nHomomorphic encryption, for example, could be used to aggregate\nencrypted data thereby allowing both privacy protection and useful\n(anonymized) aggregate information. The technique is currently not\nwidely applied; there are serious performance issues if one wants to\napply full homomorphic encryption to the large amounts of data stored\nin today’s systems. However, variants of the original homomorphic\nencryption scheme are emerging, such as Somewhat Homomorphic\nEncryption (Badawi et al. 2018), that are showing promise to be more\nwidely applied in practice. \nThe main idea behind blockchain technology was first described in the\nseminal paper on Bitcoins (Nakamoto, n.d., Other Internet Resources).\nA blockchain is basically a distributed ledger that stores\ntransactions in a non-reputable way, without the use of a trusted\nthird party. Cryptography is used to ensure that all transactions are\n“approved” by members of the blockchain and stored in such a way\nthat they are linked to previous transactions and cannot be removed.\nAlthough focused on data integrity and not inherently anonymous,\nblockchain technology enables many privacy-related applications\n(Yli-Huumo et al. 2016, Karame and Capkun 2018), such as anonymous\ncryptocurrency (Narayanan et al. 2016) and self-sovereign identity\n(see below). \nThe use and management of user’s online identifiers are crucial in the\ncurrent Internet and social networks. Online reputations become more\nand more important, both for users and for companies. In the era of\nbig data correct information about users has an\nincreasing monetary value. \n‘Single sign on’ frameworks, provided by independent third\nparties (OpenID) but also by large companies such as Facebook,\nMicrosoft and Google (Ko et al. 2010), make it easy for users to\nconnect to numerous online services using a single online identity.\nThese online identities are usually directly linked to the real world\n(off line) identities of individuals; indeed Facebook, Google and\nothers require this form of log on (den Haak 2012). Requiring a direct\nlink between online and ‘real world’ identities is\nproblematic from a privacy perspective, because they allow profiling\nof users (Benevenuto et al. 2012). Not all users will realize how\nlarge the amount of data is that companies gather in this manner, or\nhow easy it is to build a detailed profile of users. Profiling becomes\neven easier if the profile information is combined with other\ntechniques such as implicit authentication via cookies and tracking\ncookies (Mayer & Mitchell 2012). \nFrom a privacy perspective a better solution would be the use of\nattribute-based authentication (Goyal et al. 2006) which allows access\nof online services based on the attributes of users, for example their\nfriends, nationality, age etc. Depending on the attributes used, they\nmight still be traced back to specific individuals, but this is no\nlonger crucial. In addition, users can no longer be tracked to\ndifferent services because they can use different attributes to access\ndifferent services which makes it difficult to trace online identities\nover multiple transactions, thus providing unlinkability for the user.\nRecently (Allen 2016, Other Internet Resources), the concept of\nself-sovereign identity has emerged, which aims for users to have\ncomplete ownership and control about their own digital identities.\nBlockchain technology is used to make it possible for users to control\na digital identity without the use of a traditional trusted third\nparty (Baars 2016). \nIn the previous sections, we have outlined how current technologies\nmay impact privacy, as well as how they may contribute to mitigating\nundesirable effects. However, there are future and emerging\ntechnologies that may have an even more profound impact. Consider for\nexample brain-computer interfaces. In case computers are connected\ndirectly to the brain, not only behavioral characteristics are subject\nto privacy considerations, but even one’s thoughts run the risk of\nbecoming public, with decisions of others being based upon them. In\naddition, it could become possible to change one’s behavior by means\nof such technology. Such developments therefore require further\nconsideration of the reasons for protecting privacy. In particular,\nwhen brain processes could be influenced from the outside, autonomy\nwould be a value to reconsider to ensure adequate protection. \nApart from evaluating information technology against current moral\nnorms, one also needs to consider the possibility that technological\nchanges influence the norms themselves (Boenink, Swierstra &\nStemerding 2010). Technology thus does not only influence privacy by\nchanging the accessibility of information, but also by changing the\nprivacy norms themselves. For example, social networking sites invite\nusers to share more information than they otherwise might. This\n“oversharing” becomes accepted practice within certain\ngroups. With future and emerging technologies, such influences can\nalso be expected and therefore they ought to be taken into account\nwhen trying to mitigate effects. \nAnother fundamental question is whether, given the future (and even\ncurrent) level of informational connectivity, it is feasible to\nprotect privacy by trying to hide information from parties who may use\nit in undesirable ways. Gutwirth & De Hert (2008) argue that it\nmay be more feasible to protect privacy by transparency – by\nrequiring actors to justify decisions made about individuals, thus\ninsisting that decisions are not based on illegitimate information.\nThis approach comes with its own problems, as it might be hard to\nprove that the wrong information was used for a decision. Still, it\nmay well happen that citizens, in turn, start data collection on those\nwho collect data about them, e.g. governments. Such\n“counter(sur)veillance” may be used to gather information\nabout the use of information, thereby improving accountability\n(Gürses et al. 2016). The open source movement may also\ncontribute to transparency of data processing. In this context,\ntransparency can be seen as a pro-ethical condition contributing to\nprivacy (Turilli & Floridi 2009). \nIt has been argued that the precautionary principle, well known in\nenvironmental ethics, might have a role in dealing with emerging\ninformation technologies as well (Pieters & van Cleeff 2009; Som,\nHilty & Köhler 2009). The principle would see to it that the\nburden of proof for absence of irreversible effects of information\ntechnology on society, e.g. in terms of power relations and equality,\nwould lie with those advocating the new technology. Precaution, in\nthis sense, could then be used to impose restrictions at a regulatory\nlevel, in combination with or as an alternative to empowering users,\nthereby potentially contributing to the prevention of informational\noverload on the user side. Apart from general debates about the\ndesirable and undesirable features of the precautionary principle,\nchallenges to it lie in its translation to social effects and social\nsustainability, as well as to its application to consequences induced\nby intentional actions of agents. Whereas the occurrence of natural\nthreats or accidents is probabilistic in nature, those who are\ninterested in improper use of information behave strategically,\nrequiring a different approach to risk (i.e. security as opposed to\nsafety). In addition, proponents of precaution will need to balance it\nwith other important principles, viz., of informed consent and\nautonomy. \nFinally, it is appropriate to note that not all social effects of\ninformation technology concern privacy (Pieters 2017). Examples\ninclude the effects of social network sites on friendship, and the\nverifiability of results of electronic elections. Therefore,\nvalue-sensitive design approaches and impact assessments of\ninformation technology should not focus on privacy only, since\ninformation technology affects many other values as well.","contact.mail":"m.j.vandenhoven@tudelft.nl","contact.domain":"tudelft.nl"},{"date.published":"2014-11-20","date.changed":"2019-10-30","url":"https://plato.stanford.edu/entries/it-privacy/","author1":"Jeroen van den Hoven","author2":"Martijn Blaauw","author1.info":"http://homepage.tudelft.nl/e7x9k/","author2.info":"http://homepage.tudelft.nl/68x7e/","entry":"it-privacy","body.text":"\n\n\nHuman beings value their privacy and the protection of their personal\nsphere of life. They value some control over who knows what about\nthem. They certainly do not want their personal information to be\naccessible to just anyone at any time. But recent advances in\ninformation technology threaten privacy and have reduced the amount of\ncontrol over personal data and open up the possibility of a range of\nnegative consequences as a result of access to personal data. In the\nsecond half of the 20th century data protection regimes\nhave been put in place as a response to increasing levels of\nprocessing of personal data. The 21st century has become\nthe century of big data and advanced information technology (e.g.\nforms of deep learning), the rise of big tech companies and the\nplatform economy, which comes with the storage and processing of\nexabytes of data.\n\n\nThe revelations of Edward Snowden, and more recently the Cambridge\nAnalytica case (Cadwalladr & Graham-Harrison 2018) have\ndemonstrated that worries about negative consequences are real. The\ntechnical capabilities to collect, store and search large quantities\nof data concerning telephone conversations, internet searches and\nelectronic payment are now in place and are routinely used by\ngovernment agencies and corporate actors alike. The rise of China and\nthe large scale of use and spread of advanced digital technologies for\nsurveillance and control have only added to the concern of many. For\nbusiness firms, personal data about customers and potential customers\nare now also a key asset. The scope and purpose of the personal data\ncentred business models of Big Tech (Google, Amazon, Facebook,\nMicrosoft, Apple) has been described in detail by Shoshana Zuboff\n(2018) under the label “surveillance capitalism”. \n\n\nAt the same time, the meaning and value of privacy remains the subject\nof considerable controversy. The combination of increasing power of\nnew technology and the declining clarity and agreement on privacy give\nrise to problems concerning law, policy and ethics. Many of these\nconceptual debates and issues are situated in the context of\ninterpretation and analysis of the General Data Protection Regulation\n(GDPR) that was adopted by the EU in spring 2018 as the successor of\nthe EU 1995 Directives, with application far beyond the borders of the\nEuropean Union.\n\n\nThe focus of this article is on exploring the relationship between\ninformation technology and privacy. We will both illustrate the\nspecific threats that IT and innovations in IT pose for privacy and\nindicate how IT itself might be able to overcome these\nprivacy concerns by being developed in ways that can be termed\n“privacy-sensitive”, “privacy enhancing” or “privacy\nrespecting”. We will also discuss the role of emerging technologies\nin the debate, and account for the way in which moral debates are\nthemselves affected by IT.\n\nDiscussions about privacy are intertwined with the use of technology.\nThe publication that began the debate about privacy in the Western\nworld was occasioned by the introduction of the newspaper printing\npress and photography. Samuel D. Warren and Louis Brandeis wrote their\narticle on privacy in the Harvard Law Review (Warren & Brandeis\n1890) partly in protest against the intrusive activities of the\njournalists of those days. They argued that there is a “right to\nbe left alone” based on a principle of “inviolate\npersonality”. Since the publication of that article, the debate\nabout privacy has been fuelled by claims regarding the right of\nindividuals to determine the extent to which others have access to\nthem (Westin 1967) and claims regarding the right of society to know\nabout individuals. Information being a cornerstone of access to\nindividuals, the privacy debate has co-evolved with – and in\nresponse to – the development of information technology. It is\ntherefore difficult to conceive of the notions of privacy and\ndiscussions about data protection as separate from the way computers,\nthe Internet, mobile computing and the many applications of these\nbasic technologies have evolved. \nInspired by subsequent developments in U.S. law, a distinction can be\nmade between (1) constitutional (or decisional)\nprivacy and (2) tort (or informational)\nprivacy (DeCew 1997). The first refers to the freedom to make\none’s own decisions without interference by others in regard to\nmatters seen as intimate and personal, such as the decision to use\ncontraceptives or to have an abortion. The second is concerned with\nthe interest of individuals in exercising control over access to\ninformation about themselves and is most often referred to as\n“informational privacy”. Think here, for instance, about\ninformation disclosed on Facebook or other social media. All too\neasily, such information might be beyond the control of the\nindividual. \nStatements about privacy can be either descriptive or normative,\ndepending on whether they are used to describe the way people define\nsituations and conditions of privacy and the way they value them, or\nare used to indicate that there ought to be constraints on the use of\ninformation or information processing. These conditions or constraints\ntypically involve personal information regarding individuals, or ways\nof information processing that may affect individuals. Informational\nprivacy in a normative sense refers typically to a non-absolute moral\nright of persons to have direct or indirect control over access to (1)\ninformation about oneself, (2) situations in which others could\nacquire information about oneself, and (3) technology that can be used\nto generate, process or disseminate information about oneself. \nThe debates about privacy are almost always revolving around new\ntechnology, ranging from genetics and the extensive study of\nbio-markers, brain imaging, drones, wearable sensors and sensor\nnetworks, social media, smart phones, closed circuit television, to\ngovernment cybersecurity programs, direct marketing, RFID tags, Big\nData, head-mounted displays and search engines. There are basically\ntwo reactions to the flood of new technology and its impact on\npersonal information and privacy: the first reaction, held by many\npeople in IT industry and in R&D, is that we have zero privacy in\nthe digital age and that there is no way we can protect it, so we\nshould get used to the new world and get over it (Sprenger 1999). The other reaction\nis that our privacy is more important than ever and that we can and we\nmust attempt to protect it. \nIn the literature on privacy, there are many competing accounts of the\nnature and value of privacy (Negley 1966, Rössler 2005). On one end of\nthe spectrum, reductionist accounts argue that privacy claims\nare really about other values and other things that matter from a\nmoral point of view. According to these views the value of privacy is\nreducible to these other values or sources of value (Thomson 1975).\nProposals that have been defended along these lines mention property\nrights, security, autonomy, intimacy or friendship, democracy,\nliberty, dignity, or utility and economic value. Reductionist accounts\nhold that the importance of privacy should be explained and its\nmeaning clarified in terms of those other values and sources of value\n(Westin 1967). The opposing view holds that privacy is valuable in\nitself and its value and importance are not derived from other\nconsiderations (see for a discussion Rössler 2004). Views that\nconstrue privacy and the personal sphere of life as a human right\nwould be an example of this non-reductionist conception. \nMore recently a type of privacy account has been proposed in relation\nto new information technology, which acknowledges that there is a\ncluster of related moral claims underlying appeals to privacy, but\nmaintains that there is no single essential core of privacy\nconcerns. This approach is referred to as cluster accounts (DeCew\n1997; Solove 2006; van den Hoven 1999; Allen 2011; Nissenbaum\n2004). \nFrom a descriptive perspective, a recent further addition to the body of\nprivacy accounts are epistemic accounts, where the notion of privacy\nis analyzed primarily in terms of knowledge or other epistemic states.\nHaving privacy means that others don’t know certain private\npropositions; lacking privacy means that others do know certain\nprivate propositions (Blaauw 2013). An important aspect of this\nconception of having privacy is that it is seen as a relation (Rubel\n2011; Matheson 2007; Blaauw 2013) with three argument places: a\nsubject (S), a set of propositions (P) and a set of\nindividuals (I). Here S is the subject who has (a\ncertain degree of) privacy. P is composed of those propositions\nthe subject wants to keep private (call the propositions in this set\n‘personal propositions’), and I is composed of\nthose individuals with respect to whom S wants to keep the\npersonal propositions private. \nAnother distinction that is useful to make is the one between a\nEuropean and a US American approach. A bibliometric study suggests\nthat the two approaches are separate in the literature. The first\nconceptualizes issues of informational privacy in terms of ‘data\nprotection’, the second in terms of ‘privacy’\n(Heersmink et al. 2011). In discussing the relationship of privacy\nmatters with technology, the notion of data protection is most\nhelpful, since it leads to a relatively clear picture of what the\nobject of protection is and by which technical means the data can be\nprotected. At the same time it invites answers to the question why the\ndata ought to be protected, pointing to a number of distinctive moral\ngrounds on the basis of which technical, legal and institutional\nprotection of personal data can be justified. Informational privacy is\nthus recast in terms of the protection of personal data (van den Hoven\n2008). This account shows how Privacy, Technology and Data Protection\nare related, without conflating Privacy and Data Protection. \nPersonal information or data is information or data that is linked or\ncan be linked to individual persons. Examples include explicitly\nstated characteristics such as a person‘s date of birth, sexual\npreference, whereabouts, religion, but also the IP address of your\ncomputer or metadata pertaining to these kinds of information. In\naddition, personal data can also be more implicit in the form of\nbehavioural data, for example from social media, that can be linked to\nindividuals. Personal data can be contrasted with data that is\nconsidered sensitive, valuable or important for other reasons, such as\nsecret recipes, financial data, or military intelligence. Data used to\nsecure other information, such as passwords, are not considered\nhere. Although such security measures (passwords) may contribute to\nprivacy, their protection is only instrumental to the protection of\nother (more private) information, and the quality of such security\nmeasures is therefore out of the scope of our considerations here. \nA relevant distinction that has been made in philosophical semantics\nis that between the referential and the attributive use of descriptive\nlabels of persons (van den Hoven 2008). Personal data is defined in\nthe law as data that can be linked with a natural person. There are\ntwo ways in which this link can be made; a referential mode and a\nnon-referential mode. The law is primarily concerned with the\n‘referential use’ of descriptions or attributes, the type\nof use that is made on the basis of a (possible) acquaintance\nrelationship of the speaker with the object of his knowledge.\n“The murderer of Kennedy must be insane”, uttered while\npointing to him in court is an example of a referentially used\ndescription. This can be contrasted with descriptions that are used\nattributively as in “the murderer of Kennedy must be insane,\nwhoever he is”. In this case, the user of the description is\nnot – and may never be – acquainted with the person he is\ntalking about or intends to refer to. If the legal definition of\npersonal data is interpreted referentially, much of the data that\ncould at some point in time be brought to bear on persons would be\nunprotected; that is, the processing of this data would not be\nconstrained on moral grounds related to privacy or personal sphere of\nlife, since it does not “refer” to persons in a straightforward\nway and therefore does not constitute “personal data” in a strict\nsense.  \nThe following types of moral reasons for the protection of personal\ndata and for providing direct or indirect control over access to those\ndata by others can be distinguished (van den Hoven 2008): \nThese considerations all provide good moral reasons for limiting and\nconstraining access to personal data and providing individuals with\ncontrol over their data. \nAcknowledging that there are moral reasons for protecting personal\ndata, data protection laws are in force in almost all countries. The\nbasic moral principle underlying these laws is the requirement of\ninformed consent for processing by the data subject, providing the\nsubject (at least in principle) with control over potential negative\neffects as discussed above. Furthermore, processing of personal\ninformation requires that its purpose be specified, its use be\nlimited, individuals be notified and allowed to correct inaccuracies,\nand the holder of the data be accountable to oversight authorities\n(OECD 1980). Because it is impossible to guarantee compliance of all\ntypes of data processing in all these areas and applications with\nthese rules and laws in traditional ways, so-called\n“privacy-enhancing technologies” (PETs) and identity management\nsystems are expected to replace human oversight in many cases. The\nchallenge with respect to privacy in the twenty-first century is to\nassure that technology is designed in such a way that it incorporates\nprivacy requirements in the software, architecture, infrastructure,\nand work processes in a way that makes privacy violations unlikely to\noccur. New generations of privacy regulations (e.g. GDPR) now require\nstandardly a “privacy by design” approach. The data ecosystems and\nsocio-technical systems, supply chains, organisations, including\nincentive structures, business processes, and technical hardware and\nsoftware, training of personnel, should all be designed in such a way that\nthe likelihood of privacy violations is a low as possible. \nThe debates about privacy are almost always revolving around new\ntechnology, ranging from genetics and the extensive study of\nbio-markers, brain imaging, drones, wearable sensors and sensor\nnetworks, social media, smart phones, closed circuit television, to\ngovernment cybersecurity programs, direct marketing, surveillance,\nRFID tags, big data, head-mounted displays and search engines. The\nimpact of some of these new technologies, with a particular focus on\ninformation technology, is discussed in this section. \n“Information technology” refers to automated systems for\nstoring, processing, and distributing information. Typically, this\ninvolves the use of computers and communication networks. The amount\nof information that can be stored or processed in an information\nsystem depends on the technology used. The capacity of the technology\nhas increased rapidly over the past decades, in accordance with\nMoore’s law. This holds for storage capacity, processing capacity, and\ncommunication bandwidth. We are now capable of storing and processing\ndata on the exabyte level. For illustration, to store 100 exabytes of\ndata on 720 MB CD-ROM discs would require a stack of them that would\nalmost reach the moon. \nThese developments have fundamentally changed our practices of\ninformation provisioning. The rapid changes have increased the need\nfor careful consideration of the desirability of effects. Some even\nspeak of a digital revolution as a technological leap similar to the\nindustrial revolution, or a digital revolution as a revolution in\nunderstanding human nature and the world, similar to the revolutions\nof Copernicus, Darwin and Freud (Floridi 2008). In both the technical\nand the epistemic sense, emphasis has been put on connectivity and\ninteraction. Physical space has become less important, information is\nubiquitous, and social relations have adapted as well. \nAs we have described privacy in terms of moral reasons for imposing\nconstraints on access to and/or use of personal information, the\nincreased connectivity imposed by information technology poses many\nquestions. In a descriptive sense, access has increased, which, in a\nnormative sense, requires consideration of the desirability of this\ndevelopment, and evaluation of the potential for regulation by\ntechnology (Lessig 1999), institutions, and/or law. \nAs connectivity increases access to information, it also increases the\npossibility for agents to act based on the new sources of\ninformation. When these sources contain personal information, risks of\nharm, inequality, discrimination, and loss of autonomy easily emerge.\nFor example, your enemies may have less difficulty finding out where\nyou are, users may be tempted to give up privacy for perceived\nbenefits in online environments, and employers may use online\ninformation to avoid hiring certain groups of people. Furthermore,\nsystems rather than users may decide which information is displayed,\nthus confronting users only with news that matches their profiles. \nAlthough the technology operates on a device level, information\ntechnology consists of a complex system of socio-technical practices,\nand its context of use forms the basis for discussing its role in\nchanging possibilities for accessing information, and thereby\nimpacting privacy. We will discuss some specific developments and\ntheir impact in the following sections. \nThe Internet, originally conceived in the 1960s and developed in the\n1980s as a scientific network for exchanging information, was not\ndesigned for the purpose of separating information flows (Michener\n1999). The World Wide Web of today was not foreseen, and neither was\nthe possibility of misuse of the Internet. Social network sites\nemerged for use within a community of people who knew each other in\nreal life – at first, mostly in academic settings – rather\nthan being developed for a worldwide community of users (Ellison\n2007). It was assumed that sharing with close friends would not cause\nany harm, and privacy and security only appeared on the agenda when\nthe network grew larger. This means that privacy concerns often had to\nbe dealt with as add-ons rather than by-design. \nA major theme in the discussion of Internet privacy revolves around\nthe use of cookies (Palmer 2005). Cookies are small pieces of data\nthat web sites store on the user’s computer, in order to enable\npersonalization of the site. However, some cookies can be used to\ntrack the user across multiple web sites (tracking cookies), enabling\nfor example advertisements for a product the user has recently viewed\non a totally different site. Again, it is not always clear what the\ngenerated information is used for. Laws requiring user consent for the\nuse of cookies are not always successful in terms of increasing the\nlevel of control, as the consent requests interfere with task flows,\nand the user may simply click away any requests for consent (Leenes\n& Kosta 2015). Similarly, features of social network sites\nembedded in other sites (e.g. “like”-button) may allow\nthe social network site to identify the sites visited by the user\n(Krishnamurthy & Wills 2009). \nThe recent development of cloud computing increases the many privacy\nconcerns (Ruiter & Warnier 2011). Previously, whereas information\nwould be available from the web, user data and programs would still be\nstored locally, preventing program vendors from having access to the\ndata and usage statistics. In cloud computing, both data and programs\nare online (in the cloud), and it is not always clear what the\nuser-generated and system-generated data are used for. Moreover, as\ndata are located elsewhere in the world, it is not even always obvious\nwhich law is applicable, and which authorities can demand access to\nthe data. Data gathered by online services and apps such as search\nengines and games are of particular concern here. Which data are used\nand communicated by applications (browsing history, contact lists,\netc.) is not always clear, and even when it is, the only choice\navailable to the user may be not to use the application. \nSome special features of Internet privacy (social media and big data)\nare discussed in the following sections. \nSocial media pose additional challenges. The question is not merely\nabout the moral reasons for limiting access to information, it is also\nabout the moral reasons for limiting the invitations to users\nto submit all kinds of personal information. Social network sites\ninvite the user to generate more data, to increase the value of the\nsite (“your profile is …% complete”). Users are\ntempted to exchange their personal data for the benefits of\nusing services, and provide both this data and their attention as\npayment for the services. In addition, users may not even be aware of\nwhat information they are tempted to provide, as in the aforementioned\ncase of the “like”-button on other sites. Merely limiting\nthe access to personal information does not do justice to the issues\nhere, and the more fundamental question lies in steering the users’\nbehaviour of sharing. When the service is free, the data is needed as a form of payment. \nOne way of limiting the temptation of users to share is requiring\ndefault privacy settings to be strict. Even then, this limits access\nfor other users (“friends of friends”), but it does not\nlimit access for the service provider. Also, such restrictions limit\nthe value and usability of the social network sites themselves, and\nmay reduce positive effects of such services. A particular example of\nprivacy-friendly defaults is the opt-in as opposed to the opt-out\napproach. When the user has to take an explicit action to share data\nor to subscribe to a service or mailing list, the resulting effects\nmay be more acceptable to the user. However, much still depends on how\nthe choice is framed (Bellman, Johnson, & Lohse 2001). \nUsers generate loads of data when online. This is not only data\nexplicitly entered by the user, but also numerous statistics on user\nbehavior: sites visited, links clicked, search terms entered, etc. Data\nmining can be employed to extract patterns from such data, which can\nthen be used to make decisions about the user. These may only affect\nthe online experience (advertisements shown), but, depending on which\nparties have access to the information, they may also impact the user\nin completely different contexts. \nIn particular, big data may be used in profiling the user (Hildebrandt\n2008), creating patterns of typical combinations of user properties,\nwhich can then be used to predict interests and behavior. An innocent\napplication is “you may also like …”, but,\ndepending on the available data, more sensitive derivations may be\nmade, such as most probable religion or sexual preference. These\nderivations could then in turn lead to inequal treatment or\ndiscrimination.  When a user can be assigned to a particular group,\neven only probabilistically, this may influence the actions taken by\nothers (Taylor, Floridi, & Van der Sloot 2017). For example,\nprofiling could lead to refusal of insurance or a credit card, in\nwhich case profit is the main reason for discrimination. When such\ndecisions are based on profiling, it may be difficult to challenge\nthem or even find out the explanations behind them. Profiling could\nalso be used by organizations or possible future governments that have\ndiscrimination of particular groups on their political agenda, in\norder to find their targets and deny them access to services, or\nworse. \nBig data does not only emerge from Internet transactions. Similarly,\ndata may be collected when shopping, when being recorded by\nsurveillance cameras in public or private spaces, or when using\nsmartcard-based public transport payment systems. All these data could\nbe used to profile citizens, and base decisions upon such profiles.\nFor example, shopping data could be used to send information about\nhealthy food habits to particular individuals, but again also for\ndecisions on insurance. According to EU data protection law,\npermission is needed for processing personal data, and they can only\nbe processed for the purpose for which they were obtained. Specific\nchallenges, therefore, are (a) how to obtain permission when the user\ndoes not explicitly engage in a transaction (as in case of\nsurveillance), and (b) how to prevent “function creep”,\ni.e. data being used for different purposes after they are collected\n(as may happen for example with DNA databases (Dahl & Sætnan\n2009). \nOne particular concern could emerge from genetics and genomic data\n(Tavani 2004, Bruynseels & van den Hoven, 2015). Like other data, genomics can be used to make predictions, and in particular could predict risks of diseases.\nApart from others having access to detailed user profiles, a\nfundamental question here is whether the individual should know what\nis known about her. In general, users could be said to have a right to\naccess any information stored about them, but in this case, there may\nalso be a right not to know, in particular when knowledge of the data\n(e.g. risks of diseases) would reduce the well-being – by causing\nfear, for instance – without enabling treatment. With respect to\nprevious examples, one may not want to know the patterns in one’s own\nshopping behavior either. \nAs users increasingly own networked devices such as smart phones,\nmobile devices collect and send more and more data. These devices\ntypically contain a range of data-generating sensors, including GPS\n(location), movement sensors, and cameras, and may transmit the\nresulting data via the Internet or other networks. One particular\nexample concerns location data. Many mobile devices have a GPS sensor\nthat registers the user’s location, but even without a GPS sensor,\napproximate locations can be derived, for example by monitoring the\navailable wireless networks. As location data links the online world\nto the user’s physical environment, with the potential of physical\nharm (stalking, burglary during holidays, etc.), such data are often\nconsidered particularly sensitive. \nMany of these devices also contain cameras which, when applications\nhave access, can be used to take pictures. These can be considered\nsensors as well, and the data they generate may be particularly\nprivate. For sensors like cameras, it is assumed that the user is\naware when they are activated, and privacy depends on such knowledge.\nFor webcams, a light typically indicates whether the camera is on, but\nthis light may be manipulated by malicious software. In general,\n“reconfigurable technology” (Dechesne, Warnier, & van\nden Hoven 2011) that handles personal data raises the question of user\nknowledge of the configuration. \nDevices connected to the Internet are not limited to user-owned\ncomputing devices like smartphones. Many devices contain chips and/or\nare connected in the so-called Internet of Things. RFID (radio\nfrequency identification) chips can be read from a limited distance,\nsuch that you can hold them in front of a reader rather than inserting\nthem. EU and US passports have RFID chips with protected biometric\ndata, but information like the user’s nationality may easily leak when\nattempting to read such devices (see Richter, Mostowski & Poll\n2008, in Other Internet Resources). “Smart” RFIDs are also\nembedded in public transport payment systems. “Dumb”\nRFIDs, basically only containing a number, appear in many kinds of\nproducts as a replacement of the barcode, and for use in logistics.\nStill, such chips could be used to trace a person once it is known\nthat he carries an item containing a chip. \nIn the home, there are smart meters for automatically reading and\nsending electricity and water consumption, and thermostats and other devices\nthat can be remotely controlled by the owner. Such devices again\ngenerate statistics, and these can be used for mining and profiling.\nIn the future, more and more household appliances will be connected,\neach generating its own information. Ambient intelligence (Brey 2005),\nand ubiquitous computing, along with the Internet of Things\n(Friedewald & Raabe 2011), also enable automatic adaptation of the\nenvironment to the user, based on explicit preferences and implicit\nobservations, and user autonomy is a central theme in considering the\nprivacy implications of such devices. In general, the move towards a\nservice-oriented provisioning of goods, with suppliers being informed\nabout how the products are used through IT and associated\nconnectivity, requires consideration of the associated privacy and\ntransparency concerns (Pieters 2013). For example, users will need to\nbe informed when connected devices contain a microphone and how and\nwhen it is used. \nGovernment and public administration have undergone radical\ntransformations as a result of the availability of advanced IT systems\nas well. Examples of these changes are biometric passports, online\ne-government services, voting systems, a variety of online citizen\nparticipation tools and platforms or online access to recordings of\nsessions of parliament and government committee meetings.  \nConsider the case of voting in elections. Information technology may\nplay a role in different phases in the voting process, which may have\ndifferent impact on voter privacy. Most countries have a requirement\nthat elections are to be held by secret ballot, to prevent vote buying\nand coercion. In this case, the voter is supposed to keep her vote\nprivate, even if she would want to reveal it. For information\ntechnology used for casting votes, this is defined as the requirement\nof receipt-freeness or coercion-resistance (Delaune, Kremer & Ryan\n2006). In polling stations, the authorities see to it that the voter\nkeeps the vote private, but such surveillance is not possible when\nvoting by mail or online, and it cannot even be enforced by\ntechnological means, as someone can always watch while the voter\nvotes. In this case, privacy is not only a right but also a duty, and\ninformation technology developments play an important role in the\npossibilities of the voter to fulfill this duty, as well as the\npossibilities of the authorities to verify this. In a broader sense,\ne-democracy initiatives may change the way privacy is viewed in the\npolitical process. \nMore generally, privacy is important in democracy to prevent undue\ninfluence. While lack of privacy in the voting process could enable\nvote buying and coercion, there are more subtle ways of influencing\nthe democratic process, for example through targeted (mis)information\ncampaigns. Online (political) activities of citizens on for example\nsocial media facilitate such attempts because of the possibility of\ntargeting through behavioural profiling. Compared to offline political\nactivities, it is more difficult to hide preferences and activities,\nbreaches of confidentiality are more likely, and attempts to influence\nopinions become more scalable. \nInformation technology is used for all kinds of surveillance tasks. It\ncan be used to augment and extend traditional surveillance systems\nsuch as CCTV and other camera systems, for example to identify\nspecific individuals in crowds, using face recognition techniques, or\nto monitor specific places for unwanted behaviour. Such approaches\nbecome even more powerful when combined with other techniques, such as\nmonitoring of Internet-of-Things devices (Motlagh et al. 2017). \nBesides augmenting existing surveillance systems, ICT techniques are\nnowadays mainly used in the digital domain, typically grouped together\nunder the term “surveillance capitalism” (Zuboff 2019). Social\nmedia and other online systems are used to gather large amounts of\ndata about individuals – either “voluntary”, because users\nsubscribe to a specific service (Google, Facebook), or involuntary by\ngathering all kinds of user related data in a less transparent manner.\nData analysis and machine learning techniques are then used to\ngenerate prediction models of individual users that can be used, for\nexample, for targeted advertisement, but also for more malicious\nintents such as fraud or micro-targeting to influence elections\n(Albright 2016, Other Internet Resources) or referenda such as Brexit\n(Cadwalladr 2019, Other Internet Resources). \nIn addition to the private sector surveillance industry, governments\nform another traditional group that uses surveillance techniques at a\nlarge scale, either by intelligence services or law enforcement. These\ntypes of surveillance systems are typically justified with an appeal\nto the “greater good” and protecting citizens, but their\nuse is also controversial. For such systems, one would typically like\nto ensure that any negative effects on privacy are proportional to the\nbenefits achieved by the technology. Especially since these systems\nare typically shrouded in secrecy, it is difficult for outsiders to\nsee if such systems are used proportionally, or indeed useful for\ntheir tasks (Lawner 2002). This is particularly pressing when\ngovernments use private sector data or services for surveillance\npurposes.  The almost universal use of good encryption techniques\nin communication systems makes it also harder to gather effective\nsurveillance information, leading to more and more calls for “back\ndoors” that can exclusively be used by government in communication\nsystems. From a privacy standpoint this could be evaluated as\nunwanted, not only because it gives governments access to private\nconversations, but also because it lowers the overall security of\ncommunication systems that employ this technique (Abelson et al.\n2015). \nWhereas information technology is typically seen as the cause\nof privacy problems, there are also several ways in which information\ntechnology can help to solve these problems. There are rules,\nguidelines or best practices that can be used for designing\nprivacy-preserving systems. Such possibilities range from\nethically-informed design methodologies to using encryption to protect\npersonal information from unauthorized use. In particular, methods\nfrom the field of information security, aimed at protecting\ninformation against unauthorized access, can play a key role in the\nprotection of personal data. \nValue sensitive design provides a “theoretically grounded\napproach to the design of technology that accounts for human values in\na principled and comprehensive manner throughout the design\nprocess” (Friedman et al. 2006). It provides a set of rules and\nguidelines for designing a system with a certain value in mind. One\nsuch value can be ‘privacy’, and value sensitive design\ncan thus be used as a method to design privacy-friendly IT systems\n(Van den Hoven et al. 2015).  The ‘privacy by design’\napproach as advocated by Cavoukian (2009) and others can be regarded\nas one of the value sensitive design approaches that specifically\nfocuses on privacy (Warnier et al. 2015). More recently, approaches\nsuch as “privacy engineering” (Ceross & Simpson 2018)\nextend the privacy by design approach by aiming to provide a more\npractical, deployable set of methods by which to achieve system-wide\nprivacy. \nThe privacy by design approach provides high-level guidelines in the\nform of principles for designing privacy-preserving systems.\nThese principles have at their core that “data protection needs\nto be viewed in proactive rather than reactive terms, making privacy\nby design preventive and not simply remedial” (Cavoukian 2010).\nPrivacy by design’s main point is that data protection should be\ncentral in all phases of product life cycles, from initial design to\noperational use and disposal (see Colesky et al. 2016) for a critical\nanalysis of the privacy by design approach). The Privacy Impact\nAssessment approach proposed by Clarke (2009) makes a similar point.\nIt proposes “a systematic process for evaluating the potential\neffects on privacy of a project, initiative or proposed system or\nscheme” (Clarke 2009). Note that these approaches should not\nonly be seen as auditing approaches, but rather as a means to make\nprivacy awareness and compliance an integral part of the\norganizational and engineering culture. \nThere are also several industry guidelines that can be used to design\nprivacy preserving IT systems. The Payment Card Industry Data Security\nStandard (see PCI DSS v3.2, 2018, in the Other Internet Resources),\nfor example, gives very clear guidelines for privacy and security\nsensitive systems design in the domain of the credit card industry and\nits partners (retailers, banks). Various International Organization\nfor Standardization (ISO) standards (Hone & Eloff 2002) also serve\nas a source of best practices and guidelines, especially with respect\nto information security, for the design of privacy friendly systems.\nFurthermore, the principles that are formed by the EU Data Protection\nDirective, which are themselves based on the Fair Information\nPractices (Gellman 2014) from the early 70s – transparency,\npurpose, proportionality, access, transfer – are technologically\nneutral and as such can also be considered as high level ‘design\nprinciples’. Systems that are designed with these rules and\nguidelines in mind should thus – in principle – be in\ncompliance with EU privacy laws and respect the privacy of its\nusers. \nThe rules and principles described above give high-level guidance for\ndesigning privacy-preserving systems, but this does not mean that if\nthese methodologies are followed the resulting IT system will\n(automatically) be privacy friendly. Some design principles are rather\nvague and abstract. What does it mean to make a transparent design or\nto design for proportionality? The principles need to be interpreted\nand placed in a context when designing a specific system. But\ndifferent people will interpret the principles differently, which will\nlead to different design choices, with different effects on privacy.\nThere is also a difference between the design and the implementation\nof a computer system. During the implementation phase software bugs\nare introduced, some of which can be exploited to break the system and\nextract private information. How to implement bug-free computer\nsystems remains an open research question (Hoare 2003). In addition,\nimplementation is another phase wherein choices and interpretations\nare made: system designs can be implemented in infinitely many ways.\nMoreover, it is very hard to verify – for anything beyond\nnon-trivial systems – whether an implementation meets its\ndesign/specification (Loeckx, Sieber, & Stansifer 1985). This is\neven more difficult for non-functional requirements such as\n‘being privacy preserving’ or security properties in\ngeneral.  \nSome specific solutions to privacy problems aim at increasing the\nlevel of awareness and consent of the user. These solutions can be\nseen as an attempt to apply the notion of informed consent to privacy\nissues with technology (Custers et al. 2018). This is connected to the\nidea that privacy settings and policies should be explainable to users\n(Pieters 2011). For example, the Privacy Coach supports customers in\nmaking privacy decisions when confronted with RFID tags (Broenink et\nal. 2010). However, users have only a limited capability of dealing\nwith such choices, and providing too many choices may easily lead to\nthe problem of moral overload (van den Hoven, Lokhorst, & Van de\nPoel 2012). A technical solution is support for automatic matching of\na privacy policy set by the user against policies issued by web sites\nor apps. \nA growing number of software tools are available that provide some\nform of privacy (usually anonymity) for their users, such tools are\ncommonly known as privacy enhancing technologies (Danezis &\nGürses 2010, Other Internet Resources). Examples include\ncommunication-anonymizing tools such as Tor (Dingledine, Mathewson,\n& Syverson 2004) and Freenet (Clarke et al. 2001), and\nidentity-management systems for which many commercial software\npackages exist (see below). Communication anonymizing tools allow\nusers to anonymously browse the web (with Tor) or anonymously share\ncontent (Freenet). They employ a number of cryptographic techniques\nand security protocols in order to ensure their goal of anonymous\ncommunication. Both systems use the property that numerous users use\nthe system at the same time which provides k-anonymity (Sweeney\n2002): no individual can be uniquely distinguished from a group of\nsize k, for large values for k. Depending on the system,\nthe value of k can vary between a few hundred to hundreds of\nthousands. In Tor, messages are encrypted and routed along numerous\ndifferent computers, thereby obscuring the original sender of the\nmessage (and thus providing anonymity). Similarly, in Freenet content\nis stored in encrypted form from all users of the system. Since users\nthemselves do not have the necessary decryption keys, they do not know\nwhat kind of content is stored, by the system, on their own computer.\nThis provides plausible deniability and privacy. The system can at any\ntime retrieve the encrypted content and send it to different Freenet\nusers. \nPrivacy enhancing technologies also have their downsides. For example,\nTor, the tool that allows anonymized communication and browsing over\nthe Internet, is susceptible to an attack whereby, under certain\ncircumstances, the anonymity of the user is no longer guaranteed\n(Back, Möller, & Stiglic 2001; Evans, Dingledine, &\nGrothoff 2009). Freenet (and other tools) have similar problems\n(Douceur 2002). Note that for such attacks to work, an attacker needs\nto have access to large resources that in practice are only realistic\nfor intelligence agencies of countries. However, there are other\nrisks. Configuring such software tools correctly is difficult for the\naverage user, and when the tools are not correctly configured\nanonymity of the user is no longer guaranteed. And there is always the\nrisk that the computer on which the privacy-preserving software runs\nis infected by a Trojan horse (or other digital pest) that monitors\nall communication and knows the identity of the user. \nAnother option for providing anonymity is the anonymization of data\nthrough special software. Tools exist that remove patient names and\nreduce age information to intervals: the age 35 is then represented as\nfalling in the range 30–40. The idea behind such anonymization\nsoftware is that a record can no longer be linked to an individual,\nwhile the relevant parts of the data can still be used for scientific\nor other purposes. The problem here is that it is very hard to\nanonymize data in such a way that all links with an individual are\nremoved and the resulting anonymized data is still useful for research\npurposes. Researchers have shown that it is almost always possible to\nreconstruct links with individuals by using sophisticated statistical\nmethods (Danezis, Diaz, & Troncoso 2007) and by combining multiple\ndatabases (Anderson 2008) that contain personal information.\nTechniques such as k-anonymity might also help to generalize\nthe data enough to make it unfeasible to de-anonymize data (LeFevre et\nal. 2005). \nCryptography has long been used as a means to protect data, dating\nback to the Caesar cipher more than two thousand years ago. Modern\ncryptographic techniques are essential in any IT system that needs to\nstore (and thus protect) personal data, for example by providing\nsecure (confidential) connections for browsing (HTTPS) and networking\n(VPN). Note however that by itself cryptography does not provide any\nprotection against data breaching; only when applied correctly in a\nspecific context does it become a ‘fence’ around personal\ndata. In addition, cryptographic schemes that become outdated by\nfaster computers or new attacks may pose threats to (long-term)\nprivacy. Cryptography is a large\nfield, so any description here will be incomplete. The focus will be\ninstead on some newer cryptographic techniques, in particular\nhomomorphic encryption, that have the potential to become very\nimportant for processing and searching in personal data. \nVarious techniques exist for searching through encrypted data (Song et\nal. 2000, Wang et al. 2016), which provides a form of privacy\nprotection (the data is encrypted) and selective access to sensitive\ndata. One relatively new technique that can be used for designing\nprivacy-preserving systems is ‘homomorphic encryption’\n(Gentry 2009, Acar et al. 2018). Homomorphic encryption allows a data\nprocessor to process encrypted data, i.e. users could send personal\ndata in encrypted form and get back some useful results – for\nexample, recommendations of movies that online friends like – in\nencrypted form. The original user can then again decrypt the result\nand use it without revealing any personal data to the data processor.\nHomomorphic encryption, for example, could be used to aggregate\nencrypted data thereby allowing both privacy protection and useful\n(anonymized) aggregate information. The technique is currently not\nwidely applied; there are serious performance issues if one wants to\napply full homomorphic encryption to the large amounts of data stored\nin today’s systems. However, variants of the original homomorphic\nencryption scheme are emerging, such as Somewhat Homomorphic\nEncryption (Badawi et al. 2018), that are showing promise to be more\nwidely applied in practice. \nThe main idea behind blockchain technology was first described in the\nseminal paper on Bitcoins (Nakamoto, n.d., Other Internet Resources).\nA blockchain is basically a distributed ledger that stores\ntransactions in a non-reputable way, without the use of a trusted\nthird party. Cryptography is used to ensure that all transactions are\n“approved” by members of the blockchain and stored in such a way\nthat they are linked to previous transactions and cannot be removed.\nAlthough focused on data integrity and not inherently anonymous,\nblockchain technology enables many privacy-related applications\n(Yli-Huumo et al. 2016, Karame and Capkun 2018), such as anonymous\ncryptocurrency (Narayanan et al. 2016) and self-sovereign identity\n(see below). \nThe use and management of user’s online identifiers are crucial in the\ncurrent Internet and social networks. Online reputations become more\nand more important, both for users and for companies. In the era of\nbig data correct information about users has an\nincreasing monetary value. \n‘Single sign on’ frameworks, provided by independent third\nparties (OpenID) but also by large companies such as Facebook,\nMicrosoft and Google (Ko et al. 2010), make it easy for users to\nconnect to numerous online services using a single online identity.\nThese online identities are usually directly linked to the real world\n(off line) identities of individuals; indeed Facebook, Google and\nothers require this form of log on (den Haak 2012). Requiring a direct\nlink between online and ‘real world’ identities is\nproblematic from a privacy perspective, because they allow profiling\nof users (Benevenuto et al. 2012). Not all users will realize how\nlarge the amount of data is that companies gather in this manner, or\nhow easy it is to build a detailed profile of users. Profiling becomes\neven easier if the profile information is combined with other\ntechniques such as implicit authentication via cookies and tracking\ncookies (Mayer & Mitchell 2012). \nFrom a privacy perspective a better solution would be the use of\nattribute-based authentication (Goyal et al. 2006) which allows access\nof online services based on the attributes of users, for example their\nfriends, nationality, age etc. Depending on the attributes used, they\nmight still be traced back to specific individuals, but this is no\nlonger crucial. In addition, users can no longer be tracked to\ndifferent services because they can use different attributes to access\ndifferent services which makes it difficult to trace online identities\nover multiple transactions, thus providing unlinkability for the user.\nRecently (Allen 2016, Other Internet Resources), the concept of\nself-sovereign identity has emerged, which aims for users to have\ncomplete ownership and control about their own digital identities.\nBlockchain technology is used to make it possible for users to control\na digital identity without the use of a traditional trusted third\nparty (Baars 2016). \nIn the previous sections, we have outlined how current technologies\nmay impact privacy, as well as how they may contribute to mitigating\nundesirable effects. However, there are future and emerging\ntechnologies that may have an even more profound impact. Consider for\nexample brain-computer interfaces. In case computers are connected\ndirectly to the brain, not only behavioral characteristics are subject\nto privacy considerations, but even one’s thoughts run the risk of\nbecoming public, with decisions of others being based upon them. In\naddition, it could become possible to change one’s behavior by means\nof such technology. Such developments therefore require further\nconsideration of the reasons for protecting privacy. In particular,\nwhen brain processes could be influenced from the outside, autonomy\nwould be a value to reconsider to ensure adequate protection. \nApart from evaluating information technology against current moral\nnorms, one also needs to consider the possibility that technological\nchanges influence the norms themselves (Boenink, Swierstra &\nStemerding 2010). Technology thus does not only influence privacy by\nchanging the accessibility of information, but also by changing the\nprivacy norms themselves. For example, social networking sites invite\nusers to share more information than they otherwise might. This\n“oversharing” becomes accepted practice within certain\ngroups. With future and emerging technologies, such influences can\nalso be expected and therefore they ought to be taken into account\nwhen trying to mitigate effects. \nAnother fundamental question is whether, given the future (and even\ncurrent) level of informational connectivity, it is feasible to\nprotect privacy by trying to hide information from parties who may use\nit in undesirable ways. Gutwirth & De Hert (2008) argue that it\nmay be more feasible to protect privacy by transparency – by\nrequiring actors to justify decisions made about individuals, thus\ninsisting that decisions are not based on illegitimate information.\nThis approach comes with its own problems, as it might be hard to\nprove that the wrong information was used for a decision. Still, it\nmay well happen that citizens, in turn, start data collection on those\nwho collect data about them, e.g. governments. Such\n“counter(sur)veillance” may be used to gather information\nabout the use of information, thereby improving accountability\n(Gürses et al. 2016). The open source movement may also\ncontribute to transparency of data processing. In this context,\ntransparency can be seen as a pro-ethical condition contributing to\nprivacy (Turilli & Floridi 2009). \nIt has been argued that the precautionary principle, well known in\nenvironmental ethics, might have a role in dealing with emerging\ninformation technologies as well (Pieters & van Cleeff 2009; Som,\nHilty & Köhler 2009). The principle would see to it that the\nburden of proof for absence of irreversible effects of information\ntechnology on society, e.g. in terms of power relations and equality,\nwould lie with those advocating the new technology. Precaution, in\nthis sense, could then be used to impose restrictions at a regulatory\nlevel, in combination with or as an alternative to empowering users,\nthereby potentially contributing to the prevention of informational\noverload on the user side. Apart from general debates about the\ndesirable and undesirable features of the precautionary principle,\nchallenges to it lie in its translation to social effects and social\nsustainability, as well as to its application to consequences induced\nby intentional actions of agents. Whereas the occurrence of natural\nthreats or accidents is probabilistic in nature, those who are\ninterested in improper use of information behave strategically,\nrequiring a different approach to risk (i.e. security as opposed to\nsafety). In addition, proponents of precaution will need to balance it\nwith other important principles, viz., of informed consent and\nautonomy. \nFinally, it is appropriate to note that not all social effects of\ninformation technology concern privacy (Pieters 2017). Examples\ninclude the effects of social network sites on friendship, and the\nverifiability of results of electronic elections. Therefore,\nvalue-sensitive design approaches and impact assessments of\ninformation technology should not focus on privacy only, since\ninformation technology affects many other values as well.","contact.mail":"M.J.Blaauw@tudelft.nl","contact.domain":"tudelft.nl"},{"date.published":"2014-11-20","date.changed":"2019-10-30","url":"https://plato.stanford.edu/entries/it-privacy/","author1":"Jeroen van den Hoven","author2":"Martijn Blaauw","author1.info":"http://homepage.tudelft.nl/e7x9k/","author2.info":"http://homepage.tudelft.nl/68x7e/","entry":"it-privacy","body.text":"\n\n\nHuman beings value their privacy and the protection of their personal\nsphere of life. They value some control over who knows what about\nthem. They certainly do not want their personal information to be\naccessible to just anyone at any time. But recent advances in\ninformation technology threaten privacy and have reduced the amount of\ncontrol over personal data and open up the possibility of a range of\nnegative consequences as a result of access to personal data. In the\nsecond half of the 20th century data protection regimes\nhave been put in place as a response to increasing levels of\nprocessing of personal data. The 21st century has become\nthe century of big data and advanced information technology (e.g.\nforms of deep learning), the rise of big tech companies and the\nplatform economy, which comes with the storage and processing of\nexabytes of data.\n\n\nThe revelations of Edward Snowden, and more recently the Cambridge\nAnalytica case (Cadwalladr & Graham-Harrison 2018) have\ndemonstrated that worries about negative consequences are real. The\ntechnical capabilities to collect, store and search large quantities\nof data concerning telephone conversations, internet searches and\nelectronic payment are now in place and are routinely used by\ngovernment agencies and corporate actors alike. The rise of China and\nthe large scale of use and spread of advanced digital technologies for\nsurveillance and control have only added to the concern of many. For\nbusiness firms, personal data about customers and potential customers\nare now also a key asset. The scope and purpose of the personal data\ncentred business models of Big Tech (Google, Amazon, Facebook,\nMicrosoft, Apple) has been described in detail by Shoshana Zuboff\n(2018) under the label “surveillance capitalism”. \n\n\nAt the same time, the meaning and value of privacy remains the subject\nof considerable controversy. The combination of increasing power of\nnew technology and the declining clarity and agreement on privacy give\nrise to problems concerning law, policy and ethics. Many of these\nconceptual debates and issues are situated in the context of\ninterpretation and analysis of the General Data Protection Regulation\n(GDPR) that was adopted by the EU in spring 2018 as the successor of\nthe EU 1995 Directives, with application far beyond the borders of the\nEuropean Union.\n\n\nThe focus of this article is on exploring the relationship between\ninformation technology and privacy. We will both illustrate the\nspecific threats that IT and innovations in IT pose for privacy and\nindicate how IT itself might be able to overcome these\nprivacy concerns by being developed in ways that can be termed\n“privacy-sensitive”, “privacy enhancing” or “privacy\nrespecting”. We will also discuss the role of emerging technologies\nin the debate, and account for the way in which moral debates are\nthemselves affected by IT.\n\nDiscussions about privacy are intertwined with the use of technology.\nThe publication that began the debate about privacy in the Western\nworld was occasioned by the introduction of the newspaper printing\npress and photography. Samuel D. Warren and Louis Brandeis wrote their\narticle on privacy in the Harvard Law Review (Warren & Brandeis\n1890) partly in protest against the intrusive activities of the\njournalists of those days. They argued that there is a “right to\nbe left alone” based on a principle of “inviolate\npersonality”. Since the publication of that article, the debate\nabout privacy has been fuelled by claims regarding the right of\nindividuals to determine the extent to which others have access to\nthem (Westin 1967) and claims regarding the right of society to know\nabout individuals. Information being a cornerstone of access to\nindividuals, the privacy debate has co-evolved with – and in\nresponse to – the development of information technology. It is\ntherefore difficult to conceive of the notions of privacy and\ndiscussions about data protection as separate from the way computers,\nthe Internet, mobile computing and the many applications of these\nbasic technologies have evolved. \nInspired by subsequent developments in U.S. law, a distinction can be\nmade between (1) constitutional (or decisional)\nprivacy and (2) tort (or informational)\nprivacy (DeCew 1997). The first refers to the freedom to make\none’s own decisions without interference by others in regard to\nmatters seen as intimate and personal, such as the decision to use\ncontraceptives or to have an abortion. The second is concerned with\nthe interest of individuals in exercising control over access to\ninformation about themselves and is most often referred to as\n“informational privacy”. Think here, for instance, about\ninformation disclosed on Facebook or other social media. All too\neasily, such information might be beyond the control of the\nindividual. \nStatements about privacy can be either descriptive or normative,\ndepending on whether they are used to describe the way people define\nsituations and conditions of privacy and the way they value them, or\nare used to indicate that there ought to be constraints on the use of\ninformation or information processing. These conditions or constraints\ntypically involve personal information regarding individuals, or ways\nof information processing that may affect individuals. Informational\nprivacy in a normative sense refers typically to a non-absolute moral\nright of persons to have direct or indirect control over access to (1)\ninformation about oneself, (2) situations in which others could\nacquire information about oneself, and (3) technology that can be used\nto generate, process or disseminate information about oneself. \nThe debates about privacy are almost always revolving around new\ntechnology, ranging from genetics and the extensive study of\nbio-markers, brain imaging, drones, wearable sensors and sensor\nnetworks, social media, smart phones, closed circuit television, to\ngovernment cybersecurity programs, direct marketing, RFID tags, Big\nData, head-mounted displays and search engines. There are basically\ntwo reactions to the flood of new technology and its impact on\npersonal information and privacy: the first reaction, held by many\npeople in IT industry and in R&D, is that we have zero privacy in\nthe digital age and that there is no way we can protect it, so we\nshould get used to the new world and get over it (Sprenger 1999). The other reaction\nis that our privacy is more important than ever and that we can and we\nmust attempt to protect it. \nIn the literature on privacy, there are many competing accounts of the\nnature and value of privacy (Negley 1966, Rössler 2005). On one end of\nthe spectrum, reductionist accounts argue that privacy claims\nare really about other values and other things that matter from a\nmoral point of view. According to these views the value of privacy is\nreducible to these other values or sources of value (Thomson 1975).\nProposals that have been defended along these lines mention property\nrights, security, autonomy, intimacy or friendship, democracy,\nliberty, dignity, or utility and economic value. Reductionist accounts\nhold that the importance of privacy should be explained and its\nmeaning clarified in terms of those other values and sources of value\n(Westin 1967). The opposing view holds that privacy is valuable in\nitself and its value and importance are not derived from other\nconsiderations (see for a discussion Rössler 2004). Views that\nconstrue privacy and the personal sphere of life as a human right\nwould be an example of this non-reductionist conception. \nMore recently a type of privacy account has been proposed in relation\nto new information technology, which acknowledges that there is a\ncluster of related moral claims underlying appeals to privacy, but\nmaintains that there is no single essential core of privacy\nconcerns. This approach is referred to as cluster accounts (DeCew\n1997; Solove 2006; van den Hoven 1999; Allen 2011; Nissenbaum\n2004). \nFrom a descriptive perspective, a recent further addition to the body of\nprivacy accounts are epistemic accounts, where the notion of privacy\nis analyzed primarily in terms of knowledge or other epistemic states.\nHaving privacy means that others don’t know certain private\npropositions; lacking privacy means that others do know certain\nprivate propositions (Blaauw 2013). An important aspect of this\nconception of having privacy is that it is seen as a relation (Rubel\n2011; Matheson 2007; Blaauw 2013) with three argument places: a\nsubject (S), a set of propositions (P) and a set of\nindividuals (I). Here S is the subject who has (a\ncertain degree of) privacy. P is composed of those propositions\nthe subject wants to keep private (call the propositions in this set\n‘personal propositions’), and I is composed of\nthose individuals with respect to whom S wants to keep the\npersonal propositions private. \nAnother distinction that is useful to make is the one between a\nEuropean and a US American approach. A bibliometric study suggests\nthat the two approaches are separate in the literature. The first\nconceptualizes issues of informational privacy in terms of ‘data\nprotection’, the second in terms of ‘privacy’\n(Heersmink et al. 2011). In discussing the relationship of privacy\nmatters with technology, the notion of data protection is most\nhelpful, since it leads to a relatively clear picture of what the\nobject of protection is and by which technical means the data can be\nprotected. At the same time it invites answers to the question why the\ndata ought to be protected, pointing to a number of distinctive moral\ngrounds on the basis of which technical, legal and institutional\nprotection of personal data can be justified. Informational privacy is\nthus recast in terms of the protection of personal data (van den Hoven\n2008). This account shows how Privacy, Technology and Data Protection\nare related, without conflating Privacy and Data Protection. \nPersonal information or data is information or data that is linked or\ncan be linked to individual persons. Examples include explicitly\nstated characteristics such as a person‘s date of birth, sexual\npreference, whereabouts, religion, but also the IP address of your\ncomputer or metadata pertaining to these kinds of information. In\naddition, personal data can also be more implicit in the form of\nbehavioural data, for example from social media, that can be linked to\nindividuals. Personal data can be contrasted with data that is\nconsidered sensitive, valuable or important for other reasons, such as\nsecret recipes, financial data, or military intelligence. Data used to\nsecure other information, such as passwords, are not considered\nhere. Although such security measures (passwords) may contribute to\nprivacy, their protection is only instrumental to the protection of\nother (more private) information, and the quality of such security\nmeasures is therefore out of the scope of our considerations here. \nA relevant distinction that has been made in philosophical semantics\nis that between the referential and the attributive use of descriptive\nlabels of persons (van den Hoven 2008). Personal data is defined in\nthe law as data that can be linked with a natural person. There are\ntwo ways in which this link can be made; a referential mode and a\nnon-referential mode. The law is primarily concerned with the\n‘referential use’ of descriptions or attributes, the type\nof use that is made on the basis of a (possible) acquaintance\nrelationship of the speaker with the object of his knowledge.\n“The murderer of Kennedy must be insane”, uttered while\npointing to him in court is an example of a referentially used\ndescription. This can be contrasted with descriptions that are used\nattributively as in “the murderer of Kennedy must be insane,\nwhoever he is”. In this case, the user of the description is\nnot – and may never be – acquainted with the person he is\ntalking about or intends to refer to. If the legal definition of\npersonal data is interpreted referentially, much of the data that\ncould at some point in time be brought to bear on persons would be\nunprotected; that is, the processing of this data would not be\nconstrained on moral grounds related to privacy or personal sphere of\nlife, since it does not “refer” to persons in a straightforward\nway and therefore does not constitute “personal data” in a strict\nsense.  \nThe following types of moral reasons for the protection of personal\ndata and for providing direct or indirect control over access to those\ndata by others can be distinguished (van den Hoven 2008): \nThese considerations all provide good moral reasons for limiting and\nconstraining access to personal data and providing individuals with\ncontrol over their data. \nAcknowledging that there are moral reasons for protecting personal\ndata, data protection laws are in force in almost all countries. The\nbasic moral principle underlying these laws is the requirement of\ninformed consent for processing by the data subject, providing the\nsubject (at least in principle) with control over potential negative\neffects as discussed above. Furthermore, processing of personal\ninformation requires that its purpose be specified, its use be\nlimited, individuals be notified and allowed to correct inaccuracies,\nand the holder of the data be accountable to oversight authorities\n(OECD 1980). Because it is impossible to guarantee compliance of all\ntypes of data processing in all these areas and applications with\nthese rules and laws in traditional ways, so-called\n“privacy-enhancing technologies” (PETs) and identity management\nsystems are expected to replace human oversight in many cases. The\nchallenge with respect to privacy in the twenty-first century is to\nassure that technology is designed in such a way that it incorporates\nprivacy requirements in the software, architecture, infrastructure,\nand work processes in a way that makes privacy violations unlikely to\noccur. New generations of privacy regulations (e.g. GDPR) now require\nstandardly a “privacy by design” approach. The data ecosystems and\nsocio-technical systems, supply chains, organisations, including\nincentive structures, business processes, and technical hardware and\nsoftware, training of personnel, should all be designed in such a way that\nthe likelihood of privacy violations is a low as possible. \nThe debates about privacy are almost always revolving around new\ntechnology, ranging from genetics and the extensive study of\nbio-markers, brain imaging, drones, wearable sensors and sensor\nnetworks, social media, smart phones, closed circuit television, to\ngovernment cybersecurity programs, direct marketing, surveillance,\nRFID tags, big data, head-mounted displays and search engines. The\nimpact of some of these new technologies, with a particular focus on\ninformation technology, is discussed in this section. \n“Information technology” refers to automated systems for\nstoring, processing, and distributing information. Typically, this\ninvolves the use of computers and communication networks. The amount\nof information that can be stored or processed in an information\nsystem depends on the technology used. The capacity of the technology\nhas increased rapidly over the past decades, in accordance with\nMoore’s law. This holds for storage capacity, processing capacity, and\ncommunication bandwidth. We are now capable of storing and processing\ndata on the exabyte level. For illustration, to store 100 exabytes of\ndata on 720 MB CD-ROM discs would require a stack of them that would\nalmost reach the moon. \nThese developments have fundamentally changed our practices of\ninformation provisioning. The rapid changes have increased the need\nfor careful consideration of the desirability of effects. Some even\nspeak of a digital revolution as a technological leap similar to the\nindustrial revolution, or a digital revolution as a revolution in\nunderstanding human nature and the world, similar to the revolutions\nof Copernicus, Darwin and Freud (Floridi 2008). In both the technical\nand the epistemic sense, emphasis has been put on connectivity and\ninteraction. Physical space has become less important, information is\nubiquitous, and social relations have adapted as well. \nAs we have described privacy in terms of moral reasons for imposing\nconstraints on access to and/or use of personal information, the\nincreased connectivity imposed by information technology poses many\nquestions. In a descriptive sense, access has increased, which, in a\nnormative sense, requires consideration of the desirability of this\ndevelopment, and evaluation of the potential for regulation by\ntechnology (Lessig 1999), institutions, and/or law. \nAs connectivity increases access to information, it also increases the\npossibility for agents to act based on the new sources of\ninformation. When these sources contain personal information, risks of\nharm, inequality, discrimination, and loss of autonomy easily emerge.\nFor example, your enemies may have less difficulty finding out where\nyou are, users may be tempted to give up privacy for perceived\nbenefits in online environments, and employers may use online\ninformation to avoid hiring certain groups of people. Furthermore,\nsystems rather than users may decide which information is displayed,\nthus confronting users only with news that matches their profiles. \nAlthough the technology operates on a device level, information\ntechnology consists of a complex system of socio-technical practices,\nand its context of use forms the basis for discussing its role in\nchanging possibilities for accessing information, and thereby\nimpacting privacy. We will discuss some specific developments and\ntheir impact in the following sections. \nThe Internet, originally conceived in the 1960s and developed in the\n1980s as a scientific network for exchanging information, was not\ndesigned for the purpose of separating information flows (Michener\n1999). The World Wide Web of today was not foreseen, and neither was\nthe possibility of misuse of the Internet. Social network sites\nemerged for use within a community of people who knew each other in\nreal life – at first, mostly in academic settings – rather\nthan being developed for a worldwide community of users (Ellison\n2007). It was assumed that sharing with close friends would not cause\nany harm, and privacy and security only appeared on the agenda when\nthe network grew larger. This means that privacy concerns often had to\nbe dealt with as add-ons rather than by-design. \nA major theme in the discussion of Internet privacy revolves around\nthe use of cookies (Palmer 2005). Cookies are small pieces of data\nthat web sites store on the user’s computer, in order to enable\npersonalization of the site. However, some cookies can be used to\ntrack the user across multiple web sites (tracking cookies), enabling\nfor example advertisements for a product the user has recently viewed\non a totally different site. Again, it is not always clear what the\ngenerated information is used for. Laws requiring user consent for the\nuse of cookies are not always successful in terms of increasing the\nlevel of control, as the consent requests interfere with task flows,\nand the user may simply click away any requests for consent (Leenes\n& Kosta 2015). Similarly, features of social network sites\nembedded in other sites (e.g. “like”-button) may allow\nthe social network site to identify the sites visited by the user\n(Krishnamurthy & Wills 2009). \nThe recent development of cloud computing increases the many privacy\nconcerns (Ruiter & Warnier 2011). Previously, whereas information\nwould be available from the web, user data and programs would still be\nstored locally, preventing program vendors from having access to the\ndata and usage statistics. In cloud computing, both data and programs\nare online (in the cloud), and it is not always clear what the\nuser-generated and system-generated data are used for. Moreover, as\ndata are located elsewhere in the world, it is not even always obvious\nwhich law is applicable, and which authorities can demand access to\nthe data. Data gathered by online services and apps such as search\nengines and games are of particular concern here. Which data are used\nand communicated by applications (browsing history, contact lists,\netc.) is not always clear, and even when it is, the only choice\navailable to the user may be not to use the application. \nSome special features of Internet privacy (social media and big data)\nare discussed in the following sections. \nSocial media pose additional challenges. The question is not merely\nabout the moral reasons for limiting access to information, it is also\nabout the moral reasons for limiting the invitations to users\nto submit all kinds of personal information. Social network sites\ninvite the user to generate more data, to increase the value of the\nsite (“your profile is …% complete”). Users are\ntempted to exchange their personal data for the benefits of\nusing services, and provide both this data and their attention as\npayment for the services. In addition, users may not even be aware of\nwhat information they are tempted to provide, as in the aforementioned\ncase of the “like”-button on other sites. Merely limiting\nthe access to personal information does not do justice to the issues\nhere, and the more fundamental question lies in steering the users’\nbehaviour of sharing. When the service is free, the data is needed as a form of payment. \nOne way of limiting the temptation of users to share is requiring\ndefault privacy settings to be strict. Even then, this limits access\nfor other users (“friends of friends”), but it does not\nlimit access for the service provider. Also, such restrictions limit\nthe value and usability of the social network sites themselves, and\nmay reduce positive effects of such services. A particular example of\nprivacy-friendly defaults is the opt-in as opposed to the opt-out\napproach. When the user has to take an explicit action to share data\nor to subscribe to a service or mailing list, the resulting effects\nmay be more acceptable to the user. However, much still depends on how\nthe choice is framed (Bellman, Johnson, & Lohse 2001). \nUsers generate loads of data when online. This is not only data\nexplicitly entered by the user, but also numerous statistics on user\nbehavior: sites visited, links clicked, search terms entered, etc. Data\nmining can be employed to extract patterns from such data, which can\nthen be used to make decisions about the user. These may only affect\nthe online experience (advertisements shown), but, depending on which\nparties have access to the information, they may also impact the user\nin completely different contexts. \nIn particular, big data may be used in profiling the user (Hildebrandt\n2008), creating patterns of typical combinations of user properties,\nwhich can then be used to predict interests and behavior. An innocent\napplication is “you may also like …”, but,\ndepending on the available data, more sensitive derivations may be\nmade, such as most probable religion or sexual preference. These\nderivations could then in turn lead to inequal treatment or\ndiscrimination.  When a user can be assigned to a particular group,\neven only probabilistically, this may influence the actions taken by\nothers (Taylor, Floridi, & Van der Sloot 2017). For example,\nprofiling could lead to refusal of insurance or a credit card, in\nwhich case profit is the main reason for discrimination. When such\ndecisions are based on profiling, it may be difficult to challenge\nthem or even find out the explanations behind them. Profiling could\nalso be used by organizations or possible future governments that have\ndiscrimination of particular groups on their political agenda, in\norder to find their targets and deny them access to services, or\nworse. \nBig data does not only emerge from Internet transactions. Similarly,\ndata may be collected when shopping, when being recorded by\nsurveillance cameras in public or private spaces, or when using\nsmartcard-based public transport payment systems. All these data could\nbe used to profile citizens, and base decisions upon such profiles.\nFor example, shopping data could be used to send information about\nhealthy food habits to particular individuals, but again also for\ndecisions on insurance. According to EU data protection law,\npermission is needed for processing personal data, and they can only\nbe processed for the purpose for which they were obtained. Specific\nchallenges, therefore, are (a) how to obtain permission when the user\ndoes not explicitly engage in a transaction (as in case of\nsurveillance), and (b) how to prevent “function creep”,\ni.e. data being used for different purposes after they are collected\n(as may happen for example with DNA databases (Dahl & Sætnan\n2009). \nOne particular concern could emerge from genetics and genomic data\n(Tavani 2004, Bruynseels & van den Hoven, 2015). Like other data, genomics can be used to make predictions, and in particular could predict risks of diseases.\nApart from others having access to detailed user profiles, a\nfundamental question here is whether the individual should know what\nis known about her. In general, users could be said to have a right to\naccess any information stored about them, but in this case, there may\nalso be a right not to know, in particular when knowledge of the data\n(e.g. risks of diseases) would reduce the well-being – by causing\nfear, for instance – without enabling treatment. With respect to\nprevious examples, one may not want to know the patterns in one’s own\nshopping behavior either. \nAs users increasingly own networked devices such as smart phones,\nmobile devices collect and send more and more data. These devices\ntypically contain a range of data-generating sensors, including GPS\n(location), movement sensors, and cameras, and may transmit the\nresulting data via the Internet or other networks. One particular\nexample concerns location data. Many mobile devices have a GPS sensor\nthat registers the user’s location, but even without a GPS sensor,\napproximate locations can be derived, for example by monitoring the\navailable wireless networks. As location data links the online world\nto the user’s physical environment, with the potential of physical\nharm (stalking, burglary during holidays, etc.), such data are often\nconsidered particularly sensitive. \nMany of these devices also contain cameras which, when applications\nhave access, can be used to take pictures. These can be considered\nsensors as well, and the data they generate may be particularly\nprivate. For sensors like cameras, it is assumed that the user is\naware when they are activated, and privacy depends on such knowledge.\nFor webcams, a light typically indicates whether the camera is on, but\nthis light may be manipulated by malicious software. In general,\n“reconfigurable technology” (Dechesne, Warnier, & van\nden Hoven 2011) that handles personal data raises the question of user\nknowledge of the configuration. \nDevices connected to the Internet are not limited to user-owned\ncomputing devices like smartphones. Many devices contain chips and/or\nare connected in the so-called Internet of Things. RFID (radio\nfrequency identification) chips can be read from a limited distance,\nsuch that you can hold them in front of a reader rather than inserting\nthem. EU and US passports have RFID chips with protected biometric\ndata, but information like the user’s nationality may easily leak when\nattempting to read such devices (see Richter, Mostowski & Poll\n2008, in Other Internet Resources). “Smart” RFIDs are also\nembedded in public transport payment systems. “Dumb”\nRFIDs, basically only containing a number, appear in many kinds of\nproducts as a replacement of the barcode, and for use in logistics.\nStill, such chips could be used to trace a person once it is known\nthat he carries an item containing a chip. \nIn the home, there are smart meters for automatically reading and\nsending electricity and water consumption, and thermostats and other devices\nthat can be remotely controlled by the owner. Such devices again\ngenerate statistics, and these can be used for mining and profiling.\nIn the future, more and more household appliances will be connected,\neach generating its own information. Ambient intelligence (Brey 2005),\nand ubiquitous computing, along with the Internet of Things\n(Friedewald & Raabe 2011), also enable automatic adaptation of the\nenvironment to the user, based on explicit preferences and implicit\nobservations, and user autonomy is a central theme in considering the\nprivacy implications of such devices. In general, the move towards a\nservice-oriented provisioning of goods, with suppliers being informed\nabout how the products are used through IT and associated\nconnectivity, requires consideration of the associated privacy and\ntransparency concerns (Pieters 2013). For example, users will need to\nbe informed when connected devices contain a microphone and how and\nwhen it is used. \nGovernment and public administration have undergone radical\ntransformations as a result of the availability of advanced IT systems\nas well. Examples of these changes are biometric passports, online\ne-government services, voting systems, a variety of online citizen\nparticipation tools and platforms or online access to recordings of\nsessions of parliament and government committee meetings.  \nConsider the case of voting in elections. Information technology may\nplay a role in different phases in the voting process, which may have\ndifferent impact on voter privacy. Most countries have a requirement\nthat elections are to be held by secret ballot, to prevent vote buying\nand coercion. In this case, the voter is supposed to keep her vote\nprivate, even if she would want to reveal it. For information\ntechnology used for casting votes, this is defined as the requirement\nof receipt-freeness or coercion-resistance (Delaune, Kremer & Ryan\n2006). In polling stations, the authorities see to it that the voter\nkeeps the vote private, but such surveillance is not possible when\nvoting by mail or online, and it cannot even be enforced by\ntechnological means, as someone can always watch while the voter\nvotes. In this case, privacy is not only a right but also a duty, and\ninformation technology developments play an important role in the\npossibilities of the voter to fulfill this duty, as well as the\npossibilities of the authorities to verify this. In a broader sense,\ne-democracy initiatives may change the way privacy is viewed in the\npolitical process. \nMore generally, privacy is important in democracy to prevent undue\ninfluence. While lack of privacy in the voting process could enable\nvote buying and coercion, there are more subtle ways of influencing\nthe democratic process, for example through targeted (mis)information\ncampaigns. Online (political) activities of citizens on for example\nsocial media facilitate such attempts because of the possibility of\ntargeting through behavioural profiling. Compared to offline political\nactivities, it is more difficult to hide preferences and activities,\nbreaches of confidentiality are more likely, and attempts to influence\nopinions become more scalable. \nInformation technology is used for all kinds of surveillance tasks. It\ncan be used to augment and extend traditional surveillance systems\nsuch as CCTV and other camera systems, for example to identify\nspecific individuals in crowds, using face recognition techniques, or\nto monitor specific places for unwanted behaviour. Such approaches\nbecome even more powerful when combined with other techniques, such as\nmonitoring of Internet-of-Things devices (Motlagh et al. 2017). \nBesides augmenting existing surveillance systems, ICT techniques are\nnowadays mainly used in the digital domain, typically grouped together\nunder the term “surveillance capitalism” (Zuboff 2019). Social\nmedia and other online systems are used to gather large amounts of\ndata about individuals – either “voluntary”, because users\nsubscribe to a specific service (Google, Facebook), or involuntary by\ngathering all kinds of user related data in a less transparent manner.\nData analysis and machine learning techniques are then used to\ngenerate prediction models of individual users that can be used, for\nexample, for targeted advertisement, but also for more malicious\nintents such as fraud or micro-targeting to influence elections\n(Albright 2016, Other Internet Resources) or referenda such as Brexit\n(Cadwalladr 2019, Other Internet Resources). \nIn addition to the private sector surveillance industry, governments\nform another traditional group that uses surveillance techniques at a\nlarge scale, either by intelligence services or law enforcement. These\ntypes of surveillance systems are typically justified with an appeal\nto the “greater good” and protecting citizens, but their\nuse is also controversial. For such systems, one would typically like\nto ensure that any negative effects on privacy are proportional to the\nbenefits achieved by the technology. Especially since these systems\nare typically shrouded in secrecy, it is difficult for outsiders to\nsee if such systems are used proportionally, or indeed useful for\ntheir tasks (Lawner 2002). This is particularly pressing when\ngovernments use private sector data or services for surveillance\npurposes.  The almost universal use of good encryption techniques\nin communication systems makes it also harder to gather effective\nsurveillance information, leading to more and more calls for “back\ndoors” that can exclusively be used by government in communication\nsystems. From a privacy standpoint this could be evaluated as\nunwanted, not only because it gives governments access to private\nconversations, but also because it lowers the overall security of\ncommunication systems that employ this technique (Abelson et al.\n2015). \nWhereas information technology is typically seen as the cause\nof privacy problems, there are also several ways in which information\ntechnology can help to solve these problems. There are rules,\nguidelines or best practices that can be used for designing\nprivacy-preserving systems. Such possibilities range from\nethically-informed design methodologies to using encryption to protect\npersonal information from unauthorized use. In particular, methods\nfrom the field of information security, aimed at protecting\ninformation against unauthorized access, can play a key role in the\nprotection of personal data. \nValue sensitive design provides a “theoretically grounded\napproach to the design of technology that accounts for human values in\na principled and comprehensive manner throughout the design\nprocess” (Friedman et al. 2006). It provides a set of rules and\nguidelines for designing a system with a certain value in mind. One\nsuch value can be ‘privacy’, and value sensitive design\ncan thus be used as a method to design privacy-friendly IT systems\n(Van den Hoven et al. 2015).  The ‘privacy by design’\napproach as advocated by Cavoukian (2009) and others can be regarded\nas one of the value sensitive design approaches that specifically\nfocuses on privacy (Warnier et al. 2015). More recently, approaches\nsuch as “privacy engineering” (Ceross & Simpson 2018)\nextend the privacy by design approach by aiming to provide a more\npractical, deployable set of methods by which to achieve system-wide\nprivacy. \nThe privacy by design approach provides high-level guidelines in the\nform of principles for designing privacy-preserving systems.\nThese principles have at their core that “data protection needs\nto be viewed in proactive rather than reactive terms, making privacy\nby design preventive and not simply remedial” (Cavoukian 2010).\nPrivacy by design’s main point is that data protection should be\ncentral in all phases of product life cycles, from initial design to\noperational use and disposal (see Colesky et al. 2016) for a critical\nanalysis of the privacy by design approach). The Privacy Impact\nAssessment approach proposed by Clarke (2009) makes a similar point.\nIt proposes “a systematic process for evaluating the potential\neffects on privacy of a project, initiative or proposed system or\nscheme” (Clarke 2009). Note that these approaches should not\nonly be seen as auditing approaches, but rather as a means to make\nprivacy awareness and compliance an integral part of the\norganizational and engineering culture. \nThere are also several industry guidelines that can be used to design\nprivacy preserving IT systems. The Payment Card Industry Data Security\nStandard (see PCI DSS v3.2, 2018, in the Other Internet Resources),\nfor example, gives very clear guidelines for privacy and security\nsensitive systems design in the domain of the credit card industry and\nits partners (retailers, banks). Various International Organization\nfor Standardization (ISO) standards (Hone & Eloff 2002) also serve\nas a source of best practices and guidelines, especially with respect\nto information security, for the design of privacy friendly systems.\nFurthermore, the principles that are formed by the EU Data Protection\nDirective, which are themselves based on the Fair Information\nPractices (Gellman 2014) from the early 70s – transparency,\npurpose, proportionality, access, transfer – are technologically\nneutral and as such can also be considered as high level ‘design\nprinciples’. Systems that are designed with these rules and\nguidelines in mind should thus – in principle – be in\ncompliance with EU privacy laws and respect the privacy of its\nusers. \nThe rules and principles described above give high-level guidance for\ndesigning privacy-preserving systems, but this does not mean that if\nthese methodologies are followed the resulting IT system will\n(automatically) be privacy friendly. Some design principles are rather\nvague and abstract. What does it mean to make a transparent design or\nto design for proportionality? The principles need to be interpreted\nand placed in a context when designing a specific system. But\ndifferent people will interpret the principles differently, which will\nlead to different design choices, with different effects on privacy.\nThere is also a difference between the design and the implementation\nof a computer system. During the implementation phase software bugs\nare introduced, some of which can be exploited to break the system and\nextract private information. How to implement bug-free computer\nsystems remains an open research question (Hoare 2003). In addition,\nimplementation is another phase wherein choices and interpretations\nare made: system designs can be implemented in infinitely many ways.\nMoreover, it is very hard to verify – for anything beyond\nnon-trivial systems – whether an implementation meets its\ndesign/specification (Loeckx, Sieber, & Stansifer 1985). This is\neven more difficult for non-functional requirements such as\n‘being privacy preserving’ or security properties in\ngeneral.  \nSome specific solutions to privacy problems aim at increasing the\nlevel of awareness and consent of the user. These solutions can be\nseen as an attempt to apply the notion of informed consent to privacy\nissues with technology (Custers et al. 2018). This is connected to the\nidea that privacy settings and policies should be explainable to users\n(Pieters 2011). For example, the Privacy Coach supports customers in\nmaking privacy decisions when confronted with RFID tags (Broenink et\nal. 2010). However, users have only a limited capability of dealing\nwith such choices, and providing too many choices may easily lead to\nthe problem of moral overload (van den Hoven, Lokhorst, & Van de\nPoel 2012). A technical solution is support for automatic matching of\na privacy policy set by the user against policies issued by web sites\nor apps. \nA growing number of software tools are available that provide some\nform of privacy (usually anonymity) for their users, such tools are\ncommonly known as privacy enhancing technologies (Danezis &\nGürses 2010, Other Internet Resources). Examples include\ncommunication-anonymizing tools such as Tor (Dingledine, Mathewson,\n& Syverson 2004) and Freenet (Clarke et al. 2001), and\nidentity-management systems for which many commercial software\npackages exist (see below). Communication anonymizing tools allow\nusers to anonymously browse the web (with Tor) or anonymously share\ncontent (Freenet). They employ a number of cryptographic techniques\nand security protocols in order to ensure their goal of anonymous\ncommunication. Both systems use the property that numerous users use\nthe system at the same time which provides k-anonymity (Sweeney\n2002): no individual can be uniquely distinguished from a group of\nsize k, for large values for k. Depending on the system,\nthe value of k can vary between a few hundred to hundreds of\nthousands. In Tor, messages are encrypted and routed along numerous\ndifferent computers, thereby obscuring the original sender of the\nmessage (and thus providing anonymity). Similarly, in Freenet content\nis stored in encrypted form from all users of the system. Since users\nthemselves do not have the necessary decryption keys, they do not know\nwhat kind of content is stored, by the system, on their own computer.\nThis provides plausible deniability and privacy. The system can at any\ntime retrieve the encrypted content and send it to different Freenet\nusers. \nPrivacy enhancing technologies also have their downsides. For example,\nTor, the tool that allows anonymized communication and browsing over\nthe Internet, is susceptible to an attack whereby, under certain\ncircumstances, the anonymity of the user is no longer guaranteed\n(Back, Möller, & Stiglic 2001; Evans, Dingledine, &\nGrothoff 2009). Freenet (and other tools) have similar problems\n(Douceur 2002). Note that for such attacks to work, an attacker needs\nto have access to large resources that in practice are only realistic\nfor intelligence agencies of countries. However, there are other\nrisks. Configuring such software tools correctly is difficult for the\naverage user, and when the tools are not correctly configured\nanonymity of the user is no longer guaranteed. And there is always the\nrisk that the computer on which the privacy-preserving software runs\nis infected by a Trojan horse (or other digital pest) that monitors\nall communication and knows the identity of the user. \nAnother option for providing anonymity is the anonymization of data\nthrough special software. Tools exist that remove patient names and\nreduce age information to intervals: the age 35 is then represented as\nfalling in the range 30–40. The idea behind such anonymization\nsoftware is that a record can no longer be linked to an individual,\nwhile the relevant parts of the data can still be used for scientific\nor other purposes. The problem here is that it is very hard to\nanonymize data in such a way that all links with an individual are\nremoved and the resulting anonymized data is still useful for research\npurposes. Researchers have shown that it is almost always possible to\nreconstruct links with individuals by using sophisticated statistical\nmethods (Danezis, Diaz, & Troncoso 2007) and by combining multiple\ndatabases (Anderson 2008) that contain personal information.\nTechniques such as k-anonymity might also help to generalize\nthe data enough to make it unfeasible to de-anonymize data (LeFevre et\nal. 2005). \nCryptography has long been used as a means to protect data, dating\nback to the Caesar cipher more than two thousand years ago. Modern\ncryptographic techniques are essential in any IT system that needs to\nstore (and thus protect) personal data, for example by providing\nsecure (confidential) connections for browsing (HTTPS) and networking\n(VPN). Note however that by itself cryptography does not provide any\nprotection against data breaching; only when applied correctly in a\nspecific context does it become a ‘fence’ around personal\ndata. In addition, cryptographic schemes that become outdated by\nfaster computers or new attacks may pose threats to (long-term)\nprivacy. Cryptography is a large\nfield, so any description here will be incomplete. The focus will be\ninstead on some newer cryptographic techniques, in particular\nhomomorphic encryption, that have the potential to become very\nimportant for processing and searching in personal data. \nVarious techniques exist for searching through encrypted data (Song et\nal. 2000, Wang et al. 2016), which provides a form of privacy\nprotection (the data is encrypted) and selective access to sensitive\ndata. One relatively new technique that can be used for designing\nprivacy-preserving systems is ‘homomorphic encryption’\n(Gentry 2009, Acar et al. 2018). Homomorphic encryption allows a data\nprocessor to process encrypted data, i.e. users could send personal\ndata in encrypted form and get back some useful results – for\nexample, recommendations of movies that online friends like – in\nencrypted form. The original user can then again decrypt the result\nand use it without revealing any personal data to the data processor.\nHomomorphic encryption, for example, could be used to aggregate\nencrypted data thereby allowing both privacy protection and useful\n(anonymized) aggregate information. The technique is currently not\nwidely applied; there are serious performance issues if one wants to\napply full homomorphic encryption to the large amounts of data stored\nin today’s systems. However, variants of the original homomorphic\nencryption scheme are emerging, such as Somewhat Homomorphic\nEncryption (Badawi et al. 2018), that are showing promise to be more\nwidely applied in practice. \nThe main idea behind blockchain technology was first described in the\nseminal paper on Bitcoins (Nakamoto, n.d., Other Internet Resources).\nA blockchain is basically a distributed ledger that stores\ntransactions in a non-reputable way, without the use of a trusted\nthird party. Cryptography is used to ensure that all transactions are\n“approved” by members of the blockchain and stored in such a way\nthat they are linked to previous transactions and cannot be removed.\nAlthough focused on data integrity and not inherently anonymous,\nblockchain technology enables many privacy-related applications\n(Yli-Huumo et al. 2016, Karame and Capkun 2018), such as anonymous\ncryptocurrency (Narayanan et al. 2016) and self-sovereign identity\n(see below). \nThe use and management of user’s online identifiers are crucial in the\ncurrent Internet and social networks. Online reputations become more\nand more important, both for users and for companies. In the era of\nbig data correct information about users has an\nincreasing monetary value. \n‘Single sign on’ frameworks, provided by independent third\nparties (OpenID) but also by large companies such as Facebook,\nMicrosoft and Google (Ko et al. 2010), make it easy for users to\nconnect to numerous online services using a single online identity.\nThese online identities are usually directly linked to the real world\n(off line) identities of individuals; indeed Facebook, Google and\nothers require this form of log on (den Haak 2012). Requiring a direct\nlink between online and ‘real world’ identities is\nproblematic from a privacy perspective, because they allow profiling\nof users (Benevenuto et al. 2012). Not all users will realize how\nlarge the amount of data is that companies gather in this manner, or\nhow easy it is to build a detailed profile of users. Profiling becomes\neven easier if the profile information is combined with other\ntechniques such as implicit authentication via cookies and tracking\ncookies (Mayer & Mitchell 2012). \nFrom a privacy perspective a better solution would be the use of\nattribute-based authentication (Goyal et al. 2006) which allows access\nof online services based on the attributes of users, for example their\nfriends, nationality, age etc. Depending on the attributes used, they\nmight still be traced back to specific individuals, but this is no\nlonger crucial. In addition, users can no longer be tracked to\ndifferent services because they can use different attributes to access\ndifferent services which makes it difficult to trace online identities\nover multiple transactions, thus providing unlinkability for the user.\nRecently (Allen 2016, Other Internet Resources), the concept of\nself-sovereign identity has emerged, which aims for users to have\ncomplete ownership and control about their own digital identities.\nBlockchain technology is used to make it possible for users to control\na digital identity without the use of a traditional trusted third\nparty (Baars 2016). \nIn the previous sections, we have outlined how current technologies\nmay impact privacy, as well as how they may contribute to mitigating\nundesirable effects. However, there are future and emerging\ntechnologies that may have an even more profound impact. Consider for\nexample brain-computer interfaces. In case computers are connected\ndirectly to the brain, not only behavioral characteristics are subject\nto privacy considerations, but even one’s thoughts run the risk of\nbecoming public, with decisions of others being based upon them. In\naddition, it could become possible to change one’s behavior by means\nof such technology. Such developments therefore require further\nconsideration of the reasons for protecting privacy. In particular,\nwhen brain processes could be influenced from the outside, autonomy\nwould be a value to reconsider to ensure adequate protection. \nApart from evaluating information technology against current moral\nnorms, one also needs to consider the possibility that technological\nchanges influence the norms themselves (Boenink, Swierstra &\nStemerding 2010). Technology thus does not only influence privacy by\nchanging the accessibility of information, but also by changing the\nprivacy norms themselves. For example, social networking sites invite\nusers to share more information than they otherwise might. This\n“oversharing” becomes accepted practice within certain\ngroups. With future and emerging technologies, such influences can\nalso be expected and therefore they ought to be taken into account\nwhen trying to mitigate effects. \nAnother fundamental question is whether, given the future (and even\ncurrent) level of informational connectivity, it is feasible to\nprotect privacy by trying to hide information from parties who may use\nit in undesirable ways. Gutwirth & De Hert (2008) argue that it\nmay be more feasible to protect privacy by transparency – by\nrequiring actors to justify decisions made about individuals, thus\ninsisting that decisions are not based on illegitimate information.\nThis approach comes with its own problems, as it might be hard to\nprove that the wrong information was used for a decision. Still, it\nmay well happen that citizens, in turn, start data collection on those\nwho collect data about them, e.g. governments. Such\n“counter(sur)veillance” may be used to gather information\nabout the use of information, thereby improving accountability\n(Gürses et al. 2016). The open source movement may also\ncontribute to transparency of data processing. In this context,\ntransparency can be seen as a pro-ethical condition contributing to\nprivacy (Turilli & Floridi 2009). \nIt has been argued that the precautionary principle, well known in\nenvironmental ethics, might have a role in dealing with emerging\ninformation technologies as well (Pieters & van Cleeff 2009; Som,\nHilty & Köhler 2009). The principle would see to it that the\nburden of proof for absence of irreversible effects of information\ntechnology on society, e.g. in terms of power relations and equality,\nwould lie with those advocating the new technology. Precaution, in\nthis sense, could then be used to impose restrictions at a regulatory\nlevel, in combination with or as an alternative to empowering users,\nthereby potentially contributing to the prevention of informational\noverload on the user side. Apart from general debates about the\ndesirable and undesirable features of the precautionary principle,\nchallenges to it lie in its translation to social effects and social\nsustainability, as well as to its application to consequences induced\nby intentional actions of agents. Whereas the occurrence of natural\nthreats or accidents is probabilistic in nature, those who are\ninterested in improper use of information behave strategically,\nrequiring a different approach to risk (i.e. security as opposed to\nsafety). In addition, proponents of precaution will need to balance it\nwith other important principles, viz., of informed consent and\nautonomy. \nFinally, it is appropriate to note that not all social effects of\ninformation technology concern privacy (Pieters 2017). Examples\ninclude the effects of social network sites on friendship, and the\nverifiability of results of electronic elections. Therefore,\nvalue-sensitive design approaches and impact assessments of\ninformation technology should not focus on privacy only, since\ninformation technology affects many other values as well.","contact.mail":"W.Pieters@tudelft.nl","contact.domain":"tudelft.nl"},{"date.published":"2014-11-20","date.changed":"2019-10-30","url":"https://plato.stanford.edu/entries/it-privacy/","author1":"Jeroen van den Hoven","author2":"Martijn Blaauw","author1.info":"http://homepage.tudelft.nl/e7x9k/","author2.info":"http://homepage.tudelft.nl/68x7e/","entry":"it-privacy","body.text":"\n\n\nHuman beings value their privacy and the protection of their personal\nsphere of life. They value some control over who knows what about\nthem. They certainly do not want their personal information to be\naccessible to just anyone at any time. But recent advances in\ninformation technology threaten privacy and have reduced the amount of\ncontrol over personal data and open up the possibility of a range of\nnegative consequences as a result of access to personal data. In the\nsecond half of the 20th century data protection regimes\nhave been put in place as a response to increasing levels of\nprocessing of personal data. The 21st century has become\nthe century of big data and advanced information technology (e.g.\nforms of deep learning), the rise of big tech companies and the\nplatform economy, which comes with the storage and processing of\nexabytes of data.\n\n\nThe revelations of Edward Snowden, and more recently the Cambridge\nAnalytica case (Cadwalladr & Graham-Harrison 2018) have\ndemonstrated that worries about negative consequences are real. The\ntechnical capabilities to collect, store and search large quantities\nof data concerning telephone conversations, internet searches and\nelectronic payment are now in place and are routinely used by\ngovernment agencies and corporate actors alike. The rise of China and\nthe large scale of use and spread of advanced digital technologies for\nsurveillance and control have only added to the concern of many. For\nbusiness firms, personal data about customers and potential customers\nare now also a key asset. The scope and purpose of the personal data\ncentred business models of Big Tech (Google, Amazon, Facebook,\nMicrosoft, Apple) has been described in detail by Shoshana Zuboff\n(2018) under the label “surveillance capitalism”. \n\n\nAt the same time, the meaning and value of privacy remains the subject\nof considerable controversy. The combination of increasing power of\nnew technology and the declining clarity and agreement on privacy give\nrise to problems concerning law, policy and ethics. Many of these\nconceptual debates and issues are situated in the context of\ninterpretation and analysis of the General Data Protection Regulation\n(GDPR) that was adopted by the EU in spring 2018 as the successor of\nthe EU 1995 Directives, with application far beyond the borders of the\nEuropean Union.\n\n\nThe focus of this article is on exploring the relationship between\ninformation technology and privacy. We will both illustrate the\nspecific threats that IT and innovations in IT pose for privacy and\nindicate how IT itself might be able to overcome these\nprivacy concerns by being developed in ways that can be termed\n“privacy-sensitive”, “privacy enhancing” or “privacy\nrespecting”. We will also discuss the role of emerging technologies\nin the debate, and account for the way in which moral debates are\nthemselves affected by IT.\n\nDiscussions about privacy are intertwined with the use of technology.\nThe publication that began the debate about privacy in the Western\nworld was occasioned by the introduction of the newspaper printing\npress and photography. Samuel D. Warren and Louis Brandeis wrote their\narticle on privacy in the Harvard Law Review (Warren & Brandeis\n1890) partly in protest against the intrusive activities of the\njournalists of those days. They argued that there is a “right to\nbe left alone” based on a principle of “inviolate\npersonality”. Since the publication of that article, the debate\nabout privacy has been fuelled by claims regarding the right of\nindividuals to determine the extent to which others have access to\nthem (Westin 1967) and claims regarding the right of society to know\nabout individuals. Information being a cornerstone of access to\nindividuals, the privacy debate has co-evolved with – and in\nresponse to – the development of information technology. It is\ntherefore difficult to conceive of the notions of privacy and\ndiscussions about data protection as separate from the way computers,\nthe Internet, mobile computing and the many applications of these\nbasic technologies have evolved. \nInspired by subsequent developments in U.S. law, a distinction can be\nmade between (1) constitutional (or decisional)\nprivacy and (2) tort (or informational)\nprivacy (DeCew 1997). The first refers to the freedom to make\none’s own decisions without interference by others in regard to\nmatters seen as intimate and personal, such as the decision to use\ncontraceptives or to have an abortion. The second is concerned with\nthe interest of individuals in exercising control over access to\ninformation about themselves and is most often referred to as\n“informational privacy”. Think here, for instance, about\ninformation disclosed on Facebook or other social media. All too\neasily, such information might be beyond the control of the\nindividual. \nStatements about privacy can be either descriptive or normative,\ndepending on whether they are used to describe the way people define\nsituations and conditions of privacy and the way they value them, or\nare used to indicate that there ought to be constraints on the use of\ninformation or information processing. These conditions or constraints\ntypically involve personal information regarding individuals, or ways\nof information processing that may affect individuals. Informational\nprivacy in a normative sense refers typically to a non-absolute moral\nright of persons to have direct or indirect control over access to (1)\ninformation about oneself, (2) situations in which others could\nacquire information about oneself, and (3) technology that can be used\nto generate, process or disseminate information about oneself. \nThe debates about privacy are almost always revolving around new\ntechnology, ranging from genetics and the extensive study of\nbio-markers, brain imaging, drones, wearable sensors and sensor\nnetworks, social media, smart phones, closed circuit television, to\ngovernment cybersecurity programs, direct marketing, RFID tags, Big\nData, head-mounted displays and search engines. There are basically\ntwo reactions to the flood of new technology and its impact on\npersonal information and privacy: the first reaction, held by many\npeople in IT industry and in R&D, is that we have zero privacy in\nthe digital age and that there is no way we can protect it, so we\nshould get used to the new world and get over it (Sprenger 1999). The other reaction\nis that our privacy is more important than ever and that we can and we\nmust attempt to protect it. \nIn the literature on privacy, there are many competing accounts of the\nnature and value of privacy (Negley 1966, Rössler 2005). On one end of\nthe spectrum, reductionist accounts argue that privacy claims\nare really about other values and other things that matter from a\nmoral point of view. According to these views the value of privacy is\nreducible to these other values or sources of value (Thomson 1975).\nProposals that have been defended along these lines mention property\nrights, security, autonomy, intimacy or friendship, democracy,\nliberty, dignity, or utility and economic value. Reductionist accounts\nhold that the importance of privacy should be explained and its\nmeaning clarified in terms of those other values and sources of value\n(Westin 1967). The opposing view holds that privacy is valuable in\nitself and its value and importance are not derived from other\nconsiderations (see for a discussion Rössler 2004). Views that\nconstrue privacy and the personal sphere of life as a human right\nwould be an example of this non-reductionist conception. \nMore recently a type of privacy account has been proposed in relation\nto new information technology, which acknowledges that there is a\ncluster of related moral claims underlying appeals to privacy, but\nmaintains that there is no single essential core of privacy\nconcerns. This approach is referred to as cluster accounts (DeCew\n1997; Solove 2006; van den Hoven 1999; Allen 2011; Nissenbaum\n2004). \nFrom a descriptive perspective, a recent further addition to the body of\nprivacy accounts are epistemic accounts, where the notion of privacy\nis analyzed primarily in terms of knowledge or other epistemic states.\nHaving privacy means that others don’t know certain private\npropositions; lacking privacy means that others do know certain\nprivate propositions (Blaauw 2013). An important aspect of this\nconception of having privacy is that it is seen as a relation (Rubel\n2011; Matheson 2007; Blaauw 2013) with three argument places: a\nsubject (S), a set of propositions (P) and a set of\nindividuals (I). Here S is the subject who has (a\ncertain degree of) privacy. P is composed of those propositions\nthe subject wants to keep private (call the propositions in this set\n‘personal propositions’), and I is composed of\nthose individuals with respect to whom S wants to keep the\npersonal propositions private. \nAnother distinction that is useful to make is the one between a\nEuropean and a US American approach. A bibliometric study suggests\nthat the two approaches are separate in the literature. The first\nconceptualizes issues of informational privacy in terms of ‘data\nprotection’, the second in terms of ‘privacy’\n(Heersmink et al. 2011). In discussing the relationship of privacy\nmatters with technology, the notion of data protection is most\nhelpful, since it leads to a relatively clear picture of what the\nobject of protection is and by which technical means the data can be\nprotected. At the same time it invites answers to the question why the\ndata ought to be protected, pointing to a number of distinctive moral\ngrounds on the basis of which technical, legal and institutional\nprotection of personal data can be justified. Informational privacy is\nthus recast in terms of the protection of personal data (van den Hoven\n2008). This account shows how Privacy, Technology and Data Protection\nare related, without conflating Privacy and Data Protection. \nPersonal information or data is information or data that is linked or\ncan be linked to individual persons. Examples include explicitly\nstated characteristics such as a person‘s date of birth, sexual\npreference, whereabouts, religion, but also the IP address of your\ncomputer or metadata pertaining to these kinds of information. In\naddition, personal data can also be more implicit in the form of\nbehavioural data, for example from social media, that can be linked to\nindividuals. Personal data can be contrasted with data that is\nconsidered sensitive, valuable or important for other reasons, such as\nsecret recipes, financial data, or military intelligence. Data used to\nsecure other information, such as passwords, are not considered\nhere. Although such security measures (passwords) may contribute to\nprivacy, their protection is only instrumental to the protection of\nother (more private) information, and the quality of such security\nmeasures is therefore out of the scope of our considerations here. \nA relevant distinction that has been made in philosophical semantics\nis that between the referential and the attributive use of descriptive\nlabels of persons (van den Hoven 2008). Personal data is defined in\nthe law as data that can be linked with a natural person. There are\ntwo ways in which this link can be made; a referential mode and a\nnon-referential mode. The law is primarily concerned with the\n‘referential use’ of descriptions or attributes, the type\nof use that is made on the basis of a (possible) acquaintance\nrelationship of the speaker with the object of his knowledge.\n“The murderer of Kennedy must be insane”, uttered while\npointing to him in court is an example of a referentially used\ndescription. This can be contrasted with descriptions that are used\nattributively as in “the murderer of Kennedy must be insane,\nwhoever he is”. In this case, the user of the description is\nnot – and may never be – acquainted with the person he is\ntalking about or intends to refer to. If the legal definition of\npersonal data is interpreted referentially, much of the data that\ncould at some point in time be brought to bear on persons would be\nunprotected; that is, the processing of this data would not be\nconstrained on moral grounds related to privacy or personal sphere of\nlife, since it does not “refer” to persons in a straightforward\nway and therefore does not constitute “personal data” in a strict\nsense.  \nThe following types of moral reasons for the protection of personal\ndata and for providing direct or indirect control over access to those\ndata by others can be distinguished (van den Hoven 2008): \nThese considerations all provide good moral reasons for limiting and\nconstraining access to personal data and providing individuals with\ncontrol over their data. \nAcknowledging that there are moral reasons for protecting personal\ndata, data protection laws are in force in almost all countries. The\nbasic moral principle underlying these laws is the requirement of\ninformed consent for processing by the data subject, providing the\nsubject (at least in principle) with control over potential negative\neffects as discussed above. Furthermore, processing of personal\ninformation requires that its purpose be specified, its use be\nlimited, individuals be notified and allowed to correct inaccuracies,\nand the holder of the data be accountable to oversight authorities\n(OECD 1980). Because it is impossible to guarantee compliance of all\ntypes of data processing in all these areas and applications with\nthese rules and laws in traditional ways, so-called\n“privacy-enhancing technologies” (PETs) and identity management\nsystems are expected to replace human oversight in many cases. The\nchallenge with respect to privacy in the twenty-first century is to\nassure that technology is designed in such a way that it incorporates\nprivacy requirements in the software, architecture, infrastructure,\nand work processes in a way that makes privacy violations unlikely to\noccur. New generations of privacy regulations (e.g. GDPR) now require\nstandardly a “privacy by design” approach. The data ecosystems and\nsocio-technical systems, supply chains, organisations, including\nincentive structures, business processes, and technical hardware and\nsoftware, training of personnel, should all be designed in such a way that\nthe likelihood of privacy violations is a low as possible. \nThe debates about privacy are almost always revolving around new\ntechnology, ranging from genetics and the extensive study of\nbio-markers, brain imaging, drones, wearable sensors and sensor\nnetworks, social media, smart phones, closed circuit television, to\ngovernment cybersecurity programs, direct marketing, surveillance,\nRFID tags, big data, head-mounted displays and search engines. The\nimpact of some of these new technologies, with a particular focus on\ninformation technology, is discussed in this section. \n“Information technology” refers to automated systems for\nstoring, processing, and distributing information. Typically, this\ninvolves the use of computers and communication networks. The amount\nof information that can be stored or processed in an information\nsystem depends on the technology used. The capacity of the technology\nhas increased rapidly over the past decades, in accordance with\nMoore’s law. This holds for storage capacity, processing capacity, and\ncommunication bandwidth. We are now capable of storing and processing\ndata on the exabyte level. For illustration, to store 100 exabytes of\ndata on 720 MB CD-ROM discs would require a stack of them that would\nalmost reach the moon. \nThese developments have fundamentally changed our practices of\ninformation provisioning. The rapid changes have increased the need\nfor careful consideration of the desirability of effects. Some even\nspeak of a digital revolution as a technological leap similar to the\nindustrial revolution, or a digital revolution as a revolution in\nunderstanding human nature and the world, similar to the revolutions\nof Copernicus, Darwin and Freud (Floridi 2008). In both the technical\nand the epistemic sense, emphasis has been put on connectivity and\ninteraction. Physical space has become less important, information is\nubiquitous, and social relations have adapted as well. \nAs we have described privacy in terms of moral reasons for imposing\nconstraints on access to and/or use of personal information, the\nincreased connectivity imposed by information technology poses many\nquestions. In a descriptive sense, access has increased, which, in a\nnormative sense, requires consideration of the desirability of this\ndevelopment, and evaluation of the potential for regulation by\ntechnology (Lessig 1999), institutions, and/or law. \nAs connectivity increases access to information, it also increases the\npossibility for agents to act based on the new sources of\ninformation. When these sources contain personal information, risks of\nharm, inequality, discrimination, and loss of autonomy easily emerge.\nFor example, your enemies may have less difficulty finding out where\nyou are, users may be tempted to give up privacy for perceived\nbenefits in online environments, and employers may use online\ninformation to avoid hiring certain groups of people. Furthermore,\nsystems rather than users may decide which information is displayed,\nthus confronting users only with news that matches their profiles. \nAlthough the technology operates on a device level, information\ntechnology consists of a complex system of socio-technical practices,\nand its context of use forms the basis for discussing its role in\nchanging possibilities for accessing information, and thereby\nimpacting privacy. We will discuss some specific developments and\ntheir impact in the following sections. \nThe Internet, originally conceived in the 1960s and developed in the\n1980s as a scientific network for exchanging information, was not\ndesigned for the purpose of separating information flows (Michener\n1999). The World Wide Web of today was not foreseen, and neither was\nthe possibility of misuse of the Internet. Social network sites\nemerged for use within a community of people who knew each other in\nreal life – at first, mostly in academic settings – rather\nthan being developed for a worldwide community of users (Ellison\n2007). It was assumed that sharing with close friends would not cause\nany harm, and privacy and security only appeared on the agenda when\nthe network grew larger. This means that privacy concerns often had to\nbe dealt with as add-ons rather than by-design. \nA major theme in the discussion of Internet privacy revolves around\nthe use of cookies (Palmer 2005). Cookies are small pieces of data\nthat web sites store on the user’s computer, in order to enable\npersonalization of the site. However, some cookies can be used to\ntrack the user across multiple web sites (tracking cookies), enabling\nfor example advertisements for a product the user has recently viewed\non a totally different site. Again, it is not always clear what the\ngenerated information is used for. Laws requiring user consent for the\nuse of cookies are not always successful in terms of increasing the\nlevel of control, as the consent requests interfere with task flows,\nand the user may simply click away any requests for consent (Leenes\n& Kosta 2015). Similarly, features of social network sites\nembedded in other sites (e.g. “like”-button) may allow\nthe social network site to identify the sites visited by the user\n(Krishnamurthy & Wills 2009). \nThe recent development of cloud computing increases the many privacy\nconcerns (Ruiter & Warnier 2011). Previously, whereas information\nwould be available from the web, user data and programs would still be\nstored locally, preventing program vendors from having access to the\ndata and usage statistics. In cloud computing, both data and programs\nare online (in the cloud), and it is not always clear what the\nuser-generated and system-generated data are used for. Moreover, as\ndata are located elsewhere in the world, it is not even always obvious\nwhich law is applicable, and which authorities can demand access to\nthe data. Data gathered by online services and apps such as search\nengines and games are of particular concern here. Which data are used\nand communicated by applications (browsing history, contact lists,\netc.) is not always clear, and even when it is, the only choice\navailable to the user may be not to use the application. \nSome special features of Internet privacy (social media and big data)\nare discussed in the following sections. \nSocial media pose additional challenges. The question is not merely\nabout the moral reasons for limiting access to information, it is also\nabout the moral reasons for limiting the invitations to users\nto submit all kinds of personal information. Social network sites\ninvite the user to generate more data, to increase the value of the\nsite (“your profile is …% complete”). Users are\ntempted to exchange their personal data for the benefits of\nusing services, and provide both this data and their attention as\npayment for the services. In addition, users may not even be aware of\nwhat information they are tempted to provide, as in the aforementioned\ncase of the “like”-button on other sites. Merely limiting\nthe access to personal information does not do justice to the issues\nhere, and the more fundamental question lies in steering the users’\nbehaviour of sharing. When the service is free, the data is needed as a form of payment. \nOne way of limiting the temptation of users to share is requiring\ndefault privacy settings to be strict. Even then, this limits access\nfor other users (“friends of friends”), but it does not\nlimit access for the service provider. Also, such restrictions limit\nthe value and usability of the social network sites themselves, and\nmay reduce positive effects of such services. A particular example of\nprivacy-friendly defaults is the opt-in as opposed to the opt-out\napproach. When the user has to take an explicit action to share data\nor to subscribe to a service or mailing list, the resulting effects\nmay be more acceptable to the user. However, much still depends on how\nthe choice is framed (Bellman, Johnson, & Lohse 2001). \nUsers generate loads of data when online. This is not only data\nexplicitly entered by the user, but also numerous statistics on user\nbehavior: sites visited, links clicked, search terms entered, etc. Data\nmining can be employed to extract patterns from such data, which can\nthen be used to make decisions about the user. These may only affect\nthe online experience (advertisements shown), but, depending on which\nparties have access to the information, they may also impact the user\nin completely different contexts. \nIn particular, big data may be used in profiling the user (Hildebrandt\n2008), creating patterns of typical combinations of user properties,\nwhich can then be used to predict interests and behavior. An innocent\napplication is “you may also like …”, but,\ndepending on the available data, more sensitive derivations may be\nmade, such as most probable religion or sexual preference. These\nderivations could then in turn lead to inequal treatment or\ndiscrimination.  When a user can be assigned to a particular group,\neven only probabilistically, this may influence the actions taken by\nothers (Taylor, Floridi, & Van der Sloot 2017). For example,\nprofiling could lead to refusal of insurance or a credit card, in\nwhich case profit is the main reason for discrimination. When such\ndecisions are based on profiling, it may be difficult to challenge\nthem or even find out the explanations behind them. Profiling could\nalso be used by organizations or possible future governments that have\ndiscrimination of particular groups on their political agenda, in\norder to find their targets and deny them access to services, or\nworse. \nBig data does not only emerge from Internet transactions. Similarly,\ndata may be collected when shopping, when being recorded by\nsurveillance cameras in public or private spaces, or when using\nsmartcard-based public transport payment systems. All these data could\nbe used to profile citizens, and base decisions upon such profiles.\nFor example, shopping data could be used to send information about\nhealthy food habits to particular individuals, but again also for\ndecisions on insurance. According to EU data protection law,\npermission is needed for processing personal data, and they can only\nbe processed for the purpose for which they were obtained. Specific\nchallenges, therefore, are (a) how to obtain permission when the user\ndoes not explicitly engage in a transaction (as in case of\nsurveillance), and (b) how to prevent “function creep”,\ni.e. data being used for different purposes after they are collected\n(as may happen for example with DNA databases (Dahl & Sætnan\n2009). \nOne particular concern could emerge from genetics and genomic data\n(Tavani 2004, Bruynseels & van den Hoven, 2015). Like other data, genomics can be used to make predictions, and in particular could predict risks of diseases.\nApart from others having access to detailed user profiles, a\nfundamental question here is whether the individual should know what\nis known about her. In general, users could be said to have a right to\naccess any information stored about them, but in this case, there may\nalso be a right not to know, in particular when knowledge of the data\n(e.g. risks of diseases) would reduce the well-being – by causing\nfear, for instance – without enabling treatment. With respect to\nprevious examples, one may not want to know the patterns in one’s own\nshopping behavior either. \nAs users increasingly own networked devices such as smart phones,\nmobile devices collect and send more and more data. These devices\ntypically contain a range of data-generating sensors, including GPS\n(location), movement sensors, and cameras, and may transmit the\nresulting data via the Internet or other networks. One particular\nexample concerns location data. Many mobile devices have a GPS sensor\nthat registers the user’s location, but even without a GPS sensor,\napproximate locations can be derived, for example by monitoring the\navailable wireless networks. As location data links the online world\nto the user’s physical environment, with the potential of physical\nharm (stalking, burglary during holidays, etc.), such data are often\nconsidered particularly sensitive. \nMany of these devices also contain cameras which, when applications\nhave access, can be used to take pictures. These can be considered\nsensors as well, and the data they generate may be particularly\nprivate. For sensors like cameras, it is assumed that the user is\naware when they are activated, and privacy depends on such knowledge.\nFor webcams, a light typically indicates whether the camera is on, but\nthis light may be manipulated by malicious software. In general,\n“reconfigurable technology” (Dechesne, Warnier, & van\nden Hoven 2011) that handles personal data raises the question of user\nknowledge of the configuration. \nDevices connected to the Internet are not limited to user-owned\ncomputing devices like smartphones. Many devices contain chips and/or\nare connected in the so-called Internet of Things. RFID (radio\nfrequency identification) chips can be read from a limited distance,\nsuch that you can hold them in front of a reader rather than inserting\nthem. EU and US passports have RFID chips with protected biometric\ndata, but information like the user’s nationality may easily leak when\nattempting to read such devices (see Richter, Mostowski & Poll\n2008, in Other Internet Resources). “Smart” RFIDs are also\nembedded in public transport payment systems. “Dumb”\nRFIDs, basically only containing a number, appear in many kinds of\nproducts as a replacement of the barcode, and for use in logistics.\nStill, such chips could be used to trace a person once it is known\nthat he carries an item containing a chip. \nIn the home, there are smart meters for automatically reading and\nsending electricity and water consumption, and thermostats and other devices\nthat can be remotely controlled by the owner. Such devices again\ngenerate statistics, and these can be used for mining and profiling.\nIn the future, more and more household appliances will be connected,\neach generating its own information. Ambient intelligence (Brey 2005),\nand ubiquitous computing, along with the Internet of Things\n(Friedewald & Raabe 2011), also enable automatic adaptation of the\nenvironment to the user, based on explicit preferences and implicit\nobservations, and user autonomy is a central theme in considering the\nprivacy implications of such devices. In general, the move towards a\nservice-oriented provisioning of goods, with suppliers being informed\nabout how the products are used through IT and associated\nconnectivity, requires consideration of the associated privacy and\ntransparency concerns (Pieters 2013). For example, users will need to\nbe informed when connected devices contain a microphone and how and\nwhen it is used. \nGovernment and public administration have undergone radical\ntransformations as a result of the availability of advanced IT systems\nas well. Examples of these changes are biometric passports, online\ne-government services, voting systems, a variety of online citizen\nparticipation tools and platforms or online access to recordings of\nsessions of parliament and government committee meetings.  \nConsider the case of voting in elections. Information technology may\nplay a role in different phases in the voting process, which may have\ndifferent impact on voter privacy. Most countries have a requirement\nthat elections are to be held by secret ballot, to prevent vote buying\nand coercion. In this case, the voter is supposed to keep her vote\nprivate, even if she would want to reveal it. For information\ntechnology used for casting votes, this is defined as the requirement\nof receipt-freeness or coercion-resistance (Delaune, Kremer & Ryan\n2006). In polling stations, the authorities see to it that the voter\nkeeps the vote private, but such surveillance is not possible when\nvoting by mail or online, and it cannot even be enforced by\ntechnological means, as someone can always watch while the voter\nvotes. In this case, privacy is not only a right but also a duty, and\ninformation technology developments play an important role in the\npossibilities of the voter to fulfill this duty, as well as the\npossibilities of the authorities to verify this. In a broader sense,\ne-democracy initiatives may change the way privacy is viewed in the\npolitical process. \nMore generally, privacy is important in democracy to prevent undue\ninfluence. While lack of privacy in the voting process could enable\nvote buying and coercion, there are more subtle ways of influencing\nthe democratic process, for example through targeted (mis)information\ncampaigns. Online (political) activities of citizens on for example\nsocial media facilitate such attempts because of the possibility of\ntargeting through behavioural profiling. Compared to offline political\nactivities, it is more difficult to hide preferences and activities,\nbreaches of confidentiality are more likely, and attempts to influence\nopinions become more scalable. \nInformation technology is used for all kinds of surveillance tasks. It\ncan be used to augment and extend traditional surveillance systems\nsuch as CCTV and other camera systems, for example to identify\nspecific individuals in crowds, using face recognition techniques, or\nto monitor specific places for unwanted behaviour. Such approaches\nbecome even more powerful when combined with other techniques, such as\nmonitoring of Internet-of-Things devices (Motlagh et al. 2017). \nBesides augmenting existing surveillance systems, ICT techniques are\nnowadays mainly used in the digital domain, typically grouped together\nunder the term “surveillance capitalism” (Zuboff 2019). Social\nmedia and other online systems are used to gather large amounts of\ndata about individuals – either “voluntary”, because users\nsubscribe to a specific service (Google, Facebook), or involuntary by\ngathering all kinds of user related data in a less transparent manner.\nData analysis and machine learning techniques are then used to\ngenerate prediction models of individual users that can be used, for\nexample, for targeted advertisement, but also for more malicious\nintents such as fraud or micro-targeting to influence elections\n(Albright 2016, Other Internet Resources) or referenda such as Brexit\n(Cadwalladr 2019, Other Internet Resources). \nIn addition to the private sector surveillance industry, governments\nform another traditional group that uses surveillance techniques at a\nlarge scale, either by intelligence services or law enforcement. These\ntypes of surveillance systems are typically justified with an appeal\nto the “greater good” and protecting citizens, but their\nuse is also controversial. For such systems, one would typically like\nto ensure that any negative effects on privacy are proportional to the\nbenefits achieved by the technology. Especially since these systems\nare typically shrouded in secrecy, it is difficult for outsiders to\nsee if such systems are used proportionally, or indeed useful for\ntheir tasks (Lawner 2002). This is particularly pressing when\ngovernments use private sector data or services for surveillance\npurposes.  The almost universal use of good encryption techniques\nin communication systems makes it also harder to gather effective\nsurveillance information, leading to more and more calls for “back\ndoors” that can exclusively be used by government in communication\nsystems. From a privacy standpoint this could be evaluated as\nunwanted, not only because it gives governments access to private\nconversations, but also because it lowers the overall security of\ncommunication systems that employ this technique (Abelson et al.\n2015). \nWhereas information technology is typically seen as the cause\nof privacy problems, there are also several ways in which information\ntechnology can help to solve these problems. There are rules,\nguidelines or best practices that can be used for designing\nprivacy-preserving systems. Such possibilities range from\nethically-informed design methodologies to using encryption to protect\npersonal information from unauthorized use. In particular, methods\nfrom the field of information security, aimed at protecting\ninformation against unauthorized access, can play a key role in the\nprotection of personal data. \nValue sensitive design provides a “theoretically grounded\napproach to the design of technology that accounts for human values in\na principled and comprehensive manner throughout the design\nprocess” (Friedman et al. 2006). It provides a set of rules and\nguidelines for designing a system with a certain value in mind. One\nsuch value can be ‘privacy’, and value sensitive design\ncan thus be used as a method to design privacy-friendly IT systems\n(Van den Hoven et al. 2015).  The ‘privacy by design’\napproach as advocated by Cavoukian (2009) and others can be regarded\nas one of the value sensitive design approaches that specifically\nfocuses on privacy (Warnier et al. 2015). More recently, approaches\nsuch as “privacy engineering” (Ceross & Simpson 2018)\nextend the privacy by design approach by aiming to provide a more\npractical, deployable set of methods by which to achieve system-wide\nprivacy. \nThe privacy by design approach provides high-level guidelines in the\nform of principles for designing privacy-preserving systems.\nThese principles have at their core that “data protection needs\nto be viewed in proactive rather than reactive terms, making privacy\nby design preventive and not simply remedial” (Cavoukian 2010).\nPrivacy by design’s main point is that data protection should be\ncentral in all phases of product life cycles, from initial design to\noperational use and disposal (see Colesky et al. 2016) for a critical\nanalysis of the privacy by design approach). The Privacy Impact\nAssessment approach proposed by Clarke (2009) makes a similar point.\nIt proposes “a systematic process for evaluating the potential\neffects on privacy of a project, initiative or proposed system or\nscheme” (Clarke 2009). Note that these approaches should not\nonly be seen as auditing approaches, but rather as a means to make\nprivacy awareness and compliance an integral part of the\norganizational and engineering culture. \nThere are also several industry guidelines that can be used to design\nprivacy preserving IT systems. The Payment Card Industry Data Security\nStandard (see PCI DSS v3.2, 2018, in the Other Internet Resources),\nfor example, gives very clear guidelines for privacy and security\nsensitive systems design in the domain of the credit card industry and\nits partners (retailers, banks). Various International Organization\nfor Standardization (ISO) standards (Hone & Eloff 2002) also serve\nas a source of best practices and guidelines, especially with respect\nto information security, for the design of privacy friendly systems.\nFurthermore, the principles that are formed by the EU Data Protection\nDirective, which are themselves based on the Fair Information\nPractices (Gellman 2014) from the early 70s – transparency,\npurpose, proportionality, access, transfer – are technologically\nneutral and as such can also be considered as high level ‘design\nprinciples’. Systems that are designed with these rules and\nguidelines in mind should thus – in principle – be in\ncompliance with EU privacy laws and respect the privacy of its\nusers. \nThe rules and principles described above give high-level guidance for\ndesigning privacy-preserving systems, but this does not mean that if\nthese methodologies are followed the resulting IT system will\n(automatically) be privacy friendly. Some design principles are rather\nvague and abstract. What does it mean to make a transparent design or\nto design for proportionality? The principles need to be interpreted\nand placed in a context when designing a specific system. But\ndifferent people will interpret the principles differently, which will\nlead to different design choices, with different effects on privacy.\nThere is also a difference between the design and the implementation\nof a computer system. During the implementation phase software bugs\nare introduced, some of which can be exploited to break the system and\nextract private information. How to implement bug-free computer\nsystems remains an open research question (Hoare 2003). In addition,\nimplementation is another phase wherein choices and interpretations\nare made: system designs can be implemented in infinitely many ways.\nMoreover, it is very hard to verify – for anything beyond\nnon-trivial systems – whether an implementation meets its\ndesign/specification (Loeckx, Sieber, & Stansifer 1985). This is\neven more difficult for non-functional requirements such as\n‘being privacy preserving’ or security properties in\ngeneral.  \nSome specific solutions to privacy problems aim at increasing the\nlevel of awareness and consent of the user. These solutions can be\nseen as an attempt to apply the notion of informed consent to privacy\nissues with technology (Custers et al. 2018). This is connected to the\nidea that privacy settings and policies should be explainable to users\n(Pieters 2011). For example, the Privacy Coach supports customers in\nmaking privacy decisions when confronted with RFID tags (Broenink et\nal. 2010). However, users have only a limited capability of dealing\nwith such choices, and providing too many choices may easily lead to\nthe problem of moral overload (van den Hoven, Lokhorst, & Van de\nPoel 2012). A technical solution is support for automatic matching of\na privacy policy set by the user against policies issued by web sites\nor apps. \nA growing number of software tools are available that provide some\nform of privacy (usually anonymity) for their users, such tools are\ncommonly known as privacy enhancing technologies (Danezis &\nGürses 2010, Other Internet Resources). Examples include\ncommunication-anonymizing tools such as Tor (Dingledine, Mathewson,\n& Syverson 2004) and Freenet (Clarke et al. 2001), and\nidentity-management systems for which many commercial software\npackages exist (see below). Communication anonymizing tools allow\nusers to anonymously browse the web (with Tor) or anonymously share\ncontent (Freenet). They employ a number of cryptographic techniques\nand security protocols in order to ensure their goal of anonymous\ncommunication. Both systems use the property that numerous users use\nthe system at the same time which provides k-anonymity (Sweeney\n2002): no individual can be uniquely distinguished from a group of\nsize k, for large values for k. Depending on the system,\nthe value of k can vary between a few hundred to hundreds of\nthousands. In Tor, messages are encrypted and routed along numerous\ndifferent computers, thereby obscuring the original sender of the\nmessage (and thus providing anonymity). Similarly, in Freenet content\nis stored in encrypted form from all users of the system. Since users\nthemselves do not have the necessary decryption keys, they do not know\nwhat kind of content is stored, by the system, on their own computer.\nThis provides plausible deniability and privacy. The system can at any\ntime retrieve the encrypted content and send it to different Freenet\nusers. \nPrivacy enhancing technologies also have their downsides. For example,\nTor, the tool that allows anonymized communication and browsing over\nthe Internet, is susceptible to an attack whereby, under certain\ncircumstances, the anonymity of the user is no longer guaranteed\n(Back, Möller, & Stiglic 2001; Evans, Dingledine, &\nGrothoff 2009). Freenet (and other tools) have similar problems\n(Douceur 2002). Note that for such attacks to work, an attacker needs\nto have access to large resources that in practice are only realistic\nfor intelligence agencies of countries. However, there are other\nrisks. Configuring such software tools correctly is difficult for the\naverage user, and when the tools are not correctly configured\nanonymity of the user is no longer guaranteed. And there is always the\nrisk that the computer on which the privacy-preserving software runs\nis infected by a Trojan horse (or other digital pest) that monitors\nall communication and knows the identity of the user. \nAnother option for providing anonymity is the anonymization of data\nthrough special software. Tools exist that remove patient names and\nreduce age information to intervals: the age 35 is then represented as\nfalling in the range 30–40. The idea behind such anonymization\nsoftware is that a record can no longer be linked to an individual,\nwhile the relevant parts of the data can still be used for scientific\nor other purposes. The problem here is that it is very hard to\nanonymize data in such a way that all links with an individual are\nremoved and the resulting anonymized data is still useful for research\npurposes. Researchers have shown that it is almost always possible to\nreconstruct links with individuals by using sophisticated statistical\nmethods (Danezis, Diaz, & Troncoso 2007) and by combining multiple\ndatabases (Anderson 2008) that contain personal information.\nTechniques such as k-anonymity might also help to generalize\nthe data enough to make it unfeasible to de-anonymize data (LeFevre et\nal. 2005). \nCryptography has long been used as a means to protect data, dating\nback to the Caesar cipher more than two thousand years ago. Modern\ncryptographic techniques are essential in any IT system that needs to\nstore (and thus protect) personal data, for example by providing\nsecure (confidential) connections for browsing (HTTPS) and networking\n(VPN). Note however that by itself cryptography does not provide any\nprotection against data breaching; only when applied correctly in a\nspecific context does it become a ‘fence’ around personal\ndata. In addition, cryptographic schemes that become outdated by\nfaster computers or new attacks may pose threats to (long-term)\nprivacy. Cryptography is a large\nfield, so any description here will be incomplete. The focus will be\ninstead on some newer cryptographic techniques, in particular\nhomomorphic encryption, that have the potential to become very\nimportant for processing and searching in personal data. \nVarious techniques exist for searching through encrypted data (Song et\nal. 2000, Wang et al. 2016), which provides a form of privacy\nprotection (the data is encrypted) and selective access to sensitive\ndata. One relatively new technique that can be used for designing\nprivacy-preserving systems is ‘homomorphic encryption’\n(Gentry 2009, Acar et al. 2018). Homomorphic encryption allows a data\nprocessor to process encrypted data, i.e. users could send personal\ndata in encrypted form and get back some useful results – for\nexample, recommendations of movies that online friends like – in\nencrypted form. The original user can then again decrypt the result\nand use it without revealing any personal data to the data processor.\nHomomorphic encryption, for example, could be used to aggregate\nencrypted data thereby allowing both privacy protection and useful\n(anonymized) aggregate information. The technique is currently not\nwidely applied; there are serious performance issues if one wants to\napply full homomorphic encryption to the large amounts of data stored\nin today’s systems. However, variants of the original homomorphic\nencryption scheme are emerging, such as Somewhat Homomorphic\nEncryption (Badawi et al. 2018), that are showing promise to be more\nwidely applied in practice. \nThe main idea behind blockchain technology was first described in the\nseminal paper on Bitcoins (Nakamoto, n.d., Other Internet Resources).\nA blockchain is basically a distributed ledger that stores\ntransactions in a non-reputable way, without the use of a trusted\nthird party. Cryptography is used to ensure that all transactions are\n“approved” by members of the blockchain and stored in such a way\nthat they are linked to previous transactions and cannot be removed.\nAlthough focused on data integrity and not inherently anonymous,\nblockchain technology enables many privacy-related applications\n(Yli-Huumo et al. 2016, Karame and Capkun 2018), such as anonymous\ncryptocurrency (Narayanan et al. 2016) and self-sovereign identity\n(see below). \nThe use and management of user’s online identifiers are crucial in the\ncurrent Internet and social networks. Online reputations become more\nand more important, both for users and for companies. In the era of\nbig data correct information about users has an\nincreasing monetary value. \n‘Single sign on’ frameworks, provided by independent third\nparties (OpenID) but also by large companies such as Facebook,\nMicrosoft and Google (Ko et al. 2010), make it easy for users to\nconnect to numerous online services using a single online identity.\nThese online identities are usually directly linked to the real world\n(off line) identities of individuals; indeed Facebook, Google and\nothers require this form of log on (den Haak 2012). Requiring a direct\nlink between online and ‘real world’ identities is\nproblematic from a privacy perspective, because they allow profiling\nof users (Benevenuto et al. 2012). Not all users will realize how\nlarge the amount of data is that companies gather in this manner, or\nhow easy it is to build a detailed profile of users. Profiling becomes\neven easier if the profile information is combined with other\ntechniques such as implicit authentication via cookies and tracking\ncookies (Mayer & Mitchell 2012). \nFrom a privacy perspective a better solution would be the use of\nattribute-based authentication (Goyal et al. 2006) which allows access\nof online services based on the attributes of users, for example their\nfriends, nationality, age etc. Depending on the attributes used, they\nmight still be traced back to specific individuals, but this is no\nlonger crucial. In addition, users can no longer be tracked to\ndifferent services because they can use different attributes to access\ndifferent services which makes it difficult to trace online identities\nover multiple transactions, thus providing unlinkability for the user.\nRecently (Allen 2016, Other Internet Resources), the concept of\nself-sovereign identity has emerged, which aims for users to have\ncomplete ownership and control about their own digital identities.\nBlockchain technology is used to make it possible for users to control\na digital identity without the use of a traditional trusted third\nparty (Baars 2016). \nIn the previous sections, we have outlined how current technologies\nmay impact privacy, as well as how they may contribute to mitigating\nundesirable effects. However, there are future and emerging\ntechnologies that may have an even more profound impact. Consider for\nexample brain-computer interfaces. In case computers are connected\ndirectly to the brain, not only behavioral characteristics are subject\nto privacy considerations, but even one’s thoughts run the risk of\nbecoming public, with decisions of others being based upon them. In\naddition, it could become possible to change one’s behavior by means\nof such technology. Such developments therefore require further\nconsideration of the reasons for protecting privacy. In particular,\nwhen brain processes could be influenced from the outside, autonomy\nwould be a value to reconsider to ensure adequate protection. \nApart from evaluating information technology against current moral\nnorms, one also needs to consider the possibility that technological\nchanges influence the norms themselves (Boenink, Swierstra &\nStemerding 2010). Technology thus does not only influence privacy by\nchanging the accessibility of information, but also by changing the\nprivacy norms themselves. For example, social networking sites invite\nusers to share more information than they otherwise might. This\n“oversharing” becomes accepted practice within certain\ngroups. With future and emerging technologies, such influences can\nalso be expected and therefore they ought to be taken into account\nwhen trying to mitigate effects. \nAnother fundamental question is whether, given the future (and even\ncurrent) level of informational connectivity, it is feasible to\nprotect privacy by trying to hide information from parties who may use\nit in undesirable ways. Gutwirth & De Hert (2008) argue that it\nmay be more feasible to protect privacy by transparency – by\nrequiring actors to justify decisions made about individuals, thus\ninsisting that decisions are not based on illegitimate information.\nThis approach comes with its own problems, as it might be hard to\nprove that the wrong information was used for a decision. Still, it\nmay well happen that citizens, in turn, start data collection on those\nwho collect data about them, e.g. governments. Such\n“counter(sur)veillance” may be used to gather information\nabout the use of information, thereby improving accountability\n(Gürses et al. 2016). The open source movement may also\ncontribute to transparency of data processing. In this context,\ntransparency can be seen as a pro-ethical condition contributing to\nprivacy (Turilli & Floridi 2009). \nIt has been argued that the precautionary principle, well known in\nenvironmental ethics, might have a role in dealing with emerging\ninformation technologies as well (Pieters & van Cleeff 2009; Som,\nHilty & Köhler 2009). The principle would see to it that the\nburden of proof for absence of irreversible effects of information\ntechnology on society, e.g. in terms of power relations and equality,\nwould lie with those advocating the new technology. Precaution, in\nthis sense, could then be used to impose restrictions at a regulatory\nlevel, in combination with or as an alternative to empowering users,\nthereby potentially contributing to the prevention of informational\noverload on the user side. Apart from general debates about the\ndesirable and undesirable features of the precautionary principle,\nchallenges to it lie in its translation to social effects and social\nsustainability, as well as to its application to consequences induced\nby intentional actions of agents. Whereas the occurrence of natural\nthreats or accidents is probabilistic in nature, those who are\ninterested in improper use of information behave strategically,\nrequiring a different approach to risk (i.e. security as opposed to\nsafety). In addition, proponents of precaution will need to balance it\nwith other important principles, viz., of informed consent and\nautonomy. \nFinally, it is appropriate to note that not all social effects of\ninformation technology concern privacy (Pieters 2017). Examples\ninclude the effects of social network sites on friendship, and the\nverifiability of results of electronic elections. Therefore,\nvalue-sensitive design approaches and impact assessments of\ninformation technology should not focus on privacy only, since\ninformation technology affects many other values as well.","contact.mail":"M.E.Warnier@tudelft.nl","contact.domain":"tudelft.nl"}]
