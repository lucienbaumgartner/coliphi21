[{"date.published":"2002-03-07","date.changed":"2020-05-15","url":"https://plato.stanford.edu/entries/qm-collapse/","author1":"Giancarlo Ghirardi","author2":"Angelo Bassi","author1.info":"http://www.qmts.it:8080/?q=group-leader","entry":"qm-collapse","body.text":"\n\n\nQuantum mechanics, with its revolutionary implications, has posed\ninnumerable problems to philosophers of science. In particular, it has\nsuggested reconsidering basic concepts such as the existence of a\nworld that is, at least to some extent, independent of the observer,\nthe possibility of getting reliable and objective knowledge about it,\nand the possibility of taking (under appropriate circumstances) at\nleast some properties to be objectively possessed by physical systems.\nIt has also raised many others questions which are well known to those\ninvolved in the debate on the interpretation of this pillar of modern\nscience. One can argue that most of the problems are not only due to\nthe intrinsic revolutionary nature of the phenomena which have led to\nthe development of the theory. They are also related to the fact that,\nin its standard formulation and interpretation, quantum mechanics is a\ntheory which is excellent (in fact it has an unprecedented success in\nthe history of science) in telling us everything about what we\nobserve, but it meets with serious difficulties in telling us\nwhat there is. We are making here specific reference to the\ncentral problem of the theory, usually referred to as the\nmeasurement problem, which is accompanying quantum theory since\nits birth. It is just one of the many attempts to overcome the\ndifficulties posed by this problem that has led to the development of\nCollapse Theories, i.e., to the Dynamical Reduction\nProgram (DRP). As we shall see, this approach consists in\naccepting that the dynamical equation of the standard theory should be\nmodified by the addition of stochastic and nonlinear terms. The nice\nfact is that the resulting theory is capable, on the basis of a single\ndynamics which is assumed to govern all natural processes, to account\nat the same time for all well-established facts about microscopic\nsystems as described by the standard theory, as well as for the\nso-called postulate of wave packet reduction (WPR), which accompanies\nthe interaction of a microscopic system with a measuring device. As is\nwell known, such a postulate is assumed in the standard scheme just in\norder to guarantee that measurements have outcomes but, as we\nshall discuss below, it meets with insurmountable difficulties if one\ntries to derive it by assuming the measurement itself to be a process\ngoverned by the linear laws of the theory. Finally, the collapse\ntheories account in a completely satisfactory way for the classical\nbehavior of macroscopic systems. \n\n\nTwo specifications are necessary in order to make clear from the\nbeginning what the limitations and the merits of the program are. The\nonly satisfactory explicit models of this type (the model proposed by\nGhirardi, Rimini, and Weber (1986), usually referred to as the GRW\ntheory, as well as all subsequent developments) are phenomenological\nattempts to solve a foundational problem. At present, they involve\nphenomenological parameters which, if the theory is taken seriously,\nacquire the status of new constants of nature. Moreover, the problem\nof building satisfactory relativistic generalizations of collapse\nmodels is very difficult, though some improvements have been made,\nwhich have elucidated some crucial points. \n\n\nIn spite of their phenomenological character, Collapse Theories are\nassuming a growing relevance, since they provide a clear resolution\nfor the difficulties of the formalism, to close the circle in\nthe precise sense defined by Abner Shimony (1989). Moreover, they have\nallowed a clear identification of the formal features which should\ncharacterize any unified theory of micro and macro processes. \n\n\nLast but not least, Collapse Theories qualify themselves as rival\ntheories of quantum mechanics and one can easily identify some of\ntheir physical implications which, in principle, would allow crucial\ntests discriminating between the two. Getting stringent indications\nfrom such tests requires experiments, whose technology has been\ndeveloped only very recently. Actually, it is just due to remarkable\nimprovements in the field of opto-mechanics and cold atoms, as well as\nnuclear physics, that specific bounds have already been obtained for\nthe parameters characterizing the theories under investigation; more\nimportant, precise families of physical processes in which a violation\nof the linear nature of the standard formalism might emerge have been\nclearly identified and are the subject of systematic investigations\nwhich might lead, in the end, to relevant discoveries.\n\nA very natural question, which all scientists who are concerned about\nthe meaning and the value of science have to face, is whether one can\ndevelop a coherent worldview that can accommodate our knowledge of\nnatural phenomena as it is embodied in our best theories. Such a\nprogram meets serious difficulties with quantum mechanics, essentially\nbecause of two formal aspects of the theory according to its standard\nformulation, which are common to all of its versions, from the\noriginal nonrelativistic formulations of the 1920s, to current quantum\nfield theories: the linear nature of the state space and of the\nevolution equation; in other words: the validity of the superposition\nprinciple and the related phenomenon of entanglement, which, in\nSchrödinger’s words:  \nis not one but the characteristic trait of quantum mechanics, the one\nthat enforces its entire departure from classical lines of thought\n(Schrödinger 1935: 807). \nThese two formal features have embarrassing consequences, since they\nimply  \nFor the sake of generality, we shall first of all present a very\nconcise sketch of ‘the rules of the quantum game’. \nLet us recall the axiomatic structure of quantum theory:  \nStates of physical systems are associated with normalized vectors in a\nHilbert space, a complex, infinite-dimensional, complete and separable\nlinear vector space equipped with a scalar product. Linearity implies\nthat the superposition principle holds: if \\(\\ket{f}\\) is a state and\n\\(\\ket{g}\\) is a state, then (for \\(a\\) and \\(b\\) arbitrary complex\nnumbers) also \nis a state. Moreover, the state evolution is linear, i.e., it\npreserves superpositions: if \\(\\ket{f,t}\\) and \\(\\ket{g,t}\\) are the\nstates obtained by evolving the states \\(\\ket{f,0}\\) and\n\\(\\ket{g,0}\\), respectively, from the initial time \\(t=0\\) to the time\n\\(t\\), then \\(a\\ket{f,t} + b\\ket{g,t}\\) is the state obtained by the\nevolution of \\(a\\ket{f,0} + b\\ket{g,0}\\). Finally, the completeness\nassumption is made, i.e., that the knowledge of its statevector\nrepresents, in principle, the most accurate information one can have\nabout the state of an individual physical system. \nObservable quantities are represented by self-adjoint operators \\(B\\)\non the Hilbert space containing the possible states of the system. The\nassociated eigenvalue equations \\(B\\ket{b_k} = b_k \\ket{b_k}\\) and the\ncorresponding eigenmanifolds (the linear manifolds spanned by the\neigenvectors associated to a given eigenvalue, also called\neigenspaces) play a basic role for the predictive content of the\ntheory. In fact: \nWe stress that, according to the above scheme, quantum mechanics makes\nonly conditional probabilistic predictions (conditional on the\nmeasurement being actually performed) for the outcomes of prospective\n(and in general incompatible among themselves) measurement processes.\nOnly if a state belongs to an eigenmanifold of the observable, which\nis going to be measured, already before the act of measurement, one\ncan predict the outcome with certainty. In all other cases—if\nthe completeness assumption is made—one has objective\nnonepistemic probabilities for different outcomes. \nThe orthodox position gives a very simple answer to the question: what\ndetermines the outcome, when different outcomes are possible? The\nanswer is “nothing”—the theory is complete and\ntherefore it is illegitimate to raise any question about properties\npossessed prior to a measurement, when different outcomes of a\nmeasurement of an observable have non-vanishing probabilities of\noccurring, if the measurement is actually performed. Correspondingly,\nthe referents of the theory are only the results of measurements.\nThese are to be described in classical terms and involve in general\nmutually exclusive physical conditions. \nRegarding the legitimacy of attributing properties to physical\nsystems, one could say that quantum mechanics warns us against\nattributing too many properties to physical systems.\nHowever—with Einstein—one can adopt a sufficient condition\nfor the existence of an objective individual property the possibility\nto predict with certainty the outcome of a measurement. This implies\nthat, whenever the overall statevector factorizes into the state of\nthe Hilbert space of the physical system \\(S\\) times the state for the\nrest of the universe, \\(S\\) does possess some properties (actually a\ncomplete set of properties, i.e., those associated to appropriate\nmaximal sets of commuting observables). \nBefore concluding this section some comments about the measurement\nprocess are of relevance. Quantum theory was created to describe the\nbehavior of microscopic phenomena as was emerging from observations.\nIn order to obtain information about system at the molecular and\n(sub-) atomic scale, one must be able to establish strict correlations\nbetween the states of the microscopic system and the states of the\nmeasuring devices, which we directly perceive. Within the formalism,\nthis is described by considering appropriate micro-macro interactions.\nThe fact that when the measurement is completed one can make\nstatements about the outcome is accounted for by the already mentioned\nWPR postulate (Dirac 1935): a measurement always causes a system\nto jump in an eigenstate of the observed quantity.\nCorrespondingly, also the statevector of the apparatus\n‘jumps’ into the manifold associated to the recorded\noutcome. \nWe shall now clarify why the formalism we have just presented gives\nrise to the measurement problem. To this purpose we shall, first of\nall, discuss the standard oversimplified argument based on the\nso-called von Neumann ideal measurement scheme.  \nLet us begin by recalling how measurements are described in the\nstandard formalism: \nSuppose that a microsystem \\(S\\), immediately before the measurement\nof one of its observables, say \\(B\\), is in the eigenstate\n\\(\\ket{b_j}\\) of the corresponding operator. The apparatus (a\nmacrosystem) used to gain information about \\(B\\) is initially assumed\nto be in a precise macroscopic state, its ready state, corresponding\nto a definite macro property—e.g., its pointer points at 0 on a\nscale. Since the apparatus \\(A\\) is made of elementary particles,\natoms and so on, it should be possible to describe it within quantum\nmechanics, which will associate a well defined state vector\n\\(\\ket{A_0}\\) to it (at least in principle). One then assumes that\nthere is an appropriate system-apparatus interaction lasting for a\nfinite time, such that when the initial state of the apparatus is\ntriggered by the state \\(\\ket{b_j}\\), it ends up in a final\nconfiguration \\(\\ket{A_j}\\), which is macroscopically distinguishable\nfrom the initial one and from the other configurations \\(\\ket{A_k}\\)\nin which it would end up if triggered by a different eigenstate\n\\(\\ket{b_k}\\). Moreover, for simplicity one assumes that the system is\nleft by the measurement in its initial state. In brief, one assumes\nthat one can dispose things in such a way that the system-apparatus\ninteraction can be described as: \nEquation (1) and the hypothesis that the superposition principle\ngoverns all natural processes tell us that, if the initial state of\nthe microsystem is a linear superposition of different eigenstates\n(for simplicity we will consider only two of them), one has: \nSome remarks about this are in order: \nNowadays, there is a general consensus that this solution is\nabsolutely unacceptable. It corresponds to assuming that the linear\nnature of the theory is broken at some point, without clearly\nspecifying when. Thus, quantum theory is unable to explain how it can\nhappen that apparatuses behave as required by the WPR postulate (which\nis one of the axioms of the theory), instead of satisfying the\nSchrödinger equation. Even if one were to accept that quantum\nmechanics has a limited field of applicability, so that it does not\naccount for all natural processes and, in particular, it breaks down\nat the macrolevel, it is clear that the theory does not contain any\nprecise criterion for identifying the borderline between micro and\nmacro, linear and nonlinear, deterministic and stochastic, reversible\nand irreversible. To use the words of J.S. Bell, there is nothing in\nthe theory fixing such a borderline and the split between the\ntwo above types of processes is fundamentally shifty.  \nIf one looks at the historical debate on this problem, one can easily\nsee that it is precisely by continuously resorting to this ambiguity\nabout the split that adherents of the Copenhagen orthodoxy or easy\nsolvers (Bell 1990) of the measurement problem have rejected the\ncriticism of the heretics (Gottfried 2000). For instance,\nBohr succeeded in rejecting Einstein’s criticisms at the Solvay\nConferences by stressing that some macroscopic parts of the apparatus\nhad to be treated fully quantum mechanically; von Neumann and Wigner\ndisplaced the split by locating it between the physical processes and\nthe consciousness (but what is a conscious being, from the physical\npoint of view?), and so on. \nIt is not our task to review here the various attempts to solve the\nabove difficulties. One can find many exhaustive treatments of this\nproblem in the literature. We conclude this section by discussing how\nthe measurement problem is indeed a consequence of very general, in\nfact unavoidable, assumptions on the nature of measurements, and not\nspecifically of the assumptions of von (oversimplified) von\nNeumann’s model. This was established in a series of theorems of\nincreasing generality, notably the ones by Fine (1970),\nd’Espagnat (1971), Shimony (1974), Brown (1986) and Busch &\nShimony (1996). Possibly the most general and direct proof is given by\nBassi and Ghirardi (2000), whose results we briefly summarize. The\nassumptions of the theorem are: \nFrom these very general assumptions one can show that, repeating the\nmeasurement on systems prepared in the superposition of the two given\neigenstates, in the great majority of cases one ends up in a\nsuperposition of macroscopically and perceptually different situations\nof the whole universe. This, again, is the measurement problem of\nquantum mechanics, which calls for a resolution. \nThe debate on the macro-objectification problem continued for many\nyears after the early days of quantum mechanics. In the early 1950s an\nimportant step was taken by D. Bohm who presented (Bohm 1952) a\nmathematically precise deterministic completion of quantum mechanics\n(see the entry on Bohmian Mechanics), which was anticipated by de\nBroglie in the 1920s. In the area of Collapse Theories, one should\nmention the contribution by Bohm and Bub (1966), which was based on\nthe interaction of the statevector with Wiener-Siegel hidden\nvariables. But let us come to Collapse Theories in the sense currently\nattached to this expression.  \nImportant investigations during the 1970s can be considered as\npreliminary steps for the subsequent developments. In the years 1970\nthe school of L. Fonda in Italy concerned with quantum decay processes\nand in particular with the possibility of deriving, within a quantum\ncontext, the exponential decay law (Fonda, Ghirardi, and Rimini 1978).\nSome features of this approach turned out to be relevant for the\nsubsequent development of Collapse Theories: \nObviously, in these papers the considered reduction processes were not\nassumed to be ‘spontaneous and fundamental’ natural\nprocesses, but due to system- environment interactions. Accordingly,\nthese attempts did not represent original proposals for solving the\nmacro-objectification problem, yet they have paved the way for the\nelaboration of the GRW theory.  \nAround the same years, P. Pearle (1976, 1979), and subsequently N.\nGisin (1984a,b) and Diosi (1988), had developed models which accounted\nfor the reduction process in terms of stochastic differential\nequations. While they were looking for a new dynamical equation\noffering a solution to the macro-objectification problem, they did not\nsucceed in identifying the states to which the dynamical equation\nshould lead. The these states were assumed to depend on the particular\nmeasurement process one was considering, leaving the program of\nformulating a universal dynamics accounting for the quantum properties\nof microscopic systems together the classical properties of\nmacroscopic objects incomplete. In those years N. Gisin formulated\nsubsequently an interesting argument (Gisin 1989) according to which\nnonlinear modifications of the Schrödinger equation in general\nare unacceptable, since they imply the possibility of sending\nsuperluminal signals. This argument eventually proved that only a very\nspecific class of nonlinear (and stochastic) modifications of the\nSchrödinger equation is physically acceptable (Caiaffa, Smirne\n& Bassi 2017, and references therein), the class which collapse\nmodels belong to. \nAs already mentioned, the Collapse Theory we are going to describe\namounts to accepting a modification of the standard evolution law of\nquantum theory such that microprocesses and macroprocesses are\ngoverned by a single dynamics. Such a dynamics must imply that the\nmicro-macro interaction in a measurement process leads to WPR. Bearing\nthis in mind, recall that the characteristic feature distinguishing\nthe quantum evolution from WPR is that, while Schrödinger’s\nequation is linear and deterministic (at the wave function level), WPR\nis nonlinear and stochastic. It is then natural to consider, as was\nsuggested for the first time in the above quoted papers by P. Pearle,\nthe possibility of nonlinear and stochastic modifications of the\nstandard Schrödinger dynamics. Such modifications, to be\nuniversal, must satisfy one important requirement, called the trigger\nproblem by Pearle (1989): the reduction mechanism must become more and\nmore effective in going from the micro to the macro domain. The\nsolution to this problem constitutes the central feature of Collapse\nTheories of the GRW type. To discuss these points, let us briefly\nreview the GRW model, first consistent model to appear in the\nliterature. \nWithin such a model, initially referred to as QMSL (Quantum Mechanics\nwith Spontaneous Localizations), the problem of the choice of the\npreferred basis is solved by noting that the most embarrassing\nsuperpositions, at the macroscopic level, are those involving\ndifferent spatial locations of macroscopic objects. Actually, as\nEinstein has stressed, this is a crucial point which has to be faced\nby anybody aiming at taking a macro-objective position about natural\nphenomena: ‘A macro-body must always have a quasi-sharply\ndefined position in the objective description of reality’ (Born\n1971: 223). Accordingly, QMSL considers the possibility of spontaneous\nprocesses, which are assumed to occur instantaneously and at the\nmicroscopic level, which tend to suppress the linear superpositions of\ndifferently localized states. The required trigger mechanism must then\nfollow consistently. \nThe key assumption of QMSL is the following: each elementary\nconstituent of any physical system is subjected, at random times, to\nrandom and spontaneous localization processes (which we will call\nhittings) around appropriate positions. To have a precise mathematical\nmodel one has to be very specific about the above assumptions, making\nexplicit HOW the process works (which modifications of the wave\nfunction are induced by the localizations), WHERE it occurs (what\ndetermines the occurrence of a localization at a certain position\nrather than at another one), and finally WHEN (at what times), it\noccurs. The answers to these questions are now presented. \nLet us consider a system of \\(N\\) distinguishable particles and let us\ndenote by \\(F(\\boldsymbol{q}_1, \\boldsymbol{q}_2 , \\ldots\n,\\boldsymbol{q}_N )\\) the coordinate representation (wave function) of\nthe state vector (we disregard spin variables since hittings are\nassumed not to act on them). \nwhere \\(d\\) represents the localization accuracy. Let us denote as \nthe wave function immediately after the localization, as yet\nunnormalized. \nIt is straightforward to see that the hitting process leads, when it\noccurs, to the localization of states of the particle, which are\ninitially delocalized over distances greater than \\(d\\). As a simple\nexample we can consider a single particle whose wavefunction is\ndifferent from zero only in two small and far apart regions \\(h\\) and\n\\(t\\). Suppose that a localization occurs around \\(h\\); the state\nafter the hitting is then appreciably different from zero only in a\nregion around \\(h\\) itself. A completely analogous argument holds if\nthe hitting takes place around \\(t\\). Regarding the possibility for\nthe state to collapse around points, which are far from both \\(h\\) and\n\\(t\\), one easily sees that the probability density for such hittings\n, according to the multiplication rule determining \\(L_i\\), is\npractically zero; moreover, that if such a hitting were to occur,\nafter it is normalized, the wave function of the system would remain\nalmost unchanged. \nWe can now discuss the most important feature of the theory: the\nTrigger Mechanism. To understand the way in which the spontaneous\nlocalization mechanism is enhanced by increasing the number of\nparticles which are in far apart spatial regions (as compared to\n\\(d)\\), one can consider, for simplicity, the superposition\n\\(\\ket{S}\\), with equal weights, of two macroscopic pointer states\n\\(\\ket{H}\\) and \\(\\ket{T}\\), corresponding to two different pointer\npositions \\(H\\) and \\(T\\), respectively. Taking into account that the\npointer is ‘almost rigid’ and contains a macroscopic\nnumber \\(N\\) of microscopic constituents, the state can be written, in\nobvious notation, as: \nwhere \\(h_i\\) is near \\(H\\), and \\(t_i\\) is near \\(T\\). The states\nappearing in first term on the right-hand side of equation (4) are\ndifferent from zero only when their arguments \\((1,\\ldots ,N)\\) are\nall near \\(H\\), while those of the second term are different from zero\nonly when they are all near \\(T\\). It is now evident that if any of\nthe particles (say, the \\(i\\)-th particle) undergoes a hitting\nprocess, for example around \\(h_i\\), the multiplication prescription\nleads practically to the suppression of the second term in (4). Thus,\nany spontaneous localization of any of the constituents amounts to a\nlocalization of the pointer. The hitting frequency is therefore\neffectively amplified proportionally to the number of constituents.\nNotice that, for simplicity, the argument refers to an almost rigid\nbody, one for which all particles are around \\(H\\) in one of the\nstates of the superposition and around \\(T\\) in the other state. It\nshould however be obvious that what really matters in amplifying the\nreductions is the number of particles which are in different positions\nin the two states appearing in the superposition itself. \nUnder these premises we can now proceed to choose the parameters \\(d\\)\nand \\(f\\) of the theory, i.e., the localization accuracy and the mean\nlocalization frequency. The argument given above suggests how one can\nchoose the parameters in such a way that the quantum predictions for\nmicroscopic systems remain fully valid while the embarrassing\nmacroscopic superpositions in measurement-like situations are\nsuppressed in very short times. Accordingly, as a consequence of the\nunified dynamics governing all physical processes, individual\nmacroscopic objects acquire definite macroscopic properties. The\nchoice suggested in the GRW-model is: \nIt follows that a microscopic system undergoes a localization, on\naverage, every hundred million years, while a macroscopic one\nundergoes a localization every \\(10^{-7}\\) seconds. With reference to\nthe challenging version of the macro-objectification problem presented\nby Schrödinger with the famous example of his cat, J.S. Bell\ncomments (1987: 44): [within QMSL] the cat is not both dead and\nalive for more than a split second. Besides the extremely low\nfrequency of the hittings for microscopic systems, also the fact that\nthe localization width is large compared to the dimensions of atoms\n(so that even when a localization occurs it does very little violence\nto the internal economy of an atom) plays an important role in\nguaranteeing that no violation of well-tested quantum mechanical\npredictions is implied by the modified dynamics. \nContrary to standard quantum mechanics, the GRW theory allows to\nlocate precisely the ‘split’ between micro and macro,\nreversible and irreversible, quantum and classical. This is another\nway of saying that GRW solved the measurement problem. The transition\nbetween the two types of ‘regimes’ is governed by the\nnumber of particles, which are localized by the collapse process. A\nconsequence of this is that GRW makes predictions, which are different\nfrom standard quantum mechanical predictions. We will come back on\nthis important issue. \nConcerning the choice of the parameters of the model, it has to be\nstressed that, as it is obvious, the just mentioned transition region\nfrom micro to macro depends crucially on their values. departing from\nthe original choice of GRW, Adler (2003) has suggested to increase the\nvalue of \\(f\\) by a factor of the order of \\(10^9\\). The reasons for\nthis derive from requiring that during latent image formation in\nphotography, the collapse becomes effective right after a grain of the\nemulsion has been excited; this is equivalent to requiring that when a\nhuman eye is hit by few photons (the perceptual threshold being very\nlow) reduction takes place in the rods of the eye (Bassi, Deckert and\nFerialdi 2010). As we will discuss in what follows, if one takes the\noriginal GRW value for \\(f\\), reduction cannot occur in the rods\n(because a relatively small number of molecules—less than\n\\(10^5\\)—are affected), but only during the transmission along\nthe nervous signal within the brain, a process which involves the\ndisplacement of a number of ions of the order of \\(10^{12}\\). \nIt is interesting to remark that the drastic change suggested by Adler\n(2003) has physical implications which have already been\nexperimentally falsified, see Curceanu, Hiesmayr, and Piscicchia 2015;\nBassi, Deckert, and Ferialdi 2010; Vinante et al. 2016; and\nToroš and Bassi 2018.  \nThe model just presented (QMSL) has a serious drawback: it does not\nallow to deal with systems containing identical constituents, because\nit does not respect the symmetry or antisymmetry requirements for such\nparticles. A quite natural idea to overcome this difficulty is to\nrelate the hitting process not to the individual particles but to the\nparticle number density averaged over an appropriate volume. This can\nbe done by introducing a new phenomenological parameter in the theory\nwhich however can be eliminated by an appropriate limiting procedure\n(see below). \nAnother way to overcome this problem derives from injecting the\nphysically appropriate principles of the GRW model within the original\napproach of P. Pearle. This line of thought has led to what is known\nas the CSL (Continuous Spontaneous Localization) model (Pearle 1989;\nGhirardi, Pearle, and Rimini 1990) in which the discontinuous jumps\nwhich characterize QMSL are replaced by a continuous stochastic\nevolution in the Hilbert space (a sort of Brownian motion of the\nstatevector). \nThe basic working principles are CSL are similar to those of the GRW\nmodel, though the technical detail might different significantly. For\na review see (Bassi and Ghirardi 2003; Adler 2007, Bassi, Lochan, et\nal. 2013). At this regard, it is interesting to note (Ghirardi,\nPearle, & Rimini 1990) that for any CSL dynamics there is a hitting\ndynamics which, from a physical point of view, is ‘as close to\nit as one wants’. Instead of entering into the details of the\nCSL formalism, it is useful, for the discussion below, to analyze a\nsimplified version of it. \nWith the aim of understanding the physical implications of the CSL\nmodel, such as the rate of suppression of coherence, we make now some\nsimplifying assumptions. First, we assume that we are dealing with\nonly one kind of particles (e.g., the nucleons), secondly, we\ndisregard the standard Schrödinger term in the evolution and,\nfinally, we divide the whole space in cells of volume \\(d^3\\). We\ndenote by \\(\\ket{n_1, n_2 ,\\ldots}\\) a Fock state in which there are\n\\(n_i\\) particles in cell \\(i\\), and we consider a superposition of\ntwo states \\(\\ket{n_1, n_2 , \\ldots}\\) and \\(\\ket{m_1, m_2 , \\ldots}\\)\nwhich differ in the occupation numbers of the various cells of the\nuniverse. With these assumptions it is quite easy to prove that the\nrate of suppression of the coherence between the two states (so that\nthe final state is one of the two and not their superposition) is\ngoverned by the quantity:  \nall cells of the universe appearing in the sum within the square\nbrackets in the exponent. Apart from differences relating to the\nidentity of the constituents, the overall physics is quite similar to\nthat implied by QMSL.  \nEquation 6 offers the opportunity of discussing the possibility of\nrelating the suppression of coherence to gravitational effects. In\nfact, with reference to this equation we notice that the worst case\nscenario (from the point of view of the time necessary to suppress\ncoherence) is that corresponding to the superposition of two states\nfor which the occupation numbers of the individual cells differ only\nby one unit. In this case the amplifying effect of taking the square\nof the differences disappears. Let us then ask the question: how many\nnucleons (at worst) should occupy different cells, in order for the\ngiven superposition to be dynamically suppressed within the time which\ncharacterizes human perceptual processes? Since such a time is of the\norder of \\(10^{-2}\\) sec and \\(f = 10^{-16}\\text{ sec}^{-1}\\),\nthe number of displaced nucleons must be of the order of \\(10^{18}\\),\nwhich corresponds, to a remarkable accuracy, to a Planck mass. This\nfigure seems to point in the same direction as Penrose’s\nattempts to relate reduction mechanisms to quantum gravitational\neffects (Penrose 1989). \nBy modifying the quantum dynamics, CSL like all collapse models makes\npredictions, which slightly differ from the standard quantum\nmechanical ones. We review the most important cases. \nEffects in superconducting devices. A detailed analysis has\nbeen presented in (Ghirardi & Rimini 1990). As shown there and as\nfollows from estimates about possible effects for superconducting\ndevices (Rae 1990; Gallis and Fleming 1990; Rimini 1995), and for the\nexcitation of atoms (Squires 1991), it turns out not to be possible,\nwith present technology, to perform clear-cut experiments allowing to\ndiscriminate the model from standard quantum mechanics. \nLoss of coherence in diffraction experiments with\nmacromolecules. The Viennese groups of A. Zeilinger first, and\nthen of M. Arndt, have performed important diffraction experiments\ninvolving macromolecules. The most relevant ones involve C\\(_{60}\\),\n(720 nucleons) (Arndt et al. 1999), C\\(_{70}\\), (840 nucleons)\n(Hackermueller et al. 2004) and more complex molecules (over 10,000\nnucleons, Eibenberger et al. 2013, Fein et al. 2019). So far these\nexperiments are not capable of testing the proposal of Alder,\ntherefore even less the weaker proposal of GRW, for the collapse rate\n(Toroš, Gasbarri, and Bassi 2017). Significant technological\ndevelopment is necessary in order to probe these values, possibly by\nrealizing the experiment in outer space where coherences can be\nmaintained for longer times (Kaltenbaek, Hechenblaikner, et al. 2012).\n \nLoss of coherence in opto-mechanical interferometers.\nRecently, an interesting proposal of testing the superposition\nprinciple by resorting to an experimental set-up involving a\n(mesoscopic) mirror has been advanced (Marshall et al. 2003). This\nstimulating proposal has led a group of scientists directly interested\nin Collapse Theories (Bassi, Ippoliti & Adler 2005; Adler, Bassi\n& Ippoliti 2005) to check whether the proposed experiment might be a\ncrucial one for testing dynamical reduction models versus quantum\nmechanics. The problem is extremely subtle because the extension of\nthe oscillations of the mirror is much smaller than the localization\naccuracy of GRW, so that the localizations processes become almost\nineffective. However, quite recently a detailed reconsideration of the\nphysics of such systems has been performed and it has allowed to draw\nthe relevant conclusion that the proposal by Adler (2007) of changing\nthe frequency of the GRW theory of a factor like the one he has\nconsidered is untenable.  \nNon-interferometric tests in opto-mechanical systems. In\n2003, an interesting proposal of testing the superposition of a\nmesoscopic mirror was put forward (Marshall et al. 2003). This\nstimulating proposal faced strong technical difficulties, such as\nenvironmental decoherence, which prevents the detection of the\nsuperposition. There is, however, a side effect of collapse theories\nthat can be exploited in such systems. Indeed, the collapse of the\nwavefunction leads to an effective noise on the center of mass of the\nsystem (Collett & Pearle 2003), which can be eventually bounded\nthrough experiments. The optomechanical application has been proposed\n(Bahrami, Paternostro, et al. 2014; Nimmrichter, et al. 2014; Diosi\n2015) to test such an effect, and various experiments showed the\npotential of this technique. They range from nanomechanical cantilever\ncooled to millikelvin temperature (Vinante, Bahrami, et al. 2016;\nVinante, Mezzena, et al. 2017) to the gravitational wave detectors\nLIGO, AURIGA and LISA Pathfinder (Carlesso, Bassi, et al. 2016; Helou\net al. 2017) as well as optically or magnetically levitated systems at\nroom temperature (Zheng et al.  2020; Pontin et al. 2019 [Other\nInternet Resources]). Recently, several proposals were presented to\npush even further the bounds on the collapse parameters, which now\nplace constrains just below f = 10−8\nsec−1 at d = 10−7 m. These proposals\nexploit different possible modification of current experiments, for\nexample a multi-layer structure of the test mass (Carlesso, Vinante,\net al. 2018) would amplify the bound for particular values of d. One\ncan also consider different degrees of freedom, for example the\nrotational ones could in principle be more sensitive to collapse\neffects (Carlesso, Paternostro, et al. 2018; Schrinski, Stickler,\n& Hornberger 2017).  A first application was implemented in a\ntorsional experiment (Komori et al. 2020).  \nNon-interferometric experiments with cold atoms. The recent\nadvances in trapping, cooling and manipulating ensembles of atoms\npaved the way for testing collapse models with cold atoms. Similarly\nto optomechanical systems, bounds on the collapse parameters are\nderived by quantifying the Brownian noise induced by the collapse\nprocess. The focus is on the energy increase or the position\ndiffusion. To make an example, if the atomic cloud can freely evolve,\nthe energy will grow linearly with time, while the position spread\ngoes with the cubic power. Experimental bounds were obtained from both\nvariables and analyzed in Laloe, Muillin, and Pearle 2014 and\nBilardello et al. 2016 respectively.  \nSpontaneous X-ray emission from Germanium. Collapse models\nnot only forbid macroscopic superpositions to be stable, they share\nseveral other features which are forbidden by the standard theory. One\nof these is the spontaneous emission of radiation from otherwise\nstable systems, like atoms. While the standard theory predicts that\nsuch systems—if not excited—do not emit radiation,\ncollapse models allow for radiation to be produced, as a consequence\nof the interaction between the system and the noise responsible for\nthe collapse. The emission rate has been computed for free charged\nparticles (Fu 1997; Adler, Bassi, & Donadi 2013), an harmonic\noscillator (Bassi & Donadi 2014; Donadi, Deckert, & Bassi\n2014) and for hydrogenic atoms (Adler et al. 2007). A formula for the\nradiation emission from a generic system is given in (Donadi &\nBassi 2014). The theoretical predictions are compatible with current\nexperimental data (Fu 1997). At any rate, the importance of such\nexperiments lies in the fact that—so far—they provide the\nstrongest upper bounds on the collapse parameters (Adler & Bassi\n2007). But this is not the whole story: very recently (Curceanu,\nHiesmayr, & Piscicchia 2015; Curceanu, Bartalucci, et al. 2016;\nPiscicchia et al. 2017), following this line of research, it was\nproven experimentally that the proposal by Adler (2007) of a drastic\nchange of the frequency of the localizations with respect to those of\nthe original GRW paper is definitely incompatible with the\nexperimental data, unless the CSL model is modified by taking a\nnon-white noise (which is actually a reasonable assumption, if the\nnoise is physical).  \n CSL and conscious perceptions. One interesting feature of\nCSL is that when conscious perceptions are involved, the collapse time\nof two brain states in a superposition and the time which is necessary\nfor the emergence of a definite perception, are quite similar, and\nthis has some (small but significant) implications concerning the\nprobabilities of the outcomes. This point has been analyzed in detail\nand explicitly evaluated by resorting to a simple model of a quantum\nsystem subjected to reduction processes (Ghirardi & Romano 2014).\nThe idea is to consider a spin 1/2 particle whose spin rotates around\nthe x-axis with a frequency of about one hundreth of the one of\nthe random measurements ascertaining whether its spin is UP or DOWN\nwith respect to the z-axis. It turns out that for a\nsuperposition with amplitudes \\(a\\) and \\(b\\) of the two eigenstates\nof S\\(_z\\), the probability of the two supervening perceptions\nassociated to the two outcomes will differ of about 1% from those\npredicted by quantum mechanics, i.e. \\(\\lvert a\\rvert^2\\) and \\(\\lvert\nb\\rvert^2\\), respectively. \nThe test would be quite interesting also for the general meaning of\ncollapse theories because it will give some practical evidence\nconcerning the fact that, in the case in which a superposition of two\nmicroscopic different states which are able to trigger two precise\n(and different) perceptions, the brain actually collapses the\nwavefunction yielding only one perception, an clear-cut indication\nthat the standard theory cannot run the whole process. \nSummarizing, due to fast technological improvements, experiments in\nwhich one might test the deviations from Standard Quantum Theory\nimplied by Collapse Models, seems to have become more and more\nfeasible.  \nA. Pais famously recalls in his biography of Einstein:  \nWe often discussed his notions on objective reality. I recall that\nduring one walk Einstein suddenly stopped, turned to me and asked\nwhether I really believed that the moon exists only when I look at it.\n(Pais 1982: 5) \nIn the context of Einstein’s remarks in Albert Einstein,\nPhilosopher-Scientist (Schilpp 1949), we can regard this\nreference to the moon as an extreme example of ‘a fact that\nbelongs entirely within the sphere of macroscopic concepts’, as\nis also a mark on a strip of paper that is used to register the\noutcome of a decay experiment, so that  \nas a consequence, there is hardly likely to be anyone who would be\ninclined to consider seriously […] that the existence of the\nlocation is essentially dependent upon the carrying out of an\nobservation made on the registration strip. For, in the macroscopic\nsphere it simply is considered certain that one must adhere to the\nprogram of a realistic description in space and time; whereas in the\nsphere of microscopic situations one is more readily inclined to give\nup, or at least to modify, this program. (1949: 671) \nHowever,  \nthe ‘macroscopic’ and the ‘microscopic’ are so\ninter-related that it appears impracticable to give up this program in\nthe ‘microscopic’ alone. (1949: 674) \nOne might speculate that Einstein would not have taken the DRP\nseriously, given that it is a fundamentally indeterministic program.\nOn the other hand, the DRP allows precisely for this middle ground,\nbetween giving up a ‘classical description in space and\ntime’ altogether (the moon is not there when nobody looks), and\nrequiring that it be applicable also at the microscopic level (as\nwithin some kind of ‘hidden variables’ theory). It would\nseem that the pursuit of ‘realism’ for Einstein was more a\nprogram that had been very successful rather than an a priori\ncommitment, and that in principle he would have accepted attempts\nrequiring a radical change in our classical conceptions concerning\nmicrosystems, provided they would nevertheless allow to take a\nmacrorealist position matching our definite perceptions at this\nscale. \nIn the DRP, we can say of an electron in an EPR-Bohm situation that\n‘when nobody looks’, it has no definite spin in any\ndirection , and in particular that when it is in a superposition of\ntwo states localised far away from each other, it cannot be thought to\nbe at a definite place (see, however, the remarks in Section 11). In\nthe macrorealm, however, objects do have definite positions and are\ngenerally describable in classical terms. That is, in spite of the\nfact that the DRP program is not adding ‘hidden variables’\nto the theory, it implies that the moon is definitely there even if no\nsentient being looks at it. In the words of J. S. Bell, the DRP \nallows electrons (in general microsystems) to enjoy the cloudiness of\nwaves, while allowing tables and chairs, and ourselves, and black\nmarks on photographs, to be rather definitely in one place rather than\nanother, and to be described in classical terms. (Bell 1989a: 364) \nSuch a program, as we have seen, is implemented by assuming only the\nexistence of wave functions, and by proposing a unified dynamics that\ngoverns both microscopic processes and ‘measurements’.\nWith reference to the latter, no vague definitions are needed. The new\ndynamical equations govern the unfolding of any physical process, and\nthe macroscopic ambiguities that would arise from the linear evolution\nare theoretically possible, but only of momentary duration, of no\npractical importance and no source of embarrassment. \nWe have not yet analyzed the implications about locality, but since in\nthe DRP program no hidden variables are introduced, the situation can\nbe no worse than in ordinary quantum mechanics: ‘by adding\nmathematical precision to the jumps in the wave function’,\nthe GRW theory ‘simply makes precise the action at a\ndistance of ordinary quantum mechanics’ (Bell 1987: 46).\nIndeed, a detailed investigation of the locality properties of the\ntheory becomes possible as shown by Bell himself (Bell 1987: 47).\nMoreover, as it will become clear when we will discuss the\ninterpretation of the theory in terms of mass density, the QMSL and\nCSL theories naturally account for the behaviour of macroscopic\nobjects, corresponding to our definite perceptions about them, the\nmain objective of Einstein’s requirements. \nThe achievements of the DRP which are relevant for the debate about\nthe foundations of quantum mechanics can also be concisely summarized\nin the words of H.P. Stapp: \nThe collapse mechanisms so far proposed could, on the one hand, be\nviewed as ad hoc mutilations designed to force ontology to kneel to\nprejudice. On the other hand, these proposals show that one can\ncertainly erect a coherent quantum ontology that generally conforms to\nordinary ideas at the macroscopic level. (Stapp 1989: 157) \nAs soon as the GRW proposal appeared and attracted the attention of\nJ.S. Bell, it stimulated him to look at it from the point of view of\nrelativity theory. As he stated subsequently: \nWhen I saw this theory first, I thought that I could blow it out of\nthe water, by showing that it was grossly in violation of\nLorentz invariance. That’s connected with the problem of\n‘quantum entanglement’, the EPR paradox. (Bell 1989b: 1) \nActually, he had already investigated this point by studying the\neffect on the theory of a transformation mimicking a nonrelativistic\napproximation of a Lorentz transformation and he arrived at a\nsurprising conclusion: \n… the model is as Lorentz invariant as it could be in its\nnonrelativistic version. It takes away the ground of my fear that any\nexact formulation of quantum mechanics must conflict with fundamental\nLorentz invariance. (Bell 1987: 49) \nWhat Bell had actually proved by resorting to a two-times formulation\nof the Schrödinger equation is that the model violates locality\nby violating outcome independence and not, as deterministic hidden\nvariable theories do, parameter independence. \nIndeed, with reference to this point we recall that, as extensively\ndiscussed in the literature (Suppes & Zanotti 1976; van Fraassen\n1982; Jarrett 1984; Shimony 1984; see also the entry on\n Bell’s Theorem),\n Bell’s locality assumption is equivalent to the conjunction of\ntwo other assumptions, viz., in Shimony’s terminology, parameter\nindependence and outcome independence. In view of the experimental\nviolation of Bell’s inequality, one has to give up either one or\nboth these assumptions. The above splitting of the locality\nrequirement into two logically independent conditions is particularly\nuseful in discussing the different status of CSL and deterministic\nhidden variable theories with respect to relativistic requirements.\nActually, as proved by Jarrett himself, when parameter independence is\nviolated, if one had access to the variables which specify completely\nthe state of individual physical systems, one could send\nfaster-than-light signals from one wing of the apparatus to the other.\nMoreover, in Ghirardi and Grassi (1996) it has been proved that it is\nimpossible to build a genuinely relativistically invariant\ntheory which, in its nonrelativistic limit, exhibits parameter\ndependence. Here we use the term genuinely invariant to\ndenote a theory for which there is no (hidden) preferred reference\nframe. On the other hand, if locality is violated only by the\noccurrence of outcome dependence then faster-than-light signaling\ncannot be achieved (Eberhard 1978; Ghirardi, Rimini, & Weber\n1980). Few years after the just mentioned proof by Bell, it has been\nshown in complete generality (Ghirardi, Grassi, Butterfield, &\nFleming 1993) that the GRW and CSL theories, just as standard quantum\nmechanics, exhibit only outcome dependence. This is to some extent\nencouraging and shows that there are no reasons of principle making\nunviable the project of building a relativistically invariant DRM. \nLet us be more specific about this crucial problem. P. Pearle was the\nfirst to propose (Pearle 1990) a relativistic generalization of CSL to\na quantum field theory describing a fermion field coupled to a meson\nscalar field enriched with the introduction of stochastic and\nnonlinear terms. A quite detailed discussion of this proposal was\npresented in (Ghirardi, Grassi & Pearle 1990) where it was shown\nthat the theory enjoys of all properties which are necessary in order\nto meet the relativistic constraints. Pearle’s approach requires\nthe precise formulation of the idea of stochastic Lorentz\ninvariance.  \nIn this model, one considers a fermion field coupled to a meson field\nand puts forward the idea of inducing localizations for the fermions\nthrough their coupling to the mesons with a stochastic dynamical\nreduction mechanism acting on the meson variables. In practice,\nworking in the interaction picture, one considers the standard\nHeisenberg evolution equations for the two coupled fields and a\nTomonaga-Schwinger CSL-type evolution equation with a skew-hermitian\ncoupling to a c-number stochastic potential for the state vector. This\napproach has been systematically investigated by Ghirardi, Grassi, and\nPearle (1990), to which we refer the reader for a detailed discussion.\nHere we stress that, under specific approximations, one obtains in the\nnon-relativistic limit a CSL-type equation inducing spatial\nlocalization. However, due to the white noise nature of the stochastic\npotential, novel renormalization problems arise: the increase per unit\ntime and per unit volume of the energy of the meson field is infinite\ndue to the fact that infinitely many mesons are created. This point\nhas also been lucidly discussed by Bell (1989c [2007]) in the talk he\ndelivered at Trieste on the occasion of the 25th anniversary of the\nInternational Centre for Theoretical Physics. This talk appeared under\nthe title The Trieste Lecture of John Stewart Bell. For these\nreasons one cannot consider this as a satisfactory example of a\nrelativistic reduction model. \nIn the years following the just mentioned attempts there has been a\nflourishing of researches aimed at getting the desired result. Let us\nbriefly comment on them. As already mentioned, the source of the\ndivergences is the assumption of point interactions between the\nquantum field operators in the dynamical equation for the statevector,\nor, equivalently, the white character of the stochastic noise. Having\nthis aspect in mind, P. Pearle (1989), L. Diosi (1990) and A. Bassi\nand G.C. Ghirardi (2002) reconsidered the problem from the beginning\nby investigating nonrelativistic theories with nonwhite Gaussian\nnoises. The problem turns out to be very difficult from the\nmathematical point of view, but steps forward have been made. In\nrecent years, a precise formulation of the nonwhite generalization\n(Bassi & Ferialdi 2009a and 2009b) of the so-called QMUPL model,\nwhich represents a simplified version of GRW and CSL, has been\nproposed. Moreover, a perturbative approach for the CSL model has been\nworked out (Adler & Bassi 2007, 2008). Further work is\nnecessary. This line of thought is very interesting at the\nnonrelativistic level; however, it is not yet clear whether it will\nlead to a real step forward in the development of relativistic\ntheories of spontaneous collapse.  \nIn the same spirit, Nicrosini and Rimini (2003) tried to smear out the\npoint interactions without success because, in their approach, a\npreferred reference frame had to be chosen in order to circumvent the\nnonintegrability of the Tomonaga-Schwinger equation  \nOther interesting and different approaches have been suggested. Among\nthem we mention the one by Dove and Squires (1996) based on discrete\nrather than continuous stochastic processes and those by Dowker and\nHerbauts (2004) and Dawker and Henson (2004) formulated on a discrete\nspace-time. \nPrecisely in the same years similar attempts to formulate a\nrelativistic generalization of Bohmian Mechanics were ongoing,\nencountering difficulties. Relevant steps are represented by a paper\n(Dürr 1999) resorting to a preferred spacetime slicing, by the\ninvestigations of Goldstein and Tumulka (2003) and by other scientists\n(Berndl et al. 1996). However, we must acknowledge that no one of\nthese attempts has led to a fully satisfactory solution of the problem\nof having a theory without observers, like Bohmian mechanics, which is\nperfectly satisfactory from the relativistic point of view, precisely\ndue to the fact that they are not genuinely Lorentz invariant\nin the sense we have made precise before. Mention should be made also\nof the attempt by Horton and Dewdney (2001) to build a\nrelativistically invariant model based on particle trajectories.  \nLet us come back to the relativistic DRP. Some important changes have\noccurred quite recently. Tumulka (2006a) succeeded in proposing a\nrelativistic version of the GRW theory for N non-interacting\ndistinguishable particles, based on the consideration of a multi-time\nwavefunction whose evolution is governed by Dirac like equations and\nadopts as its Primitive Ontology (see the next section) the one which\nattaches a primary role to the space and time points at which\nspontaneous localizations occur, as originally suggested by Bell\n(1987). To our knowledge this represents the first proposal of a\nrelativistic dynamical reduction mechanism, which satisfies all\nrelativistic requirements. In particular it is free from divergences\nand foliation independent. However, it is formulated only for systems\ncontaining a fixed number of noninteracting fermions. \nD. Bedingham (2011) following strictly the original proposal by Pearle\n(1990) of a quantum field theory inducing reductions based on a\nTomonaga- Schwinger equation, has worked out an analogous model which,\nhowever, overcomes the difficulties of the original model. In fact,\nBedingham has circumvented the crucial problems arising from point\ninteractions by (paying the price of) introducing, besides the fields\ncharacterizing the Quantum Field Theories he is interested in, an\nauxiliary relativistic field that amounts to a smearing of the\ninteractions whilst preserving Lorentz invariance and frame\nindependence. Adopting this point of view and taking advantage also of\nthe proposal by Ghirardi (2000) concerning the appropriate way to\ndefine objective properties at any space-time point \\(x\\), he has been\nable to work out a fully satisfactory and consistent relativistic\nscheme for quantum field theories in which reduction processes may\noccur. \nTaking once more advantage of the ideas of the paper by Ghirardi\n(2000), various of the just quoted authors (Bedingham, Duerr,\nGhirardi, et al. 2014), have been able to prove that it is possible to\nwork out a relativistic generalization of Collapse models when their\nprimitive ontology is taken to be the one given by the mass density\ninterpretation for the nonrelativistic case we will present in what\nfollows. \nIn view of these results and taking into account the interesting\ninvestigations concerning relativistic Bohmian-like theories,the\nconclusions that Tumulka has drawn concerning the status of attempts\nto account for the macro-objectification process from a relativistic\nperspective are well-founded: \nA somewhat surprising feature of the present situation is that we seem\nto arrive at the following alternative: Bohmian mechanics shows that\none can explain quantum mechanics, exactly and completely, if one is\nwilling to pay with using a preferred slicing of spacetime; our model\nsuggests that one should be able to avoid a preferred slicing of\nspacetime if one is willing to pay with a certain deviation from\nquantum mechanics, (Tumulka 2006a: 842) \na conclusion that he has rephrased and reinforced in Tumulka\n(2006c: 350): \nThus, with the presently available models we have the alternative:\neither the conventional understanding of relativity is not right, or\nquantum mechanics is not exact. \nVery recently, a thorough and illuminating discussion of the important\napproach by Tumulka has been presented by Tim Maudlin (2011) in the\nthird revised edition of his book Quantum Non-Locality and\nRelativity. Tumulka’s position is perfectly consistent with\nthe present ideas concerning the attempts to transform relativistic\nstandard quantum mechanics into an ‘exact’ theory in the\nsense which has been made precise by J. Bell. Since the only unified,\nmathematically precise and formally consistent formulations of the\nquantum description of natural processes are Bohmian mechanics and\nGRW-like theories, if one chooses the first alternative one has to\naccept the existence of a preferred reference frame, while in the\nsecond case one is not led to such a drastic change of position with\nrespect to relativistic concepts but must accept that the ensuing\ntheory disagrees with the predictions of quantum mechanics and\nacquires the status of a rival theory with respect to it. \nIn spite of the fact that the situation is, to some extent, still open\nand requires further investigations, it has to be recognized that the\nefforts which have been spent on such a program have made a better\nunderstanding of some crucial points possible and have thrown light on\nsome important conceptual issues. First, they have led to a completely\ngeneral and rigorous formulation of the concept of stochastic\ninvariance. Second, they have prompted a critical reconsideration,\nbased on the discussion of smeared observables with compact support,\nof the problem of locality at the individual level. This analysis has\nbrought out the necessity of reconsidering the criteria for the\nattribution of objective local properties to physical systems. In\nspecific situations, one cannot attribute any local property to a\nmicrosystem: any attempt to do so gives rise to ambiguities. However,\nwhen dealing with macroscopic systems, the impossibility of\nattributing to them local properties (or, equivalently, the ambiguity\nassociated to such properties) lasts only for time intervals necessary\nfor the dynamical reduction to take place. Moreover, no objective\nproperty corresponding to a local observable, even for microsystems,\ncan emerge as a consequence of a measurement-like event occurring in a\nspace-like separated region: such properties emerge only in the future\nlight cone of the considered macroscopic event. Finally, recent\ninvestigations (Ghirardi & Grassi 1996; Ghirardi 2000) have shown\nthat the very formal structure of the theory is such that it does not\nallow, even conceptually, to establish cause-effect relations between\nspace-like events. \nThe conclusion of this section, is that the question of whether a\nrelativistic dynamical reduction program can find a satisfactory\nformulation seems to admit a positive answer. \nConnected to collapses and relativity, a paper by Conway and Kochen\n(2006a, 2006b [Other Internet Resources]) is of relevance. The first\nand most important aim of the paper is the derivation of what the\nauthors have called The Free Will Theorem, putting forward\nthe provocative idea that if human beings are free to make their\nchoices about the measurements they will perform on one of a pair of\nfar-away entangled particles, then one must admit that also the\nelementary particles involved in the experiment have free will. A\ndetailed discussion of what the Free Will theorem implies would be\nneeded; for us here the relevant fact is that the authors claim that\ntheir theorem implies, as a byproduct, the impossibility of\nelaborating a relativistically invariant dynamical reduction model. A\nlively debate has arisen. At the end, Goldstein et al. (2010) have\nmade clear why the argument of Conway and Kochen is not pertinent. We\nmay conclude that nothing in principle forbids a perfectly\nsatisfactory relativistic generalization of the GRW theory, and,\nactually, as repeatedly stressed, there are many elements which\nindicate that this is actually feasible. \nSome authors (Albert & Vaidman 1989; Albert 1990, 1992) have\nraised an interesting objection concerning the emergence of definite\nperceptions within Collapse Theories. The objection is based on the\nfact that one can easily imagine situations leading to definite\nperceptions, that nevertheless do not involve the displacement of a\nlarge number of particles up to the stage of the perception itself.\nThese cases would then constitute actual measurement situations which\ncannot be described by the GRW theory, contrary to what happens for\nthe idealized (according to the authors) situations considered in many\npresentations of it, i.e., those involving the displacement of some\nsort of pointer. To be more specific, the above papers consider a\n‘measurement-like’ process whose output is the emission of\na burst of few photons triggered by the position in which a particle\nhits a screen. This can easily be devised by considering, e.g., a\nStern-Gerlach set-up in which a spin 1/2 microsystem, according to the\nvalue of its spin component, hits a fluorescent screen in different\nplaces and excites a small number of atoms which subsequently decay,\nemitting a small number of photons.  \nThe argument goes as follows: if one triggers the apparatus with a\nsuperposition of two spin states, since only a few atoms are excited,\nsince the excitations involve displacements which are smaller than the\ncharacteristic localization distance of GRW, since GRW does not induce\nreductions on photon states and, finally, since the photon states\nimmediately overlap, there is no way for the spontaneous localization\nmechanism to become effective in suppressing the ensuing superposition\nof the states ‘photons emerging from point \\(A\\) of the\nscreen’ and ‘photons emerging from point \\(B\\) of the\nscreen’. On the other hand, since the visual perception\nthreshold is quite low (about 6–7 photons), there is no doubt\nthat the naked eye of a human observer is sufficient to detect whether\nthe luminous spot on the screen is at \\(A\\) or at \\(B\\). The\nconclusion follows: in the case under consideration no dynamical\nreduction can take place and as a consequence no measurement is over,\nno outcome is definite, up to the moment in which a conscious observer\nperceives the spot. \nAicardi et al. (1991) have presented a detailed answer to this\ncriticism: it is agreed that in the considered case the superposition\npersists for long times (actually the superposition must persist\nsince, being the system under consideration microscopic, one could\nperform interference experiments which everybody would expect to\nconfirm quantum mechanics). However, to deal in the appropriate and\ncorrect way with such a criticism, one has to consider all the systems\nwhich enter into play (electron, screen, photons and brain) and the\nuniversal dynamics governing all relevant physical processes. A simple\nestimate of the number of ions which are involved in the transmission\nof the nervous signal up to the higher virtual cortex makes perfectly\nplausible that, in the process, a sufficient number of particles are\ndisplaced by a sufficient spatial amount to satisfy the conditions\nunder which, according to the GRW theory, the suppression of the\nsuperposition of the two nervous signals will take place within the\ntime scale of the perception. \nThis analysis by no means amounts to attributing a special role to the\nconscious observer or to the perception process. The observer’s\nbrain is the only system present in the set-up in which a\nsuperposition of two states involving different locations of a large\nnumber of particles occurs. As such it is the only place where the\nreduction can and actually must take place according to the theory. It\nis extremely important to stress that if in place of the eye of a\nhuman being one puts in front of the photons’ beam a spark chamber or\na device leading to the displacement of a macroscopic pointer, or\nproducing ink spots on a computer output, reduction will equally take\nplace. In the given example, the human nervous system is simply a\nphysical system, a specific assembly of particles, which performs the\nsame function as any other device, if no other such device interacts\nwith the photons before the human observer does. It follows that it is\nincorrect and seriously misleading to claim that the GRW theory\nrequires a conscious observer in order for measurements to have a\ndefinite outcome. \nA further remark may be appropriate. The above analysis could be taken\nby the reader as indicating a very naive and oversimplified attitude\ntowards the deep problem of the mind-brain correspondence. There is no\nclaim and no presumption that GRW allows a physicalist explanation of\nconscious perception. It is only pointed out that, based on what we\nknow about the purely physical aspects of the process, one can state\nthat before the nervous pulses reach the higher visual cortex, the\nconditions guaranteeing the suppression of one of the two signals are\nverified. In brief, a consistent use of the dynamical reduction\nmechanism in the above situation accounts for the definiteness of the\nconscious perception, even in the extremely peculiar situation devised\nby Albert and Vaidman. \nAs stressed in the opening sentences of this contribution, the most\nserious problem of standard quantum mechanics lies in its being\nextremely successful in telling us about what we observe, but\nbeing basically silent on what there is. This specific\nfeature is closely related to the probabilistic interpretation of the\nstatevector, combined with the completeness assumption of the theory.\nNotice that what is under discussion is the probabilistic\ninterpretation, not the probabilistic character, of the theory. Also\ncollapse theories have a fundamentally stochastic character but, due\nto their most specific feature, i.e., that of driving the statevector\nof any individual physical system into appropriate and physically\nmeaningful manifolds, they allow for a different interpretation. One\ncould even say (if one wants to avoid that they too, as the standard\ntheory, speak only of what we find) that they\nrequire a different interpretation, one that accounts for our\nperceptions at the appropriate, i.e., macroscopic, level. \nWe must admit that this opinion is not universally shared. According\nto various authors, the ‘rules of the game’ embodied in\nthe precise formulation of the GRW and CSL theories represent all\nthere is to say about them. However, this cannot be the whole story:\nstricter and more precise requirements than the purely formal ones\nmust be imposed for a theory to be taken seriously as a fundamental\ndescription of natural processes (an opinion shared by J. Bell). This\nrequest of going beyond the purely formal aspects of a theoretical\nscheme has been denoted as (the necessity of specifying) the Primitive\nOntology (PO) of the theory in an extremely interesting paper (Allori,\net al. 2008). The fundamental requisite of the PO is that it should\nmake absolutely precise what the theory is fundamentally about. \nThis is not a new problem; as already mentioned it has been raised by\nJ. Bell since his first presentation of the GRW theory. Let me\nsummarize the terms of the debate. Given that the wavefunction of a\nmany-particle system lives in a (high-dimensional) configuration\nspace, which is not endowed with a direct physical meaning connected\nto our experience of the world around us, Bell wanted to identify the\n‘local beables’ of the theory, the quantities on which one\ncould base a description of the perceived reality in ordinary\nthree-dimensional space. In the specific context of QMSL, he (Bell\n1987: 45) suggested that the ‘GRW jumps’, which we called\n‘hittings’, could play this role. In fact they occur at\nprecise times in precise positions of the three-dimensional space. As\nsuggested in (Allori, et al. 2008) we will denote this position\nconcerning the PO of the GRW theory as the ‘flashes\nontology.’ \nHowever, later Bell himself suggested that the most natural\ninterpretation of the wavefunction in the context of a collapse theory\nwould be that it describes the ‘density […] of\nstuff’ in the 3N-dimensional configuration space (Bell 1990:\n30), the natural mathematical framework for describing a system of\n\\(N\\) particles. Allori et al. (2008) appropriately have pointed out\nthat this position amounts to avoiding commitment about the PO\nontology of the theory and, consequently, to leaving vague the precise\nand meaningful connections it permits to be established between the\nmathematical description of the unfolding of physical processes and\nour perception of them. \nThe interpretation which, in our opinion, is most appropriate for\ncollapse theories, has been proposed in (Ghirardi, Grassi, &\nBenatti 1995) and has been referred in Allori et al. 2008 as\n‘the mass density ontology’. Let us briefly describe\nit. \nFirst of all, various investigations (Pearle & Squires 1994) had\nmade clear that QMSL and CSL needed a modification, i.e., the\ncharacteristic localization frequency of the elementary constituents\nof matter had to be made proportional to the mass characterizing the\nparticle under consideration. In particular, the original frequency\nfor the hitting processes \\(f = 10^{-16}\\) sec\\(^{-1}\\) is the one\ncharacterizing the nucleons, while, e.g., electrons would suffer\nhittings with a frequency reduced by about 2000 times. Unfortunately\nwe have no space to discuss here the physical reasons which make this\nchoice appropriate; we refer the reader to the above paper, as well as\nto the detailed analysis by Peruzzi and Rimini (2000). With this\nmodification, what the nonlinear dynamics strives to make\n‘objectively definite’ is the mass distribution in the\nwhole universe. Second, a deep critical reconsideration (Ghirardi,\nGrassi, & Benatti 1995) has made evident how the concept of\n‘distance’ that characterizes the Hilbert space is\ninappropriate in accounting for the similarity or difference between\nmacroscopic situations. Just to give a convincing example, consider\nthree states \\(\\ket{h} , \\ket{h^*}\\) and \\(\\ket{t}\\) of a macrosystem\n(let us say a massive macroscopic bulk of matter), the first\ncorresponding to its being located here, the second to its having the\nsame location but one of its atoms (or molecules) being in a state\northogonal to the corresponding state in \\(\\ket{h}\\), and the third\nhaving exactly the same internal state of the first but being\ndifferently located (there). Then, despite the fact that the first two\nstates are indistinguishable from each other at the macrolevel, while\nthe first and the third correspond to completely different and\ndirectly perceivable situations, the Hilbert space distance between\n\\(\\ket{h}\\) and \\(\\ket{h^*}\\), is equal to that between \\(\\ket{h}\\)\nand \\(\\ket{t}\\). \nWhen the localization frequency is related to the mass of the\nconstituents, then, in completely generality (i.e., even when one is\ndealing with a body which is not almost rigid, such as a gas or a\ncloud), the mechanism leading to the suppression of the superpositions\nof macroscopically different states is fundamentally governed by the\nthe integral of the squared differences of the mass densities\nassociated to the two superposed states. Actually, in the original\npaper the mass density at a point was identified with its average over\nthe characteristic volume of the theory, i.e., \\(10^{-15}\\) cm\\(^3\\)\naround that point. It is however easy to convince oneself that there\nis no need to do so and that the mass density at any point, directly\nidentified by the statevector (see below), is the appropriate quantity\non which to base an appropriate ontology. Accordingly, we take the\nfollowing attitude: what the theory is about, what is real ‘out\nthere’ at a given space point \\(\\boldsymbol{x}\\), is just a\nfield, i.e., a variable \\(m(\\mathbf{x},t)\\) given by the expectation\nvalue of the mass density operator \\(M(\\boldsymbol{x})\\) at\n\\(\\boldsymbol{x}\\) obtained by multiplying the mass of any kind of\nparticle times the number density operator for the considered type of\nparticle and summing over all possible types of particles which can be\npresent: \nHere \\(\\ket{F,t}\\) is the statevector characterizing the system at the\ngiven time, and \\(a^*_{(k)}(\\boldsymbol{x})\\) and\n\\(a_{(k)}(\\boldsymbol{x})\\) are the creation and annihilation\noperators for a particle of type \\(k\\) at point \\(\\boldsymbol{x}\\). It\nis obvious that within standard quantum mechanics such a function\ncannot be endowed with any objective physical meaning due to the\noccurrence of linear superpositions which give rise to values that do\nnot correspond to what we find in a measurement process or what we\nperceive. In the case of GRW or CSL theories, if one considers only\nthe states allowed by the dynamics one can give a description of the\nworld in terms of \\(m(\\boldsymbol{x},t)\\), i.e., one recovers a\nphysically meaningful account of physical reality in the usual\n3-dimensional space and time. To illustrate this crucial point we\nconsider, first of all, the embarrassing situation of a macroscopic\nobject in the superposition of two differently located position\nstates. We have then simply to recall that in a collapse model\nrelating reductions to mass density differences, the dynamics\nsuppresses in extremely short times the embarrassing superpositions of\nsuch states to recover the mass distribution corresponding to our\nperceptions. Let us come now to a microsystem and let us consider the\nequal weight superposition of two states \\(\\ket{h}\\) and \\(\\ket{t}\\)\ndescribing a microscopic particle in two different locations. Such a\nstate gives rise to a mass distribution corresponding to 1/2 of the\nmass of the particle in the two considered space regions. This seems,\nat first sight, to contradict what is revealed by any measurement\nprocess. But in such a case we know that the theory implies that the\ndynamics running all natural processes within GRW ensures that\nwhenever one tries to locate the particle they will always find it in\na definite position, e.g., one and only one of the Geiger counters\nwhich might be triggered by the passage of the proton will fire, just\nbecause a superposition of ‘a counter which has fired’ and\n‘one which has not fired’ is dynamically forbidden. \nThis analysis shows that one can consider at all levels (the micro and\nthe macroscopic ones) the field \\(m(\\mathbf{x},t)\\) as accounting for\n‘what is out there’, as originally suggested by\nSchrödinger with his realistic interpretation of the square of\nthe wave function of a particle as representing the\n‘fuzzy’ character of the mass (or charge) of the particle.\nObviously, within standard quantum mechanics such a position cannot be\nmaintained because   \nwavepackets diffuse, and with the passage of time become infinitely\nextended … but however far the wavefunction has extended, the\nreaction of a detector … remains spotty (Bell 1990: 39).  \nAs we hope to have made\nclear, the picture is radically different when one takes into account\nthe new dynamics which succeeds perfectly in reconciling the spread\nand sharp features of the wavefunction and of the detection process,\nrespectively. \nIt is also extremely important to stress that, by resorting to the\nquantity (7) one can define an appropriate ‘distance’\nbetween two states as the integral over the whole 3-dimensional space\nof the square of the difference of \\(m(\\boldsymbol{x},t)\\) for the two\ngiven states, a quantity which turns out to be perfectly appropriate\nto ground the concept of macroscopically similar or distinguishable\nHilbert space states. In turn, this distance can be used as a basis to\ndefine a sensible psychophysical correspondence within the theory. \nThere has been a lively debate around a problem which has its origin,\naccording to some of the authors which have raised it, in the fact\nthat even though the localization process which corresponds to\nmultiplying the wave function times a Gaussian, thus leading to wave\nfunctions strongly peaked around the position of the hitting, they\nallow nevertheless the final wavefuntion to be different from zero\nover the whole space. The first criticism of this kind was raised by\nA. Shimony (1990) and can be summarized by his sentence, \n[one should not] tolerate“tails” which are so broad that\ndifferent parts [...] can be discriminated by the senses, even if very\nlow probability amplitude is assigned to the tail (1990: 53)\n \nAfter a localization of a macroscopic system, typically the pointer of\nthe apparatus, its centre of mass will be associated to a wave\nfunction which is different from zero over the whole space. If one\nadopts the probabilistic interpretation of the standard theory, this\nmeans that even when the measurement process is over, there is a\nnonzero (even though extremely small) probability of finding its\npointer in an arbitrary position, instead of the one corresponding to\nthe registered outcome. This is taken as unacceptable, as indicating\nthat the DRP does not actually overcome the macro-objectification\nproblem. \nLet us state immediately that the (alleged) problem arises entirely\nfrom keeping the standard interpretation of the wave function\nunchanged, in particular assuming that its squared modulus gives the\nprobability density of the position variable. However, as we have\ndiscussed in the previous section, there are much more serious reasons\nof principle which require to abandon the probabilistic interpretation\nand replace it either with the ‘flash ontology’, or with\nthe ‘ mass density ontology’ which we have discussed\nabove. \nBefore entering into a detailed discussion of this subtle point we\nneed to focus the problem better. Suppose one adopts, for the moment,\nthe conventional quantum position. We agree that, within such a\nframework, the fact that wave functions never have strictly compact\nspatial support can be considered puzzling. However this is an\nunavoidable problem arising directly from the mathematical features\n(spreading of wave functions) and from the probabilistic\ninterpretation of the theory, and not at all a problem peculiar to\ndynamical reduction models. Indeed, the fact that, e.g., the wave\nfunction of the center of mass of a pointer or of a table has not a\ncompact support has never been taken to be a problem for standard\nquantum mechanics. When, e.g., the center of mass of a table is\nextremely well peaked around a given point in space, it has always\nbeen accepted that it describes a table located at some position, and\nthat this corresponds in some way to our perception of it. It is\nobviously true that, for the given wave function, the quantum rules\nentail that if a measurement were performed the table could be found\n(with an extremely small probability) to be kilometers far away, but\nthis is not the measurement or the macro-objectification\nproblem of the standard theory. The latter concerns a completely\ndifferent situation, i.e., that in which one is confronted with a\nsuperposition with comparable weights of two macroscopically separated\nwave functions, both of which possess tails (i.e., have non-compact\nsupport) but are appreciably different from zero only in far-away\nnarrow intervals. This is the really embarrassing situation which\nconventional quantum mechanics is unable to make understandable. To\nwhich perception of the position of the pointer (of the table) does\nthis wave function correspond? \nWithin GRW, the superposition of two states which, when considered\nindividually, are assumed to lead to different and definite\nperceptions of macroscopic locations, are dynamically forbidden. If\nsome process tends to produce such superpositions, then the reducing\ndynamics induces the localization of the centre of mass (the\nassociated wave function being appreciably different from zero only in\na narrow and precise interval). Correspondingly, the possibility\narises of attributing to the system the property of being in a\ndefinite place and thus of accounting for our definite perception of\nit. Summarizing, we stress once more that the criticism about the\ntails as well as the requirement that the appearance of\nmacroscopically extended (even though extremely small) tails be\nstrictly forbidden is exclusively motivated by uncritically committing\noneself to the probabilistic interpretation of the theory, even for\nwhat concerns the psycho-physical correspondence: when this position\nis taken, states assigning non-exactly vanishing probabilities to\ndifferent outcomes of position measurements should correspond to\nambiguous perceptions about these positions. Since neither within the\nstandard formalism nor within the framework of dynamical reduction\nmodels a wave function can have compact support, taking such a\nposition leads to conclude that it is just the linear character of the\nHilbert space description of physical systems which has to be given\nup. \nIt ought to be stressed that there is nothing in the GRW theory which\nforbids or makes problematic to assume that the localization function\nhas compact support, but it also has to be noted that following this\nline would be totally useless: since the evolution equation contains\nthe kinetic energy term, any function, even if it has compact support\nat a given time, will instantaneously spread, acquiring a tail\nextending over the whole space. If one sticks to the probabilistic\ninterpretation and one accepts the completeness of the description of\nthe states of physical systems in terms of the wave function, the tail\nproblem cannot be avoided. \nThe solution to the tails problem can only derive from abandoning\ncompletely the probabilistic interpretation and from adopting a more\nphysical and realistic interpretation relating ‘what is out\nthere’ to, e.g., the mass density distribution over the whole\nuniverse. In this connection, the following example will be\ninstructive. Take a massive sphere of normal density and mass of about\n1 kg. Classically, the mass of this body would be totally concentrated\nwithin the radius of the sphere, call it \\(r\\). In QMSL, after the\nextremely short time interval in which the collapse dynamics leads to\na ‘regime’ situation, and if one considers a sphere with\nradius \\(r + 10^{-5}\\) cm, the integral of the mass density over the\nrest of space turns out to be an incredibly small fraction (of the\norder of 1 over 10 to the power \\(10^{15})\\) of the mass of a single\nproton. In such conditions, it seems quite legitimate to claim that\nthe macroscopic body is localised within the sphere. \nHowever, also this quite reasonable conclusion has been questioned and\nit has been claimed (Lewis 1997), that the very existence of the tails\nimplies that the enumeration principle (i.e., the fact that the claim\n‘particle 1 is within this box & particle 2 is within this\nbox & … & particle \\(n\\) is within this box &\nno other particle is within this box’ implies the claim\n‘there are \\(n\\) particles within this box’) does not\nhold, if one takes seriously the mass density interpretation of\ncollapse theories. This paper has given rise to a long debate which\nwould be inappropriate to reproduce here.  \nWe conclude this brief analysis by stressing once more that, in our\nopinion, all the disagreements and the misunderstandings concerning\nthis problem have their origin in the fact that the idea that the\nprobabilistic interpretation of the wave function must be abandoned\nhas not been fully accepted by the authors who find some difficulties\nin the proposed mass density interpretation of the Collapse Theories.\nFor a more recent reconsideration of the problem we refer the reader\nto the paper by Lewis (2003). \nWe recall that, as stated in Section 3, the macro-objectification\nproblem has been at the centre of the most lively and most challenging\ndebate originated by the quantum view of natural processes. According\nto the majority of those who adhere to the orthodox position such a\nproblem does not deserve a particular attention: classical concepts\nare a logical prerequisite for the very formulation of quantum\nmechanics and, consequently, the measurement process itself, the\ndividing line between the quantum and the classical world, cannot and\nmust not be investigated, but simply accepted. This position has been\nlucidly summarized by J. Bell himself: \nMaking a virtue of necessity and influenced by positivistic and\ninstrumentalist philosophies, many came to hold not only that it is\ndifficult to find a coherent picture but that it is wrong to look for\none—if not actually immoral then certainly unprofessional.\n(1981: 45) \nThe situation has seen many changes in the course of time, and the\nnecessity of making a clear distinction between what is quantum and\nwhat is classical has given rise to many proposals for ‘easy\nsolutions’ to the problem which are based on the possibility,\nfor all practical purposes (FAPP), of locating the splitting\nbetween these two faces of reality at different levels. \nThen came Bohmian mechanics, a theory which has made clear, in a lucid\nand perfectly consistent way, that there is no reason of principle to\nrequire a dichotomic description of the world. A universal dynamical\nprinciple runs all physical processes and even though ‘it\ncompletely agrees with standard quantum predictions’, it\naccounts for the standard wave-packet reduction in micro-macro\ninteractions as well as the classical behaviour of macroscopic\nobjects. \nAs we have mentioned, the other consistent proposal, at the\nnonrelativistic level, of a conceptually satisfactory solution of the\nmacro-objectification problem is represented by the Collapse Theories\nwhich are the subject of these pages. Contrary to Bohmian mechanics,\nthey are rival to quantum mechanics, since they make different\npredictions (even though quite difficult to put into evidence)\nconcerning various physical processes. \nA common criticism makes reference to the fact that within any\ncollapse model the ensuing dynamics for the statistical operator can\nbe considered as the reduced dynamics deriving from a unitary (and,\nconsequently, essentially a standard quantum) dynamics for the states\nof an enlarged Hilbert space of a composite quantum system \\(S+E\\)\ninvolving, besides the physical system \\(S\\) of interest, an ancilla\n\\(E\\) whose degrees of freedom are completely unaccessible. Due to the\nquantum dynamical semigroup nature of the evolution equation for the\nstatistical operator, any GRW-like model can always be seen as a\nphenomenological model deriving from a standard quantum evolution on a\nlarger Hilbert space. In this way, the unitary deterministic evolution\ncharacterizing quantum mechanics would be fully restored. \nApart from the obvious remark that such a critical attitude completely\nfails to grasp the most important feature of collapse theories, i.e.,\nof dealing with individual quantum systems and not with statistical\nensembles and of yielding a perfectly satisfactory description,\nmatching our perceptions concerning individual macroscopic\nsystems, invoking an unaccessible ancilla to account for the\nnonlinear and stochastic character of GRW-type theories is once more a\npurely verbal way of avoiding facing the real puzzling aspects of the\nquantum description of macroscopic systems.  \nOther reasons for ignoring the dynamical reduction program have been\nput forward within the quantum information community. We will not\nspend too much time in analyzing and discussing the new position about\nthe foundational issues which have motivated the elaboration of\ncollapse theories. The crucial fact is that, from this perspective,\none takes the theory not to be about something real ‘occurring\nout there’ in a real word, but simply about information. This\npoint is made extremely explicit by Zeilinger (2002: 252): \ninformation is the most basic notion of quantum mechanics, and it is\ninformation about possible measurement results that is represented in\nthe quantum state. Measurement results are nothing more than states of\nthe classical apparatus used by the experimentalist. The quantum\nsystem then is nothing other than the consistently constructed\nreferent of the information represented in the quantum state.\n \nIt is clear that if one takes such a position almost all motivations\nto be worried by the measurement problem disappear, and with them the\nreasons to work out what Bell has denoted as ‘an exact version\nof quantum mechanics’. The most appropriate reply to this type\nof criticisms is to recall that J. Bell (1990) has included\n‘information’ among the words which must have no place in\na formulation with any pretension to physical precision. In particular\nhe has stressed that one cannot even mention information unless one\nhas given a precise answer to the two following questions: Whose\ninformation? and Information about what? \nA much more serious attitude is to call attention, as many serious\nauthors do, to the fact that since collapse theories represent rival\ntheories with respect to standard quantum mechanics they lead to the\nidentification of experimental situations which would allow, in\nprinciple, crucial tests to discriminate between the two. As we have\ndiscussed above, presently, fully discriminating tests are not out of\nreach. \nWe presented a comprehensive picture of the ideas, the implications,\nthe achievements and the problems of the DRP. We conclude by stressing\nonce more our position with respect to Collapse Theories. Their\ninterest derives entirely from the fact that they have given some\nhints about a possible way out from the difficulties characterizing\nstandard quantum mechanics, by proving that explicit and precise\nmodels can be worked out, which agree with all known predictions of\nthe theory and nevertheless allow, on the basis of a universal\ndynamics governing all natural processes, to overcome in a\nmathematically clean and precise way the basic problems of the\nstandard theory. In particular, Collapse Models show how one can work\nout a theory that makes perfectly legitimate to take a macrorealistic\nposition about natural processes, without contradicting any of the\nexperimentally tested predictions of standard quantum mechanics.\nFinally, they might give precise hints about where to look in order to\nput into evidence, experimentally, possible violations of the\nsuperposition principle.","contact.mail":"bassi@ts.infn.it","contact.domain":"ts.infn.it"}]
