[{"date.published":"2009-08-12","date.changed":"2017-10-12","url":"https://plato.stanford.edu/entries/scientific-underdetermination/","author1":"Kyle Stanford","entry":"scientific-underdetermination","body.text":"\n\n\nAt the heart of the underdetermination of scientific theory by\nevidence is the simple idea that the evidence available to us at a\ngiven time may be insufficient to determine what beliefs we should\nhold in response to it. In a textbook example, if all I know is that\nyou spent $10 on apples and oranges and that apples cost $1 while\noranges cost $2, then I know that you did not buy six oranges, but I\ndo not know whether you bought one orange and eight apples, two\noranges and six apples, and so on. A simple scientific example can be\nfound in the rationale behind the sensible methodological adage that\n“correlation does not imply causation”. If watching lots\nof cartoons causes children to be more violent in their playground\nbehavior, then we should (barring complications) expect to find a\ncorrelation between levels of cartoon viewing and violent playground\nbehavior. But that is also what we would expect to find if\nchildren who are prone to violence tend to enjoy and seek out cartoons\nmore than other children, or if propensities to violence and increased\ncartoon viewing are both caused by some third factor (like general\nparental neglect or excessive consumption of Twinkies). So a high\ncorrelation between cartoon viewing and violent playground behavior is\nevidence that (by itself) simply underdetermines what we\nshould believe about the causal relationship between the two. But it\nturns out that this simple and familiar predicament only scratches the\nsurface of the various ways in which problems of underdetermination\ncan arise in the course of scientific investigation.\n\nThe scope of the epistemic challenge arising from underdetermination\nis not limited only to scientific contexts, as is perhaps most readily\nseen in classical skeptical attacks on our knowledge more generally.\nRené Descartes ([1640] 1996) famously sought to doubt any and\nall of his beliefs which could possibly be doubted by supposing that\nthere might be an all-powerful Evil Demon who sought only to deceive\nhim. Descartes’ challenge essentially appeals to a form of\nunderdetermination: he notes that all our sensory experiences would be\njust the same if they were caused by this Evil Demon rather than an\nexternal world of tables and chairs. Likewise, Nelson Goodman’s\n(1955) “New Riddle of Induction” turns on the idea that\nthe evidence we now have could equally well be taken to support\ninductive generalizations quite different from those we usually take\nthem to support, with radically different consequences for the course\nof future\n events.[1]\n Nonetheless, underdetermination has been thought to arise in\nscientific contexts in a variety of distinctive and important ways\nthat do not simply recreate such radically skeptical\npossibilities. \nJohn Stuart Mill articulated a distinctively scientific version of the\nconcern with impressive clarity in A System of Logic, where\nhe writes: \nHowever, the traditional locus classicus for underdetermination in\nscience is the work of Pierre Duhem, a French physicist as well as\nhistorian and philosopher of science who lived at the turn of the\n20th Century. In The Aim and Structure of Physical\nTheory, Duhem formulated various problems of scientific\nunderdetermination in an especially perspicuous and compelling way,\nalthough he himself argued that these problems posed serious\nchallenges only to our efforts to confirm theories in physics. In the\nmiddle of the 20th Century, W. V. O. Quine suggested that\nsuch challenges applied not only to the confirmation of all types of\nscientific theories, but to all knowledge claims whatsoever, and his\nincorporation and further development of these problems as part of a\ngeneral account of human knowledge was one of the most significant\ndevelopments of 20th Century epistemology. But neither\nDuhem nor Quine was careful to systematically distinguish a number of\nfundamentally distinct lines of thinking about underdetermination that\nmay be discerned in their works. Perhaps the most important division\nis between what we might call holist and contrastive forms of\nunderdetermination. Holist underdetermination (Section 2 below) arises\nwhenever our inability to test hypotheses in isolation leaves us\nunderdetermined in our response to a failed prediction or\nsome other piece of disconfirming evidence. That is, because\nhypotheses have empirical implications or consequences only when\nconjoined with other hypotheses and/or background beliefs\nabout the world, a failed prediction or falsified empirical\nconsequence typically leaves open to us the possibility of blaming and\nabandoning one of these background beliefs and/or\n‘auxiliary’ hypotheses rather than the hypothesis we set\nout to test in the first place. But contrastive underdetermination\n(Section 3 below) involves the quite different possibility that for\nany body of evidence confirming a theory, there might well be\nother theories that are also well confirmed by that very same\nbody of evidence. Moreover, claims of underdetermination of either of\nthese two fundamental varieties can vary in strength and character in\nany number of further ways: one might, for example, suggest that the\nchoice between two theories or two ways of revising our beliefs is\ntransiently underdetermined simply by the evidence we happen\nto have at present, or instead permanently\nunderdetermined by all possible evidence. Indeed, the variety\nof forms of underdetermination that have been suggested to confront\nscientific inquiry, and the causes and consequences claimed for these\ndifferent varieties, are sufficiently heterogeneous that attempts to\naddress “the” problem of underdetermination for scientific\ntheories have often engendered considerable confusion and\nargumentation at\n cross-purposes.[2] \nMoreover, such differences in the character and strength of various\nclaims of underdetermination turn out to be crucial for resolving the\nsignificance of the issue. For example, in some recently influential\ndiscussions of science it has become commonplace for scholars in a\nwide variety of academic disciplines to make casual appeal to claims\nof underdetermination (especially of the holist variety) to support\nthe idea that something besides evidence must step in to do\nthe further work of determining beliefs and/or changes of belief in\nscientific contexts: perhaps most prominent among these are adherents\nof the sociology of scientific knowledge (SSK) movement and some\nfeminist science critics who have argued that it is typically the\nsociopolitical interests and/or pursuit of power and influence by\nscientists themselves which play a crucial and even decisive role in\ndetermining which beliefs are actually abandoned or retained in\nresponse to conflicting evidence. As we will see in Section 2.2,\nhowever, Larry Laudan has argued that such claims depend upon simple\nequivocation between the comparatively weak or trivial forms of\nunderdetermination that their partisans have managed to establish and\nthe far stronger forms from which they draw radical conclusions about\nthe limited reach of evidence and rationality in science. In the\nsections that follow we will seek to clearly characterize and\ndistinguish the various forms of both holist and contrastive\nunderdetermination that have been suggested to arise in scientific\ncontexts (noting some important connections between them along the\nway), assess the strength and significance of the heterogeneous\nargumentative considerations offered in support of and against them,\nand consider just which forms of underdetermination pose genuinely\nconsequential challenges for scientific inquiry. \nDuhem’s original case for holist underdetermination is, perhaps\nunsurprisingly, intimately bound up with his arguments for\nconfirmational holism: the claim that theories or hypotheses can only\nbe subjected to empirical testing in groups or collections, never in\nisolation. The idea here is that a single scientific hypothesis does\nnot by itself carry any implications about what we should expect to\nobserve in nature; rather, we can derive empirical consequences from\nan hypothesis only when it is conjoined with many other beliefs and\nhypotheses, including background assumptions about the world, beliefs\nabout how measuring instruments operate, further hypotheses about the\ninteractions between objects in the original hypothesis’ field\nof study and the surrounding environment, etc. For this reason, Duhem\nargues, when an empirical prediction turns out to be falsified, we do\nnot know whether the fault lies with the hypothesis we originally\nsought to test or with one of the many other beliefs and hypotheses\nthat were also needed and used to generate the failed prediction: \nDuhem supports this claim with examples from physical theory,\nincluding one designed to illustrate a celebrated further consequence\nhe draws from it. Holist underdetermination ensures, Duhem argues,\nthat there cannot be any such thing as a “crucial\nexperiment”: a single experiment whose outcome is predicted\ndifferently by two competing theories and which therefore serves to\ndefinitively confirm one and refute the other. For example, in a\nfamous scientific episode intended to resolve the ongoing heated\nbattle between partisans of the theory that light consists of a stream\nof particles moving at extremely high speed (the particle or\n“emission” theory of light) and defenders of the view that\nlight consists instead of waves propagated through a mechanical medium\n(the wave theory), the physicist Foucault designed an apparatus to\ntest the two theories’ competing claims about the speed of\ntransmission of light in different media: the particle theory implied\nthat light would travel faster in water than in air, while the wave\ntheory implied that the reverse was true. Although the outcome of the\nexperiment was taken to show that light travels faster in air than in\n water,[3]\n Duhem argues that this is far from a refutation of the hypothesis of\nemission: \nFrom this and similar examples, Duhem drew the quite general\nconclusion that our response to the experimental or observational\nfalsification of a theory is always underdetermined in this way. When\nthe world does not live up to our theory-grounded expectations, we\nmust give up something, but because no hypothesis is ever\ntested in isolation, no experiment ever tells us precisely which\nbelief it is that we must revise or give up as mistaken: \nThe predicament Duhem here identifies is no mere rainy day puzzle for\nphilosophers of science, but a methodological challenge that\nconstantly arises in the course of scientific practice itself. It is\nsimply not true that for practical purposes and in concrete contexts a\nsingle revision of our beliefs in response to disconfirming evidence\nis always obviously correct, or the most promising, or the only or\neven most sensible avenue to pursue. To cite a classic example, when\nNewton’s celestial mechanics failed to correctly predict the\norbit of Uranus, scientists at the time did not simply abandon the\ntheory but protected it from refutation by instead challenging the\nbackground assumption that the solar system contained only seven\nplanets. This strategy bore fruit, notwithstanding the falsity of\nNewton’s theory: by calculating the location of a hypothetical\neighth planet influencing the orbit of Uranus, the astronomers Adams\nand Leverrier were eventually led to discover Neptune in 1846. But the\nvery same strategy failed when used to try to explain the advance of\nthe perihelion in Mercury’s orbit by postulating the existence\nof “Vulcan”, an additional planet located between Mercury\nand the sun, and this phenomenon would resist satisfactory explanation\nuntil the arrival of Einstein’s theory of general relativity. So\nit seems that Duhem was right to suggest not only that hypotheses must\nbe tested as a group or a collection, but also that it is by no means\na foregone conclusion which member of such a collection should be\nabandoned or revised in response to a failed empirical test or false\nimplication. Indeed, this very example illustrates why Duhem’s\nown rather hopeful appeal to the ‘good sense’ of\nscientists themselves in deciding when a given hypothesis ought to be\nabandoned promises very little if any relief from the general\npredicament of holist underdetermination.  \nAs noted above, Duhem thought that the sort of underdetermination he\nhad described presented a challenge only for theoretical physics, but\nsubsequent thinking in the philosophy of science has tended to the\nopinion that the predicament Duhem described applies to theoretical\ntesting in all fields of scientific inquiry. We cannot, for example,\ntest an hypothesis about the phenotypic effects of a particular gene\nwithout presupposing a host of further beliefs about what genes are,\nhow they work, how we can identify them, what other genes are doing,\nand so on. And in the middle of the 20th Century, W. V. O.\nQuine would incorporate confirmational holism and its associated\nconcerns about underdetermination into an extraordinarily influential\naccount of knowledge in general. As part of his famous (1951) critique\nof the widely accepted distinction between truths that are analytic\n(true by definition, or as a matter of logic or language alone) and\nthose that are synthetic (true in virtue of some contingent fact about\nthe way the world is), Quine argued instead that all of the\nbeliefs we hold at any given time are linked in an interconnected web,\nwhich encounters our sensory experience only at its periphery: \nOne consequence of this general picture of human knowledge is that any\nand all of our beliefs are tested against experience only as a\ncorporate body—or as Quine sometimes puts it, “The unit of\nempirical significance is the whole of science” (1951, p.\n 42).[4]\n A mismatch between what the web as a whole leads us to expect and the\nsensory experiences we actually receive will occasion some\nrevision in our beliefs, but which revision we should make to bring\nthe web as a whole back into conformity with our experiences is\nradically underdetermined by those experiences themselves. If we find\nour belief that there are brick houses on Elm Street to be in conflict\nwith our immediate sense experience, we might revise our beliefs about\nthe houses on Elm Street, but we might equally well modify instead our\nbeliefs about the appearance of brick, or about our present location,\nor innumerable other beliefs constituting the interconnected\nweb—in a pinch we might even decide that our present sensory\nexperiences are simply hallucinations! Quine’s point was not\nthat any of these are particularly likely responses to recalcitrant\nexperiences (indeed, an important part of his account is the\nexplanation of why they are not), but instead that they would serve\nequally well to bring the web of belief as a whole in line with our\nexperience. And if the belief that there are brick houses on Elm\nStreet were sufficiently important to us, Quine insisted, it would be\npossible for us to preserve it “come what may” (in the way\nof empirical evidence), by making sufficiently radical adjustments\nelsewhere in the web of belief. It is in principle open to us, Quine\nargued, to revise even beliefs about logic, mathematics, or the\nmeanings of our terms in response to recalcitrant experience; it might\nseem a tempting solution to certain persistent difficulties in quantum\nmechanics, for example, to reject classical logic’s law of the\nexcluded middle (allowing physical particles to both have and not have\nsome determinate classical physical property like position or momentum\nat a given time). The only test of a belief, Quine argued, is whether\nit fits into a web of connected beliefs that accords well with our\nexperience on the whole. And because this leaves any and all\nbeliefs in that web at least potentially subject to revision on the\nbasis of our ongoing sense experience or empirical evidence, he\ninsisted, there simply are no beliefs that are analytic in the\noriginally supposed sense of immune to revision in light of experience\nor true no matter what the world is like. \nQuine recognized, of course, that many of the logically possible ways\nof revising our beliefs in response to recalcitrant experiences that\nremain open to us strike us as ad hoc, perfectly ridiculous, or worse.\nHe argues (1955) that our actual revisions of the web of belief seek\nto maximize the theoretical “virtues” of simplicity,\nfamiliarity, scope, and fecundity, along with conformity to\nexperience, and elsewhere suggests that we typically seek to resolve\nconflicts between the web of our beliefs and our sensory experiences\nin accordance with a principle of “conservatism”, that is,\nby making the smallest possible number of changes to the least central\nbeliefs we can that will suffice to reconcile the web with experience.\nThat is, Quine recognized that when we encounter recalcitrant\nexperience we are not usually at a loss to decide which of our beliefs\nto revise in response to it, but he claimed that this is simply\nbecause we are strongly disposed as a matter of fundamental psychology\nto prefer whatever revision requires the most minimal mutilation of\nthe existing web of beliefs and/or maximizes virtues that he\nexplicitly recognizes as pragmatic in character. Indeed, it would seem\nthat on Quine’s view the very notion of a belief being more\ncentral or peripheral or in lesser or greater “proximity”\nto sense experience should be cashed out simply as a measure of our\nwillingness to revise it in response to recalcitrant experience. That\nis, it would seem that what it means for one belief to be\nlocated “closer” to the sensory periphery of the web than\nanother is simply that we are more likely to revise the first than the\nsecond if doing so would enable us to bring the web as a whole into\nconformity with otherwise recalcitrant sense experience. Thus, Quine\nsaw the traditional distinction between analytic and synthetic beliefs\nas simply registering the endpoints of a psychological continuum\nordering our beliefs according to the ease and likelihood with which\nwe are prepared to revise them in order to reconcile the web as a\nwhole with our sense experience. \nIt is perhaps unsurprising that such holist underdetermination has\noften been taken to pose a threat to the fundamental rationality of\nthe scientific enterprise. The claim that the empirical evidence alone\nunderdetermines our response to failed predictions or recalcitrant\nexperience might even seem to fairly invite the suggestion\nthat what systematically steps into the breach to do the further work\nof singling out just one or a few candidate responses to disconfirming\nevidence is (even if “pragmatic”) something irrational or\nat least arational in character. Imre Lakatos and Paul Feyerabend each\nsuggested that because of underdetermination, the difference between\nempirically successful and unsuccessful theories or research programs\nis largely a function of the differences in talent, creativity,\nresolve, and resources of those who advocate them. And at least since\nthe influential work of Thomas Kuhn, one important line of thinking\nabout science has held that it is ultimately the social and political\ninterests (in a suitably broad sense) of scientists themselves which\nserve to determine their responses to disconfirming evidence and\ntherefore the further empirical, methodological, and other commitments\nof any given scientist or scientific community. Mary Hesse suggests\nthat Quinean underdetermination showed why certain\n“non-logical” and “extra-empirical”\nconsiderations must play a role in theory choice, and claims that\n“it is only a short step from this philosophy of science to the\nsuggestion that adoption of such criteria, that can be seen to be\ndifferent for different groups and at different periods, should be\nexplicable by social rather than logical factors” (1980, 33).\nAnd perhaps the most prominent modern day inheritors of this line of\nthinking are those scholars in the sociology of scientific knowledge\n(SSK) movement and in feminist science studies who argue that it is\ntypically the career interests, political affiliations, intellectual\nallegiances, gender biases, and/or pursuit of power and influence by\nscientists themselves which play a crucial or even decisive role in\ndetermining precisely which beliefs are abandoned or retained in\nresponse to conflicting evidence. The shared argumentative schema here\nis one on which holist underdetermination ensures that the evidence\nalone cannot do the work of picking out a single response to such\nconflicting evidence, thus something else must step in to do the job,\nand sociologists of scientific knowledge, feminist critics of science,\nand other interest-driven theorists of science each have their favored\nsuggestions close to hand.  \nIn a justly celebrated discussion, Larry Laudan (1990) argues that the\nsignificance of such underdetermination has been greatly exaggerated.\nUnderdetermination actually comes in a wide variety of strengths, he\ninsists, depending on precisely what is being asserted about the\ncharacter, the availability, and (most importantly) the rational\ndefensibility of the various competing hypotheses or ways of\nrevising our beliefs that the evidence supposedly leaves us free to\naccept. Laudan usefully distinguishes a number of different dimensions\nalong which claims of underdetermination vary in strength, and he goes\non to insist that those who attribute dramatic significance to the\nthesis that our scientific theories are underdetermined by the\nevidence invariably defend only the weaker versions of that thesis,\nwhile they go on to draw dire consequences and shocking morals\nregarding the character and status of the scientific enterprise from\nmuch stronger versions. He suggests, for instance, that Quine’s\nfamous claim that any hypothesis can be preserved “come what\nmay” can perhaps be defended simply as a description of what it\nis psychologically possible for human beings to do, but\nLaudan insists that in this form the thesis is simply bereft of\ninteresting or important consequences for epistemology— the\nstudy of knowledge. The strong version of the thesis along\nthis dimension instead asserts that it is always normatively or\nrationally defensible to retain any hypothesis in the light\nof any evidence whatsoever, but this latter, stronger version of the\nclaim, Laudan suggests, is one for which no convincing evidence or\nargument has ever been offered. More generally, he insists, arguments\nfor underdetermination turn on implausibly treating all logically\npossible responses to the evidence as equally justified or rationally\ndefensible. For example, Laudan suggests that we might reasonably hold\nthe resources of deductive logic to be insufficient to single\nout just one acceptable response to disconfirming evidence, but not\nthat deductive logic plus the sorts of ampliative principles of\ngood reasoning typically deployed in scientific contexts are\ninsufficient to do so. Similarly, defenders of underdetermination\nmight assert the nonuniqueness claim that for any given\ntheory or web of beliefs there is at least one alternative that can\nalso be reconciled with the available evidence, or the stronger\negalitarian claim that all of the contraries of any\ngiven theory can be reconciled with the available evidence equally\nwell. And the claim of such “reconciliation” itself\ndisguises a wide range of further alternative possibilities: that our\ntheories can be made logically compatible with any amount of\ndisconfirming evidence (perhaps by the simple expedient of removing\nany claim(s) with which the evidence is in conflict), that any theory\nmay be reformulated or revised so as to entail any piece of\npreviously disconfirming evidence, or so as to explain\npreviously disconfirming evidence, or that any theory can be made to\nbe as well supported empirically by any collection of\nevidence as any other theory. And in all of these respects,\nLaudan claims, partisans have defended only the weaker forms of\nunderdetermination while founding their further claims about and\nconceptions of the scientific enterprise on versions much stronger\nthan those they have managed or even attempted to defend.  \nLaudan is certainly right to distinguish these various versions of\nholist underdetermination, and he is equally right to suggest that\nmany of the thinkers he confronts have derived grand morals concerning\nthe scientific enterprise from much stronger versions of\nunderdetermination than they are able to defend, but the underlying\nsituation is somewhat more complex than he suggests. Laudan’s\noverarching claim is that champions of holist underdetermination show\nonly that a wide variety of responses to disconfirming evidence are\nlogically possible (or even just psychologically possible), rather\nthan that these are all rationally defensible or equally\nwell-supported by the evidence. But his straightforward appeal to\nfurther epistemic resources like ampliative principles of belief\nrevision that are supposed to help narrow the merely logical\npossibilities down to those which are reasonable or rationally\ndefensible is itself problematic, at least as part of any attempt to\nrespond to Quine. This is because on Quine’s holist picture of\nknowledge such further ampliative principles governing legitimate\nbelief revision are, of course, themselves simply part of the\nweb of our beliefs, and are therefore open to revision in response to\nrecalcitrant experience as well—indeed, this is true even for\nthe principles of deductive logic and the (consequent) demand for\nparticular forms of logical consistency between parts of the web\nitself! So while it is true that the ampliative principles we\ncurrently embrace do not leave all logically or even psychologically\npossible responses to the evidence open to us (or leave us free to\npreserve any hypothesis “come what may”), our continued\nadherence to these very principles, rather than being willing\nto revise the web of belief so as to abandon them, is part of the\nphenomenon to which Quine is using underdetermination to draw our\nattention and cannot be taken for granted without begging the\nquestion. Put another way, Quine does not simply ignore the further\nprinciples that function to ensure that we revise the web of belief in\none way rather than others, but it follows from his account that such\nprinciples are themselves part of the web and therefore candidates for\nrevision in our efforts to bring the web of beliefs into conformity\n(by the resulting web’s own lights) with\nsensory experience. This recognition makes clear why it will be\nextremely difficult to say how the shift to an alternative web of\nbelief (with alternative ampliative or even deductive principles of\nbelief revision) should or even can be evaluated for its rational\ndefensibility—each proposed revision will be maximally rational\nby the lights of the principles it itself\n sanctions.[5]\n Of course we can rightly say that many candidate revisions would\nviolate our presently accepted ampliative principles of\nrational belief revision, but the preference we have for those rather\nthan the alternatives is itself a matter of their position in the\nexisting web of belief we have inherited and the role that they\nthemselves play in guiding the revisions we are inclined to make to\nthat web in light of ongoing experience. \nThus, if we accept Quine’s general picture of knowledge, it\nbecomes quite difficult to disentangle normative from descriptive\nissues, or questions about the psychology of human belief revision\nfrom questions about the justifiability or rational defensibility of\nsuch revisions. It is in part for this reason that Quine famously\nsuggests (1969, 82; see also p 75–76) that epistemology itself\n“falls into place as a chapter of psychology and hence of\nnatural science”: the point is not that epistemology should\nsimply be abandoned in favor of psychology, but instead that there is\nultimately no way to draw a meaningful distinction between the two.\n(James Woodward, in comments on an earlier draft of this entry,\npointed out that this makes it all the harder to assess the\nsignificance of Quinean underdetermination in light of Laudan’s\ncomplaint or even know the rules for doing so, but in an important way\nthis difficulty was Quine’s point all along!) Quine’s\nclaim is that “[e]ach man is given a scientific heritage plus a\ncontinuing barrage of sensory stimulation; and the considerations\nwhich guide him in warping his scientific heritage to fit his\ncontinuing sensory promptings are, where rational, pragmatic”\n(1951, 46), but the role of these “pragmatic”\nconsiderations or principles in selecting just one of the many\npossible revisions of the web of belief in response to recalcitrant\nexperience is not to be contrasted with those same principles having a\nrational or epistemic justification. Far from conflicting with or even\nbeing orthogonal to the search for truth and our efforts to render our\nbeliefs maximally responsive to the evidence, Quine insists, revising\nour beliefs in accordance with such pragmatic principles “at\nbottom, is what evidence is” (1955, 251). Whether or not this\nstrongly naturalistic conception of epistemology can ultimately be\ndefended, it is misleading for Laudan to suggest that the thesis of\nunderdetermination becomes trivial or obviously insupportable the\nmoment we inquire into the rational defensibility rather than the mere\nlogical or psychological possibility of alternative revisions to the\nholist’s web of belief.  \nIn fact, there is an important connection between this lacuna in\nLaudan’s famous discussion and the further uses made of the\nthesis of underdetermination by sociologists of scientific knowledge,\nfeminist epistemologists, and other vocal champions of holist\nunderdetermination. When faced with the invocation of further\nampliative standards or principles that supposedly rule out some\nresponses to disconfirmation as irrational or unreasonable, these\nthinkers typically respond by insisting that the embrace of such\nfurther standards or principles (or perhaps their application to\nparticular cases) is itself underdetermined, historically\ncontingent, and/or subject to ongoing social negotiation. For this\nreason, they suggest, such appeals (and their success or failure in\nconvincing the members of a given community) should be explained by\nreference to the same broadly social and political interests that they\nclaim are at the root of theory choice and belief change in science\nmore generally (see, e.g., Shapin and Schaffer, 1982). On both\naccounts, then, our response to recalcitrant evidence or a failed\nprediction is constrained in important ways by preexisting features of\nthe existing web of beliefs, but for Quine the continuing force of\nthese constraints is ultimately imposed by the fundamental principles\nof human psychology (such as our preference for minimal mutilation of\nthe web, or the pragmatic virtues of simplicity, fecundity, etc.),\nwhile for interest-driven theorists of science the continuing force of\nany such constraints is limited only by the ongoing negotiated\nagreement of the communities of scientists who respect them. \nAs this last contrast makes clear, however, recognizing the\nlimitations of Laudan’s critique of Quine and the fact that we\ncannot dismiss holist underdetermination with any straightforward\nappeal to ampliative principles of good reasoning by itself does\nnothing to establish the further positive claims about belief\nrevision advanced by interest-driven theorists of science. Even simply\nconceding that theory choice or belief revision in science is indeed\nunderdetermined by the evidence in just the ways that Duhem and/or\nQuine suggested leaves entirely open whether it is instead the\n(suitably broad) social or political interests of scientists\nthemselves that do the further work of singling out the particular\nbeliefs or responses to falsifying evidence that any particular\nscientist or scientific community will actually adopt or find\ncompelling. Even many of those philosophers of science who are most\nstrongly convinced of the general significance of various forms of\nunderdetermination itself remain deeply skeptical of this latter\nthesis and thoroughly unconvinced by the empirical evidence that has\nbeen offered in support of it (usually in the form of case studies of\nparticular historical episodes in science). \nAlthough it is also a form of underdetermination, what we described in\nSection 1 above as contrastive underdetermination raises fundamentally\ndifferent issues from the holist variety considered in Section 2 (Bonk\n2008 is a book-length treatment of many of these issues). This is\nclearly evident in Duhem’s original writings concerning\nso-called crucial experiments, where he seeks to show that even when\nwe explicitly suspend any concerns about holist underdetermination,\nthe contrastive variety remains an obstacle to our discovery of truth\nin theoretical science: \nContrastive underdetermination is so-called because it questions the\nability of the evidence to confirm any given hypothesis against\nalternatives, and the central focus of discussion in this\nconnection (equally often regarded as “the” problem of\nunderdetermination) concerns the character of the supposed\nalternatives. Of course the two problems are not entirely\ndisconnected, because it is open to us to consider alternative\npossible modifications of the web of beliefs as alternative theories\nor theoretical “systems” between which the empirical\nevidence alone is powerless to decide. But we have already seen that\none need not think of the alternative responses to\nrecalcitrant experience as competing theoretical alternatives to\nappreciate the character of the holist’s challenge, and we will\nsee that one need not embrace any version of holism about confirmation\nto appreciate the quite distinct problem that the available evidence\nmight support more than one theoretical alternative. It is perhaps\nmost useful here to think of holist underdetermination as starting\nfrom a particular theory or body of beliefs and claiming that our\nrevision of those beliefs in response to new evidence may be\nunderdetermined, while contrastive underdetermination instead starts\nfrom a given body of evidence and claims that more than one theory may\nbe well-supported by that very evidence. Part of what has contributed\nto the conflation of these two problems is the holist presuppositions\nof those who originally made them famous. After all, on Quine’s\nview we simply revise the web of belief in response to recalcitrant\nexperience, and so the suggestion that there are multiple possible\nrevisions of the web available in response to any particular\nevidential finding just is the claim that there are in fact\nmany different “theories” (i.e. candidate webs of belief)\nthat are equally well-supported by any given body of\n data.[6]\n But if we give up such extreme holist views of evidence, meaning,\nand/or confirmation, the two problems take on very different\nidentities, with very different considerations in favor of taking them\nseriously, very different consequences, and very different candidate\nsolutions. Notice, for instance, that even if we somehow knew that no\nother hypothesis on a given subject was well-confirmed by a given body\nof data, that would not tell us where to place the blame or which of\nour beliefs to give up if the remaining hypothesis in conjunction with\nothers subsequently resulted in a failed empirical prediction. And as\nDuhem suggests above, even if we supposed that we somehow knew exactly\nwhich of our hypotheses to blame in response to a failed empirical\nprediction, this would not help us to decide whether or not there are\nother hypotheses available that are equally well-confirmed by the data\nwe actually have. \nOne way to see why not is to consider an analogy that champions of\ncontrastive underdetermination have sometimes used to support their\ncase. If we consider any finite group of data points, an elementary\nproof reveals that there are an infinite number of distinct\nmathematical functions describing different curves that will pass\nthrough all of them. As we add further data to our initial set we will\ndefinitively eliminate functions describing curves which no longer\ncapture all of the data points in the new, larger set, but no matter\nhow much data we accumulate, the proof guarantees that there will\nalways be an infinite number of functions remaining that\ndefine curves including all the data points in the new set and which\nwould therefore seem to be equally well supported by the empirical\nevidence. No finite amount of data will ever be able\nto narrow the possibilities down to just a single function or indeed,\nany finite number of candidate functions, from which the distribution\nof data points we have might have been generated. Each new data point\nwe gather eliminates an infinite number of curves that\npreviously fit all the data (so the problem here is not the\nholist’s challenge that we do not know which beliefs to give up\nin response to failed predictions or disconfirming evidence), but also\nleaves an infinite number still in contention. \nOf course, generating and testing fundamental scientific hypotheses is\nrarely if ever a matter of finding curves that fit collections of data\npoints, so nothing follows directly from this mathematical analogy for\nthe significance of contrastive underdetermination in most scientific\ncontexts. But Bas van Fraassen has offered an extremely influential\nline of argument intended to show that such contrastive\nunderdetermination is a serious concern for scientific theorizing more\ngenerally. In The Scientific Image (1980), van Fraassen uses\na now-classic example to illustrate the possibility that even our best\nscientific theories might have empirical equivalents: that\nis, alternative theories making the very same empirical predictions,\nand which therefore cannot be better or worse supported by any\npossible body of evidence. Consider Newton’s cosmology,\nwith its laws of motion and gravitational attraction. As Newton\nhimself realized, van Fraassen points out, exactly the same\npredictions are made by the theory whether we assume that the entire\nuniverse is at rest or assume instead that it is moving with some\nconstant velocity in any given direction: from our position within it,\nwe have no way to detect constant, absolute motion by the universe as\na whole. Thus, van Fraassen argues, we are here faced with empirically\nequivalent scientific theories: Newtonian mechanics and gravitation\nconjoined either with the fundamental assumption that the universe is\nat absolute rest (as Newton himself believed), or with any one of an\ninfinite variety of alternative assumptions about the constant\nvelocity with which the universe is moving in some particular\ndirection. All of these theories make all and only the same empirical\npredictions, so no evidence will ever permit us to decide between them\non empirical\n grounds.[7] \nVan Fraassen is widely (though mistakenly) regarded as holding that\nthe prospect of contrastive underdetermination grounded in such\nempirical equivalents demands that we restrict our epistemic ambitions\nfor the scientific enterprise itself. His constructive empiricism\nholds that the aim of science is not to find true theories, but only\ntheories that are empirically adequate: that is, theories whose claims\nabout observable phenomena are all true. Since the empirical\nadequacy of a theory is not threatened by the existence of another\nthat is empirically equivalent to it, fulfilling this aim has nothing\nto fear from the possibility of such empirical equivalents. In reply,\nmany critics have suggested that van Fraassen gives no reasons for\nrestricting belief to empirical adequacy that could not also be used\nto argue for suspending our belief in the future empirical\nadequacy of our best present theories: of course there could\nbe empirical equivalents to our best theories, but there could also be\ntheories equally well-supported by all the evidence up to the present\nwhich diverge in their predictions about observables in future cases\nnot yet tested. This challenge seems to miss the point of Van\nFraassen’s epistemic voluntarism: his claim is that we should\nbelieve no more but also no less than we need to make sense\nof and take full advantage of our scientific theories, and a\ncommitment to the empirical adequacy of our theories, he suggests, is\nthe least we can get away with in this regard. Of course it is true\nthat we are running some epistemic risk in believing in even the full\nempirical adequacy of our present theories, but the risk is\nconsiderably less than what we assume in believing in their truth, it\nis the minimum we need to take full advantage of the fruits of our\nscientific labors, and, he famously suggests, “it is not an\nepistemic principle that one might as well hang for a sheep as a\nlamb” (1980, 72).  \nIn an influential discussion, Larry Laudan and Jarrett Leplin (1991)\nargue that philosophers of science have invested even the bare\npossibility that our theories might have empirical equivalents with\nfar too much epistemic significance. Notwithstanding the popularity of\nthe presumption that there are empirically equivalent rivals to every\ntheory, they argue, the conjunction of several familiar and relatively\nuncontroversial epistemological theses is sufficient to defeat it.\nBecause the boundaries of what is observable change as we develop new\nexperimental methods and instruments, because auxiliary assumptions\nare always needed to derive empirical consequences from a theory (cf.\nconfirmational holism, above), and because these auxiliary assumptions\nare themselves subject to change over time, Laudan and Leplin conclude\nthat there simply is no guarantee that any two theories judged to be\nempirically equivalent at a given time will remain so as the state of\nour knowledge advances. Accordingly, any judgment of empirical\nequivalence is both defeasible and relativized to a particular state\nof science. So even if two theories are empirically equivalent at a\ngiven time this is no guarantee that they will remain so, and\nthus there is no foundation for a general pessimism about our ability\nto distinguish theories that are empirically equivalent to each other\non empirical grounds. Although they concede that we could have good\nreason to think that particular theories have empirically equivalent\nrivals, this must be established case-by-case rather than by any\ngeneral argument or presumption. \nA fairly standard reply to this line of argument is to suggest that\nwhat Laudan and Leplin really show is that the notion of empirical\nequivalence must be applied to larger collections of beliefs than\nthose traditionally identified as scientific theories—at least\nlarge enough to encompass the auxiliary assumptions needed to derive\nempirical predictions from them. At the extreme, perhaps this means\nthat the notion of empirical equivalents (or at least timeless\nempirical equivalents) cannot be applied to anything less than\n“systems of the world” (i.e. total Quinean webs of\nbelief), but even that is not fatal: what the champion of contrastive\nunderdetermination asserts is that there are empirically equivalent\nsystems of the world that incorporate different theories of\nthe nature of light, or spacetime, or whatever. On the other hand, it\nmight seem that quick examples like van Fraassen’s variants of\nNewtonian cosmology do not serve to make this thesis as\nplausible as the more limited claim of empirical equivalence for\nindividual theories. It seems equally natural, however, to respond to\nLaudan and Leplin simply by conceding the variability in empirical\nequivalence but insisting that this is not enough to undermine the\nproblem. Empirical equivalents create a serious obstacle to belief in\na theory so long as there is some empirical equivalent to\nthat theory at any given time, but it need not be the same one at each\ntime. On this line of thinking, cases like van Fraassen’s\nNewtonian example illustrate how easy it is for theories to admit of\nempirical equivalents at any given time, and thus constitute a reason\nfor thinking that there probably are or will be empirical equivalents\nto any given theory at any particular time we consider it, assuring\nthat whenever the question of belief in a given theory arises, the\nchallenge posed to it by constrastive underdetermination arises as\nwell. \nLaudan and Leplin also suggest, however, that even if the universal\nexistence of empirical equivalents were conceded, this would do much\nless to establish the significance of underdetermination than its\nchampions have supposed, because “theories with exactly the same\nempirical consequences may admit of differing degrees of evidential\nsupport” (1991, 465). A theory may be better supported than an\nempirical equivalent, for instance, because the former but not the\nlatter is derivable from a more general theory whose consequences\ninclude a third, well supported, hypothesis. More generally, the\nbelief-worthiness of an hypothesis depends crucially on how it is\nconnected or related to other things we believe and the evidential\nsupport we have for those other\n beliefs.[8]\n Laudan and Leplin suggest that we have invited the specter of rampant\nunderdetermination only by failing to keep this familiar home truth in\nmind and instead implausibly identifying the evidence bearing on a\ntheory exclusively with the theory’s own entailments or\nempirical consequences (but cf. Tulodziecki 2012). This impoverished\nview of evidential support, they argue, is in turn the legacy of a\nfailed foundationalist and positivistic approach to the philosophy of\nscience which mistakenly assimilates epistemic questions about how to\ndecide whether or not to believe a theory to semantic questions about\nhow to establish a theory’s meaning or truth-conditions.  \nJohn Earman (1993) has argued that this dismissive diagnosis does not\ndo justice to the threat posed by underdetermination. He argues that\nworries about underdetermination are an aspect of the more general\nquestion of the reliability of our inductive methods for determining\nbeliefs, and notes that we cannot decide how serious a problem\nunderdetermination poses without specifying (as Laudan and Leplin do\nnot) the inductive methods we are considering. Earman regards some\nversion of Bayesianism as our most promising form of inductive\nmethodology, and he proceeds to show that challenges to the long-run\nreliability of our Bayesian methods can be motivated by considerations\nof the empirical indistinguishability (in several different and\nprecisely specified senses) of hypotheses stated in any language\nricher than that of the evidence itself that do not amount simply to\ngeneral skepticism about those inductive methods. In other words, he\nshows that there are more reasons to worry about underdetermination\nconcerning inferences to hypotheses about unobservables than to, say,\ninferences about unobserved observables. He also goes on to argue that\nat least two genuine cosmological theories have serious, nonskeptical,\nand nonparasitic empirical equivalents: the first essentially replaces\nthe gravitational field in Newtonian mechanics with curvature in\nspacetime itself,\n [9]\n while the second recognizes that Einstein’s General Theory of\nRelativity permits cosmological models exhibiting different global\ntopological features which cannot be distinguished by any evidence\ninside the light cones of even idealized observers who live\n forever.[10]\n And he suggests that “the production of a few concrete examples\nis enough to generate the worry that only a lack of imagination on our\npart prevents us from seeing comparable examples of underdetermination\nall over the map” (1993, 31) even as he concedes that his case\nleaves open just how far the threat of underdetermination extends\n(1993, 36). \nMost philosophers of science, however, have not embraced the idea that\nit is only lack of imagination which prevents us from finding\nempirical equivalents to our scientific theories generally. They note\nthat the convincing examples of empirical equivalents we do have are\nall drawn from a single domain of highly mathematized scientific\ntheorizing in which the background constraints on serious theoretical\nalternatives are far from clear, and suggest that it is therefore\nreasonable to ask whether even a small handful of such examples should\nmake us believe that there are probably empirical equivalents to most\nof our scientific theories most of the time. They concede that it is\nalways possible that there are empirical equivalents to even\nour best scientific theories concerning any domain of nature, but\ninsist that we should not be willing to suspend belief in any\nparticular theory until some convincing alternative to it can\nactually be produced: as Philip Kitcher puts it, “give us a\nrival explanation, and we’ll consider whether it is sufficiently\nserious to threaten our confidence” (1993, 154; see also Leplin\n1997, Achinstein 2002). That is, these thinkers insist that until we\nare able to actually construct an empirically equivalent\nalternative to a given theory, the bare possibility that such\nequivalents exist is insufficient to justify suspending belief in the\nbest theories we do have. And for this same reason most philosophers\nof science are unwilling to follow van Fraassen into what they regard\nas constructive empiricism’s unwarranted epistemic modesty. Even\nif van Fraassen is right about the most minimal beliefs we must hold\nin order to take full advantage of our scientific theories, most\nthinkers do not see why we should believe the least we can get away\nwith rather than believing the most we are entitled to by the evidence\nwe have. \nPhilosophers of science have responded in a variety of ways to the\nsuggestion that a few or even a small handful of serious examples of\nempirical equivalents does not suffice to establish that there are\nprobably such equivalents to most scientific theories in most domains\nof inquiry. One such reaction has been to invite more careful\nattention to the details of particular examples of putative\nunderdetermination: considerable work has been devoted to assessing\nthe threat of underdetermination in the case of particular scientific\ntheories (for recent examples see Pietsch 2012; Tulodziecki 2013;\nWerndl 2013; Belot 2014; Butterfield 2014; Miyake 2015, and others).\nAnother reaction has been to investigate whether particular kinds of\ntheories or domains of science (e.g. ‘historical’ vs.\n‘experimental’ sciences) are more vulnerable to problems\nof underdetermination than others and, if so, why (see Cleland (2002),\nCarman (2005), Turner (2005, 2007), Stanford (2010), Forber and\nGriffith (2011)). But champions of contrastive underdetermination have\nmost frequently responded by seeking to argue that all\ntheories have empirical equivalents, typically by proposing something\nlike an algorithmic procedure for generating such equivalents from any\ntheory whatsoever. Stanford (2001, 2006) suggests that these efforts\nto prove that all our theories must have empirical\nequivalents fall roughly but reliably into global and local varieties,\nand that neither makes a convincing case for a distinctive scientific\nproblem of contrastive underdetermination. Global algorithms are\nwell-represented by Andre Kukla’s (1996) suggestion that from\nany theory T we can immediately generate such empirical\nequivalents as T′ (the claim that T’s\nobservable consequences are true, but T itself is false),\nT″ (the claim that the world behaves according to\nT when observed, but some specific incompatible alternative\notherwise), and the hypothesis that our experience is being\nmanipulated by powerful beings in such a way as to make it appear that\nT is true. But such possibilities, Stanford argues, amount to\nnothing more than the sort of Evil Deceiver to which Descartes\nappealed in order to doubt any of his beliefs that could possibly be\ndoubted (see Section 1, above). Such radically skeptical scenarios\npose an equally powerful (or powerless) challenge to any knowledge\nclaim whatsoever, no matter how it is arrived at or justified, and\nthus pose no special problem or challenge for beliefs offered\nto us by theoretical science. If global algorithms like Kukla’s\nare the only reasons we can give for taking underdetermination\nseriously in a scientific context, then there is no distinctive\nproblem of the underdetermination of scientific theories by data, only\na salient reminder of the irrefutability of classically Cartesian or\nradical\n skepticism.[11] \nBy contrast to such global strategies for generating empirical\nequivalents, local algorithmic strategies instead begin with some\nparticular scientific theory and proceed to generate alternative\nversions that are equally well supported by all possible evidence.\nThis is what van Fraassen does with the example of Newtonian\ncosmology, showing that an infinite variety of supposed empirical\nequivalents can be produced by ascribing different constant absolute\nvelocities to the universe as a whole. But Stanford suggests that\nempirical equivalents generated in this way are also insufficient to\nshow that there is a distinctive and genuinely troubling form of\nunderdetermination afflicting scientific theories, because they rely\non simply saddling particular scientific theories with further claims\nfor which those theories themselves (together with whatever background\nbeliefs we actually hold) imply that we cannot have any evidence. Such\nempirical equivalents invite the natural response that they force our\ntheories to undertake commitments that they never should have in the\nfirst place. Such claims, it seems, should simply be excised from the\ntheories themselves, leaving over just the claims that sensible\ndefenders would have held were all we were entitled to believe by the\nevidence in any case. In van Fraassen’s Newtonian example, for\ninstance, this could be done simply by undertaking no commitment\nconcerning the absolute velocity and direction (or lack thereof) of\nthe universe as a whole. To put the point another way, if we believe a\ngiven scientific theory when one of the empirical equivalents we could\ngenerate from it by the local algorithmic strategy is correct instead,\nmost of what we originally believed will nonetheless turn out to be\nstraightforwardly true.  \nStanford (2001, 2006) concludes that no convincing general case has\nbeen made for the presumption that there are empirically equivalent\nrivals to all or most scientific theories, or to any theories besides\nthose for which such equivalents can actually be constructed. But he\ngoes on to insist that empirical equivalents are no essential part of\nthe case for a significant problem of constrastive underdetermination.\nOur efforts to confirm scientific theories, he suggests, are no less\nthreatened by what Larry Sklar (1975, 1981) has called\n“transient” underdetermination, that is, theories which\nare not empirically equivalent but are equally (or at least\nreasonably) well confirmed by all the evidence we happen to have in\nhand at the moment, so long as this transient predicament is also\n“recurrent”, that is, so long as we think that there is\n(probably) at least one such (fundamentally distinct) alternative\navailable—and thus the transient predicament\nre-arises—whenever we are faced with a decision about whether to\nbelieve a given theory at a given time. Stanford argues that a\nconvincing case for contrastive underdetermination of this recurrent,\ntransient variety can indeed be made, and that the evidence for it is\navailable in the historical record of scientific inquiry itself. \nStanford concedes that present theories are not transiently\nunderdetermined by the theoretical alternatives we have actually\ndeveloped and considered to date: we think that our own scientific\ntheories are considerably better confirmed by the evidence than any\nrivals we have actually produced. The central question, he argues, is\nwhether we should believe that there are well confirmed alternatives\nto our best scientific theories that are presently\nunconceived by us. And the primary reason we should believe that\nthere are, he claims, is the long history of repeated transient\nunderdetermination by previously unconceived alternatives\nacross the course of scientific inquiry. In the progression from\nAristotelian to Cartesian to Newtonian to contemporary mechanical\ntheories, for instance, the evidence available at the time each\nearlier theory dominated the practice of its day also offered\ncompelling support for each of the later alternatives (unconceived at\nthe time) that would ultimately come to displace it. Stanford’s\n“New Induction” over the history of science claims that\nthis situation is typical; that is, that “we have, throughout\nthe history of scientific inquiry and in virtually every scientific\nfield, repeatedly occupied an epistemic position in which we could\nconceive of only one or a few theories that were well confirmed by the\navailable evidence, while subsequent inquiry would routinely (if not\ninvariably) reveal further, radically distinct alternatives as well\nconfirmed by the previously available evidence as those we were\ninclined to accept on the strength of that evidence” (2006, 19).\nIn other words, Stanford claims that in the past we have repeatedly\nfailed to exhaust the space of fundamentally distinct theoretical\npossibilities that were well confirmed by the existing evidence, and\nthat we have every reason to believe that we are probably also failing\nto exhaust the space of such alternatives that are well confirmed by\nthe evidence we have at present. Much of the rest of his case is taken\nup with discussing historical examples illustrating that earlier\nscientists did not simply ignore or dismiss, but instead genuinely\nfailed to conceive of the serious, fundamentally distinct\ntheoretical possibilities that would ultimately come to displace the\ntheories they defended, only to be displaced in turn by others that\nwere similarly unconceived at the time. He concludes that “the\nhistory of scientific inquiry itself offers a straightforward\nrationale for thinking that there typically are alternatives to our\nbest theories equally well confirmed by the evidence, even when we are\nunable to conceive of them at the time” (2006, 20; for\nreservations and criticisms concerning this line of argument, see\nMagnus 2006, 2010; Godfrey-Smith 2008; Chakravartty 2008; Devitt 2011;\nRuhmkorff 2011; Lyons 2013). Stanford concedes, however, that the\nhistorical record can offer only fallible evidence of a distinctive,\ngeneral problem of contrastive scientific underdetermination, rather\nthan the kind of deductive proof that champions of the case from\nempirical equivalents have typically sought. Thus, claims and\narguments about the various forms that underdetermination may take,\ntheir causes and consequences, and the further significance they hold\nfor the scientific enterprise as a whole continue to evolve in the\nlight of ongoing controversy, and the underdetermination of scientific\ntheory by evidence remains very much a live and unresolved issue in\nthe philosophy of science.","contact.mail":"stanford@uci.edu","contact.domain":"uci.edu"}]
