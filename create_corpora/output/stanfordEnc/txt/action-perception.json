[{"date.published":"2015-07-08","url":"https://plato.stanford.edu/entries/action-perception/","author1":"Robert Briscoe","author1.info":"http://rickgrush.net/","entry":"action-perception","body.text":"\n\nAction is a means of acquiring perceptual information about the\nenvironment. Turning around, for example, alters your spatial\nrelations to surrounding objects and, hence, which of their properties\nyou visually perceive. Moving your hand over an object’s surface\nenables you to feel its shape, temperature, and texture. Sniffing and\nwalking around a room enables you to track down the source of an unpleasant\nsmell. Active or passive movements of the body can also generate\nuseful sources of perceptual information (Gibson 1966, 1979). The\npattern of optic flow in the retinal image produced by forward\nlocomotion, for example, contains information about the direction in\nwhich you are heading, while motion parallax is a “cue”\nused by the visual system to estimate the relative distances of\nobjects in your field of view. In these uncontroversial ways and\nothers, perception is instrumentally dependent on action. According to\nan explanatory framework that Susan Hurley\n(1998) dubs the\n“Input-Output Picture”, the dependence of perception on\naction is purely instrumental:\n\n Movement can alter sensory inputs and so result in\ndifferent perceptions… changes in output are merely a means to\nchanges in input, on which perception depends directly. (1998:\n342) \n\nThe action-based theories of perception, reviewed in this entry,\nchallenge the Input-Output Picture. They maintain that perception can\nalso depend in a noninstrumental or constitutive way on\naction (or, more generally, on capacities for object-directed motor control). This\nposition has taken many different forms in the history of philosophy\nand psychology. Most action-based theories of perception in the last\n300 years, however, have looked to action in order to explain how\nvision, in particular, acquires either all or some of\nits spatial representational content. Accordingly, these are\nthe theories on which we shall focus here.\n\nWe begin in Section 1 by discussing\nGeorge Berkeley’s Towards a New Theory of Vision\n(1709), the historical locus classicus of action-based\ntheories of perception, and one of the most influential texts on\nvision ever written. Berkeley argues that the basic or\n“proper” deliverance of vision is not an arrangement of\nvoluminous objects in three-dimensional space, but rather a\ntwo-dimensional manifold of light and color. We then turn to a\ndiscussion of Lotze, Helmholtz, and the local sign\ndoctrine. The “local signs” were felt cues for the mind to\nknow what sort of spatial content to imbue visual experience with. For\nLotze, these cues were “inflowing” kinaesthetic feelings\nthat result from actually moving the eyes, while, for Helmholtz, they\nwere “outflowing” motor commands sent to move the\neyes.\n\nIn Section 2, we discuss sensorimotor\ncontingency theories, which became prominent in the 20th\ncentury. These views maintain that an ability to predict the sensory\nconsequences of self-initiated actions is necessary for\nperception. Among the motivations for this family of theories is the\nproblem of visual direction constancy—why do objects\nappear to be stationary even though the locations on the retina to\nwhich they reflect light change with every eye movement?—as well\nas experiments on adaptation to optical rearrangement devices (ORDs)\nand sensory substitution.\n\nSection 3 examines two other\nimportant 20th century theories. According to what we shall\ncall the motor component theory, efference copies\ngenerated in the oculomotor system and/or proprioceptive feedback from\neye-movements are used together with incoming sensory inputs to\ndetermine the spatial attributes of perceived\nobjects. Efferent readiness theories, by contrast,\nlook to the particular ways in which perceptual\nstates prepare the observer to move and act in relation to\nthe environment. The modest readiness theory, as we\nshall call it, claims that the way an object’s spatial\nattributes are represented in visual experience can be modulated by\none or another form of covert action planning. The bold\nreadiness theory argues for the stronger claim that\nperception just is covert readiness for action.\n\nIn Section 4, we move to\nthe disposition theory, most influentially\narticulated by Gareth Evans (1982, 1985), but more recently defended\nby Rick Grush (2000, 2007). Evans’ theory is, at its core, very\nsimilar to the bold efferent readiness theory. There are some notable\ndifferences, though. Evans’ account is more finely articulated\nin some philosophical respects. It also does not posit a reduction of\nperception to behavioral dispositions, but rather posits that certain\ncomplicated relations between perceptual input and behavioral provide\nspatial content. Grush proposes a very specific theory that is like\nEvans’ in that it does not posit a reduction, but unlike\nEvans’ view, does not put behavioral dispositions and sensory\ninput on an undifferentiated footing.\n\nTwo doctrines dominate philosophical and psychological discussions\nof the relationship between action and space perception from the\n18th to the early 20th century. The first\nis that the immediate objects of sight are two-dimensional manifolds\nof light and color, lacking perceptible extension in\ndepth. The second is that vision must be\n“educated” by the sense of touch—understood as\nincluding both kinaesthesis and proprioceptive position sense—if\nthe former is to acquire its apparent outward, three-dimensional\nspatial significance. The relevant learning process is associationist:\nnormal vision results when tangible ideas of distance (derived from\nexperiences of unimpeded movement) and solid shape (derived from\nexperiences of contact and differential resistance) are elicited by\nthe visible ideas of light and color with which they have been\nhabitually associated. The widespread acceptance of both doctrines\nowes much to the influence of George Berkeley’s New Theory\nof Vision (1709). The Berkeleyan approach looks to action in order to explain how\ndepth is “added” to a phenomenally two-dimensional visual\nfield. The spatial ordering of the visual field itself, however, is\ntaken to be immediately given in experience (Hatfield & Epstein\n1979; Falkenstein 1994; but see Grush 2007). Starting in the\n19th century, a number of theorists, including Johann\nSteinbuch (1770–1818), Hermann Lotze (1817–1881), Hermann\nvon Helmholtz (1821–1894), Wilhelm Wundt (1832–1920), and\nErnst Mach (1838–1916), argued that all abilities for\nvisual spatial localization, including representation of up/down and\nleft/right direction within the two-dimensional visual field, depend\non motor factors, in particular, gaze-directing movements of the eye\n(Hatfield 1990: chaps. 4–5). This idea is the basis of the\n“local sign” doctrine, which we examine\nin Section 2.3. There are three principal respects in which motor action is central\nto Berkeley’s project in the New Theory of Vision\n(1709). First, Berkeley argues that visual experiences convey\ninformation about three-dimensional space only to the extent that they\nenable perceivers to anticipate the tactile consequences of actions\ndirected at surrounding objects. In §45 of the New\nTheory, Berkeley writes:  …I say, neither distance, nor things placed at\na distance are themselves, or their ideas, truly perceived by\nsight…. whoever will look narrowly into his own thoughts, and\nexamine what he means by saying, he sees this or that thing at a\ndistance, will agree with me, that what he sees only suggests to his\nunderstanding, that after having passed a certain distance, to be\nmeasured by the motion of his body, which is perceivable by touch, he\nshall come to perceive such and such tangible ideas which have been\nusually connected with such and such visible ideas.  And later in the Treatise Concerning the Principles of Human\nKnowledge (1734: §44):  …in strict truth the ideas of sight, when we\napprehend by them distance and things placed at a distance, do not\nsuggest or mark out to us things actually existing at a distance, but\nonly admonish us what ideas of touch will be imprinted in our minds at\nsuch and such distances of time, and in consequence of such or such\nactions. …[V]isible ideas are the language whereby the\ngoverning spirit … informs us what tangible ideas he is about\nto imprint upon us, in case we excite this or that motion in our own\nbodies. The view Berkeley defends in these passages has recognizable\nantecedents in Locke’s Essay Concerning Human\nUnderstanding (1690: Book II, Chap. 9,\n§§8–10). There Locke maintained that the immediate\nobjects of sight are “flat” or lack outward depth; that\nsight must be coordinated with touch in order to mediate judgments\nconcerning the disposition of objects in three-dimensional space; and\nthat visible ideas “excite” in the mind movement-based\nideas of distance through an associative process akin to that whereby\nwords suggest their meanings: the process is   performed so constantly, and so quick, that we take\nthat for the perception of our sensation, which is an idea\nformed by our judgment.  \nA long line of philosophers—including Condillac (1754), Reid\n(1785), Smith (1811), Mill (1842, 1843), Bain (1855, 1868), and Dewey\n(1891)—accepted this view of the relation between sight and\ntouch. The second respect in which action plays a prominent role in\nthe New Theory is teleological. Sight not only derives its\nthree-dimensional spatial significance from bodily movement, its\npurpose is to help us engage in such movement adaptively:  …the proper objects of vision constitute an\nuniversal language of the Author of nature, whereby we are instructed\nhow to regulate our actions, in order to attain those things, that are\nnecessary to the preservation and well-being of our bodies, as also to\navoid whatever may be hurtful and destructive of them. It is by their\ninformation that we are principally guided in all the transactions and\nconcerns of life. (1709: §147) Although Berkeley does not explain how vision instructs us\nin regulating our actions, the answer is reasonably clear from the\npreceding account of depth perception: seeing an object or scene can\nelicit tangible ideas that directly motivate self-preserving\naction. The tactual ideas associated with a rapidly looming ball in\nthe visual field, for example, can directly motivate the subject to\nshift position defensively or to catch it before being struck.  The third respect in which action is central to the New\nTheory is psychological. Tangible ideas of distance are elicited\nnot only by (1) visual or “pictorial” depth cues such as\nobject’s degree of blurriness (objects appear increasingly\n“confused” as they approach the observer), but also by\nkinaesthetic, muscular sensations resulting from (2) changes in the\nvergence angle of the eyes (1709: §16) and (3) accommodation of\nthe lens (1709: §27). Like many contemporary theories of spatial\nvision, the Berkeleyan account thus acknowledges an important role for\noculomotor factors in our perception of distance. Critics of Berkeley’s theory in the 18th and\n19th centuries (for reviews, see Bain 1868; Smith 2000;\nAtherton 2005) principally targeted three claims: Most philosophers and perceptual psychologists now concur with\nArmstrong’s (1960) assessment that the “single\npoint” argument for claim (a)—“distance being a line\ndirected end-wise to the eye, it projects only one point in the fund\nof the eye, which point remains invariably the same, whether the\ndistance be longer or shorter” (Berkeley 1709: §2)—conflates\nspatial properties of the retinal image with those of the objects of\nsight (also see Condillac 1746/2001: 102; Abbott 1864: chap. 1). In\ncontrast with claim (a), we should note, both contemporary\n“ecological” and information-processing approaches in\nvision science assume that the spatial representational contents of\nvisual experience are robustly three-dimensional: vision is no less a\ndistance sense than touch. Three sorts of objections targeted on claim (b) were\nprominent. First, it is not evident to introspection that visual\nexperiences reliably elicit tactile and kinaesthetic images as\nBerkeley suggests. As Bain succinctly formulates this objection:   In perceiving distance, we are not conscious of\ntactual feelings or locomotive reminiscences; what we see is a visible\nquality, and nothing more. (1868: 194) Second, sight is often the refractory party when conflicts with\ntouch arise. Consider the experience of seeing a three-dimensional\nscene in a painting: “I know, without any doubt”, writes\nCondillac,  that it is painted on a flat surface; I have touched\nit, and yet this knowledge, repeated experience, and all the judgments\nI can make do not prevent me from seeing convex figures. Why does this\nappearance persist? (1746/2001: I, §6, 3) Last, vision in many animals does not need tutoring by touch before\nit is able to guide spatially directed movement and action. Cases in\nwhich non-human neonates respond adaptively to the distal sources of\nvisual stimulation   imply that external objects are seen to be\nso…. They prove, at least, the possibility that the opening of\nthe eye may be at once followed by the perception of external objects\nas such, or, in other words, by the perception or sensation of\noutness. (Bailey 1842: 30; for replies, see Smith 1811:\n385–390) \nHere it would be in principle possible for a proponent of\nBerkeley’s position to maintain that, at least for such animals,\nthe connection between visual ideas and ideas of touch is innate and\nnot learned (see Stewart 1829: 241–243; Mill 1842:\n106–110). While this would abandon Berkeley’s empiricism\nand associationism, it would maintain the claim that vision provides\ndepth information only because its ideas are connected to tangible\nideas. Regarding claim (c), many critics denied that the supposed\n“habitual connexion” between vision and touch actually\nobtains. Suppose that the novice perceiver sees a remote tree at\ntime1 and walks in its direction until she makes contact\nwith it at time2. The problem is that the perceiver’s\ninitial visual experience of the tree at time1 is not\ntemporally contiguous with the locomotion-based experience of the\ntree’s distance completed at time2. Indeed, at\ntime2 the former experience no longer exists. “The\nassociation required”, Abbott thus writes,   cannot take place, for the simple reason that the\nideas to be associated cannot co-exist. We cannot at one and the same\nmoment be looking at an object five, ten, fifty yards off, and be\nachieving our last step towards it. (1864: 24) Finally, findings from perceptual psychology have more recently\nbeen leveled against the view that vision is educated by\ntouch. Numerous studies of how subjects respond to lens-, mirror-, and\nprism-induced distortions of visual experience (Gibson 1933; Harris\n1965, 1980; Hay et al. 1965; Rock & Harris 1967) indicate\nthat not only is sight resistant to correction from touch, it will\noften dominate or “capture” the latter when intermodal\nconflicts arise. This point will be discussed in greater depth\nin Section 3 below. Like Berkeley, Hermann Lotze (1817–1881) and Hermann von\nHelmholtz (1821–1894) affirm the role played by active movement\nand touch in the genesis of three-dimensional visuospatial\nawareness:  …there can be no possible sense in speaking of\nany other truth of our perceptions other than practical\ntruth. Our perceptions of things cannot be anything other than\nsymbols, naturally given signs for things, which we have learned to\nuse in order to control our motions and actions. When we have learned\nto read those signs in the proper manner, we are in a condition to use\nthem to orient our actions such that they achieve their intended\neffect; that is to say, that new sensations arise in an expected\nmanner (Helmholtz 2005 [1924]: 19, our\nemphasis).  Lotze and Helmholtz go further than Berkeley in maintaining that\nbodily movement also plays a role in the construction of the\ntwo-dimensional visual field, taken for granted by most previous\naccounts of vision (but for exceptions, see Hatfield 1990: ch. 4). The problem of two-dimensional spatial localization, as Lotze and\nHelmholtz understand it, is the problem of assigning a unique,\neye-relative (or “oculocentric”) direction to every point\nin the visual field. Lotze’s commitment to\nmind-body dualism precluded looking to any physical\nor anatomical spatial ordering in the visual system for a solution to\nthis problem (Lotze 1887 [1879]: §§276–77). Rather,\nLotze maintains that every discrete visual impression is attended by a\n“special extra sensation” whose phenomenal character\nvaries as a function of its origin on the retina. Collectively, these\nextra sensations or “local signs” constitute a\n“system of graduated, qualitative tokens” (1887 [1879]:\n§283) that bridge the gap between the spatial structure of the\nnonconscious retinal image and the spatial structure represented in\nconscious visual awareness. What sort of sensation, however, is suited to play the\nindividuating role attributed to a local sign? Lotze appeals to\nkinaesthetic sensations that accompany gaze-directing movements of the eyes (1887 [1879]:\n§§284–86). If P is the location on the retina\nstimulated by a distal point d and F is the fovea,\nthen PF is the arc that must be traversed in order to align\nthe direction of gaze with d. As the eye moves through\narc PF, its changing position gives rise to a corresponding\nseries of kinaesthetic\nsensations p0, p1, p2,\n…pn, and it is this consciously\nexperienced series, unique to P, that\nconstitutes P’s local sign. By contrast, if Q\nwere rather the location on the retina stimulated by d, then\nthe eye’s foveating movement through arc QF would\nelicit a different series of kinaesthetic\nsensations k0, k1, k2,\n…kn unique to Q. Importantly, Lotze allows that retinal stimulation need not\ntrigger an overt movement of the eye. Rather, even in the absence of\nthe corresponding saccade, stimulating point P will elicit\nkinaesthetic sensation p0, and this sensation\nwill, in turn, recall from memory the rest of the series with which it\nis associated p1,\n…pn.   Accordingly, though there is no movement of the eye,\nthere arises the recollection of something, greater or smaller, that\nmust be accomplished if the stimuli at P and Q,\nwhich arouse only a weak sensation, are to arouse sensations of the\nhighest degree of strength and clearness. (1887 [1879]:\n§285) \nIn this way, Lotze accounts for our ability to perceive multiple\nlocations in the visual field at the same time. Helmholtz 2005 [1924]\nfully accepts the need for local signs in two-dimensional spatial\nlocalization, but makes an important modification to Lotze’s\ntheory. In particular, he maintains that local signs are not feelings\nthat originate in the adjustment of the ocular musculature, i.e., a\nform of afferent, sensory “inflow” from the eyes, but\nrather feelings of innervation (Innervationsgefühlen)\nproduced by the effort of the will (Willensanstrengung) to\nmove the eyes, i.e., a form of efferent, motor\n“outflow”. In general, to each perceptible location in the\nvisual field there is an associated readiness or impulse of the will\n(Willensimpuls) to move eyes in the manner required in order\nto fixate it. As Ernst Mach later formulates Helmholtz’s view:\n“The will to perform movements of the eyes, or the innervation\nto the act, is itself the space sensation” (Mach 1897 [1886]:\n59). Helmholtz favored a motor outflow version of the local sign\ndoctrine for two main reasons. First, he was skeptical that afferent\nregistrations of eye position are precise enough to play the role\nassigned to them by Lotze’s theory (2005 [1924]:\n47–49). Recent\nresearch has shown that proprioceptive inflow from ocular muscular\nstretch receptors does in fact play a quantifiable role in estimating\ndirection of gaze, but efferent outflow is normally the more heavily\nweighted source of information (Bridgeman 2010; see\n Section 2.1.1 below). Second, attempting a saccade when the eyes are paralyzed or\notherwise immobilized results in an apparent shift of the visual scene\nin the same direction (Helmholtz 2005 [1924]: 205–06; Mach 1897\n[1886]: 59–60). This finding would make sense if efferent\nsignals to the eye are used to determine the direction of gaze: the\nvisual system “infers” that perceived objects are moving\nbecause they would have to be in order for retinal stimulation to\nremain constant despite the change in eye direction predicted on the\nbasis of motor outflow. Although Helmholtz was primarily concerned to show that “our\njudgments as to the direction of the visual axis are simply the result\nof the effort of will involved in trying to alter the adjustment of\nthe eyes” (2005 [1924]: 205–06), the evidence he adduces also implies that efferent\nsignals play a critical role in our perception of stability in the\nworld across saccadic eye movements. In the next section, we trace the\ninfluence of this idea on theories in the 20th century. Action-based accounts of perception proliferate diversely in\n20th century. In this section, we focus on the reafference\ntheory of Richard Held and the more recent enactive approach of\nJ. Kevin O’Regan and Alva Noë. Central to both accounts is\nthe view that perception and perceptually guided action depend on\nabilities to anticipate the sensory effects of bodily movements. To\nbe a perceiver it is necessary to have knowledge of what O’Regan\nand Noë call the laws of sensorimotor\ncontingency—“the structure of the rules governing the\nsensory changes produced by various motor actions”\n(O’Regan & Noë 2001: 941). We start with two sources of motivation for theories that make\nknowledge of sensorimotor contingencies necessary and/or sufficient\nfor spatially contentful perceptual experience. The first is the idea\nthat the visual system exploits efference copy, i.e., a copy\nof the outflowing saccade command signal, in order to distinguish\nchanges in visual stimulation caused by movement of the eye from those\ncaused by object movement. The second is a long line of experiments,\nfirst performed by Stratton and Helmholtz in the 19th century,\non how subjects adapt to lens-, mirror-, and prism-induced\nmodifications of visual experience. We follow up with objections to\nthese theories and alternatives. The problem of visual direction constancy (VDC) is the problem of\nhow we perceive a stable world despite variations in visual\nstimulation caused by saccadic eye movements. When we execute a\nsaccade, the image of the world projected on the retina rapidly\ndisplaces in the direction of rotation, yet the directions of\nperceived objects appear constant. Such perceptual stability is\ncrucial for ordinary visuomotor interaction with surrounding the\nenvironment. As Bruce Bridgeman writes,  Perceiving a stable visual world establishes the platform on which all other visual\nfunction rests, making possible judgments about the positions and\nmotions of the self and of other objects. (2010: 94) The problem of VDC divides into two questions (MacKay 1973): First,\nwhich sources of information are used to determine whether\nthe observer-relative position of an object has changed between\nfixations? Second, how are relevant sources of\ninformation used by the visual system to achieve this\nfunction? The historically most influential answer to the first question is\nthat the visual system has access to a copy of the efferent or\n“outflowing” saccade command signal. These signals carry\ninformation specifying the direction and magnitude of eye movements\nthat can be used to compensate for or “cancel out”\ncorresponding displacements of the retinal image. In the 19th century, Bell (1823), Purkyně (1825), and\nHering (1861 [1990]), Helmholtz (2005 [1924]), and Mach (1897 [1886])\ndeployed the efference copy theory to illuminate a variety\nof experimental findings, e.g., the tendency in subjects with partially\nparalyzed eye muscles to perceive movement of the visual scene when\nattempting to execute a saccade (for a review, see Bridgeman 2010.)\nThe theory’s most influential formulation, however, came from\nErich von Holst and Horst Mittelstädt in the early\n1950s. According to what they dubbed the “reafference\nprinciple” (von Holst & Mittelstädt 1950; von Holst\n1954), the visual system exploits a copy of motor directives to the\neye in order to distinguish between exafferent visual\nstimulation, caused by changes in the world, and reafferent\nvisual stimulation, caused by changes in the direction of gaze:  Let us imagine an active CNS sending out orders, or\n“commands” … to the effectors and receiving signals\nfrom its sensory organs. Signals that predictably come when nothing\noccurs in the environment are necessarily a result of its own\nactivity, i.e., are reafferences. All signals that come when\nno commands are given are exafferences and signify changes in\nthe environment or in the state of the organism caused by external\nforces. … The difference between that which is to be expected\nas the result of a command and the totality of what is reported by the\nsensory organs is the proportion of exafference…. It is only\nthis difference to which there are compensatory reflexes; only this\ndifference determines, for example during a moving glance at movable\nobjects, the actually perceived direction of visual objects. This,\nthen, is the solution that we propose, which we have termed the\n“reafference principle”: distinction of reafference and\nexafference by a comparison of the total afference with the\nsystem’s state—the\n“command”. (Mittelstädt\n1971; translated by\nBridgeman et al. 1994: 251). It is only when the displacement of the retinal image differs from\nthe displacement predicted on the basis of the efference copy, i.e.,\nwhen the latter fails to “cancel out” the former, that\nsubjects experience a change of some sort in the perceived scene\n(see Figure 1). The relevant upshot is that VDC\nhas an essential motoric component: the apparent stability of an\nobject’s eye-relative position in the world depends on the\nperceiver’s ability to integrate incoming retinal signals with\nextraretinal information concerning the magnitude and direction of\nimpending eye movements. Figure 1: (a) When the eye is stationary, both efference copy (EC) and afference produced by displacement of the retinal image (A) are absent. (b) Turning the eye 10° to the right results in a corresponding shift of the retinal image. Since the magnitude of the eye movement specified by EC and the magnitude of retinal image displacement cancel out, no movement in the world or “exafference” (EA) is registered. The foregoing solution to the problem of VDC faces challenges on\nmultiple, empirical fronts. First, there is evidence\nthat proprioceptive signals from the extraocular muscles make a\nnon-trivial contribution to estimates of eye position, although the\ngain of efference copy is approximately 2.4 times greater (Bridgeman\n& Stark 1991). Second, in the autokinetic\neffect, a fixed luminous dot appears to wander when the field of\nview is dark and thus completely unstructured. This finding is\ninconsistent with theories according to which retinotopic location and\nefference copy are the sole determinants of eye-relative\ndirection. Third, the hypothesized compensation\nprocess, if psychologically real, would be highly inaccurate since\nsubjects fail to notice displacements of the visual world up to 30% of\ntotal saccade magnitude (Bridgeman et al. 1975), and the\nlocations of flashed stimuli are systematically misperceived when\npresented near the time of a saccade (Deubel\n2004). Last, when image displacements concurrent with\na saccade are large, but just below threshold for\ndetection, visually attended objects appear to\n“jump” or “jiggle” against a stable background\n(Brune and Lücking 1969; Bridgeman 1981). Efference copy\ntheories, however, as Bridgeman observes,  do not allow the possibility that parts of the image\ncan move relative to one another—the visual world is conceived\nas a monolithic object. The observation would seem to eliminate all\nefference copy and related theories in a single stroke. (2010:\n102) The reference object theory of Deubel and Bridgeman denies\nthat efference copy is used to “cancel out” displacements\nof the retinal image caused by saccadic eye-movements (Deubel et al.\n2002; Deubel 2004;\nBridgeman 2010). According to\nthis theory, visual attention shifts to the saccade target and a small number\nof other objects in its vicinity (perhaps four or fewer) before eye\nmovement is initiated. Although little visual scene information is\npreserved from one fixation to the next, the features of these objects\nas well as precise information about their presaccadic, eye-relative\nlocations is preserved. After the eye has landed, the visual system\nsearches for the target or one of its neighbors within a limited\nspatial region around the landing site. If the postsaccadic\nlocalization of this “landmark” object succeeds, the world\nappears to be stable. If this object is not found, however,\ndisplacement is perceived. On this approach, efference copy does not\ndirectly support VDC. Rather, the role of efference copy is to\nmaintain an estimate of the direction of gaze, which can be integrated\nwith incoming retinal stimulation to determine the static,\nobserver-relative locations of perceived objects. For a recent,\nphilosophically oriented discussion, see Wu 2014. A related alternative to the von Holst-Mittelstädt model is\nthe spatial remapping theory of Duhamel and Colby\n(Duhamel et al. 1992; Colby et al. 1995). The role\nof saccade efference copy on this theory is to initiate an updating of\nthe eye-relative locations of a small number of attended or otherwise\nsalient objects. When post-saccadic object locations are sufficiently\ncongruent with the updated map, stability is perceived. Single-cell\nand fMRI studies show that neurons at various stages in the\nvisual-processing hierarchy exploit a copy of the saccade command\nsignal in order to shift their receptive field locations in the\ndirection of an impending eye movement microseconds before its\ninitiation (Merriam & Colby 2005; Merriam et al.\n2007). Efference copy indicating an impending saccade 20° to the\nright, in effect, tells relevant neurons:   If you are now firing in response to an\nitem x in your receptive field, then stop firing\nat x. If there is currently an item y in the region\nof oculocentric visual space that would be coincident with your\nreceptive field after a saccade 20° to the right, then start\nfiring at y.  \nSuch putative updating responses are strongest in parietal cortex and\nat higher levels in visual processing (V3A and hV4) and weakest at\nlower levels (V1 and V2). In 1961, Richard Held proposed that the reafference principle could\nbe used to construct a general “neural model” of\nperception and perceptually guided action. Held’s\nreafference theory goes beyond the account of\nvon Holst and Mittelstädt in three main\nways. First, information about movement parameters\nspecified by efference copy is not simply summated with reafferent\nstimulation. Rather, subjects are assumed to acquire knowledge of\nthe specific sensory consequences of different bodily\nmovements. This knowledge is contained in a hypothesized\n“correlational storage” area and used to determine whether\nor not the reafferent stimulations that result from a given type of\naction match those that resulted in the past (Held 1961:\n30). Second, the reafference theory is not\nlimited to eye movements, but extends to “any motor system that\ncan be a source of reafferent visual\nstimulation”. Third, knowledge of the way\nreafferent stimulation depends on self-produced movement is used for\npurposes of sensorimotor control: planning and controlling\nobject-directed actions in the present depends on access to\ninformation concerning the visual consequences of performing such\nactions in the past. The reafference theory was also significantly motivated by studies\nof how subjects adapt to devices that alter the relationship between\nthe distal visual world and sensory input by rotating, reversing, or\nlaterally displacing the retinal image (for helpful guides to the\nliterature on this topic, see Rock 1966; Howard & Templeton 1966;\nEpstein 1967; and Welch 1978). We will refer to these as optical\nrearrangement devices (or ORDs for short). The American psychologist George Stratton conducted two\nexperiments using a lens system that effected an 180º\nrotation of the retinal image in his right eye (his left eye was kept\ncovered). The first experiment involved wearing the device for 21.5\nhours over the course of three days (1896); the second experiment\ninvolved wearing the device for 81.5 hours over the course of 8 days\n(1897a,b). In both cases, Stratton kept a detailed diary of how his\nvisual, imaginative, and proprioceptive experiences underwent\nmodification as a consequence of inverted vision. In 1899, he\nperformed a lesser-known but equally dramatic three-day experiment,\nusing a pair of mirrors that presented his eyes with a view of his own\nbody from a position in space directly above his head\n(Figure 2). Figure 2: The apparatus designed by Stratton (1899). Stratton saw a view of his own body from the perspective of mirror AB, worn above his head. In both experiments, Stratton reported a brief period of initial\nvisual confusion and breakdown in visuomotor skill:   Almost all movements performed under the direct\nguidance of sight were laborious and embarrassed. Inappropriate\nmovements were constantly made; for instance, in order to move my hand\nfrom a place in the visual field to some other place which I had\nselected, the muscular contraction which would have accomplished this\nif the normal visual arrangement had existed, now carried my hand to\nan entirely different place. (1897a: 344) \nFurther bewilderment was caused by a “swinging” of the\nvisual field with head movements as well as jarring discord between\nwhere things were respectively seen and imagined to be:   Objects lying at the moment outside the visual field\n(things at the side of the observer, for example) were at first\nmentally represented as they would have appeared in normal\nvision…. The actual present perception remained in this way\nentirely isolated and out of harmony with the larger whole made up by\n[imaginative] representation. (1896: 615) After a seemingly short period of adjustment, Stratton reported a\ngradual re-establishment of harmony between the deliverances of sight\nand touch. By the end of his experiments on inverted vision, it was\nnot only possible for Stratton to perform many visuomotor actions\nfluently and without error, the visual world often appeared to him to be\n“right side up” (1897a: 358) and “in normal\nposition” (1896: 616). Just what this might mean will\nbe discussed below in Section\n2.2.6. Another influential experiment was performed by Helmholtz (2005\n[1924]: §29), who practiced reaching to targets while wearing\nprisms that displaced the retinal image 16–18° to the\nleft. The initial tendency was to reach too far in the direction of\nlateral displacement. After a number of trials, however, reaching\ngradually regained its former level of accuracy. Helmholtz made two\nadditional discoveries. First, there was an intermanual transfer\neffect: visuomotor adaptation to prisms extended to his\nnon-exposed hand. Second, immediately after removing the prisms from\nhis eyes, errors were made in the opposite direction, i.e., when\nreaching for a target, Helmholtz now moved his hand too far to the\nright. This negative after-effect is now standardly used as a\nmeasure of adaptation to lateral displacement. Stratton and Helmholtz’s findings catalyzed a research\ntradition on ORD adaptation that experienced its heyday in the 1960s\nand 1970s. Two questions dominated studies conducted during this\nperiod. First, what are the necessary and sufficient conditions for\nadaptation to occur? In particular, which sources\nof information do subjects use when adapting to the various\nperceptual and sensorimotor discrepancies caused by ORDs? Second,\njust what happens when subjects adapt to perceptual\nrearrangement? What is the “end product” of the relevant\nform of perceptual learning? Held’s answer to the first question is that subjects must\nreceive visual feedback from active movement, i.e., reafferent\nvisual stimulation, in order for significant and stable\nadaptation to occur (Held & Hein 1958; Held 1961; Held &\nBossom 1961). Evidence for this conclusion came from experiments in\nwhich participants wore laterally displacing prisms during both active\nand passive movement conditions. In the active movement condition, the\nsubject moved her visible hand back and forth along a fixed arc in\nsynchrony with a metronome. In the passive movement condition, the\nsubject’s hand was passively moved at the same rate by the\nexperimenters. Although the overall pattern of visual stimulation was\nidentical in both conditions, adaptation was reported only when\nsubjects engaged in self-movement. Reafferent stimulation, Held\nand Bossom concluded on the basis of this and other studies,  is the source of ordered contact with the environment\nwhich is responsible for both the stability, under typical conditions,\nand the adaptability, to certain atypical conditions, of\nvisual-spatial performance. (1961: 37) Held’s answer to the second question is couched in terms of\nthe reafference theory: subjects adapt to ORDs only when they have\nrelearned the sensory consequences of their bodily movements. In the\ncase of adaptation to lateral displacement, they must relearn the way\nretinal stimulations vary as a function of reaching for targets at\ndifferent body-relative locations. This relearning is assumed to\ninvolve an updating of the mappings from motor output to reafferent\nsensory feedback in the hypothesized \"correlational storage\" module\nmentioned above. The reafference theory faces a number of\nobjections. First, the theory is an extension of von\nHolst and Mittelstädt’s reafference principle, according to\nwhich efference copy is used to cancel out shifts of the retinal image\ncaused by saccadic eye movements. The latter was specifically intended\nto explain why we do not experience object displacement in the world\nwhenever we change the direction of gaze. There is nothing, at first\nblush, however, that is analogous to the putative need for\n“cancellation” or “discounting” of the retinal\nimage in the case of prism adaptation. As Welch puts it, “There\nis no visual position constancy here, so why should a model originally\ndevised to explain this constancy be appropriate?” (1978:\n16). Second, the reafference theory fails to explain\njust how stored efference-reafference correlations are supposed to\nexplain visuomotor control. How does having the ability to anticipate\nthe retinal stimulations that would caused by a certain type of hand\nmovement enable one actually to perform the movement in question?\nWithout elaboration, all that Held’s theory seems to explain is\nwhy subjects are surprised when reafferences generated by their\nmovements are non-standard (Rock 1966: 117). Third, adaptation to ORDs, contrary to the theory,\nis not restricted to situations in which subjects receive reafferent\nvisual feedback, but may also take place when subjects receive\nfeedback generated by passive effector or whole-body movement (Singer\n& Day 1966; Templeton et al. 1966; Fishkin\n1969). Adaptation is even possible in the complete absence of motor\naction (Howard et al. 1965; Kravitz & Wallach\n1966). In general, the extent to which adaptation occurs depends not on\nthe availability of reafferent stimulation, but rather on the presence\nof either of two related kinds of information concerning “the\npresence and nature of the optical rearrangement” (Welch 1978:\n24). Following Welch, we shall refer to this view as the\n“information hypothesis”. One source of information present in a displaced visual array\nconcerns the veridical directions of objects from the observer (Rock\n1966: chaps. 2–4). Normally, when engaging in forward\nlocomotion, the perceived radial direction of an object straight ahead\nof the observer’s body remains constant while the perceived\nradial directions of objects to either side undergo constant\nchange. This pattern also obtains when the observer wears prisms that\ndisplace the retinal image to side. Hence, “an object seen\nthrough prisms which retains the same radial direction as we approach\nmust be seen to be moving in toward the sagittal plane” (Rock\n1966: 105). On Rock’s view, at least some forms of adaptation to\nORDs can be explained by our ability to detect and exploit such\ninvariant sources of spatial informational in locomotion-generated\npatterns of optic flow. Another related source of information for adaptation derives from\nthe conflict between seen and proprioceptively experienced\nlimb position (Wallach 1968; Ularik & Canon 1971). When this\ndiscrepancy is made conspicuous, proponents of the information\nhypothesis have found that passively moved (Melamed et al.\n1973), involuntarily moved (Mather & Lackner 1975), and even\nimmobile subjects (Kravitz & Wallach 1966) exhibit significant\nadaption. Although self-produced bodily movement is not necessary for\nadaptation to occur, it provides subjects with especially salient\ninformation about the discrepancy between sight and touch (Moulden\n1971): subjects are able proprioceptively to determine the location of\na moving limb much more accurately than a stationary or passively\nmoved limb. It is the enhancement of the visual-proprioceptive\nconflict rather than reafferent visual stimulation, on this\ninterpretation, that explains why active movement yields more\nadaptation than passive movement in Held’s experiments. A final objection to the reafference theory\nconcerns the end product of adaptation to ORDs. According to\nthe theory, adaptation occurs when subjects learn new rules of\nsensorimotor dependence that govern how actions affect sensory\ninputs. There is a significant body of evidence, however, that much,\nif not all, adaptation rather occurs at the proprioceptive\nlevel. Stratton, summarizing the results of his experiment on\nmirror-based optical rearrangement, wrote:  …the principle stated in an earlier\npaper—that in the end we would feel a thing to be wherever\nwe constantly saw it—can be justified in a wider sense than\nI then intended it to be taken…. We may now, I think, safely\ninclude differences of distance as well, and assert that the spatial\ncoincidence of touch and sight does not require that an object in a\ngiven tactual position should appear visually in any particular\ndirection or at any particular distance. In whatever place the tactual\nimpression’s visual counterpart regularly appeared, this would\neventually seem the only appropriate place for it to appear in. If we\nwere always to see our bodies a hundred yards away, we would probably\nalso feel them there. (1899: 498, our emphasis) On this interpretation, the plasticity revealed by ORDs is\nprimarily proprioceptive and kinaesthetic, rather than\nvisual. Stratton’s world came to look “right side\nup” (1897b: 469) after adaptation to the inverted retinal image\nbecause things were felt where they were visually perceived\nto be—not because, his “entire visual field flipped\nover” (Kuhn 2012 [1962]: 112). This is clear from the absence of a visual negative\naftereffect when Stratton finally removed his inverting lenses at\nthe end of his eight-day experiment:   The visual arrangement was immediately recognized as\nthe old one of pre-experimental days; yet the reversal of everything\nfrom the order to which I had grown accustomed during the past week,\ngave the scene a surprising, bewildering air which lasted for several\nhours. It was hardly the feeling, though, that things were upside\ndown. (1897b: 470) \nMoreover, Stratton reported changes in kinaesthesis during\nthe course of the experiment consistent with the alleged\nproprioceptive shift:   when one was most at home in the unusual\nexperience the head seemed to be moving in the very opposite\ndirection from that which the motor sensations themselves would\nsuggest. (1907: 156) On this view, the end product of adaptation to an ORD is a\nrecalibration of proprioceptive position sense at one or more points\nof articulation in the body (see the entry on\n bodily awareness). As\nyou practice reaching for a target while wearing laterally displacing\nprisms, for example, the muscle spindles, joint receptors, and Golgi\ntendon organs in your shoulder and arm continue to generate the same\npatterns of action potentials as before, but the proprioceptive and\nkinaesthetic meaning assigned to them by their\n“consumers” in the brain undergoes change: whereas before\nthey signified that your arm was moving along one path through the\nseven-dimensional space of possible arm configurations (the human arm\nhas seven degrees of freedom: three at the wrist, one at the elbow,\nand three at the shoulder), they gradually come to signify that it is\nmoving along a different path in that kinematic space, namely, the one\nconsistent with the prismatically distorted visual feedback you are\nreceiving. Similar recalibrations are possible with respect to sources\nof information for head and eye position. After adapting to laterally\ndisplacing prisms, signals from receptors in your neck that previously\nsignified the alignment of your head and torso, for example, may come\nto signify that your head is turned slightly to the side. For\ndiscussion, see Harris 1965, 1980 and Welch 1978: chap. 3. The enactive approach defended by J. Kevin O’Regan and Alva\nNoë (O’Regan & Noë 2001; Noë 2004, 2005, 2010;\nO’Regan 2011) is best viewed as an extension of the reafference\ntheory. According to the enactive approach, spatially contentful,\nworld-presenting perceptual experience depends on implicit knowledge\nof the way sensory stimulations vary as a function of bodily\nmovement. “Over the course of life”, O’Regan and\nNoë write,   a person will have encountered myriad visual\nattributes and visual stimuli, and each of these will have particular\nsets of sensorimotor contingencies associated with it. Each such set\nwill have been recorded and will be latent, potentially available for\nrecall: the brain thus has mastery of all these sensorimotor\nsets. (2001: 945) \nTo see an object o as having the location and shape\nproperties it has it is necessary (1) to receive sensory stimulations\nfrom o and (2) to use those stimulations in order to retrieve\nthe set of sensorimotor contingencies associated with o on\nthe basis of past encounters. In this sense, seeing is a\n“two-step” process (Noë 2004: 164). It is important\nto emphasize, however, that the enactive approach distances itself\nfrom the idea that vision is functionally dedicated, in whole or in\npart, to the guidance of spatially directed actions:\n“Our claim”, Noë writes,   is that seeing depends on an appreciation of the\nsensory effects of movement (not, as it were, on the practical\nsignificance of sensation)…. Actionism is not committed to the\ngeneral claim that seeing is a matter of knowing how to act in respect\nof or in relation to the things we see. (Noë 2010:\n249) The enactive approach also has strong affinities with\nthe sense-data tradition. According to Noë, an\nobject’s visually apparent shape is the shape of the 2D patch\nthat would occlude the object on a plane perpendicular to the line of\nsight, i.e., the shape of the patch projected by the object on the\nfrontal plane in accordance with the laws of linear\nperspective. Noë calls this the object’s\n“perspectival shape” (P-shape). An object’s visually\napparent size, in turn, is the size of the patch projected by the\nobject on the frontal plane. Noë calls this the object’s\n“perspectival size” (P-size). Appearances are\n“perceptually basic” (Noë 2004: 81) because in order\nto see an object’s actual spatial properties it is necessary\nboth to see its 2D P-properties and to understand how they would vary\n(undergo transformation) with changes in one’s point of\nview. This conception of what it is to perceive objects as voluminous\nspace-occupiers is closely to akin to views defended by Russell\n(1918), Broad (1923), and Price (1950). It also worth mentioning that\nthe enactive approach has strong affinities to views in the\nphenomenological tradition that are beyond the scope of this entry\n(but for discussion, see Thompson 2005; Hickerson 2007; and\nthe entry on phenomenology). Assessment of the enactive approach is complicated by questions\nconcerning the nature of P-properties. First, there is a tendency on the\npart of its main proponents to speak interchangeably of consciously\nperceived P-properties (or ‘looks’), on the one hand, and\nproximal sensory stimulations, on the other. Noë, e.g.,\nwrites:  The sensorimotor profile of an object is the way\nits appearance changes as you move with respect to it\n(strictly speaking, it is the way sensory stimulation varies\nas you move). (2004: 78, our emphasis) It is far from clear how these different characterizations are to be\nrelated, however (Briscoe 2008; Kiverstein 2010). P-properties,\naccording to the enactive approach, are distal, relational properties\nof the objects we see: “If there is a mind/world divide…\nthen P-properties are on the world side of the divide” (2004:\n83). Moreover, Noë clearly assumes that they are visible:\n“P-properties are themselves objects of sight, that is, things\nthat we see” (2004: 83). Sensory stimulations, by contrast, are\nproximal, subpersonal vehicles of visual perception. They are not\nobjects of sight. Quite different, if not incommensurable, notions\nof sensorimotor profile and, so, of sensorimotor\nknowledge would thus seem to be implied by the two\ncharacterizations. There is also an ambiguity with the “-motor” in\n“sensorimotor knowledge”. On the one hand, Noë argues\nthat perception is active in the sense that perceivers require\nknowledge of the proximal, sensory effects of movement. E.g., in order\nto see an object’s shape and size it is necessary to have\ncertain anticipations concerning the way in which retinal stimulations\ncaused by the object would vary as a function of her point of\nview. “This perspectival aspect”, Noë writes,\n“marks the place of action in perception” (Noë 2004:\n34). On this conception there is no commitment to the view that vision\nis for the guidance of action, that vision constitutively has\nsomething to do with adapting animal behavior to the spatial layout of\nthe distal environment (Noë 2004: 18–19). Rather, vision is\nactive in the sense that it involves learned expectations concerning\nthe ways in which sensory stimulations would be\n“perturbed” by possible bodily movements (Noë 2010:\n247–248). On the other hand, Noë adverts to a more world-engaging\nconception of sensorimotor knowledge in order to explain our visual\nexperience of P-properties:   variation in looks reveals how things are. But what of\nthe looks themselves, what of P-properties? Do we see them by\nseeing how they look? This would threaten to lead to infinite\nregress…. (2004: 87) \nThe solution to the regress problem is that seeing an object’s\nP-properties involves a kind of practical know-how. A tilted plate,\ne.g., looks elliptical and small from here because one has to move\none’s hand in a certain way in order to indicate its shape and\nsize in the visual field (2004: 89). Whereas seeing an object’s\nintrinsic properties, according to the enactive approach, requires\nknowledge of the way P-properties would vary as a function of\nmovement, seeing P-properties involves knowing how one would need to\nmove one’s body in relation to what one sees in order to achieve\na certain goal. While this seems to suggest that the first kind of sensorimotor\nknowledge is asymmetrically dependent on the latter, Noë\nmaintains that just the opposite is the case. “I do not wish to\nargue”, he writes,  that to experience something as having a certain\n[P-shape] is to experience it as affording a range of possible\nmovements; rather I want to suggest that one experiences it as having\na certain P-shape, and so as affording possible movements, only\ninsofar as, in encountering it, one is able to draw on one’s\nappreciation of the sensorimotor patterns mediating (or that might be\nmediating) your relation to it. (2004: 90) \nThe problem with this suggestion, however, is that it leads the\nenactive approach directly back to the explanatory regress that the\nsecond, affordance-detecting kind of sensorimotor knowledge was\nintroduced to avoid. The enactive approach rests its case on three main sources of\nempirical support. The first derives from experiments\nwith optical rearrangement devices (ORDs), discussed\nin Section 2.2 above. Hurley and Noë (2003)\nmaintain that adaptation to ORDs only occurs when subjects relearn the\nsystematic patterns of interdependence between active movement and\nreafferent visual stimulation. Moreover, contrary to the\nproprioceptive change theory of Stratton, Harris, and Rock, Hurley and\nNoë argue that the end product of adaptation to inversion and\nreversal of the retinal image is genuinely visual in nature: during\nthe final stage of adaptation, visual experience “rights\nitself”. In Section 2.2 above, we reviewed empirical\nevidence against the view that active movement and corresponding\nreafferent stimulation are necessary for adaptation to\nORDs. Accordingly, we will focus here on Hurley and Noë’s\nobjections to the proprioceptive-change theory. According to the\nlatter, “what is actually modified [by the adaptation process]\nis the interpretation of nonvisual information about positions of body\nparts” (Harris 1980: 113). Once intermodal harmony is restored,\nthe subject will again be able to perform visuomotor actions without\nerror or difficulty, and she will again feel at home in the visually\nperceived world. Hurley and Noë do not contest the numerous sources of empirical\nand introspective evidence that Stratton, Harris, and Rock adduce for\nthe proprioceptive-change theory. Rather they reject the theory on the\nbasis of what they take to be an untoward epistemic implication\nconcerning adaptation to left-right reversal:   while rightward things really look and feel\nleftward to you, they come to seem to look and feel\nrightward. So the true qualities of your experience are no longer\nself-evident to you. (2003: 155) The proprioceptive-change theory, however, does not imply such\nradical introspective error. According to proponents of the theory,\nexperience normalizes after adaptation to reversal not because things\nthat really look leftward “seem to look rightward” (what\nthis might mean is enigmatic at best), but rather because the subjects\neventually become familiar with the way things look when\nreversed—much as ordinary subjects can learn to read\nmirror-reversed writing fluently (Harris 1965: 435–36). Things\nseem “normal” after adaptation, in other words, because\nsubjects are again able to cope with the visually perceived world in a\nfluent and unreflective manner. A second line of evidence for the enactive approach\ncomes from well-known experiments on tactile-visual sensory\nsubstitution (TVSS) devices that transform outputs from a\nlow-resolution video camera into a matrix of vibrotactile stimulation\non the skin of one’s back (Bach-y-Rita 1972, 2004) or\nelectrotactile stimulation on the surface of one’s tongue\n(Sampaio et al. 2001). At first, blind subjects equipped with a TVSS device experience its\noutputs as purely tactile. After a short time, however, many subjects\ncease to notice the tactile stimulations themselves and instead report\nhaving quasi-visual experiences of the objects arrayed in space in\nfront of them. Indeed, with a significant amount of supervised\ntraining, blind subjects can learn to discriminate spatial properties\nsuch as shape, size, and location and even to perform simple\n“eye”-hand coordination tasks such as catching or batting\na ball. A main finding of relevance in early experiments was that\nsubjects learn to “see” by means of TVSS only when they\nhave active control over movement of the video camera. Subjects who\nreceive visual input passively—and therefore lack any knowledge of\nhow (or whether) the camera is moving—experience only\nmeaningless, tactile stimulation. Hurley and Noë argue that passively stimulated subjects do not\nlearn to “see” by means of sensory substitution because\nthey are unable to learn the laws of sensorimotor contingency that\ngovern the prosthetic modality:   active movement is required in order for the subject\nto acquire practical knowledge of the change from sensorimotor\ncontingencies characteristic of touch to those characteristic of\nvision and the ability to exploit this change skillfully. (Hurley\n& Noë 2003: 145) An alternative explanation, however, is that subjects who do not\ncontrol camera movement—and who are not otherwise attuned to how\nthe camera is moving—are simply unable to extract\nany information about the structure of the distal scene from\nthe incoming pattern of sensory stimulations. In consequence they do\nnot engage in “distal attribution” (Epstein et al.\n1986; Loomis 1992; Siegel\n& Warren 2010): they do not perceive through the changing\npattern of proximal stimulation to a spatially external scene in the\nenvironment. For development of this alternative explanation in the\ncontext of Bayesian perceptual psychology, see Briscoe\nforthcoming. A final source of evidence for the enactive\napproach comes from studies of visuomotor development in the absence\nof normal, reafferent visual stimulation. Held & Hein 1963\nperformed an experiment in which pairs of kittens were harnessed to a\ncarousel in a small, cylindrical chamber. One of the kittens was able\nto engage in free circumambulation while wearing a harness. The other\nkitten was suspended in the air in a metal gondola whose motions were\ndriven by the first harnessed kitten. When the first kitten walked,\nboth kittens moved and received identical visual stimulation. However,\nonly the first kitten received reafferent visual feedback as the\nresult of self-movement. Held and Hein reported that\nonly mobile kittens developed normal depth\nperception—as evidenced by their unwillingness to step over the\nedge of a visual cliff, blinking reactions to looming objects, and\nvisually guided paw placing responses. Noë (2004) argues that\nthis experiment supports the enactive approach: in order to develop\nnormal visual depth perception it is necessary to learn how motor\noutputs lead to changes to visual inputs. There are two main reasons to be skeptical of this\nassessment. First, there is evidence that passive\ntransport in the gondola may have disrupted the development of the\nkittens’ innate visual paw placing responses (Ganz 1975:\n206). Second, the fact that passive kittens were\nprepared to walk over the edge of a visual cliff does not show that\ntheir visual experience of depth was abnormal. Rather, as\nJesse Prinz (2006)\nargues, it may only indicate that they “did not have enough\nexperience walking on edges to anticipate the bodily affordances of\nthe visual world”. The enactive approach confronts objections on multiple fronts. We\nfocus on just three of them here (but see Block 2005; Prinz 2006;\nBriscoe 2008; Clark 2009: chap. 8; and Block\n2012). First, the approach is essentially an\nelaboration of Held’s reafference theory and, as such, faces\nmany of the same empirical obstacles. Evidence, for example, that\nactive movement per se is not necessary for perceptual\nadaptation to optical rearrangement\n(Section 2.2.1) is at variance\nwith predictions made by the reafference theory and the enactive\napproach alike. A second line of criticism targets the alleged\nperceptual priority of P-properties. According to the enactive\napproach, P-properties are “perceptually basic” (Noë\n2004: 81) because in order to see an object’s intrinsic, 3D\nspatial properties it is necessary to see its 2D P-properties and to\nunderstand how they would undergo transformation with variation in\none’s point of view. When we view a tilted coin, critics argue,\nhowever, we do not see something that looks—in either\nan epistemic or non-epistemic sense of “looks”—like\nan upright ellipse. Rather, we see what looks like a disk that is\npartly nearer and partly farther away from us. In general, the\napparent shapes of the objects we perceive are not 2D but have\nextension in depth (Austin 1962; Gibson 1979; Smith 2000; Schwitzgebel\n2006; Briscoe 2008; Hopp 2013). Support for this objection comes from work in mainstream vision\nscience. In particular, there is abundant empirical evidence that an\nobject’s 3D shape is specified by sources of spatial information\nin the light reflected or emitted from the object’s surfaces to\nthe perceiver’s eyes as well as by oculomotor factors (for\nreviews, see Cutting & Vishton 1995; Palmer 1999; and Bruce et\nal. 2003). Examples include binocular disparity, vergence,\naccommodation, motion parallax, texture gradients, occlusion, height\nin the visual field, relative angular size, reflections, and\nshading. That such shape-diagnostic information having once been\nprocessed by the visual system is not lost in conscious visual\nexperience of the object is shown by standard psychophysical methods\nin which experimenters manipulate the availability of different\nspatial depth cues and gauge the perceptual effects. Objects, for\nexample, look somewhat flattened under uniform illumination conditions\nthat eliminate shadows and highlights, and egocentric distances are\nunderestimated for objects positioned beyond the operative range of\nbinocular disparity, accommodation, and vergence. Results of such\nexperimentation show that observers can literally see the difference\nmade by the presence or absence of a certain cue in the light\navailable to the eyes (Smith 2000; Briscoe 2008). According to the influential dual systems model (DSM) of visual\nprocessing (Milner & Goodale 1995/2006; Goodale & Milner\n2004), visual consciousess and visuomotor control are supported by\nfunctionally and anatomically distinct visual subsystems (these are\nthe ventral and dorsal information processing\nstreams, respectively). In particular, proponents of the DSM maintain\nthat the contents of visual experience are not used by motor\nprogramming areas in the primate brain: The visual information used by the dorsal stream for\nprogramming and on-line control, according to the model, is\nnot perceptual in nature …[I]t cannot be accessed\nconsciously, even in principle. In other words, although we may be\nconscious of the actions we perform, the visual information used to\nprogram and control those actions can never be experienced. (Milner\n& Goodale 2008: 775–776) A final criticism of the enactive approach is that\nit is empirically falsified by evidence for the DSM (see the\ncommentaries on O’Regan & Noë\n2001; Clark 2009:\nchap. 8; and the essays collected in Gangopadhyay et al.\n2010): the bond it posits between what we see and what we do is much\ntoo tight to comport with what neuroscience has to tells us about\ntheir functional relations. The enactivist can make two points in reply to this\nobjection. First, experimental findings indicate that there are a\nnumber of contexts in which information present in conscious vision is\nutilized for purposes of motor programming (see Briscoe 2009 and\nBriscoe & Schwenkler forthcoming). Action and perception are not as sharply dissociated as\nproponents of DSM sometimes claim. Second, the enactive approach, as emphasized above, rejects the\nidea that the function of vision is to guide actions. It   does not claim that visual awareness depends on\nvisuomotor skill, if by “visuomotor skill” one means the\nability to make use of vision to reach out and manipulate or\ngrasp. Our claim is that seeing depends on an appreciation of the\nsensory effects of movement (not, as it were, on the practical\nsignificance of sensation). (Noë 2010: 249) \nSince the enactive approach is not committed to the idea that seeing\ndepends on knowing how to act in relation to what we see, it is not\nthreatened by empirical evidence for a functional dissociation between\nvisual awareness and visually guided action. At this point, it should be clear that the claim that perception\nis active or action-based is far from\nunambiguous. Perceiving may implicate action in the sense that it is\ntaken constitutively to involve associations with touch (Berkeley\n1709), kinaesthetic feedback from changes in eye position (Lotze 1887\n[1879]), consciously experienced “effort of the will”\n(Helmholtz 2005 [1924]), or\nknowledge of the way reafferent sensory stimulation varies as a\nfunction of movement (Held 1961; O’Regan & Noë \n2001; Hurley &\nNoë 2003). In this section, we shall examine two additional conceptions of the\nrole of action in perception. According to the motor component\ntheory, as we shall call it, efference copies generated in\nthe oculomotor system and/or proprioceptive feedback from\neye-movements are used in tandem with incoming sensory inputs to\ndetermine the spatial attributes of perceived objects (Helmholtz 2005 [1924]; Mack 1979;\nShebilske 1984, 1987; Ebenholtz 2002). Efferent readiness\ntheories, by contrast, appeal to the particular ways in which\nperceptual states prepare the observer to move and act in\nrelation to the environment. The modest readiness\ntheory, as we shall call it, claims that the way an\nobject’s spatial attributes are represented in visual experience\nis sometimes modulated by one or another form of covert action\nplanning (Festinger et al. 1967; Coren 1986; Vishton et\nal. 2007). The bold readiness theory argues for\nthe stronger, constitutive claim that, as J.G. Taylor puts its,\n“perception and multiple simultaneous readiness for action are\none and the same thing” (1968: 432). As pointed out in Section 2.3.2, there\nare numerous, independently variable sources of information about the\nspatial layout of the environment in the light sampled by the eye. In\nmany cases, however, processing of stimulus information requires or is\noptimized by recruiting sources of auxiliary information from outside\nthe visual system. These may be directly integrated with incoming\nvisual information or used to change the weighting assigned to one or\nanother source of optical stimulus information (Shams & Kim 2010;\nErnst 2012). An importantly different recruitment strategy involves combining\nvisual input with non-perceptual information originating in the\nbody’s motor control systems, in particular, efference copy,\nand/or proprioceptive feedback from active movement\n(kinaesthesis). The motor component theory, as we\nshall call it, is premised on evidence for such motor-modal\nprocessing. The motor component theory can be made more concrete by examining\nthree situations in which the spatial contents of visual experience\nare modulated by information concerning recently initiated or\nimpending bodily movements: The motor component theory is a version of the view that perception\nis embodied in the sense of Prinz 2009 (see\nthe entry on embodied cognition).\n Prinz explains that embodied mental capacities, are ones that depend on\nmental representations or processes that relate to the\nbody…. Such representations and processes come in two forms:\nthere are representations and processes that represent or respond to\nbody, such as a perception of bodily movement, and there are\nrepresentations and processes that affect the body, such as motor\ncommands. (2009: 420; for relevant discussion of various senses of\nembodiment, see Alsmith and Vignemount 2012) \nThe three examples presented above provide empirical support for the\nthesis that visual perception is embodied in this sense. For\nadditional examples, see Ebenholtz 2002: chap. 4. Patients with frontal lobe damage sometimes exhibit pathological\n“utilization behaviour” (Lhermitte 1983) in which the\nsight of an object automatically elicits behaviors typically\nassociated with it, such as automatically pouring water into a glass and\ndrinking it whenever a bottle of water and a glass are present\n(Frith et al. 2000: 1782). That normal subjects often do not\nautomatically perform actions afforded by a perceived object, however, does not mean\nthat they do not plan, or imaginatively rehearse, or otherwise\nrepresent them. (On the contrary, recent neuroscientific findings\nsuggest that merely perceiving an object often covertly prepares the motor\nsystem to engage with it in a certain manner. For overviews, see\nJeannerod 2006 and Rizzolatti 2008.) Efferent readiness theories are based on the idea that covert\npreparation for action is “an integral part of the perceptual\nprocess” and not “merely a consequence of the\nperceptual process that has preceded it” (Coren 1986:\n394). According to the modest readiness theory, as we\nshall call it, covert motor preparation can sometimes influence the\nway an object’s spatial attributes are represented in perceptual\nexperience. The bold readiness theory, by contrast,\nargues for the stronger, constitutive claim that to perceive an\nobject’s spatial properties just is to be prepared or\nready to act in relation to the object in certain ways (Sperry\n1952; Taylor 1962, 1965,\n1968). A number of empirical findings motivate the modest readiness\ntheory. Festinger et al. 1967 tested the view that visual\ncontour perception is   determined by the particular sets of preprogrammed\nefferent instructions that are activated by the visual input into a\nstate of readiness for immediate use. (p. 34) \nContact lenses that produce curved retinal input were placed on the\nright eye of three observers, who were instructed to scan a\nhorizontally oriented line with their left eye covered for 40\nminutes. The experimenters reported that there was an average of 44%\nadaptation when the line was physically straight but retinally curved,\nand an average of 18% adaptation when the line was physically curved\nbut retinally straight (see Miller & Festinger 1977, however, for\nconflicting results). An elegantly designed set of experiments by Coren 1986 examined the\nrole of efferent readiness in the visual perception of direction and\nextent. Coren’s experiments support the hypothesis that the\nspatial parameter controlling the length of a saccade is not the\nangular direction of the target relative to the line of sight, but\nrather the direction of the center of gravity (COG) of all the stimuli\nin its vicinity (Coren & Hoenig 1972; Findlay\n1982). Importantly,   the bias arises from the computation of the saccade\nthat would be made and, hence, is held in readiness, rather\nthan the saccade actually emitted. (Coren 1986: 399) The COG bias is illustrated in Figure 3. In\nthe first row (top), there are no extraneous stimuli\nnear the saccade target. Hence, the saccade from the point of fixation to\nthe target is unbiased. In the second row, by\ncontrast, the location of an extraneous stimulus (×) results in\na saccade from the point of fixation that undershoots its target, while in\nthe third row the saccade overshoots its\ntarget. In the fourth row, changing the location of\nthe extraneous stimulus eliminates the COG bias: because the\nextraneous stimulus is near the point of fixation rather than the saccade\ntarget, the saccade is accurate. Figure 3: \nThe effect of starting eye position on saccade programming (after Coren 1986: 405) The COG bias is evolutionarily adaptive: eye movements will bring\nboth the saccade target as well as nearby objects into high acuity\nvision, thereby maximizing the amount of information obtained with\neach saccade. Motor preparation or “efferent readiness” to\nexecute an undershooting or overshooting saccade, Coren found,\nhowever, can also give rise to a corresponding illusion of\nextent (1986: 404–406). Observers, e.g., will perceptually\nunderestimate the length of the distance between the point of fixation\nand the saccade target when there is an extraneous stimulus on the\nnear side of the target (as in the second row\nof Figure 3) and will perceptually overestimate\nthe length of the distance when there is an extraneous stimulus on the\nfar side of the target (as in the third row of Figure\n3). According to Coren, the well known Müller-Lyer illusion can be\nexplained within this framework. The outwardly turned wings in\nMüller-Lyer display shift the COG outward from each vertex, while\nthe inwardly turned wings in this figure shift the COG inward. This\ninfluences both saccade length from vertex to vertex as well as the\napparent length of the central line segments. The influence of COG on\nefferent readiness to execute eye movements, Coren argues (1986:\n400–403), also explains why the line segments in the\nMüller-Lyer display can be replaced with small dots while leaving\nthe illusion intact as well as the effects of varying wing length and\nwing angle on the magnitude of the illusion. The modest readiness theory holds that the way an\nobject’s spatial attributes are represented in visual experience\nis sometimes modulated by one or another form of covert action\nplanning. The bold readiness theory argues for a\nstronger, constitutive claim: to perceive an object’s spatial\nproperties just is to be prepared or ready to act in relation\nto the object in certain ways. We begin by examining\nJ.G. Taylor’s “behavioral theory” of perception\n(Taylor 1962, 1965, 1968). Taylor’s behavioral theory of perception identifies\nthe conscious experience of seeing an object’s spatial\nproperties with the passive activation of a specific set of learned or\n“preprogrammed” motor routines:  [P]erception is a state of multiple simultaneous\nreadiness for actions directed to the objects in the environment that\nare acting on the receptor organs at any one moment. The actions in\nquestion have been acquired by the individual in the course of his\nlife and have been determined by the reinforcing contingencies in the\nenvironment in which he grew up. What determines the content of\nperception is not the properties of the sensory transducers that are\noperated on by stimulus energies from the environment, but the\nproperties of the behaviour conditioned to those stimulus\nenergies…. (1965: 1, our emphasis) According to Taylor’s theory, sensory stimulation gives rises\nto spatially contentful visual experience as a consequence of\nassociative, reinforcement learning: we perceive an object as having\nthe spatial attribute G when the types of proximal sensory\nstimulation caused by the object have been conditioned to the\nperformance of actions sensitive to G (1962: 42). The\nconscious experience of seeing an object’s distance,\ne.g., is constituted by the subject’s learned readiness to\nperform specific whole body and limb movements that were reinforced\nwhen the subject previously received stimulation from objects at the\nsame remove. In general, differences in the spatial content of a\nvisual experience are identified with differences in the\nsubject’s state of “multiple simultaneous readiness”\nto interact with the objects represented in the experience. The main problem with Taylor’s theory is one that besets\nbehaviorist theories of perception in general: it assumes that for any\nvisible spatial property G, there will be some distinctive\nset of behavioral responses that are constitutive of perceiving the\nobject as having G. The problem with this assumption, as\nMohan Matthen (1988) puts it,   there is no such thing as the proper\nresponse, or even a range of functionally appropriate responses, to\nwhat perception tells us. (p. 20, see also Hurley 2001:\n17) The last approach we shall discuss has roots in, and similarities\nto, many of the proposals covered above, but is most closely aligned\nwith the bold readiness theory. We will follow Grush (2007) in calling\nthis approach the disposition theory (see Grush 2007: 394,\nfor discussion of the name). The primary proponent of this position is\nGareth Evans, whose work on spatial representation focused on\nunderstanding how we manage to perceive objects as occupying locations\nin egocentric space. The starting point of Evans’ theory is that the\nsubject’s perceptual systems have isolated a channel of sensory\ninput, an “information link”, through which she receives\ninformation about the object. The information link by itself does not\nallow the subject to know the location of this object. Rather, it is\nwhen the information link is able to induce in the subject appropriate\nkinds of behavioral dispositions that it becomes imbued with spatial\nimport:  The subject hears the sound as coming from\nsuch-and-such a position, but how is the position to be specified?\nPresumably in egocentric terms (he hears the sound as up, or\ndown, to the right or to the left, in front or behind). These terms\nspecify the position of the sound in relation to the observer’s\nown body; and they derive their meaning in part from their complicated\nconnections with the subject’s actions. (Evans 1982:\n155) This is not a version of a motor theory (e.g., Poincaré\n1907: 71). The behavioral responses in question are not to be\nunderstood as raw patterns of motor activations, or even muscular\nsensations. Such a reduction would face challenges anyway, since for\nany location in egocentric space, there are an infinite number of\nkinematic configurations (movements) that would, for example, effect a\ngrasp to that location; and for any kinematic configuration, there are\nan infinite number of dynamic profiles (temporal patterns of muscular\nforce) that would yield that configuration. The behavioral responses\nin question are overt environmental behavior:  It may well be that the input-output connections can\nbe finitely stated only if the output is described in explicitly\nspatial terms (e.g., ‘extending the arm’, ‘walking\nforward two feet’, etc.). If this is so, it would rule out the\nreduction of the egocentric spatial vocabulary to a muscular\nvocabulary. But such a reduction is certainly not needed for the point\nbeing urged here, which is that the spatial information embodied in\nauditory perception is specifiable only in a vocabulary whose terms\nderive their meaning partly from being linked with bodily\nactions. Even given an irreducibility, it would remain the case that\npossession of such information is directly manifestable in behaviour\nissuing from no calculation; it is just that there would be\nindefinitely many ways in which the manifestation can occur. (Evans\n1982: 156) Also, on this proposal, all modalities are in the same boat. As\nsuch the disposition theory is more ambitious than most of the\ntheories already discussed, which are limited to vision. Not only is\nthere no reduction of perceptual spatial content to a “muscular\nvocabulary”, there is also no reduction of the spatial content\nof some perceptual modalities to that of one or more others—as\nthere was for Berkeley, who sought to reduce the spatial content of\nvision to that of touch, and whose program forced a distinction\nbetween two spaces, visual space and tangible space:  The spatial content of auditory and\ntactual-kinaesthetic perceptions must be specified in the same\nterms—egocentric terms. … It is a consequence of this\nthat perceptions from both systems will be used to build up a unitary\npicture of the world. There is only one egocentric space, because\nthere is only one behavioural space. (Evans 1982:\n160) Relatedly, for Evans it is not even the case that spatial\nperceptual content, for all modalities, is being reduced to\nbehavioral dispositions. Rather, perceptual inputs and behavioral\noutputs jointly and holistically yield a single behavioral\nspace:  Egocentric spatial terms are the terms in which the\ncontent of our spatial experiences would be formulated, and those in\nwhich our immediate behavioural plans would be expressed. This duality\nis no coincidence: an egocentric space can exist only for an animal in\nwhich a complex network of connections exists between perceptual input\nand behavioural output. A perceptual input—even if, in some\nloose sense, it encapsulates spatial information (because it belongs\nto a range of inputs which vary systematically with some spatial\nfacts)—cannot have a spatial significance for an organism except\nin so far as it has a place in such a complex network of input-output\nconnections. (Evans 1982: 154)  Egocentric spatial terms and spatial descriptions of bodily\nmovement would, on this view, form a structure familiar to\nphilosophers under the title “holistic”. (Evans 1982: 156,\nfn. 26) This last point and the associated quotes address a common\nmisconception of the disposition theory. It would be easy to read the\ntheory as providing a proposal of the following sort: A creature\ngets sensory information from a stimulus, and the problem is to\ndetermine where that stimulus is located in egocentric space; the\nsolution is that features of that sensory episode induce dispositions\nto behavior targeting some egocentric location. While this sort\nof thing is indeed a problem, it is relatively superficial. Any\ncreature facing this problem must already have the capacity\nto grasp egocentric spatial location contents, and the problem is\nwhich of these ready-at-hand contents it should assign to the\nstimulus. But the disposition theory is addressing a deeper question:\nin virtue of what does this creature have a capacity to grasp\negocentric spatial contents to begin with? The answer is that the\ncreature must have a rich set of interconnections between sensory\ninputs (and their attendant information links) and dispositions for\nbehavioral outputs. Rick Grush (2000, 2007) has adopted Evans’ theory, and\nattempted to clarify and expand upon it, particularly in three areas:\nfirst, the distinction between the disposition theory and other\napproaches; second, the neural implementation of the disposition\ntheory; and finally the specific kinds of dispositions that are\nrelevant for the issue of spatial experience. The theory depends on behavioral dispositions. Grush (2007) argues\nthat there are two distinctions that need to be made: first, the\norganism might possess i) knowledge of what the consequences (bodily,\nenvironmental, or sensory) of a given action will be; or ii) knowledge\nof which motor commands will bring about a given desired end state\n(of the body, environment, or sensory channels) (Grush 2007: 408). I\nmight be able to recognize that a series of moves someone shows me\nwill force my grandmaster opponent into checkmate (knowledge of the\nfirst sort, the consequences of a given set of actions), and yet not\nhave been anywhere near the skill level to have come up with that\nseries of moves on my own (knowledge of the second sort, what actions\nwill achieve a desired effect). Sensorimotor contingency theorists\nappeal to knowledge of the first sort—though as was discussed\nin Section 2.3.1, Noë flirts with\nappealing to knowledge of the second sort to explain the perceptual\ngrasp of P-shapes; to the extent he does, he is embracing a\ndisposition theoretic account of P-shapes. Disposition theorists, and\nbold readiness theorists (Section\n3.2.2) appeal to\nknowledge of the second sort. These are the dispositions of the\ndisposition theory: given some goal, the organism is disposed to\nexecute certain actions. This leads to the second distinction,\nbetween type-specifying and detail-specifying\ndispositions. Grush (2007: 393) maintains that only the latter are\ndirectly relevant for spatial perception. A type-specifying\ndisposition is a disposition to execute some type of behavior\nwith respect to an object or place. For example, an organism might be\ndisposed to grasp, bite, flee, or foveate some object. This sort of\ndisposition is not relevant to the spatial content of the experience\non the disposition theory. Rather, what are relevant are\ndetail-specifying dispositions: the specifics of how I am disposed to\nact to execute any of these behavior types. When reaching to grab the\ncup to take a drink (type), do I move my hand like so (straight ahead,\nsay), or like such (off to the right)? When I want to foveate or\norient towards (behavior type) the ant crawling up the wall, do a I\nmove my head and eyes like this, or like that? This latter distinction allows the disposition theory to answer one\nof the main objections to the bold readiness theory (described at the\nend of section 3.2.2) that there is no single\nspecial disposition connected to perceiving any given object. That is\ntrue of type-specifying dispositions, but not of detail-specifying\ndispositions. Given the ant’s location there is indeed a very\nlimited range of detail specifying dispositions that will allow me to\nfoveate it (though this might require constraints on possible actions,\nsuch as minimum jerk or other such constraints). Grush (2007; 2009) has proposed a detailed implementation of the disposition\ntheory in terms of neural information processing. The proposal\ninvolves more mathematics than is appropriate here, and so a quick\nqualitative description will have to suffice (for more detail, see\nGrush 2007; 2009). The basic idea is that relevant cortical areas learn\nsets of basis functions which, to put it very roughly, encode\nequivalence classes of combinations of sensory and postural signals\n(for discussion, see Pouget et al. 2002). For example, many\ncombinations of eye orientation and location of stimulation on the\nretina correspond to a visual stimulus that is directly in front of\nthe head. Sorting such bodily postural information (not just eye\norientation, but any postural information that affects sensation,\nwhich is most) and sensory condition pairs into useful equivalence\nclasses is the first half of the job. What this does is encode incoming information in a way that renders\nit ready to be of use in guiding behavior, since the equivalence\nclasses are precisely those for which a given kind of motor program is\nappropriate. The next part corresponds to how this information, so\nrepresented, can be used to produce the details of such a motor\nprogram. For every type of action in a creature’s\nbehavioral repertoire (grasp, approach, avoid, foveate, bite, etc.)\nits motor areas have a set of linear coefficients, easily implemented\nas a set of neural connection strengths, and when these are applied to\na set of basis function values, a detailed behavior is specified. For\nexample, when a creature senses an object O1, a\nset of basis function values B1 for that stimulus\nis produced. If the creature decides to execute overt\naction A1, then the B1 basis\nfunction values are multiplied by the coefficient corresponding\nto A1. The result is an instance of behavior\ntype A1 executed with respect to\nobject O1. If the creature had decide instead to\nexecute action A2, with respect\nto O1, the B1 basis function\nvalues would have been multiplied by the A2 set of\ncoefficients, and the result would be a motor behavior\nexecuting A2 on object O1. Accordingly, the disposition theory has a very different account of\nwhat is happening with sensory substitution devices than Susan Hurley and Alva\nNoë (see Section 2.3.2 above). On the\ndisposition theory, what allows the user of such a device to have\nspatial experience is not the ability to anticipate how the sensory\ninput will change upon execution of movement as the sensorimotor\ncontingency theory would have it. Rather, it is that the\nsubject’s brain has learned to take these sensory inputs\ntogether with postural signals to produce sets of basis functions that\npoise the subject to act with respect to the object that is causing\nthe sensory signals (see Grush 2007: 406). One objection to disposition theories is what Hurley has called The Myth of the\nGiving:  To suppose that … the content of intentions\ncan be taken as unproblematically primitive in explaining how the\ncontent of experience is possible, is to succumb to the myth of the\ngiving. (Hurley 1998: 241) The idea behind this objection is that one is simply shifting\nthe debt from one credit card to another when one takes as problematic\nthe spatial content of perception, and then appeals to motor behavior\nas the supplier of this content. For then, of course, the question\nwill be: Whence the spatial content of motor behavior? The disposition theory, however, does not posit any such unilateral\nreduction (though Taylor’s bold readiness theory arguably does,\nsee Section 3.2.2 above). As discussed above,\nEvans explicitly claims that the behavioral space is holistically\ndetermined by both behavior and perception. And on Grush’s\naccount spatial content is implemented in the construction of basis\nfunction values, and these values coordinate transitions from\nperceptual input to behavioral output. As such, they are highly\nanalogous to inferences whose conditions of application are given in\nsensory-plus-postural terms and whose consequences of application\nmanifest in behavioral terms. The import of the states that represent\nthese basis function values is no more narrowly motor than the meaning\nof a conditional can be identified with its consequent (or its\nantecedent, for that matter) in isolation. Another very common objection, one that is often leveled at many\nforms of motor theory, has to do with the fact that even paralyzed\npeople, with very few possibilities for action, seem capable in many\ncases of normal spatial perception. Such objections would, at a minimum,\nplace significant pressure on any views that explain perceptual\ncontent by appeal to actual behavior. It is also easy to see how even\nhypothetical behavior would be called into question in such cases,\nsince in many such cases behavior is not physically\npossible. Grush’s theory (2007), right or wrong, has something\nspecific to say about this objection. Since spatial content is taken\nto be manifested in the production of basis function values in the\ncortex, the prediction is that any impairments manifesting farther\ndown the chain, the brain stem or spinal cord, for example, need have\nno direct effect on spatial content. So long as the relevant brain\nareas have the wherewithal to produce sets of basis function values\nsuitable for constructing a motor sequence (if multiplied by the\naction-type-specific coefficients), then the occasioning perceptual\nepisode will have spatial content.","contact.mail":"rbriscoe@gmail.com","contact.domain":"gmail.com"},{"date.published":"2015-07-08","url":"https://plato.stanford.edu/entries/action-perception/","author1":"Robert Briscoe","author1.info":"http://rickgrush.net/","entry":"action-perception","body.text":"\n\nAction is a means of acquiring perceptual information about the\nenvironment. Turning around, for example, alters your spatial\nrelations to surrounding objects and, hence, which of their properties\nyou visually perceive. Moving your hand over an object’s surface\nenables you to feel its shape, temperature, and texture. Sniffing and\nwalking around a room enables you to track down the source of an unpleasant\nsmell. Active or passive movements of the body can also generate\nuseful sources of perceptual information (Gibson 1966, 1979). The\npattern of optic flow in the retinal image produced by forward\nlocomotion, for example, contains information about the direction in\nwhich you are heading, while motion parallax is a “cue”\nused by the visual system to estimate the relative distances of\nobjects in your field of view. In these uncontroversial ways and\nothers, perception is instrumentally dependent on action. According to\nan explanatory framework that Susan Hurley\n(1998) dubs the\n“Input-Output Picture”, the dependence of perception on\naction is purely instrumental:\n\n Movement can alter sensory inputs and so result in\ndifferent perceptions… changes in output are merely a means to\nchanges in input, on which perception depends directly. (1998:\n342) \n\nThe action-based theories of perception, reviewed in this entry,\nchallenge the Input-Output Picture. They maintain that perception can\nalso depend in a noninstrumental or constitutive way on\naction (or, more generally, on capacities for object-directed motor control). This\nposition has taken many different forms in the history of philosophy\nand psychology. Most action-based theories of perception in the last\n300 years, however, have looked to action in order to explain how\nvision, in particular, acquires either all or some of\nits spatial representational content. Accordingly, these are\nthe theories on which we shall focus here.\n\nWe begin in Section 1 by discussing\nGeorge Berkeley’s Towards a New Theory of Vision\n(1709), the historical locus classicus of action-based\ntheories of perception, and one of the most influential texts on\nvision ever written. Berkeley argues that the basic or\n“proper” deliverance of vision is not an arrangement of\nvoluminous objects in three-dimensional space, but rather a\ntwo-dimensional manifold of light and color. We then turn to a\ndiscussion of Lotze, Helmholtz, and the local sign\ndoctrine. The “local signs” were felt cues for the mind to\nknow what sort of spatial content to imbue visual experience with. For\nLotze, these cues were “inflowing” kinaesthetic feelings\nthat result from actually moving the eyes, while, for Helmholtz, they\nwere “outflowing” motor commands sent to move the\neyes.\n\nIn Section 2, we discuss sensorimotor\ncontingency theories, which became prominent in the 20th\ncentury. These views maintain that an ability to predict the sensory\nconsequences of self-initiated actions is necessary for\nperception. Among the motivations for this family of theories is the\nproblem of visual direction constancy—why do objects\nappear to be stationary even though the locations on the retina to\nwhich they reflect light change with every eye movement?—as well\nas experiments on adaptation to optical rearrangement devices (ORDs)\nand sensory substitution.\n\nSection 3 examines two other\nimportant 20th century theories. According to what we shall\ncall the motor component theory, efference copies\ngenerated in the oculomotor system and/or proprioceptive feedback from\neye-movements are used together with incoming sensory inputs to\ndetermine the spatial attributes of perceived\nobjects. Efferent readiness theories, by contrast,\nlook to the particular ways in which perceptual\nstates prepare the observer to move and act in relation to\nthe environment. The modest readiness theory, as we\nshall call it, claims that the way an object’s spatial\nattributes are represented in visual experience can be modulated by\none or another form of covert action planning. The bold\nreadiness theory argues for the stronger claim that\nperception just is covert readiness for action.\n\nIn Section 4, we move to\nthe disposition theory, most influentially\narticulated by Gareth Evans (1982, 1985), but more recently defended\nby Rick Grush (2000, 2007). Evans’ theory is, at its core, very\nsimilar to the bold efferent readiness theory. There are some notable\ndifferences, though. Evans’ account is more finely articulated\nin some philosophical respects. It also does not posit a reduction of\nperception to behavioral dispositions, but rather posits that certain\ncomplicated relations between perceptual input and behavioral provide\nspatial content. Grush proposes a very specific theory that is like\nEvans’ in that it does not posit a reduction, but unlike\nEvans’ view, does not put behavioral dispositions and sensory\ninput on an undifferentiated footing.\n\nTwo doctrines dominate philosophical and psychological discussions\nof the relationship between action and space perception from the\n18th to the early 20th century. The first\nis that the immediate objects of sight are two-dimensional manifolds\nof light and color, lacking perceptible extension in\ndepth. The second is that vision must be\n“educated” by the sense of touch—understood as\nincluding both kinaesthesis and proprioceptive position sense—if\nthe former is to acquire its apparent outward, three-dimensional\nspatial significance. The relevant learning process is associationist:\nnormal vision results when tangible ideas of distance (derived from\nexperiences of unimpeded movement) and solid shape (derived from\nexperiences of contact and differential resistance) are elicited by\nthe visible ideas of light and color with which they have been\nhabitually associated. The widespread acceptance of both doctrines\nowes much to the influence of George Berkeley’s New Theory\nof Vision (1709). The Berkeleyan approach looks to action in order to explain how\ndepth is “added” to a phenomenally two-dimensional visual\nfield. The spatial ordering of the visual field itself, however, is\ntaken to be immediately given in experience (Hatfield & Epstein\n1979; Falkenstein 1994; but see Grush 2007). Starting in the\n19th century, a number of theorists, including Johann\nSteinbuch (1770–1818), Hermann Lotze (1817–1881), Hermann\nvon Helmholtz (1821–1894), Wilhelm Wundt (1832–1920), and\nErnst Mach (1838–1916), argued that all abilities for\nvisual spatial localization, including representation of up/down and\nleft/right direction within the two-dimensional visual field, depend\non motor factors, in particular, gaze-directing movements of the eye\n(Hatfield 1990: chaps. 4–5). This idea is the basis of the\n“local sign” doctrine, which we examine\nin Section 2.3. There are three principal respects in which motor action is central\nto Berkeley’s project in the New Theory of Vision\n(1709). First, Berkeley argues that visual experiences convey\ninformation about three-dimensional space only to the extent that they\nenable perceivers to anticipate the tactile consequences of actions\ndirected at surrounding objects. In §45 of the New\nTheory, Berkeley writes:  …I say, neither distance, nor things placed at\na distance are themselves, or their ideas, truly perceived by\nsight…. whoever will look narrowly into his own thoughts, and\nexamine what he means by saying, he sees this or that thing at a\ndistance, will agree with me, that what he sees only suggests to his\nunderstanding, that after having passed a certain distance, to be\nmeasured by the motion of his body, which is perceivable by touch, he\nshall come to perceive such and such tangible ideas which have been\nusually connected with such and such visible ideas.  And later in the Treatise Concerning the Principles of Human\nKnowledge (1734: §44):  …in strict truth the ideas of sight, when we\napprehend by them distance and things placed at a distance, do not\nsuggest or mark out to us things actually existing at a distance, but\nonly admonish us what ideas of touch will be imprinted in our minds at\nsuch and such distances of time, and in consequence of such or such\nactions. …[V]isible ideas are the language whereby the\ngoverning spirit … informs us what tangible ideas he is about\nto imprint upon us, in case we excite this or that motion in our own\nbodies. The view Berkeley defends in these passages has recognizable\nantecedents in Locke’s Essay Concerning Human\nUnderstanding (1690: Book II, Chap. 9,\n§§8–10). There Locke maintained that the immediate\nobjects of sight are “flat” or lack outward depth; that\nsight must be coordinated with touch in order to mediate judgments\nconcerning the disposition of objects in three-dimensional space; and\nthat visible ideas “excite” in the mind movement-based\nideas of distance through an associative process akin to that whereby\nwords suggest their meanings: the process is   performed so constantly, and so quick, that we take\nthat for the perception of our sensation, which is an idea\nformed by our judgment.  \nA long line of philosophers—including Condillac (1754), Reid\n(1785), Smith (1811), Mill (1842, 1843), Bain (1855, 1868), and Dewey\n(1891)—accepted this view of the relation between sight and\ntouch. The second respect in which action plays a prominent role in\nthe New Theory is teleological. Sight not only derives its\nthree-dimensional spatial significance from bodily movement, its\npurpose is to help us engage in such movement adaptively:  …the proper objects of vision constitute an\nuniversal language of the Author of nature, whereby we are instructed\nhow to regulate our actions, in order to attain those things, that are\nnecessary to the preservation and well-being of our bodies, as also to\navoid whatever may be hurtful and destructive of them. It is by their\ninformation that we are principally guided in all the transactions and\nconcerns of life. (1709: §147) Although Berkeley does not explain how vision instructs us\nin regulating our actions, the answer is reasonably clear from the\npreceding account of depth perception: seeing an object or scene can\nelicit tangible ideas that directly motivate self-preserving\naction. The tactual ideas associated with a rapidly looming ball in\nthe visual field, for example, can directly motivate the subject to\nshift position defensively or to catch it before being struck.  The third respect in which action is central to the New\nTheory is psychological. Tangible ideas of distance are elicited\nnot only by (1) visual or “pictorial” depth cues such as\nobject’s degree of blurriness (objects appear increasingly\n“confused” as they approach the observer), but also by\nkinaesthetic, muscular sensations resulting from (2) changes in the\nvergence angle of the eyes (1709: §16) and (3) accommodation of\nthe lens (1709: §27). Like many contemporary theories of spatial\nvision, the Berkeleyan account thus acknowledges an important role for\noculomotor factors in our perception of distance. Critics of Berkeley’s theory in the 18th and\n19th centuries (for reviews, see Bain 1868; Smith 2000;\nAtherton 2005) principally targeted three claims: Most philosophers and perceptual psychologists now concur with\nArmstrong’s (1960) assessment that the “single\npoint” argument for claim (a)—“distance being a line\ndirected end-wise to the eye, it projects only one point in the fund\nof the eye, which point remains invariably the same, whether the\ndistance be longer or shorter” (Berkeley 1709: §2)—conflates\nspatial properties of the retinal image with those of the objects of\nsight (also see Condillac 1746/2001: 102; Abbott 1864: chap. 1). In\ncontrast with claim (a), we should note, both contemporary\n“ecological” and information-processing approaches in\nvision science assume that the spatial representational contents of\nvisual experience are robustly three-dimensional: vision is no less a\ndistance sense than touch. Three sorts of objections targeted on claim (b) were\nprominent. First, it is not evident to introspection that visual\nexperiences reliably elicit tactile and kinaesthetic images as\nBerkeley suggests. As Bain succinctly formulates this objection:   In perceiving distance, we are not conscious of\ntactual feelings or locomotive reminiscences; what we see is a visible\nquality, and nothing more. (1868: 194) Second, sight is often the refractory party when conflicts with\ntouch arise. Consider the experience of seeing a three-dimensional\nscene in a painting: “I know, without any doubt”, writes\nCondillac,  that it is painted on a flat surface; I have touched\nit, and yet this knowledge, repeated experience, and all the judgments\nI can make do not prevent me from seeing convex figures. Why does this\nappearance persist? (1746/2001: I, §6, 3) Last, vision in many animals does not need tutoring by touch before\nit is able to guide spatially directed movement and action. Cases in\nwhich non-human neonates respond adaptively to the distal sources of\nvisual stimulation   imply that external objects are seen to be\nso…. They prove, at least, the possibility that the opening of\nthe eye may be at once followed by the perception of external objects\nas such, or, in other words, by the perception or sensation of\noutness. (Bailey 1842: 30; for replies, see Smith 1811:\n385–390) \nHere it would be in principle possible for a proponent of\nBerkeley’s position to maintain that, at least for such animals,\nthe connection between visual ideas and ideas of touch is innate and\nnot learned (see Stewart 1829: 241–243; Mill 1842:\n106–110). While this would abandon Berkeley’s empiricism\nand associationism, it would maintain the claim that vision provides\ndepth information only because its ideas are connected to tangible\nideas. Regarding claim (c), many critics denied that the supposed\n“habitual connexion” between vision and touch actually\nobtains. Suppose that the novice perceiver sees a remote tree at\ntime1 and walks in its direction until she makes contact\nwith it at time2. The problem is that the perceiver’s\ninitial visual experience of the tree at time1 is not\ntemporally contiguous with the locomotion-based experience of the\ntree’s distance completed at time2. Indeed, at\ntime2 the former experience no longer exists. “The\nassociation required”, Abbott thus writes,   cannot take place, for the simple reason that the\nideas to be associated cannot co-exist. We cannot at one and the same\nmoment be looking at an object five, ten, fifty yards off, and be\nachieving our last step towards it. (1864: 24) Finally, findings from perceptual psychology have more recently\nbeen leveled against the view that vision is educated by\ntouch. Numerous studies of how subjects respond to lens-, mirror-, and\nprism-induced distortions of visual experience (Gibson 1933; Harris\n1965, 1980; Hay et al. 1965; Rock & Harris 1967) indicate\nthat not only is sight resistant to correction from touch, it will\noften dominate or “capture” the latter when intermodal\nconflicts arise. This point will be discussed in greater depth\nin Section 3 below. Like Berkeley, Hermann Lotze (1817–1881) and Hermann von\nHelmholtz (1821–1894) affirm the role played by active movement\nand touch in the genesis of three-dimensional visuospatial\nawareness:  …there can be no possible sense in speaking of\nany other truth of our perceptions other than practical\ntruth. Our perceptions of things cannot be anything other than\nsymbols, naturally given signs for things, which we have learned to\nuse in order to control our motions and actions. When we have learned\nto read those signs in the proper manner, we are in a condition to use\nthem to orient our actions such that they achieve their intended\neffect; that is to say, that new sensations arise in an expected\nmanner (Helmholtz 2005 [1924]: 19, our\nemphasis).  Lotze and Helmholtz go further than Berkeley in maintaining that\nbodily movement also plays a role in the construction of the\ntwo-dimensional visual field, taken for granted by most previous\naccounts of vision (but for exceptions, see Hatfield 1990: ch. 4). The problem of two-dimensional spatial localization, as Lotze and\nHelmholtz understand it, is the problem of assigning a unique,\neye-relative (or “oculocentric”) direction to every point\nin the visual field. Lotze’s commitment to\nmind-body dualism precluded looking to any physical\nor anatomical spatial ordering in the visual system for a solution to\nthis problem (Lotze 1887 [1879]: §§276–77). Rather,\nLotze maintains that every discrete visual impression is attended by a\n“special extra sensation” whose phenomenal character\nvaries as a function of its origin on the retina. Collectively, these\nextra sensations or “local signs” constitute a\n“system of graduated, qualitative tokens” (1887 [1879]:\n§283) that bridge the gap between the spatial structure of the\nnonconscious retinal image and the spatial structure represented in\nconscious visual awareness. What sort of sensation, however, is suited to play the\nindividuating role attributed to a local sign? Lotze appeals to\nkinaesthetic sensations that accompany gaze-directing movements of the eyes (1887 [1879]:\n§§284–86). If P is the location on the retina\nstimulated by a distal point d and F is the fovea,\nthen PF is the arc that must be traversed in order to align\nthe direction of gaze with d. As the eye moves through\narc PF, its changing position gives rise to a corresponding\nseries of kinaesthetic\nsensations p0, p1, p2,\n…pn, and it is this consciously\nexperienced series, unique to P, that\nconstitutes P’s local sign. By contrast, if Q\nwere rather the location on the retina stimulated by d, then\nthe eye’s foveating movement through arc QF would\nelicit a different series of kinaesthetic\nsensations k0, k1, k2,\n…kn unique to Q. Importantly, Lotze allows that retinal stimulation need not\ntrigger an overt movement of the eye. Rather, even in the absence of\nthe corresponding saccade, stimulating point P will elicit\nkinaesthetic sensation p0, and this sensation\nwill, in turn, recall from memory the rest of the series with which it\nis associated p1,\n…pn.   Accordingly, though there is no movement of the eye,\nthere arises the recollection of something, greater or smaller, that\nmust be accomplished if the stimuli at P and Q,\nwhich arouse only a weak sensation, are to arouse sensations of the\nhighest degree of strength and clearness. (1887 [1879]:\n§285) \nIn this way, Lotze accounts for our ability to perceive multiple\nlocations in the visual field at the same time. Helmholtz 2005 [1924]\nfully accepts the need for local signs in two-dimensional spatial\nlocalization, but makes an important modification to Lotze’s\ntheory. In particular, he maintains that local signs are not feelings\nthat originate in the adjustment of the ocular musculature, i.e., a\nform of afferent, sensory “inflow” from the eyes, but\nrather feelings of innervation (Innervationsgefühlen)\nproduced by the effort of the will (Willensanstrengung) to\nmove the eyes, i.e., a form of efferent, motor\n“outflow”. In general, to each perceptible location in the\nvisual field there is an associated readiness or impulse of the will\n(Willensimpuls) to move eyes in the manner required in order\nto fixate it. As Ernst Mach later formulates Helmholtz’s view:\n“The will to perform movements of the eyes, or the innervation\nto the act, is itself the space sensation” (Mach 1897 [1886]:\n59). Helmholtz favored a motor outflow version of the local sign\ndoctrine for two main reasons. First, he was skeptical that afferent\nregistrations of eye position are precise enough to play the role\nassigned to them by Lotze’s theory (2005 [1924]:\n47–49). Recent\nresearch has shown that proprioceptive inflow from ocular muscular\nstretch receptors does in fact play a quantifiable role in estimating\ndirection of gaze, but efferent outflow is normally the more heavily\nweighted source of information (Bridgeman 2010; see\n Section 2.1.1 below). Second, attempting a saccade when the eyes are paralyzed or\notherwise immobilized results in an apparent shift of the visual scene\nin the same direction (Helmholtz 2005 [1924]: 205–06; Mach 1897\n[1886]: 59–60). This finding would make sense if efferent\nsignals to the eye are used to determine the direction of gaze: the\nvisual system “infers” that perceived objects are moving\nbecause they would have to be in order for retinal stimulation to\nremain constant despite the change in eye direction predicted on the\nbasis of motor outflow. Although Helmholtz was primarily concerned to show that “our\njudgments as to the direction of the visual axis are simply the result\nof the effort of will involved in trying to alter the adjustment of\nthe eyes” (2005 [1924]: 205–06), the evidence he adduces also implies that efferent\nsignals play a critical role in our perception of stability in the\nworld across saccadic eye movements. In the next section, we trace the\ninfluence of this idea on theories in the 20th century. Action-based accounts of perception proliferate diversely in\n20th century. In this section, we focus on the reafference\ntheory of Richard Held and the more recent enactive approach of\nJ. Kevin O’Regan and Alva Noë. Central to both accounts is\nthe view that perception and perceptually guided action depend on\nabilities to anticipate the sensory effects of bodily movements. To\nbe a perceiver it is necessary to have knowledge of what O’Regan\nand Noë call the laws of sensorimotor\ncontingency—“the structure of the rules governing the\nsensory changes produced by various motor actions”\n(O’Regan & Noë 2001: 941). We start with two sources of motivation for theories that make\nknowledge of sensorimotor contingencies necessary and/or sufficient\nfor spatially contentful perceptual experience. The first is the idea\nthat the visual system exploits efference copy, i.e., a copy\nof the outflowing saccade command signal, in order to distinguish\nchanges in visual stimulation caused by movement of the eye from those\ncaused by object movement. The second is a long line of experiments,\nfirst performed by Stratton and Helmholtz in the 19th century,\non how subjects adapt to lens-, mirror-, and prism-induced\nmodifications of visual experience. We follow up with objections to\nthese theories and alternatives. The problem of visual direction constancy (VDC) is the problem of\nhow we perceive a stable world despite variations in visual\nstimulation caused by saccadic eye movements. When we execute a\nsaccade, the image of the world projected on the retina rapidly\ndisplaces in the direction of rotation, yet the directions of\nperceived objects appear constant. Such perceptual stability is\ncrucial for ordinary visuomotor interaction with surrounding the\nenvironment. As Bruce Bridgeman writes,  Perceiving a stable visual world establishes the platform on which all other visual\nfunction rests, making possible judgments about the positions and\nmotions of the self and of other objects. (2010: 94) The problem of VDC divides into two questions (MacKay 1973): First,\nwhich sources of information are used to determine whether\nthe observer-relative position of an object has changed between\nfixations? Second, how are relevant sources of\ninformation used by the visual system to achieve this\nfunction? The historically most influential answer to the first question is\nthat the visual system has access to a copy of the efferent or\n“outflowing” saccade command signal. These signals carry\ninformation specifying the direction and magnitude of eye movements\nthat can be used to compensate for or “cancel out”\ncorresponding displacements of the retinal image. In the 19th century, Bell (1823), Purkyně (1825), and\nHering (1861 [1990]), Helmholtz (2005 [1924]), and Mach (1897 [1886])\ndeployed the efference copy theory to illuminate a variety\nof experimental findings, e.g., the tendency in subjects with partially\nparalyzed eye muscles to perceive movement of the visual scene when\nattempting to execute a saccade (for a review, see Bridgeman 2010.)\nThe theory’s most influential formulation, however, came from\nErich von Holst and Horst Mittelstädt in the early\n1950s. According to what they dubbed the “reafference\nprinciple” (von Holst & Mittelstädt 1950; von Holst\n1954), the visual system exploits a copy of motor directives to the\neye in order to distinguish between exafferent visual\nstimulation, caused by changes in the world, and reafferent\nvisual stimulation, caused by changes in the direction of gaze:  Let us imagine an active CNS sending out orders, or\n“commands” … to the effectors and receiving signals\nfrom its sensory organs. Signals that predictably come when nothing\noccurs in the environment are necessarily a result of its own\nactivity, i.e., are reafferences. All signals that come when\nno commands are given are exafferences and signify changes in\nthe environment or in the state of the organism caused by external\nforces. … The difference between that which is to be expected\nas the result of a command and the totality of what is reported by the\nsensory organs is the proportion of exafference…. It is only\nthis difference to which there are compensatory reflexes; only this\ndifference determines, for example during a moving glance at movable\nobjects, the actually perceived direction of visual objects. This,\nthen, is the solution that we propose, which we have termed the\n“reafference principle”: distinction of reafference and\nexafference by a comparison of the total afference with the\nsystem’s state—the\n“command”. (Mittelstädt\n1971; translated by\nBridgeman et al. 1994: 251). It is only when the displacement of the retinal image differs from\nthe displacement predicted on the basis of the efference copy, i.e.,\nwhen the latter fails to “cancel out” the former, that\nsubjects experience a change of some sort in the perceived scene\n(see Figure 1). The relevant upshot is that VDC\nhas an essential motoric component: the apparent stability of an\nobject’s eye-relative position in the world depends on the\nperceiver’s ability to integrate incoming retinal signals with\nextraretinal information concerning the magnitude and direction of\nimpending eye movements. Figure 1: (a) When the eye is stationary, both efference copy (EC) and afference produced by displacement of the retinal image (A) are absent. (b) Turning the eye 10° to the right results in a corresponding shift of the retinal image. Since the magnitude of the eye movement specified by EC and the magnitude of retinal image displacement cancel out, no movement in the world or “exafference” (EA) is registered. The foregoing solution to the problem of VDC faces challenges on\nmultiple, empirical fronts. First, there is evidence\nthat proprioceptive signals from the extraocular muscles make a\nnon-trivial contribution to estimates of eye position, although the\ngain of efference copy is approximately 2.4 times greater (Bridgeman\n& Stark 1991). Second, in the autokinetic\neffect, a fixed luminous dot appears to wander when the field of\nview is dark and thus completely unstructured. This finding is\ninconsistent with theories according to which retinotopic location and\nefference copy are the sole determinants of eye-relative\ndirection. Third, the hypothesized compensation\nprocess, if psychologically real, would be highly inaccurate since\nsubjects fail to notice displacements of the visual world up to 30% of\ntotal saccade magnitude (Bridgeman et al. 1975), and the\nlocations of flashed stimuli are systematically misperceived when\npresented near the time of a saccade (Deubel\n2004). Last, when image displacements concurrent with\na saccade are large, but just below threshold for\ndetection, visually attended objects appear to\n“jump” or “jiggle” against a stable background\n(Brune and Lücking 1969; Bridgeman 1981). Efference copy\ntheories, however, as Bridgeman observes,  do not allow the possibility that parts of the image\ncan move relative to one another—the visual world is conceived\nas a monolithic object. The observation would seem to eliminate all\nefference copy and related theories in a single stroke. (2010:\n102) The reference object theory of Deubel and Bridgeman denies\nthat efference copy is used to “cancel out” displacements\nof the retinal image caused by saccadic eye-movements (Deubel et al.\n2002; Deubel 2004;\nBridgeman 2010). According to\nthis theory, visual attention shifts to the saccade target and a small number\nof other objects in its vicinity (perhaps four or fewer) before eye\nmovement is initiated. Although little visual scene information is\npreserved from one fixation to the next, the features of these objects\nas well as precise information about their presaccadic, eye-relative\nlocations is preserved. After the eye has landed, the visual system\nsearches for the target or one of its neighbors within a limited\nspatial region around the landing site. If the postsaccadic\nlocalization of this “landmark” object succeeds, the world\nappears to be stable. If this object is not found, however,\ndisplacement is perceived. On this approach, efference copy does not\ndirectly support VDC. Rather, the role of efference copy is to\nmaintain an estimate of the direction of gaze, which can be integrated\nwith incoming retinal stimulation to determine the static,\nobserver-relative locations of perceived objects. For a recent,\nphilosophically oriented discussion, see Wu 2014. A related alternative to the von Holst-Mittelstädt model is\nthe spatial remapping theory of Duhamel and Colby\n(Duhamel et al. 1992; Colby et al. 1995). The role\nof saccade efference copy on this theory is to initiate an updating of\nthe eye-relative locations of a small number of attended or otherwise\nsalient objects. When post-saccadic object locations are sufficiently\ncongruent with the updated map, stability is perceived. Single-cell\nand fMRI studies show that neurons at various stages in the\nvisual-processing hierarchy exploit a copy of the saccade command\nsignal in order to shift their receptive field locations in the\ndirection of an impending eye movement microseconds before its\ninitiation (Merriam & Colby 2005; Merriam et al.\n2007). Efference copy indicating an impending saccade 20° to the\nright, in effect, tells relevant neurons:   If you are now firing in response to an\nitem x in your receptive field, then stop firing\nat x. If there is currently an item y in the region\nof oculocentric visual space that would be coincident with your\nreceptive field after a saccade 20° to the right, then start\nfiring at y.  \nSuch putative updating responses are strongest in parietal cortex and\nat higher levels in visual processing (V3A and hV4) and weakest at\nlower levels (V1 and V2). In 1961, Richard Held proposed that the reafference principle could\nbe used to construct a general “neural model” of\nperception and perceptually guided action. Held’s\nreafference theory goes beyond the account of\nvon Holst and Mittelstädt in three main\nways. First, information about movement parameters\nspecified by efference copy is not simply summated with reafferent\nstimulation. Rather, subjects are assumed to acquire knowledge of\nthe specific sensory consequences of different bodily\nmovements. This knowledge is contained in a hypothesized\n“correlational storage” area and used to determine whether\nor not the reafferent stimulations that result from a given type of\naction match those that resulted in the past (Held 1961:\n30). Second, the reafference theory is not\nlimited to eye movements, but extends to “any motor system that\ncan be a source of reafferent visual\nstimulation”. Third, knowledge of the way\nreafferent stimulation depends on self-produced movement is used for\npurposes of sensorimotor control: planning and controlling\nobject-directed actions in the present depends on access to\ninformation concerning the visual consequences of performing such\nactions in the past. The reafference theory was also significantly motivated by studies\nof how subjects adapt to devices that alter the relationship between\nthe distal visual world and sensory input by rotating, reversing, or\nlaterally displacing the retinal image (for helpful guides to the\nliterature on this topic, see Rock 1966; Howard & Templeton 1966;\nEpstein 1967; and Welch 1978). We will refer to these as optical\nrearrangement devices (or ORDs for short). The American psychologist George Stratton conducted two\nexperiments using a lens system that effected an 180º\nrotation of the retinal image in his right eye (his left eye was kept\ncovered). The first experiment involved wearing the device for 21.5\nhours over the course of three days (1896); the second experiment\ninvolved wearing the device for 81.5 hours over the course of 8 days\n(1897a,b). In both cases, Stratton kept a detailed diary of how his\nvisual, imaginative, and proprioceptive experiences underwent\nmodification as a consequence of inverted vision. In 1899, he\nperformed a lesser-known but equally dramatic three-day experiment,\nusing a pair of mirrors that presented his eyes with a view of his own\nbody from a position in space directly above his head\n(Figure 2). Figure 2: The apparatus designed by Stratton (1899). Stratton saw a view of his own body from the perspective of mirror AB, worn above his head. In both experiments, Stratton reported a brief period of initial\nvisual confusion and breakdown in visuomotor skill:   Almost all movements performed under the direct\nguidance of sight were laborious and embarrassed. Inappropriate\nmovements were constantly made; for instance, in order to move my hand\nfrom a place in the visual field to some other place which I had\nselected, the muscular contraction which would have accomplished this\nif the normal visual arrangement had existed, now carried my hand to\nan entirely different place. (1897a: 344) \nFurther bewilderment was caused by a “swinging” of the\nvisual field with head movements as well as jarring discord between\nwhere things were respectively seen and imagined to be:   Objects lying at the moment outside the visual field\n(things at the side of the observer, for example) were at first\nmentally represented as they would have appeared in normal\nvision…. The actual present perception remained in this way\nentirely isolated and out of harmony with the larger whole made up by\n[imaginative] representation. (1896: 615) After a seemingly short period of adjustment, Stratton reported a\ngradual re-establishment of harmony between the deliverances of sight\nand touch. By the end of his experiments on inverted vision, it was\nnot only possible for Stratton to perform many visuomotor actions\nfluently and without error, the visual world often appeared to him to be\n“right side up” (1897a: 358) and “in normal\nposition” (1896: 616). Just what this might mean will\nbe discussed below in Section\n2.2.6. Another influential experiment was performed by Helmholtz (2005\n[1924]: §29), who practiced reaching to targets while wearing\nprisms that displaced the retinal image 16–18° to the\nleft. The initial tendency was to reach too far in the direction of\nlateral displacement. After a number of trials, however, reaching\ngradually regained its former level of accuracy. Helmholtz made two\nadditional discoveries. First, there was an intermanual transfer\neffect: visuomotor adaptation to prisms extended to his\nnon-exposed hand. Second, immediately after removing the prisms from\nhis eyes, errors were made in the opposite direction, i.e., when\nreaching for a target, Helmholtz now moved his hand too far to the\nright. This negative after-effect is now standardly used as a\nmeasure of adaptation to lateral displacement. Stratton and Helmholtz’s findings catalyzed a research\ntradition on ORD adaptation that experienced its heyday in the 1960s\nand 1970s. Two questions dominated studies conducted during this\nperiod. First, what are the necessary and sufficient conditions for\nadaptation to occur? In particular, which sources\nof information do subjects use when adapting to the various\nperceptual and sensorimotor discrepancies caused by ORDs? Second,\njust what happens when subjects adapt to perceptual\nrearrangement? What is the “end product” of the relevant\nform of perceptual learning? Held’s answer to the first question is that subjects must\nreceive visual feedback from active movement, i.e., reafferent\nvisual stimulation, in order for significant and stable\nadaptation to occur (Held & Hein 1958; Held 1961; Held &\nBossom 1961). Evidence for this conclusion came from experiments in\nwhich participants wore laterally displacing prisms during both active\nand passive movement conditions. In the active movement condition, the\nsubject moved her visible hand back and forth along a fixed arc in\nsynchrony with a metronome. In the passive movement condition, the\nsubject’s hand was passively moved at the same rate by the\nexperimenters. Although the overall pattern of visual stimulation was\nidentical in both conditions, adaptation was reported only when\nsubjects engaged in self-movement. Reafferent stimulation, Held\nand Bossom concluded on the basis of this and other studies,  is the source of ordered contact with the environment\nwhich is responsible for both the stability, under typical conditions,\nand the adaptability, to certain atypical conditions, of\nvisual-spatial performance. (1961: 37) Held’s answer to the second question is couched in terms of\nthe reafference theory: subjects adapt to ORDs only when they have\nrelearned the sensory consequences of their bodily movements. In the\ncase of adaptation to lateral displacement, they must relearn the way\nretinal stimulations vary as a function of reaching for targets at\ndifferent body-relative locations. This relearning is assumed to\ninvolve an updating of the mappings from motor output to reafferent\nsensory feedback in the hypothesized \"correlational storage\" module\nmentioned above. The reafference theory faces a number of\nobjections. First, the theory is an extension of von\nHolst and Mittelstädt’s reafference principle, according to\nwhich efference copy is used to cancel out shifts of the retinal image\ncaused by saccadic eye movements. The latter was specifically intended\nto explain why we do not experience object displacement in the world\nwhenever we change the direction of gaze. There is nothing, at first\nblush, however, that is analogous to the putative need for\n“cancellation” or “discounting” of the retinal\nimage in the case of prism adaptation. As Welch puts it, “There\nis no visual position constancy here, so why should a model originally\ndevised to explain this constancy be appropriate?” (1978:\n16). Second, the reafference theory fails to explain\njust how stored efference-reafference correlations are supposed to\nexplain visuomotor control. How does having the ability to anticipate\nthe retinal stimulations that would caused by a certain type of hand\nmovement enable one actually to perform the movement in question?\nWithout elaboration, all that Held’s theory seems to explain is\nwhy subjects are surprised when reafferences generated by their\nmovements are non-standard (Rock 1966: 117). Third, adaptation to ORDs, contrary to the theory,\nis not restricted to situations in which subjects receive reafferent\nvisual feedback, but may also take place when subjects receive\nfeedback generated by passive effector or whole-body movement (Singer\n& Day 1966; Templeton et al. 1966; Fishkin\n1969). Adaptation is even possible in the complete absence of motor\naction (Howard et al. 1965; Kravitz & Wallach\n1966). In general, the extent to which adaptation occurs depends not on\nthe availability of reafferent stimulation, but rather on the presence\nof either of two related kinds of information concerning “the\npresence and nature of the optical rearrangement” (Welch 1978:\n24). Following Welch, we shall refer to this view as the\n“information hypothesis”. One source of information present in a displaced visual array\nconcerns the veridical directions of objects from the observer (Rock\n1966: chaps. 2–4). Normally, when engaging in forward\nlocomotion, the perceived radial direction of an object straight ahead\nof the observer’s body remains constant while the perceived\nradial directions of objects to either side undergo constant\nchange. This pattern also obtains when the observer wears prisms that\ndisplace the retinal image to side. Hence, “an object seen\nthrough prisms which retains the same radial direction as we approach\nmust be seen to be moving in toward the sagittal plane” (Rock\n1966: 105). On Rock’s view, at least some forms of adaptation to\nORDs can be explained by our ability to detect and exploit such\ninvariant sources of spatial informational in locomotion-generated\npatterns of optic flow. Another related source of information for adaptation derives from\nthe conflict between seen and proprioceptively experienced\nlimb position (Wallach 1968; Ularik & Canon 1971). When this\ndiscrepancy is made conspicuous, proponents of the information\nhypothesis have found that passively moved (Melamed et al.\n1973), involuntarily moved (Mather & Lackner 1975), and even\nimmobile subjects (Kravitz & Wallach 1966) exhibit significant\nadaption. Although self-produced bodily movement is not necessary for\nadaptation to occur, it provides subjects with especially salient\ninformation about the discrepancy between sight and touch (Moulden\n1971): subjects are able proprioceptively to determine the location of\na moving limb much more accurately than a stationary or passively\nmoved limb. It is the enhancement of the visual-proprioceptive\nconflict rather than reafferent visual stimulation, on this\ninterpretation, that explains why active movement yields more\nadaptation than passive movement in Held’s experiments. A final objection to the reafference theory\nconcerns the end product of adaptation to ORDs. According to\nthe theory, adaptation occurs when subjects learn new rules of\nsensorimotor dependence that govern how actions affect sensory\ninputs. There is a significant body of evidence, however, that much,\nif not all, adaptation rather occurs at the proprioceptive\nlevel. Stratton, summarizing the results of his experiment on\nmirror-based optical rearrangement, wrote:  …the principle stated in an earlier\npaper—that in the end we would feel a thing to be wherever\nwe constantly saw it—can be justified in a wider sense than\nI then intended it to be taken…. We may now, I think, safely\ninclude differences of distance as well, and assert that the spatial\ncoincidence of touch and sight does not require that an object in a\ngiven tactual position should appear visually in any particular\ndirection or at any particular distance. In whatever place the tactual\nimpression’s visual counterpart regularly appeared, this would\neventually seem the only appropriate place for it to appear in. If we\nwere always to see our bodies a hundred yards away, we would probably\nalso feel them there. (1899: 498, our emphasis) On this interpretation, the plasticity revealed by ORDs is\nprimarily proprioceptive and kinaesthetic, rather than\nvisual. Stratton’s world came to look “right side\nup” (1897b: 469) after adaptation to the inverted retinal image\nbecause things were felt where they were visually perceived\nto be—not because, his “entire visual field flipped\nover” (Kuhn 2012 [1962]: 112). This is clear from the absence of a visual negative\naftereffect when Stratton finally removed his inverting lenses at\nthe end of his eight-day experiment:   The visual arrangement was immediately recognized as\nthe old one of pre-experimental days; yet the reversal of everything\nfrom the order to which I had grown accustomed during the past week,\ngave the scene a surprising, bewildering air which lasted for several\nhours. It was hardly the feeling, though, that things were upside\ndown. (1897b: 470) \nMoreover, Stratton reported changes in kinaesthesis during\nthe course of the experiment consistent with the alleged\nproprioceptive shift:   when one was most at home in the unusual\nexperience the head seemed to be moving in the very opposite\ndirection from that which the motor sensations themselves would\nsuggest. (1907: 156) On this view, the end product of adaptation to an ORD is a\nrecalibration of proprioceptive position sense at one or more points\nof articulation in the body (see the entry on\n bodily awareness). As\nyou practice reaching for a target while wearing laterally displacing\nprisms, for example, the muscle spindles, joint receptors, and Golgi\ntendon organs in your shoulder and arm continue to generate the same\npatterns of action potentials as before, but the proprioceptive and\nkinaesthetic meaning assigned to them by their\n“consumers” in the brain undergoes change: whereas before\nthey signified that your arm was moving along one path through the\nseven-dimensional space of possible arm configurations (the human arm\nhas seven degrees of freedom: three at the wrist, one at the elbow,\nand three at the shoulder), they gradually come to signify that it is\nmoving along a different path in that kinematic space, namely, the one\nconsistent with the prismatically distorted visual feedback you are\nreceiving. Similar recalibrations are possible with respect to sources\nof information for head and eye position. After adapting to laterally\ndisplacing prisms, signals from receptors in your neck that previously\nsignified the alignment of your head and torso, for example, may come\nto signify that your head is turned slightly to the side. For\ndiscussion, see Harris 1965, 1980 and Welch 1978: chap. 3. The enactive approach defended by J. Kevin O’Regan and Alva\nNoë (O’Regan & Noë 2001; Noë 2004, 2005, 2010;\nO’Regan 2011) is best viewed as an extension of the reafference\ntheory. According to the enactive approach, spatially contentful,\nworld-presenting perceptual experience depends on implicit knowledge\nof the way sensory stimulations vary as a function of bodily\nmovement. “Over the course of life”, O’Regan and\nNoë write,   a person will have encountered myriad visual\nattributes and visual stimuli, and each of these will have particular\nsets of sensorimotor contingencies associated with it. Each such set\nwill have been recorded and will be latent, potentially available for\nrecall: the brain thus has mastery of all these sensorimotor\nsets. (2001: 945) \nTo see an object o as having the location and shape\nproperties it has it is necessary (1) to receive sensory stimulations\nfrom o and (2) to use those stimulations in order to retrieve\nthe set of sensorimotor contingencies associated with o on\nthe basis of past encounters. In this sense, seeing is a\n“two-step” process (Noë 2004: 164). It is important\nto emphasize, however, that the enactive approach distances itself\nfrom the idea that vision is functionally dedicated, in whole or in\npart, to the guidance of spatially directed actions:\n“Our claim”, Noë writes,   is that seeing depends on an appreciation of the\nsensory effects of movement (not, as it were, on the practical\nsignificance of sensation)…. Actionism is not committed to the\ngeneral claim that seeing is a matter of knowing how to act in respect\nof or in relation to the things we see. (Noë 2010:\n249) The enactive approach also has strong affinities with\nthe sense-data tradition. According to Noë, an\nobject’s visually apparent shape is the shape of the 2D patch\nthat would occlude the object on a plane perpendicular to the line of\nsight, i.e., the shape of the patch projected by the object on the\nfrontal plane in accordance with the laws of linear\nperspective. Noë calls this the object’s\n“perspectival shape” (P-shape). An object’s visually\napparent size, in turn, is the size of the patch projected by the\nobject on the frontal plane. Noë calls this the object’s\n“perspectival size” (P-size). Appearances are\n“perceptually basic” (Noë 2004: 81) because in order\nto see an object’s actual spatial properties it is necessary\nboth to see its 2D P-properties and to understand how they would vary\n(undergo transformation) with changes in one’s point of\nview. This conception of what it is to perceive objects as voluminous\nspace-occupiers is closely to akin to views defended by Russell\n(1918), Broad (1923), and Price (1950). It also worth mentioning that\nthe enactive approach has strong affinities to views in the\nphenomenological tradition that are beyond the scope of this entry\n(but for discussion, see Thompson 2005; Hickerson 2007; and\nthe entry on phenomenology). Assessment of the enactive approach is complicated by questions\nconcerning the nature of P-properties. First, there is a tendency on the\npart of its main proponents to speak interchangeably of consciously\nperceived P-properties (or ‘looks’), on the one hand, and\nproximal sensory stimulations, on the other. Noë, e.g.,\nwrites:  The sensorimotor profile of an object is the way\nits appearance changes as you move with respect to it\n(strictly speaking, it is the way sensory stimulation varies\nas you move). (2004: 78, our emphasis) It is far from clear how these different characterizations are to be\nrelated, however (Briscoe 2008; Kiverstein 2010). P-properties,\naccording to the enactive approach, are distal, relational properties\nof the objects we see: “If there is a mind/world divide…\nthen P-properties are on the world side of the divide” (2004:\n83). Moreover, Noë clearly assumes that they are visible:\n“P-properties are themselves objects of sight, that is, things\nthat we see” (2004: 83). Sensory stimulations, by contrast, are\nproximal, subpersonal vehicles of visual perception. They are not\nobjects of sight. Quite different, if not incommensurable, notions\nof sensorimotor profile and, so, of sensorimotor\nknowledge would thus seem to be implied by the two\ncharacterizations. There is also an ambiguity with the “-motor” in\n“sensorimotor knowledge”. On the one hand, Noë argues\nthat perception is active in the sense that perceivers require\nknowledge of the proximal, sensory effects of movement. E.g., in order\nto see an object’s shape and size it is necessary to have\ncertain anticipations concerning the way in which retinal stimulations\ncaused by the object would vary as a function of her point of\nview. “This perspectival aspect”, Noë writes,\n“marks the place of action in perception” (Noë 2004:\n34). On this conception there is no commitment to the view that vision\nis for the guidance of action, that vision constitutively has\nsomething to do with adapting animal behavior to the spatial layout of\nthe distal environment (Noë 2004: 18–19). Rather, vision is\nactive in the sense that it involves learned expectations concerning\nthe ways in which sensory stimulations would be\n“perturbed” by possible bodily movements (Noë 2010:\n247–248). On the other hand, Noë adverts to a more world-engaging\nconception of sensorimotor knowledge in order to explain our visual\nexperience of P-properties:   variation in looks reveals how things are. But what of\nthe looks themselves, what of P-properties? Do we see them by\nseeing how they look? This would threaten to lead to infinite\nregress…. (2004: 87) \nThe solution to the regress problem is that seeing an object’s\nP-properties involves a kind of practical know-how. A tilted plate,\ne.g., looks elliptical and small from here because one has to move\none’s hand in a certain way in order to indicate its shape and\nsize in the visual field (2004: 89). Whereas seeing an object’s\nintrinsic properties, according to the enactive approach, requires\nknowledge of the way P-properties would vary as a function of\nmovement, seeing P-properties involves knowing how one would need to\nmove one’s body in relation to what one sees in order to achieve\na certain goal. While this seems to suggest that the first kind of sensorimotor\nknowledge is asymmetrically dependent on the latter, Noë\nmaintains that just the opposite is the case. “I do not wish to\nargue”, he writes,  that to experience something as having a certain\n[P-shape] is to experience it as affording a range of possible\nmovements; rather I want to suggest that one experiences it as having\na certain P-shape, and so as affording possible movements, only\ninsofar as, in encountering it, one is able to draw on one’s\nappreciation of the sensorimotor patterns mediating (or that might be\nmediating) your relation to it. (2004: 90) \nThe problem with this suggestion, however, is that it leads the\nenactive approach directly back to the explanatory regress that the\nsecond, affordance-detecting kind of sensorimotor knowledge was\nintroduced to avoid. The enactive approach rests its case on three main sources of\nempirical support. The first derives from experiments\nwith optical rearrangement devices (ORDs), discussed\nin Section 2.2 above. Hurley and Noë (2003)\nmaintain that adaptation to ORDs only occurs when subjects relearn the\nsystematic patterns of interdependence between active movement and\nreafferent visual stimulation. Moreover, contrary to the\nproprioceptive change theory of Stratton, Harris, and Rock, Hurley and\nNoë argue that the end product of adaptation to inversion and\nreversal of the retinal image is genuinely visual in nature: during\nthe final stage of adaptation, visual experience “rights\nitself”. In Section 2.2 above, we reviewed empirical\nevidence against the view that active movement and corresponding\nreafferent stimulation are necessary for adaptation to\nORDs. Accordingly, we will focus here on Hurley and Noë’s\nobjections to the proprioceptive-change theory. According to the\nlatter, “what is actually modified [by the adaptation process]\nis the interpretation of nonvisual information about positions of body\nparts” (Harris 1980: 113). Once intermodal harmony is restored,\nthe subject will again be able to perform visuomotor actions without\nerror or difficulty, and she will again feel at home in the visually\nperceived world. Hurley and Noë do not contest the numerous sources of empirical\nand introspective evidence that Stratton, Harris, and Rock adduce for\nthe proprioceptive-change theory. Rather they reject the theory on the\nbasis of what they take to be an untoward epistemic implication\nconcerning adaptation to left-right reversal:   while rightward things really look and feel\nleftward to you, they come to seem to look and feel\nrightward. So the true qualities of your experience are no longer\nself-evident to you. (2003: 155) The proprioceptive-change theory, however, does not imply such\nradical introspective error. According to proponents of the theory,\nexperience normalizes after adaptation to reversal not because things\nthat really look leftward “seem to look rightward” (what\nthis might mean is enigmatic at best), but rather because the subjects\neventually become familiar with the way things look when\nreversed—much as ordinary subjects can learn to read\nmirror-reversed writing fluently (Harris 1965: 435–36). Things\nseem “normal” after adaptation, in other words, because\nsubjects are again able to cope with the visually perceived world in a\nfluent and unreflective manner. A second line of evidence for the enactive approach\ncomes from well-known experiments on tactile-visual sensory\nsubstitution (TVSS) devices that transform outputs from a\nlow-resolution video camera into a matrix of vibrotactile stimulation\non the skin of one’s back (Bach-y-Rita 1972, 2004) or\nelectrotactile stimulation on the surface of one’s tongue\n(Sampaio et al. 2001). At first, blind subjects equipped with a TVSS device experience its\noutputs as purely tactile. After a short time, however, many subjects\ncease to notice the tactile stimulations themselves and instead report\nhaving quasi-visual experiences of the objects arrayed in space in\nfront of them. Indeed, with a significant amount of supervised\ntraining, blind subjects can learn to discriminate spatial properties\nsuch as shape, size, and location and even to perform simple\n“eye”-hand coordination tasks such as catching or batting\na ball. A main finding of relevance in early experiments was that\nsubjects learn to “see” by means of TVSS only when they\nhave active control over movement of the video camera. Subjects who\nreceive visual input passively—and therefore lack any knowledge of\nhow (or whether) the camera is moving—experience only\nmeaningless, tactile stimulation. Hurley and Noë argue that passively stimulated subjects do not\nlearn to “see” by means of sensory substitution because\nthey are unable to learn the laws of sensorimotor contingency that\ngovern the prosthetic modality:   active movement is required in order for the subject\nto acquire practical knowledge of the change from sensorimotor\ncontingencies characteristic of touch to those characteristic of\nvision and the ability to exploit this change skillfully. (Hurley\n& Noë 2003: 145) An alternative explanation, however, is that subjects who do not\ncontrol camera movement—and who are not otherwise attuned to how\nthe camera is moving—are simply unable to extract\nany information about the structure of the distal scene from\nthe incoming pattern of sensory stimulations. In consequence they do\nnot engage in “distal attribution” (Epstein et al.\n1986; Loomis 1992; Siegel\n& Warren 2010): they do not perceive through the changing\npattern of proximal stimulation to a spatially external scene in the\nenvironment. For development of this alternative explanation in the\ncontext of Bayesian perceptual psychology, see Briscoe\nforthcoming. A final source of evidence for the enactive\napproach comes from studies of visuomotor development in the absence\nof normal, reafferent visual stimulation. Held & Hein 1963\nperformed an experiment in which pairs of kittens were harnessed to a\ncarousel in a small, cylindrical chamber. One of the kittens was able\nto engage in free circumambulation while wearing a harness. The other\nkitten was suspended in the air in a metal gondola whose motions were\ndriven by the first harnessed kitten. When the first kitten walked,\nboth kittens moved and received identical visual stimulation. However,\nonly the first kitten received reafferent visual feedback as the\nresult of self-movement. Held and Hein reported that\nonly mobile kittens developed normal depth\nperception—as evidenced by their unwillingness to step over the\nedge of a visual cliff, blinking reactions to looming objects, and\nvisually guided paw placing responses. Noë (2004) argues that\nthis experiment supports the enactive approach: in order to develop\nnormal visual depth perception it is necessary to learn how motor\noutputs lead to changes to visual inputs. There are two main reasons to be skeptical of this\nassessment. First, there is evidence that passive\ntransport in the gondola may have disrupted the development of the\nkittens’ innate visual paw placing responses (Ganz 1975:\n206). Second, the fact that passive kittens were\nprepared to walk over the edge of a visual cliff does not show that\ntheir visual experience of depth was abnormal. Rather, as\nJesse Prinz (2006)\nargues, it may only indicate that they “did not have enough\nexperience walking on edges to anticipate the bodily affordances of\nthe visual world”. The enactive approach confronts objections on multiple fronts. We\nfocus on just three of them here (but see Block 2005; Prinz 2006;\nBriscoe 2008; Clark 2009: chap. 8; and Block\n2012). First, the approach is essentially an\nelaboration of Held’s reafference theory and, as such, faces\nmany of the same empirical obstacles. Evidence, for example, that\nactive movement per se is not necessary for perceptual\nadaptation to optical rearrangement\n(Section 2.2.1) is at variance\nwith predictions made by the reafference theory and the enactive\napproach alike. A second line of criticism targets the alleged\nperceptual priority of P-properties. According to the enactive\napproach, P-properties are “perceptually basic” (Noë\n2004: 81) because in order to see an object’s intrinsic, 3D\nspatial properties it is necessary to see its 2D P-properties and to\nunderstand how they would undergo transformation with variation in\none’s point of view. When we view a tilted coin, critics argue,\nhowever, we do not see something that looks—in either\nan epistemic or non-epistemic sense of “looks”—like\nan upright ellipse. Rather, we see what looks like a disk that is\npartly nearer and partly farther away from us. In general, the\napparent shapes of the objects we perceive are not 2D but have\nextension in depth (Austin 1962; Gibson 1979; Smith 2000; Schwitzgebel\n2006; Briscoe 2008; Hopp 2013). Support for this objection comes from work in mainstream vision\nscience. In particular, there is abundant empirical evidence that an\nobject’s 3D shape is specified by sources of spatial information\nin the light reflected or emitted from the object’s surfaces to\nthe perceiver’s eyes as well as by oculomotor factors (for\nreviews, see Cutting & Vishton 1995; Palmer 1999; and Bruce et\nal. 2003). Examples include binocular disparity, vergence,\naccommodation, motion parallax, texture gradients, occlusion, height\nin the visual field, relative angular size, reflections, and\nshading. That such shape-diagnostic information having once been\nprocessed by the visual system is not lost in conscious visual\nexperience of the object is shown by standard psychophysical methods\nin which experimenters manipulate the availability of different\nspatial depth cues and gauge the perceptual effects. Objects, for\nexample, look somewhat flattened under uniform illumination conditions\nthat eliminate shadows and highlights, and egocentric distances are\nunderestimated for objects positioned beyond the operative range of\nbinocular disparity, accommodation, and vergence. Results of such\nexperimentation show that observers can literally see the difference\nmade by the presence or absence of a certain cue in the light\navailable to the eyes (Smith 2000; Briscoe 2008). According to the influential dual systems model (DSM) of visual\nprocessing (Milner & Goodale 1995/2006; Goodale & Milner\n2004), visual consciousess and visuomotor control are supported by\nfunctionally and anatomically distinct visual subsystems (these are\nthe ventral and dorsal information processing\nstreams, respectively). In particular, proponents of the DSM maintain\nthat the contents of visual experience are not used by motor\nprogramming areas in the primate brain: The visual information used by the dorsal stream for\nprogramming and on-line control, according to the model, is\nnot perceptual in nature …[I]t cannot be accessed\nconsciously, even in principle. In other words, although we may be\nconscious of the actions we perform, the visual information used to\nprogram and control those actions can never be experienced. (Milner\n& Goodale 2008: 775–776) A final criticism of the enactive approach is that\nit is empirically falsified by evidence for the DSM (see the\ncommentaries on O’Regan & Noë\n2001; Clark 2009:\nchap. 8; and the essays collected in Gangopadhyay et al.\n2010): the bond it posits between what we see and what we do is much\ntoo tight to comport with what neuroscience has to tells us about\ntheir functional relations. The enactivist can make two points in reply to this\nobjection. First, experimental findings indicate that there are a\nnumber of contexts in which information present in conscious vision is\nutilized for purposes of motor programming (see Briscoe 2009 and\nBriscoe & Schwenkler forthcoming). Action and perception are not as sharply dissociated as\nproponents of DSM sometimes claim. Second, the enactive approach, as emphasized above, rejects the\nidea that the function of vision is to guide actions. It   does not claim that visual awareness depends on\nvisuomotor skill, if by “visuomotor skill” one means the\nability to make use of vision to reach out and manipulate or\ngrasp. Our claim is that seeing depends on an appreciation of the\nsensory effects of movement (not, as it were, on the practical\nsignificance of sensation). (Noë 2010: 249) \nSince the enactive approach is not committed to the idea that seeing\ndepends on knowing how to act in relation to what we see, it is not\nthreatened by empirical evidence for a functional dissociation between\nvisual awareness and visually guided action. At this point, it should be clear that the claim that perception\nis active or action-based is far from\nunambiguous. Perceiving may implicate action in the sense that it is\ntaken constitutively to involve associations with touch (Berkeley\n1709), kinaesthetic feedback from changes in eye position (Lotze 1887\n[1879]), consciously experienced “effort of the will”\n(Helmholtz 2005 [1924]), or\nknowledge of the way reafferent sensory stimulation varies as a\nfunction of movement (Held 1961; O’Regan & Noë \n2001; Hurley &\nNoë 2003). In this section, we shall examine two additional conceptions of the\nrole of action in perception. According to the motor component\ntheory, as we shall call it, efference copies generated in\nthe oculomotor system and/or proprioceptive feedback from\neye-movements are used in tandem with incoming sensory inputs to\ndetermine the spatial attributes of perceived objects (Helmholtz 2005 [1924]; Mack 1979;\nShebilske 1984, 1987; Ebenholtz 2002). Efferent readiness\ntheories, by contrast, appeal to the particular ways in which\nperceptual states prepare the observer to move and act in\nrelation to the environment. The modest readiness\ntheory, as we shall call it, claims that the way an\nobject’s spatial attributes are represented in visual experience\nis sometimes modulated by one or another form of covert action\nplanning (Festinger et al. 1967; Coren 1986; Vishton et\nal. 2007). The bold readiness theory argues for\nthe stronger, constitutive claim that, as J.G. Taylor puts its,\n“perception and multiple simultaneous readiness for action are\none and the same thing” (1968: 432). As pointed out in Section 2.3.2, there\nare numerous, independently variable sources of information about the\nspatial layout of the environment in the light sampled by the eye. In\nmany cases, however, processing of stimulus information requires or is\noptimized by recruiting sources of auxiliary information from outside\nthe visual system. These may be directly integrated with incoming\nvisual information or used to change the weighting assigned to one or\nanother source of optical stimulus information (Shams & Kim 2010;\nErnst 2012). An importantly different recruitment strategy involves combining\nvisual input with non-perceptual information originating in the\nbody’s motor control systems, in particular, efference copy,\nand/or proprioceptive feedback from active movement\n(kinaesthesis). The motor component theory, as we\nshall call it, is premised on evidence for such motor-modal\nprocessing. The motor component theory can be made more concrete by examining\nthree situations in which the spatial contents of visual experience\nare modulated by information concerning recently initiated or\nimpending bodily movements: The motor component theory is a version of the view that perception\nis embodied in the sense of Prinz 2009 (see\nthe entry on embodied cognition).\n Prinz explains that embodied mental capacities, are ones that depend on\nmental representations or processes that relate to the\nbody…. Such representations and processes come in two forms:\nthere are representations and processes that represent or respond to\nbody, such as a perception of bodily movement, and there are\nrepresentations and processes that affect the body, such as motor\ncommands. (2009: 420; for relevant discussion of various senses of\nembodiment, see Alsmith and Vignemount 2012) \nThe three examples presented above provide empirical support for the\nthesis that visual perception is embodied in this sense. For\nadditional examples, see Ebenholtz 2002: chap. 4. Patients with frontal lobe damage sometimes exhibit pathological\n“utilization behaviour” (Lhermitte 1983) in which the\nsight of an object automatically elicits behaviors typically\nassociated with it, such as automatically pouring water into a glass and\ndrinking it whenever a bottle of water and a glass are present\n(Frith et al. 2000: 1782). That normal subjects often do not\nautomatically perform actions afforded by a perceived object, however, does not mean\nthat they do not plan, or imaginatively rehearse, or otherwise\nrepresent them. (On the contrary, recent neuroscientific findings\nsuggest that merely perceiving an object often covertly prepares the motor\nsystem to engage with it in a certain manner. For overviews, see\nJeannerod 2006 and Rizzolatti 2008.) Efferent readiness theories are based on the idea that covert\npreparation for action is “an integral part of the perceptual\nprocess” and not “merely a consequence of the\nperceptual process that has preceded it” (Coren 1986:\n394). According to the modest readiness theory, as we\nshall call it, covert motor preparation can sometimes influence the\nway an object’s spatial attributes are represented in perceptual\nexperience. The bold readiness theory, by contrast,\nargues for the stronger, constitutive claim that to perceive an\nobject’s spatial properties just is to be prepared or\nready to act in relation to the object in certain ways (Sperry\n1952; Taylor 1962, 1965,\n1968). A number of empirical findings motivate the modest readiness\ntheory. Festinger et al. 1967 tested the view that visual\ncontour perception is   determined by the particular sets of preprogrammed\nefferent instructions that are activated by the visual input into a\nstate of readiness for immediate use. (p. 34) \nContact lenses that produce curved retinal input were placed on the\nright eye of three observers, who were instructed to scan a\nhorizontally oriented line with their left eye covered for 40\nminutes. The experimenters reported that there was an average of 44%\nadaptation when the line was physically straight but retinally curved,\nand an average of 18% adaptation when the line was physically curved\nbut retinally straight (see Miller & Festinger 1977, however, for\nconflicting results). An elegantly designed set of experiments by Coren 1986 examined the\nrole of efferent readiness in the visual perception of direction and\nextent. Coren’s experiments support the hypothesis that the\nspatial parameter controlling the length of a saccade is not the\nangular direction of the target relative to the line of sight, but\nrather the direction of the center of gravity (COG) of all the stimuli\nin its vicinity (Coren & Hoenig 1972; Findlay\n1982). Importantly,   the bias arises from the computation of the saccade\nthat would be made and, hence, is held in readiness, rather\nthan the saccade actually emitted. (Coren 1986: 399) The COG bias is illustrated in Figure 3. In\nthe first row (top), there are no extraneous stimuli\nnear the saccade target. Hence, the saccade from the point of fixation to\nthe target is unbiased. In the second row, by\ncontrast, the location of an extraneous stimulus (×) results in\na saccade from the point of fixation that undershoots its target, while in\nthe third row the saccade overshoots its\ntarget. In the fourth row, changing the location of\nthe extraneous stimulus eliminates the COG bias: because the\nextraneous stimulus is near the point of fixation rather than the saccade\ntarget, the saccade is accurate. Figure 3: \nThe effect of starting eye position on saccade programming (after Coren 1986: 405) The COG bias is evolutionarily adaptive: eye movements will bring\nboth the saccade target as well as nearby objects into high acuity\nvision, thereby maximizing the amount of information obtained with\neach saccade. Motor preparation or “efferent readiness” to\nexecute an undershooting or overshooting saccade, Coren found,\nhowever, can also give rise to a corresponding illusion of\nextent (1986: 404–406). Observers, e.g., will perceptually\nunderestimate the length of the distance between the point of fixation\nand the saccade target when there is an extraneous stimulus on the\nnear side of the target (as in the second row\nof Figure 3) and will perceptually overestimate\nthe length of the distance when there is an extraneous stimulus on the\nfar side of the target (as in the third row of Figure\n3). According to Coren, the well known Müller-Lyer illusion can be\nexplained within this framework. The outwardly turned wings in\nMüller-Lyer display shift the COG outward from each vertex, while\nthe inwardly turned wings in this figure shift the COG inward. This\ninfluences both saccade length from vertex to vertex as well as the\napparent length of the central line segments. The influence of COG on\nefferent readiness to execute eye movements, Coren argues (1986:\n400–403), also explains why the line segments in the\nMüller-Lyer display can be replaced with small dots while leaving\nthe illusion intact as well as the effects of varying wing length and\nwing angle on the magnitude of the illusion. The modest readiness theory holds that the way an\nobject’s spatial attributes are represented in visual experience\nis sometimes modulated by one or another form of covert action\nplanning. The bold readiness theory argues for a\nstronger, constitutive claim: to perceive an object’s spatial\nproperties just is to be prepared or ready to act in relation\nto the object in certain ways. We begin by examining\nJ.G. Taylor’s “behavioral theory” of perception\n(Taylor 1962, 1965, 1968). Taylor’s behavioral theory of perception identifies\nthe conscious experience of seeing an object’s spatial\nproperties with the passive activation of a specific set of learned or\n“preprogrammed” motor routines:  [P]erception is a state of multiple simultaneous\nreadiness for actions directed to the objects in the environment that\nare acting on the receptor organs at any one moment. The actions in\nquestion have been acquired by the individual in the course of his\nlife and have been determined by the reinforcing contingencies in the\nenvironment in which he grew up. What determines the content of\nperception is not the properties of the sensory transducers that are\noperated on by stimulus energies from the environment, but the\nproperties of the behaviour conditioned to those stimulus\nenergies…. (1965: 1, our emphasis) According to Taylor’s theory, sensory stimulation gives rises\nto spatially contentful visual experience as a consequence of\nassociative, reinforcement learning: we perceive an object as having\nthe spatial attribute G when the types of proximal sensory\nstimulation caused by the object have been conditioned to the\nperformance of actions sensitive to G (1962: 42). The\nconscious experience of seeing an object’s distance,\ne.g., is constituted by the subject’s learned readiness to\nperform specific whole body and limb movements that were reinforced\nwhen the subject previously received stimulation from objects at the\nsame remove. In general, differences in the spatial content of a\nvisual experience are identified with differences in the\nsubject’s state of “multiple simultaneous readiness”\nto interact with the objects represented in the experience. The main problem with Taylor’s theory is one that besets\nbehaviorist theories of perception in general: it assumes that for any\nvisible spatial property G, there will be some distinctive\nset of behavioral responses that are constitutive of perceiving the\nobject as having G. The problem with this assumption, as\nMohan Matthen (1988) puts it,   there is no such thing as the proper\nresponse, or even a range of functionally appropriate responses, to\nwhat perception tells us. (p. 20, see also Hurley 2001:\n17) The last approach we shall discuss has roots in, and similarities\nto, many of the proposals covered above, but is most closely aligned\nwith the bold readiness theory. We will follow Grush (2007) in calling\nthis approach the disposition theory (see Grush 2007: 394,\nfor discussion of the name). The primary proponent of this position is\nGareth Evans, whose work on spatial representation focused on\nunderstanding how we manage to perceive objects as occupying locations\nin egocentric space. The starting point of Evans’ theory is that the\nsubject’s perceptual systems have isolated a channel of sensory\ninput, an “information link”, through which she receives\ninformation about the object. The information link by itself does not\nallow the subject to know the location of this object. Rather, it is\nwhen the information link is able to induce in the subject appropriate\nkinds of behavioral dispositions that it becomes imbued with spatial\nimport:  The subject hears the sound as coming from\nsuch-and-such a position, but how is the position to be specified?\nPresumably in egocentric terms (he hears the sound as up, or\ndown, to the right or to the left, in front or behind). These terms\nspecify the position of the sound in relation to the observer’s\nown body; and they derive their meaning in part from their complicated\nconnections with the subject’s actions. (Evans 1982:\n155) This is not a version of a motor theory (e.g., Poincaré\n1907: 71). The behavioral responses in question are not to be\nunderstood as raw patterns of motor activations, or even muscular\nsensations. Such a reduction would face challenges anyway, since for\nany location in egocentric space, there are an infinite number of\nkinematic configurations (movements) that would, for example, effect a\ngrasp to that location; and for any kinematic configuration, there are\nan infinite number of dynamic profiles (temporal patterns of muscular\nforce) that would yield that configuration. The behavioral responses\nin question are overt environmental behavior:  It may well be that the input-output connections can\nbe finitely stated only if the output is described in explicitly\nspatial terms (e.g., ‘extending the arm’, ‘walking\nforward two feet’, etc.). If this is so, it would rule out the\nreduction of the egocentric spatial vocabulary to a muscular\nvocabulary. But such a reduction is certainly not needed for the point\nbeing urged here, which is that the spatial information embodied in\nauditory perception is specifiable only in a vocabulary whose terms\nderive their meaning partly from being linked with bodily\nactions. Even given an irreducibility, it would remain the case that\npossession of such information is directly manifestable in behaviour\nissuing from no calculation; it is just that there would be\nindefinitely many ways in which the manifestation can occur. (Evans\n1982: 156) Also, on this proposal, all modalities are in the same boat. As\nsuch the disposition theory is more ambitious than most of the\ntheories already discussed, which are limited to vision. Not only is\nthere no reduction of perceptual spatial content to a “muscular\nvocabulary”, there is also no reduction of the spatial content\nof some perceptual modalities to that of one or more others—as\nthere was for Berkeley, who sought to reduce the spatial content of\nvision to that of touch, and whose program forced a distinction\nbetween two spaces, visual space and tangible space:  The spatial content of auditory and\ntactual-kinaesthetic perceptions must be specified in the same\nterms—egocentric terms. … It is a consequence of this\nthat perceptions from both systems will be used to build up a unitary\npicture of the world. There is only one egocentric space, because\nthere is only one behavioural space. (Evans 1982:\n160) Relatedly, for Evans it is not even the case that spatial\nperceptual content, for all modalities, is being reduced to\nbehavioral dispositions. Rather, perceptual inputs and behavioral\noutputs jointly and holistically yield a single behavioral\nspace:  Egocentric spatial terms are the terms in which the\ncontent of our spatial experiences would be formulated, and those in\nwhich our immediate behavioural plans would be expressed. This duality\nis no coincidence: an egocentric space can exist only for an animal in\nwhich a complex network of connections exists between perceptual input\nand behavioural output. A perceptual input—even if, in some\nloose sense, it encapsulates spatial information (because it belongs\nto a range of inputs which vary systematically with some spatial\nfacts)—cannot have a spatial significance for an organism except\nin so far as it has a place in such a complex network of input-output\nconnections. (Evans 1982: 154)  Egocentric spatial terms and spatial descriptions of bodily\nmovement would, on this view, form a structure familiar to\nphilosophers under the title “holistic”. (Evans 1982: 156,\nfn. 26) This last point and the associated quotes address a common\nmisconception of the disposition theory. It would be easy to read the\ntheory as providing a proposal of the following sort: A creature\ngets sensory information from a stimulus, and the problem is to\ndetermine where that stimulus is located in egocentric space; the\nsolution is that features of that sensory episode induce dispositions\nto behavior targeting some egocentric location. While this sort\nof thing is indeed a problem, it is relatively superficial. Any\ncreature facing this problem must already have the capacity\nto grasp egocentric spatial location contents, and the problem is\nwhich of these ready-at-hand contents it should assign to the\nstimulus. But the disposition theory is addressing a deeper question:\nin virtue of what does this creature have a capacity to grasp\negocentric spatial contents to begin with? The answer is that the\ncreature must have a rich set of interconnections between sensory\ninputs (and their attendant information links) and dispositions for\nbehavioral outputs. Rick Grush (2000, 2007) has adopted Evans’ theory, and\nattempted to clarify and expand upon it, particularly in three areas:\nfirst, the distinction between the disposition theory and other\napproaches; second, the neural implementation of the disposition\ntheory; and finally the specific kinds of dispositions that are\nrelevant for the issue of spatial experience. The theory depends on behavioral dispositions. Grush (2007) argues\nthat there are two distinctions that need to be made: first, the\norganism might possess i) knowledge of what the consequences (bodily,\nenvironmental, or sensory) of a given action will be; or ii) knowledge\nof which motor commands will bring about a given desired end state\n(of the body, environment, or sensory channels) (Grush 2007: 408). I\nmight be able to recognize that a series of moves someone shows me\nwill force my grandmaster opponent into checkmate (knowledge of the\nfirst sort, the consequences of a given set of actions), and yet not\nhave been anywhere near the skill level to have come up with that\nseries of moves on my own (knowledge of the second sort, what actions\nwill achieve a desired effect). Sensorimotor contingency theorists\nappeal to knowledge of the first sort—though as was discussed\nin Section 2.3.1, Noë flirts with\nappealing to knowledge of the second sort to explain the perceptual\ngrasp of P-shapes; to the extent he does, he is embracing a\ndisposition theoretic account of P-shapes. Disposition theorists, and\nbold readiness theorists (Section\n3.2.2) appeal to\nknowledge of the second sort. These are the dispositions of the\ndisposition theory: given some goal, the organism is disposed to\nexecute certain actions. This leads to the second distinction,\nbetween type-specifying and detail-specifying\ndispositions. Grush (2007: 393) maintains that only the latter are\ndirectly relevant for spatial perception. A type-specifying\ndisposition is a disposition to execute some type of behavior\nwith respect to an object or place. For example, an organism might be\ndisposed to grasp, bite, flee, or foveate some object. This sort of\ndisposition is not relevant to the spatial content of the experience\non the disposition theory. Rather, what are relevant are\ndetail-specifying dispositions: the specifics of how I am disposed to\nact to execute any of these behavior types. When reaching to grab the\ncup to take a drink (type), do I move my hand like so (straight ahead,\nsay), or like such (off to the right)? When I want to foveate or\norient towards (behavior type) the ant crawling up the wall, do a I\nmove my head and eyes like this, or like that? This latter distinction allows the disposition theory to answer one\nof the main objections to the bold readiness theory (described at the\nend of section 3.2.2) that there is no single\nspecial disposition connected to perceiving any given object. That is\ntrue of type-specifying dispositions, but not of detail-specifying\ndispositions. Given the ant’s location there is indeed a very\nlimited range of detail specifying dispositions that will allow me to\nfoveate it (though this might require constraints on possible actions,\nsuch as minimum jerk or other such constraints). Grush (2007; 2009) has proposed a detailed implementation of the disposition\ntheory in terms of neural information processing. The proposal\ninvolves more mathematics than is appropriate here, and so a quick\nqualitative description will have to suffice (for more detail, see\nGrush 2007; 2009). The basic idea is that relevant cortical areas learn\nsets of basis functions which, to put it very roughly, encode\nequivalence classes of combinations of sensory and postural signals\n(for discussion, see Pouget et al. 2002). For example, many\ncombinations of eye orientation and location of stimulation on the\nretina correspond to a visual stimulus that is directly in front of\nthe head. Sorting such bodily postural information (not just eye\norientation, but any postural information that affects sensation,\nwhich is most) and sensory condition pairs into useful equivalence\nclasses is the first half of the job. What this does is encode incoming information in a way that renders\nit ready to be of use in guiding behavior, since the equivalence\nclasses are precisely those for which a given kind of motor program is\nappropriate. The next part corresponds to how this information, so\nrepresented, can be used to produce the details of such a motor\nprogram. For every type of action in a creature’s\nbehavioral repertoire (grasp, approach, avoid, foveate, bite, etc.)\nits motor areas have a set of linear coefficients, easily implemented\nas a set of neural connection strengths, and when these are applied to\na set of basis function values, a detailed behavior is specified. For\nexample, when a creature senses an object O1, a\nset of basis function values B1 for that stimulus\nis produced. If the creature decides to execute overt\naction A1, then the B1 basis\nfunction values are multiplied by the coefficient corresponding\nto A1. The result is an instance of behavior\ntype A1 executed with respect to\nobject O1. If the creature had decide instead to\nexecute action A2, with respect\nto O1, the B1 basis function\nvalues would have been multiplied by the A2 set of\ncoefficients, and the result would be a motor behavior\nexecuting A2 on object O1. Accordingly, the disposition theory has a very different account of\nwhat is happening with sensory substitution devices than Susan Hurley and Alva\nNoë (see Section 2.3.2 above). On the\ndisposition theory, what allows the user of such a device to have\nspatial experience is not the ability to anticipate how the sensory\ninput will change upon execution of movement as the sensorimotor\ncontingency theory would have it. Rather, it is that the\nsubject’s brain has learned to take these sensory inputs\ntogether with postural signals to produce sets of basis functions that\npoise the subject to act with respect to the object that is causing\nthe sensory signals (see Grush 2007: 406). One objection to disposition theories is what Hurley has called The Myth of the\nGiving:  To suppose that … the content of intentions\ncan be taken as unproblematically primitive in explaining how the\ncontent of experience is possible, is to succumb to the myth of the\ngiving. (Hurley 1998: 241) The idea behind this objection is that one is simply shifting\nthe debt from one credit card to another when one takes as problematic\nthe spatial content of perception, and then appeals to motor behavior\nas the supplier of this content. For then, of course, the question\nwill be: Whence the spatial content of motor behavior? The disposition theory, however, does not posit any such unilateral\nreduction (though Taylor’s bold readiness theory arguably does,\nsee Section 3.2.2 above). As discussed above,\nEvans explicitly claims that the behavioral space is holistically\ndetermined by both behavior and perception. And on Grush’s\naccount spatial content is implemented in the construction of basis\nfunction values, and these values coordinate transitions from\nperceptual input to behavioral output. As such, they are highly\nanalogous to inferences whose conditions of application are given in\nsensory-plus-postural terms and whose consequences of application\nmanifest in behavioral terms. The import of the states that represent\nthese basis function values is no more narrowly motor than the meaning\nof a conditional can be identified with its consequent (or its\nantecedent, for that matter) in isolation. Another very common objection, one that is often leveled at many\nforms of motor theory, has to do with the fact that even paralyzed\npeople, with very few possibilities for action, seem capable in many\ncases of normal spatial perception. Such objections would, at a minimum,\nplace significant pressure on any views that explain perceptual\ncontent by appeal to actual behavior. It is also easy to see how even\nhypothetical behavior would be called into question in such cases,\nsince in many such cases behavior is not physically\npossible. Grush’s theory (2007), right or wrong, has something\nspecific to say about this objection. Since spatial content is taken\nto be manifested in the production of basis function values in the\ncortex, the prediction is that any impairments manifesting farther\ndown the chain, the brain stem or spinal cord, for example, need have\nno direct effect on spatial content. So long as the relevant brain\nareas have the wherewithal to produce sets of basis function values\nsuitable for constructing a motor sequence (if multiplied by the\naction-type-specific coefficients), then the occasioning perceptual\nepisode will have spatial content.","contact.mail":"rgrush@ucsd.edu","contact.domain":"ucsd.edu"}]
