[{"date.published":"2009-08-17","date.changed":"2020-04-21","url":"https://plato.stanford.edu/entries/mathematics-nondeductive/","author1":"Alan Baker","entry":"mathematics-nondeductive","body.text":"\n\n\nAs it stands, there is no single, well-defined philosophical subfield\ndevoted to the study of non-deductive methods in mathematics. As the\nterm is being used here, it incorporates a cluster of different\nphilosophical positions, approaches, and research programs whose\ncommon motivation is the view that (i) there are non-deductive aspects\nof mathematical methodology and that (ii) the identification and\nanalysis of these aspects has the potential to be philosophically\nfruitful.\n\nPhilosophical views concerning the ontology of mathematics run the\ngamut from platonism (mathematics is about a realm of abstract\nobjects), to fictionalism (mathematics is a fiction whose subject\nmatter does not exist), to formalism (mathematical statements are\nmeaningless strings manipulated according to formal rules), with no\nconsensus about which is correct. By contrast, it seems fair to say\nthat there is a philosophically established received view of the basic\nmethodology of mathematics. Roughly, it is that mathematicians aim to\nprove mathematical claims of various sorts, and that proof consists of\nthe logical derivation of a given claim from axioms. This view has a\nlong history; thus Descartes writes in his Rules for the Direction\nof the Mind (1627–28) that a mathematical proposition must\nbe “deduced from true and known principles by the continuous and\nuninterrupted action of a mind that has a clear vision of each step in\nthe process” (47). An important implication of this view is that\nthere is no room, at least ideally, in mathematics for non-deductive\nmethods. Frege, for example, states that “it is in the nature of\nmathematics always to prefer proof, where proof is possible, to any\nconfirmation by induction” (1884, 2). Berry (2016) offers a more\nrecent defense of proof as promoting key virtues of shared enquiry\nwithin the mathematical community. \nIn the philosophical literature, perhaps the most famous challenge to\nthis received view has come from Imre Lakatos, in his influential\n(posthumously published) 1976 book, Proofs and\nRefutations: \nBefore proceeding, it will be worthwhile to make a few distinctions in\norder to focus the topics of subsequent discussion. \nThe broad claim that there are some non-deductive aspects of\nmathematical activity seems relatively uncontroversial. For\nthis merely amounts to the claim that not everything that\nmathematicians do when they do mathematics consists of deriving\nstatements from other statements. As James Franklin puts it: \nOne way to narrow the general claim so as to make it more substantive\nis make use of the familiar (though not entirely unproblematic)\ndistinction between ‘context of discovery’ and\n‘context of justification’. On the one hand, this\ndistinction may allow the traditional deductivist view to be\nmaintained in the face of Lakatos’s critique, by arguing that\nwhat Lakatos is pointing to concerns the context of discovery in\nmathematics. Within the context of justification, derivation of\nresults from axioms may still be the correct and complete story. Some\nof the reactions of mathematicians to Lakatos’s views have this\ncharacter, for example the following remark by Morris Kline in a\nletter written to Lakatos: \nIt is also possible to find passages along similar lines in the work\nof Pólya, who was a major influence on Lakatos: \nConversely, in order to pose a genuine challenge to the familiar\ndeductivist position, the counterclaim needs to be that non-deductive\nmethods play a role in the justification of mathematical\nresults (Paseau 2015). It will therefore be primarily justificatory\ncontexts which will be focused on in the remainder of this\n survey.[2] \nThis is not the place for a detailed analysis of deduction. For\npresent purposes, this notion will be presumed to be fairly\nstraightforward, at least in principle. A deduction is any sequence of\nstatements each of which is derived from some initial set of\nstatements (the premises) or from a prior statement in the sequence.\nHowever, one issue that does need to be addressed is the relationship\nbetween deduction and formalization (see, e.g., Azzouni 2013). \nAn argument may be deductive without being formal. Although the\nparadigm cases of deduction do tend to occur in highly formalized\nsystems, this is not necessary. “All even numbers greater than 2\nare composite; 1058 is greater than 2; 1058 is even; hence 1058 is\ncomposite” is a perfectly good deduction, despite not being\nformalized. Hence, contrary to what is sometimes assumed in\ndiscussions of these issues, it is not true that all informal aspects\nof mathematical practice are thereby non-deductive. \nOn the other hand, the development of formal logic has been closely\nbound up with providing a clear language for presenting (and\nevaluating) deductive mathematical reasoning. Indeed, as John Burgess\nargues in his (1992), modern classical logic largely developed as a\nbasis for mathematical reasoning, especially proof. The increase in\nrigor within mathematics during the 19th Century is\nproperly viewed as a cause, not an effect, of the logical revolution\nset off by Frege’s work. Logic, in Burgess’s view, is\ndescriptive: its goal is to construct mathematical models of\nreasoning. Classical logic constitutes an idealized description of\nclassical mathematical proof. \nIt may also be important to distinguish informal elements of\na given mathematical proof from unformalizable elements (if\nthere are any such\n things).[3]\n In Section 4, this issue will be taken up in connection with the use\nof diagrams in mathematical reasoning. \nIn addition to the development of formal logic, another aspect of\ndeductivism is its emphasis on ‘foundations’. The reason\nfor this is that the passage from axioms to theorem is\nstraightforward, in principle, since it is a matter of logical\nderivation. Indeed there is nothing distinctively mathematical\ninvolved in this transition. Hence attention is shifted to the\nstarting point of the deductive process, namely the axioms. And if\nthese axioms are themselves theorems of some more basic theory, then\nthis pursuit of a secure starting point can be pursued down through a\nhierarchy of ever more foundational mathematical theories. \nIt is undeniable that issues in the foundations of mathematics have\nbeen the central preoccupation of philosophers of mathematics through\nmost of the 20th Century. This is not, of course, because\nfoundational areas such as set theory are the only areas of\nmathematics where philosophers think that deduction takes place, but\nrather because—as pointed out above—focusing on deduction\nputs particular emphasis on the starting points of proofs. Even those\nsympathetic with this focus on foundational issues are likely to\nacknowledge that many areas of mathematical practice are thereby\nignored. The question is what—if anything—of philosophical\ninterest is lost in the process. \nAs mentioned in 1.2 above, one feature of the deductivist style is\nthat paradigmatic mathematical proofs are expressed entirely in some\nappropriate formal language (for example, first-order predicate logic\nwith identity). This allows the validity of a given proof to be\neasily, indeed mechanically, ascertained. But of course few, if any,\nof the proofs circulated and published by mathematicians have this\nform. What qualifies as a proof for working mathematicians ranges from\nthe completely informal to the detailed and precise, with every (or\nalmost every) gap filled in. However, even detailed and precise proofs\nare rarely expressed purely in the language of logic; rather, they are\na mixture of ordinary language, mathematical, and logical symbols and\nterminology. \nSometimes philosophers writing in the deductivist tradition make it\nsound as if this is a fairly trivial point; it is just a matter of\nmathematicians having a ‘translation scheme’ to hand, but\nnot writing out the proof in pure logic to make it more accessible and\neasier to read. In point of fact, it is often far from obvious how to\ntranslate a given proof into formal logic. Furthermore, it is not\nclear that the notion of ‘translating’ an informal proof\ninto a formal language is necessarily the right way of looking at the\nsituation. Stewart Shapiro presents essentially this view at the\nbeginning of his 1991 book, Foundations Without\nFoundationalism, writing that: \nAn alternative picture is that the formal and informal languages offer\ndifferent ways of expressing mathematical theorems and proofs. The\nformal language is not used to ‘translate’, and hence does\nnot need to be measured against what is expressed in an informal\nproof. Rather it offers its own, arguably superior, resources for\nexpressing the content of mathematical statements in a precise and\nrigorous setting that has been specifically designed for this purpose.\nWhichever picture is adopted of the relation between formal and\ninformal presentations of mathematics, two points remain. First,\ndeductive mathematical arguments—arguments that are produced,\ntransmitted, and built upon by mathematicians—can be either\nformal or informal. Second, the evaluation of such arguments as being\ndeductively valid or invalid is easier to carry out definitively in\nthe context of a formal system of some sort. \nIt is also worth noting that Lakatos argues for a third category of\nproof, in addition to formal and informal, that he calls\n“quasi-formal”. Lakatos writes that: \nThe talk above of “every gap being filled in” in the\ntransition to an ideal proof glosses over the fact that the notion of\na “gap” in a proof is itself in need of further\nclarification. For one thing, the most straightforward way of defining\na proof gap—as given below—is only applicable to fully\nformal systems. \nThe reason for the condition that any rule be an explicitly stated\nrule of inference for the system is because we want to make room for\ngappy yet valid proofs. For example, “2 + 2 = 4, hence there are\ninfinitely many primes” is a valid argument, but clearly there\nis a large gap between its premise and its conclusion. On the other\nhand, despite the above definition only working for formal proofs,\ngaplessness and formality do not always go together. Thus a\ntraditional syllogism such as, “All men are mortal; Socrates is\na man; hence Socrates is mortal” is an example of a gapless\ninformal proof. One way to extend the notion of gappiness (and\ngaplessness) to informal proofs is via the notion of a basic\nmathematical inference, in other words an inference that is\n“accepted by the mathematical community as usable in proof\nwithout any further need of argument” (Fallis 2003, 49). \nHowever we end up characterizing gaps, it is undeniably the case that\nmost actual proofs as presented by mathematicians have gaps. Don\nFallis proposes a taxonomy of kinds of proof gaps in his (2003): \nIn addition to this taxonomical work, Fallis also argues for the\nphilosophical thesis that gaps in proofs are not necessarily a bad\nthing. Building on (iii) above, he introduces the notion of a\nuniversally untraversed gap, in other words a gap that has\nnot been bridged by any member of the mathematical community. Fallis\nclaims that such gaps are not unusual and that at least some of the\ntime proofs containing them are accepted by mathematicians in a\njustificatory context. This view is borne out in more recent work by\nAndersen (2018). \nOne currently active area of work that has led to the uncovering of\nhitherto unrecognized gaps of various sorts is automated proof\nchecking. Specially designed computer programs are used to check the\nvalidity of proofs that have been rendered in an appropriate formal\nlanguage. The main focus thus far has not been on discovering new\nresults but on checking the status of proofs of already established\nresults. George Gonthier has used this approach to verify a proof of\nthe four color theorem (Gonthier 2008) and a proof of the odd order\ntheorem in group theory (Gonthier et al. 2013), and Thomas Hales has\nverified a proof of the Jordan curve theorem (Hales 2007). In each\ncase, a number of gaps were found and then traversed. Formal\nverification of this sort can also reveal other information hidden in\nthe content of ordinary mathematical arguments. Georg Kreisel has\ndescribed this general process as “unwinding proofs”,\nwhile Ulrich Kohlenbach has more recently coined the term “proof\nmining.” In connection with the methods described above, Avigad\nwrites that \nDelariviere and Van Kerkhove (2017) point out, however, that while\ncomputer methods may play an increasingly important rule in proof\nverification, it is much less clear that such methods can play a\ncorrespondingly central role in advancing mathematical understanding.\n \nAnother aspect of informal proof that has been the subject of renewed\nattention in the recent philosophical literature is the role of\ndiagrams (Giaquinto 2007; Shin & Lemon 2008). What is not in\ndispute is that proofs—especially in geometry but also in other\nareas ranging from analysis to group theory—are often\naccompanied by diagrams. One issue concerns whether such diagrams play\nan indispensable role in the chain of reasoning leading from the\npremises of a given proof to its conclusion. Prima facie,\nthere would seem to be three possible situations: \nThe initial wave of philosophical work done on diagrammatic reasoning\nfocused on Euclid’s Elements, partly because of the\ncentrality and historical importance of this work, and partly because\nit is so often held up as a canonical example of the deductive method\n(see, e.g., Mumma 2010). If some or all of the diagrams in the\nElements fall under option (iii) above, then deleting all the\ndiagrams will render many of the proofs invalid. This raises the\nfurther question of whether a distinctively diagrammatic form of\nreasoning can be identified and analyzed, and—if\nso—whether it can be captured in a purely deductive system. One\ndifficulty for any proposed rigorization is the ‘generalization\nproblem’: how can a proof that is linked to a specific diagram\nbe generalized to other cases? This is intertwined with the issue of\ndistinguishing, in formal terms, between the essential and\ncoincidental features of a given diagram.  \nMore recent work on the role of diagrams in proofs has included a\ndefense of the position that diagrammatic proofs can sometimes be\nfully rigorous (Azzouni, 2013), and exploration of diagram-based\nreasoning in areas of mathematical practice other than geometry (de\nToffoli and Giardino, 2014; de Toffoli, 2017).  \nEven if we restrict attention to the context of justification, a\ndeductive proof yields categorical knowledge only if it proceeds from\na secure starting point and if the rules of inference are\ntruth-preserving. Can our confidence that these two conditions obtain\nalso be grounded purely deductively? These conditions will be\nconsidered in turn. \nIn one sense, it seems quite straightforward to give a deductive\njustification for some favored set of rules of inference. It can be\nshown, for example, that if the premises of an application of Modus\nPonens are true then the conclusion must also be true. The problem, at\nleast potentially, is that such justifications typically make use of\nthe very rule which they seek to justify. In the above case: if MP is\napplied to true premises then the conclusion is true; MP is applied to\ntrue premises; hence the conclusion is true. Haack (1976) and others\nhave debated whether the circularity here is vicious or not. One\ncrucial consideration is whether analogous\n‘justifications’ can be given for invalid rules, for\nexample Prior’s introduction and elimination rules for\n‘tonk,’ that also have this feature of using a rule to\njustify\n itself.[5]\n (A closely related issue can be traced back to Lewis Carroll and his\nclassic (1895) paper.) \nLet us assume, then, that an idealized deductive proof provides one\nkind of security: the transparency of each step ensures the validity\nof the argument as a whole, and hence guarantees that if the\npremises are all true then the conclusion must be true. But\nwhat of the axioms that are brought in at the beginning of the proof\nprocess? The traditional answer to this question is to claim that the\ntruth of the axioms is secure because the axioms are\n“self-evident”. This certainly seems to have been the\ngenerally accepted view of the axioms of Euclidean geometry, for\nexample. However, this attitude is much less prevalent in contemporary\nmathematics, for various reasons. Firstly, the discovery of\nnon-Euclidean geometry in the early 19th Century showed\nthat apparent self-evidence, at least in the case of the Parallel\nPostulate, is no guarantee of necessary truth. Secondly, the\nincreasing range and complexity of mathematical theories—and\ntheir axiomatizations—made it much less plausible to claim that\neach individual axiom was transparently true. Thirdly, many\nmathematical subfields have become abstracted to a considerable degree\nfrom any concrete models, and this has gone hand-in-hand with the\ntendency for at least some mathematicians to adopt a formalist\nattitude to the theories they develop. Rather than expressing\nfundamental truths, on this view axioms serve simply to provide the\nstarting position for a formal game. \nThe slide towards this sort of formalist attitude to axioms can also\nbe traced through Frege’s logicism. The logicist program sought\nto show that mathematics is reducible to logic, in other words that\nmathematical proofs can be shown to consist of logical deductions from\nlogically true premises. For Frege, these logically true premises are\ndefinitions of the terms which occur in them. But this again\nraises the issue of what distinguishes acceptable from unacceptable\ndefinitions. The worry here is not just whether our axioms are true\nbut whether they are even consistent (a pitfall which famously befell\nFrege’s own system). And this is a problem once self-evidence is\nabandoned as the ‘gold standard’ for axioms, whether we\nmove from here to a formalist view or a logicist view. In both cases,\nsome other bounds on the acceptability of candidate axioms must be\nprovided. \nIs there a middle ground, then, between the high standard of\nself-evidence on the one hand and the ‘anything goes’\nattitude on the other? One idea, a version of which can be traced back\nto Bertrand Russell, is to invoke a version of inference to the best\nexplanation. Russell’s view, plausibly enough, is that the\npropositions of elementary arithmetic—“2 + 2 = 4”,\n“7 is prime”, etc.—are much more self-evident than\nthe axioms of whatever logical or set-theoretic system one might come\nup with to ground them. So rather than viewing axioms as maximally\nself-evident we ought instead to think of them as being chosen on the\nbasis of their (collective) capacity to systematize, derive, and\nexplain the basic arithmetical facts. In other words, the direction of\nlogical implication remains from axioms to arithmetical facts, but the\ndirection of justification may go the other way, at least in the case\nof very simple, obvious arithmetical facts. Deriving “2 + 2 =\n4” from our set-theoretic axioms does not increase our\nconfidence in the truth of “2 + 2 = 4”, but the fact that\nwe can derive this antecedently known fact (and not derive other\npropositions which we know to be false) does increase our confidence\nin the truth of the axioms. \nThe direction of justification here mirrors the direction of\njustification in inference to the best explanation. Once we have a\nmeasure of confidence in a particular choice of axioms then the\ndirection of justification can also flow in the more conventional\ndirection, in step with the deductive inferences of a proof. This will\nhappen when the theorem proved was not one whose truth was\nantecedently obvious. Easwaran (2005), Mancosu (2008), and Schlimm\n(2013) have developed this basic account of axiom choice in different\nways. For example, Mancosu argues that an analogous process may\nunderlie the development of new mathematical theories that extend the\ndomain of application or the ontology of previous theories. Making\nfurther progress on analyzing this process will depend on giving a\nsatisfactory account of mathematical explanation, and this has become\nan area of considerable interest in the recent literature on\nphilosophy of mathematics. \nAnother approach, pursued by Maddy (1988, 1997, 2001, 2011) is to look\nin more detail at the actual practice of mathematicians and the\nreasons they give for accepting or rejecting different candidate\naxioms. Maddy’s main focus is on axioms for set theory, and she\nargues that there are various theoretical virtues, with no direct link\nto ‘self-evidence’, which axioms may possess. What these\nvirtues are, and how they are weighted relative to one another, may\nwell vary across different areas of mathematics. Two core virtues\nwhich Maddy identifies for set-theoretic axioms are UNIFY (i.e. that\nthey provide a single foundational theory for deciding set-theoretic\nquestions) and MAXIMIZE (i.e. that they not arbitrarily restrict the\nrange of isomorphism types). The issue of axiom choice in set theory\nhas also been taken up in recent work by Lingamneni (2017) and by\nFontanella (2019). \nUndoubtedly the most notorious of the limitations on the deductive\nmethod in mathematics are those which stem from Gödel’s\nincompleteness results. Although these results apply only to\nmathematical theories strong enough to embed arithmetic, the\ncentrality of the natural numbers (and their extensions into the\nrationals, reals, complexes, etc.) as a focus of mathematical activity\nmeans that the implications are widespread. \nNor should the precise implications of Gödel’s work be\noverstated. The order of the quantifiers is important. What Gödel\nshowed is that, for any consistent, recursively axiomatized formal\nsystem, F, strong enough for arithmetic, there are truths expressible\nin purely arithmetical language which are not provable in F. He did\nnot show that there are arithmetical truths which are unprovable in\nany formal system. Nonetheless, Gödel’s results\ndid hammer some significant nails into the coffin of one version of\nthe deductive ideal of mathematics. There cannot be a single,\nrecursively axiomatizable formal system for all of mathematics which\nis (a) consistent, (b) purely deductive, and (c) complete. One line of\nresponse to this predicament is to explore options for non-deductive\nmethods of justification in mathematics. \nThe role of non-deductive methods in empirical science is readily\napparent and relatively uncontroversial (pace Karl Popper).\nIndeed the canonical pattern of justification in science is a\nposteriori and inductive. What makes empirical science empirical\nis the crucial role played by observation, and—in\nparticular—by experiment. A natural starting point, therefore,\nin a survey of non-deductive methods in mathematics, is to look at the\nrise of a genre known as “experimental mathematics.” The\npast 15 years or so have seen the appearance of journals (e.g.,\nThe Journal of Experimental Mathematics), institutes (e.g.,\nthe Institute for Experimental Mathematics at the University of\nEssen), colloquia (e.g., the Experimental Mathematics Colloquium at\nRutgers University), and books (e.g., Borwein and Bailey 2003 and\n2004) devoted to this theme. These latter authors also argue, in\nBorwein and Bailey (2015), for the significance of experimental\nmathematics within mathematical practice more generally, while\nSorensen (2016) provides a broader historical and sociological\nanalysis of experimental mathematics.  \nAgainst the background of the traditional dichotomy between\nmathematical and empirical routes to knowledge, the very term\n“experimental mathematics” seems at best oxymoronic and at\nworst downright paradoxical. One natural suggestion is that\nexperimental mathematics involves performing mathematical\nexperiments, where the term “experiment” here is\nconstrued as literally as possible. This is the approach adopted by\nvan Bendegem (1998). According to van Bendegem, an experiment involves\n“the manipulation of objects, … setting up processes in\nthe ‘real’ world and … observing possible outcomes\nof these processes” (Van Bendegem 1998, 172). His suggestion is\nthat the natural way to get an initial grip on what a mathematical\nexperiment might be is to consider how an experiment in this\nparadigmatic sense might have mathematical ramifications. \nOne example that van Bendegem cites dates back to work done by the\n19th-century Belgian physicist Plateau on minimal surface\narea problems. By building various geometrical shapes out of wire and\ndipping these wire frames into a soap solution, Plateau was able to\nanswer specific questions about the minimum surface bounding various\nparticular shapes, and—eventually—to formulate some\ngeneral principles governing the configurations of such\n surfaces.[6]\n One way of understanding what is going on in this example is that a\nphysical experiment—the dipping of a wire frame into a soap\nsolution—is producing results that are directly relevant to a\ncertain class of mathematical problem. The main drawback of this way\nof characterizing experimental mathematics is that it is too\nrestrictive. Examples of the sort van Bendegem cites are extremely\nrare, hence the impact of mathematical experiments of this sort on\nactual mathematical practice can only be very limited at best.\nMoreover, it cannot be only this, literal sense of experiment that\nmathematicians have in mind when they talk about—and\ndo—experimental mathematics. \nSo much for the most literal reading of “mathematical\nexperiment.” A potentially more fruitful approach is to think in\nanalogical or functional terms. In other words, perhaps\n“experimental mathematics” is being used to label\nactivities which function within mathematics in a way analogous to the\nrole of experiment in empirical science. Thus mathematical experiments\nmay share some features with literal experiments, but not other\nfeatures (Baker 2008; McEvoy 2008, 2013; Sorensen 2010; van Kerkhove\n2008). Before proceeding with this line of analysis, it may be helpful\nto look briefly at a case study. \nA nice example of current work in experimental mathematics appears in\none of the two recent books by Borwein and Bailey (1995b, Ch. 4). A\nreal number is said to be normal in base n if every sequence\nof digits for base n (of any given length) occurs equally often in its\nbase-n expansion. A number is absolutely normal if it is\nnormal in every base. Consider the following hypothesis: \nBorwein and Bailey used a computer to compute to 10,000 decimal digits\nthe square roots and cube roots of the positive integers smaller than\n1,000, and then they subjected these data to certain statistical\ntests. \nThere are a couple of striking features of this example that may point\nto a more general characterization of experimental mathematics.\nFirstly, the route from evidence to hypothesis is via enumerative\ninduction. Secondly, it involves the use of computers. In what\nfollows, these two features will be examined in turn. \nIn a letter to Euler written in 1742, Christian Goldbach conjectured\nthat all even numbers greater than 2 are expressible as the sum of two\n primes.[7]\n Over the following two and a half centuries, mathematicians have been\nunable to prove Goldbach’s Conjecture. However it has been\nverified for many billions of examples, and there appears to be a\nconsensus among mathematicians that the conjecture is most likely\ntrue. Below is a partial list (as of October 2007) showing the order\nof magnitude up to which all even numbers have been checked and shown\nto conform to GC. \nDespite this vast accumulation of individual positive instances of GC,\naided since the early 1960s by the introduction—and subsequent\nrapid increases in speed—of the digital computer, no proof of GC\nhas yet been found. Not only this, but few number theorists are\noptimistic that there is any proof in the offing. Fields medalist Alan\nBaker stated in a 2000 interview: “It is unlikely that we will\nget any further [in proving GC] without a big breakthrough.\nUnfortunately there is no such big idea on the horizon.” Also in\n2000, publishers Faber and Faber offered a $1,000,000 prize to anyone\nwho proved GC between March 20 2000 and March 20 2002, confident that\ntheir money was relatively safe. \nWhat makes this situation especially interesting is that\nmathematicians have long been confident in the truth of GC. Hardy\n& Littlewood asserted, back in 1922, that “there is no\nreasonable doubt that the theorem is correct,” and Echeverria,\nin a recent survey article, writes that “the certainty of\nmathematicians about the truth of GC is complete” (Echeverria\n1996, 42). Moreover this confidence in the truth of GC is typically\nlinked explicitly to the inductive evidence: for instance, G.H. Hardy\ndescribed the numerical evidence supporting the truth of GC as\n“overwhelming.” Thus it seems reasonable to conclude that\nthe grounds for mathematicians’ belief in GC is the enumerative\ninductive evidence. \nOne distinctive feature of the mathematical case which may make a\ndifference to the justificatory power of enumerative induction is the\nimportance of order. The instances falling under a given mathematical\nhypothesis (at least in number theory) are intrinsically ordered, and\nfurthermore position in this order can make a crucial difference to\nthe mathematical properties involved. As Frege writes, with regard to\nmathematics: \nFrege then goes on to quote Leibniz, who argues that difference in\nmagnitude leads to all sorts of other relevant differences between the\nnumbers: \nFrege also explicitly compares the mathematical and non-mathematical\ncontexts for induction: \nAs Frege’s remarks suggest, one way to underpin an argument\nagainst the use of enumerative induction in mathematics is via some\nsort of non-uniformity principle: in the absence of proof, we\nshould not expect numbers (in general) to share any interesting\nproperties. Hence establishing that a property holds for some\nparticular number gives no reason to think that a second, arbitrarily\nchosen number will also have that\n property.[8]\n Rather than the Uniformity Principle which Hume suggests is the only\nway to ground induction, we have almost precisely the opposite\nprinciple! It would seem to follow from this principle that\nenumerative induction is unjustified, since we should not expect\n(finite) samples from the totality of natural numbers to be indicative\nof universal properties. \nA potentially even more serious problem, in the case of GC and in all\nother cases of induction in mathematics, is that the sample we are\nlooking at is biased. Note first that all known\ninstances of GC (and indeed all instances it is possible to know)\nare—in an important sense—small. \nOf course, it would be wrong to simply complain that all instances of\nGC are finite. After all, every number is finite, so if GC\nholds for all finite numbers than GC holds\n simpliciter.[9]\n But we can isolate a more extreme sense of smallness, which might be\ntermed minuteness. \nVerified instances of GC to date are not just small, they are minute.\nAnd minuteness, though admittedly rather vaguely defined, is known to\nmake a difference. Consider, for example, the logarithmic estimate of\nprime density (i.e. the proportion of numbers less than a given\nn that are prime) which is known to become an underestimate\nfor large enough n. Let n* be the first\nnumber for which the logarithmic estimate is too small. If the Riemann\nHypothesis is true, then it can be proven that an upper bound for\nn* (the first Skewes number) is 8 ×\n10370. Though an impressively large number, it is\nnonetheless minute according to the above definition. However if the\nRiemann Hypothesis is false than our best known upper bound for\nn* (the second Skewes number) is\n 10↑10↑10↑10↑3.[10]\n The necessity of inventing an ‘arrow’ notation here to\nrepresent this number tells us that it is not minute. The second part\nof this result, therefore, although admittedly conditional on a result\nthat is considered unlikely (viz. the falsity of RH), implies that\nthere is a property which holds of all minute numbers but does not\nhold for all numbers. Minuteness can make a difference. \nWhat about the seeming confidence that number theorists have in the\ntruth of GC? Echeverria (1996) discusses the important role played by\nCantor’s publication, in 1894, of a table of values of the\nGoldbach partition function, G(n), for n = 2 to\n1,000 (Echeverria 1996,29–30). The partition function measures\nthe number of distinct ways in which a given (even) number can be\nexpressed as the sum of two primes. Thus G(4) = 1, G(6) = 1, G(8) = 1,\nG(10) = 2, etc. This shift of focus onto the partition function\ncoincided with a dramatic increase in mathematicians’ confidence\nin GC. What became apparent from Cantor’s work is that\nG(n) tends to increase as n increases. Note that\nwhat GC amounts to in this context is that G(n) never takes\nthe value 0 (for any even n greater than 2). The overwhelming\nimpression made by data on the partition function is that it\nis highly unlikely for GC to fail for some large n. For\nexample, for numbers on the order of 100,000, there is always at least\n500 distinct ways to express each even number as the sum of two\nprimes! \nHowever, as it stands these results are purely heuristic. The thirty\nyears following Cantor’s publication of his table of values\n(described by Echeverria as the “2nd period” of\nresearch into GC) saw numerous attempts to find an analytic expression\nfor G(n). If this could be done then it would presumably be\ncomparatively straightforward to prove that this analytic function\nnever takes the value 0 (Echeverria 1996, 31). By around 1921,\npessimism about the chances of finding such an expression led to a\nchange of emphasis, and mathematicians started directing their\nattention to trying to find lower bounds for G(n). This too\nhas proved unsuccessful, at least to date. \nThus consideration of the partition function has not brought a proof\nof GC any closer. However it does allow us to give an interesting\ntwist to the argument of the previous section. The graph suggests that\nthe hardest test cases for GC are likely to occur among the smallest\nnumbers; hence the inductive sample for GC is biased, but it\nis biased against the chances of GC. Mathematicians’\nconfidence in the truth of GC is not based purely on enumerative\ninduction. The values taken by the partition function indicate that\nthe sample of positive instances of GC is indeed biased, and biased\nsamples do not—as a general rule—lend much support to an\nhypothesis. But in this particular case the nature of the bias makes\nthe evidence stronger, not weaker. So it is possible to argue that\nenumerative induction is unjustified while simultaneously agreeing\nthat mathematicians are rational to believe GC on the basis of the\navailable evidence. (Note that there is a delicate balance to maintain\nhere because evidence for the behavior of the partition function is\nitself non-deductive. However the impression that G(n) is\nlikely to be bounded below by some increasing analytic function is not\nbased on enumerative induction per se, so the\njustification—while non-deductive—is not circular.) \nThe upshot of the above discussion, albeit based on a single case\nstudy, is that mathematicians ought not to—and in general do\nnot—give weight to enumerative induction per se in the\njustification of mathematical claims. (To what extent enumerative\ninduction plays a role in the discovery of new hypotheses, or in the\nchoice of what open problems mathematicians decide to work on, is a\nseparate issue which has not been addressed here.) More precisely, the\nthesis is in two parts: \nA striking feature of contemporary work in experimental mathematics is\nthat it is done using computers. Is this reliance on\ncomplex pieces of electronics what makes the field\n‘experimental’? If one looks at what gets published in\ncontemporary journals, books, and conferences devoted to experimental\nmathematics, the impression is that all the items are closely bound up\nwith computers. For example, there does not appear to be a single\npaper published in more than a decade’s worth of issues of\nExperimental Mathematics that does not involve the use of\ncomputers. What about the kinds of examples which mathematicians tend\nto offer as paradigms of experimental mathematics? Here the data is\nless clear. On the one hand, an informal survey suggests that the\nmajority of such exemplars do involve the explicit use of computers.\nOn the other hand, it is not uncommon for mathematicians also to cite\none or more historical examples, from well before the computer age, to\nillustrate the purported pedigree of the subdiscipline. \nThe biggest practice-based challenge to equating experimental\nmathematics with computer-based mathematics comes from what\nself-styled experimental mathematicians say about their nascent\ndiscipline. For when mathematicians self-consciously reflect on the\nnotion of experimental mathematics, they tend to reject the claim that\ncomputer use is a necessary feature. For example, the editors of the\njournal Experimental Mathematics—in their\n“statement of philosophy” concerning the scope and nature\nof the journal—make the following remarks: \nAnd here is another passage with a similar flavor from mathematician\nDoron Zeilberger: \nIt seems fair to say that tying experimental mathematics to computer\nuse fits well with what contemporary experimental mathematicians do\nbut not so well with what they\n say.[11] \nA second problem with the proposed characterization is more\nphilosophical in nature. Consider another widely-cited example of\nexperimental mathematics which arises in connection with\nGoldbach’s Conjecture. As of April 2007, all even numbers up to\n1018 had been verified to conform to GC, and this project\n(under the direction of Oliveira e Silva) is ongoing. This massive\ncomputation task is generally considered to be a paradigm example of\nexperimental mathematics. And it seems clear that computers are\nplaying an essential role here: no mathematician, or group of\nmathematicians, could hope to duplicate 1018 calculations\nby hand. \nIn the current context, the central question is not whether\ncomputer-based mathematics is ‘experimental’ but whether\nit is—at least sometimes—non-deductive. In one\nsense, of course, all of the individual calculations performed by a\ncomputer are deductive, or at least they are isomorphic to the\noperations of a purely deductive formal system. When a computer\nverifies an instance of GC, this verification is completely deductive.\nWe may then separate out two distinct questions. Firstly, are these\ncomputations playing a non-deductive role in some larger mathematical\nargument? And, secondly, are the beliefs we form directly from the\nresults of computer computations deductively grounded beliefs? The\nfirst of these questions does not turn on anything specific to\ncomputers, and hence collapses back to the issue discussed in Section\n3(B) above on enumerative induction. The second question will be\nexamined below. \nPhilosophical discussion of the status of computer proofs was prompted\nin large part by Appel and Haken’s computer-based proof of the\nFour Color Theorem in 1976. In his (1979), Tymoczko\nargues—controversially—that mathematical knowledge based\non computer proofs is essentially empirical in character. This is\nbecause such proofs are not a priori, not certain, not\nsurveyable, and not checkable by human mathematicians. In all these\nrespects, according to Tymoczko, computer proofs are unlike\ntraditional ‘pencil-and-paper’ proofs. Concerning\nsurveyability, Tymoczko writes: \nAssume for sake of argument that the computer proof in question is\ndeductively correct but is also unsurveyable in the above sense. Does\nour decision to rely on the output of the computer here constitute a\nnon-deductive method? One way of viewing this kind of example\nis as driving a wedge between a deductive method and our non-deductive\naccess to the results of that method. Compare, for instance,\nbeing told of a particular mathematical result by an expert\nmathematician (with a good track record). Is this a\n‘non-deductive\n method’?[12] \nThere is a small, but growing, subset of mathematical methods which\nare essentially probabilistic in nature. In the context of\njustification, these methods do not deductively imply their conclusion\nbut rather establish that there is some (often precisely specifiable)\nhigh probability that the conclusion is true. Philosophical discussion\nof these methods began with Fallis (1997, 2002), while Berry (2019) is\na useful recent contribution to the debate.  \nOne type of probabilistic method links back to the earlier discussion\nof experimental mathematics in that it involves performing experiments\nin a quite literal sense. The idea is to harness the processing power\nof DNA to effectively create a massively parallel computer for solving\ncertain otherwise intractable combinatorial problems. The most famous\nof these is the ‘Traveling Salesman’ problem, which\ninvolves determining whether there is some possible route through the\nnodes of a graph connected by unidirectional arrows that visits each\nnode exactly once. Adleman (1994) shows how the problem can be coded\nusing strands of DNA which can then be spliced and recombined using\ndifferent chemical reactions. The appearance of certain longer DNA\nstrands at the end of the process corresponds to the finding of a\nsolution path through the graph. Probabilistic considerations come in\nmost clearly in the case where no longer DNA strands are found. This\nindicates that there is no path through the graph, but even if the\nexperiment is carried out correctly the support here falls short of\nfull certainty. For there is a small chance that there is a solution\nbut that it fails to be coded by any DNA strand at the start of the\nexperiment. \nThere are also probabilistic methods in mathematics which are not\nexperimental in the above sense. For example, there are properties of\ncomposite (i.e. non-prime) numbers which can be shown to hold in\nrelation to about half of the numbers less than a given composite\nnumber. If various numbers smaller than N are selected at\nrandom and none of them bear this relation to N, then it\nfollows that N is almost certainly prime. The level of\nprobability here can be precisely calculated, and can be made as high\nas needed by picking more ‘witness’ numbers to test. \nNote that these sorts of probabilistic methods contain plenty of\npurely deductive reasoning. Indeed, in the second example, the fact\nthat the probability of N being prime is .99 is established\npurely deductively. Nonetheless, there is general consensus in the\nmathematical community that such methods are not acceptable\nsubstitutes for deductive proof of the conclusion. Fallis (1997, 2002)\nargues that this rejection is not reasonable because any property of\nprobabilistic methods that can be pointed to as being problematic is\nshared by some proofs that the mathematical community does accept.\nFallis’s focus is on establishing truth as the key epistemic\ngoal of mathematics. However it seems plausible that one major reason\nfor mathematicians’ dissatisfaction with probabilistic methods\nis that they do not explain why their conclusions are true.\nIn addition, Easwaran argues, against Fallis, that there is a\nproperty, which he terms ‘transferability’, that\nprobabilistic proofs lack and acceptable proofs have (Easwaran 2009;\nJackson 2009). Fallis (2011) is a reply to some of these\nobjections. \nOn the other hand, there may be cases where the bare truth or falsity\nof a claim is important even in the absence of accompanying\nexplanation. For example, one could imagine a situation in which an\nimportant and interesting conjecture—say the Riemann\nHypothesis—is being considered, and a probabilistic method is\nused to show that some number is very likely a counterexample to it.\nIt is interesting to speculate what the reaction of the mathematical\ncommunity might be to this situation. Would work on trying to prove RH\ncease? Would it continue until a rigorous deductive proof of the\ncounterexample is constructed? \nIt is not clear why one should expect the various non-deductive\nmethods used in mathematics to share any substantive features other\nthan their non-deductiveness. Philosophers looking at the role of\nnon-deductive reasoning in the context of discovery have often talked\nas if there is some unity to be found (for example, the subtitle to\nLakatos’s Proofs and Refutations is “the Logic of\nMathematical Discovery.” More likely is that the array of\nnon-deductive methods is diverse and heterogeneous. (Compare Stanislaw\nUlam’s remark that “the study of non-linear physics is\nlike the study of non-elephant biology.”) \nWork by contemporary philosophers of mathematics is continuing to push\nthe study of non-deductive mathematical methods in new directions. One\narea of interest is in ‘mathematical natural kinds’ and\nwhether such a notion can be used to ground the use of analogy in\nmathematical reasoning (Corfield 2004 [Other Internet Resources]).\nAnother area being investigated is the putative role of heuristic\nprinciples in mathematics. (Much of this work traces back to\nPólya (1945).) \nA background issue in all of these debates concerns the extent to\nwhich each particular non-deductive method plays an essential\nrole in the justificatory practices of mathematics. This question\narises at both a local and global level. At the local level, a\nparticular piece of reasoning to justify a given result may be\nunavoidably non-deductive, yet the result may also be established by\nsome other, purely deductive piece of reasoning. At the global level,\nit may be that our only justification for certain mathematical claims\nis non-deductive. The extent to which our use of non-deductive methods\nis due to limitations in practice rather than limitations in principle\nremains an issue for further investigation.","contact.mail":"abaker1@swarthmore.edu","contact.domain":"swarthmore.edu"}]
