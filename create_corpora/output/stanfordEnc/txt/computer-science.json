[{"date.published":"2013-08-20","date.changed":"2021-01-19","url":"https://plato.stanford.edu/entries/computer-science/","author1":"Nicola Angius","author2":"Giuseppe Primiero","author1.info":"https://uniss.academia.edu/NicolaAngius","author2.info":"http://cswww.essex.ac.uk/staff/turnr/","entry":"computer-science","body.text":"\n\n\nThe philosophy of computer science is concerned with the ontological\nand methodological issues arising from within the academic discipline\nof computer science, and from the practice of software development and\nits commercial and industrial deployment. More specifically, the\nphilosophy of computer science considers the ontology and epistemology\nof computational systems, focusing on problems associated with their\nspecification, programming, implementation, verification and testing.\nThe complex nature of computer programs ensures that many of the\nconceptual questions raised by the philosophy of computer science have\nrelated ones in the\n philosophy of mathematics,\n the philosophy of empirical sciences, and the\n philosophy of technology.\n We shall provide an analysis of such topics that reflects the layered\nnature of the ontology of computational systems in Sections 1–5;\nwe then discuss topics involved in their methodology in Sections\n6–8.\n\nComputational systems are widespread in everyday life. Their design,\ndevelopment and analysis are the proper object of study of the\ndiscipline of computer science. The philosophy of computer science\ntreats them instead as objects of theoretical analysis. Its first aim\nis to define such systems, i.e., to develop an ontology of\ncomputational systems. The literature offers two main approaches on\nthe topic. A first one understands computational systems as defined by\ndistinct ontologies for software and hardware, usually taken to be\ntheir elementary components. A different approach sees computational\nsystems as comprising several other elements around the\nsoftware-hardware dichotomy: under this second view, computational\nsystems are defined on the basis of a hierarchy of levels of\nabstraction, arranging hardware levels at the bottom of such a\nhierarchy and extending upwards to elements of the design and\ndownwards to include the user. In the following we present these two\napproaches. \nUsually, computational systems are seen as composed of two\nontologically distinct entities: software and hardware. Algorithms,\nsource codes, and programs fall in the first category of abstract\nentities; microprocessors, hard drives, and computing machines are\nconcrete, physical entities. \nMoore (1978) argues that such a duality is one of the three myths of\ncomputer science, in that the dichotomy software/hardware has a\npragmatic, but not an ontological, significance. Computer programs, as\nthe set of instructions a computer may execute, can be examined both\nat the symbolic level, as encoded instructions, and at the physical\nlevel, as the set of instructions stored in a physical medium. Moore\nstresses that no program exists as a pure abstract entity, that is,\nwithout a physical realization (a flash drive, a hard disk on a\nserver, or even a piece of paper). Early programs were even hardwired\ndirectly and, at the beginning of the computer era, programs consisted\nonly in patterns of physical levers. By the software/hardware\nopposition, one usually identifies software with the symbolic level of\nprograms, and hardware with the corresponding physical level. The\ndistinction, however, can be only pragmatically justified in that it\ndelimits the different tasks of developers. For them, software may be\ngiven by algorithms and the source code implementing them, while\nhardware is given by machine code and the microprocessors able to\nexecute it. By contrast, engineers realizing circuits implementing\nhardwired programs may be inclined to call software many physical\nparts of a computing machine. In other words, what counts as software\nfor one professional may count as hardware for another one. \nSuber (1988) goes even further, maintaining that hardware is a kind of\nsoftware. Software is defined as any pattern that is amenable to being\nread and executed: once one realizes that all physical objects display\npatterns, one is forced to accept the conclusion that hardware, as a\nphysical object, is also software. Suber defines a pattern as\n“any definite structure, not in the narrow sense that requires\nsome recurrence, regularity, or symmetry” (1988, 90) and argues\nthat any such structure can indeed be read and executed: for any\ndefinite pattern to which no meaning is associated, it is always\npossible to conceive a syntax and a semantics giving a meaning,\nthereby making the pattern an executable program. \nColburn (1999, 2000), while keeping software and hardware apart,\nstresses that the former has a dual nature, it is a “concrete\nabstraction” as being both abstract and concrete. To define\nsoftware, one needs to make reference to both a “medium of\ndescription”, i.e., the language used to express an algorithm,\nand a “medium of execution”, namely the circuits composing\nthe hardware. While software is always concrete in that there is no\nsoftware without a concretization in some physical medium, it is\nnonetheless abstract, because programmers do not consider the\nimplementing machines in their activities: they would rather develop a\nprogram executable by any machine. This aspect is called by Colburn\n(1999) “enlargement of content” and it defines abstraction\nin computer science as an “abstraction of content”:\ncontent is enlarged rather than deleted, as it happens with\nmathematical abstraction. \nIrmak (2012) criticizes the dual nature of software proposed by\nColburn (1999, 2000). He understands an abstract entity as one lacking\nspatio-temporal properties, while being concrete means having those\nproperties. Defining software as a concrete abstraction would\ntherefore imply for software to have contradictory properties.\nSoftware does have temporal properties: as an object of human\ncreation, it starts to exist at some time once conceived and\nimplemented; and it can cease to exist at a certain subsequent time.\nSoftware ceases to exist when all copies are destroyed, their authors\ndie and nobody else remembers the respective algorithms. As an object\nof human creation, software is an artifact. However, software lacks\nspatial properties in that it cannot be identified with any concrete\nrealization of it. Destroying all the physical copies of a given\nsoftware would not imply that a particular software ceases to exist,\nas stated above, nor, for the very same reason, would deleting all\ntexts implementing the software algorithms in some high-level\nlanguage. Software is thus an abstract entity endowed with temporal\nproperties. For these reasons, Irmak (2010) definies software as an\nabstract artifact. \nDuncan (2011) points out that distinguishing software from hardware\nrequires a finer ontology than the one involving the simple\nabstract/concrete dichotomy. Duncan (2017) aims at providing such an\nontology by focusing on Turner’s (2011) notion of specification\nas an expression that gives correctness conditions for a program (see\n §2).\n Duncan (2017) stresses that a program acts also as a specification\nfor the implementing machine, meaning that a program specifies all\ncorrect behaviors that the machine is required to perform. If the\nmachine does not act consistently with the program, the machine is\nsaid to malfunction, in the same way a program which is not correct\nwith respect to its specification is said to be flawed or containing a\nbug. Another ontological category necessary to define the distinction\nsoftware/hardware is that of artifact, which Duncan (2017) defines as\na physical, spatio-temporal entity, which has been constructed so as\nto fulfill some functions and such that there is a community\nrecognizing the artifact as serving that purpose. That said, software\nis defined as a set of instructions encoded in some programming\nlanguage which act as specifications for an artifact able to read\nthose instructions; hardware is defined as an artifact whose function\nis to carry out the specified computation. \nAs shown above, the distinction between software and hardware is not a\nsharp one. A different ontological approach to computational systems\nrelies on the role of abstraction. Abstraction is a crucial element in\ncomputer science, and it takes many different forms. Goguen &\nBurstall (1985) describe some of this variety, of which the following\nexamples are instances. Code can be repeated during programming, by\nnaming text and a parameter, a practice known as procedural\nabstraction. This operation has its formal basis in the abstraction\noperation of the lambda calculus (see the entry on the\n lambda calculus)\n and it allows a formal mechanism known as polymorphism (Hankin 2004).\nAnother example is typing, typical of functional programming, which\nprovides an expressive system of representation for the syntactic\nconstructors of the language. Or else, in object-oriented design,\npatterns (Gamma et al. 1994) are abstracted from the common structures\nthat are found in software systems and used as interfaces between the\nimplementation of an object and its specification. \nAll these examples share an underlying methodology in the Levels of\nAbstraction (henceforth LoA), used also in mathematics (Mitchelmore\nand White 2004) and philosophy (Floridi 2008). Abstractions in\nmathematics are piled upon each other in a never-ending search for\nmore and more abstract concepts. On this account, abstraction is\nself-contained: an abstract mathematical object takes its meaning only\nfrom the system within which it is defined and the only constraint is\nthat new objects be related to each other in a consistent system that\ncan be operated on without reference to previous or external meanings.\nSome argue that, in this respect at least, abstraction in computer\nscience is fundamentally different from abstraction in mathematics:\ncomputational abstraction must leave behind an implementation trace\nand this means that information is hidden but not destroyed (Colburn\n& Shute 2007). Any details that are ignored at one LoA must not be\nignored by one of the lower LoAs: for example, programmers need not\nworry about the precise location in memory associated with a\nparticular variable, but the virtual machine is required to handle all\nmemory allocations. This reliance of abstraction on different levels\nis reflected in the property of computational systems to depend upon\nthe existence of an implementation: for example, even though classes\nhide details of their methods, they must have implementations. Hence,\ncomputational abstractions preserve both an abstract guise and an\nimplementation. \nA full formulation of LoAs for the ontology of digital computational\nsystems has been devised in Primiero (2016), including: \nIntention is the cognitive act that defines a computational\nproblem to be solved: it formulates the request to create a\ncomputational process to perform a certain task. Requests of this sort\nare usually provided by customers, users, and other stakeholders\ninvolved in a given software development project.\nSpecification is the formulation of the set of requirements\nnecessary for solving the computational problem at hand: it concerns\nthe possibly formal determination of the operations the software must\nperform, through the process known as requirements elicitation.\nAlgorithm expresses the procedure providing a solution to the\nproposed computational problem, one which must meet the requirements\nof the specification. High-level programming language (such\nas C, Java, or Python) instructions constitute the linguistic\nimplementation of the proposed algorithm, often called the source\ncode, and they can be understood by trained programmers but cannot be\ndirectly executed by a machine. The instructions coded in high-level\nlanguage are compiled, i.e., translated, by a compiler into\nassembly code and then assembled in machine code\noperations, executable by a processor. Finally, the\nexecution LoA is the physical level of the running software,\ni.e., of the computer architecture executing the instructions. \nAccording to this view, no LoA taken in isolation is able to define\nwhat a computational system is, nor to determine how to distinguish\nsoftware from hardware. Computational systems are rather defined by\nthe whole abstraction hierarchy; each LoA in itself expresses a\nsemantic level associated with a realization, either linguistic or\nphysical. \nIntention refers to a cognitive state outside the computational system\nwhich expresses the formulation of a computational problem to be\nsolved. Specifications describe the functions that the\ncomputational system to be developed must fulfil. Whereas\nintentions, per se, do not pose specific\nphilosophical controversies inside the philosophy of computer\nscience, issues arise in connection with the definition of what a\nspecification is and its relation with intentions. \nIntentions articulate the criteria to determine whether a\ncomputational system is appropriate (i.e., correct, see\n §7),\n and therefore it is considered as the first LoA of the computational\nsystem appropriate to that problem. For instance, customers and users\nmay require a smartphone app able to filter out annoying calls from\ncall centers; such request constitutes the intention LoA in the\ndevelopment of a computational system able to perform such a task. In\nthe software development process of non-naive systems, intentions are\nusually gathered by such techniques as brainstorming, surveys,\nprototyping, and even focus groups (Clarke and Moreira 1999), aimed at\ndefining a structured set of the various stakeholders’\nintentions. At this LoA, no reference is made to how to solve\nthe computational problem, but only the description of the problem\nthat must be solved is provided. \nIn contemporary literature, intentions have been the object of\nphilosophical inquiry at least since Anscombe (1963). Philosophers\nhave investigated “intentions with which” an action is\nperformed (Davidson 1963), intentions of doing something in the future\n(Davidson 1978), and intentional actions (Anscombe 1963, Baier 1970,\nFerrero 2017). Issues arise concerning which of the three kinds of\nintention is primary, how they are connected, the relation between\nintentions and belief, whether intentions are or presuppose specific\nmental states, and whether intentions act as causes of actions (see\nthe entry on\n intention).\n More formal problems concern the opportunity for an agent of having\ninconsistent intentions and yet being considered rational (Bratman\n1987, Duijf et al. 2019). \nIn their role as the first LoA in the ontology of computational\nsystems, intentions can certainly be acknowledged as intentions for\nthe future, in that they express the objective of constructing systems\nable to perform some desired computational tasks. Since intentions, as\nstated above, confine themselves to the definition of the\ncomputational problem to be solved, without specifying its\ncomputational solution, their ontological and epistemological analysis\ndoes not differ from those referred to in the philosophical\nliterature. In other words, there is nothing specifically\ncomputational in the intentions defining computational systems which\ndeserves a separate treatment in the philosophy of computer science.\nWhat matters here is the relation between intention and specification,\nin that intentions provide correctness criteria for specifications;\nspecifications are asked to express how the computational problem put\nforward by intentions is to be solved. \nConsider the example of the call filtering app again; a\nspecification may require to create a black-list of phone numbers\nassociated with call centers; to update the list every n\ndays; to check, upon an incoming call, whether the number is on the\nblack-list; to communicate to the call management system not to\nallow the incoming call in case of an affirmative answer, and to allow\nthe call in case of negative answer. \nThe latter is a full-fledged specification, though expressed in a\nnatural language. Specifications are often advanced in a natural\nlanguage to be closer to the stakeaholder’s intentions and only\nsubsequently they are formalized in a proper formal language.\nSpecifications may be expressed by means of graphical languages such\nas UML (Fowler 2003), or more formal languages such as TPL (Turner\n2009a) and VDM (Jones 1990), using predicate logic, or Z (Woodcock and\nDavies 1996), focusing on set theory. For instance, Type Predicate\nLogic (TPL) expresses the requirements of computational systems using\npredicate logic formulas, wherein the type of the quantified variables\nis specified. The choice of the variable types allows one to define\nspecifications at the more appropriate abstraction level. Whether\nspecifications are expressed in an informal or formal guise often\ndepends on the development method followed, with formal specifications\nusually preferred in the context of formal development methods.\nMoreover, formal specifications facilitate verification of correctness\nfor computational systems (see\n §6). \nTurner (2018) asks what difference is there between models and\nspecifications, both of which are extensively used in computer\nscience. The difference is located in what Turner (2011) calls the\nintentional stance: models describe an intended\nsystem to be developed and, in case of a mismatch between the two, the\nmodels are to be refined; specifications prescribe how the\nsystem is to be built so as to comply with the intended functions, and\nin case of mismatch it is the system that needs to be refined.\nMatching between model and system reflects a correspondence between\nintentions — describing what system is to be\nconstructed in terms of the computational problem the system must be\nable to solve — and specifications — determining\nhow the system is to be constructed, in terms of the set of\nrequirements necessary for solving the computational problem, as\nexemplified for the call filtering app. In Turner’s (2011)\nwords, “something is a specification when it is given\ncorrectness jurisdiction over an artefact”: specifications\nprovide correctness criteria for computational systems. Computational\nsystems are thus correct when they comply with their specifications,\nthat is, when they behave according to them. Conversely, they provide\ncriteria of malfunctioning\n (§7.3):\n a computational system malfunctions when it does not behave\nconsistently with its specifications. Turner (2011) is careful to\nnotice that such a definition of specifications is an idealization:\nspecifications are themselves revised in some cases, such as when the\nspecified computational systems cannot be realized because of physical\nlaws constraints or cost limitations, or when it turns out that the\nadvanced specifications are not correct formalizations of the\nintentions of clients and users. \nMore generally, the correctness problem does not only deal with\nspecifications, but with any two LoAs defining computational systems,\nas the next subsection will examine. \nFully implemented and constructed computational systems are\ntechnical artifacts, i.e., human-made systems designed and\nimplemented with the explicit aim of fulfilling specific functions\n(Kroes 2012). Technical artifacts so defined include tables,\nscrewdrivers, cars, bridges, or televisions, and they are distinct\nboth from natural objects (e.g. rocks, cats, or dihydrogen monoxide\nmolecules), which are not human-made, and artworks, which do not\nfulfill functions. As such, the ontology of computational systems\nfalls under that of technical artifacts (Meijers 2000) characterized\nby a duality, as they are defined by both functional\nand structural properties (Kroes 2009, see also the entry on\n philosophy of technology).\n Functional properties specify the functions the artifact is required\nto perform; structural properties express the physical properties\nthrough which the artifact can perform them. Consider a screwdriver:\nfunctional properties may include the function of screwing and\nunscrewing; structural properties can refer to a piece of metal\ncapable of being inserted on the head of the screw and a plastic\nhandle that allows a clockwise and anticlockwise motion. Functions can\nbe realized in multiple ways by their structural counterparts. For\ninstance, the function for the screwdriver could well be realized by a\nfull metal screwdriver, or by an electric screwdriver defined by very\ndifferent structural properties. \nThe layered ontology of computational systems characterized by many\ndifferent LoAs seems to extend the dual ontology defining technical\nartifacts (Floridi et al. 2015). Turner (2018) argues that\ncomputational systems are still artifacts in the sense of (Kroes 2009,\n2012), as each LoA is a functional level for lower LoAs and a\nstructural level for upper LoAs: \nIt follows, according to Turner (2018), that structural levels need\nnot be necessarily physical levels, and that the notion of abstract\nartifact holds in computer science. For this reason, Turner (2011)\ncomes to define high-level language programs themselves as technical\nartifacts, in that they constitute a structural level implementing\nspecifications as their functional level\n (see §4.2). \nA first consequence is that each LoA – expressing\nwhat function to accomplish – can be realized by a\nmultiplicity of potential structural levels expressing how\nthose functions are accomplished: an intended functionality can be\nrealized by a specification in multiple ways; a computational problem\nexpressed by a specification has solutions by a multiplicity of\ndifferent algorithms, which can differ for some important properties\nbut are all equally valid (see\n §3);\n an algorithm may be implemented in different programs, each written\nin a different high-level programming language, all expressing the\nsame program if they implement the same algorithm (Angius and\nPrimiero 2019); source code can be compiled in a multiplicity of\nmachine languages, adopting different ISAs (Instruction Set\nArchitectures); executable code can be installed and run on a\nmultiplicity of machines (provided that these share the same ISA). \nA second consequence is that each LoA as a functional level provides\ncorrectness criteria for lower levels (Primiero 2020). Not just at the\nimplementation level, correctness is required at any LoA from\nspecification to execution, and the cause of malfunctions may be\nlocated at any LoA not correctly implementing its proper functional\nlevel (see\n §7.3 and\n Fresco, Primiero (2013)). According to Turner (2018), the\nspecification level can be said to be correct or incorrect with\nrespect to intentions, despite the difficulty of verifying their\ncorrectness. Correctness of any non-physical layer can be verified\nmathematically through formal verification, and the execution physical\nlevel can be verified empirically, through testing\n (§6).\n Verifying correctness of specifications with respect to\nclients’ intentions would require instead having access to the\nmental states of the involved agents. \nThis latter problem relates to the more general one of establishing\nhow artifacts possess functions, and what it means that structural\nproperties are related to the intentions of agents. The problem is\nwell-known also in the philosophy of biology and the cognitive\nsciences, and two main theories have been put forward as solutions.\nAccording to the causal theory of function (Cummins 1975),\nfunctions are determined by the physical capacities of artifacts: for\nexample, the physical ability of the heart of contracting and\nexpanding determines its function of pumping blood in the circulatory\nsystem. However, this theory faces serious problems when applied to\ntechnical artifacts. First, it prevents defining correctness and\nmalfunctioning (Kroes 2010): suppose the call filtering app installed\non our smartphone starts banning calls from contacts in our mobile\nphonebook; according to the causal theory of function this would be a\nnew function of the app. Second, the theory does not distinguish\nintended functions from side effects (Turner 2011): in case of a\nlong-lasting call, our smartphone would certainly start heating;\nhowever, this is not a function intended by clients or developers.\nAccording to the intentional theory of function (McLaughlin\n2001, Searle 1995), the function fixed by the designer or the user is\nthe intended one of the artifact, and structural properties of\nartifacts are selected so as to be able to fulfill it. This theory is\nable to explain correctness and malfunction, as well as to distinguish\nside effects from intended functions. However, it does not say where\nthe function actually resides, whether in the artifact or in the mind\nof the agent. In the former case, one is back at the question of how\nartifacts possess functions. In the latter case, a further explanation\nis needed about how mental states are related to physical properties\nof artifacts (Kroes 2010). Turner (2018) holds that the intuitions\nbehind both the causal and the intentional theories of function are\nuseful to understand the relation between function and structure in\ncomputational systems, and suggests that the two theories be combined\ninto a single one. On the one hand, there is no function without\nimplementation; on the other hand, there is no intention without\nclients, developers, and users. \nEven though known and widely used since antiquity, the problem of\ndefining what algorithms are is still open (Vardi 2012). The word\n“algorithm” originates from the name of the\nninth-century Persian mathematician Abū Jaʿfar\nMuḥammad ibn Mūsā al-Khwārizmī, who\nprovided rules for arithmetic operations using Arabic numerals.\nIndeed, the rules one follows to compute basic arithmetic operations\nsuch as multiplication or division, are everyday examples of\nalgorithms. Other well-known examples include rules to bisect an angle\nusing compass and straightedge, or Euclid’s algorithm for\ncalculating the greatest common divisor. Intuitively, an algorithm is\na set of instructions allowing the fulfillment of a given task.\nDespite this ancient tradition in mathematics, only modern logical and\nphilosophical reflection put forward the task of providing a\ndefinition of what an algorithm is, in connection with the\nfoundational crisis of mathematics of the early twentieth century (see\nthe entry on the\n philosophy of mathematics).\n The notion of effective calculability arose from logical\nresearch, providing some formal counterpart to the intuitive notion of\nalgorithm and giving birth to the theory of computation. Since then,\ndifferent definitions of algorithms have been proposed,\nranging from formal to non-formal approaches, as sketched in the\nnext sections. \nMarkov (1954) provides a first precise definition of algorithm as a\ncomputational process that is determined,\napplicable, and effective. A computational process\nis determined if the instructions involved are precise enough\nnot to allow for any “arbitrary choice” in their\nexecution. The (human or artificial) computer must never be\nunsure about what step to carry out next. Algorithms are\napplicable for Markov in that they hold for classes of inputs\n(natural numbers for basic arithmetic operations) rather than for\nsingle inputs (specific natural numbers). Markov (1954:1) defines\neffectiveness as “the tendency of the algorithm to\nobtain a certain result”. In other words, an algorithm is\neffective in that it will eventually produce the answer to the\ncomputational problem. \nKleene (1967) specifies finiteness as a further important\nproperty: an algorithm is a procedure which can be described by means\nof a finite set of instructions and needs a finite number of steps to\nprovide an answer to the computational problem. As a counterexample,\nconsider a while loop defined by a finite number of steps,\nbut which runs forever since the condition in the loop is always\nsatisfied. Instructions should also be amenable to mechanical\nexecution, that is, no insight is required for the machine to follow\nthem. Following Markov’s determinability and strengthening\neffectiveness, Kleene (1967) additionally specifies that instructions\nshould be able to recognize that the solution to the computational\nproblem has been achieved, and halt the computation. \nKnuth (1973) recalls and deepens the analyses of Markov (1954) and\nKleene (1967) by stating that: \nBesides merely being a finite set of rules that gives a sequence of\noperations for solving a specific type of problem, an algorithm has\nfive important features: \nAs in Kleene (1967), finiteness affects both the number of\ninstructions and the number of implemented computational steps. As in\nMarkov’s determinacy, Knuth’s definiteness principle\nrequires that each successive computational step be unambiguously\nspecified. Furthermore, Knuth (1973) more explicitly requires that\nalgorithms have (potentially empty sets of) inputs and outputs. By\nalgorithms with no inputs or outputs Knuth probably refers to\nalgorithms using internally stored data as inputs or algorithms not\nreturning data to an external user (Rapaport 2019, ch. 7, in Other\nInternet Resources). As for effectiveness, besides Markov’s\ntendency “to obtain a certain result”, Knuth requires that\nthe result be obtained in a finite amount of time and that the\ninstructions be atomic, that is, simple enough to be understandable\nand executable by a human or artificial computer. \nGurevich (2011) maintains, on the one hand, that it is not possible to\nprovide formal definitions of algorithms, as the notion continues to\nevolve over time: consider how sequential algorithms, used in\nancient mathematics, are flanked by parallel, analog, or quantum\nalgorithms in current computer science practice, and how new kinds of\nalgorithms are likely to be envisioned in the near future. On the\nother hand, a formal analysis can be advanced if concerned only with\nclassical sequential algorithms. In particular, Gurevich (2000)\nprovides an axiomatic definition for this class of algorithms. \nAny sequential algorithm can be simulated by a sequential abstract\nstate machine satisfying three axioms: \nMoschovakis (2001) objects that the intuitive notion of algorithm is\nnot captured in full by abstract machines. Given a general recursive\nfunction f: ℕ → ℕ defined on natural\nnumbers, there are usually many different algorithms computing it;\n“essential, implementation-independent properties” are not\ncaptured by abstract machines, but rather by a system of recursive\nequations. Consider the algorithm mergesort for sorting\nlists; there are many different abstract machines for\nmergesort, and the question arises which one is to be chosen\nas the mergesort algorithm. The mergesort algorithm\nis instead the system of recursive equations specifying the involved\nfunction, whereas abstract machines for the mergesort\nprocedure are different implementations of the same\nalgorithm. Two questions are put forward by Moschovakis’\nformal analysis: different implementations of the same\nalgorithm should be equivalent implementations, and yet, an\nequivalence relation among algorithm implementations is to be formally\ndefined. Furthermore, it remains to be clarified what the intuitive\nnotion of algorithm formalized by systems of recursive equations\namounts to. \nPrimiero (2020) proposes a reading of the nature of algorithms at\nthree different levels of abstraction. At a very high LoA, algorithms\ncan be defined abstracting from the procedure they describe, allowing\nfor many different sets of states and transitions. At this LoA\nalgorithms can be understood as informal specifications, that\nis, as informal descriptions of a procedure P. At a lower\nLoA, algorithms specify the instructions needed to solve the given\ncomputational problem; in other words, they specify a procedure.\nAlgorithms can thus be defined as procedures, or descriptions\nin some given formal language L of how to execute a procedure\nP. Many important properties of algorithms, including those\nrelated to complexity classes and data structures, cannot be\ndetermined at the procedural LoA, and instead make reference to an\nabstract machine implementing the procedure is needed. At a bottom\nLoA, algorithms can be defined as implementable abstract\nmachines, viz. as the specification, in a formal language\nL, of the executions of a program P for a given\nabstract machine M. The threefold definition of algorithms\nallows Primiero (2020) to supply a formal definition of equivalence\nrelations for algorithms in terms of the algebraic notions of\nsimulation and bisimulation (Milner 1973, see also\nAngius and Primero 2018). A machine Mi executing a\nprogram Pi implements the same algorithm of a\nmachine Mj executing a program\nPj if and only if the abstract machines\ninterpreting Mi and Mj are in\na bisimulation relation. \nVardi (2012) underlines how, despite the many formal and informal\ndefinitions available, there is no general consensus on what an\nalgorithm is. The approaches of Gurevich (2000) and Moschovakis\n(2001), which can even be proved to be logically equivalent, only\nprovide logical constructs for algorithms, leaving unanswered the main\nquestion. Hill (2013) suggests that an informal definition of\nalgorithms, taking into account the intuitive understanding one has\nabout algorithms, may be more useful, especially for the public\ndiscourse and the communication between practitioners and users. \nRapaport (2012, Appendix) provides an attempt to summarize the three\nclassical definitions of algorithm sketched above stating that: \nRapaport stresses that an algorithm is a procedure, i.e., a finite\nsequence of statements taking the form of rules or instructions.\nFiniteness is here expressed by requiring that instructions contain a\nfinite number of symbols from a finite alphabet. \nHill (2016) aims at providing an informal definition of algorithm,\nstarting from Rapaport’s (2012): \nFirst of all, algorithms are compound structures rather than\natomic objects, i.e., they are composed of smaller units, namely\ncomputational steps. These structures are finite and effective, as\nexplicitly mentioned by Markov, Kleene, and Knuth. While these authors\ndo not explicitly mention abstractness, Hill (2016) maintains it is\nimplicit in their analysis. Algorithms are abstract simply in\nthat they lack spatio-temporal properties and are independent from\ntheir instances. They provide control, that is,\n“content that brings about some kind of change from one state to\nanother, expressed in values of variables and consequent\nactions” (p. 45). Algorithms are imperatively given, as\nthey command state transitions to carry out specified operations.\nFinally, algorithms operate to achieve certain purposes under\nsome usually well-specified provisions, or preconditions.\nFrom this viewpoint, the author argues, algorithms are on a par with\nspecifications in their specifying a goal under certain resources.\nThis definition allows to distinguish algorithms from other compound\ncontrol structures. For instance, recipes are not algorithms because\nthey are not effective; nor are games, which are not imperatively\ngiven. \nThe ontology of computer programs is strictly related to the subsumed\nnature of computational systems (see\n §1).\n If computational systems are defined on the basis of the\nsoftware-hardware dichotomy, programs are abstract entities\ninterpreting the former and opposed to the concrete nature of\nhardware. Examples of such interpretations are provided in\n §1.1\n and include the “concrete abstraction” definition by\nColburn (2000), the “abstract artifact” characterization\nby Irmak (2012), and programs as specifications of machines proposed\nby Duncan (2011). By contrast, under the interpretation of\ncomputational systems by a hierarchy of LoAs, programs are\nimplementations of algorithms. We refer to\n §5\n on implementation for an analysis of the ontology of programs in this\nsense. This section focuses on definitions of programs with a\nsignificant relevance in the literature, namely those views that\nconsider programs as theories or as artifacts, with a focus on the\nproblem of the relation between programs and the world. \nThe view that programs are theories goes back to approaches in\ncognitive science. In the context of the so-called Information\nProcessing Psychology (IPP) for the simulative investigation on human\ncognitive processes, Newell and Simon (1972) advanced the thesis that\nsimulative programs are empirical theories of their simulated\nsystems. Newell and Simon assigned to a computer program the role of\ntheory of the simulated system as well as of the simulative system,\nnamely the machine running the program, to formulate predictions on\nthe simulated system. In particular, the execution traces of the\nsimulative program, given a specific problem to solve, are used to\npredict the mental operation strategies that will be performed by the\nhuman subject when asked to accomplish the same task. In case of a\nmismatch between execution traces and the verbal reports of the\noperation strategies of the human subject, the empirical theory\nprovided by the simulative program is revised. The predictive use of\nsuch a computer program is comparable, according to Newell and Simon,\nto the predictive use of the evolution laws of a system that are\nexpressed by differential or difference equations. \nNewell and Simon’s idea that programs are theories has been\nshared by the cognitive scientists Pylyshyn (1984) and Johnson-Laird\n(1988). Both agree that programs, in contrast to typical theories, are\nbetter at facing the complexity of the simulative process to\nbe modelled, forcing one to fill-in all the details that are necessary\nfor the program to be executed. Whereas incomplete or incoherent\ntheories may be advanced at some stage of scientific inquiry, this is\nnot the case for programs. \nOn the other hand, Moore (1978) considers the programs-as-theories\nthesis another myth of computer science. As programs can\nonly simulate some set of empirical phenomena, at most they play the\nrole of computational models of those phenomena. Moore\nnotices that for programs to be acknowledged as models, semantic\nfunctions are nevertheless needed to interpret the empirical system\nbeing simulated. However, the view that programs are models should not\nbe mistaken for the definition of programs as theories: theories\nexplain and predict the empirical phenomena\nsimulated by models, while simulation by programs does not offer\nthat. \nAccording to computer scientist Paul Thagard (1984), understanding\nprograms as theories would require a syntactic or a\nsemantic view of theories (see the entry on\n the structure of scientific theories).\n But programs do not comply with either of the two views. According to\nthe syntactic view (Carnap 1966, Hempel 1970), theories are sets of\nsentences expressed in some defined language able to describe target\nempirical systems; some of those sentences define the axioms of the\ntheory, and some are law-like statements expressing regularities of\nthose systems. Programs are sets of instructions written in some\ndefined programming language which, however, do not describe any\nsystem, insofar as they are procedural linguistic entities and not\ndeclarative ones. To this, Rapaport (2020, see Other Internet\nResources) objects that procedural programming languages can often be\ntranslated into declarative languages and that there are languages,\nsuch as Prolog, that can be interpreted both procedurally and\ndeclaratively. According to the semantic view (Suppe 1989, Van\nFraassen 1980), theories are introduced by a collection of models,\ndefined as set-theoretic structures satisfying the theory’s\nsentences. However, in contrast to Moore (1978), Thagard (1984) denies\nprograms the epistemological status of models: programs simulate\nphysical systems without satisfying theories’ laws and axioms.\nRather, programs include, for simulation purposes, implementation\ndetails for the programming language used, but not of the target\nsystem being simulated. \nA yet different approach to the problem of whether programs are\ntheories comes from the computer scientist Peter Naur (1985).\nAccording to Naur, programming is a theory building process not in the\nsense that programs are theories, but because the successful\nprogram’s development and life-cycle require that programmers\nand developers have theories of programs available. A theory is here\nunderstood, following Ryle (2009), as a corpus of knowledge shared by\na scientific community about some set of empirical phenomena, and not\nnecessarily expressed axiomatically or formally. Theories of\nprograms are necessary during the program life-cycle to be able to\nmanage requests of program modifications pursuant to observed\nmiscomputations or unsatisfactory solutions to the computational\nproblem the program was asked to solve. In particular, theories of\nprograms should allow developers to modify the program so that new\nsolutions to the problem at stake can be provided. For this reason,\nNaur (1985) deems such theories more fundamental, in software\ndevelopment, than documentations and specifications. \nFor Turner (2010, 2018 ch. 10), programming languages are mathematical\nobjects defined by a formal grammar and a formal semantics. In\nparticular, each syntactic construct, such as an assignment, a\nconditional or a while loop, is defined by a grammatical rule\ndetermining its syntax, and by a semantic rule associating a meaning\nto it. Depending on whether an operational or a denotational semantics\nis preferred, meaning is given in terms of respectively the operations\nof an abstract machine or of mathematical partial functions from set\nof states to set of states. For instance, the simple assignment\nstatement \\(x := E\\) is associated, under an operational semantics,\nwith the machine operation \\(update(s,x,v)\\) which assigns variable\n\\(v\\) interpreted as \\(E\\) to variable \\(x\\) in state \\(s\\). Both in\nthe case of an operational and of a denotational semantics, programs\ncan be understood as mathematical theories expressing the operations\nof an implementing machine. Consider operational semantics: a\nsyntactic rule of the form \\(\\langle P,s \\rangle \\Downarrow s'\\)\nstates semantically that program \\(P\\) executed in state \\(s\\) results\nin \\(s'.\\) According to Turner (2010, 2018), a programming language\nwith an operational semantics is akin to an axiomatic theory of\noperations in which rules provide axioms for the relation\n\\(\\Downarrow\\). \nPrograms can be understood as technical artifacts because programming\nlanguages are defined, as any other artifact, on the basis of both\nfunctional and structural properties (Turner 2014, 2018 ch. 5).\nFunctional properties of (high level) programming languages are\nprovided by the semantics associated with each syntactic construct of\nthe language. Turner (2014) points out that programming languages can\nindeed be understood as axiomatic theories only when their functional\nlevel is isolated. Structural properties, on the other hand, are\nspecified in terms of the implementation of the language, but not\nidentified with physical components of computing machines: given a\nsyntactic construct of the language with an associated functional\ndescription, its structural property is determined by the physical\noperations that a machine performs to implement an instruction for the\nconstruct at hand. For instance, the assignment construct \\(x := E\\)\nis to be linked to the physical computation of the value of expression\n\\(E\\) and to the placement of the value of \\(E\\) in the physical\nlocation \\(x\\). \nAnother requirement for a programming language to be considered a\ntechnical artifact is that it has to be endowed with a semantics\nproviding correctness criteria for the language implementation. The\nprogrammer attests to functional and structural properties of a\nprogram by taking the semantics to have correctness jurisdiction over\nthe program. \nThe problem of whether computer programs are theories is tied with the\nrelation that programs entertain with the outside world. If programs\nwere theories, they would have to represent some empirical system, and\na semantic relation would be directly established between the program\nand the world. By contrast, some have argued that the relation between\nprograms and natural systems is mediated by models of the outside\nworld (Colburn et al. 1993, Smith 1985). In particular, Smith\n(1985) argues that models are abstract descriptions of empirical\nsystems, and computational systems operating in them have programs\nthat act as models of the models, i.e., they represent abstract models\nof reality. Such an account of the ontology of programs comes in handy\nwhen describing the correctness problem in computer science (see\n § 7):\n if specifications are considered as models requiring certain\nbehaviors from computational systems, programs can be seen as models\nsatisfying specifications. \nTwo views of programs can be given depending on whether one admits\ntheir relation with the world (Rapaport 2020, ch. 17, see Other\nInternet Resource). According to a first view, programs are\n“wide”, “external” and “semantic”:\nthey grant direct reference to objects of an empirical system and\noperations on those objects. According to a second view, programs are\n“narrow”, “internal”, and\n“syntactic”: they make only reference to the atomic\noperations of an implementing machine carrying out computations.\nRapaport (2020, see Other Internet Resources) argues that programs\nneed not be “external” and\n“semantic”. First, computation itself needs not to be\n“external”: a Turing machine executes the instructions\ncontained in its finite table by using data written on its tape and\nhalting after the data resulting from the computation have been\nwritten on the tape. Data are not, strictly speaking, in-put-from\nand out-put-to an external user. Furthemore, Knuth (1973) required\nalgorithms to have zero or more inputs and outputs (see\n § 3.1).\n A computer program requiring no inputs may be a program, say,\noutputting all prime numbers from 1; and a program with no outputs can\nbe a program that computes the value of some given variable x without\nreturning the value stored in x as output. Second, programs need not\nbe “external”, teleological, i.e., goal oriented. This\nview opposes other known positions in the literature. Suber (1988)\nargues that, without considering goals and purposes, it would not be\npossible to assess whether a computer program is correct, that is, if\nit behaves as intended. And as recalled in\n §3.3.,\n Hill (2016) specifies in her informal definition that algorithms\naccomplish “a given purpose, under given provisions.”\n(Hill 2016: 48). To these views, Rapaport (2020, ch. 17, see Other\nInternet Resource) replies that whereas goals, purposes, and\nprogrammers’ intentions may be very useful for a human computor\nto understand a program, they are not necessary for an artificial\ncomputer to carry out the computations instructed by the program code.\nIndeed, the principle of effectiveness that classical approaches\nrequire for algorithms (see\n §3.1)\n demands, among other properties, that algorithms be executed without\nany recourse to intuition. In other words, a machine executing a\nprogram for adding natural numbers does not “understand”\nthat it is adding; at the same time, knowing that a given program\nperforms addition may help a human agent to understand the\nprogram’s code. \nAccording to this view, computing involves just symbols, not meanings.\nTuring machines become symbols manipulators and not a single but\nmultiple meanings can be associated with its operations. How can then\none identify when two programs are the same program, if not\nby their meanings, that is, by considering what function they perform?\nOne answer comes from Piccini’s analysis of computation and its\n“internal semantics” (Piccini 2008, 2015 ch. 3):\ntwo programs can be identified as identical by analysing only their\nsyntax and the operations the programs carry out on their symbols. The\neffects of string manipulation operations can be considered an\ninternal semantics of a program. The latter can be easily determined\nby isolating subroutines or methods in the program’s code and\ncan afterwards be used to identify a program or to establish whether\ntwo programs are the same, namely when they are defined by the same\nsubroutines. \nHowever, it has been argued that there are cases in which it is not\npossible to determine whether two programs are the same without making\nreference to an external semantics. Sprevak (2010) proposes to\nconsider two programs for addition which differ from the fact that one\noperates on Arabic, the other one on Roman numerals. The two programs\ncompute the same function, namely addition, but this cannot always be\nestablished by inspecting the code with its subroutines; it must be\ndetermined by assigning content to the input/output strings,\ninterpreting Arabic and Roman numerals as numbers. In that regard,\nAngius and Primiero (2018) underline how the problem of identity for\ncomputer programs does not differ from the problem of identity for\nnatural kinds (Lowe 1998) and technical artifacts (Carrara et al.\n2014). The problem can be tackled by fixing an identity criterion,\nnamely a formal relation, that any two programs should entertain in\norder to be defined as identical. Angius and Primiero (2018) show how\nto use the process algebra relation of bisimulation between the two\nautomata implemented by two programs under examination as such an\nidentity criterion. Bisimulation allows to establish matching\nstructural properties of programs implementing the same function, as\nwell as providing weaker criteria for copies in terms of simulation.\nThis brings the discussion back to the notion of programs as\nimplementations. We now turn to analyze this latter concept. \nThe word ‘implementation’ is often associated with a\nphysical realization of a computing system, i.e., to a machine\nexecuting a computer program. In particular, according to the dual\nontology of computing systems examined in\n §1.1,\n implementation in this sense reduces to the structural hardware, as\nopposed to the functional software. By contrast, following the method\nof the levels of abstraction\n (§ 1.2),\n implementation becomes a wider relation holding between any LoA\ndefining a computational system and the levels higher in the\nhierarchy. Accordingly, an algorithm is an implementation of a (set\nof) specification(s); a program expressed in a high level programming\nlanguage can be defined as an implementation of an algorithm (see\n §4);\n assembly and machine code instructions can be seen as an\nimplementation of a set of high-level programming language\ninstructions with respect to a given ISA; finally, executions are\nphysical, observable, implementations of those machine code\ninstructions. By the same token, programs formulated in a high-level\nlanguage are also implementations of specifications, and, as similarly\nargued by the dual-ontology paradigm, executions are implementations\nof high-level programming language instructions. According to Turner\n(2018), even the specification can be understood as an implementation\nof what has been called intention. \nWhat remains to be examined here is the nature of the implementation\nrelation thus defined. Analyzing this relation is essential to define\nthe notion of correctness\n (§7).\n Indeed, a correct program amounts to a correct implementation of an\nalgorithm; and a correct computing system is a correct implementation\nof a set of specifications. In other words, under this view, the\nnotion of correctness is paired with that of implementation for any\nLoA: any level can be said to be correct with respect to upper levels\nif and only if it is a correct implementation thereof. \nThe following three subsections examine three main definitions of the\nimplementation relation that have been advanced in the philosophy of\ncomputer science literature. \nA first philosophical analysis of the notion of implementation in\ncomputer science is advanced by Rapaport (1999, 2005). He defines an\nimplementation I as the semantic interpretation of a\nsyntactic or abstract domain A in a medium of implementation\nM. If implementation is understood as a relation holding\nbetween a given LoA and any upper level in the hierarchical ontology\nof a computational system, it follows that Rapaport’s definition\nextends accordingly, so that any LoA provides a semantic\ninterpretation in a given medium of implementation for the upper\nlevels. Under this view, specifications provide semantic\ninterpretations of intentions expressed by stakeholders in the\nspecification (formal) language, and algorithms provide semantic\ninterpretations of specifications using one of the many languages\nalgorithms can be formulated in (natural languages, pseudo-code, logic\nlanguages, functional languages etc.). The medium of implementation\ncan be either abstract or concrete. A computer program is the\nimplementation of an algorithm in that the former provides a semantic\ninterpretation of the syntactic constructs of the latter in a\nhigh-level programming language as its medium of implementation. The\nprogram’s instructions interpret the algorithm's tasks in a\nprogramming language. Also the execution LoA provides a semantic\ninterpretation of the assembly/machine code operations into the medium\ngiven by the structural properties of the physical machine. According\nto the analysis in (Rapaport 1999, 2005), implementation is an\nasymmetric relation: if I is an implementation of A,\nA cannot be an implementation of I. However, the\nauthor argues that any LoA can be both a syntactic and a semantic\nlevel, that is, it can play the role of both the implementation I and\nof a syntactic domain A. Whereas an algorithm is assigned a semantic\ninterpretation by a program expressed in a high-level language, the\nsame algorithm provides a semantic interpretation for the\nspecification. It follows that the abstraction-implementation relation\npairs the functional-structural relation for computational\nsystems. \nPrimiero (2020) considers this latter aspect as one main limit of\nRapaport’s (1999, 2005) account of implementation:\nimplementation reduces to a unique relation between a\nsyntactic level and its semantic interpretation and it does not\naccount for the layered ontology of computational systems seen in\n §1.2.\n In order to extend the present definition of implementation to all\nLoAs, each level has to be reinterpreted each time either as syntactic\nor as a semantic level. This, in turn, has a repercussion on the\nsecond difficulty characterizing, according to Primero (2020),\nimplementation as a semantic interpretation: on the one hand, this\napproach does not take into account incorrect\nimplementations; on the other hand, for a given incorrect\nimplementation, the unique relation so defined can relate\nincorrectness only to one syntactic level, excluding all other levels\nas potential error locations. \nTurner (2018) aims to show that semantic interpretation not only does\nnot account for incorrect implementation, but not even to correct\nones. One first example is provided by the implementation of one\nlanguage into another: the implementing language here is not providing\na semantic interpretation of the implemented language, unless the\nformer is associated with a semantics providing meaning and\ncorrectness criteria for the latter. Such semantics will remain\nexternal to the implementation relation: whereas correctness is\nassociated with semantic interpretation, implementation does not\nalways come with a semantic interpretation. A second example is given\nby considering an abstract stack implemented by an array; again, the\narray does not provide correctness criteria for the stack. Quite to\nthe contrary, it is the stack that specifies correctness criteria for\nany of its implementation, arrays included. \nThe fact that correctness criteria for the implementation relation are\nprovided by the abstract level induces Turner (2012, 2014, 2018)\nto define implementation as the relation\nspecification-artefact. As examined in\n §2,\n specifications have correctness jurisdiction over artifacts, that is,\nthey prescribe the allowed behaviors of artifacts. Also recall that\nartifacts can be both abstract and concrete entities, and that any LoA\ncan play the role of specification for lower levels. This amounts to\nsaying that the specification-artefact relation is able to define any\nimplementation relation across the layered ontology of computational\nsystems. \nDepending on how the specification-artifact relation is defined,\nTurner (2012) distinguishes as many as three different notions of\nimplementation. Consider the case of a physical machine implementing a\ngiven abstract machine. According to an intentional notion of\nimplementation, an abstract machine works as a specification for a\nphysical machine, provided it advances all the functional requirements\nthe latter must fulfill, i.e., it specifies (in principle) all the\nallowed behaviors of the implementing physical machine. According to\nan extensional notion of implementation, a physical machine\nis a correct implementation of an abstract machine if and only if\nisomorphisms can be established mapping states of the latter to states\nof the former, and transitions in the abstract machine correspond to\nactual executions (computational traces) of the artifact. Finally, an\nempirical notion of implementation requires the physical\nmachine to display computations that match those prescribed by the\nabstract machine; that is to say, correct implementation has to be\nevaluated empirically through testing. \nPrimiero (2020) underlines how, while this approach addresses the\nissue of correctness and miscomputation as it allows to distinguish a\ncorrect from an incorrect implementation, it still identifies a unique\nimplementation relation between a specification level and an artifact\nlevel. Again, if this account is allowed to involve the layered\nontology of computational systems by reinterpreting each time any LoA\neither as a specification or artifact, Turner’s account prevents\nfrom referring to more than one level at the same time as the cause of\nmiscomputation: a miscomputation always occurs here as an\nincorrect implementation of a specification by an artifact. By\ndefining implementation as a relation holding accross all the LoAs,\none would be able to identify multiple incorrect implementations which\ndo not directly refer to the abstract specification. A miscomputation\nmay indeed be caused by an incorrect implementation of lower levels\nwhich is then inherited all the way down to the execution level. \nPrimiero (2020) proposes a definition of implementation not as a\nrelation between two fixed levels, but one that is allowed to range\nover any LoA. Under this view, an implementation I is a\nrelation of instantiation holding between a LoA and any other\none higher in the abstraction hierarchy. Accordingly, a physical\ncomputing machine is an implementation of assembly/machine code\noperations; by transitivity, it can also be considered as an\ninstantiation of a set of instructions expressed in high-level\nprogramming language instructions. A program expressed in a high-level\nlanguage is an implementation of an algorithm; but it can also be\ntaken to be the instantiation of a set of specifications. \nSuch a definition of implementation allows Primiero (2020) to provide\na general definition of correctness: a physical computing system is\ncorrect if and only if it is characterized by correct implementations\nat any LoA. Hence correctness and implementation are coupled and\ndefined at any LoA. Functional correctness is the property of\na computational system that displays the functionalities required by\nthe specifications of that system. Procedural correctness\ncharacterizes computational systems displaying the functionalities\nintended by the implemented algorithms. And executional\ncorrectness is defined as the property of a system that is able\nto correctly execute the program on its architecture. Each of these\nforms of correctness can also be classified quantitatively, depending\non the amount of functionalities being satisfied. A functionally\nefficient computational system displays a minimal subset of\nthe functionalities required by the specifications; a functionally\noptimal system is able to display a maximal subset of those\nfunctionalities. Similarly, the author defines procedurally as well as\nexecutionally efficient and optimal computational systems. \nAccording to this definition, implementation shifts from level to\nlevel: a set of algorithms defining a computational system are\nimplemented as procedures in some formal language, as\ninstructions in a high-level language, or as operations in a\nlow-level programming language. An interesting question is whether\nany system, beyond computational artifacts, implementing\nprocedures of this sort qualifies as a computational system. In other\nwords, asking about the nature of physical implementation amounts to\nasking what is a computational system. If any system implementing an\nalgorithm would qualify as computational, the class of such systems\ncould be extended to biological systems, such as the brain or the\ncell; to physical systems, including the universe or some portion of\nit; and eventually to any system whatsoever, a thesis known as\npancomputationalism (for an exhaustive overview on the topic\nsee Rapaport 2018). \nTraditionally, a computational system is intended as a mechanical\nartifact that takes input data, elaborates them\nalgorithmically according to a set of instructions, and\nreturns manipulated data as outputs. For instance, von Neumann (1945,\np.1) states that “An automatic computing system is a (usually\nhighly composite) device, which can carry out instructions to perform\ncalculations of a considerable order of complexity”. Such an\ninformal and well-accepted definition leaves some questions open,\nincluding whether computational systems have to be machines, whether\nthey have to process data algorithmically and, consequently, whether\ncomputations have to be Turing complete. \nRapaport (2018) provides a more explicit characterization of a\ncomputational system defined as any “physical plausible\nimplementation of anything logically equivalent to a universal Turing\nmachine”. Strictly speaking personal computers are not physical\nTuring machines, but register machines are known to be Turing\nequivalent. To qualify as computational, systems must be\nplausible implementations thereof, in that Turing machines,\ncontrary to physical machines, have access to infinite memory space\nand are, as abstract machines, error free. According to\nRapaport’s (2018) definition, any physical\nimplementation of this sort is thus a computational system, including\nnatural systems. This raises the question about which class of natural\nsystems is able to implement Turing equivalent computations. Searle\nfamously argued that anything can be an implementation of a Turing\nmachine, or of a logical equivalent model (Searle 1990). His argument\nlevers on the fact that being a Turing machine is a syntactic\nproperty, in that it is all about manipulating tokens of 0’s and\n1’s. According to Searle, syntactic properties are not intrinsic\nto physical systems, but they are assigned to them by an observer. In\nother words, a physical state of a system is not intrinsically a\ncomputational state: there must be an observer, or user, who assigns\nto that state a computational role. It follows that any system whose\nbehavior can be described as syntactic manipulation of 0’s and\n1’s is a computational system. \nHayes (1997) objects to Searle (1990) that if everything was a\ncomputational system, the property “being a computational\nsystem” would become vacuous, as all entities would possess it.\nInstead, there are entities which are computational systems, and\nentities which are not. Computational systems are those in which the\npatterns received as inputs and saved into memory are able to change\nthemselves. In other words, Hayes makes reference to the fact that\nstored inputs can be both data and instructions and that instructions,\nwhen executed, are able to modify the value of some input data.\n“If it were paper, it would be ‘magic paper’ on\nwhich writing might spontaneously change, or new writing appear”\n(Hayes 1997, p. 393). Only systems able to act as “magic\npaper” can be acknowledged as computational. \nA yet different approach comes from Piccinini (2007, 2008) in the\ncontext of his mechanistic analysis of physical computations\n(Piccinini 2015; see also the entry on\n computation in physical systems).\n A physical computing system is a system whose behaviors can be\nexplained mechanistically by describing the computing\nmechanism that brings about those behaviors. Mechanisms can be defined\nby “entities and activities organized such that they are\nproductive of regular changes from start or set-up to finish or\ntermination condition” (Machamer et al. 2000; see the entry on\n mechanisms in science).\n Computations, as physical processes, can be understood as those\nmechanisms that “generate output strings from input strings in\naccordance with general rules that apply to all input strings and\ndepend on the input (and sometimes internal states)” (Piccinini\n2007, p. 108). It is easy to identify set-up and termination\nconditions for computational processes. Any system which can be\nexplained by describing an underlying computing mechanism is to be\nconsidered a computational system. The focus on explanation helps\nPiccinini avoid the Searlean conclusion that any system is a\ncomputational system: even if one may interpret, in principle, any\ngiven set of entities and activities as a computing mechanism, only\nthe need to explain a certain observed phenomenon in terms of a\ncomputing mechanism defines the system under examination as\ncomputational. \nA crucial step in the software development process is verification.\nThis consists in the process of evaluating whether a given\ncomputational system is correct with respect to the specification of\nits design. In the early days of the computer industry, validity and\ncorrectness checking methods included several design and construction\ntechniques, see for example (Arif et al. 2018). Nowadays,\ncorrectness evaluation methods can be roughly sorted into two main\ngroups: formal verification and testing. Formal verification (Monin\nand Hinchey 2003) involves a proof of correctness with mathematical\ntools; software testing (Ammann and Offutt 2008) rather consists in\nrunning the implemented program to observe whether performed\nexecutions comply or not with the advanced specifications. In many\npractical cases, a combination of both methods is used (see for\ninstance Callahan et al. 1996). \nFormal verification methods require a representation of the\nsoftware under verification. In theorem proving (see van\nLeeuwen 1990), programs are represented in terms of axiomatic systems\nand a set of rules of inference representing the pre- and\npost-conditions of program transitions. A proof of correctness is then\nobtained by deriving formulas expressing specifications from the\naxioms. In model checking (Baier and Katoen 2008), a program\nis represented in terms of a state transition system, its property\nspecifications are formalised by temporal logic formulas (Kröger\nand Merz 2008), and a proof of correctness is achieved by a\ndepth-first search algorithm that checks whether those formulas hold\nof the state transition system. \nAxiomatic systems and state transition systems used for correctness\nevaluation can be understood as theories of the represented\nartifacts, in that they are used to predict and explain their future\nbehaviors. Methodologically state transition systems in model checking\ncan be compared with scientific models in empirical sciences (Angius\nand Tamburrini 2011). For instance, Kripke Structures (see Clarke\net al. 1999 ch. 2) are in compliance with Suppes’\n(1960) definition of scientific models as set-theoretic structures\nestablishing proper mapping relations with models of data collected by\nmeans of experiments on the target empirical system (see also the\nentry on\n models in science).\n Kripke Structures and other state transition systems utilized in\nformal verification methods are often called system specifications.\nThey are distinguished from common specifications, also called\nproperty specifications. The latter specify some required behavioral\nproperties the program to be encoded must instantiate, while the\nformer specify (in principle) all potential executions of an already\nencoded program, thus allowing for algorithmic checks on its traces\n(Clarke et al. 1999). In order to achieve this goal, system\nspecifications are considered as abductive structures,\nhypothesizing the set of potential executions of a target\ncomputational system on the basis of the program’s code and the\nallowed state transitions (Angius 2013b). Indeed, once it has been\nchecked whether some temporal logic formula holds of the modeled\nKripke Structure, the represented program is empirically tested\nagainst the behavioral property corresponding to the checked formula,\nin order to evaluate whether the model-hypothesis is an adequate\nrepresentation of the target computational system. Accordingly,\nproperty specifications and system specifications differ also in their\nintentional stance (Turner 2011): the former are requirements\non the program to be encoded, the latter are (hypothetical)\ndescriptions of the encoded program. The descriptive and abductive\ncharacter of state transition systems in model checking is an\nadditional and essential feature putting state transition systems on a\npar with scientific models. \nTesting is the more ‘empirical’ process of launching a\nprogram and observing its executions in order to evaluate whether they\ncomply with the supplied property specifications. Such technique is\nextensively used in the software development process. Philosophers and\nphilosophically-minded computer scientists have considered software\ntesting under the light of traditional methodological approaches in\nscientific discovery (Snelting 1998; Gagliardi 2007; Northover et\nal. 2008; Angius 2014) and questioned whether software tests can\nbe acknowledged as scientific experiments evaluating the\ncorrectness of programs (Schiaffonati and Verdicchio 2014,\nSchiaffonati 2015; Tedre 2015). \nDijkstra’s well-known dictum “Program testing can be used\nto show the presence of bugs, but never to show their absence”\n(Dijkstra 1970, p.7), introduces Popper’s (1959) principle of\nfalsifiability into computer science (Snelting 1998). Testing\na program against an advanced property specification for a given\ninterval of time may exhibit some failures, but if no failure occurs\nwhile observing the running program one cannot conclude that the\nprogram is correct. An incorrect execution might be observed at the\nvery next system’s run. The reason is that testers can only\nlaunch the program with a finite subset of the potential\nprogram’s input set and only for a finite interval of time;\naccordingly, not all potential executions of the program to be tested\ncan be empirically observed. For this reason, the aim of software\ntesting is to detect programs’ faults and not to guarantee their\nabsence (Ammann and Offutt 2008, p. 11). A program is falsifiable in\nthat tests can reveal faults (Northover et al. 2008). Hence,\ngiven a computational system and a property specification, a test is\nakin to a scientific experiment which, by observing the system’s\nbehaviors, tries to falsify the hypothesis that the program is correct\nwith respect to the interested specification. \nHowever, other methodological and epistemological traits\ncharacterizing scientific experiments are not shared by software\ntests. A first methodological distinction can be recognized in that a\nfalsifying test leads to the revision of the computational system, not\nof the hypothesis, as in the case of testing scientific hypotheses.\nThis is due to the difference in the intentional stance of\nspecifications and empirical hypotheses in science (Turner 2011).\nSpecifications are requirements whose violation demands for program\nrevisions until the program becomes a correct instantiation of the\nspecifications. \nFor this, among other reasons, the traditional notion of scientific\nexperiment needs to be ‘stretched’ in order to be applied\nto software testing activities (Schiaffonati 2015). Theory-driven\nexperiments, characterizing most of the experimental sciences,\nfind no counterpart in actual computer science practice. If one\nexcludes the cases wherein testing is combined with formal methods,\nmost experiments performed by software engineers are rather\nexplorative, i.e. aimed at ‘exploring’\n“the realm of possibilities pertaining to the functioning of an\nartefact and its interaction with the environment in the absence of a\nproper theory or theoretical background” (Schiaffonati 2015:\n662). Software testers often do not have theoretical control on the\nexperiments they perform; exploration on the behaviors of the\ncomputational system interacting with users and environments rather\nallows testers to formulate theoretical generalizations on the\nobserved behaviors. Explorative experiments in computer science are\nalso characterized by the fact that programs are often tested in a\nreal-like environment wherein testers play the role of users. However,\nit is an essential feature of theory-driven experiments that\nexperimenters do not take part in the experiment to be carried\nout. \nAs a result, while some software testing activities are closer to the\nexperimental activities one finds in empirical sciences, some others\nrather define a new typology of experiment that turns out to belong to\nthe software development process. Five typologies of experiments can\nbe distinguished in the process of specifying, implementing, and\nevaluating computational systems (Tedre 2015): \nA software test is considered successful when miscomputations are\ndetected (assuming that no computational artifact is 100% correct).\nThe successive step is to find out what caused the execution to be\nincorrect, that is, to trace back the fault (more familiarly named\n‘bug’), before proceeding to the debugging phase and then\ntesting the system again. In other words, an explanation of\nthe observed miscomputation is to be advanced. \nEfforts have been made to consider explanations in computer science\n(Piccinini 2007; Piccinini and Craver 2011; Piccinini 2015; Angius and\nTamburrini 2016) in relation to the different models of explanations\nelaborated in the philosophy of science. In particular, computational\nexplanations can be understood as a specific kind of mechanistic\nexplanation (Glennan 1996; Machamer et al. 2000; Bechtel\nand Abrahamsen 2005), insofar as computing processes can be analyzed\nas mechanisms (Piccinini 2007; 2015; see also the entry on\n computation in physical systems). \nConsider a processor executing an instruction. The involved process\ncan be understood as a mechanism whose components are states and\ncombinatory elements in the processor instantiating the functions\nprescribed by the relevant hardware specifications (specifications for\nregisters, for the Arithmetic Logic Unit etc..), organized in such a\nway that they are capable of carrying out the observed execution.\nProviding the description of such a mechanism counts as advancing a\nmechanist explanation of the observed computation, such as the\nexplanation of an operational malfunction. \nFor every type of miscomputation (see\n §7.3),\n a corresponding mechanist explanation can be defined at the adequate\nLoA and with respect to the set of specifications characterizing that\nLoA. Indeed, abstract descriptions of mechanisms still supply one with\na mechanist explanation in the form of a mechanism schema,\ndefined as “a truncated abstract description of a mechanism that\ncan be filled with descriptions of known component parts and\nactivities” (Machamer et al. 2000, p. 15). For\ninstance, suppose the very common case in which a machine miscomputes\nby executing a program containing syntax errors, called slips. The\ncomputing machine is unable to correctly implement the functional\nrequirements provided by the program specifications. However, for\nexplanatory purposes, it would be redundant to provide an explanation\nof the occurred slip at the hardware level of abstraction, by\nadvancing the detailed description of the hardware components and\ntheir functional organization. In such cases, a satisfactory\nexplanation may consist in showing that the program’s code is\nnot a correct instantiation of the provided program specifications\n(Angius and Tamburrini 2016). In order to explain mechanistically an\noccurred miscomputation, it may be sufficient to provide the\ndescription of the incorrect program, abstracting from the rest of the\ncomputing mechanism (Piccinini and Craver 2011). Abstraction is a\nvirtue not only in software development and specification, but also in\nthe explanation of computational systems’ behaviors. \nEach of the different approaches on software verification examined in\nthe previous section assumes a different understanding of correctness\nfor software. Standardly, correctness has been understood as a\nrelation holding between an abstraction and its implementation, such\nthat it holds if the latter fulfills the properties formulated by the\nformer. Once computational systems are described as having a layered\nontology, correctness needs to be reformulated as the relation that\nany structural level entertains with respect to its functional level\n(Primiero, 2020). Hence, correctness can still be considered as a\nmathematical relationship when formulated between abstract and\nfunctional level; while it can be considered as an empirical\nrelationship when formulated between the functional and the\nimplementation levels. One of the earlier debates in the philosophy of\ncomputer science (De Millo et al. 1979; Fetzer 1988) was\nindeed around this distinction. \nFormal verification methods grant an a-priori analysis of the\nbehaviors of programs, without requiring the observation of any of\ntheir implementation or considering their execution. In particular,\ntheorem proving allows one to deduce any potential behavior\nof the program under consideration and its behavioral properties from\na suitable axiomatic representation. In the case of model checking,\none knows in advance the behavioural properties displayed by the\nexecution of a program by performing an algorithmic search of the\nformulas valid in a given set-theoretic model. These considerations\nfamously led Hoare (1969) to conclude that program development is an\n“exact science”, which should be characterized by\nmathematical proofs of correctness, epistemologically on a par with\nstandard proofs in mathematical practice. \nDe Millo et al. (1979) question Hoare’s thesis: correct\nmathematical proofs are usually elegant and\ngraspable, implying that any (expert) reader can\n“see” that the conclusion follows from the premises (for\nthe notion of elegance in software see also Hill (2018)). What are\noften called Cartesian proofs (Hacking 2014) do not have a\ncounterpart in correctness proofs, typically long and cumbersome,\ndifficult to grasp and not explaining why the conclusion necessarily\nfollows from the premises. Yet, many proofs in mathematics are long\nand complex, but they are in principle surveyable, thanks to\nthe use of lemmas, abstractions and the analytic construction of new\nconcepts leading step by step to the statement to be proved.\nCorrectness proofs, on the contrary, do not involve the creation of\nnew concepts, nor the modularity one typically finds in mathematical\nproofs (Turner, 2018). And yet, proofs that are not surveyable cannot\nbe considered mathematical proofs (Wittgenstein 1956). \nA second theoretical difficulty concerning proofs of correctness for\ncomputer programs concerns their complexity and that of the programs\nto be verified. Already Hoare (1981) admitted that while verification\nof correctness is always possible in principle, in practice it is\nhardly achievable. Except for trivial cases, contemporary software is\nmodularly encoded, is required to satisfy a large set of\nspecifications, and it is developed so as to interact with other\nprograms, systems, users. Embedded and reactive software are cases in\npoint. In order to verify such complex software, correctness proofs\nare carried out automatically. Hence, on the one hand, the correctness\nproblem shifts from the program under examination to the program\nperforming the verification, e.g. a theorem prover; on the other hand,\nproofs carried out by a physical process can go wrong, due to\nmechanical mistakes of the machine. Against this infinite regress\nargument, Arkoudas and Bringsjord (2007) argue that one can make use\nof a proof checker which, by being a relatively small program, is\nusually easier to verify. \nMost recently, formal methods for checking correctness based on a\ncombination of logical and statistical analysis have given new\nstimulus to this research area: the ability of Separation Logics\n(Reynolds, 2002) to offer a representation of the logical behavior of\nthe physical memory of computational systems, and the possibility of\nconsidering probabilistic distributions of inputs as statistical\nsource of errors, have allowed formal correctness check of large\ninteractive systems like the Facebook platform (see also Pym et\nal. 2019). \nFetzer (1988) objected that deductive reasoning is only able to\nguarantee for the correctness of a program with respect to its\nspecifications, but not for the correctness of a computational system,\nthat is also accounting for the program’s physical\nimplementation. Even if the program were correct with respect to any\nof the related upper LoAs (algorithms, specifications, requirements),\nits implementation could still violate one or more of the intended\nspecifications due to a physical malfunctioning. The former kind of\ncorrectness can in principle be proved mathematically, but the\ncorrectness of the execution LoA requires an empirical assessment. As\nexamined in\n §6.2,\n software testing can show only in principle the correctness of a\ncomputational system. In practice, the number of allowed executions of\nnon-trivial systems are potentially infinite and cannot be\nexhaustively checked in a finite (or reasonable) amount of time\n(Dijkstra 1974). Most successful testing methods rather see both\nformal verification and testing used together to reach a satisfactory\ncorrection level. \nAnother objection to the theoretical possibility of mathematical\ncorrectness is that since proofs are carried out by a theorem prover,\ni.e. a physical machine, the knowledge one attains about computational\nsystems is not a-priori but empirical (see Turner 2018 ch.\n25). However, Burge (1988) argues that computer-based proofs of\ncorrectness can still be regarded as a-priori, in that even\nthough their possibility depends on sensory experience, their\njustification does not (as it is for a-posteriori knowledge).\nFor instance, the knowledge that red is a color is a-priori\neven though it requires having sensory experience of red; this is\nbecause ‘red is a colour’ is true independently of any\nsensory experience. For further discussion on the nature of the use of\ncomputers in mathematical proofs, see (Hales 2008; Harrison 2008;\nTymoczko 1979, 1980). \nThe problem of correctness eventually reduces to asking what it means\nfor a physical machine to satisfy an abstract requirement. According\nto the simple mapping account, a computational system\nS is a correct implementation of specification SP\nonly if: \nThe simple mapping account only demands for an extensional agreement\nbetween the description of S and SP. The weakness of\nthis account is that it is quite easy to identify an extensional\nagreement between any couple of physical system-specification, leaving\nroom for a pancomputationalist perspective. \nThe danger of pancomputationalism has led some authors to attempt an\naccount of correct implementation that somehow restricts the class of\npossible interpretations. In particular, \nFrom what has been said so far, it follows that correctness of\nimplemented programs does not automatically establish the\nwell-functioning of a computational system. Turing (1950) already\ndistinguished between errors of functioning and errors of\nconclusion. The former are caused by a faulty implementation\nunable to execute the instructions of some high-level language\nprogram; errors of conclusion characterize correct abstract machines\nthat nonetheless fail to carry out the tasks they were supposed to\naccomplish. This may happen in those cases in which a program\ninstantiates correctly some specifications which do not properly\nexpress the users’ requirements on such a program. In both\ncases, machines implementing correct programs can still be said to\nmiscompute. \nTuring’s distinction between errors of functioning and errors of\nconclusion has been expanded into a complete taxonomy of\nmiscomputations (Fresco and Primiero 2013). The classification is\nestablished on the basis of the different LoAs defining computational\nsystems. Errors can be: \nPerformable errors clearly emerge only at the execution level, and\nthey correspond with Turing’s (1950) error of functioning, also\ncalled operational malfunctions. Conceptual and material\nerrors may arise at any level of abstraction from the intention level\ndown to the physical implementation level. Conceptual errors engender\nmistakes, while material errors induce failures. For\ninstance, a mistake at the intention level consists of an inconsistent\nset of requirements, while at the physical implementation level it may\ncorrespond to an invalid hardware design (such as in the choice of the\nlogic gates for the truth-functional connectives). Failures occurring\nat the specification level may be due to a design that is deemed to be\nincomplete with respect to the set of desired functional requirements,\nwhile a failure at the algorithm level occurs in those frequent cases\nin which the algorithm is found not to fulfill the specifications.\nBeyond mistakes, failures, and operational malfunctions,\nslips are a source of miscomputations at the high-level\nprogramming language instructions level: they may be conceptual or\nmaterial errors due to, respectively, a syntactic or a semantic flaw\nin the program. Conceptual slips appear in all those cases in which\nthe syntactical rules of high-level languages are violated; material\nslips involve the violation of semantic rules of programming\nlanguages, such as when a variable is used but not initialized. \nA further distinction has to be made between dysfunctions and\nmisfunctions for software-based computational systems\n(Floridi, Fresco and Primiero 2015). Software can only misfunction but\ncannot ever dysfunction. A software token can dysfunction in case its\nphysical implementation fails to satisfy intentions or specifications.\nDysfunctions only apply to single tokens since a token dysfunctions in\nthat it does not behave as the other tokens of the same type do with\nrespect to the implemented functions. For this reason, dysfunctions do\nnot apply to the intention level and the specification level. On the\ncontrary, both software types and tokens can misfunction, since\nmisfunctions do not depend on comparisons with tokens of the same type\nbeing able to perform some implemented function or not. Misfunction of\ntokens usually depends on the dysfunction of some other component,\nwhile misfunction of types is often due to poor design. A software\ntoken cannot dysfunction, because all tokens of a given type implement\nfunctions specified uniformly at the intention and specification\nlevels. Those functions are implemented at the algorithm\nimplementation level before being performed at the execution level; in\ncase of correct implementation, all tokens will behave correctly at\nthe execution level (provided that no operational malfunction occurs).\nFor the very same reason, software tokens cannot misfunction, since\nthey are implementations of the same intentions and specifications.\nOnly software types can misfunction in case of poor design;\nmisfunctioning software types are able to correctly perform their\nfunctions but may also produce some undesired side-effect. For the\napplication of the notion of malfunctioning to the problem of malware\nclassification, see (Primiero et al. 2019). \nBetween the 1960s and the 1970s, computer science emerged as an\nacademic discipline independent from its older siblings, mathematics\nand physics, and with it the problem of defining its epistemological\nstatus as influenced by mathematical, empirical, and engineering\nmethods (Tedre and Sutien 2008, Tedre 2011, Tedre 2015, Primiero\n2020). A debate is still in place today concerning whether computer\nscience has to be mostly considered as a mathematical\ndiscipline, a branch of engineering, or as a scientific\ndiscipline. \nAny epistemological characterization of computer science is based on\nontological, methodological, and epistemological commitments, namely\non assumptions about the nature of computational systems, the methods\nguiding the software development process, and the kind of reasoning\nthereby involved, whether deductive, inductive, or a combination of\nboth (Eden 2007). \nThe origin of the analysis of computation as a mathematical notion\ncame notoriously from logic, with Hilbert's question concerning\nthe decidability of predicate calculus, known as the\nEntschiedungsproblem (Hilbert and Ackermann 1950): could\nthere be a mechanical procedure for deciding of an arbitrary sentence\nof logic whether it is provable? To address this question, a rigorous\nmodel of the informal concept of an effective or mechanical method in\nlogic and mathematics was required. This is first and foremost a\nmathematical endeavor: one has to develop a mathematical analogue of\nthe informal notion. Supporters of the view that computer science is\nmathematical in nature assume that a computer program can be seen as a\nphysical realization of such a mathematical entity and that one can\nreason about programs deductively through the formal methods of\ntheoretical computer science. Dijkstra (1974) and Hoare (1986)\nwere very explicit in considering programs’ instructions as\nmathematical sentences, and considering a formal semantics for\nprogramming languages in terms of an axiomatic system (Hoare 1969).\nProvided that program specifications and instructions are advanced in\nthe same formal language, formal semantics provide the means to prove\ncorrectness. Accordingly, knowledge about the behaviors of\ncomputational systems is acquired by the deductive reasoning involved\nin mathematical proofs of correctness. The reason at the basis of such\na rationalist optimism (Eden 2007) about what can be known about\ncomputational systems is that they are artifacts, that is,\nhuman-made systems and, as such, one can predict their\nbehaviors with certainty (Knuth 1974). \nAlthough a central concern of theoretical computer science, the topics\nof computability and complexity are covered in existing entries on the\n Church-Turing thesis,\n computational complexity theory, and\n recursive functions. \nIn the late 1970s, the increasing number of applications of\ncomputational systems in everyday contexts, and the consequent booming\nof market demands caused a deviation of interests for computer\nscientists in Academia and in Industry: from focusing on methods of\nproving programs’ correctness, they turned to methods for\nmanaging complexity and evaluating the reliability of those system\n(Wegner 1976). Indeed, expressing formally the specifications,\nstructure, and input of highly complex programs embedded in larger\nsystems and interacting with users is practically impossible, and\nhence providing mathematical proofs of their correctness becomes\nmostly unfeasible. Computer science research developed in the\ndirection of testing techniques able to provide a statistical\nevaluation of correctness, often called reliability (Littlewood and\nStrigini 2000), in terms of estimation of error distributions in a\nprogram’s code. \nIn line with this engineering account of computer science is the\nthesis that reliability of computational systems is evaluated in the\nsame way that civil engineering does for bridges and aerospace\nengineering for airplanes (DeMillo et al. 1979). In\nparticular, whereas empirical sciences examine what exists, computer\nscience focuses on what can exist, i.e., on how to produce\nartifacts, and it should be therefore acknowledged as an\n“engineering of mathematics” (Hartmanis 1981). Similarly,\nwhereas scientific inquiries are involved in discovering laws\nconcerning the phenomena under observation, one cannot identify proper\nlaws in computer science practice, insofar as the latter is rather\ninvolved in the production of phenomena concerning computational\nartifacts (Brooks 1996). \nAs examined in\n §6,\n because software testing and reliability measuring techniques are\nknown for their incapability of assuring for the absence of code\nfaults (Dijkstra 1970), in many cases, and especially for the\nevaluation of the so-called safety-critical systems (such as\ncontrollers of airplanes, rockets, nuclear plants etc..), a\ncombination of formal methods and empirical testing is used to\nevaluate correctness and dependability. Computer science can\naccordingly be understood as a scientific discipline, in that it makes\nuse of both deductive and inductive probabilistic reasoning to examine\ncomputational systems (Denning et al. 1981, 2005, 2007; Tichy 1998;\nColburn 2000). \nThe thesis that computer science is, from a methodological viewpoint,\non a par with empirical sciences traces back to Newell, Perlis, and\nSimon’s 1967 letter to Science (Newell et al. 1967) and\ndominated all the 1980’s (Wegner 1976). In the 1975 Turing Award\nlecture, Newell and Simon argued: \nSince Newell and Simon’s Turing award lecture, it has been clear\nthat computer science can be understood as an empirical science but of\na special sort, and this is related to the nature of experiments in\ncomputing. Indeed, much current debate on the epistemological status\nof computer science concerns the problem of defining what kind of\nscience it is (Tedre 2011, Tedre 2015) and, in particular, the nature\nof experiments in computer science (Schiaffonati and Verdicchio 2014),\nthe nature, if any, of laws and theorems in computing (Hartmanis 1993;\nRombach and Seelish 2008), and the methodological relation between\ncomputer science and software engineering (Gruner 2011).","contact.mail":"nangius@uniss.it","contact.domain":"uniss.it"},{"date.published":"2013-08-20","date.changed":"2021-01-19","url":"https://plato.stanford.edu/entries/computer-science/","author1":"Nicola Angius","author2":"Giuseppe Primiero","author1.info":"https://uniss.academia.edu/NicolaAngius","author2.info":"http://cswww.essex.ac.uk/staff/turnr/","entry":"computer-science","body.text":"\n\n\nThe philosophy of computer science is concerned with the ontological\nand methodological issues arising from within the academic discipline\nof computer science, and from the practice of software development and\nits commercial and industrial deployment. More specifically, the\nphilosophy of computer science considers the ontology and epistemology\nof computational systems, focusing on problems associated with their\nspecification, programming, implementation, verification and testing.\nThe complex nature of computer programs ensures that many of the\nconceptual questions raised by the philosophy of computer science have\nrelated ones in the\n philosophy of mathematics,\n the philosophy of empirical sciences, and the\n philosophy of technology.\n We shall provide an analysis of such topics that reflects the layered\nnature of the ontology of computational systems in Sections 1–5;\nwe then discuss topics involved in their methodology in Sections\n6–8.\n\nComputational systems are widespread in everyday life. Their design,\ndevelopment and analysis are the proper object of study of the\ndiscipline of computer science. The philosophy of computer science\ntreats them instead as objects of theoretical analysis. Its first aim\nis to define such systems, i.e., to develop an ontology of\ncomputational systems. The literature offers two main approaches on\nthe topic. A first one understands computational systems as defined by\ndistinct ontologies for software and hardware, usually taken to be\ntheir elementary components. A different approach sees computational\nsystems as comprising several other elements around the\nsoftware-hardware dichotomy: under this second view, computational\nsystems are defined on the basis of a hierarchy of levels of\nabstraction, arranging hardware levels at the bottom of such a\nhierarchy and extending upwards to elements of the design and\ndownwards to include the user. In the following we present these two\napproaches. \nUsually, computational systems are seen as composed of two\nontologically distinct entities: software and hardware. Algorithms,\nsource codes, and programs fall in the first category of abstract\nentities; microprocessors, hard drives, and computing machines are\nconcrete, physical entities. \nMoore (1978) argues that such a duality is one of the three myths of\ncomputer science, in that the dichotomy software/hardware has a\npragmatic, but not an ontological, significance. Computer programs, as\nthe set of instructions a computer may execute, can be examined both\nat the symbolic level, as encoded instructions, and at the physical\nlevel, as the set of instructions stored in a physical medium. Moore\nstresses that no program exists as a pure abstract entity, that is,\nwithout a physical realization (a flash drive, a hard disk on a\nserver, or even a piece of paper). Early programs were even hardwired\ndirectly and, at the beginning of the computer era, programs consisted\nonly in patterns of physical levers. By the software/hardware\nopposition, one usually identifies software with the symbolic level of\nprograms, and hardware with the corresponding physical level. The\ndistinction, however, can be only pragmatically justified in that it\ndelimits the different tasks of developers. For them, software may be\ngiven by algorithms and the source code implementing them, while\nhardware is given by machine code and the microprocessors able to\nexecute it. By contrast, engineers realizing circuits implementing\nhardwired programs may be inclined to call software many physical\nparts of a computing machine. In other words, what counts as software\nfor one professional may count as hardware for another one. \nSuber (1988) goes even further, maintaining that hardware is a kind of\nsoftware. Software is defined as any pattern that is amenable to being\nread and executed: once one realizes that all physical objects display\npatterns, one is forced to accept the conclusion that hardware, as a\nphysical object, is also software. Suber defines a pattern as\n“any definite structure, not in the narrow sense that requires\nsome recurrence, regularity, or symmetry” (1988, 90) and argues\nthat any such structure can indeed be read and executed: for any\ndefinite pattern to which no meaning is associated, it is always\npossible to conceive a syntax and a semantics giving a meaning,\nthereby making the pattern an executable program. \nColburn (1999, 2000), while keeping software and hardware apart,\nstresses that the former has a dual nature, it is a “concrete\nabstraction” as being both abstract and concrete. To define\nsoftware, one needs to make reference to both a “medium of\ndescription”, i.e., the language used to express an algorithm,\nand a “medium of execution”, namely the circuits composing\nthe hardware. While software is always concrete in that there is no\nsoftware without a concretization in some physical medium, it is\nnonetheless abstract, because programmers do not consider the\nimplementing machines in their activities: they would rather develop a\nprogram executable by any machine. This aspect is called by Colburn\n(1999) “enlargement of content” and it defines abstraction\nin computer science as an “abstraction of content”:\ncontent is enlarged rather than deleted, as it happens with\nmathematical abstraction. \nIrmak (2012) criticizes the dual nature of software proposed by\nColburn (1999, 2000). He understands an abstract entity as one lacking\nspatio-temporal properties, while being concrete means having those\nproperties. Defining software as a concrete abstraction would\ntherefore imply for software to have contradictory properties.\nSoftware does have temporal properties: as an object of human\ncreation, it starts to exist at some time once conceived and\nimplemented; and it can cease to exist at a certain subsequent time.\nSoftware ceases to exist when all copies are destroyed, their authors\ndie and nobody else remembers the respective algorithms. As an object\nof human creation, software is an artifact. However, software lacks\nspatial properties in that it cannot be identified with any concrete\nrealization of it. Destroying all the physical copies of a given\nsoftware would not imply that a particular software ceases to exist,\nas stated above, nor, for the very same reason, would deleting all\ntexts implementing the software algorithms in some high-level\nlanguage. Software is thus an abstract entity endowed with temporal\nproperties. For these reasons, Irmak (2010) definies software as an\nabstract artifact. \nDuncan (2011) points out that distinguishing software from hardware\nrequires a finer ontology than the one involving the simple\nabstract/concrete dichotomy. Duncan (2017) aims at providing such an\nontology by focusing on Turner’s (2011) notion of specification\nas an expression that gives correctness conditions for a program (see\n §2).\n Duncan (2017) stresses that a program acts also as a specification\nfor the implementing machine, meaning that a program specifies all\ncorrect behaviors that the machine is required to perform. If the\nmachine does not act consistently with the program, the machine is\nsaid to malfunction, in the same way a program which is not correct\nwith respect to its specification is said to be flawed or containing a\nbug. Another ontological category necessary to define the distinction\nsoftware/hardware is that of artifact, which Duncan (2017) defines as\na physical, spatio-temporal entity, which has been constructed so as\nto fulfill some functions and such that there is a community\nrecognizing the artifact as serving that purpose. That said, software\nis defined as a set of instructions encoded in some programming\nlanguage which act as specifications for an artifact able to read\nthose instructions; hardware is defined as an artifact whose function\nis to carry out the specified computation. \nAs shown above, the distinction between software and hardware is not a\nsharp one. A different ontological approach to computational systems\nrelies on the role of abstraction. Abstraction is a crucial element in\ncomputer science, and it takes many different forms. Goguen &\nBurstall (1985) describe some of this variety, of which the following\nexamples are instances. Code can be repeated during programming, by\nnaming text and a parameter, a practice known as procedural\nabstraction. This operation has its formal basis in the abstraction\noperation of the lambda calculus (see the entry on the\n lambda calculus)\n and it allows a formal mechanism known as polymorphism (Hankin 2004).\nAnother example is typing, typical of functional programming, which\nprovides an expressive system of representation for the syntactic\nconstructors of the language. Or else, in object-oriented design,\npatterns (Gamma et al. 1994) are abstracted from the common structures\nthat are found in software systems and used as interfaces between the\nimplementation of an object and its specification. \nAll these examples share an underlying methodology in the Levels of\nAbstraction (henceforth LoA), used also in mathematics (Mitchelmore\nand White 2004) and philosophy (Floridi 2008). Abstractions in\nmathematics are piled upon each other in a never-ending search for\nmore and more abstract concepts. On this account, abstraction is\nself-contained: an abstract mathematical object takes its meaning only\nfrom the system within which it is defined and the only constraint is\nthat new objects be related to each other in a consistent system that\ncan be operated on without reference to previous or external meanings.\nSome argue that, in this respect at least, abstraction in computer\nscience is fundamentally different from abstraction in mathematics:\ncomputational abstraction must leave behind an implementation trace\nand this means that information is hidden but not destroyed (Colburn\n& Shute 2007). Any details that are ignored at one LoA must not be\nignored by one of the lower LoAs: for example, programmers need not\nworry about the precise location in memory associated with a\nparticular variable, but the virtual machine is required to handle all\nmemory allocations. This reliance of abstraction on different levels\nis reflected in the property of computational systems to depend upon\nthe existence of an implementation: for example, even though classes\nhide details of their methods, they must have implementations. Hence,\ncomputational abstractions preserve both an abstract guise and an\nimplementation. \nA full formulation of LoAs for the ontology of digital computational\nsystems has been devised in Primiero (2016), including: \nIntention is the cognitive act that defines a computational\nproblem to be solved: it formulates the request to create a\ncomputational process to perform a certain task. Requests of this sort\nare usually provided by customers, users, and other stakeholders\ninvolved in a given software development project.\nSpecification is the formulation of the set of requirements\nnecessary for solving the computational problem at hand: it concerns\nthe possibly formal determination of the operations the software must\nperform, through the process known as requirements elicitation.\nAlgorithm expresses the procedure providing a solution to the\nproposed computational problem, one which must meet the requirements\nof the specification. High-level programming language (such\nas C, Java, or Python) instructions constitute the linguistic\nimplementation of the proposed algorithm, often called the source\ncode, and they can be understood by trained programmers but cannot be\ndirectly executed by a machine. The instructions coded in high-level\nlanguage are compiled, i.e., translated, by a compiler into\nassembly code and then assembled in machine code\noperations, executable by a processor. Finally, the\nexecution LoA is the physical level of the running software,\ni.e., of the computer architecture executing the instructions. \nAccording to this view, no LoA taken in isolation is able to define\nwhat a computational system is, nor to determine how to distinguish\nsoftware from hardware. Computational systems are rather defined by\nthe whole abstraction hierarchy; each LoA in itself expresses a\nsemantic level associated with a realization, either linguistic or\nphysical. \nIntention refers to a cognitive state outside the computational system\nwhich expresses the formulation of a computational problem to be\nsolved. Specifications describe the functions that the\ncomputational system to be developed must fulfil. Whereas\nintentions, per se, do not pose specific\nphilosophical controversies inside the philosophy of computer\nscience, issues arise in connection with the definition of what a\nspecification is and its relation with intentions. \nIntentions articulate the criteria to determine whether a\ncomputational system is appropriate (i.e., correct, see\n §7),\n and therefore it is considered as the first LoA of the computational\nsystem appropriate to that problem. For instance, customers and users\nmay require a smartphone app able to filter out annoying calls from\ncall centers; such request constitutes the intention LoA in the\ndevelopment of a computational system able to perform such a task. In\nthe software development process of non-naive systems, intentions are\nusually gathered by such techniques as brainstorming, surveys,\nprototyping, and even focus groups (Clarke and Moreira 1999), aimed at\ndefining a structured set of the various stakeholders’\nintentions. At this LoA, no reference is made to how to solve\nthe computational problem, but only the description of the problem\nthat must be solved is provided. \nIn contemporary literature, intentions have been the object of\nphilosophical inquiry at least since Anscombe (1963). Philosophers\nhave investigated “intentions with which” an action is\nperformed (Davidson 1963), intentions of doing something in the future\n(Davidson 1978), and intentional actions (Anscombe 1963, Baier 1970,\nFerrero 2017). Issues arise concerning which of the three kinds of\nintention is primary, how they are connected, the relation between\nintentions and belief, whether intentions are or presuppose specific\nmental states, and whether intentions act as causes of actions (see\nthe entry on\n intention).\n More formal problems concern the opportunity for an agent of having\ninconsistent intentions and yet being considered rational (Bratman\n1987, Duijf et al. 2019). \nIn their role as the first LoA in the ontology of computational\nsystems, intentions can certainly be acknowledged as intentions for\nthe future, in that they express the objective of constructing systems\nable to perform some desired computational tasks. Since intentions, as\nstated above, confine themselves to the definition of the\ncomputational problem to be solved, without specifying its\ncomputational solution, their ontological and epistemological analysis\ndoes not differ from those referred to in the philosophical\nliterature. In other words, there is nothing specifically\ncomputational in the intentions defining computational systems which\ndeserves a separate treatment in the philosophy of computer science.\nWhat matters here is the relation between intention and specification,\nin that intentions provide correctness criteria for specifications;\nspecifications are asked to express how the computational problem put\nforward by intentions is to be solved. \nConsider the example of the call filtering app again; a\nspecification may require to create a black-list of phone numbers\nassociated with call centers; to update the list every n\ndays; to check, upon an incoming call, whether the number is on the\nblack-list; to communicate to the call management system not to\nallow the incoming call in case of an affirmative answer, and to allow\nthe call in case of negative answer. \nThe latter is a full-fledged specification, though expressed in a\nnatural language. Specifications are often advanced in a natural\nlanguage to be closer to the stakeaholder’s intentions and only\nsubsequently they are formalized in a proper formal language.\nSpecifications may be expressed by means of graphical languages such\nas UML (Fowler 2003), or more formal languages such as TPL (Turner\n2009a) and VDM (Jones 1990), using predicate logic, or Z (Woodcock and\nDavies 1996), focusing on set theory. For instance, Type Predicate\nLogic (TPL) expresses the requirements of computational systems using\npredicate logic formulas, wherein the type of the quantified variables\nis specified. The choice of the variable types allows one to define\nspecifications at the more appropriate abstraction level. Whether\nspecifications are expressed in an informal or formal guise often\ndepends on the development method followed, with formal specifications\nusually preferred in the context of formal development methods.\nMoreover, formal specifications facilitate verification of correctness\nfor computational systems (see\n §6). \nTurner (2018) asks what difference is there between models and\nspecifications, both of which are extensively used in computer\nscience. The difference is located in what Turner (2011) calls the\nintentional stance: models describe an intended\nsystem to be developed and, in case of a mismatch between the two, the\nmodels are to be refined; specifications prescribe how the\nsystem is to be built so as to comply with the intended functions, and\nin case of mismatch it is the system that needs to be refined.\nMatching between model and system reflects a correspondence between\nintentions — describing what system is to be\nconstructed in terms of the computational problem the system must be\nable to solve — and specifications — determining\nhow the system is to be constructed, in terms of the set of\nrequirements necessary for solving the computational problem, as\nexemplified for the call filtering app. In Turner’s (2011)\nwords, “something is a specification when it is given\ncorrectness jurisdiction over an artefact”: specifications\nprovide correctness criteria for computational systems. Computational\nsystems are thus correct when they comply with their specifications,\nthat is, when they behave according to them. Conversely, they provide\ncriteria of malfunctioning\n (§7.3):\n a computational system malfunctions when it does not behave\nconsistently with its specifications. Turner (2011) is careful to\nnotice that such a definition of specifications is an idealization:\nspecifications are themselves revised in some cases, such as when the\nspecified computational systems cannot be realized because of physical\nlaws constraints or cost limitations, or when it turns out that the\nadvanced specifications are not correct formalizations of the\nintentions of clients and users. \nMore generally, the correctness problem does not only deal with\nspecifications, but with any two LoAs defining computational systems,\nas the next subsection will examine. \nFully implemented and constructed computational systems are\ntechnical artifacts, i.e., human-made systems designed and\nimplemented with the explicit aim of fulfilling specific functions\n(Kroes 2012). Technical artifacts so defined include tables,\nscrewdrivers, cars, bridges, or televisions, and they are distinct\nboth from natural objects (e.g. rocks, cats, or dihydrogen monoxide\nmolecules), which are not human-made, and artworks, which do not\nfulfill functions. As such, the ontology of computational systems\nfalls under that of technical artifacts (Meijers 2000) characterized\nby a duality, as they are defined by both functional\nand structural properties (Kroes 2009, see also the entry on\n philosophy of technology).\n Functional properties specify the functions the artifact is required\nto perform; structural properties express the physical properties\nthrough which the artifact can perform them. Consider a screwdriver:\nfunctional properties may include the function of screwing and\nunscrewing; structural properties can refer to a piece of metal\ncapable of being inserted on the head of the screw and a plastic\nhandle that allows a clockwise and anticlockwise motion. Functions can\nbe realized in multiple ways by their structural counterparts. For\ninstance, the function for the screwdriver could well be realized by a\nfull metal screwdriver, or by an electric screwdriver defined by very\ndifferent structural properties. \nThe layered ontology of computational systems characterized by many\ndifferent LoAs seems to extend the dual ontology defining technical\nartifacts (Floridi et al. 2015). Turner (2018) argues that\ncomputational systems are still artifacts in the sense of (Kroes 2009,\n2012), as each LoA is a functional level for lower LoAs and a\nstructural level for upper LoAs: \nIt follows, according to Turner (2018), that structural levels need\nnot be necessarily physical levels, and that the notion of abstract\nartifact holds in computer science. For this reason, Turner (2011)\ncomes to define high-level language programs themselves as technical\nartifacts, in that they constitute a structural level implementing\nspecifications as their functional level\n (see §4.2). \nA first consequence is that each LoA – expressing\nwhat function to accomplish – can be realized by a\nmultiplicity of potential structural levels expressing how\nthose functions are accomplished: an intended functionality can be\nrealized by a specification in multiple ways; a computational problem\nexpressed by a specification has solutions by a multiplicity of\ndifferent algorithms, which can differ for some important properties\nbut are all equally valid (see\n §3);\n an algorithm may be implemented in different programs, each written\nin a different high-level programming language, all expressing the\nsame program if they implement the same algorithm (Angius and\nPrimiero 2019); source code can be compiled in a multiplicity of\nmachine languages, adopting different ISAs (Instruction Set\nArchitectures); executable code can be installed and run on a\nmultiplicity of machines (provided that these share the same ISA). \nA second consequence is that each LoA as a functional level provides\ncorrectness criteria for lower levels (Primiero 2020). Not just at the\nimplementation level, correctness is required at any LoA from\nspecification to execution, and the cause of malfunctions may be\nlocated at any LoA not correctly implementing its proper functional\nlevel (see\n §7.3 and\n Fresco, Primiero (2013)). According to Turner (2018), the\nspecification level can be said to be correct or incorrect with\nrespect to intentions, despite the difficulty of verifying their\ncorrectness. Correctness of any non-physical layer can be verified\nmathematically through formal verification, and the execution physical\nlevel can be verified empirically, through testing\n (§6).\n Verifying correctness of specifications with respect to\nclients’ intentions would require instead having access to the\nmental states of the involved agents. \nThis latter problem relates to the more general one of establishing\nhow artifacts possess functions, and what it means that structural\nproperties are related to the intentions of agents. The problem is\nwell-known also in the philosophy of biology and the cognitive\nsciences, and two main theories have been put forward as solutions.\nAccording to the causal theory of function (Cummins 1975),\nfunctions are determined by the physical capacities of artifacts: for\nexample, the physical ability of the heart of contracting and\nexpanding determines its function of pumping blood in the circulatory\nsystem. However, this theory faces serious problems when applied to\ntechnical artifacts. First, it prevents defining correctness and\nmalfunctioning (Kroes 2010): suppose the call filtering app installed\non our smartphone starts banning calls from contacts in our mobile\nphonebook; according to the causal theory of function this would be a\nnew function of the app. Second, the theory does not distinguish\nintended functions from side effects (Turner 2011): in case of a\nlong-lasting call, our smartphone would certainly start heating;\nhowever, this is not a function intended by clients or developers.\nAccording to the intentional theory of function (McLaughlin\n2001, Searle 1995), the function fixed by the designer or the user is\nthe intended one of the artifact, and structural properties of\nartifacts are selected so as to be able to fulfill it. This theory is\nable to explain correctness and malfunction, as well as to distinguish\nside effects from intended functions. However, it does not say where\nthe function actually resides, whether in the artifact or in the mind\nof the agent. In the former case, one is back at the question of how\nartifacts possess functions. In the latter case, a further explanation\nis needed about how mental states are related to physical properties\nof artifacts (Kroes 2010). Turner (2018) holds that the intuitions\nbehind both the causal and the intentional theories of function are\nuseful to understand the relation between function and structure in\ncomputational systems, and suggests that the two theories be combined\ninto a single one. On the one hand, there is no function without\nimplementation; on the other hand, there is no intention without\nclients, developers, and users. \nEven though known and widely used since antiquity, the problem of\ndefining what algorithms are is still open (Vardi 2012). The word\n“algorithm” originates from the name of the\nninth-century Persian mathematician Abū Jaʿfar\nMuḥammad ibn Mūsā al-Khwārizmī, who\nprovided rules for arithmetic operations using Arabic numerals.\nIndeed, the rules one follows to compute basic arithmetic operations\nsuch as multiplication or division, are everyday examples of\nalgorithms. Other well-known examples include rules to bisect an angle\nusing compass and straightedge, or Euclid’s algorithm for\ncalculating the greatest common divisor. Intuitively, an algorithm is\na set of instructions allowing the fulfillment of a given task.\nDespite this ancient tradition in mathematics, only modern logical and\nphilosophical reflection put forward the task of providing a\ndefinition of what an algorithm is, in connection with the\nfoundational crisis of mathematics of the early twentieth century (see\nthe entry on the\n philosophy of mathematics).\n The notion of effective calculability arose from logical\nresearch, providing some formal counterpart to the intuitive notion of\nalgorithm and giving birth to the theory of computation. Since then,\ndifferent definitions of algorithms have been proposed,\nranging from formal to non-formal approaches, as sketched in the\nnext sections. \nMarkov (1954) provides a first precise definition of algorithm as a\ncomputational process that is determined,\napplicable, and effective. A computational process\nis determined if the instructions involved are precise enough\nnot to allow for any “arbitrary choice” in their\nexecution. The (human or artificial) computer must never be\nunsure about what step to carry out next. Algorithms are\napplicable for Markov in that they hold for classes of inputs\n(natural numbers for basic arithmetic operations) rather than for\nsingle inputs (specific natural numbers). Markov (1954:1) defines\neffectiveness as “the tendency of the algorithm to\nobtain a certain result”. In other words, an algorithm is\neffective in that it will eventually produce the answer to the\ncomputational problem. \nKleene (1967) specifies finiteness as a further important\nproperty: an algorithm is a procedure which can be described by means\nof a finite set of instructions and needs a finite number of steps to\nprovide an answer to the computational problem. As a counterexample,\nconsider a while loop defined by a finite number of steps,\nbut which runs forever since the condition in the loop is always\nsatisfied. Instructions should also be amenable to mechanical\nexecution, that is, no insight is required for the machine to follow\nthem. Following Markov’s determinability and strengthening\neffectiveness, Kleene (1967) additionally specifies that instructions\nshould be able to recognize that the solution to the computational\nproblem has been achieved, and halt the computation. \nKnuth (1973) recalls and deepens the analyses of Markov (1954) and\nKleene (1967) by stating that: \nBesides merely being a finite set of rules that gives a sequence of\noperations for solving a specific type of problem, an algorithm has\nfive important features: \nAs in Kleene (1967), finiteness affects both the number of\ninstructions and the number of implemented computational steps. As in\nMarkov’s determinacy, Knuth’s definiteness principle\nrequires that each successive computational step be unambiguously\nspecified. Furthermore, Knuth (1973) more explicitly requires that\nalgorithms have (potentially empty sets of) inputs and outputs. By\nalgorithms with no inputs or outputs Knuth probably refers to\nalgorithms using internally stored data as inputs or algorithms not\nreturning data to an external user (Rapaport 2019, ch. 7, in Other\nInternet Resources). As for effectiveness, besides Markov’s\ntendency “to obtain a certain result”, Knuth requires that\nthe result be obtained in a finite amount of time and that the\ninstructions be atomic, that is, simple enough to be understandable\nand executable by a human or artificial computer. \nGurevich (2011) maintains, on the one hand, that it is not possible to\nprovide formal definitions of algorithms, as the notion continues to\nevolve over time: consider how sequential algorithms, used in\nancient mathematics, are flanked by parallel, analog, or quantum\nalgorithms in current computer science practice, and how new kinds of\nalgorithms are likely to be envisioned in the near future. On the\nother hand, a formal analysis can be advanced if concerned only with\nclassical sequential algorithms. In particular, Gurevich (2000)\nprovides an axiomatic definition for this class of algorithms. \nAny sequential algorithm can be simulated by a sequential abstract\nstate machine satisfying three axioms: \nMoschovakis (2001) objects that the intuitive notion of algorithm is\nnot captured in full by abstract machines. Given a general recursive\nfunction f: ℕ → ℕ defined on natural\nnumbers, there are usually many different algorithms computing it;\n“essential, implementation-independent properties” are not\ncaptured by abstract machines, but rather by a system of recursive\nequations. Consider the algorithm mergesort for sorting\nlists; there are many different abstract machines for\nmergesort, and the question arises which one is to be chosen\nas the mergesort algorithm. The mergesort algorithm\nis instead the system of recursive equations specifying the involved\nfunction, whereas abstract machines for the mergesort\nprocedure are different implementations of the same\nalgorithm. Two questions are put forward by Moschovakis’\nformal analysis: different implementations of the same\nalgorithm should be equivalent implementations, and yet, an\nequivalence relation among algorithm implementations is to be formally\ndefined. Furthermore, it remains to be clarified what the intuitive\nnotion of algorithm formalized by systems of recursive equations\namounts to. \nPrimiero (2020) proposes a reading of the nature of algorithms at\nthree different levels of abstraction. At a very high LoA, algorithms\ncan be defined abstracting from the procedure they describe, allowing\nfor many different sets of states and transitions. At this LoA\nalgorithms can be understood as informal specifications, that\nis, as informal descriptions of a procedure P. At a lower\nLoA, algorithms specify the instructions needed to solve the given\ncomputational problem; in other words, they specify a procedure.\nAlgorithms can thus be defined as procedures, or descriptions\nin some given formal language L of how to execute a procedure\nP. Many important properties of algorithms, including those\nrelated to complexity classes and data structures, cannot be\ndetermined at the procedural LoA, and instead make reference to an\nabstract machine implementing the procedure is needed. At a bottom\nLoA, algorithms can be defined as implementable abstract\nmachines, viz. as the specification, in a formal language\nL, of the executions of a program P for a given\nabstract machine M. The threefold definition of algorithms\nallows Primiero (2020) to supply a formal definition of equivalence\nrelations for algorithms in terms of the algebraic notions of\nsimulation and bisimulation (Milner 1973, see also\nAngius and Primero 2018). A machine Mi executing a\nprogram Pi implements the same algorithm of a\nmachine Mj executing a program\nPj if and only if the abstract machines\ninterpreting Mi and Mj are in\na bisimulation relation. \nVardi (2012) underlines how, despite the many formal and informal\ndefinitions available, there is no general consensus on what an\nalgorithm is. The approaches of Gurevich (2000) and Moschovakis\n(2001), which can even be proved to be logically equivalent, only\nprovide logical constructs for algorithms, leaving unanswered the main\nquestion. Hill (2013) suggests that an informal definition of\nalgorithms, taking into account the intuitive understanding one has\nabout algorithms, may be more useful, especially for the public\ndiscourse and the communication between practitioners and users. \nRapaport (2012, Appendix) provides an attempt to summarize the three\nclassical definitions of algorithm sketched above stating that: \nRapaport stresses that an algorithm is a procedure, i.e., a finite\nsequence of statements taking the form of rules or instructions.\nFiniteness is here expressed by requiring that instructions contain a\nfinite number of symbols from a finite alphabet. \nHill (2016) aims at providing an informal definition of algorithm,\nstarting from Rapaport’s (2012): \nFirst of all, algorithms are compound structures rather than\natomic objects, i.e., they are composed of smaller units, namely\ncomputational steps. These structures are finite and effective, as\nexplicitly mentioned by Markov, Kleene, and Knuth. While these authors\ndo not explicitly mention abstractness, Hill (2016) maintains it is\nimplicit in their analysis. Algorithms are abstract simply in\nthat they lack spatio-temporal properties and are independent from\ntheir instances. They provide control, that is,\n“content that brings about some kind of change from one state to\nanother, expressed in values of variables and consequent\nactions” (p. 45). Algorithms are imperatively given, as\nthey command state transitions to carry out specified operations.\nFinally, algorithms operate to achieve certain purposes under\nsome usually well-specified provisions, or preconditions.\nFrom this viewpoint, the author argues, algorithms are on a par with\nspecifications in their specifying a goal under certain resources.\nThis definition allows to distinguish algorithms from other compound\ncontrol structures. For instance, recipes are not algorithms because\nthey are not effective; nor are games, which are not imperatively\ngiven. \nThe ontology of computer programs is strictly related to the subsumed\nnature of computational systems (see\n §1).\n If computational systems are defined on the basis of the\nsoftware-hardware dichotomy, programs are abstract entities\ninterpreting the former and opposed to the concrete nature of\nhardware. Examples of such interpretations are provided in\n §1.1\n and include the “concrete abstraction” definition by\nColburn (2000), the “abstract artifact” characterization\nby Irmak (2012), and programs as specifications of machines proposed\nby Duncan (2011). By contrast, under the interpretation of\ncomputational systems by a hierarchy of LoAs, programs are\nimplementations of algorithms. We refer to\n §5\n on implementation for an analysis of the ontology of programs in this\nsense. This section focuses on definitions of programs with a\nsignificant relevance in the literature, namely those views that\nconsider programs as theories or as artifacts, with a focus on the\nproblem of the relation between programs and the world. \nThe view that programs are theories goes back to approaches in\ncognitive science. In the context of the so-called Information\nProcessing Psychology (IPP) for the simulative investigation on human\ncognitive processes, Newell and Simon (1972) advanced the thesis that\nsimulative programs are empirical theories of their simulated\nsystems. Newell and Simon assigned to a computer program the role of\ntheory of the simulated system as well as of the simulative system,\nnamely the machine running the program, to formulate predictions on\nthe simulated system. In particular, the execution traces of the\nsimulative program, given a specific problem to solve, are used to\npredict the mental operation strategies that will be performed by the\nhuman subject when asked to accomplish the same task. In case of a\nmismatch between execution traces and the verbal reports of the\noperation strategies of the human subject, the empirical theory\nprovided by the simulative program is revised. The predictive use of\nsuch a computer program is comparable, according to Newell and Simon,\nto the predictive use of the evolution laws of a system that are\nexpressed by differential or difference equations. \nNewell and Simon’s idea that programs are theories has been\nshared by the cognitive scientists Pylyshyn (1984) and Johnson-Laird\n(1988). Both agree that programs, in contrast to typical theories, are\nbetter at facing the complexity of the simulative process to\nbe modelled, forcing one to fill-in all the details that are necessary\nfor the program to be executed. Whereas incomplete or incoherent\ntheories may be advanced at some stage of scientific inquiry, this is\nnot the case for programs. \nOn the other hand, Moore (1978) considers the programs-as-theories\nthesis another myth of computer science. As programs can\nonly simulate some set of empirical phenomena, at most they play the\nrole of computational models of those phenomena. Moore\nnotices that for programs to be acknowledged as models, semantic\nfunctions are nevertheless needed to interpret the empirical system\nbeing simulated. However, the view that programs are models should not\nbe mistaken for the definition of programs as theories: theories\nexplain and predict the empirical phenomena\nsimulated by models, while simulation by programs does not offer\nthat. \nAccording to computer scientist Paul Thagard (1984), understanding\nprograms as theories would require a syntactic or a\nsemantic view of theories (see the entry on\n the structure of scientific theories).\n But programs do not comply with either of the two views. According to\nthe syntactic view (Carnap 1966, Hempel 1970), theories are sets of\nsentences expressed in some defined language able to describe target\nempirical systems; some of those sentences define the axioms of the\ntheory, and some are law-like statements expressing regularities of\nthose systems. Programs are sets of instructions written in some\ndefined programming language which, however, do not describe any\nsystem, insofar as they are procedural linguistic entities and not\ndeclarative ones. To this, Rapaport (2020, see Other Internet\nResources) objects that procedural programming languages can often be\ntranslated into declarative languages and that there are languages,\nsuch as Prolog, that can be interpreted both procedurally and\ndeclaratively. According to the semantic view (Suppe 1989, Van\nFraassen 1980), theories are introduced by a collection of models,\ndefined as set-theoretic structures satisfying the theory’s\nsentences. However, in contrast to Moore (1978), Thagard (1984) denies\nprograms the epistemological status of models: programs simulate\nphysical systems without satisfying theories’ laws and axioms.\nRather, programs include, for simulation purposes, implementation\ndetails for the programming language used, but not of the target\nsystem being simulated. \nA yet different approach to the problem of whether programs are\ntheories comes from the computer scientist Peter Naur (1985).\nAccording to Naur, programming is a theory building process not in the\nsense that programs are theories, but because the successful\nprogram’s development and life-cycle require that programmers\nand developers have theories of programs available. A theory is here\nunderstood, following Ryle (2009), as a corpus of knowledge shared by\na scientific community about some set of empirical phenomena, and not\nnecessarily expressed axiomatically or formally. Theories of\nprograms are necessary during the program life-cycle to be able to\nmanage requests of program modifications pursuant to observed\nmiscomputations or unsatisfactory solutions to the computational\nproblem the program was asked to solve. In particular, theories of\nprograms should allow developers to modify the program so that new\nsolutions to the problem at stake can be provided. For this reason,\nNaur (1985) deems such theories more fundamental, in software\ndevelopment, than documentations and specifications. \nFor Turner (2010, 2018 ch. 10), programming languages are mathematical\nobjects defined by a formal grammar and a formal semantics. In\nparticular, each syntactic construct, such as an assignment, a\nconditional or a while loop, is defined by a grammatical rule\ndetermining its syntax, and by a semantic rule associating a meaning\nto it. Depending on whether an operational or a denotational semantics\nis preferred, meaning is given in terms of respectively the operations\nof an abstract machine or of mathematical partial functions from set\nof states to set of states. For instance, the simple assignment\nstatement \\(x := E\\) is associated, under an operational semantics,\nwith the machine operation \\(update(s,x,v)\\) which assigns variable\n\\(v\\) interpreted as \\(E\\) to variable \\(x\\) in state \\(s\\). Both in\nthe case of an operational and of a denotational semantics, programs\ncan be understood as mathematical theories expressing the operations\nof an implementing machine. Consider operational semantics: a\nsyntactic rule of the form \\(\\langle P,s \\rangle \\Downarrow s'\\)\nstates semantically that program \\(P\\) executed in state \\(s\\) results\nin \\(s'.\\) According to Turner (2010, 2018), a programming language\nwith an operational semantics is akin to an axiomatic theory of\noperations in which rules provide axioms for the relation\n\\(\\Downarrow\\). \nPrograms can be understood as technical artifacts because programming\nlanguages are defined, as any other artifact, on the basis of both\nfunctional and structural properties (Turner 2014, 2018 ch. 5).\nFunctional properties of (high level) programming languages are\nprovided by the semantics associated with each syntactic construct of\nthe language. Turner (2014) points out that programming languages can\nindeed be understood as axiomatic theories only when their functional\nlevel is isolated. Structural properties, on the other hand, are\nspecified in terms of the implementation of the language, but not\nidentified with physical components of computing machines: given a\nsyntactic construct of the language with an associated functional\ndescription, its structural property is determined by the physical\noperations that a machine performs to implement an instruction for the\nconstruct at hand. For instance, the assignment construct \\(x := E\\)\nis to be linked to the physical computation of the value of expression\n\\(E\\) and to the placement of the value of \\(E\\) in the physical\nlocation \\(x\\). \nAnother requirement for a programming language to be considered a\ntechnical artifact is that it has to be endowed with a semantics\nproviding correctness criteria for the language implementation. The\nprogrammer attests to functional and structural properties of a\nprogram by taking the semantics to have correctness jurisdiction over\nthe program. \nThe problem of whether computer programs are theories is tied with the\nrelation that programs entertain with the outside world. If programs\nwere theories, they would have to represent some empirical system, and\na semantic relation would be directly established between the program\nand the world. By contrast, some have argued that the relation between\nprograms and natural systems is mediated by models of the outside\nworld (Colburn et al. 1993, Smith 1985). In particular, Smith\n(1985) argues that models are abstract descriptions of empirical\nsystems, and computational systems operating in them have programs\nthat act as models of the models, i.e., they represent abstract models\nof reality. Such an account of the ontology of programs comes in handy\nwhen describing the correctness problem in computer science (see\n § 7):\n if specifications are considered as models requiring certain\nbehaviors from computational systems, programs can be seen as models\nsatisfying specifications. \nTwo views of programs can be given depending on whether one admits\ntheir relation with the world (Rapaport 2020, ch. 17, see Other\nInternet Resource). According to a first view, programs are\n“wide”, “external” and “semantic”:\nthey grant direct reference to objects of an empirical system and\noperations on those objects. According to a second view, programs are\n“narrow”, “internal”, and\n“syntactic”: they make only reference to the atomic\noperations of an implementing machine carrying out computations.\nRapaport (2020, see Other Internet Resources) argues that programs\nneed not be “external” and\n“semantic”. First, computation itself needs not to be\n“external”: a Turing machine executes the instructions\ncontained in its finite table by using data written on its tape and\nhalting after the data resulting from the computation have been\nwritten on the tape. Data are not, strictly speaking, in-put-from\nand out-put-to an external user. Furthemore, Knuth (1973) required\nalgorithms to have zero or more inputs and outputs (see\n § 3.1).\n A computer program requiring no inputs may be a program, say,\noutputting all prime numbers from 1; and a program with no outputs can\nbe a program that computes the value of some given variable x without\nreturning the value stored in x as output. Second, programs need not\nbe “external”, teleological, i.e., goal oriented. This\nview opposes other known positions in the literature. Suber (1988)\nargues that, without considering goals and purposes, it would not be\npossible to assess whether a computer program is correct, that is, if\nit behaves as intended. And as recalled in\n §3.3.,\n Hill (2016) specifies in her informal definition that algorithms\naccomplish “a given purpose, under given provisions.”\n(Hill 2016: 48). To these views, Rapaport (2020, ch. 17, see Other\nInternet Resource) replies that whereas goals, purposes, and\nprogrammers’ intentions may be very useful for a human computor\nto understand a program, they are not necessary for an artificial\ncomputer to carry out the computations instructed by the program code.\nIndeed, the principle of effectiveness that classical approaches\nrequire for algorithms (see\n §3.1)\n demands, among other properties, that algorithms be executed without\nany recourse to intuition. In other words, a machine executing a\nprogram for adding natural numbers does not “understand”\nthat it is adding; at the same time, knowing that a given program\nperforms addition may help a human agent to understand the\nprogram’s code. \nAccording to this view, computing involves just symbols, not meanings.\nTuring machines become symbols manipulators and not a single but\nmultiple meanings can be associated with its operations. How can then\none identify when two programs are the same program, if not\nby their meanings, that is, by considering what function they perform?\nOne answer comes from Piccini’s analysis of computation and its\n“internal semantics” (Piccini 2008, 2015 ch. 3):\ntwo programs can be identified as identical by analysing only their\nsyntax and the operations the programs carry out on their symbols. The\neffects of string manipulation operations can be considered an\ninternal semantics of a program. The latter can be easily determined\nby isolating subroutines or methods in the program’s code and\ncan afterwards be used to identify a program or to establish whether\ntwo programs are the same, namely when they are defined by the same\nsubroutines. \nHowever, it has been argued that there are cases in which it is not\npossible to determine whether two programs are the same without making\nreference to an external semantics. Sprevak (2010) proposes to\nconsider two programs for addition which differ from the fact that one\noperates on Arabic, the other one on Roman numerals. The two programs\ncompute the same function, namely addition, but this cannot always be\nestablished by inspecting the code with its subroutines; it must be\ndetermined by assigning content to the input/output strings,\ninterpreting Arabic and Roman numerals as numbers. In that regard,\nAngius and Primiero (2018) underline how the problem of identity for\ncomputer programs does not differ from the problem of identity for\nnatural kinds (Lowe 1998) and technical artifacts (Carrara et al.\n2014). The problem can be tackled by fixing an identity criterion,\nnamely a formal relation, that any two programs should entertain in\norder to be defined as identical. Angius and Primiero (2018) show how\nto use the process algebra relation of bisimulation between the two\nautomata implemented by two programs under examination as such an\nidentity criterion. Bisimulation allows to establish matching\nstructural properties of programs implementing the same function, as\nwell as providing weaker criteria for copies in terms of simulation.\nThis brings the discussion back to the notion of programs as\nimplementations. We now turn to analyze this latter concept. \nThe word ‘implementation’ is often associated with a\nphysical realization of a computing system, i.e., to a machine\nexecuting a computer program. In particular, according to the dual\nontology of computing systems examined in\n §1.1,\n implementation in this sense reduces to the structural hardware, as\nopposed to the functional software. By contrast, following the method\nof the levels of abstraction\n (§ 1.2),\n implementation becomes a wider relation holding between any LoA\ndefining a computational system and the levels higher in the\nhierarchy. Accordingly, an algorithm is an implementation of a (set\nof) specification(s); a program expressed in a high level programming\nlanguage can be defined as an implementation of an algorithm (see\n §4);\n assembly and machine code instructions can be seen as an\nimplementation of a set of high-level programming language\ninstructions with respect to a given ISA; finally, executions are\nphysical, observable, implementations of those machine code\ninstructions. By the same token, programs formulated in a high-level\nlanguage are also implementations of specifications, and, as similarly\nargued by the dual-ontology paradigm, executions are implementations\nof high-level programming language instructions. According to Turner\n(2018), even the specification can be understood as an implementation\nof what has been called intention. \nWhat remains to be examined here is the nature of the implementation\nrelation thus defined. Analyzing this relation is essential to define\nthe notion of correctness\n (§7).\n Indeed, a correct program amounts to a correct implementation of an\nalgorithm; and a correct computing system is a correct implementation\nof a set of specifications. In other words, under this view, the\nnotion of correctness is paired with that of implementation for any\nLoA: any level can be said to be correct with respect to upper levels\nif and only if it is a correct implementation thereof. \nThe following three subsections examine three main definitions of the\nimplementation relation that have been advanced in the philosophy of\ncomputer science literature. \nA first philosophical analysis of the notion of implementation in\ncomputer science is advanced by Rapaport (1999, 2005). He defines an\nimplementation I as the semantic interpretation of a\nsyntactic or abstract domain A in a medium of implementation\nM. If implementation is understood as a relation holding\nbetween a given LoA and any upper level in the hierarchical ontology\nof a computational system, it follows that Rapaport’s definition\nextends accordingly, so that any LoA provides a semantic\ninterpretation in a given medium of implementation for the upper\nlevels. Under this view, specifications provide semantic\ninterpretations of intentions expressed by stakeholders in the\nspecification (formal) language, and algorithms provide semantic\ninterpretations of specifications using one of the many languages\nalgorithms can be formulated in (natural languages, pseudo-code, logic\nlanguages, functional languages etc.). The medium of implementation\ncan be either abstract or concrete. A computer program is the\nimplementation of an algorithm in that the former provides a semantic\ninterpretation of the syntactic constructs of the latter in a\nhigh-level programming language as its medium of implementation. The\nprogram’s instructions interpret the algorithm's tasks in a\nprogramming language. Also the execution LoA provides a semantic\ninterpretation of the assembly/machine code operations into the medium\ngiven by the structural properties of the physical machine. According\nto the analysis in (Rapaport 1999, 2005), implementation is an\nasymmetric relation: if I is an implementation of A,\nA cannot be an implementation of I. However, the\nauthor argues that any LoA can be both a syntactic and a semantic\nlevel, that is, it can play the role of both the implementation I and\nof a syntactic domain A. Whereas an algorithm is assigned a semantic\ninterpretation by a program expressed in a high-level language, the\nsame algorithm provides a semantic interpretation for the\nspecification. It follows that the abstraction-implementation relation\npairs the functional-structural relation for computational\nsystems. \nPrimiero (2020) considers this latter aspect as one main limit of\nRapaport’s (1999, 2005) account of implementation:\nimplementation reduces to a unique relation between a\nsyntactic level and its semantic interpretation and it does not\naccount for the layered ontology of computational systems seen in\n §1.2.\n In order to extend the present definition of implementation to all\nLoAs, each level has to be reinterpreted each time either as syntactic\nor as a semantic level. This, in turn, has a repercussion on the\nsecond difficulty characterizing, according to Primero (2020),\nimplementation as a semantic interpretation: on the one hand, this\napproach does not take into account incorrect\nimplementations; on the other hand, for a given incorrect\nimplementation, the unique relation so defined can relate\nincorrectness only to one syntactic level, excluding all other levels\nas potential error locations. \nTurner (2018) aims to show that semantic interpretation not only does\nnot account for incorrect implementation, but not even to correct\nones. One first example is provided by the implementation of one\nlanguage into another: the implementing language here is not providing\na semantic interpretation of the implemented language, unless the\nformer is associated with a semantics providing meaning and\ncorrectness criteria for the latter. Such semantics will remain\nexternal to the implementation relation: whereas correctness is\nassociated with semantic interpretation, implementation does not\nalways come with a semantic interpretation. A second example is given\nby considering an abstract stack implemented by an array; again, the\narray does not provide correctness criteria for the stack. Quite to\nthe contrary, it is the stack that specifies correctness criteria for\nany of its implementation, arrays included. \nThe fact that correctness criteria for the implementation relation are\nprovided by the abstract level induces Turner (2012, 2014, 2018)\nto define implementation as the relation\nspecification-artefact. As examined in\n §2,\n specifications have correctness jurisdiction over artifacts, that is,\nthey prescribe the allowed behaviors of artifacts. Also recall that\nartifacts can be both abstract and concrete entities, and that any LoA\ncan play the role of specification for lower levels. This amounts to\nsaying that the specification-artefact relation is able to define any\nimplementation relation across the layered ontology of computational\nsystems. \nDepending on how the specification-artifact relation is defined,\nTurner (2012) distinguishes as many as three different notions of\nimplementation. Consider the case of a physical machine implementing a\ngiven abstract machine. According to an intentional notion of\nimplementation, an abstract machine works as a specification for a\nphysical machine, provided it advances all the functional requirements\nthe latter must fulfill, i.e., it specifies (in principle) all the\nallowed behaviors of the implementing physical machine. According to\nan extensional notion of implementation, a physical machine\nis a correct implementation of an abstract machine if and only if\nisomorphisms can be established mapping states of the latter to states\nof the former, and transitions in the abstract machine correspond to\nactual executions (computational traces) of the artifact. Finally, an\nempirical notion of implementation requires the physical\nmachine to display computations that match those prescribed by the\nabstract machine; that is to say, correct implementation has to be\nevaluated empirically through testing. \nPrimiero (2020) underlines how, while this approach addresses the\nissue of correctness and miscomputation as it allows to distinguish a\ncorrect from an incorrect implementation, it still identifies a unique\nimplementation relation between a specification level and an artifact\nlevel. Again, if this account is allowed to involve the layered\nontology of computational systems by reinterpreting each time any LoA\neither as a specification or artifact, Turner’s account prevents\nfrom referring to more than one level at the same time as the cause of\nmiscomputation: a miscomputation always occurs here as an\nincorrect implementation of a specification by an artifact. By\ndefining implementation as a relation holding accross all the LoAs,\none would be able to identify multiple incorrect implementations which\ndo not directly refer to the abstract specification. A miscomputation\nmay indeed be caused by an incorrect implementation of lower levels\nwhich is then inherited all the way down to the execution level. \nPrimiero (2020) proposes a definition of implementation not as a\nrelation between two fixed levels, but one that is allowed to range\nover any LoA. Under this view, an implementation I is a\nrelation of instantiation holding between a LoA and any other\none higher in the abstraction hierarchy. Accordingly, a physical\ncomputing machine is an implementation of assembly/machine code\noperations; by transitivity, it can also be considered as an\ninstantiation of a set of instructions expressed in high-level\nprogramming language instructions. A program expressed in a high-level\nlanguage is an implementation of an algorithm; but it can also be\ntaken to be the instantiation of a set of specifications. \nSuch a definition of implementation allows Primiero (2020) to provide\na general definition of correctness: a physical computing system is\ncorrect if and only if it is characterized by correct implementations\nat any LoA. Hence correctness and implementation are coupled and\ndefined at any LoA. Functional correctness is the property of\na computational system that displays the functionalities required by\nthe specifications of that system. Procedural correctness\ncharacterizes computational systems displaying the functionalities\nintended by the implemented algorithms. And executional\ncorrectness is defined as the property of a system that is able\nto correctly execute the program on its architecture. Each of these\nforms of correctness can also be classified quantitatively, depending\non the amount of functionalities being satisfied. A functionally\nefficient computational system displays a minimal subset of\nthe functionalities required by the specifications; a functionally\noptimal system is able to display a maximal subset of those\nfunctionalities. Similarly, the author defines procedurally as well as\nexecutionally efficient and optimal computational systems. \nAccording to this definition, implementation shifts from level to\nlevel: a set of algorithms defining a computational system are\nimplemented as procedures in some formal language, as\ninstructions in a high-level language, or as operations in a\nlow-level programming language. An interesting question is whether\nany system, beyond computational artifacts, implementing\nprocedures of this sort qualifies as a computational system. In other\nwords, asking about the nature of physical implementation amounts to\nasking what is a computational system. If any system implementing an\nalgorithm would qualify as computational, the class of such systems\ncould be extended to biological systems, such as the brain or the\ncell; to physical systems, including the universe or some portion of\nit; and eventually to any system whatsoever, a thesis known as\npancomputationalism (for an exhaustive overview on the topic\nsee Rapaport 2018). \nTraditionally, a computational system is intended as a mechanical\nartifact that takes input data, elaborates them\nalgorithmically according to a set of instructions, and\nreturns manipulated data as outputs. For instance, von Neumann (1945,\np.1) states that “An automatic computing system is a (usually\nhighly composite) device, which can carry out instructions to perform\ncalculations of a considerable order of complexity”. Such an\ninformal and well-accepted definition leaves some questions open,\nincluding whether computational systems have to be machines, whether\nthey have to process data algorithmically and, consequently, whether\ncomputations have to be Turing complete. \nRapaport (2018) provides a more explicit characterization of a\ncomputational system defined as any “physical plausible\nimplementation of anything logically equivalent to a universal Turing\nmachine”. Strictly speaking personal computers are not physical\nTuring machines, but register machines are known to be Turing\nequivalent. To qualify as computational, systems must be\nplausible implementations thereof, in that Turing machines,\ncontrary to physical machines, have access to infinite memory space\nand are, as abstract machines, error free. According to\nRapaport’s (2018) definition, any physical\nimplementation of this sort is thus a computational system, including\nnatural systems. This raises the question about which class of natural\nsystems is able to implement Turing equivalent computations. Searle\nfamously argued that anything can be an implementation of a Turing\nmachine, or of a logical equivalent model (Searle 1990). His argument\nlevers on the fact that being a Turing machine is a syntactic\nproperty, in that it is all about manipulating tokens of 0’s and\n1’s. According to Searle, syntactic properties are not intrinsic\nto physical systems, but they are assigned to them by an observer. In\nother words, a physical state of a system is not intrinsically a\ncomputational state: there must be an observer, or user, who assigns\nto that state a computational role. It follows that any system whose\nbehavior can be described as syntactic manipulation of 0’s and\n1’s is a computational system. \nHayes (1997) objects to Searle (1990) that if everything was a\ncomputational system, the property “being a computational\nsystem” would become vacuous, as all entities would possess it.\nInstead, there are entities which are computational systems, and\nentities which are not. Computational systems are those in which the\npatterns received as inputs and saved into memory are able to change\nthemselves. In other words, Hayes makes reference to the fact that\nstored inputs can be both data and instructions and that instructions,\nwhen executed, are able to modify the value of some input data.\n“If it were paper, it would be ‘magic paper’ on\nwhich writing might spontaneously change, or new writing appear”\n(Hayes 1997, p. 393). Only systems able to act as “magic\npaper” can be acknowledged as computational. \nA yet different approach comes from Piccinini (2007, 2008) in the\ncontext of his mechanistic analysis of physical computations\n(Piccinini 2015; see also the entry on\n computation in physical systems).\n A physical computing system is a system whose behaviors can be\nexplained mechanistically by describing the computing\nmechanism that brings about those behaviors. Mechanisms can be defined\nby “entities and activities organized such that they are\nproductive of regular changes from start or set-up to finish or\ntermination condition” (Machamer et al. 2000; see the entry on\n mechanisms in science).\n Computations, as physical processes, can be understood as those\nmechanisms that “generate output strings from input strings in\naccordance with general rules that apply to all input strings and\ndepend on the input (and sometimes internal states)” (Piccinini\n2007, p. 108). It is easy to identify set-up and termination\nconditions for computational processes. Any system which can be\nexplained by describing an underlying computing mechanism is to be\nconsidered a computational system. The focus on explanation helps\nPiccinini avoid the Searlean conclusion that any system is a\ncomputational system: even if one may interpret, in principle, any\ngiven set of entities and activities as a computing mechanism, only\nthe need to explain a certain observed phenomenon in terms of a\ncomputing mechanism defines the system under examination as\ncomputational. \nA crucial step in the software development process is verification.\nThis consists in the process of evaluating whether a given\ncomputational system is correct with respect to the specification of\nits design. In the early days of the computer industry, validity and\ncorrectness checking methods included several design and construction\ntechniques, see for example (Arif et al. 2018). Nowadays,\ncorrectness evaluation methods can be roughly sorted into two main\ngroups: formal verification and testing. Formal verification (Monin\nand Hinchey 2003) involves a proof of correctness with mathematical\ntools; software testing (Ammann and Offutt 2008) rather consists in\nrunning the implemented program to observe whether performed\nexecutions comply or not with the advanced specifications. In many\npractical cases, a combination of both methods is used (see for\ninstance Callahan et al. 1996). \nFormal verification methods require a representation of the\nsoftware under verification. In theorem proving (see van\nLeeuwen 1990), programs are represented in terms of axiomatic systems\nand a set of rules of inference representing the pre- and\npost-conditions of program transitions. A proof of correctness is then\nobtained by deriving formulas expressing specifications from the\naxioms. In model checking (Baier and Katoen 2008), a program\nis represented in terms of a state transition system, its property\nspecifications are formalised by temporal logic formulas (Kröger\nand Merz 2008), and a proof of correctness is achieved by a\ndepth-first search algorithm that checks whether those formulas hold\nof the state transition system. \nAxiomatic systems and state transition systems used for correctness\nevaluation can be understood as theories of the represented\nartifacts, in that they are used to predict and explain their future\nbehaviors. Methodologically state transition systems in model checking\ncan be compared with scientific models in empirical sciences (Angius\nand Tamburrini 2011). For instance, Kripke Structures (see Clarke\net al. 1999 ch. 2) are in compliance with Suppes’\n(1960) definition of scientific models as set-theoretic structures\nestablishing proper mapping relations with models of data collected by\nmeans of experiments on the target empirical system (see also the\nentry on\n models in science).\n Kripke Structures and other state transition systems utilized in\nformal verification methods are often called system specifications.\nThey are distinguished from common specifications, also called\nproperty specifications. The latter specify some required behavioral\nproperties the program to be encoded must instantiate, while the\nformer specify (in principle) all potential executions of an already\nencoded program, thus allowing for algorithmic checks on its traces\n(Clarke et al. 1999). In order to achieve this goal, system\nspecifications are considered as abductive structures,\nhypothesizing the set of potential executions of a target\ncomputational system on the basis of the program’s code and the\nallowed state transitions (Angius 2013b). Indeed, once it has been\nchecked whether some temporal logic formula holds of the modeled\nKripke Structure, the represented program is empirically tested\nagainst the behavioral property corresponding to the checked formula,\nin order to evaluate whether the model-hypothesis is an adequate\nrepresentation of the target computational system. Accordingly,\nproperty specifications and system specifications differ also in their\nintentional stance (Turner 2011): the former are requirements\non the program to be encoded, the latter are (hypothetical)\ndescriptions of the encoded program. The descriptive and abductive\ncharacter of state transition systems in model checking is an\nadditional and essential feature putting state transition systems on a\npar with scientific models. \nTesting is the more ‘empirical’ process of launching a\nprogram and observing its executions in order to evaluate whether they\ncomply with the supplied property specifications. Such technique is\nextensively used in the software development process. Philosophers and\nphilosophically-minded computer scientists have considered software\ntesting under the light of traditional methodological approaches in\nscientific discovery (Snelting 1998; Gagliardi 2007; Northover et\nal. 2008; Angius 2014) and questioned whether software tests can\nbe acknowledged as scientific experiments evaluating the\ncorrectness of programs (Schiaffonati and Verdicchio 2014,\nSchiaffonati 2015; Tedre 2015). \nDijkstra’s well-known dictum “Program testing can be used\nto show the presence of bugs, but never to show their absence”\n(Dijkstra 1970, p.7), introduces Popper’s (1959) principle of\nfalsifiability into computer science (Snelting 1998). Testing\na program against an advanced property specification for a given\ninterval of time may exhibit some failures, but if no failure occurs\nwhile observing the running program one cannot conclude that the\nprogram is correct. An incorrect execution might be observed at the\nvery next system’s run. The reason is that testers can only\nlaunch the program with a finite subset of the potential\nprogram’s input set and only for a finite interval of time;\naccordingly, not all potential executions of the program to be tested\ncan be empirically observed. For this reason, the aim of software\ntesting is to detect programs’ faults and not to guarantee their\nabsence (Ammann and Offutt 2008, p. 11). A program is falsifiable in\nthat tests can reveal faults (Northover et al. 2008). Hence,\ngiven a computational system and a property specification, a test is\nakin to a scientific experiment which, by observing the system’s\nbehaviors, tries to falsify the hypothesis that the program is correct\nwith respect to the interested specification. \nHowever, other methodological and epistemological traits\ncharacterizing scientific experiments are not shared by software\ntests. A first methodological distinction can be recognized in that a\nfalsifying test leads to the revision of the computational system, not\nof the hypothesis, as in the case of testing scientific hypotheses.\nThis is due to the difference in the intentional stance of\nspecifications and empirical hypotheses in science (Turner 2011).\nSpecifications are requirements whose violation demands for program\nrevisions until the program becomes a correct instantiation of the\nspecifications. \nFor this, among other reasons, the traditional notion of scientific\nexperiment needs to be ‘stretched’ in order to be applied\nto software testing activities (Schiaffonati 2015). Theory-driven\nexperiments, characterizing most of the experimental sciences,\nfind no counterpart in actual computer science practice. If one\nexcludes the cases wherein testing is combined with formal methods,\nmost experiments performed by software engineers are rather\nexplorative, i.e. aimed at ‘exploring’\n“the realm of possibilities pertaining to the functioning of an\nartefact and its interaction with the environment in the absence of a\nproper theory or theoretical background” (Schiaffonati 2015:\n662). Software testers often do not have theoretical control on the\nexperiments they perform; exploration on the behaviors of the\ncomputational system interacting with users and environments rather\nallows testers to formulate theoretical generalizations on the\nobserved behaviors. Explorative experiments in computer science are\nalso characterized by the fact that programs are often tested in a\nreal-like environment wherein testers play the role of users. However,\nit is an essential feature of theory-driven experiments that\nexperimenters do not take part in the experiment to be carried\nout. \nAs a result, while some software testing activities are closer to the\nexperimental activities one finds in empirical sciences, some others\nrather define a new typology of experiment that turns out to belong to\nthe software development process. Five typologies of experiments can\nbe distinguished in the process of specifying, implementing, and\nevaluating computational systems (Tedre 2015): \nA software test is considered successful when miscomputations are\ndetected (assuming that no computational artifact is 100% correct).\nThe successive step is to find out what caused the execution to be\nincorrect, that is, to trace back the fault (more familiarly named\n‘bug’), before proceeding to the debugging phase and then\ntesting the system again. In other words, an explanation of\nthe observed miscomputation is to be advanced. \nEfforts have been made to consider explanations in computer science\n(Piccinini 2007; Piccinini and Craver 2011; Piccinini 2015; Angius and\nTamburrini 2016) in relation to the different models of explanations\nelaborated in the philosophy of science. In particular, computational\nexplanations can be understood as a specific kind of mechanistic\nexplanation (Glennan 1996; Machamer et al. 2000; Bechtel\nand Abrahamsen 2005), insofar as computing processes can be analyzed\nas mechanisms (Piccinini 2007; 2015; see also the entry on\n computation in physical systems). \nConsider a processor executing an instruction. The involved process\ncan be understood as a mechanism whose components are states and\ncombinatory elements in the processor instantiating the functions\nprescribed by the relevant hardware specifications (specifications for\nregisters, for the Arithmetic Logic Unit etc..), organized in such a\nway that they are capable of carrying out the observed execution.\nProviding the description of such a mechanism counts as advancing a\nmechanist explanation of the observed computation, such as the\nexplanation of an operational malfunction. \nFor every type of miscomputation (see\n §7.3),\n a corresponding mechanist explanation can be defined at the adequate\nLoA and with respect to the set of specifications characterizing that\nLoA. Indeed, abstract descriptions of mechanisms still supply one with\na mechanist explanation in the form of a mechanism schema,\ndefined as “a truncated abstract description of a mechanism that\ncan be filled with descriptions of known component parts and\nactivities” (Machamer et al. 2000, p. 15). For\ninstance, suppose the very common case in which a machine miscomputes\nby executing a program containing syntax errors, called slips. The\ncomputing machine is unable to correctly implement the functional\nrequirements provided by the program specifications. However, for\nexplanatory purposes, it would be redundant to provide an explanation\nof the occurred slip at the hardware level of abstraction, by\nadvancing the detailed description of the hardware components and\ntheir functional organization. In such cases, a satisfactory\nexplanation may consist in showing that the program’s code is\nnot a correct instantiation of the provided program specifications\n(Angius and Tamburrini 2016). In order to explain mechanistically an\noccurred miscomputation, it may be sufficient to provide the\ndescription of the incorrect program, abstracting from the rest of the\ncomputing mechanism (Piccinini and Craver 2011). Abstraction is a\nvirtue not only in software development and specification, but also in\nthe explanation of computational systems’ behaviors. \nEach of the different approaches on software verification examined in\nthe previous section assumes a different understanding of correctness\nfor software. Standardly, correctness has been understood as a\nrelation holding between an abstraction and its implementation, such\nthat it holds if the latter fulfills the properties formulated by the\nformer. Once computational systems are described as having a layered\nontology, correctness needs to be reformulated as the relation that\nany structural level entertains with respect to its functional level\n(Primiero, 2020). Hence, correctness can still be considered as a\nmathematical relationship when formulated between abstract and\nfunctional level; while it can be considered as an empirical\nrelationship when formulated between the functional and the\nimplementation levels. One of the earlier debates in the philosophy of\ncomputer science (De Millo et al. 1979; Fetzer 1988) was\nindeed around this distinction. \nFormal verification methods grant an a-priori analysis of the\nbehaviors of programs, without requiring the observation of any of\ntheir implementation or considering their execution. In particular,\ntheorem proving allows one to deduce any potential behavior\nof the program under consideration and its behavioral properties from\na suitable axiomatic representation. In the case of model checking,\none knows in advance the behavioural properties displayed by the\nexecution of a program by performing an algorithmic search of the\nformulas valid in a given set-theoretic model. These considerations\nfamously led Hoare (1969) to conclude that program development is an\n“exact science”, which should be characterized by\nmathematical proofs of correctness, epistemologically on a par with\nstandard proofs in mathematical practice. \nDe Millo et al. (1979) question Hoare’s thesis: correct\nmathematical proofs are usually elegant and\ngraspable, implying that any (expert) reader can\n“see” that the conclusion follows from the premises (for\nthe notion of elegance in software see also Hill (2018)). What are\noften called Cartesian proofs (Hacking 2014) do not have a\ncounterpart in correctness proofs, typically long and cumbersome,\ndifficult to grasp and not explaining why the conclusion necessarily\nfollows from the premises. Yet, many proofs in mathematics are long\nand complex, but they are in principle surveyable, thanks to\nthe use of lemmas, abstractions and the analytic construction of new\nconcepts leading step by step to the statement to be proved.\nCorrectness proofs, on the contrary, do not involve the creation of\nnew concepts, nor the modularity one typically finds in mathematical\nproofs (Turner, 2018). And yet, proofs that are not surveyable cannot\nbe considered mathematical proofs (Wittgenstein 1956). \nA second theoretical difficulty concerning proofs of correctness for\ncomputer programs concerns their complexity and that of the programs\nto be verified. Already Hoare (1981) admitted that while verification\nof correctness is always possible in principle, in practice it is\nhardly achievable. Except for trivial cases, contemporary software is\nmodularly encoded, is required to satisfy a large set of\nspecifications, and it is developed so as to interact with other\nprograms, systems, users. Embedded and reactive software are cases in\npoint. In order to verify such complex software, correctness proofs\nare carried out automatically. Hence, on the one hand, the correctness\nproblem shifts from the program under examination to the program\nperforming the verification, e.g. a theorem prover; on the other hand,\nproofs carried out by a physical process can go wrong, due to\nmechanical mistakes of the machine. Against this infinite regress\nargument, Arkoudas and Bringsjord (2007) argue that one can make use\nof a proof checker which, by being a relatively small program, is\nusually easier to verify. \nMost recently, formal methods for checking correctness based on a\ncombination of logical and statistical analysis have given new\nstimulus to this research area: the ability of Separation Logics\n(Reynolds, 2002) to offer a representation of the logical behavior of\nthe physical memory of computational systems, and the possibility of\nconsidering probabilistic distributions of inputs as statistical\nsource of errors, have allowed formal correctness check of large\ninteractive systems like the Facebook platform (see also Pym et\nal. 2019). \nFetzer (1988) objected that deductive reasoning is only able to\nguarantee for the correctness of a program with respect to its\nspecifications, but not for the correctness of a computational system,\nthat is also accounting for the program’s physical\nimplementation. Even if the program were correct with respect to any\nof the related upper LoAs (algorithms, specifications, requirements),\nits implementation could still violate one or more of the intended\nspecifications due to a physical malfunctioning. The former kind of\ncorrectness can in principle be proved mathematically, but the\ncorrectness of the execution LoA requires an empirical assessment. As\nexamined in\n §6.2,\n software testing can show only in principle the correctness of a\ncomputational system. In practice, the number of allowed executions of\nnon-trivial systems are potentially infinite and cannot be\nexhaustively checked in a finite (or reasonable) amount of time\n(Dijkstra 1974). Most successful testing methods rather see both\nformal verification and testing used together to reach a satisfactory\ncorrection level. \nAnother objection to the theoretical possibility of mathematical\ncorrectness is that since proofs are carried out by a theorem prover,\ni.e. a physical machine, the knowledge one attains about computational\nsystems is not a-priori but empirical (see Turner 2018 ch.\n25). However, Burge (1988) argues that computer-based proofs of\ncorrectness can still be regarded as a-priori, in that even\nthough their possibility depends on sensory experience, their\njustification does not (as it is for a-posteriori knowledge).\nFor instance, the knowledge that red is a color is a-priori\neven though it requires having sensory experience of red; this is\nbecause ‘red is a colour’ is true independently of any\nsensory experience. For further discussion on the nature of the use of\ncomputers in mathematical proofs, see (Hales 2008; Harrison 2008;\nTymoczko 1979, 1980). \nThe problem of correctness eventually reduces to asking what it means\nfor a physical machine to satisfy an abstract requirement. According\nto the simple mapping account, a computational system\nS is a correct implementation of specification SP\nonly if: \nThe simple mapping account only demands for an extensional agreement\nbetween the description of S and SP. The weakness of\nthis account is that it is quite easy to identify an extensional\nagreement between any couple of physical system-specification, leaving\nroom for a pancomputationalist perspective. \nThe danger of pancomputationalism has led some authors to attempt an\naccount of correct implementation that somehow restricts the class of\npossible interpretations. In particular, \nFrom what has been said so far, it follows that correctness of\nimplemented programs does not automatically establish the\nwell-functioning of a computational system. Turing (1950) already\ndistinguished between errors of functioning and errors of\nconclusion. The former are caused by a faulty implementation\nunable to execute the instructions of some high-level language\nprogram; errors of conclusion characterize correct abstract machines\nthat nonetheless fail to carry out the tasks they were supposed to\naccomplish. This may happen in those cases in which a program\ninstantiates correctly some specifications which do not properly\nexpress the users’ requirements on such a program. In both\ncases, machines implementing correct programs can still be said to\nmiscompute. \nTuring’s distinction between errors of functioning and errors of\nconclusion has been expanded into a complete taxonomy of\nmiscomputations (Fresco and Primiero 2013). The classification is\nestablished on the basis of the different LoAs defining computational\nsystems. Errors can be: \nPerformable errors clearly emerge only at the execution level, and\nthey correspond with Turing’s (1950) error of functioning, also\ncalled operational malfunctions. Conceptual and material\nerrors may arise at any level of abstraction from the intention level\ndown to the physical implementation level. Conceptual errors engender\nmistakes, while material errors induce failures. For\ninstance, a mistake at the intention level consists of an inconsistent\nset of requirements, while at the physical implementation level it may\ncorrespond to an invalid hardware design (such as in the choice of the\nlogic gates for the truth-functional connectives). Failures occurring\nat the specification level may be due to a design that is deemed to be\nincomplete with respect to the set of desired functional requirements,\nwhile a failure at the algorithm level occurs in those frequent cases\nin which the algorithm is found not to fulfill the specifications.\nBeyond mistakes, failures, and operational malfunctions,\nslips are a source of miscomputations at the high-level\nprogramming language instructions level: they may be conceptual or\nmaterial errors due to, respectively, a syntactic or a semantic flaw\nin the program. Conceptual slips appear in all those cases in which\nthe syntactical rules of high-level languages are violated; material\nslips involve the violation of semantic rules of programming\nlanguages, such as when a variable is used but not initialized. \nA further distinction has to be made between dysfunctions and\nmisfunctions for software-based computational systems\n(Floridi, Fresco and Primiero 2015). Software can only misfunction but\ncannot ever dysfunction. A software token can dysfunction in case its\nphysical implementation fails to satisfy intentions or specifications.\nDysfunctions only apply to single tokens since a token dysfunctions in\nthat it does not behave as the other tokens of the same type do with\nrespect to the implemented functions. For this reason, dysfunctions do\nnot apply to the intention level and the specification level. On the\ncontrary, both software types and tokens can misfunction, since\nmisfunctions do not depend on comparisons with tokens of the same type\nbeing able to perform some implemented function or not. Misfunction of\ntokens usually depends on the dysfunction of some other component,\nwhile misfunction of types is often due to poor design. A software\ntoken cannot dysfunction, because all tokens of a given type implement\nfunctions specified uniformly at the intention and specification\nlevels. Those functions are implemented at the algorithm\nimplementation level before being performed at the execution level; in\ncase of correct implementation, all tokens will behave correctly at\nthe execution level (provided that no operational malfunction occurs).\nFor the very same reason, software tokens cannot misfunction, since\nthey are implementations of the same intentions and specifications.\nOnly software types can misfunction in case of poor design;\nmisfunctioning software types are able to correctly perform their\nfunctions but may also produce some undesired side-effect. For the\napplication of the notion of malfunctioning to the problem of malware\nclassification, see (Primiero et al. 2019). \nBetween the 1960s and the 1970s, computer science emerged as an\nacademic discipline independent from its older siblings, mathematics\nand physics, and with it the problem of defining its epistemological\nstatus as influenced by mathematical, empirical, and engineering\nmethods (Tedre and Sutien 2008, Tedre 2011, Tedre 2015, Primiero\n2020). A debate is still in place today concerning whether computer\nscience has to be mostly considered as a mathematical\ndiscipline, a branch of engineering, or as a scientific\ndiscipline. \nAny epistemological characterization of computer science is based on\nontological, methodological, and epistemological commitments, namely\non assumptions about the nature of computational systems, the methods\nguiding the software development process, and the kind of reasoning\nthereby involved, whether deductive, inductive, or a combination of\nboth (Eden 2007). \nThe origin of the analysis of computation as a mathematical notion\ncame notoriously from logic, with Hilbert's question concerning\nthe decidability of predicate calculus, known as the\nEntschiedungsproblem (Hilbert and Ackermann 1950): could\nthere be a mechanical procedure for deciding of an arbitrary sentence\nof logic whether it is provable? To address this question, a rigorous\nmodel of the informal concept of an effective or mechanical method in\nlogic and mathematics was required. This is first and foremost a\nmathematical endeavor: one has to develop a mathematical analogue of\nthe informal notion. Supporters of the view that computer science is\nmathematical in nature assume that a computer program can be seen as a\nphysical realization of such a mathematical entity and that one can\nreason about programs deductively through the formal methods of\ntheoretical computer science. Dijkstra (1974) and Hoare (1986)\nwere very explicit in considering programs’ instructions as\nmathematical sentences, and considering a formal semantics for\nprogramming languages in terms of an axiomatic system (Hoare 1969).\nProvided that program specifications and instructions are advanced in\nthe same formal language, formal semantics provide the means to prove\ncorrectness. Accordingly, knowledge about the behaviors of\ncomputational systems is acquired by the deductive reasoning involved\nin mathematical proofs of correctness. The reason at the basis of such\na rationalist optimism (Eden 2007) about what can be known about\ncomputational systems is that they are artifacts, that is,\nhuman-made systems and, as such, one can predict their\nbehaviors with certainty (Knuth 1974). \nAlthough a central concern of theoretical computer science, the topics\nof computability and complexity are covered in existing entries on the\n Church-Turing thesis,\n computational complexity theory, and\n recursive functions. \nIn the late 1970s, the increasing number of applications of\ncomputational systems in everyday contexts, and the consequent booming\nof market demands caused a deviation of interests for computer\nscientists in Academia and in Industry: from focusing on methods of\nproving programs’ correctness, they turned to methods for\nmanaging complexity and evaluating the reliability of those system\n(Wegner 1976). Indeed, expressing formally the specifications,\nstructure, and input of highly complex programs embedded in larger\nsystems and interacting with users is practically impossible, and\nhence providing mathematical proofs of their correctness becomes\nmostly unfeasible. Computer science research developed in the\ndirection of testing techniques able to provide a statistical\nevaluation of correctness, often called reliability (Littlewood and\nStrigini 2000), in terms of estimation of error distributions in a\nprogram’s code. \nIn line with this engineering account of computer science is the\nthesis that reliability of computational systems is evaluated in the\nsame way that civil engineering does for bridges and aerospace\nengineering for airplanes (DeMillo et al. 1979). In\nparticular, whereas empirical sciences examine what exists, computer\nscience focuses on what can exist, i.e., on how to produce\nartifacts, and it should be therefore acknowledged as an\n“engineering of mathematics” (Hartmanis 1981). Similarly,\nwhereas scientific inquiries are involved in discovering laws\nconcerning the phenomena under observation, one cannot identify proper\nlaws in computer science practice, insofar as the latter is rather\ninvolved in the production of phenomena concerning computational\nartifacts (Brooks 1996). \nAs examined in\n §6,\n because software testing and reliability measuring techniques are\nknown for their incapability of assuring for the absence of code\nfaults (Dijkstra 1970), in many cases, and especially for the\nevaluation of the so-called safety-critical systems (such as\ncontrollers of airplanes, rockets, nuclear plants etc..), a\ncombination of formal methods and empirical testing is used to\nevaluate correctness and dependability. Computer science can\naccordingly be understood as a scientific discipline, in that it makes\nuse of both deductive and inductive probabilistic reasoning to examine\ncomputational systems (Denning et al. 1981, 2005, 2007; Tichy 1998;\nColburn 2000). \nThe thesis that computer science is, from a methodological viewpoint,\non a par with empirical sciences traces back to Newell, Perlis, and\nSimon’s 1967 letter to Science (Newell et al. 1967) and\ndominated all the 1980’s (Wegner 1976). In the 1975 Turing Award\nlecture, Newell and Simon argued: \nSince Newell and Simon’s Turing award lecture, it has been clear\nthat computer science can be understood as an empirical science but of\na special sort, and this is related to the nature of experiments in\ncomputing. Indeed, much current debate on the epistemological status\nof computer science concerns the problem of defining what kind of\nscience it is (Tedre 2011, Tedre 2015) and, in particular, the nature\nof experiments in computer science (Schiaffonati and Verdicchio 2014),\nthe nature, if any, of laws and theorems in computing (Hartmanis 1993;\nRombach and Seelish 2008), and the methodological relation between\ncomputer science and software engineering (Gruner 2011).","contact.mail":"giuseppe.primiero@unimi.it","contact.domain":"unimi.it"},{"date.published":"2013-08-20","date.changed":"2021-01-19","url":"https://plato.stanford.edu/entries/computer-science/","author1":"Nicola Angius","author2":"Giuseppe Primiero","author1.info":"https://uniss.academia.edu/NicolaAngius","author2.info":"http://cswww.essex.ac.uk/staff/turnr/","entry":"computer-science","body.text":"\n\n\nThe philosophy of computer science is concerned with the ontological\nand methodological issues arising from within the academic discipline\nof computer science, and from the practice of software development and\nits commercial and industrial deployment. More specifically, the\nphilosophy of computer science considers the ontology and epistemology\nof computational systems, focusing on problems associated with their\nspecification, programming, implementation, verification and testing.\nThe complex nature of computer programs ensures that many of the\nconceptual questions raised by the philosophy of computer science have\nrelated ones in the\n philosophy of mathematics,\n the philosophy of empirical sciences, and the\n philosophy of technology.\n We shall provide an analysis of such topics that reflects the layered\nnature of the ontology of computational systems in Sections 1–5;\nwe then discuss topics involved in their methodology in Sections\n6–8.\n\nComputational systems are widespread in everyday life. Their design,\ndevelopment and analysis are the proper object of study of the\ndiscipline of computer science. The philosophy of computer science\ntreats them instead as objects of theoretical analysis. Its first aim\nis to define such systems, i.e., to develop an ontology of\ncomputational systems. The literature offers two main approaches on\nthe topic. A first one understands computational systems as defined by\ndistinct ontologies for software and hardware, usually taken to be\ntheir elementary components. A different approach sees computational\nsystems as comprising several other elements around the\nsoftware-hardware dichotomy: under this second view, computational\nsystems are defined on the basis of a hierarchy of levels of\nabstraction, arranging hardware levels at the bottom of such a\nhierarchy and extending upwards to elements of the design and\ndownwards to include the user. In the following we present these two\napproaches. \nUsually, computational systems are seen as composed of two\nontologically distinct entities: software and hardware. Algorithms,\nsource codes, and programs fall in the first category of abstract\nentities; microprocessors, hard drives, and computing machines are\nconcrete, physical entities. \nMoore (1978) argues that such a duality is one of the three myths of\ncomputer science, in that the dichotomy software/hardware has a\npragmatic, but not an ontological, significance. Computer programs, as\nthe set of instructions a computer may execute, can be examined both\nat the symbolic level, as encoded instructions, and at the physical\nlevel, as the set of instructions stored in a physical medium. Moore\nstresses that no program exists as a pure abstract entity, that is,\nwithout a physical realization (a flash drive, a hard disk on a\nserver, or even a piece of paper). Early programs were even hardwired\ndirectly and, at the beginning of the computer era, programs consisted\nonly in patterns of physical levers. By the software/hardware\nopposition, one usually identifies software with the symbolic level of\nprograms, and hardware with the corresponding physical level. The\ndistinction, however, can be only pragmatically justified in that it\ndelimits the different tasks of developers. For them, software may be\ngiven by algorithms and the source code implementing them, while\nhardware is given by machine code and the microprocessors able to\nexecute it. By contrast, engineers realizing circuits implementing\nhardwired programs may be inclined to call software many physical\nparts of a computing machine. In other words, what counts as software\nfor one professional may count as hardware for another one. \nSuber (1988) goes even further, maintaining that hardware is a kind of\nsoftware. Software is defined as any pattern that is amenable to being\nread and executed: once one realizes that all physical objects display\npatterns, one is forced to accept the conclusion that hardware, as a\nphysical object, is also software. Suber defines a pattern as\n“any definite structure, not in the narrow sense that requires\nsome recurrence, regularity, or symmetry” (1988, 90) and argues\nthat any such structure can indeed be read and executed: for any\ndefinite pattern to which no meaning is associated, it is always\npossible to conceive a syntax and a semantics giving a meaning,\nthereby making the pattern an executable program. \nColburn (1999, 2000), while keeping software and hardware apart,\nstresses that the former has a dual nature, it is a “concrete\nabstraction” as being both abstract and concrete. To define\nsoftware, one needs to make reference to both a “medium of\ndescription”, i.e., the language used to express an algorithm,\nand a “medium of execution”, namely the circuits composing\nthe hardware. While software is always concrete in that there is no\nsoftware without a concretization in some physical medium, it is\nnonetheless abstract, because programmers do not consider the\nimplementing machines in their activities: they would rather develop a\nprogram executable by any machine. This aspect is called by Colburn\n(1999) “enlargement of content” and it defines abstraction\nin computer science as an “abstraction of content”:\ncontent is enlarged rather than deleted, as it happens with\nmathematical abstraction. \nIrmak (2012) criticizes the dual nature of software proposed by\nColburn (1999, 2000). He understands an abstract entity as one lacking\nspatio-temporal properties, while being concrete means having those\nproperties. Defining software as a concrete abstraction would\ntherefore imply for software to have contradictory properties.\nSoftware does have temporal properties: as an object of human\ncreation, it starts to exist at some time once conceived and\nimplemented; and it can cease to exist at a certain subsequent time.\nSoftware ceases to exist when all copies are destroyed, their authors\ndie and nobody else remembers the respective algorithms. As an object\nof human creation, software is an artifact. However, software lacks\nspatial properties in that it cannot be identified with any concrete\nrealization of it. Destroying all the physical copies of a given\nsoftware would not imply that a particular software ceases to exist,\nas stated above, nor, for the very same reason, would deleting all\ntexts implementing the software algorithms in some high-level\nlanguage. Software is thus an abstract entity endowed with temporal\nproperties. For these reasons, Irmak (2010) definies software as an\nabstract artifact. \nDuncan (2011) points out that distinguishing software from hardware\nrequires a finer ontology than the one involving the simple\nabstract/concrete dichotomy. Duncan (2017) aims at providing such an\nontology by focusing on Turner’s (2011) notion of specification\nas an expression that gives correctness conditions for a program (see\n §2).\n Duncan (2017) stresses that a program acts also as a specification\nfor the implementing machine, meaning that a program specifies all\ncorrect behaviors that the machine is required to perform. If the\nmachine does not act consistently with the program, the machine is\nsaid to malfunction, in the same way a program which is not correct\nwith respect to its specification is said to be flawed or containing a\nbug. Another ontological category necessary to define the distinction\nsoftware/hardware is that of artifact, which Duncan (2017) defines as\na physical, spatio-temporal entity, which has been constructed so as\nto fulfill some functions and such that there is a community\nrecognizing the artifact as serving that purpose. That said, software\nis defined as a set of instructions encoded in some programming\nlanguage which act as specifications for an artifact able to read\nthose instructions; hardware is defined as an artifact whose function\nis to carry out the specified computation. \nAs shown above, the distinction between software and hardware is not a\nsharp one. A different ontological approach to computational systems\nrelies on the role of abstraction. Abstraction is a crucial element in\ncomputer science, and it takes many different forms. Goguen &\nBurstall (1985) describe some of this variety, of which the following\nexamples are instances. Code can be repeated during programming, by\nnaming text and a parameter, a practice known as procedural\nabstraction. This operation has its formal basis in the abstraction\noperation of the lambda calculus (see the entry on the\n lambda calculus)\n and it allows a formal mechanism known as polymorphism (Hankin 2004).\nAnother example is typing, typical of functional programming, which\nprovides an expressive system of representation for the syntactic\nconstructors of the language. Or else, in object-oriented design,\npatterns (Gamma et al. 1994) are abstracted from the common structures\nthat are found in software systems and used as interfaces between the\nimplementation of an object and its specification. \nAll these examples share an underlying methodology in the Levels of\nAbstraction (henceforth LoA), used also in mathematics (Mitchelmore\nand White 2004) and philosophy (Floridi 2008). Abstractions in\nmathematics are piled upon each other in a never-ending search for\nmore and more abstract concepts. On this account, abstraction is\nself-contained: an abstract mathematical object takes its meaning only\nfrom the system within which it is defined and the only constraint is\nthat new objects be related to each other in a consistent system that\ncan be operated on without reference to previous or external meanings.\nSome argue that, in this respect at least, abstraction in computer\nscience is fundamentally different from abstraction in mathematics:\ncomputational abstraction must leave behind an implementation trace\nand this means that information is hidden but not destroyed (Colburn\n& Shute 2007). Any details that are ignored at one LoA must not be\nignored by one of the lower LoAs: for example, programmers need not\nworry about the precise location in memory associated with a\nparticular variable, but the virtual machine is required to handle all\nmemory allocations. This reliance of abstraction on different levels\nis reflected in the property of computational systems to depend upon\nthe existence of an implementation: for example, even though classes\nhide details of their methods, they must have implementations. Hence,\ncomputational abstractions preserve both an abstract guise and an\nimplementation. \nA full formulation of LoAs for the ontology of digital computational\nsystems has been devised in Primiero (2016), including: \nIntention is the cognitive act that defines a computational\nproblem to be solved: it formulates the request to create a\ncomputational process to perform a certain task. Requests of this sort\nare usually provided by customers, users, and other stakeholders\ninvolved in a given software development project.\nSpecification is the formulation of the set of requirements\nnecessary for solving the computational problem at hand: it concerns\nthe possibly formal determination of the operations the software must\nperform, through the process known as requirements elicitation.\nAlgorithm expresses the procedure providing a solution to the\nproposed computational problem, one which must meet the requirements\nof the specification. High-level programming language (such\nas C, Java, or Python) instructions constitute the linguistic\nimplementation of the proposed algorithm, often called the source\ncode, and they can be understood by trained programmers but cannot be\ndirectly executed by a machine. The instructions coded in high-level\nlanguage are compiled, i.e., translated, by a compiler into\nassembly code and then assembled in machine code\noperations, executable by a processor. Finally, the\nexecution LoA is the physical level of the running software,\ni.e., of the computer architecture executing the instructions. \nAccording to this view, no LoA taken in isolation is able to define\nwhat a computational system is, nor to determine how to distinguish\nsoftware from hardware. Computational systems are rather defined by\nthe whole abstraction hierarchy; each LoA in itself expresses a\nsemantic level associated with a realization, either linguistic or\nphysical. \nIntention refers to a cognitive state outside the computational system\nwhich expresses the formulation of a computational problem to be\nsolved. Specifications describe the functions that the\ncomputational system to be developed must fulfil. Whereas\nintentions, per se, do not pose specific\nphilosophical controversies inside the philosophy of computer\nscience, issues arise in connection with the definition of what a\nspecification is and its relation with intentions. \nIntentions articulate the criteria to determine whether a\ncomputational system is appropriate (i.e., correct, see\n §7),\n and therefore it is considered as the first LoA of the computational\nsystem appropriate to that problem. For instance, customers and users\nmay require a smartphone app able to filter out annoying calls from\ncall centers; such request constitutes the intention LoA in the\ndevelopment of a computational system able to perform such a task. In\nthe software development process of non-naive systems, intentions are\nusually gathered by such techniques as brainstorming, surveys,\nprototyping, and even focus groups (Clarke and Moreira 1999), aimed at\ndefining a structured set of the various stakeholders’\nintentions. At this LoA, no reference is made to how to solve\nthe computational problem, but only the description of the problem\nthat must be solved is provided. \nIn contemporary literature, intentions have been the object of\nphilosophical inquiry at least since Anscombe (1963). Philosophers\nhave investigated “intentions with which” an action is\nperformed (Davidson 1963), intentions of doing something in the future\n(Davidson 1978), and intentional actions (Anscombe 1963, Baier 1970,\nFerrero 2017). Issues arise concerning which of the three kinds of\nintention is primary, how they are connected, the relation between\nintentions and belief, whether intentions are or presuppose specific\nmental states, and whether intentions act as causes of actions (see\nthe entry on\n intention).\n More formal problems concern the opportunity for an agent of having\ninconsistent intentions and yet being considered rational (Bratman\n1987, Duijf et al. 2019). \nIn their role as the first LoA in the ontology of computational\nsystems, intentions can certainly be acknowledged as intentions for\nthe future, in that they express the objective of constructing systems\nable to perform some desired computational tasks. Since intentions, as\nstated above, confine themselves to the definition of the\ncomputational problem to be solved, without specifying its\ncomputational solution, their ontological and epistemological analysis\ndoes not differ from those referred to in the philosophical\nliterature. In other words, there is nothing specifically\ncomputational in the intentions defining computational systems which\ndeserves a separate treatment in the philosophy of computer science.\nWhat matters here is the relation between intention and specification,\nin that intentions provide correctness criteria for specifications;\nspecifications are asked to express how the computational problem put\nforward by intentions is to be solved. \nConsider the example of the call filtering app again; a\nspecification may require to create a black-list of phone numbers\nassociated with call centers; to update the list every n\ndays; to check, upon an incoming call, whether the number is on the\nblack-list; to communicate to the call management system not to\nallow the incoming call in case of an affirmative answer, and to allow\nthe call in case of negative answer. \nThe latter is a full-fledged specification, though expressed in a\nnatural language. Specifications are often advanced in a natural\nlanguage to be closer to the stakeaholder’s intentions and only\nsubsequently they are formalized in a proper formal language.\nSpecifications may be expressed by means of graphical languages such\nas UML (Fowler 2003), or more formal languages such as TPL (Turner\n2009a) and VDM (Jones 1990), using predicate logic, or Z (Woodcock and\nDavies 1996), focusing on set theory. For instance, Type Predicate\nLogic (TPL) expresses the requirements of computational systems using\npredicate logic formulas, wherein the type of the quantified variables\nis specified. The choice of the variable types allows one to define\nspecifications at the more appropriate abstraction level. Whether\nspecifications are expressed in an informal or formal guise often\ndepends on the development method followed, with formal specifications\nusually preferred in the context of formal development methods.\nMoreover, formal specifications facilitate verification of correctness\nfor computational systems (see\n §6). \nTurner (2018) asks what difference is there between models and\nspecifications, both of which are extensively used in computer\nscience. The difference is located in what Turner (2011) calls the\nintentional stance: models describe an intended\nsystem to be developed and, in case of a mismatch between the two, the\nmodels are to be refined; specifications prescribe how the\nsystem is to be built so as to comply with the intended functions, and\nin case of mismatch it is the system that needs to be refined.\nMatching between model and system reflects a correspondence between\nintentions — describing what system is to be\nconstructed in terms of the computational problem the system must be\nable to solve — and specifications — determining\nhow the system is to be constructed, in terms of the set of\nrequirements necessary for solving the computational problem, as\nexemplified for the call filtering app. In Turner’s (2011)\nwords, “something is a specification when it is given\ncorrectness jurisdiction over an artefact”: specifications\nprovide correctness criteria for computational systems. Computational\nsystems are thus correct when they comply with their specifications,\nthat is, when they behave according to them. Conversely, they provide\ncriteria of malfunctioning\n (§7.3):\n a computational system malfunctions when it does not behave\nconsistently with its specifications. Turner (2011) is careful to\nnotice that such a definition of specifications is an idealization:\nspecifications are themselves revised in some cases, such as when the\nspecified computational systems cannot be realized because of physical\nlaws constraints or cost limitations, or when it turns out that the\nadvanced specifications are not correct formalizations of the\nintentions of clients and users. \nMore generally, the correctness problem does not only deal with\nspecifications, but with any two LoAs defining computational systems,\nas the next subsection will examine. \nFully implemented and constructed computational systems are\ntechnical artifacts, i.e., human-made systems designed and\nimplemented with the explicit aim of fulfilling specific functions\n(Kroes 2012). Technical artifacts so defined include tables,\nscrewdrivers, cars, bridges, or televisions, and they are distinct\nboth from natural objects (e.g. rocks, cats, or dihydrogen monoxide\nmolecules), which are not human-made, and artworks, which do not\nfulfill functions. As such, the ontology of computational systems\nfalls under that of technical artifacts (Meijers 2000) characterized\nby a duality, as they are defined by both functional\nand structural properties (Kroes 2009, see also the entry on\n philosophy of technology).\n Functional properties specify the functions the artifact is required\nto perform; structural properties express the physical properties\nthrough which the artifact can perform them. Consider a screwdriver:\nfunctional properties may include the function of screwing and\nunscrewing; structural properties can refer to a piece of metal\ncapable of being inserted on the head of the screw and a plastic\nhandle that allows a clockwise and anticlockwise motion. Functions can\nbe realized in multiple ways by their structural counterparts. For\ninstance, the function for the screwdriver could well be realized by a\nfull metal screwdriver, or by an electric screwdriver defined by very\ndifferent structural properties. \nThe layered ontology of computational systems characterized by many\ndifferent LoAs seems to extend the dual ontology defining technical\nartifacts (Floridi et al. 2015). Turner (2018) argues that\ncomputational systems are still artifacts in the sense of (Kroes 2009,\n2012), as each LoA is a functional level for lower LoAs and a\nstructural level for upper LoAs: \nIt follows, according to Turner (2018), that structural levels need\nnot be necessarily physical levels, and that the notion of abstract\nartifact holds in computer science. For this reason, Turner (2011)\ncomes to define high-level language programs themselves as technical\nartifacts, in that they constitute a structural level implementing\nspecifications as their functional level\n (see §4.2). \nA first consequence is that each LoA – expressing\nwhat function to accomplish – can be realized by a\nmultiplicity of potential structural levels expressing how\nthose functions are accomplished: an intended functionality can be\nrealized by a specification in multiple ways; a computational problem\nexpressed by a specification has solutions by a multiplicity of\ndifferent algorithms, which can differ for some important properties\nbut are all equally valid (see\n §3);\n an algorithm may be implemented in different programs, each written\nin a different high-level programming language, all expressing the\nsame program if they implement the same algorithm (Angius and\nPrimiero 2019); source code can be compiled in a multiplicity of\nmachine languages, adopting different ISAs (Instruction Set\nArchitectures); executable code can be installed and run on a\nmultiplicity of machines (provided that these share the same ISA). \nA second consequence is that each LoA as a functional level provides\ncorrectness criteria for lower levels (Primiero 2020). Not just at the\nimplementation level, correctness is required at any LoA from\nspecification to execution, and the cause of malfunctions may be\nlocated at any LoA not correctly implementing its proper functional\nlevel (see\n §7.3 and\n Fresco, Primiero (2013)). According to Turner (2018), the\nspecification level can be said to be correct or incorrect with\nrespect to intentions, despite the difficulty of verifying their\ncorrectness. Correctness of any non-physical layer can be verified\nmathematically through formal verification, and the execution physical\nlevel can be verified empirically, through testing\n (§6).\n Verifying correctness of specifications with respect to\nclients’ intentions would require instead having access to the\nmental states of the involved agents. \nThis latter problem relates to the more general one of establishing\nhow artifacts possess functions, and what it means that structural\nproperties are related to the intentions of agents. The problem is\nwell-known also in the philosophy of biology and the cognitive\nsciences, and two main theories have been put forward as solutions.\nAccording to the causal theory of function (Cummins 1975),\nfunctions are determined by the physical capacities of artifacts: for\nexample, the physical ability of the heart of contracting and\nexpanding determines its function of pumping blood in the circulatory\nsystem. However, this theory faces serious problems when applied to\ntechnical artifacts. First, it prevents defining correctness and\nmalfunctioning (Kroes 2010): suppose the call filtering app installed\non our smartphone starts banning calls from contacts in our mobile\nphonebook; according to the causal theory of function this would be a\nnew function of the app. Second, the theory does not distinguish\nintended functions from side effects (Turner 2011): in case of a\nlong-lasting call, our smartphone would certainly start heating;\nhowever, this is not a function intended by clients or developers.\nAccording to the intentional theory of function (McLaughlin\n2001, Searle 1995), the function fixed by the designer or the user is\nthe intended one of the artifact, and structural properties of\nartifacts are selected so as to be able to fulfill it. This theory is\nable to explain correctness and malfunction, as well as to distinguish\nside effects from intended functions. However, it does not say where\nthe function actually resides, whether in the artifact or in the mind\nof the agent. In the former case, one is back at the question of how\nartifacts possess functions. In the latter case, a further explanation\nis needed about how mental states are related to physical properties\nof artifacts (Kroes 2010). Turner (2018) holds that the intuitions\nbehind both the causal and the intentional theories of function are\nuseful to understand the relation between function and structure in\ncomputational systems, and suggests that the two theories be combined\ninto a single one. On the one hand, there is no function without\nimplementation; on the other hand, there is no intention without\nclients, developers, and users. \nEven though known and widely used since antiquity, the problem of\ndefining what algorithms are is still open (Vardi 2012). The word\n“algorithm” originates from the name of the\nninth-century Persian mathematician Abū Jaʿfar\nMuḥammad ibn Mūsā al-Khwārizmī, who\nprovided rules for arithmetic operations using Arabic numerals.\nIndeed, the rules one follows to compute basic arithmetic operations\nsuch as multiplication or division, are everyday examples of\nalgorithms. Other well-known examples include rules to bisect an angle\nusing compass and straightedge, or Euclid’s algorithm for\ncalculating the greatest common divisor. Intuitively, an algorithm is\na set of instructions allowing the fulfillment of a given task.\nDespite this ancient tradition in mathematics, only modern logical and\nphilosophical reflection put forward the task of providing a\ndefinition of what an algorithm is, in connection with the\nfoundational crisis of mathematics of the early twentieth century (see\nthe entry on the\n philosophy of mathematics).\n The notion of effective calculability arose from logical\nresearch, providing some formal counterpart to the intuitive notion of\nalgorithm and giving birth to the theory of computation. Since then,\ndifferent definitions of algorithms have been proposed,\nranging from formal to non-formal approaches, as sketched in the\nnext sections. \nMarkov (1954) provides a first precise definition of algorithm as a\ncomputational process that is determined,\napplicable, and effective. A computational process\nis determined if the instructions involved are precise enough\nnot to allow for any “arbitrary choice” in their\nexecution. The (human or artificial) computer must never be\nunsure about what step to carry out next. Algorithms are\napplicable for Markov in that they hold for classes of inputs\n(natural numbers for basic arithmetic operations) rather than for\nsingle inputs (specific natural numbers). Markov (1954:1) defines\neffectiveness as “the tendency of the algorithm to\nobtain a certain result”. In other words, an algorithm is\neffective in that it will eventually produce the answer to the\ncomputational problem. \nKleene (1967) specifies finiteness as a further important\nproperty: an algorithm is a procedure which can be described by means\nof a finite set of instructions and needs a finite number of steps to\nprovide an answer to the computational problem. As a counterexample,\nconsider a while loop defined by a finite number of steps,\nbut which runs forever since the condition in the loop is always\nsatisfied. Instructions should also be amenable to mechanical\nexecution, that is, no insight is required for the machine to follow\nthem. Following Markov’s determinability and strengthening\neffectiveness, Kleene (1967) additionally specifies that instructions\nshould be able to recognize that the solution to the computational\nproblem has been achieved, and halt the computation. \nKnuth (1973) recalls and deepens the analyses of Markov (1954) and\nKleene (1967) by stating that: \nBesides merely being a finite set of rules that gives a sequence of\noperations for solving a specific type of problem, an algorithm has\nfive important features: \nAs in Kleene (1967), finiteness affects both the number of\ninstructions and the number of implemented computational steps. As in\nMarkov’s determinacy, Knuth’s definiteness principle\nrequires that each successive computational step be unambiguously\nspecified. Furthermore, Knuth (1973) more explicitly requires that\nalgorithms have (potentially empty sets of) inputs and outputs. By\nalgorithms with no inputs or outputs Knuth probably refers to\nalgorithms using internally stored data as inputs or algorithms not\nreturning data to an external user (Rapaport 2019, ch. 7, in Other\nInternet Resources). As for effectiveness, besides Markov’s\ntendency “to obtain a certain result”, Knuth requires that\nthe result be obtained in a finite amount of time and that the\ninstructions be atomic, that is, simple enough to be understandable\nand executable by a human or artificial computer. \nGurevich (2011) maintains, on the one hand, that it is not possible to\nprovide formal definitions of algorithms, as the notion continues to\nevolve over time: consider how sequential algorithms, used in\nancient mathematics, are flanked by parallel, analog, or quantum\nalgorithms in current computer science practice, and how new kinds of\nalgorithms are likely to be envisioned in the near future. On the\nother hand, a formal analysis can be advanced if concerned only with\nclassical sequential algorithms. In particular, Gurevich (2000)\nprovides an axiomatic definition for this class of algorithms. \nAny sequential algorithm can be simulated by a sequential abstract\nstate machine satisfying three axioms: \nMoschovakis (2001) objects that the intuitive notion of algorithm is\nnot captured in full by abstract machines. Given a general recursive\nfunction f: ℕ → ℕ defined on natural\nnumbers, there are usually many different algorithms computing it;\n“essential, implementation-independent properties” are not\ncaptured by abstract machines, but rather by a system of recursive\nequations. Consider the algorithm mergesort for sorting\nlists; there are many different abstract machines for\nmergesort, and the question arises which one is to be chosen\nas the mergesort algorithm. The mergesort algorithm\nis instead the system of recursive equations specifying the involved\nfunction, whereas abstract machines for the mergesort\nprocedure are different implementations of the same\nalgorithm. Two questions are put forward by Moschovakis’\nformal analysis: different implementations of the same\nalgorithm should be equivalent implementations, and yet, an\nequivalence relation among algorithm implementations is to be formally\ndefined. Furthermore, it remains to be clarified what the intuitive\nnotion of algorithm formalized by systems of recursive equations\namounts to. \nPrimiero (2020) proposes a reading of the nature of algorithms at\nthree different levels of abstraction. At a very high LoA, algorithms\ncan be defined abstracting from the procedure they describe, allowing\nfor many different sets of states and transitions. At this LoA\nalgorithms can be understood as informal specifications, that\nis, as informal descriptions of a procedure P. At a lower\nLoA, algorithms specify the instructions needed to solve the given\ncomputational problem; in other words, they specify a procedure.\nAlgorithms can thus be defined as procedures, or descriptions\nin some given formal language L of how to execute a procedure\nP. Many important properties of algorithms, including those\nrelated to complexity classes and data structures, cannot be\ndetermined at the procedural LoA, and instead make reference to an\nabstract machine implementing the procedure is needed. At a bottom\nLoA, algorithms can be defined as implementable abstract\nmachines, viz. as the specification, in a formal language\nL, of the executions of a program P for a given\nabstract machine M. The threefold definition of algorithms\nallows Primiero (2020) to supply a formal definition of equivalence\nrelations for algorithms in terms of the algebraic notions of\nsimulation and bisimulation (Milner 1973, see also\nAngius and Primero 2018). A machine Mi executing a\nprogram Pi implements the same algorithm of a\nmachine Mj executing a program\nPj if and only if the abstract machines\ninterpreting Mi and Mj are in\na bisimulation relation. \nVardi (2012) underlines how, despite the many formal and informal\ndefinitions available, there is no general consensus on what an\nalgorithm is. The approaches of Gurevich (2000) and Moschovakis\n(2001), which can even be proved to be logically equivalent, only\nprovide logical constructs for algorithms, leaving unanswered the main\nquestion. Hill (2013) suggests that an informal definition of\nalgorithms, taking into account the intuitive understanding one has\nabout algorithms, may be more useful, especially for the public\ndiscourse and the communication between practitioners and users. \nRapaport (2012, Appendix) provides an attempt to summarize the three\nclassical definitions of algorithm sketched above stating that: \nRapaport stresses that an algorithm is a procedure, i.e., a finite\nsequence of statements taking the form of rules or instructions.\nFiniteness is here expressed by requiring that instructions contain a\nfinite number of symbols from a finite alphabet. \nHill (2016) aims at providing an informal definition of algorithm,\nstarting from Rapaport’s (2012): \nFirst of all, algorithms are compound structures rather than\natomic objects, i.e., they are composed of smaller units, namely\ncomputational steps. These structures are finite and effective, as\nexplicitly mentioned by Markov, Kleene, and Knuth. While these authors\ndo not explicitly mention abstractness, Hill (2016) maintains it is\nimplicit in their analysis. Algorithms are abstract simply in\nthat they lack spatio-temporal properties and are independent from\ntheir instances. They provide control, that is,\n“content that brings about some kind of change from one state to\nanother, expressed in values of variables and consequent\nactions” (p. 45). Algorithms are imperatively given, as\nthey command state transitions to carry out specified operations.\nFinally, algorithms operate to achieve certain purposes under\nsome usually well-specified provisions, or preconditions.\nFrom this viewpoint, the author argues, algorithms are on a par with\nspecifications in their specifying a goal under certain resources.\nThis definition allows to distinguish algorithms from other compound\ncontrol structures. For instance, recipes are not algorithms because\nthey are not effective; nor are games, which are not imperatively\ngiven. \nThe ontology of computer programs is strictly related to the subsumed\nnature of computational systems (see\n §1).\n If computational systems are defined on the basis of the\nsoftware-hardware dichotomy, programs are abstract entities\ninterpreting the former and opposed to the concrete nature of\nhardware. Examples of such interpretations are provided in\n §1.1\n and include the “concrete abstraction” definition by\nColburn (2000), the “abstract artifact” characterization\nby Irmak (2012), and programs as specifications of machines proposed\nby Duncan (2011). By contrast, under the interpretation of\ncomputational systems by a hierarchy of LoAs, programs are\nimplementations of algorithms. We refer to\n §5\n on implementation for an analysis of the ontology of programs in this\nsense. This section focuses on definitions of programs with a\nsignificant relevance in the literature, namely those views that\nconsider programs as theories or as artifacts, with a focus on the\nproblem of the relation between programs and the world. \nThe view that programs are theories goes back to approaches in\ncognitive science. In the context of the so-called Information\nProcessing Psychology (IPP) for the simulative investigation on human\ncognitive processes, Newell and Simon (1972) advanced the thesis that\nsimulative programs are empirical theories of their simulated\nsystems. Newell and Simon assigned to a computer program the role of\ntheory of the simulated system as well as of the simulative system,\nnamely the machine running the program, to formulate predictions on\nthe simulated system. In particular, the execution traces of the\nsimulative program, given a specific problem to solve, are used to\npredict the mental operation strategies that will be performed by the\nhuman subject when asked to accomplish the same task. In case of a\nmismatch between execution traces and the verbal reports of the\noperation strategies of the human subject, the empirical theory\nprovided by the simulative program is revised. The predictive use of\nsuch a computer program is comparable, according to Newell and Simon,\nto the predictive use of the evolution laws of a system that are\nexpressed by differential or difference equations. \nNewell and Simon’s idea that programs are theories has been\nshared by the cognitive scientists Pylyshyn (1984) and Johnson-Laird\n(1988). Both agree that programs, in contrast to typical theories, are\nbetter at facing the complexity of the simulative process to\nbe modelled, forcing one to fill-in all the details that are necessary\nfor the program to be executed. Whereas incomplete or incoherent\ntheories may be advanced at some stage of scientific inquiry, this is\nnot the case for programs. \nOn the other hand, Moore (1978) considers the programs-as-theories\nthesis another myth of computer science. As programs can\nonly simulate some set of empirical phenomena, at most they play the\nrole of computational models of those phenomena. Moore\nnotices that for programs to be acknowledged as models, semantic\nfunctions are nevertheless needed to interpret the empirical system\nbeing simulated. However, the view that programs are models should not\nbe mistaken for the definition of programs as theories: theories\nexplain and predict the empirical phenomena\nsimulated by models, while simulation by programs does not offer\nthat. \nAccording to computer scientist Paul Thagard (1984), understanding\nprograms as theories would require a syntactic or a\nsemantic view of theories (see the entry on\n the structure of scientific theories).\n But programs do not comply with either of the two views. According to\nthe syntactic view (Carnap 1966, Hempel 1970), theories are sets of\nsentences expressed in some defined language able to describe target\nempirical systems; some of those sentences define the axioms of the\ntheory, and some are law-like statements expressing regularities of\nthose systems. Programs are sets of instructions written in some\ndefined programming language which, however, do not describe any\nsystem, insofar as they are procedural linguistic entities and not\ndeclarative ones. To this, Rapaport (2020, see Other Internet\nResources) objects that procedural programming languages can often be\ntranslated into declarative languages and that there are languages,\nsuch as Prolog, that can be interpreted both procedurally and\ndeclaratively. According to the semantic view (Suppe 1989, Van\nFraassen 1980), theories are introduced by a collection of models,\ndefined as set-theoretic structures satisfying the theory’s\nsentences. However, in contrast to Moore (1978), Thagard (1984) denies\nprograms the epistemological status of models: programs simulate\nphysical systems without satisfying theories’ laws and axioms.\nRather, programs include, for simulation purposes, implementation\ndetails for the programming language used, but not of the target\nsystem being simulated. \nA yet different approach to the problem of whether programs are\ntheories comes from the computer scientist Peter Naur (1985).\nAccording to Naur, programming is a theory building process not in the\nsense that programs are theories, but because the successful\nprogram’s development and life-cycle require that programmers\nand developers have theories of programs available. A theory is here\nunderstood, following Ryle (2009), as a corpus of knowledge shared by\na scientific community about some set of empirical phenomena, and not\nnecessarily expressed axiomatically or formally. Theories of\nprograms are necessary during the program life-cycle to be able to\nmanage requests of program modifications pursuant to observed\nmiscomputations or unsatisfactory solutions to the computational\nproblem the program was asked to solve. In particular, theories of\nprograms should allow developers to modify the program so that new\nsolutions to the problem at stake can be provided. For this reason,\nNaur (1985) deems such theories more fundamental, in software\ndevelopment, than documentations and specifications. \nFor Turner (2010, 2018 ch. 10), programming languages are mathematical\nobjects defined by a formal grammar and a formal semantics. In\nparticular, each syntactic construct, such as an assignment, a\nconditional or a while loop, is defined by a grammatical rule\ndetermining its syntax, and by a semantic rule associating a meaning\nto it. Depending on whether an operational or a denotational semantics\nis preferred, meaning is given in terms of respectively the operations\nof an abstract machine or of mathematical partial functions from set\nof states to set of states. For instance, the simple assignment\nstatement \\(x := E\\) is associated, under an operational semantics,\nwith the machine operation \\(update(s,x,v)\\) which assigns variable\n\\(v\\) interpreted as \\(E\\) to variable \\(x\\) in state \\(s\\). Both in\nthe case of an operational and of a denotational semantics, programs\ncan be understood as mathematical theories expressing the operations\nof an implementing machine. Consider operational semantics: a\nsyntactic rule of the form \\(\\langle P,s \\rangle \\Downarrow s'\\)\nstates semantically that program \\(P\\) executed in state \\(s\\) results\nin \\(s'.\\) According to Turner (2010, 2018), a programming language\nwith an operational semantics is akin to an axiomatic theory of\noperations in which rules provide axioms for the relation\n\\(\\Downarrow\\). \nPrograms can be understood as technical artifacts because programming\nlanguages are defined, as any other artifact, on the basis of both\nfunctional and structural properties (Turner 2014, 2018 ch. 5).\nFunctional properties of (high level) programming languages are\nprovided by the semantics associated with each syntactic construct of\nthe language. Turner (2014) points out that programming languages can\nindeed be understood as axiomatic theories only when their functional\nlevel is isolated. Structural properties, on the other hand, are\nspecified in terms of the implementation of the language, but not\nidentified with physical components of computing machines: given a\nsyntactic construct of the language with an associated functional\ndescription, its structural property is determined by the physical\noperations that a machine performs to implement an instruction for the\nconstruct at hand. For instance, the assignment construct \\(x := E\\)\nis to be linked to the physical computation of the value of expression\n\\(E\\) and to the placement of the value of \\(E\\) in the physical\nlocation \\(x\\). \nAnother requirement for a programming language to be considered a\ntechnical artifact is that it has to be endowed with a semantics\nproviding correctness criteria for the language implementation. The\nprogrammer attests to functional and structural properties of a\nprogram by taking the semantics to have correctness jurisdiction over\nthe program. \nThe problem of whether computer programs are theories is tied with the\nrelation that programs entertain with the outside world. If programs\nwere theories, they would have to represent some empirical system, and\na semantic relation would be directly established between the program\nand the world. By contrast, some have argued that the relation between\nprograms and natural systems is mediated by models of the outside\nworld (Colburn et al. 1993, Smith 1985). In particular, Smith\n(1985) argues that models are abstract descriptions of empirical\nsystems, and computational systems operating in them have programs\nthat act as models of the models, i.e., they represent abstract models\nof reality. Such an account of the ontology of programs comes in handy\nwhen describing the correctness problem in computer science (see\n § 7):\n if specifications are considered as models requiring certain\nbehaviors from computational systems, programs can be seen as models\nsatisfying specifications. \nTwo views of programs can be given depending on whether one admits\ntheir relation with the world (Rapaport 2020, ch. 17, see Other\nInternet Resource). According to a first view, programs are\n“wide”, “external” and “semantic”:\nthey grant direct reference to objects of an empirical system and\noperations on those objects. According to a second view, programs are\n“narrow”, “internal”, and\n“syntactic”: they make only reference to the atomic\noperations of an implementing machine carrying out computations.\nRapaport (2020, see Other Internet Resources) argues that programs\nneed not be “external” and\n“semantic”. First, computation itself needs not to be\n“external”: a Turing machine executes the instructions\ncontained in its finite table by using data written on its tape and\nhalting after the data resulting from the computation have been\nwritten on the tape. Data are not, strictly speaking, in-put-from\nand out-put-to an external user. Furthemore, Knuth (1973) required\nalgorithms to have zero or more inputs and outputs (see\n § 3.1).\n A computer program requiring no inputs may be a program, say,\noutputting all prime numbers from 1; and a program with no outputs can\nbe a program that computes the value of some given variable x without\nreturning the value stored in x as output. Second, programs need not\nbe “external”, teleological, i.e., goal oriented. This\nview opposes other known positions in the literature. Suber (1988)\nargues that, without considering goals and purposes, it would not be\npossible to assess whether a computer program is correct, that is, if\nit behaves as intended. And as recalled in\n §3.3.,\n Hill (2016) specifies in her informal definition that algorithms\naccomplish “a given purpose, under given provisions.”\n(Hill 2016: 48). To these views, Rapaport (2020, ch. 17, see Other\nInternet Resource) replies that whereas goals, purposes, and\nprogrammers’ intentions may be very useful for a human computor\nto understand a program, they are not necessary for an artificial\ncomputer to carry out the computations instructed by the program code.\nIndeed, the principle of effectiveness that classical approaches\nrequire for algorithms (see\n §3.1)\n demands, among other properties, that algorithms be executed without\nany recourse to intuition. In other words, a machine executing a\nprogram for adding natural numbers does not “understand”\nthat it is adding; at the same time, knowing that a given program\nperforms addition may help a human agent to understand the\nprogram’s code. \nAccording to this view, computing involves just symbols, not meanings.\nTuring machines become symbols manipulators and not a single but\nmultiple meanings can be associated with its operations. How can then\none identify when two programs are the same program, if not\nby their meanings, that is, by considering what function they perform?\nOne answer comes from Piccini’s analysis of computation and its\n“internal semantics” (Piccini 2008, 2015 ch. 3):\ntwo programs can be identified as identical by analysing only their\nsyntax and the operations the programs carry out on their symbols. The\neffects of string manipulation operations can be considered an\ninternal semantics of a program. The latter can be easily determined\nby isolating subroutines or methods in the program’s code and\ncan afterwards be used to identify a program or to establish whether\ntwo programs are the same, namely when they are defined by the same\nsubroutines. \nHowever, it has been argued that there are cases in which it is not\npossible to determine whether two programs are the same without making\nreference to an external semantics. Sprevak (2010) proposes to\nconsider two programs for addition which differ from the fact that one\noperates on Arabic, the other one on Roman numerals. The two programs\ncompute the same function, namely addition, but this cannot always be\nestablished by inspecting the code with its subroutines; it must be\ndetermined by assigning content to the input/output strings,\ninterpreting Arabic and Roman numerals as numbers. In that regard,\nAngius and Primiero (2018) underline how the problem of identity for\ncomputer programs does not differ from the problem of identity for\nnatural kinds (Lowe 1998) and technical artifacts (Carrara et al.\n2014). The problem can be tackled by fixing an identity criterion,\nnamely a formal relation, that any two programs should entertain in\norder to be defined as identical. Angius and Primiero (2018) show how\nto use the process algebra relation of bisimulation between the two\nautomata implemented by two programs under examination as such an\nidentity criterion. Bisimulation allows to establish matching\nstructural properties of programs implementing the same function, as\nwell as providing weaker criteria for copies in terms of simulation.\nThis brings the discussion back to the notion of programs as\nimplementations. We now turn to analyze this latter concept. \nThe word ‘implementation’ is often associated with a\nphysical realization of a computing system, i.e., to a machine\nexecuting a computer program. In particular, according to the dual\nontology of computing systems examined in\n §1.1,\n implementation in this sense reduces to the structural hardware, as\nopposed to the functional software. By contrast, following the method\nof the levels of abstraction\n (§ 1.2),\n implementation becomes a wider relation holding between any LoA\ndefining a computational system and the levels higher in the\nhierarchy. Accordingly, an algorithm is an implementation of a (set\nof) specification(s); a program expressed in a high level programming\nlanguage can be defined as an implementation of an algorithm (see\n §4);\n assembly and machine code instructions can be seen as an\nimplementation of a set of high-level programming language\ninstructions with respect to a given ISA; finally, executions are\nphysical, observable, implementations of those machine code\ninstructions. By the same token, programs formulated in a high-level\nlanguage are also implementations of specifications, and, as similarly\nargued by the dual-ontology paradigm, executions are implementations\nof high-level programming language instructions. According to Turner\n(2018), even the specification can be understood as an implementation\nof what has been called intention. \nWhat remains to be examined here is the nature of the implementation\nrelation thus defined. Analyzing this relation is essential to define\nthe notion of correctness\n (§7).\n Indeed, a correct program amounts to a correct implementation of an\nalgorithm; and a correct computing system is a correct implementation\nof a set of specifications. In other words, under this view, the\nnotion of correctness is paired with that of implementation for any\nLoA: any level can be said to be correct with respect to upper levels\nif and only if it is a correct implementation thereof. \nThe following three subsections examine three main definitions of the\nimplementation relation that have been advanced in the philosophy of\ncomputer science literature. \nA first philosophical analysis of the notion of implementation in\ncomputer science is advanced by Rapaport (1999, 2005). He defines an\nimplementation I as the semantic interpretation of a\nsyntactic or abstract domain A in a medium of implementation\nM. If implementation is understood as a relation holding\nbetween a given LoA and any upper level in the hierarchical ontology\nof a computational system, it follows that Rapaport’s definition\nextends accordingly, so that any LoA provides a semantic\ninterpretation in a given medium of implementation for the upper\nlevels. Under this view, specifications provide semantic\ninterpretations of intentions expressed by stakeholders in the\nspecification (formal) language, and algorithms provide semantic\ninterpretations of specifications using one of the many languages\nalgorithms can be formulated in (natural languages, pseudo-code, logic\nlanguages, functional languages etc.). The medium of implementation\ncan be either abstract or concrete. A computer program is the\nimplementation of an algorithm in that the former provides a semantic\ninterpretation of the syntactic constructs of the latter in a\nhigh-level programming language as its medium of implementation. The\nprogram’s instructions interpret the algorithm's tasks in a\nprogramming language. Also the execution LoA provides a semantic\ninterpretation of the assembly/machine code operations into the medium\ngiven by the structural properties of the physical machine. According\nto the analysis in (Rapaport 1999, 2005), implementation is an\nasymmetric relation: if I is an implementation of A,\nA cannot be an implementation of I. However, the\nauthor argues that any LoA can be both a syntactic and a semantic\nlevel, that is, it can play the role of both the implementation I and\nof a syntactic domain A. Whereas an algorithm is assigned a semantic\ninterpretation by a program expressed in a high-level language, the\nsame algorithm provides a semantic interpretation for the\nspecification. It follows that the abstraction-implementation relation\npairs the functional-structural relation for computational\nsystems. \nPrimiero (2020) considers this latter aspect as one main limit of\nRapaport’s (1999, 2005) account of implementation:\nimplementation reduces to a unique relation between a\nsyntactic level and its semantic interpretation and it does not\naccount for the layered ontology of computational systems seen in\n §1.2.\n In order to extend the present definition of implementation to all\nLoAs, each level has to be reinterpreted each time either as syntactic\nor as a semantic level. This, in turn, has a repercussion on the\nsecond difficulty characterizing, according to Primero (2020),\nimplementation as a semantic interpretation: on the one hand, this\napproach does not take into account incorrect\nimplementations; on the other hand, for a given incorrect\nimplementation, the unique relation so defined can relate\nincorrectness only to one syntactic level, excluding all other levels\nas potential error locations. \nTurner (2018) aims to show that semantic interpretation not only does\nnot account for incorrect implementation, but not even to correct\nones. One first example is provided by the implementation of one\nlanguage into another: the implementing language here is not providing\na semantic interpretation of the implemented language, unless the\nformer is associated with a semantics providing meaning and\ncorrectness criteria for the latter. Such semantics will remain\nexternal to the implementation relation: whereas correctness is\nassociated with semantic interpretation, implementation does not\nalways come with a semantic interpretation. A second example is given\nby considering an abstract stack implemented by an array; again, the\narray does not provide correctness criteria for the stack. Quite to\nthe contrary, it is the stack that specifies correctness criteria for\nany of its implementation, arrays included. \nThe fact that correctness criteria for the implementation relation are\nprovided by the abstract level induces Turner (2012, 2014, 2018)\nto define implementation as the relation\nspecification-artefact. As examined in\n §2,\n specifications have correctness jurisdiction over artifacts, that is,\nthey prescribe the allowed behaviors of artifacts. Also recall that\nartifacts can be both abstract and concrete entities, and that any LoA\ncan play the role of specification for lower levels. This amounts to\nsaying that the specification-artefact relation is able to define any\nimplementation relation across the layered ontology of computational\nsystems. \nDepending on how the specification-artifact relation is defined,\nTurner (2012) distinguishes as many as three different notions of\nimplementation. Consider the case of a physical machine implementing a\ngiven abstract machine. According to an intentional notion of\nimplementation, an abstract machine works as a specification for a\nphysical machine, provided it advances all the functional requirements\nthe latter must fulfill, i.e., it specifies (in principle) all the\nallowed behaviors of the implementing physical machine. According to\nan extensional notion of implementation, a physical machine\nis a correct implementation of an abstract machine if and only if\nisomorphisms can be established mapping states of the latter to states\nof the former, and transitions in the abstract machine correspond to\nactual executions (computational traces) of the artifact. Finally, an\nempirical notion of implementation requires the physical\nmachine to display computations that match those prescribed by the\nabstract machine; that is to say, correct implementation has to be\nevaluated empirically through testing. \nPrimiero (2020) underlines how, while this approach addresses the\nissue of correctness and miscomputation as it allows to distinguish a\ncorrect from an incorrect implementation, it still identifies a unique\nimplementation relation between a specification level and an artifact\nlevel. Again, if this account is allowed to involve the layered\nontology of computational systems by reinterpreting each time any LoA\neither as a specification or artifact, Turner’s account prevents\nfrom referring to more than one level at the same time as the cause of\nmiscomputation: a miscomputation always occurs here as an\nincorrect implementation of a specification by an artifact. By\ndefining implementation as a relation holding accross all the LoAs,\none would be able to identify multiple incorrect implementations which\ndo not directly refer to the abstract specification. A miscomputation\nmay indeed be caused by an incorrect implementation of lower levels\nwhich is then inherited all the way down to the execution level. \nPrimiero (2020) proposes a definition of implementation not as a\nrelation between two fixed levels, but one that is allowed to range\nover any LoA. Under this view, an implementation I is a\nrelation of instantiation holding between a LoA and any other\none higher in the abstraction hierarchy. Accordingly, a physical\ncomputing machine is an implementation of assembly/machine code\noperations; by transitivity, it can also be considered as an\ninstantiation of a set of instructions expressed in high-level\nprogramming language instructions. A program expressed in a high-level\nlanguage is an implementation of an algorithm; but it can also be\ntaken to be the instantiation of a set of specifications. \nSuch a definition of implementation allows Primiero (2020) to provide\na general definition of correctness: a physical computing system is\ncorrect if and only if it is characterized by correct implementations\nat any LoA. Hence correctness and implementation are coupled and\ndefined at any LoA. Functional correctness is the property of\na computational system that displays the functionalities required by\nthe specifications of that system. Procedural correctness\ncharacterizes computational systems displaying the functionalities\nintended by the implemented algorithms. And executional\ncorrectness is defined as the property of a system that is able\nto correctly execute the program on its architecture. Each of these\nforms of correctness can also be classified quantitatively, depending\non the amount of functionalities being satisfied. A functionally\nefficient computational system displays a minimal subset of\nthe functionalities required by the specifications; a functionally\noptimal system is able to display a maximal subset of those\nfunctionalities. Similarly, the author defines procedurally as well as\nexecutionally efficient and optimal computational systems. \nAccording to this definition, implementation shifts from level to\nlevel: a set of algorithms defining a computational system are\nimplemented as procedures in some formal language, as\ninstructions in a high-level language, or as operations in a\nlow-level programming language. An interesting question is whether\nany system, beyond computational artifacts, implementing\nprocedures of this sort qualifies as a computational system. In other\nwords, asking about the nature of physical implementation amounts to\nasking what is a computational system. If any system implementing an\nalgorithm would qualify as computational, the class of such systems\ncould be extended to biological systems, such as the brain or the\ncell; to physical systems, including the universe or some portion of\nit; and eventually to any system whatsoever, a thesis known as\npancomputationalism (for an exhaustive overview on the topic\nsee Rapaport 2018). \nTraditionally, a computational system is intended as a mechanical\nartifact that takes input data, elaborates them\nalgorithmically according to a set of instructions, and\nreturns manipulated data as outputs. For instance, von Neumann (1945,\np.1) states that “An automatic computing system is a (usually\nhighly composite) device, which can carry out instructions to perform\ncalculations of a considerable order of complexity”. Such an\ninformal and well-accepted definition leaves some questions open,\nincluding whether computational systems have to be machines, whether\nthey have to process data algorithmically and, consequently, whether\ncomputations have to be Turing complete. \nRapaport (2018) provides a more explicit characterization of a\ncomputational system defined as any “physical plausible\nimplementation of anything logically equivalent to a universal Turing\nmachine”. Strictly speaking personal computers are not physical\nTuring machines, but register machines are known to be Turing\nequivalent. To qualify as computational, systems must be\nplausible implementations thereof, in that Turing machines,\ncontrary to physical machines, have access to infinite memory space\nand are, as abstract machines, error free. According to\nRapaport’s (2018) definition, any physical\nimplementation of this sort is thus a computational system, including\nnatural systems. This raises the question about which class of natural\nsystems is able to implement Turing equivalent computations. Searle\nfamously argued that anything can be an implementation of a Turing\nmachine, or of a logical equivalent model (Searle 1990). His argument\nlevers on the fact that being a Turing machine is a syntactic\nproperty, in that it is all about manipulating tokens of 0’s and\n1’s. According to Searle, syntactic properties are not intrinsic\nto physical systems, but they are assigned to them by an observer. In\nother words, a physical state of a system is not intrinsically a\ncomputational state: there must be an observer, or user, who assigns\nto that state a computational role. It follows that any system whose\nbehavior can be described as syntactic manipulation of 0’s and\n1’s is a computational system. \nHayes (1997) objects to Searle (1990) that if everything was a\ncomputational system, the property “being a computational\nsystem” would become vacuous, as all entities would possess it.\nInstead, there are entities which are computational systems, and\nentities which are not. Computational systems are those in which the\npatterns received as inputs and saved into memory are able to change\nthemselves. In other words, Hayes makes reference to the fact that\nstored inputs can be both data and instructions and that instructions,\nwhen executed, are able to modify the value of some input data.\n“If it were paper, it would be ‘magic paper’ on\nwhich writing might spontaneously change, or new writing appear”\n(Hayes 1997, p. 393). Only systems able to act as “magic\npaper” can be acknowledged as computational. \nA yet different approach comes from Piccinini (2007, 2008) in the\ncontext of his mechanistic analysis of physical computations\n(Piccinini 2015; see also the entry on\n computation in physical systems).\n A physical computing system is a system whose behaviors can be\nexplained mechanistically by describing the computing\nmechanism that brings about those behaviors. Mechanisms can be defined\nby “entities and activities organized such that they are\nproductive of regular changes from start or set-up to finish or\ntermination condition” (Machamer et al. 2000; see the entry on\n mechanisms in science).\n Computations, as physical processes, can be understood as those\nmechanisms that “generate output strings from input strings in\naccordance with general rules that apply to all input strings and\ndepend on the input (and sometimes internal states)” (Piccinini\n2007, p. 108). It is easy to identify set-up and termination\nconditions for computational processes. Any system which can be\nexplained by describing an underlying computing mechanism is to be\nconsidered a computational system. The focus on explanation helps\nPiccinini avoid the Searlean conclusion that any system is a\ncomputational system: even if one may interpret, in principle, any\ngiven set of entities and activities as a computing mechanism, only\nthe need to explain a certain observed phenomenon in terms of a\ncomputing mechanism defines the system under examination as\ncomputational. \nA crucial step in the software development process is verification.\nThis consists in the process of evaluating whether a given\ncomputational system is correct with respect to the specification of\nits design. In the early days of the computer industry, validity and\ncorrectness checking methods included several design and construction\ntechniques, see for example (Arif et al. 2018). Nowadays,\ncorrectness evaluation methods can be roughly sorted into two main\ngroups: formal verification and testing. Formal verification (Monin\nand Hinchey 2003) involves a proof of correctness with mathematical\ntools; software testing (Ammann and Offutt 2008) rather consists in\nrunning the implemented program to observe whether performed\nexecutions comply or not with the advanced specifications. In many\npractical cases, a combination of both methods is used (see for\ninstance Callahan et al. 1996). \nFormal verification methods require a representation of the\nsoftware under verification. In theorem proving (see van\nLeeuwen 1990), programs are represented in terms of axiomatic systems\nand a set of rules of inference representing the pre- and\npost-conditions of program transitions. A proof of correctness is then\nobtained by deriving formulas expressing specifications from the\naxioms. In model checking (Baier and Katoen 2008), a program\nis represented in terms of a state transition system, its property\nspecifications are formalised by temporal logic formulas (Kröger\nand Merz 2008), and a proof of correctness is achieved by a\ndepth-first search algorithm that checks whether those formulas hold\nof the state transition system. \nAxiomatic systems and state transition systems used for correctness\nevaluation can be understood as theories of the represented\nartifacts, in that they are used to predict and explain their future\nbehaviors. Methodologically state transition systems in model checking\ncan be compared with scientific models in empirical sciences (Angius\nand Tamburrini 2011). For instance, Kripke Structures (see Clarke\net al. 1999 ch. 2) are in compliance with Suppes’\n(1960) definition of scientific models as set-theoretic structures\nestablishing proper mapping relations with models of data collected by\nmeans of experiments on the target empirical system (see also the\nentry on\n models in science).\n Kripke Structures and other state transition systems utilized in\nformal verification methods are often called system specifications.\nThey are distinguished from common specifications, also called\nproperty specifications. The latter specify some required behavioral\nproperties the program to be encoded must instantiate, while the\nformer specify (in principle) all potential executions of an already\nencoded program, thus allowing for algorithmic checks on its traces\n(Clarke et al. 1999). In order to achieve this goal, system\nspecifications are considered as abductive structures,\nhypothesizing the set of potential executions of a target\ncomputational system on the basis of the program’s code and the\nallowed state transitions (Angius 2013b). Indeed, once it has been\nchecked whether some temporal logic formula holds of the modeled\nKripke Structure, the represented program is empirically tested\nagainst the behavioral property corresponding to the checked formula,\nin order to evaluate whether the model-hypothesis is an adequate\nrepresentation of the target computational system. Accordingly,\nproperty specifications and system specifications differ also in their\nintentional stance (Turner 2011): the former are requirements\non the program to be encoded, the latter are (hypothetical)\ndescriptions of the encoded program. The descriptive and abductive\ncharacter of state transition systems in model checking is an\nadditional and essential feature putting state transition systems on a\npar with scientific models. \nTesting is the more ‘empirical’ process of launching a\nprogram and observing its executions in order to evaluate whether they\ncomply with the supplied property specifications. Such technique is\nextensively used in the software development process. Philosophers and\nphilosophically-minded computer scientists have considered software\ntesting under the light of traditional methodological approaches in\nscientific discovery (Snelting 1998; Gagliardi 2007; Northover et\nal. 2008; Angius 2014) and questioned whether software tests can\nbe acknowledged as scientific experiments evaluating the\ncorrectness of programs (Schiaffonati and Verdicchio 2014,\nSchiaffonati 2015; Tedre 2015). \nDijkstra’s well-known dictum “Program testing can be used\nto show the presence of bugs, but never to show their absence”\n(Dijkstra 1970, p.7), introduces Popper’s (1959) principle of\nfalsifiability into computer science (Snelting 1998). Testing\na program against an advanced property specification for a given\ninterval of time may exhibit some failures, but if no failure occurs\nwhile observing the running program one cannot conclude that the\nprogram is correct. An incorrect execution might be observed at the\nvery next system’s run. The reason is that testers can only\nlaunch the program with a finite subset of the potential\nprogram’s input set and only for a finite interval of time;\naccordingly, not all potential executions of the program to be tested\ncan be empirically observed. For this reason, the aim of software\ntesting is to detect programs’ faults and not to guarantee their\nabsence (Ammann and Offutt 2008, p. 11). A program is falsifiable in\nthat tests can reveal faults (Northover et al. 2008). Hence,\ngiven a computational system and a property specification, a test is\nakin to a scientific experiment which, by observing the system’s\nbehaviors, tries to falsify the hypothesis that the program is correct\nwith respect to the interested specification. \nHowever, other methodological and epistemological traits\ncharacterizing scientific experiments are not shared by software\ntests. A first methodological distinction can be recognized in that a\nfalsifying test leads to the revision of the computational system, not\nof the hypothesis, as in the case of testing scientific hypotheses.\nThis is due to the difference in the intentional stance of\nspecifications and empirical hypotheses in science (Turner 2011).\nSpecifications are requirements whose violation demands for program\nrevisions until the program becomes a correct instantiation of the\nspecifications. \nFor this, among other reasons, the traditional notion of scientific\nexperiment needs to be ‘stretched’ in order to be applied\nto software testing activities (Schiaffonati 2015). Theory-driven\nexperiments, characterizing most of the experimental sciences,\nfind no counterpart in actual computer science practice. If one\nexcludes the cases wherein testing is combined with formal methods,\nmost experiments performed by software engineers are rather\nexplorative, i.e. aimed at ‘exploring’\n“the realm of possibilities pertaining to the functioning of an\nartefact and its interaction with the environment in the absence of a\nproper theory or theoretical background” (Schiaffonati 2015:\n662). Software testers often do not have theoretical control on the\nexperiments they perform; exploration on the behaviors of the\ncomputational system interacting with users and environments rather\nallows testers to formulate theoretical generalizations on the\nobserved behaviors. Explorative experiments in computer science are\nalso characterized by the fact that programs are often tested in a\nreal-like environment wherein testers play the role of users. However,\nit is an essential feature of theory-driven experiments that\nexperimenters do not take part in the experiment to be carried\nout. \nAs a result, while some software testing activities are closer to the\nexperimental activities one finds in empirical sciences, some others\nrather define a new typology of experiment that turns out to belong to\nthe software development process. Five typologies of experiments can\nbe distinguished in the process of specifying, implementing, and\nevaluating computational systems (Tedre 2015): \nA software test is considered successful when miscomputations are\ndetected (assuming that no computational artifact is 100% correct).\nThe successive step is to find out what caused the execution to be\nincorrect, that is, to trace back the fault (more familiarly named\n‘bug’), before proceeding to the debugging phase and then\ntesting the system again. In other words, an explanation of\nthe observed miscomputation is to be advanced. \nEfforts have been made to consider explanations in computer science\n(Piccinini 2007; Piccinini and Craver 2011; Piccinini 2015; Angius and\nTamburrini 2016) in relation to the different models of explanations\nelaborated in the philosophy of science. In particular, computational\nexplanations can be understood as a specific kind of mechanistic\nexplanation (Glennan 1996; Machamer et al. 2000; Bechtel\nand Abrahamsen 2005), insofar as computing processes can be analyzed\nas mechanisms (Piccinini 2007; 2015; see also the entry on\n computation in physical systems). \nConsider a processor executing an instruction. The involved process\ncan be understood as a mechanism whose components are states and\ncombinatory elements in the processor instantiating the functions\nprescribed by the relevant hardware specifications (specifications for\nregisters, for the Arithmetic Logic Unit etc..), organized in such a\nway that they are capable of carrying out the observed execution.\nProviding the description of such a mechanism counts as advancing a\nmechanist explanation of the observed computation, such as the\nexplanation of an operational malfunction. \nFor every type of miscomputation (see\n §7.3),\n a corresponding mechanist explanation can be defined at the adequate\nLoA and with respect to the set of specifications characterizing that\nLoA. Indeed, abstract descriptions of mechanisms still supply one with\na mechanist explanation in the form of a mechanism schema,\ndefined as “a truncated abstract description of a mechanism that\ncan be filled with descriptions of known component parts and\nactivities” (Machamer et al. 2000, p. 15). For\ninstance, suppose the very common case in which a machine miscomputes\nby executing a program containing syntax errors, called slips. The\ncomputing machine is unable to correctly implement the functional\nrequirements provided by the program specifications. However, for\nexplanatory purposes, it would be redundant to provide an explanation\nof the occurred slip at the hardware level of abstraction, by\nadvancing the detailed description of the hardware components and\ntheir functional organization. In such cases, a satisfactory\nexplanation may consist in showing that the program’s code is\nnot a correct instantiation of the provided program specifications\n(Angius and Tamburrini 2016). In order to explain mechanistically an\noccurred miscomputation, it may be sufficient to provide the\ndescription of the incorrect program, abstracting from the rest of the\ncomputing mechanism (Piccinini and Craver 2011). Abstraction is a\nvirtue not only in software development and specification, but also in\nthe explanation of computational systems’ behaviors. \nEach of the different approaches on software verification examined in\nthe previous section assumes a different understanding of correctness\nfor software. Standardly, correctness has been understood as a\nrelation holding between an abstraction and its implementation, such\nthat it holds if the latter fulfills the properties formulated by the\nformer. Once computational systems are described as having a layered\nontology, correctness needs to be reformulated as the relation that\nany structural level entertains with respect to its functional level\n(Primiero, 2020). Hence, correctness can still be considered as a\nmathematical relationship when formulated between abstract and\nfunctional level; while it can be considered as an empirical\nrelationship when formulated between the functional and the\nimplementation levels. One of the earlier debates in the philosophy of\ncomputer science (De Millo et al. 1979; Fetzer 1988) was\nindeed around this distinction. \nFormal verification methods grant an a-priori analysis of the\nbehaviors of programs, without requiring the observation of any of\ntheir implementation or considering their execution. In particular,\ntheorem proving allows one to deduce any potential behavior\nof the program under consideration and its behavioral properties from\na suitable axiomatic representation. In the case of model checking,\none knows in advance the behavioural properties displayed by the\nexecution of a program by performing an algorithmic search of the\nformulas valid in a given set-theoretic model. These considerations\nfamously led Hoare (1969) to conclude that program development is an\n“exact science”, which should be characterized by\nmathematical proofs of correctness, epistemologically on a par with\nstandard proofs in mathematical practice. \nDe Millo et al. (1979) question Hoare’s thesis: correct\nmathematical proofs are usually elegant and\ngraspable, implying that any (expert) reader can\n“see” that the conclusion follows from the premises (for\nthe notion of elegance in software see also Hill (2018)). What are\noften called Cartesian proofs (Hacking 2014) do not have a\ncounterpart in correctness proofs, typically long and cumbersome,\ndifficult to grasp and not explaining why the conclusion necessarily\nfollows from the premises. Yet, many proofs in mathematics are long\nand complex, but they are in principle surveyable, thanks to\nthe use of lemmas, abstractions and the analytic construction of new\nconcepts leading step by step to the statement to be proved.\nCorrectness proofs, on the contrary, do not involve the creation of\nnew concepts, nor the modularity one typically finds in mathematical\nproofs (Turner, 2018). And yet, proofs that are not surveyable cannot\nbe considered mathematical proofs (Wittgenstein 1956). \nA second theoretical difficulty concerning proofs of correctness for\ncomputer programs concerns their complexity and that of the programs\nto be verified. Already Hoare (1981) admitted that while verification\nof correctness is always possible in principle, in practice it is\nhardly achievable. Except for trivial cases, contemporary software is\nmodularly encoded, is required to satisfy a large set of\nspecifications, and it is developed so as to interact with other\nprograms, systems, users. Embedded and reactive software are cases in\npoint. In order to verify such complex software, correctness proofs\nare carried out automatically. Hence, on the one hand, the correctness\nproblem shifts from the program under examination to the program\nperforming the verification, e.g. a theorem prover; on the other hand,\nproofs carried out by a physical process can go wrong, due to\nmechanical mistakes of the machine. Against this infinite regress\nargument, Arkoudas and Bringsjord (2007) argue that one can make use\nof a proof checker which, by being a relatively small program, is\nusually easier to verify. \nMost recently, formal methods for checking correctness based on a\ncombination of logical and statistical analysis have given new\nstimulus to this research area: the ability of Separation Logics\n(Reynolds, 2002) to offer a representation of the logical behavior of\nthe physical memory of computational systems, and the possibility of\nconsidering probabilistic distributions of inputs as statistical\nsource of errors, have allowed formal correctness check of large\ninteractive systems like the Facebook platform (see also Pym et\nal. 2019). \nFetzer (1988) objected that deductive reasoning is only able to\nguarantee for the correctness of a program with respect to its\nspecifications, but not for the correctness of a computational system,\nthat is also accounting for the program’s physical\nimplementation. Even if the program were correct with respect to any\nof the related upper LoAs (algorithms, specifications, requirements),\nits implementation could still violate one or more of the intended\nspecifications due to a physical malfunctioning. The former kind of\ncorrectness can in principle be proved mathematically, but the\ncorrectness of the execution LoA requires an empirical assessment. As\nexamined in\n §6.2,\n software testing can show only in principle the correctness of a\ncomputational system. In practice, the number of allowed executions of\nnon-trivial systems are potentially infinite and cannot be\nexhaustively checked in a finite (or reasonable) amount of time\n(Dijkstra 1974). Most successful testing methods rather see both\nformal verification and testing used together to reach a satisfactory\ncorrection level. \nAnother objection to the theoretical possibility of mathematical\ncorrectness is that since proofs are carried out by a theorem prover,\ni.e. a physical machine, the knowledge one attains about computational\nsystems is not a-priori but empirical (see Turner 2018 ch.\n25). However, Burge (1988) argues that computer-based proofs of\ncorrectness can still be regarded as a-priori, in that even\nthough their possibility depends on sensory experience, their\njustification does not (as it is for a-posteriori knowledge).\nFor instance, the knowledge that red is a color is a-priori\neven though it requires having sensory experience of red; this is\nbecause ‘red is a colour’ is true independently of any\nsensory experience. For further discussion on the nature of the use of\ncomputers in mathematical proofs, see (Hales 2008; Harrison 2008;\nTymoczko 1979, 1980). \nThe problem of correctness eventually reduces to asking what it means\nfor a physical machine to satisfy an abstract requirement. According\nto the simple mapping account, a computational system\nS is a correct implementation of specification SP\nonly if: \nThe simple mapping account only demands for an extensional agreement\nbetween the description of S and SP. The weakness of\nthis account is that it is quite easy to identify an extensional\nagreement between any couple of physical system-specification, leaving\nroom for a pancomputationalist perspective. \nThe danger of pancomputationalism has led some authors to attempt an\naccount of correct implementation that somehow restricts the class of\npossible interpretations. In particular, \nFrom what has been said so far, it follows that correctness of\nimplemented programs does not automatically establish the\nwell-functioning of a computational system. Turing (1950) already\ndistinguished between errors of functioning and errors of\nconclusion. The former are caused by a faulty implementation\nunable to execute the instructions of some high-level language\nprogram; errors of conclusion characterize correct abstract machines\nthat nonetheless fail to carry out the tasks they were supposed to\naccomplish. This may happen in those cases in which a program\ninstantiates correctly some specifications which do not properly\nexpress the users’ requirements on such a program. In both\ncases, machines implementing correct programs can still be said to\nmiscompute. \nTuring’s distinction between errors of functioning and errors of\nconclusion has been expanded into a complete taxonomy of\nmiscomputations (Fresco and Primiero 2013). The classification is\nestablished on the basis of the different LoAs defining computational\nsystems. Errors can be: \nPerformable errors clearly emerge only at the execution level, and\nthey correspond with Turing’s (1950) error of functioning, also\ncalled operational malfunctions. Conceptual and material\nerrors may arise at any level of abstraction from the intention level\ndown to the physical implementation level. Conceptual errors engender\nmistakes, while material errors induce failures. For\ninstance, a mistake at the intention level consists of an inconsistent\nset of requirements, while at the physical implementation level it may\ncorrespond to an invalid hardware design (such as in the choice of the\nlogic gates for the truth-functional connectives). Failures occurring\nat the specification level may be due to a design that is deemed to be\nincomplete with respect to the set of desired functional requirements,\nwhile a failure at the algorithm level occurs in those frequent cases\nin which the algorithm is found not to fulfill the specifications.\nBeyond mistakes, failures, and operational malfunctions,\nslips are a source of miscomputations at the high-level\nprogramming language instructions level: they may be conceptual or\nmaterial errors due to, respectively, a syntactic or a semantic flaw\nin the program. Conceptual slips appear in all those cases in which\nthe syntactical rules of high-level languages are violated; material\nslips involve the violation of semantic rules of programming\nlanguages, such as when a variable is used but not initialized. \nA further distinction has to be made between dysfunctions and\nmisfunctions for software-based computational systems\n(Floridi, Fresco and Primiero 2015). Software can only misfunction but\ncannot ever dysfunction. A software token can dysfunction in case its\nphysical implementation fails to satisfy intentions or specifications.\nDysfunctions only apply to single tokens since a token dysfunctions in\nthat it does not behave as the other tokens of the same type do with\nrespect to the implemented functions. For this reason, dysfunctions do\nnot apply to the intention level and the specification level. On the\ncontrary, both software types and tokens can misfunction, since\nmisfunctions do not depend on comparisons with tokens of the same type\nbeing able to perform some implemented function or not. Misfunction of\ntokens usually depends on the dysfunction of some other component,\nwhile misfunction of types is often due to poor design. A software\ntoken cannot dysfunction, because all tokens of a given type implement\nfunctions specified uniformly at the intention and specification\nlevels. Those functions are implemented at the algorithm\nimplementation level before being performed at the execution level; in\ncase of correct implementation, all tokens will behave correctly at\nthe execution level (provided that no operational malfunction occurs).\nFor the very same reason, software tokens cannot misfunction, since\nthey are implementations of the same intentions and specifications.\nOnly software types can misfunction in case of poor design;\nmisfunctioning software types are able to correctly perform their\nfunctions but may also produce some undesired side-effect. For the\napplication of the notion of malfunctioning to the problem of malware\nclassification, see (Primiero et al. 2019). \nBetween the 1960s and the 1970s, computer science emerged as an\nacademic discipline independent from its older siblings, mathematics\nand physics, and with it the problem of defining its epistemological\nstatus as influenced by mathematical, empirical, and engineering\nmethods (Tedre and Sutien 2008, Tedre 2011, Tedre 2015, Primiero\n2020). A debate is still in place today concerning whether computer\nscience has to be mostly considered as a mathematical\ndiscipline, a branch of engineering, or as a scientific\ndiscipline. \nAny epistemological characterization of computer science is based on\nontological, methodological, and epistemological commitments, namely\non assumptions about the nature of computational systems, the methods\nguiding the software development process, and the kind of reasoning\nthereby involved, whether deductive, inductive, or a combination of\nboth (Eden 2007). \nThe origin of the analysis of computation as a mathematical notion\ncame notoriously from logic, with Hilbert's question concerning\nthe decidability of predicate calculus, known as the\nEntschiedungsproblem (Hilbert and Ackermann 1950): could\nthere be a mechanical procedure for deciding of an arbitrary sentence\nof logic whether it is provable? To address this question, a rigorous\nmodel of the informal concept of an effective or mechanical method in\nlogic and mathematics was required. This is first and foremost a\nmathematical endeavor: one has to develop a mathematical analogue of\nthe informal notion. Supporters of the view that computer science is\nmathematical in nature assume that a computer program can be seen as a\nphysical realization of such a mathematical entity and that one can\nreason about programs deductively through the formal methods of\ntheoretical computer science. Dijkstra (1974) and Hoare (1986)\nwere very explicit in considering programs’ instructions as\nmathematical sentences, and considering a formal semantics for\nprogramming languages in terms of an axiomatic system (Hoare 1969).\nProvided that program specifications and instructions are advanced in\nthe same formal language, formal semantics provide the means to prove\ncorrectness. Accordingly, knowledge about the behaviors of\ncomputational systems is acquired by the deductive reasoning involved\nin mathematical proofs of correctness. The reason at the basis of such\na rationalist optimism (Eden 2007) about what can be known about\ncomputational systems is that they are artifacts, that is,\nhuman-made systems and, as such, one can predict their\nbehaviors with certainty (Knuth 1974). \nAlthough a central concern of theoretical computer science, the topics\nof computability and complexity are covered in existing entries on the\n Church-Turing thesis,\n computational complexity theory, and\n recursive functions. \nIn the late 1970s, the increasing number of applications of\ncomputational systems in everyday contexts, and the consequent booming\nof market demands caused a deviation of interests for computer\nscientists in Academia and in Industry: from focusing on methods of\nproving programs’ correctness, they turned to methods for\nmanaging complexity and evaluating the reliability of those system\n(Wegner 1976). Indeed, expressing formally the specifications,\nstructure, and input of highly complex programs embedded in larger\nsystems and interacting with users is practically impossible, and\nhence providing mathematical proofs of their correctness becomes\nmostly unfeasible. Computer science research developed in the\ndirection of testing techniques able to provide a statistical\nevaluation of correctness, often called reliability (Littlewood and\nStrigini 2000), in terms of estimation of error distributions in a\nprogram’s code. \nIn line with this engineering account of computer science is the\nthesis that reliability of computational systems is evaluated in the\nsame way that civil engineering does for bridges and aerospace\nengineering for airplanes (DeMillo et al. 1979). In\nparticular, whereas empirical sciences examine what exists, computer\nscience focuses on what can exist, i.e., on how to produce\nartifacts, and it should be therefore acknowledged as an\n“engineering of mathematics” (Hartmanis 1981). Similarly,\nwhereas scientific inquiries are involved in discovering laws\nconcerning the phenomena under observation, one cannot identify proper\nlaws in computer science practice, insofar as the latter is rather\ninvolved in the production of phenomena concerning computational\nartifacts (Brooks 1996). \nAs examined in\n §6,\n because software testing and reliability measuring techniques are\nknown for their incapability of assuring for the absence of code\nfaults (Dijkstra 1970), in many cases, and especially for the\nevaluation of the so-called safety-critical systems (such as\ncontrollers of airplanes, rockets, nuclear plants etc..), a\ncombination of formal methods and empirical testing is used to\nevaluate correctness and dependability. Computer science can\naccordingly be understood as a scientific discipline, in that it makes\nuse of both deductive and inductive probabilistic reasoning to examine\ncomputational systems (Denning et al. 1981, 2005, 2007; Tichy 1998;\nColburn 2000). \nThe thesis that computer science is, from a methodological viewpoint,\non a par with empirical sciences traces back to Newell, Perlis, and\nSimon’s 1967 letter to Science (Newell et al. 1967) and\ndominated all the 1980’s (Wegner 1976). In the 1975 Turing Award\nlecture, Newell and Simon argued: \nSince Newell and Simon’s Turing award lecture, it has been clear\nthat computer science can be understood as an empirical science but of\na special sort, and this is related to the nature of experiments in\ncomputing. Indeed, much current debate on the epistemological status\nof computer science concerns the problem of defining what kind of\nscience it is (Tedre 2011, Tedre 2015) and, in particular, the nature\nof experiments in computer science (Schiaffonati and Verdicchio 2014),\nthe nature, if any, of laws and theorems in computing (Hartmanis 1993;\nRombach and Seelish 2008), and the methodological relation between\ncomputer science and software engineering (Gruner 2011).","contact.mail":"turnr@essex.ac.uk","contact.domain":"essex.ac.uk"}]
