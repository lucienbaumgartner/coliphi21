[{"date.published":"1997-01-25","date.changed":"2019-03-08","url":"https://plato.stanford.edu/entries/game-theory/","author1":"Don Ross","author1.info":"http://uct.academia.edu/DonRoss","entry":"game-theory","body.text":"\n\n\nGame theory is the study of the ways in which interacting\nchoices of economic agents produce outcomes\nwith respect to the preferences (or utilities) of\nthose agents, where the outcomes in question might have been intended\nby none of the agents. The meaning of this statement will not be clear\nto the non-expert until each of the italicized words and phrases has\nbeen explained and featured in some examples. Doing this will be the\nmain business of this article. First, however, we provide some\nhistorical and philosophical context in order to motivate the reader\nfor the technical work ahead. \n\nGame theory in the form known to economists, social scientists, and\nbiologists, was given its first general mathematical formulation by\nJohn von Neuman and Oskar Morgenstern \n (1944).\n For reasons to be discussed later, limitations in their formal\nframework initially made the theory applicable only under special and\nlimited conditions. This situation has dramatically changed, in ways\nwe will examine as we go along, over the past seven decades, as the\nframework has been deepened and generalized. Refinements are still\nbeing made, and we will review a few outstanding problems that lie\nalong the advancing front edge of these developments towards the end\nof the article. However, since at least the late 1970s it has been\npossible to say with confidence that game theory is the most important\nand useful tool in the analyst’s kit whenever she confronts\nsituations in which what counts as one agent’s best action (for\nher) depends on expectations about what one or more other agents will\ndo, and what counts as their best actions (for them) similarly depend\non expectations about her.  \nDespite the fact that game theory has been rendered mathematically and\nlogically systematic only since 1944, game-theoretic insights can be\nfound among commentators going back to ancient times. For example, in\ntwo of Plato’s texts, the Laches and the\nSymposium, Socrates recalls an episode from the Battle of\nDelium that some commentators have interpreted (probably\nanachronistically) as involving the following situation. Consider a\nsoldier at the front, waiting with his comrades to repulse an enemy\nattack. It may occur to him that if the defense is likely to be\nsuccessful, then it isn’t very probable that his own personal\ncontribution will be essential. But if he stays, he runs the risk of\nbeing killed or wounded—apparently for no point. On the other\nhand, if the enemy is going to win the battle, then his chances of\ndeath or injury are higher still, and now quite clearly to no point,\nsince the line will be overwhelmed anyway. Based on this reasoning, it\nwould appear that the soldier is better off running away regardless of\nwho is going to win the battle. Of course, if all of the soldiers\nreason this way—as they all apparently should, since\nthey’re all in identical situations—then this will\ncertainly bring about the outcome in which the battle is\nlost. Of course, this point, since it has occurred to us as analysts,\ncan occur to the soldiers too. Does this give them a reason for\nstaying at their posts? Just the contrary: the greater the\nsoldiers’ fear that the battle will be lost, the greater their\nincentive to get themselves out of harm’s way. And the greater\nthe soldiers’ belief that the battle will be won, without the\nneed of any particular individual’s contributions, the less\nreason they have to stay and fight. If each soldier\nanticipates this sort of reasoning on the part of the others,\nall will quickly reason themselves into a panic, and their horrified\ncommander will have a rout on his hands before the enemy has even\nengaged. \nLong before game theory had come along to show analysts how to think\nabout this sort of problem systematically, it had occurred to some\nactual military leaders and influenced their strategies. Thus the\nSpanish conqueror Cortez, when landing in Mexico with a small force\nwho had good reason to fear their capacity to repel attack from the\nfar more numerous Aztecs, removed the risk that his troops might think\ntheir way into a retreat by burning the ships on which they had\nlanded. With retreat having thus been rendered physically impossible,\nthe Spanish soldiers had no better course of action than to stand and\nfight—and, furthermore, to fight with as much determination as\nthey could muster. Better still, from Cortez’s point of view,\nhis action had a discouraging effect on the motivation of the Aztecs.\nHe took care to burn his ships very visibly, so that the Aztecs would\nbe sure to see what he had done. They then reasoned as follows: Any\ncommander who could be so confident as to willfully destroy his own\noption to be prudent if the battle went badly for him must have good\nreasons for such extreme optimism. It cannot be wise to attack an\nopponent who has a good reason (whatever, exactly, it might be) for\nbeing sure that he can’t lose. The Aztecs therefore retreated\ninto the surrounding hills, and Cortez had the easiest possible\nvictory. \nThese two situations, at Delium and as manipulated by Cortez, have a\ncommon and interesting underlying logic. Notice that the soldiers are\nnot motivated to retreat just, or even mainly, by their\nrational assessment of the dangers of battle and by their\nself-interest. Rather, they discover a sound reason to run away by\nrealizing that what it makes sense for them to do depends on what it\nwill make sense for others to do, and that all of the others can\nnotice this too. Even a quite brave soldier may prefer to run rather\nthan heroically, but pointlessly, die trying to stem the oncoming tide\nall by himself. Thus we could imagine, without contradiction, a\ncircumstance in which an army, all of whose members are brave, flees\nat top speed before the enemy makes a move. If the soldiers really\nare brave, then this surely isn’t the outcome any of\nthem wanted; each would have preferred that all stand and fight. What\nwe have here, then, is a case in which the interaction of\nmany individually rational decision-making processes—one process\nper soldier—produces an outcome intended by no one. (Most armies\ntry to avoid this problem just as Cortez did. Since they can’t\nusually make retreat physically impossible, they make it\neconomically impossible: they shoot deserters. Then standing\nand fighting is each soldier’s individually rational course of\naction after all, because the cost of running is sure to be at least\nas high as the cost of staying.) \nAnother classic source that invites this sequence of reasoning is\nfound in Shakespeare’s Henry V. During the Battle of\nAgincourt Henry decided to slaughter his French prisoners, in full\nview of the enemy and to the surprise of his subordinates, who\ndescribe the action as being out of moral character. The reasons Henry\ngives allude to non-strategic considerations: he is afraid that the\nprisoners may free themselves and threaten his position. However, a\ngame theorist might have furnished him with supplementary strategic\n(and similarly prudential, though perhaps not moral) justification.\nHis own troops observe that the prisoners have been killed, and\nobserve that the enemy has observed this. Therefore, they know what\nfate will await them at the enemy’s hand if they don’t\nwin. Metaphorically, but very effectively, their boats have been\nburnt. The slaughter of the prisoners plausibly sent a signal to the\nsoldiers of both sides, thereby changing their incentives in ways that\nfavoured English prospects for victory. \nThese examples might seem to be relevant only for those who find\nthemselves in sordid situations of cut-throat competition. Perhaps,\none might think, it is important for generals, politicians, mafiosi,\nsports coaches and others whose jobs involve strategic manipulation of\nothers, but the philosopher should only deplore its amorality. Such a\nconclusion would be highly premature, however. The study of the\nlogic that governs the interrelationships amongst incentives,\nstrategic interactions and outcomes has been fundamental in modern\npolitical philosophy, since centuries before anyone had an explicit\nname for this sort of logic. Philosophers share with social scientists\nthe need to be able to represent and systematically model not only\nwhat they think people normatively ought to do, but what they\noften actually do in interactive situations. \nHobbes’s Leviathan is often regarded as the founding\nwork in modern political philosophy, the text that began the\ncontinuing round of analyses of the function and justification of the\nstate and its restrictions on individual liberties. The core of\nHobbes’s reasoning can be given straightforwardly as follows.\nThe best situation for all people is one in which each is free to do\nas she pleases. (One may or may not agree with this as a matter of\npsychology or ideology, but it is Hobbes’s assumption.) Often,\nsuch free people will wish to cooperate with one another in order to\ncarry out projects that would be impossible for an individual acting\nalone. But if there are any immoral or amoral agents around, they will\nnotice that their interests might at least sometimes be best served by\ngetting the benefits from cooperation and not returning them. Suppose,\nfor example, that you agree to help me build my house in return for my\npromise to help you build yours. After my house is finished, I can\nmake your labour free to me simply by reneging on my promise. I then\nrealize, however, that if this leaves you with no house, you will have\nan incentive to take mine. This will put me in constant fear of you,\nand force me to spend valuable time and resources guarding myself\nagainst you. I can best minimize these costs by striking first and\nkilling you at the first opportunity. Of course, you can anticipate\nall of this reasoning by me, and so have good reason to try to beat me\nto the punch. Since I can anticipate this reasoning by\nyou, my original fear of you was not paranoid; nor was yours\nof me. In fact, neither of us actually needs to be immoral to get this\nchain of mutual reasoning going; we need only think that there is some\npossibility that the other might try to cheat on bargains.\nOnce a small wedge of doubt enters any one mind, the incentive induced\nby fear of the consequences of being preempted—hit\nbefore hitting first—quickly becomes overwhelming on both sides.\nIf either of us has any resources of our own that the other might\nwant, this murderous logic can take hold long before we are so silly\nas to imagine that we could ever actually get as far as making deals\nto help one another build houses in the first place. Left to their own\ndevices, agents who are at least sometimes narrowly self-interested\ncan repeatedly fail to derive the benefits of cooperation, and instead\nbe trapped in a state of ‘war of all against all’, in\nHobbes’s words. In these circumstances, human life, as he\nvividly and famously put it, will be “solitary, poor, nasty,\nbrutish and short.” \nHobbes’s proposed solution to this problem was tyranny. The\npeople can hire an agent—a government—whose job is to\npunish anyone who breaks any promise. So long as the threatened\npunishment is sufficiently dire then the cost of reneging on promises\nwill exceed the cost of keeping them. The logic here is identical to\nthat used by an army when it threatens to shoot deserters. If all\npeople know that these incentives hold for most others, then\ncooperation will not only be possible, but can be the expected norm,\nso that the war of all against all becomes a general peace. \nHobbes pushes the logic of this argument to a very strong conclusion,\narguing that it implies not only a government with the right and the\npower to enforce cooperation, but an ‘undivided’\ngovernment in which the arbitrary will of a single ruler must impose\nabsolute obligation on all. Few contemporary political theorists think\nthat the particular steps by which Hobbes reasons his way to this\nconclusion are both sound and valid. Working through these issues\nhere, however, would carry us away from our topic into details of\ncontractarian political philosophy. What is important in the present\ncontext is that these details, as they are in fact pursued in\ncontemporary debates, involve sophisticated interpretation of the\nissues using the resources of modern game theory. Furthermore,\nHobbes’s most basic point, that the fundamental justification\nfor the coercive authority and practices of governments is\npeoples’ own need to protect themselves from what game theorists\ncall ‘social dilemmas’, is accepted by many, if not most,\npolitical theorists. Notice that Hobbes has not argued that\ntyranny is a desirable thing in itself. The structure of his argument\nis that the logic of strategic interaction leaves only two general\npolitical outcomes possible: tyranny and anarchy. Sensible agents then\nchoose tyranny as the lesser of two evils. \nThe reasoning of the Athenian soldiers, of Cortez, and of\nHobbes’s political agents has a common logic, one derived from\ntheir situations. In each case, the aspect of the environment that is\nmost important to the agents’ achievement of their preferred\noutcomes is the set of expectations and possible reactions to their\nstrategies by other agents. The distinction between acting\nparametrically on a passive world and acting\nnon-parametrically on a world that tries to act in\nanticipation of these actions is fundamental. If you wish to kick a\nrock down a hill, you need only concern yourself with the rock’s\nmass relative to the force of your blow, the extent to which it is\nbonded with its supporting surface, the slope of the ground on the\nother side of the rock, and the expected impact of the collision on\nyour foot. The values of all of these variables are independent of\nyour plans and intentions, since the rock has no interests of its own\nand takes no actions to attempt to assist or thwart you. By contrast,\nif you wish to kick a person down the hill, then unless that person is\nunconscious, bound or otherwise incapacitated, you will likely not\nsucceed unless you can disguise your plans until it’s too late\nfor him to take either evasive or forestalling action. Furthermore,\nhis probable responses should be expected to visit costs upon you,\nwhich you would be wise to consider. Finally, the relative\nprobabilities of his responses will depend on his expectations about\nyour probable responses to his responses. (Consider the difference it\nwill make to both of your reasoning if one or both of you are armed,\nor one of you is bigger than the other, or one of you is the\nother’s boss.) The logical issues associated with the second\nsort of situation (kicking the person as opposed to the rock) are\ntypically much more complicated, as a simple hypothetical example will\nillustrate. \nSuppose first that you wish to cross a river that is spanned by three\nbridges. (Assume that swimming, wading or boating across are\nimpossible.) The first bridge is known to be safe and free of\nobstacles; if you try to cross there, you will succeed. The second\nbridge lies beneath a cliff from which large rocks sometimes fall. The\nthird is inhabited by deadly cobras. Now suppose you wish to\nrank-order the three bridges with respect to their preferability as\ncrossing-points. Unless you get positive enjoyment from risking your\nlife—which, as a human being, you might, a complication\nwe’ll take up later in this article—then your decision\nproblem here is straightforward. The first bridge is obviously best,\nsince it is safest. To rank-order the other two bridges, you require\ninformation about their relative levels of danger. If you can study\nthe frequency of rock-falls and the movements of the cobras for\nawhile, you might be able to calculate that the probability of your\nbeing crushed by a rock at the second bridge is 10% and of being\nstruck by a cobra at the third bridge is 20%. Your reasoning here is\nstrictly parametric because neither the rocks nor the cobras are\ntrying to influence your actions, by, for example, concealing their\ntypical patterns of behaviour because they know you are studying them.\nIt is obvious what you should do here: cross at the safe bridge. Now\nlet us complicate the situation a bit. Suppose that the bridge with\nthe rocks was immediately before you, while the safe bridge was a\nday’s difficult hike upstream. Your decision-making situation\nhere is slightly more complicated, but it is still strictly\nparametric. You would have to decide whether the cost of the long hike\nwas worth exchanging for the penalty of a 10% chance of being hit by a\nrock. However, this is all you must decide, and your probability of a\nsuccessful crossing is entirely up to you; the environment is not\ninterested in your plans. \nHowever, if we now complicate the situation by adding a non-parametric\nelement, it becomes more challenging. Suppose that you are a fugitive\nof some sort, and waiting on the other side of the river with a gun is\nyour pursuer. She will catch and shoot you, let us suppose, only if\nshe waits at the bridge you try to cross; otherwise, you will escape.\nAs you reason through your choice of bridge, it occurs to you that she\nis over there trying to anticipate your reasoning. It will seem that,\nsurely, choosing the safe bridge straight away would be a mistake,\nsince that is just where she will expect you, and your chances of\ndeath rise to certainty. So perhaps you should risk the rocks, since\nthese odds are much better. But wait … if you can reach this\nconclusion, your pursuer, who is just as rational and well-informed as\nyou are, can anticipate that you will reach it, and will be waiting\nfor you if you evade the rocks. So perhaps you must take your chances\nwith the cobras; that is what she must least expect. But, then, no\n… if she expects that you will expect that she will least\nexpect this, then she will most expect it. This dilemma, you realize\nwith dread, is general: you must do what your pursuer least expects;\nbut whatever you most expect her to least expect is automatically what\nshe will most expect. You appear to be trapped in indecision. All that\nmight console you a bit here is that, on the other side of the river,\nyour pursuer is trapped in exactly the same quandary, unable to decide\nwhich bridge to wait at because as soon as she imagines committing to\none, she will notice that if she can find a best reason to pick a\nbridge, you can anticipate that same reason and then avoid her. \nWe know from experience that, in situations such as this, people do\nnot usually stand and dither in circles forever. As we’ll see\nlater, there is a unique best solution available to each\nplayer. However, until the 1940s neither philosophers nor economists\nknew how to find it mathematically. As a result, economists were\nforced to treat non-parametric influences as if they were\ncomplications on parametric ones. This is likely to strike the reader\nas odd, since, as our example of the bridge-crossing problem was meant\nto show, non-parametric features are often fundamental features of\ndecision-making problems. Part of the explanation for game\ntheory’s relatively late entry into the field lies in the\nproblems with which economists had historically been concerned.\nClassical economists, such as Adam Smith and David Ricardo, were\nmainly interested in the question of how agents in very large\nmarkets—whole nations—could interact so as to bring about\nmaximum monetary wealth for themselves. Smith’s basic insight,\nthat efficiency is best maximized by agents first differentiating\ntheir potential contributions and then freely seeking mutually\nadvantageous bargains, was mathematically verified in the twentieth\ncentury. However, the demonstration of this fact applies only in\nconditions of ‘perfect competition,’ that is, when\nindividuals or firms face no costs of entry or exit into markets, when\nthere are no economies of scale, and when no agents’ actions\nhave unintended side-effects on other agents’ well-being.\nEconomists always recognized that this set of assumptions is purely an\nidealization for purposes of analysis, not a possible state of affairs\nanyone could try (or should want to try) to institutionally establish.\nBut until the mathematics of game theory matured near the end of the\n1970s, economists had to hope that the more closely a market\napproximates perfect competition, the more efficient it will\nbe. No such hope, however, can be mathematically or logically\njustified in general; indeed, as a strict generalization the\nassumption was shown to be false as far back as the 1950s. \nThis article is not about the foundations of economics, but it is\nimportant for understanding the origins and scope of game theory to\nknow that perfectly competitive markets have built into them a feature\nthat renders them susceptible to parametric analysis. Because agents\nface no entry costs to markets, they will open shop in any given\nmarket until competition drives all profits to zero. This implies that\nif production costs are fixed and demand is exogenous, then agents\nhave no options about how much to produce if they are trying to\nmaximize the differences between their costs and their revenues. These\nproduction levels can be determined separately for each agent, so none\nneed pay attention to what the others are doing; each agent treats her\ncounterparts as passive features of the environment. The other kind of\nsituation to which classical economic analysis can be applied without\nrecourse to game theory is that of a monopoly facing many customers.\nHere, as long as no customer has a share of demand large enough to\nexert strategic leverage, non-parametric considerations drop out and\nthe firm’s task is only to identify the combination of price and\nproduction quantity at which it maximizes profit. However, both\nperfect and monopolistic competition are very special and unusual\nmarket arrangements. Prior to the advent of game theory, therefore,\neconomists were severely limited in the class of circumstances to\nwhich they could straightforwardly apply their models. \nPhilosophers share with economists a professional interest in the\nconditions and techniques for the maximization of welfare. In\naddition, philosophers have a special concern with the logical\njustification of actions, and often actions must be justified by\nreference to their expected outcomes. (One tradition in moral\nphilosophy, utilitarianism, is based on the idea that all justifiable\nactions must be justified in this way.) Without game theory, both of\nthese problems resist analysis wherever non-parametric aspects are\nrelevant. We will demonstrate this shortly by reference to the most\nfamous (though not the most typical) game, the so-called\nPrisoner’s Dilemma, and to other, more typical, games.\nIn doing this, we will need to introduce, define and illustrate the\nbasic elements and techniques of game theory. \nAn economic agent is, by definition, an entity with\npreferences. Game theorists, like economists and philosophers\nstudying rational decision-making, describe these by means of an\nabstract concept called utility. This refers to some ranking,\non some specified scale, of the subjective welfare or change in\nsubjective welfare that an agent derives from an object or an event.\nBy ‘welfare’ we refer to some normative index of relative\nalignment between states of the world and agents’ valuations of\nthe states in question, justified by reference to some background\nframework. For example, we might evaluate the relative welfare of\ncountries (which we might model as agents for some purposes) by\nreference to their per capita incomes, and we might evaluate the\nrelative welfare of an animal, in the context of predicting and\nexplaining its behavioral dispositions, by reference to its expected\nevolutionary fitness. In the case of people, it is most typical in\neconomics and applications of game theory to evaluate their relative\nwelfare by reference to their own implicit or explicit judgments of\nit. This is why we referred above to subjective welfare.\nConsider a person who adores the taste of pickles but dislikes onions.\nShe might be said to associate higher utility with states of the world\nin which, all else being equal, she consumes more pickles and fewer\nonions than with states in which she consumes more onions and fewer\npickles. Examples of this kind suggest that ‘utility’\ndenotes a measure of subjective psychological fulfillment,\nand this is indeed how the concept was originally interpreted by\neconomists and philosophers influenced by the utilitarianism of Jeremy\nBentham. However, economists in the early 20th century recognized\nincreasingly clearly that their main interest was in the market\nproperty of decreasing marginal demand, regardless of whether that was\nproduced by satiated individual consumers or by some other factors. In\nthe 1930s this motivation of economists fit comfortably with the\ndominance of behaviourism and radical empiricism in psychology and in\nthe philosophy of science respectively. Behaviourists and radical\nempiricists objected to the theoretical use of such unobservable\nentities as ‘psychological fulfillment quotients.’ The\nintellectual climate was thus receptive to the efforts of the\neconomist Paul Samuelson\n (1938)\n to redefine utility in such a way that it becomes a purely technical\nconcept rather than one rooted in speculative psychology. Since\nSamuelson’s redefinition became standard in the 1950s, when we\nsay that an agent acts so as to maximize her utility, we mean by\n‘utility’ simply whatever it is that the agent’s\nbehavior suggests her to consistently act so as to make more probable.\nIf this looks circular to you, it should: theorists who follow\nSamuelson intend the statement ‘agents act so as to\nmaximize their utility’ as a tautology, where an\n‘(economic) agent’ is any entity that can be accurately\ndescribed as acting to maximize a utility function, an\n‘action’ is any utility-maximizing selection from a set of\npossible alternatives, and a‘utility function’ is what an\neconomic agent maximizes. Like other tautologies occurring in the\nfoundations of scientific theories, this interlocking (recursive)\nsystem of definitions is useful not in itself, but because it helps to\nfix our contexts of inquiry. \nThough the behaviourism of the 1930s has since been displaced by\nwidespread interest in cognitive processes, many theorists continue to\nfollow Samuelson’s way of understanding utility because they\nthink it important that game theory apply to any kind of\nagent—a person, a bear, a bee, a firm or a country—and not\njust to agents with human minds. When such theorists say that agents\nact so as to maximize their utility, they want this to be part of the\ndefinition of what it is to be an agent, not an empirical\nclaim about possible inner states and motivations. Samuelson’s\nconception of utility, defined by way of Revealed Preference\nTheory (RPT) introduced in his classic paper\n (Samuelson (1938))\n satisfies this demand. \nEconomists and others who interpret game theory in terms of RPT should\nnot think of game theory as in any way an empirical account of the\nmotivations of some flesh-and-blood actors (such as actual people).\nRather, they should regard game theory as part of the body of\nmathematics that is used to model those entities (which might or might\nnot literally exist) who consistently select elements from mutually\nexclusive action sets, resulting in patterns of choices, which,\nallowing for some stochasticity and noise, can be statistically\nmodeled as maximization of utility functions. On this interpretation,\ngame theory could not be refuted by any empirical observations, since\nit is not an empirical theory in the first place. Of course,\nobservation and experience could lead someone favoring this\ninterpretation to conclude that game theory is of little help\nin describing actual human behavior. \nSome other theorists understand the point of game theory differently.\nThey view game theory as providing an explanatory account of actual\nhuman strategic reasoning processes. For this idea to be applicable,\nwe must suppose that agents at least sometimes do what they do in\nnon-parametric settings because game-theoretic logic\nrecommends certain actions as the ‘rational’ ones. Such an\nunderstanding of game theory incorporates a normative aspect,\nsince ‘rationality’ is taken to denote a property that an\nagent should at least generally want to have. These two very general\nways of thinking about the possible uses of game theory are compatible\nwith the tautological interpretation of utility maximization. The\nphilosophical difference is not idle from the perspective of the\nworking game theorist, however. As we will see in a later section,\nthose who hope to use game theory to explain strategic\nreasoning, as opposed to merely strategic behavior,\nface some special philosophical and practical problems. \nSince game theory is a technology for formal modeling, we must have a\ndevice for thinking of utility maximization in mathematical terms.\nSuch a device is called a utility function. We will introduce\nthe general idea of a utility function through the special case of an\nordinal utility function. (Later, we will encounter utility\nfunctions that incorporate more information.) The utility-map for an\nagent is called a ‘function’ because it maps ordered\npreferences onto the real numbers. Suppose that agent x\nprefers bundle a to bundle b and bundle b\nto bundle c. We then map these onto a list of numbers, where\nthe function maps the highest-ranked bundle onto the largest number in\nthe list, the second-highest-ranked bundle onto the next-largest\nnumber in the list, and so on, thus: \nbundle b ≫ 2 \nbundle c ≫ 1 \nThe only property mapped by this function is order. The\nmagnitudes of the numbers are irrelevant; that is, it must not be\ninferred that x gets 3 times as much utility from bundle\na as she gets from bundle c. Thus we could represent\nexactly the same utility function as that above by  \nbundle b ≫ 12.6 \nbundle c ≫ −1,000,000 \nThe numbers featuring in an ordinal utility function are thus not\nmeasuring any quantity of anything. A utility-function in\nwhich magnitudes do matter is called ‘cardinal’.\nWhenever someone refers to a utility function without specifying which\nkind is meant, you should assume that it’s ordinal. These are\nthe sorts we’ll need for the first set of games we’ll\nexamine. Later, when we come to seeing how to solve games that involve\n(ex ante) uncertainty—our river-crossing game from Part\n1 above, for example—we’ll need to build cardinal utility\nfunctions. The technique for doing this was given by\n von Neumann & Morgenstern (1944),\n and was an essential aspect of their invention of game theory. For\nthe moment, however, we will need only ordinal functions. \nAll situations in which at least one agent can only act to maximize\nhis utility through anticipating (either consciously, or just\nimplicitly in his behavior) the responses to his actions by one or\nmore other agents is called a game. Agents involved in games\nare referred to as players. If all agents have optimal\nactions regardless of what the others do, as in purely parametric\nsituations or conditions of monopoly or perfect competition (see\n Section 1\n above) we can model this without appeal to game theory; otherwise, we\nneed it. \nGame theorists assume that players have sets of capacities that are\ntypically referred to in the literature of economics as comprising\n‘rationality’. Usually this is formulated by simple\nstatements such as ‘it is assumed that players are\nrational’. In literature critical of economics in general, or of\nthe importation of game theory into humanistic disciplines, this kind\nof rhetoric has increasingly become a magnet for attack. There is a\ndense and intricate web of connections associated with\n‘rationality’ in the Western cultural tradition, and the\nword has often been used to normatively marginalize characteristics as\nnormal and important as emotion, femininity and empathy. Game\ntheorists’ use of the concept need not, and generally does not,\nimplicate such ideology. For present purposes we will use\n‘economic rationality’ as a strictly technical, not\nnormative, term to refer to a narrow and specific set of restrictions\non preferences that are shared by von Neumann and Morgenstern’s\noriginal version of game theory, and RPT. Economists use a second,\nequally important (to them) concept of rationality when they are\nmodeling markets, which they call ‘rational expectations’.\nIn this phrase, ‘rationality’ refers not to restrictions\non preferences but to non-restrictions on information\nprocessing: rational expectations are idealized beliefs that reflect\nstatistically accurately weighted use of all information available to\nan agent. The reader should note that these two uses of one word\nwithin the same discipline are technically unconnected. Furthermore,\noriginal RPT has been specified over the years by several different\nsets of axioms for different modeling purposes. Once we decide to\ntreat rationality as a technical concept, each time we adjust the\naxioms we effectively modify the concept. Consequently, in any\ndiscussion involving economists and philosophers together, we can find\nourselves in a situation where different participants use the same\nword to refer to something different. For readers new to economics,\ngame theory, decision theory and the philosophy of action, this\nsituation naturally presents a challenge. \nIn this article, ‘economic rationality’ will be used in\nthe technical sense shared within game theory, microeconomics and\nformal decision theory, as follows. An economically rational player is\none who can (i) assess outcomes, in the sense of rank-ordering them\nwith respect to their contributions to her welfare; (ii) calculate\npaths to outcomes, in the sense of recognizing which sequences of\nactions are probabilistically associated with which outcomes; and\n(iii) select actions from sets of alternatives (which we’ll\ndescribe as ‘choosing’ actions) that yield her\nmost-preferred outcomes, given the actions of the other players. We\nmight summarize the intuition behind all this as follows: an entity is\nusefully modeled as an economically rational agent to the extent that\nit has alternatives, and chooses from amongst these in a way that is\nmotivated, at least more often than not, by what seems best for its\npurposes. (For readers who are antecedently familiar with the work of\nthe philosopher Daniel Dennett, we could equate the idea of an\neconomically rational agent with the kind of entity Dennett\ncharacterizes as intentional, and then say that we can\nusefully predict an economically rational agent’s behavior from\n‘the intentional stance’.) \nEconomic rationality might in some cases be satisfied by internal\ncomputations performed by an agent, and she might or might not be\naware of computing or having computed its conditions and implications.\nIn other cases, economic rationality might simply be embodied in\nbehavioral dispositions built by natural, cultural or market\nselection. In particular, in calling an action ‘chosen’ we\nimply no necessary deliberation, conscious or otherwise. We mean\nmerely that the action was taken when an alternative action was\navailable, in some sense of ‘available’ normally\nestablished by the context of the particular analysis.\n(‘Available’, as used by game theorists and economists,\nshould never be read as if it meant merely\n‘metaphysically’ or ‘logically’ available; it\nis almost always pragmatic, contextual and endlessly revisable by more\nrefined modeling.) \nEach player in a game faces a choice among two or more possible\nstrategies. A strategy is a predetermined ‘programme of\nplay’ that tells her what actions to take in response to\nevery possible strategy other players might use. The\nsignificance of the italicized phrase here will become clear when we\ntake up some sample games below. \nA crucial aspect of the specification of a game involves the\ninformation that players have when they choose strategies. The\nsimplest games (from the perspective of logical structure) are those\nin which agents have perfect information, meaning that at\nevery point where each agent’s strategy tells her to take an\naction, she knows everything that has happened in the game up to that\npoint. A board-game of sequential moves in which both players watch\nall the action (and know the rules in common), such as chess, is an\ninstance of such a game. By contrast, the example of the\nbridge-crossing game from Section 1 above illustrates a game of\nimperfect information, since the fugitive must choose a\nbridge to cross without knowing the bridge at which the pursuer has\nchosen to wait, and the pursuer similarly makes her decision in\nignorance of the choices of her quarry. Since game theory is about\neconomically rational action given the strategically significant\nactions of others, it should not surprise you to be told that what\nagents in games believe, or fail to believe, about each others’\nactions makes a considerable difference to the logic of our analyses,\nas we will see. \nThe difference between games of perfect and of imperfect information\nis related to (though certainly not identical with!) a distinction\nbetween ways of representing games that is based on order\nof play. Let us begin by distinguishing between sequential-move\nand simultaneous-move games in terms of information. It is natural, as\na first approximation, to think of sequential-move games as being ones\nin which players choose their strategies one after the other, and of\nsimultaneous-move games as ones in which players choose their\nstrategies at the same time. This isn’t quite right, however,\nbecause what is of strategic importance is not the temporal\norder of events per se, but whether and when players know\nabout other players’ actions relative to having to choose\ntheir own. For example, if two competing businesses are both planning\nmarketing campaigns, one might commit to its strategy months before\nthe other does; but if neither knows what the other has committed to\nor will commit to when they make their decisions, this is a\nsimultaneous-move game. Chess, by contrast, is normally played as a\nsequential-move game: you see what your opponent has done before\nchoosing your own next action. (Chess can be turned into a\nsimultaneous-move game if the players each call moves on a common\nboard while isolated from one another; but this is a very different\ngame from conventional chess.) \nIt was said above that the distinction between sequential-move and\nsimultaneous-move games is not identical to the distinction between\nperfect-information and imperfect-information games. Explaining why\nthis is so is a good way of establishing full understanding of both\nsets of concepts. As simultaneous-move games were characterized in the\nprevious paragraph, it must be true that all simultaneous-move games\nare games of imperfect information. However, some games may contain\nmixes of sequential and simultaneous moves. For example, two firms\nmight commit to their marketing strategies independently and in\nsecrecy from one another, but thereafter engage in pricing competition\nin full view of one another. If the optimal marketing strategies were\npartially or wholly dependent on what was expected to happen in the\nsubsequent pricing game, then the two stages would need to be analyzed\nas a single game, in which a stage of sequential play followed a stage\nof simultaneous play. Whole games that involve mixed stages of this\nsort are games of imperfect information, however temporally staged\nthey might be. Games of perfect information (as the name implies)\ndenote cases where no moves are simultaneous (and where no\nplayer ever forgets what has gone before). \nAs previously noted, games of perfect information are the (logically)\nsimplest sorts of games. This is so because in such games (as long as\nthe games are finite, that is, terminate after a known number of\nactions) players and analysts can use a straightforward procedure for\npredicting outcomes. A player in such a game chooses her first action\nby considering each series of responses and counter-responses that\nwill result from each action open to her. She then asks herself which\nof the available final outcomes brings her the highest utility, and\nchooses the action that starts the chain leading to this outcome. This\nprocess is called backward induction (because the reasoning\nworks backwards from eventual outcomes to present choice\nproblems). \nThere will be much more to be said about backward induction and its\nproperties in a later section (when we come to discuss equilibrium and\nequilibrium selection). For now, it has been described just so we can\nuse it to introduce one of the two types of mathematical objects used\nto represent games: game trees. A game tree is an example of\nwhat mathematicians call a directed graph. That is, it is a\nset of connected nodes in which the overall graph has a direction. We\ncan draw trees from the top of the page to the bottom, or from left to\nright. In the first case, nodes at the top of the page are interpreted\nas coming earlier in the sequence of actions. In the case of a tree\ndrawn from left to right, leftward nodes are prior in the sequence to\nrightward ones. An unlabelled tree has a structure of the following\nsort: Figure 1 \nThe point of representing games using trees can best be grasped by\nvisualizing the use of them in supporting backward-induction\nreasoning. Just imagine the player (or analyst) beginning at the end\nof the tree, where outcomes are displayed, and then working backwards\nfrom these, looking for sets of strategies that describe paths leading\nto them. Since a player’s utility function indicates which\noutcomes she prefers to which, we also know which paths she will\nprefer. Of course, not all paths will be possible because the other\nplayer has a role in selecting paths too, and won’t take actions\nthat lead to less preferred outcomes for him. We will present some\nexamples of this interactive path selection, and detailed techniques\nfor reasoning through these examples, after we have described a\nsituation we can use a tree to model. \nTrees are used to represent sequential games, because they\nshow the order in which actions are taken by the players. However,\ngames are sometimes represented on matrices rather than\ntrees. This is the second type of mathematical object used to\nrepresent games. Matrices, unlike trees, simply show the outcomes,\nrepresented in terms of the players’ utility functions, for\nevery possible combination of strategies the players might use. For\nexample, it makes sense to display the river-crossing game from\n Section 1\n on a matrix, since in that game both the fugitive and the hunter have\njust one move each, and each chooses their move in ignorance of what\nthe other has decided to do. Here, then, is part of the\nmatrix: Figure 2 \nThe fugitive’s three possible strategies—cross at the safe\nbridge, risk the rocks, or risk the cobras—form the rows of the\nmatrix. Similarly, the hunter’s three possible\nstrategies—waiting at the safe bridge, waiting at the rocky\nbridge and waiting at the cobra bridge—form the columns of the\nmatrix. Each cell of the matrix shows—or, rather would\nshow if our matrix was complete—an outcome defined in\nterms of the players’ payoffs. A player’s payoff\nis simply the number assigned by her ordinal utility function to the\nstate of affairs corresponding to the outcome in question. For each\noutcome, Row’s payoff is always listed first, followed by\nColumn’s. Thus, for example, the upper left-hand corner above\nshows that when the fugitive crosses at the safe bridge and the hunter\nis waiting there, the fugitive gets a payoff of 0 and the hunter gets\na payoff of 1. We interpret these by reference to the two\nplayers’ utility functions, which in this game are very simple.\nIf the fugitive gets safely across the river he receives a payoff of\n1; if he doesn’t he gets 0. If the fugitive doesn’t make\nit, either because he’s shot by the hunter or hit by a rock or\nbitten by a cobra, then the hunter gets a payoff of 1 and the fugitive\ngets a payoff of 0. \nWe’ll briefly explain the parts of the matrix that have been\nfilled in, and then say why we can’t yet complete the rest.\nWhenever the hunter waits at the bridge chosen by the fugitive, the\nfugitive is shot. These outcomes all deliver the payoff vector (0, 1).\nYou can find them descending diagonally across the matrix above from\nthe upper left-hand corner. Whenever the fugitive chooses the safe\nbridge but the hunter waits at another, the fugitive gets safely\nacross, yielding the payoff vector (1, 0). These two outcomes are\nshown in the second two cells of the top row. All of the other cells\nare marked, for now, with question marks. Why? The problem\nhere is that if the fugitive crosses at either the rocky bridge or the\ncobra bridge, he introduces parametric factors into the game. In these\ncases, he takes on some risk of getting killed, and so producing the\npayoff vector (0, 1), that is independent of anything the hunter does.\nWe don’t yet have enough concepts introduced to be able to show\nhow to represent these outcomes in terms of utility\nfunctions—but by the time we’re finished we will, and this\nwill provide the key to solving our puzzle from\n Section 1. \nMatrix games are referred to as ‘normal-form’ or\n‘strategic-form’ games, and games as trees are referred to\nas ‘extensive-form’ games. The two sorts of games are not\nequivalent, because extensive-form games contain\ninformation—about sequences of play and players’ levels of\ninformation about the game structure—that strategic-form games\ndo not. In general, a strategic-form game could represent any one of\nseveral extensive-form games, so a strategic-form game is best thought\nof as being a set of extensive-form games. When order of play\nis irrelevant to a game’s outcome, then you should study its\nstrategic form, since it’s the whole set you want to know about.\nWhere order of play is relevant, the extensive form\nmust be specified or your conclusions will be unreliable. \nThe distinctions described above are difficult to fully grasp if all\none has to go on are abstract descriptions. They’re best\nillustrated by means of an example. For this purpose, we’ll use\nthe most famous of all games: the Prisoner’s Dilemma. It in fact\ngives the logic of the problem faced by Cortez’s and Henry\nV’s soldiers (see\n Section 1 above),\n and by Hobbes’s agents before they empower the tyrant. However,\nfor reasons which will become clear a bit later, you should not take\nthe PD as a typical game; it isn’t. We use it as an\nextended example here only because it’s particularly helpful for\nillustrating the relationship between strategic-form and\nextensive-form games (and later, for illustrating the relationships\nbetween one-shot and repeated games; see\n Section 4\n below).  \nThe name of the Prisoner’s Dilemma game is derived from the\nfollowing situation typically used to exemplify it. Suppose that the\npolice have arrested two people whom they know have committed an armed\nrobbery together. Unfortunately, they lack enough admissible evidence\nto get a jury to convict. They do, however, have enough\nevidence to send each prisoner away for two years for theft of the\ngetaway car. The chief inspector now makes the following offer to each\nprisoner: If you will confess to the robbery, implicating your\npartner, and she does not also confess, then you’ll go free and\nshe’ll get ten years. If you both confess, you’ll each get\n5 years. If neither of you confess, then you’ll each get two\nyears for the auto theft. \nOur first step in modeling the two prisoners’ situation as a\ngame is to represent it in terms of utility functions. Following the\nusual convention, let us name the prisoners ‘Player I’ and\n‘Player II’. Both Player I’s and Player II’s\nordinal utility functions are identical: Go free ≫ 4 2 years ≫ 3 5 years ≫ 2 10 years ≫ 0 \nThe numbers in the function above are now used to express each\nplayer’s payoffs in the various outcomes possible in\nthe situation. We can represent the problem faced by both of them on a\nsingle matrix that captures the way in which their separate choices\ninteract; this is the strategic form of their game: Figure 3 \nEach cell of the matrix gives the payoffs to both players for each\ncombination of actions. Player I’s payoff appears as the first\nnumber of each pair, Player II’s as the second. So, if both\nplayers confess then they each get a payoff of 2 (5 years in prison\neach). This appears in the upper-left cell. If neither of them\nconfess, they each get a payoff of 3 (2 years in prison each). This\nappears as the lower-right cell. If Player I confesses and Player II\ndoesn’t then Player I gets a payoff of 4 (going free) and Player\nII gets a payoff of 0 (ten years in prison). This appears in the\nupper-right cell. The reverse situation, in which Player II confesses\nand Player I refuses, appears in the lower-left cell. \nEach player evaluates his or her two possible actions here by\ncomparing their personal payoffs in each column, since this shows you\nwhich of their actions is preferable, just to themselves, for each\npossible action by their partner. So, observe: If Player II confesses\nthen Player I gets a payoff of 2 by confessing and a payoff of 0 by\nrefusing. If Player II refuses, then Player I gets a payoff of 4 by\nconfessing and a payoff of 3 by refusing. Therefore, Player I is\nbetter off confessing regardless of what Player II does. Player II,\nmeanwhile, evaluates her actions by comparing her payoffs down each\nrow, and she comes to exactly the same conclusion that Player I does.\nWherever one action for a player is superior to her other actions for\neach possible action by the opponent, we say that the first action\nstrictly dominates the second one. In the PD, then,\nconfessing strictly dominates refusing for both players. Both players\nknow this about each other, thus entirely eliminating any temptation\nto depart from the strictly dominated path. Thus both players will\nconfess, and both will go to prison for 5 years. \nThe players, and analysts, can predict this outcome using a mechanical\nprocedure, known as iterated elimination of strictly dominated\nstrategies. Player 1 can see by examining the matrix that his payoffs\nin each cell of the top row are higher than his payoffs in each\ncorresponding cell of the bottom row. Therefore, it can never be\nutility-maximizing for him to play his bottom-row strategy, viz.,\nrefusing to confess, regardless of what Player II does. Since\nPlayer I’s bottom-row strategy will never be played, we can\nsimply delete the bottom row from the matrix. Now it is\nobvious that Player II will not refuse to confess, since her payoff\nfrom confessing in the two cells that remain is higher than her payoff\nfrom refusing. So, once again, we can delete the one-cell column on\nthe right from the game. We now have only one cell remaining, that\ncorresponding to the outcome brought about by mutual confession. Since\nthe reasoning that led us to delete all other possible outcomes\ndepended at each step only on the premise that both players are\neconomically rational — that is, will choose strategies that\nlead to higher payoffs over strategies that lead to lower\nones—there are strong grounds for viewing joint confession as\nthe solution to the game, the outcome on which its play\nmust converge to the extent that economic rationality\ncorrectly models the behavior of the players. You should note that the\norder in which strictly dominated rows and columns are deleted\ndoesn’t matter. Had we begun by deleting the right-hand column\nand then deleted the bottom row, we would have arrived at the same\nsolution. \nIt’s been said a couple of times that the PD is not a typical\ngame in many respects. One of these respects is that all its rows and\ncolumns are either strictly dominated or strictly dominant. In any\nstrategic-form game where this is true, iterated elimination of\nstrictly dominated strategies is guaranteed to yield a unique\nsolution. Later, however, we will see that for many games this\ncondition does not apply, and then our analytic task is less\nstraightforward. \nThe reader will probably have noticed something disturbing about the\noutcome of the PD. Had both players refused to confess, they’d\nhave arrived at the lower-right outcome in which they each go to\nprison for only 2 years, thereby both earning higher utility\nthan either receives when both confess. This is the most important\nfact about the PD, and its significance for game theory is quite\ngeneral. We’ll therefore return to it below when we discuss\nequilibrium concepts in game theory. For now, however, let us stay\nwith our use of this particular game to illustrate the difference\nbetween strategic and extensive forms. \nWhen people introduce the PD into popular discussions, one will often\nhear them say that the police inspector must lock his prisoners into\nseparate rooms so that they can’t communicate with one another.\nThe reasoning behind this idea seems obvious: if the players could\ncommunicate, they’d surely see that they’re each better\noff if both refuse, and could make an agreement to do so, no? This,\none presumes, would remove each player’s conviction that he or\nshe must confess because they’ll otherwise be sold up the river\nby their partner. In fact, however, this intuition is misleading and\nits conclusion is false. \nWhen we represent the PD as a strategic-form game, we implicitly\nassume that the prisoners can’t attempt collusive agreement\nsince they choose their actions simultaneously. In this case,\nagreement before the fact can’t help. If Player I is convinced\nthat his partner will stick to the bargain then he can seize the\nopportunity to go scot-free by confessing. Of course, he realizes that\nthe same temptation will occur to Player II; but in that case he again\nwants to make sure he confesses, as this is his only means of avoiding\nhis worst outcome. The prisoners’ agreement comes to naught\nbecause they have no way of enforcing it; their promises to each other\nconstitute what game theorists call ‘cheap talk’. \nBut now suppose that the prisoners do not move\nsimultaneously. That is, suppose that Player II can choose\nafter observing Player I’s action. This is the sort of\nsituation that people who think non-communication important must have\nin mind. Now Player II will be able to see that Player I has remained\nsteadfast when it comes to her choice, and she need not be concerned\nabout being suckered. However, this doesn’t change anything, a\npoint that is best made by re-representing the game in extensive form.\nThis gives us our opportunity to introduce game-trees and the method\nof analysis appropriate to them. \nFirst, however, here are definitions of some concepts that will be\nhelpful in analyzing game-trees: \nInitial node: the point at which the first action in the game\noccurs. \nTerminal node: any node which, if reached, ends the game.\nEach terminal node corresponds to an outcome. \nSubgame: any connected set of nodes and branches descending\nuniquely from one node. \nPayoff: an ordinal utility number assigned to a player at an\noutcome. \nOutcome: an assignment of a set of payoffs, one to each\nplayer in the game. \nStrategy: a program instructing a player which action to take\nat every node in the tree where she could possibly be called on to\nmake a choice. \nThese quick definitions may not mean very much to you until you follow\nthem being put to use in our analyses of trees below. It will probably\nbe best if you scroll back and forth between them and the examples as\nwe work through them. By the time you understand each example,\nyou’ll find the concepts and their definitions natural and\nintuitive.  \nTo make this exercise maximally instructive, let’s suppose that\nPlayers I and II have studied the matrix above and, seeing that\nthey’re both better off in the outcome represented by the\nlower-right cell, have formed an agreement to cooperate. Player I is\nto commit to refusal first, after which Player II will reciprocate\nwhen the police ask for her choice. We will refer to a strategy of\nkeeping the agreement as ‘cooperation’, and will denote it\nin the tree below with ‘C’. We will refer to a strategy of\nbreaking the agreement as ‘defection’, and will denote it\non the tree below with ‘D’. Each node is numbered 1, 2, 3,\n… , from top to bottom, for ease of reference in discussion.\nHere, then, is the tree: Figure 4 \nLook first at each of the terminal nodes (those along the bottom).\nThese represent possible outcomes. Each is identified with an\nassignment of payoffs, just as in the strategic-form game, with Player\nI’s payoff appearing first in each set and Player II’s\nappearing second. Each of the structures descending from the nodes 1,\n2 and 3 respectively is a subgame. We begin our backward-induction\nanalysis—using a technique called Zermelo’s\nalgorithm—with the sub-games that arise last in the\nsequence of play. If the subgame descending from node 3 is played,\nthen Player II will face a choice between a payoff of 4 and a payoff\nof 3. (Consult the second number, representing her payoff, in each set\nat a terminal node descending from node 3.) II earns her higher payoff\nby playing D. We may therefore replace the entire subgame with an\nassignment of the payoff (0,4) directly to node 3, since this is the\noutcome that will be realized if the game reaches that node. Now\nconsider the subgame descending from node 2. Here, II faces a choice\nbetween a payoff of 2 and one of 0. She obtains her higher payoff, 2,\nby playing D. We may therefore assign the payoff (2,2) directly to\nnode 2. Now we move to the subgame descending from node 1. (This\nsubgame is, of course, identical to the whole game; all games are\nsubgames of themselves.) Player I now faces a choice between outcomes\n(2,2) and (0,4). Consulting the first numbers in each of these sets,\nhe sees that he gets his higher payoff—2—by playing D. D\nis, of course, the option of confessing. So Player I confesses, and\nthen Player II also confesses, yielding the same outcome as in the\nstrategic-form representation. \nWhat has happened here intuitively is that Player I realizes that if\nhe plays C (refuse to confess) at node 1, then Player II will be able\nto maximize her utility by suckering him and playing D. (On the tree,\nthis happens at node 3.) This leaves Player I with a payoff of 0 (ten\nyears in prison), which he can avoid only by playing D to begin with.\nHe therefore defects from the agreement. \nWe have thus seen that in the case of the Prisoner’s Dilemma,\nthe simultaneous and sequential versions yield the same outcome. This\nwill often not be true of other games, however. Furthermore, only\nfinite extensive-form (sequential) games of perfect information can be\nsolved using Zermelo’s algorithm. \nAs noted earlier in this section, sometimes we must represent\nsimultaneous moves within games that are otherwise\nsequential. (In all such cases the game as a whole will be one of\nimperfect information, so we won’t be able to solve it using\nZermelo’s algorithm.) We represent such games using the device\nof information sets. Consider the following tree: Figure 5 \nThe oval drawn around nodes b and c indicates that\nthey lie within a common information set. This means that at these\nnodes players cannot infer back up the path from whence they came;\nPlayer II does not know, in choosing her strategy, whether she is at\nb or c. (For this reason, what properly bear numbers\nin extensive-form games are information sets, conceived as\n‘action points’, rather than nodes themselves; this is why\nthe nodes inside the oval are labelled with letters rather than\nnumbers.) Put another way, Player II, when choosing, does not know\nwhat Player I has done at node a. But you will recall from\nearlier in this section that this is just what defines two moves as\nsimultaneous. We can thus see that the method of representing games as\ntrees is entirely general. If no node after the initial node is alone\nin an information set on its tree, so that the game has only one\nsubgame (itself), then the whole game is one of simultaneous play. If\nat least one node shares its information set with another, while\nothers are alone, the game involves both simultaneous and sequential\nplay, and so is still a game of imperfect information. Only if all\ninformation sets are inhabited by just one node do we have a game of\nperfect information. \nIn the Prisoner’s Dilemma, the outcome we’ve represented\nas (2,2), indicating mutual defection, was said to be the\n‘solution’ to the game. Following the general practice in\neconomics, game theorists refer to the solutions of games as\nequilibria. Philosophically minded readers will want to pose\na conceptual question right here: What is ‘equilibrated’\nabout some game outcomes such that we are motivated to call them\n‘solutions’? When we say that a physical system is in\nequilibrium, we mean that it is in a stable state, one in\nwhich all the causal forces internal to the system balance each other\nout and so leave it ‘at rest’ until and unless it is\nperturbed by the intervention of some exogenous (that is,\n‘external’) force. This is what economists have\ntraditionally meant in talking about ‘equilibria’; they\nread economic systems as being networks of mutually constraining\n(often causal) relations, just like physical systems, and the\nequilibria of such systems are then their endogenously stable states.\n(Note that, in both physical and economic systems, endogenously stable\nstates might never be directly observed because the systems in\nquestion are never isolated from exogenous influences that move and\ndestabilize them. In both classical mechanics and in economics,\nequilibrium concepts are tools for analysis, not predictions\nof what we expect to observe.) As we will see in later sections, it is\npossible to maintain this understanding of equilibria in the case of\ngame theory. However, as we noted in Section 2.1, some people\ninterpret game theory as being an explanatory theory of strategic\nreasoning. For them, a solution to a game must be an outcome that a\nrational agent would predict using the mechanisms of rational\ncomputation alone. Such theorists face some puzzles about\nsolution concepts that are less important to the theorist who\nisn’t trying to use game theory to under-write a general\nanalysis of rationality. The interest of philosophers in game theory\nis more often motivated by this ambition than is that of the economist\nor other scientist.  \nIt’s useful to start the discussion here from the case of the\nPrisoner’s Dilemma because it’s unusually simple from the\nperspective of the puzzles about solution concepts. What we referred\nto as its ‘solution’ is the unique Nash\nequilibrium of the game. (The ‘Nash’ here refers to\nJohn Nash, the Nobel Laureate mathematician who in\n Nash (1950)\n did most to extend and generalize von Neumann &\nMorgenstern’s pioneering work.) Nash equilibrium (henceforth\n‘NE’) applies (or fails to apply, as the case may be) to\nwhole sets of strategies, one for each player in a game. A\nset of strategies is a NE just in case no player could improve her\npayoff, given the strategies of all other players in the game, by\nchanging her strategy. Notice how closely this idea is related to the\nidea of strict dominance: no strategy could be a NE strategy if it is\nstrictly dominated. Therefore, if iterative elimination of strictly\ndominated strategies takes us to a unique outcome, we know that the\nvector of strategies that leads to it is the game’s unique NE.\nNow, almost all theorists agree that avoidance of strictly dominated\nstrategies is a minimum requirement of economic rationality.\nA player who knowingly chooses a strictly dominated strategy directly\nviolates clause (iii) of the definition of economic agency as given in\n Section 2.2.\n This implies that if a game has an outcome that is a unique\nNE, as in the case of joint confession in the PD, that must be its\nunique solution. This is one of the most important respects in which\nthe PD is an ‘easy’ (and atypical) game. \nWe can specify one class of games in which NE is always not only\nnecessary but sufficient as a solution concept. These are\nfinite perfect-information games that are also zero-sum. A\nzero-sum game (in the case of a game involving just two players) is\none in which one player can only be made better off by making the\nother player worse off. (Tic-tac-toe is a simple example of such a\ngame: any move that brings one player closer to winning brings her\nopponent closer to losing, and vice-versa.) We can determine whether a\ngame is zero-sum by examining players’ utility functions: in\nzero-sum games these will be mirror-images of each other, with one\nplayer’s highly ranked outcomes being low-ranked for the other\nand vice-versa. In such a game, if I am playing a strategy such that,\ngiven your strategy, I can’t do any better, and if you are\nalso playing such a strategy, then, since any change of\nstrategy by me would have to make you worse off and vice-versa, it\nfollows that our game can have no solution compatible with our mutual\neconomic rationality other than its unique NE. We can put this another\nway: in a zero-sum game, my playing a strategy that maximizes my\nminimum payoff if you play the best you can, and your simultaneously\ndoing the same thing, is just equivalent to our both playing\nour best strategies, so this pair of so-called ‘maximin’\nprocedures is guaranteed to find the unique solution to the game,\nwhich is its unique NE. (In tic-tac-toe, this is a draw. You\ncan’t do any better than drawing, and neither can I, if both of\nus are trying to win and trying not to lose.) \nHowever, most games do not have this property. It won’t be\npossible, in this one article, to enumerate all of the ways\nin which games can be problematic from the perspective of their\npossible solutions. (For one thing, it is highly unlikely that\ntheorists have yet discovered all of the possible problems.) However,\nwe can try to generalize the issues a bit. \nFirst, there is the problem that in most non-zero-sum games, there is\nmore than one NE, but not all NE look equally plausible as the\nsolutions upon which strategically alert players would hit. Consider\nthe strategic-form game below (taken from\n Kreps (1990),\n p. 403): Figure 6 \nThis game has two NE: s1-t1 and s2-t2. (Note that no rows or columns\nare strictly dominated here. But if Player I is playing s1 then Player\nII can do no better than t1, and vice-versa; and similarly for the\ns2-t2 pair.) If NE is our only solution concept, then we shall be\nforced to say that either of these outcomes is equally persuasive as a\nsolution. However, if game theory is regarded as an explanatory and/or\nnormative theory of strategic reasoning, this seems to be leaving\nsomething out: surely sensible players with perfect information would\nconverge on s1-t1? (Note that this is not like the situation\nin the PD, where the socially superior situation is unachievable\nbecause it is not a NE. In the case of the game above, both players\nhave every reason to try to converge on the NE in which they are\nbetter off.) \nThis illustrates the fact that NE is a relatively (logically)\nweak solution concept, often failing to predict intuitively\nsensible solutions because, if applied alone, it refuses to allow\nplayers to use principles of equilibrium selection that, if not\ndemanded by economic rationality—or a more ambitious\nphilosopher’s concept of rationality—at least seem both\nsensible and computationally accessible. Consider another example from\n Kreps (1990),\n p. 397: Figure 7 \nHere, no strategy strictly dominates another. However, Player\nI’s top row, s1, weakly dominates s2, since I does\nat least as well using s1 as s2 for any reply by Player II,\nand on one reply by II (t2), I does better. So should not the players\n(and the analyst) delete the weakly dominated row s2? When they do so,\ncolumn t1 is then strictly dominated, and the NE s1-t2 is selected as\nthe unique solution. However, as Kreps goes on to show using this\nexample, the idea that weakly dominated strategies should be deleted\njust like strict ones has odd consequences.\n Suppose\n we change the payoffs of the game just a bit, as follows: Figure 8 \ns2 is still weakly dominated as before; but of our two NE, s2-t1 is\nnow the most attractive for both players; so why should the analyst\neliminate its possibility? (Note that this game, again, does\nnot replicate the logic of the PD. There, it makes sense to\neliminate the most attractive outcome, joint refusal to confess,\nbecause both players have incentives to unilaterally deviate from it,\nso it is not an NE. This is not true of s2-t1 in the present game. You\nshould be starting to clearly see why we called the PD game\n‘atypical’.) The argument for eliminating weakly\ndominated strategies is that Player 1 may be nervous, fearing that\nPlayer II is not completely sure to be economically rational\n(or that Player II fears that Player I isn’t completely reliably\neconomically rational, or that Player II fears that Player I fears\nthat Player II isn’t completely reliably economically rational,\nand so on ad infinitum) and so might play t2 with some positive\nprobability. If the possibility of departures from reliable economic\nrationality is taken seriously, then we have an argument for\neliminating weakly dominated strategies: Player I thereby insures\nherself against her worst outcome, s2-t2. Of course, she pays a cost\nfor this insurance, reducing her expected payoff from 10 to 5. On the\nother hand, we might imagine that the players could communicate before\nplaying the game and agree to play correlated strategies so\nas to coordinate on s2-t1, thereby removing some, most or all\nof the uncertainty that encourages elimination of the weakly dominated\nrow s1, and eliminating s1-t2 as a viable solution instead!  \nAny proposed principle for solving games that may have the effect of\neliminating one or more NE from consideration as solutions is referred\nto as a refinement of NE. In the case just discussed,\nelimination of weakly dominated strategies is one possible refinement,\nsince it refines away the NE s2-t1, and correlation is another, since\nit refines away the other NE, s1-t2, instead. So which refinement is\nmore appropriate as a solution concept? People who think of game\ntheory as an explanatory and/or normative theory of strategic\nrationality have generated a substantial literature in which the\nmerits and drawbacks of a large number of refinements are debated. In\nprinciple, there seems to be no limit on the number of refinements\nthat could be considered, since there may also be no limits on the set\nof philosophical intuitions about what principles a rational agent\nmight or might not see fit to follow or to fear or hope that other\nplayers are following. \nWe now digress briefly to make a point about terminology. Theorists\nwho adopt the revealed preference interpretation of the utility\nfunctions in game theory are sometimes referred to in the philosophy\nof economics literature as ‘behaviorists’. This reflects\nthe fact the revealed preference approaches equate choices with\neconomically consistent actions, rather than being intended to refer\nto mental constructs. Historically, there was a relationship of\ncomfortable alignment, though not direct theoretical co-construction,\nbetween revealed preference in economics and the methodological and\nontological behaviorism that dominated scientific psychology during\nthe middle decades of the twentieth century. However, this usage is\nincreasingly likely to cause confusion due to the more recent rise of\nbehavioral game theory\n (Camerer 2003).\n This program of research aims to directly incorporate into\ngame-theoretic models generalizations, derived mainly from experiments\nwith people, about ways in which people differ from purer economic\nagents in the inferences they draw from information\n(‘framing’). Applications also typically incorporate\nspecial assumptions about utility functions, also derived from\nexperiments. For example, players may be taken to be willing to make\ntrade-offs between the magnitudes of their own payoffs and\ninequalities in the distribution of payoffs among the players. We will\nturn to some discussion of behavioral game theory in\n Section 8.1,\n Section 8.2 and\n Section 8.3.\n For the moment, note that this use of game theory crucially rests on\nassumptions about psychological representations of value thought to be\ncommon among people. Thus it would be misleading to refer to\nbehavioral game theory as ‘behaviorist’. But then it just\nwould invite confusion to continue referring to conventional economic\ngame theory that relies on revealed preference as\n‘behaviorist’ game theory. We will therefore refer to it\nas ‘non-psychological’ game theory. We mean by this the\nkind of game theory used by most economists who are not\nrevisionist behavioral economists. (We use the qualifier\n‘revisionist’ to reflect the further complication that\nincreasingly many economists who apply revealed preference concepts\nconduct experiments, and some of them call themselves\n‘behavioral economists’! For a proposed new set of\nconventions to reduce this labeling chaos, see\n Ross (2014),\n pp. 200–201.) These ‘establishment’ economists\ntreat game theory as the abstract mathematics of strategic\ninteraction, rather than as an attempt to directly characterize\nspecial psychological dispositions that might be typical in\nhumans. \nNon-psychological game theorists tend to take a dim view of much of\nthe refinement program. This is for the obvious reason that it relies\non intuitions about which kinds of inferences people should\nfind sensible. Like most scientists, non-psychological game theorists\nare suspicious of the force and basis of philosophical assumptions as\nguides to empirical and mathematical modeling. \nBehavioral game theory, by contrast, can be understood as a refinement\nof game theory, though not necessarily of its solution concepts, in a\ndifferent sense. It restricts the theory’s underlying axioms for\napplication to a special class of agents, individual, psychologically\ntypical humans. It motivates this restriction by reference to\ninferences, along with preferences, that people do find\nnatural, regardless of whether these seem rational,\nwhich they frequently do not. Non-psychological and behavioral game\ntheory have in common that neither is intended to be\nnormative—though both are often used to try to describe\nnorms that prevail in groups of players, as well to explain\nwhy norms might persist in groups of players even when they appear to\nbe less than fully rational to philosophical intuitions. Both see the\njob of applied game theory as being to predict outcomes of\nempirical games given some distribution of strategic\ndispositions, and some distribution of expectations about the\nstrategic dispositions of others, that are shaped by dynamics in\nplayers’ environments, including institutional pressures and\nstructures and evolutionary selection. Let us therefore group\nnon-psychological and behavioral game theorists together, just for\npurposes of contrast with normative game theorists, as\ndescriptive game theorists. \nDescriptive game theorists are often inclined to doubt that the goal\nof seeking a general theory of rationality makes sense as a\nproject. Institutions and evolutionary processes build many\nenvironments, and what counts as rational procedure in one environment\nmay not be favoured in another. On the other hand, an entity that does\nnot at least stochastically (i.e., perhaps noisily but statistically\nmore often than not) satisfy the minimal restrictions of economic\nrationality cannot, except by accident, be accurately characterized as\naiming to maximize a utility function. To such entities game theory\nhas no application in the first place.  \nThis does not imply that non-psychological game theorists abjure all\nprincipled ways of restricting sets of NE to subsets based on their\nrelative probabilities of arising. In particular, non-psychological\ngame theorists tend to be sympathetic to approaches that shift\nemphasis from rationality onto considerations of the informational\ndynamics of games. We should perhaps not be surprised that NE analysis\nalone often fails to tell us much of applied, empirical interest about\nstrategic-form games (e.g., Figure 6 above), in which informational\nstructure is suppressed. Equilibrium selection issues are often more\nfruitfully addressed in the context of extensive-form games. \nIn order to deepen our understanding of extensive-form games, we need\nan example with more interesting structure than the PD offers.  \nConsider the game described by this tree: Figure 9 \nThis game is not intended to fit any preconceived situation; it is\nsimply a mathematical object in search of an application. (L and R\nhere just denote ‘left’ and ‘right’\nrespectively.)  \nNow consider the strategic form of this game: Figure 10 \nIf you are confused by this, remember that a strategy must tell a\nplayer what to do at every information set where that player\nhas an action. Since each player chooses between two actions at each\nof two information sets here, each player has four strategies in\ntotal. The first letter in each strategy designation tells each player\nwhat to do if he or she reaches their first information set, the\nsecond what to do if their second information set is reached. I.e., LR\nfor Player II tells II to play L if information set 5 is reached and R\nif information set 6 is reached.  \nIf you examine the matrix in Figure 10, you will discover that (LL,\nRL) is among the NE. This is a bit puzzling, since if Player I reaches\nher second information set (7) in the extensive-form game, she would\nhardly wish to play L there; she earns a higher payoff by playing R at\nnode 7. Mere NE analysis doesn’t notice this because NE is\ninsensitive to what happens off the path of play. Player I,\nin choosing L at node 4, ensures that node 7 will not be reached; this\nis what is meant by saying that it is ‘off the path of\nplay’. In analyzing extensive-form games, however, we\nshould care what happens off the path of play, because\nconsideration of this is crucial to what happens on the path.\nFor example, it is the fact that Player I would play R if\nnode 7 were reached that would cause Player II to play L if\nnode 6 were reached, and this is why Player I won’t choose R at\nnode 4. We are throwing away information relevant to game solutions if\nwe ignore off-path outcomes, as mere NE analysis does. Notice that\nthis reason for doubting that NE is a wholly satisfactory equilibrium\nconcept in itself has nothing to do with intuitions about rationality,\nas in the case of the refinement concepts discussed in Section\n2.5. \nNow apply Zermelo’s algorithm to the extensive form of our\ncurrent example. Begin, again, with the last subgame, that descending\nfrom node 7. This is Player I’s move, and she would choose R\nbecause she prefers her payoff of 5 to the payoff of 4 she gets by\nplaying L. Therefore, we assign the payoff (5, −1) to node 7.\nThus at node 6 II faces a choice between (−1, 0) and (5,\n−1). He chooses L. At node 5 II chooses R. At node 4 I is thus\nchoosing between (0, 5) and (−1, 0), and so plays L. Note that,\nas in the PD, an outcome appears at a terminal node—(4, 5) from\nnode 7—that is Pareto superior to the NE. Again, however, the\ndynamics of the game prevent it from being reached. \nThe fact that Zermelo’s algorithm picks out the strategy vector\n(LR, RL) as the unique solution to the game shows that it’s\nyielding something other than just an NE. In fact, it is generating\nthe game’s subgame perfect equilibrium (SPE). It gives\nan outcome that yields a NE not just in the whole game but in\nevery subgame as well. This is a persuasive solution concept because,\nagain unlike the refinements of Section 2.5, it does not demand\n‘extra’ rationality of agents in the sense of expecting\nthem to have and use philosophical intuitions about ‘what makes\nsense’. It does, however, assume that players not only know\neverything strategically relevant to their situation but also\nuse all of that information. In arguments about the\nfoundations of economics, this is often referred to as an aspect of\nrationality, as in the phrase ‘rational expectations’.\nBut, as noted earlier, it is best to be careful not to confuse the\ngeneral normative idea of rationality with computational power and the\npossession of budgets, in time and energy, to make the most of it. \nAn agent playing a subgame perfect strategy simply chooses, at every\nnode she reaches, the path that brings her the highest payoff in\nthe subgame emanating from that node. SPE predicts a game’s\noutcome just in case, in solving the game, the players foresee that\nthey will all do that. \nA main value of analyzing extensive-form games for SPE is that this\ncan help us to locate structural barriers to social optimization. In\nour current example, Player I would be better off, and Player II no\nworse off, at the left-hand node emanating from node 7 than at the SPE\noutcome. But Player I’s economic rationality, and Player\nII’s awareness of this, blocks the socially efficient outcome.\nIf our players wish to bring about the more socially efficient outcome\n(4,5) here, they must do so by redesigning their institutions so as to\nchange the structure of the game. The enterprise of changing\ninstitutional and informational structures so as to make efficient\noutcomes more likely in the games that agents (that is, people,\ncorporations, governments, etc.) actually play is known as\nmechanism design, and is one of the leading areas of\napplication of game theory. The main techniques are reviewed in\n Hurwicz and Reiter (2006),\n the first author of which was awarded the Nobel Prize for his\npioneering work in the area. \nMany readers, but especially philosophers, might wonder why, in the\ncase of the example taken up in the previous section, mechanism design\nshould be necessary unless players are morbidly selfish sociopaths.\nSurely, the players might be able to just see that outcome\n(4,5) is socially and morally superior; and since the whole problem\nalso takes for granted that they can also see the path of actions that\nleads to this efficient outcome, who is the game theorist to announce\nthat, unless their game is changed, it’s unattainable? This\nobjection, which applies the distinctive idea of rationality urged by\nImmanuel Kant, indicates the leading way in which many philosophers\nmean more by ‘rationality’ than descriptive game theorists\ndo. This theme is explored with great liveliness and polemical force\nin Binmore\n (1994,\n 1998). \nThis weighty philosophical controversy about rationality is sometimes\nconfused by misinterpretation of the meaning of ‘utility’\nin non-psychological game theory. To root out this mistake, consider\nthe Prisoner’s Dilemma again. We have seen that in the unique NE\nof the PD, both players get less utility than they could have through\nmutual cooperation. This may strike you, even if you are not a Kantian\n(as it has struck many commentators) as perverse. Surely, you may\nthink, it simply results from a combination of selfishness and\nparanoia on the part of the players. To begin with they have no regard\nfor the social good, and then they shoot themselves in the feet by\nbeing too untrustworthy to respect agreements. \nThis way of thinking is very common in popular discussions, and badly\nmixed up. To dispel its influence, let us first introduce some\nterminology for talking about outcomes. Welfare economists typically\nmeasure social good in terms of Pareto efficiency. A\ndistribution of utility β is said to be Pareto superior\nover another distribution δ just in case from state δ\nthere is a possible redistribution of utility to β such that at\nleast one player is better off in β than in δ and no player\nis worse off. Failure to move from a Pareto-inferior to a\nPareto-superior distribution is inefficient because the\nexistence of β as a possibility, at least in principle, shows\nthat in δ some utility is being wasted. Now, the outcome (3,3)\nthat represents mutual cooperation in our model of the PD is clearly\nPareto superior to mutual defection; at (3,3) both players\nare better off than at (2,2). So it is true that PDs lead to\ninefficient outcomes. This was true of our example in Section 2.6 as\nwell. \nHowever, inefficiency should not be associated with immorality. A\nutility function for a player is supposed to represent everything\nthat player cares about, which may be anything at all. As we have\ndescribed the situation of our prisoners they do indeed care only\nabout their own relative prison sentences, but there is nothing\nessential in this. What makes a game an instance of the PD is strictly\nand only its payoff structure. Thus we could have two Mother Theresa\ntypes here, both of whom care little for themselves and wish only to\nfeed starving children. But suppose the original Mother Theresa wishes\nto feed the children of Calcutta while Mother Juanita wishes to feed\nthe children of Bogota. And suppose that the international aid agency\nwill maximize its donation if the two saints nominate the same city,\nwill give the second-highest amount if they nominate each\nothers’ cities, and the lowest amount if they each nominate\ntheir own city. Our saints are in a PD here, though hardly selfish or\nunconcerned with the social good. \nTo return to our prisoners, suppose that, contrary to our assumptions,\nthey do value each other’s well-being as well as their\nown. In that case, this must be reflected in their utility functions,\nand hence in their payoffs. If their payoff structures are changed so\nthat, for example, they would feel so badly about contributing to\ninefficiency that they’d rather spend extra years in prison than\nendure the shame, then they will no longer be in a PD. But all this\nshows is that not every possible situation is a PD; it does\nnot show that selfishness is among the assumptions of game\ntheory. It is the logic of the prisoners’ situation,\nnot their psychology, that traps them in the inefficient outcome, and\nif that really is their situation then they are stuck in it\n(barring further complications to be discussed below). Agents who wish\nto avoid inefficient outcomes are best advised to prevent certain\ngames from arising; the defender of the possibility of Kantian\nrationality is really proposing that they try to dig themselves out of\nsuch games by turning themselves into different kinds of agents. \nIn general, then, a game is partly defined by the payoffs\nassigned to the players. In any application, such assignments should\nbe based on sound empirical evidence. If a proposed solution involves\ntacitly changing these payoffs, then this ‘solution’ is in\nfact a disguised way of changing the subject and evading the\nimplications of best modeling practice. \nOur last point above opens the way to a philosophical puzzle, one of\nseveral that still preoccupy those concerned with the logical\nfoundations of game theory. It can be raised with respect to any\nnumber of examples, but we will borrow an elegant one from C.\nBicchieri\n (1993).\n Consider the following game:  Figure 11 \nThe NE outcome here is at the single leftmost node descending from\nnode 8. To see this, backward induct again. At node 10, I would play L\nfor a payoff of 3, giving II a payoff of 1. II can do better than this\nby playing L at node 9, giving I a payoff of 0. I can do better than\nthis by playing L at node 8; so that is what I does, and the game\nterminates without II getting to move. A puzzle is then raised by\nBicchieri (along with other authors, including\n Binmore (1987)\n and\n Pettit and Sugden (1989))\n by way of the following reasoning. Player I plays L at node 8 because\nshe knows that Player II is economically rational, and so would, at\nnode 9, play L because Player II knows that Player I is economically\nrational and so would, at node 10, play L. But now we have the\nfollowing paradox: Player I must suppose that Player II, at node 9,\nwould predict Player I’s economically rational play at node 10\ndespite having arrived at a node (9) that could only be reached if\nPlayer I is not economically rational! If Player I is not economically\nrational then Player II is not justified in predicting that Player I\nwill not play R at node 10, in which case it is not clear that Player\nII shouldn’t play R at 9; and if Player II plays R at 9, then\nPlayer I is guaranteed of a better payoff then she gets if she plays L\nat node 8. Both players use backward induction to solve the game;\nbackward induction requires that Player I know that Player II knows\nthat Player I is economically rational; but Player II can solve the\ngame only by using a backward induction argument that takes as a\npremise the failure of Player I to behave in accordance with economic\nrationality. This is the paradox of backward induction. \nA standard way around this paradox in the literature is to invoke the\nso-called ‘trembling hand’ due to\n Selten (1975).\n The idea here is that a decision and its consequent act may\n‘come apart’ with some nonzero probability, however small.\nThat is, a player might intend to take an action but then slip up in\nthe execution and send the game down some other path instead. If there\nis even a remote possibility that a player may make a\nmistake—that her ‘hand may tremble’—then no\ncontradiction is introduced by a player’s using a backward\ninduction argument that requires the hypothetical assumption that\nanother player has taken a path that an economically rational player\ncould not choose. In our example, Player II could reason about what to\ndo at node 9 conditional on the assumption that Player I chose L at\nnode 8 but then slipped. \n\n Gintis (2009a)\n points out that the apparent paradox does not arise merely from our\nsupposing that both players are economically rational. It rests\ncrucially on the additional premise that each player must know, and\nreasons on the basis of knowing, that the other player is economically\nrational. This is the premise with which each player’s\nconjectures about what would happen off the equilibrium path of play\nare inconsistent. A player has reason to consider out-of-equilibrium\npossibilities if she either believes that her opponent is economically\nrational but his hand may tremble or she attaches some\nnonzero probability to the possibility that he is not economically\nrational or she attaches some doubt to her conjecture about\nhis utility function. As Gintis also stresses, this issue with solving\nextensive-form games games for SEP by Zermelo’s algorithm\ngeneralizes: a player has no reason to play even a Nash\nequilibrium strategy unless she expects other players to also play\nNash equilibrium strategies. We will return to this issue in\n Section 7\n below. \nThe paradox of backward induction, like the puzzles raised by\nequilibrium refinement, is mainly a problem for those who view game\ntheory as contributing to a normative theory of rationality\n(specifically, as contributing to that larger theory the theory of\nstrategic rationality). The non-psychological game theorist\ncan give a different sort of account of apparently\n“irrational” play and the prudence it encourages. This\ninvolves appeal to the empirical fact that actual agents, including\npeople, must learn the equilibrium strategies of games they\nplay, at least whenever the games are at all complicated. Research\nshows that even a game as simple as the Prisoner’s Dilemma\nrequires learning by people\n (Ledyard 1995,\n Sally 1995,\n Camerer 2003,\n p. 265). What it means to say that people must learn equilibrium\nstrategies is that we must be a bit more sophisticated than was\nindicated earlier in constructing utility functions from behavior in\napplication of Revealed Preference Theory. Instead of constructing\nutility functions on the basis of single episodes, we must do so on\nthe basis of observed runs of behavior once it has\nstabilized, signifying maturity of learning for the subjects in\nquestion and the game in question. Once again, the Prisoner’s\nDilemma makes a good example. People encounter few one-shot\nPrisoner’s Dilemmas in everyday life, but they encounter many\nrepeated PD’s with non-strangers. As a result, when set\ninto what is intended to be a one-shot PD in the experimental\nlaboratory, people tend to initially play as if the game were a single\nround of a repeated PD. The repeated PD has many Nash equilibria that\ninvolve cooperation rather than defection. Thus experimental subjects\ntend to cooperate at first in these circumstances, but learn after\nsome number of rounds to defect. The experimenter cannot infer that\nshe has successfully induced a one-shot PD with her experimental setup\nuntil she sees this behavior stabilize. \nIf players of games realize that other players may need to learn game\nstructures and equilibria from experience, this gives them reason to\ntake account of what happens off the equilibrium paths of\nextensive-form games. Of course, if a player fears that other players\nhave not learned equilibrium, this may well remove her incentive to\nplay an equilibrium strategy herself. This raises a set of deep\nproblems about social learning\n (Fudenberg and Levine 1998.\n How can ignorant players learn to play equilibria if sophisticated\nplayers don’t show them, because the sophisticated are not\nincentivized to play equilibrium strategies until the ignorant have\nlearned? The crucial answer in the case of applications of game theory\nto interactions among people is that young people are\nsocialized by growing up in networks of\ninstitutions, including cultural norms. Most complex\ngames that people play are already in progress among people who were\nsocialized before them—that is, have learned game structures and\nequilibria\n (Ross 2008a).\n Novices must then only copy those whose play appears to be expected\nand understood by others. Institutions and norms are rich with\nreminders, including homilies and easily remembered rules of thumb, to\nhelp people remember what they are doing\n (Clark 1997). \nAs noted in\n Section 2.7\n above, when observed behavior does not stabilize around\nequilibria in a game, and there is no evidence that learning is still\nin process, the analyst should infer that she has incorrectly modeled\nthe situation she is studying. Chances are that she has either\nmis-specified players’ utility functions, the strategies\navailable to the players, or the information that is available to\nthem. Given the complexity of many of the situations that social\nscientists study, we should not be surprised that mis-specification of\nmodels happens frequently. Applied game theorists must do lots of\nlearning, just like their subjects.  \nThe paradox of backward induction is one of a family of paradoxes that\narise if one builds possession and use of literally complete\ninformation into a concept of rationality. (Consider, by analogy, the\nstock market paradox that arises if we suppose that economically\nrational investment incorporates literally rational expectations:\nassume that no individual investor can beat the market in the long run\nbecause the market always knows everything the investor knows; then no\none has incentive to gather knowledge about asset values; then no one\nwill ever gather any such information and so from the assumption that\nthe market knows everything it follows that the market cannot know\nanything!)As we will see in detail in various discussions below, most\napplications of game theory explicitly incorporate uncertainty and\nprospects for learning by players. The extensive-form games with SPE\nthat we looked at above are really conceptual tools to help us prepare\nconcepts for application to situations where complete and perfect\ninformation is unusual. We cannot avoid the paradox if we think, as\nsome philosophers and normative game theorists do, that one of the\nconceptual tools we want to use game theory to sharpen is a fully\ngeneral idea of rationality itself. But this is not a concern\nentertained by economists and other scientists who put game theory to\nuse in empirical modeling. In real cases, unless players have\nexperienced play at equilibrium with one another in the past, even if\nthey are all economically rational and all believe this about one\nanother, we should predict that they will attach some positive\nprobability to the conjecture that understanding of game structures\namong some players is imperfect. This then explains why people, even\nif they are economically rational agents, may often, or even usually,\nplay as if they believe in trembling hands. \nLearning of equilibria may take various forms for different agents and\nfor games of differing levels of complexity and risk. Incorporating it\ninto game-theoretic models of interactions thus introduces an\nextensive new set of technicalities. For the most fully developed\ngeneral theory, the reader is referred to\n Fudenberg and Levine (1998);\n the same authors provide a non-technical overview of the issues in\n Fudenberg and Levine (2016)).\n A first important distinction is between learning specific parameters\nbetween rounds of a repeated game (see\n Section 4)\n with common players, and learning about general strategic\nexpectations across different games. The latter can include learning\nabout players if the learner is updating expectations based on her\nmodels of types of players she recurrently encounters. Then\nwe can distinguish between passive learning, in which a\nplayer merely updates her subjective priors based on her\nobservation of moves and outcomes, and strategic choices she infers\nfrom these, and active learning, in which she probes—in\ntechnical language screens—for information about other\nplayers’ strategies by choosing strategies that test her\nconjectures about what will occur off what she believes to be the\ngame’s equilibrium path. A major difficulty for both players and\nmodelers is that screening moves might be misinterpreted if players\nare also incentivized to make moves to signal information to\none another (see\n Section 4).\n In other words: trying to learn about strategies can under some\ncircumstances interfere with players’ abilities to learn\nequilibria. Finally, the discussion so far has assumed that all\npossible learning in a game is about the structure of the game itself.\n Wilcox (2008)\n shows that if players are learning new information about causal\nprocesses occurring outside a game while simultaneously trying to\nupdate expectations about other players’ strategies, the modeler\ncan find herself reaching beyond the current limits of technical\nknowledge. \nIt was said above that people might usually play as if they\nbelieve in trembling hands. A very general reason for this is that\nwhen people interact, the world does not furnish them with cue-cards\nadvising them about the structures of the games they’re playing.\nThey must make and test conjectures about this from their social\ncontexts. Sometimes, contexts are fixed by institutional rules. For\nexample, when a person walks into a retail shop and sees a price tag\non something she’d like to have, she knows without needing to\nconjecture or learn anything that she’s involved in a simple\n‘take it or leave it’ game. In other markets, she might\nknow she is expect to haggle, and know the rules for that too.  \nGiven the unresolved complex relationship between learning theory and\ngame theory, the reasoning above might seem to imply that game theory\ncan never be applied to situations involving human players that are\nnovel for them. Fortunately, however, we face no such impasse. In a\npair of influential papers in the mid-to-late 1990s, McKelvey and\nPalfrey\n (1995,\n 1998) developed the solution concept of quantal response\nequilibrium (QRE). QRE is not a refinement of NE, in the sense of\nbeing a philosophically motivated effort to strengthen NE by reference\nto normative standards of rationality. It is, rather, a method for\ncalculating the equilibrium properties of choices made by players\nwhose conjectures about possible errors in the choices of other\nplayers are uncertain. QRE is thus standard equipment in the toolkit\nof experimental economists who seek to estimate the distribution of\nutility functions in populations of real people placed in situations\nmodeled as games. QRE would not have been practically serviceable in\nthis way before the development of econometrics packages such as Stata\n(TM) allowed computation of QRE given adequately powerful observation\nrecords from interestingly complex games. QRE is rarely utilized by\nbehavioral economists, and is almost never used by psychologists, in\nanalyzing laboratory data. In consequence, many studies by researchers\nof these types make dramatic rhetorical points by\n‘discovering’ that real people often fail to converge on\nNE in experimental games. But NE, though it is a minimalist solution\nconcept in one sense because it abstracts away from much informational\nstructure, is simultaneously a demanding empirical expectation if it\nis imposed categorically (that is, if players are expected to play as\nif they are all certain that all others are playing NE strategies).\nPredicting play consistent with QRE is consistent with—indeed,\nis motivated by—the view that NE captures the core general\nconcept of a strategic equilibrium. One way of framing the\nphilosophical relationship between NE and QRE is as follows. NE\ndefines a logical principle that is well adapted for\ndisciplining thought and for conceiving new strategies for generic\nmodeling of new classes of social phenomena. For purposes of\nestimating real empirical data one needs to be able to define\nequilibrium statistically. QRE represents one way of doing\nthis, consistently with the logic of NE. The idea is sufficiently rich\nthat its depths remain an open domain of investigation by game\ntheorists. The current state of understanding of QRE is\ncomprehensively reviewed in\n (Goeree, Holt and Palfrey (2016).\n  \nThe games we’ve modeled to this point have all involved players\nchoosing from amongst pure strategies, in which each seeks a\nsingle optimal course of action at each node that constitutes a best\nreply to the actions of others. Often, however, a player’s\nutility is optimized through use of a mixed strategy, in\nwhich she flips a weighted coin amongst several possible actions. (We\nwill see later that there is an alternative interpretation of mixing,\nnot involving randomization at a particular information set; but we\nwill start here from the coin-flipping interpretation and then build\non it in\n Section 3.1.)\n Mixing is called for whenever no pure strategy maximizes the\nplayer’s utility against all opponent strategies. Our\nriver-crossing game from\n Section 1\n exemplifies this. As we saw, the puzzle in that game consists in the\nfact that if the fugitive’s reasoning selects a particular\nbridge as optimal, his pursuer must be assumed to be able to duplicate\nthat reasoning. The fugitive can escape only if his pursuer cannot\nreliably predict which bridge he’ll use. Symmetry of logical\nreasoning power on the part of the two players ensures that the\nfugitive can surprise the pursuer only if it is possible for him to\nsurprise himself. \nSuppose that we ignore rocks and cobras for a moment, and imagine that\nthe bridges are equally safe. Suppose also that the fugitive has no\nspecial knowledge about his pursuer that might lead him to venture a\nspecially conjectured probability distribution over the\npursuer’s available strategies. In this case, the\nfugitive’s best course is to roll a three-sided die, in which\neach side represents a different bridge (or, more conventionally, a\nsix-sided die in which each bridge is represented by two sides). He\nmust then pre-commit himself to using whichever bridge is selected by\nthis randomizing device. This fixes the odds of his survival\nregardless of what the pursuer does; but since the pursuer has no\nreason to prefer any available pure or mixed strategy, and since in\nany case we are presuming her epistemic situation to be symmetrical to\nthat of the fugitive, we may suppose that she will roll a three-sided\ndie of her own. The fugitive now has a 2/3 probability of escaping and\nthe pursuer a 1/3 probability of catching him. Neither the fugitive\nnor the pursuer can improve their chances given the other’s\nrandomizing mix, so the two randomizing strategies are in Nash\nequilibrium. Note that if one player is randomizing then the\nother does equally well on any mix of probabilities over\nbridges, so there are infinitely many combinations of best replies.\nHowever, each player should worry that anything other than a random\nstrategy might be coordinated with some factor the other player can\ndetect and exploit. Since any non-random strategy is exploitable by\nanother non-random strategy, in a zero-sum game such as our example,\nonly the vector of randomized strategies is a NE. \nNow let us re-introduce the parametric factors, that is, the falling\nrocks at bridge #2 and the cobras at bridge #3. Again, suppose that\nthe fugitive is sure to get safely across bridge #1, has a 90% chance\nof crossing bridge #2, and an 80% chance of crossing bridge #3. We can\nsolve this new game if we make certain assumptions about the two\nplayers’ utility functions. Suppose that Player 1, the fugitive,\ncares only about living or dying (preferring life to death) while the\npursuer simply wishes to be able to report that the fugitive is dead,\npreferring this to having to report that he got away. (In other words,\nneither player cares about how the fugitive lives or dies.)\nSuppose also for now that neither player gets any utility or\ndisutility from taking more or less risk. In this case, the fugitive\nsimply takes his original randomizing formula and weights it according\nto the different levels of parametric danger at the three bridges.\nEach bridge should be thought of as a lottery over the\nfugitive’s possible outcomes, in which each lottery has a\ndifferent expected payoff in terms of the items in his\nutility function.  \nConsider matters from the pursuer’s point of view. She will be\nusing her NE strategy when she chooses the mix of probabilities over\nthe three bridges that makes the fugitive indifferent among his\npossible pure strategies. The bridge with rocks is 1.1 times more\ndangerous for him than the safe bridge. Therefore, he will be\nindifferent between the two when the pursuer is 1.1 times more likely\nto be waiting at the safe bridge than the rocky bridge. The cobra\nbridge is 1.2 times more dangerous for the fugitive than the safe\nbridge. Therefore, he will be indifferent between these two bridges\nwhen the pursuer’s probability of waiting at the safe bridge is\n1.2 times higher than the probability that she is at the cobra bridge.\nSuppose we use s1, s2 and s3 to represent the fugitive’s\nparametric survival rates at each bridge. Then the pursuer minimizes\nthe net survival rate across any pair of bridges by adjusting the\nprobabilities p1 and p2 that she will wait at them so that \nSince p1 + p2 = 1, we can rewrite this as \nso \nThus the pursuer finds her NE strategy by solving the following\nsimultaneous equations: \np1 + p2 + p3 = 1. \nThen \nNow let f1, f2, f3 represent the probabilities with which the fugitive\nchooses each respective bridge. Then the fugitive finds his NE\nstrategy by solving \nso \nsimultaneously with \nThen \nThese two sets of NE probabilities tell each player how to weight his\nor her die before throwing it. Note the—perhaps\nsurprising—result that the fugitive, though by hypothesis he\ngets no enjoyment from gambling, uses riskier bridges with  higher\n probability. This is the only way of making the pursuer\nindifferent over which bridge she stakes out, which in turn is what\nmaximizes the fugitive’s probability of survival. \nWe were able to solve this game straightforwardly because we set the\nutility functions in such a way as to make it zero-sum, or\nstrictly competitive. That is, every gain in expected utility\nby one player represents a precisely symmetrical loss by the other.\nHowever, this condition may often not hold. Suppose now that the\nutility functions are more complicated. The pursuer most prefers an\noutcome in which she shoots the fugitive and so claims credit for his\napprehension to one in which he dies of rockfall or snakebite; and she\nprefers this second outcome to his escape. The fugitive prefers a\nquick death by gunshot to the pain of being crushed or the terror of\nan encounter with a cobra. Most of all, of course, he prefers to\nescape. Suppose, plausibly, that the fugitive cares more\nstrongly about surviving than he does about getting killed\none way rather than another. We cannot solve this game, as before,\nsimply on the basis of knowing the players’ ordinal utility\nfunctions, since the intensities of their respective\npreferences will now be relevant to their strategies. \nPrior to the work of\n von Neumann & Morgenstern (1947),\n situations of this sort were inherently baffling to analysts. This is\nbecause utility does not denote a hidden psychological variable such\nas pleasure. As we discussed in\n Section 2.1,\n utility is merely a measure of relative behavioural dispositions\ngiven certain consistency assumptions about relations between\npreferences and choices. It therefore makes no sense to imagine\ncomparing our players’ cardinal—that is,\nintensity-sensitive—preferences with one another’s, since\nthere is no independent, interpersonally constant yardstick we could\nuse. How, then, can we model games in which cardinal information is\nrelevant? After all, modeling games requires that all players’\nutilities be taken simultaneously into account, as we’ve\nseen. \nA crucial aspect of\n von Neumann & Morgenstern’s (1947)\n work was the solution to this problem. Here, we will provide a brief\noutline of their ingenious technique for building cardinal utility\nfunctions out of ordinal ones. It is emphasized that what follows is\nmerely an outline, so as to make cardinal utility\nnon-mysterious to you as a student who is interested in knowing about\nthe philosophical foundations of game theory, and about the range of\nproblems to which it can be applied. Providing a manual you could\nfollow in building your own cardinal utility functions would\nrequire many pages. Such manuals are available in many textbooks. \nSuppose that we now assign the following ordinal utility function to\nthe river-crossing fugitive: \nDeath by shooting ≫ 3 \nDeath by rockfall ≫ 2 \nDeath by snakebite ≫ 1 \nWe are supposing that his preference for escape over any form\nof death is stronger than his preferences between causes of death.\nThis should be reflected in his choice behaviour in the following way.\nIn a situation such as the river-crossing game, he should be willing\nto run greater risks to increase the relative probability of escape\nover shooting than he is to increase the relative probability of\nshooting over snakebite. This bit of logic is the crucial insight\nbehind\n von Neumann & Morgenstern’s (1947)\n solution to the cardinalization problem.  \nSuppose we asked the fugitive to pick, from the available set of\noutcomes, a best one and a worst one.\n‘Best’ and ‘worst’ are defined in terms of\nexpected payoffs as illustrated in our current zero-sum game example:\na player maximizes his expected payoff if, when choosing among\nlotteries that contain only two possible prizes, he always chooses so\nas to maximize the probability of the best outcome—call this\nW—and to minimize the probability of the worst\noutcome—call this L. Now imagine expanding the\nset of possible prizes so that it includes prizes that the agent\nvalues as intermediate between W and\nL. We find, for a set of outcomes containing such\nprizes, a lottery over them such that our agent is indifferent between\nthat lottery and a lottery including only W and\nL. In our example, this is a lottery that includes\nbeing shot and being crushed by rocks. Call this lottery\nT . We define a utility function q =\nu(T) from outcomes to the real (as opposed\nto ordinal) number line such that if q is the expected prize\nin T , the agent is indifferent between winning\nT and winning a lottery T* in which\nW occurs with probability\nu(T) and L occurs with\nprobability 1 − u(T).\nAssuming that the agent’s behaviour respects the principle of\nreduction of compound lotteries (ROCL)—that is, he does\nnot gain or lose utility from considering more complex lotteries\nrather than simple ones—the set of mappings of outcomes in\nT to uT* gives a von\nNeumann—Morgenstern utility function (vNMuf) with cardinal\nstructure over all outcomes in T.  \nWhat exactly have we done here? We’ve given our agent choices\nover lotteries, instead of directly over resolved outcomes, and\nobserved how much extra risk of death he’s willing to run to\nchange the odds of getting one form of death relative to an\nalternative form of death. Note that this cardinalizes the\nagent’s preference structure only relative to agent-specific\nreference points W and L; the\nprocedure reveals nothing about comparative extra-ordinal preferences\nbetween agents, which helps to make clear that constructing a\nvNMuf does not introduce a potentially objective psychological\nelement. Furthermore, two agents in one game, or one agent under\ndifferent sorts of circumstances, may display varying attitudes to\nrisk. Perhaps in the river-crossing game the pursuer, whose life is\nnot at stake, will enjoy gambling with her glory while our fugitive is\ncautious. In analyzing the river-crossing game, however, we\ndon’t have to be able to compare the pursuer’s\ncardinal utilities with the fugitive’s. Both agents, after all,\ncan find their NE strategies if they can estimate the probabilities\neach will assign to the actions of the other. This means that each\nmust know both vNMufs; but neither need try to comparatively value the\noutcomes over which they’re choosing. \nWe can now fill in the rest of the matrix for the bridge-crossing game\nthat we started to draw in Section 2. If both players are risk-neutral\nand their revealed preferences respect ROCL, then we have enough\ninformation to be able to assign expected utilities, expressed by\nmultiplying the original payoffs by the relevant probabilities, as\noutcomes in the matrix. Suppose that the hunter waits at the cobra\nbridge with probability x and at the rocky bridge with\nprobability y. Since her probabilities across the three\nbridges must sum to 1, this implies that she must wait at the safe\nbridge with probability 1 − (x + y). Then,\ncontinuing to assign the fugitive a payoff of 0 if he dies and 1 if he\nescapes, and the hunter the reverse payoffs, our complete matrix is as\nfollows: Figure 12 \nWe can now read the following facts about the game directly from the\nmatrix. No pair of pure strategies is a pair of best replies to the\nother. Therefore, the game’s only NE require at least one player\nto use a mixed strategy. \nIn all of our examples and workings to this point, we have presupposed\nthat players’ beliefs about probabilities in lotteries match\nobjective probabilities. But in real interactive choice situations,\nagents must often rely on their subjective estimations or perceptions\nof probabilities. In one of the greatest contributions to\ntwentieth-century behavioral and social science,\n Savage (1954)\n showed how to incorporate subjective probabilities, and their\nrelationships to preferences over risk, within the framework of von\nNeumann-Morgenstern expected utility theory. Indeed, Savage’s\nachievement amounts to the formal completion of EUT. Then, just over a\ndecade later,\n Harsanyi (1967)\n showed how to solve games involving maximizers of Savage expected\nutility. This is often taken to have marked the true maturity of game\ntheory as a tool for application to behavioral and social science, and\nwas recognized as such when Harsanyi joined Nash and Selten as a\nrecipient of the first Nobel prize awarded to game theorists in\n1994. \nAs we observed in considering the need for people playing games to\nlearn trembling hand equilibria and QRE, when we model the strategic\ninteractions of people we must allow for the fact that people are\ntypically uncertain about their models of one another. This\nuncertainty is reflected in their choices of strategies. Furthermore,\nsome actions might be taken specifically for the sake of learning\nabout the accuracy of a player’s conjectures about other\nplayers. Harsanyi’s extension of game theory incorporates these\ncrucial elements. \nConsider the three-player imperfect-information game below known as\n‘Selten’s horse’ (for its inventor, Nobel Laureate\nReinhard Selten, and because of the shape of its tree; taken from\n Kreps (1990),\n p. 426): Figure 13 \nThis game has four NE: (L, l2, l3), (L,\nr2, l3), (R, r2, l3) and\n(R, r2, r3). Consider the fourth of these NE. It\narises because when Player I plays R and Player II plays\nr2, Player III’s entire information set is off the\npath of play, and it doesn’t matter to the outcome what Player\nIII does. But Player I would not play R if Player III could tell the\ndifference between being at node 13 and being at node 14. The\nstructure of the game incentivizes efforts by Player I to supply\nPlayer III with information that would open up her closed information\nset. Player III should believe this information because the structure\nof the game shows that Player I has incentive to communicate it\ntruthfully. The game’s solution would then be the SPE of the\n(now) perfect information game: (L, r2, l3).\n \nTheorists who think of game theory as part of a normative theory of\ngeneral rationality, for example most philosophers, and refinement\nprogram enthusiasts among economists, have pursued a strategy that\nwould identify this solution on general principles. Notice what Player\nIII in Selten’s Horse might wonder about as he selects his\nstrategy. “Given that I get a move, was my action node reached\nfrom node 11 or from node 12?” What, in other words, are the\nconditional probabilities that Player III is at node 13 or 14\ngiven that he has a move? Now, if conditional probabilities are what\nPlayer III wonders about, then what Players I and II might make\nconjectures about when they select their strategies are\nPlayer III’s beliefs about these conditional\nprobabilities. In that case, Player I must conjecture about Player\nII’s beliefs about Player III’s beliefs, and Player\nIII’s beliefs about Player II’s beliefs and so on. The\nrelevant beliefs here are not merely strategic, as before, since they\nare not just about what players will do given a set of\npayoffs and game structures, but about what understanding of\nconditional probability they should expect other players to operate\nwith. \nWhat beliefs about conditional probability is it reasonable for\nplayers to expect from each other? If we follow\n Savage (1954)\n we would suggest as a normative principle that they should reason and\nexpect others to reason in accordance with Bayes’s\nrule. This tells them how to compute the probability of an event\nF given information E (written ‘pr(F/E)’): \nIf we assume that players’ beliefs are always consistent with\nthis equality, then we may define a sequential equilibrium. A\nSE has two parts: (1) a strategy profile § for each player, as\nbefore, and (2) a system of beliefs μ for each player.\nμ assigns to each information set h a probability\ndistribution over the nodes in h, with the interpretation\nthat these are the beliefs of player i(h) about\nwhere in his information set he is, given that information set\nh has been reached. Then a sequential equilibrium is a\nprofile of strategies § and a system of beliefs μ consistent\nwith Bayes’s rule such that starting from every information set\nh in the tree player i(h) plays optimally\nfrom then on, given that what he believes to have transpired\npreviously is given by μ(h) and what will transpire at\nsubsequent moves is given by §. \nLet us apply this solution concept to Selten’s Horse. Consider\nagain the NE (R, r2, r3). Suppose that Player\nIII assigns pr(1) to her belief that if she gets a move she is at node\n13. Then Player I, given a consistent μ(I), must believe that\nPlayer III will play l3, in which case her only SE strategy\nis L. So although (R, r2, l3) is a NE, it is not\na SE.  \nThe use of the consistency requirement in this example is somewhat\ntrivial, so consider now a second case (also taken from\n Kreps (1990),\n p. 429): Figure 14 \nSuppose that Player I plays L, Player II plays l2 and\nPlayer III plays l3. Suppose also that μ(II) assigns\npr(.3) to node 16. In that case, l2 is not a SE strategy\nfor Player II, since l2 returns an expected payoff of .3(4)\n+ .7(2) = 2.6, while r2 brings an expected payoff of 3.1.\nNotice that if we fiddle the strategy profile for player III while\nleaving everything else fixed, l2 could become a\nSE strategy for Player II. If §(III) yielded a play of\nl3 with pr(.5) and r3 with pr(.5), then if\nPlayer II plays r2 his expected payoff would now be 2.2, so\n(Ll2l3) would be a SE. Now imagine setting\nμ(III) back as it was, but change μ(II) so that Player II thinks\nthe conditional probability of being at node 16 is greater than .5; in\nthat case, l2 is again not a SE strategy. \nThe idea of SE is hopefully now clear. We can apply it to the\nriver-crossing game in a way that avoids the necessity for the pursuer\nto flip any coins of we modify the game a bit. Suppose now that the\npursuer can change bridges twice during the fugitive’s passage,\nand will catch him just in case she meets him as he leaves the bridge.\nThen the pursuer’s SE strategy is to divide her time at the\nthree bridges in accordance with the proportion given by the equation\nin the third paragraph of Section 3 above. \nIt must be noted that since Bayes’s rule cannot be applied to\nevents with probability 0, its application to SE requires that players\nassign non-zero probabilities to all actions available in extensive\nform. This requirement is captured by supposing that all strategy\nprofiles be strictly mixed, that is, that every action at\nevery information set be taken with positive probability. You will see\nthat this is just equivalent to supposing that all hands sometimes\ntremble, or alternatively that no expectations are quite certain. A SE\nis said to be trembling-hand perfect if all strategies played\nat equilibrium are best replies to strategies that are strictly mixed.\nYou should also not be surprised to be told that no weakly dominated\nstrategy can be trembling-hand perfect, since the possibility of\ntrembling hands gives players the most persuasive reason for avoiding\nsuch strategies. \nHow can the non-psychological game theorist understand the concept of\nan NE that is an equilibrium in both actions and beliefs? Decades of\nexperimental study have shown that when human subjects play games,\nespecially games that ideally call for use of Bayes’s rule in\nmaking conjectures about other players’ beliefs, we should\nexpect significant heterogeneity in strategic responses.\nMultiple kinds of informational channels typically link different\nagents with the incentive structures in their environments. Some\nagents may actually compute equilibria, with more or less error.\nOthers may settle within error ranges that stochastically drift around\nequilibrium values through more or less myopic conditioned learning.\nStill others may select response patterns by copying the behavior of\nother agents, or by following rules of thumb that are embedded in\ncultural and institutional structures and represent historical\ncollective learning. Note that the issue here is specific to game\ntheory, rather than merely being a reiteration of a more general\npoint, which would apply to any behavioral science, that people behave\nnoisily from the perspective of ideal theory. In a given game, whether\nit would be rational for even a trained, self-aware, computationally\nwell resourced agent to play NE would depend on the frequency with\nwhich he or she expected others to do likewise. If she expects some\nother players to stray from NE play, this may give her a reason to\nstray herself. Instead of predicting that human players will reveal\nstrict NE strategies, the experienced experimenter or modeler\nanticipates that there will be a relationship between their play and\nthe expected costs of departures from NE. Consequently, maximum\nlikelihood estimation of observed actions typically identifies a QRE\nas providing a better fit than any NE.  \nAn analyst handling empirical data in this way should not be\ninterpreted as ‘testing the hypothesis’ that the agents\nunder analysis are ‘rational’. Rather, she conjectures\nthat they are agents, that is, that there is a systematic relationship\nbetween changes in statistical patterns in their behavior and some\nrisk-weighted cardinal rankings of possible goal-states. If the agents\nare people or institutionally structured groups of people that monitor\none another and are incentivized to attempt to act collectively, these\nconjectures will often be regarded as reasonable by critics, or even\nas pragmatically beyond question, even if always defeasible given the\nnon-zero possibility of bizarre unknown circumstances of the kind\nphilosophers sometimes consider (e.g., the apparent people are\npre-programmed unintelligent mechanical simulacra that would be\nrevealed as such if only the environment incentivized responses not\nwritten into their programs). The analyst might assume that all of the\nagents respond to incentive changes in accordance with Savage\nexpected-utility theory, particularly if the agents are firms that\nhave learned response contingencies under normatively demanding\nconditions of market competition with many players. If the\nanalyst’s subjects are individual people, and especially if they\nare in a non-standard environment relative to their cultural and\ninstitutional experience, she would more wisely estimate a maximum\nlikelihood mixture model that allows that a range of different utility\nstructures govern different subsets of her choice data. All this is to\nsay that use of game theory does not force a scientist to empirically\napply a model that is likely to be too precise and narrow in its\nspecifications to plausibly fit the messy complexities of real\nstrategic interaction. A good applied game theorist should also be a\nwell-schooled econometrician. \nSo far we’ve restricted our attention to one-shot\ngames, that is, games in which players’ strategic concerns\nextend no further than the terminal nodes of their single interaction.\nHowever, games are often played with future games in mind,\nand this can significantly alter their outcomes and equilibrium\nstrategies. Our topic in this section is repeated games, that\nis, games in which sets of players expect to face each other in\nsimilar situations on multiple occasions. We approach these first\nthrough the limited context of repeated prisoner’s dilemmas.\n \nWe’ve seen that in the one-shot PD the only NE is mutual\ndefection. This may no longer hold, however, if the players expect to\nmeet each other again in future PDs. Imagine that four firms, all\nmaking widgets, agree to maintain high prices by jointly restricting\nsupply. (That is, they form a cartel.) This will only work if each\nfirm maintains its agreed production quota. Typically, each firm can\nmaximize its profit by departing from its quota while the others\nobserve theirs, since it then sells more units at the higher market\nprice brought about by the almost-intact cartel. In the one-shot case,\nall firms would share this incentive to defect and the cartel would\nimmediately collapse. However, the firms expect to face each other in\ncompetition for a long period. In this case, each firm knows that if\nit breaks the cartel agreement, the others can punish it by\nunderpricing it for a period long enough to more than eliminate its\nshort-term gain. Of course, the punishing firms will take short-term\nlosses too during their period of underpricing. But these losses may\nbe worth taking if they serve to reestablish the cartel and bring\nabout maximum long-term prices. \nOne simple, and famous (but not, contrary to widespread myth,\nnecessarily optimal) strategy for preserving cooperation in repeated\nPDs is called tit-for-tat. This strategy tells each player to\nbehave as follows: \nA group of players all playing tit-for-tat will never see any\ndefections. Since, in a population where others play tit-for-tat,\ntit-for-tat is the rational response for each player, everyone playing\ntit-for-tat is a NE. You may frequently hear people who know a\nlittle (but not enough) game theory talk as if this is the\nend of the story. It is not.  \nThere are two complications. First, the players must be uncertain as\nto when their interaction ends. Suppose the players know when the last\nround comes. In that round, it will be utility-maximizing for players\nto defect, since no punishment will be possible. Now consider the\nsecond-last round. In this round, players also face no punishment for\ndefection, since they expect to defect in the last round anyway. So\nthey defect in the second-last round. But this means they face no\nthreat of punishment in the third-last round, and defect there too. We\ncan simply iterate this backwards through the game tree until we reach\nthe first round. Since cooperation is not a NE strategy in that round,\ntit-for-tat is no longer a NE strategy in the repeated game, and we\nget the same outcome—mutual defection—as in the one-shot\nPD. Therefore, cooperation is only possible in repeated PDs where the\nexpected number of repetitions is indeterminate. (Of course, this does\napply to many real-life games.) Note that in this context any amount\nof uncertainty in expectations, or possibility of trembling hands,\nwill be conducive to cooperation, at least for awhile. When people in\nexperiments play repeated PDs with known end-points, they indeed tend\nto cooperate for awhile, but learn to defect earlier as they gain\nexperience. \nNow we introduce a second complication. Suppose that players’\nability to distinguish defection from cooperation is imperfect.\nConsider our case of the widget cartel. Suppose the players observe a\nfall in the market price of widgets. Perhaps this is because a cartel\nmember cheated. Or perhaps it has resulted from an exogenous drop in\ndemand. If tit-for-tat players mistake the second case for the first,\nthey will defect, thereby setting off a chain-reaction of mutual\ndefections from which they can never recover, since every player will\nreply to the first encountered defection with defection, thereby\nbegetting further defections, and so on. \nIf players know that such miscommunication is possible, they have\nincentive to resort to more sophisticated strategies. In particular,\nthey may be prepared to sometimes risk following defections with\ncooperation in order to test their inferences. However, if they are\ntoo forgiving, then other players can exploit them through\nadditional defections. In general, sophisticated strategies have a\nproblem. Because they are more difficult for other players to infer,\ntheir use increases the probability of miscommunication. But\nmiscommunication is what causes repeated-game cooperative equilibria\nto unravel in the first place. The complexities surrounding\ninformation signaling, screening and inference in repeated PDs help to\nintuitively explain the folk theorem, so called because no\none is sure who first recognized it, that in repeated PDs, for\nany strategy S there exists a possible distribution\nof strategies among other players such that the vector of S\nand these other strategies is a NE. Thus there is nothing special,\nafter all, about tit-for-tat. \nReal, complex, social and political dramas are seldom straightforward\ninstantiations of simple games such as PDs.\n Hardin (1995)\n offers an analysis of two tragically real political cases, the\nYugoslavian civil war of 1991–95, and the 1994 Rwandan genocide,\nas PDs that were nested inside coordination games. \nA coordination game occurs whenever the utility of two or more players\nis maximized by their doing the same thing as one another, and where\nsuch correspondence is more important to them than whatever it is, in\nparticular, that they both do. A standard example arises with rules of\nthe road: ‘All drive on the left’ and ‘All drive on\nthe right’ are both outcomes that are NEs, and neither is more\nefficient than the other. In games of ‘pure’ coordination,\nit doesn’t even help to use more selective equilibrium criteria.\nFor example, suppose that we require our players to reason in\naccordance with Bayes’s rule (see Section 3 above). In these\ncircumstances, any strategy that is a best reply to any vector of\nmixed strategies available in NE is said to be\nrationalizable. That is, a player can find a set of systems\nof beliefs for the other players such that any history of the game\nalong an equilibrium path is consistent with that set of systems. Pure\ncoordination games are characterized by non-unique vectors of\nrationalizable strategies. The Nobel laureate Thomas\n Schelling (1978)\n conjectured, and empirically demonstrated, that in such situations,\nplayers may try to predict equilibria by searching for focal\npoints, that is, features of some strategies that they believe\nwill be salient to other players, and that they believe other players\nwill believe to be salient to them. For example, if two people want to\nmeet on a given day in a big city but can’t contact each other\nto arrange a specific time and place, both might sensibly go to the\ncity’s most prominent downtown plaza at noon. In general, the\nbetter players know one another, or the more often they have been able\nto observe one another’s strategic behavior, the more likely\nthey are to succeed in finding focal points on which to coordinate.\n \nCoordination was, indeed, the first topic of game-theoretic\napplication that came to the widespread attention of philosophers. In\n1969, the philosopher\n David Lewis (1969)\n published Convention, in which the conceptual framework of\ngame-theory was applied to one of the fundamental issues of\ntwentieth-century epistemology, the nature and extent of conventions\ngoverning semantics and their relationship to the justification of\npropositional beliefs. The basic insight can be captured using a\nsimple example. The word ‘chicken’ denotes chickens and\n‘ostrich’ denotes ostriches. We would not be better or\nworse off if ‘chicken’ denoted ostriches and\n‘ostrich’ denoted chickens; however, we would be\nworse off if half of us used the pair of words the first way and half\nthe second, or if all of us randomized between them to refer to\nflightless birds generally. This insight, of course, well preceded\nLewis; but what he recognized is that this situation has the logical\nform of a coordination game. Thus, while particular conventions may be\narbitrary, the interactive structures that stabilize and maintain them\nare not. Furthermore, the equilibria involved in coordinating on noun\nmeanings appear to have an arbitrary element only because we cannot\nPareto-rank them; but\n Millikan (1984)\n shows implicitly that in this respect they are atypical of linguistic\ncoordinations. They are certainly atypical of coordinating conventions\nin general, a point on which Lewis was misled by over-valuing\n‘semantic intuitions’ about ‘the meaning’of\n‘convention’\n (Bacharach 2006,\n Ross 2008a). \n\n Ross & LaCasse (1995)\n present the following example of a real-life coordination game in\nwhich the NE are not Pareto-indifferent, but the Pareto-inferior NE is\nmore frequently observed. In a city, drivers must coordinate on one of\ntwo NE with respect to their behaviour at traffic lights. Either all\nmust follow the strategy of rushing to try to race through lights that\nturn yellow (or amber) and pausing before proceeding when red lights\nshift to green, or all must follow the strategy of slowing down on\nyellows and jumping immediately off on shifts to green. Both patterns\nare NE, in that once a community has coordinated on one of them then\nno individual has an incentive to deviate: those who slow down on\nyellows while others are rushing them will get rear-ended, while those\nwho rush yellows in the other equilibrium will risk collision with\nthose who jump off straightaway on greens. Therefore, once a\ncity’s traffic pattern settles on one of these equilibria it\nwill tend to stay there. And, indeed, these are the two patterns that\nare observed in the world’s cities. However, the two equilibria\nare not Pareto-indifferent, since the second NE allows more cars to\nturn left on each cycle in a left-hand-drive jurisdiction, and right\non each cycle in a right-hand jurisdiction, which reduces the main\ncause of bottlenecks in urban road networks and allows all drivers to\nexpect greater efficiency in getting about. Unfortunately, for reasons\nabout which we can only speculate pending further empirical work and\nanalysis, far more cities are locked onto the Pareto-inferior NE than\non the Pareto-superior one. Conditional game theory (see\n Section 5\n below) provides promising resources for modeling cases such as this\none, in which maintenance of coordination game equilibria likely must\nbe supported by stable social norms, because players are anonymous and\nencounter regular opportunities to gain once-off advantages by\ndefecting from supporting the prevailing equilibrium. This work is\ncurrently ongoing. \nConventions on standards of evidence and scientific rationality, the\ntopics from philosophy of science that set up the context for\nLewis’s analysis, are likely to be of the Pareto-rankable\ncharacter. While various arrangements might be NE in the social game\nof science, as followers of Thomas Kuhn like to remind us, it is\nhighly improbable that all of these lie on a single\nPareto-indifference curve. These themes, strongly represented in\ncontemporary epistemology, philosophy of science and philosophy of\nlanguage, are all at least implicit applications of game theory. (The\nreader can find a broad sample of applications, and references to the\nlarge literature, in\n Nozick (1998).) \nMost of the social and political coordination games played by people\nalso have this feature. Unfortunately for us all, inefficiency traps\nrepresented by Pareto-inferior NE are extremely common in them. And\nsometimes dynamics of this kind give rise to the most terrible of all\nrecurrent human collective behaviors. Hardin’s analysis of two\nrecent genocidal episodes relies on the idea that the biologically\nshallow properties by which people sort themselves into racial and\nethnic groups serve highly efficiently as focal points in coordination\ngames, which in turn produce deadly PDs between them. \nAccording to Hardin, neither the Yugoslavian nor the Rwandan disasters\nwere PDs to begin with. That is, in neither situation, on either side,\ndid most people begin by preferring the destruction of the other to\nmutual cooperation. However, the deadly logic of coordination,\ndeliberately abetted by self-serving politicians, dynamically\ncreated PDs. Some individual Serbs (Hutus) were encouraged to\nperceive their individual interests as best served through\nidentification with Serbian (Hutu) group-interests. That is, they\nfound that some of their circumstances, such as those involving\ncompetition for jobs, had the form of coordination games. They thus\nacted so as to create situations in which this was true for other\nSerbs (Hutus) as well. Eventually, once enough Serbs (Hutus)\nidentified self-interest with group-interest, the identification\nbecame almost universally correct, because (1) the most\nimportant goal for each Serb (Hutu) was to do roughly what every other\nSerb (Hutu) would, and (2) the most distinctively Serbian\nthing to do, the doing of which signalled coordination, was to exclude\nCroats (Tutsi). That is, strategies involving such exclusionary\nbehavior were selected as a result of having efficient focal points.\nThis situation made it the case that an individual—and\nindividually threatened—Croat’s (Tutsi’s)\nself-interest was best maximized by coordinating on assertive Croat\n(Tutsi) group-identity, which further increased pressures on Serbs\n(Hutus) to coordinate, and so on. Note that it is not an aspect of\nthis analysis to suggest that Serbs or Hutus started things; the\nprocess could have been (even if it wasn’t in fact) perfectly\nreciprocal. But the outcome is ghastly: Serbs and Croats (Hutus and\nTutsis) seem progressively more threatening to each other as they\nrally together for self-defense, until both see it as imperative to\npreempt their rivals and strike before being struck. If Hardin is\nright—and the point here is not to claim that he is,\nbut rather to point out the worldly importance of determining which\ngames agents are in fact playing—then the mere presence of an\nexternal enforcer (NATO?) would not have changed the game, pace the\nHobbesian analysis, since the enforcer could not have threatened\neither side with anything worse than what each feared from the other.\nWhat was needed was recalibration of evaluations of interests, which\n(arguably) happened in Yugoslavia when the Croatian army began to\ndecisively win, at which point Bosnian Serbs decided that their\nself/group interests were better served by the arrival of NATO\npeacekeepers. The Rwandan genocide likewise ended with a military\nsolution, in this case a Tutsi victory. (But this became the seed for\nthe most deadly international war on earth since 1945, the Congo War\nof 1998–2006.) \nOf course, it is not the case that most repeated games lead to\ndisasters. The biological basis of friendship in people and other\nanimals is partly a function of the logic of repeated games. The\nimportance of payoffs achievable through cooperation in future games\nleads those who expect to interact in them to be less selfish than\ntemptation would otherwise encourage in present games. The fact that\nsuch equilibria become more stable through learning gives friends the\nlogical character of built-up investments, which most people take\ngreat pleasure in sentimentalizing. Furthermore, cultivating shared\ninterests and sentiments provides networks of focal points around\nwhich coordination can be increasingly facilitated. \nFollowing\n Lewis’s (1969)\n introduction of coordination games into the philosophical literature,\nthe philosopher Margaret\n Gilbert (1989)\n argued, as against Lewis, that game theory is the wrong kind of\nanalytical technology for thinking about human conventions because,\namong other problems, it is too ‘individualistic’, whereas\nconventions are essentially social phenomena. More directly, her claim\nwas that conventions are not merely the products of decisions of many\nindividual people, as might be suggested by a theorist who modeled a\nconvention as an equilibrium of an n-person game in which each\nplayer was a single person. Similar concerns about allegedly\nindividualistic foundations of game theory have been echoed by another\nphilosopher, Martin\n Hollis (1998)\n and economists Robert Sugden\n (1993,\n 2000,\n 2003)\n and Michael\n Bacharach (2006).\n In particular, it motivated Bacharach to propose a theory of team\nreasoning, which was completed by Sugden, along with Nathalie\nGold, after Bacharach’s death. This theory constitutes a key\npart of the background context for appreciating the value of a major\nrecent extension to game theory, Wynn\n Stirling’s (2012)\n theory of conditional games.  \nConsider again the one-shot Prisoner’s Dilemma as discussed in\n Section 2.4\n and produced, with an inverted matrix for ease of subsequent\ndiscussion, as follows: \n(C denotes the strategy of cooperating with one’s opponent\n(i.e., refusing to confess) and D denotes the strategy of defecting on\na deal with one’s opponent (i.e., confessing).) Many people find\nit incredible when a game theorist tells them that players designated\nwith the honorific ‘rational’ must choose in this game in\nsuch a way as to produce the outcome (D,D). The explanation seems to\nrequire appeal to very strong forms of both descriptive and normative\nindividualism. After all, if the players attached higher value to the\nsocial good (for their 2-person society of thieves) than to their\nindividual welfare, they could then do better individually too;\ngame-theoretic ‘rationality’, it is objected, yields\nbehavior that is perverse even from the individually optimizing point\nof view. The players undermine their own welfare, one might argue,\nbecause they obstinately refuse to pay any attention to the social\ncontext of their\n choices.Sugden (1993)\n seems to have been the first to suggest that players who truly\ndeserve to be called ‘rational’, including non-altruistic\nones, would in the one-shot PD reason as a team, that is,\nwould each arrive at their choices of strategies by asking ‘What\nis best for us?’ instead of ’What is best for\nme?’. \n\n Binmore (1994)\n forcefully argues that this line of criticism confuses game theory as\nmathematics with questions about which game theoretic models are most\ntypically applicable to situations in which people find themselves. If\nplayers value the utility of a team they’re part of over and\nabove their more narrowly individualistic interests, then this should\nbe represented in the payoffs associated with a game theoretic model\nof their choices. In the situation modeled as a PD above, if the two\nplayers’ concern for ‘the team’ were strong enough\nto induce a switch in strategies from D to C, then the payoffs in the\n(cardinally interpreted) upper left cell would have to be raised to at\nleast 3. (At 3, players would be indifferent between\ncooperating and defecting.) Then we get the following transformation\nof the game:  \nThis is no longer a PD; it is an Assurance game, which has\ntwo NE at (C,C) and (D,D), with the former being Pareto superior to\nthe latter. Thus if the players find this equilibrium, we should not\nsay that they have played non-NE strategies in a PD. Rather, we should\nsay that the PD was the wrong model of their situation. \nWhat is at issue here is the best choice of a convention for applying\nmathematics to empirical description. Binmore is clearly right, and\nthe majority of commentators have come to recognize that he is right,\nif we interpret the payoffs of games by reference to utility functions\nwith unrestricted domains. This is the overwhelmingly standard\npractice in both economics and formal decision theory. For a number of\nyears this issue was regarded as closed in the mainstream literature.\nHowever,\n Sugden (2018)\n argues in very recent work that there are reasons, quite independent\nof technical considerations about which conventions are most\nconvenient for representing empirical interactions as games, for\navoiding appeal to preferences over unrestricted domains in analyzing\nwelfare (that is, in doing normative economics). On the basis of this\nargument, Sugden reverts to using game-theoretic models in which\npayoffs are restricted to objectively specifiable metrics, such as\nmonetary returns. The substantive issues in welfare economics on which\nSugden sheds now light are too interesting for a critic to reasonably\nrefuse to engage with them out of mere stubbornness about adhering to\nconvention in interpreting game representations. It is too soon to\nassess whether the advances in welfare analysis that Sugden seeks are\nsustainable under critical stress-testing. If they prove not to be,\nthen his motivation for an alternative convention on payoff\ninterpretation will dissolve. I think it more likely, however, that a\nperiod of intensive innovation in welfare economics lies just ahead of\nus, and that in the course of this economists and other analysts will\ngrow comfortable with operating two different representational\nconventions depending on problem contexts. If that is indeed our\nfuture, then we can anticipate a further stage in which, because\nproblem contexts tend not to remain conveniently isolated from one\nanother, new formalism is demanded to allow both conventions to be\noperated in a single application without confusion. But these\nspeculations run well ahead of the current state of theory. \nLet us then return to the thread of theory development that followed\nwidespread accommodation of Binmore’s critique.\nBacharach’s scientific executors, Sugden and Gold, in\n Bacharach (2006),\n pp. 171–173), unlike\n Hollis and Sugden (1993),\n use the standard convention for payoff interpretation, under which\nplayers can only be modeled as cooperating in a one-shot PD if at\nleast one player makes an error. (For some error specifications, (C,C)\ncould arise consistently with QRE as the solution concept.) Under this\nassumption, Bacharach, Sugden and Gold argue, human game players will\noften or usually avoid framing situations in such a way that a\none-shot PD is the right model of their circumstances. A situation\nthat ‘individualistic’ agents would frame as a PD might be\nframed by ‘team reasoning’ agents as the Assurance game\ntransformation above. Note that the welfare of the team might make a\ndifference to (cardinal) payoffs without making enough of a\ndifference to trump the lure of unilateral defection. Suppose it\nbumped them up to 2.5 for each player; then the game would remain a\nPD. This point is important, since in experiments in which subjects\nplay sequences of one-shot PDs (not repeated PDs, since\nopponents in the experiments change from round to round), majorities\nof subjects begin by cooperating but learn to defect as the\nexperiments progress. On Bacharach’s account of this phenomenon,\nthese subjects initially frame the game as team reasoners. However, a\nminority of subjects frame it as individualistic reasoners and defect,\ntaking free riders’ profits. The team reasoners then re-frame\nthe situation to defend themselves. This introduces a crucial aspect\nof Bacharach’s account. Individualistic reasoners and team\nreasoners are not claimed to be different types of people. People,\nBacharach maintains, flip back and forth between individualistic\nagency and participation in team agency. \nNow consider the following Pure Coordination game: \nWe can interpret this as representing a situation in which players are\nnarrowly individualistic, and thus each indifferent between the two NE\nof (U, L) and (D, R), or are team reasoners but haven’t\nrecognized that their team is better off if they stabilize around one\nof the NE rather than the other. If they do come to such recognition,\nperhaps by finding a focal point, then the Pure Coordination game is\ntransformed into the following game known as Hi-Lo: \nCrucially, here the transformation requires more than mere\nteam reasoning. The players also need focal points to know which of\nthe two Pure Coordination equilibria offers the less risky prospect\nfor social stabilization\n (Binmore 2008).\n In fact, Bacharach and his executors are interested in the\nrelationship between Pure Coordination games and Hi-Lo games for a\nspecial reason. It does not seem to imply any criticism of NE as a\nsolution concept that it doesn’t favor one strategy vector over\nanother in a Pure Coordination game. However, NE also\ndoesn’t favor the choice of (U,L) over (D,R) in the Hi-Lo game\ndepicted, because (D,R) is also a NE. At this point Bacharach and his\nfriends adopt the philosophical reasoning of the refinement program.\nSurely, they complain, ‘rationality’ recommends (U,L).\nTherefore, they conclude, axioms for team reasoning should be built\ninto refined foundations of game theory. \nWe need not endorse the idea that game theoretic solution concepts\nshould be refined to accommodate an intuitive general concept of\nrationality to motivate interest in Bacharach’s contribution.\nThe non-psychological game theorist can propose a subtle shift of\nemphasis: instead of worrying about whether our models should respect\na team-centred norm of rationality, we might simply point to empirical\nevidence that people, and perhaps other agents, seem to often make\nchoices that reveal preferences that are conditional on the welfare of\ngroups with which they are associated. To this extent their agency is\npartly or wholly—and perhaps stochastically—identified\nwith these groups, and this will need to be reflected when we model\ntheir agency using utility functions. Then we could better describe\nthe theory we want as a theory of team-centred choice rather than as a\ntheory of team reasoning. Note that this philosophical\ninterpretation is consistent with the idea that some of our evidence,\nperhaps even our best evidence, for the existence of team-centred\nchoice is psychological. It is also consistent with the suggestion\nthat the processes that flip people between individualized and\nteam-centred agency are often not deliberative or consciously\nrepresented. The point is simply that we need not follow Bacharach in\nthinking of game theory as a model of reasoning or rationality in\norder to be persuaded that he has identified a gap we would like to\nhave formal resources to fill. \nSo, do people’s choices seem to reveal team-centred\npreferences? Standard examples, including Bacharach’s own, are\ndrawn from team sports. Members of such teams are under considerable\nsocial pressure to choose actions that maximize prospects for victory\nover actions that augment their personal statistics. The problem with\nthese examples is that they embed difficult identification problems\nwith respect to the estimation of utility functions; a narrowly\nself-interested player who wants to be popular with fans might behave\nidentically to a team-centred player. Soldiers in battle conditions\nprovide more persuasive examples. Though trying to convince soldiers\nto sacrifice their lives in the interests of their countries is often\nineffective, most soldiers can be induced to take extraordinary risks\nin defense of their buddies, or when enemies directly menace their\nhome towns and families. It is easy to think of other kinds of teams\nwith which most people plausibly identify some or most of the time:\nproject groups, small companies, political constituency committees,\nlocal labor unions, clans and households. Strongly individualistic\nsocial theory tries to construct such teams as equilibria in games\namongst individual people, but no assumption built into game theory\n(or, for that matter, mainstream economic theory) forces this\nperspective (see\n Guala (2016)\n for a critical review of options). We can instead suppose that teams\nare often exogenously welded into being by complex interrelated\npsychological and institutional processes. This invites the game\ntheorist to conceive of a mathematical mission that consists not in\nmodeling team reasoning, but rather in modeling choice that is\nconditional on the existence of team dynamics. \nThis brings us to\n Stirling’s (2012)\n extension of game theory to cover such conditional interactions.\nStirling’s aim is to formalize, and derive equilibrium\nconditions for, a notion of group preference that is, on the one hand,\nnot a mere aggregation of individual preferences but also does not, on\nthe other hand, simply assume the existence of a transcendent\ncollective will that is imposed on individuals. The intuitive target\nStirling has in mind is that of processes by which people derive their\nactual preferences partly on the basis of the comparative consequences\nfor group welfare of different possible profiles of preferences that\nmembers could severally hypothetically reveal. A key constraint\nStirling respects is that the theory’s solution concepts (i.e.,\nits equilibria) must formally generalize the standard\nsolution concepts (NE, SPE, QRE), not replace them.\nConditional game theory is supposed to be ‘real’ game\ntheory, not ‘pseudo’ game theory. \nLet us develop the intuitive idea of preference conditionalization in\nmore detail. People may often—perhaps typically—defer full\nresolution of their preferences until they get more information about\nthe preferences of others who are their current or potential\nteam-mates. Stirling himself provides a simple (arguably too simple)\nexample from\n Keeney and Raiffa (1976),\n in which a farmer forms a clear preference among different climate\nconditions for a land purchase only after, and partly in light of,\nlearning the preferences of his wife. This little thought experiment\nis plausible, but not ideal as an illustration because it is easily\nconflated with vague notions we might entertain about fusion\nof agency in the ideal of marriage—and it is important to\ndistinguish the dynamics of preference conditionalization in teams of\ndistinct agents from the simple collapse of individual\nagency. So let us construct a better example. Imagine a corporate\nChairperson consulting her risk-averse Board about whether they should\npursue a dangerous hostile takeover bid. Compare two possible\nprocedures she might use: in process (i) she sends each Board member\nan individual e-mail about the idea a week prior to the meeting; in\nprocess (ii) she springs it on them collectively at the\nmeeting. Most people will agree that the two processes might yield\ndifferent outcomes, and that a main reason for this is that on process\n(i), but not (ii), some members might entrench personal opinions that\nthey would not have time to settle into if they received information\nabout one another’s willingness to challenge the Chair in public\nat the same time as they heard the proposal for the first time. In\nboth imagined processes there are, at the point of voting, sets of\nindividual preferences to be aggregated by the vote. But it is more\nlikely that some preferences in the set generated by the second\nprocess were conditional on preferences of others. A\nconditional preference as Stirling defines it is a preference that is\ninfluenced by information about the preferences of (specified)\nothers. \nA second notion formalized in Stirling’s theory is\nconcordance. This refers to the extent of controversy or\ndiscord to which a set of preferences, including a set of conditional\npreferences, would generate if equilibrium among them were\nimplemented. Members or leaders of teams do not always want to\nmaximize concordance by engineering all internal games as Assurance or\nHi-lo (though they will always likely want to eliminate PDs). For\nexample, a manager might want to encourage a degree of competition\namong profit centers in a firm, while wanting the cost centers to\nidentify completely with the team as a whole. \nStirling formally defines representation theorems for three kinds of\nordered utility functions: conditional utility, concordant utility and\nconditional concordant utility. These may be applied recursively, i.e.\nto individuals, to teams and to teams of teams. Then the core of the\nformal development is the theory that aggregates individuals’\nconditional concordant preferences to build models of team choice that\nare not exogenously imposed on team members, but instead derive from\ntheir several preferences. In stating Stirling’s aggregation\nprocedure in the present context, it is useful to change his\nterminology, and therefore paraphrase him rather than quote directly.\nThis is because Stirling refers to “groups” rather than to\n“teams”. Stirling’s initial work on CGT was entirely\nindependent of Bacharach’s work,so was not configured within the\ncontext of team reasoning (or what we might reinterpret as\nteam-centred choice). But Bacharach’s ideas provide a natural\nsetting in which to frame Stirling’s technical achievement as an\nenrichment of the applicability of game theory in social science (see\n Hofmeyr and Ross (2019)).\n We can then paraphrase his five constraints on aggregation as\nfollows: \n(1) Conditioning: A team member’s preference ordering\nmay be influenced by the preferences of other team members, i.e. may\nbe conditional. (Influence may be set to zero, in which case the\nconditional preference ordering collapses to the categorical\npreference ordering to standard RPT.) \n(2) Endogeny: A concordant ordering for a team must be\ndetermined by the social interactions of its sub-teams. (This\ncondition ensures that team preferences are not simply imposed on\nindividual preferences.) \n(3) Acyclicity: Social influence relations are not\nreciprocal. (This will likely look at first glance to be a strange\nrestriction: surely most social influence relationships, among people\nat any rate, are reciprocal. But, as noted earlier, we need\nto keep conditional preference distinct from agent fusion, and this\ncondition helps to do that. More importantly, as a matter of\nmathematics it allows teams to be represented in directed graphs. The\ncondition is not as restrictive, where modeling flexibility is\nconcerned, as one might at first think, for two reasons. First, it\nonly bars us from representing an agent j influenced by\nanother agent i from directly influencing\ni. We are free to represent j as influencing\nk who in turn influences i.) Second, and more\nimportantly, in light of the exchangeability constraint below,\naggregation is insensitive to the ordering of pairs of players between\nwhom there is a social influence relationship.) \n(4) Exchangeability: Concordant preference orderings are\ninvariant under representational transformations that are equivalent\nwith respect to information about conditional preferences. \n(5) Monotonicity: If one sub-team prefers choice alternative\nA to B and all other sub-teams are indifferent between A and B, then\nthe team does not prefer B to A. \nUnder these restrictions, Stirling proves an aggregation theorem which\nfollows a general result for updating utility in light of new\ninformation that was developed by\n Abbas (2003, Other Internet Resources).\n Individual team members each calculate the team preference by\naggregating conditional concordant preferences. Then the analyst\napplies marginalization. Let \\(X^n\\) be a team. Let\n\\(X^m=\\{X_{j1},\\ldots,X_{jm}\\}\\) and \\(X = \\{X_{i1},\\ldots, X_{ik}\\}\\)\nbe disjoint sub-teams of \\(X^n\\). Then the marginal concordant utility\nof \\(X^m\\) with respect to the sub-team \\(\\{X^m, X^k\\}\\) is obtained\nby summing over \\(\\mathcal{A}^k\\), yielding  \nand the marginal utility of the individual team member \\(X_i\\) is\ngiven by  \nwhere the notation \\(\\sum_{\\sim \\mathbb{a}_i}\\) means that the sum is\ntaken over all arguments except \\(\\mathbb{a}_i\\)\n (Stirling (2012),\n p. 62). This operation produces the non-conditional\npreferences of individual i ex post—that is, updated in\nlight of her conditional concordant preferences and the information on\nwhich they are conditioned, namely, the conditional concordant\npreferences of the team. Once all ex post preferences of agents have\nbeen calculated, the resulting games in which they are involved can be\nsolved by standard analysis. \nStirling’s construction is, as he says, a true generalization of\nstandard utility theory so as to make non-conditioned\n(“categorical”) utility a special case. It provides a\nbasis for formalization of team utility, which can be compared with\nany of the following: the pre-conditioned categorical utility of an\nindividual or sub-team; the conditional utility of an individual or\nsub-team; or the conditional concordant utility of an individual or\nsub-team. Once every individual’s preferences in a team choice\nproblem have been marginalized, NE, SPE or QRE analyses can be\nproposed as solutions to the problem given full information about\nsocial influences. Situations of incomplete information can be solved\nusing Byes-Nash or sequential equilibrium. \nIn case the reader has struggled to follow the overall point of the\ntechnical constructions above, we can summarize the achievement of\nconditional game theory (CGT) in higher-level terms as follows. CGT\nmodels the propagation of influence flows by applying the formal\nsyntax of probability theory (through the operation of\nmarginalization) to game theory, and constructing graph theoretical\nrepresentations. As social influence propagates through a group and\nplayers modulate their preferences on the basis of other\nplayers’ preferences, a group preference may emerge. Group\npreferences are not a direct basis for action, but encapsulate a\nsocial model incorporating the relationships and interdependencies\namong the agents. CGT shows us how to derive a coordination ordering\nfor a group which combines the conditional and categorical preferences\nof its members, in much the same way as, in probability theory, the\njoint probability of an event is determined by conditional and\nmarginal probabilities. So, just as the conventional application of\nthe probability syntax is a means of expressing a cognizer’s\nepistemological uncertainty regarding belief, so extending this syntax\nto game theory allows us to represent an agent’s practical\nuncertainty regarding preference. \nIf this were the end of the story, then CGT would be little more than\na pre-processing mechanism for identifying standard games. The real\ninnovation lies in representing the influence of concordance\nconsiderations on equilibrium determination. The social model can be\nused to generate an operational definition of group preference, and to\ndefine truly coordinated choices. There is no assumption that groups\nnecessarily optimize their preferences or that individual agents\ncoordinate their choices. The point is merely that we can formally\nrepresent conditions under which agents in games can do what actual\npeople often seem to: adapt and settle their individual\npreferences in light both of what others prefer, and of what promotes\na group’s stability and efficiency. Team agency is thus\nincorporated into game theory instead of being left as an exogenous\npsychological construct that the analyst must investigate in advance\nof building a game-theoretic model of socially embedded agents. \nIn some games, a player can improve her outcome by taking an action\nthat makes it impossible for her to take what would be her best action\nin the corresponding simultaneous-move game. Such actions are referred\nto as commitments, and they can serve as alternatives to\nexternal enforcement in games which would otherwise settle on\nPareto-inefficient equilibria. \nConsider the following hypothetical example (which is not a\nPD). Suppose you own a piece of land adjacent to mine, and I’d\nlike to buy it so as to expand my lot. Unfortunately, you don’t\nwant to sell at the price I’m willing to pay. If we move\nsimultaneously—you post a selling price and I independently give\nmy agent an asking price—there will be no sale. So I might try\nto change your incentives by playing an opening move in which I\nannounce that I’ll build a putrid-smelling sewage disposal plant\non my land beside yours unless you sell, thereby inducing you to lower\nyour price. I’ve now turned this into a sequential-move game.\nHowever, this move so far changes nothing. If you refuse to sell in\nthe face of my threat, it is then not in my interest to carry it out,\nbecause in damaging you I also damage myself. Since you know this you\nshould ignore my threat. My threat is incredible, a case of\ncheap talk. \nHowever, I could make my threat credible by committing\nmyself. For example, I could sign a contract with some farmers\npromising to supply them with treated sewage (fertilizer) from my\nplant, but including an escape clause in the contract releasing me\nfrom my obligation only if I can double my lot size and so put it to\nsome other use. Now my threat is credible: if you don’t sell,\nI’m committed to building the sewage plant. Since you know this,\nyou now have an incentive to sell me your land in order to escape its\nruination. \nThis sort of case exposes one of many fundamental differences between\nthe logic of non-parametric and parametric maximization. In parametric\nsituations, an agent can never be made worse off by having more\noptions. (Even if a new option is worse than the options with which\nshe began, she can just ignore it.) But where circumstances are\nnon-parametric, one agent’s strategy can be influenced in\nanother’s favour if options are visibly restricted.\nCortez’s burning of his boats (see\n Section 1)\n is, of course, an instance of this, one which serves to make the\nusual metaphor literal. \nAnother example will illustrate this, as well as the applicability of\nprinciples across game-types. Here we will build an imaginary\nsituation that is not a PD—since only one player has an\nincentive to defect—but which is a social dilemma insofar as its\nNE in the absence of commitment is Pareto-inferior to an outcome that\nis achievable with a commitment device. Suppose that two of\nus wish to poach a rare antelope from a national park in order to sell\nthe trophy. One of us must flush the animal down towards the second\nperson, who waits in a blind to shoot it and load it onto a truck. You\npromise, of course, to share the proceeds with me. However, your\npromise is not credible. Once you’ve got the buck, you have no\nreason not to drive it away and pocket the full value from it. After\nall, I can’t very well complain to the police without getting\nmyself arrested too. But now suppose I add the following opening move\nto the game. Before our hunt, I rig out the truck with an alarm that\ncan be turned off only by punching in a code. Only I know the code. If\nyou try to drive off without me, the alarm will sound and we’ll\nboth get caught. You, knowing this, now have an incentive to wait for\nme. What is crucial to notice here is that you prefer that I\nrig up the alarm, since this makes your promise to give me my share\ncredible. If I don’t do this, leaving your promise\nincredible, we’ll be unable to agree to try the crime\nin the first place, and both of us will lose our shot at the profit\nfrom selling the trophy. Thus, you benefit from my preventing you from\ndoing what’s optimal for you in a subgame. \nWe may now combine our analysis of PDs and commitment devices in\ndiscussion of the application that first made game theory famous\noutside of the academic community. The nuclear stand-off between the\nsuperpowers during the Cold War was intensively studied by the first\ngeneration of game theorists, many of whom received direct or indirect\nfunding support from the US military.\n Poundstone 1992\n provides the relatively ‘sanitized’ history of this\ninvolvement that has long been available to the casual historian who\nrelies on secondary sources in addition to theorists’ public\nreminiscences. Recently, a more skeptically alert and professional\nhistorical study has been produced by\n Amadae (2016),\n which provides scholarly context for the still more hair-raising\nmemoir of a pioneer of applied game theory, participant in the\ndevelopment of Cold War nuclear strategy, and famous leaker of the\nPentagon’s secret files on the Vietnam War, Daniel Ellsberg\n (Ellsberg 2017).\n History consistent with these accounts but stimulating less pupil\ndilation in the reader is\n Erickson (2015).\n  \nIn the conventional telling of the tale, the nuclear stand-off between\nthe USA and the USSR attributes the following policy to both parties.\nEach threatened to answer a first strike by the other with a\ndevastating counter-strike. This pair of reciprocal strategies, which\nby the late 1960s would effectively have meant blowing up the world,\nwas known as ‘Mutually Assured Destruction’, or\n‘MAD’. Game theorists at the time objected that MAD was\nmad, because it set up a PD as a result of the fact that the\nreciprocal threats were incredible. The reasoning behind this\ndiagnosis went as follows. Suppose the USSR launches a first strike\nagainst the USA. At that point, the American President finds his\ncountry already destroyed. He doesn’t bring it back to life by\nnow blowing up the world, so he has no incentive to carry out his\noriginal threat to retaliate, which has now manifestly failed to\nachieve its point. Since the Russians can anticipate this, they should\nignore the threat to retaliate and strike first. Of course, the\nAmericans are in an exactly symmetric position, so they too should\nstrike first. Each power recognizes this incentive on the part of the\nother, and so anticipates an attack if they don’t rush to\npreempt it. What we should therefore expect, because it is the only NE\nof the game, is a race between the two powers to be the first to\nattack. The clear implication is the destruction of the world. \nThis game-theoretic analysis caused genuine consternation and fear on\nboth sides during the Cold War, and is reputed to have produced some\nstriking attempts at setting up strategic commitment devices. Some\nanecdotes, for example, allege that President Nixon had the CIA try to\nconvince the Russians that he was insane or frequently drunk, so that\nthey’d believe that he’d launch a retaliatory strike even\nwhen it was no longer in his interest to do so. Similarly, the Soviet\nKGB is sometimes claimed, during Brezhnev’s later years, to to\nhave fabricated medical reports exaggerating the extent of his\nsenility with the same end in mind. Even if these stories aren’t\ntrue, their persistent circulation indicates understanding of the\nlogic of strategic commitment. Ultimately, the strategic symmetry that\nconcerned the Pentagon’s analysts was complicated and perhaps\nbroken by changes in American missile deployment tactics. They\nequipped a worldwide fleet of submarines with enough missiles to\nlaunch a devastating counterattack by themselves. This made the\nreliability of the US military communications network less\nstraightforward, and in so doing introduced an element of\nstrategically relevant uncertainty. The President probably could be\nless sure to be able to reach the submarines and cancel their orders\nto attack if prospects of American survival had become hopeless. Of\ncourse, the value of this in breaking symmetry depended on the\nRussians being aware of the potential problem. In Stanley\nKubrick’s classic film Dr. Strangelove, the world is\ndestroyed by accident because the Russians build a doomsday machine\nthat will automatically trigger a retaliatory strike regardless of\ntheir leadership’s resolve to follow through on the implicit MAD\nthreat but then keep it a secret. As a result, when an\nunequivocally mad American colonel launches missiles at Russia on his\nown accord, and the American President tries to convince his Soviet\ncounterpart that the attack was unintended, the Russian Premier\nsheepishly tells him about the secret doomsday machine. Now the two\nleaders can do nothing but watch in dismay as the world is blown up\ndue to a game-theoretic mistake. \nThis example of the Cold War standoff, while famous and of\nconsiderable importance in the history of game theory and its popular\nreception, relied at the time on analyses that weren’t very\nsubtle. The military game theorists were almost certainly mistaken to\nthe extent that they modeled the Cold War as a one-shot PD in the\nfirst place. For one thing, the nuclear balancing game was enmeshed in\nlarger global power games of great complexity. For another, it is far\nfrom clear that, for either superpower, annihilating the other while\navoiding self-annihilation was in fact the highest-ranked outcome. If\nit wasn’t, in either or both cases, then the game wasn’t a\nPD. A cynic might suggest that the operations researchers on both\nsides were playing a cunning strategy in a game over funding, one that\ninvolved them cooperating with one another in order to convince their\npoliticians to allocate more resources to weapons.  \nIn more mundane circumstances, most people exploit a ubiquitous\ncommitment device that Adam Smith long ago made the centerpiece of his\ntheory of social order: the value to people of their own\nreputations. Even if I am secretly stingy, I may wish to\ncause others to think me generous by tipping in restaurants, including\nrestaurants in which I never intend to eat again. The more I do this\nsort of thing, the more I invest in a valuable reputation which I\ncould badly damage through a single act of obvious, and observed,\nmean-ness. Thus my hard-earned reputation for generosity functions as\na commitment mechanism in specific games, itself enforcing continued\nre-investment. In time, my benevolence may become habitual, and\nconsequently insensitive to circumstantial variations, to the point\nwhere an analyst has no remaining empirical justification for\ncontinuing to model me as having a preference for stinginess. There is\na good deal of evidence that the hyper-sociality of humans is\nsupported by evolved biological dispositions (found in most but not\nall people) to suffer emotionally from negative gossip and the fear of\nit. People are also naturally disposed to enjoy gossiping,\nwhich means that punishing others by spreading the news when their\ncommitment devices fail is a form of social policing they don’t\nfind costly and happily take up. A nice feature of this form of\npunishment is that it can, unlike (say) hitting people with sticks, be\nwithdrawn without leaving long-term damage to the punishee. This is a\nhappy property of a device that has as its point the maintenance of\nincentives to contribute to joint social projects; collaboration is\ngenerally more fruitful with team-mates whose bones aren’t\nbroken. Thus forgiveness conventions also play a strategic role in\nthis elegant commitment mechanism that natural selection built for us.\nFinally, norms are culturally evolved mutual expectations in\na group of people (or, perhaps, in a few other intelligent social\nanimals) that have the further property that individuals who violate\nthem may punish themselves by feeling guilt or shame. Thus\nthey may often take cooperative actions against their narrow\nself-interest even when no one else is paying attention. Religious\nstories, or philosophical ones involving Kantian moral\n‘rationality’, are especially likely to be told in\nexplanation of norms because the underlying game-theoretic basis\ndoesn’t occur to people; and the norms in question may function\nmore effectively for that very reason. \nThough the so-called ‘moral emotions’are extremely useful\nfor maintaining commitment, they are not necessary for it. Larger\nhuman institutions are, famously, highly morally obtuse; however,\ncommitment is typically crucial to their functional logic. For\nexample, a government tempted to negotiate with terrorists to secure\nthe release of hostages on a particular occasion may commit to a\n‘line in the sand’ strategy for the sake of maintaining a\nreputation for toughness intended to reduce terrorists’\nincentives to launch future attacks. A different sort of example is\nprovided by Qantas Airlines of Australia. Qantas has never suffered a\nfatal accident, and for a time (until it suffered some embarrassing\nnon-fatal accidents to which it likely feared drawing attention) made\nmuch of this in its advertising. This means that its planes, at least\nduring that period, probably were safer than average even if\nthe initial advantage was merely a bit of statistical good fortune,\nbecause the value of its ability to claim a perfect record rose the\nlonger it lasted, and so gave the airline continuous incentives to\nincur greater costs in safety assurance. It likely still has incentive\nto take extra care to prevent its record of fatalities from crossing\nthe magic reputational line between 0 and 1. \nCertain conditions must hold if reputation effects are to underwrite\ncommitment. A person’s reputation can have a standing value\nacross a range of games she plays, but in that case her concern for\nits value should be factored into payoffs in specifying each specific\ngame into which she enters. Reputation can be built up\nthrough play of a game only in a case of a repeated game.\nThen the value of the reputation must be greater to its cultivator\nthan the value to her of sacrificing it in any particular\nround of the repeated game. Thus players may establish commitment by\nreducing the value of each round so that the temptation to defect in\nany round never gets high enough to constitute a hard-to-resist\ntemptation. For example, parties to a contract may exchange their\nobligations in small increments to reduce incentives on both sides to\nrenege. Thus builders in construction projects may be paid in weekly\nor monthly installments. Similarly, the International Monetary Fund\noften dispenses loans to governments in small tranches, thereby\nreducing governments’ incentives to violate loan conditions once\nthe money is in hand; and governments may actually prefer such\narrangements in order to remove domestic political pressure for\nnon-compliant use of the money. Of course, we are all familiar with\ncases in which the payoff from a defection in a current round becomes\ntoo great relative to the longer-run value of reputation to future\ncooperation, and we awake to find that the society treasurer has\nabsconded overnight with the funds. Commitment through concern for\nreputation is the cement of society, but any such natural bonding\nagent will be far from perfectly effective. \n\n Gintis (2009b,\n 2009b) feels justified in stating that “game theory is a\nuniversal language for the unification of the behavioral\nsciences.” There are good examples of such unifying work.\nBinmore\n (1998,\n 2005a) models social history as a series of convergences on\nincreasingly efficient equilibria in commonly encountered transaction\ngames, interrupted by episodes in which some people try to shift to\nnew equilibria by moving off stable equilibrium paths, resulting in\nperiodic catastrophes. (Stalin, for example, tried to shift his\nsociety to a set of equilibria in which people cared more about the\nfuture industrial, military and political power of their state than\nthey cared about their own lives. He was not successful; however, his\nefforts certainly created a situation in which, for a few decades,\nmany Soviet people attached far less importance to other\npeople’s lives than usual.) A game-theoretic perspective\nindeed seems pervasively useful in understanding phenomena across the\nfull range of social sciences. In\n Section 4,\n for example, we considered Lewis’s recognition that each human\nlanguage amounts to a network of Nash equilibria in coordination games\naround conveyance of information. \nGiven his work’s vintage, Lewis restricted his attention to\nstatic game theory, in which agents are modeled as deliberately\nchoosing strategies given exogenously fixed\nutility-functions. As a result of this restriction, his account\ninvited some philosophers to pursue a misguided quest for a general\nanalytic theory of the rationality of conventions (as noted by\n Bickhard 2008).\n Though Binmore has criticized this focus repeatedly through a\ncareer’s worth of contributions (see the references for a\nselection),\n Gintis (2009a)\n has recently isolated the underlying problem with particular clarity\nand tenacity. NE and SPE are brittle solution concepts when\napplied to naturally evolved computational mechanisms like animal\n(including human) brains. As we saw in\n Section 3\n above, in coordination (and other) games with multiple NE, what it is\neconomically rational for a player to do is highly sensitive to the\nlearning states of other players. In general, when players find\nthemselves in games where they do not have strictly dominant\nstrategies, they only have uncomplicated incentives to play NE or SPE\nstrategies to the extent that other players can be expected to find\ntheir NE or SPE strategies. Can a general theory of\nstrategic rationality, of the sort that philosophers have sought, be\nreasonably expected to cover the resulting contingencies? Resort to\nBayesian reasoning principles, as we reviewed in\n Section 3.1,\n is the standard way of trying to incorporate such uncertainty into\ntheories of rational, strategic decision. However, as\n Binmore (2009)\n argues following the lead of\n Savage (1954),\n Bayesian principles are only plausible as principles of\nrationality itself in so-called ‘small worlds’, that\nis, environments in which distributions of risk are quantified in a\nset of known and enumerable parameters, as in the solution to our\nriver crossing game from\n Section 3.\n In large worlds, where utility functions, strategy sets and\ninformational structure are difficult to estimate and subject to\nchange by contingent exogenous influences, the idea that Bayes’s\nrule tells players how to ‘be rational’ is quite\nimplausible. But then why should we expect players to choose NE or SPE\nor sequential-equilibrium strategies in wide ranges of social\ninteractions? \nAs\n Binmore (2009)\n and\n Gintis (2009a)\n both stress, if game theory is to be used to model actual, natural\nbehavior and its history, outside of the small-world settings on which\nmicroeconomists (but not macroeconomists or political scientists or\nsociologists or philosophers of science) mainly traffic, then we need\nsome account of what is attractive about equilibria in games even when\nno analysis can identify them by taming all uncertainty in such a way\nthat it can be represented as pure risk. To make reference again to\nLewis’s topic, when human language developed there was no\nexternal referee to care about and arrange for Pareto-efficiency by\nproviding focal points for coordination. Yet somehow people agreed,\nwithin linguistic communities, to use roughly the same words and\nconstructions to say similar things. It seems unlikely that any\nexplicit, deliberate strategizing on anyone’s part played a role\nin these processes. Nevertheless, game theory has turned out to\nfurnish the essential concepts for understanding stabilization of\nlanguages. This is a striking point of support for Gintis’s\noptimism about the reach of game theory. To understand it, we must\nextend our attention to evolutionary games. \nGame theory has been fruitfully applied in evolutionary biology, where\nspecies and/or genes are treated as players, since pioneering work by\n Maynard Smith (1982)\n and his collaborators. Evolutionary (or dynamic) game theory\nnow constitutes a significant new mathematical extension applicable to\nmany settings apart from the biological.\n Skyrms (1996)\n uses evolutionary game theory to try to answer questions Lewis could\nnot even ask, about the conditions under which language, concepts of\njustice, the notion of private property, and other non-designed,\ngeneral phenomena of interest to philosophers would be likely to\narise. What is novel about evolutionary game theory is that moves are\nnot chosen through deliberation by the individual agents. Instead,\nagents are typically hard-wired with particular strategies, and\nsuccess for a strategy is defined in terms of the number of copies of\nitself that it will leave to play in the games of succeeding\ngenerations, given a population in which other strategies with which\nit acts are distributed at particular frequencies. In this kind of\nproblem setting, the strategies themselves are the players, and\nindividuals who play these strategies are their mere executors who\nreceive the immediate-run costs and benefits associated with\noutcomes. \nThe discussion here will closely follow Skyrms’s. We begin by\nintroducing the replicator dynamics. Consider first how\nnatural selection works to change lineages of animals, modifying,\ncreating and destroying species. The basic mechanism is\ndifferential reproduction. Any animal with heritable\nfeatures that increase its expected number of offspring in a\ngiven environment will tend to leave more offspring than others so\nlong as the environment remains relatively stable. These offspring\nwill be more likely to inherit the features in question. Therefore,\nthe proportion of these features in the population will gradually\nincrease as generations pass. Some of these features may go to\nfixation, that is, eventually take over the entire population\n(until the environment changes). \nHow does game theory enter into this? Often, one of the most important\naspects of an organism’s environment will be the behavioural\ntendencies of other organisms. We can think of each lineage as\n‘trying’ to maximize its reproductive fitness (= future\nfrequencies of its distinctive genetic structures) through finding\nstrategies that are optimal given the strategies of other lineages. So\nevolutionary theory is another domain of application for\nnon-parametric analysis. \nIn evolutionary game theory, we no longer think of individuals as\nchoosing strategies as they move from one game to another. This is\nbecause our interests are different. We’re now concerned less\nwith finding the equilibria of single games than with discovering\nwhich equilibria are stable, and how they will change over time. So we\nnow model the strategies themselves as playing against each\nother. One strategy is ‘better’ than another if it is\nlikely to leave more copies of itself in the next generation, when the\ngame will be played again. We study the changes in distribution of\nstrategies in the population as the sequence of games unfolds. \nFor evolutionary game theory, we introduce a new equilibrium concept,\ndue to\n Maynard Smith (1982).\n A set of strategies, in some particular proportion (e.g., 1/3:2/3,\n1/2:1/2, 1/9:8/9, 1/3:1/3:1/6:1/6—always summing to 1) is at an\nESS (Evolutionary Stable Strategy) equilibrium just in case\n(1) no individual playing one strategy could improve its reproductive\nfitness by switching to one of the other strategies in the proportion,\nand (2) no mutant playing a different strategy altogether could\nestablish itself (‘invade’) in the population. \nThe principles of evolutionary game theory are best explained through\nexamples. Skyrms begins by investigating the conditions under which a\nsense of justice—understood for purposes of his specific\nanalysis as a disposition to view equal divisions of resources as fair\nunless efficiency considerations suggest otherwise in special\ncases—might arise. He asks us to consider a population in which\nindividuals regularly meet each other and must bargain over resources.\nBegin with three types of individuals: \nEach single encounter where the total demands sum to 100% is\na NE of that individual game. Similarly, there can be many dynamic\nequilibria. Suppose that Greedies demand 2/3 of the resource and\nModests demand 1/3. Then, given random pairing for interaction, the\nfollowing two proportions are ESSs:  \nNotice that equilibrium (i) is inefficient, since the average payoff\nacross the whole population is smaller. However, just as inefficient\noutcomes can be NE of static games, so they can be ESSs of\nevolutionary ones.  \nWe refer to equilibria in which more than one strategy occurs as\npolymorphisms. In general, in Skyrms’s game, any\npolymorphism in which Greedy demands x and Modest demands\n1−x is an ESS. The question that interests the student\nof justice concerns the relative likelihood with which these different\nequilibria arise. \nThis depends on the proportions of strategies in the original\npopulation state. If the population begins with more than one Fairman,\nthen there is some probability that Fairmen will encounter each other,\nand get the highest possible average payoff. Modests by themselves do\nnot inhibit the spread of Fairmen; only Greedies do. But Greedies\nthemselves depend on having Modests around in order to be viable. So\nthe more Fairmen there are in the population relative to\npairs of Greedies and Modests, the better Fairmen do on\naverage. This implies a threshold effect. If the proportion of Fairmen\ndrops below 33%, then the tendency will be for them to fall to\nextinction because they don’t meet each other often enough. If\nthe population of Fairmen rises above 33%, then the tendency will be\nfor them to rise to fixation because their extra gains when they meet\neach other compensates for their losses when they meet Greedies. You\ncan see this by noticing that when each strategy is used by 33% of the\npopulation, all have an expected average payoff of 1/3. Therefore, any\nrise above this threshold on the part of Fairmen will tend to push\nthem towards fixation. \nThis result shows that and how, given certain relatively general\nconditions, justice as we have defined it can arise\ndynamically. The news for the fans of justice gets more cheerful still\nif we introduce correlated play. \nThe model we just considered assumes that strategies are not\ncorrelated, that is, that the probability with which every strategy\nmeets every other strategy is a simple function of their relative\nfrequencies in the population. We now examine what happens in our\ndynamic resource-division game when we introduce correlation. Suppose\nthat Fairmen have a slight ability to distinguish and seek out other\nFairmen as interaction partners. In that case, Fairmen on average do\nbetter, and this must have the effect of lowering their threshold for\ngoing to fixation. \nAn evolutionary game modeler studies the effects of correlation and\nother parametric constraints by means of running large computer\nsimulations in which the strategies compete with one another, round\nafter round, in the virtual environment. The starting proportions of\nstrategies, and any chosen degree of correlation, can simply be set in\nthe programme. One can then watch its dynamics unfold over time, and\nmeasure the proportion of time it stays in any one equilibrium. These\nproportions are represented by the relative sizes of the basins of\nattraction for different possible equilibria. Equilibria are\nattractor points in a dynamic space; a basin of attraction for each\nsuch point is then the set of points in the space from which the\npopulation will converge to the equilibrium in question. \nIn introducing correlation into his model, Skyrms first sets the\ndegree of correlation at a very small .1. This causes the basin of\nattraction for equilibrium (i) to shrink by half. When the degree of\ncorrelation is set to .2, the polymorphic basin reduces to the point\nat which the population starts in the polymorphism. Thus very small\nincreases in correlation produce large proportionate increases in the\nstability of the equilibrium where everyone plays Fairman. A small\namount of correlation is a reasonable assumption in most populations,\ngiven that neighbours tend to interact with one another and to mimic\none another (either genetically or because of tendencies to\ndeliberately copy each other), and because genetically and culturally\nsimilar animals are more likely to live in common environments. Thus\nif justice can arise at all it will tend to be dominant and\nstable. \nMuch of political philosophy consists in attempts to produce deductive\nnormative arguments intended to convince an unjust agent that she has\nreasons to act justly. Skyrms’s analysis suggests a quite\ndifferent approach. Fairman will do best of all in the dynamic game if\nhe takes active steps to preserve correlation. Therefore, there is\nevolutionary pressure for both moral approval of justice and\njust institutions to arise. Most people may think that\n50–50 splits are ‘fair’, and worth maintaining by\nmoral and institutional reward and sanction, because we are\nthe products of a dynamic game that promoted our tendency to think\nthis way. \nThe topic that has received most attention from evolutionary game\ntheorists is altruism, defined as any behaviour by an\norganism that decreases its own expected fitness in a single\ninteraction but increases that of the other interactor. It is arguably\ncommon in nature. How can it arise, however, given Darwinian\ncompetition? \nSkyrms studies this question using the dynamic Prisoner’s\nDilemma as his example. This is simply a series of PD games played in\na population, some of whose members are defectors and some of whom are\ncooperators. Payoffs, as always in evolutionary games, are measured in\nterms of expected numbers of copies of each strategy in future\ngenerations. \nLet U(A) be the average fitness of strategy\nA in the population. Let U be the average\nfitness of the whole population. Then the proportion of strategy\nA in the next generation is just the ratio\nU(A)/U. So if A\nhas greater fitness than the population average A increases.\nIf A has lower fitness than the population average then\nA decreases. \nIn the dynamic PD where interaction is random (i.e., there’s no\ncorrelation), defectors do better than the population average as long\nas there are cooperators around. This follows from the fact that, as\nwe saw in\n Section 2.4,\n defection is always the dominant strategy in a single game. 100%\ndefection is therefore the ESS in the dynamic game without\ncorrelation, corresponding to the NE in the one-shot static PD. \nHowever, introducing the possibility of correlation radically changes\nthe picture. We now need to compute the average fitness of a strategy\ngiven its probability of meeting each other possible\nstrategy. In the evolutionary PD, cooperators whose probability\nof meeting other cooperators is high do better than defectors whose\nprobability of meeting other defectors is high. Correlation thus\nfavours cooperation. \nIn order to be able to say something more precise about this\nrelationship between correlation and cooperation (and in order to be\nable to relate evolutionary game theory to issues in decision theory,\na matter falling outside the scope of this article), Skyrms introduces\na new technical concept. He calls a strategy adaptively\nratifiable if there is a region around its fixation point in the\ndynamic space such that from anywhere within that region it will go to\nfixation. In the evolutionary PD, both defection and cooperation are\nadaptively ratifiable. The relative sizes of basins of attraction are\nhighly sensitive to the particular mechanisms by which correlation is\nachieved. To illustrate this point, Skyrms builds several\nexamples. \nOne of Skyrms’s models introduces correlation by means of a\nfilter on pairing for interaction. Suppose that in round 1 of\na dynamic PD individuals inspect each other and interact, or not,\ndepending on what they find. In the second and subsequent rounds, all\nindividuals who didn’t pair in round 1 are randomly paired. In\nthis game, the basin of attraction for defection is large\nunless there is a high proportion of cooperators in round\none. In this case, defectors fail to pair in round 1, then get paired\nmostly with each other in round 2 and drive each other to extinction.\nA model which is more interesting, because its mechanism is less\nartificial, does not allow individuals to choose their partners, but\nrequires them to interact with those closest to them. Because of\ngenetic relatedness (or cultural learning by copying) individuals are\nmore likely to resemble their neighbours than not. If this (finite)\npopulation is arrayed along one dimension (i.e., along a line), and\nboth cooperators and defectors are introduced into positions along it\nat random, then we get the following dynamics. Isolated cooperators\nhave lower expected fitness than the surrounding defectors and are\ndriven locally to extinction. Members of groups of two cooperators\nhave a 50% probability of interacting with each other, and a 50%\nprobability of each interacting with a defector. As a result, their\naverage expected fitness remains smaller than that of their\nneighbouring defectors, and they too face probable extinction. Groups\nof three cooperators form an unstable point from which both extinction\nand expansion are equally likely. However, in groups of four or more\ncooperators at least one encounter of a cooperator with a cooperator\nsufficient to at least replace the original group is guaranteed. Under\nthis circumstance, the cooperators as a group do better than the\nsurrounding defectors and increase at their expense. Eventually\ncooperators go almost to fixation—but nor quite. Single\ndefectors on the periphery of the population prey on the cooperators\nat the ends and survive as little ‘criminal communities’.\nWe thus see that altruism can not only be maintained by the dynamics\nof evolutionary games, but, with correlation, can even spread and\ncolonize originally non-altruistic populations. \nDarwinian dynamics thus offers qualified good news for cooperation.\nNotice, however, that this holds only so long as individuals are stuck\nwith their natural or cultural programming and can’t re-evaluate\ntheir utilities for themselves. If our agents get too smart and\nflexible, they may notice that they’re in PDs and would each be\nbest off defecting. In that case, they’ll eventually drive\nthemselves to extinction—unless they develop stable, and\neffective, moral norms that work to reinforce cooperation. But, of\ncourse, these are just what we would expect to evolve in populations\nof animals whose average fitness levels are closely linked to their\ncapacities for successful social cooperation. Even given this, these\npopulations will go extinct unless they care about future generations\nfor some reason. But there’s no non-sentimental reason that\ndoesn’t already presuppose altruistic morality as to why agents\nshould care about future generations if each new generation\nwholly replaces the preceding one at each change of cohorts. For this\nreason, economists use ‘overlapping generations’ models\nwhen modeling intertemporal distribution games. Individuals in\ngeneration 1 who will last until generation 5 save resources for the\ngeneration 3 individuals with whom they’ll want to cooperate;\nand by generation 3 the new individuals care about generation 6; and\nso on. \n\n Gintis (2009a)\n argues that when we set out to use evolutionary game theory to unify\nthe behavioral sciences, we should begin by using it to unify game\ntheory itself. We have pointed out at several earlier points in the\npresent article that NE and SPE are problematic solution concepts in\nmany applications where explicit institutional rules are missing\nbecause agents only have incentives to play NE or SPE to the extent\nthat they are confident that other agents will do likewise. To the\nextent that agents do not have such confidence — and this, by\nthe way, is itself an insight due to game theory — what should\nbe predicted is general disorder and social confusion. Gintis shows in\ndetail how the key to this problem is the existence of what he calls a\n‘choreographer’. By this he means some exogenous element\nthat informs agents about which equilibrium strategies they should\nexpect others to play. As discussed in\n Section 6,\n cultural norms are probably the most important choreographers for\npeople. Interesting utility functions that incorporate norms of the\nrelevant sort are extensively studied in\n Bicchieri (2006).\n In this context, Gintis demonstrates a further unifying element of\ngreat importance: if agents attach positive utility to following the\nchoreographer’s suggestions (that is, to being strategically\ncorrelated with others for the sheer sake of it), then wherever\ncompeting potential payoffs do not overwhelm this incentive, agents\ncan also be expected to consistently estimate Bayesian priors, and\nthus arrive at equilibria-in-beliefs, as discussed in\n Section 3.1,\n in games of imperfect information. Finally, as discussed in\n Section 5,\n Conditional Game Theory promises to provide the resources for\nmodeling the endogenous emergence of the choreographer within the\ndynamics of games. \nIn light of this, when we wonder about the value of game-theoretic\nmodels in application to human behavior outside of well-structured\nmarkets, much hinges on what we take to be plausible and empirically\nvalidated sources of people’s incentives to be coordinated with\none another. This has been a subject of extensive recent debate, which\nwe will review in\n Section 8.3\n below. \nIn earlier sections, we reviewed some problems that arise from\ntreating classical (non-evolutionary) game theory as a normative\ntheory that tells people what they ought to do if they wish to be\nrational in strategic situations. The difficulty, as we saw, is that\nthere seems to be no one solution concept we can unequivocally\nrecommend for all situations, particularly where agents have private\ninformation. However, in the previous section we showed how appeal to\nevolutionary foundations sheds light on conditions under which utility\nfunctions that have been explicitly worked out can plausibly be\napplied to groups of people, leading to game-theoretic models with\nplausible and stable solutions. So far, however, we have not reviewed\nany actual empirical evidence from behavioral observations or\nexperiments. Has game theory indeed helped empirical researchers make\nnew discoveries about behavior (human or otherwise)? If so, what in\ngeneral has the content of these discoveries been? \nIn addressing these questions, an immediate epistemological issue\nconfronts us. There is no way of applying game theory ‘all by\nitself’, independently of other modelling technologies. Using\nterminology standard in the philosophy of science, one can test a\ngame-theoretic model of a phenomenon only in tandem with\n‘auxiliary assumptions’ about the phenomenon in question.\nAt least, this follows if one is strict about treating game theory\npurely as mathematics, with no empirical content of its own. In one\nsense, a theory with no empirical content is never open to testing at\nall; one can only worry about whether the axioms on which the theory\nis based are mutually consistent. A mathematical theory can\nnevertheless be evaluated with respect to empirical\nusefulness. One kind of philosophical criticism that has\nsometimes been made of game theory, interpreted as a mathematical tool\nfor modelling behavioral phenomena, is that its application always or\nusually requires resort to false, misleading or badly simplistic\nassumptions about those phenomena. We would expect this criticism to\nhave different degrees of force in different contexts of application,\nas the auxiliary assumptions vary. \nSo matters turn out. There is no interesting domain in which\napplications of game theory have been completely uncontroversial.\nHowever, there has been generally easier consensus on how to use game\ntheory (both classical and evolutionary) to understand non-human\nanimal behavior than on how to deploy it for explanation and\nprediction of the strategic activities of people. Let us first briefly\nconsider philosophical and methodological issues that have arisen\naround application of game theory in non-human biology, before\ndevoting fuller attention to game-theoretic social science. \nThe least controversial game-theoretic modelling has applied the\nclassical form of the theory to consideration of strategies by which\nnon-human animals seek to acquire the basic resource relevant to their\nevolutionary tournament: opportunities to produce offspring that are\nthemselves likely to reproduce. In order to thereby maximize their\nexpected fitness, animals must find optimal trade-offs among various\nintermediate goods, such as nutrition, security from predation and\nability to out-compete rivals for mates. Efficient trade-off points\namong these goods can often be estimated for particular species in\nparticular environmental circumstances, and, on the basis of these\nestimations, both parametric and non-parametric equilibria can be\nderived. Models of this sort have an impressive track record in\npredicting and explaining independent empirical data on such strategic\nphenomena as competitive foraging, mate selection, nepotism, sibling\nrivalry,herding, collective anti-predator vigilance and signaling,\nreciprocal grooming, and interspecific mutuality (symbiosis). (For\nexamples see\n Krebs and Davies 1984,\n Bell 1991,\n Dugatkin and Reeve 1998,\n Dukas 1998, and\n Noe, van Hoof and Hammerstein 2001.)\n On the other hand, as\n Hammerstein (2003)\n observes, reciprocity, and its exploitation and metaexploitation, are\nmuch more rarely seen in social non-human animals than game-theoretic\nmodeling would lead us to anticipate. One explanation for this\nsuggested by Hammerstein is that non-human animals typically have less\nability to restrict their interaction partners than do people. Our\ndiscussion in the previous section of the importance of correlation\nfor stabilizing game solutions lends theoretical support to this\nsuggestion. \nWhy has classical game theory helped to predict non-human animal\nbehavior more straightforwardly than it has done most human behavior?\nThe answer is presumed to lie in different levels of complication\namongst the relationships between auxiliary assumptions and phenomena.\n Ross (2005a)\n offers the following account. Utility-maximization and\nfitness-maximization problems are the domain of economics. Economic\ntheory identifies the maximizing units—economic\nagents—with unchanging preference fields. Identification of\nwhole biological individuals with such agents is more plausible the\nless cognitively sophisticated the organism. Thus insects (for\nexample) are tailor-made for easy application of Revealed Preference\nTheory (see\n Section 2.1).\n As nervous systems become more complex, however, we encounter animals\nthat learn. Learning can cause a sufficient degree of permanent\nmodification in an animal’s behavioral patterns that we can\npreserve the identification of the biological individual with a single\nagent across the modification only at the cost of explanatory\nemptiness (because assignments of utility functions become\nincreasingly ad hoc). Furthermore, increasing complexity confounds\nsimple modeling on a second dimension: cognitively sophisticated\nanimals not only change their preferences over time, but are governed\nby distributed control processes that make them sites of competition\namong internal agents\n (Schelling 1980;\n Ainslie 1992,\n Ainslie 2001).\n Thus they are not straightforward economic agents even at a\ntime. In setting out to model the behavior of people using any part of\neconomic theory, including game theory, we must recognize that the\nrelationship between any given person and an economic agent we\nconstruct for modeling purposes will always be more complicated than\nsimple identity. \nThere is no sudden crossing point at which an animal becomes too\ncognitively sophisticated to be modeled as a single economic agent,\nand for all animals (including humans) there are contexts in which we\ncan usefully ignore the synchronic dimension of complexity. However,\nwe encounter a phase shift in modeling dynamics when we turn from\nasocial animals to non-eusocial social ones. (This refers to animals\nthat are social but that don’t, like ants, bees, wasps, termites\nand naked mole rats, achieve cooperation thanks to fundamental changes\nin their population genetics that make individuals within groups into\nnear clones. Some known instances are parrots, corvids, bats, rats,\ncanines, hyenas, pigs, raccoons, otters, elephants, hyraxes,\ncetaceans, and primates.) In their cases stabilization of internal\ncontrol dynamics is partly located outside the individuals,\nat the level of group dynamics. With these creatures, modeling an\nindividual as an economic agent, with a single comprehensive utility\nfunction, is a drastic idealization, which can only be done with the\ngreatest methodological caution and attention to specific contextual\nfactors relevant to the particular modeling exercise. Applications of\ngame theory here can only be empirically adequate to the extent that\nthe economic modeling is empirically adequate. \nH. sapiens is the extreme case in this respect. Individual\nhumans are socially controlled to an extreme degree by comparison with\nmost other non-eusocial species. At the same time, their great\ncognitive plasticity allows them to vary significantly between\ncultures. People are thus the least straightforward economic agents\namong all organisms. (It might thus be thought ironic that they were\ntaken, originally and for many years, to be the exemplary instances of\neconomic agency, on account of their allegedly superior\n‘rationality’.) We will consider the implications of this\nfor applications of game theory below. \nFirst, however, comments are in order concerning the empirical\nadequacy of evolutionary game theory to explain and predict\ndistributions of strategic dispositions in populations of agents. Such\nmodeling is applied both to animals as products of natural selection\n (Hofbauer and Sigmund 1998),\n and to non-eusocial social animals (but especially humans) as\nproducts of cultural selection\n (Young 1998).\n There are two main kinds of auxiliary assumptions one must justify,\nrelative to a particular instance at hand, in constructing such\napplications. First, one must have grounds for confidence that the\ndispositions one seeks to explain are (either biological or cultural,\nas the case may be) adaptations—that is, dispositions\nthat were selected and are maintained because of the way in which they\npromote their own fitness or the fitness of the wider system, rather\nthan being accidents or structurally inevitable byproducts of other\nadaptations. (See\n Dennett 1995\n for a general discussion of this issue.) Second, one must be able to\nset the modeling enterprise in the context of a justified set of\nassumptions about interrelationships among nested evolutionary\nprocesses on different time scales. (For example, in the case of a\nspecies with cultural dynamics, how does slow genetic evolution\nconstrain fast cultural evolution? How does cultural evolution feed\nback into genetic evolution, if it feeds back at all? For a masterful\ndiscussion of these issues, see\n Sterelny 2003.)\n Conflicting views over which such assumptions should be made about\nhuman evolution are the basis for lively current disputes in the\nevolutionary game-theoretic modeling of human behavioral dispositions\nand institutions. This is where issues in evolutionary game theory\nmeet issues in the booming field of behavioral-experimental\ngame theory. I will therefore first describe the second field before\ngiving a sense of the controversies just alluded to, which now\nconstitute the liveliest domain of philosophical argument in the\nfoundations of game theory and its applications. \nEconomists have been testing theories by running laboratory\nexperiments with human and other animal subjects since pioneering work\nby\n Thurstone (1931).\n In recent decades, the volume of such work has become positively\ngigantic. The vast majority of it sets subjects in microeconomic\nproblem environments that are imperfectly competitive. Since this is\nprecisely the condition in which microeconomics collapses into game\ntheory, most experimental economics has been experimental game theory.\nIt is thus difficult to distinguish between experimentally motivated\nquestions about the empirical adequacy of microeconomic theory and\nquestions about the empirical adequacy of game theory. \nWe can here give only a broad overview of an enormous and complicated\nliterature. Readers are referred to critical surveys in\n Kagel and Roth (1995),\n Camerer (2003),\n Samuelson (2005),\n and\n Guala (2005).\n A useful high-level principle for sorting the literature indexes it\nto the different auxiliary assumptions with which game-theoretic\naxioms are applied. It is often said in popular presentations (e.g.,\n Ormerod 1994)\n that the experimental data generally refute the hypothesis that\npeople are rational economic agents. Such claims are too imprecise to\nbe sustainable interpretations of the results. All data are consistent\nwith the view that people are approximate economic agents, at\nleast for stretches of time long enough to permit game-theoretic\nanalysis of particular scenarios, in the minimal sense that their\nbehavior can be modeled compatibly with Revealed Preference Theory\n(see\n Section 2.1).\n However, RPT makes so little in the way of empirical demands that\nthis is not nearly as surprising as many non-economists suppose\n (Ross 2005a).\n What is really at issue in many of the debates around the general\ninterpretation of experimental evidence is the extent to which people\nare maximizers of expected utility. As we saw in\n Section 3,\n expected utility theory (EUT) is generally applied in tandem with\ngame theory in order to model situations involving uncertainty —\nwhich is to say, most situations of interest in behavioral science.\nHowever, a variety of alternative structural models of utility lend\nthemselves to Von Neumann-Morgenstern cardinalization of preferences\nand are definable in terms of subsets of the\n Savage (1954)\n axioms of subjective utility. The empirical usefulness of game theory\nwould be called into question only if we thought that people’s\nbehavior is not generally describable by means of cardinal vNMufs. \nWhat the experimental literature truly appears to show is a world of\nbehavior that is usually noisy from the theorist’s point of\nview. The noise in question arises from substantial heterogeneity,\nboth among people and among (person, situation) vectors. There is no\nsingle structural utility function such that all people act so as to\nmaximize a function of that structure in all circumstances. Faced with\nwell-learned problems in contexts that are not unduly demanding, or\nthat are highly institutionally structured people often behave like\nexpected utility maximizers. For general reviews of theoretical issues\nand evidence, see\n Smith (2008)\n and\n Binmore (2007).\n For an extended sequence of examples of empirical studies, see the\nso-called ‘continuous double auction’ experiments\ndiscussed in\n Plott and Smith 1978\n and Smith\n 1962,\n 1964,\n 1965,\n 1976,\n 1982.\n As a result, classical game theory can be used in such domains with\nhigh reliability to predict behavior and implement public policy, as\nis demonstrated by the dozens of extremely successful government\nauctions of utilities and other assets designed by game theorists to\nincrease public revenue\n (Binmore and Klemperer 2002). \nIn other contexts, interpreting people’s behavior as\ngenerally expected-utility maximizing requires undue violence\nto the need for generality in theory construction. We get better\nprediction using fewer case-specific restrictions if we suppose that\nsubjects are maximizing according to one or (typically) more\nof several alternatives (which will not be described here because they\nare not directly about game theory): cumulative prospect theory\n (Tversky and Kahneman 1992),\n or alpha-nu utility theory\n (Chew and MacCrimmon 1979),\n or rank-dependent utility theory\n (Quiggin 1982,\n Yaari 1987). (The last alternative in fact denotes a family of\nalternative specifications. One of these, the specification of\n Prelec (1998),\n has emerged in an accumulating mass of empirical estimations as the\nstatistically most common human utility function.)\n Harrison and Rutstrom (2008)\n show how to design and code maximum likelihood mixture\nmodels, which allow an empirical modeler to apply a range of\nthese decision functions to a single set of choice data. The resulting\nanalysis identifies the proportion of the total choice set best\nexplained by each model in the mixture.\n Andersen et al (2014)\n take this approach to the current state of the art, demonstrating the\nempirical value of including a model of non-maximizing psychological\nprocesses in a mixture along with maximizing economic models. This new\neffective flexibility with respect to the decision modeling that can\nbe deployed in empirical applications of game theory relieves most\npressure to seek adjustments in the game theoretic structures\nthemselves. Thus it fits well with the interpretation of game theory\nas part of the behavioral scientist’s mathematical toolkit,\nrather than as a first-order empirical model of human psychology. \nA more serious threat to the usefulness of game theory is evidence of\nsystematic reversal of preferences, in both humans and other animals.\nThis is more serious both because it extends beyond the human case,\nand because it challenges Revealed Preference Theory (RPT) rather than\njust unnecessarily rigid commitment to EUT. As explained in\n Section 2.1,\n RPT, unlike EUT, is among the axiomatic foundations of game theory\ninterpreted non-psychologically. (Not all writers agree that apparent\npreference reversal phenomena threaten RPT rather than EUT; but see\nthe discussions in\n Camerer (1995),\n pp. 660–665, and\n Ross (2005a),\n pp. 177–181.) A basis for preference reversals that seems to be\ncommon in animals with brains is hyperbolic discounting of the\nfuture\n (Strotz 1956,\n Ainslie 1992). This is the phenomenon whereby agents discount future\nrewards more steeply in close temporal distances from the current\nreference point than at more remote temporal distances. This is best\nunderstood by contrast with the idea found in most traditional\neconomic models of exponential discounting, in which there is\na linear relationship between the rate of change in the distance to a\npayoff and the rate at which the value of the payoff from the\nreference point declines. The figure below shows exponential and\nhyperbolic curves for the same interval from a reference point to a\nfuture payoff. The bottom one graphs the hyperbolic function; the\nbowed shape results from the change in the rate of discounting. Figure 15 \nA result of this is that, as later prospects come closer to the point\nof possible consumption, people and other animals will sometimes spend\nresources undoing the consequences of previous actions that also cost\nthem resources. For example: deciding today whether to mark a pile of\nundergraduate essays or watch a baseball game, I procrastinate,\ndespite knowing that by doing so I put out of reach some even more fun\npossibility that might come up for tomorrow (when there’s an\nequally attractive ball game on if the better option doesn’t\narise). So far, this can be accounted for in a way that preserves\nconsistency of preferences: if the world might end tonight, with a\ntiny but nonzero probability, then there’s some level of risk\naversion at which I’d rather leave the essays unmarked. The\nfigure below compares two exponential discount curves, the lower one\nfor the value of the game I watch before finishing my marking, and the\nhigher one for the more valuable game I enjoy after completing the\njob. Both have higher value from the reference point the closer they\nare to it; but the curves do not cross, so my revealed preferences are\nconsistent over time no matter how impatient I might be. Figure 16 \nHowever, if I bind myself against procrastination by buying a ticket\nfor tomorrow’s game, when in the absence of the awful task I\nwouldn’t have done so, then I’ve violated intertemporal\npreference consistency. More vividly, had I been in a position to\nchoose last week whether to procrastinate today, I’d have chosen\nnot to. In this case, my discount curve drawn from the reference point\nof last week crosses the curve drawn from the perspective of today,\nand my preferences reverse. The figure below shows this situation. Figure 17 \nThis phenomenon complicates applications of classical game theory to\nintelligent animals. However, it clearly doesn’t vitiate it\naltogether, since people (and other animals) often\ndon’t reverse their preferences. (If this weren’t\ntrue, the successful auction models and other s-called\n‘mechanism designs’ would be mysterious.) Interestingly,\nthe leading theories that aim to explain why hyperbolic discounters\nmight often behave in accordance with RPT themselves appeal to game\ntheoretic principles.\n Ainslie (1992,\n 2001) has produced an account of people as communities of\ninternal bargaining interests, in which subunits based on short-term,\nmedium-term and long-term interests face conflict that they must\nresolve because if they don’t, and instead generate an internal\nHobbesian breakdown\n (Section 1),\n outside agents who avoid the Hobbesian outcome can ruin them all. The\ndevice of the Hobbesian tyrant is unavailable to the brain. Therefore,\nits behavior (when system-level insanity is avoided) is a sequence of\nself-enforcing equilibria of the sort studied by game-theoretic public\nchoice literature on coalitional bargaining in democratic\nlegislatures. That is, the internal politics of the brain consists in\n‘logrolling’\n (Stratmann 1997).\n These internal dynamics are then partly regulated and stabilized by\nthe wider social games in which coalitions (people as wholes over\ntemporal subparts of their biographies) are embedded\n (Ross 2005a ,\n pp. 334–353). (For example: social expectations about\nsomeone’s role as a salesperson set behavioral equilibrium\ntargets for the logrolling processes in their brain.) This potentially\nadds further relevant elements to the explanation of why and how\nstable institutions with relatively transparent rules are key\nconditions that help people more closely resemble straightforward\neconomic agents, such that classical game theory finds reliable\napplication to them as entire units. \nOne important note of caution is in order here. Much of the recent\nbehavioral literature takes for granted that temporally inconsistent\ndiscounting is the standard or default case for people. However,\n Andersen et al (2008)\n show empirically that this arises from (i) assuming that groups of\npeople are homogenous with respect to which functional forms best\ndescribe their discounting behavior, and (ii) failure to independently\nelicit and control for people’s differing levels of risk\naversion in estimating their discount functions. In a range of\npopulations that have been studied with these two considerations in\nmind, data suggest that temporally consistent discounting describes\nsubstantially higher proportions of choices than does temporally\ninconsistent choices. Over-generalization of hyperbolic discounting\nmodels should thus be avoided.  \nThe idea that game theory can find novel application to the internal\ndynamics of brains, as suggested in the previous section, has been\ndeveloped from independent motivations by the research program known\nas neuroeconomics\n (Montague and Berns 2002,\n Glimcher 2003,\n Ross 2005a,\n pp. 320–334,\n Camerer, Loewenstein and Prelec 2005).\n Thanks to new non-invasive scanning technologies, especially\nfunctional magnetic resonance imaging (fMRI), it has recently become\npossible to study synaptic activity in working brains while they\nrespond to controlled cues. This has allowed a new path of\naccess—though still a highly indirect one\n (Harrison and Ross 2010)—\n to the brain’s computation of expected values of rewards, which\nare (naturally) taken to play a crucial role in determining behavior.\nEconomic theory is used to frame the derivation of the functions\nmaximized by synaptic-level computation of these expected values;\nhence the name ‘neuroeconomics’. \nGame theory plays a leading role in neuroeconomics at two levels.\nFirst, game theory has been used to predict the computations that\nindividual neurons and groups of neurons serving the reward system\nmust perform. In the best publicized example,\n Glimcher (2003)\n and colleagues have fMRI-scanned monkeys they had trained to play\nso-called ‘inspection games’ against computers. In an\ninspection game, one player faces a series of choices either to work\nfor a reward, in which case he is sure to receive it, or to perform\nanother, easier action (“shirking”), in which case he will\nreceive the reward only if the other player (the\n“inspector”) is not monitoring him. Assume that the first\nplayer’s (the “worker’s”) behavior reveals a\nutility function bounded on each end as follows: he will work on every\noccasion if the inspector always monitors and he will shirk on every\noccasion if the inspector never monitors. The inspector prefers to\nobtain the highest possible amount of work for the lowest possible\nmonitoring rate. In this game, the only NE for both players are in\nmixed strategies, since any pattern in one player’s strategy\nthat can be detected by the other can be exploited. For any given pair\nof specific utility functions for the two players meeting the\nconstraints described above, any pair of strategies in which, on each\ntrial, either the worker is indifferent between working and shirking\nor the inspector is indifferent between monitoring and not monitoring,\nis a NE. \nApplying inspection game analyses to pairs or groups of agents\nrequires us to have either independently justified their\nutility functions over all variables relevant to their play, in which\ncase we can define NE and then test to see whether they successfully\nmaximize expected utility; or to assume that they maximize\nexpected utility, or obey some other rule such as a matching function,\nand then infer their utility functions from their behavior. Either\nsuch procedure can be sensible in different empirical contexts. But\nepistemological leverage increases greatly if the utility function of\nthe inspector is exogenously determined, as it often is. (Police\nimplementing random roadside inspections to catch drunk drivers, for\nexample, typically have a maximum incidence of drunk driving assigned\nto them as a target by policy, and an exogenously set budget. These\ndetermine their utility function, given a distribution of preferences\nand attitudes to risk among the population of drivers.) In the case of\nGlimcher’s experiments the inspector is a computer, so its\nprogram is under experimental control and its side of the payoff\nmatrix is known. Proxies for the subjects’ expected utility, in\nthis case squirts of fruit juice for the monkeys, can be antecedently\ndetermined in parametric test settings. The computer is then\nprogrammed with the economic model of the monkeys, and can search the\ndata in their behavior in game conditions for exploitable patterns,\nvarying its strategy accordingly. With these variables fixed,\nexpected-utility-maximizing NE behavior by the monkeys can be\ncalculated and tested by manipulating the computer’s utility\nfunction in various runs of the game. \nMonkey behavior after training tracks NE very robustly (as does the\nbehavior of people playing similar games for monetary prizes;\n Glimcher 2003,\n pp. 307–308). Working with trained monkeys, Glimcher and\ncolleagues could then perform the experiments of significance here.\nWorking and shirking behaviors for the monkeys had been associated by\ntheir training with staring either to the right or to the left on a\nvisual display. In earlier experiments,\n Platt and Glimcher (1999)\n had established that, in parametric settings, as juice rewards varied\nfrom one block of trials to another, firing rates of each parietal\nneuron that controls eye movements could be trained to encode the\nexpected utility to the monkey of each possible movement relative to\nthe expected utility of the alternative movement. Thus\n“movements that were worth 0.4 ml of juice were represented\ntwice as strongly [in neural firing probabilities] as movements worth\n0.2 ml of juice” (p. 314). Unsurprisingly, when amounts of juice\nrewarded for each movement were varied from one block of trials to\nanother, firing rates also varied. \nAgainst this background, Glimcher and colleagues could investigate the\nway in which monkeys’ brains implemented the tracking of NE.\nWhen the monkeys played the inspection game against the computer, the\ntarget associated with shirking could be set at the optimal location,\ngiven the prior training, for a specific neuron under study, while the\nwork target would appear at a null location. This permitted Glimcher\nto test the answer to the following question: did the monkeys maintain\nNE in the game by keeping the firing rate of the neuron constant while\nthe actual and optimal behavior of the monkey as a whole varied? The\ndata robustly gave the answer ‘yes’. Glimcher reasonably\ninterprets these data as suggesting that neural firing rates, at least\nin this cortical region for this task, encode expected utility in both\nparametric and nonparametric settings. Here we have an apparent\nvindication of the empirical applicability of classical game theory in\na context independent of institutions or social conventions. \nFurther analysis pushed the hypothesis deeper. The computer playing\nInspector was presented with the same sequence of outcomes as its\nmonkey opponent had received on the previous day’s play, and for\neach move was asked to assess the relative expected values of the\nshirking and working actions available on the next move. Glimcher\nreports a positive correlation between small fluctuations around the\nstable NE firing rates in the individual neuron and the expected\nvalues estimated by the computer trying to track the same NE. Glimcher\ncomments on this finding as follows: \nThus we find game theory reaching beyond its traditional role as a\ntechnology for framing high-level constraints on evolutionary dynamics\nor on behavior by well-informed agents operating in institutional\nstraitjackets. In Glimcher’s hands, it is used to directly model\nactivity in a monkey’s brain.\n Ross (2005a)\n argues that groups of neurons thus modeled should not be identified\nwith the sub-personal game-playing units found in Ainslie’s\ntheory of intra-personal bargaining described earlier; that would\ninvolve a kind of straightforward reduction that experience in the\nbehavioral and life sciences has taught us not to expect. This issue\nhas since arisen in a direct dispute between neuroeconomists over\nrival interpretations of fMRI observations of intertemporal choice and\ndiscounting\n (McClure et al. 2004),\n Glimcher et al. 2007). The weight of evidence so far favors the view that if\nit is sometimes useful to analyze people’s choices as equilibria\nin games amongst sub-personal agents, the sub-personal agents in\nquestion should not be identified with separate brain areas. The\nopposite interpretation is unfortunately still most common in less\nspecialized literature.  \nWe have now seen the first level at which neuroeconomics applies game\ntheory. A second level involves seeking conditioning variables in\nneural activity that might impact people’s choices of strategies\nwhen they play games. This has typically involved repeating protocols\nfrom the behavioral game theory literature with research subjects who\nare lying in fMRI scanners during play.\n Harrison (2008)\n and\n Ross (2008b)\n have argued for skepticism about the value of work of this kind,\nwhich involves various uncomfortably large leaps of inference in\nassociating the observed behavior with specific imputed neural\nresponses. It can also be questioned whether much generalizable new\nknowledge is gained to the extent that such associations can\nbe successfully identified. \nLet us provide an example of this kind of “game in a\nscanner”— that directly involves strategic interaction.\n King-Casas et al. (2005)\n took a standard protocol from behavioral game theory, the so-called\n‘trust’ game, and implemented it with subjects whose\nbrains were jointly scanned using a technology for linking the\nfunctional maps of their respective brains, known as\n‘hyperscanning’). This game involves two players. In its\nrepeated format as used in the King-Casas et al. experiment,\nthe first player is designated the ‘investor’ and the\nsecond the ‘trustee’. The investor begins with $20, of\nwhich she can keep any portion of her choice while investing the\nremainder with the trustee. In the trustee’s hands the invested\namount is tripled by the experimenter. The trustee may then return as\nmuch or as little of this profit to the investor as he deems fit. The\nprocedure is run for ten rounds, with players’ identities kept\nanonymous from one another. \nThis game has an infinite number of NE. Previous data from behavioral\neconomics are consistent with the claim that the modal NE in human\nplay approximates both players using\n‘Tit-for-tat’ strategies (see\n Section 4)\n modified by occasional defections to probe for information, and some\npost-defection cooperation that manifests (limited) toleration of such\nprobes. This is a very weak result, since it is compatible with a wide\nrange of hypotheses on exactly which variations of Tit-for-tat are\nused and sustained, and thus licenses no inferences about potential\ndynamics under different learning conditions, institutions, or\ncross-cultural transfers. \nWhen they ran this game under hyperscanning, the researchers\ninterpreted their observations as follows. Neurons in the\ntrustee’s caudate nucleus (generally thought to implement\ncomputations or outputs of midbrain dopaminergic systems) were thought\nto show strong response when investors benevolently reciprocated\ntrust—that is, responded to defection with increased generosity.\nAs the game progressed, these responses were believed to have shifted\nfrom being reactionary to being anticipatory. Thus reputational\nprofiles as predicted by classical game-theoretic models were inferred\nto have been constructed directly by the brain. A further aspect of\nthe findings not predictable by theoretical modeling alone, and which\npurely behavioral observation had not been sufficient to discriminate,\nwas taken to be that responses by the caudate neurons to malevolent\nreciprocity—that is, reduced generosity in response to\ncooperation—were significantly smaller in amplitude. This was\nhypothesized to be a mechanism by which the brain implements\nmodification of Tit-for-tat so as to prevent occasional defections for\ninformational probing from unraveling cooperation permanently.  \nThe advance in understanding for which practitioners of this style of\nneuroeconomics hope consists not in what it tells us about particular\ntypes of games, but rather in comparative inferences it facilitates\nabout the ways in which contextual framing influences people’s\nconjectures about which games they’re playing. fMRI or other\nkinds of probes of working brains might, it is conjectured, enable us\nto quantitatively estimate degrees of strategic surprise.\nReciprocally interacting expectations about surprise may themselves be\nsubject to strategic manipulation, but this is an idea that has barely\nbegun to be theoretically explored by game theorists (see\n Ross and Dumouchel 2004).\n The view of some neuroeconomists that we now have the prospect of\nempirically testing such new theories, as opposed to just\nhypothetically modeling them, has stimulated growth in this line of\nresearch. \nThe developments reviewed in the previous section bring us up to the\nmoving frontier of experimental / behavioral applications of classical\ngame theory. We can now return to the branch point left off several\nparagraphs back, where this stream of investigation meets that coming\nfrom evolutionary game theory. There is no serious doubt that, by\ncomparison to other non-eusocial animals —including our nearest\nrelatives, chimpanzees and bonobos—humans achieve prodigious\nfeats of coordination (see\n Section 4)\n (Tomasello et al. 2004). A lively controversy, with important philosophical\nimplications and fought on both sides with game-theoretic arguments,\ncurrently rages over the question of whether this capacity can be\nwholly explained by cultural adaptation, or is better explained by\ninference to a genetic change early in the career of H.\nsapiens. \nHenrich et al.\n (2004,\n 2005) have run a series of experimental games with\npopulations drawn from fifteen small-scale human societies in South\nAmerica, Africa, and Asia, including three groups of foragers, six\ngroups of slash-and-burn horticulturists, four groups of nomadic\nherders, and two groups of small-scale agriculturists. The games\n(Ultimatum, Dictator, Public Goods) they implemented all place\nsubjects in situations broadly resembling that of the Trust game\ndiscussed in the previous section. That is, Ultimatum and Public Goods\ngames are scenarios in which both social welfare and each\nindividual’s welfare are optimized (Pareto efficiency achieved)\nif and only if at least some players use strategies that are not\nsub-game perfect equilibrium strategies (see\n Section 2.6).\n In Dictator games, a narrowly selfish first mover would capture all\navailable profits. Thus in each of the three game types, SPE players\nwho cared only about their own monetary welfare would get outcomes\nthat would involve highly inegalitarian payoffs. In none of the\nsocieties studied by Henrich et al. (or any other society in\nwhich games of this sort have been run) are such outcomes observed.\nThe players whose roles are such that they would take away all but\nepsilon of the monetary profits if they and their partners played SPE\nalways offered the partners substantially more than epsilon, and even\nthen partners sometimes refused such offers at the cost of receiving\nno money. Furthermore, unlike the traditional subjects of experimental\neconomics—university students in industrialized\ncountries—Henrich et al.’s subjects did not even\nplay Nash equilibrium strategies with respect to monetary\npayoffs. (That is, strategically advantaged players offered larger\nprofit splits to strategically disadvantaged ones than was necessary\nto induce agreement to their offers.) Henrich et al.\ninterpret these results by suggesting that all actual people, unlike\n‘rational economic man’, value egalitarian outcomes to\nsome extent. However, their experiments also show that this extent\nvaries significantly with culture, and is correlated with variations\nin two specific cultural variables: typical payoffs to cooperation\n(the extent to which economic life in the society depends on\ncooperation with non-immediate kin) and aggregate market integration\n(a construct built out of independently measured degrees of social\ncomplexity, anonymity, privacy, and settlement size). As the values of\nthese two variables increase, game behavior shifts (weakly) in the\ndirection of Nash equilibrium play. Thus the researchers conclude that\npeople are genetically endowed with preferences for egalitarianism,\nbut that the relative weight of these preferences is programmable by\nsocial learning processes conditioned on local cultural cues. \nIn evaluating Henrich et al.’s interpretation of these\ndata, we should first note that no axioms of RPT, or of the various\nmodels of decision mentioned in\n Section 8.1,\n which are applied jointly with game theoretic modeling to human\nchoice data, specify or entail the property of narrow selfishness.\n(See\n Ross (2005a)\n ch. 4;\n Binmore (2005b)\n and\n (2009);\n and any economics or game theory text that lets the mathematics speak\nfor itself.) Orthodox game theory thus does not predict that people\nwill play SPE or NE strategies derived by treating monetary payoffs as\nequivalent to utility.\n Binmore (2005b)\n is therefore justified in criticizing Henrich et al for\nrhetoric suggesting that their empirical work embarrasses orthodox\ntheory.  \nThis is not to suggest that the anthropological interpretation of the\nempirical results should be taken as uncontroversial. Binmore\n (1994,\n 1998,\n 2005a,\n 2005b) has argued for many years, based on a wide range of\nbehavioral data, that when people play games with non-relatives they\ntend to learn to play Nash equilibrium with respect to utility\nfunctions that approximately correspond to income functions. As he\npoints out in\n Binmore (2005b),\n Henrich et al.’s data do not test this hypothesis for\ntheir small-scale societies, because their subjects were not exposed\nto the test games for the (quite long, in the case of the Ultimatum\ngame) learning period that theoretical and computational models\nsuggest are required for people to converge on NE. When people play\nunfamiliar games, they tend to model them by reference to games they\nare used to in everyday experience. In particular, they tend to play\none-shot laboratory games as though they were familiar\nrepeated games, since one-shot games are rare in normal\nsocial life outside of special institutional contexts. Many of the\ninterpretive remarks made by Henrich et al. are consistent\nwith this hypothesis concerning their subjects, though they\nnevertheless explicitly reject the hypothesis itself. What is\ncontroversial here—the issues of spin around\n‘orthodox’ theory aside—is less about what the\nparticular subjects in this experiment were doing than about what\ntheir behavior should lead us to infer about human evolution. \n\n Gintis (2004),\n (2009a) argues that data of the sort we have been discussing\nsupport the following conjecture about human evolution. Our ancestors\napproximated maximizers of individual fitness. Somewhere along the\nevolutionary line these ancestors arrived in circumstances where\nenough of them optimized their individual fitness by acting so as to\noptimize the welfare of their group\n (Sober and Wilson 1998)\n that a genetic modification went to fixation in the species: we\ndeveloped preferences not just over our own individual welfare, but\nover the relative welfare of all members of our communities, indexed\nto social norms programmable in each individual by cultural\nlearning. Thus the contemporary researcher applying game theory to\nmodel a social situation is advised to unearth her subjects’\nutility functions by (i) finding out what community (or communities)\nthey are members of, and then (ii) inferring the utility function(s)\nprogrammed into members of that community (communities) by studying\nrepresentatives of each relevant community in a range of games and\nassuming that the outcomes are coordinated equilibria. Since the\nutility functions are the dependent variables here, the games must be\nindependently determined. We can typically hold at least the strategic\nforms of the relevant games fixed, Gintis supposes, by virtue of (a)\nour confidence that people prefer egalitarian outcomes, all else being\nequal, to inegalitarian ones within the culturally evolved\n‘insider groups’ to which they perceive themselves as\nbelonging and (b) a requirement that game equilibria are drawn from\nstable attractors in plausible evolutionary game-theoretic models of\nthe culture’s historical dynamics. \nRequirement (b) as a constraint on game-theoretic modeling of general\nhuman strategic dispositions is no longer very controversial —\nor, at least, is no more controversial than the generic adaptationism\nin evolutionary anthropology of which it is one expression. However,\nsome commentators are skeptical of Gintis’s suggestion that\nthere was a genetic discontinuity in the evolution of human sociality.\n(For a cognitive-evolutionary anthropology that explicitly denies such\ndiscontinuity, see\n Sterelny 2003.)\n Based partly on such skepticism (but more directly on behavioral\ndata) Binmore\n (2005a,\n 2005b) resists modeling people as having built-in preferences\nfor egalitarianism. According to Binmore’s\n (1994,\n 1998,\n 2005a)\n model, the basic class of strategic problems facing non-eusocial\nsocial animals are coordination games. Human communities evolve\ncultural norms to select equilibria in these games, and many of these\nequilibria will be compatible with high levels of apparently\naltruistic behavior in some (but not all) games. Binmore argues that\npeople adapt their conceptions of fairness to whatever happen to be\ntheir locally prevailing equilibrium selection rules. However, he\nmaintains that the dynamic development of such norms must be\ncompatible, in the long run, with bargaining equilibria among\nself-regarding individuals. Indeed, he argues that as societies evolve\ninstitutions that encourage what Henrich et al. call\naggregate market integration (discussed above), their utility\nfunctions and social norms tend to converge on self-regarding economic\nrationality with respect to welfare. This does not mean that Binmore\nis pessimistic about the prospects for egalitarianism: he develops a\nmodel showing that societies of broadly self-interested bargainers can\nbe pulled naturally along dynamically stable equilibrium paths towards\nnorms of distribution corresponding to Rawlsian justice\n (Rawls 1971).\n The principal barriers to such evolution, according to Binmore, are\nprecisely the kinds of other-regarding preferences that conservatives\nvalorize as a way of discouraging examination of more egalitarian\nbargaining equilibria that are within reach along societies’\nequilibrium paths. \nResolution of this debate between Gintis and Binmore fortunately need\nnot wait upon discoveries about the deep human evolutionary past that\nwe may never have. The models make rival empirical predictions of some\ntestable phenomena. If Gintis is right then there are limits, imposed\nby the discontinuity in hominin evolution, on the extent to which\npeople can learn to be self-regarding. This is the main significance\nof the controversy discussed above over Henrich et\nal.’s interpretation of their field data. Binmore’s\nmodel of social equilibrium selection also depends, unlike\nGintis’s, on widespread dispositions among people to inflict\nsecond-order punishment on members of society who fail to sanction\nviolators of social norms.\n Gintis (2005)\n shows using a game theory model that this is implausible if\npunishment costs are significant. However,\n Ross (2008a)\n argues that the widespread assumption in the literature that\npunishment of norm-violation must be costly results from failure to\nadequately distinguish between models of the original evolution of\nsociality, on the one hand, and models of the maintenance and\ndevelopment of norms and institutions once an initial set of them has\nstabilized. Finally, Ross also points out that Binmore’s\nobjectives are as much normative as descriptive: he aims to show\negalitarians how to diagnose the errors in conservative\nrationalisations of the status quo without calling for revolutions\nthat put equilibrium path stability (and, therefore, social welfare)\nat risk. It is a sound principle in constructing reform proposals that\nthey should be ‘knave-proof’ (as Hume put it), that is,\nshould be compatible with less altruism than might prevail in\npeople. Thus, despite the fact that the majority of researchers\nworking on game-theoretic foundations of social organization presently\nappear to side with Gintis and the other members of the Henrich et\nal. team, Binmore’s alternative model has some strong\nconsiderations in its favor. Here, then, is another issue along the\nfrontier of game theory application awaiting resolution in the years\nto come. \nIn 2016 the Journal of Economic Perspectives published a\nsymposium on “What is Happening in Game Theory?” Each of\nthe participants noted independently that game theory has become so\ntightly entangled with microeconomic theory in general that the\nquestion becomes difficult to distinguish from inquiry into the moving\nfrontier of that entire sub-discipline, which is in turn the largest\npart of economics as a whole. Thus the boundary between the\nphilosophy of game theory and the philosophy of\nmicroeconomics is now similarly indistinct. Of course, as has been\nstressed, applications of game theory extend beyond the traditional\ndomain of economics, into all of the behavioral and social sciences.\nBut as the methods of game theory have fused with the methods of\nmicroeconomics, a commentator might equally view these extensions as\nbeing exported applications of microeconomics. \nFollowing decades of development (incompletely) surveyed in the\npresent article, the past few years have been relatively quiet ones\nwhere foundational innovations of the kind that invite contributions\nfrom philosophers are concerned. Some parts of the original\nfoundations are being newly revisited, however. \n\n von Neumann and Morgenstern’s (1944)\n introduction of game theory divided the inquiry into two parts.\nNoncooperative game theory analyzes cases built on the\nassumption that each player maximizes her own utility function while\ntreating the expected strategic responses of other players as\nconstraints. As discussed above, the specific game to which von\nNeumann and Morgenstern applied their modeling was poker, which is a\nzero-sum game. Most of the present article has focused on the many\ntheoretical challenges and insights that arose from extending\nnoncooperative game theory beyond the zero-sum domain. But this in\nfact develops only half of von Neumann and Morgenstern’s\nclassic. The other half developed cooperative game theory,\nabout which nothing has so far been said here. The reason for this\nsilence is that for most game theorists cooperative game theory is a\ndistraction at best and at worst a technology that confuses\nthe point of game theory by bypassing the aspect of games that mainly\nmakes them potentially interesting and insightful in application,\nnamely, the requirement that equilibria be selected endogenously under\nthe restrictions imposed by\n Nash (1950a).\n This, after all, is what makes equilibria self-enforcing, just in the\nway that prices in competitive markets are, and thus renders them\nstable unless shocked from outside.\n Nash (1953)\n argued that solutions to cooperative games should always be verified\nby showing that they are also solutions to formally equivalent\nnoncooperative games. Nash’s accomplishment in the paper wa the\nanalytical identification of the relevant equivalence. One way of\ninterpreting this was as demonstrating the ultimate redundancy of\ncooperative game theory.  \nCooperative game theory begins from the assumption that players have\nalready, by some unspecified process, agreed on a vector of\nstrategies, and thus on an outcome. Then the analyst deploys the\ntheory to determine the minimal set of conditions under which the\nagreement remains stable. The idea is typically illustrated by the\nexample of a parliamentary coalition. Suppose that there is one\ndominant party that must be a member of any coalition if it is to\ncommand a majority of parliamentary votes on legislation and\nconfidence. There might then be a range of alternative possible\ngroupings of other parties that could sustain it. Imagine, to make the\nexample more structured and interesting, that some parties will not\nserve in a coalition that includes certain specific others; so the\nproblem faced by the coalition organizers is not simply a matter of\nsumming potential votes. The cooperative game theorist identifies the\nset of possible coalitions. There may be some other parties, in\naddition to the dominant party, that turn out to be needed in every\npossible coalition. Identifying these parties would, in this example,\nreveal the core of the game, the elements shared by all\nequilibria. The core is the key solution concept of cooperative game\ntheory, for which Shapley shared the Nobel prize.\n (Shapley (1953)\n is the great paper.)\n Nash (1953)\n defined the “Nash program” as consisting of verifying a\nparticular cooperative equilibrium by showing that noncooperative\nplayers could arrive at it through the sequential bargaining\nprocess specified in\n Nash (1950b),\n and that all outcomes of such bargaining would include the\ncore. \nIn light of the example, it is no surprise that political scientists\nwere the primary users of cooperative theory during the years while\nnoncooperative game theory was still being fully developed. It has\nalso been applied usefully by labor economists studying settlement\nnegotiations between firms and unions, and by analysts of\ninternational trade negotiations. We might illustrate the value of\nsuch application by reference to the second example. Suppose that,\ngiven the weight of domestic lobbies in South Africa, the South\nAfrican government will never agree to any trade agreement that does\nnot allow it to protect its automative assembly sector. (This has in\nfact been the case so far.) Then allowance for such protection is part\nof the core of any trade treaty another country or bloc might conclude\nwith South Africa. Knowing this can help the parties during\nnegotiations avoid rhetoric or commitments to other lobbies, in any of\nthe negotiating countries, that would put the core out of reach and\nthus guarantee negotiation failure. This example also helps us\nillustrate the limitations of cooperative game theory. South Africa\nwill have to trade off the interests of some other lobbies to protect\nits automative industry. Which others will get traded off\nwill be a function of the extensive-form play of non-cooperative\nsequential proposals and counter-proposals, and the South African\nbargainers, if they have done their due diligence, must be attentive\nto which paths through the tree throw which specific domestic\ninterests under the proverbial bus. Thus carrying out the cooperative\nanalysis does not relieve them of the need to also conduct the\nnoncooperative analysis. Their game theory consultants might as well\nsimply code the non-cooperative parameters into their Gambit software,\nwhich will output the core if asked. \nBut cooperative game theory did not die, or become confined to\npolitical science applications. There has turned out to be a range of\npolicy problems, involving many players whose attributes vary but\nwhose ordinal utility functions are symmetrical, for which\nnoncooperative modeling, while possible in principle, is absurdly\ncumbersome and computationally demanding, but for which cooperative\nmodeling is beautifully suited. That we be dealing with ordinal\nutility functions is important, because in the relevant markets there\nare often no prices. The classic example\n (Gale and Shapley 1962)\n is a marriage market. Abstracting from the scale of individual\nromantic dramas and comedies, society features, as it were, a vast set\nof people who want to form into pairs, but care very much who they end\nup paired with. Suppose we have a finite set of such people. Imagine\nthat the match-maker, or app, first splits the set into two proper\nsubsets, and announces a rule that everyone in subset A will\npropose to someone in subset B. Each of those in B who\nreceive a proposal knows that she is the first choice of someone in\nA. She selects her first choice from the proposals she has\nreceived and throws the rest back into the pool. Those in A\nwhose initial proposals were not accepted now each propose to someone\nthey did not propose to before, but possibly including people who are\nholding proposals from a previous round — Nkosi knows that\nBarbara preferred Amalia in round 1, but Nkosi wasn’t part of\nthat choice set and so might displace Amalia in round 2). Provably\nthere exists a terminal round after which no further proposals will be\nmade, and the matchmaking app will have found the core of the\ncooperative game because no person i in set B will\nprefer to pair with someone from set A who prefers i\nto whoever is holding that A-set dreamboat’s proposal.\nEveryone from set B will now accept the proposal they are holding,\nand, if the two sets had the same cardinality and everyone would\nrather pair with someone than pair with no one, then nobody will go\noff alone. \nThis is not a directly applicable model of a marriage market, so there\nis no money to be made in selling the simple matchmaking app described\nabove. The problem is that we have no guarantee that, in the example,\nNkosi and Amalia aren’t one another’s partners of destiny,\nbut cannot get paired because they both began in subset A. In\ngame theory textbooks this problem is often finessed by assuming that\nSet A contains men and Set B contains women, and that\neveryone is so committed to heterosexuality that they’d rather\npair with anyone of the opposite sex than anyone of their own sex. On\nthe other hand, the model provides some insight, in the way that\nmodels typically do, if we don’t insist on applying it too\nliterally. After working through it, one sees the logic of facts about\nsociety that someone designing a real matchmaking app had better\nunderstand: that the app will have to log proposals under\nconsideration but not yet accepted, leave people holding proposals\nunder consideration on the market, and remember who has previously\nrejected whom (without creating a generalised emotional catastrophe by\npublicly posting this information). The real app will not be able to\nreliably find the core of the cooperative game, unless the set of\npeople in the market is small, restricted, and has self-sorted into\nsubsets to at least some extent by providing such information as\n“X-type person seeks Y-type person” for\nX and Y properties that everyone prioritizes. (Are there\nsuch properties, at least as an approximation?) But the real\nmatchmaking apps seem to work well enough to be transforming the way\nin which most young people now find mates in countries with generally\navailable internet access. Relationships between theoretically\nidealized and real marriage markets are comprehensively reviewed in\n Chiappori (2017).\n  \nThe revival of cooperative game theory as site of renewed interest has\noccurred because policy problems have been encountered that, unlike\nthe original toy illustration using the all-straights marriage market,\nsatisfy the model’s crucial assumptions. Leading instances are\nmatching university applicants and universities, and matching people\nneeding organ transplants with donors (see\n Roth 2015).\n In these markets, there is no ambivalence about partitioning the sets\nto be matched. Ordinal preferences are the relevant ones: universities\ndon’t auction off places to the highest bidder (or at least not\nin general), and organs are not for sale (or at least not legally).\nThe models are really applied, and they demonstrably have improved\nefficiency and saved lives. \nIt is common in science for models that are practically clumsy fits to\ntheir original problems to turn out to furnish highly efficient\nsolutions to new problems thrown up by technological change. The\ninternet has created an environment for applications of matching\nalgorithms — travellers and flat renters, diners and\nrestaurants, students and tutors, and (regrettably) socially alienated\npeople and purveyors of propaganda and fanaticism — that could\nhave been designed by a theorist at any time since Shapley’s\noriginal innovations, but would previously have been practically\nimpossible to implement. These applications of cooperative game theory\nare often applied conjointly with the noncooperative game theory of\nauctions\n (Klemperer 2004)\n to drive market designs for goods and services so efficient as to be\nannihilating the once mighty shopping mall in even the suburban USA.\nWhy are hotels far more profitable and easily available than was the\ncase in all but the largest cities before about 2007? The answer is\nthat dynamic pricing algorithms\n (Gershkov and Moldovanu 2014)\n blend matching theory and auction theory to allow hotels, combined\nwith online travel service aggregators, to find customers willing to\npay premium rates for their ideal locations and times, and then fill\nthe remaining rooms with bargain hunters whose preferences are more\nflexible. Airlines operate similar technology. Game theory thus\ncontinues to be one of the 20th-century inventions that is driving\nsocial revolutions in the 21st, and\n Samuelson (2016)\n predicts a coming surge of renewed interest in the deeper mathematics\nof cooperative games and their relationships to noncooperative games.\n \nAn range of further applications of both classical and evolutionary\ngame theory have been developed, but we have hopefully now provided\nenough to convince the reader of the tremendous, and constantly\nexpanding, utility of this analytical tool. The reader whose appetite\nfor more has been aroused should find that she now has sufficient\ngrasp of fundamentals to be able to work through the large literature,\nof which some highlights are listed below.","contact.mail":"don.ross@uct.ac.za","contact.domain":"uct.ac.za"}]
