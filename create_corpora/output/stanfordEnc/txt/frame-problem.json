[{"date.published":"2004-02-23","date.changed":"2016-02-08","url":"https://plato.stanford.edu/entries/frame-problem/","author1":"Murray Shanahan","entry":"frame-problem","body.text":"\n\n\n\nTo most AI researchers, the frame problem is the challenge of\nrepresenting the effects of action in logic without having to represent\nexplicitly a large number of intuitively obvious non-effects. But to many\nphilosophers, the AI researchers' frame problem is suggestive of\nwider epistemological issues. Is it possible, in principle, to limit\nthe scope of the reasoning required to derive the consequences of an\naction? And, more generally, how do we account for our apparent ability\nto make decisions on the basis only of what is relevant to an ongoing\nsituation without having explicitly to consider all that is not\nrelevant?\n\n\n\nThe frame problem originated as a narrowly defined technical problem\nin\n logic-based artificial intelligence\n (AI).\nBut it was taken up in an embellished and modified form by\nphilosophers of mind, and given a wider interpretation. The tension\nbetween its origin in the laboratories of AI researchers and its\ntreatment at the hands of philosophers engendered an interesting\nand sometimes heated debate in the 1980s and 1990s.\nBut since the narrow, technical problem is largely solved, recent\ndiscussion has tended to focus less on matters of interpretation and\nmore on the implications of the wider frame problem for \n cognitive science.\nTo gain an understanding of the issues,\nthis article will begin with a look at the frame problem in its\ntechnical guise. Some of the ways in which philosophers have\nre-interpreted the problem will then be examined. The article\nwill conclude with an assessment of the significance of the frame\nproblem today. \n\nPut succinctly, the frame problem in its narrow, technical form is\nthis (McCarthy & Hayes 1969). Using mathematical logic, how is it\npossible to write formulae that describe the effects of actions without\nhaving to write a large number of accompanying formulae that describe\nthe mundane, obvious non-effects of those actions? Let's take a look at\nan example. The difficulty can be illustrated without the full\napparatus of formal logic, but it should be borne in mind that the\ndevil is in the mathematical details. Suppose we write two formulae,\none describing the effects of painting an object and the other\ndescribing the effects of moving an object. \n\nNow, suppose we have an initial situation in which\nColour(A, Red)\nand Position(A, House) hold. According to\nthe machinery of deductive logic, what then holds after the action\nPaint(A, Blue) followed by the\naction Move(A, Garden)?  Intuitively, we\nwould expect Colour(A, Blue) and\nPosition(A, Garden) to hold. Unfortunately,\nthis is not the case. If written out more formally in classical\npredicate logic, using a suitable formalism for representing time and\naction such as the situation calculus (McCarthy & Hayes 1969), the\ntwo formulae above only license the conclusion\nthat Position(A, Garden) holds.  This is\nbecause they don't rule out the possibility that the colour of\nA gets changed by the Move action. \n\nThe most obvious way to augment such a formalisation so that the\nright common sense conclusions fall out is to add a number of formulae\nthat explicitly describe the non-effects of each action. These formulae\nare called frame axioms. For the example at hand, we need a\npair of frame axioms. \n\nIn other words, painting an object will not affect its position, and\nmoving an object will not affect its colour. With the addition of these\ntwo formulae (written more formally in predicate logic), all the\ndesired conclusions can be drawn. However, this is not at all a\nsatisfactory solution. Since most actions do not affect\nmost properties of a situation, in a domain comprising\nM actions and N properties we will, in general, have\nto write out almost MN frame axioms. Whether these formulae\nare destined to be stored explicitly in a computer's memory, or are\nmerely part of the designer's specification, this is an unwelcome\nburden. \n\nThe challenge, then, is to find a way to capture the non-effects of\nactions more succinctly in formal logic. What we need, it seems, is\nsome way of declaring the general rule-of-thumb that an action can be\nassumed not to change a given property of a situation unless\nthere is evidence to the contrary. This default assumption is known as\nthe common sense law of inertia. The (technical) frame problem\ncan be viewed as the task of formalising this law. \n\nThe main obstacle to doing this is the monotonicity of\nclassical logic. In classical logic, the set of conclusions that can be\ndrawn from a set of formulae always increases with the\naddition of further formulae. This makes it impossible to express a\nrule that has an open-ended set of exceptions, and the common sense law\nof inertia is just such a rule. For example, in due course we might\nwant to add a formula that captures the exception to Axiom 3 that\narises when we move an object into a pot of paint. But our not having\nthought of this exception before should not prevent us from applying\nthe common sense law of inertia and drawing a wide enough set of\n(defeasible) conclusions to get off the ground. \n\nAccordingly, researchers in logic-based AI have put a lot of effort\ninto developing a variety of non-monotonic reasoning\nformalisms, such as circumscription (McCarthy 1986), and\ninvestigating their application to the frame problem. None of this has\nturned out to be at all straightforward. One of the most troublesome\nbarriers to progress was highlighted in the so-called Yale shooting\nproblem (Hanks & McDermott 1987), a simple scenario that gives\nrise to counter-intuitive conclusions if naively represented with a\nnon-monotonic formalism. To make matters worse, a full solution needs\nto work in the presence of concurrent actions, actions with\nnon-deterministic effects, continuous change, and actions with indirect\nramifications. In spite of these subtleties, a number of solutions to\nthe technical frame problem now exist that are adequate for logic-based\nAI research. Although improvements and extensions continue to be found,\nit is fair to say that the dust has settled, and that the frame\nproblem, in its technical guise, is more-or-less solved (Shanahan\n1997; Lifschitz 2015). \n\nLet's move on now to the frame problem as it has been re-interpreted\nby various philosophers. The first significant mention of the frame\nproblem in the philosophical literature was made by Dennett (1978,\n125). The puzzle, according to Dennett, is how “a cognitive\ncreature … with many beliefs about the world” can update\nthose beliefs when it performs an act so that they remain\n“roughly faithful to the world”? In The Modularity of\nMind, Fodor steps into a roboticist's shoes and, with the frame\nproblem in mind, asks much the same question: “How … does\nthe machine's program determine which beliefs the robot ought to\nre-evaluate given that it has embarked upon some or other course of\naction?” (Fodor 1983, 114). \n\nAt first sight, this question is only impressionistically related to\nthe logical problem exercising the AI researchers. In contrast to the\nAI researcher's problem, the philosopher's question isn't\nexpressed in\nthe context of formal logic, and doesn't specifically concern the\nnon-effects of actions. In a later essay, Dennett acknowledges the\nappropriation of the AI researchers' term (1987). Yet he goes on to\nreaffirm his conviction that, in the frame problem, AI has discovered\n“a new, deep epistemological problem—accessible in\nprinciple but unnoticed by generations of philosophers”. \n\nThe best way to gain an understanding of the issue is to imagine\nbeing the designer of a robot that has to carry out an everyday task,\nsuch as making a cup of tea. Moreover, for the frame problem\nto be neatly highlighted, we must confine our thought experiment to a\ncertain class of robot designs, namely those using explicitly\nstored, sentence-like representations of the world, reflecting the\nmethodological tenets of classical AI. The AI researchers\nwho tackled the original frame problem in its narrow, technical guise\nwere working under this constraint, since logic-based AI is a variety of\nclassical AI. Philosophers sympathetic to the\n computational theory of mind—who \nsuppose that mental states comprise sets of\npropositional attitudes and mental processes are forms of inference\nover the propositions in question—also tend to feel at home\nwith this prescription. \n\nNow, suppose the robot has to take a tea-cup from the cupboard. The\npresent location of the cup is represented as a sentence in its\ndatabase of facts alongside those representing innumerable other\nfeatures of the ongoing situation, such as the ambient temperature, the\nconfiguration of its arms, the current date, the colour of the tea-pot,\nand so on. Having grasped the cup and withdrawn it from the cupboard,\nthe robot needs to update this database. The location of the cup has\nclearly changed, so that's one fact that demands revision. But which\nother sentences require modification? The ambient temperature is\nunaffected. The location of the tea-pot is unaffected. But if it so\nhappens that a spoon was resting in the cup, then the spoon's new\nlocation, inherited from its container, must also be updated. \n\nThe epistemological difficulty now discerned by philosophers is this.\nHow could the robot limit the scope of the propositions\nit must reconsider in the light of its actions? \nIn a sufficiently simple robot, this doesn't seem like much of a\nproblem. Surely the robot can simply examine its entire database of\npropositions one-by-one and work out which require modification. But\nif we imagine that our robot has near human-level intelligence, and is\ntherefore burdened with an enormous database of facts to examine every\ntime it so much as spins a motor, such a strategy starts to look\ncomputationally intractable.\n \n\nThus,\na related issue in AI has been dubbed the computational aspect of\nthe frame problem (McDermott 1987). This is the question of how to compute\nthe consequences of an action without the computation having to range\nover the action's non-effects. The solution to the computational aspect\nof the frame problem adopted in most symbolic AI programs is some\nvariant of what McDermott calls the “sleeping dog” strategy\n(McDermott 1987). The idea here is that not every part of the data\nstructure representing an ongoing situation needs to be examined when\nit is updated to reflect a change in the world. Rather, those parts\nthat represent facets of the world that have changed are modified, and\nthe rest is simply left as it is (following the dictum “let\nsleeping dogs lie”).\nIn our example of the robot and the tea-cup, we might apply the\nsleeping dog strategy by having the robot update its beliefs about the\nlocation of the cup and the contents of the cupboard. But the robot\nwould not worry about some possible spoon that may or may not be on or\nin the cup, since the robot's goal did not directly involve any spoon.\n\n \n\nHowever, the philosophical problem is not exhausted by this\ncomputational issue. The outstanding philosophical question is how the\nrobot could ever determine that it had successfully revised all its\nbeliefs to match the consequences of its actions.  Only then would it\nbe in a position safely to apply the “common sense law of\ninertia” and assume the rest of the world is untouched.  Fodor\nsuggestively likens this to “Hamlet's problem: when to stop\nthinking” (Fodor 1987, 140). The frame problem, he claims, is\n“Hamlet's problem viewed from an engineer's perspective”.\nSo construed, the obvious way to try to avoid the frame problem is by\nappealing to the notion of relevance. Only certain properties\nof a situation are relevant in the context of any given action, so the\ncounter-argument goes, and consideration of the action's consequences\ncan be conveniently confined to those. \n\nHowever, the appeal to relevance is unhelpful. For the difficulty now\nis to determine what is and what isn't relevant, and this is dependent\non context. Consider again the action of removing a tea-cup from the\ncupboard. If the robot's job is to make tea, it is relevant that this\nfacilitates filling the cup from a tea-pot. But if the robot's task\nis to clean the cupboard, a more relevant consequence is the\nexposure of the surface the cup was resting on.\nAn AI researcher in the classical mould could rise to this challenge\nby attempting to specify what propositions are relevant to what context.\nBut philosophers such as Wheeler (2005; 2008), taking their cue from\nDreyfus (1992), perceive the threat of infinite regress here. As Dreyfus\nputs it, “if each context can be recognized only in terms of features\nselected as relevant and interpreted in a broader context, the AI worker\nis faced with a regress of contexts” (Dreyfus 1992, 289). \n\nOne way to mitigate the threat of infinite regress is by appeal to the\nfact that, while humans are more clever than today's robots,\nthey still make mistakes (McDermott 1987). People often fail to foresee\nevery consequence of their actions even though they lack none\nof the information required to derive those consequences, as any novice\nchess player can testify. Fodor asserts that “the frame problem\ngoes very deep; it goes as deep as the analysis of rationality”\n(Fodor 1987). But the analysis of rationality can accommodate the\nboundedness of the computational resources available to derive relevant\nconclusions (Simon 1957; Russell & Wefald 1991; Sperber &\nWilson 1996). Because it sometimes jumps to premature conclusions,\nbounded rationality is logically flawed, but no more so than human\nthinking.\nHowever, as Fodor points out, appealing to human limitations\nto justify the imposition of a heuristic boundary on the kind of\ninformation available to an inferential process does not in itself solve\nthe epistemological frame problem (Fodor 2000, Ch.2; Fodor 2008, Ch.4; see also Chow 2013).\nThis is because it neglects the issue\nof how the heuristic boundary is to be drawn, which is to say it\nfails to address the original question of how to specify what is and\nisn't relevant to the inferential process. \n\nNevertheless, the classical AI researcher, convinced that the regress of\ncontexts will bottom out eventually, may still elect to pursue the research\nagenda of building systems based on rules for determining relevance,\ndrawing inspiration from the past successes of classical AI.\nWhereupon the dissenting philosopher might point out that AI's past\nsuccesses have always been confined to narrow domains, such as playing chess,\nor reasoning in limited microworlds where the set of potentially relevant\npropositions is fixed and known in advance. By contrast, human intelligence\ncan cope with an open-ended, ever-changing set of contexts (Dreyfus 1992;\nDreyfus 2008; Wheeler 2005; Wheeler 2008; Rietveld 2012).\nFurthermore, the classical AI researcher is vulnerable to\nan argument from holism. A key claim in Fodor's work is that when it\ncomes to circumscribing the consequences of an action, just as in the business\nof theory confirmation in science, anything could be relevant (Fodor\n1983, 105). There are no a priori limits to the properties\nof the ongoing situation that might come into play. Accordingly, in his\nmodularity thesis, Fodor uses the frame problem to bolster the view\nthat the mind's central processes—those that are involved in\nfixing belief—are “informationally unencapsulated”,\nmeaning that they can draw on information from any source (Fodor 1983;\nFodor 2000; Fodor 2008; Dreyfus 1991, 115–121; Dreyfus 1992, 258).\nFor Fodor, this is a fundamental barrier to the provision of a\ncomputational account of these processes. \n\nIt is tempting to see Fodor's concerns as resting on a fallacious\nargument to the effect that a process must be informationally encapsulated\nto be computationally tractable. We only need to consider the\neffectiveness of Internet search engines to see that, thanks to clever\nindexing techniques, this is not the case. Submit any pair of seemingly\nunrelated keywords (such as “banana” and\n“mandolin”) to a Web search engine, and in a fraction of a\nsecond it will identify every web page, in a database of several\nbillion, that mentions those two keywords (now including this page, no\ndoubt). But this is not the issue at hand. The real issue, to reiterate\nthe point, is one of relevance. A process might indeed be able to index\ninto everything the system knows about, say, bananas and mandolins, but\nthe purported mystery is how it could ever work out that, of all things,\nbananas and mandolins were relevant to its reasoning task in the first\nplace. \n\nTo summarize, it is possible to discern an epistemological frame\nproblem, and to distinguish it from a computational counterpart. The\nepistemological problem is this: How is it possible for holistic,\nopen-ended, context-sensitive relevance to be captured by a set of\npropositional, language-like representations of the sort used in\nclassical AI?  The computational counterpart to the\nepistemological problem is this. How could an inference process\ntractably be confined to just what is relevant, given that relevance\nis holistic, open-ended, and context-sensitive? \n\nAn additional dimension to the frame problem is uncovered in (Fodor\n1987), where the metaphysical justification for the common sense law of\ninertia is challenged. Although Fodor himself doesn't clearly\ndistinguish this issue from other aspects of the wider frame problem,\nit appears on examination to be a separate philosophical conundrum.\nHere is the argument. As stated above, solutions to the logical frame\nproblem developed by AI researchers typically appeal to some version of\nthe common sense law of inertia, according to which properties of a\nsituation are assumed by default not to change as the result of an\naction. This assumption is supposedly justified by the very observation\nthat gave rise to the logical frame problem in the first place, namely\nthat most things don't change when an action is performed or an event\noccurs. \n\nAccording to Fodor, this metaphysical justification is unwarranted.\nTo begin with, some actions change many, many things. Those who affirm\nthat painting an object has little or no effect on most properties of\nmost of the objects in the room are likely to concede that detonating a\nbomb actually does affect most of those properties. But a deeper\ndifficulty presents itself when we ask what is meant by “most\nproperties”. What predicates should be included in our ontology\nfor any of these claims about “most properties” to fall\nout? To sharpen the point, Fodor introduces the concept of a\n“fridgeon”. Any particle is defined as a fridgeon\nat a given time if and only if Fodor's fridge is switched on at that\ntime. Now, it seems, the simple act of turning Fodor's fridge on or off\nbrings about an astronomical number of incidental changes. In a\nuniverse that can include fridgeons, can it really be the case that\nmost actions leave most things unchanged? \n\nThe point here is not a logical one. The effect on fridgeons of\nswitching Fodor's fridge on and off can concisely be represented\nwithout any difficulty (Shanahan 1997, 25). Rather, the point is\nmetaphysical. The common sense law of inertia is only justified in the\ncontext of the right ontology, the right choice of objects and\npredicates. But what is the right ontology to make the common sense law\nof inertia work? Clearly, fridgeons and the like are to be excluded.\nBut what metaphysical principle underpins such a decision? \n\nThese questions and the argument leading to them are very\nreminiscent of Goodman's treatment of induction (Goodman 1954).\nGoodman's\n “new riddle of induction”,\n commonly called the grue paradox,\ninvites us to consider the predicate grue, which is true\nbefore time t only of objects that are green and after time\nt only of objects that are blue. The puzzle is that every\ninstance of a green emerald examined before time t is also an\ninstance of a grue emerald. So, the inductive inference that all\nemeralds are grue seems to be no less legitimate than the inductive\ninference that all emeralds are green. The problem, of course, is the\nchoice of predicates. Goodman showed that inductive inference only\nworks in the context of the right set of predicates, and Fodor\ndemonstrates much the same point for the common sense law of\ninertia. \n\nAn intimate relationship of a different kind between the frame\nproblem and the problem of induction is proposed by Fetzer (1991), who\nwrites that “The problem of induction [is] one of justifying some\ninferences about the future as opposed to others. The frame problem,\nlikewise, is one of justifying some inferences about the future as\nopposed to others. The second problem is an instance of the\nfirst.” This view of the frame problem is highly controversial,\nhowever (Hayes 1991). \n\nThe narrow, technical frame problem generated a great deal of work\nin logic-based artificial intelligence in the late 1980s and early 1990s,\nand its wider philosophical implications came to the fore at around the\nsame time. But the importance each thinker accords to the frame problem\ntoday will typically depend on their stance on other matters. \n\nWithin classical AI, a variety of workable solutions to the logical\nframe problem have been developed, and it is no longer considered a\nserious obstacle even for those working in a strictly logic-based\nparadigm (Shanahan 1997; Reiter 2001; Shanahan 2003; Lifschitz 2015).\nIt's worth\nnoting that logically-minded AI researchers can consistently retain\ntheir methodology and yet, to the extent that they view their products\npurely as engineering, can reject the traditional cognitive\nscientist's belief in the importance of computation over\nrepresentations for understanding the mind.  Moreover, insofar as the\ngoal of classical AI is not computers with human-level intelligence,\nbut is simply the design of better and more useful computer programs,\nit is immune to the philosophical objections of Fodor, Dreyfus, and\nthe like.  Significantly though, for AI researchers working outside\nthe paradigm of symbolic representation altogether—those\nworking in situated robotics, for example—the logical frame\nproblem simply doesn't feature in day-to-day investigations. \n\nAlthough it can be argued that it arises even in a\n connectionist setting (Haselager &\nVan Rappard 1998; Samuels 2010), the frame problem inherits much of its\nphilosophical significance from the classical assumption of the\nexplanatory value of computation over representations, an assumption\nthat has been under vigorous attack for some time (Clark 1997; Wheeler\n2005).  Despite this, many philosophers of mind, in the company of\nFodor and Pylyshyn, still subscribe to the view that human mental\nprocesses consist chiefly of inferences over a set of propositions,\nand that those inferences are carried out by some form of computation.\nTo such philosophers, the epistemological frame problem and its\ncomputational counterpart remain a genuine threat.  For Wheeler and others, classical AI and cognitive science rest on\nCartesian assumptions that need to be overthrown in favour of a more\nHeideggerian stance before the frame problem can be overcome (Dreyfus\n2008; Wheeler 2005; 2008; Rietveld 2012).  According to Wheeler (2005;\n2008), the situated robotics movement in AI that originated with the\nwork of Brooks (1991) exemplifies the right way to go.  Dreyfus is in\npartial agreement, but contends that the early products of situated\nrobotics “finesse rather than solve the frame problem”\nbecause “Brooks's robots respond only to fixed isolable features\nof the environment, not to context or changing significance”\n(Dreyfus 2008, 335).  Dreyfus regards the neurodynamics work of\nFreeman (2000) as a better foundation for the sort of Heideggerian\napproach to AI in which the frame problem might be dissolved (see also\nShanahan 2010, Ch.5; Rietveld 2012; Bruineberg & Rietveld\n2014). Dreyfus is impressed by Freeman's approach because the\nneurodynamical record of significance is neither a representation nor\nan association, but (in dynamical systems terms) “a repertoire\nof attractors” that classify possible responses, “the\nattractors themselves being the product of past experience”\n(Dreyfus 2008, 354). \n\nOne philosophical legacy of the frame problem is that it has drawn\nattention to a cluster of issues relating to holism, or so-called\ninformational unencapsulation.\nRecall that a process is informationally unencapsulated (Fodor sometimes\nuses the term “isotropic”) if there is no a priori\nboundary to what information is relevant to it.\nIn recent writing, Fodor uses the term “frame\nproblem” in the context of all informationally unencapsulated\nprocesses, and not just those to do with inferring the consequences\nof change (Fodor 2000, Ch.2; Fodor 2006, Ch.4).\nIt's clear that idealised rationality is informationally\nunencapsulated, in this sense.\nIt has also been suggested that isotropy is damaging to the so-called\n theory theory\n of folk psychology (Heal 1996).\n(For Heal, this lends support to the rival\n simulation theory,\n but Wilkerson (2001) argues that informational unencapsulation is a\nproblem for both accounts of folk psychology.)\nAnalogical reasoning, as Fodor says, is an example of “isotropy\nin the purest form: a process which depends precisely upon the transfer\nof information among cognitive domains previously assumed to be\nirrelevant” (Fodor 1983, 105).\nArguably, a capacity for analogical and metaphorical\nthinking—a talent for creatively transcending the boundaries\nbetween different domains of understanding—is the source of\nhuman cognitive prowess (Lakoff & Johnson 1980; Mithen 1996). So\nthe informational unencapsulation of analogical reasoning is potentially\nvery troublesome, and especially so for modular theories of mind in which\nmodules are viewed as (context-insensitive) specialists\n(Carruthers 2003; 2006).  Dreyfus claims that this “extreme version of the frame\nproblem” is no less a consequence of the Cartesian assumptions\nof classical AI and cognitive science than its less demanding\nrelatives (Dreyfus 2008, 361). He advances the view that a suitably\nHeideggerian account of mind is the basis for dissolving the frame\nproblem here too, and that our “background familiarity with how\nthings in the world behave” is sufficient, in such cases, to\nallow us to “step back and figure out what is relevant and\nhow”.  Dreyfus doesn't explain how, given the holistic,\nopen-ended, context-sensitive character of relevance, this\nfiguring-out is achieved. But Wheeler, from a similarly Heideggerian\nposition, claims that the way to address the\n“inter-context” frame problem, as he calls it, is with a\ndynamical system in which “the causal contribution of each\nsystemic component partially determines, and is partially determined\nby, the causal contributions of large numbers of other systemic\ncomponents” (Wheeler 2008, 341).  A related proposal is put\nforward by Shanahan and Baars (2005; see also Shanahan 2010, Ch.6),\nbased on global workspace theory (Baars 1988), according to which the\nbrain incorporates a solution to the problem of informational\nunencapsulation by instantiating an architecture in which a) the\nresponsibility for determining relevance is not centralised but is\ndistributed among parallel specialist processes, and b) a serially\nunfolding global workspace state integrates relevant contributions\nfrom multiple domains.","contact.mail":"m.shanahan@ic.ac.uk","contact.domain":"ic.ac.uk"}]
