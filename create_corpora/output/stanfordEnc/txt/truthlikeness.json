[{"date.published":"2001-07-11","date.changed":"2014-01-30","url":"https://plato.stanford.edu/entries/truthlikeness/","author1":"Graham Oddie","author1.info":"http://spot.Colorado.EDU/~oddie/","entry":"truthlikeness","body.text":"\n\n\n\nTruth is widely held to be the constitutive aim of\ninquiry. Even those who think the aim of inquiry is something more\naccessible than the truth (such as the empirically discernible truth),\nas well as those who think the aim is something more robust than\npossessing truth (such as the possession of knowledge) still affirm\ntruth as a necessary component of the end of inquiry. And, other\nthings being equal, it seems better to end an inquiry by endorsing\ntruths rather than falsehoods.\n\nEven if there is something to the thought that inquiry aims at\ntruth, it has to be admitted that truth is a rather coarse-grained\nproperty of propositions. Some falsehoods seem to realize the aim of\ngetting at the truth better than others. Some truths better realize\nthe aim than other truths. And perhaps some falsehoods even realize\nthe aim better than some truths do. The dichotomy of the class of\npropositions into truths and falsehoods needs to be supplemented with\na more fine-grained ordering — one which classifies propositions\naccording to their closeness to the truth, their degree\nof truthlikeness, or their verisimilitude.\n\n\n\nWe begin with the logical problem of truthlikeness: the problem of\ngiving an adequate account of the concept and determining its logical\nproperties.  In §1 we lay at the logical problem and various\npossible solutions to it.  In §1.1 we examine the basic\nassumptions which generate the logical problem, which in part explain\nwhy the problem emerged when it did. Attempted solutions to the\nproblem quickly proliferated, but they can be gathered together under\nthree broad lines of attack. The first two, the content\napproach (§1.2) and the consequence approach\n(§1.3), were both initiated by Popper in his ground-breaking work\nand both deliver what he regarded as an essential desideratum for any\ntheory of truthlikeness — what we will call the value of\ncontent for truths. Although Popper's specific proposals did not\ncapture the intuitive concept, the content and consequence approaches\nare still being actively developed and refined. The third,\nthe likeness approach (§1.4), takes the likeness\nin truthlikeness seriously.  Assuming likeness relations among\nworlds, the likeness of a proposition to the truth would seem to be\nsome function of the likeness of worlds that make the proposition true\nto the actual world. The main problems facing the likeness approach\nare outlined in §1.4.1 –§1.4.4.\n\n Given that there are at least three different approaches to the\nlogical problem, a natural question is whether there might be a way of\ncombining the different desiderata that motivate them (§1),\nthereby incorporating the most desirable features of each. Recent\nresults suggest that unfortunately the three approaches, while not\nlogically incompatible, cannot be fruitfully combined.  Any attempt to\nunify these approaches will have to jettison or radically modify at\nleast one of the motivating desiderata.\n\nThere are two further problems of truthlikeness, both of which\npresuppose the solution of the logical problem. One is\nthe epistemological problem of truthlikeness (§2).  Even\ngiven a suitable solution to the logical problem, there remains a\nnagging question about our epistemic access to truthlikeness.  The\nother is the axiological problem.  Truth and truthlikeness are\ninteresting, at least in part because they appear to be cognitive\nvalues of some sort.  Even if they are not cognitively valuable\nthemselves, they are closely connected to what is of cognitive\nvalue. The relations between truth, truthlikeness and cognitive value\nare explored in §3.\n\n\n\nTruth, perhaps even more than beauty and goodness, has been the target\nof an extraordinary amount of philosophical dissection and\nspeculation.  This is unsurprising. After all, truth is the\nconstitutive aim of all inquiry and a necessary condition of\nknowledge. And yet (as the redundancy theorists of truth have\nemphasized) there is something disarmingly simple about\ntruth. That the number of planets is 8 is true just in case,\nwell, … the number of planets is 8. By comparison with\ntruth, the more complex and much more interesting concept of\ntruthlikeness has only recently become the subject of serious\ninvestigation.The logical problem of truthlikeness is the problem of\ngiving a consistent and materially adequate account of the\nconcept. But before embarking on the project, we have to make it\nplausible that there is a coherent concept in the offing to be\ninvestigated. The proposition the number of planets in our solar system is\n9 may be false, but quite a bit closer to the truth than the\nproposition that the number of planets in our solar system is 9\nbillion. (One falsehood may be closer to the truth than another\nfalsehood.) The true proposition  the number of the planets is\nbetween 7 and 9 inclusive is closer to the truth than the true\nproposition that the number of the planets is greater than equal to\n0.  (So a truth may be closer to the truth than another truth.)\nFinally, with the demotion of Pluto to planetoid status, the\nproposition that the number of the planets is either less than or\ngreater than 9 may be true but it is arguably not as close to\nthe whole truth of this matter (namely, that there are\nprecisely 8 planets) as its highly accurate but strictly false\nnegation: that there are 9 planets.  \n\nThis particular numerical example is admittedly extremely simple, but\na wide variety of judgments of relative likeness to truth crop up both\nin everyday parlance as well as in scientific discourse. While some\ninvolve the relative accuracy of claims concerning the value of\nnumerical magnitudes, others involve the sharing of properties,\nstructural similarity, or closeness among putative laws. \n\nConsider a non-numerical example, also highly simplified but quite\ntopical in the light of the recent rise in status of the concept\nof fundamentality.  Suppose you are interested in the truth\nabout which particles are fundamental.  At the outset of your inquiry\nall you know are various logical truths, like the tautology either\nelectrons are fundamental or they are not.  Tautologies like these\nare pretty much useless in helping you locate the truth about\nfundamental particles.  Now, suppose that the standard model is\nactually on the right tracks. Then learning the that electrons are\nfundamental (which we suppose, for the sake of the example, is true)\nedges you a little bit closer to your goal.  It is by no means the\ncomplete truth about fundamental particles, but surely it is a piece\nof it. If you go on to learn that electrons, along with muons and tau\nparticles, are a kind of lepton and that all leptons are fundamental,\nyou have presumably edged a little closer. \n\nIf this is right, then some truths are closer to the truth about\nfundamental particles than others.  \n\nThe discovery that atoms are not fundamental, that they are in fact\ncomposite objects, displaced the earlier hypothesis that atoms are\nfundamental.  For a while the proposition that protons,\nneutrons and electrons are the fundamental components of atoms was\nembraced, but unfortunately it too turned out to be false.  Still,\nthis latter falsehood seems quite a bit closer to the truth than its\npredecessor (assuming, again, that the standard model is true). And\neven if the standard model contains errors, as surely it does, it is\npresumably closer to the truth about fundamental particles than these\nother falsehoods.  At least, it makes sense to suppose that it might\nbe. \n\nSo again, some falsehoods may be closer to the truth about fundamental\nparticles than other falsehoods. \n\nAs we have seen, a tautology is not a terrific truth locator, but if\nyou moved from the tautology that electrons either are or are not\nfundamental to embrace the false proposition that electrons are\nnot fundamental you would have moved further from your goal. \nSo, some truths are closer to the truth than some falsehoods.   \n\nBut it is by no means obvious that all truths about fundamental\nparticles are closer to the whole truth than any falsehood.  If you\nmove from the tautology to the false proposition that electrons,\nprotons and neutrons are the fundamental components of atoms, you\nmay well have taken a step towards the truth.   \n\nIf this is right, certain falsehoods may be closer to the truth than\nsome truths. \nInvestigations into the concept of truthlikeness began in earnest with\na tiny trickle of activity in the early nineteen sixties; became\nsomething of a torrent from the mid-seventies until the late eighties;\nand is now a relatively steady stream.  Why is truthlikeness\nsuch a latecomer to the philosophical scene?  The reason is simple. It\nwasn't until the latter half of the twentieth century that mainstream\nphilosophers gave up on the Cartesian goal of infallible\nknowledge. The idea that we are quite possibility, even probably,\nmistaken in our most cherished beliefs, that they might well be\njust false, was mostly considered tantamount to capitulation\nto the skeptic. By the middle of the twentieth century, however, it\nwas clear that natural science postulated a very odd world behind the\nphenomena, one rather remote from our everyday experience, one which\nrenders many of our commonsense beliefs, as well as previous\nscientific theories, strictly speaking, false.  Further, the\nincreasingly rapid turnover of scientific theories suggested that, far\nfrom being established as certain, they are ever vulnerable to\nrefutation, and typically are eventually refuted, to be replaced by\nsome new theory. Taking the dismal view, the history of inquiry is a\nhistory of theories shown to be false, replaced by other theories\nawaiting their turn at the guillotine. (This is the “dismal\ninduction”.) \n\nRealism holds that the constitutive aim of inquiry is the\ntruth of some matter. Optimism holds that the history of\ninquiry is one of progress with respect to its constitutive aim. But\nfallibilism holds that, typically, our theories are false \nor very likely to be false, and when shown to be false they are \nreplaced by other false theories. To combine all three ideas, we must\naffirm that some false propositions better realize the goal of truth \n— are closer to the truth — than others. So the \noptimistic realist who has discarded infallibilism has a problem \n— the logical problem of truthlikeness. \n\nBefore exploring possible solutions to the logical problem a couple of\ncommon confusions should be cleared away.  Truthlikeness should not be\nconflated with either epistemic probability or with vagueness. One common mistake is to conflate truthlikeness with vagueness.\nSuppose vagueness is not an epistemic phenomenon, and that it can be\nexplained by treating truth and falsehood as extreme points on a scale\nof distinct truth values.  Even if there are vagueness-related\n“degrees of truth”, ranging from clearly true\nthrough to clearly false, they should not be confused with\ndegrees of closeness to the truth. To see this, suppose Alan is\nexactly 179 cm tall. Then the proposition that Alan is exactly\n178.5 cm tall should turn out to be clearly false on any\ngood theory of vagueness.  Nevertheless it is pretty close to the\ntruth.  That Alan is tall, on the other hand, is a vague claim,\none that in the circumstances is neither clearly true nor clearly\nfalse.  However, since it closer to the clearly true end of the\nspectrum, it has a high (vagueness-related) degree of truth.\nStill, Alan is tall is not as close to the truth as the quite\nprecise, but nevertheless clearly false proposition that Alan is\nexactly 178.5 cm tall.  So closeness to the truth and\nvagueness-related degrees of truth (if there are such) can also pull\nin different directions. Neither does truthlikeness — likeness to the whole\ntruth of some matter — have much to do with high\nprobability. The probability that the number of planets is greater\nthan or equal to 0 is maximal but not terribly close to the whole\ntruth. Suppose some non-tautological true propositions can be known\nfor certain — call their conjunction the evidence. Then\nany truth that goes beyond the evidence will be less probable than the\nevidence. However truths that go beyond the evidence might well be\ncloser to the whole truth than the evidence is.  The true proposition\nwhich goes most beyond the evidence is the strongest possible truth\n— it is the truth, the whole truth and nothing but the\ntruth. And that true proposition is clearly the one that is\nclosest to the whole truth. So the truth with the least probability on\nthe evidence is the proposition that is closest to the whole\ntruth. What, then, is the source of the widespread conflation of \ntruthlikeness with probability? Probability — at least of the \nepistemic variety — measures the degree of seeming to be \ntrue, while truthlikeness measures the degree of being similar \nto the truth. Seeming and being similar might \nat first strike one as closely related, but of course they are very\ndifferent. Seeming concerns the appearances whereas \nbeing similar concerns the objective facts, facts about\nsimilarity or likeness. Even more important, there is a difference\nbetween being true and being the truth. The truth, of course, has the\nproperty of being true, but not every proposition that is true is the\ntruth in the sense of the aim of inquiry. The truth of a matter at\nwhich an inquiry aims is ideally the complete, true answer to its\ncentral query. Thus there are two dimensions along which probability\n(seeming to be true) and truthlikeness (being similar to the truth)\ndiffer radically. \nWhile a multitude of apparently different solutions to the problem\nhave been proposed, they can be classified them into three main\napproaches, each with its own heuristic — the content\napproach, the consequence approach and the likeness\napproach. \n\nKarl Popper was the first philosopher to take the logical problem of\ntruthlikeness seriously enough to make an assay on it. This is not\nsurprising, since Popper was also the first prominent realist to\nembrace a very radical fallibilism about science while also trumpeting the\nepistemic superiority of the enterprise.  \n\nAccording to Popper, Hume had shown not only that we can't verify any\ninteresting theory, we can't even render it more probable. Luckily,\nthere is an asymmetry between verification and falsification. While no\nfinite amount of data can verify or probabilify any interesting\nscientific theory, they can falsify the theory. According to Popper,\nit is the falsifiability of a theory which makes it scientific, the\nmore falsifiable the better. In his early work, he implied that the\nonly kind of progress an inquiry can make consists in falsification of\ntheories. This is a little depressing, to say the least. What it lacks\nis the idea that a succession of falsehoods can constitute genuine\ncognitive progress.  Perhaps this is why, for many years after first\npublishing these ideas in his 1934 Logik der Forschung Popper\nreceived a pretty short shrift from the philosophers. If all we can\never .say with confidence is “Missed again!” and “A\nmiss is as good as a mile!”, and the history of inquiry is a\nsequence of such misses, then epistemic pessimism pretty much\nfollows. Popper eventually realized that this naive falsificationism\nis compatible with optimism provided we have an acceptable notion of\nverisimilitude (or truthlikeness). If some false hypotheses are closer\nto the truth than others, if verisimilitude admits of degrees, then\nthe history of inquiry may well turn out to be one of progress towards\nthe goal of truth. Moreover, it may be reasonable, on the basis of the\nevidence, to conjecture that our theories are indeed making such\nprogress even though we know they are all false, or highly likely to\nbe false. \n\nPopper saw clearly that the concept of truthlikeness should not be\nconfused with the concept of epistemic probability, and that it has\noften been so confused. (See Popper 1963 for a history of the\nconfusion.) Popper's insight here was undoubtedly facilitated by his\ndeep but largely unjustified antipathy to epistemic probability.  He\nthought his starkly falsificationist account favored bold, contentful\ntheories. Degree of informative content varies inversely with\nprobability — the greater the content the less likely a theory\nis to be true. So if you are after theories which seem, on the\nevidence, to be true, then you will eschew those which make bold\n— that is, highly improbable — predictions. On this\npicture, the quest for theories with high probability is simply\nwrongheaded. \n\nTo see this distinction clearly, and to articulate it, was one of\nPopper's most significant contributions, not only to the debate about\ntruthlikeness, but to philosophy of science and logic in general. As\nwe will see, however, his deep antagonism to probability combined with\nhis passionate love affair with boldness was both a blessing and a\ncurse. The blessing: it led him to produce not only the first\ninteresting and important account of truthlikeness, but to initiate a\nwhole approach to the problem — the content approach (Zwart\n2001). The curse: content alone is insufficient to characterize\ntruthlikeness. \n\nPopper made the first real assay on the logical problem of\ntruthlikeness in his famous collection Conjectures and\nRefutations. Since he was a great admirer of Tarski's assay on\nthe concept of truth, he strove to model his theory of truthlikeness\non Tarski's theory. First, let a matter for investigation be\ncircumscribed by a formalized language L adequate for\ndiscussing it.  Tarski showed us how the actual world induces a\npartition of sentences of L into those that are true and those\nthat are false. The set of all true sentences is thus a complete true\naccount of the world, as far as that investigation goes. It is aptly\ncalled the Truth, T.\nT is the target of the investigation couched in L. It is the\ntheory that we are seeking, and, if truthlikeness is to make sense, \ntheories other than T, even false theories, come more or \nless close to capturing T. \n\nT, the Truth, is a theory only in the technical Tarskian\nsense, not in the ordinary everyday sense of that term. It is a set of\nsentences closed under the consequence relation: a consequence of some\nsentences in the set is also a sentence in the set. T may not\nbe finitely axiomatizable, or even axiomatizable at all.  Where the\nlanguage involves elementary arithmetic it follows (from Gödel's\nincompleteness theorem) that T won't be\naxiomatizable. However, it is a perfectly good set of sentences all\nthe same. In general we will follow the Tarski-Popper usage here and\ncall any set of sentences closed under consequence a theory,\nand we will assume that each proposition we deal with is identified\nwith the theory it generates in this sense. (Note that when theories\nare classes of sentences, theory A logically entails theory\nB just in case B is a subset of A.) \n\nThe complement of T, the set of false sentences F,\nis not a theory even in this technical sense. Since falsehoods always\nentail truths, F is not closed under the consequence\nrelation. (This is part of the reason we have no complementary\nexpression like the Falsth. The set of false sentences does\nnot describe a possible alternative to the actual world.) But\nF too is a perfectly good set of sentences. The consequences\nof any theory A that can be formulated in L will thus divide\nits consequences between T and F. Popper called the\nintersection of A and T, the truth content\nof A (AT), and \nthe intersection of A and F, the falsity \ncontent of A (AF). Any theory A is thus the \nunion of its non-overlapping truth content and falsity content. Note \nthat since every theory entails all logical truths, these will \nconstitute a special set, at the center of T, which will be \nincluded in every theory, whether true or false. \n\nA false theory will cover some of F, but because every false\ntheory has true consequences, it will also overlap with some of\nT (Diagram 1). \n\nA true theory, however, will only cover T (Diagram 2): \n\nAmongst true theories, then, it seems that the more true sentences\nthat are entailed, the closer we get to T, hence the more\ntruthlike.  Set theoretically that simply means that, where A\nand\nB are both true, A will be more truthlike than \nB just in case B is a proper subset of A \n(which for true theories means that BT is a proper subset of AT). Call this principle: the value of\ncontent for truths. \n\nThis essentially syntactic account of truthlikeness has some nice\nfeatures. It induces a partial ordering of truths, with the whole\nTruth T at the top of the ordering: T is closer to\nthe Truth than any other true theory. The set of logical truths is at\nthe bottom: further from the Truth than any other true theory. In\nbetween these two extremes, true theories are ordered simply by\nlogical strength: the more logical content, the closer to the Truth.\nSince probability varies inversely with logical strength, amongst\ntruths the theory with the greatest truthlikeness (T) must\nhave the smallest probability, and the theory with the largest\nprobability (the logical truth) is the furthest from the Truth.\nPopper made a bold and simple generalization of this. Just as truth\ncontent (coverage of T) counts in favor of truthlikeness,\nfalsity content (coverage of F) counts against. In general\nthen, a theory A is closer to the truth if it has more truth content\nwithout engendering more falsity content, or has less falsity content\nwithout sacrificing truth content (diagram 4): \n\nThe generalization of the truth content comparison also has some nice\nfeatures. It preserves the comparisons of true theories mentioned\nabove. The truth content AT\nof a false theory A (itself a theory in the Tarskian sense)\nwill clearly be closer to the truth than A (diagram 1). More\ngenerally, a true theory A will be closer to the truth than a\nfalse theory B provided\nA's truth content exceeds B's. \n\nDespite these nice features the account has a couple of disastrous\nconsequences.  Firstly, since a falsehood has some false consequences,\nand no truth has any, it follow that no falsehood can be as close to\nthe truth as a logical truth — the weakest of all truths.  A\nlogical truth leaves the location of the truth wide open, so it is\npractically worthless as an approximation to the whole truth.  So on\nPopper's account a falsehood is never more worthwhile than a worthless\nlogical truth.  (We could call this result the absolute\nworthlessness of falsehoods.  \nFurthermore, it is impossible to add a true consequence to a false\ntheory without thereby adding additional false consequences (or\nsubtract a false consequence without subtracting true\nconsequences). So the account entails that no false theory is closer\nto the truth than any other. We could call this result the\nrelative worthlessness of all falsehoods. These worthlessness\nresults were proved independently by Pavel Tichý and David\nMiller (Miller 1974, and Tichý 1974).  \nIt is instructive to see why this latter result holds. Let us suppose\nthat A and B are both false, and that A's\ntruth content exceeds B's. Let a be a true sentence\nentailed by\nA but not by B. Let f be any falsehood \nentailed by A. Since A entails both a and \nf the conjunction, a&f is a falsehood \nentailed by A, and so part of A's falsity content. \nIf a&f were also part of B's falsity \ncontent B would entail both a and f. But \nthen it would entail a contrary to the assumption. Hence \na&f is in A's falsity content and not \nin B's. So A's truth content cannot exceeds \nB's without A's falsity content also exceeding \nB's. Suppose now that B's falsity content exceeds \nA's. Let g be some falsehood entailed by B\nbut not by A, and let f, as before, be some \nfalsehood entailed by A. The sentence \nf→g is a truth, and since it is \nentailed by g, is in B's truth content. If it were \nalso in A's then both f and \nf→g would be consequences of \nA and hence so would g, contrary to the assumption.\nThus A's truth content lacks a sentence, \nf→g, which is in B's. So \nB's falsity content cannot exceeds A's without \nB's truth content also exceeding A's. The \nrelationship depicted in diagram 4 simply cannot obtain. \n\nIt is tempting at this point (and Popper was so tempted) to retreat \nto something like the comparison of truth contents alone. That is to \nsay, A is as close to the truth as B if A \nentails all of B's truth content, and A is closer \nto the truth than B just in case A is at least as \nclose as B, and B is not at least as close as \nA. Call this the Simple Truth Content account. \n\nThis Simple Truth Content account preserves Popper's ordering of true\npropositions. However, it also deems a false proposition the closer to\nthe truth the stronger it is. (Call this principle: the value of\ncontent for falsehoods.) According to this principle, since the\nfalse proposition that there are seven planets, and all of them\nare made of green cheese is logically stronger than the false\nproposition that there are seven planets the former is closer\nto the truth than the latter. So, once we know a theory is false we\ncan be confident that tacking on any old arbitrary proposition, no\nmatter how misleading it is, will lead us inexorably closer to the\ntruth. Amongst false theories, brute logical strength becomes\nthe sole criterion of a theory's likeness to truth. This is\nthe brute strength objection. \n\nPopper also dabbled in some measures of verisimilitude, based\non measures of content, which in turn he derived from measures of\nlogical probability (somewhat ironically, given his dim view of\nlogical probability). Unfortunately his measures suffered from defects\nvery similar to his purely qualitative proposals. \n\nAfter the failure of Popper's attempts to capture the notion of\ntruthlikeness, a number of variations on the content approach have\nbeen explored. Some stay within Popper's essentially syntactic\nparadigm, comparing classes of true and false sentences (e.g. Newton\nSmith 1981). Others make the switch to a more semantic paradigm,\nsearching for a plausible theory of distance between the\nsemantic content of sentences, construing these semantic contents as\nclasses of possibilities. A variant of this approach takes the class\nof models of a language as a surrogate for possible states of affairs\n(Miller 1978a).  The other utilizes a semantics of incomplete possible\nstates like those favored by structuralist accounts of scientific\ntheories (Kuipers 1987b). The idea which these share in common is that\nthe distance between two propositions is measured by the symmetric\ndifference of the two sets of possibilities. Roughly speaking,\nthe larger the symmetric difference, the greater the distance between\nthe two propositions.  Symmetric differences might be compared\nqualitatively – by means of set-theoretic inclusion - or\nquantitatively, using some kind of probability measure. Both can be\nshown to have the general features of a measure of distance. \n\nIf the truth is taken to be given by a complete possible world (or\nperhaps represented by a unique model) then we end up with results\nrather close to the truncated version of Popper's account, comparing\nby truth contents alone (Oddie 1978). In particular, amongst both\ntruths and falsehoods, one proposition is closer to the truth than\nanother the stronger it is. However, if we take the structuralist\napproach then we will take the relevant possibilities to be\n“small” states of affairs — small chunks of\nthe world, rather than an entire world — and then the\npossibility of more fine-grained distinctions between theories opens\nup. A rather promising exploration of this idea can be found in Volpe\n1995. \n\nThe fundamental problem with the original content approach lies not in\nthe way it has been articulated, but rather in the basic underlying\nassumption: that truthlikeness is a function of just two variables\n— content and truth value. This assumption has a number of\nrather problematic consequences. \n\nTwo things follow if truthlikeness is a function just of the logical\ncontent of a proposition and of its truth value. Firstly, any given\nproposition A can have only two degrees of verisimilitude:\none in case it is false and the other in case it is true. This is\nobviously wrong. A theory can be false in very many different ways.\nThe proposition that there are eight planets is false whether\nthere are nine planets or a thousand planets, but its degree of\ntruthlikeness is much higher in the first case than in the latter.  As\nwe will see below, the degree of truthlikeness of a true theory may\nalso vary according to where the truth lies. Secondly, if we combine\nthe value of content for truths and the value of content for\nfalsehoods, then if we fix truth value, verisimilitude will vary only\naccording to amount of content. So, for example, two equally strong\nfalse theories will have to have the same degree of verisimilitude.\nThat's pretty far-fetched. That there are ten planets and\nthat there are ten billion planets are (roughly) equally\nstrong, and both are false in fact, but the latter seems much further\nfrom the truth than the former. \n\nFinally, how might strength determine verisimilitude amongst false\ntheories? There seem to be just two plausible candidates: that\nverisimilitude increases with increasing strength (the principle of\nthe value of content for falsehoods) or that it decreases with\nincreasing strength (the principle of the disvalue of content for\nfalsehoods). Both proposals are at odds with attractive judgements and\nprinciples. One does not necessarily make a step toward the truth by\nreducing the content of a false proposition.  The proposition\nthat the moon is made of green cheese is logically stronger\nthan the proposition that either the moon is made of green cheese\nor it is made of dutch gouda, but the latter hardly seems a step\ntowards the truth. Nor does one necessarily make a step toward the\ntruth by increasing the content of a false theory. The false\nproposition that all heavenly bodies are made of green cheese\nis logically stronger than the false proposition all heavenly\nbodies orbiting the earth are made of green cheese but it doesn't\nseem to be an improvement. \n\nPopper crafted his initial proposal in terms of the true and false\nconsequences of a theory. Any sentence at all that follows from a\ntheory is counted as a consequence that, if true, contributes to its\noverall truthlikeness, and if false, detracts from that. But it has\nstruck many that this both involves an enormous amount of double\ncounting, and that it is the indiscriminate counting of arbitrary\nconsequences that lies behind the Tichý-Miller trivialization\nresult.  \n\nConsider a very simple framework with three primitive\nsentences: h (for the state hot), r (for\nrainy) and w (for windy). This framework\ngenerates a very small space of eight possibilities. The eight maximal\nconjunctions of the three primitive sentences express those\npossibilities. Suppose that in fact it is hot, rainy and windy (expressed by the\nmaximal conjunction h&r&w).  Then\nthe claim that it is cold, dry and still (expressed by the sentence\n~h&~r&~w) is further from the truth\nthan the claim that it is cold, rainy and windy (expressed by the\nsentence ~h&r&w). And the claim\nthat it is cold, dry and windy (expressed by the sentence\n~h&~r&w) is somewhere between the\ntwo. These kinds of judgements, which seem both innocent and\nintuitively correct, Popper's theory cannot accommodate.  And if they\nare to be accommodated we cannot treat all true and false consequences\nalike. For the three false claims mentioned here have exactly the same\nnumber of true and false consequences. \n\nClearly, if we are going to measure closeness to truth by counting\ntrue and false consequences, some true consequences have to count more\nthan others if we are to discriminate amongst them.  For\nexample, h and r are both true, and ~h and\n~r are false. The former should surely count in favor of a\nclaim, and the latter against. But ~h→~r is\ntrue and h→~r is false. After we have counted\nthe truth h in favor of a claim's truthlikeness and the\nfalsehood ~r against it, should we also count the true\nconsequence ~h→~r in favor, and the\nfalsehood h→~r against? Surely this is both\nunnecessary and misleading.  And it is precisely counting sentences\nlike these that renders Popper's account, in terms of all true and\nfalse consequences, susceptible to the Tichý-Miller\nargument. \n\nAccording to the consequence approach, Popper was right in thinking\nthat truthlikeness depends on the relative sizes of classes of true\nand false consequences, but erred in thinking that all consequences of\na theory count the same.  Some consequences are relevant,\nsome aren't. Let R be some criterion of relevance of\nconsequences; let AR be the set\nof relevant consequences of A. Whatever the\ncriterion R is it has to satisfy the constraint\nthat A be recoverable from (and hence equivalent to\n) AR. Popper's account is the limiting one —\nall consequences are relevant. (Popper's relevance criterion is the\nempty one, P, according to which AP is\njust A itself.) The relevant truth content of A\n(abbreviated ART) can be defined\nas AR∩T\n(or A∩TR), and similarly the relevant\nfalsity content of A can be defined\nas AR∩F. Since AR\n=\n(AR∩T)∪(AR∩F)\nit follows that the union of true and false relevant consequences\nof A is equivalent to A. And where A is\ntrue AR∩F is empty, so that A is\nequivalent to AR∩T alone.  With this restriction to relevant consequences we can basically\napply Popper's definitions: one theory is more truthlike than another\nif its relevant truth content is larger and its relevant falsity\ncontent no larger; or its relevant falsity content is smaller, and its\nrelevant truth content is no smaller. \nAlthough this basic idea was first explored by Mortensen in his 1983,\nexplicitly using a notion of relevant entailment, by the end\nof his paper Mortensen had abandoned the basic idea as unworkable, at\nleast within the framework of standard relevant logics. Others,\nhowever, have used the relevant consequence approach to avoid the\ntrivialization results that plague the content approach, and capture\nquite a few of the basic intuitive judgments of truthlikeness.\nSubsequent proposals within the broad program have been offered by\nBurger and Heidema 1994, Schurz and Weingartner 1987 and 2010, and\nGemes 2007. (Gerla also uses the notion of the relevance of a\n“test” or factor, but his account is best located more\nsquarely within the likeness approach.)  \n\nOne possible relevance criterion that\nthe h-r-w framework might suggest\nis atomicity. (See Cevolani, Festa and Kuipers 2013) for an\napproach along these lines.) But even if we could avoid the problem of\nsaying what it is for a sentence to be atomic, since many distinct\npropositions imply the same atomic sentences, this criterion would not\nsatisfy requirement that A be equivalent\nto AR. For example, (h∨r) and\n(~h∨~r), like tautologies, imply no atomic\nsentences at all.  \n\nBurger and Heidema 1994 compare theories by positive and negative\nsentences.  A positive sentence is one that can be constructed out of\n&, ∨and any true basic sentence (where a basic sentence is\neither an atomic sentence or its negation).  A negative sentence is\none that can be constructed out of &, ∨ and any false basic\nsentence.  Call a sentence pure if it is either positive or\nnegative. If we take the relevance criterion to be purity,\nand combine that with the relevant consequence schema above, we have\nBurger and Heidema's proposal, which yields a reasonable set intuitive\njudgments like those above.  Unfortunately purity (like atomicity)\ndoes not quite satisfy the constraint that A be equivalent to\nthe class of its relevant consequences.  For example, if h\nand r are both true then (~h∨r) and\n(h∨~r) both have the same pure consequences\n(namely, none).  \n\nSchurz and Weingartner 2010 use the following notion of\nrelevance S: being equivalent to a disjunction of atomic\npropositions or their negations.  With this criterion they can\naccommodate a range of intuitive judgments in the simple weather\nframework that Popper's account cannot. \n\n For example, where >S is the relation\n of greater S-truthlikeness we capture the following\n relations among false claims, which, on Popper's account, are mostly\n incommensurable:  \nand  \nThe relevant consequence approach faces three major hurdles.   The first is an extension problem: the approach does produce some\nintuitively acceptable results in a finite propositional framework,\nbut it needs to be extended to more realistic frameworks — for\nexample, first-order and higher-order frameworks.  Gemes's recent\nproposal in his 2007 is promising in this regard.  More research is\nrequired to demonstrate its adequacy. \n\nThe second is that, like Popper's original proposal, it judges no\nfalse proposition to be closer to the truth than any truth, including\nlogical truths. Schurz and Weingartner have answered this objection by\nextending their qualitative account to a quantitative account, by\nassigning weights to relevant consequences and summing. The problem\nwith this is that it assumes finite consequence classes. \n\nThe third involves the language-dependence of any adequate relevance\ncriterion.  This problem will be outlined and discussed below in\nconnection with the likeness approach (§1.4.4).  \n\nIn the wake of the collapse of Popper's articulation of the content\napproach two philosophers, working quite independently, suggested a\nradically different approach: one which takes the likeness in\ntruthlikeness seriously (Tichý 1974, Hilpinen 1976). This shift\nfrom content to likeness was also marked by an immediate shift from\nPopper's essentially syntactic approach (something it shares with the\nconsequence program) to a semantic approach, one which trafficks in\nthe contents of sentences.  \n\nTraditionally the semantic contents of sentences have been taken to be\nnon-linguistic, or rather non-syntactic, items —\npropositions. What propositions are is, of course, highly \ncontested, but most agree that a proposition carves the class of \npossibilities into two sub-classes — those in which the \nproposition is true and those in which it is false. Call the class of\nworlds in which the proposition is true its range. Some have\nproposed that propositions be identified with their ranges \n(for example, David Lewis, in his 1986). This identification is \nimplausible since, for example, the informative content of \n7+5=12 seems distinct from the informative content of \n12=12, which in turn seems distinct from the informative \ncontent of Gödel's first incompleteness theorem – and yet all \nthree have the same range. They are all true in all possible worlds. \nClearly if semantic content is supposed to be sensitive to \ninformative content, classes of possible worlds will not be \ndiscriminating enough. We need something more fine-grained for a full\ntheory of semantic content.  Despite this, the range of a proposition is certainly an important\naspect of informative content, and it is not immediately obvious why\ntruthlikeness should be sensitive to differences in the way a\nproposition picks out its range. (Perhaps there are cases of logical\nfalsehoods some of which seem further from the truth than others. For\nexample 7+5=113 might be considered further from the truth\nthan 7+5=13 though both have the same range — namely,\nthe empty set of worlds. See Sorenson 2007.) But as a first\napproximation to the concept, we will assume that it is not\nhyperintensional and that logically equivalent propositions have the\nsame degree of truthlikess. The proposition that the number of\nplanets is eight for example, should have the same degree of\ntruthlikeness as the proposition that the square of the number of\nthe planets is sixty four. \n\nThere is also not a little controversy over the nature of possible \nworlds. One view — perhaps Leibniz's and more recently David \nLewis's — is that worlds are maximal collections of possible \nthings. Another — perhaps the early Wittgenstein's \n— is that possible worlds are complete possible ways for \nthings to be. On this latter state-conception, a world is a \ncomplete distribution of properties, relations and magnitudes over \nthe appropriate kinds of entities. Since invoking “all” properties, \nrelations and so on will certainly land us in paradox, these \ndistributions, or possibilities, are going to have to be relativized \nto some circumscribed array of properties and relations. Call the \ncomplete collection of possibilities, given some array of features, \nthe logical space, and call the array of properties and \nrelations which underlie that logical space, the framework of\nthe space. \n\nFamiliar logical relations and operations correspond to \nwell-understood set-theoretic relations and operations on ranges. The\nrange of the conjunction of two proposition is the intersection of \nthe ranges of the two conjuncts. Entailment corresponds to the subset\nrelation on ranges. The actual world is a single point in logical \nspace — a complete specification of every matter of fact (with \nrespect to the framework of features) — and a proposition is \ntrue if its range contains the actual world, false otherwise. The \nwhole Truth is a true proposition that is also complete: it entails \nall true propositions. The range of the Truth is none other than the \nsingleton of the actual world. That singleton is the target, the \nbullseye, the thing at which the most comprehensive inquiry is \naiming. \n\nWithout additional structure on the logical space we have just three\nfactors for a theorist of truthlikeness to work with — the size\nof a proposition (content factor), whether it contains the actual\nworld (truth factor), and which propositions it implies (consequence\nfactor).  The likeness approach requires some additional structure to\nthe logical space. For example, worlds might be more or less like\nother worlds. There might be a betweenness relation amongst worlds, or\neven a fully-fledged distance metric. If that's the case we can start\nto see how one proposition might be closer to the Truth — the\nproposition whose range singles out the actual world — than\nanother. The core of the likeness approach is that the truthlikeness\nof a proposition supervenes on the likeness between worlds, or the\ndistance between worlds.   \nThe likeness theorist has two initial tasks: firstly, making it\nplausible that there is an appropriate likeness or distance function\non worlds; and secondly, extending likeness between individual worlds\nto likeness of propositions (i.e. sets of worlds) to the actual\nworld.  \t\nSuppose, for example, that worlds are arranged \nin similarity spheres nested around the actual world, familiar from \nthe Stalnaker-Lewis approach to counterfactuals. Consider Diagram 5. \n\nThe bullseye is the actual world and the small sphere which includes \nit is T, the Truth. The nested spheres represent likeness to\nthe actual world. A world is less like the actual world the larger \nthe first sphere of which it is a member. Propositions A and\nB are false, C and D are true. A carves \nout a class of worlds which are rather close to the actual world \n— all within spheres two to four — whereas B \ncarves out a class rather far from the actual world — all \nwithin spheres five to seven. Intuitively A is closer to the\nbullseye than is B. \n\nThe largest sphere which does not overlap at all with a proposition \nis plausibly a measure of how close the proposition is to being true.\nCall that the truth factor. A proposition X is \ncloser to being true than Y if the truth factor of \nX is included in the truth factor of Y. The truth \nfactor of A, for example, is the smallest non-empty sphere, \nT itself, whereas the truth factor of B is the \nfourth sphere, of which T is a proper subset. \n\nIf a proposition includes the bullseye then of course it is true \nsimpliciter, it has the maximal truth factor (the empty set). So all \ntrue propositions are equally close to being true. But truthlikeness \nis not just a matter of being close to being true. The tautology, \nD, C and the Truth itself are equally true, but in \nthat order they increase in their closeness to the whole truth. \n\nTaking a leaf out of Popper's book, Hilpinen argued that closeness to\nthe whole truth is in part a matter of degree of informativeness of a\nproposition. In the case of the true propositions, this correlates \nroughly with the smallest sphere which totally includes the \nproposition. The further out the outermost sphere, the less \ninformative the proposition is, because the larger the area of the \nlogical space which it covers. So, in a way which echoes Popper's \naccount, we could take truthlikeness to be a combination of a truth \nfactor (given by the likeness of that world in the range of a \nproposition that is closest to the actual world) and a content factor\n(given by the likeness of that world in the range of a proposition \nthat is furthest from the actual world): \n\nApplying Hilpinen's definition we capture two more particular \njudgements, in addition to those already mentioned, that seem \nintuitively acceptable: that C is closer to the truth than \nA, and that D is closer than B. (Note, \nhowever, that we have here a partial ordering: A and \nD, for example, are not ranked.) We can derive from this\nvarious apparently desirable features of the relation closer to\nthe truth: for example, that the relation is transitive,\nasymmetric and irreflexive; that the Truth is closer to the Truth than\nany other theory; that the tautology is at least as far from the Truth\nas any other truth; that one cannot make a true theory worse by\nstrengthening it by a truth (a weak version of the value of content\nfor truths); that a falsehood is not necessarily improved by adding\nanother falsehood, or even by adding another truth (a repudiation of\nthe value of content for falsehoods). \n\nBut there are also some worrying features here. While it avoids the\nrelative worthlessness of falsehoods, Hilpinen's account, just like\nPopper's, entails the absolute worthlessness of all falsehoods: no\nfalsehood is closer to the truth than any truth. So, for example,\nNewton's theory is deemed to be no more truthlike, no closer to the\nwhole truth, than the tautology. \n\nCharacterizing Hilpinen's account as a combination of a truth factor \nand an information factor seems to mask its quite radical departure \nfrom Popper's account. The incorporation of similarity spheres \nsignals a fundamental break with the pure content approach, and opens\nup a range of possible new accounts: what such accounts have in \ncommon is that the truthlikeness of a proposition is a \nnon-trivial function of the likeness to the actual world of \nworlds in the range of the proposition. \n\nThere are three main problems for any concrete proposal within the\nlikeness approach. The first concerns an account of likeness between\nstates of affairs – in what does this consist and how can it be\nanalyzed or defined? The second concerns the dependence of the\ntruthlikeness of a proposition on the likeness of worlds in its range\nto the actual world: what is the correct function? (This can be called\n“the extension problem”.) And finally, there is the famous\nproblem of “translation variance” or “framework\ndependence” of judgements of likeness and of truthlikeness. This\nlast problem will be taken up in §1.4.4. \n\nOne objection to Hilpinen's proposal (like Lewis's proposal for\ncounterfactuals) is that it assumes the similarity relation on worlds\nas a primitive, there for the taking. At the end of his 1974 paper\nTichý not only suggested the use of similarity rankings on\nworlds, but also provided a ranking in propositional frameworks and\nindicated how to generalize this to more complex frameworks.  \n\nExamples and counterexamples in Tichý 1974 are exceedingly\nsimple, utilizing the little propositional framework introduced above,\nwith three primitives — h (for the\nstate hot), r (for\nrainy) and w (for windy).  \n\nCorresponding to the eight-members of the logical space generated by\ndistributions of truth values through the three basic conditions,\nthere are eight maximal conjunctions (or constituents): \n\nWorlds differ in the distributions of these traits, and a natural,\nalbeit simple, suggestion is to measure the likeness between two\nworlds by the number of agreements on traits. This is tantamount to\ntaking distance to be measured by the size of the symmetric difference\nof generating states — the so-called city-block measure. As is\nwell known, this will generate a genuine metric, in particular\nsatisfying the triangular inequality. If w1 is the actual\nworld this immediately induces a system of nested spheres, but one in\nwhich the spheres come with numbers attached:  \n\nThose worlds orbiting on the sphere n are of distance\nn from the actual world. \nIn fact the structure of the space is better represented not by\nsimilarity circles, but rather but rather by a three-dimensional\ncube: This way of representing the space makes a clearer connection\nbetween distances between worlds and the role of the atomic\npropositions in generating those distances through the city-block\nmetric. It also eliminates inaccuracies in the relations between the\nworlds that are not at the center that the similarity circle diagram\nsuggests. \n\nNow that we numerical distances between worlds, numerical measures of\npropositional likeness to, and distance from, the truth can be defined\nas some function of the distances, from the actual world, of worlds in\nthe range of a proposition. But which function is the right one? This\nis the  extension problem . Suppose, once more, that h&r&w\nis the whole truth about the matter of the weather. Following\nHilpinen's lead, we might consider overall distance of a propositions\nfrom the truth to be some function of the distances from actuality of\ntwo extreme worlds. Let truth(A) be the truth value\nof A in the actual world. Let min(A) be the\ndistance from actuality of that world in A closest to the\nactual world, and max(A) be the distance from actuality\nof that world in A furthest from the actual world.  The simplest proposal (made first in Niiniluoto 1977) would be to\ntake the average of these two quantities might (call this\nmeasure min-max-average). This would remedy a rather glaring\nshortcoming which Hilpinen's qualitative proposal shares with Popper's\noriginal proposal, namely that no falsehood is closer to the truth\nthan any truth (even the worthless tautology). This numerical\nequivalent of Hilpinen's proposal renders all propositions comparable\nfor truthlikeness, and some falsehoods it deems more truthlike than\nsome truths. But now that we have distances between all worlds, why take only\nthe extreme worlds in a proposition into account? Why shouldn't every\nworld in a proposition potentially count towards its overall distance\nfrom the actual world?  A simple measure which does count all worlds is average distance\nfrom the actual world.  Average delivers all of the particular\njudgements we used above to motivate Hilpinen's proposal in the first\nplace, and in conjunction with the simple metric on worlds it delivers\nthe following ordering of propositions in our simple framework: \n\nThis ordering look quite promising. Propositions are closer to \nthe truth the more they get the basic weather traits right, further \naway the more mistakes they make. A false proposition may be made \neither worse or better by strengthening (~w is the same \ndistance from the Truth as ~h; \nh&r&~w is better than ~w \nwhile ~h&~r&~w is worse). A false \nproposition (like h&r&~w) can be \ncloser to the truth than some true propositions (like h). These judgments may be sufficient to show that average is superior\nto min-max-average), at least on this group of propositions,\nbut they are clearly not sufficient to show that averaging is the\nright procedure. What we need are some straightforward and compelling\ngeneral desiderata which jointly yield a single correct function. In\nthe absence of such a proof, we can only resort to case by case\ncomparisons.  Furthermore average has by no means found universal favor\non the score of particular judgments either. Notably, there are pairs\nof true propositions such that the average measure deems the stronger\nof the two to be the further from the truth. According to the average\nmeasure, the tautology, for example, is not the true proposition\nfurthest from the truth.  Averaging thus violates the Popperian\nprinciple of the value of content for truths (Popper 1976). Consider the sum function — the sum of the distances\nof worlds in the range of a proposition from the actual world. \n\nThe sum function is an interesting measure in its own right,\nthough no one has proposed it as a stand-alone account of closeness to\ntruth.  Although sum, like average is sensitive to the\ndistances of all worlds in a proposition from the actual world, it is\nnot plausible as a measure of distance from the truth.\nWhat sum does measure is a special kind logical weakness. In\ngeneral the weaker a proposition is, the larger its sum\nvalue. But adding words far from the actual world makes the sum\nvalue larger than adding worlds closer in. This guarantees, for\nexample, that of two truths the sum of the logically weaker is\nalways greater than the sum of the stronger.  Thus sum\nmight play a role in capturing the value of content for truths.  But\nit also delivers the implausible value of content for falsehoods.  If\nyou think that there is anything to the likeness program it is hardly\nplausible that the falsehood\n~h&~r&~w is closer to the truth\nthan its consequence ~h.  Niiniluoto argues that sum\nis a good likeness-based candidate for measuring Hilpinen's\n“information factor”. It is obviously much more sensitive\nthan is max to the proposition's informativeness about the\nlocation of the truth. \nNiiniluoto thus proposes, as a measure of distance from the truth, the\naverage of this information factor and Hilpinen's truth factor:\nmin-sum-average. Averaging the more sensitive information\nfactor (sum) and the closeness-to-being-true factor\n(min) yields some interesting results.  For example, this\nmeasure deems h&r&w more truthlike\nthan h&r, and the latter more truthlike than\nh. And in general min-sum-average delivers the value\nof content for truths. For any two truths the min factor is the\nsame (0), and the sum factor increases as content decreases.\nFurthermore, unlike the symmetric difference measures,\nmin-sum-average doesn't deliver the objectionable value of\ncontents for falsehoods.  For example,\n~h&~r&~w is deemed further from the\ntruth than ~h.  But min-sum-average is not quite home\nfree.  For example from an intuitive point of view. For example,\n~h&~r&~w is deemed closer to the\ntruth than ~h&~r. This is because what\n~h&~r&~w loses in closeness to the\nactual world (min) it makes up for by an increase in strength\n(sum). In deciding how to proceed here we confront a methodological\nproblem.  The methodology favored by Tichý is very much\nbottom-up.  For the purposes of deciding between rival accounts it\ntakes the intuitive data very seriously. Popper (along with Popperians\nlike Miller) favor a more top-down approach. They are deeply\nsuspicious of folk intuitions, and sometimes appear to be in the\nbusiness of constructing a new concept rather than explicating an\nexisting one.  They place enormous weight on certain plausible general\nprinciples, largely those that fit in with other principles of their\noverall theory of science: for example, the principle that strength is\na virtue and that the stronger of two true theories (and maybe even of\ntwo false theories) is the closer to the truth. A third approach, one\nwhich lies between these two extremes, is that of reflective\nequilibrium. This recognizes the claims of both intuitive judgements\non low-level cases, and plausible high-level principles, and enjoins\nus to bring principle and judgement into equilibrium, possibly by\ntinkering with both. Neither intuitive low-level judgements nor\nplausible high-level principles are given advance priority. The\nprotagonist in the truthlikeness debate who has argued most\nconsistently for this approach is Niiniluoto. \n\nHow might reflective equilibrium be employed to help resolve the\ncurrent dispute? Consider a different space of possibilities,\ngenerated by a single magnitude like the number of the planets\n(N). Suppose that N is in fact 8 and that the further n is from\n8, the further the proposition that N=n from the\nTruth. Consider three sets of propositions. In the left-hand column we\nhave a sequence of false propositions which, intuitively, decrease in\ntruthlikeness while increasing in strength. In the middle column we\nhave a sequence of corresponding true propositions, in each case the\nstrongest true consequence of its false counterpart on the left\n(Popper's “truth content”). Again members of this sequence\nsteadily increase in strength. Finally on the right we have another\ncolumn of falsehoods. These are also steadily increasing in strength,\nand like the left-hand falsehoods, seem (intuitively) to be decreasing\nin truthlikeness as well. \n\nJudgements about the closeness of the true propositions in the center\ncolumn to the truth may be less intuitively clear than are judgments\nabout their left-hand counterparts. However, it would seem highly\nincongruous to judge the truths in table 4 to be steadily increasing\nin truthlikeness, while the falsehoods both to the left and the right,\nboth marginally different in their overall likeness relations to\ntruth, steadily decrease in truthlikeness. This suggests that that all\nthree are sequences of steadily increasing strength combined with\nsteadily decreasing truthlikeness. And if that's right, it\nmight be enough to overturn Popper's principle that amongst true\ntheories strength and truthlikeness must covary (even while granting\nthat this is not so for falsehoods). If this argument is sound, it removes an objection to averaging\ndistances, but it does not settle the issue in its favor, for there\nmay still be other more plausible counterexamples to averaging that we\nhave not considered. \nSchurz and Weingartner argue that this extension problem is the main\ndefect of the likeness approach: One way of answering this objection head on is to identify\nprinciples that, given a distance function on worlds, constrain the\ndistances between worlds and sets of worlds, principles perhaps\npowerful enough to identify a unique extension.  (An argument of this\nkind is considered in §1.5.) \n\nSimple propositional examples are convenient for the purposes of\nillustration, but what the likeness approach needs is some evidence\nthat it can transcend such simple examples. (Popper's content\napproach, whatever else its shortcomings, can be applied in principle\nto theories expressible in any language, no matter how sophisticated.)\nCan the likeness program be generalized to arbitrarily complex\nframeworks?  For example, does the idea extend even to first-order\nframeworks the possible worlds of which may well be infinitely\ncomplex? There is no straightforward, natural or obvious way to construct\ndistance or likeness measures on worlds considered as non-denumerably\ninfinite collections of basic or “atomic” states.  But a\nfruitful way of implementing the likeness approach in frameworks that\nare more interesting and realistic than the toy propositional weather\nframework, involves cutting the space of possibilities down into\nmanageable chunks. This is not just an ad hoc response to the\ndifficulties imposed by infinite states, but is based on a principled\nview of the nature of inquiry — of what constitutes\nan inquiry, a question, a query, a cognitive\nproblem, or a subject matter. Although these terms might\nseem to signify quite different notions they all have something deep\nin common. Consider the notion of a query or a question. Each\nwell-formed question Q receives an answer in each complete\npossible state of the world.  Two worlds are equivalent with\nrespect to the question Q if they receive the same\nanswer in both worlds.  Given that Q induces an equivalence\nrelation on worlds, the question partitions the logical space into a\nset of mutually exclusive and jointly exhaustive cells:\n{C1, C2,\n... , Ci, ...}. Each\ncell Ci is a complete possible answer to the\nquestion Q.  And each incomplete answer to Q is\ntantamount to the union, or disjunction, of some collection of the\ncomplete answers. For example, the question what is the number of\nthe planets? partitions the set of worlds into those in which N=0,\nthose in which N=1, ..., and so on. The question what is the state\nof the weather? (relative to our three toy factors) partitions the\nset of worlds into those in which is hot, rainy and windy, those in\nwhich it is hot, rainy and still, ... and so on. (See Oddie 1986a.)\nIn other words, the so-called “worlds” in the simple\nweather framework are in fact surrogates for large classes of worlds,\nand in each such class the answer to the coarse-grained weather\nquestion is the same. The elements of this partition are all complete\nanswers to the weather question, and distances between the elements of\nthe partition can be handled rather easily as we have seen.\nEssentially the same idea can be applied to much more complex\nquestions. Niiniluoto characterizes the notion of a cognitive problem\nin the same way, as a partition of a space of possibilities.  And this\nsame explication has been remarkably fruitful in clarifying the\nnotions of aboutness and of a subject matter (Oddie\n1986a, Lewis 1988).   Each inquiry is based on some question that admits of a range of\npossible complete answers, which effect a partition\n{C1, C2,\n... , Cn, ...} of the space of worlds.\nIncomplete answers to the question are equivalent to disjunctions of\nthose complete answers which entail the incomplete answer.  Now, often\nthere is a rather obvious measure of distance between the elements of\nsuch partitions, which can be extended to a measure of distance of an\narbitrary answer (whether complete or partial) from the true\nanswer. One rather promising source of natural partitions (Tichý\n1976, Niiniluoto 1977) are Hintikka's distributive normal\nforms (Hintikka 1963).  These are smooth generalizations of the\nfamiliar maximal conjunctions of propositional frameworks used\nabove. What corresponds to the maximal conjunctions are known\nas constituents, which, like maximal conjunctions in the\npropositional case, are jointly exhaustive and mutually\nexclusive. Constituents lay out, in a very perspicuous manner, all the\ndifferent ways individuals can be related to each other—\nrelative, of course, to some collection of basic attributes. (These\ncorrespond to the collection of atomic propositions in the\npropositional case.)  The basic attributes can be combined by the\nusual boolean operations into complex attributes which are also like\nthe maximal conjunctions, in that they form a partition of the class\nof individuals.  These complex predicates are\ncalled Q-predicates in the simple monadic case,\nand attributive constituents in the more general case.\nConstituents specify, for each member in this set of mutually\nexclusive and jointly exhaustive complex attributes, whether or not\nthat complex attribute is exemplified. In the simplest case a framework of three basic monadic attributes\n(F,G and H) gives rise to eight complex\nattributes: — Carnap's Q-predicates:  Q1\n     F(x)&G(x)&H(x), Q2     \nF(x)&G(x)&~H(x) … Q8     \n~F(x)&~G(x)&~H(x) The simplest constituents run through these Q – predicates\n(or attributive constituents) specifying for each one whether\nor not it is instantiated. C1     \n∃xQ1(x) &\n∃xQ2(x) &\n... &\n∃xQ7(x) &\n∃xQ8(x)  C2      \n∃xQ1(x) &\n∃xQ2(x) &\n... &\n∃xQ7(x) &\n~∃xQ8(x) … C255 ~∃xQ1(x) &\n~∃xQ2(x) &\n... &\n~∃xQ7(x) &\n∃xQ8(x)  C256 ~∃xQ1(x) &\n~∃xQ2(x) &\n... & ~\n∃xQ7(x) &\n~∃xQ8(x)  In effect the partition induced by the set constituents corresponds\nto the question: Which Q-predicates are instantiated?  A\ncomplete answer to that question will specify one of the constituents,\nan incomplete answer will specify a disjunction of such. Note that the last constituent in this listing,\nC256, is logically inconsistent. Given that the set\nof Q-predicates is jointly exhaustive, each individual has to\nsatisfy one of them, so at least one of them has to be\ninstantiated. Once such inconsistent constituents are omitted, every\nsentence in a monadic language (without identity or constants) is\nequivalent to a disjunction of a unique set of these constituents. (In\nthe simple monadic case here there is only one inconsistent\nconstituent, but in the general case there are many inconsistent\nconstituents. Identifying inconsistent constituents is equivalent to\nthe decision problem for first-order logic.) For example, the sentence\n∀x(F(x)&G(x)&H(x))\nis logically equivalent to: The sentence\n∀x(~F(x)&G(x)&H(x))\nis logically equivalent to: The sentence\n∀x(~F(x)&~G(x)&~H(x))\nis logically equivalent to: C255     ~∃xQ1(x) & \n~∃xQ2(x) &\n... &~\n∃xQ7(x) &\n∃xQ8(x). More typically, a sentence is equivalent to disjunction of constituents. For example the sentence ∀x(G(x)&H(x)) is logically equivalent to: \nAnd the sentence\n∀x(F(x)∨G(x)∨H(x))\nis logically equivalent to: Before considering whether distributive normal forms (disjunctions\nof constituents) can be generalized to more complex and interesting\nframeworks (involving relations and greater quantificational\ncomplexity) it is worth thinking about how to define distances between\nthese constituents, since they are the simplest of all. \nIf we had a good measure of distance between constituents then\npresumably the distance of some disjunction of constituents from one\nparticular constituent could be obtained using the right extension\nfunction (§1.4.2) whatever that happens to be. And then the\ndistance of an arbitrary expressible proposition from the truth could\nbe defined as the distance of its normal form from the true\nconstituent.  As far as distances between constituents go, there is an obvious\ngeneralization of the city-block (or symmetric difference) measure on\nthe corresponding maximal conjunctions (or propositional\nconstituents): namely, the size of the symmetric difference of\nthe sets of Q-predicates associated with each constituent.  So,\nfor example, since C1 and C2\ndiffer on the instantiation of just one Q-predicate\n(namelyQ8) the distance between them would be 1. At\nthe other extreme, since C2\nand C255 disagree on the instantiation of\nevery Q-predicate, the distance between them is 8\n(maximal). Niiniluoto first proposed this as the appropriate distance measure\nfor these constituents in his 1977, and since he found a very similar\nproposal in Clifford, he called it the Clifford measure. Since\nit is based on symmetric differences it yields a function that\nsatisfies the triangular inequality (the distance between X\nand Z is no greater than the sum of the distances\nbetween X and Y and Y and Z).  Note that on the Clifford measure the distance\nbetween C64 (which says that Q1 is\nthe only instantiated Q-predicate) and C128\n(which says that Q2 is the only\ninstantiated Q-predicate) is the same as the distance\nbetween C64 and C255 (which says\nthat Q8 is the only\ninstantiated Q-predicate). C64 says that everything has F, G\nand H; C128, that everything has\n~F, G and H; C255, that\neverything has ~F, ~G and ~H. If the likeness\nintuitions in the weather framework have anything to them,\nthen C128 is closer to C64\nthan C255 is.  Tichý (1976 and 1978) captured\nthis intuition by first measuring the distances\nbetween Q-predicates using the city-block measure. Then the\nbasic idea is to extend this distances between sets\nof Q-predicates by identifying the “minimal routes”\nfrom one set of such Q-predicates to another. The distance\nbetween two sets of Q-predicates is the distance along some\nminimal route. \nSuppose that two normal forms feature the same number of constituents.\nThat is, the two sets of Q-predicates associated with two\nconstituents are the same size. Let a linkage be any 1-1\nmapping from one set to the other. Let the breadth of the\nlinkage be the average distance between linked items.  Then according\nto Tichý the distance between two constituents (or their\nassociated sets of Q-predicates) is the breadth of the\nnarrowest linkage between them. If one constituent C contains\nmore Q-predicates than the other then a linkage is defined as a\nsurjection from the larger set to the smaller set, and the rest of the\ndefinition is the same.  This idea certainly captures the intuition concerning the relative\ndistances between C128, C64\nand C255.  But it suffers the following defect: it\ndoes not satisfy the triangular inequality. This is because in a\nlinkage between sets with different numbers of members,\nsome Q-predicates will be unfairly represented, thereby\npossibly distorting the intuitive distance between the sets.  This can\nbe remedied, however, by altering the account of a linkage to make\nthem fair. The details need not detain us here (for those see\nOddie 1986a) but in a fair linkage each Q-predicate in a\nconstituent receives the same weight in the linkage as it possesses in\nthe constituent itself. The simple Clifford measure ignores the distances between\nthe Q-predicates, but it can be modified and refined in various\nways to take account of such distances (see Niiniluoto 1987).   \nHintikka showed that the distributive normal form theorem can be\ngeneralized to first-order sentences of any degree of relational or\nquantificational complexity.  Any sentence of a first-order sentence\ncomes with a certain quantificational depth —the number\nof its overlapping quantifiers. So, for example, analysis in the\nfirst-order idiom would reveal that (1) is a depth-1 sentence; (2) is\na depth-2 sentence; and (3) is a depth-3 sentence.  \n\nWe could call a proposition depth-d if the shallowest depth\nat which it can be expressed is d. Hintikka showed how to\ndefine both attributive constituents and constituents of\ndepth-d recursively, so that every depth-d\nproposition can be expressed by a disjunction of a unique set of\n(consistent) depth-d constituents.  Constituents can be \nrepresented as finite tree-structures, the nodes of which are all like \nQ-predicates – that is, strings of atomic formulas either\nnegated or unnegated. Consequently, if we can measure distance between\nsuch trees we will be well down the path of measuring the\ntruthlikeness of depth-d propositions – it will be some\nfunction of the distance of constituents in its normal form from the\ntrue depth-d constituent. And we can measure the distance between such\ntrees using the same idea of a minimal path from one to the other.\nThis too can be spelt out in terms of fair linkages between trees,\nthus guaranteeing the triangular inequality. \n\nThis program for defining distance between expressible propositions\nhas proved quite flexible and fruitful, delivering a wide range of\nintuitively appealing results, at least in simple first-order cases.\nAnd the approach can be further refined – for example, by means\nof Carnap's concept of a family of properties. There are yet more extension problems for the likeness\nprogram. First-order theories can be infinitely deep — that is\nto say, there is no finite depth at which such a theory theory can be\nexpressed. The very feature of first-order constituents that makes\nthem manageable — their finite structure — ensures that\nthey cannot express propositions that are not finitely axiomatizable\nin a first-order language.  Clearly it would be desirable for an\naccount of truthlikeness to be able, in principle, to rank\npropositions that go beyond the expressive power of first-order\nsentences.  First-order sentences lack the expressive power of\nfirst-order theories in general, and first-order languages lack the\nexpressive resources of higher-order languages. A theory of\ntruthlikeness that applies only to first-order sentences, while not a\nnegligible achievement, would still have limited application.  There are plausible ways of extending the normal-form approach to\nthese more complex cases.  The relative truthlikeness of infinitely\ndeep first-order propositions can be captured by taking limits as\ndepth increases to infinity (Oddie 1978, Niiniluoto 1987). And\nHintikka's normal form theorem, along with suitable measures of\ndistance between higher-order constituents, can be extended to embrace\nhigher-order languages (These higher-order “permutative\nconstituents” are defined in Oddie 1986a and a normal form\ntheorem proved.) Since there are propositions that are only\nexpressible by a non-finitely axiomatizable theory in first-order\nlogic that can be expressed by a single sentence of higher-order logic\nthis greatly expands the reach of the normal-form approach. Although scientific progress sometimes seems to consist in the\naccumulation facts about the existence of certain kinds of individuals\n(witness the recent excitement evoked by the discovery of the Higgs\nboson, for example) mostly the scientific enterprise is concerned with\ncausation and laws of nature.  L. J. Cohen has accused the\ntruthlikeness program of completely ignoring this aspect of the aim of\nscientific inquiry.  The discovery of instances of new kinds (as well as the inability\nto find them when a theory predicts that they will appear under\ncertain circumstances) is generally welcomed because of the light\nsuch discoveries throw on the structure of the laws of nature. The\ntrouble with first-order languages is that they lack adequate\nresources to represent genuine laws.  Extensional first-order logic\nembodies the Humean thesis that there are no necessary connections\nbetween distinct existences. At best first-order theories can capture\npervasive, but essentially accidental, regularities. There are various ways of modeling laws.  According to the theory\nof laws favored by Tooley 1977 and Armstrong 1983, laws of nature\ninvolve higher-order relations between first-order universals.  These,\nof course, can be represented easily in a higher-order language (Oddie\n1982). But one might also capture lawlikeness in a first-order modal\nlanguage supplemented with sentential operators for natural necessity\nand natural possibility (Niiniluoto 1983). And an even simpler\nrepresentation in terms of nomic conjunctions in a propositional\nframework, which utilizes the simple city-block measure on partial\nconstituents, has recently been developed in Cevolani, Festa and\nKuipers 2012.  Another problem for the normal-form approach involves the fact that\nmost interesting theories in science are not expressible in frameworks\ngenerated by basic properties and relations alone, but rather require\ncontinuous magnitudes — world-dependent functions from various\nentities to numbers.  All the normal-form based accounts (even those\ndeveloped for the higher-order) are for systems that involve basic\nproperties and relations, but exclude basic functions.  Of course,\nmagnitudes can be finitely approximated by Carnap's families of\nproperties, and relations between those can be represented in\nhigher-order frameworks, but if those families are infinite then we\nlose the finitude characteristic of regular constituents.  Once we\nlose those finite representations of states, the problem of defining\ndistances becomes intractable again.  There is a well-known way of reducing n-argument functions\nto n+1-place relations.  One could represent a magnitude\nof n-tuples of objects as a relation between n objects\nand numbers.  One would have to include the set of real or complex\nnumbers in the domain of individuals, or else have multiple domains at\nthe lowest level.  However, not only is that a somewhat ad hoc\nsolution to the problem, but no interesting applications based on such\na reduction have been explored. So each space has to be treated\ndifferently and the resulting theory lacks a certain unity. It would\nclearly be preferable to be able to give a unified treatment with\ndifferent applications.  It would also be preferable to be able to\ntreat basic magnitudes on a par with basic properties and relations in\nthe generation of logical space.  Niiniluoto (in his 1987) showed how\nto extend the likeness approach to functional spaces (see also\nKieseppa 1996) but so far there is no good extension of the\ndistributive normal-form approach that both combines functions with\nproperties and relations, and reduces the infinite complexity that\nthese generate to finite or manageable proportions.  Until such an\naccount is forthcoming, the normal form approach does not provide a\ncomprehensive theory of closeness to truth.   The problem of complexity might force a reconsideration of the\nmethodology here. Instead of trying to define specific distance\nmeasures in a bottom-up piecemeal way, a more modest, more tractable,\ntop-down approach might be to posit the existence of a (possibly\npartial) relation of likeness on worlds, satisfying various plausible\nstructural features and desirable principles.  Assuming these\nprinciples are consistent, this would in turn determine a class of\nnumerical distance functions (all those that represent or realize\nthem).  Plausible principles could then be explored to narrow the\nclass of admissible likeness functions. This idea is developed a\nlittle further in §1.5. But before we go there the most important\nobjection to the likeness approach needs to be addressed. \n\nThe single most powerful and influential argument against the whole\nlikeness approach is the charge that it is “language\ndependent” or “framework dependent” (Miller 1974a,\n1975 a, 1976, and most recently defended, vigorously as usual, in his\n2006). Early formulations of the likeness approach (Tichý 1974,\n1976, Niiniluoto 1976) proceeded in terms of syntactic surrogates for\ntheir semantic correlates — sentences for propositions,\npredicates for properties, constituents for partitions of the logical\nspace, and the like. The question naturally arises, then, whether we\nobtain the same measures if all the syntactic items are translated\ninto an essentially equivalent language — one capable of\nexpressing the same propositions and properties with a different set\nof primitive predicates.  Newton's theory can be formulated with a\nvariety of different primitive concepts, but these formulations are\ntypically taken to be equivalent. If the degree of truthlikeness of\nNewton's theory were to vary from one such formulation to another,\nthen while such a concept might still might have useful applications,\nit would hardly help to vindicate realism.   \n\nTake our simple weather-framework above. This trafficks in three \nprimitives — hot, rainy, and windy.  \nSuppose, however, that we define the following two new weather \nconditions: minnesotan   =df  \nhot if and only if rainy  arizonan   =df   hot if and \nonly if windy \n\nNow it appears as though we can describe the same sets of weather\nstates in an h-m-a-ese based on these\nconditions.  \n\nIf T is the truth about the weather then theory A, \nin h-r-w-ese, seems to make just one error\nconcerning the original weather states, while B makes two \nand C makes three. However, if we express these two theories\nin h-m-a-ese however, then this is \nreversed: A appears to make three errors and B \nstill makes two and C makes only one error. But that means \nthe account makes truthlikeness, unlike truth, radically \nlanguage-relative. \n\nThere are two live responses to this criticism.  But before detailing \nthem, note a dead one: the similarity theorist cannot object that \nh-m-a is somehow logically inferior to \nh-r-w, on the grounds that the primitives \nof the latter are essentially “biconditional” whereas the primitives \nof the former are not. This is because there is a perfect symmetry \nbetween the two sets of primitives. Starting within \nh-m-a-ese we can arrive at the original \nprimitives by exactly analogous definitions: \n\nThus if we are going to object to \nh-m-a-ese it will have to be on other than\npurely logical grounds.  \n\nFirstly, then, the similarity theorist could maintain that certain \npredicates (presumably “hot”, “rainy” and \n“windy”) are primitive in some absolute, realist, sense. \nSuch predicates “carve reality at the joints” whereas \nothers (like “minnesotan” and “arizonan”) are\ngerrymandered affairs. With the demise of predicate nominalism as a \nviable account of properties and relations this approach is not as \nunattractive as it might have seemed in the middle of the last \ncentury. Realism about universals is certainly on the rise. While \nthis version of realism presupposes a sparse theory of properties \n— that is to say, it is not the case that to every definable \npredicate there corresponds a genuine universal — such theories\nhave been championed both by those doing traditional a priori \nmetaphysics of properties (e.g. Bealer 1982) as well as those who \nfavor a more empiricist, scientifically informed approach (e.g. \nArmstrong 1978, Tooley 1977). According to Armstrong, for example, \nwhich predicates pick out genuine universals is a matter for \ndeveloped science. The primitive predicates of our best fundamental \nphysical theory will give us our best guess at what the genuine \nuniversals in nature are. They might be predicates like electron or \nmass, or more likely something even more abstruse and remote from the\nphenomena — like the primitives of String Theory. \n\nOne apparently cogent objection to this realist solution is that it \nwould render the task of empirically estimating degree of \ntruthlikeness completely hopeless. If we know a priori which \nprimitives should be used in the computation of distances between \ntheories it will be difficult to estimate truthlikeness, but not \nimpossible. For example, we might compute the distance of a theory \nfrom the various possibilities for the truth, and then make a \nweighted average, weighting each possible true theory by its \nprobability on the evidence. That would be the credence-mean estimate\nof truthlikeness. However, if we don't even know which features \nshould count towards the computation of similarities and distances \nthen it appears that we cannot get off first base. \n\nTo see this consider our simple weather frameworks. Suppose that all \nI learn is that it is rainy. Do I thereby have some grounds for \nthinking A is closer to the truth than B? I would \nif I also knew that h-r-w-ese is the \nlanguage for calculating distances. For then, whatever the truth is, \nA makes one fewer mistake than B makes. A \ngets it right on the rain factor, while B doesn't, and they \nmust score the same on the other two factors whatever the truth of \nthe matter. But if we switch to h-m-a-ese \nthen A's epistemic superiority is no longer guaranteed. If, \nfor example, T is the truth then B will be closer \nto the truth than A. That's because in the \nh-m-a framework raininess as such doesn't \ncount in favor or against the truthlikeness of a proposition. \n\nThis objection would fail if there were empirical indicators \nnot just of which atomic states obtain, but also of which are the \ngenuine ones, the ones that really reality at the joints. \nObviously the framework would have to contain more than just \nh, m and a. It would have to contain \nresources for describing the states that indicate whether these were \ngenuine universals. Maybe whether they enter into genuine causal \nrelations will be crucial, for example. Once we can distribute \nprobabilities over the candidates for the real universals, then we \ncan use those probabilities to weight the various possible distances \nwhich a hypothesis might be from any given theory. \n\nThe second live response is both more modest and more radical. It is \nmore modest in that it is not hostage to the objective priority of a \nparticular conceptual scheme, whether that priority is accessed a \npriori or a posteriori. It is more radical in that it denies a \npremise of the invariance argument that at first blush is apparently \nobvious. It denies the equivalence of the two conceptual schemes. It \ndenies that h&r&w, for example, \nexpresses the very same proposition as \nh&m&a expresses. If we deny \ntranslatability then we can grant the invariance principle, and grant\nthe judgements of distance in both cases, but remain untroubled. \nThere is no contradiction (Tichý 1978). \n\nAt first blush this response seems somewhat desperate. Haven't the respective \nconditions been defined in such a way that they are simple \nequivalents by fiat? That would, of course, be the case if m\nand a had been introduced as defined terms into \nh-r-w. But if that were the intention then\nthe similarity theorist could retort that the calculation of \ndistances should proceed in terms of the primitives, not the \nintroduced terms. However that is not the only way the argument can \nbe read. We are asked to contemplate two partially overlapping \nsequences of conditions, and two spaces of possibilities generated by\nthose two sequences. We can thus think of each possibility as a point\nin a simple three dimensional space. These points are ordered triples\nof 0s and 1s, the nth entry being 0 if the nth \ncondition is satisfied and 1 if it isn't. Thinking of possibilities \nin this way, we already have rudimentary geometrical features \ngenerated simply by the selection of generating conditions. Points \nare adjacent if they differ on only one dimension. A path is a \nsequence of adjacent points. A point q is between two points\np and r if q lies on a shortest path from \np to r. A region of possibility space is convex if \nit is closed under the betweenness relation — anything between \ntwo points in the region is also in the region (Oddie 1987, Goldstick and O'Neill 1988). \n\nEvidently we have two spaces of possibilities, S1 and S2, and the \nquestion now arises whether a sentence interpreted over one of these \nspaces expresses the very same thing as any sentence interpreted over\nthe other. Does h&r&w express the \nsame thing as h&m&a? \nh&r&w expresses (the singleton of)\nu1 (which is the entity <1,1,1> in S1 or \n<1,1,1>S1) and \nh&m&a expresses v1 (the entity \n<1,1,1>S2). \n~h&r&w expresses u2 \n(<0,1,1>S1), a point adjacent to that expressed by \nh&r&w. However \n~h&~m&~a expresses v8 \n(<0,0,0>S2), which is not adjacent to v1 \n(<1,1,1>S2). So now we can construct a simple proof \nthat the two sentences do not express the same thing. \n\nThus at least one of the two required intertranslatability claims \nfails, and h-r-w-ese is not \nintertranslatable with h-m-a-ese. The \nimportant point here is that a space of possibilities already comes \nwith a structure and the points in such a space cannot be \nindividuated without reference to rest of the space and its \nstructure. The identity of a possibility is bound up with its \ngeometrical relations to other possibilities. Different relations, \ndifferent possibilities. This kind of rebuttal to the Miller argument would have radical\nimplications for the comparability of actual theories that appear to\nbe constructed from quite different sets of primitives. Classical\nmechanics can be formulated using mass and position as basic, or it\ncan be formulated using mass and momentum The classical concepts of\nvelocity and of mass are different from their relativistic\ncounterparts, and even if they were “intertranslatable” in\nthe way that the concepts of h-r-w-ese are\nintertranslatable with h-m-a-ese.\n \n\nThis idea meshes well with recent work on conceptual spaces in \nGärdenfors [2000]. Gärdenfors is concerned both with the \nsemantics and the nature of genuine properties, and his bold and \nsimple hypothesis is that properties carve out convex regions of an \nn-dimensional quality space. He supports this hypothesis with\nan impressive array of logical, linguistic and empirical data.\n(Looking back at our little spaces above it is not hard to see that\nthe convex regions are those that correspond to the generating (or\natomic) conditions and conjunctions of those. See Burger and Heidema\n1994.) While Gärdenfors is dealing with properties it is not hard\nto see that similar considerations apply to propositions, since\npropositions can be regarded as 0-ary properties. \n\nUltimately, however, this response may seem less than entirely \nsatisfactory by itself. If the choice of a conceptual space is merely\na matter of taste then we may be forced to embrace a radical kind of \nincommensurability. Those who talk \nh-r-w-ese and conjecture \n~h&r&w on the basis of the \navailable evidence will be close to the truth. Those who talk \nh-m-a-ese while exposed to the \n“same” circumstances would presumably conjecture \n~h&~m&~a on the basis of the \n“same” evidence (or the corresponding evidence that they \ngather). If in fact h&r&w is the \ntruth (in h-r-w-ese) then the \nh-r-w weather researchers will be close to\nthe truth. But the h-m-a researchers will \nbe very far from the truth. This may not be an explicit contradiction, but it should be\nworrying all the same. Realists started out with the ambition of\ndefending a concept of truthlikeness which would enable them to\nembrace both fallibilism and optimism. But what the likeness theorists\nseem to have ended up with here is something that suggests a rather\nunpalatable incommensurability of competing conceptual frameworks. To\navoid this the realist will need to affirm that some conceptual\nframeworks really are better than others. Some really do “carve\nreality at the joints” and others don't. But is that something\nthe realist should be reluctant to affirm? The three different approaches are motivated by somewhat different\ndesiderata.  An interesting question thus arises as to whether the\nthree approaches are compatible. If they are compatible, then the\ndifferent desiderata that motivate them might be accommodated in one\nhappy hybrid.  Consider, for example, Hilpinen's proposal, which is typically\nlocated within the likeness approach. Interestingly, Hilpinen himself\nthought of his proposal as a refined and improved articulation of\nPopper's content approach.  Popper's truth factor Hilpinen\nidentified with that world, in the range of a proposition, closest to\nthe actual world. Popper's\ncontent or information factor he identified with \nthat world, in the range of a proposition, furthest from the actual \nworld. An improvement in truthlikeness involves an improvement in \neither the truth factor or the information factor. His proposal \nclearly departs from Popper's in as much as it incorporates likeness \ninto both of the determining factors but Hilpinen was also attempting\nto capture, in some way or other, Popper's penchant for content as \nwell as truth. And his account achieves a good deal of that. In \nparticular his proposal delivers a weak version of the value of \ncontent for truths: namely, that of two truths the logically stronger\ncannot be further from the truth than the logically weaker. It fails,\nhowever, to deliver the stonger principle of the value of content for \ntruths: that the logically stronger of two truths is closer to the \ntruth.  \n\nTo answer the compatibility question we need precise characterizations\nof the approaches. Zwart (2001) characterized the approaches in terms\nof that proposition they judge to be furthest from the truth. Suppose\nthat z is the world furthest from the actual world, and\nlet Z be a proposition that is true only in z.  On all\nlikeness approaches Z is the proposition that is furthest from\nthe truth. Call this principle Worst. Worst is at least\na necessary condition for a theory to lie within the likeness\napproach, though it seems insufficient.  Content theorists judge\ntheories more or less good in terms of two factors: truth value and\ncontent.  So the worst theory will have to be false.  And presumably\nit will also be weak.  Consider ~T, the negation of the\ntruth. It is both false, and it is the logically weakest falsehood.\nSo, according to Zwart 2001, content theorists ought to judge\n~T to be the worst theory on offer.  Call this\nprinciple Weakest. Weakest assumes something like the\nseparability of content and truth factors in the evaluation of\ntruthlikeness.  While that captures Miller's and Kuiper's symmetric\ndifference account it would banish from the content-based fold those\naccounts that judge false theories to be worse the logically stronger\nthey are. Zwart and Franssen 2007 adopted somewhat stronger characterizations\nof the approaches. Their characterization of the content approach is\nessentially that it encompass the Simple Truth Content\naccount: viz that A is as close to the truth\nas B if A entails all of B's truth content,\nand A is closer to the truth than B just in\ncase A is at least as close as B, and B is\nnot at least as close as A. This guarantees that any\narticulation of the content approach will embody the value of content\nfor truths, but it goes somewhat further as we saw above, guaranteeing\nthe value of content for falsehoods as well.  (It is thus much\nstronger than Weakest.) Their characterization of the likeness approach is that it deliver\nall the judgments delivered by Hilpinen's proposal. (This is clearly\nmuch stronger than Worst.) \nWith these characterizations in hand Zwart and Franssen go on to show\nthat Arrow's famous theorem in social choice theory can be applied to\nobtain a surprising general result about truthlikeness orderings: that\nthere is a precise sense in which there can be no compromise between\nthe content and likeness approaches, that any apparent compromise\neffectively capitulates to one paradigm or the other. (Given their\ncharacterization of the two approaches, Hilpinen's apparent compromise\nis deemed to err on the side of the likeness approach.) \n\nThis theorem represents an interesting new development in the \ntruthlikeness debate. As already noted, much of the debate has been \nconducted on the battlefield of intuition, with protagonists from \ndifferent camps firing off cases which appear to refute their \nopponent's definition while confirming their own. The \nZwart-Franssen-Arrow theorem is not only an interesting result in \nitself, but it represents an innovative and welcome development in \nthe debate, since most of the theorizing has lacked this kind of \ntheoretical generality. \n\nOne problem with this incompatibility result lies in Zwart and\nFranssen's characterization of the two approaches. If delivering all\nthe judgments that are delivered by the Simple Truth Content is a\nnecessary condition for a proposal to be welcomed in the content camp,\nthen while the symmetric difference proposals of Miller and Kuipers\nare ruled in, Popper's original proposal is ruled out. Further, if\ndelivering all the judgments delivered by Hilpinen's proposal is\nstipulated to be a necessary for any likeness account then\nTichý's averaging account is ruled out of the likeness camp. So\nboth characterizations appear to be too narrow. They rule out what are\nperhaps the central paradigms of two different approaches. \n\nA rather more liberal characterization of the content \napproach would count in any proposal that guarantees the value of \ncontent for truths. That, at least, was Popper's litmus test for \nacceptability and what primarily motivated his original proposal. A more \ninclusive characterization of the likeness approach would count in any \nproposal that makes truthlikeness supervene on a measure\nor ordering of likeness on worlds.  On these more inclusive characterizations, Popper's theory\nqualifies as a content account; Tichý's theory qualifies as a\nlikeness account. And that is as it should be. Further, Hilpinen's\ntheory falls within the likeness approach, but fails to qualify as a\ngenuine content account. It does not deliver the full value of content\nfor truths.  So on these characterizations Hilpinen's account is not a\ngenuine hybrid. \n\nAs we have seen, one shortcoming which Hilpinen's proposal shares with\nPopper's original proposal is the absolute worthlessness of all\nfalsehoods: that no falsehood is closer to the truth than any truth\n(even the worthless tautology). This defect of Hilpinen's qualitative\nproposal can be remedied by assuming quantitative distances between\nworlds, and letting A's distance from the truth be a weighted\naverage of the distance of the closest world in A from the\nactual world, and the distance of the furthest world in\nA from the actual world. This quantitative version (call it \nmin-max-average) of Hilpinen's account renders all \npropositions comparable for truthlikeness, and some falsehoods it \ndeems more truthlike than some truths. Although min-max-average falls within the likeness\napproach broadly characterized, it too fails to deliver the value of\ncontent for truths. So it does not qualify as a content ordering\neither. Moreover, it is not entirely satisfactory from a likeness\nperspective either, despite satisfying the rather weak likeness\nconstraint that truthlikeness supervene on likeness.  To illustrate\nthis, let A be a true proposition with a number of worlds\ntightly clustered around the actual world\na. Let B be a false proposition with a number of \nworlds tightly clustered around a world z maximally distant \nfrom actuality. A is highly truthlike, and B highly\nuntruthlike and min-max-average agrees. But now let \nB+ be B plus a, and let A+ be \nA plus z. Considerations of both continuity and \nlikeness suggest that A+ should be much more truthlike than \nB+, but they are deemed equally truthlike by \nmin-max-average. \n\nPart of the problem with min-max-average proposal is that the\nfurthest world in a proposition is, as noted above, a very crude\nestimator of overall content. It is precisely for this reason that\nNiiniluoto suggests a different content measure: the\n(normalized) sum of the distances of worlds in\nA from the actual world. As we have seen, sum is \nnot itself a good measure of distance of a proposition from the \ntruth. However formally, sum is a probability measure, and \nhence a measure of a kind of logical weakness. But sum is \nalso a content-likeness hybrid, rendering a proposition more \ncontentful the closer its worlds are to actuality. Being genuinely \nsensitive to size, sum is clearly a better measure of \nlogical weakness than the world furthest from actuality. Hence \nNiiniluoto proposes a weighted average of the closest world (the \ntruth factor) and sum (the information factor).  Niiniluoto's measure,\nmin-sum-average, ranks a tautology, B+ and\nA+ in that order of closeness to the\ntruth. min-sum-average also delivers the value of content for\ntruths: if A is true and is logically stronger\nthan B then both have the same truth factor (0), but since\nthe range of B contains more worlds, its\nsum will be greater, making it further from the truth. So \nmin-sum-average falls within the content approach on this \ncharacterization. On the other hand, min-sum-average also seems \nto fall within the likeness camp, since it deems truthlikeness to be \na non-trivial function of the likenesses of worlds, in the range of a\nproposition, to the actual world. \n\nAccording to min-sum-average: all propositions are \ncommensurable for truthlikeness; the full principle of the value of \ncontent for truths holds provided the content factor gets non-zero \nweight; the Truth has greater truthlikeness than any other \nproposition provided all non-actual worlds are some distance from the\nactual world; some false propositions are closer to the truth than \nothers; the principle of the value of content for falsehoods is \nappropriately repudiated, provided the truth factor gets some weight; \nif A is false, the truth content of A is more \ntruthlike than A itself, again provided the truth factor \ngets some weight. min-sum-average thus seems like a \nconsistent and appealing compromise between content and \nlikeness approaches. \nThis compatibility result may be too quick and dirty for the following\nreason.  We laid down a somewhat stringent condition on content-based\nmeasures (namely, the value of content for truths) but we have only\nrequired a very lax, supervenience condition for likeness-based\nmeasures (namely, that the likeness of a proposition to the truth\nbe some function or other of the likeness of the worlds in the\nproposition to the actual world). This latter condition allows any old\nfunction of likeness to count. For example, summing the distances of\nworlds from the actual world is a function of likeness, but it hardly\nsatisfies basic intuitive constraints on the likeness of a proposition\nto the truth. There might well be more demanding but still plausible\nconstraints on the likeness approach, and those constraints might\nblock the compatibility of likeness and content. It also has to be\nadmitted that there is something a little unsatisfactory with the\nrather piecemeal method that was used to arrive at an extension from\ndistance between worlds to distance from the truth. A better way of\nproceeding would be to discover some highly plausible general\nprinciples that any likeness theorist would find compelling, which\nwould ideally uniquely identify the correct extension. The following three constraints on any extension of distances\nbetween worlds to distances of propositions from the truth have been\nproposed (Oddie 2013).   First, suppose that all the worlds in the range of A are\nexactly the same distance from the actual world.  What is the overall\ndistance of A from the actual world?  One very plausible\nanswer is that A is exactly the same distance as the worlds\nit contains:  Note that average and min-max-average both\nobey uniform distance while min-sum-average does\nnot.  min-sum-average is based on the intuition that adding new\ndisjuncts decreases truthlikeness, unless the new disjunct improves\nthe minimum distance. For example, on min-sum-average if it is\nhot and rainy, the false proposition\n(h&~r)∨(~h&r) is further from\nthe truth than either of its two false disjuncts, even though both\ndisjuncts are the same distance from the truth. Let Av/u be any proposition that differs\nfrom A only in that it contains v rather\nthan u, and suppose that v is closer to the actual\nworld than u.  Clearly Av/u cannot\nbe further from the actual world than A is.    This gives us: \nIf v is closer to the actual world than u is then there\nshould be a difference between the distance of A from the\ntruth and the distance of Av/u from the truth.\nWhat should that difference depend on?  Given\nthat Av/u differs from A only over the\ndistance from the actual world of worlds u and v, the\ndifference in closeness to truth of A and B can\ncertainly depend on the distance of u from the actual world and\nthe distance of v from the actual world. The following argument\nshows that the difference may also depend on the size\nof A.   The smaller A is the more the replacement changes what we\nmight call A's distance profile.  In the limit if A is a\nsingleton (viz. {u}), Av/u is also a\nsingleton (viz., {v}).  From the uniform distance principle, we\nknow that the difference between the distances of A and\nof Av/u in this case is the difference between the\ndistance of v and the distance of u from the actual\nworld.  And that is the largest difference that replacing u\nwith v could make. The larger A is the less of an impact\nthe replacement will have.  So size of A may make a difference\nto the impact of replacement.  However, we don't have to stipulate any\nparticular function here, or even that it be a decreasing function of\nthe size of A (as indeed it should be). Rather, we merely allow\nthat the difference between the two distances is some function or\nother of these three factors. These three extension principles individually should be very\nattractive to a likeness theorist.  And it is easy to check that\naveraging satisfies the extension principles. Interestingly it can\nalso be shown that averaging is the only extension principle to do so.\nAny other extension will violate one of the three constraints. By\nrelaxing the implicit assumption that all worlds are of equal weight,\na generalized argument shows that weighted average distance is\nthe only function to satisfy the extension principles.  Call a distance/likeness function δ flat\nprovided δvw=1 if and only if v≠w.  A\nflat distance function embodies an extreme version of likeness\nnihilism — namely, that as a matter of brute necessity no world\nis more like the actual world than is any other.  It still counts as a\npossible view of likeness, albeit an extreme view, one which is\nperhaps supported by a generalized language dependence argument (see\nsection §1.4.4). Given a flat distance function on worlds,\ntogether with weighted averaging, the distance of proposition A\nfrom the truth is (1−(P(A)/P(T))\nif A is true, and 1 if A is false. Since this is\ngenerated by a distance function this measure of distance from the\ntruth falls within the likeness approach broadly construed, and since\nwe used weighted averaging, it also satisfies the distance extension\nprinciples.  Further, since the ordering delivers the value of content\nfor truths it falls within the content approach, broadly\ncharacterized.   So, it turns out that the content and likeness approaches are\ncompatible.  Indeed Popper's original ordering satisfies the\nstrictures of both content and likeness approaches.  It is obviously a\ncontent ordering, and, since averaging the flat distance function\ninduces an extension of Popper's ordering, it falls within the\nlikeness approach as well.  Notice that averaging a flat distance\nfunction delivers both kinds of worthlessness for falsehoods. It\nyields the result that no falsehood is closer to the truth than any\nother, and no falsehood is closer to the truth than the least\ntruthlike truth.  Furthermore, this is not just a peculiar feature of\nthe flat distance function, for at least one half of this result is\ncompletely general: Although the three approaches are, strictly speaking, compatible,\nthere is still a deep tension between them.  If you accept the three\nplausible likeness principles (uniform distance, pareto,\nand difference) then you either have to reject the principle of\nthe value of content for truths or you have to accept the absolute\nworthlessness of falsehoods. The latter is not a serious option.\nBoth uniform distance and pareto seem rather compelling.\nIf this is right the choice is between rejecting the difference\nprinciple and rejecting the value of content for truths. The quest to nail down a viable concept of truthlikeness is\nmotivated, at least in part, by fallibilism (§1.1). It is\ncertainly true that with a viable notion of distance from the truth\nprogress in an inquiry through a succession of false theories is\nrendered possible.  It is also true that if there is no such viable\nnotion then truth can be retained as the goal of inquiry only at the\ncost of making partial progress towards it virtually impossible.  But\ndoes the mere possibility of making progress towards the truth\nimprove our epistemic lot? Some have argued that it doesn't (see for\nexample Laudan, 1977, Cohen 1980, Newton-Smith 1981).  One common\nargument can be recast in the form of a simple dilemma.  Either we can\nascertain the truth or we can't. If we can ascertain the truth then we\nhave no need for a concept of truthlikeness – it is an entirely\nuseless addition to our intellectual repertoire. But if we cannot\nascertain the truth then then we cannot ascertain the degree of\ntruthlikeness of our theories either. So again, the concept is useless\nfor all practical\npurposes. (See Scientific\nProgress §2.4) Consider the second horn of this dilemma. Is it true that if we\ncan't know what the (whole) truth of some matter is, we also cannot\nascertain whether or not we are making progress towards it? Suppose\nyou are interested in the truth about the weather tomorrow. Suppose\nyou learn (from a highly reliable source) that it will be hot. Even\nthough you don't know the whole truth about the weather\ntomorrow, you do know that you have added a truth to your existing\ncorpus of weather beliefs.  One does not need to be able to ascertain\nthe whole truth to ascertain some less encompassing truths.  And it\nseems to follow that you can also know that, with a little discovery\nlike this one, you have made at least some progress towards the whole\nweather truth.  This rebuttal is too swift. It presupposes that the addition of a\nnew truth A to an existing corpus K guarantees that your\nrevised belief K*A constitutes progress towards the\ntruth. But whether or not K*A is closer to the truth\nthan K depends not only on a theory of truthlikeness but also\non a theory of belief\nrevision. (See Logic of Belief\nRevision.)   Let's consider a simple case. Suppose A is some newly\ndiscovered truth, and that A is compatible with K.\nAssume that belief revision in such cases is simply a matter of\nso-called expansion — i.e.  conjoining A\nto K. Consider the case in which K also happens to be\ntrue. Then any account of truthlikeness that endorses the value of\ncontent for truths (e.g.  Niiniluoto's min-sum-average)\nguarantees that K*A is closer to the truth\nthan K.  That's a welcome result but it has rather limited\napplication. Typically one doesn't know that K is true so even\nif one knows that A is true one cannot use this fact to\ncelebrate progress.  The situation is even more dire when it comes to falsehoods.\nIf K is in fact false then, without the disastrous principle of\nthe value of content for falsehoods, there is certainly no guarantee\nthat K*A will constitute a step towards the truth. (And\neven if one endorsed the disastrous principle one would hardly be\nbetter off.  For then the addition of any proposition, whether true or\nfalse, would constitute an improvement on a false theory.) Consider\nagain the number of the planets, N. Suppose that the truth is N=8, and\nthat your existing corpus K is (N=7 ∨ N=100). Suppose you\nsomehow acquire the truth A: N>7.  Then K*A is\nN=100, which (on average, min-max-average\nand min-sum-average) is further from the truth\nthan K. So revising a false theory by adding truths by no means\nguarantees progress towards the truth.  For theories that reject the value of content for truths\n(e.g. the average proposal) the situation is worse still. Even\nif K happens to be true, there is no guarantee that\nexpanding K with truths will constitute progress.  Of course,\nthere will be certain general conditions under which the value of\ncontent for truths holds. For example on the average proposal,\nthe expansion of a true K by an atomic truth (or, more\ngenerally, by a convex truth) will guarantee progress towards the\ntruth. So under very special conditions one can know that the acquisition\na truth will enhance the overall truthlikeness of one's theories, but\nthese conditions are exceptionally narrow and provide at best a very\nweak defense against the dilemma. (See Niiniluoto 2011.  For rather\nmore optimistic views of the relation between truthlikeness and belief\nrevision see Kuipers 2000, Lavalette, Renardel & Zwart 2011, and\nCevolani, Festa and Kuipers 2013.) A different tack is to deny that a concept is useless if there is\nno effective empirical decision procedure for ascertaining whether it\napplies. For even if we cannot know for sure what the value of a\ncertain unobservable magnitude is, we might well have better or\nworse estimates of the value of the magnitude on the\nevidence. And that may be all we need for the concept to be of\npractical value. Consider, for example, the propensity of a certain\ncoin-tossing set-up to produce heads — a magnitude which, for\nthe sake of the example, we assume to be not directly observable. Any\nnon-extreme value of this magnitude is compatible with any number of\nheads in a sequence of n tosses. So we can never know with\ncertainty what the actual propensity is, no matter how many tosses we\nobserve. But we can certainly make rational estimates of the\npropensity on the basis of the accumulating evidence. Suppose one's\ninitial state of ignorance of the propensity is represented by an even\ndistribution of credences over the space of possibilities for the\npropensity (i.e. the unit interval).  Using Bayes theorem, and the\nPrincipal Principle, after a fairly small number of tosses we can\nbecome quite confident that the propensity lies in a small interval\naround the observed relative frequency. Our best estimate of\nthe value of the magnitude is its expected value on the\nevidence. Similarly, suppose we don't and perhaps cannot know which\nconstituent (at a certain depth d) is in fact true (or more\ngenerally which answer to our query is the right answer).  But suppose\nthat we do have a good measure of distance between\nconstituents Ci and we have selected the right\nextension function.  So we have a measure of the truthlikeness of a\nproposition A given that constituent Ci is\ntrue. (Let this be TL(A|Ci).) Provided\nwe also have a measure of epistemic probability P\n(where P(Ci|e) is the degree of\nrational credence in Ci given evidence e) we\nalso have a measure of the expected degree of truthlikeness\nof A on the evidence (ETL(A|e)\nwhich we can identify with the best epistemic estimate of\ntruthlikeness. (Niiniluoto, who first explored this concept in his\n1977, calls the epistemic estimate of degree of truthlikeness on the\nevidence, or expected degree of truthlikeness, verisimilitude.\nSince verisimilitude is typically taken to be a synonym\nfor truthlikeness I will not follow him in this, and will stick\ninstead with expected truthlikeness for the epistemic\nnotion. See also Maher (1993).)  Clearly the expected degree of truthlikeness of a\nproposition is epistemically accessible, and it can serve as\nour best empirical estimate of the objective degree of\ntruthlikeness. Progress occurs in an inquiry when actual truthlikeness\nincreases. And apparent progress occurs when expected degree of\ntruthlikeness increases. Like truthlikness, expected degree of truthlikeness behaves quite\ndifferently from probability. The expected degree of truthlikeness of\na proposition can be low even though its probability is high. A\ntautology, for example, has low degree of truthlikeness (closeness to\nthe whole truth) whatever the truth is, and hence a low degree of\nexpected truthlikeness, while its probability is maximal. More\ninterestingly a proposition can be known to be false, and so have zero\nprobability, and yet have a a higher degree of truthlikeness than some\nknown truths. For example, that the number of planets is 7 is known to\nbe false but since we know that the number of planets is 8, its degree\nof expected truthlikeness is identical to its actual truthlikeness,\nwhich is as good as a false answer to the question about the number of\nthe planets gets.  (See Niiniluoto 2011 for other examples.)  Of course, the possibility of estimating truthlikeness relies on\nthe availability of a reasonable theory of probability. But the basic\nidea can be implemented whether one is a subjectivist about\nprobability, or one subscribes to some more objective version of a\ntheory of logical probability. It turns out, not surprisingly, that\nthere is a well-known framework for logical probability which fits the\nnormal-form approach to truthlikeness like a glove — namely,\nthat based on Hintikka's distributive normal forms (Hintikka\n1965). Hintikka's approach to inductive logic can be seen as a rather\nnatural development of Carnap's continuum of inductive methods\n(Carnap 1952).  Recall that Carnap advocated distributing prior\nprobabilities equally over structure descriptions, rather than\nover state descriptions, in order to make it possible to learn from\nexperience. But in an infinite universe there are still infinitely\nmany structure descriptions, and while Carnap's approach makes it\npossible for a finite amount of data to change the probability of a\nsingular prediction, it renders probabilistic learning from experience\nimpossible for all genuine universal generalizations.  Since universal\nclaims start with zero probability, updating by conditionalization on\nnew evidence can never change that.  Constituents partition the logical space according to the kinds of\nindividuals that exist. Constituents can be plausibly assigned equal\nprobabilities, perhaps motivated by a principle of indifference of\nsome kind. In the simplest case — first-order monadic\nconstituents of depth-1 — each constituent says of some set\nof Q-predicates that they are instantiated and that they are\nthe only instantiated Q-predicates. The width of a\nconstituent is the number of Q-predicates it says are\ninstantiated. Suppose that n pieces of evidence have been\naccumulated to date, and that the total accumulated\nevidence en entails that all and only\nthe Q-predicates in Ce are instantiated. That\nis, Ce, is the narrowest constituent\ncompatible with the total evidence. Then given Hintikka's initial\nequal distribution, Ce emerges as the constituent\nwith the highest probability on the evidence. Further, suppose that we\nreach a point in the gathering of evidence after which no new kinds of\nindividuals are observed. (Since there are only a finite number of\nkinds in the monadic framework envisaged, there must be such a point.)\nThen P(Ce|en) → 1\nas n → ∞. And then it follows that for any\nproposition A expressible in the framework: Further, provided that the evidence is eventually exhaustive (that\nis, instances of all the different kinds of individuals that are in\nfact instantiated eventually turn up at some stage in the evidence)\nexpected truthlikeness will come arbitrarily close to actual degree of\ntruthlikeness. While this idea has been developed in detail for monadic\nfirst-order frameworks only, the model does demonstrate something\ninteresting: namely the consilience of two important but apparently\nantagonistic traditions of twentieth century philosophy of science. On\nthe one hand there is the Carnapian tradition, which stressed the\nprobabilification of scientific hypotheses through the application of\ninductive methods. On the other, there is the Popperian tradition\nwhich completely rejected so-called inductive methods, along with the\napplication of probability theory to epistemology, embracing instead\nthe possibility of progress towards the whole truth through highly\nimprobable conjectures and their refutation. If the model is on the\nright lines, then there is a rational kernel at the core of both these\ntraditions. Inquiry can, and hopefully does, progress towards the\ntruth through a sequence of false conjectures and their nearly\ninevitable refutation, but we can also have fallible evidence of such\nprogress.  Expected degree of truthlikeness will typically approach\nactual degree of truthlikeness in the long run.   With this proposal Niiniluoto also made a connection with the\napplication of decision theory to epistemology. Decision theory is an\naccount of what it is rational to do in the light of one's beliefs and\ndesires.  One's goal, it is assumed, is to maximize utility. But given\nthat one does not have perfect information about the state of the\nworld, one cannot know for sure how to accomplish that. Given\nuncertainty, the rule to maximize utility or value cannot be applied\nin normal circumstances. So under conditions of uncertainty, what it\nis rational to do, according to the theory, is to\nmaximize subjective expected utility.  Starting with Hempel's\nclassic 1960 essay, epistemologists conjectured that\ndecision-theoretic tools might be applied to the problem of\ntheory acceptance — which hypothesis it is rational to\naccept on the basis of the total available evidence to hand. But, as\nHempel argued, the values or utilities involved in a decision to\naccept an hypothesis cannot be simply regular practical values. These\nare typically thought to be generated by one's desires for various\nstates of affairs to obtain in the world.  But the fact that one would\nvery much like a certain favored hypothesis to be true does not\nincrease the cognitive value of accepting that hypothesis. If we had a decent theory of epistemic utility (also known\nas cognitive utility and cognitive value) perhaps what\nhypotheses one ought to accept, or what experiments one ought to\nperform, or how one ought to revise one's corpus of belief in the\nlight of new information, could be determined by the rule: maximize\nexpected epistemic utility (or maximize expected cognitive\nvalue). Thus decision-theoretic epistemology was born. Hempel went on to ask what epistemic utilities are implied in the\nstandard conception of scientific inquiry — “..construing\nthe proverbial 'pursuit of truth' in science as aimed at the\nestablishment of a maximal system of true statements...” Hempel\n(1960), p 465) Already we have here the germ of the idea central to the\ntruthlikeness program: that the goal of an inquiry is to end up\naccepting (or “establishing”) the theory which yields the\nwhole truth of some matter. It is interesting that, around the same\ntime that Popper was trying to articulate a content-based account of\ntruthlikeness, Hempel was attempting to characterize partial\nfulfillment of the goal (that is, of maximally contentful truth) in\nterms of some combination of truth and content. These\ndecision-theoretic considerations lead naturally to a brief\nconsideration of the axiological problem of truthlikeness.  Our interest in the concept of truthlikeness is presumably rooted\nin the value of lighting on highly truthlike theories.  And that in\nturn seems rooted in the value of truth (Oddie 2008). This\nsuggests that truth and truthlikeness are cognitive values and, one\nwould hope, closely related values.  If so, how are they related?  Let's start with the putative value of truth.  Truth is not, of\ncourse, a good-making property of the objects of belief states.  The\nproposition h&r&w is not a\nbetter proposition when it happens to be hot, rainy and windy\nthan when it happens to be cold, dry and still. Rather, the cognitive\nstate of believing h&r&w is often\ndeemed to be a good state to be in if the proposition is true, not\ngood if it is false.  So the state of believing truly is better\nthan the state of believing falsely.  At least some, perhaps most, of the value of believing truly can be\naccounted for instrumentally. Desires mesh with beliefs to produce\nactions that will best achieve what is desired.  True beliefs will\ngenerally do a better job of this than false beliefs.  If you are\nthirsty and you believe the glass in front of you contains safe\ndrinking water rather than lethal poison then you will be motivated to\ndrink.  And if you do drink, you will be better off if the belief you\nacted on was true (you quench your thirst) rather than false (you end\nup dead).  We can do more with decision theory utilizing purely practical or\nnon-cognitive values.  For example, there is a well-known,\ndecision-theoretic, value-of-learning theorem, the Ramsey-Good\ntheorem, which partially vindicates the value of gathering new\ninformation, and it does so in terms of “practical”\nvalues, without assuming any purely cognitive values. (Good 1967.)\nSuppose you have a choice to make, and you can either choose now, or\nchoose after doing some experiment. Suppose further that you are\nrational (you always choose by expected value) and that the experiment\nis cost-free. It follows that performing the experiment and then\nchoosing always has at least as much expected value as choosing\nwithout further ado. Further, doing the experiment has higher expected\nvalue if one possible outcome of the experiment would alter the\nrelative values of some your options. So, you should do the experiment\njust in case the outcome of the experiment could make a difference to\nwhat you choose to do. Of course, the expected gain of doing the\nexperiment has to be worth the expected cost.  Spending $10 to find\nout information that makes only a $5 difference to the expected value\nof your final choice would be irrational. But that seems right.  Not\nall information is worth pursuing when you have limited time and\nresources.  These kinds of considerations do not, of course, deliver either\nthe invariable instrumental superiority of true belief or of the value\nof learning.  Suppose you are diagnosed with an illness with a very low survival\nrate.  However, those who are deluded into believing that they have a\ngood chance of survival have a slightly better (though still not good)\nchance of survival.  Then firmly embracing a falsehood (if you can\npull it off) has greater overall expected utility than believing its\ntrue negation. So practical values, non-epistemic values, don't\nuniformly favor believing truly.  David Miller notes the following powerful objection to the\nRamsey-Good value-of-learning theorem: There are some things that we think are worth knowing about, the\nknowledge of which would not change the expected value of any action\nthat one might be contemplating performing.  We have just spent\nbillions of dollars conducting an elaborate experiment to try to\ndetermine whether the Higgs boson exists, and the results are\npromising enough that Higgs et al were awarded the Nobel Prize\nfor their prediction.  The results of the experiment have shifted the\ncredences in favor of the standard model.  The discovery may, of\ncourse, also yield practical benefits, but it is the cognitive\nchange it induces that makes us think it was worthwhile.  It was\nvaluable simply for what it did to our credal state — not just\nour credence in the existence of the Higgs, but our overall credal\nstate. We may of course be wrong about the Higgs — the results\nmight be misleading — but from our new epistemic viewpoint it\ncertainly appears to us that we have made cognitive gains. We have a\nlittle bit more evidence for the truth, or at least the truthlikeness,\nof the standard model.  What we need, it seems, is some account of pure cognitive value,\none that embodies the idea that getting at the truth is itself\nvaluable whatever its practical benefits. As noted this possibility\nwas first explicitly raised by Hempel, and developed in various\ndifference ways in the 1960s and 1970s by Levi (1967), Hilpinen\n(1968), amongst others. (For recent developments see Epistemic Utility\nArguments for Probabilism (§6).) We could take the cognitive value of believing a single\nproposition A to be positive when A is true and negative\nwhen A is false.  But acknowledging that belief comes in\ndegrees, the idea might be that the stronger one's belief in a truth\n(or of one's disbelief in a falsehood), the greater the cognitive\nvalue.  Let V be the characteristic function of answers )both\ncomplete and partial, to the question\n{C1, C2,\n... , Cn, ...}.  That is to say,\nwhere A is equivalent to some disjunction of complete\nanswers:  Vi(A) = 1 if A\nis entailed by the complete\nanswer Ci (i.e A is true according toCi);  Vi(A)\n= 0 if the negation of A is entailed by the complete\nanswer Ci (i.e. A is false\naccording to Ci.  Then the simple view is that the cognitive value of\nbelieving A to degree P(A)\n(where Ci is the complete true answer) is\ngreater the closer P(A) is to the actual value\nof Vi(A)— that is, the smaller\n|Vi(A)−P(A)| is. Variants\nof this idea have been endorsed, for example by Horwich (1982,\n127–9), and Goldman (1999) (who calls this “veristic\nvalue”).    |Vi(A)−P(A)| is a\nmeasure of how far P(A) is\nfrom Vi(A), and\n−|Vi(A)−P(A)| of\nhow close it is.  So here is the simplest linear realization of this\ndesideratum: \n But there are of course many other measures satisfying the basic\n idea.  For example we have the following quadratic measure:  Both Cv¹ and Cv² reach a maximum\nwhen P(A) is maximal and A is true,\nor P(A) is minimal and A is false; and a minimum\nwhen P(A) is maximal and A is false,\nor P(A) is minimal and A is true.\n These are measures of local value — the value of\ninvesting a certain degree of belief in a single answer A to\nthe question Q.  But we can agglomerate these local values into\nthe value of a total credal state P. This involves a\nsubstantive assumption: that the value of a credal state is some\nadditive function of the values of the individual beliefs states it\nunderwrites. A credal state, relative to inquiry Q, is\ncharacterized by its distribution of credences over the elements of\nthe partition Q. An opinionated credal state is one that\nassigns credence 1 to just one of the complete answers.  The best\ncredal state to be in, relative to the inquiry Q, is the\nopinionated state that assigns credence 1 to the complete correct\nanswer.  This will also assign credence 1 to every true answer, and 0\nto every false answer, whether partial or complete.  In other words,\nif the correct answer to Q is Ci then the\nbest credal state to be in is the one identical\nto Vi: for each partial or complete answer A\nto Q, P(A) = Vi(A). This\ncredal state is the state of believing the truth, the whole truth and\nnothing but the truth about Q. Other total credal states\n(whether opinionated or not) should turn out to be less good than this\nperfectly accurate credal state. But there will, of course, have to be\nmore constraints on the accuracy and value of total cognitive\nstates.  Assuming additivity, the value of the credal state P can be\ntaken to be the (weighted) sum of the cognitive values of local belief\nstates. Plugging in either of the above local cognitive value measures,\ntotal cognitive value is maximal for an assignment of maximal\nprobability to the true complete answer, and falls off as confidence\nin the correct answer falls away.  Since there are many different measures that reflect the basic idea\nof the value of true belief how are we to decide between them?  We\nhave here a familiar problem of underdetermination.  Joyce (1998) lays\ndown a number of desiderata, most of them very plausible, for a\nmeasure of what he calls the accuracy of a cognitive state.\nThese desiderata are satisfied by the Cv² but not\nby Cv¹. In fact all the members of a family of closely\nrelated measures, of which Cv² is a member, satisfy\nJoyce's desiderata: Giving equal weight to all propositions\n(λ A = some non-zero constant c for\nall A), this is equivalent to the Brier measure\n(see Epistemic Utility Arguments for\nProbabilism (§6)).  Given the\ncharacter:lambda]–weightings can vary, there is considerable\nflexibility in the family of admissible measures. Whether there are\nother quite different measures satisfying Joyce's desiderata is not\nknown. Joyce's primary aim is to vindicate probabilism — the\nthesis that rational credences must conform to the probability\ncalculus. Assuming probabilism, our main interest here is whether any\nof these so-called scoring rules (rules which score the overall\naccuracy of a credal state) might constitute an acceptable measure of\nthe cognitive value of a credal state. Absent specific refinements to the λ–weightings, the\nquadratic measures seem unsatisfactory as a solution to the problem of\nboth accuracy and cognitive value.  Suppose you are inquiring into the\nnumber of the planets and you end up fully believing that the number\nof the planets is 9. Given that the correct answer is 8, your credal\nstate is not perfect.  But it is pretty good, and it is surely a much\nbetter credal state to be in than the opinionated state that sets\nprobability 1 on the number of planets being 9 billion.  It seems\nrather natural to hold that the cognitive value of an opinionated\ncredal state is sensitive to the degree of truthlikeness of the\ncomplete answer that it fixes on, not just to its truth\nvalue. But this is by no means endorsed by the family of quadratic\nmeasures.  Joyce's desiderata build in the thesis that accuracy is\ninsensitive to the distances of various different complete answers\nfrom the complete true answer.  The crucial principle\nis Extensionality.  We have already seen that this seems wrong for opinionated states\nthat confine all probability to false propositions.  Being convinced\nthat the number of planets in the solar system is 9 is better than\nbeing convinced that it is 9 billion.  But the same idea holds for\ntruths.  Suppose you believe truly that the number of the planets is\neither 7, 8, 9 or 9 billion.  This can be true in four different ways.\nIf the number of the planets is 8 then, intuitively, your belief is a\nlittle bit closer to the truth than if it is 9 billion.  (This\njudgment is endorsed by both the average and\nthe min-sum-average proposals.)  So even in the case of a true\nanswer to some query, the value of believing that truth can depend not\njust on its truth and the strength of the belief, but on where the\ntruth lies.  If this is right Extensionality is misguided.  Joyce considers a variant of this objection: namely, that given\nExtensionality Kepler's beliefs about planetary motion would be judged\nto be no more accurate than Copernicus's.  His response is that there\nwill always be propositions which distinguish between the accuracy of\nthese falsehoods:   Unfortunately this contention — that considerations of\naccuracy over the whole set of answers which the two theories give\nwill sort them into the right ranking — doesn't seem correct, at\nleast as it stands.  For example, consider any question Q to\nwhich Kepler's theory K and Copernicus's theory C are\ncomplete but false answers, and let the correct complete answer\nbe T.  Let PK and PC be the\nopinionated credal states corresponding to those\ntheories. Let AC/K be just like A except\nthat K is replaced by C. For any answer A\nto Q that PK gets right\nand PC gets wrong there will be a corresponding\nanswer AC/K that PC gets right\nand PK gets wrong.  Let A include\nboth K and T as disjuncts, while excluding C.\nThen A is true, P(A)=1, and PK\nscores full marks on it (namely, λA on the\nweighted quadratic measure).  PC(A)=0,\nso PC scores bottom marks\n(−λA).  AC/K is also\ntrue, PC(AC/K)=1,\nand PC scores top marks\n(λAC/K),\nwhile PK(AC/K)=0,\nand PK scores bottom marks\n(−λAC/K). Since A\nand AC/K have the same strength, there seems little\njustification for assigning them distinct λ–values. As\nfar as A and AC/K go, (provided\nλA =\nλAC/K), K and C are\nlevel-pegging.  But we can pair off correct and incorrect answers\nacross the board like this showing that, absent some gerrymandered\nλ-values, K and C will end up being on a par. Wallace and Greaves (2005) assert that the weighted quadratic\nmeasures “can take account of the value of\nverisimilitude.” They suggest this can be done “by a\njudicious choice of the coefficients” (i.e. the\nλ–values). They go on to say: “... we simply assign\nhigh λA when A is a set of 'close'\nstates.” (Wallace and Greaves 2005, 628). 'Close' here\npresumably means 'close to the actual world'.  But whether an\nanswer A contains complete answers that are close to the actual\nworld — that is, whether A is truthlike — is\nclearly a world-dependent matter. The coefficients\nλA are not world-dependent.  One could, of\ncourse, append world-dependent coefficients to the local quadratic\nmeasure, but then there is no guarantee that the resulting scoring\nrule will satisfy an important desideratum— namely \npropriety or cogency. (See below.)  This defect of the quadratic class of measures corresponds to a\nproblem familiar from the investigation of attempts to capture\ntruthlikeness simply in terms of classes of true and false\nconsequences, or in terms of truth value and content alone.  Any two\ncomplete false answers yield precisely the same number of true\nconsequences and the same number of false consequences.  The local\nquadratic measure accords the same value to each true answer and the\nsame disvalue to each false answer. So if, for example, all answers\nare given the same λ–weighting in the global quadratic\nmeasure, any two opinionated false answers will be accorded the same\ndegree of accuracy by the corresponding global measure.   It may be that there are ways of tinkering with the\nλ–terms to avoid the objection, but on the face of it the\nnotions of local cognitive value embodied in both the linear and\nquadratic rules seem to be deficient because they omit considerations\nof truthlikeness even while valuing truth.  It seems that any adequate\nmeasure of the cognitive value of investing a certain degree of belief\nin A should take into account not just whether A is true\nor false, but here the truth lies in relation to the worlds\nin A. One rather simple articulation of this thought for local cognitive\nvalue is this: Suppose we calibrate degrees of truthlikeness so that the complete\ntrue answer has truthlikeness degree 1, the least truthlike\nproposition has truthlikeness degree −1, and tautologous answers\nhave truthlikeness degree 0. (This is easy to do in the propositional\nframework with the city-block measure and extending\nby average.) Then this measure of cognitive value entails that\nbelieving a tautology to the correct degree (namely 1) has 0 cognitive\nvalue, as does investing any degree of credence in any proposition\nwith the same degree of truthlikeness as a tautology.  So believing\ntrivial truths would add nothing to cognitive value.  Only if the\ntruthlikeness of a proposition exceeds that of a tautology would\nbelieving it add anything to cognitive value. And any degree of belief\nin a proposition less truthlike than the tautology would detract from\ncognitive value.  If there are truths that are less truthlike\nthan tautologies (as there are on the average proposal) then\nbelieving such truths will also detract from overall cognitive\nvalue. Whatever our account of cognitive value, each credal state P\nassigns an expected cognitive value to every credal state Q\n(including P itself of course). Suppose we accept the injunction that one ought to maximize\nexpected value as calculated from the perspective of one's current\ncredal state.  Let us say that a credal state P\nis self-defeating if to maximize expected value from the\nperspective of P itself one would have to adopt some distinct\ncredal state Q,  without the benefit of any new\ninformation: The requirement of cogency requires that no credal state\nbe self-defeating. No credal state demands that you shift to another\ncredal state without new information. One feature of the quadratic\nfamily of proper scoring rules is that they guarantee cogency, and\ncogency is an extremely attractive feature of cognitive value.  From\ncogency alone one can construct arguments for the various elements of\nprobabilism.  For example, cogency effectively guarantees that the\ncredence function must obey the standard axioms governing additivity\n(Leitgeb and Pettigrew 2010); cogency guarantees a version of the\nvalue-of-learning theorem in purely cognitive terms (Oddie, 1997);\ncogency provides an argument that conditionalization is the only\nmethod of updating compatible with the maximization of expected\ncognitive value (Oddie 1997, Greaves and Wallace 2005, and Leitgeb and\nPettigrew 2010). Any account of cognitive value that doesn't deliver\ncogency would seem to be somewhat problematic. One problem with the truthlikeness-sensitive local\nmeasure Cv4 is that it yields, by additivity, a\nglobal measure of cognitive value that violates cogency.  For example,\nif we use the city-block measure for propositional worlds, and extend\nthat to propositions by any of the three plausible extension functions\n(average, min-max-average, min-sum-average) some\ncredal states turn out to be self-defeating.  Perhaps there are\nplausible accounts of total cognitive value that capture the idea\nthat, other things being equal, beliefs are better the greater their\ntruthlikeness, as well as the idea that a credal state is better the\ncloser the credence is to the truth value. If there is no such account\nof cognitive value then the cognitive value of truthlikeness and the\ncognitive value of believing truly pull in different directions. Maher (1993) argues both that truthlikeness is an important and\nviable notion, and that it is intimately connected with cognitive\nvalue. But instead of trying to define verisimilitude first and then\nderiving cognitive value from that, perhaps together with various\nother ingredients, Maher suggests we should proceed in the opposite\ndirection. We should start with plausible constraints on cognitive\nvalue (or as he prefers to call it, cognitive utility) and derive a\nnotion of verisimilitude from that.  According to Maher, the cognitive\nutility of a proposition A, given that Ci is\nthe truth, is identical to the cognitive utility of accepting\nthe proposition in those circumstances. The verisimilitude of a\nproposition can then be identified with the cognitive utility of\naccepting it. (Maher 1993, 228).  Maher does not\ndefine acceptance explicitly.  But, roughly speaking, according\nto Maher a person accepts P if she asserts P sincerely\n(or would be prepared to assert P sincerely). But her investing\na high probability in P is neither necessary nor sufficient for\nher to accept P. Maher lays down what appear to be fairly modest constraints on what\nwould count as an acceptable cognitive utility function for acceptance\n(CU) all of which are independent of the agent's credences.\nFor example, his first constraint is Respect for truth (Maher\n1990, 210): Note that this is not a strict inequality.  All this guarantees is\nthat it is not cognitively worse to accept A when it is true\nthan when it is false.  It need not be cognitively better to\naccept A when true than when false.  However, even this\napparently weak condition is not uncontroversial.  If a false\nproposition can be closer to the truth than its own (true) negation,\nthen the corresponding principle will not hold of verisimilitude. And\nalmost all the accounts we have considered so far allow this. The\nproposition that the number of planets is 9 is false, but it comes as\nclose to the truth as any false proposition can come. Compare this\nwith the nearly tautologous proposition that the number is either less\nthan 9 or greater than 9. That true proposition allows a vast range of\npossibilities almost all of which lie vast distances from the\ntruth. Mayer also stipulates that the cognitive utility of\naccepting A is insensitive to where the truth lies within the\nrange of A. We could call this Truth-location\nindifference (Maher 1990, 211): Truth location indifference is also controversial.  Suppose you\nacccept that the number of the planets is either 7, 8, 9 or 9 billion.\nThis can be true in four different ways.  If the number of the planets\nis 8 then, intuitively, your belief is overall closer to the truth\nthan if the number is 9 billion. Maher's third main constraint is Respect for information\n(Maher 1990, 213):  This corresponds to the weak value of content for truths.  As we\nhave seen, this too is a controversial principle in the truthlikeness\ndebate. Some theorists accept it, but there are some worthy arguments\nagainst it. Maher's constraints allow a good deal of latitude in what can be\ncognitively valued, and according to Maher that is precisely as it\nshould be. It is the same with practical utility functions. There are\na wide range of legitimate cognitive utility functions, all of which\nrespect the core values of scientific inquiry. Even if the core values\nof scientific inquiry are not captured by these three constraints,\nMaher's approach — of deriving verisimilitude from cognitive\nvalue rather than vice versa — may well be a fruitful one, and\nworthy of further exploration.  We are all fallibilists now, but we are not all skeptics, or\nantirealists or nihilists.  Most of us think inquiries can and do\nprogress even when they fall short of their goal of locating the truth\nof the matter.  We think that an inquiry can progress by moving from\none falsehood to another falsehood, or from one imperfect credal state\nto another.  To reconcile epistemic optimism with realism in the teeth\nof the dismal induction we need a viable concept of truthlikeness, a\nviable account of the empirical indicators of truthlikeness, and a\nviable account of the role of truthlikeness in cognitive value.  And\nall three accounts must fit together appropriately.  There are a number of approaches to the logical problem of\ntruthlikeness but, unfortunately, there is as yet little consensus on\nwhat constitutes the best or most promising approach, and prospects\nfor combining the best features of each approach do not at this stage\nseem bright.  There is, however, much work to be done on both the\nepistemological and the axiological aspects of truthlikeness, and it\nmay well be that new constraints will emerge from those investigations\nthat will help facilitate a fully adequate solution to the logical\nproblem as well.","contact.mail":"oddie@colorado.edu","contact.domain":"colorado.edu"}]
