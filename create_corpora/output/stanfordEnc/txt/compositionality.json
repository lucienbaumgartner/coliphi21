[{"date.published":"2004-04-08","date.changed":"2020-08-17","url":"https://plato.stanford.edu/entries/compositionality/","author1":"Zoltán Gendler Szabó","author1.info":"http://campuspress.yale.edu/zoltanszabo/","entry":"compositionality","body.text":"\n\n\nAnything that deserves to be called a language must contain meaningful\nexpressions built up from other meaningful expressions. How are their\ncomplexity and meaning related? The traditional view is that the\nrelationship is fairly tight: the meaning of a complex expression is\nfully determined by its structure and the meanings of its\nconstituents—once we fix what the parts mean and how they are\nput together we have no more leeway regarding the meaning of the\nwhole. This is the principle of compositionality, a fundamental\npresupposition of most contemporary work in semantics.\n\n\nProponents of compositionality typically emphasize the productivity\nand systematicity of our linguistic understanding. We can understand a\nlarge—perhaps infinitely large—collection of complex\nexpressions the first time we encounter them, and if we understand\nsome complex expressions we tend to understand others that can be\nobtained by recombining their constituents. Compositionality is\nsupposed to feature in the best explanation of these phenomena.\nOpponents of compositionality typically point to cases when meanings\nof larger expressions seem to depend on the intentions of the speaker,\non the linguistic environment, or on the setting in which the\nutterance takes place without their parts displaying a similar\ndependence. They try to respond to the arguments from productivity and\nsystematicity by insisting that the phenomena are limited, and by\nsuggesting alternative explanations.\n\nThere are many theses called ‘the principle of\ncompositionality’. The following can serve as a common reference\npoint: \nImportant variants of the compositionality principle will be presented\nbelow in a form most similar to (C) to facilitate\n comparisons.[1]\n When formulating more precise versions it is crucial to keep the\npre-theoretical intuitions that led many to accept compositionality\nfirmly in mind. \nThe principle of compositionality is normally taken to quantify over\nexpressions of some particular language L: \nQuestions of structure and constituency are settled by the\nsyntax of L, while the meanings of simple expressions\nare given by the lexical semantics of L.\nCompositionality entails (although on many elaborations is not\nentailed by) the claim that syntax plus lexical semantics determines\nthe entire semantics for L. \nIt makes a big difference whether L is a natural or an\nartificial language. Syntactic and semantic questions about a natural\nlanguage are settled by and large through empirical investigation;\nsyntactic and semantic questions about an artificial language are\nsettled usually by checking what the appropriate stipulations are.\nPrima facie, natural languages might turn out not to be\ncompositional, whereas many artificial languages were designed to meet\nsuch a requirement. (Compositionality is a bonus when it comes to\nproof-checking in computer languages, or inductive proofs in logical\ncalculi.) Unless explicitly noted, talk of compositionality is to be\ntaken as talk of compositionality of some particular natural language,\nor of natural languages in general. \nIf thought is a kind of language, we can raise the question whether it\nis compositional. Thought would not have to be much like\nSwahili or the language of set theory for the question to make sense,\nbut we do need the assumptions that thoughts have meanings (and so,\npresumably, are not themselves meanings) and that they have meaningful\nconstituents. These assumptions follow from\n the language of thought hypothesis\n (see entry). Those who reject this hypothesis may still speak of the\ncompositionality of thought—but only in an extended sense. \nWhat would such an extended sense be? The key to generalizing\ncompositionality for non-linguistic representational systems is to\nrelax the syntactic ideas of constituency and structure. Consider, for\nexample, the No-Left-Turn sign: \nThis could be viewed as a complex sign decomposable into meaningful\nfeatures—the shape, the color pattern, the arrow, etc. These\nfeatures are the analogues of simple expressions: they appear in many\nother complex signs and they appear to contribute more or less\nuniformly to their meanings. \nOnce we have an initial grip on what counts as a constituent and how\nconstituents compose we can legitimately raise the question whether\nthis system of representations is\n compositional.[2]\n We may even be able to answer\n it.[3] \nThere is a major debate within the philosophy of mind between\nproponents of classical cognitive architecture and proponents of\n connectionism\n (see entry). The debate is typically presented as a debate about\ncompositionality, but it is not exactly about that. The issue tends to\nbe whether there are such things as meaningful constituents of thought\n(perhaps in the extended sense in which traffic signs can be said to\nhave meaningful constituents), and if there are, whether these\ncontribute the same thing (presumably their meaning) to all thoughts\nin which they occur. If the answer to the first question is negative,\nthe question of compositionality does not arise. If the answer to the\nfirst question is positive, the second is independent of\ncompositionality. (It could be that thought-constituents contribute\nalways the same thing to a thought of which they are constituents, but\nthese contributions, even together with the way the constituents are\ncombined, severely underdetermine the meaning of the thought. And it\ncould be that thought-constituents contribute different things to\ndifferent thoughts—depending, perhaps, on the surroundings of\nthe thinking—but these variable contributions, plus the way the\nconstituents are combined, fully determine the meaning of the\nthought.) The debate about connectionism is more closely related to\nthe question whether reverse compositionality holds (cf.\n section 1.6.4.)\n \nThe principle of compositionality is not committed to a specific\nconception of meaning. In fact, it is frequently announced as a\nprinciple that is applicable to whatever a semantic theory might\nassign to expressions of a language. Furthermore, although the\nreference of an arbitrary expression is definitely not\nsomething one would normally call its ‘meaning’, versions\nof the following principle are frequently called ‘the\nprinciple of compositionality’: \n(I use the word ‘reference’ here roughly the way Frege\nused ‘Bedeutung’ after 1892. But it could also be\ntaken the way Lewis uses ‘extension’. The differences are\nsignificant, but they do not matter for present purposes.) To avoid\nconfusion, we should call this the principle of compositionality\nof reference, and\n (C)\n the principle of compositionality of meaning; when I speak\nof compositionality unqualified, what is meant is always the\nlatter. Since the arguments in favor of compositionality tend\nto be based on general considerations about linguistic\nunderstanding—which, I shall suppose amounts to nothing more or\nless than understanding what linguistic expressions\n mean[4]—proponents\n of (C\\(_{\\textit{ref}}\\)) have a choice to make. They can advocate\n(C\\(_{\\textit{ref}}\\)) on different grounds or they can claim that an\nappropriate theory of the sort that assigns (relative to a variety of\ncontextually determined\n factors[5])\n references to expressions can serve as a theory of meaning. \nFormalizations of\n (C)\n typically make no assumptions about what meanings are. This way we\nachieve generality and stay clear of dogmatic pronouncements. Still,\nit is a mistake to abandon all constraints for that turns\ncompositionality into a vacuous requirement. It is trivial that we can\ncompositionally assign something to each expression of a\nlanguage (for example, if expressions serve as their own meanings,\nsemantics is certainly compositional!) but it does not follow that it\nis trivial to adequately assign meanings to\nthem. \nThe point applies to more subtle attempts to trivialize\ncompositionality as well. Consider a famous result due to Zadrozny\n(1994). Given a set S of strings generated from an arbitrary\nalphabet via concatenation and a meaning function m which\nassigns the members of an arbitrary set M to the members of\nS, we can construct a new meaning function \\(\\mu\\) such that\nfor all \\(s, t \\in S \\mu(s{.}t) = \\mu(s)(\\mu(t))\\) and \\(\\mu(s)(s) =\nm(s)\\). What this shows is that we can turn an arbitrary meaning\nfunction into a compositional\n one,[6]\n as long as we replace the old meanings with new “meanings” from which they\nare uniformly\n recoverable.[7]\nWhat is not clear is whether these new “meanings” really are meanings. After all, someone ignorant of m could assign entities to members of S following \\(\\mu\\). Such a person would know what each expression in S “means” but would not necessarily know what any of them means. (For further discussion\nof Zadrozny’s result, see Kazmi and Pelletier (1998),\nWesterståhl (1998), Dever (1999).) \nCompositionality obviously constrains what meanings might be. But the\nconstraints apply only to the meanings of complex\nexpressions—for all\n (C)\n tells us the meanings of simple expressions could be tables and\nchairs. For let the meanings of complex expressions be interpreted\nlogical forms, i.e., phrase structure trees with the meanings of the\nconstituent lexical items assigned to their terminal nodes. In a\nfairly straightforward sense the meanings of lexical items are then\nparts of the meanings of complex expressions in which they occur, and\nso the meanings of complexes are determined from the relevant tables\nand chairs together with their syntactic mode of composition; for\nsimilar remarks see Horwich (1997). \nThat compositionality does not constrain lexical meaning might appear\nparadoxical at first, but the source of paradox is just instability in\nhow the label ‘compositionality’ is used. Sometimes\ncompositionality is said to be that feature in a language (or\nnon-linguistic representational system) which best explains the\nproductivity and systematicity of our understanding; cf. Fodor 2001:\n6.\n (C)\n is but one of the features such explanations use—others include\nthe context-invariance of most lexical meaning, the finiteness of the\nlexicon, the relative simplicity of syntax, and probably much else.\nThese features together put significant constraints on what\nlexical meanings might be; cf. the papers collected in Fodor and\nLepore (2002) and Szabó (2004). \nMuch of what was said above about the need to constrain what counts as\nmeaning applies to structure as well. Janssen (1983) has a proof that we can turn\nany meaning assignment on a recursively enumerable set of expressions\ninto a compositional one, as long as we can replace the syntactic\noperations with different ones. If we insist—as we\nshould—that any acceptable semantic theory must respect what\nsyntax tells us about the structure of complex expressions, this\nresult says nothing about the possibility of providing an adequate\ncompositional semantics; cf. Westerståhl\n 1998.[8]\n The moral of the result is that although commitment to\ncompositionality requires allegiance to no particular sect of\nsyntacticians, one cannot be oblivious to syntactic evidence in\nsemantic theorizing. \n\n (C)\n does not require the kind of tight correspondence between syntax and\nsemantics we intuitively associate with compositionality. To\nillustrate this, consider a view, according to which the meaning of a\ndeclarative sentence s is the set of possible worlds where\ns is true. According to such a view, tautologies are\nsynonymous, even though (since Rudolf presumably has some tautological\nbeliefs and lacks others) sentences resulting from embedding\ntautologies under ‘Rudolf believes that…’ are not.\n(I also assume a straightforward semantics for propositional attitudes\nwithout hidden indexicals or tacit quantification.) Intuitively, this\nis a violation of compositionality. Still, the semantics is\nnot in conflict with\n (C):\n tautologies might differ structurally or in the meaning of their\nconstituents, which could explain how their embedding can yield\nnon-synonymous sentences; cf. Carnap 1947 and Lewis 1970. \nTo rule a semantic theory like this one non-compositional, we need to\ndemand that the meaning of a complex expression be determined by its\nimmediate structure, and the meanings of its\nimmediate constituents. (The immediate structure of an\nexpression is the syntactic mode its immediate constituents are\ncombined. e is an immediate constituent of \\(e'\\) iff e\nis a constituent of \\(e'\\) and \\(e'\\) has no constituent of which\ne is a constituent.) \nCall the strengthened principle local compositionality, and\n (C)\n global compositionality; when unqualified,\n‘compositionality’ should be taken as global. The local\nprinciple is more intuitive, and semanticists frequently presuppose\nit. In fact, some theorists assume not only\n(C\\(_{\\textit{local}}\\)),\nbut also that concatenation is uniformly interpreted as functional\napplication, or perhaps as conjunction; cf. Pietroski (2005, 2012).\nOthers insist that the relevant notion of structure\n(C\\(_{\\textit{local}}\\)) appeals to is the one apparent on the surface\nof sentences, and accordingly, our syntax should not postulate\nmovement or empty elements; cf. Jacobson (2002, 2012). Clearly, appeal\nto our ability to understand novel expressions in itself provides no\ndirect support for these strong claims. \nIntuitively, if a language is compositional it cannot contain a pair\nof non-synonymous complex expressions with identical structure and\npairwise synonymous constituents. This should follow from the fact\nthat the same structure and the same meanings of constituents cannot\ndetermine more than one meaning within a language. But to\nensure that the inference really holds, we need to rule out a certain\nreading of\n (C).\n  \nFine (2007) advocates the following view: ‘Cicero’ and\n‘Tully’ are synonyms, but ‘Cicero is Cicero’\nand ‘Cicero is Tully’ are not, despite the fact that these\nsentences do have the same structure. The meaning-difference arises\nfrom the fact that the former sentence encodes semantic co-reference\nbut the latter does not. All this is fully compatible with English not\nbeing a counterexample to (C\\(_{\\textit{coll}}\\)): \nOn Fine’s view, what the constituents of ‘Cicero is\nCicero’ collectively mean goes beyond what the\nconstituents of ‘Cicero is Tully’ do. The collective\nmeaning comprises the individual meanings plus certain\nmeaning-relations that hold among them. Call the weak principle\n(C\\(_{\\textit{coll}}\\)) collective compositionality;\nfollowing usual practice\n (C)\n will be understood as distributive compositionality.  \nIt is clear that the meanings of complex expressions depend on the\nindividual meanings of their parts. Thus, the only chance for\n(C\\(_{\\textit{coll}}\\)) to be true is if the collective meaning of\nconstituents (whatever that might be) determines each of the\nindividual meanings of constituents. This is certainly so on\nFine’s view: he thinks the meaning of ‘Cicero is\nCicero’ depends on its structure (this is why it is not\nsynonymous with ‘Is Cicero Cicero?’), on the individual\nmeanings of its constituents (this is why it is not synonymous with\n‘Cicero is Caesar’), and in addition on the\nintended co-reference relation between the subject and the object,\nwhich is an aspect of the collective meaning of its constituents (this\nis why it is not synonymous with ‘Cicero is Tully’). \nGiven the distributive reading,\n (C)\n rules out only the existence of a pair of non-synonymous complex\nexpressions with identical structure and pairwise synonymous\nconstituents within a single language. This is a problem\nbecause the existence of such a pair is a clear violation of what we\nnormally mean by determination even if the expressions belong to\ndistinct languages.  \nHere is an illustration from Szabó (2000b). Suppose English is\ncompositional. Take two of its non-synonymous sentences—say,\n‘Elephants are grey’ and ‘Julius Caesar was murdered\non the ides of March’—and define Crypto-English as the\nlanguage with the same expressions, the same syntax and almost the\nsame semantics as English. The only difference is that if a\nsentence is synonymous in English with one of the two designated\nsentences, then it is synonymous with the other in Crypto-English. We\nassumed English is compositional and hence that there is no pair of\nnon-synonymous complex expressions in English with identical structure\nand pairwise synonymous constituents. Trivially, the same must hold\nfor Crypto-English as well. But intuitively, Crypto-English is\nnot compositional. The structure and the meanings of\nconstituents of the Crypto-English sentence ‘Elephants are\ngrey’ cannot determine what this sentence means in\nCrypto-English—if they did then the structure and the meanings\nof constituents of the English sentence ‘Elephants are\ngrey’ would have to determine what ‘Julius Caesar was\nmurdered on the ides of March’ means in English. \nIf we want a better match with our intuitions, we must demand more\nfrom a compositional language than the mere existence of a\nfunction from structures and the meanings of parts to the\nmeanings of wholes. One possibility would be to put constraints on\nthis function—we could demand, for example that it be\ncomputable, or perhaps even that the computation be reasonably quick.\nBut the above example shows that such a strengthening would not solve\nthe problem: if computing the meanings of complex expressions is easy\nin English, it will not be hard in Crypto-English either. We might\ninstead opt for the following strengthening of\n (C): \nCall the strengthened principle cross-linguistic\ncompositionality, and\n (C)—when\n ‘determine’ is simply read as ‘functionally\ndetermine’ and we may have different functions for different\nlanguages—language-bound compositionality. Note that\nformal languages that are designed to satisfy language-bound\ncompositionality may nonetheless violate cross-linguistic\ncompositionality simply because their syntax or the meanings of their\nconstituents violate some universal constraint on human languages.\nNote also that whatever the epistemic status of other versions of the\nprinciple of compositionality might be, cross-linguistic\ncompositionality is clearly an empirical\n hypothesis.[9] \nWhen speaking of compositionality unqualified, I will always mean\nlanguage-bound compositionality. Again, the stronger principle is much\ncloser to our pre-theoretic intuitions and it is often tacitly assumed\nin practice. But the traditional considerations in favor of\ncompositionality support the weaker thesis only. \nThe fact that natural languages contain indexicals forces us to\ndistinguish between two notions of meaning. On the one hand,\nexpressions have a standing meaning fixed by convention and\nknown to those who are linguistically competent. On the other hand,\nexpressions in use are associated with occasion meanings\nwhich is discerned by interpreters in part on the basis of contextual\ninformation. The terminology is from Quine (1960). Kaplan (1977) uses\nthe terms character and content but he makes a\nnumber of substantive assumptions about what these are which I intend\nto abstract from. Thus, we should not assume that occasion meanings\nare structured entities built from objects, properties, and relations,\nor even that the occasion meanings of declarative sentences are always\npropositions. Also, we need not assume that standing meanings are\nfunctions from contexts to occasion meanings, or even that they\ndetermine in context what occasion meanings are. \nWhen we speak of meaning, usually we have standing meaning in mind.\nBut not always—when a contract specifies that within its main\ntext ‘current edition of building code’ means the 2012\nedition of the Florida Building Code, it obviously fixes occasion\nmeaning. Corresponding to these two notions of meaning, there are two\nversions of the principle of compositionality. Since occasion-meaning\nis determined, in part, by context (C\\(_{\\textit{occ}}\\)) must be\nrelativized to\n context:[10] \nLet’s call expressions whose occasion meaning sometimes deviates\nfrom their standing meaning context-dependent. The scope of\ncontext-dependent lexical items is a matter of controversy. On the one\nextreme, there are semantic minimalists who think these\ninclude only a handful expressions: the personal and demonstrative\npronouns, a few adverbs (e.g., ‘here’, ‘now’,\n‘next’), and a few adjectives (e.g.‘actual’,\n‘present‘, ‘local’); cf. Cappelen and Lepore\n(2005). On the other\nextreme are radical contextualists who think essentially all\nlexical items are context-dependent; e.g., Searle (1980). As usual,\nmost theorists are somewhere in the middle—taking heat from both\nsides that their view is untenable. \nRadical contextualism is sometimes seen as a challenge to\ncompositionality, more precisely, to\n (C\\(_{\\textit{occ}}\\));\n cf. Cohen (1986), Lahav (1989), Fodor (2001). It shouldn’t be.\nAn effective argument from context-dependence against\n (C\\(_{\\textit{occ}}\\))\n would need to show that there is at least one complex expression in\nL whose occasion meaning varies with context, while the\noccasion meanings of its constituents all remain the same. The usual\nconsiderations against compositionality typically omit the second\npart. Take for example Searle’s observation that “[t]he\nsort of thing that constitutes cutting the grass is quite different\nfrom e.g., the sort of thing that constitutes cutting a cake”\n(Searle 1980: 222). What follows from this? Nothing more than the fact\nthat the occasion meaning of ‘cut’ is sensitive to a\nfeature of the context in which it is used, in particular, to its\nlinguistic environment. This is fully compatible with\n (C\\(_{\\textit{occ}}\\)).\n  \nOf course, we should not insist that the occasion meaning of\n‘cut’ depends on nothing but its standing meaning and the\nlinguistic environment in which it occurs. As Searle himself\nemphasized, ‘cut the grass’ can pick out one sort of thing\nif we are using it in a context of selling strips of grass turf and\nanother it we are using in a context of selling lawn mowers. Arguably,\nthis shows that the occasion meaning of ‘cut’ depends on\nextra-linguistic factors as well. No matter: this too is fully\ncompatible with\n (C\\(_{\\textit{occ}}\\)).\n Compositionality demands nothing more than that all\ncontext-dependence be accounted for via context-dependence in the\nlexicon and it takes no stance of how much and what kind of lexical\ncontext-dependence there might be; cf. Szabó (2010), Lasersohn\n(2012), and Recanati\n (2012).[11] \nWe have distinguished several interpretations for the seemingly simple\nclaim that a certain language is compositional and we picked a fairly\nnatural one. Thus, we proposed to read\n (C)\n as being about meaning (as opposed to reference or some other value\none might assign to expressions), that it postulates functional\ndetermination of meaning within a particular language (as opposed to\nacross a class of languages), and that the determinants of the meaning\nof a complex expression are its entire structure (as opposed to just\nits immediate structure) and the meanings of its constituents\nindividually (as opposed to collectively). We saw that\n (C)\n remains ambiguous even after these clarifications, for there are at\nleast two kinds of meaning it could be about (standing meaning and\noccasion meaning). If it is intended to be about occasion meaning, it\nmust involve a suppressed quantification over contexts (just as it\ncontains a suppressed quantification over languages).  \nThere are a number of principles worth mentioning that are often\ndiscussed along with (and are occasionally confused with) the\nprinciple of compositionality. It is useful to see which, if any of\nthem is equivalent to\n (C).\n  \nConsider first the often cited principle that says that substitution\nof synonyms is always meaning-preserving. As stated, the principle\nrequires clarification. For one thing, not every case of replacement\ncounts as substitution: the expression we replace with its synonym\nwithin a larger expression must be a constituent of the larger\nexpression. Otherwise, as Geach pointed out, the synonymy of\n‘Plato was bald’ with ‘Baldness was an attribute of\nPlato’ would guarantee the synonymy of ‘The philosopher\nwhose most eminent pupil was Plato was bald’ and ‘The\nphilosopher whose most eminent pupil was baldness was an attribute of\nPlato’; (Geach 1965: 110). \nIn addition, we need to separate two issues: whether substitution of\nsynonyms can turn a meaningful expression into a meaningless one, and\nwhether it can turn a meaningful expression into an expression with a\ndifferent meaning. The principle that rules out the former possibility\nwas first proposed by Husserl (1913: 318), and it is usually stated in\nterms of the notion of a semantic category. Two expressions\nbelong to the same semantic category just in case they are\nintersubstitutable within any meaningful expression salva\nsignificatione (without loss of meaningfulness). According to\nHusserl’s principle: \n(H) is a rather controversial—intuitively, there are many\nsynonyms that are not everywhere intersubstitutable. For example,\n‘likely’ and ‘probable’ mean pretty much the\nsame even though ‘Jacques is likely to leave’ is\nmeaningful while ‘Jacques is probable to leave’ is\narguably not; cf. Gazdar (1985:\n 32).[12]\n And—more controversially—there might be synonyms that are\nalmost nowhere intersubstitutable: ‘quick’ and\n‘quickly’ are good candidates. \n\n(C) even combined with (H) does not rule out the possibility that some lexical items are syncategorematic, i.e. that they are meaningless but contribute nevertheless to the meanings of larger expressions in which they occur. Formal languages often contain such expressions. For example, in the language of the propositional calculus, the Boolean connectives are not assigned anything by the interpretation function. Rather, semantic clauses specify the interpretation of complex expressions in which they occur as the main connective. \n\n \nThe principle that rules out the possibility that substitution of\nsynonyms could turn a meaningful expression into one with a different\nmeaning comes in two versions: \nAssuming the language under discussion has a grammar that requires\nthat each constituent of a meaningful complex expression be itself\nmeaningful (S\\(_{\\textit{plural}}\\)) is stronger than\n (C)—it\n is equivalent to local, distributive, language bound\ncompositionality of meaning. Assuming in addition that the language\nsatisfies\n (H),\n (S\\(_{\\textit{singular}}\\)) is equivalent to\n(S\\(_{\\textit{plural}}\\)); cf. Hodges (2001: Theorem 4). \nThe fact that compositionality—at least in the weak form it is usually stated and supported by arguments from productivity—does not require substitutivity is important. Substituting a complex expression for a simple one within some expression effects the structure of that expression, and since compositionality allows meaning to depend on structure, such a substitution can\naffect meaning. Consider one of Quine’s arguments; cf. (Quine 1953: 143). ‘It is necessary that the number of planets is greater than seven’ is\nfalse, ‘It is necessary that eight is greater than seven’ is true, even though\n‘the number of planets’ and ‘eight’ have the same extension, so the semantics of English cannot be extensional.(Quine uses ‘nine’ because he thought Pluto was a planet.) The argument assumes that the extension of a definite description is whatever uniquely satisfies it—a claim rejected by proponents of quantificational accounts of definite\ndescriptions (e.g. Russellians). But even if we grant this assumption as well as the compositionality of English, the argument still fails: the difference\nin truth-value could be the result of the different syntactic structure of\nthe two sentences. \nSometimes the claim that L is compositional is presented\ndirectly as a claim about the relationship between its syntax and\nsemantics. The following thesis is often called the rule-to-rule\nprinciple:  \nHow strong a claim (RR) is depends on what counts as a rule. If an\narbitrary function deserves that name, the rule-to-rule principle is\nstronger than\n (C):\n it is equivalent to local, distributive, language bound\ncompositionality of meaning. But if we insist—quite\nplausibly—that a semantic rule must be computable (or perhaps\neasily computable) the rule-to-rule principle is stronger than that.\nAnd if we assume that rules must have some sort of psychological\nreality, (RR) says something completely different from\n (C).\n  \nOrdinarily when we say that something determines something else we\nthink of the former as being causally or explanatorily prior to the\nlatter. Although the principle of compositionality is usually not\nunderstood in this way, sometimes philosophers read it as a principle\nthat asserts the priority of word meaning over sentence meaning, or\nmore generally, the priority of the meanings of lexical items over the\nmeanings of complex expressions: \n(P) is often thought to be in tension with the idea that each\nexpression has the meaning it does in virtue of the way it is used\nwithin some linguistic community. The conflict is supposed to arise\nbecause (i) the use of an expression is exhausted by its employment in\nspeech acts, and (ii) it is sentences, not words, that can be employed\nto make speech acts. Against this, it can be argued that\nreferring is among the speech acts speakers routinely perform\nand that this speech act is done with words, not sentences. One might\ntry to replace (i) with a stronger claim, for example, that the use of\nan expression is exhausted by its employment in asserting,\nasking, commanding, and a few of other speech acts\nnot including referring. But even if true the stronger claim\nmay not save the argument against (P) because, at least prima\nfacie, we can make assertions uttering isolated words; cf.\nStainton (2006). Davis (2003) develops a detailed theory of meaning\nthat combines (P) with a version of the use theory of meaning. \nTo say that there is no easy argument against\n (P)\n is a far cry from saying that it must be true. It is important to\nkeep in mind that\n (P)\n is significantly stronger than\n (C)\n and that the usual arguments in favor of compositionality cannot by\nthemselves justify it. (For some arguments against (P), see Szabó (2019).) \n \nIn section 60 of the Foundations of Arithmetic (1884) Frege\nfamously declares that only within a complete sentence do words have\nmeaning. This has come to be referred to in the literature as\nFrege’s context principle. Frege writes that “it\nis enough if the sentence as whole has meaning; thereby also its parts\nobtain their meanings” (Frege [1884] 1950: section\n 60).[13]\n On the face of it, this asserts that words have their meanings in\nvirtue of the meaning of sentences in which they occur as\nconstituents. This is incompatible with\n (P),\n but not with\n (C).\n Even if words are meaningful only because they occur as constituents\nwithin sentences, there could still be a function (perhaps even a\nsingle function across all possible human languages) that maps the\nstructure of a sentence and the meanings of its constituent words to\nthe meaning of that sentence. \nThere is an alternative way to construe Frege’s principle, a way\nthat makes it a determination claim, not a primacy claim. To state it\nin a form that matches the generality of\n (C)\n we should drop the talk of words and sentences, and talk instead\nabout complex expressions and their constituents: \nLike the principle of compositionality, (F\\(_{\\textit{all}}\\)) can be\ninterpreted as a claim about reference or meaning, locally or\nglobally, collectively or distributively, in a language-bound manner\nor cross-linguistically. Compositionality is about bottom-up\nmeaning-determination, while the context principle about top-down\nmeaning-determination. As long as it is not understood as a causal or\nexplanatory relation determination can be symmetric, so any version of\n (C)\n is compatible with the corresponding version of\n(F\\(_{\\textit{all}}\\)).  \nThere is a strengthening of\n (F\\(_{\\textit{all}}\\)),\n according to which the meaning of an expression is determined not\nonly by the meanings of all expressions in which it occurs as\na constituent, but by the meaning of any one of these\nexpressions: \n(F\\(_{\\textit{any}}\\)) is an immediate consequence of the converse of\n (C)—sometimes\n called reverse compositionality—according to which the\nmeaning of a complex expression determines the structure of the\nexpression and the meanings of its constituents. (Fodor (1998b), Fodor\n& Lepore (2001), Pagin (2003) advocate reverse compositionality;\nPatterson (2005), Robbins (2005), Johnson (2006) are among its\nopponents. The debate is complex, in part because at least some\nproponents of reverse compositionality advocate it only for the\nlanguage of thought; cf. Fodor 2001.) \n\n (F\\(_{\\textit{any}}\\))\n is a very strong thesis and most standard semantic theories are\nincompatible with it. Take, for example, a simple Carnapian semantics\nthat assigns to each sentence the set of possible worlds where it is\ntrue. Suppose we are considering a language that contains the standard\nlogical operators, and so any sentence is a constituent of a\nnecessarily true sentence. Since the meaning of a necessary truth is\nthe set of all possible worlds, this set would have to determine the\nmeanings of all sentences in the language, which is absurd. \nA principle of intermediate strength between\n (F\\(_{\\textit{all}}\\))\n and\n (F\\(_{\\textit{any}}\\))\n is (F\\(_{\\textit{cofinal}}\\)): \n(A cofinal set of expressions is a set such that any expression occurs\nas a constituent in at least one member of the set. Except for very\nodd languages, the set of all expressions within the language in which\nsome given expression occurs as a constituent is one of many cofinal\nsets of expressions, so\n (F\\(_{\\textit{all}}\\))\n follows from (F\\(_{\\textit{cof}}\\)) but not the other way around.\nThat (F\\(_{\\textit{cof}}\\)) follows from\n (F\\(_{\\textit{any}}\\))\n but not the other way around is trivial.)  \nOne interesting feature of (F\\(_{\\textit{cofinal}}\\)) is that it\nappears to be in conflict with a Quine’s thesis of the\nindeterminacy of translation (taken as a thesis that implies the\nindeterminacy of meaning). Assume that the set of all observation\nsentences is cofinal within a reasonably large fragment of a natural\nlanguage and that the meaning of an observation sentence is identical\nto its stimulus meaning—(F\\(_{\\textit{cofinal}}\\)) ensures then\nthat the meanings of all the words are determined within our fragment.\nThere has been an attempt to show that\n(F\\(_{\\textit{cofinal}}\\)) follows from less controversial claims, and\nperhaps even from claims that Quine himself was committed to; cf.\nWerning (2004). The heart of Werning’s argument is the\nExtension Theorem; cf. Theorem 14 in Hodges 2001. The theorem\nstates that a meaning assignment to a cofinal set of expressions that\nsatisfies\n (H)\n and\n (S\\(_{\\textit{singular}}\\))\n has a unique extension to a meaning assignment to all expressions\nthat satisfies\n (H),\n (S\\(_{\\textit{singular}}\\)) as well as its converse (there is a generalized result\nmentioned in Hodges 2012: 257). The extra assumptions needed to get\nfrom the Extension Theorem to a denial of indeterminacy remain\nquestionable; cf. Leitgeb (2005). \nThe claim that L is compositional is often taken to mean that\nthe meaning of an arbitrary complex expression in L is built up\nfrom the meanings of its constituents in L—call this the\nbuilding principle for L. This is a fairly strong\nclaim, at least if we take the building metaphor seriously. For then\nthe meanings of complex expressions must themselves be complex\nentities whose structure mirrors that of the sentence; cf. Frege\n(1892, 1919). This\npresumably entails but is not entailed by local distributive\ncross-linguistic compositionality of meaning. \nMontague (1970) suggested a perspicuous way to capture the principle\nof compositionality formally. The key idea is that compositionality\nrequires the existence of a homomorphism between the\nexpressions of a language and the meanings of those expressions. \nLet us think of the expressions of a language as a set upon which a\nnumber of operations (syntactic rules) are defined. Let us require\nthat syntactic rules always apply to a fixed number of expressions and\nyield a single expression, and let us allow that syntactic rules be\nundefined for certain expressions. So, a syntactic algebra is\na partial algebra \\(\\mathbf{E} = \\langle E, (F_{\\gamma})_{\\gamma \\in\n\\Gamma}\\rangle\\), where E is the set of (simple and complex)\nexpressions and every \\(F_{\\gamma}\\) is a partial syntactic operation\non E with a fixed arity. The syntactic algebra is interpreted\nthrough a meaning-assignment m, a function from E to\nM, the set of available meanings for the expressions of\nE. \nConsider now F, a k-ary syntactic operation on E.\nm is F-compositional just in case there is a\nk-ary partial function G on M such that whenever\n\\(F(e_1 ,\\ldots ,e_k)\\) is defined, \n(In English: there is a partial function G from the meanings of\n\\(e_1 ,\\ldots ,e_k\\) to the meaning of the expression built from \\(e_1\n,\\ldots ,e_k\\) through an application of the syntactic rule\nF.) \nFinally, we can say that m is compositional\nsimpliciter just in case m is F-compositional\nfor each syntactic operation in E. Whenever m\nis compositional, it induces the semantic algebra \\(\\mathbf{M} =\n\\langle M, (G_{\\gamma})_{\\gamma \\in \\Gamma}\\rangle\\) on M, and\nit is a homomorphism between E and\nM; cf. Westerståhl (1998). (For details,\nvariants, and formal results, see Janssen (1983, 1997), Hodges (2001), and\nPagin & Westerståhl (2010a). For generalizations that cover\nlanguages with various sorts of context-dependence, see Pagin (2005),\nPagin & Pelletier (2007), and Westerståhl (2012).) \nSince there are no restrictions on what m assigns to members of\nE, the formal statement captures both compositionality of\nreference and compositionality of meaning. As stated, the principle\ncaptures local distributive language-bound compositionality: it\nrequires that each application of each syntactic rule within a\nlanguage be matched by an application of an appropriate semantic\nfunction. To capture cross-linguistic compositionality is easy: all we\nneed to say is that the expressions within E are the\nexpressions of all possible human languages. (Of course, if we allow\nthe syntactic algebra to contain expressions of different languages,\nwe may want to insist that syntactic operations map expressions of a\nlanguage onto complex expressions of the same language and that they\nremain undefined for cases when their argument positions are filled by\nexpressions from different\n languages.[14])\n  \nTo capture global compositionality is more complicated. Here is an\nattempt. Let us say that the expressions e and \\(e'\\) are\nlocal equivalents just in case they are the results of\napplying the same syntactic operation to lists of expressions such\nthat corresponding members of the lists are synonymous. (More\nformally: for some natural number k there is a k-ary\nF in E, and there are some expressions\n\\(e_1, \\ldots, e_k\\), \\(e_1 ', \\ldots, e_k '\\) in E,\nsuch that \\(e = F(e_1 ,\\ldots ,e_k)\\), \\(e' = F(e_1 ',\\ldots ,e_k\n')\\), and for every \\(1\\le i \\le k\\), \\(m(e_i) = m(e_i ')\\).) It is\nclear that m is locally compositional just in case locally\nequivalent pairs of expressions are all synonyms. Let us say that the\nexpressions e and \\(e'\\) are global equivalents just\nin case they are the results of applying the same syntactic operation\nto lists of expressions such that corresponding members of the lists\nare either (i) simple and synonymous or (ii) complex and globally\nequivalent. (Here is the recursive definition more formally. Let us\nsay that the expressions e and \\(e'\\) are 1-global\nequivalents just in case they are synonymous simple expressions.\nLet us say that the expressions e and \\(e'\\) are n-global\nequivalents just in case for some natural number k there\nis a k-ary F in E, and there are some\nexpressions \\(e_1 ,\\ldots ,e_k, e_1 ',\\ldots ,e_k '\\) in E,\nsuch that \\(e = F(e_1 ,\\ldots ,e_k), e' = F(e_1 ',\\ldots ,e_k ')\\),\nand for every \\(1 \\le i \\le k\\) there is a \\(1 \\le j \\lt n\\) such that\n\\(e_i\\) and \\(e_i '\\) are j-global equivalents. Finally, let us\nsay that the expressions e and \\(e'\\) are global\nequivalents just in case for some natural number n they\nare n-global\n equivalents.)[15]\n I suggest that m is globally compositional just in case\nglobally equivalent pairs of expressions are all synonyms. \nCollective compositionality is a further weakening of global\ncompositionality. It could be formalized using the same trick. Thus,\nwe can say that m is collectively compositional just in case\ncollectively equivalent pairs of expressions are all synonyms, where\nwe define collective equivalence exactly like global equivalence with\none difference. In the recursive step we demand not only that \\(e_i\\)\nand \\(e_i '\\) be j-collective equivalents but also that the\nvery same semantic relations should hold among \\(e_1 ,\\ldots ,e_k\\)\nand among \\(e_1 ',\\ldots ,e_k '\\). Thus, we leave space for the\npossibility that ‘Cicero is Cicero’ is not collectively\nequivalent to ‘Cicero is Tully’, even though they have the\nsame structure and their proper constituents are all collectively\nequivalent; see\n section 1.4.\n  \nThe simplest argument for compositionality is that it is supported by\nintuitions many claim to have about meaning and structure. Although\nthere are interesting putative counterexamples (see\n section 4.2)\n they probably can be explained away through modest revisions of our\nsyntactic and/or semantic theories. This defense is reasonable but\nmuch too modest. For even if it succeeds in convincing some who\naren’t already convinced, it leaves us all in the dark\nwhy compositionality is true. Defenders of compositionality\nshould do better than this. \nThe argument most often used to support compositionality is based on\nproductivity. It goes back (at least) to Frege, who claimed that  \nthe possibility of our understanding sentences which we have never\nheard before rests evidently on this, that we can construct the sense\nof a sentence out of parts that correspond to words. (Frege [c. 1914]\n1980: 79)  \nThe argument is an inference to the best explanation, which can be\nexpanded and rephrased without assuming that meanings are Fregean\n senses.[16] \nTo bolster the claim that we do, in fact, understand complex\nexpressions we never heard before, philosophers often appeal to\nunboundedness: although we are finite beings we have the\ncapacity to understand each of an infinitely large set of complex\nexpressions. Although there are dissenters—e.g., Ziff (1974) \nand Pullum & Scholz (2010)—the claim that natural languages \ncontain infinitely many complex expressions is\n plausible.[17]\n But it is equally plausible that nobody who reads this entry the\nfirst time has ever encountered this very sentence before, and\nconsequently, the detour through cardinality considerations seems\nsuperfluous. Occasionally, the fact that natural languages are\nlearnable is also used to argue for compositionality. This is\nnot an independent argument: the reason it is remarkable that we can\nlearn a natural language is that once we have learnt it our\nunderstanding is productive. If we could not understand expressions we\nnever encountered before, without detailed empirical study we could\nnot rule out the hypothesis that we learned the language in question\nby rote. \nThe first thing to point out about the argument from productivity is\nthat it is an argument in favor of\n (C)—global\n distributive language-bound compositionality of meaning. As it\nstands, it provides no reason for believing anything this principle\ndoes not entail; in particular it cannot establish\n (C\\(_{\\textit{ref}}\\)),\n (C\\(_{\\textit{local}}\\)), or\n (C\\(_{\\textit{cross}}\\)).\n  \nThe argument can be criticized on the ground that considerations of\nthis sort simply cannot establish a universal claim. Suppose someone\nsuggests that the complex expression e is a counterexample to\n (C).\n The fact that we tend to understand all sorts of complex expressions\nwe never heard before does not mean that we would understand e\non the first encounter. But suppose we would. Still, even if in\ngeneral we tend to understand complex expressions we never heard\nbefore in virtue of our knowledge of their structure and the meanings\nof their simple constituents, we might understand e in some\nother way. General considerations of productivity cannot rule out\nisolated exceptions to compositionality.  \nIsolated putative\nexceptions are called idioms— and natural languages contain many of them. (Jackendoff (1997) estimates the number of English idioms to be around twenty-five thousand. Defenders of the compositionality of natural languages sometimes argue that the syntactic complexity of these expressions is only apparent, and hence, they can be viewed as lexical items. But unless we are given clear\nnon-semantic grounds for singling out idioms, the move is\nquestion-begging. Such criteria have been proposed, but they tend to\nbe rather controversial; cf. Nunberg, Sag, and Wasow (1994).) \nIf we lower our sights and seek to prove nothing more than the claim\nthat natural languages by and large obey global distributive\nlanguage-bound compositionality of meaning, the argument from\nproductivity is reasonably strong. \nAnother argument in favor of compositionality is based on\nsystematicity, the fact that there are definite and\npredictable patterns among the sentences we understand. For example,\nanyone who understands ‘The rug is under the chair’ can\nunderstand ‘The chair is under the rug’ and vice\nversa. This is also an inference to the best explanation, and can\nbe summarized as follows: \nAlthough the arguments from productivity and systematicity are usually\nalluded to in the same breath, they are very different\nconsiderations. Unlike the main premise of the former, the main\npremise of the latter is anything but obvious. Particular instances\nare plausible enough: it seems reasonable that anyone who can\nunderstand ‘The dog is asleep’ and ‘The cat is\nawake’ can also understand ‘The dog is awake’ and\n‘The cat is asleep’, and that anyone who can understand\n‘black dog’ and ‘white cat’ can also\nunderstand ‘black cat’ and ‘white dog’. But do\nall who understand ‘within an hour’ and ‘without a\nwatch’ also understand ‘within a watch’ and\n‘without an hour’? And do all who understand\n‘halfway closed’ and ‘firmly believed’ also\nunderstand ‘halfway believed’ and ‘firmly\nclosed’? As Johnson (2004) argues, the claim that natural\nlanguages are systematic presupposes a natural non-overlapping\nlinguistic categorization of all the expressions. The existence of\nsuch a categorization is a bold empirical hypothesis. \nFodor (1998b) does offer an empirical argument in favor of\nsystematicity. The idea is that if complex expressions could be\nunderstood without understanding their constituents then it is unclear\nhow exposure to a corpus made up almost entirely of complex\nexpressions could suffice to learn the meanings of lexical items. But,\nas a matter of empirical fact, children learn the meanings of words by\nencountering them almost exclusively within other expressions.\nHowever, as Robbins (2005) points out, this observation can at best\nlead one to conclude that understanding a suitably large set\nof complex expressions in which a given expression occurs as a\nconstituent suffices for understanding the constituent itself. It does\nnot show that understanding any complex expression suffices\nfor understanding its constituents. \nThe arguments from productivity and systematicity differ in what they\naim to prove. First, the argument from systematicity proves something\nweaker than (any version of) compositionality. If we run the argument\nfor the pair of sentences ‘The dog is asleep’ and\n‘The cat is awake’ we can conclude that the meanings of\n‘the dog’, ‘the cat’, ‘is asleep’\nand ‘is awake’ plus predication determine the meaning of\n‘The dog is awake’. It does not follow that the\nmeanings of ‘the dog’ and ‘is awake’ plus\npredication do that. Second, if this problem can be fixed somehow, the\nargument from systematicity proves not only global, but\nlocal compositionality: it tells us that the meanings of\nimmediate constituents and immediate structure fix the meanings of\ncomplex expressions. Finally, if successful, the argument from\nsystematicity proves not only a version of the compositionality\nprinciple, but also reverse compositionality. We are invited to\nconclude that the meaning of an arbitrary complex expression\ndetermines its immediate structure and the meanings of its immediate\nconstituents; cf.\n section 1.6.4,\nFodor & Lepore 2001: 59;\nPagin 2003: 292. \nAs with the argument from productivity, the argument from\nsystematicity is unable to screen out isolated counterexamples. Still,\nit is a reasonably strong consideration in favor of the claim that\nnatural languages by and large obey language-bound\ndistributive local compositionality of meaning and its reverse. \nBy far the most popular reason for believing in compositionality is\nthat it works. Linguists have adopted various versions of the\nprinciple as a working hypothesis and developed semantic theories on\ntheir basis. These theories have provided intuitively satisfactory\nexplanations for certain data, such as the validity or invalidity of\ninferences or contrasts in acceptability between minimal pairs.\nMoreover, whenever it was suggested that certain phenomena require\nabandonment of the principle, it was subsequently shown that this is\nnot so: reasonably elegant and comparatively natural compositional\ntheories were just around the corner; cf.\n section 4.2.\n  \nDespite its popularity, this is not a very good reason to believe in\ncompositionality. The fact that compositional semantic theories can\nexplain certain things does not show that they explain those things\nbecause they are compositional. Do we have reason to think\nthat without assuming compositionality we would not be able to explain\nthe same things? It seems to me that we have no such reason:\nsemanticists have focused on whether they can hold on to\ncompositionality while providing satisfactory explanations, not on\nwhether they have to embrace compositionality in order to\nprovide satisfactory explanations. We are not entitled to assume that\nadopting compositionality as a working hypothesis has in any way\ncontributed to explanatory success in semantics.  \nA much more promising methodological argument for compositionality\ngoes as follows. The fact that we are able to communicate in real time\nmakes it overwhelmingly likely that the computational complexity of\nthe interpretation algorithm we employ is relatively low. In fact, it\nseems reasonable to think that semantic theories with minimal\ncomplexity are, other things being equal, preferable. And there are\ncertain results that show that, under certain conditions, semantics\ntheories that conform to a certain strengthening of\n (C)\n will be minimally complex; cf. Pagin (2012). Alas, the conditions in\nquestion tend to be unrealistic for natural languages. Nonetheless, we\ncan think of them as idealizations, which makes adoption of\ncompositionality as a working hypothesis still a reasonable one.  \nThe arguments from productivity and systematicity are persuasive. Many\nlinguists and philosophers have suggested that the explanation of\nthese phenomena that presupposes compositionality is not only the\nbest, but also the only one imaginable. So, before I survey some of\nthe putative counterexamples to compositionality from the semantic\nliterature, to bolster the imagination I will discuss a simple\nnon-linguistic case where our understanding is productive and\nsystematic despite apparent lack of compositionality in the system of\nrepresentations. \nConsider the Algebraic notation for\n chess.[18]\n Here are the basics. The rows of the chessboard are represented by\nthe numerals \\(\\b{1},\\b{2}, \\ldots ,\\b{8}\\); the columns are represented by the\nlower case letters \\(\\ba, \\bb, \\ldots ,\\bh\\). The squares are identified by\ncolumn and row; for example \\(\\b{b5}\\) is at the intersection of the\nsecond column from the left and the fifth row from the top. Upper case\nletters represent the pieces: \\(\\bK\\) stands for king, \\(\\bQ\\) for\nqueen, \\(\\bR\\) for rook, \\(\\bB\\) for bishop, and \\(\\bN\\) for\nknight. Moves are typically represented by a triplet consisting of an\nupper case letter standing for the piece that makes the move and a\nsign standing for the square where the piece moves. There are five\nexceptions to this: (i) moves made by pawns lack the upper case letter\nfrom the beginning, (ii) when more than one piece of the same type\ncould reach the same square, the sign for the square of departure is\nplaced immediately in front of the sign for the square of arrival,\n(iii) when a move results in a capture an \\(\\bx\\) is placed\nimmediately in front of the sign for the square of arrival, (iv) the\nsymbol \\(\\b0\\hy\\b0\\) represents castling on the king’s side, (v)\nthe symbol \\(\\b{0\\hy 0\\hy 0}\\) represents castling on the queen’s\nside. \\(\\bpl\\) stands for check, and \\(\\bpl\\bpl\\) for mate. The rest of the\nnotation serves to make commentaries about the moves and is\ninessential for understanding it. \nSomeone who understands the Algebraic notation must be able to follow\ndescriptions of particular chess games in it and someone who can do\nthat must be able to tell which move is represented by\nparticular lines within such a description. Nonetheless, it is clear\nthat when someone sees the line \\(\\b{Bb5}\\) in the middle of such a\ndescription, knowing what \\(\\bB, \\bb\\), and \\(\\b{5}\\) mean will not be\nenough to figure out what this move is supposed to be. It must be a\nmove to \\(\\b{b5}\\) made by a bishop, but we don’t know which bishop\n(not even whether it is white or black) and we don’t know which\nsquare it is coming from. All this can be determined by following the\ndescription of the game from the beginning, assuming that one knows\nwhat the initial configurations of figures are on the chessboard, that\nwhite moves first, and that afterwards black and white move one after\nthe other. But staring at \\(\\b{Bb5}\\) itself will not help. \nThe first moral of the example is that we can have productive and\nsystematic understanding of representations even if we do not\nunderstand complex representations merely by understanding\ntheir simple components and the way those components are combined. The\nreason this could happen is that all who understand the system know\ncertain things (e.g., the initial configuration of pieces and the\norder of moves) from which they can figure out the missing information\n(e.g., which figure is moving and from where). \nThe second moral is that—given certain assumptions about meaning\nin chess notation—we can have productive and systematic\nunderstanding of representations even if the system itself is not\ncompositional. The assumptions in question are that (i) the\ndescription I gave in the first paragraph of this section fully\ndetermines what the simple expressions of chess notation mean and also\nhow they can be combined to form complex expressions, and that (ii)\nthe meaning of a line within a chess notation determines a move. One\ncan reject (i) and argue, for example, that the meaning of \\(\\bB\\) in\n\\(\\b{Bb5}\\) contains an indexical component and within the context of\na description, it picks out a particular bishop moving from a\nparticular square. One can also reject (ii) and argue, for example,\nthat the meaning of \\(\\b{Bb5}\\) is nothing more than the meaning of\n‘some bishop moves from somewhere to square\n\\(\\b{b5}\\)’—utterances of \\(\\b{Bb5}\\) might carry extra\ninformation but that is of no concern for the semantics of the\nnotation. Both moves would save compositionality at a price. The first\ncomplicates considerably what we have to say about lexical meanings;\nthe second widens the gap between meanings of expressions and the\ninformation conveyed by their utterances. Whether saving\ncompositionality is worth either of these costs (or whether there is\nsome other story to be told about our understanding of the Algebraic\nnotation) is by no means clear. For all we know, Algebraic notation\nmight be non-compositional. \nWe now discuss briefly four famous putative counterexamples to the\ncompositionality of English from the semantic literature. The list is\nsupposed to be representative but by no means exhaustive. (For a more\nsystematic survey of how compositionality problems are typically\nsolved in formal semantics, see Zimmerman 2012.) In each case, we also\nindicate what reasonable responses to the challenges might look\nlike. \nPutative counterexamples to\n (C)\n are always complex expressions whose meaning appears to depend not\nonly on the meanings of their constituents and on their structure but\non some third factor as well. Sometimes this third factor is\nlinguistic context: what a complex expression means seems to depend in\npart on how it is embedded into a sentence\n (§ 4.2.1)\n or a sequence of sentences\n (§ 4.2.2).\n In other cases the third factor is extra-linguistic: the setting in\nwhich the complex expression is used\n (§ 4.2.3)\n or someone’s beliefs about what the expression means\n (§ 4.2.4). \nSuch putative counterexamples are not all on the same level. Although\nthey all violate the letter of\n (C),\n some could be more easily reconciled with productivity and\nsystematicity than others. If it turned out that in order to interpret\nan embedded sentence, one needs information about the embedding\nsentence as well we would have to conclude that the algorithm for\ncalculating the meanings of complex expressions is more complicated\nthen we thought. But a complicated algorithm is still an algorithm and\nthe core explanation of how we understand complex expressions would\nremain untouched. By contrast, if it turned out that in order to\ninterpret a sentence we must know all sorts of ephemeral\nnon-linguistic facts we would have to conclude that the fact that we\ncan reliably understand all sorts of unfamiliar sentences is a\nmystery. Those who accept putative counterexamples of this latter kind\nmust provide alternative explanations for productivity and\nsystematicity. \nConsider the following minimal pair: \nA good translation of (1) into a first-order language is (1′).\nBut the analogous translation of (2) would yield (2′), which is\ninadequate. A good translation for (2) would be (2″) but it is\nunclear why. We might convert ‘\\(\\neg \\exists\\)’ to the\nequivalent ‘\\(\\forall \\neg\\)’ but then we must also\ninexplicably push the negation into the consequent of the embedded\nconditional. \nThis gives rise to a problem for the compositionality of English,\nsince is seems rather plausible that the syntactic structure of (1)\nand (2) is the same and that ‘if’ contributes some sort of\nconditional connective—not necessarily a material\nconditional!—to the meaning of (1). But it seems that it cannot\ncontribute just that to the meaning of (2). More precisely, the\ninterpretation of an embedded conditional clause appears to be\nsensitive to the nature of the quantifier in the embedding\nsentence—a violation of compositionality (the problem is\noriginally raised in Higginbotham 1986). \nOne response might be to claim that ‘if’ does not\ncontribute a conditional connective to the meaning of either (1) or\n(2)—rather, it marks a restriction on the domain of the\nquantifier, as the paraphrases under (1″) and (2″)\n suggest:[19] \nBut this simple proposal (however it may be implemented) runs into\ntrouble when it comes to quantifiers like ‘most’. Unlike\n(3′), (3) says that those students (in the contextually given\ndomain) who succeed if they work hard are most of the students (in the\ncontextually relevant domain): \n\nSection 6.4. of Stalnaker (2014) contains a detailed defense of the claim that this problem and related ones are properly handled if we use the semantics of Stalnaker (1975) for indicative conditionals. But the predictions depend essentially on the fact that on this account, conditionals are context-dependent and their interpretation is constrained by certain pragmatic principles. There are many alternative approaches on the market and the question how best account for these examples remains controversial.[20] \nConsider the following minimal pair from Barbara Partee: \nThere is a clear difference between (4) and (5)—the first one is\nunproblematic, the second markedly odd. This difference is plausibly a\nmatter of meaning, and so (4) and (5) cannot be synonyms. Nonetheless,\nthe first sentences are at least truth-conditionally equivalent. If we\nadopt a conception of meaning where truth-conditional equivalence is\nsufficient for synonymy, we have an apparent counterexample to\ncompositionality. \nFew would insist that the first sentences of (4) and (5) are really\nsynonymous. What is interesting about this example is that even if we\nconclude that we should opt for a more fine grained conception of\nmeaning, it is not immediately clear how that will account for the\ncontrast between these sentences. The difference is obviously due to\nthe fact that ‘one’ occurs in the first sentence of (4),\nwhich is available as a proper antecedent for ‘it’ and\nthat there is nothing in the first sentence of (5) that could play a\nsimilar role. Some authors have suggested that the right way to\napproach this problem is to opt for a dynamic conception of meaning,\none that can encode anaphoric possibilities for subsequent sentences\n(cf. Heim 1982; Groenendijk & Stokhof 1990, 1991; and Chierchia\n1995). \nInteresting though these cases might be, it is not at all clear that\nwe are faced with a genuine challenge to compositionality, even if we\nwant to stick with the idea that meanings are just truth-conditions.\nFor it is not clear that (5) lacks the normal reading of (4)—on\nreflection it seems better to say that the reading is available even\nthough it is considerably harder to get. (Contrast this with an\nexample due to—I think—Irene Heim: ‘They got\nmarried. She is beautiful.’ This is like (5) because the first\nsentence lacks an explicit antecedent for the pronoun in the second.\nNonetheless, it is clear that the bride is said to be beautiful.) If\nthe difference between (4) and (5) is only this, it is no longer clear\nthat we must accept the idea that they must differ in meaning. \nSuppose a Japanese maple leaf, turned brown, has been painted green.\nConsider someone pointing at this leaf uttering (6): \nThe utterance could be true on one occasion (say, when the speaker is\nsorting leaves for decoration) and false on another (say, when the\nspeaker is trying to identify the species of tree the leaf belongs\nto). The meanings of the words are the same on both occasions and so\nis their syntactic composition. But the meaning of (6) on these two\noccasions—what (6) says when uttered in these\noccasions—is different. As Charles Travis, the inventor of this\nexample puts it: “…words may have all the stipulated\nfeatures while saying something true, but also while saying something\nfalse” (Travis 1994: 171–172; see also Travis 1996 and Lahav 1989). \nThere are many possible responses to this challenge. One is to deny\nthe relevant intuition. Perhaps the leaf really is green if it is\npainted green and (6) is uttered truly in both situations.\nNonetheless, we might be sometimes reluctant to make such a true\nutterance for fear of being misleading. We might be taken to falsely\nsuggest that the leaf is green under the paint or that it is not\npainted at all (cf. Sainsbury 2001 and Berg 2002).\nThe second option is to point out that the fact that a sentence can\nsay one thing on one occasion and something else on another is not in\nconflict with its meaning remaining the same. Do we have then a\nchallenge to compositionality of reference, or perhaps to\ncompositionality of content? Not clear, for the reference or content\nof ‘green’ may also change between the two situations.\nThis could happen, for example, if the lexical representation of this\nword contains an indexical\n element.[21]\n If this seems ad hoc, we can say instead that although there\nis no context-dependent expression in (6) it can still be used to make\nboth true and false assertions. Perhaps the compositionally determined\noccasion meanings are impoverished (perhaps not even propositional),\nwhich is why they tend to be different from what speakers assert (cf.\nBach 1994 and Soames 2005). \nPerhaps the most widely known objection to compositionality comes from\nthe observation that even if e and \\(e'\\) are synonyms, the\ntruth-values of sentences where they occur embedded within the clausal\ncomplement of a mental attitude verb may well differ. So, despite the\nfact that ‘eye-doctor’ and ‘ophthalmologist’\nare synonyms (7) may be true and (8) false if Carla is ignorant of\nthis fact: \nSo, we have a case of apparent violation of compositionality; cf.\nPelletier (1994). \nThere is a sizable literature on the semantics of\n propositional attitude reports\n (see entry). Some think that considerations like this show that there\nare no genuine synonyms in natural languages. If so, compositionality\n(at least the language-bound version) is of course vacuously true.\nSome deny the intuition that (7) and (8) may differ in\ntruth-conditions and seek explanations for the contrary appearance in\nterms of\n implicature.[22]\n Some give up the letter of compositionality but still provide\nrecursive semantic\n clauses.[23]\n And some preserve compositionality by postulating a hidden indexical\nassociated with ‘believe’ (for example, Richard 1990, Crimmins & Perry\n1989, and Crimmins 1992).","contact.mail":"zoltan.szabo@yale.edu","contact.domain":"yale.edu"}]
