[{"date.published":"2001-02-26","date.changed":"2019-08-28","url":"https://plato.stanford.edu/entries/epistemology-social/","author1":"Alvin Goldman","author2":"Cailin O'Connor","author1.info":"http://philosophy.rutgers.edu/index.php?option=com_content&task=view&id=102&Itemid=210","entry":"epistemology-social","body.text":"\n\n\nUntil recently, epistemology—the study of knowledge and\njustified belief—was heavily individualistic in focus. The\nemphasis was on evaluating doxastic attitudes (beliefs and disbeliefs)\nof individuals in abstraction from their social environment. Social\nepistemology seeks to redress this imbalance by investigating the\nepistemic effects of social interactions and social systems. After\ngiving an introduction, and reviewing the history of the field in\nsections 1 and 2, we move on to discuss central topics in social\nepistemology in section 3. These include testimony, peer disagreement,\nand judgment aggregation, among others. Section 4 turns to recent\napproaches which have used formal methods to address core topics in\nsocial epistemology, as well as wider questions about the functioning\nof epistemic communities like those in science. In section 5 we\nbriefly turn to questions related to social epistemology and the\nproper functioning of democratic societies. \n\nWhat do we mean by the phrase “social epistemology”, the\ntopic covered in this entry? \nSocial epistemology gets its distinctive character by standing in\ncontrast with what might be dubbed “individual”\nepistemology. Epistemology in general is concerned with how people\nshould go about the business of trying to determine what is true, or\nwhat are the facts of the matter, on selected topics. In the\ncase of individual epistemology, the person or agent in question who\nseeks the truth is a single individual who undertakes the task all by\nhimself/herself, without consulting others. By contrast\nsocial epistemology is, in the first instance, an enterprise\nconcerned with how people can best pursue the truth (whichever truth\nis in question) with the help of, or in the face of,\nothers. It is also concerned with truth acquisition by groups, or\ncollective agents.  \nAccording to the most influential tradition in (Western) epistemology,\nillustrated vividly by René Descartes (1637), standard\nepistemology has taken the form of individual epistemology, in which\nthe object of study is how epistemic agents, using their personal\ncognitive devices, can soundly investigate assorted questions.\nDescartes contended that the most promising way to pursue truth is by\none’s own reasoning. The remaining question was how, exactly,\ntruth was to be found by suitable individualistic maneuvers, starting\nfrom one’s own introspected mental contents. Another major\nfigure in the history of the field was John Locke (1690), who insisted\nthat knowledge be acquired through intellectual self-reliance. As he\nput it, “other men’s opinions floating in one’s\nbrain” do not constitute genuine knowledge.  \nIn contrast with the individualistic orientations of Descartes and\nLocke, social epistemology proceeds on the commonsensical idea that\ninformation can often be acquired from others. To be sure, this step\ncannot be taken unless the primary investigator has already determined\nthat there are such people, a determination that presumably\nrequires the use of individual resources (hearing, seeing, language,\netc.) Social epistemology should thus not be understood as a wholly\ndistinct and independent form of epistemology, but one that rests on\nindividual epistemology.  \nSurprisingly, social epistemology does not have a very long, or rich,\nhistory. With perhaps a few exceptions, it has not been explored by\nphilosophy with much systematicity until recent times. The case of\nmodern science, by contrast, was rather different. The Royal Society of London was created\nin 1660, intended to highlight the importance of multiple observers in\nestablishing recognized facts. (And, as we will see throughout the\nentry, there are important connections between philosophy of science\nand social epistemology.) But the leading figures of philosophy\ncommonly tended to present themselves as solitary investigators. To be\nsure, they did not refrain from debating with one another. But insofar\nas the topic was “epistemology” (as it came to be called),\nit centered on the challenges and practices of individual agents. \nA movement somewhat analogous to social epistemology was developed in\nthe middle part of the 20th century, in which sociologists and\ndeconstructionists set out to debunk orthodox epistemology, sometimes\nchallenging the very possibility of truth, rationality, factuality,\nand/or other presumed desiderata of mainstream epistemology. Members\nof the “strong program” in the sociology of science, such\nas Bruno Latour and Steve Woolgar (1986), challenged the notions of\nobjective truth and factuality, arguing that so-called\n“facts” are not discovered or revealed by science, but\ninstead “constructed”, “constituted”, or\n“fabricated”. “There is no object beyond\ndiscourse,” they wrote. “The organization of discourse\nis the object” (1986: 73). \nA similar version of postmodernism was offered by the philosopher\nRichard Rorty (1979). Rorty rejected the traditional conception of\nknowledge as “accuracy of representation” and sought to\nreplace it with a notion of “social justification of\nbelief”. As he expressed it, there is no such thing as a\nclassical “objective truth”. The closest thing to (so\ncalled) truth is merely the practice of “keeping the\nconversation going” (1979: 377).  \nOther forms of deconstruction were also inspired by social factors but\nwere less extreme in embracing anti-objectivist conclusions about\nscience. Thomas Kuhn (1962/1970) held that purely objective\nconsiderations could never settle disputes between competing theories;\nhence scientific beliefs must be influenced by social factors.\nAnalogously, Michel Foucault developed a radically political view of\nknowledge and science, arguing that practices of so-called\nknowledge-seeking are driven by quests for power and social domination\n(1969 [1972], 1975 [1977]). \nDebates about these topics persisted under the heading of “the\nscience wars”. Within the mainstreams of both science and\nphilosophy, however, the foregoing views have generally been rejected\nas implausibly radical. This did not mean that no lessons were learned\nabout the status of social factors in science and philosophy. These\ndebates gave important insight into the role of cultural beliefs and\nbiases in the creation of knowledge. What we shall pursue, however, in\nthe remainder of this entry is how social epistemology has created a\nnew branch, or sphere, of mainstream epistemology. According to this\n“extension” of epistemology, social factors can and do\nmake major contributions to traditional, truth-oriented epistemology\nby introducing, broadening, and refining new problems, new techniques,\nand new methodologies. But such factors do not undercut the very\nnotions of truth and falsity, knowledge and error. \nSharply departing from the debunking themes sketched above, modern\nsocial epistemology is prepared to advance proposals quite continuous\nwith traditional epistemology. It sees no reason to think that social\nfactors or practices inevitably interfere with, or pose threats to,\nthe attainment of truth and/or other epistemic desiderata, such as\njustified belief, rational belief, etc. There may indeed be\nidentifiable cases (which we shall explore) in which specific types of\nsocial factors or social interactions pose threats to truth\nacquisition. But, conversely, the right kinds of social organization\nmay enhance the prospects of truth acquisition. \nA general overview of these different kinds of cases was advanced in a\nwide-ranging monograph by Alvin Goldman: Knowledge in a Social\nWorld (1999). This book emerged from several earlier papers\ncritiquing postmodernist attacks on truth and the prospects of truth\nacquisition. They included papers on argumentation (Goldman 1994),\nfreedom of speech (Goldman and Cox 1996), and scientific inquiry\n(Goldman 1987). Other contributions with broadly similar orientations\nincluded C.A.J. Coady’s Testimony (1992), Edward\nCraig’s Knowledge and the State of Nature (1990), and\nPhilip Kitcher’s “The Division of Cognitive Labor”\n(1990) and The Advancement of Science (1993). Margaret\nGilbert’s monograph On Social Facts (1989) made a\nforceful case for the existence of “plural subjects”, a\ncrucial metaphysical component for social epistemology. The journal\nEpisteme: A Journal of Social Epistemology (Goldman, editor),\nwas begun in 2004 and played a major role in the positive development\nof the field. (A different journal with a similar title, Social\nEpistemology (Steve Fuller, editor) began somewhat earlier in\n1988, but tilted heavily toward a debunking orientation). \nIn exploring social epistemology we explore how assorted\nsocial-epistemic activities or practices have an impact on the\nepistemic outcomes of the agents (or groups) in question. Which\nchanges in social-epistemic practices are likely to promote, enhance,\nor impede epistemic outcomes? To put more flesh on the varieties of\nsocial-epistemic methods and outcomes, let us look at some core\ntopics.  \nThe first kind of social-epistemic scenario is very common. An\nindividual seeks to determine the truth-value of proposition p\nby soliciting the opinion(s) of others. She might direct her question\nto one of her personal confidants, or consult what is in print or\navailable online. Having received responses to her queries, she weighs\nwhat has been said to help her assess the truth of the proposition in\nquestion. This is commonly referred to as\n“testimony-based” belief. The selected informant may still\nbe a single individual. But appealing to another individual for\ntestimony already locates the example as within the domain of social\nepistemology. \nThe main discussion here is framed in terms of justification rather\nthan knowledge. The standard question is: Under what circumstances is\na hearer justified in trusting an assertion made by a stranger, or by\na consultant or speaker of any variety? David Hume argued that we are\ngenerally entitled to trust what others tell us; but this entitlement\nonly arises by virtue of what we previously learned from others. Each\nof us can recall occasions on which we were told things that we could\nnot independently verify (from perception, e.g.) but later determined\nto be true. This reliable track-record from the past (which we\nremember) warrants us in inferring (via induction) that testimony is\ngenerally reliable. As James Van Cleve formulates the view: \nTestimony gives us justified belief … not because it shines by\nits own light, but because it has often enough been revealed true by\nour other lights. (Van Cleve 2006: 69) \nThis sort of view is called “reductionism” (about\ntestimony) because it “reduces” the\njustification-conferring force of testimony to the combined forces of\nperception, memory, and inductive inference. More precisely, the view\nis usually called global reductionism, because it argues that\nhearers of testimony are justified in believing particular instances\nof testimony by inferential appeal to testimony’s general\nreliability. \n[It] seems absurd to suggest that, individually, we have done anything\nlike the amount of field-work that [reductionism] requires …\n[M]any of us have never seen a baby born, nor have many of us examined\nthe circulation of the blood nor the actual geography of the world\n… nor a vast number of other observations that [reductionism]\nwould seem to require. (Coady 1992: 82) \nAn alternative to global reductionism is local reductionism\n(E. Fricker 1994). Local reductionism does not require hearers to be\njustified in believing the general reliability of testimony.\nIt only requires hearers to be justified in trusting the reliability\nof the specific speakers whose current testimony is in question on a\nparticular subject. This requirement is more easily satisfied than\nglobal reductionism. Local reductionism may still be too strong,\nhowever, but for a different reason. Is a speaker S trustworthy\nfor hearer H only if H has positive evidence or\njustification for S’s general reliability? This is far\nfrom clear. If I am at an airport or train station and hear a public\nannouncement of the departure gate (or track), am I justified in\nbelieving this testimony only if I have prior evidence of the\nannouncer’s general reliability? Normally I do not possess such\nevidence for a given public address announcer. But, surely, I am\njustified in trusting such announcements. \nGiven these problems for both kinds of reductionism, some\nepistemologists embrace testimonial anti-reductionism (Coady\n1992; Burge 1993; Foley 1994; Lackey 2008). Anti-reductionism holds\nthat testimony is itself a basic source of evidence or justifiedness.\nNo matter how little positive evidence a hearer has about the\nreliability and sincerity of a given speaker, or of speakers in\ngeneral, she has default or prima facie warrant in believing\nwhat the speaker says. Thus Tyler Burge writes: \n[A] person is entitled to accept as true something that is presented\nas true and that is intelligible to him, unless there are stronger\nreasons not to do so. (Burge 1993: 457)  \nIn the preceding example, it was tacitly assumed that the hearer had\nno prior belief (one way or the other) about the topic in question.\nHis/her mind was completely open prior to receiving the testimony of\nthe speaker. Now let us consider a different class of examples\n(modified from one given by David Christensen [2007]). Suppose that\ntwo people start out with opposing views on a given topic; one of them\nbelieves proposition p and the other believes not-p. To\nadd a little color, consider an example in which two friends, Harry\nand Mary, have each acquired some evidence about a traffic accident\ndescribed in the morning newspaper. Neither has any other evidence\nabout the incident. For example, neither has any background knowledge\nor information about the people said to be involved in the incident.\nHowever, having read the newspaper account, both Harry and Mary form\nbeliefs about it. Harry believes that Jones (described in the\nnewspaper) was responsible for the accident. Mary believes that Jones\nwas not responsible. Harry and Mary now run into each other, find out\nthat their friend has read the same story, and discuss their views\nabout the accident. To make the example even more interesting, suppose\nthe two friends have as much respect for the other person’s\njudgment (in such matters) as they do for themselves. This might be\nbased on the fact that when they have disagreed with one another in\nthe past, each turned out to be right about 50% of the time.  \nIn such a situation, we shall call the two people epistemic peers.\nEpistemologists have adopted the label “peer disagreement”\nfor the problem generated by such cases (actual or hypothetical). The\nproblem is: how, if at all, should an agent adjust her initial belief\nabout the specified proposition upon learning that her peer holds a\ncontrary position? Should she (always?) modify her belief (or strength\nof belief) in the direction of the peer? Or is it sometimes\nepistemically permissible to hold “steadfast” to\none’s own original conviction? \n“Conciliationism” is the view that (some degree of)\nmodification is always called for, because it requires epistemic\nagents in the specified type of situation to make some obeisance to\nthe belief of their peer, rather than ignoring or dismissing it\nentirely. Proponents of conciliationism include Christensen (2007),\nRichard Feldman (2007), and Adam Elga (2007). \nFeldman (2007) offers an abstract argument for conciliationism based\non what he calls the “uniqueness thesis”. This is the view\nthat for any proposition p and any body of evidence E,\nexactly one doxastic attitude is the rational attitude to have toward\np, where the possible attitudes include believing p,\ndisbelieving p, and suspending judgment. Feldman’s\nargument seems to be that if the uniqueness theory is true, then when\nI believe p and my peer believes not-p, at least one of\nus must have formed an irrational opinion. Since I have no good reason\nto believe that I am not the misguided believer, the only rational\noption is to suspend judgment. But few people accept the uniqueness\nthesis. Are epistemic rules or principles really (always) so precise\nand restrictive? \nCritics of conciliationism offer a number of reasons for rejecting it\nas a systematic epistemic requirement. One line of criticism runs as\nfollows. Conciliationism may well be self-refuting. Since the truth of\nconciliationism is itself a matter of controversy, a proponent of\nconciliationism should become much less convinced of its truth when he\nlearns about this controversy. One may worry that there is something\nwrong with a principle that tells you not to believe in its own truth.\n(For further discussion, see Christensen 2013.) \nThomas Kelly (2010) offers a different reason for rejecting\nconciliationism, or at least for denying its ubiquitous\nappropriateness. He argues as follows:  \n[If] you and I have arrived at our opinions in response to a\nsubstantial body of evidence, and your opinion is a reasonable\nresponse to the evidence while mine is not, then you are not required\nto give equal weight to my opinion and to your own. Indeed, one might\nwonder whether you are required to give any weight to my\nopinion in such circumstances. (2010: 135)  \nHe acknowledges that whether one reasons well or badly, one might be\nequally confident in one’s conclusion in both cases. However, he\ncontends, “We should not thereby be drawn to the conclusion that\nthe deliverances of good reasoning and bad reasoning have the same\nepistemic status” (2010: 141). Rather, the person that reasons\nbetter (from the same evidence) may well be more entitled to his/her\nconclusion than the person who reasons worse. It may be rational for\nher to hold fast to her initial belief. This approach is what Kelly\ncalls the “Total Evidence View”. \nThe cases discussed thus far focus on epistemic agents who are\nindividuals. We subsumed these cases under the heading of\nsocial epistemology not because the believers themselves have\nsome sort of social character, but because there are players who rely\nupon, or appeal to, other epistemic agents. However, there are other\ntypes of cases that are naturally treated as social epistemology for\nan entirely different reason. \nIt is common to ascribe actions, intentions, and representational\nstates—including belief states—to collections or groups of\npeople. Such collections would include juries, panels, governments,\nassemblies, teams, etc. The State of California might be said to know\nthat chemical XYZ is a cause of cancer. What does it take for a group\nto believe something? Some take what is called the “summative\napproach”—a group believes something just in case all, or\nalmost all, of its members hold the belief (see, for instance, Quinton\n1976: 17). Margaret Gilbert, however, has pointed out that in ordinary\nlanguage use it is common to ascribe a belief to a group without\nassuming that all members hold the belief in question. In seeking to\naddress why this usage is acceptable, she advances a\n“collective” account of group belief. Under this view: \nA group G believes that p if and only if the members of\nG are jointly committed to believe that p as a\nbody. \nJoint commitments create normative requirements for group members to\nemulate a single believer of p. On Gilbert’s account,\nthe commitment to act this way is common knowledge, and if group\nmembers do not act accordingly they can be held normatively\nresponsible by their peers for failing to do so (see Gilbert 1987,\n1989, 2004). Frederick Schmitt (1994a), similarly offers a collective,\ncommitment-based account of group belief. \nSome have argued that these views are not properly about group belief\nbecause they focus on responsibility to peers, and not on the\nbeliefs-states of the group members. Wray (2001) suggests that these\nshould be considered accounts of group acceptance instead. Later in\nthe entry, we will address some accounts that take individual beliefs\nas inputs and output some group belief. It will become clear how\ndifferent these are from a joint commitment type view. \nA different approach is taken by Alexander Bird (2014) who contends\nthat the acceptance model of group belief is only one of many\ndifferent (but legitimate) models. For instance, he introduces the\n“distributed model” to deal with systems that feature\ninformation-intensive tasks which cannot be processed by a single\nindividual. Several individuals must gather different pieces of\ninformation while others coordinate this information and use it to\ncomplete the task. A famous example is provided by Edwin Hutchins\n(1995) who describes a large ship where different crew members take\ndifferent bearings so that a plotter can determine the ship’s\nposition and course. Members in such a distributed system will not\nordinarily satisfy the conditions of mutual awareness and commitment\nthat Gilbert and Schmitt require. Nonetheless, this seems to be a\nperfectly plausible species of group belief. Indeed, Bird contends\nthat this is a fairly standard type of group model that occurs in\nscience. Notice that for this model, again, it is quite possible that\nnot all members of the group hold the same belief. \nWhat about other attempts to provide an account of group belief?\nChristian List and Philip Pettit (2011) explore how individuals may\nyield a group belief through “judgment aggregation”.\nAlthough List and Pettit do not put it this way, it seems that\naggregation refers to metaphysical relations between beliefs of group\nmembers and beliefs of the groups they compose. It refers to ways in\nwhich a set of beliefs by group members can give rise to beliefs held\nby the group. Here is how List and Pettit express these metaphysical\nrelations: \nThe things a group agent does are clearly determined by the things its\nmembers do: they cannot emerge independently. In particular, no group\nagent can form propositional attitudes without the latter\nattitudes’ being determined, in one way or another, by certain\ncontributions of its members, and no group agent can act without one\nor more of its members acting. (2011: 64) \nThe foregoing passage places some limits on the metaphysical relations\nbetween member attitudes and group attitudes. It tells us something\nabout when group propositional attitudes occur, without implying\nanything about when the latter attitudes are justified. But social\nepistemologists are interested in the relation between the epistemic\nstatuses of members’ beliefs and the epistemic status of group\nbeliefs. List and Pettit address (something like) this question by\nexploring the sorts of judgment “functions”, i.e., rules\nfor aggregation, that might come into play. \nA sticky problem that emerges here is the “doctrinal\nparadox,” originally formulated by Kornhauser and Sager (1986)\nin the context of legal judgments. Suppose that a court consisting of\nthree judges must render a judgment in a breach-of-contract case. The\ngroup judgment is to be based on each of three related propositions,\nwhere the first two propositions are premises and the third\nproposition is the conclusion. For example: \nLegal doctrine entails that obligation and action are jointly\nnecessary and sufficient for liability. That is, conclusion (3) is\ntrue if and only if the two preceding premises, (1) and (2), are each\ntrue. Suppose, however, as shown in the table below, that the three\njudges form the indicated beliefs, vote accordingly, and the\njudgment-aggregation function delivers a conclusion guided by majority\nrule. In this case, the court as a whole forms beliefs and casts votes\nas shown below: \nIn this example, each of the three judges has a logically\nself-consistent set of beliefs. Moreover, a majority aggregation\nfunction seems eminently reasonable. Nonetheless, the upshot is that\nthe court’s judgments are jointly inconsistent. \nThis kind of problem arises easily when a judgment is made by multiple\nmembers of a collective entity. This led List and Pettit to prove an\nimpossibility theorem in which a reasonable-looking combination of\nconstraints is nonetheless shown to be jointly unsatisfiable in\njudgment aggregation (List and Pettit 2011: 50). They begin by\nintroducing four conditions that seem to be ones that a reasonable\naggregation function should satisfy: \nUniversal domain: The aggregation function admits as input\nany possible profile of individual attitudes toward the propositions\non the agenda, assuming that individual attitudes are consistent and\ncomplete. \nCollective rationality: The aggregation function produces as\noutput consistent and complete group attitudes toward the propositions\non the agenda. \nAnonymity: All individuals’ attitudes are given equal\nweight in determining the group attitude. \nSystematicity: The group attitude on each proposition depends\non the individuals’ attitudes toward it, not on their attitudes\ntoward other propositions, and the pattern of dependence between\nindividual and collective attitudes is the same for all\npropositions.  \nAlthough these four conditions seem initially plausible, they cannot\nbe jointly satisfied. Having proved this List and Pettit proceed to\nexplore “escape routes” from this impossibility. They\noffer ways to relax the requirements so that majority voting, for\nexample, does satisfy the collective rationality\ndesideratum.  \nBriggs et al. (2014) offer a particular way out. As they argue, it may\nbe too strong to require that entities always have logically\nconsistent beliefs. For instance, we might have many beliefs about\nmatters of fact, and also believe it likely we are wrong about some of\nthem. Following Joyce (1998), they introduce a weaker notion of\ncoherence of beliefs. They show that the majority voting aggregation\nof logically consistent beliefs will always be coherent, and the\naggregation of coherent beliefs will typically be coherent as well. In\nother words, if we demand less from majority voting as a means of\njudgment aggregation, we can get it. \nIn thinking about practical cases outside jury deliberation, we might\nask whether the theory of judgment aggregation can apply to the\nincreasingly common problem of how collaborating scientific authors\nshould decide what statements to endorse. Solomon (2006), for\ninstance, argues that voting might help scientists avoid\n“groupthink” arising from group deliberation. While Wray\n(2014) defends deliberation as crucial to the production of group\nconsensus, Bright et al. (2018) point out that consensus is not always\nnecessary (or possible) in scientific reporting. In such cases, they\nargue, majority voting is a good way to decide what statements a\nreport will endorse, even if there is disagreement in the group. \nUntil this point, we have only looked at how a group belief may arise\nfrom, or be determined by, its member beliefs. We have not yet\nconsidered how—or whether—a group entity can achieve some\nsignificant level of epistemic “status”, or\n“achievement”. Believing something that is true is\ncertainly worth something, but if one gets the truth only by luck or\naccident, that is not very significant. Similarly, simply having a set\nof coherent beliefs isn’t of unqualified significance, since\nmere coherence can arise from pure imagination coupled with\ninconsistency avoidance. Philosophers have long agreed that one of the\nmost important types of epistemic attainment is knowledge.\nBut what is knowledge? Almost everyone agrees that mere true belief\nisn’t knowledge. Again, true beliefs can sometimes arise\nfortuitously, by pure guessing or wishful thinking. \nOne popular way of strengthening the requirements for knowledge is to\nadd a justification requirement. A true belief doesn’t\nper se qualify as knowledge unless the belief is\njustified. Justification is widely accepted as one species,\nor variety, of epistemic achievement. In addition to providing\n(arguably) an essential condition for knowledge, it seems to be an\nachievement, or attainment, in its own right, even when it\ndoesn’t bring truth in along with it. For this reason,\nepistemologists have paid considerable attention to the nature of\njustification. Of course, most of this attention is concentrated on\njustification by individual believers. But, as we shall see,\nit is worth pursuing the possibility that group beliefs can\nalso be evaluated as either justified or unjustified. \nIf the government of the United States believes (or were to believe)\nthat there is a crisis of global warming, it would probably be\njustified in believing this, because of the overwhelming consensus of\nclimate scientists to this effect. It is probably not\nrequired, however, that all members of a group must believe a\ngiven proposition in order that the group belief be justified, as\nJennifer Lackey rightly says in criticizing the view of Frederick\nSchmitt (1994a: 265). It would make group justification too hard to\ncome by (Lackey 2016: 249–250). But how, more precisely, should\nthe requirement be specified? \nGoldman (1979) advocates process reliabilism as an account of\nindividual justification. He proposes an analogous approach to group\njustification in Goldman (2014). Under individual process reliabilism,\na person is justified in believing a proposition W only if she\nbelieves W through using a generally reliable belief-forming\nprocess (or sequences of processes). In addition, if a person’s\nbelief in W were produced by previous beliefs of hers, those\nother beliefs must also have been justified in order that her belief\nin W qualify as justified. \nAs indicated, Goldman’s approach to collective justifiedness is\nanalogous to individual justifiedness. But how exactly is it\nanalogous? First, members’ premises that contribute to the\njustification of a group’s belief must themselves be justified.\nA further requirement for group justification is that the\ngroup’s belief be caused by a type of belief-forming process\nthat takes inputs from member beliefs in proposition p and\noutputs a group belief in p. The constraint on group\njustifiedness is a requirement that such a process type must be\ngenerally conditionally reliable. An example of such a process type\nmight be a majoritarian process in which member beliefs (of the group)\nare aggregated into a group belief. The process is conditionally\nreliable only if, when it receives true input beliefs, most of the\noutput beliefs are also true. \nLackey (2016), in critiquing Goldman’s theory, offers a number\nof challenging and insightful problems. Their numerosity and detail\nmake it unfeasible to present or even summarize many of them. Let us\nconsider one such objection. \nGoldman advances a certain principle (GJ) which runs as follows: If a\ngroup belief in p is aggregated based on a profile of\nmembers’ attitudes toward p, then ceteris\nparibus the greater the proportion of members who justifiedly\nbelieve p and the smaller the proportion of members who\njustifiedly reject p, the greater the group’s level, or\ngrade, of group justifiedness in believing p (Goldman 2014:\n28). In other words, group justifiedness increases with a greater\npercentage of individual members with justified beliefs. \nLackey acknowledges that (GJ) “is not only intuitively\nplausible, it also is easily supported by applying an aggregative\nframework to group justifiedness” (2016: 361). Nonetheless, she\nproceeds to argue that (GJ) should be rejected. One major theme in her\ncritique is that (GJ) falls prey to a paradox (on contradiction),\nwhich she calls the “Group Justification Paradox.” It\nquestionably allows a group to end up justifiedly believing\nboth a certain relevant proposition and its\nnegation. Lackey hastens to acknowledge that not all contradictions,\nor inconsistencies, are necessarily fatal. For example, the well-known\npreface paradox is a case that generates inconsistent propositions,\nbut it is nevertheless reasonable to have. An author’s apology\nin the preface is epistemically reasonable, given our grounds for\nholding that we all have our own fallibility (2016: 364). However,\nLackey continues, the Group Justification Paradox admits no such\napproval. So the incidence of such contradictions does\nconstitute a challenge to the proposed theory of group justification,\neven if the preface paradox does not present a problem. \nWe return now to the basic form of social epistemology, in which\nindividual cognizers seek information from other individuals. Here we\nare interested not in the concepts of justification or knowledge in\ngeneral, but in particular applications. This might be called\napplied social epistemology. For example, how can one\nidentify other individuals as sources of accurate information? A\nsalient type of case is when one seeks information from an\n“expert,” where the seeker is himself/herself a layperson.\nUnfortunately, experts don’t always agree with one another. If\nyou are offered advice by two different (putative) experts, but the\ntwo advisers disagree, which one should you trust, or believe? \nWhat is meant by an “expert”? For present purposes, let us\nfocus on factual subject matters (rather than matters of taste, for\nexample). By an expert, then, we shall mean someone who—in a\nspecified domain—possesses a greater quantity of (accurate)\ninformation than most other people do. A layperson, by contrast, is\nsomeone who has very little information in the specified domain, and\nwho doesn’t take him/herself to have such information. The\nproblem facing the layperson is how to select, among the disagreeing\nputative experts, the one that is best (Goldman 2001). \nThere are several possible methods by which the layperson might\nproceed. A first possible method is to seek statements or arguments\nfrom the competing (putative) experts to assess which one seems to be\nmore knowledgeable, better-informed, and savvy (in the domain) as\ncompared to his/her rivals. The problem here is: how can a layperson\nmake an accurate assessment of the competing experts’\nperformance? By definition, the layperson is someone who is\nill-informed on the subject. Moreover, experts often use language that\nis technical, arcane, rarified, or esoteric. \nA second possible method is for the layperson to obtain information\nabout the competing experts’ credentials. Where were the\ncompeting experts trained in the domain in question? What were the\ncompeting experts’ records of performance during their\neducation? These are matters that the layperson may be unable to\ndetermine. And even if they are determined, is the layperson in a\nposition to assess their significance? \nA third method that a layperson might consider employing is how many\nother putative experts agree with a specified target expert\non many of the questions in the target domain. If a given expert is an\noutlier on the issue in question, that might be a reason to avoid\nhim/her. Conversely, if a large number of (putative) experts agree\nwith the selected target, doesn’t that give extra weight to the\nlatter’s promise as a sound advisor? Surely, a large number of\nconcurring experts should provide strong support for a selected\nindividual. \nThis is a tricky matter. There are many possible reasons why so many\npeople in a field might agree, and such agreement doesn’t always\nsignal that they are all correct. One possibility is that all the\nforegoing putative experts were trained by one and the same\n“guru”, who was a very persuasive and compelling figure.\n(Using a Bayesian analysis, it becomes clear that identifying an extra\nconcurring believer need not strengthen one’s evidence as\ncompared with a single believer. See Goldman (2001:\n99–102).) \nA fourth possible method is for the layperson to verify, by his/her\nown observation, that a candidate expert was correct in\nprevious cases where he/she offered a view on the topic in question.\nThe problem here is whether a layperson is capable of making such\nobservations or determinations. It is possible in a selected\nrange of cases. A layperson might consult with a broker to decide\nwhether a certain investment is advisable. The broker gives her the\ngo-ahead based on a prediction that the market in question will rise\nin the next three years. The layperson makes the investment, waits\nthree years, and notes that the broker was right: the market did rise\nin those three years. The layperson couldn’t have made this\nprediction on the basis of her own knowledge. But she is able (after\nthree years) to conclude that the putative expert was correct.\nRepeated instances of this kind would strengthen the support. \nWe have now seen some of the problems that face those who develop\nknowledge within a community. In recent years philosophers have turned\nto formal methods to understand some of these social aspects of belief\nand knowledge formation. There are broadly two approaches in this\nvein. The first comes from the field of formal epistemology, which\nmostly uses proof-based methods to consider questions that mostly\noriginate within individual-focused epistemology. Some work in this\nfield, though, considers questions related to, for example, judgment\naggregation and testimony. The second approach, sometimes dubbed\n“formal social epistemology”, stems largely from\nphilosophy of science, where researchers have employed modeling\nmethods to understand the workings of epistemic communities. While\nmuch of this work has been motivated by a desire to understand the\nworkings of science, it is often widely applicable to social aspects\nof belief formation. \nAnother distinction between these traditions is that while formal\nepistemologists tend to focus on questions related to ideal belief\ncreation, such as what constitutes rationality, formal social\nepistemologists have been more interested in explaining real human\nbehavior, and designing good knowledge-creation systems. We will now\nbriefly discuss relevant work from formal epistemology, and then look\nat three topics in formal social epistemology. \nAs mentioned, formal epistemology has mostly focused on issues related\nto individual epistemology. This said, there is a significant portion\nof this literature addressing questions central to social\nepistemology. In particular: how should a group aggregate their\nbeliefs? And: how should Bayesians update on the testimony of\nothers? \nAs we have already seen, in aggregating propositional judgments groups\nof people can reach paradoxical outcomes as a result of majority\nvoting. Let’s change focus, though, from propositional beliefs,\nto a more fine-grained notion of belief. Formal epistemologists more\ncommonly discuss degrees of belief, or “credences”. These\nare numbers between 0 and 1 representing an agent’s degree of\ncertainty in a statement. (For instance, if I think there is a 90%\nchance it is raining, my credence that it is raining is .9.) This\nrepresentation changes the question of judgement aggregation to\nsomething like this: Suppose a group of people hold different\ncredences, what should the group credence be? \nIn formal epistemology, this ends up being very closely related to the\nquestion of how an individual ought to update their credences upon\nlearning the credences of others. If a rational group ought to adopt\nsome aggregated belief, then it might also make sense for an\nindividual in the group to adopt the same belief as a result of\nlearning about the credences of his/her peers. In other words, the\nproblems of judgment aggregation, peer disagreement, and testimony are\nentangled in this literature. (Though see Easwaran et al. (2016) for a\ndiscussion of distinctions between these issues.) We’ll focus\nhere on belief aggregation, though we will comment throughout on these\nother issues. \nIn principle, there are many ways that one can go about aggregating\ncredences or pooling opinions (Genest and Zidek 1986). A simple option\nis to combine opinions by linear pooling—taking a weighted\naverage of credences. This averaging could respect all credences\nequally, or put extra weights on the opinions of, say, recognized\nexperts. This option has some nice properties, such as preserving\nunanimous agreement, and allowing groups to aggregate over different\ntopics independently (DeGroot 1974; Lehrer and Wagner\n 1981).[1] \nIn thinking about ideal knowledge creation though, we might ask how a\nBayesian should update their credences in light of peer\ndisagreement or how a group of Bayesians should aggregate beliefs.\nBayes rule gives the rational way an agent should update a prior\nprobabilistic credence in light of evidence to obtain a posterior\ncredence. In general, if evidence appears that is more likely given\nA than B, a Bayesian will increase their credence that\nA in fact obtains upon observing that evidence. (So if I have a\n.9 credence that it is raining, and someone walks in wearing shorts\nand with perfectly dry hair, my credence in rain should decrease\nbecause my observation is more likely to occur on a sunny day than a\nrainy one.) Why is this approach rational? An individual who does not\nchange credences according to Bayes rule can be Dutch\nbooked—offered a series of bets that they will take, but that\nare guaranteed to lose money. \nA Bayesian will not simply average across beliefs, except under\nparticular assumptions or in special cases (Genest and Zidek 1986;\nBradley 2007; Steele 2012; Russell et al. 2015). And a group that\nengages in linear averaging of this sort can typically be Dutch\nbooked. \nA fully-fledged Bayesian approach to aggregation demands that the\nfinal credence be derived by Bayesian updating in light of the\nopinions held by each group member (Keeney and Raiffa 1993). Notice,\nthis is also what a Bayesian individual should do to update on the\ncredences of others. To do this properly, though, is very\n complicated.[2]\n It requires prior probabilities about what obtains in the world, as\nwell as probabilities about how likely each group member is to develop\ntheir credences in light of what might obtain in world. This will not\nbe practical in real cases. \nInstead, many approaches consider features that are desirable for\nrational aggregation, and then ask which simpler aggregation rules\nsatisfy them. For instance, one thing a rational aggregation method\nshould do (to prevent Dutch booking) is yield the same credence\nregardless of whether information is obtained before or after\naggregating. For instance, if we all have credences about the rain,\nand someone comes in wearing shorts, it should not matter to the final\ngroup output whether 1) they entered and we all updated our credences\n(in a Bayesian way) and then aggregated them, or 2) we aggregated our\ncredences, they entered, and we updated the aggregated credence (in a\nBayesian way). Geometric methods, which take the geometric average of\nprobabilities over worlds, yield this desirable property in\nmany cases (Genest 1984; Dietrich and List\n 2015).[3]\n These methods proceed by multiplying (weighted) credences over worlds\nthat might obtain and then renormalizing them to sum to 1. \nOne thing that geometric averaging does not do, though, is allow for\ncredences over different propositions to be aggregated completely\nindependently from each other. (Recall that this was something List\nand Pettit treated as a desideratum for judgment aggregation.) For\ninstance, our beliefs about the probabilities of hail might influence\nhow we will aggregate our beliefs over the probabilities of rain.\nInstead, a more holistic approach to aggregation is required. This is\nan important lesson for approaches to social epistemology which focus\non individual topics of interest in addressing peer disagreement and\ntestimony (Russell et al. 2015). \nAnother thing that some take to be strange about geometric averaging\nis that it sometimes will aggregate identical credences to a different\ngroup credence. For instance, we might all have credence .7 that it is\nraining, but our group credence might be .9. Easwaran et al. (2016)\nargue, though, that this often makes sense when updating on the\ncredences of others—their confidence should make us\nmore confident (see also Christensen\n 2009).[4] \nThis question of whether aggregated credences can be more extreme than\nindividual ones echoes much earlier work bearing on the question: are\ngroups smart? In 1785, the Marquis de Condorcet wrote an essay proving\nthe following. Suppose a group of individuals form independent beliefs\nabout a topic and they are each more than 50% likely to reach a\ncorrect judgement. If they take a majority vote, the group is more\nlikely to vote correctly the larger it gets (in the limit this\nlikelihood approaches 1). This result, now known as the\n“Condorcet Jury Theorem”, underlies what is sometimes\ncalled the “wisdom of the crowds”: in the right conditions\ncombining the knowledge of many can be very effective. \nIn many cases, though, real groups are prone to epistemic problems\nwhen it comes to combining beliefs. Consider the phenomenon of\ninformation cascades, first identified by Bikhchandani et al. (1992).\nTake a group of agents who almost all have private information that\nNissan stock is better than GM stock. The first agent buys GM stock\nbased on their (minority) private information. The second agent has\ninformation that Nissan is better, but on the basis of this observed\naction updates their belief to think GM is likely better. They also\nbuy GM stock. The third agent now sees that two peers purchased GM and\nlikewise updates their beliefs to prefer GM stock. This sets off a\ncascade of GM buying among observers who, without social information,\nwould have bought Nissan. The problem here is a lack of independence\nin the “vote”—each individual is influenced by the\nbeliefs and actions of the previous individuals in a way that obscures\nthe presence of private information. In updating on the credences of\nothers, we thus may need to be careful to take into account that they\nmight already have updated on the credences of others. \nAs noted, much of the work in formal social epistemology is by\nphilosophers of science, who investigate scientific communities in\nparticular. It will be useful to divide this literature into three\ncategories: credit economy models of science, network epistemology\nmodels, and modeling approaches to diversity in epistemic\ncommunities. \nSuppose you are a scientist choosing what to work on. There are two\nlive options. One is more promising, and you suspect that if an\nadvancement will be made, it will be on that problem. The other is\nless promising, but, as a result, fewer scientists are attracted to\nit. This means that should a discovery be made in that area, each\nscientist will be more likely to have made it. \nThe subfield of formal social epistemology arguably started with\nPhilip Kitcher’s 1990 paper “The Division of Cognitive\nLabor”, which takes a rationality-based approach to the question\nof why scientists might divide labor effectively, even when they agree\non which problems are most promising. By rationality-based approach,\nwe mean that Kitcher represents scientists as utility-maximizers, in\nmuch the same way that economists represent people as\nutility-maximizers. The key innovation, though, is that scientists are\nassumed to derive utility from credit—a proxy for\nrecognition and approbation of one’s scientific work, along with\nall the benefits (promotions, grants, etc.) that follow. The\nsociologist Robert Merton was one of the first to recognize the credit\nmotives of scientists (Merton 1973). Models using this assumption are\noften called credit economy models. \nKitcher’s model shows that scientists in the sort of scenario\nsketched above will divide labor more effectively when they are\nmotivated by credit than when they are pure truth-seekers. Truth\nseekers will each take the more promising approach. For\ncredit-maximizers, once too many individuals work on the better\nproblem, the expected credit for each scientist decreases to the point\nthat some individuals will prefer to switch. Strevens (2003) extends\nKitcher’s work by arguing that an existing feature of credit\nincentives, the priority rule, can lead to an even better division of\nlabor. This is the rule, identified by Merton, which stipulates that\ncredit will be allocated only to the scientist who first makes a\ndiscovery. In Strevens’s model, researchers incentivized by the\npriority rule divide labor in a highly efficient way compared to\nresearchers incentivized by other credit schemes. \nNot all researchers are as optimistic about the consequences of\nscientific credit norms like the priority rule. Romero (2017) points\nout, for example, that the benefits Kitcher and Strevens identify\ndisappear in cases where results are not always replicable. (I.e.,\nwhere, as in real scientific communities, any particular study could\ngive a misleading result, and thus findings require replications\nbefore they can be considered “truth”.) In particular, the\npriority rule strongly disincentivizes scientists from performing\nreplications because credit is so strongly associated with new,\npositive findings. \nThis debate reflects a deep question, going back as far as Du Bois\n(1898), and at the heart of much of the work in this section: what is\nthe best motive for an epistemic community? Is it credit seeking or\n“pure” truth seeking? Or some combination of the two? \nThis sort of tension over the benefits and detriments of credit\npractices also plays out with respect to debates about scientific\nfraud. Starting with Merton many have argued that the desire to claim\npriority, and thus credit, drives\n fraud.[5]\n These arguments seem to suggest that scientists who ignore credit,\nand instead attempt to yield truth will do a better job. Bright\n(2017a), though, uses a model to point out that credit-seekers who\nfear retaliation may publish more accurate results than truth-seekers\nwho are convinced of some fact, despite their experimental results to\nthe contrary. In other words, a true believer may be just as\nincentivized to commit fraud as someone who simply seeks approval from\ntheir community. \nWe see this debate again over the Matthew effect, identified by Merton\n(1968). As Merton points out, pre-eminent scholars often get more\ncredit for work than a less famous scholar would have gotten. Strevens\n(2006) argues that this follows the scientific norm to reward credit\nbased on the benefit a discovery yields to science and society.\nBecause famous scientists are more trusted, their discoveries do more\ngood. Strevens claims that this norm of credit thus improves discovery\nin a community. Heesen (2017), on the other hand, uses a credit\neconomy model to show how someone who gets credit early on due to luck\nmay later accrue more and more credit because of the Matthew effect.\nWhen this kind of compounding luck happens, he argues, the resulting\nstratification of credit in a scientific community does not improve\ninquiry. \nAs we have seen, credit economy models help answer questions like:\nwhat is the best credit structure for an epistemic community? How do\nwe promote truth? And: what credit incentives should be\n avoided?[6]\n Once we accept that epistemic communities are more than the sum of\ntheir individual parts, it is crucial to probe the incentive\nstructures that members of these communities face in thinking about\nhow best to shape them. As credit economy models show us, designing\ngood epistemic communities is by no means a trivial task. \nAnother paradigm widely used by philosophers to explore social aspects\nof epistemology is the epistemic network. This kind of model uses\nnetworks to explicitly represent social or informational ties where\nbeliefs, evidence, and testimony can be shared. \nThere are different ways to do this. In the social sciences generally,\nthe most popular approach takes a “diffusion” or\n“contagion” view of beliefs. A belief or idea is\ntransmitted from individual to individual across their network\nconnections, much like a virus can be transmitted (Rogers 1962).\nImagine you live in a farming community where a new species of large\ncaterpillar has started decimating crops. Suppose you come to believe\nthat this kind of caterpillar is poisonous. You will immediately tell\nyour friends and neighbors what you have learned, they will tell their\nfriends and neighbors, and so on. Figure 1 shows this. Black nodes\nrepresent those “infected” with an idea. It starts with a\nfocal individual and spreads, virally, through the community. \nFigure 1: A network contagion model\nwhere a focal individual infects others with a belief. [An\n extended description of figure 1\n is in the supplement.]  \nIn these diffusion/contagion models, though, the individuals do not\ngather evidence from the world, share evidence with each other, or\nform beliefs in any sort of rational way. For this reason,\nphilosophers of science have tended to use the network\nepistemology framework instead. This framework was introduced by\neconomists Bala and Goyal (1998) to model how individuals learn from\nneighbors. It was imported to philosophy by Kevin Zollman, who first\nused it to represent scientific communities (Zollman 2007, 2010). \nNow imagine the same caterpillar example, but where the individuals\ninvolved form evidence-based beliefs. One becomes suspicious that the\ncaterpillar is poisonous, and tests to see if this is true. She shares\nthe evidence she gathered (not just her belief) with those she is\nconnected to. Her neighbors, on the basis of this evidence, themselves\nbecome more suspicious that the caterpillar is dangerous, and test for\nthemselves. They, in turn, share the evidence they gather with their\nneighbors. Beliefs can still spread through a network, but now they do\nso on the basis of at least semi-rational belief-forming\nmechanisms. \nIn more detail: network epistemology models start with a collection of\nagents on a network, who choose from some set of options. One option\nis preferable to the rest, but to find out which this is, the agents\nmust actually try them and see what results. These could represent\nchoices of action-guiding theories (like “caterpillars are\nsafe” and “caterpillars are poisonous” or else\n“vaccines are safe” and “vaccines cause\nautism”). They could alternatively represent research approaches\nthat yield different levels of scientific success. \nAgents have beliefs about which option is preferable, and change these\nbeliefs in light of the evidence they gather from their actions. In\naddition, they also update on evidence gathered by neighbors in the\nnetwork, typically using some version of Bayes’ rule. It is in\nthis sense that agents are part of an epistemic community. Figure 2\nshows what this might look like. The numbers next to each agent\nrepresent their degree of belief in some proposition like\n“vaccines are safe”. The black agents think this is more\nlikely than not. As this model progresses these agents gather data,\nwhich increases their neighbors’ degrees of belief in turn. \nFigure 2: Agents in a network\nepistemology model use their credences to guide theory testing. Their\nresults change their credences, and those of their neighbors. [An\n extended description of figure 2\n is in the supplement.]  \nCommunities in this model can develop beliefs that the better theory\n(vaccines are safe) is indeed better, or else they can pre-emptively\nsettle on the worse theory (vaccines cause autism) as a result of\nmisleading evidence. Generally, since networks of agents are sensitive\nto the evidence they gather, they are more likely to figure out the\n“truth” of which is best (Zollman 2013; Rosenstock et al.\n2017). \nZollman (2007, 2010) describes what has now been dubbed the\n“Zollman effect” in these models; the surprising\nobservation that it is sometimes worse for communities to communicate\nmore (see also, Grim 2009). In particular, groups with more network\nconnections will be generically less likely to arrive at a correct\nconsensus. The group needs to entertain all the possible options long\nenough to gather good evidence and settle on the best one. In tightly\nconnected networks, misleading evidence is widely shared, and may\ncause the community to pre-emptively settle on a poor theory. \nMayo-Wilson et al. (2011, 2013) likewise defend a surprising thesis,\nwhich supports central claims from social epistemology espoused by\nGoldman (1999). They use epistemic network models, to support the so\ncalled “independence thesis”—that rational groups\nmay be composed of irrational individuals, and rational individuals\nmay constitute irrational groups. For instance, consider a learner who\ntests some preferred theory. Alone, she may fail to test other\nsuccessful theories, but a community representing a full diversity of\npreferred theories will be expected to learn which is best. \nOne thing we know about human learners is that they have various\ncognitive and social biases that influence how they take up\ninformation from peers. One of these is conformity bias, or a tendency\nto espouse the views of group members, even if one secretly disagrees\nwith them (Asch 1951). Weatherall and O’Connor (2018, Other\nInternet Resources) show how conformity can prevent the adoption of\nsuccessful beliefs because agents who conform to their neighbors are\noften unwilling to pass on good information that goes against the\ngrain. Mohseni and Williams (2019, Other Internet Resources) \nsimilarly find that conformity\nslows learning, likewise because it prevents agents from sharing\ninformation, and because group members who expect this are less\ntrusting of their \npeers.[7] \nSeveral authors have used variants on the epistemic network model to\nexplore the phenomenon of “polarization” within\n groups.[8]\n Both Olsson (2013) and O’Connor and Weatherall (2018) consider\nversions of the model where actors place less trust in the evidence\n(or testimony) of those who do not share their beliefs. A vaccine\nskeptic, for instance, might be skeptical of evidence shared by a\nphysician, but accepting of evidence from a fellow skeptic. This can\nlead to stable, polarized camps that each ignore evidence and\ntestimony coming from the other\n camp.[9]\n The two sets of results just described help answer the question: in\nlight of real social and learning biases, what can go wrong? And: how\ncan we shape good epistemic networks to counteract these biases?\n(These questions tie into how we should understand democracy in light\nof social epistemology.) \nOne of the most interesting recent uses of the network epistemology\nframework involves investigating the role of pernicious influencers,\nespecially from industry, on epistemic communities. Holman and Bruner\n(2015) look at a network model where one agent shares only fraudulent\nevidence meant to support an inferior theory. As they show, this agent\ncan keep a network from reaching successful consensus by muddying the\nwater with misleading data. Holman and Bruner (2017) and Weatherall et\nal. (forthcoming) use network epistemology models to explore specific\nstrategies that industry has used to influence scientific research. As\nHolman and Bruner show, industry can shape the output of a community\nthrough “industrial selection”—funding only agents\nwhose methods bias them towards preferred findings. Weatherall et al.\nadd a group of “policy makers” to the model, to show how a\npropagandist can mislead these public agents simply by sharing a\nbiased sample of the real results produced in an epistemic network.\nFor example, Big Tobacco might gather up real, independent studies\nthat happen to find no link between smoking and cancer, and share\nthese widely (Oreskes and Conway 2011). Together these two papers give\ninsight into how strategies that do not involve fraud can shape\nscientific research and mislead the public. \nOne truth about epistemic communities is that relationships matter.\nThese are the ties that ground testimony, disagreement, and trust.\nEpistemic network models allow philosophers to explore processes of\ninfluence in social networks, yield insights into why social ties\nmatter to the way communities form beliefs, and think about how to\ncreate better knowledge systems. \nDiversity has emerged several times in our discussion of formal social\nepistemology. Credit incentives can encourage scientists to choose a\ndiversity of problems. In network models, a transient diversity of\nbeliefs is necessary for good inquiry. Let us now turn to models that\ntackle the influence of diversity more\n explicitly.[10]\n It has been suggested that cognitive diversity benefits epistemic\ncommunities because a group where members start with different\nassumptions, use different methodologies, or reason in different ways\nmay be more likely to find truth. \nWeisberg and Muldoon (2009) introduce a model where actors investigate\nan “epistemic landscape”—a grid where each section\nrepresents a problem in science, of varying epistemic importance.\nFigure 3 shows an example of such a landscape. Scientists are randomly\nscattered on the landscape, and follow search rules that are sensitive\nto this importance. Investigators can then ask: how well did\nscientists do? Did they fully search the landscape? Did they find the\npeaks? \nFigure 3: An epistemic landscape.\nLocation represents problem choice, and height represents epistemic\nsignificance. \nWeisberg and Muldoon use the model to argue that a combination of\n“followers” (scientists who work on problems similar to\nother scientists) and “mavericks” (who prefer to explore\nnew terrain) do better than either group alone; i.e., there is a\nbenefit to cognitive diversity. Their modeling choices and main result\nhave been convincingly criticized (Alexander et al. 2015; Thoma 2015; Poyhönen\n2017; Fernández Pinto and Fernández Pinto 2018), but the\nframework has been co-opted by other philosophers to useful ends.\nThoma (2015) and Poyhönen (2017), for instance, show that in\nmodified versions of the model, cognitive diversity indeed provides\nthe sort of benefit Weisberg and Muldoon\n hypothesize.[11] \nHong and Page (2004) use a simple model to derive their famous\n“Diversity Trumps Ability” result. Agents face a problem\nmodeled as a ring with some number of locations on it. Each location\nis associated with a number representing its goodness as a solution.\nAn agent, in the model, is represented as a finite set of\n“heuristics”, or integers, such as 〈3, 7, 10〉.\nSuch an agent is placed on the ring, and can see the locations 3, 7,\nand 10 spots ahead of their current position. They then move to\nwhichever has the highest number until they reach a location where\nthey can no longer improve their score. \nThe central result is that randomly selected groups of agents\nwho tackle the task together tend to outperform groups created of top\nperformers. This is because the top performers have similar\nheuristics, and thus gain relatively little from group membership,\nwhereas random agents have a greater variety of heuristics. This\nresult has been widely cited, though there have been criticisms of the\nmodel either as insufficient to show something so complicated, as\nlacking crucial representational features, or as failing to show what\nit claims (Thompson 2014; Singer 2019). \nTo this point we have addressed cognitive diversity. But we might also\nbe interested in diversity of social identity in epistemic\ncommunities. Social diversity is an important source of cognitive\ndiversity, and for this reason can benefit the functioning of\nepistemic groups. For instance, different life histories and\nexperiences may lead individuals to hold different assumptions and\ntackle different research programs (Haraway 1989; Longino 1990;\nHarding 1991; Hong and Page 2004). If so, then we may want to know:\nwhy are some groups of people often excluded from epistemic\ncommunities like those in academia? And what might we do about\nthis? \nIn recent work, scholars have used models of bargaining to represent\nacademic collaboration. They have shown 1) how the emergence of\nbargaining norms across social identity groups can lead to\ndiscrimination with respect to credit sharing in collaboration (Bruner\nand O’Connor 2017; O’Connor and Bruner 2019) and 2) why\nthis may lead some groups to avoid academia, or else cluster in\ncertain subfields (Rubin and O’Connor 2018). In the\ncredit-economy tradition, Bright (2017b) explains why a noted\nphenomenon—that women tend to publish fewer papers than\nmen—may not indicate a gap in quality of research. As he points\nout, anticipation of rejection may lead women to overshoot by\nproducing papers of higher quality than necessary for publication.\nThis gap contributes to the underrepresentation of women in some\ndisciplines. \nAs we have seen in this section, models can help explain how and when\ncognitive diversity might matter to the production of knowledge by a\ncommunity. They can also tell us something about why epistemic\ncommunities often, nonetheless, fail to be diverse with respect to\nsocial identity. \nLet us now move on to see how topics from social epistemology\nintersect with important questions about the proper functioning of\ndemocratic societies, and questions about the ethics of social\nknowledge and learning. \nIn our portrayal of social epistemology thus far, several different\nmosaics have been sketched. In some cases, a single epistemic agent\nseeks epistemic help from another agent. In other cases, a collective\nagent seeks answers to questions using its members in a collaborative\nfashion. A third kind of case, which arose especially in the last\nsection, is what we shall call a “system-oriented” or\n“institution-oriented” application of social\nepistemology. \nBy a “system” we mean some entity with a multiplicity of\nworking “parts” and multiple goals that the system aims to\nachieve. A question that arises, quite frequently, is how best to\ndesign a system that will maximize the attainment, or satisfaction, of\nits (most important) goals over time. As we have seen, this is a\nquestion that philosophers have attempted to answer with respect to\nthe structure of scientific communities. Another good example of such\nsystems are political systems, especially democratic political\nsystems. There are many current democratic theorists who place much\nemphasis on the epistemological, or epistemic, properties of\ndemocratic institutions. \nElizabeth Anderson (2006) focuses on the question of how the epistemic\nproperties of democratic systems can be designed to attain the best\npossible form of democracy. She provides three epistemic models of\ndemocracy: the Condorcet Jury Theorem, the Diversity Trumps Ability\nresult, and John Dewey’s experimentalism (see Landemore 2011).\nAnderson plumps for Dewey’s experimentalist approach. He\nhighlights the importance of bringing together citizens from different\nwalks of life to define, through discussion, the principal problems\nthey confront and what might be the most promising solutions. Their\ndifferent walks of life constitute, in effect, a range of experiments\nthat can help them collectively appraise alternative solutions, thus\ntaking advantage of cognitive diversity. Anderson provides a\nconvincing illustration of Dewey’s thesis by relating how women\nin a South Asian village were able to manage their forests better when\nthey were given opportunities to make fuller use of their\n“situated knowledge” (see Agarwal 2000). \nWhen we contemplate the meaning of democracy, we often mean (as a\nstarting point, anyway) a governmental system that features equal\nvoting rights for all citizens. A little reflection, however, readily\nindicates how shallow a role is played by mere voting rights. A\ncitizen may be entitled to cast a vote for any of the candidates on\nthe ballot. But this will not help the voter promote positive results\n(positive by her lights) if she has misguided views of what specific\ncandidates for office would do if they were actually elected, or about\nwhat policy measures will be effective (for details, see Goldman 1999:\n315–348). \nThis raises the question of just how informed or misinformed ordinary\nvoters are, and what prospects there are for improving the present\nsituation. Many political scientists have shown that American voters\nare strikingly uninformed with respect to textbook facts about their\ngovernment. Nonetheless, there are some rays of light. Several books\nargue that ordinary citizens can make sense of their political world\ndespite a lack of detailed information about policies and candidates\n(see Berelson, Lazarsfeld, & McPhee 1954, and Katz &\nLazarsfeld 1955; cited in Goldman 1999). One semi-optimistic idea is\nthat of a two-step flow of communication from well-informed\n“opinion leaders” to the public at large. This approach\nsuggests that ordinary citizens—even those who pay little\nattention to the details of politics—can learn what they need to\nknow to make suitable choices by listening to the opinions of experts\nor news junkies. Cues and informational “shortcuts” are\navailable that can lead them to the same answers that more informed\ncitizens arrive at (cf. Goldman 1999: 318). \nIf this account is correct, it suggests that a wide range of citizens\ncan make fairly “accurate” voting decisions if two\nconditions hold: (1) there are political experts whose knowledge\nenables them to pinpoint who would be good electoral choices relative\nto specifiable citizens; and (2) these initially less-informed\ncitizens are capable of identifying who are (some of) the genuine\nexperts and who are not. An ability to recognize genuine expertise in\npolitical matters can play a significant role in promoting the kind of\ndemocratic success described above. As noted, though, it can be\ndifficult for laypeople to decide which experts to trust. And as we\nsaw in the last section, in our discussion of industrial influence on\npublic belief, there are forces that work to undermine the functioning\nof democracy, and that often bring us away from this more optimistic\npicture. In the next section we will briefly discuss some related\nissues. \nThe latest challenge confronting the informational state of the public\nis the accelerating spread of misinformation and disinformation on the\ninternet. On Twitter falsehoods spread further and faster than the\ntruth (Temming 2018a,b). In the run-up to the 2016 U.S. presidential\nelection, the most popular bogus articles got more Facebook shares,\nreactions, and comments than the top real-news stories, according to a\nBuzzFeed News analysis. And, as many have documented, online\nmisinformation and disinformation in a wide variety of forms have\ncreated serious issues vis-à-vis public belief and\ndemocratic functioning. \nIn trying to tackle the spread of misinformation, many online\nplatforms have implemented algorithms. For instance, in response to\n“fake news”, programmers have built automated systems that\naim to judge the veracity of online stories. Researchers explore which\nfeatures of an article are the most reliable identifiers of fake news\n(Temming 2018b: 24). Clearly, these sorts of tools have some promise\nas part of the enterprise of social epistemology. But their power to\ndiscriminate true stories from false ones still has limited\nreliability. Furthermore, as O’Connor and Weatherall (2019)\npoint out (drawing on the work of Holman (2015) who looks at arms\nraces between pharmaceutical companies and regulators) online\nmisinformation constitutes a kind of arms race. As platforms and\nprogrammers and governments develop tools to fight it, the purveyors\nof misinformation (the Russian state, various partisan groups,\nadvertisers, trolls, etc.) will develop new methods of shaping public\nbelief. \nAll ill-informed populace, as noted, may not be able to effectively\nrepresent their interests in a democratic society. In order to protect\ndemocratic functioning, going forward it will be necessary for those\nfighting online misinformation to keep adapting with the best tools\nand theory available to them. This includes understanding social\naspects of knowledge and belief formation. In other words, social\nepistemology has much to say to those faced with the challenging task\nof protecting democracy from misinformation. \nSome recent writers seek to expand the notion of social epistemology\nby incorporating moral or ethical elements. Miranda Fricker (2007) in\nparticular has made significant contributions to this literature.\nFricker introduces the notion of “epistemic injustice,”\nwhich arises when somebody is wronged in their capacity as a knower.\nAn easily recognizable form of such injustice is when a person or a\nsocial group is unfairly deprived of knowledge because of their lack\nof adequate access to education or other epistemic resources.\nFricker’s work also focuses on two less obvious forms of\nepistemic injustice. The first is testimonial injustice,\nwhich occurs when a speaker is given less credibility than she\ndeserves because the hearer has prejudices about a social group to\nwhich the speaker belongs. The second kind is hermeneutical\ninjustice. This occurs when, as a result of a group being\nsocially powerless, members of the group lack the conceptual resources\nto make sense of certain distinctive social experiences. For instance,\nbefore the 1970s, victims of sexual harassment had trouble\nunderstanding and describing the behavior of which they were the\nvictims, because the concept had not yet been articulated. Christopher\nHookway (2010) builds on Fricker’s work and argues that there\nare other forms of epistemic injustice that do not involve testimony\nor conceptual resources \nThese issues are relevant epistemological ones for those in democratic\nsocieties. Epistemic injustices may leave some members of society\nill-equipped to engage in the debates that fuel a well-functioning\ndemocracy. Testimonial injustice may prevent the spread of important\ninformation and perspectives through a community. \nAs we saw in the last section, misinformation can also pose epistemic\nthreats to democratic functioning. With respect to internet\nmisinformation, we might ask: do we have a right to protection against\nsuch misinformation? Is it morally acceptable, or even morally\nmandatory, for internet platforms, or government bodies, to protect\npublic belief by regulating and limiting misinformation? There are\ndeep political and moral issues here that we cannot possibly cover in\nthis entry. But we will note a fundamental tension that is relevant.\nFree speech is protected in most democratic societies, but part of the\ndefense of free speech by thinkers like Mill (1859 [1966]) is that it\nis crucial for freedom of thought. Once we recognize that human\nbeliefs are deeply social, and do not always follow Descartes’\nmodel of the individual, fully rational reasoner, we might acknowledge\nthat some sorts of speech interfere with our freedom of thought, and\nin some cases we may need to decide to protect one in lieu of the\nother.","contact.mail":"goldman@philosophy.rutgers.edu","contact.domain":"philosophy.rutgers.edu"},{"date.published":"2001-02-26","date.changed":"2019-08-28","url":"https://plato.stanford.edu/entries/epistemology-social/","author1":"Alvin Goldman","author2":"Cailin O'Connor","author1.info":"http://philosophy.rutgers.edu/index.php?option=com_content&task=view&id=102&Itemid=210","entry":"epistemology-social","body.text":"\n\n\nUntil recently, epistemology—the study of knowledge and\njustified belief—was heavily individualistic in focus. The\nemphasis was on evaluating doxastic attitudes (beliefs and disbeliefs)\nof individuals in abstraction from their social environment. Social\nepistemology seeks to redress this imbalance by investigating the\nepistemic effects of social interactions and social systems. After\ngiving an introduction, and reviewing the history of the field in\nsections 1 and 2, we move on to discuss central topics in social\nepistemology in section 3. These include testimony, peer disagreement,\nand judgment aggregation, among others. Section 4 turns to recent\napproaches which have used formal methods to address core topics in\nsocial epistemology, as well as wider questions about the functioning\nof epistemic communities like those in science. In section 5 we\nbriefly turn to questions related to social epistemology and the\nproper functioning of democratic societies. \n\nWhat do we mean by the phrase “social epistemology”, the\ntopic covered in this entry? \nSocial epistemology gets its distinctive character by standing in\ncontrast with what might be dubbed “individual”\nepistemology. Epistemology in general is concerned with how people\nshould go about the business of trying to determine what is true, or\nwhat are the facts of the matter, on selected topics. In the\ncase of individual epistemology, the person or agent in question who\nseeks the truth is a single individual who undertakes the task all by\nhimself/herself, without consulting others. By contrast\nsocial epistemology is, in the first instance, an enterprise\nconcerned with how people can best pursue the truth (whichever truth\nis in question) with the help of, or in the face of,\nothers. It is also concerned with truth acquisition by groups, or\ncollective agents.  \nAccording to the most influential tradition in (Western) epistemology,\nillustrated vividly by René Descartes (1637), standard\nepistemology has taken the form of individual epistemology, in which\nthe object of study is how epistemic agents, using their personal\ncognitive devices, can soundly investigate assorted questions.\nDescartes contended that the most promising way to pursue truth is by\none’s own reasoning. The remaining question was how, exactly,\ntruth was to be found by suitable individualistic maneuvers, starting\nfrom one’s own introspected mental contents. Another major\nfigure in the history of the field was John Locke (1690), who insisted\nthat knowledge be acquired through intellectual self-reliance. As he\nput it, “other men’s opinions floating in one’s\nbrain” do not constitute genuine knowledge.  \nIn contrast with the individualistic orientations of Descartes and\nLocke, social epistemology proceeds on the commonsensical idea that\ninformation can often be acquired from others. To be sure, this step\ncannot be taken unless the primary investigator has already determined\nthat there are such people, a determination that presumably\nrequires the use of individual resources (hearing, seeing, language,\netc.) Social epistemology should thus not be understood as a wholly\ndistinct and independent form of epistemology, but one that rests on\nindividual epistemology.  \nSurprisingly, social epistemology does not have a very long, or rich,\nhistory. With perhaps a few exceptions, it has not been explored by\nphilosophy with much systematicity until recent times. The case of\nmodern science, by contrast, was rather different. The Royal Society of London was created\nin 1660, intended to highlight the importance of multiple observers in\nestablishing recognized facts. (And, as we will see throughout the\nentry, there are important connections between philosophy of science\nand social epistemology.) But the leading figures of philosophy\ncommonly tended to present themselves as solitary investigators. To be\nsure, they did not refrain from debating with one another. But insofar\nas the topic was “epistemology” (as it came to be called),\nit centered on the challenges and practices of individual agents. \nA movement somewhat analogous to social epistemology was developed in\nthe middle part of the 20th century, in which sociologists and\ndeconstructionists set out to debunk orthodox epistemology, sometimes\nchallenging the very possibility of truth, rationality, factuality,\nand/or other presumed desiderata of mainstream epistemology. Members\nof the “strong program” in the sociology of science, such\nas Bruno Latour and Steve Woolgar (1986), challenged the notions of\nobjective truth and factuality, arguing that so-called\n“facts” are not discovered or revealed by science, but\ninstead “constructed”, “constituted”, or\n“fabricated”. “There is no object beyond\ndiscourse,” they wrote. “The organization of discourse\nis the object” (1986: 73). \nA similar version of postmodernism was offered by the philosopher\nRichard Rorty (1979). Rorty rejected the traditional conception of\nknowledge as “accuracy of representation” and sought to\nreplace it with a notion of “social justification of\nbelief”. As he expressed it, there is no such thing as a\nclassical “objective truth”. The closest thing to (so\ncalled) truth is merely the practice of “keeping the\nconversation going” (1979: 377).  \nOther forms of deconstruction were also inspired by social factors but\nwere less extreme in embracing anti-objectivist conclusions about\nscience. Thomas Kuhn (1962/1970) held that purely objective\nconsiderations could never settle disputes between competing theories;\nhence scientific beliefs must be influenced by social factors.\nAnalogously, Michel Foucault developed a radically political view of\nknowledge and science, arguing that practices of so-called\nknowledge-seeking are driven by quests for power and social domination\n(1969 [1972], 1975 [1977]). \nDebates about these topics persisted under the heading of “the\nscience wars”. Within the mainstreams of both science and\nphilosophy, however, the foregoing views have generally been rejected\nas implausibly radical. This did not mean that no lessons were learned\nabout the status of social factors in science and philosophy. These\ndebates gave important insight into the role of cultural beliefs and\nbiases in the creation of knowledge. What we shall pursue, however, in\nthe remainder of this entry is how social epistemology has created a\nnew branch, or sphere, of mainstream epistemology. According to this\n“extension” of epistemology, social factors can and do\nmake major contributions to traditional, truth-oriented epistemology\nby introducing, broadening, and refining new problems, new techniques,\nand new methodologies. But such factors do not undercut the very\nnotions of truth and falsity, knowledge and error. \nSharply departing from the debunking themes sketched above, modern\nsocial epistemology is prepared to advance proposals quite continuous\nwith traditional epistemology. It sees no reason to think that social\nfactors or practices inevitably interfere with, or pose threats to,\nthe attainment of truth and/or other epistemic desiderata, such as\njustified belief, rational belief, etc. There may indeed be\nidentifiable cases (which we shall explore) in which specific types of\nsocial factors or social interactions pose threats to truth\nacquisition. But, conversely, the right kinds of social organization\nmay enhance the prospects of truth acquisition. \nA general overview of these different kinds of cases was advanced in a\nwide-ranging monograph by Alvin Goldman: Knowledge in a Social\nWorld (1999). This book emerged from several earlier papers\ncritiquing postmodernist attacks on truth and the prospects of truth\nacquisition. They included papers on argumentation (Goldman 1994),\nfreedom of speech (Goldman and Cox 1996), and scientific inquiry\n(Goldman 1987). Other contributions with broadly similar orientations\nincluded C.A.J. Coady’s Testimony (1992), Edward\nCraig’s Knowledge and the State of Nature (1990), and\nPhilip Kitcher’s “The Division of Cognitive Labor”\n(1990) and The Advancement of Science (1993). Margaret\nGilbert’s monograph On Social Facts (1989) made a\nforceful case for the existence of “plural subjects”, a\ncrucial metaphysical component for social epistemology. The journal\nEpisteme: A Journal of Social Epistemology (Goldman, editor),\nwas begun in 2004 and played a major role in the positive development\nof the field. (A different journal with a similar title, Social\nEpistemology (Steve Fuller, editor) began somewhat earlier in\n1988, but tilted heavily toward a debunking orientation). \nIn exploring social epistemology we explore how assorted\nsocial-epistemic activities or practices have an impact on the\nepistemic outcomes of the agents (or groups) in question. Which\nchanges in social-epistemic practices are likely to promote, enhance,\nor impede epistemic outcomes? To put more flesh on the varieties of\nsocial-epistemic methods and outcomes, let us look at some core\ntopics.  \nThe first kind of social-epistemic scenario is very common. An\nindividual seeks to determine the truth-value of proposition p\nby soliciting the opinion(s) of others. She might direct her question\nto one of her personal confidants, or consult what is in print or\navailable online. Having received responses to her queries, she weighs\nwhat has been said to help her assess the truth of the proposition in\nquestion. This is commonly referred to as\n“testimony-based” belief. The selected informant may still\nbe a single individual. But appealing to another individual for\ntestimony already locates the example as within the domain of social\nepistemology. \nThe main discussion here is framed in terms of justification rather\nthan knowledge. The standard question is: Under what circumstances is\na hearer justified in trusting an assertion made by a stranger, or by\na consultant or speaker of any variety? David Hume argued that we are\ngenerally entitled to trust what others tell us; but this entitlement\nonly arises by virtue of what we previously learned from others. Each\nof us can recall occasions on which we were told things that we could\nnot independently verify (from perception, e.g.) but later determined\nto be true. This reliable track-record from the past (which we\nremember) warrants us in inferring (via induction) that testimony is\ngenerally reliable. As James Van Cleve formulates the view: \nTestimony gives us justified belief … not because it shines by\nits own light, but because it has often enough been revealed true by\nour other lights. (Van Cleve 2006: 69) \nThis sort of view is called “reductionism” (about\ntestimony) because it “reduces” the\njustification-conferring force of testimony to the combined forces of\nperception, memory, and inductive inference. More precisely, the view\nis usually called global reductionism, because it argues that\nhearers of testimony are justified in believing particular instances\nof testimony by inferential appeal to testimony’s general\nreliability. \n[It] seems absurd to suggest that, individually, we have done anything\nlike the amount of field-work that [reductionism] requires …\n[M]any of us have never seen a baby born, nor have many of us examined\nthe circulation of the blood nor the actual geography of the world\n… nor a vast number of other observations that [reductionism]\nwould seem to require. (Coady 1992: 82) \nAn alternative to global reductionism is local reductionism\n(E. Fricker 1994). Local reductionism does not require hearers to be\njustified in believing the general reliability of testimony.\nIt only requires hearers to be justified in trusting the reliability\nof the specific speakers whose current testimony is in question on a\nparticular subject. This requirement is more easily satisfied than\nglobal reductionism. Local reductionism may still be too strong,\nhowever, but for a different reason. Is a speaker S trustworthy\nfor hearer H only if H has positive evidence or\njustification for S’s general reliability? This is far\nfrom clear. If I am at an airport or train station and hear a public\nannouncement of the departure gate (or track), am I justified in\nbelieving this testimony only if I have prior evidence of the\nannouncer’s general reliability? Normally I do not possess such\nevidence for a given public address announcer. But, surely, I am\njustified in trusting such announcements. \nGiven these problems for both kinds of reductionism, some\nepistemologists embrace testimonial anti-reductionism (Coady\n1992; Burge 1993; Foley 1994; Lackey 2008). Anti-reductionism holds\nthat testimony is itself a basic source of evidence or justifiedness.\nNo matter how little positive evidence a hearer has about the\nreliability and sincerity of a given speaker, or of speakers in\ngeneral, she has default or prima facie warrant in believing\nwhat the speaker says. Thus Tyler Burge writes: \n[A] person is entitled to accept as true something that is presented\nas true and that is intelligible to him, unless there are stronger\nreasons not to do so. (Burge 1993: 457)  \nIn the preceding example, it was tacitly assumed that the hearer had\nno prior belief (one way or the other) about the topic in question.\nHis/her mind was completely open prior to receiving the testimony of\nthe speaker. Now let us consider a different class of examples\n(modified from one given by David Christensen [2007]). Suppose that\ntwo people start out with opposing views on a given topic; one of them\nbelieves proposition p and the other believes not-p. To\nadd a little color, consider an example in which two friends, Harry\nand Mary, have each acquired some evidence about a traffic accident\ndescribed in the morning newspaper. Neither has any other evidence\nabout the incident. For example, neither has any background knowledge\nor information about the people said to be involved in the incident.\nHowever, having read the newspaper account, both Harry and Mary form\nbeliefs about it. Harry believes that Jones (described in the\nnewspaper) was responsible for the accident. Mary believes that Jones\nwas not responsible. Harry and Mary now run into each other, find out\nthat their friend has read the same story, and discuss their views\nabout the accident. To make the example even more interesting, suppose\nthe two friends have as much respect for the other person’s\njudgment (in such matters) as they do for themselves. This might be\nbased on the fact that when they have disagreed with one another in\nthe past, each turned out to be right about 50% of the time.  \nIn such a situation, we shall call the two people epistemic peers.\nEpistemologists have adopted the label “peer disagreement”\nfor the problem generated by such cases (actual or hypothetical). The\nproblem is: how, if at all, should an agent adjust her initial belief\nabout the specified proposition upon learning that her peer holds a\ncontrary position? Should she (always?) modify her belief (or strength\nof belief) in the direction of the peer? Or is it sometimes\nepistemically permissible to hold “steadfast” to\none’s own original conviction? \n“Conciliationism” is the view that (some degree of)\nmodification is always called for, because it requires epistemic\nagents in the specified type of situation to make some obeisance to\nthe belief of their peer, rather than ignoring or dismissing it\nentirely. Proponents of conciliationism include Christensen (2007),\nRichard Feldman (2007), and Adam Elga (2007). \nFeldman (2007) offers an abstract argument for conciliationism based\non what he calls the “uniqueness thesis”. This is the view\nthat for any proposition p and any body of evidence E,\nexactly one doxastic attitude is the rational attitude to have toward\np, where the possible attitudes include believing p,\ndisbelieving p, and suspending judgment. Feldman’s\nargument seems to be that if the uniqueness theory is true, then when\nI believe p and my peer believes not-p, at least one of\nus must have formed an irrational opinion. Since I have no good reason\nto believe that I am not the misguided believer, the only rational\noption is to suspend judgment. But few people accept the uniqueness\nthesis. Are epistemic rules or principles really (always) so precise\nand restrictive? \nCritics of conciliationism offer a number of reasons for rejecting it\nas a systematic epistemic requirement. One line of criticism runs as\nfollows. Conciliationism may well be self-refuting. Since the truth of\nconciliationism is itself a matter of controversy, a proponent of\nconciliationism should become much less convinced of its truth when he\nlearns about this controversy. One may worry that there is something\nwrong with a principle that tells you not to believe in its own truth.\n(For further discussion, see Christensen 2013.) \nThomas Kelly (2010) offers a different reason for rejecting\nconciliationism, or at least for denying its ubiquitous\nappropriateness. He argues as follows:  \n[If] you and I have arrived at our opinions in response to a\nsubstantial body of evidence, and your opinion is a reasonable\nresponse to the evidence while mine is not, then you are not required\nto give equal weight to my opinion and to your own. Indeed, one might\nwonder whether you are required to give any weight to my\nopinion in such circumstances. (2010: 135)  \nHe acknowledges that whether one reasons well or badly, one might be\nequally confident in one’s conclusion in both cases. However, he\ncontends, “We should not thereby be drawn to the conclusion that\nthe deliverances of good reasoning and bad reasoning have the same\nepistemic status” (2010: 141). Rather, the person that reasons\nbetter (from the same evidence) may well be more entitled to his/her\nconclusion than the person who reasons worse. It may be rational for\nher to hold fast to her initial belief. This approach is what Kelly\ncalls the “Total Evidence View”. \nThe cases discussed thus far focus on epistemic agents who are\nindividuals. We subsumed these cases under the heading of\nsocial epistemology not because the believers themselves have\nsome sort of social character, but because there are players who rely\nupon, or appeal to, other epistemic agents. However, there are other\ntypes of cases that are naturally treated as social epistemology for\nan entirely different reason. \nIt is common to ascribe actions, intentions, and representational\nstates—including belief states—to collections or groups of\npeople. Such collections would include juries, panels, governments,\nassemblies, teams, etc. The State of California might be said to know\nthat chemical XYZ is a cause of cancer. What does it take for a group\nto believe something? Some take what is called the “summative\napproach”—a group believes something just in case all, or\nalmost all, of its members hold the belief (see, for instance, Quinton\n1976: 17). Margaret Gilbert, however, has pointed out that in ordinary\nlanguage use it is common to ascribe a belief to a group without\nassuming that all members hold the belief in question. In seeking to\naddress why this usage is acceptable, she advances a\n“collective” account of group belief. Under this view: \nA group G believes that p if and only if the members of\nG are jointly committed to believe that p as a\nbody. \nJoint commitments create normative requirements for group members to\nemulate a single believer of p. On Gilbert’s account,\nthe commitment to act this way is common knowledge, and if group\nmembers do not act accordingly they can be held normatively\nresponsible by their peers for failing to do so (see Gilbert 1987,\n1989, 2004). Frederick Schmitt (1994a), similarly offers a collective,\ncommitment-based account of group belief. \nSome have argued that these views are not properly about group belief\nbecause they focus on responsibility to peers, and not on the\nbeliefs-states of the group members. Wray (2001) suggests that these\nshould be considered accounts of group acceptance instead. Later in\nthe entry, we will address some accounts that take individual beliefs\nas inputs and output some group belief. It will become clear how\ndifferent these are from a joint commitment type view. \nA different approach is taken by Alexander Bird (2014) who contends\nthat the acceptance model of group belief is only one of many\ndifferent (but legitimate) models. For instance, he introduces the\n“distributed model” to deal with systems that feature\ninformation-intensive tasks which cannot be processed by a single\nindividual. Several individuals must gather different pieces of\ninformation while others coordinate this information and use it to\ncomplete the task. A famous example is provided by Edwin Hutchins\n(1995) who describes a large ship where different crew members take\ndifferent bearings so that a plotter can determine the ship’s\nposition and course. Members in such a distributed system will not\nordinarily satisfy the conditions of mutual awareness and commitment\nthat Gilbert and Schmitt require. Nonetheless, this seems to be a\nperfectly plausible species of group belief. Indeed, Bird contends\nthat this is a fairly standard type of group model that occurs in\nscience. Notice that for this model, again, it is quite possible that\nnot all members of the group hold the same belief. \nWhat about other attempts to provide an account of group belief?\nChristian List and Philip Pettit (2011) explore how individuals may\nyield a group belief through “judgment aggregation”.\nAlthough List and Pettit do not put it this way, it seems that\naggregation refers to metaphysical relations between beliefs of group\nmembers and beliefs of the groups they compose. It refers to ways in\nwhich a set of beliefs by group members can give rise to beliefs held\nby the group. Here is how List and Pettit express these metaphysical\nrelations: \nThe things a group agent does are clearly determined by the things its\nmembers do: they cannot emerge independently. In particular, no group\nagent can form propositional attitudes without the latter\nattitudes’ being determined, in one way or another, by certain\ncontributions of its members, and no group agent can act without one\nor more of its members acting. (2011: 64) \nThe foregoing passage places some limits on the metaphysical relations\nbetween member attitudes and group attitudes. It tells us something\nabout when group propositional attitudes occur, without implying\nanything about when the latter attitudes are justified. But social\nepistemologists are interested in the relation between the epistemic\nstatuses of members’ beliefs and the epistemic status of group\nbeliefs. List and Pettit address (something like) this question by\nexploring the sorts of judgment “functions”, i.e., rules\nfor aggregation, that might come into play. \nA sticky problem that emerges here is the “doctrinal\nparadox,” originally formulated by Kornhauser and Sager (1986)\nin the context of legal judgments. Suppose that a court consisting of\nthree judges must render a judgment in a breach-of-contract case. The\ngroup judgment is to be based on each of three related propositions,\nwhere the first two propositions are premises and the third\nproposition is the conclusion. For example: \nLegal doctrine entails that obligation and action are jointly\nnecessary and sufficient for liability. That is, conclusion (3) is\ntrue if and only if the two preceding premises, (1) and (2), are each\ntrue. Suppose, however, as shown in the table below, that the three\njudges form the indicated beliefs, vote accordingly, and the\njudgment-aggregation function delivers a conclusion guided by majority\nrule. In this case, the court as a whole forms beliefs and casts votes\nas shown below: \nIn this example, each of the three judges has a logically\nself-consistent set of beliefs. Moreover, a majority aggregation\nfunction seems eminently reasonable. Nonetheless, the upshot is that\nthe court’s judgments are jointly inconsistent. \nThis kind of problem arises easily when a judgment is made by multiple\nmembers of a collective entity. This led List and Pettit to prove an\nimpossibility theorem in which a reasonable-looking combination of\nconstraints is nonetheless shown to be jointly unsatisfiable in\njudgment aggregation (List and Pettit 2011: 50). They begin by\nintroducing four conditions that seem to be ones that a reasonable\naggregation function should satisfy: \nUniversal domain: The aggregation function admits as input\nany possible profile of individual attitudes toward the propositions\non the agenda, assuming that individual attitudes are consistent and\ncomplete. \nCollective rationality: The aggregation function produces as\noutput consistent and complete group attitudes toward the propositions\non the agenda. \nAnonymity: All individuals’ attitudes are given equal\nweight in determining the group attitude. \nSystematicity: The group attitude on each proposition depends\non the individuals’ attitudes toward it, not on their attitudes\ntoward other propositions, and the pattern of dependence between\nindividual and collective attitudes is the same for all\npropositions.  \nAlthough these four conditions seem initially plausible, they cannot\nbe jointly satisfied. Having proved this List and Pettit proceed to\nexplore “escape routes” from this impossibility. They\noffer ways to relax the requirements so that majority voting, for\nexample, does satisfy the collective rationality\ndesideratum.  \nBriggs et al. (2014) offer a particular way out. As they argue, it may\nbe too strong to require that entities always have logically\nconsistent beliefs. For instance, we might have many beliefs about\nmatters of fact, and also believe it likely we are wrong about some of\nthem. Following Joyce (1998), they introduce a weaker notion of\ncoherence of beliefs. They show that the majority voting aggregation\nof logically consistent beliefs will always be coherent, and the\naggregation of coherent beliefs will typically be coherent as well. In\nother words, if we demand less from majority voting as a means of\njudgment aggregation, we can get it. \nIn thinking about practical cases outside jury deliberation, we might\nask whether the theory of judgment aggregation can apply to the\nincreasingly common problem of how collaborating scientific authors\nshould decide what statements to endorse. Solomon (2006), for\ninstance, argues that voting might help scientists avoid\n“groupthink” arising from group deliberation. While Wray\n(2014) defends deliberation as crucial to the production of group\nconsensus, Bright et al. (2018) point out that consensus is not always\nnecessary (or possible) in scientific reporting. In such cases, they\nargue, majority voting is a good way to decide what statements a\nreport will endorse, even if there is disagreement in the group. \nUntil this point, we have only looked at how a group belief may arise\nfrom, or be determined by, its member beliefs. We have not yet\nconsidered how—or whether—a group entity can achieve some\nsignificant level of epistemic “status”, or\n“achievement”. Believing something that is true is\ncertainly worth something, but if one gets the truth only by luck or\naccident, that is not very significant. Similarly, simply having a set\nof coherent beliefs isn’t of unqualified significance, since\nmere coherence can arise from pure imagination coupled with\ninconsistency avoidance. Philosophers have long agreed that one of the\nmost important types of epistemic attainment is knowledge.\nBut what is knowledge? Almost everyone agrees that mere true belief\nisn’t knowledge. Again, true beliefs can sometimes arise\nfortuitously, by pure guessing or wishful thinking. \nOne popular way of strengthening the requirements for knowledge is to\nadd a justification requirement. A true belief doesn’t\nper se qualify as knowledge unless the belief is\njustified. Justification is widely accepted as one species,\nor variety, of epistemic achievement. In addition to providing\n(arguably) an essential condition for knowledge, it seems to be an\nachievement, or attainment, in its own right, even when it\ndoesn’t bring truth in along with it. For this reason,\nepistemologists have paid considerable attention to the nature of\njustification. Of course, most of this attention is concentrated on\njustification by individual believers. But, as we shall see,\nit is worth pursuing the possibility that group beliefs can\nalso be evaluated as either justified or unjustified. \nIf the government of the United States believes (or were to believe)\nthat there is a crisis of global warming, it would probably be\njustified in believing this, because of the overwhelming consensus of\nclimate scientists to this effect. It is probably not\nrequired, however, that all members of a group must believe a\ngiven proposition in order that the group belief be justified, as\nJennifer Lackey rightly says in criticizing the view of Frederick\nSchmitt (1994a: 265). It would make group justification too hard to\ncome by (Lackey 2016: 249–250). But how, more precisely, should\nthe requirement be specified? \nGoldman (1979) advocates process reliabilism as an account of\nindividual justification. He proposes an analogous approach to group\njustification in Goldman (2014). Under individual process reliabilism,\na person is justified in believing a proposition W only if she\nbelieves W through using a generally reliable belief-forming\nprocess (or sequences of processes). In addition, if a person’s\nbelief in W were produced by previous beliefs of hers, those\nother beliefs must also have been justified in order that her belief\nin W qualify as justified. \nAs indicated, Goldman’s approach to collective justifiedness is\nanalogous to individual justifiedness. But how exactly is it\nanalogous? First, members’ premises that contribute to the\njustification of a group’s belief must themselves be justified.\nA further requirement for group justification is that the\ngroup’s belief be caused by a type of belief-forming process\nthat takes inputs from member beliefs in proposition p and\noutputs a group belief in p. The constraint on group\njustifiedness is a requirement that such a process type must be\ngenerally conditionally reliable. An example of such a process type\nmight be a majoritarian process in which member beliefs (of the group)\nare aggregated into a group belief. The process is conditionally\nreliable only if, when it receives true input beliefs, most of the\noutput beliefs are also true. \nLackey (2016), in critiquing Goldman’s theory, offers a number\nof challenging and insightful problems. Their numerosity and detail\nmake it unfeasible to present or even summarize many of them. Let us\nconsider one such objection. \nGoldman advances a certain principle (GJ) which runs as follows: If a\ngroup belief in p is aggregated based on a profile of\nmembers’ attitudes toward p, then ceteris\nparibus the greater the proportion of members who justifiedly\nbelieve p and the smaller the proportion of members who\njustifiedly reject p, the greater the group’s level, or\ngrade, of group justifiedness in believing p (Goldman 2014:\n28). In other words, group justifiedness increases with a greater\npercentage of individual members with justified beliefs. \nLackey acknowledges that (GJ) “is not only intuitively\nplausible, it also is easily supported by applying an aggregative\nframework to group justifiedness” (2016: 361). Nonetheless, she\nproceeds to argue that (GJ) should be rejected. One major theme in her\ncritique is that (GJ) falls prey to a paradox (on contradiction),\nwhich she calls the “Group Justification Paradox.” It\nquestionably allows a group to end up justifiedly believing\nboth a certain relevant proposition and its\nnegation. Lackey hastens to acknowledge that not all contradictions,\nor inconsistencies, are necessarily fatal. For example, the well-known\npreface paradox is a case that generates inconsistent propositions,\nbut it is nevertheless reasonable to have. An author’s apology\nin the preface is epistemically reasonable, given our grounds for\nholding that we all have our own fallibility (2016: 364). However,\nLackey continues, the Group Justification Paradox admits no such\napproval. So the incidence of such contradictions does\nconstitute a challenge to the proposed theory of group justification,\neven if the preface paradox does not present a problem. \nWe return now to the basic form of social epistemology, in which\nindividual cognizers seek information from other individuals. Here we\nare interested not in the concepts of justification or knowledge in\ngeneral, but in particular applications. This might be called\napplied social epistemology. For example, how can one\nidentify other individuals as sources of accurate information? A\nsalient type of case is when one seeks information from an\n“expert,” where the seeker is himself/herself a layperson.\nUnfortunately, experts don’t always agree with one another. If\nyou are offered advice by two different (putative) experts, but the\ntwo advisers disagree, which one should you trust, or believe? \nWhat is meant by an “expert”? For present purposes, let us\nfocus on factual subject matters (rather than matters of taste, for\nexample). By an expert, then, we shall mean someone who—in a\nspecified domain—possesses a greater quantity of (accurate)\ninformation than most other people do. A layperson, by contrast, is\nsomeone who has very little information in the specified domain, and\nwho doesn’t take him/herself to have such information. The\nproblem facing the layperson is how to select, among the disagreeing\nputative experts, the one that is best (Goldman 2001). \nThere are several possible methods by which the layperson might\nproceed. A first possible method is to seek statements or arguments\nfrom the competing (putative) experts to assess which one seems to be\nmore knowledgeable, better-informed, and savvy (in the domain) as\ncompared to his/her rivals. The problem here is: how can a layperson\nmake an accurate assessment of the competing experts’\nperformance? By definition, the layperson is someone who is\nill-informed on the subject. Moreover, experts often use language that\nis technical, arcane, rarified, or esoteric. \nA second possible method is for the layperson to obtain information\nabout the competing experts’ credentials. Where were the\ncompeting experts trained in the domain in question? What were the\ncompeting experts’ records of performance during their\neducation? These are matters that the layperson may be unable to\ndetermine. And even if they are determined, is the layperson in a\nposition to assess their significance? \nA third method that a layperson might consider employing is how many\nother putative experts agree with a specified target expert\non many of the questions in the target domain. If a given expert is an\noutlier on the issue in question, that might be a reason to avoid\nhim/her. Conversely, if a large number of (putative) experts agree\nwith the selected target, doesn’t that give extra weight to the\nlatter’s promise as a sound advisor? Surely, a large number of\nconcurring experts should provide strong support for a selected\nindividual. \nThis is a tricky matter. There are many possible reasons why so many\npeople in a field might agree, and such agreement doesn’t always\nsignal that they are all correct. One possibility is that all the\nforegoing putative experts were trained by one and the same\n“guru”, who was a very persuasive and compelling figure.\n(Using a Bayesian analysis, it becomes clear that identifying an extra\nconcurring believer need not strengthen one’s evidence as\ncompared with a single believer. See Goldman (2001:\n99–102).) \nA fourth possible method is for the layperson to verify, by his/her\nown observation, that a candidate expert was correct in\nprevious cases where he/she offered a view on the topic in question.\nThe problem here is whether a layperson is capable of making such\nobservations or determinations. It is possible in a selected\nrange of cases. A layperson might consult with a broker to decide\nwhether a certain investment is advisable. The broker gives her the\ngo-ahead based on a prediction that the market in question will rise\nin the next three years. The layperson makes the investment, waits\nthree years, and notes that the broker was right: the market did rise\nin those three years. The layperson couldn’t have made this\nprediction on the basis of her own knowledge. But she is able (after\nthree years) to conclude that the putative expert was correct.\nRepeated instances of this kind would strengthen the support. \nWe have now seen some of the problems that face those who develop\nknowledge within a community. In recent years philosophers have turned\nto formal methods to understand some of these social aspects of belief\nand knowledge formation. There are broadly two approaches in this\nvein. The first comes from the field of formal epistemology, which\nmostly uses proof-based methods to consider questions that mostly\noriginate within individual-focused epistemology. Some work in this\nfield, though, considers questions related to, for example, judgment\naggregation and testimony. The second approach, sometimes dubbed\n“formal social epistemology”, stems largely from\nphilosophy of science, where researchers have employed modeling\nmethods to understand the workings of epistemic communities. While\nmuch of this work has been motivated by a desire to understand the\nworkings of science, it is often widely applicable to social aspects\nof belief formation. \nAnother distinction between these traditions is that while formal\nepistemologists tend to focus on questions related to ideal belief\ncreation, such as what constitutes rationality, formal social\nepistemologists have been more interested in explaining real human\nbehavior, and designing good knowledge-creation systems. We will now\nbriefly discuss relevant work from formal epistemology, and then look\nat three topics in formal social epistemology. \nAs mentioned, formal epistemology has mostly focused on issues related\nto individual epistemology. This said, there is a significant portion\nof this literature addressing questions central to social\nepistemology. In particular: how should a group aggregate their\nbeliefs? And: how should Bayesians update on the testimony of\nothers? \nAs we have already seen, in aggregating propositional judgments groups\nof people can reach paradoxical outcomes as a result of majority\nvoting. Let’s change focus, though, from propositional beliefs,\nto a more fine-grained notion of belief. Formal epistemologists more\ncommonly discuss degrees of belief, or “credences”. These\nare numbers between 0 and 1 representing an agent’s degree of\ncertainty in a statement. (For instance, if I think there is a 90%\nchance it is raining, my credence that it is raining is .9.) This\nrepresentation changes the question of judgement aggregation to\nsomething like this: Suppose a group of people hold different\ncredences, what should the group credence be? \nIn formal epistemology, this ends up being very closely related to the\nquestion of how an individual ought to update their credences upon\nlearning the credences of others. If a rational group ought to adopt\nsome aggregated belief, then it might also make sense for an\nindividual in the group to adopt the same belief as a result of\nlearning about the credences of his/her peers. In other words, the\nproblems of judgment aggregation, peer disagreement, and testimony are\nentangled in this literature. (Though see Easwaran et al. (2016) for a\ndiscussion of distinctions between these issues.) We’ll focus\nhere on belief aggregation, though we will comment throughout on these\nother issues. \nIn principle, there are many ways that one can go about aggregating\ncredences or pooling opinions (Genest and Zidek 1986). A simple option\nis to combine opinions by linear pooling—taking a weighted\naverage of credences. This averaging could respect all credences\nequally, or put extra weights on the opinions of, say, recognized\nexperts. This option has some nice properties, such as preserving\nunanimous agreement, and allowing groups to aggregate over different\ntopics independently (DeGroot 1974; Lehrer and Wagner\n 1981).[1] \nIn thinking about ideal knowledge creation though, we might ask how a\nBayesian should update their credences in light of peer\ndisagreement or how a group of Bayesians should aggregate beliefs.\nBayes rule gives the rational way an agent should update a prior\nprobabilistic credence in light of evidence to obtain a posterior\ncredence. In general, if evidence appears that is more likely given\nA than B, a Bayesian will increase their credence that\nA in fact obtains upon observing that evidence. (So if I have a\n.9 credence that it is raining, and someone walks in wearing shorts\nand with perfectly dry hair, my credence in rain should decrease\nbecause my observation is more likely to occur on a sunny day than a\nrainy one.) Why is this approach rational? An individual who does not\nchange credences according to Bayes rule can be Dutch\nbooked—offered a series of bets that they will take, but that\nare guaranteed to lose money. \nA Bayesian will not simply average across beliefs, except under\nparticular assumptions or in special cases (Genest and Zidek 1986;\nBradley 2007; Steele 2012; Russell et al. 2015). And a group that\nengages in linear averaging of this sort can typically be Dutch\nbooked. \nA fully-fledged Bayesian approach to aggregation demands that the\nfinal credence be derived by Bayesian updating in light of the\nopinions held by each group member (Keeney and Raiffa 1993). Notice,\nthis is also what a Bayesian individual should do to update on the\ncredences of others. To do this properly, though, is very\n complicated.[2]\n It requires prior probabilities about what obtains in the world, as\nwell as probabilities about how likely each group member is to develop\ntheir credences in light of what might obtain in world. This will not\nbe practical in real cases. \nInstead, many approaches consider features that are desirable for\nrational aggregation, and then ask which simpler aggregation rules\nsatisfy them. For instance, one thing a rational aggregation method\nshould do (to prevent Dutch booking) is yield the same credence\nregardless of whether information is obtained before or after\naggregating. For instance, if we all have credences about the rain,\nand someone comes in wearing shorts, it should not matter to the final\ngroup output whether 1) they entered and we all updated our credences\n(in a Bayesian way) and then aggregated them, or 2) we aggregated our\ncredences, they entered, and we updated the aggregated credence (in a\nBayesian way). Geometric methods, which take the geometric average of\nprobabilities over worlds, yield this desirable property in\nmany cases (Genest 1984; Dietrich and List\n 2015).[3]\n These methods proceed by multiplying (weighted) credences over worlds\nthat might obtain and then renormalizing them to sum to 1. \nOne thing that geometric averaging does not do, though, is allow for\ncredences over different propositions to be aggregated completely\nindependently from each other. (Recall that this was something List\nand Pettit treated as a desideratum for judgment aggregation.) For\ninstance, our beliefs about the probabilities of hail might influence\nhow we will aggregate our beliefs over the probabilities of rain.\nInstead, a more holistic approach to aggregation is required. This is\nan important lesson for approaches to social epistemology which focus\non individual topics of interest in addressing peer disagreement and\ntestimony (Russell et al. 2015). \nAnother thing that some take to be strange about geometric averaging\nis that it sometimes will aggregate identical credences to a different\ngroup credence. For instance, we might all have credence .7 that it is\nraining, but our group credence might be .9. Easwaran et al. (2016)\nargue, though, that this often makes sense when updating on the\ncredences of others—their confidence should make us\nmore confident (see also Christensen\n 2009).[4] \nThis question of whether aggregated credences can be more extreme than\nindividual ones echoes much earlier work bearing on the question: are\ngroups smart? In 1785, the Marquis de Condorcet wrote an essay proving\nthe following. Suppose a group of individuals form independent beliefs\nabout a topic and they are each more than 50% likely to reach a\ncorrect judgement. If they take a majority vote, the group is more\nlikely to vote correctly the larger it gets (in the limit this\nlikelihood approaches 1). This result, now known as the\n“Condorcet Jury Theorem”, underlies what is sometimes\ncalled the “wisdom of the crowds”: in the right conditions\ncombining the knowledge of many can be very effective. \nIn many cases, though, real groups are prone to epistemic problems\nwhen it comes to combining beliefs. Consider the phenomenon of\ninformation cascades, first identified by Bikhchandani et al. (1992).\nTake a group of agents who almost all have private information that\nNissan stock is better than GM stock. The first agent buys GM stock\nbased on their (minority) private information. The second agent has\ninformation that Nissan is better, but on the basis of this observed\naction updates their belief to think GM is likely better. They also\nbuy GM stock. The third agent now sees that two peers purchased GM and\nlikewise updates their beliefs to prefer GM stock. This sets off a\ncascade of GM buying among observers who, without social information,\nwould have bought Nissan. The problem here is a lack of independence\nin the “vote”—each individual is influenced by the\nbeliefs and actions of the previous individuals in a way that obscures\nthe presence of private information. In updating on the credences of\nothers, we thus may need to be careful to take into account that they\nmight already have updated on the credences of others. \nAs noted, much of the work in formal social epistemology is by\nphilosophers of science, who investigate scientific communities in\nparticular. It will be useful to divide this literature into three\ncategories: credit economy models of science, network epistemology\nmodels, and modeling approaches to diversity in epistemic\ncommunities. \nSuppose you are a scientist choosing what to work on. There are two\nlive options. One is more promising, and you suspect that if an\nadvancement will be made, it will be on that problem. The other is\nless promising, but, as a result, fewer scientists are attracted to\nit. This means that should a discovery be made in that area, each\nscientist will be more likely to have made it. \nThe subfield of formal social epistemology arguably started with\nPhilip Kitcher’s 1990 paper “The Division of Cognitive\nLabor”, which takes a rationality-based approach to the question\nof why scientists might divide labor effectively, even when they agree\non which problems are most promising. By rationality-based approach,\nwe mean that Kitcher represents scientists as utility-maximizers, in\nmuch the same way that economists represent people as\nutility-maximizers. The key innovation, though, is that scientists are\nassumed to derive utility from credit—a proxy for\nrecognition and approbation of one’s scientific work, along with\nall the benefits (promotions, grants, etc.) that follow. The\nsociologist Robert Merton was one of the first to recognize the credit\nmotives of scientists (Merton 1973). Models using this assumption are\noften called credit economy models. \nKitcher’s model shows that scientists in the sort of scenario\nsketched above will divide labor more effectively when they are\nmotivated by credit than when they are pure truth-seekers. Truth\nseekers will each take the more promising approach. For\ncredit-maximizers, once too many individuals work on the better\nproblem, the expected credit for each scientist decreases to the point\nthat some individuals will prefer to switch. Strevens (2003) extends\nKitcher’s work by arguing that an existing feature of credit\nincentives, the priority rule, can lead to an even better division of\nlabor. This is the rule, identified by Merton, which stipulates that\ncredit will be allocated only to the scientist who first makes a\ndiscovery. In Strevens’s model, researchers incentivized by the\npriority rule divide labor in a highly efficient way compared to\nresearchers incentivized by other credit schemes. \nNot all researchers are as optimistic about the consequences of\nscientific credit norms like the priority rule. Romero (2017) points\nout, for example, that the benefits Kitcher and Strevens identify\ndisappear in cases where results are not always replicable. (I.e.,\nwhere, as in real scientific communities, any particular study could\ngive a misleading result, and thus findings require replications\nbefore they can be considered “truth”.) In particular, the\npriority rule strongly disincentivizes scientists from performing\nreplications because credit is so strongly associated with new,\npositive findings. \nThis debate reflects a deep question, going back as far as Du Bois\n(1898), and at the heart of much of the work in this section: what is\nthe best motive for an epistemic community? Is it credit seeking or\n“pure” truth seeking? Or some combination of the two? \nThis sort of tension over the benefits and detriments of credit\npractices also plays out with respect to debates about scientific\nfraud. Starting with Merton many have argued that the desire to claim\npriority, and thus credit, drives\n fraud.[5]\n These arguments seem to suggest that scientists who ignore credit,\nand instead attempt to yield truth will do a better job. Bright\n(2017a), though, uses a model to point out that credit-seekers who\nfear retaliation may publish more accurate results than truth-seekers\nwho are convinced of some fact, despite their experimental results to\nthe contrary. In other words, a true believer may be just as\nincentivized to commit fraud as someone who simply seeks approval from\ntheir community. \nWe see this debate again over the Matthew effect, identified by Merton\n(1968). As Merton points out, pre-eminent scholars often get more\ncredit for work than a less famous scholar would have gotten. Strevens\n(2006) argues that this follows the scientific norm to reward credit\nbased on the benefit a discovery yields to science and society.\nBecause famous scientists are more trusted, their discoveries do more\ngood. Strevens claims that this norm of credit thus improves discovery\nin a community. Heesen (2017), on the other hand, uses a credit\neconomy model to show how someone who gets credit early on due to luck\nmay later accrue more and more credit because of the Matthew effect.\nWhen this kind of compounding luck happens, he argues, the resulting\nstratification of credit in a scientific community does not improve\ninquiry. \nAs we have seen, credit economy models help answer questions like:\nwhat is the best credit structure for an epistemic community? How do\nwe promote truth? And: what credit incentives should be\n avoided?[6]\n Once we accept that epistemic communities are more than the sum of\ntheir individual parts, it is crucial to probe the incentive\nstructures that members of these communities face in thinking about\nhow best to shape them. As credit economy models show us, designing\ngood epistemic communities is by no means a trivial task. \nAnother paradigm widely used by philosophers to explore social aspects\nof epistemology is the epistemic network. This kind of model uses\nnetworks to explicitly represent social or informational ties where\nbeliefs, evidence, and testimony can be shared. \nThere are different ways to do this. In the social sciences generally,\nthe most popular approach takes a “diffusion” or\n“contagion” view of beliefs. A belief or idea is\ntransmitted from individual to individual across their network\nconnections, much like a virus can be transmitted (Rogers 1962).\nImagine you live in a farming community where a new species of large\ncaterpillar has started decimating crops. Suppose you come to believe\nthat this kind of caterpillar is poisonous. You will immediately tell\nyour friends and neighbors what you have learned, they will tell their\nfriends and neighbors, and so on. Figure 1 shows this. Black nodes\nrepresent those “infected” with an idea. It starts with a\nfocal individual and spreads, virally, through the community. \nFigure 1: A network contagion model\nwhere a focal individual infects others with a belief. [An\n extended description of figure 1\n is in the supplement.]  \nIn these diffusion/contagion models, though, the individuals do not\ngather evidence from the world, share evidence with each other, or\nform beliefs in any sort of rational way. For this reason,\nphilosophers of science have tended to use the network\nepistemology framework instead. This framework was introduced by\neconomists Bala and Goyal (1998) to model how individuals learn from\nneighbors. It was imported to philosophy by Kevin Zollman, who first\nused it to represent scientific communities (Zollman 2007, 2010). \nNow imagine the same caterpillar example, but where the individuals\ninvolved form evidence-based beliefs. One becomes suspicious that the\ncaterpillar is poisonous, and tests to see if this is true. She shares\nthe evidence she gathered (not just her belief) with those she is\nconnected to. Her neighbors, on the basis of this evidence, themselves\nbecome more suspicious that the caterpillar is dangerous, and test for\nthemselves. They, in turn, share the evidence they gather with their\nneighbors. Beliefs can still spread through a network, but now they do\nso on the basis of at least semi-rational belief-forming\nmechanisms. \nIn more detail: network epistemology models start with a collection of\nagents on a network, who choose from some set of options. One option\nis preferable to the rest, but to find out which this is, the agents\nmust actually try them and see what results. These could represent\nchoices of action-guiding theories (like “caterpillars are\nsafe” and “caterpillars are poisonous” or else\n“vaccines are safe” and “vaccines cause\nautism”). They could alternatively represent research approaches\nthat yield different levels of scientific success. \nAgents have beliefs about which option is preferable, and change these\nbeliefs in light of the evidence they gather from their actions. In\naddition, they also update on evidence gathered by neighbors in the\nnetwork, typically using some version of Bayes’ rule. It is in\nthis sense that agents are part of an epistemic community. Figure 2\nshows what this might look like. The numbers next to each agent\nrepresent their degree of belief in some proposition like\n“vaccines are safe”. The black agents think this is more\nlikely than not. As this model progresses these agents gather data,\nwhich increases their neighbors’ degrees of belief in turn. \nFigure 2: Agents in a network\nepistemology model use their credences to guide theory testing. Their\nresults change their credences, and those of their neighbors. [An\n extended description of figure 2\n is in the supplement.]  \nCommunities in this model can develop beliefs that the better theory\n(vaccines are safe) is indeed better, or else they can pre-emptively\nsettle on the worse theory (vaccines cause autism) as a result of\nmisleading evidence. Generally, since networks of agents are sensitive\nto the evidence they gather, they are more likely to figure out the\n“truth” of which is best (Zollman 2013; Rosenstock et al.\n2017). \nZollman (2007, 2010) describes what has now been dubbed the\n“Zollman effect” in these models; the surprising\nobservation that it is sometimes worse for communities to communicate\nmore (see also, Grim 2009). In particular, groups with more network\nconnections will be generically less likely to arrive at a correct\nconsensus. The group needs to entertain all the possible options long\nenough to gather good evidence and settle on the best one. In tightly\nconnected networks, misleading evidence is widely shared, and may\ncause the community to pre-emptively settle on a poor theory. \nMayo-Wilson et al. (2011, 2013) likewise defend a surprising thesis,\nwhich supports central claims from social epistemology espoused by\nGoldman (1999). They use epistemic network models, to support the so\ncalled “independence thesis”—that rational groups\nmay be composed of irrational individuals, and rational individuals\nmay constitute irrational groups. For instance, consider a learner who\ntests some preferred theory. Alone, she may fail to test other\nsuccessful theories, but a community representing a full diversity of\npreferred theories will be expected to learn which is best. \nOne thing we know about human learners is that they have various\ncognitive and social biases that influence how they take up\ninformation from peers. One of these is conformity bias, or a tendency\nto espouse the views of group members, even if one secretly disagrees\nwith them (Asch 1951). Weatherall and O’Connor (2018, Other\nInternet Resources) show how conformity can prevent the adoption of\nsuccessful beliefs because agents who conform to their neighbors are\noften unwilling to pass on good information that goes against the\ngrain. Mohseni and Williams (2019, Other Internet Resources) \nsimilarly find that conformity\nslows learning, likewise because it prevents agents from sharing\ninformation, and because group members who expect this are less\ntrusting of their \npeers.[7] \nSeveral authors have used variants on the epistemic network model to\nexplore the phenomenon of “polarization” within\n groups.[8]\n Both Olsson (2013) and O’Connor and Weatherall (2018) consider\nversions of the model where actors place less trust in the evidence\n(or testimony) of those who do not share their beliefs. A vaccine\nskeptic, for instance, might be skeptical of evidence shared by a\nphysician, but accepting of evidence from a fellow skeptic. This can\nlead to stable, polarized camps that each ignore evidence and\ntestimony coming from the other\n camp.[9]\n The two sets of results just described help answer the question: in\nlight of real social and learning biases, what can go wrong? And: how\ncan we shape good epistemic networks to counteract these biases?\n(These questions tie into how we should understand democracy in light\nof social epistemology.) \nOne of the most interesting recent uses of the network epistemology\nframework involves investigating the role of pernicious influencers,\nespecially from industry, on epistemic communities. Holman and Bruner\n(2015) look at a network model where one agent shares only fraudulent\nevidence meant to support an inferior theory. As they show, this agent\ncan keep a network from reaching successful consensus by muddying the\nwater with misleading data. Holman and Bruner (2017) and Weatherall et\nal. (forthcoming) use network epistemology models to explore specific\nstrategies that industry has used to influence scientific research. As\nHolman and Bruner show, industry can shape the output of a community\nthrough “industrial selection”—funding only agents\nwhose methods bias them towards preferred findings. Weatherall et al.\nadd a group of “policy makers” to the model, to show how a\npropagandist can mislead these public agents simply by sharing a\nbiased sample of the real results produced in an epistemic network.\nFor example, Big Tobacco might gather up real, independent studies\nthat happen to find no link between smoking and cancer, and share\nthese widely (Oreskes and Conway 2011). Together these two papers give\ninsight into how strategies that do not involve fraud can shape\nscientific research and mislead the public. \nOne truth about epistemic communities is that relationships matter.\nThese are the ties that ground testimony, disagreement, and trust.\nEpistemic network models allow philosophers to explore processes of\ninfluence in social networks, yield insights into why social ties\nmatter to the way communities form beliefs, and think about how to\ncreate better knowledge systems. \nDiversity has emerged several times in our discussion of formal social\nepistemology. Credit incentives can encourage scientists to choose a\ndiversity of problems. In network models, a transient diversity of\nbeliefs is necessary for good inquiry. Let us now turn to models that\ntackle the influence of diversity more\n explicitly.[10]\n It has been suggested that cognitive diversity benefits epistemic\ncommunities because a group where members start with different\nassumptions, use different methodologies, or reason in different ways\nmay be more likely to find truth. \nWeisberg and Muldoon (2009) introduce a model where actors investigate\nan “epistemic landscape”—a grid where each section\nrepresents a problem in science, of varying epistemic importance.\nFigure 3 shows an example of such a landscape. Scientists are randomly\nscattered on the landscape, and follow search rules that are sensitive\nto this importance. Investigators can then ask: how well did\nscientists do? Did they fully search the landscape? Did they find the\npeaks? \nFigure 3: An epistemic landscape.\nLocation represents problem choice, and height represents epistemic\nsignificance. \nWeisberg and Muldoon use the model to argue that a combination of\n“followers” (scientists who work on problems similar to\nother scientists) and “mavericks” (who prefer to explore\nnew terrain) do better than either group alone; i.e., there is a\nbenefit to cognitive diversity. Their modeling choices and main result\nhave been convincingly criticized (Alexander et al. 2015; Thoma 2015; Poyhönen\n2017; Fernández Pinto and Fernández Pinto 2018), but the\nframework has been co-opted by other philosophers to useful ends.\nThoma (2015) and Poyhönen (2017), for instance, show that in\nmodified versions of the model, cognitive diversity indeed provides\nthe sort of benefit Weisberg and Muldoon\n hypothesize.[11] \nHong and Page (2004) use a simple model to derive their famous\n“Diversity Trumps Ability” result. Agents face a problem\nmodeled as a ring with some number of locations on it. Each location\nis associated with a number representing its goodness as a solution.\nAn agent, in the model, is represented as a finite set of\n“heuristics”, or integers, such as 〈3, 7, 10〉.\nSuch an agent is placed on the ring, and can see the locations 3, 7,\nand 10 spots ahead of their current position. They then move to\nwhichever has the highest number until they reach a location where\nthey can no longer improve their score. \nThe central result is that randomly selected groups of agents\nwho tackle the task together tend to outperform groups created of top\nperformers. This is because the top performers have similar\nheuristics, and thus gain relatively little from group membership,\nwhereas random agents have a greater variety of heuristics. This\nresult has been widely cited, though there have been criticisms of the\nmodel either as insufficient to show something so complicated, as\nlacking crucial representational features, or as failing to show what\nit claims (Thompson 2014; Singer 2019). \nTo this point we have addressed cognitive diversity. But we might also\nbe interested in diversity of social identity in epistemic\ncommunities. Social diversity is an important source of cognitive\ndiversity, and for this reason can benefit the functioning of\nepistemic groups. For instance, different life histories and\nexperiences may lead individuals to hold different assumptions and\ntackle different research programs (Haraway 1989; Longino 1990;\nHarding 1991; Hong and Page 2004). If so, then we may want to know:\nwhy are some groups of people often excluded from epistemic\ncommunities like those in academia? And what might we do about\nthis? \nIn recent work, scholars have used models of bargaining to represent\nacademic collaboration. They have shown 1) how the emergence of\nbargaining norms across social identity groups can lead to\ndiscrimination with respect to credit sharing in collaboration (Bruner\nand O’Connor 2017; O’Connor and Bruner 2019) and 2) why\nthis may lead some groups to avoid academia, or else cluster in\ncertain subfields (Rubin and O’Connor 2018). In the\ncredit-economy tradition, Bright (2017b) explains why a noted\nphenomenon—that women tend to publish fewer papers than\nmen—may not indicate a gap in quality of research. As he points\nout, anticipation of rejection may lead women to overshoot by\nproducing papers of higher quality than necessary for publication.\nThis gap contributes to the underrepresentation of women in some\ndisciplines. \nAs we have seen in this section, models can help explain how and when\ncognitive diversity might matter to the production of knowledge by a\ncommunity. They can also tell us something about why epistemic\ncommunities often, nonetheless, fail to be diverse with respect to\nsocial identity. \nLet us now move on to see how topics from social epistemology\nintersect with important questions about the proper functioning of\ndemocratic societies, and questions about the ethics of social\nknowledge and learning. \nIn our portrayal of social epistemology thus far, several different\nmosaics have been sketched. In some cases, a single epistemic agent\nseeks epistemic help from another agent. In other cases, a collective\nagent seeks answers to questions using its members in a collaborative\nfashion. A third kind of case, which arose especially in the last\nsection, is what we shall call a “system-oriented” or\n“institution-oriented” application of social\nepistemology. \nBy a “system” we mean some entity with a multiplicity of\nworking “parts” and multiple goals that the system aims to\nachieve. A question that arises, quite frequently, is how best to\ndesign a system that will maximize the attainment, or satisfaction, of\nits (most important) goals over time. As we have seen, this is a\nquestion that philosophers have attempted to answer with respect to\nthe structure of scientific communities. Another good example of such\nsystems are political systems, especially democratic political\nsystems. There are many current democratic theorists who place much\nemphasis on the epistemological, or epistemic, properties of\ndemocratic institutions. \nElizabeth Anderson (2006) focuses on the question of how the epistemic\nproperties of democratic systems can be designed to attain the best\npossible form of democracy. She provides three epistemic models of\ndemocracy: the Condorcet Jury Theorem, the Diversity Trumps Ability\nresult, and John Dewey’s experimentalism (see Landemore 2011).\nAnderson plumps for Dewey’s experimentalist approach. He\nhighlights the importance of bringing together citizens from different\nwalks of life to define, through discussion, the principal problems\nthey confront and what might be the most promising solutions. Their\ndifferent walks of life constitute, in effect, a range of experiments\nthat can help them collectively appraise alternative solutions, thus\ntaking advantage of cognitive diversity. Anderson provides a\nconvincing illustration of Dewey’s thesis by relating how women\nin a South Asian village were able to manage their forests better when\nthey were given opportunities to make fuller use of their\n“situated knowledge” (see Agarwal 2000). \nWhen we contemplate the meaning of democracy, we often mean (as a\nstarting point, anyway) a governmental system that features equal\nvoting rights for all citizens. A little reflection, however, readily\nindicates how shallow a role is played by mere voting rights. A\ncitizen may be entitled to cast a vote for any of the candidates on\nthe ballot. But this will not help the voter promote positive results\n(positive by her lights) if she has misguided views of what specific\ncandidates for office would do if they were actually elected, or about\nwhat policy measures will be effective (for details, see Goldman 1999:\n315–348). \nThis raises the question of just how informed or misinformed ordinary\nvoters are, and what prospects there are for improving the present\nsituation. Many political scientists have shown that American voters\nare strikingly uninformed with respect to textbook facts about their\ngovernment. Nonetheless, there are some rays of light. Several books\nargue that ordinary citizens can make sense of their political world\ndespite a lack of detailed information about policies and candidates\n(see Berelson, Lazarsfeld, & McPhee 1954, and Katz &\nLazarsfeld 1955; cited in Goldman 1999). One semi-optimistic idea is\nthat of a two-step flow of communication from well-informed\n“opinion leaders” to the public at large. This approach\nsuggests that ordinary citizens—even those who pay little\nattention to the details of politics—can learn what they need to\nknow to make suitable choices by listening to the opinions of experts\nor news junkies. Cues and informational “shortcuts” are\navailable that can lead them to the same answers that more informed\ncitizens arrive at (cf. Goldman 1999: 318). \nIf this account is correct, it suggests that a wide range of citizens\ncan make fairly “accurate” voting decisions if two\nconditions hold: (1) there are political experts whose knowledge\nenables them to pinpoint who would be good electoral choices relative\nto specifiable citizens; and (2) these initially less-informed\ncitizens are capable of identifying who are (some of) the genuine\nexperts and who are not. An ability to recognize genuine expertise in\npolitical matters can play a significant role in promoting the kind of\ndemocratic success described above. As noted, though, it can be\ndifficult for laypeople to decide which experts to trust. And as we\nsaw in the last section, in our discussion of industrial influence on\npublic belief, there are forces that work to undermine the functioning\nof democracy, and that often bring us away from this more optimistic\npicture. In the next section we will briefly discuss some related\nissues. \nThe latest challenge confronting the informational state of the public\nis the accelerating spread of misinformation and disinformation on the\ninternet. On Twitter falsehoods spread further and faster than the\ntruth (Temming 2018a,b). In the run-up to the 2016 U.S. presidential\nelection, the most popular bogus articles got more Facebook shares,\nreactions, and comments than the top real-news stories, according to a\nBuzzFeed News analysis. And, as many have documented, online\nmisinformation and disinformation in a wide variety of forms have\ncreated serious issues vis-à-vis public belief and\ndemocratic functioning. \nIn trying to tackle the spread of misinformation, many online\nplatforms have implemented algorithms. For instance, in response to\n“fake news”, programmers have built automated systems that\naim to judge the veracity of online stories. Researchers explore which\nfeatures of an article are the most reliable identifiers of fake news\n(Temming 2018b: 24). Clearly, these sorts of tools have some promise\nas part of the enterprise of social epistemology. But their power to\ndiscriminate true stories from false ones still has limited\nreliability. Furthermore, as O’Connor and Weatherall (2019)\npoint out (drawing on the work of Holman (2015) who looks at arms\nraces between pharmaceutical companies and regulators) online\nmisinformation constitutes a kind of arms race. As platforms and\nprogrammers and governments develop tools to fight it, the purveyors\nof misinformation (the Russian state, various partisan groups,\nadvertisers, trolls, etc.) will develop new methods of shaping public\nbelief. \nAll ill-informed populace, as noted, may not be able to effectively\nrepresent their interests in a democratic society. In order to protect\ndemocratic functioning, going forward it will be necessary for those\nfighting online misinformation to keep adapting with the best tools\nand theory available to them. This includes understanding social\naspects of knowledge and belief formation. In other words, social\nepistemology has much to say to those faced with the challenging task\nof protecting democracy from misinformation. \nSome recent writers seek to expand the notion of social epistemology\nby incorporating moral or ethical elements. Miranda Fricker (2007) in\nparticular has made significant contributions to this literature.\nFricker introduces the notion of “epistemic injustice,”\nwhich arises when somebody is wronged in their capacity as a knower.\nAn easily recognizable form of such injustice is when a person or a\nsocial group is unfairly deprived of knowledge because of their lack\nof adequate access to education or other epistemic resources.\nFricker’s work also focuses on two less obvious forms of\nepistemic injustice. The first is testimonial injustice,\nwhich occurs when a speaker is given less credibility than she\ndeserves because the hearer has prejudices about a social group to\nwhich the speaker belongs. The second kind is hermeneutical\ninjustice. This occurs when, as a result of a group being\nsocially powerless, members of the group lack the conceptual resources\nto make sense of certain distinctive social experiences. For instance,\nbefore the 1970s, victims of sexual harassment had trouble\nunderstanding and describing the behavior of which they were the\nvictims, because the concept had not yet been articulated. Christopher\nHookway (2010) builds on Fricker’s work and argues that there\nare other forms of epistemic injustice that do not involve testimony\nor conceptual resources \nThese issues are relevant epistemological ones for those in democratic\nsocieties. Epistemic injustices may leave some members of society\nill-equipped to engage in the debates that fuel a well-functioning\ndemocracy. Testimonial injustice may prevent the spread of important\ninformation and perspectives through a community. \nAs we saw in the last section, misinformation can also pose epistemic\nthreats to democratic functioning. With respect to internet\nmisinformation, we might ask: do we have a right to protection against\nsuch misinformation? Is it morally acceptable, or even morally\nmandatory, for internet platforms, or government bodies, to protect\npublic belief by regulating and limiting misinformation? There are\ndeep political and moral issues here that we cannot possibly cover in\nthis entry. But we will note a fundamental tension that is relevant.\nFree speech is protected in most democratic societies, but part of the\ndefense of free speech by thinkers like Mill (1859 [1966]) is that it\nis crucial for freedom of thought. Once we recognize that human\nbeliefs are deeply social, and do not always follow Descartes’\nmodel of the individual, fully rational reasoner, we might acknowledge\nthat some sorts of speech interfere with our freedom of thought, and\nin some cases we may need to decide to protect one in lieu of the\nother.","contact.mail":"cailino@uci.edu","contact.domain":"uci.edu"}]
