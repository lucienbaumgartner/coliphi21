[{"date.published":"2000-09-16","date.changed":"2018-03-11","url":"https://plato.stanford.edu/entries/logic-classical/","author1":"Stewart Shapiro","author1.info":"https://philosophy.osu.edu/people/shapiro.4/","author2.info":"https://sites.google.com/site/teresakouri/","entry":"logic-classical","body.text":"\n\n\n\nTypically, a logic consists of a formal or informal language\ntogether with a deductive system and/or a model-theoretic semantics.\nThe language has components that correspond to a part of a natural language like\nEnglish or Greek. The deductive system is to capture, codify, or\nsimply record arguments that are valid for the given\nlanguage, and the semantics is to capture, codify, or record the\nmeanings, or truth-conditions for at least part of the language.\n\n\n The following sections provide the basics of a typical logic,\nsometimes called “classical elementary logic” or “classical\nfirst-order logic”. Section 2 develops a formal language, with a\nrigorous syntax and grammar. The formal language is a recursively\ndefined collection of strings on a fixed alphabet. As such, it has\nno meaning, or perhaps better, the meaning of its formulas is given\nby the deductive system and the semantics. Some of the symbols have\ncounterparts in ordinary language. We define an argument to\nbe a non-empty collection of sentences in the formal language, one of\nwhich is designated to be the conclusion. The other sentences (if\nany) in an argument are its premises. Section 3 sets up a deductive\nsystem for the language, in the spirit of natural deduction. An\nargument is derivable if there is a deduction from some or all of\nits premises to its conclusion. Section 4 provides a model-theoretic\nsemantics. An argument is valid if there is no\ninterpretation (in the semantics) in which its premises are all true\nand its conclusion false. This reflects the longstanding view that a\nvalid argument is truth-preserving.\n\n\n In Section 5, we turn to relationships between the deductive system\nand the semantics, and in particular, the relationship between\nderivability and validity. We show that an argument is derivable only\nif it is valid. This pleasant feature, called soundness,\nentails that no deduction takes one from true premises to a false\nconclusion. Thus, deductions preserve truth. Then we establish a converse, called\ncompleteness, that an argument is valid only if it is\nderivable. This shows that the deductive system is rich enough\nto provide a deduction for every valid argument. So there are enough\ndeductions: all and only valid arguments are derivable. We briefly\nindicate other features of the logic, some of which are corollaries to\nsoundness and completeness.\n\n\nThe final section, Section 6, is devoted to the a brief examination of\nthe philosophical position that classical logic is “the one right\nlogic”.\n\n\n\nToday, logic is a branch of mathematics and a branch of philosophy.\nIn most large universities, both departments offer courses in logic,\nand there is usually a lot of overlap between them. Formal languages,\ndeductive systems, and model-theoretic semantics are mathematical\nobjects and, as such, the logician is interested in their mathematical\nproperties and relations. Soundness, completeness, and most of the\nother results reported below are typical examples. Philosophically,\nlogic is at least closely related to the study of correct\nreasoning. Reasoning is an epistemic, mental activity. So logic\nis at least closely allied with epistemology. Logic is also a central\nbranch of computer science, due, in part, to interesting computational\nrelations in logical systems, and, in part, to the close connection\nbetween formal deductive argumentation and reasoning (see the entries\non\n recursive functions,\n computability and complexity, and\n philosophy of computer science). \nThis raises questions concerning the philosophical relevance of the\nvarious mathematical aspects of logic. How do deducibility and\nvalidity, as properties of formal languages--sets of strings on a\nfixed alphabet--relate to correct reasoning? What do the mathematical\nresults reported below have to do with the original philosophical\nissues concerning valid reasoning? This is an instance of the\nphilosophical problem of explaining how mathematics applies to\nnon-mathematical reality.  \n Typically, ordinary deductive reasoning takes place in a natural language, or\nperhaps a natural language augmented with some mathematical symbols. So\nour question begins with the relationship between a natural language and a\nformal language. Without attempting to be comprehensive, it may help to\nsketch several options on this matter. \n One view is that the formal languages accurately exhibit actual\nfeatures of certain fragments of a natural language. Some\nphilosophers claim that declarative sentences of natural language\nhave underlying logical forms and that these forms are\ndisplayed by formulas of a formal language. Other writers hold that\n(successful) declarative sentences express propositions; and\nformulas of formal languages somehow display the forms of these\npropositions. On views like this, the components of a logic provide\nthe underlying deep structure of correct reasoning. A chunk of\nreasoning in natural language is correct if the forms underlying the\nsentences constitute a valid or deducible argument. See for example,\nMontague [1974], Davidson [1984], Lycan [1984] (and the\nentry on \n logical form). \n Another view, held at least in part by Gottlob Frege and Wilhelm\nLeibniz, is that because natural languages are fraught with vagueness\nand ambiguity, they should be replaced by formal languages. A\nsimilar view, held by W. V. O. Quine (e.g., [1960], [1986]), is that a\nnatural language should be regimented, cleaned up for serious\nscientific and metaphysical work. One desideratum of the enterprise is\nthat the logical structures in the regimented language should be\ntransparent. It should be easy to “read off” the logical\nproperties of each sentence. A regimented language is similar to a\nformal language regarding, for example, the explicitly presented rigor\nof its syntax and its truth conditions. \n On a view like this, deducibility and validity represent\nidealizations of correct reasoning in natural language. A\nchunk of reasoning is correct to the extent that it corresponds to,\nor can be regimented by, a valid or deducible argument in a formal\nlanguage. \nWhen mathematicians and many philosophers engage in deductive\nreasoning, they occasionally invoke formulas in a formal language to\nhelp disambiguate, or otherwise clarify what they mean. In other\nwords, sometimes formulas in a formal language are used in\nordinary reasoning. This suggests that one might think of a formal\nlanguage as an\naddendum to a natural language. Then our present question\nconcerns the relationship between this addendum and the original\nlanguage. What do deducibility and validity, as sharply defined on\nthe addendum, tell us about correct deductive reasoning in general? \n Another view is that a formal language is a mathematical\nmodel of a natural language in roughly the same sense as, say, a\ncollection of point masses is a model of a system of physical objects,\nand the Bohr construction is a model of an atom. In other words, a\nformal language displays certain features of natural languages, or\nidealizations thereof, while ignoring or simplifying other\nfeatures. The purpose of mathematical models is to shed light on what\nthey are models of, without claiming that the model is accurate in all\nrespects or that the model should replace what it is a model of. On a\nview like this, deducibility and validity represent mathematical\nmodels of (perhaps different aspects of) correct reasoning in natural\nlanguages. Correct chunks of deductive reasoning correspond, more or\nless, to valid or deducible arguments; incorrect chunks of reasoning\nroughly correspond to invalid or non-deducible arguments. See, for\nexample, Corcoran [1973], Shapiro [1998], and Cook [2002]. \n There is no need to adjudicate this matter here. Perhaps the truth\nlies in a combination of the above options, or maybe some other\noption is the correct, or most illuminating one. We raise the matter\nonly to lend some philosophical perspective to the formal treatment\nthat follows. \n\nHere we develop the basics of a formal language, or to be precise, a\nclass of formal languages. Again, a formal language is a recursively\ndefined set of strings on a fixed alphabet. Some aspects of the\nformal languages correspond to, or have counterparts in, natural\nlanguages like English. Technically, this “counterpart relation” is\nnot part of the formal development, but we will mention it from time\nto time, to motivate some of the features and results. \n\nWe begin with analogues of singular terms, linguistic items\nwhose function is to denote a person or object. We call these\nterms. We assume a stock of individual constants.\nThese are lower-case letters, near the beginning of the Roman\nalphabet, with or without numerical subscripts: \n We envisage a potential infinity of individual constants. In the\npresent system each constant is a single character, and so individual\nconstants do not have an internal syntax. Thus we have an infinite\nalphabet. This could be avoided by taking a constant like\n\\(d_{22}\\), for example, to consist of three characters,\na lowercase “\\(d\\)” followed by a pair of subscript\n“2”s.  \n We also assume a stock of individual variables. These are\nlower-case letters, near the end of the alphabet, with or without\nnumerical subscripts: \nIn ordinary mathematical reasoning, there are two functions terms need\nto fulfill. We need to be able to denote specific, but unspecified (or\narbitrary) objects, and sometimes we need to express generality. In\nour system, we use some constants in the role of unspecified reference\nand variables to express generality. Both uses are recapitulated in\nthe formal treatment below. Some logicians employ different symbols\nfor unspecified objects (sometimes called “individual\nparameters”) and variables used to express generality.\n \nConstants and variables are the only terms in our formal language, so\nall of our terms are simple, corresponding to proper names and some\nuses of pronouns. We call a term closed if it is not a variable. In general, we use \\(v\\) to represent variables, and \\(t\\)\nto represent a closed term, an individual constant. Some authors also introduce function\nletters, which allow complex terms corresponding to:\n“\\(7+4\\)” and “the wife of Bill Clinton”, or\ncomplex terms containing variables, like “the father of\n\\(x\\)” and “\\(x/y\\)”. Logic books aimed at\nmathematicians are likely to contain function letters, probably due to\nthe centrality of functions in mathematical discourse. Books aimed at\na more general audience (or at philosophy students), may leave out\nfunction letters, since it simplifies the syntax and theory. We follow\nthe latter route here. This is an instance of a general tradeoff\nbetween presenting a system with greater expressive resources, at the\ncost of making its formal treatment more complex.\n \n For each natural number \\(n\\), we introduce a stock of\n\\(n\\)-place predicate letters. These are upper-case\nletters at the beginning or middle of the alphabet. A superscript\nindicates the number of places, and there may or may not be a\nsubscript. For example,  \nare three-place predicate letters. We often omit the superscript, when\nno confusion will result. We also add a special two-place predicate\nsymbol “\\(=\\)” for identity. \nZero-place predicate letters are sometimes called “sentence\nletters”. They correspond to free-standing sentences whose\ninternal structure does not matter. One-place predicate letters,\ncalled “monadic predicate letters”, correspond to\nlinguistic items denoting properties, like “being a man”,\n“being red”, or “being a prime\nnumber”. Two-place predicate letters, called “binary\npredicate letters”, correspond to linguistic items denoting\nbinary relations, like “is a parent of” or “is\ngreater than”. Three-place predicate letters correspond to\nthree-place relations, like “lies on a straight line\nbetween”. And so on.  \n The non-logical terminology of the language consists of its\nindividual constants and predicate letters. The symbol “\\(=\\)”, for\nidentity, is not a non-logical symbol. In taking identity to be\nlogical, we provide explicit treatment for it in the deductive system\nand in the model-theoretic semantics. Most authors do the same, but\nthere is some controversy over the issue (Quine [1986, Chapter\n5]). If \\(K\\) is a set of constants and predicate letters, then\nwe give the fundamentals of a language \\(\\LKe\\) \n built on this set of non-logical terminology. It may be called the\nfirst-order language with identity on \\(K\\). A similar\nlanguage that lacks the symbol for identity (or which takes identity\nto be non-logical) may be called\n \\(\\mathcal{L}1K\\),\nthe first-order language without identity on \\(K\\). \n\nIf \\(V\\) is an \\(n\\)-place predicate letter in \\(K\\),\nand \\(t_1, \\ldots,t_n\\)\nare terms of \\(K\\),\nthen \\(Vt_1 \\ldots t_n\\)\nis an atomic formula of\n \\(\\LKe\\). \n Notice that the terms \\(t_1, \\ldots,t_n\\) need not be distinct. Examples of\natomic formulas include: \n The last one is an analogue of a statement that a certain relation\n\\((A)\\) holds between three objects \\((a, b,\nc)\\). If \\(t_1\\) and \\(t_2\\) are\nterms, then \\(t_1 =t_2\\) is also an\natomic formula of\n \\(\\LKe\\). It corresponds\nto an assertion that \\(t_1\\) is identical to\n\\(t_2\\). \nIf an atomic formula has no variables, then it is called an\natomic sentence. If it does have variables, it is\ncalled open. In the above list of examples, the first and\nsecond are open; the rest are sentences.  \n\nWe now introduce the final items of the lexicon: \n We give a recursive definition of a formula of\n \\(\\LKe\\): \n A formula corresponding to \\(\\neg \\theta\\) thus says that it is not the\n case that \\(\\theta\\). The symbol “\\(\\neg\\)” is called\n “negation”, and is a unary connective. \n The ampersand “\\(\\amp\\)” corresponds to the English\n“and” (when “and” is used to connect\nsentences). So \\((\\theta \\amp \\psi)\\) can be read “\\(\\theta\\) and\n\\(\\psi\\)”. The formula \\((\\theta \\amp \\psi)\\) is called the\n“conjunction” of \\(\\theta\\) and \\(\\psi\\). \nThe symbol “\\(\\vee\\)” corresponds to “either …\nor … or both”, so \\((\\theta \\vee \\psi)\\) can be read\n“\\(\\theta\\) or \\(\\psi\\)”.  The formula \\((\\theta \\vee\n\\psi)\\) is called the “disjunction” of \\(\\theta\\) and\n\\(\\psi\\). \nThe arrow “\\(\\rightarrow\\)” roughly corresponds to\n“if … then … ”, so \\((\\theta \\rightarrow\n\\psi)\\) can be read “if \\(\\theta\\) then \\(\\psi\\)” or\n“\\(\\theta\\) only if \\(\\psi\\)”. \n The symbols “\\(\\amp\\)”, “\\(\\vee\\)”, and\n “\\(\\rightarrow\\)” are called “binary connectives”,\n since they serve to “connect” two formulas into\n one. Some authors introduce \\((\\theta \\leftrightarrow \\psi)\\) as an abbreviation\n of \\(((\\theta \\rightarrow \\psi) \\amp(\\psi \\rightarrow \\theta))\\). The symbol\n “\\(\\leftrightarrow\\)” is an analogue of the locution “if and\n only if”. \nThe symbol “\\(\\forall\\)” is called a universal\nquantifier, and is an analogue of “for all”; so\n\\(\\forall v\\theta\\) can be read “for all \\(v,\n\\theta\\)”. \n The symbol “\\(\\exists\\)” is called an\nexistential quantifier, and is an analogue of “there\nexists” or “there is”; so \\(\\exists v \\theta\\)\ncan be read “there is a \\(v\\) such that \\(\\theta\\)”. \n Clause (8) allows us to do inductions on the complexity of\nformulas. If a certain property holds of the atomic formulas and is\nclosed under the operations presented in clauses (2)–(7), then the\nproperty holds of all formulas. Here is a simple example: \n Theorem 1. Every formula of\n \\(\\LKe\\)\n has the same number of left and right parentheses. Moreover, each\nleft parenthesis corresponds to a unique right parenthesis, which\noccurs to the right of the left parenthesis. Similarly, each right\nparenthesis corresponds to a unique left parenthesis, which occurs to\nthe left of the given right parenthesis. If a parenthesis occurs\nbetween a matched pair of parentheses, then its mate also occurs\nwithin that matched pair. In other words, parentheses that occur\nwithin a matched pair are themselves matched.  \n Proof: By clause (8), every formula is built up\nfrom the atomic formulas using clauses (2)–(7). The atomic formulas\nhave no parentheses. Parentheses are introduced only in clauses (3)–(5), and\neach time they are introduced as a matched set. So at any stage in\nthe construction of a formula, the parentheses are paired off. \nWe next define the notion of an occurrence of a variable being\nfree or bound in a formula. A variable that\nimmediately follows a quantifier (as in “\\(\\forall x\\)”\nand “\\(\\exists y\\)”) is neither free nor bound. We do not\neven think of those as occurrences of the variable. All variables that\noccur in an atomic formula are free. If a variable occurs free (or\nbound) in \\(\\theta\\) or in \\(\\psi\\), then that same occurrence is free\n(or bound) in \\(\\neg \\theta, (\\theta \\amp \\psi), (\\theta \\vee \\psi)\\),\nand \\((\\theta \\rightarrow \\psi)\\). That is, the (unary and binary)\nconnectives do not change the status of variables that occur in\nthem. All occurrences of the variable \\(v\\) in \\(\\theta\\) are bound in\n\\(\\forall v \\theta\\) and \\(\\exists v \\theta\\). Any free\noccurrences of \\(v\\) in \\(\\theta\\) are bound by the initial\nquantifier. All other variables that occur in \\(\\theta\\) are free or\nbound in \\(\\forall v \\theta\\) and \\(\\exists v \\theta\\), as they are in\n\\(\\theta\\). \n For example, in the formula \n \\((\\forall\\)x(Axy\n \\(\\vee Bx) \\amp Bx)\\), the occurrences of “\\(x\\)” in \n Axy and in the first \\(Bx\\) are bound by the\nquantifier. The occurrence of “\\(y\\)” and last\noccurrence of “\\(x\\)” are free. In\n\\(\\forall x(Ax \\rightarrow \\exists\\)xBx), the\n“\\(x\\)” in \\(Ax\\) is bound by the initial\nuniversal quantifier, while the other occurrence of \\(x\\) is\nbound by the existential quantifier. The above syntax allows this\n“double-binding”. Although it does not create any\nambiguities (see below), we will avoid such formulas, as a matter of\ntaste and clarity. \nThe syntax also allows so-called vacuous binding, as in\n\\(\\forall\\)x\\(Bc\\). These, too, will be avoided in what follows.\nSome treatments of logic rule out vacuous binding and double binding\nas a matter of syntax. That simplifies some of the treatments below,\nand complicates others. \n Free variables correspond to place-holders, while bound variables\nare used to express generality. If a formula has no free variables,\nthen it is called a sentence. If a formula has free\nvariables, it is called open.  \n\nBefore turning to the deductive system and semantics, we mention a few\nfeatures of the language, as developed so far. This helps draw the\ncontrast between formal languages and natural languages like English.  \n We assume at the outset that all of the categories are disjoint. For\nexample, no connective is also a quantifier or a variable, and the\nnon-logical terms are not also parentheses or connectives. Also, the\nitems within each category are distinct. For example, the sign for\ndisjunction does not do double-duty as the negation symbol, and\nperhaps more significantly, no two-place predicate is also a\none-place predicate. \n One difference between natural languages like English and formal\nlanguages like\n \\(\\LKe\\) is that the latter are not\nsupposed to have any ambiguities. The policy that the different\ncategories of symbols do not overlap, and that no symbol does\ndouble-duty, avoids the kind of ambiguity, sometimes called\n“equivocation”, that occurs when a single word has two meanings:\n“I’ll meet you at the bank.” But there are other kinds of\nambiguity. Consider the English sentence: \n John is married, and Mary is single, or Joe is crazy.\n \n It can mean that John is married and either Mary is single or Joe is\ncrazy, or else it can mean that either both John is married and Mary\nis single, or else Joe is crazy. An ambiguity like this, due to\ndifferent ways to parse the same sentence, is sometimes called an\n“amphiboly”. If our formal language did not have the\nparentheses in it, it would have amphibolies. For example, there would\nbe a “formula” \\(A \\amp B \\vee\\)\nC. Is this supposed to be \\(((A \\amp B)\n\\vee C)\\), or is it \\((A \\amp(B \\vee C))\\)? The parentheses resolve what would be an\namphiboly. \n Can we be sure that there are no other amphibolies in our language?\nThat is, can we be sure that each formula of\n \\(\\LKe\\) can be put\ntogether in only one way? Our next task is to answer this question.  \n Let us temporarily use the term “unary marker” for the negation\nsymbol \\((\\neg)\\) or a quantifier followed by a variable (e.g.,\n \\(\\forall x,\n \\exists z)\\). \n Lemma 2. Each formula consists of a string of zero\nor more unary markers followed by either an atomic formula or a\nformula produced using a binary connective, via one of clauses\n(3)–(5).  \n Proof: We proceed by induction on the complexity of\nthe formula or, in other words, on the number of formation rules that\nare applied. The Lemma clearly holds for atomic formulas. Let \\(n\\) be\na natural number, and suppose that the Lemma holds for any formula\nconstructed from \\(n\\) or fewer instances of clauses\n(2)–(7). Let \\(\\theta\\) be a formula constructed from \\(n+1\\)\ninstances. The Lemma holds if the last clause used to construct\n\\(\\theta\\) was either (3), (4), or (5). If the last clause used to\nconstruct \\(\\theta\\) was (2), then \\(\\theta\\) is \\(\\neg \\psi\\).  Since\n\\(\\psi\\) was constructed with \\(n\\) instances of the rule, the Lemma\nholds for \\(\\psi\\) (by the induction hypothesis), and so it holds for\n\\(\\theta\\).  Similar reasoning shows the Lemma to hold for \\(\\theta\\)\nif the last clause was (6) or (7). By clause (8), this exhausts the\ncases, and so the Lemma holds for \\(\\theta\\), by induction. \n Lemma 3. If a formula\n \\(\\theta\\) contains a left parenthesis,\nthen it ends with a right parenthesis, which matches the leftmost left\nparenthesis in\n \\(\\theta\\). \n Proof: Here we also proceed by induction on the\nnumber of instances of (2)–(7) used to construct the\nformula. Clearly, the Lemma holds for atomic formulas, since they\nhave no parentheses. Suppose, then, that the Lemma holds for formulas\nconstructed with \\(n\\) or fewer instances of (2)–(7), and let\n \\(\\theta\\) be constructed with\n\\(n+1\\)\n instances. If the last clause applied was (3)–(5), then the Lemma\nholds since\n \\(\\theta\\) itself begins with a left\nparenthesis\n and ends with the matching right parenthesis. If the last clause\napplied was\n (2), then\n \\(\\theta\\) is\n \\(\\neg \\psi\\),\n and the induction hypothesis applies to\n \\(\\psi\\).\n Similarly, if the last clause applied was (6) or (7), then\n \\(\\theta\\)\n consists of a quantifier, a variable, and a formula to which we can\napply the\n induction hypothesis. It follows that the Lemma holds for\n \\(\\theta\\). \n Lemma 4. Each formula contains at least one atomic\nformula. \n The proof proceeds by induction on the number of instances of (2)–(7)\nused to construct the formula, and we leave it as an exercise.\n \n Theorem 5. Let \\(\\alpha, \\beta\\) be nonempty\nsequences of characters on our alphabet, such that \\(\\alpha \\beta\\)\n(i.e \\(\\alpha\\) followed by \\(\\beta)\\) is a formula. Then \\(\\alpha\\)\nis not a formula. \n Proof: By Theorem 1 and Lemma 3, if \\(\\alpha\\)\ncontains a left parenthesis, then the right parenthesis that matches\nthe leftmost left parenthesis in \\(\\alpha \\beta\\) comes at the end of\n\\(\\alpha \\beta\\), and so the matching right parenthesis is in\n\\(\\beta\\).  So, \\(\\alpha\\) has more left parentheses than right\nparentheses. By Theorem \\(1, \\alpha\\) is not a formula. So now suppose\nthat \\(\\alpha\\) does not contain any left parentheses. By Lemma \\(2,\n\\alpha \\beta\\) consists of a string of zero or more unary markers\nfollowed by either an atomic formula or a formula produced using a\nbinary connective, via one of clauses (3)–(5). If the latter\nformula was produced via one of clauses (3)–(5), then it begins\nwith a left parenthesis. Since \\(\\alpha\\) does not contain any\nparentheses, it must be a string of unary markers. But then \\(\\alpha\\)\ndoes not contain any atomic formulas, and so by Lemma \\(4, \\alpha\\) is\nnot a formula. The only case left is where \\(\\alpha \\beta\\) consists\nof a string of unary markers followed by an atomic formula, either in\nthe form \\(t_1 =t_2\\) or \\(Pt_1 \\ldots t_n\\).  Again, if \\(\\alpha\\)\njust consisted of unary markers, it would not be a formula, and so\n\\(\\alpha\\) must consist of the unary markers that start \\(\\alpha\n\\beta\\), followed by either \\(t_1\\) by itself, \\(t_1 =\\) by itself, or\nthe predicate letter \\(P\\), and perhaps some (but not all) of the\nterms \\(t_1, \\ldots,t_n\\).  In the first two cases, \\(\\alpha\\) does\nnot contain an atomic formula, by the policy that the categories do\nnot overlap. Since \\(P\\) is an \\(n\\)-place predicate letter, by the\npolicy that the predicate letters are distinct, \\(P\\) is not an\n\\(m\\)-place predicate letter for any \\(m \\ne n\\). So the part of\n\\(\\alpha\\) that consists of \\(P\\) followed by the terms is not an\natomic formula. In all of these cases, then, \\(\\alpha\\) does not\ncontain an atomic formula. By Lemma \\(4, \\alpha\\) is not a\nformula. We are finally in position to show that there is no amphiboly in our\nlanguage. \n Theorem 6. Let \\(\\theta\\) be any formula of\n\\(\\LKe\\).  If \\(\\theta\\) is not atomic, then there is one and only one\namong (2)–(7) that was the last clause applied to construct\n\\(\\theta\\).  That is, \\(\\theta\\) could not be produced by two\ndifferent clauses. Moreover, no formula produced by clauses\n(2)–(7) is atomic. \n Proof: By Clause (8), either \\(\\theta\\) is atomic or\nit was produced by one of clauses (2)–(7). Thus, the first\nsymbol in \\(\\theta\\) must be either a predicate letter, a term, a\nunary marker, or a left parenthesis. If the first symbol in \\(\\theta\\)\nis a predicate letter or term, then \\(\\theta\\) is atomic. In this\ncase, \\(\\theta\\) was not produced by any of (2)–(7), since all\nsuch formulas begin with something other than a predicate letter or\nterm. If the first symbol in \\(\\theta\\) is a negation sign\n“\\(\\neg\\)”, then was \\(\\theta\\) produced by clause (2),\nand not by any other clause (since the other clauses produce formulas\nthat begin with either a quantifier or a left parenthesis). Similarly,\nif \\(\\theta\\) begins with a universal quantifier, then it was produced\nby clause (6), and not by any other clause, and if \\(\\theta\\) begins\nwith an existential quantifier, then it was produced by clause (7),\nand not by any other clause. The only case left is where \\(\\theta\\)\nbegins with a left parenthesis. In this case, it must have been\nproduced by one of (3)–(5), and not by any other clause. We only\nneed to rule out the possibility that \\(\\theta\\) was produced by more\nthan one of (3)–(5). To take an example, suppose that \\(\\theta\\)\nwas produced by (3) and (4). Then \\(\\theta\\) is \\((\\psi_1 \\amp\n\\psi_2)\\) and \\(\\theta\\) is also \\((\\psi_3 \\vee \\psi_4)\\), where\n\\(\\psi_1, \\psi_2, \\psi_3\\), and \\(\\psi_4\\) are themselves\nformulas. That is, \\((\\psi_1 \\amp \\psi_2)\\) is the very same formula\nas \\((\\psi_3 \\vee \\psi_4)\\). By Theorem \\(5, \\psi_1\\) cannot be a\nproper part of \\(\\psi_3\\), nor can \\(\\psi_3\\) be a proper part of\n\\(\\psi_1\\). So \\(\\psi_1\\) must be the same formula as \\(\\psi_3\\). But\nthen “\\(\\amp\\)” must be the same symbol as\n“\\(\\vee\\)”, and this contradicts the policy that all of\nthe symbols are different. So \\(\\theta\\) was not produced by both\nClause (3) and Clause (4). Similar reasoning takes care of the other\ncombinations. \n This result is sometimes called “unique readability”. It\nshows that each formula is produced from the atomic formulas via the\nvarious clauses in exactly one way. If \\(\\theta\\) was produced by\nclause (2), then its main connective is the initial\n“\\(\\neg\\)”. If \\(\\theta\\) was produced by clauses (3),\n(4), or (5), then its main connective is the introduced\n“\\(\\amp\\)”, “\\(\\vee\\)”, or\n“\\(\\rightarrow\\)”, respectively. If \\(\\theta\\) was\nproduced by clauses (6) or (7), then its main connective is\nthe initial quantifier. We apologize for the tedious details. We\nincluded them to indicate the level of precision and rigor for the\nsyntax. \n\nWe now introduce a deductive system, \\(D\\), for our\nlanguages. As above, we define an argument to be a non-empty\ncollection of sentences in the formal language, one of which is\ndesignated to be the conclusion. If there are any other\nsentences in the argument, they are its\n premises.[1]\n By convention, we use “\\(\\Gamma\\)”,\n“\\(\\Gamma'\\)”, “\\(\\Gamma_1\\)”, etc, to range\nover sets of sentences, and we use the letters “\\(\\phi\\)”,\n“\\(\\psi\\)”, “\\(\\theta\\)”, uppercase or\nlowercase, with or without subscripts, to range over single\nsentences. We write “\\(\\Gamma, \\Gamma'\\)” for the union of\n\\(\\Gamma\\) and \\(\\Gamma'\\), and “\\(\\Gamma, \\phi\\)” for the\nunion of \\(\\Gamma\\) with \\(\\{\\phi\\}\\). \nWe write an argument in the form \\(\\langle \\Gamma, \\phi \\rangle\\),\nwhere \\(\\Gamma\\) is a set of sentences, the premises, and \\(\\phi\\) is\na single sentence, the conclusion. Remember that \\(\\Gamma\\) may be\nempty. We write \\(\\Gamma \\vdash \\phi\\) to indicate that \\(\\phi\\) is\ndeducible from \\(\\Gamma\\), or, in other words, that the argument\n\\(\\langle \\Gamma, \\phi \\rangle\\) is deducible in \\(D\\). We may write\n\\(\\Gamma \\vdash_D \\phi\\) to emphasize the deductive system \\(D\\). We\nwrite \\(\\vdash \\phi\\) or \\(\\vdash_D \\phi\\) to indicate that \\(\\phi\\)\ncan be deduced (in \\(D)\\) from the empty set of premises. \n The rules in \\(D\\) are chosen to match logical relations\nconcerning the English analogues of the logical terminology in the\nlanguage. Again, we define the deducibility relation by recursion. We\nstart with a rule of assumptions:\n  \nWe thus have that \\(\\{\\phi \\}\\vdash \\phi\\); each premise follows\nfrom itself. We next present two clauses for each connective and\nquantifier. The clauses indicate how to “introduce” and\n“eliminate” sentences in which each symbol is the main\nconnective.  \nFirst, recall that “\\(\\amp\\)” is an analogue of the\nEnglish connective “and”. Intuitively, one can deduce a\nsentence in the form \\((\\theta \\amp \\psi)\\) if one has deduced\n\\(\\theta\\) and one has deduced \\(\\psi\\). Conversely, one can deduce\n\\(\\theta\\) from \\((\\theta \\amp \\psi)\\) and one can deduce \\(\\psi\\)\nfrom \\((\\theta \\amp \\psi)\\): \nThe name “&I” stands for\n“&-introduction”; “&E” stands for\n“&-elimination”. \nSince, the symbol “\\(\\vee\\)” corresponds to the English\n“or”, \\((\\theta \\vee \\psi)\\) should be deducible from\n\\(\\theta\\), and \\((\\theta \\vee \\psi)\\) should also be deducible from\n\\(\\psi\\):\n \nThe elimination rule is a bit more complicated. Suppose that\n“\\(\\theta\\) or \\(\\psi\\)” is true. Suppose also that\n\\(\\phi\\) follows from \\(\\theta\\) and that \\(\\phi\\) follows from\n\\(\\psi\\). One can reason that if \\(\\theta\\) is true, then \\(\\phi\\) is\ntrue. If instead \\(\\psi\\) is true, we still have that \\(\\phi\\) is\ntrue.  So either way, \\(\\phi\\) must be true. \n For the next clauses, recall that the symbol, “\\(\\rightarrow\\)”, is\n an analogue of the English “if … then … ”\n construction. If one knows, or assumes \\((\\theta \\rightarrow \\psi)\\) and\n also knows, or assumes \\(\\theta\\), then one can conclude\n \\(\\psi\\). Conversely, if one deduces \\(\\psi\\) from an assumption \\(\\theta\\),\n then one can conclude that \\((\\theta \\rightarrow \\psi)\\). \nThis elimination rule is sometimes called “modus ponens”.\nIn some logic texts, the introduction rule is proved as a\n“deduction theorem”. \nOur next clauses are for the negation sign, “\\(\\neg\\)”. The\nunderlying idea is that a sentence \\(\\psi\\) is inconsistent with its\nnegation \\(\\neg \\psi\\). They cannot both be true. We call a pair of\nsentences \\(\\psi, \\neg \\psi\\) contradictory opposites. If one\ncan deduce such a pair from an assumption \\(\\theta\\), then one can\nconclude that \\(\\theta\\) is false, or, in other words, one can conclude\n\\(\\neg \\theta\\). \nBy (As), we have that \\(\\{A,\\neg A\\}\\vdash A\\) and\n\\(\\{\\)A,\\(\\neg\\)A\\(\\}\\vdash \\neg A\\). So by \\(\\neg\\)I we have\nthat \\(\\{A\\}\\vdash \\neg \\neg A\\). However, we do not have the converse\nyet. Intuitively, \\(\\neg \\neg \\theta\\) corresponds to “it is not\nthe case that it is not the case that” . One might think that\nthis last is equivalent to \\(\\theta\\), and we have a rule to that\neffect:\n \nThe name DNE stands for “double-negation\nelimination”. There is some controversy over this inference. It\nis rejected by philosophers and mathematicians who do not hold that\neach meaningful sentence is either true or not\ntrue. Intuitionistic logic does not sanction the inference in\nquestion (see, for example Dummett [2000], or the entry on\n intuitionistic logic, or\n history of intuitionistic logic),\nbut, again, classical logic does. \n To illustrate the parts of the deductive system \\(D\\) presented\nthus far, we show that \\(\\vdash(A \\vee \\neg A)\\): \nThe principle \\((\\theta \\vee \\neg \\theta)\\) is sometimes called the \nlaw of excluded middle. It is not valid in intuitionistic\nlogic.  Let \\(\\theta, \\neg \\theta\\) be a pair of contradictory opposites,\nand let \\(\\psi\\) be any sentence at all. By (As) we have \\(\\{\\theta,\n\\neg \\theta, \\neg \\psi \\}\\vdash \\theta\\) and \\(\\{\\theta, \\neg \\theta,\n\\neg \\psi \\}\\vdash \\neg \\theta\\). So by \\((\\neg\\)I), \\(\\{\\theta, \\neg\n\\theta \\}\\vdash \\neg \\neg \\psi\\). So, by (DNE) we have \\(\\{\\theta ,\n\\neg \\theta \\}\\vdash \\psi\\) . That is, anything at all follows from a\npair of contradictory opposites. Some logicians introduce a rule to\ncodify a similar inference:  \nIf \\(\\Gamma_1 \\vdash \\theta\\) and \\(\\Gamma_2 \\vdash \\neg \\theta\\),\nthen for any sentence \\(\\psi, \\Gamma_1, \\Gamma_2 \\vdash \\psi\\)\n \nThe inference is sometimes called ex falso quodlibet or, more\ncolorfully, explosion. Some call it\n“\\(\\neg\\)-elimination”, but perhaps this stretches the notion\nof “elimination” a bit. We do not officially\ninclude ex falso quodlibet as a separate rule in \\(D\\),\nbut as will be shown below (Theorem 10), each instance of it is\nderivable in our system \\(D\\). \n Some logicians object to ex falso quodlibet, on the ground\nthat the sentence \\(\\psi\\) may be irrelevant to any of the\npremises in \\(\\Gamma\\). Suppose, for example, that one starts with some\npremises \\(\\Gamma\\) about human nature and facts about certain people,\nand then deduces both the sentence “Clinton had extra-marital\nsexual relations” and “Clinton did not have extra-marital\nsexual relations”. One can perhaps conclude that there is\nsomething wrong with the premises \\(\\Gamma\\). But should we be allowed to\nthen deduce anything at all from \\(\\Gamma\\)? Should we be\nallowed to deduce “The economy is sound”? A small minority of logicians, called dialetheists, hold\nthat some contradictions are actually true. For them, ex falso\nquodlibet is not truth-preserving. \nDeductive systems that demur from ex falso quodlibet are\ncalled paraconsistent. Most relevant logics are\nparaconsistent. See the entries on \n relevance logic, \n paraconsistent logic,\n and\n dialetheism. \nOr see Anderson and Belnap [1975], Anderson, Belnap, and Dunn [1992],\nand Tennant [1997] for fuller overviews of relevant logic; and Priest\n[2006],[2006a] for dialetheism. Deep philosophical issues concerning\nthe nature of \n logical consequence\n are involved. Far be it for\nan article in a philosophy encyclopedia to avoid philosophical issues,\nbut space considerations preclude a fuller treatment of this issue\nhere. Suffice it to note that the inference ex falso\nquodlibet is sanctioned in systems of classical logic,\nthe subject of this article. It is essential to establishing the\nbalance between the deductive system and the semantics (see §5\nbelow). \n The next pieces of \\(D\\) are the clauses for the quantifiers. Let\n \\(\\theta\\) be a formula, \\(v\\) a\nvariable, and \\(t\\) a term (i.e., a variable or a constant). Then\ndefine\n \\(\\theta(v|t)\\) to be\nthe result of substituting \\(t\\) for each free occurrence of\n\\(v\\) in\n \\(\\theta\\). So, if\n \\(\\theta\\) is \\((Qx \\amp \\exists\\)xPxy), then\n \\(\\theta(x|c)\\) is\n\\((Qc \\amp \\exists\\)xPxy). The last\noccurrence of \\(x\\) is not free. \nA sentence in the form \\(\\forall v \\theta\\) is an analogue of the\nEnglish “for every \\(v, \\theta\\) holds”. So one should be\nable to infer \\(\\theta(v|t)\\) from \\(\\forall v \\theta\\) for any closed\nterm \\(t\\). Recall that the only closed terms in our system are\nconstants. \nThe idea here is that if \\(\\forall v \\theta\\) is true, then \\(\\theta\\)\nshould hold of \\(t\\), no matter what \\(t\\) is. \nThe introduction clause for the universal quantifier is a bit more\ncomplicated. Suppose that a sentence \\(\\theta\\) contains a closed term\n\\(t\\), and that \\(\\theta\\) has been deduced from a set of premises\n\\(\\Gamma\\). If the closed term \\(t\\) does not occur in any member of\n\\(\\Gamma\\), then \\(\\theta\\) will hold no matter which object \\(t\\) may\ndenote. That is, \\(\\forall v \\theta\\) follows.  \nThis rule \\((\\forall \\mathbf{I})\\) corresponds to a common inference in\nmathematics. Suppose that a mathematician says “let \\(n\\)\nbe a natural number” and goes on to show that \\(n\\) has a\ncertain property \\(P\\), without assuming anything\nabout \\(n\\) (except that it is a natural number). She then\nreminds the reader that \\(n\\) is “arbitrary”, and\nconcludes that \\(P\\) holds for all natural numbers. The\ncondition that the term\n\\(t\\) not occur in any premise is what guarantees that it is\nindeed “arbitrary”. It could be any object, and so anything we\nconclude about it holds for all objects.  \n The existential quantifier is an analogue of the English expression\n“there exists”, or perhaps just “there is”. If\nwe have established (or assumed) that a given object \\(t\\) has a\ngiven property, then it follows that there is something that has that\nproperty.   \nThe elimination rule for \\(\\exists\\) is not quite as simple: \n This elimination rule also corresponds to a common inference. Suppose\nthat a mathematician assumes or somehow concludes that there is a\nnatural number with a given property \\(P\\). She then says “let\n\\(n\\) be such a natural number, so that \\(Pn\\)”, and goes on to\nestablish a sentence \\(\\phi\\), which does not mention the number\n\\(n\\). If the derivation of \\(\\phi\\) does not invoke anything about\n\\(n\\) (other than the assumption that it has the given property\n\\(P)\\), then \\(n\\) could have been any number that has the property\n\\(P\\). That is, \\(n\\) is an arbitrary number with property\n\\(P\\). It does not matter which number \\(n\\) is. Since\n\\(\\phi\\) does not mention \\(n\\), it follows from the assertion that\nsomething has property \\(P\\). The provisions added to \\((\\exists\\)E)\nare to guarantee that \\(t\\) is “arbitrary”. \n The final items are the rules for the identity sign “=”. The\nintroduction rule is about a simple as can be: \n This “inference” corresponds to the truism that everything is\nidentical to itself. The elimination rule corresponds to a principle\nthat if \\(a\\) is identical to \\(b\\), then anything true of\n\\(a\\) is also true of \\(b\\).  \n The rule \\(({=}\\mathrm{E})\\) indicates a certain restriction in the\nexpressive resources of our language. Suppose, for example, that Harry\nis identical to Donald (since his mischievous parents gave him two\nnames). According to most people’s intuitions, it would not\nfollow from this and “Dick knows that Harry is wicked”\nthat “Dick knows that Donald is wicked”, for the reason\nthat Dick might not know that Harry is identical to Donald. Contexts\nlike this, in which identicals cannot safely be substituted for each\nother, are called “opaque”. We assume that our language\n\\(\\LKe\\) has no opaque contexts. \n One final clause completes the description of the deductive system\n\\(D\\): \n Again, this clause allows proofs by induction on the rules used to\nestablish an argument. If a property of arguments holds of all\ninstances of (As) and \\(({=}\\mathrm{I})\\), and if the other rules\npreserve the property, then every argument that is deducible in \\(D\\)\nenjoys the property in question. \nBefore moving on to the model theory for \\(\\LKe\\), we pause to note a\nfew features of the deductive system. To illustrate the level of\nrigor, we begin with a lemma that if a sentence does not contain a\nparticular closed term, we can make small changes to the set of\nsentences we prove it from without problems. We allow ourselves the\nliberty here of extending some previous notation: for any terms \\(t\\)\nand \\(t'\\), and any formula \\(\\theta\\), we say that \\(\\theta(t|t')\\)\nis the result of replacing all free occurrences of \\(t\\) in \\(\\theta\\)\nwith \\(t'\\). \nLemma 7.  If \\(\\Gamma_1\\) and \\(\\Gamma_2\\) differ\nonly in that wherever \\(\\Gamma_1\\) contains \\(\\theta\\), \\(\\Gamma_2\\)\ncontains \\(\\theta(t|t')\\), then for any sentence \\(\\phi\\) not\ncontaining \\(t\\) or \\(t'\\), if \\(\\Gamma_1\\vdash\\phi\\) then\n\\(\\Gamma_2\\vdash\\phi\\).\n  \nProof: The proof proceeds by induction on the number\nof steps in the proof of \\(\\phi\\). Crucial to this proof is the fact\nthat \\(\\theta=\\theta(t|t')\\) whenever \\(\\theta\\) does not contain\n\\(t\\) or \\(t'\\). When the number of steps in the proof of \\(\\phi\\) is\none, this means that the last (and only) rule applied is (As) or\n(=I). Then, since \\(\\phi\\) does not contain \\(t\\) or \\(t'\\), if\n\\(\\Gamma_1\\vdash\\phi\\) we simply apply the same rule ((As) or (=I)) to\n\\(\\Gamma_2\\) to get \\(\\Gamma_2\\vdash\\phi\\). Assume that there are\n\\(n>1\\) steps in the proof of \\(\\phi\\), and that Lemma 7 holds for any\nproof with less than \\(n\\) steps. Suppose that the \\(n^{th}\\) rule\napplied to \\(\\Gamma_1\\) was (\\(\\amp I\\)). Then \\(\\phi\\) is\n\\(\\psi\\amp\\chi\\), and \\(\\Gamma_1\\vdash\\phi\\amp\\chi\\). But then we know\nthat previous steps in the proof include \\(\\Gamma_1\\vdash\\psi\\) and\n\\(\\Gamma_1\\vdash\\chi\\), and by induction, we have\n\\(\\Gamma_2\\vdash\\psi\\) and \\(\\Gamma_2\\vdash\\chi\\), since neither\n\\(\\psi\\) nor \\(\\chi\\) contain \\(t\\) or \\(t'\\). So, we simply apply\n(\\(\\amp I\\)) to \\(\\Gamma_2\\) to get \\(\\Gamma_2\\vdash\\psi\\amp\\chi\\) as\nrequired. Suppose now that the last step applied in the proof of\n\\(\\Gamma_1\\vdash\\phi\\) was (\\(\\amp E\\)). Then, at a previous step in\nthe proof of \\(\\phi\\), we know \\(\\Gamma_1\\vdash\\phi\\amp\\psi\\) for some\nsentence \\(\\psi\\). If \\(\\psi\\) does not contain \\(t\\), then we simply\napply (\\(\\amp E\\)) to \\(\\Gamma_2\\) to obtain the desired result. The\nonly complication is if \\(\\psi\\) contains \\(t\\). Then we would have\nthat \\(\\Gamma_2\\vdash (\\phi\\amp\\psi)(t|t')\\). But, since\n\\((\\phi\\amp\\psi)(t|t')\\) is \\(\\phi(t|t')\\amp\\psi(t|t')\\), and\n\\(\\phi(t|t')\\) is just \\(\\phi\\), we can just apply (\\(\\amp E\\)) to get\n\\(\\Gamma_2\\vdash\\phi\\) as required. The cases for the other rules are\nsimilar. \nTheorem 8. The rule of Weakening. If \\(\\Gamma_1\n\\vdash \\phi\\) and \\(\\Gamma_1 \\subseteq \\Gamma_2\\), then \\(\\Gamma_2\n\\vdash \\phi\\). \nProof: Again, we proceed by induction on the number\nof rules that were used to arrive at \\(\\Gamma_1 \\vdash \\phi\\).\nSuppose that \\(n\\gt 0\\) is a natural number, and that the theorem\nholds for any argument that was derived using fewer than \\(n\\)\nrules. Suppose that \\(\\Gamma_1 \\vdash \\phi\\) using exactly \\(n\\)\nrules. If \\(n=1\\), then the rule is either (As) or \\((=\\)I). In these\ncases, \\(\\Gamma_2 \\vdash \\phi\\) by the same rule. If the last rule\napplied was (&I), then \\(\\phi\\) has the form \\((\\theta \\amp\n\\psi)\\), and we have \\(\\Gamma_3 \\vdash \\theta\\) and \\(\\Gamma_4 \\vdash\n\\psi\\), with \\(\\Gamma_1 = \\Gamma_3, \\Gamma_4\\).  We apply the\ninduction hypothesis to the deductions of \\(\\theta\\) and \\(\\psi\\), to\nget \\(\\Gamma_2 \\vdash \\theta\\) and \\(\\Gamma_2 \\vdash \\psi\\).  and then\napply (&I) to the result to get \\(\\Gamma_2 \\vdash \\phi\\).  Most of\nthe other cases are exactly like this. Slight complications arise only\nin the rules \\((\\forall\\)I) and \\((\\exists\\)E), because there we have\nto pay attention to the conditions for the rules. \nSuppose that the last rule applied to get \\(\\Gamma_1 \\vdash \\phi\\) is\n\\((\\forall\\)I). So \\(\\phi\\) is a sentence of the form \\(\\forall\nv\\theta\\), and we have \\(\\Gamma_1 \\vdash \\theta (v|t)\\) and \\(t\\) does\noccur in any member of \\(\\Gamma_1\\) or in \\(\\theta\\).  The problem is\nthat \\(t\\) may occur in a member of \\(\\Gamma_2\\), and so we cannot\njust invoke the induction hypothesis and apply \\((\\forall\\)I) to the\nresult. So, let \\(t'\\) be a term not occurring in any sentence in\n\\(\\Gamma_2\\). Let \\(\\Gamma'\\) be the result of substituting \\(t'\\) for\nall \\(t\\) in \\(\\Gamma_2\\). Then, since \\(t\\) does not occur in\n\\(\\Gamma_1\\), \\(\\Gamma_1\\subseteq\\Gamma'\\). So, the induction\nhypothesis gives us \\(\\Gamma'\\vdash\\theta (v|t)\\), and we know that\n\\(\\Gamma'\\) does not contain \\(t\\), so we can apply (\\(\\forall I\\)) to\nget \\(\\Gamma'\\vdash\\forall v\\theta\\). But \\(\\forall v\\theta\\) does not\ncontain \\(t\\) or \\(t'\\), so \\(\\Gamma_2\\vdash\\forall v\\theta\\) by Lemma\n7. \nSuppose that the last rule applied was \\((\\exists\\)E), we have\n\\(\\Gamma_3 \\vdash \\exists v\\theta\\) and \\(\\Gamma_4, \\theta (v|t)\n\\vdash \\phi\\), with \\(\\Gamma_1\\) being \\(\\Gamma_3, \\Gamma_4\\), and\n\\(t\\) not in \\(\\phi\\), \\(\\Gamma_4\\) or \\(\\theta\\).  If \\(t\\) does not\noccur free in \\(\\Gamma_2\\), we apply the induction hypothesis to get\n\\(\\Gamma_2 \\vdash \\exists v\\theta\\), and then \\((\\exists\\)E) to end up\nwith \\(\\Gamma_2 \\vdash \\phi\\). If \\(t\\) does occur free in\n\\(\\Gamma_2\\), then we follow a similar proceedure to \\(\\forall I\\),\nusing Lemma 7.  \n Theorem 8 allows us to add on premises at will. It follows that\n \\(\\Gamma \\vdash \\phi\\) if and only if there is a subset\n \\(\\Gamma'\\subseteq \\Gamma\\) such that\n \\(\\Gamma'\\vdash \\phi\\). Some systems of relevant\n logic do not have weakening, nor does substructural logic (See the\n entries on\n relevance logic,\n substructural logics,\nand\n linear logic). \nBy clause (*), all derivations are established in a finite number of steps. \nSo we have \nTheorem 9.  \\(\\Gamma \\vdash \\phi\\) if and only if\nthere is a finite \\(\\Gamma'\\subseteq \\Gamma\\) such that\n\\(\\Gamma'\\vdash \\phi\\).  \nTheorem 10. The rule of ex falso quodlibet\nis a “derived rule” of \\(D\\): if \\(\\Gamma_1 \\vdash\n\\theta\\) and \\(\\Gamma_2 \\vdash \\neg \\theta\\), then \\(\\Gamma_1,\\Gamma_2\n\\vdash \\psi\\), for any sentence \\(\\psi\\).  \nProof: Suppose that \\(\\Gamma_1 \\vdash \\theta\\) and\n\\(\\Gamma_2 \\vdash \\neg \\theta\\). Then by Theorem \\(8, \\Gamma_1,\\neg\n\\psi \\vdash \\theta\\), and \\(\\Gamma_2,\\neg \\psi \\vdash \\neg\n\\theta\\). So by \\((\\neg\\)I), \\(\\Gamma_1, \\Gamma_2 \\vdash \\neg \\neg\n\\psi\\). By (DNE), \\(\\Gamma_1, \\Gamma_2 \\vdash \\psi\\).  \nTheorem 11. The rule of Cut.  If \\(\\Gamma_1 \\vdash\n\\psi\\) and \\(\\Gamma_2, \\psi \\vdash \\theta\\), then \\(\\Gamma_1, \\Gamma_2\n\\vdash \\theta\\). \nProof: Suppose \\(\\Gamma_1 \\vdash \\psi\\) and\n\\(\\Gamma_2, \\psi \\vdash \\theta\\).  We proceed by induction on the\nnumber of rules used to establish \\(\\Gamma_2, \\psi \\vdash \\theta\\).\nSuppose that \\(n\\) is a natural number, and that the theorem holds for\nany argument that was derived using fewer than \\(n\\) rules. Suppose\nthat \\(\\Gamma_2, \\psi \\vdash \\theta\\) was derived using exactly \\(n\\)\nrules. If the last rule used was \\((=\\)I), then \\(\\Gamma_1, \\Gamma_2\n\\vdash \\theta\\) is also an instance of \\((=\\)I). If \\(\\Gamma_2, \\psi\n\\vdash \\theta\\) is an instance of (As), then either \\(\\theta\\) is\n\\(\\psi\\), or \\(\\theta\\) is a member of \\(\\Gamma_2\\). In the former\ncase, we have \\(\\Gamma_1 \\vdash \\theta\\) by supposition, and get\n\\(\\Gamma_1, \\Gamma_2 \\vdash \\theta\\) by Weakening (Theorem 8).  In the\nlatter case, \\(\\Gamma_1, \\Gamma_2 \\vdash \\theta\\) is itself an\ninstance of (As). Suppose that \\(\\Gamma_2, \\psi \\vdash \\theta\\) was\nobtained using (&E). Then we have \\(\\Gamma_2, \\psi \\vdash(\\theta\n\\amp \\phi)\\).  The induction hypothesis gives us \\(\\Gamma_1, \\Gamma_2\n\\vdash(\\theta \\amp \\phi)\\), and (&E) produces \\(\\Gamma_1, \\Gamma_2\n\\vdash \\theta\\).  The remaining cases are similar.  \nTheorem 11 allows us to chain together inferences. This fits the\npractice of establishing theorems and lemmas and then using those\ntheorems and lemmas later, at will. The cut principle is, some think,\nessential to reasoning. In some logical systems, the cut principle is\na deep theorem; in others it is invalid. The system here was designed,\nin part, to make the proof of Theorem 11 straightforward. \n If \\(\\Gamma \\vdash_D \\theta\\), then\n we say that the sentence \\(\\theta\\) is a deductive consequence\n of the set of sentences \\(\\Gamma\\), and that the argument\n \\(\\langle \\Gamma,\\theta \\rangle\\) is deductively valid. A sentence\n \\(\\theta\\) is a logical theorem, or a deductive logical\n truth,\n if \\(\\vdash_D \\theta\\). That is,\n \\(\\theta\\) is a logical theorem if it is a deductive consequence of the\n empty set. A set \\(\\Gamma\\) of sentences is consistent if there\n is no sentence \\(\\theta\\) such that\n \\(\\Gamma \\vdash_D \\theta\\) and\n \\(\\Gamma \\vdash_D \\neg \\theta\\).\n That is, a set is consistent if it does not entail a pair of\n contradictory opposite sentencess. \nTheorem 12. A set \\(\\Gamma\\) is consistent if and\nonly if there is a sentence \\(\\theta\\) such that it is not the case\nthat \\(\\Gamma \\vdash \\theta\\). \nProof: Suppose that \\(\\Gamma\\) is consistent and let\n\\(\\theta\\) be any sentence. Then either it is not the case that\n\\(\\Gamma \\vdash \\theta\\) or it is not the case that \\(\\Gamma \\vdash\n\\neg \\theta\\).  For the converse, suppose that \\(\\Gamma\\) is\ninconsistent and let \\(\\psi\\) be any sentence. We have that there is a\nsentence such that both \\(\\Gamma \\vdash \\theta\\) and \\(\\Gamma \\vdash\n\\neg \\theta\\). By ex falso quodlibet (Theorem 10), \\(\\Gamma\n\\vdash \\psi\\).  \nDefine a set \\(\\Gamma\\) of sentences of the language \\(\\LKe\\) to\nbe maximally consistent if \\(\\Gamma\\) is consistent and for\nevery sentence \\(\\theta\\) of \\(\\LKe\\), if \\(\\theta\\) is not in\n\\(\\Gamma\\), then \\(\\Gamma,\\theta\\) is inconsistent. In other words,\n\\(\\Gamma\\) is maximally consistent if \\(\\Gamma\\) is consistent, and\nadding any sentence in the language not already in \\(\\Gamma\\) renders\nit inconsistent. Notice that if \\(\\Gamma\\) is maximally consistent\nthen \\(\\Gamma \\vdash \\theta\\) if and only if \\(\\theta\\) is in\n\\(\\Gamma\\). \nTheorem 13. The Lindenbaum Lemma.  Let \\(\\Gamma\\) be\nany consistent set of sentences of \\(\\LKe .\\) Then there is a set\n\\(\\Gamma'\\) of sentences of \\(\\LKe\\) such that \\(\\Gamma \\subseteq\n\\Gamma'\\) and \\(\\Gamma'\\) is maximally consistent. \nProof: Although this theorem holds in general, we\nassume here that the set \\(K\\) of non-logical terminology is either\nfinite or denumerably infinite (i.e., the size of the natural numbers,\nusually called \\(\\aleph_0)\\).  It follows that there is an enumeration\n\\(\\theta_0, \\theta_1,\\ldots\\) of the sentences of \\(\\LKe\\), such that\nevery sentence of \\(\\LKe\\) eventually occurs in the list. Define a\nsequence of sets of sentences, by recursion, as follows: \\(\\Gamma_0\\)\nis \\(\\Gamma\\); for each natural number \\(n\\), if \\(\\Gamma_n,\n\\theta_n\\) is consistent, then let \\(\\Gamma_{n+1} = \\Gamma_n,\n\\theta_n\\). Otherwise, let \\(\\Gamma_{n+1} = \\Gamma_n\\). Let\n\\(\\Gamma'\\) be the union of all of the sets \\(\\Gamma_n\\). Intuitively,\nthe idea is to go through the sentences of \\(\\LKe\\), throwing each one\ninto \\(\\Gamma'\\) if doing so produces a consistent set. Notice that\neach \\(\\Gamma_n\\) is consistent. Suppose that \\(\\Gamma'\\) is\ninconsistent. Then there is a sentence \\(\\theta\\) such that\n\\(\\Gamma'\\vdash \\theta\\) and \\(\\Gamma'\\vdash \\neg \\theta\\). By Theorem\n9 and Weakening (Theorem 8), there is finite subset \\(\\Gamma''\\) of\n\\(\\Gamma'\\) such that \\(\\Gamma''\\vdash \\theta\\) and \\(\\Gamma''\\vdash\n\\neg \\theta\\).  Because \\(\\Gamma''\\) is finite, there is a natural\nnumber \\(n\\) such that every member of \\(\\Gamma''\\) is in\n\\(\\Gamma_n\\). So, by Weakening again, \\(\\Gamma_n \\vdash \\theta\\) and\n\\(\\Gamma_n \\vdash \\neg \\theta\\). So \\(\\Gamma_n\\) is inconsistent,\nwhich contradicts the construction. So \\(\\Gamma'\\) is consistent. Now\nsuppose that a sentence \\(\\theta\\) is not in \\(\\Gamma'\\). We have to\nshow that \\(\\Gamma', \\theta\\) is inconsistent. The sentence \\(\\theta\\)\nmust occur in the aforementioned list of sentences; say that \\(\\theta\\)\nis \\(\\theta_m\\).  Since \\(\\theta_m\\) is not in \\(\\Gamma'\\), then it is\nnot in \\(\\Gamma_{m+1}\\). This happens only if \\(\\Gamma_m,\n\\theta_m\\) is inconsistent. So a pair of contradictory opposites can\nbe deduced from \\(\\Gamma_m,\\theta_m\\).  By Weakening, a pair of\ncontradictory opposites can be deduced from \\(\\Gamma', \\theta_m\\). So\n\\(\\Gamma', \\theta_m\\) is inconsistent. Thus, \\(\\Gamma'\\) is maximally\nconsistent. \nNotice that this proof uses a principle corresponding to the law of\nexcluded middle. In the construction of \\(\\Gamma'\\), we assumed that,\nat each stage, either \\(\\Gamma_n\\) is consistent or it is\nnot. Intuitionists, who demur from excluded middle, do not accept the\nLindenbaum lemma. \n\nLet \\(K\\) be a set of non-logical terminology. An\ninterpretation for the language \\(\\LKe\\) is a structure \\(M =\n\\langle d,I\\rangle\\), where \\(d\\) is a non-empty set, called\nthe domain-of-discourse, or simply the\ndomain, of the interpretation, and \\(I\\) is an\ninterpretation function. Informally, the domain is what we\ninterpret the language\n \\(\\LKe\\) to be\nabout. It is what the variables range over. The interpretation\nfunction assigns appropriate extensions to the non-logical terms. In\nparticular, \nIf \\(c\\) is a constant in \\(K\\), then \\(I(c)\\) is a member of the\ndomain \\(d\\).\n \n Thus we assume that every constant denotes something. Systems where\n this is not assumed are called free logics (see the entry\n on free logic). Continuing, \nIf \\(P^0\\) is a zero-place predicate letter in \\(K\\), then \\(I(P)\\) is\na truth value, either truth or falsehood. \nIf \\(Q^1\\) is a one-place predicate letter in \\(K\\), then \\(I(Q)\\) is\na subset of \\(d\\). Intuitively, \\(I(Q)\\) is the set of members of the\ndomain that the predicate \\(Q\\) holds of. For example,  \\(I(Q)\\) might be the set of red members of the\ndomain. \nIf \\(R^2\\) is a two-place predicate letter in \\(K\\), then \\(I(R)\\) is\na set of ordered pairs of members of \\(d\\). Intuitively, \\(I(R)\\) is\nthe set of pairs of members of the domain that the relation \\(R\\)\nholds between. For example, \\(I(R)\\) might be the set of pairs \\(\\langle a,b\\rangle\\) such that \\(a\\) and \\(b\\)\nare the members of the domain for which \\(a\\) loves \\(b\\). \nIn general, if S\\(^n\\) is an \\(n\\)-place predicate letter in\n\\(K\\), then \\(I(S)\\) is a set of ordered \\(n\\)-tuples of members of\n\\(d\\). \n Define \\(s\\) to be a variable-assignment, or simply an\nassignment, on an interpretation \\(M\\), if \\(s\\) is a\nfunction from the variables to the domain \\(d\\) of \\(M\\). The role of\nvariable-assignments is to assign denotations to the free\nvariables of open formulas. (In a sense, the quantifiers determine the\n“meaning” of the bound variables.)  \nLet \\(t\\) be a term of \\(\\LKe\\).  We define the denotation of\n\\(t\\) in \\(M\\) under \\(s\\), in terms of the interpretation function\nand variable-assignment:  \nIf \\(t\\) is a constant, then \\(D_{M,s}(t)\\) is \\(I(t)\\), and if \\(t\\)\nis a variable, then \\(D_{M,s}(t)\\) is \\(s(t)\\).\n \n That is, the interpretation \\(M\\) assigns denotations to the\nconstants, while the variable-assignment assigns denotations to the\n(free) variables. If the language contained function symbols, the\ndenotation function would be defined by recursion. \nWe now define a relation of satisfaction between\ninterpretations, variable-assignments, and formulas of \\(\\LKe\\). If\n\\(\\phi\\) is a formula of \\(\\LKe, M\\) is an interpretation for\n\\(\\LKe\\), and \\(s\\) is a variable-assignment on \\(M\\), then we write\n\\(M,s\\vDash \\phi\\) for \\(M\\) satisfies \\(\\phi\\) under the\nassignment \\(s\\). The idea is that \\(M,s\\vDash \\phi\\) is an\nanalogue of “\\(\\phi\\) comes out true when interpreted as in\n\\(M\\) via \\(s\\)”.  \nWe proceed by recursion on the complexity of the formulas of\n \\(\\LKe\\).  \nIf \\(t_1\\) and \\(t_2\\) are terms, then \\(M,s\\vDash t_1 =t_2\\) if and\nonly if \\(D_{M,s}(t_1)\\) is the same as \\(D_{M,s}(t_2)\\).\n \n This is about as straightforward as it gets. An identity\n\\(t_1 =t_2\\) comes out true if and\nonly if the terms \\(t_1\\) and \\(t_2\\)\ndenote the same thing.  \nIf \\(P^0\\) is a zero-place predicate letter in \\(K\\), then \\(M,s\\vDash\nP\\) if and only if \\(I(P)\\) is truth.\n  \nIf S\\(^n\\) is an \\(n\\)-place predicate letter in \\(K\\) and\n\\(t_1, \\ldots,t_n\\) are terms, then \\(M,s\\vDash St_1 \\ldots t_n\\) if\nand only if the \\(n\\)-tuple \\(\\langle D_{M,s}(t_1),\n\\ldots,D_{M,s}(t_n)\\rangle\\) is in \\(I(S)\\). \nThis takes care of the atomic formulas. We now proceed to the\ncompound formulas of the language, more or less following the meanings of the\nEnglish counterparts of the logical terminology.  \\(M,s\\vDash \\neg \\theta\\) if and only if it is not the case that\n\\(M,s\\vDash \\theta\\).  \\(M,s\\vDash(\\theta \\amp \\psi)\\) if and only if both \\(M,s\\vDash\n\\theta\\) and \\(M,s\\vDash \\psi\\).  \\(M,s\\vDash(\\theta \\vee \\psi)\\) if and only if either \\(M,s\\vDash\n\\theta\\) or \\(M,s\\vDash \\psi\\).  \\(M,s\\vDash(\\theta \\rightarrow \\psi)\\) if and only if either it is\nnot the case that \\(M,s\\vDash \\theta\\), or \\(M,s\\vDash \\psi\\).  \\(M,s\\vDash \\forall v\\theta\\) if and only if \\(M,s'\\vDash\n\\theta\\), for every assignment \\(s'\\) that agrees with \\(s\\) except\npossibly at the variable \\(v\\). \nThe idea here is that \\(\\forall v\\theta\\) comes out true if and only\nif \\(\\theta\\) comes out true no matter what is assigned to the\nvariable \\(v\\). The final clause is similar. \n\\(M,s\\vDash \\exists v\\theta\\) if and only if \\(M,s'\\vDash \\theta\\),\nfor some assignment \\(s'\\) that agrees with \\(s\\) except possibly at\nthe variable \\(v\\).\n \nSo \\(\\exists v\\theta\\) comes out true if there is an assignment to\n\\(v\\) that makes \\(\\theta\\) true.  \n Theorem 6, unique readability, assures us that this definition is\ncoherent. At each stage in breaking down a formula, there is exactly\none clause to be applied, and so we never get contradictory verdicts\nconcerning satisfaction.  \n As indicated, the role of variable-assignments is to give\ndenotations to the free variables. We now show that\nvariable-assignments play no other role. \nTheorem 14. For any formula \\(\\theta\\), if \\(s_1\\)\nand \\(s_2\\) agree on the free variables in \\(\\theta\\), then \\(M,s_1\n\\vDash \\theta\\) if and only if \\(M,s_2 \\vDash \\theta\\).  \nProof: We proceed by induction on the complexity of\nthe formula \\(\\theta\\). The theorem clearly holds if \\(\\theta\\) is\natomic, since in those cases only the values of the\nvariable-assignments at the variables in \\(\\theta\\) figure in the\ndefinition. Assume, then, that the theorem holds for all formulas less\ncomplex than \\(\\theta\\). And suppose that \\(s_1\\) and \\(s_2\\) agree on\nthe free variables of \\(\\theta\\). Assume, first, that \\(\\theta\\) is a\nnegation, \\(\\neg \\psi\\). Then, by the induction hypothesis, \\(M,s_1\n\\vDash \\psi\\) if and only if \\(M,s_2 \\vDash \\psi\\).  So, by the clause\nfor negation, \\(M,s_1 \\vDash \\neg \\psi\\) if and only if \\(M,s_2 \\vDash\n\\neg \\psi\\). The cases where the main connective in \\(\\theta\\) is\nbinary are also straightforward. Suppose that \\(\\theta\\) is \\(\\exists\nv\\psi\\), and that \\(M,s_1 \\vDash \\exists v\\psi\\).  Then there is an\nassignment \\(s_1'\\) that agrees with \\(s_1\\) except possibly at \\(v\\)\nsuch that \\(M,s_1'\\vDash \\psi\\).  Let \\(s_2'\\) be the assignment that\nagrees with \\(s_2\\) on the free variables not in \\(\\psi\\) and agrees\nwith \\(s_1'\\) on the others. Then, by the induction hypothesis,\n\\(M,s_2'\\vDash \\psi\\).  Notice that \\(s_2'\\) agrees with \\(s_2\\) on\nevery variable except possibly \\(v\\). So \\(M,s_2 \\vDash \\exists\nv\\psi\\).  The converse is the same, and the case where \\(\\theta\\)\nbegins with a universal quantifier is similar. \nBy Theorem 14, if \\(\\theta\\) is a sentence, and \\(s_1, s_2\\), are any two\nvariable-assignments, then \\(M,s_1 \\vDash \\theta\\) if and only if\n\\(M,s_2 \\vDash \\theta\\).  So we can just write \\(M\\vDash \\theta\\) if\n\\(M,s\\vDash \\theta\\) for some, or all, variable-assignments \\(s\\). So we define   \\(M\\vDash \\theta\\) where \\(\\theta\\) is a sentence\njust in case \\(M,s\\vDash\\theta\\) for all variable assignments \\(s\\).\n \nIn this case, we call \\(M\\) a  model  of \\(\\theta\\). \nSuppose that \\(K'\\subseteq K\\) are two sets of non-logical terms. If\n\\(M = \\langle d,I\\rangle\\) is an interpretation of \\(\\LKe\\), then we\ndefine the restriction of \\(M\\) to \\(\\mathcal{L}1K'{=}\\) to be the\ninterpretation \\(M'=\\langle d,I'\\rangle\\) such that \\(I'\\) is the\nrestriction of \\(I\\) to \\(K'\\). That is, \\(M\\) and \\(M'\\) have the\nsame domain and agree on the non-logical terminology in \\(K'\\). A\nstraightforward induction establishes the following: \nTheorem 15. If \\(M'\\) is the restriction of \\(M\\) to\n\\(\\mathcal{L}1K'{=}\\), then for every sentence \\(\\theta\\) of\n\\(\\mathcal{L}1K'\\), \\(M\\vDash\\theta\\) if and only if \\(M'\\vDash \\theta\\).  \nTheorem 16. If two interpretations \\(M_1\\) and\n\\(M_2\\) have the same domain and agree on all of the non-logical\nterminology of a sentence \\(\\theta\\), then \\(M_1\\vDash\\theta\\) if and\nonly if \\(M_2\\vDash \\theta\\). \n In short, the satisfaction of a sentence \\(\\theta\\) only\ndepends on the domain of discourse and the interpretation of the\nnon-logical terminology in \\(\\theta\\). \nWe say that an argument \\(\\langle \\Gamma,\\theta \\rangle\\)\nis semantically valid, or just valid, written\n\\(\\Gamma \\vDash \\theta\\), if for every interpretation \\(M\\) of the\nlanguage, if \\(M\\vDash\\psi\\), for every member \\(\\psi\\) of \\(\\Gamma\\),\nthen \\(M\\vDash\\theta\\). If \\(\\Gamma \\vDash \\theta\\), we also say that\n\\(\\theta\\) is a logical consequence, or semantic\nconsequence, or\nmodel-theoretic consequence of \\(\\Gamma\\). The definition\ncorresponds to the informal idea that an argument is valid if it is\nnot possible for its premises to all be true and its conclusion\nfalse. Our definition of logical consequence also sanctions the common\nthesis that a valid argument is truth-preserving--to the extent that\nsatisfaction represents truth. Officially, an argument in \\(\\LKe\\) is\nvalid if its conclusion comes out true under every interpretation of\nthe language in which the premises are true. Validity is the\nmodel-theoretic counterpart to deducibility. \nA sentence \\(\\theta\\) is logically true, or\nvalid, if \\(M\\vDash \\theta\\), for every interpretation\n\\(M\\). A sentence is logically true if and only if\nit is a consequence of the empty set. If \\(\\theta\\) is logically true,\nthen for any set \\(\\Gamma\\) of sentences, \\(\\Gamma \\vDash \\theta\\).\nLogical truth is the model-theoretic counterpart of theoremhood. \nA sentence \\(\\theta\\) is satisfiable if there is an\ninterpretation \\(M\\) such\nthat \\(M\\vDash \\theta\\).  That is, \\(\\theta\\) is satisfiable if\nthere is an interpretation that satisfies it. A set\n\\(\\Gamma\\) of sentences is satisfiable if there is an interpretation\n\\(M\\) such that \\(M\\vDash\\theta\\), for every sentence \\(\\theta\\) in \\(\\Gamma\\). If \\(\\Gamma\\) is\na set of sentences and if \\(M\\vDash \\theta\\) for each sentence\n\\(\\theta\\) in \\(\\Gamma\\), then we say that \\(M\\) is a model\nof  \\(\\Gamma\\). So a set of sentences is satisfiable if it has a\nmodel. Satisfiability is the model-theoretic counterpart to\nconsistency. \nNotice that \\(\\Gamma \\vDash \\theta\\) if and only if the set\n\\(\\Gamma,\\neg \\theta\\) is not satisfiable. It follows that if a set\n\\(\\Gamma\\) is not satisfiable, then if \\(\\theta\\) is any sentence,\n\\(\\Gamma \\vDash \\theta\\).  This is a model-theoretic counterpart\nto ex falso quodlibet (see Theorem 10). We have the\nfollowing, as an analogue to Theorem 12: \nTheorem 17. Let \\(\\Gamma\\) be a set of sentences. The\nfollowing are equivalent: (a) \\(\\Gamma\\) is satisfiable; (b) there is\nno sentence \\(\\theta\\) such that both \\(\\Gamma \\vDash \\theta\\) and\n\\(\\Gamma \\vDash \\neg \\theta\\); (c) there is some sentence \\(\\psi\\) such\nthat it is not the case that \\(\\Gamma \\vDash \\psi\\). \nProof: (a)\\(\\Rightarrow\\)(b): Suppose that \\(\\Gamma\\)\nis satisfiable and let \\(\\theta\\) be any sentence. There is an\ninterpretation \\(M\\) such that \\(M\\vDash \\psi\\)\nfor every member \\(\\psi\\) of \\(\\Gamma\\). By the clause for negations,\nwe cannot have both \\(M\\vDash \\theta\\) and \\(M\\vDash \\neg\n\\theta\\).  So either \\(\\langle \\Gamma,\\theta \\rangle\\) is not valid or\nelse \\(\\langle \\Gamma,\\neg \\theta \\rangle\\) is not valid.\n(b)\\(\\Rightarrow\\)(c): This is immediate.  (c)\\(\\Rightarrow\\)(a):\nSuppose that it is not the case that \\(\\Gamma \\vDash \\psi\\). Then\nthere is an interpretation \\(M\\) such that\n\\(M\\vDash \\theta\\), for every sentence \\(\\theta\\) in \\(\\Gamma\\) and\nit is not the case that \\(M\\vDash \\psi\\).  A fortiori, \\(M\\)\nsatisfies every member of \\(\\Gamma\\), and so \\(\\Gamma\\) is\nsatisfiable. \n\nWe now present some results that relate the deductive notions to their\nmodel-theoretic counterparts. The first one is probably the most\nstraightforward. We motivated both the various rules of the deductive\nsystem \\(D\\) and the various clauses in the definition of\nsatisfaction in terms of the meaning of the English counterparts to\nthe logical terminology (more or less, with the same simplifications\nin both cases). So one would expect that an argument is deducible, or\ndeductively valid, only if it is semantically valid. \nTheorem 18. Soundness. For any sentence \\(\\theta\\) and\nset \\(\\Gamma\\) of sentences, if \\(\\Gamma \\vdash_D \\theta\\), then\n\\(\\Gamma \\vDash \\theta\\). \nProof: We proceed by induction on the number of\nclauses used to establish \\(\\Gamma \\vdash \\theta\\).  So let \\(n\\) be a\nnatural number, and assume that the theorem holds for any argument\nestablished as deductively valid with fewer than \\(n\\) steps. And\nsuppose that \\(\\Gamma \\vdash \\theta\\) was established using exactly\n\\(n\\) steps. If the last rule applied was \\((=\\)I) then \\(\\theta\\) is\na sentence in the form \\(t=t\\), and so \\(\\theta\\) is logically true. A\nfortiori, \\(\\Gamma \\vDash \\theta\\).  If the last rule applied was\n(As), then \\(\\theta\\) is a member of \\(\\Gamma\\), and so of course any\ninterpretation that satisfies every member of\n\\(\\Gamma\\) also satisfies \\(\\theta\\). Suppose the last rule applied is\n(&I). So \\(\\theta\\) has the form \\((\\phi \\amp \\psi)\\), and we have\n\\(\\Gamma_1 \\vdash \\phi\\) and \\(\\Gamma_2 \\vdash \\psi\\), with \\(\\Gamma =\n\\Gamma_1, \\Gamma_2\\). The induction hypothesis gives us \\(\\Gamma_1\n\\vDash \\phi\\) and \\(\\Gamma_2 \\vDash \\psi\\).  Suppose that \\(M\\)\nsatisfies every member of \\(\\Gamma\\). Then \\(M\\) satisfies every\nmember of \\(\\Gamma_1\\), and so \\(M\\) satisfies \\(\\phi\\).  Similarly,\n\\(M\\) satisfies every member of \\(\\Gamma_2\\), and so \\(M\\)\nsatisfies \\(\\psi\\). Thus, by the clause for “\\(\\amp\\)” in\nthe definition of satisfaction, \\(M\\) satisfies \\(\\theta\\). So\n\\(\\Gamma \\vDash \\theta\\).   \nSuppose the last clause applied was \\((\\exists\\mathrm{E})\\). So we\nhave \\(\\Gamma_1 \\vdash \\exists v\\phi\\) and \\(\\Gamma_2, \\phi(v|t)\n\\vdash \\theta\\), where \\(\\Gamma = \\Gamma_1, \\Gamma_2\\), and \\(t\\) does\nnot occur in \\(\\phi , \\theta \\), or in any member of \\(\\Gamma_2\\). \nWe need to show that \\(\\Gamma\\vDash\\theta\\). By the induction\nhypothesis, we have that \\(\\Gamma_1\\vDash\\exists v\\phi\\) and\n\\(\\Gamma_2, \\phi(v|t)\\vDash\\theta\\). Let \\(M\\) be an interpretation\nsuch that \\(M\\) makes every member of \\(\\Gamma\\) true. So, \\(M\\) makes\nevery member of \\(\\Gamma_1\\) and \\(\\Gamma_2\\) true. Then\n\\(M,s\\vDash\\exists v\\phi\\) for all variable assignments \\(s\\), so\nthere is an \\(s'\\) such that \\(M,s'\\vDash\\phi\\). Let \\(M'\\) differ\nfrom \\(M\\) only in that \\(I_{M'}(t)=s'(v)\\). Then,\n\\(M',s'\\vDash\\phi(v|t)\\) and \\(M',s'\\vDash\\Gamma_2\\) since \\(t\\) does\nnot occur in \\(\\phi\\) or \\(\\Gamma_2\\). So,\n\\(M',s'\\vDash\\theta\\). Since \\(t\\) does not occur in \\(\\theta\\) and\n\\(M'\\) differs from \\(M\\) only with respect to \\(I_{M'}(t)\\),\n\\(M,s'\\vDash\\theta\\). Since \\(\\theta\\) is a sentence, \\(s'\\) doesn't\nmatter, so \\(M\\vDash\\theta\\) as desired. Notice the role of the\nrestrictions on \\((\\exists\\)E) here. The other cases are about as\nstraightforward.  \nCorollary 19. Let \\(\\Gamma\\) be a set of sentences. If\n\\(\\Gamma\\) is satisfiable, then \\(\\Gamma\\) is consistent.   \nProof: Suppose that \\(\\Gamma\\) is satisfiable. So let\n\\(M\\) be an interpretation such that \\(M\\)\nsatisfies every member of \\(\\Gamma\\). Assume that \\(\\Gamma\\) is\ninconsistent. Then there is a sentence \\(\\theta\\) such that \\(\\Gamma\n\\vdash \\theta\\) and \\(\\Gamma \\vdash \\neg \\theta\\). By soundness\n(Theorem 18), \\(\\Gamma \\vDash \\theta\\) and \\(\\Gamma \\vDash \\neg\n\\theta\\). So we have that \\(M\\vDash \\theta\\) and \\(M\\vDash \\neg\n\\theta\\). But this is impossible, given the clause for negation in the\ndefinition of satisfaction. \n Even though the deductive system \\(D\\) and the model-theoretic\nsemantics were developed with the meanings of the logical terminology\nin mind, one should not automatically expect the converse to\nsoundness (or Corollary 19) to hold. For all we know so far, we may\nnot have included enough rules of inference to deduce every valid\nargument. The converses to soundness and Corollary 19 are among the\nmost important and influential results in mathematical logic. We begin\nwith the latter. \nTheorem 20. Completeness. Gödel [1930]. Let\n\\(\\Gamma\\) be a set of sentences. If \\(\\Gamma\\) is consistent, then\n\\(\\Gamma\\) is satisfiable. \nProof: The proof of completeness is rather\ncomplex. We only sketch it here. Let \\(\\Gamma\\) be a consistent set of\nsentences of \\(\\LKe\\). Again, we assume for simplicity that the set\n\\(K\\) of non-logical terminology is either finite or countably\ninfinite (although the theorem holds even if \\(K\\) is\nuncountable). The task at hand is to find an interpretation \\(M\\) such\nthat \\(M\\) satisfies every member of \\(\\Gamma\\). Consider the language\nobtained from \\(\\LKe\\) by adding a denumerably infinite stock of new\nindividual constants \\(c_0, c_1,\\ldots\\) We stipulate that the\nconstants, \\(c_0, c_1,\\ldots\\), are all different from each other and\nnone of them occur in \\(K\\). One interesting feature of this\nconstruction, due to Leon Henkin, is that we build an interpretation\nof the language from the language itself, using some of the constants\nas members of the domain of discourse. Let \\(\\theta_0 (x), \\theta_1\n(x),\\ldots\\) be an enumeration of the formulas of the expanded\nlanguage with at most one free variable, so that each formula with at\nmost one free variable occurs in the list eventually. Define a\nsequence \\(\\Gamma_0, \\Gamma_1,\\ldots\\) of sets of sentences (of the\nexpanded language) by recursion as follows: \\(\\Gamma_0 = \\Gamma\\); and\n\\(\\Gamma_{n+1} = \\Gamma_n,(\\exists x\\theta_n \\rightarrow\n\\theta_{n}(x|c_i))\\), where \\(c_i\\) is the first constant in the above\nlist that does not occur in \\(\\theta_n\\) or in any member of\n\\(\\Gamma_n\\). The underlying idea here is that if \\(\\exists\nx\\theta_n\\)is true, then \\(c_i\\) is to be one such \\(x\\). Let\n\\(\\Gamma\\) be the union of the sets \\(\\Gamma_n\\).  \nWe sketch a proof that \\(\\Gamma'\\) is consistent. Suppose that\n\\(\\Gamma'\\) is inconsistent. By Theorem 9, there is a finite subset of\n\\(\\Gamma\\) that is inconsistent, and so one of the sets \\(\\Gamma_m\\)\nis inconsistent. By hypothesis, \\(\\Gamma_0 = \\Gamma\\) is\nconsistent. Let \\(n\\) be the smallest number such that \\(\\Gamma_n\\) is\nconsistent, but \\(\\Gamma_{n+1} = \\Gamma_n,(\\exists x\\theta_n\n\\rightarrow \\theta_{n}(x|c_i))\\) is inconsistent. By \\((\\neg\\)I), we\nhave that  By ex falso quodlibet (Theorem 10), \\(\\Gamma_n, \\neg\n\\exists x\\theta_n, \\exists x\\theta_n \\vdash \\theta_n (x|c_i)\\).  So by\n\\((\\rightarrow\\)I), \\(\\Gamma_n, \\neg \\exists x\\theta_n \\vdash(\\exists\nx\\theta_n \\rightarrow \\theta_n (x|c_i))\\).  From this and (1), we have\n\\(\\Gamma_n \\vdash \\neg \\neg \\exists x\\theta_n\\), by \\((\\neg\\)I), and\nby (DNE) we have \nBy (As), \\(\\Gamma_n, \\theta_n (x|c_i), \\exists x\\theta_n \\vdash\n\\theta_n (x|c_i)\\).  So by \\((\\rightarrow\\)I), \\(\\Gamma_n, \\theta_n\n(x|c_i)\\vdash(\\exists x\\theta_{n} \\rightarrow\n\\theta_{n}(x|c_i))\\).  From this and (1), we have \\(\\Gamma_n \\vdash\n\\neg \\theta_n (x|c_i)\\), by \\((\\neg\\)I). Let \\(t\\) be a term that\ndoes not occur in \\(\\theta_n\\) or in any member of\n\\(\\Gamma_n\\). By uniform substitution of \\(t\\) for \\(c_i\\), we can\nturn the derivation of \\(\\Gamma_n \\vdash \\neg \\theta_n (x|c_i)\\) into\n\\(\\Gamma_n \\vdash \\neg \\theta_n (x|t)\\).  By \\((\\forall\\)I), we\nhave \nBy (As) we have \\(\\{\\forall v\\neg \\theta_n (x|v),\\theta_n\\}\\vdash\n\\theta_n\\) and by \\((\\forall\\)E) we have \\(\\{\\forall v\\neg \\theta_n\n(x|v), \\theta_n\\}\\vdash \\neg \\theta_n\\).  So \\(\\{\\forall v\\neg\n\\theta_n (x|v), \\theta_n\\}\\) is inconsistent. Let \\(\\phi\\) be any\nsentence of the language. By ex falso quodlibet (Theorem 10), we have that\n\\(\\{\\forall v\\neg \\theta_n (x|v),\\theta_n\\}\\vdash \\phi\\) and\n\\(\\{\\forall v\\neg \\theta_n (x|v), \\theta_n\\}\\vdash \\neg \\phi\\). So\nwith (2), we have that \\(\\Gamma_n, \\forall v\\neg \\theta_n (x|v)\\vdash\n\\phi\\) and \\(\\Gamma_n, \\forall v\\neg \\theta_n (x|v)\\vdash \\neg \\phi\\),\nby \\((\\exists\\)E). By Cut (Theorem 11), \\(\\Gamma_n \\vdash \\phi\\) and\n\\(\\Gamma_n \\vdash \\neg \\phi\\). So \\(\\Gamma_n\\) is inconsistent,\ncontradicting the assumption. So \\(\\Gamma'\\) is consistent.  Applying the Lindenbaum Lemma (Theorem 13), let \\(\\Gamma''\\) be a\nmaximally consistent set of sentences (of the expanded language) that\ncontains \\(\\Gamma'\\). So, of course, \\(\\Gamma''\\) contains\n\\(\\Gamma\\). We can now define an interpretation \\(M\\) such that \\(M\\) satisfies every\nmember of \\(\\Gamma''\\). \n If we did not have a sign for identity in the language, we would let\nthe domain of \\(M\\) be the collection of new constants \\(\\{c_0, c_1,\n\\ldots \\}\\). But as it is, there may be a sentence in the form\n\\(c_{i}=c_{j}\\), with \\(i\\ne j\\), in \\(\\Gamma''\\). If so, we\ncannot have both \\(c_i\\) and \\(c_j\\) in the domain of the\ninterpretation (as they are distinct constants). So we define the\ndomain \\(d\\) of \\(M\\) to be the set \\(\\{c_i\\) | there is no \\(j\\lt i\\)\nsuch that \\(c_{i}=c_{j}\\) is in \\(\\Gamma''\\}\\). In other words, a\nconstant \\(c_i\\) is in the domain of \\(M\\) if \\(\\Gamma''\\) does not\ndeclare it to be identical to an earlier constant in the list. Notice\nthat for each new constant \\(c_i\\), there is exactly one \\(j\\le i\\)\nsuch that \\(c_j\\) is in \\(d\\) and the sentence \\(c_{i}=c_{j}\\) is\nin \\(\\Gamma''\\). \nWe now define the interpretation function \\(I\\). Let \\(a\\) be any\nconstant in the expanded language. By \\((=\\)I) and \\((\\exists\\)I),\n\\(\\Gamma''\\vdash \\exists x x=a\\), and so \\(\\exists x x=a \\in\n\\Gamma''\\).  By the construction of \\(\\Gamma'\\), there is a sentence\nin the form \\((\\exists x x=a \\rightarrow c_i =a)\\) in \\(\\Gamma''\\). We\nhave that \\(c_i =a\\) is in \\(\\Gamma''\\).  As above, there is exactly\none \\(c_j\\) in \\(d\\) such that \\(c_{i}=c_{j}\\) is in\n\\(\\Gamma''\\). Let \\(I(a)=c_j\\). Notice that if \\(c_i\\) is a constant\nin the domain \\(d\\), then \\(I\\)(c\\(_i)=c_i\\).  That is each \\(c_i\\) in\n\\(d\\) denotes itself. \nLet \\(P\\) be a zero-place predicate letter in \\(K\\).  Then \\(I(P)\\) is\ntruth if \\(P\\) is in \\(\\Gamma''\\) and \\(I(P)\\) is falsehood otherwise.\nLet \\(Q\\) be a one-place predicate letter in \\(K\\). Then \\(I(Q)\\) is\nthe set of constants \\(\\{\\)c\\(_i | c_i\\) is in \\(d\\) and the sentence\n\\(Qc\\) is in \\(\\Gamma''\\}\\). Let \\(R\\) be a binary predicate letter in\n\\(K\\). Then \\(I(R)\\) is the set of pairs of constants \\(\\{\\langle\nc_i,c_j\\rangle | c_i\\) is in \\(d, c_j\\) is in \\(d\\), and the sentence\n\\(Rc_{i}c_{j}\\) is in \\(\\Gamma''\\}\\). Three-place predicates,\netc. are interpreted similarly. In effect, \\(I\\) interprets the\nnon-logical terminology as they are in \\(\\Gamma''\\). \nThe final item in this proof is a lemma that for every sentence \\(\\theta\\) in the expanded language, \\(M\\vDash \\theta\\) if and only\nif \\(\\theta\\) is in \\(\\Gamma''\\). This proceeds by induction on the\ncomplexity of \\(\\theta\\). The case where \\(\\theta\\) is atomic follows\nfrom the definitions of \\(M\\) (i.e., the domain \\(d\\) and the\ninterpretation function \\(I\\)). The\nother cases follow from the various clauses in the definition of\nsatisfaction. \nSince \\(\\Gamma \\subseteq \\Gamma''\\), we have that \\(M\\) satisfies\nevery member of \\(\\Gamma\\). By Theorem 15, the restriction of \\(M\\) to\nthe original language \\(\\LKe\\) and \\(s\\) also satisfies every member\nof \\(\\Gamma\\). Thus \\(\\Gamma\\) is satisfiable.  \nA converse to Soundness (Theorem 18) is a straightforward\ncorollary: \nTheorem 21. For any sentence \\(\\theta\\) and set\n\\(\\Gamma\\) of sentences, if \\(\\Gamma \\vDash \\theta\\), then \\(\\Gamma\n\\vdash_D \\theta\\). \nProof: Suppose that \\(\\Gamma \\vDash \\theta\\).  Then\nthere is no interpretation \\(M\\) such\nthat M satisfies every member of \\(\\Gamma\\) but does not\nsatisfy \\(\\theta\\). So the set \\(\\Gamma,\\neg \\theta\\) is not\nsatisfiable. By Completeness (Theorem 20), \\(\\Gamma,\\neg \\theta\\) is\ninconsistent. So there is a sentence \\(\\phi\\) such that \\(\\Gamma,\\neg\n\\theta \\vdash \\phi\\) and \\(\\Gamma,\\neg \\theta \\vdash \\neg \\phi\\). By\n\\((\\neg\\)I), \\(\\Gamma \\vdash \\neg \\neg \\theta\\), and by (DNE) \\(\\Gamma\n\\vdash \\theta\\). \nOur next item is a corollary of Theorem 9, Soundness (Theorem 18),\nand Completeness: \nCorollary 22. Compactness. A set \\(\\Gamma\\) of\nsentences is satisfiable if and only if every finite subset of\n\\(\\Gamma\\) is satisfiable. \nProof: If \\(M\\) satisfies every member of\n\\(\\Gamma\\), then \\(M\\) satisfies every member of each finite subset\nof \\(\\Gamma\\). For the converse, suppose that \\(\\Gamma\\) is not\nsatisfiable. Then we show that some finite subset of \\(\\Gamma\\) is not\nsatisfiable. By Completeness (Theorem 20), \\(\\Gamma\\) is\ninconsistent. By Theorem 9 (and Weakening), there is a finite subset\n\\(\\Gamma'\\subseteq \\Gamma\\) such that \\(\\Gamma'\\) is inconsistent.  By\nCorollary \\(19, \\Gamma'\\) is not satisfiable. \n Soundness and completeness together entail that an argument is\ndeducible if and only if it is valid, and a set of sentences is\nconsistent if and only if it is satisfiable. So we can go back and\nforth between model-theoretic and proof-theoretic notions,\ntransferring properties of one to the other. Compactness holds in the\nmodel theory because all derivations use only a finite number of\npremises. \n Recall that in the proof of Completeness (Theorem 20), we made the\nsimplifying assumption that the set \\(K\\) of non-logical\nconstants is either finite or denumerably infinite. The\ninterpretation we produced was itself either finite or denumerably\ninfinite. Thus, we have the following: \nCorollary 23. Löwenheim-Skolem Theorem. Let\n\\(\\Gamma\\) be a satisfiable set of sentences of the language\n\\(\\LKe\\). If \\(\\Gamma\\) is either finite or denumerably infinite, then\n\\(\\Gamma\\) has a model whose domain is either finite or denumerably\ninfinite.\n \nIn general, let \\(\\Gamma\\) be a satisfiable set of sentences of\n\\(\\LKe\\), and let \\(\\kappa\\) be the larger of the size of \\(\\Gamma\\)\nand denumerably infinite. Then \\(\\Gamma\\) has a model whose domain is\nat most size \\(\\kappa\\). \nThere is a stronger version of Corollary 23. Let \\(M_1 =\\langle\nd_1,I_1\\rangle\\) and \\(M_2 =\\langle d_2,I_2\\rangle\\) be\ninterpretations of the language \\(\\LKe\\). Define \\(M_1\\) to be\na submodel of \\(M_2\\) if \\(d_1 \\subseteq d_2, I_1 (c) = I_2\n(c)\\) for each constant \\(c\\), and \\(I_1\\) is the restriction of\n\\(I_2\\) to \\(d_1\\). For example, if \\(R\\) is a binary relation letter\nin \\(K\\), then for all \\(a,b\\) in \\(d_1\\), the pair \\(\\langle\na,b\\rangle\\) is in \\(I_1 (R)\\) if and only if \\(\\langle a,b\\rangle\\)\nis in \\(I_2 (R)\\). If we had included function letters among the\nnon-logical terminology, we would also require that \\(d_1\\) be closed\nunder their interpretations in \\(M_2\\). Notice that if \\(M_1\\) is a\nsubmodel of \\(M_2\\), then any variable-assignment on \\(M_1\\) is also a\nvariable-assignment on \\(M_2\\). \nSay that two interpretations \\(M_1 =\\langle d_1,I_1\\rangle, M_2\n=\\langle d_2,I_2\\rangle\\) are equivalent if one of them is a\nsubmodel of the other, and for any formula of the language and any\nvariable-assignment \\(s\\) on the submodel, \\(M_1,s\\vDash \\theta\\) if\nand only if \\(M_2,s\\vDash \\theta\\).  Notice that if two\ninterpretations are equivalent, then they satisfy the same\nsentences. \nTheorem 25. Downward Löwenheim-Skolem Theorem.\nLet \\(M = \\langle d,I\\rangle\\) be an interpretation of the language\n\\(\\LKe\\). Let \\(d_1\\) be any subset of \\(d\\), and let \\(\\kappa\\) be\nthe maximum of the size of \\(K\\), the size of \\(d_1\\), and denumerably\ninfinite. Then there is a submodel \\(M' = \\langle d',I'\\rangle\\) of\n\\(M\\) such that (1) \\(d'\\) is not larger than \\(\\kappa\\), and (2)\n\\(M\\) and \\(M'\\) are equivalent. In particular, if the set \\(K\\) of\nnon-logical terminology is either finite or denumerably infinite, then\nany interpretation has an equivalent submodel whose domain is either\nfinite or denumerably infinite. \nProof: Like completeness, this proof is complex, and\nwe rest content with a sketch. The downward Löwenheim-Skolem\ntheorem invokes the axiom of choice, and indeed, is equivalent to the\naxiom of choice (see the entry on\n the axiom of choice). \nSo let \\(C\\) be a choice function on the powerset of \\(d\\), so that\nfor each non-empty subset \\(e\\subseteq d, C(e)\\) is a member of\n\\(e\\). We stipulate that if \\(e\\) is the empty set, then \\(C(e)\\) is\n\\(C(d)\\). \nLet \\(s\\) be a variable-assignment on \\(M\\), let \\(\\theta\\) be a\nformula of \\(\\LKe\\), and let \\(v\\) be a variable. Define the\n\\(v\\)-witness of \\(\\theta\\) over s, written \\(w_v\n(\\theta,s)\\), as follows: Let \\(q\\) be the set of all elements \\(c\\in\nd\\) such that there is a variable-assignment \\(s'\\) on \\(M\\) that\nagrees with \\(s\\) on every variable except possibly \\(v\\), such that\n\\(M,s'\\vDash \\theta\\), and \\(s'(v)=c\\). Then \\(w_v (\\theta,s) =\nC(q)\\). Notice that if \\(M,s\\vDash \\exists v\\theta\\), then \\(q\\) is\nthe set of elements of the domain that can go for \\(v\\) in\n\\(\\theta\\). Indeed, \\(M,s\\vDash \\exists v\\theta\\) if and only if \\(q\\)\nis non-empty. So if \\(M,s\\vDash \\exists v\\theta\\), then \\(w_v\n(\\theta,s)\\) (i.e., \\(C(q))\\) is a chosen element of the domain that\ncan go for \\(v\\) in \\(\\theta\\). In a sense, it is a\n“witness” that verifies \\(M,s\\vDash \\exists v\\theta\\). \nIf \\(e\\) is a non-empty subset of the domain \\(d\\), then define a\nvariable-assignment \\(s\\) to be an \\(e\\)-assignment if for\nall variables \\(u, s(u)\\) is in \\(e\\). That is, \\(s\\) is an\n\\(e\\)-assignment if \\(s\\) assigns an element of \\(e\\) to each\nvariable. Define \\(sk(e)\\), the\nSkolem-hull of \\(e\\), to be the set: \n That is, the Skolem-Hull of \\(e\\) is the set \\(e\\) together with\nevery \\(v\\)-witness of every formula over every\n\\(e\\)-assignment. Roughly, the idea is to start with \\(e\\) and then\nthrow in enough elements to make each existentially quantified formula\ntrue. But we cannot rest content with the Skolem-hull, however. Once\nwe throw the “witnesses” into the domain, we need to deal\nwith \\(sk(e)\\) assignments. In effect, we need a set which is its own\nSkolem-hull, and also contains the given subset \\(d_1\\). \n We define a sequence of non-empty sets \\(e_0, e_1,\\ldots\\) as\nfollows: if the given subset \\(d_1\\) of \\(d\\) is empty and there are\nno constants in \\(K\\), then let \\(e_0\\) be \\(C(d)\\), the choice\nfunction applied to the entire domain; otherwise let \\(e_0\\) be the\nunion of \\(d_1\\) and the denotations under \\(I\\) of the constants in\n\\(K\\). For each natural number \\(n, e_{n+1}\\) is \\(sk(e_n)\\). Finally,\nlet \\(d'\\) be the union of the sets \\(e_n\\), and let \\(I'\\) be the\nrestriction of \\(I\\) to \\(d'\\). Our interpretation is \\(M' = \\langle\nd',I'\\rangle\\). \nClearly, \\(d_1\\) is a subset of \\(d'\\), and so \\(M'\\) is a submodel of\n\\(M\\). Let \\(\\kappa\\) be the maximum of the size of \\(K\\), the size of\n\\(d_1\\), and denumerably infinite. A calculation reveals that the size\nof \\(d'\\) is at most \\(\\kappa\\), based on the fact that there are at\nmost \\(\\kappa\\)-many formulas, and thus, at most \\(\\kappa\\)-many\nwitnesses at each stage. Notice, incidentally, that this calculation\nrelies on the fact that a denumerable union of sets of size at most\n\\(\\kappa\\) is itself at most \\(\\kappa\\). This also relies on the axiom\nof choice. \nThe final item is to show that \\(M'\\) is equivalent to \\(M\\): For\nevery formula \\(\\theta\\) and every variable-assignment \\(s\\) on\n\\(M'\\), \nThe proof proceeds by induction on the complexity of\n\\(\\theta\\). Unfortunately, space constraints require that we leave\nthis step as an exercise. \n Another corollary to Compactness (Corollary 22) is the opposite\nof the Löwenheim-Skolem theorem: \nTheorem 26. Upward Löwenheim-Skolem Theorem.\nLet \\(\\Gamma\\) be any set of sentences of \\(\\LKe,\\) such that for each\nnatural number \\(n\\), there is an interpretation \\(M_n = \\langle\nd_n,I_n\\rangle\\), such that\n\\(d_n\\) has at least \\(n\\) elements, and \\(M_n\\) satisfies every\nmember of \\(\\Gamma\\). In other words, \\(\\Gamma\\) is satisfiable and\nthere is no finite upper bound to the size of the interpretations that\nsatisfy every member of \\(\\Gamma\\). Then for any infinite cardinal\n\\(\\kappa\\), there is an interpretation \\(M=\\langle d,I\\rangle\\), such that the size of \\(d\\) is at\nleast \\(\\kappa\\) and \\(M\\) satisfies every member of\n\\(\\Gamma\\). \nProof: Add a collection of new constants\n\\(\\{c_{\\alpha} | \\alpha \\lt \\kappa \\}\\), of size \\(\\kappa\\), to the\nlanguage, so that if \\(c\\) is a constant in \\(K\\), then \\(c_{\\alpha}\\)\nis different from \\(c\\), and if \\(\\alpha \\lt \\beta \\lt \\kappa\\), then\n\\(c_{\\alpha}\\) is a different constant than \\(c_{\\beta}\\).  Consider\nthe set of sentences \\(\\Gamma'\\) consisting of \\(\\Gamma\\) together with\nthe set \\(\\{\\neg c_{\\alpha}=c_{\\beta} | \\alpha \\ne \\beta \\}\\).\nThat is, \\(\\Gamma'\\) consists of \\(\\Gamma\\) together with statements\nto the effect that any two different new constants denote different\nobjects. Let \\(\\Gamma''\\) be any finite subset of \\(\\Gamma'\\), and let\n\\(m\\) be the number of new constants that occur in \\(\\Gamma''\\).  Then\nexpand the interpretation \\(M_m\\) to an interpretation \\(M_m'\\) of the\nnew language, by interpreting each of the new constants in\n\\(\\Gamma''\\) as a different member of the domain \\(d_m\\). By\nhypothesis, there are enough members of \\(d_m\\) to do this. One can\ninterpret the other new constants at will. So \\(M_m\\) is a restriction\nof \\(M_m'\\). By hypothesis (and Theorem 15), \\(M'_m\\) satisfies\nevery member of \\(\\Gamma\\). Also \\(M'_m\\) satisfies the members of\n\\(\\{\\neg c_{\\alpha}=c_{\\beta} | \\alpha \\ne \\beta \\}\\) that are in\n\\(\\Gamma''\\). So \\(M'_m\\) satisfies every member of \\(\\Gamma''\\).\nBy compactness, there is an interpretation \\(M = \\langle d,I\\rangle\\)\nsuch that \\(M\\) satisfies every\nmember of \\(\\Gamma'\\). Since \\(\\Gamma'\\) contains every member of\n\\(\\{\\neg c_{\\alpha}=c_{\\beta} | \\alpha \\ne \\beta \\}\\), the domain\n\\(d\\) of \\(M\\) must be of size at least \\(\\kappa\\), since each of the\nnew constants must have a different denotation. By Theorem 15, the\nrestriction of \\(M\\) to the original language \\(\\LKe\\) satisfies every\nmember of \\(\\Gamma\\). \nCombined, the proofs of the downward and upward Löwenheim-Skolem\ntheorems show that for any satisfiable set \\(\\Gamma\\) of sentences, if\nthere is no finite bound on the models of \\(\\Gamma\\), then for any\ninfinite cardinal \\(\\kappa\\), there is a model of \\(\\Gamma\\) whose\ndomain has size exactly \\(\\kappa\\). Moreover, if \\(M\\) is any\ninterpretation whose domain is infinite, then for any infinite\ncardinal \\(\\kappa\\), there is an interpretation \\(M'\\) whose domain\nhas size exactly \\(\\kappa\\) such that \\(M\\) and \\(M'\\) are\nequivalent. \nThese results indicate a weakness in the expressive resources of\nfirst-order languages like \\(\\LKe\\). No satisfiable set of sentences\ncan guarantee that its models are all denumerably infinite, nor can\nany satisfiable set of sentences guarantee that its models are\nuncountable. So in a sense, first-order languages cannot express the\nnotion of “denumerably infinite”, at least not in the\nmodel theory. (See the entry on\n second-order and higher-order logic.) \n Let \\(A\\) be any set of sentences in a first-order language \\(\\LKe\\),\nwhere \\(K\\) includes terminology for arithmetic, and assume that every\nmember of \\(A\\) is true of the natural numbers. We can even let \\(A\\)\nbe the set of all sentences in \\(\\LKe\\) that are true of the natural\nnumbers. Then \\(A\\) has uncountable models, indeed models of any\ninfinite cardinality. Such interpretations are among those that are\nsometimes called unintended, or non-standard models\nof arithmetic. Let \\(B\\) be any set of first-order sentences that are\ntrue of the real numbers, and let \\(C\\) be any first-order\naxiomatization of set theory. Then if \\(B\\) and \\(C\\) are satisfiable\n(in infinite interpretations), then each of them has denumerably\ninfinite models. That is, any first-order, satisfiable set theory or\ntheory of the real numbers, has (unintended) models the size of the\nnatural numbers. This is despite the fact that a sentence (seemingly)\nstating that the universe is uncountable is provable in most\nset-theories. This situation, known as the\nSkolem paradox, has generated much discussion, but we must\nrefer the reader elsewhere for a sample of it (see the entry on\n Skolem’s paradox and\n Shapiro 1996). \nLogic and reasoning go hand in hand. We say that someone has reasoned\npoorly about something if they have not reasoned logically, or that an\nargument is bad because it is not logically valid. To date, research\nhas been devoted to exactly just what types of logical systems are\nappropriate for guiding our reasoning. Traditionally, classical logic\nhas been the logic suggested as the ideal for guiding reasoning (for\nexample, see Quine [1986], Resnik [1996] or Rumfitt [2015]). For this\nreason, classical logic has often been called “the one right\nlogic”. See Priest [2006a] for a description of how being the best\nreasoning-guiding logic could make a logic the one right logic.  \nThat classical logic has been given as the answer to which logic ought\nto guide reasoning is not unexpected. It has rules which are more or\nless intuitive, and is surprisingly simple for how strong it is. Plus,\nit is both sound and complete, which is an added bonus. There are some\nissues, though. As indicated in Section 5, there are certain\nexpressive limitations to classical logic. Thus, much literature has\nbeen written challenging this status quo. This literature in general\nstems from three positions. The first is that classical logic is not\nreason-guiding because some other single logic is. Examples of this\ntype of argument can be found in Brouwer [1949], Heyting [1956] and\nDummett [2000] who argue that intuitionistic logic is correct, and\nAnderson and Belnap [1975], who argue relevance logic is correct,\namong many others. Further, some people propose that an extension of\nclassical logic which can express the notion of “denumerably infinite”\n(see Shapiro [1991]). The second objection to the claim that classical\nlogic is the one right logic comes from a different perspective:\nlogical pluralists claim that classical logic is not the (single) one\nright logic, because more than one logic is right. See Beall and\nRestall [2006] and Shapiro [2014] for examples of this type of view\n(see also the entry on logical\npluralism). Finally, the last objection to the claim that\nclassical logic is the one right logic is that logic(s) is not\nreasoning-guiding, and so there is no one right logic.  \nSuffice it to say that, though classical logic has traditionally been\nthought of as “the one right logic”, this is not accepted by\neveryone. An interesting feature of these debates, though, is that\nthey demonstrate clearly the strengths and weaknesses of various\nlogics (including classical logic) when it comes to capturing\nreasoning.","contact.mail":"shapiro.4@osu.edu","contact.domain":"osu.edu"},{"date.published":"2000-09-16","date.changed":"2018-03-11","url":"https://plato.stanford.edu/entries/logic-classical/","author1":"Stewart Shapiro","author1.info":"https://philosophy.osu.edu/people/shapiro.4/","author2.info":"https://sites.google.com/site/teresakouri/","entry":"logic-classical","body.text":"\n\n\n\nTypically, a logic consists of a formal or informal language\ntogether with a deductive system and/or a model-theoretic semantics.\nThe language has components that correspond to a part of a natural language like\nEnglish or Greek. The deductive system is to capture, codify, or\nsimply record arguments that are valid for the given\nlanguage, and the semantics is to capture, codify, or record the\nmeanings, or truth-conditions for at least part of the language.\n\n\n The following sections provide the basics of a typical logic,\nsometimes called “classical elementary logic” or “classical\nfirst-order logic”. Section 2 develops a formal language, with a\nrigorous syntax and grammar. The formal language is a recursively\ndefined collection of strings on a fixed alphabet. As such, it has\nno meaning, or perhaps better, the meaning of its formulas is given\nby the deductive system and the semantics. Some of the symbols have\ncounterparts in ordinary language. We define an argument to\nbe a non-empty collection of sentences in the formal language, one of\nwhich is designated to be the conclusion. The other sentences (if\nany) in an argument are its premises. Section 3 sets up a deductive\nsystem for the language, in the spirit of natural deduction. An\nargument is derivable if there is a deduction from some or all of\nits premises to its conclusion. Section 4 provides a model-theoretic\nsemantics. An argument is valid if there is no\ninterpretation (in the semantics) in which its premises are all true\nand its conclusion false. This reflects the longstanding view that a\nvalid argument is truth-preserving.\n\n\n In Section 5, we turn to relationships between the deductive system\nand the semantics, and in particular, the relationship between\nderivability and validity. We show that an argument is derivable only\nif it is valid. This pleasant feature, called soundness,\nentails that no deduction takes one from true premises to a false\nconclusion. Thus, deductions preserve truth. Then we establish a converse, called\ncompleteness, that an argument is valid only if it is\nderivable. This shows that the deductive system is rich enough\nto provide a deduction for every valid argument. So there are enough\ndeductions: all and only valid arguments are derivable. We briefly\nindicate other features of the logic, some of which are corollaries to\nsoundness and completeness.\n\n\nThe final section, Section 6, is devoted to the a brief examination of\nthe philosophical position that classical logic is “the one right\nlogic”.\n\n\n\nToday, logic is a branch of mathematics and a branch of philosophy.\nIn most large universities, both departments offer courses in logic,\nand there is usually a lot of overlap between them. Formal languages,\ndeductive systems, and model-theoretic semantics are mathematical\nobjects and, as such, the logician is interested in their mathematical\nproperties and relations. Soundness, completeness, and most of the\nother results reported below are typical examples. Philosophically,\nlogic is at least closely related to the study of correct\nreasoning. Reasoning is an epistemic, mental activity. So logic\nis at least closely allied with epistemology. Logic is also a central\nbranch of computer science, due, in part, to interesting computational\nrelations in logical systems, and, in part, to the close connection\nbetween formal deductive argumentation and reasoning (see the entries\non\n recursive functions,\n computability and complexity, and\n philosophy of computer science). \nThis raises questions concerning the philosophical relevance of the\nvarious mathematical aspects of logic. How do deducibility and\nvalidity, as properties of formal languages--sets of strings on a\nfixed alphabet--relate to correct reasoning? What do the mathematical\nresults reported below have to do with the original philosophical\nissues concerning valid reasoning? This is an instance of the\nphilosophical problem of explaining how mathematics applies to\nnon-mathematical reality.  \n Typically, ordinary deductive reasoning takes place in a natural language, or\nperhaps a natural language augmented with some mathematical symbols. So\nour question begins with the relationship between a natural language and a\nformal language. Without attempting to be comprehensive, it may help to\nsketch several options on this matter. \n One view is that the formal languages accurately exhibit actual\nfeatures of certain fragments of a natural language. Some\nphilosophers claim that declarative sentences of natural language\nhave underlying logical forms and that these forms are\ndisplayed by formulas of a formal language. Other writers hold that\n(successful) declarative sentences express propositions; and\nformulas of formal languages somehow display the forms of these\npropositions. On views like this, the components of a logic provide\nthe underlying deep structure of correct reasoning. A chunk of\nreasoning in natural language is correct if the forms underlying the\nsentences constitute a valid or deducible argument. See for example,\nMontague [1974], Davidson [1984], Lycan [1984] (and the\nentry on \n logical form). \n Another view, held at least in part by Gottlob Frege and Wilhelm\nLeibniz, is that because natural languages are fraught with vagueness\nand ambiguity, they should be replaced by formal languages. A\nsimilar view, held by W. V. O. Quine (e.g., [1960], [1986]), is that a\nnatural language should be regimented, cleaned up for serious\nscientific and metaphysical work. One desideratum of the enterprise is\nthat the logical structures in the regimented language should be\ntransparent. It should be easy to “read off” the logical\nproperties of each sentence. A regimented language is similar to a\nformal language regarding, for example, the explicitly presented rigor\nof its syntax and its truth conditions. \n On a view like this, deducibility and validity represent\nidealizations of correct reasoning in natural language. A\nchunk of reasoning is correct to the extent that it corresponds to,\nor can be regimented by, a valid or deducible argument in a formal\nlanguage. \nWhen mathematicians and many philosophers engage in deductive\nreasoning, they occasionally invoke formulas in a formal language to\nhelp disambiguate, or otherwise clarify what they mean. In other\nwords, sometimes formulas in a formal language are used in\nordinary reasoning. This suggests that one might think of a formal\nlanguage as an\naddendum to a natural language. Then our present question\nconcerns the relationship between this addendum and the original\nlanguage. What do deducibility and validity, as sharply defined on\nthe addendum, tell us about correct deductive reasoning in general? \n Another view is that a formal language is a mathematical\nmodel of a natural language in roughly the same sense as, say, a\ncollection of point masses is a model of a system of physical objects,\nand the Bohr construction is a model of an atom. In other words, a\nformal language displays certain features of natural languages, or\nidealizations thereof, while ignoring or simplifying other\nfeatures. The purpose of mathematical models is to shed light on what\nthey are models of, without claiming that the model is accurate in all\nrespects or that the model should replace what it is a model of. On a\nview like this, deducibility and validity represent mathematical\nmodels of (perhaps different aspects of) correct reasoning in natural\nlanguages. Correct chunks of deductive reasoning correspond, more or\nless, to valid or deducible arguments; incorrect chunks of reasoning\nroughly correspond to invalid or non-deducible arguments. See, for\nexample, Corcoran [1973], Shapiro [1998], and Cook [2002]. \n There is no need to adjudicate this matter here. Perhaps the truth\nlies in a combination of the above options, or maybe some other\noption is the correct, or most illuminating one. We raise the matter\nonly to lend some philosophical perspective to the formal treatment\nthat follows. \n\nHere we develop the basics of a formal language, or to be precise, a\nclass of formal languages. Again, a formal language is a recursively\ndefined set of strings on a fixed alphabet. Some aspects of the\nformal languages correspond to, or have counterparts in, natural\nlanguages like English. Technically, this “counterpart relation” is\nnot part of the formal development, but we will mention it from time\nto time, to motivate some of the features and results. \n\nWe begin with analogues of singular terms, linguistic items\nwhose function is to denote a person or object. We call these\nterms. We assume a stock of individual constants.\nThese are lower-case letters, near the beginning of the Roman\nalphabet, with or without numerical subscripts: \n We envisage a potential infinity of individual constants. In the\npresent system each constant is a single character, and so individual\nconstants do not have an internal syntax. Thus we have an infinite\nalphabet. This could be avoided by taking a constant like\n\\(d_{22}\\), for example, to consist of three characters,\na lowercase “\\(d\\)” followed by a pair of subscript\n“2”s.  \n We also assume a stock of individual variables. These are\nlower-case letters, near the end of the alphabet, with or without\nnumerical subscripts: \nIn ordinary mathematical reasoning, there are two functions terms need\nto fulfill. We need to be able to denote specific, but unspecified (or\narbitrary) objects, and sometimes we need to express generality. In\nour system, we use some constants in the role of unspecified reference\nand variables to express generality. Both uses are recapitulated in\nthe formal treatment below. Some logicians employ different symbols\nfor unspecified objects (sometimes called “individual\nparameters”) and variables used to express generality.\n \nConstants and variables are the only terms in our formal language, so\nall of our terms are simple, corresponding to proper names and some\nuses of pronouns. We call a term closed if it is not a variable. In general, we use \\(v\\) to represent variables, and \\(t\\)\nto represent a closed term, an individual constant. Some authors also introduce function\nletters, which allow complex terms corresponding to:\n“\\(7+4\\)” and “the wife of Bill Clinton”, or\ncomplex terms containing variables, like “the father of\n\\(x\\)” and “\\(x/y\\)”. Logic books aimed at\nmathematicians are likely to contain function letters, probably due to\nthe centrality of functions in mathematical discourse. Books aimed at\na more general audience (or at philosophy students), may leave out\nfunction letters, since it simplifies the syntax and theory. We follow\nthe latter route here. This is an instance of a general tradeoff\nbetween presenting a system with greater expressive resources, at the\ncost of making its formal treatment more complex.\n \n For each natural number \\(n\\), we introduce a stock of\n\\(n\\)-place predicate letters. These are upper-case\nletters at the beginning or middle of the alphabet. A superscript\nindicates the number of places, and there may or may not be a\nsubscript. For example,  \nare three-place predicate letters. We often omit the superscript, when\nno confusion will result. We also add a special two-place predicate\nsymbol “\\(=\\)” for identity. \nZero-place predicate letters are sometimes called “sentence\nletters”. They correspond to free-standing sentences whose\ninternal structure does not matter. One-place predicate letters,\ncalled “monadic predicate letters”, correspond to\nlinguistic items denoting properties, like “being a man”,\n“being red”, or “being a prime\nnumber”. Two-place predicate letters, called “binary\npredicate letters”, correspond to linguistic items denoting\nbinary relations, like “is a parent of” or “is\ngreater than”. Three-place predicate letters correspond to\nthree-place relations, like “lies on a straight line\nbetween”. And so on.  \n The non-logical terminology of the language consists of its\nindividual constants and predicate letters. The symbol “\\(=\\)”, for\nidentity, is not a non-logical symbol. In taking identity to be\nlogical, we provide explicit treatment for it in the deductive system\nand in the model-theoretic semantics. Most authors do the same, but\nthere is some controversy over the issue (Quine [1986, Chapter\n5]). If \\(K\\) is a set of constants and predicate letters, then\nwe give the fundamentals of a language \\(\\LKe\\) \n built on this set of non-logical terminology. It may be called the\nfirst-order language with identity on \\(K\\). A similar\nlanguage that lacks the symbol for identity (or which takes identity\nto be non-logical) may be called\n \\(\\mathcal{L}1K\\),\nthe first-order language without identity on \\(K\\). \n\nIf \\(V\\) is an \\(n\\)-place predicate letter in \\(K\\),\nand \\(t_1, \\ldots,t_n\\)\nare terms of \\(K\\),\nthen \\(Vt_1 \\ldots t_n\\)\nis an atomic formula of\n \\(\\LKe\\). \n Notice that the terms \\(t_1, \\ldots,t_n\\) need not be distinct. Examples of\natomic formulas include: \n The last one is an analogue of a statement that a certain relation\n\\((A)\\) holds between three objects \\((a, b,\nc)\\). If \\(t_1\\) and \\(t_2\\) are\nterms, then \\(t_1 =t_2\\) is also an\natomic formula of\n \\(\\LKe\\). It corresponds\nto an assertion that \\(t_1\\) is identical to\n\\(t_2\\). \nIf an atomic formula has no variables, then it is called an\natomic sentence. If it does have variables, it is\ncalled open. In the above list of examples, the first and\nsecond are open; the rest are sentences.  \n\nWe now introduce the final items of the lexicon: \n We give a recursive definition of a formula of\n \\(\\LKe\\): \n A formula corresponding to \\(\\neg \\theta\\) thus says that it is not the\n case that \\(\\theta\\). The symbol “\\(\\neg\\)” is called\n “negation”, and is a unary connective. \n The ampersand “\\(\\amp\\)” corresponds to the English\n“and” (when “and” is used to connect\nsentences). So \\((\\theta \\amp \\psi)\\) can be read “\\(\\theta\\) and\n\\(\\psi\\)”. The formula \\((\\theta \\amp \\psi)\\) is called the\n“conjunction” of \\(\\theta\\) and \\(\\psi\\). \nThe symbol “\\(\\vee\\)” corresponds to “either …\nor … or both”, so \\((\\theta \\vee \\psi)\\) can be read\n“\\(\\theta\\) or \\(\\psi\\)”.  The formula \\((\\theta \\vee\n\\psi)\\) is called the “disjunction” of \\(\\theta\\) and\n\\(\\psi\\). \nThe arrow “\\(\\rightarrow\\)” roughly corresponds to\n“if … then … ”, so \\((\\theta \\rightarrow\n\\psi)\\) can be read “if \\(\\theta\\) then \\(\\psi\\)” or\n“\\(\\theta\\) only if \\(\\psi\\)”. \n The symbols “\\(\\amp\\)”, “\\(\\vee\\)”, and\n “\\(\\rightarrow\\)” are called “binary connectives”,\n since they serve to “connect” two formulas into\n one. Some authors introduce \\((\\theta \\leftrightarrow \\psi)\\) as an abbreviation\n of \\(((\\theta \\rightarrow \\psi) \\amp(\\psi \\rightarrow \\theta))\\). The symbol\n “\\(\\leftrightarrow\\)” is an analogue of the locution “if and\n only if”. \nThe symbol “\\(\\forall\\)” is called a universal\nquantifier, and is an analogue of “for all”; so\n\\(\\forall v\\theta\\) can be read “for all \\(v,\n\\theta\\)”. \n The symbol “\\(\\exists\\)” is called an\nexistential quantifier, and is an analogue of “there\nexists” or “there is”; so \\(\\exists v \\theta\\)\ncan be read “there is a \\(v\\) such that \\(\\theta\\)”. \n Clause (8) allows us to do inductions on the complexity of\nformulas. If a certain property holds of the atomic formulas and is\nclosed under the operations presented in clauses (2)–(7), then the\nproperty holds of all formulas. Here is a simple example: \n Theorem 1. Every formula of\n \\(\\LKe\\)\n has the same number of left and right parentheses. Moreover, each\nleft parenthesis corresponds to a unique right parenthesis, which\noccurs to the right of the left parenthesis. Similarly, each right\nparenthesis corresponds to a unique left parenthesis, which occurs to\nthe left of the given right parenthesis. If a parenthesis occurs\nbetween a matched pair of parentheses, then its mate also occurs\nwithin that matched pair. In other words, parentheses that occur\nwithin a matched pair are themselves matched.  \n Proof: By clause (8), every formula is built up\nfrom the atomic formulas using clauses (2)–(7). The atomic formulas\nhave no parentheses. Parentheses are introduced only in clauses (3)–(5), and\neach time they are introduced as a matched set. So at any stage in\nthe construction of a formula, the parentheses are paired off. \nWe next define the notion of an occurrence of a variable being\nfree or bound in a formula. A variable that\nimmediately follows a quantifier (as in “\\(\\forall x\\)”\nand “\\(\\exists y\\)”) is neither free nor bound. We do not\neven think of those as occurrences of the variable. All variables that\noccur in an atomic formula are free. If a variable occurs free (or\nbound) in \\(\\theta\\) or in \\(\\psi\\), then that same occurrence is free\n(or bound) in \\(\\neg \\theta, (\\theta \\amp \\psi), (\\theta \\vee \\psi)\\),\nand \\((\\theta \\rightarrow \\psi)\\). That is, the (unary and binary)\nconnectives do not change the status of variables that occur in\nthem. All occurrences of the variable \\(v\\) in \\(\\theta\\) are bound in\n\\(\\forall v \\theta\\) and \\(\\exists v \\theta\\). Any free\noccurrences of \\(v\\) in \\(\\theta\\) are bound by the initial\nquantifier. All other variables that occur in \\(\\theta\\) are free or\nbound in \\(\\forall v \\theta\\) and \\(\\exists v \\theta\\), as they are in\n\\(\\theta\\). \n For example, in the formula \n \\((\\forall\\)x(Axy\n \\(\\vee Bx) \\amp Bx)\\), the occurrences of “\\(x\\)” in \n Axy and in the first \\(Bx\\) are bound by the\nquantifier. The occurrence of “\\(y\\)” and last\noccurrence of “\\(x\\)” are free. In\n\\(\\forall x(Ax \\rightarrow \\exists\\)xBx), the\n“\\(x\\)” in \\(Ax\\) is bound by the initial\nuniversal quantifier, while the other occurrence of \\(x\\) is\nbound by the existential quantifier. The above syntax allows this\n“double-binding”. Although it does not create any\nambiguities (see below), we will avoid such formulas, as a matter of\ntaste and clarity. \nThe syntax also allows so-called vacuous binding, as in\n\\(\\forall\\)x\\(Bc\\). These, too, will be avoided in what follows.\nSome treatments of logic rule out vacuous binding and double binding\nas a matter of syntax. That simplifies some of the treatments below,\nand complicates others. \n Free variables correspond to place-holders, while bound variables\nare used to express generality. If a formula has no free variables,\nthen it is called a sentence. If a formula has free\nvariables, it is called open.  \n\nBefore turning to the deductive system and semantics, we mention a few\nfeatures of the language, as developed so far. This helps draw the\ncontrast between formal languages and natural languages like English.  \n We assume at the outset that all of the categories are disjoint. For\nexample, no connective is also a quantifier or a variable, and the\nnon-logical terms are not also parentheses or connectives. Also, the\nitems within each category are distinct. For example, the sign for\ndisjunction does not do double-duty as the negation symbol, and\nperhaps more significantly, no two-place predicate is also a\none-place predicate. \n One difference between natural languages like English and formal\nlanguages like\n \\(\\LKe\\) is that the latter are not\nsupposed to have any ambiguities. The policy that the different\ncategories of symbols do not overlap, and that no symbol does\ndouble-duty, avoids the kind of ambiguity, sometimes called\n“equivocation”, that occurs when a single word has two meanings:\n“I’ll meet you at the bank.” But there are other kinds of\nambiguity. Consider the English sentence: \n John is married, and Mary is single, or Joe is crazy.\n \n It can mean that John is married and either Mary is single or Joe is\ncrazy, or else it can mean that either both John is married and Mary\nis single, or else Joe is crazy. An ambiguity like this, due to\ndifferent ways to parse the same sentence, is sometimes called an\n“amphiboly”. If our formal language did not have the\nparentheses in it, it would have amphibolies. For example, there would\nbe a “formula” \\(A \\amp B \\vee\\)\nC. Is this supposed to be \\(((A \\amp B)\n\\vee C)\\), or is it \\((A \\amp(B \\vee C))\\)? The parentheses resolve what would be an\namphiboly. \n Can we be sure that there are no other amphibolies in our language?\nThat is, can we be sure that each formula of\n \\(\\LKe\\) can be put\ntogether in only one way? Our next task is to answer this question.  \n Let us temporarily use the term “unary marker” for the negation\nsymbol \\((\\neg)\\) or a quantifier followed by a variable (e.g.,\n \\(\\forall x,\n \\exists z)\\). \n Lemma 2. Each formula consists of a string of zero\nor more unary markers followed by either an atomic formula or a\nformula produced using a binary connective, via one of clauses\n(3)–(5).  \n Proof: We proceed by induction on the complexity of\nthe formula or, in other words, on the number of formation rules that\nare applied. The Lemma clearly holds for atomic formulas. Let \\(n\\) be\na natural number, and suppose that the Lemma holds for any formula\nconstructed from \\(n\\) or fewer instances of clauses\n(2)–(7). Let \\(\\theta\\) be a formula constructed from \\(n+1\\)\ninstances. The Lemma holds if the last clause used to construct\n\\(\\theta\\) was either (3), (4), or (5). If the last clause used to\nconstruct \\(\\theta\\) was (2), then \\(\\theta\\) is \\(\\neg \\psi\\).  Since\n\\(\\psi\\) was constructed with \\(n\\) instances of the rule, the Lemma\nholds for \\(\\psi\\) (by the induction hypothesis), and so it holds for\n\\(\\theta\\).  Similar reasoning shows the Lemma to hold for \\(\\theta\\)\nif the last clause was (6) or (7). By clause (8), this exhausts the\ncases, and so the Lemma holds for \\(\\theta\\), by induction. \n Lemma 3. If a formula\n \\(\\theta\\) contains a left parenthesis,\nthen it ends with a right parenthesis, which matches the leftmost left\nparenthesis in\n \\(\\theta\\). \n Proof: Here we also proceed by induction on the\nnumber of instances of (2)–(7) used to construct the\nformula. Clearly, the Lemma holds for atomic formulas, since they\nhave no parentheses. Suppose, then, that the Lemma holds for formulas\nconstructed with \\(n\\) or fewer instances of (2)–(7), and let\n \\(\\theta\\) be constructed with\n\\(n+1\\)\n instances. If the last clause applied was (3)–(5), then the Lemma\nholds since\n \\(\\theta\\) itself begins with a left\nparenthesis\n and ends with the matching right parenthesis. If the last clause\napplied was\n (2), then\n \\(\\theta\\) is\n \\(\\neg \\psi\\),\n and the induction hypothesis applies to\n \\(\\psi\\).\n Similarly, if the last clause applied was (6) or (7), then\n \\(\\theta\\)\n consists of a quantifier, a variable, and a formula to which we can\napply the\n induction hypothesis. It follows that the Lemma holds for\n \\(\\theta\\). \n Lemma 4. Each formula contains at least one atomic\nformula. \n The proof proceeds by induction on the number of instances of (2)–(7)\nused to construct the formula, and we leave it as an exercise.\n \n Theorem 5. Let \\(\\alpha, \\beta\\) be nonempty\nsequences of characters on our alphabet, such that \\(\\alpha \\beta\\)\n(i.e \\(\\alpha\\) followed by \\(\\beta)\\) is a formula. Then \\(\\alpha\\)\nis not a formula. \n Proof: By Theorem 1 and Lemma 3, if \\(\\alpha\\)\ncontains a left parenthesis, then the right parenthesis that matches\nthe leftmost left parenthesis in \\(\\alpha \\beta\\) comes at the end of\n\\(\\alpha \\beta\\), and so the matching right parenthesis is in\n\\(\\beta\\).  So, \\(\\alpha\\) has more left parentheses than right\nparentheses. By Theorem \\(1, \\alpha\\) is not a formula. So now suppose\nthat \\(\\alpha\\) does not contain any left parentheses. By Lemma \\(2,\n\\alpha \\beta\\) consists of a string of zero or more unary markers\nfollowed by either an atomic formula or a formula produced using a\nbinary connective, via one of clauses (3)–(5). If the latter\nformula was produced via one of clauses (3)–(5), then it begins\nwith a left parenthesis. Since \\(\\alpha\\) does not contain any\nparentheses, it must be a string of unary markers. But then \\(\\alpha\\)\ndoes not contain any atomic formulas, and so by Lemma \\(4, \\alpha\\) is\nnot a formula. The only case left is where \\(\\alpha \\beta\\) consists\nof a string of unary markers followed by an atomic formula, either in\nthe form \\(t_1 =t_2\\) or \\(Pt_1 \\ldots t_n\\).  Again, if \\(\\alpha\\)\njust consisted of unary markers, it would not be a formula, and so\n\\(\\alpha\\) must consist of the unary markers that start \\(\\alpha\n\\beta\\), followed by either \\(t_1\\) by itself, \\(t_1 =\\) by itself, or\nthe predicate letter \\(P\\), and perhaps some (but not all) of the\nterms \\(t_1, \\ldots,t_n\\).  In the first two cases, \\(\\alpha\\) does\nnot contain an atomic formula, by the policy that the categories do\nnot overlap. Since \\(P\\) is an \\(n\\)-place predicate letter, by the\npolicy that the predicate letters are distinct, \\(P\\) is not an\n\\(m\\)-place predicate letter for any \\(m \\ne n\\). So the part of\n\\(\\alpha\\) that consists of \\(P\\) followed by the terms is not an\natomic formula. In all of these cases, then, \\(\\alpha\\) does not\ncontain an atomic formula. By Lemma \\(4, \\alpha\\) is not a\nformula. We are finally in position to show that there is no amphiboly in our\nlanguage. \n Theorem 6. Let \\(\\theta\\) be any formula of\n\\(\\LKe\\).  If \\(\\theta\\) is not atomic, then there is one and only one\namong (2)–(7) that was the last clause applied to construct\n\\(\\theta\\).  That is, \\(\\theta\\) could not be produced by two\ndifferent clauses. Moreover, no formula produced by clauses\n(2)–(7) is atomic. \n Proof: By Clause (8), either \\(\\theta\\) is atomic or\nit was produced by one of clauses (2)–(7). Thus, the first\nsymbol in \\(\\theta\\) must be either a predicate letter, a term, a\nunary marker, or a left parenthesis. If the first symbol in \\(\\theta\\)\nis a predicate letter or term, then \\(\\theta\\) is atomic. In this\ncase, \\(\\theta\\) was not produced by any of (2)–(7), since all\nsuch formulas begin with something other than a predicate letter or\nterm. If the first symbol in \\(\\theta\\) is a negation sign\n“\\(\\neg\\)”, then was \\(\\theta\\) produced by clause (2),\nand not by any other clause (since the other clauses produce formulas\nthat begin with either a quantifier or a left parenthesis). Similarly,\nif \\(\\theta\\) begins with a universal quantifier, then it was produced\nby clause (6), and not by any other clause, and if \\(\\theta\\) begins\nwith an existential quantifier, then it was produced by clause (7),\nand not by any other clause. The only case left is where \\(\\theta\\)\nbegins with a left parenthesis. In this case, it must have been\nproduced by one of (3)–(5), and not by any other clause. We only\nneed to rule out the possibility that \\(\\theta\\) was produced by more\nthan one of (3)–(5). To take an example, suppose that \\(\\theta\\)\nwas produced by (3) and (4). Then \\(\\theta\\) is \\((\\psi_1 \\amp\n\\psi_2)\\) and \\(\\theta\\) is also \\((\\psi_3 \\vee \\psi_4)\\), where\n\\(\\psi_1, \\psi_2, \\psi_3\\), and \\(\\psi_4\\) are themselves\nformulas. That is, \\((\\psi_1 \\amp \\psi_2)\\) is the very same formula\nas \\((\\psi_3 \\vee \\psi_4)\\). By Theorem \\(5, \\psi_1\\) cannot be a\nproper part of \\(\\psi_3\\), nor can \\(\\psi_3\\) be a proper part of\n\\(\\psi_1\\). So \\(\\psi_1\\) must be the same formula as \\(\\psi_3\\). But\nthen “\\(\\amp\\)” must be the same symbol as\n“\\(\\vee\\)”, and this contradicts the policy that all of\nthe symbols are different. So \\(\\theta\\) was not produced by both\nClause (3) and Clause (4). Similar reasoning takes care of the other\ncombinations. \n This result is sometimes called “unique readability”. It\nshows that each formula is produced from the atomic formulas via the\nvarious clauses in exactly one way. If \\(\\theta\\) was produced by\nclause (2), then its main connective is the initial\n“\\(\\neg\\)”. If \\(\\theta\\) was produced by clauses (3),\n(4), or (5), then its main connective is the introduced\n“\\(\\amp\\)”, “\\(\\vee\\)”, or\n“\\(\\rightarrow\\)”, respectively. If \\(\\theta\\) was\nproduced by clauses (6) or (7), then its main connective is\nthe initial quantifier. We apologize for the tedious details. We\nincluded them to indicate the level of precision and rigor for the\nsyntax. \n\nWe now introduce a deductive system, \\(D\\), for our\nlanguages. As above, we define an argument to be a non-empty\ncollection of sentences in the formal language, one of which is\ndesignated to be the conclusion. If there are any other\nsentences in the argument, they are its\n premises.[1]\n By convention, we use “\\(\\Gamma\\)”,\n“\\(\\Gamma'\\)”, “\\(\\Gamma_1\\)”, etc, to range\nover sets of sentences, and we use the letters “\\(\\phi\\)”,\n“\\(\\psi\\)”, “\\(\\theta\\)”, uppercase or\nlowercase, with or without subscripts, to range over single\nsentences. We write “\\(\\Gamma, \\Gamma'\\)” for the union of\n\\(\\Gamma\\) and \\(\\Gamma'\\), and “\\(\\Gamma, \\phi\\)” for the\nunion of \\(\\Gamma\\) with \\(\\{\\phi\\}\\). \nWe write an argument in the form \\(\\langle \\Gamma, \\phi \\rangle\\),\nwhere \\(\\Gamma\\) is a set of sentences, the premises, and \\(\\phi\\) is\na single sentence, the conclusion. Remember that \\(\\Gamma\\) may be\nempty. We write \\(\\Gamma \\vdash \\phi\\) to indicate that \\(\\phi\\) is\ndeducible from \\(\\Gamma\\), or, in other words, that the argument\n\\(\\langle \\Gamma, \\phi \\rangle\\) is deducible in \\(D\\). We may write\n\\(\\Gamma \\vdash_D \\phi\\) to emphasize the deductive system \\(D\\). We\nwrite \\(\\vdash \\phi\\) or \\(\\vdash_D \\phi\\) to indicate that \\(\\phi\\)\ncan be deduced (in \\(D)\\) from the empty set of premises. \n The rules in \\(D\\) are chosen to match logical relations\nconcerning the English analogues of the logical terminology in the\nlanguage. Again, we define the deducibility relation by recursion. We\nstart with a rule of assumptions:\n  \nWe thus have that \\(\\{\\phi \\}\\vdash \\phi\\); each premise follows\nfrom itself. We next present two clauses for each connective and\nquantifier. The clauses indicate how to “introduce” and\n“eliminate” sentences in which each symbol is the main\nconnective.  \nFirst, recall that “\\(\\amp\\)” is an analogue of the\nEnglish connective “and”. Intuitively, one can deduce a\nsentence in the form \\((\\theta \\amp \\psi)\\) if one has deduced\n\\(\\theta\\) and one has deduced \\(\\psi\\). Conversely, one can deduce\n\\(\\theta\\) from \\((\\theta \\amp \\psi)\\) and one can deduce \\(\\psi\\)\nfrom \\((\\theta \\amp \\psi)\\): \nThe name “&I” stands for\n“&-introduction”; “&E” stands for\n“&-elimination”. \nSince, the symbol “\\(\\vee\\)” corresponds to the English\n“or”, \\((\\theta \\vee \\psi)\\) should be deducible from\n\\(\\theta\\), and \\((\\theta \\vee \\psi)\\) should also be deducible from\n\\(\\psi\\):\n \nThe elimination rule is a bit more complicated. Suppose that\n“\\(\\theta\\) or \\(\\psi\\)” is true. Suppose also that\n\\(\\phi\\) follows from \\(\\theta\\) and that \\(\\phi\\) follows from\n\\(\\psi\\). One can reason that if \\(\\theta\\) is true, then \\(\\phi\\) is\ntrue. If instead \\(\\psi\\) is true, we still have that \\(\\phi\\) is\ntrue.  So either way, \\(\\phi\\) must be true. \n For the next clauses, recall that the symbol, “\\(\\rightarrow\\)”, is\n an analogue of the English “if … then … ”\n construction. If one knows, or assumes \\((\\theta \\rightarrow \\psi)\\) and\n also knows, or assumes \\(\\theta\\), then one can conclude\n \\(\\psi\\). Conversely, if one deduces \\(\\psi\\) from an assumption \\(\\theta\\),\n then one can conclude that \\((\\theta \\rightarrow \\psi)\\). \nThis elimination rule is sometimes called “modus ponens”.\nIn some logic texts, the introduction rule is proved as a\n“deduction theorem”. \nOur next clauses are for the negation sign, “\\(\\neg\\)”. The\nunderlying idea is that a sentence \\(\\psi\\) is inconsistent with its\nnegation \\(\\neg \\psi\\). They cannot both be true. We call a pair of\nsentences \\(\\psi, \\neg \\psi\\) contradictory opposites. If one\ncan deduce such a pair from an assumption \\(\\theta\\), then one can\nconclude that \\(\\theta\\) is false, or, in other words, one can conclude\n\\(\\neg \\theta\\). \nBy (As), we have that \\(\\{A,\\neg A\\}\\vdash A\\) and\n\\(\\{\\)A,\\(\\neg\\)A\\(\\}\\vdash \\neg A\\). So by \\(\\neg\\)I we have\nthat \\(\\{A\\}\\vdash \\neg \\neg A\\). However, we do not have the converse\nyet. Intuitively, \\(\\neg \\neg \\theta\\) corresponds to “it is not\nthe case that it is not the case that” . One might think that\nthis last is equivalent to \\(\\theta\\), and we have a rule to that\neffect:\n \nThe name DNE stands for “double-negation\nelimination”. There is some controversy over this inference. It\nis rejected by philosophers and mathematicians who do not hold that\neach meaningful sentence is either true or not\ntrue. Intuitionistic logic does not sanction the inference in\nquestion (see, for example Dummett [2000], or the entry on\n intuitionistic logic, or\n history of intuitionistic logic),\nbut, again, classical logic does. \n To illustrate the parts of the deductive system \\(D\\) presented\nthus far, we show that \\(\\vdash(A \\vee \\neg A)\\): \nThe principle \\((\\theta \\vee \\neg \\theta)\\) is sometimes called the \nlaw of excluded middle. It is not valid in intuitionistic\nlogic.  Let \\(\\theta, \\neg \\theta\\) be a pair of contradictory opposites,\nand let \\(\\psi\\) be any sentence at all. By (As) we have \\(\\{\\theta,\n\\neg \\theta, \\neg \\psi \\}\\vdash \\theta\\) and \\(\\{\\theta, \\neg \\theta,\n\\neg \\psi \\}\\vdash \\neg \\theta\\). So by \\((\\neg\\)I), \\(\\{\\theta, \\neg\n\\theta \\}\\vdash \\neg \\neg \\psi\\). So, by (DNE) we have \\(\\{\\theta ,\n\\neg \\theta \\}\\vdash \\psi\\) . That is, anything at all follows from a\npair of contradictory opposites. Some logicians introduce a rule to\ncodify a similar inference:  \nIf \\(\\Gamma_1 \\vdash \\theta\\) and \\(\\Gamma_2 \\vdash \\neg \\theta\\),\nthen for any sentence \\(\\psi, \\Gamma_1, \\Gamma_2 \\vdash \\psi\\)\n \nThe inference is sometimes called ex falso quodlibet or, more\ncolorfully, explosion. Some call it\n“\\(\\neg\\)-elimination”, but perhaps this stretches the notion\nof “elimination” a bit. We do not officially\ninclude ex falso quodlibet as a separate rule in \\(D\\),\nbut as will be shown below (Theorem 10), each instance of it is\nderivable in our system \\(D\\). \n Some logicians object to ex falso quodlibet, on the ground\nthat the sentence \\(\\psi\\) may be irrelevant to any of the\npremises in \\(\\Gamma\\). Suppose, for example, that one starts with some\npremises \\(\\Gamma\\) about human nature and facts about certain people,\nand then deduces both the sentence “Clinton had extra-marital\nsexual relations” and “Clinton did not have extra-marital\nsexual relations”. One can perhaps conclude that there is\nsomething wrong with the premises \\(\\Gamma\\). But should we be allowed to\nthen deduce anything at all from \\(\\Gamma\\)? Should we be\nallowed to deduce “The economy is sound”? A small minority of logicians, called dialetheists, hold\nthat some contradictions are actually true. For them, ex falso\nquodlibet is not truth-preserving. \nDeductive systems that demur from ex falso quodlibet are\ncalled paraconsistent. Most relevant logics are\nparaconsistent. See the entries on \n relevance logic, \n paraconsistent logic,\n and\n dialetheism. \nOr see Anderson and Belnap [1975], Anderson, Belnap, and Dunn [1992],\nand Tennant [1997] for fuller overviews of relevant logic; and Priest\n[2006],[2006a] for dialetheism. Deep philosophical issues concerning\nthe nature of \n logical consequence\n are involved. Far be it for\nan article in a philosophy encyclopedia to avoid philosophical issues,\nbut space considerations preclude a fuller treatment of this issue\nhere. Suffice it to note that the inference ex falso\nquodlibet is sanctioned in systems of classical logic,\nthe subject of this article. It is essential to establishing the\nbalance between the deductive system and the semantics (see §5\nbelow). \n The next pieces of \\(D\\) are the clauses for the quantifiers. Let\n \\(\\theta\\) be a formula, \\(v\\) a\nvariable, and \\(t\\) a term (i.e., a variable or a constant). Then\ndefine\n \\(\\theta(v|t)\\) to be\nthe result of substituting \\(t\\) for each free occurrence of\n\\(v\\) in\n \\(\\theta\\). So, if\n \\(\\theta\\) is \\((Qx \\amp \\exists\\)xPxy), then\n \\(\\theta(x|c)\\) is\n\\((Qc \\amp \\exists\\)xPxy). The last\noccurrence of \\(x\\) is not free. \nA sentence in the form \\(\\forall v \\theta\\) is an analogue of the\nEnglish “for every \\(v, \\theta\\) holds”. So one should be\nable to infer \\(\\theta(v|t)\\) from \\(\\forall v \\theta\\) for any closed\nterm \\(t\\). Recall that the only closed terms in our system are\nconstants. \nThe idea here is that if \\(\\forall v \\theta\\) is true, then \\(\\theta\\)\nshould hold of \\(t\\), no matter what \\(t\\) is. \nThe introduction clause for the universal quantifier is a bit more\ncomplicated. Suppose that a sentence \\(\\theta\\) contains a closed term\n\\(t\\), and that \\(\\theta\\) has been deduced from a set of premises\n\\(\\Gamma\\). If the closed term \\(t\\) does not occur in any member of\n\\(\\Gamma\\), then \\(\\theta\\) will hold no matter which object \\(t\\) may\ndenote. That is, \\(\\forall v \\theta\\) follows.  \nThis rule \\((\\forall \\mathbf{I})\\) corresponds to a common inference in\nmathematics. Suppose that a mathematician says “let \\(n\\)\nbe a natural number” and goes on to show that \\(n\\) has a\ncertain property \\(P\\), without assuming anything\nabout \\(n\\) (except that it is a natural number). She then\nreminds the reader that \\(n\\) is “arbitrary”, and\nconcludes that \\(P\\) holds for all natural numbers. The\ncondition that the term\n\\(t\\) not occur in any premise is what guarantees that it is\nindeed “arbitrary”. It could be any object, and so anything we\nconclude about it holds for all objects.  \n The existential quantifier is an analogue of the English expression\n“there exists”, or perhaps just “there is”. If\nwe have established (or assumed) that a given object \\(t\\) has a\ngiven property, then it follows that there is something that has that\nproperty.   \nThe elimination rule for \\(\\exists\\) is not quite as simple: \n This elimination rule also corresponds to a common inference. Suppose\nthat a mathematician assumes or somehow concludes that there is a\nnatural number with a given property \\(P\\). She then says “let\n\\(n\\) be such a natural number, so that \\(Pn\\)”, and goes on to\nestablish a sentence \\(\\phi\\), which does not mention the number\n\\(n\\). If the derivation of \\(\\phi\\) does not invoke anything about\n\\(n\\) (other than the assumption that it has the given property\n\\(P)\\), then \\(n\\) could have been any number that has the property\n\\(P\\). That is, \\(n\\) is an arbitrary number with property\n\\(P\\). It does not matter which number \\(n\\) is. Since\n\\(\\phi\\) does not mention \\(n\\), it follows from the assertion that\nsomething has property \\(P\\). The provisions added to \\((\\exists\\)E)\nare to guarantee that \\(t\\) is “arbitrary”. \n The final items are the rules for the identity sign “=”. The\nintroduction rule is about a simple as can be: \n This “inference” corresponds to the truism that everything is\nidentical to itself. The elimination rule corresponds to a principle\nthat if \\(a\\) is identical to \\(b\\), then anything true of\n\\(a\\) is also true of \\(b\\).  \n The rule \\(({=}\\mathrm{E})\\) indicates a certain restriction in the\nexpressive resources of our language. Suppose, for example, that Harry\nis identical to Donald (since his mischievous parents gave him two\nnames). According to most people’s intuitions, it would not\nfollow from this and “Dick knows that Harry is wicked”\nthat “Dick knows that Donald is wicked”, for the reason\nthat Dick might not know that Harry is identical to Donald. Contexts\nlike this, in which identicals cannot safely be substituted for each\nother, are called “opaque”. We assume that our language\n\\(\\LKe\\) has no opaque contexts. \n One final clause completes the description of the deductive system\n\\(D\\): \n Again, this clause allows proofs by induction on the rules used to\nestablish an argument. If a property of arguments holds of all\ninstances of (As) and \\(({=}\\mathrm{I})\\), and if the other rules\npreserve the property, then every argument that is deducible in \\(D\\)\nenjoys the property in question. \nBefore moving on to the model theory for \\(\\LKe\\), we pause to note a\nfew features of the deductive system. To illustrate the level of\nrigor, we begin with a lemma that if a sentence does not contain a\nparticular closed term, we can make small changes to the set of\nsentences we prove it from without problems. We allow ourselves the\nliberty here of extending some previous notation: for any terms \\(t\\)\nand \\(t'\\), and any formula \\(\\theta\\), we say that \\(\\theta(t|t')\\)\nis the result of replacing all free occurrences of \\(t\\) in \\(\\theta\\)\nwith \\(t'\\). \nLemma 7.  If \\(\\Gamma_1\\) and \\(\\Gamma_2\\) differ\nonly in that wherever \\(\\Gamma_1\\) contains \\(\\theta\\), \\(\\Gamma_2\\)\ncontains \\(\\theta(t|t')\\), then for any sentence \\(\\phi\\) not\ncontaining \\(t\\) or \\(t'\\), if \\(\\Gamma_1\\vdash\\phi\\) then\n\\(\\Gamma_2\\vdash\\phi\\).\n  \nProof: The proof proceeds by induction on the number\nof steps in the proof of \\(\\phi\\). Crucial to this proof is the fact\nthat \\(\\theta=\\theta(t|t')\\) whenever \\(\\theta\\) does not contain\n\\(t\\) or \\(t'\\). When the number of steps in the proof of \\(\\phi\\) is\none, this means that the last (and only) rule applied is (As) or\n(=I). Then, since \\(\\phi\\) does not contain \\(t\\) or \\(t'\\), if\n\\(\\Gamma_1\\vdash\\phi\\) we simply apply the same rule ((As) or (=I)) to\n\\(\\Gamma_2\\) to get \\(\\Gamma_2\\vdash\\phi\\). Assume that there are\n\\(n>1\\) steps in the proof of \\(\\phi\\), and that Lemma 7 holds for any\nproof with less than \\(n\\) steps. Suppose that the \\(n^{th}\\) rule\napplied to \\(\\Gamma_1\\) was (\\(\\amp I\\)). Then \\(\\phi\\) is\n\\(\\psi\\amp\\chi\\), and \\(\\Gamma_1\\vdash\\phi\\amp\\chi\\). But then we know\nthat previous steps in the proof include \\(\\Gamma_1\\vdash\\psi\\) and\n\\(\\Gamma_1\\vdash\\chi\\), and by induction, we have\n\\(\\Gamma_2\\vdash\\psi\\) and \\(\\Gamma_2\\vdash\\chi\\), since neither\n\\(\\psi\\) nor \\(\\chi\\) contain \\(t\\) or \\(t'\\). So, we simply apply\n(\\(\\amp I\\)) to \\(\\Gamma_2\\) to get \\(\\Gamma_2\\vdash\\psi\\amp\\chi\\) as\nrequired. Suppose now that the last step applied in the proof of\n\\(\\Gamma_1\\vdash\\phi\\) was (\\(\\amp E\\)). Then, at a previous step in\nthe proof of \\(\\phi\\), we know \\(\\Gamma_1\\vdash\\phi\\amp\\psi\\) for some\nsentence \\(\\psi\\). If \\(\\psi\\) does not contain \\(t\\), then we simply\napply (\\(\\amp E\\)) to \\(\\Gamma_2\\) to obtain the desired result. The\nonly complication is if \\(\\psi\\) contains \\(t\\). Then we would have\nthat \\(\\Gamma_2\\vdash (\\phi\\amp\\psi)(t|t')\\). But, since\n\\((\\phi\\amp\\psi)(t|t')\\) is \\(\\phi(t|t')\\amp\\psi(t|t')\\), and\n\\(\\phi(t|t')\\) is just \\(\\phi\\), we can just apply (\\(\\amp E\\)) to get\n\\(\\Gamma_2\\vdash\\phi\\) as required. The cases for the other rules are\nsimilar. \nTheorem 8. The rule of Weakening. If \\(\\Gamma_1\n\\vdash \\phi\\) and \\(\\Gamma_1 \\subseteq \\Gamma_2\\), then \\(\\Gamma_2\n\\vdash \\phi\\). \nProof: Again, we proceed by induction on the number\nof rules that were used to arrive at \\(\\Gamma_1 \\vdash \\phi\\).\nSuppose that \\(n\\gt 0\\) is a natural number, and that the theorem\nholds for any argument that was derived using fewer than \\(n\\)\nrules. Suppose that \\(\\Gamma_1 \\vdash \\phi\\) using exactly \\(n\\)\nrules. If \\(n=1\\), then the rule is either (As) or \\((=\\)I). In these\ncases, \\(\\Gamma_2 \\vdash \\phi\\) by the same rule. If the last rule\napplied was (&I), then \\(\\phi\\) has the form \\((\\theta \\amp\n\\psi)\\), and we have \\(\\Gamma_3 \\vdash \\theta\\) and \\(\\Gamma_4 \\vdash\n\\psi\\), with \\(\\Gamma_1 = \\Gamma_3, \\Gamma_4\\).  We apply the\ninduction hypothesis to the deductions of \\(\\theta\\) and \\(\\psi\\), to\nget \\(\\Gamma_2 \\vdash \\theta\\) and \\(\\Gamma_2 \\vdash \\psi\\).  and then\napply (&I) to the result to get \\(\\Gamma_2 \\vdash \\phi\\).  Most of\nthe other cases are exactly like this. Slight complications arise only\nin the rules \\((\\forall\\)I) and \\((\\exists\\)E), because there we have\nto pay attention to the conditions for the rules. \nSuppose that the last rule applied to get \\(\\Gamma_1 \\vdash \\phi\\) is\n\\((\\forall\\)I). So \\(\\phi\\) is a sentence of the form \\(\\forall\nv\\theta\\), and we have \\(\\Gamma_1 \\vdash \\theta (v|t)\\) and \\(t\\) does\noccur in any member of \\(\\Gamma_1\\) or in \\(\\theta\\).  The problem is\nthat \\(t\\) may occur in a member of \\(\\Gamma_2\\), and so we cannot\njust invoke the induction hypothesis and apply \\((\\forall\\)I) to the\nresult. So, let \\(t'\\) be a term not occurring in any sentence in\n\\(\\Gamma_2\\). Let \\(\\Gamma'\\) be the result of substituting \\(t'\\) for\nall \\(t\\) in \\(\\Gamma_2\\). Then, since \\(t\\) does not occur in\n\\(\\Gamma_1\\), \\(\\Gamma_1\\subseteq\\Gamma'\\). So, the induction\nhypothesis gives us \\(\\Gamma'\\vdash\\theta (v|t)\\), and we know that\n\\(\\Gamma'\\) does not contain \\(t\\), so we can apply (\\(\\forall I\\)) to\nget \\(\\Gamma'\\vdash\\forall v\\theta\\). But \\(\\forall v\\theta\\) does not\ncontain \\(t\\) or \\(t'\\), so \\(\\Gamma_2\\vdash\\forall v\\theta\\) by Lemma\n7. \nSuppose that the last rule applied was \\((\\exists\\)E), we have\n\\(\\Gamma_3 \\vdash \\exists v\\theta\\) and \\(\\Gamma_4, \\theta (v|t)\n\\vdash \\phi\\), with \\(\\Gamma_1\\) being \\(\\Gamma_3, \\Gamma_4\\), and\n\\(t\\) not in \\(\\phi\\), \\(\\Gamma_4\\) or \\(\\theta\\).  If \\(t\\) does not\noccur free in \\(\\Gamma_2\\), we apply the induction hypothesis to get\n\\(\\Gamma_2 \\vdash \\exists v\\theta\\), and then \\((\\exists\\)E) to end up\nwith \\(\\Gamma_2 \\vdash \\phi\\). If \\(t\\) does occur free in\n\\(\\Gamma_2\\), then we follow a similar proceedure to \\(\\forall I\\),\nusing Lemma 7.  \n Theorem 8 allows us to add on premises at will. It follows that\n \\(\\Gamma \\vdash \\phi\\) if and only if there is a subset\n \\(\\Gamma'\\subseteq \\Gamma\\) such that\n \\(\\Gamma'\\vdash \\phi\\). Some systems of relevant\n logic do not have weakening, nor does substructural logic (See the\n entries on\n relevance logic,\n substructural logics,\nand\n linear logic). \nBy clause (*), all derivations are established in a finite number of steps. \nSo we have \nTheorem 9.  \\(\\Gamma \\vdash \\phi\\) if and only if\nthere is a finite \\(\\Gamma'\\subseteq \\Gamma\\) such that\n\\(\\Gamma'\\vdash \\phi\\).  \nTheorem 10. The rule of ex falso quodlibet\nis a “derived rule” of \\(D\\): if \\(\\Gamma_1 \\vdash\n\\theta\\) and \\(\\Gamma_2 \\vdash \\neg \\theta\\), then \\(\\Gamma_1,\\Gamma_2\n\\vdash \\psi\\), for any sentence \\(\\psi\\).  \nProof: Suppose that \\(\\Gamma_1 \\vdash \\theta\\) and\n\\(\\Gamma_2 \\vdash \\neg \\theta\\). Then by Theorem \\(8, \\Gamma_1,\\neg\n\\psi \\vdash \\theta\\), and \\(\\Gamma_2,\\neg \\psi \\vdash \\neg\n\\theta\\). So by \\((\\neg\\)I), \\(\\Gamma_1, \\Gamma_2 \\vdash \\neg \\neg\n\\psi\\). By (DNE), \\(\\Gamma_1, \\Gamma_2 \\vdash \\psi\\).  \nTheorem 11. The rule of Cut.  If \\(\\Gamma_1 \\vdash\n\\psi\\) and \\(\\Gamma_2, \\psi \\vdash \\theta\\), then \\(\\Gamma_1, \\Gamma_2\n\\vdash \\theta\\). \nProof: Suppose \\(\\Gamma_1 \\vdash \\psi\\) and\n\\(\\Gamma_2, \\psi \\vdash \\theta\\).  We proceed by induction on the\nnumber of rules used to establish \\(\\Gamma_2, \\psi \\vdash \\theta\\).\nSuppose that \\(n\\) is a natural number, and that the theorem holds for\nany argument that was derived using fewer than \\(n\\) rules. Suppose\nthat \\(\\Gamma_2, \\psi \\vdash \\theta\\) was derived using exactly \\(n\\)\nrules. If the last rule used was \\((=\\)I), then \\(\\Gamma_1, \\Gamma_2\n\\vdash \\theta\\) is also an instance of \\((=\\)I). If \\(\\Gamma_2, \\psi\n\\vdash \\theta\\) is an instance of (As), then either \\(\\theta\\) is\n\\(\\psi\\), or \\(\\theta\\) is a member of \\(\\Gamma_2\\). In the former\ncase, we have \\(\\Gamma_1 \\vdash \\theta\\) by supposition, and get\n\\(\\Gamma_1, \\Gamma_2 \\vdash \\theta\\) by Weakening (Theorem 8).  In the\nlatter case, \\(\\Gamma_1, \\Gamma_2 \\vdash \\theta\\) is itself an\ninstance of (As). Suppose that \\(\\Gamma_2, \\psi \\vdash \\theta\\) was\nobtained using (&E). Then we have \\(\\Gamma_2, \\psi \\vdash(\\theta\n\\amp \\phi)\\).  The induction hypothesis gives us \\(\\Gamma_1, \\Gamma_2\n\\vdash(\\theta \\amp \\phi)\\), and (&E) produces \\(\\Gamma_1, \\Gamma_2\n\\vdash \\theta\\).  The remaining cases are similar.  \nTheorem 11 allows us to chain together inferences. This fits the\npractice of establishing theorems and lemmas and then using those\ntheorems and lemmas later, at will. The cut principle is, some think,\nessential to reasoning. In some logical systems, the cut principle is\na deep theorem; in others it is invalid. The system here was designed,\nin part, to make the proof of Theorem 11 straightforward. \n If \\(\\Gamma \\vdash_D \\theta\\), then\n we say that the sentence \\(\\theta\\) is a deductive consequence\n of the set of sentences \\(\\Gamma\\), and that the argument\n \\(\\langle \\Gamma,\\theta \\rangle\\) is deductively valid. A sentence\n \\(\\theta\\) is a logical theorem, or a deductive logical\n truth,\n if \\(\\vdash_D \\theta\\). That is,\n \\(\\theta\\) is a logical theorem if it is a deductive consequence of the\n empty set. A set \\(\\Gamma\\) of sentences is consistent if there\n is no sentence \\(\\theta\\) such that\n \\(\\Gamma \\vdash_D \\theta\\) and\n \\(\\Gamma \\vdash_D \\neg \\theta\\).\n That is, a set is consistent if it does not entail a pair of\n contradictory opposite sentencess. \nTheorem 12. A set \\(\\Gamma\\) is consistent if and\nonly if there is a sentence \\(\\theta\\) such that it is not the case\nthat \\(\\Gamma \\vdash \\theta\\). \nProof: Suppose that \\(\\Gamma\\) is consistent and let\n\\(\\theta\\) be any sentence. Then either it is not the case that\n\\(\\Gamma \\vdash \\theta\\) or it is not the case that \\(\\Gamma \\vdash\n\\neg \\theta\\).  For the converse, suppose that \\(\\Gamma\\) is\ninconsistent and let \\(\\psi\\) be any sentence. We have that there is a\nsentence such that both \\(\\Gamma \\vdash \\theta\\) and \\(\\Gamma \\vdash\n\\neg \\theta\\). By ex falso quodlibet (Theorem 10), \\(\\Gamma\n\\vdash \\psi\\).  \nDefine a set \\(\\Gamma\\) of sentences of the language \\(\\LKe\\) to\nbe maximally consistent if \\(\\Gamma\\) is consistent and for\nevery sentence \\(\\theta\\) of \\(\\LKe\\), if \\(\\theta\\) is not in\n\\(\\Gamma\\), then \\(\\Gamma,\\theta\\) is inconsistent. In other words,\n\\(\\Gamma\\) is maximally consistent if \\(\\Gamma\\) is consistent, and\nadding any sentence in the language not already in \\(\\Gamma\\) renders\nit inconsistent. Notice that if \\(\\Gamma\\) is maximally consistent\nthen \\(\\Gamma \\vdash \\theta\\) if and only if \\(\\theta\\) is in\n\\(\\Gamma\\). \nTheorem 13. The Lindenbaum Lemma.  Let \\(\\Gamma\\) be\nany consistent set of sentences of \\(\\LKe .\\) Then there is a set\n\\(\\Gamma'\\) of sentences of \\(\\LKe\\) such that \\(\\Gamma \\subseteq\n\\Gamma'\\) and \\(\\Gamma'\\) is maximally consistent. \nProof: Although this theorem holds in general, we\nassume here that the set \\(K\\) of non-logical terminology is either\nfinite or denumerably infinite (i.e., the size of the natural numbers,\nusually called \\(\\aleph_0)\\).  It follows that there is an enumeration\n\\(\\theta_0, \\theta_1,\\ldots\\) of the sentences of \\(\\LKe\\), such that\nevery sentence of \\(\\LKe\\) eventually occurs in the list. Define a\nsequence of sets of sentences, by recursion, as follows: \\(\\Gamma_0\\)\nis \\(\\Gamma\\); for each natural number \\(n\\), if \\(\\Gamma_n,\n\\theta_n\\) is consistent, then let \\(\\Gamma_{n+1} = \\Gamma_n,\n\\theta_n\\). Otherwise, let \\(\\Gamma_{n+1} = \\Gamma_n\\). Let\n\\(\\Gamma'\\) be the union of all of the sets \\(\\Gamma_n\\). Intuitively,\nthe idea is to go through the sentences of \\(\\LKe\\), throwing each one\ninto \\(\\Gamma'\\) if doing so produces a consistent set. Notice that\neach \\(\\Gamma_n\\) is consistent. Suppose that \\(\\Gamma'\\) is\ninconsistent. Then there is a sentence \\(\\theta\\) such that\n\\(\\Gamma'\\vdash \\theta\\) and \\(\\Gamma'\\vdash \\neg \\theta\\). By Theorem\n9 and Weakening (Theorem 8), there is finite subset \\(\\Gamma''\\) of\n\\(\\Gamma'\\) such that \\(\\Gamma''\\vdash \\theta\\) and \\(\\Gamma''\\vdash\n\\neg \\theta\\).  Because \\(\\Gamma''\\) is finite, there is a natural\nnumber \\(n\\) such that every member of \\(\\Gamma''\\) is in\n\\(\\Gamma_n\\). So, by Weakening again, \\(\\Gamma_n \\vdash \\theta\\) and\n\\(\\Gamma_n \\vdash \\neg \\theta\\). So \\(\\Gamma_n\\) is inconsistent,\nwhich contradicts the construction. So \\(\\Gamma'\\) is consistent. Now\nsuppose that a sentence \\(\\theta\\) is not in \\(\\Gamma'\\). We have to\nshow that \\(\\Gamma', \\theta\\) is inconsistent. The sentence \\(\\theta\\)\nmust occur in the aforementioned list of sentences; say that \\(\\theta\\)\nis \\(\\theta_m\\).  Since \\(\\theta_m\\) is not in \\(\\Gamma'\\), then it is\nnot in \\(\\Gamma_{m+1}\\). This happens only if \\(\\Gamma_m,\n\\theta_m\\) is inconsistent. So a pair of contradictory opposites can\nbe deduced from \\(\\Gamma_m,\\theta_m\\).  By Weakening, a pair of\ncontradictory opposites can be deduced from \\(\\Gamma', \\theta_m\\). So\n\\(\\Gamma', \\theta_m\\) is inconsistent. Thus, \\(\\Gamma'\\) is maximally\nconsistent. \nNotice that this proof uses a principle corresponding to the law of\nexcluded middle. In the construction of \\(\\Gamma'\\), we assumed that,\nat each stage, either \\(\\Gamma_n\\) is consistent or it is\nnot. Intuitionists, who demur from excluded middle, do not accept the\nLindenbaum lemma. \n\nLet \\(K\\) be a set of non-logical terminology. An\ninterpretation for the language \\(\\LKe\\) is a structure \\(M =\n\\langle d,I\\rangle\\), where \\(d\\) is a non-empty set, called\nthe domain-of-discourse, or simply the\ndomain, of the interpretation, and \\(I\\) is an\ninterpretation function. Informally, the domain is what we\ninterpret the language\n \\(\\LKe\\) to be\nabout. It is what the variables range over. The interpretation\nfunction assigns appropriate extensions to the non-logical terms. In\nparticular, \nIf \\(c\\) is a constant in \\(K\\), then \\(I(c)\\) is a member of the\ndomain \\(d\\).\n \n Thus we assume that every constant denotes something. Systems where\n this is not assumed are called free logics (see the entry\n on free logic). Continuing, \nIf \\(P^0\\) is a zero-place predicate letter in \\(K\\), then \\(I(P)\\) is\na truth value, either truth or falsehood. \nIf \\(Q^1\\) is a one-place predicate letter in \\(K\\), then \\(I(Q)\\) is\na subset of \\(d\\). Intuitively, \\(I(Q)\\) is the set of members of the\ndomain that the predicate \\(Q\\) holds of. For example,  \\(I(Q)\\) might be the set of red members of the\ndomain. \nIf \\(R^2\\) is a two-place predicate letter in \\(K\\), then \\(I(R)\\) is\na set of ordered pairs of members of \\(d\\). Intuitively, \\(I(R)\\) is\nthe set of pairs of members of the domain that the relation \\(R\\)\nholds between. For example, \\(I(R)\\) might be the set of pairs \\(\\langle a,b\\rangle\\) such that \\(a\\) and \\(b\\)\nare the members of the domain for which \\(a\\) loves \\(b\\). \nIn general, if S\\(^n\\) is an \\(n\\)-place predicate letter in\n\\(K\\), then \\(I(S)\\) is a set of ordered \\(n\\)-tuples of members of\n\\(d\\). \n Define \\(s\\) to be a variable-assignment, or simply an\nassignment, on an interpretation \\(M\\), if \\(s\\) is a\nfunction from the variables to the domain \\(d\\) of \\(M\\). The role of\nvariable-assignments is to assign denotations to the free\nvariables of open formulas. (In a sense, the quantifiers determine the\n“meaning” of the bound variables.)  \nLet \\(t\\) be a term of \\(\\LKe\\).  We define the denotation of\n\\(t\\) in \\(M\\) under \\(s\\), in terms of the interpretation function\nand variable-assignment:  \nIf \\(t\\) is a constant, then \\(D_{M,s}(t)\\) is \\(I(t)\\), and if \\(t\\)\nis a variable, then \\(D_{M,s}(t)\\) is \\(s(t)\\).\n \n That is, the interpretation \\(M\\) assigns denotations to the\nconstants, while the variable-assignment assigns denotations to the\n(free) variables. If the language contained function symbols, the\ndenotation function would be defined by recursion. \nWe now define a relation of satisfaction between\ninterpretations, variable-assignments, and formulas of \\(\\LKe\\). If\n\\(\\phi\\) is a formula of \\(\\LKe, M\\) is an interpretation for\n\\(\\LKe\\), and \\(s\\) is a variable-assignment on \\(M\\), then we write\n\\(M,s\\vDash \\phi\\) for \\(M\\) satisfies \\(\\phi\\) under the\nassignment \\(s\\). The idea is that \\(M,s\\vDash \\phi\\) is an\nanalogue of “\\(\\phi\\) comes out true when interpreted as in\n\\(M\\) via \\(s\\)”.  \nWe proceed by recursion on the complexity of the formulas of\n \\(\\LKe\\).  \nIf \\(t_1\\) and \\(t_2\\) are terms, then \\(M,s\\vDash t_1 =t_2\\) if and\nonly if \\(D_{M,s}(t_1)\\) is the same as \\(D_{M,s}(t_2)\\).\n \n This is about as straightforward as it gets. An identity\n\\(t_1 =t_2\\) comes out true if and\nonly if the terms \\(t_1\\) and \\(t_2\\)\ndenote the same thing.  \nIf \\(P^0\\) is a zero-place predicate letter in \\(K\\), then \\(M,s\\vDash\nP\\) if and only if \\(I(P)\\) is truth.\n  \nIf S\\(^n\\) is an \\(n\\)-place predicate letter in \\(K\\) and\n\\(t_1, \\ldots,t_n\\) are terms, then \\(M,s\\vDash St_1 \\ldots t_n\\) if\nand only if the \\(n\\)-tuple \\(\\langle D_{M,s}(t_1),\n\\ldots,D_{M,s}(t_n)\\rangle\\) is in \\(I(S)\\). \nThis takes care of the atomic formulas. We now proceed to the\ncompound formulas of the language, more or less following the meanings of the\nEnglish counterparts of the logical terminology.  \\(M,s\\vDash \\neg \\theta\\) if and only if it is not the case that\n\\(M,s\\vDash \\theta\\).  \\(M,s\\vDash(\\theta \\amp \\psi)\\) if and only if both \\(M,s\\vDash\n\\theta\\) and \\(M,s\\vDash \\psi\\).  \\(M,s\\vDash(\\theta \\vee \\psi)\\) if and only if either \\(M,s\\vDash\n\\theta\\) or \\(M,s\\vDash \\psi\\).  \\(M,s\\vDash(\\theta \\rightarrow \\psi)\\) if and only if either it is\nnot the case that \\(M,s\\vDash \\theta\\), or \\(M,s\\vDash \\psi\\).  \\(M,s\\vDash \\forall v\\theta\\) if and only if \\(M,s'\\vDash\n\\theta\\), for every assignment \\(s'\\) that agrees with \\(s\\) except\npossibly at the variable \\(v\\). \nThe idea here is that \\(\\forall v\\theta\\) comes out true if and only\nif \\(\\theta\\) comes out true no matter what is assigned to the\nvariable \\(v\\). The final clause is similar. \n\\(M,s\\vDash \\exists v\\theta\\) if and only if \\(M,s'\\vDash \\theta\\),\nfor some assignment \\(s'\\) that agrees with \\(s\\) except possibly at\nthe variable \\(v\\).\n \nSo \\(\\exists v\\theta\\) comes out true if there is an assignment to\n\\(v\\) that makes \\(\\theta\\) true.  \n Theorem 6, unique readability, assures us that this definition is\ncoherent. At each stage in breaking down a formula, there is exactly\none clause to be applied, and so we never get contradictory verdicts\nconcerning satisfaction.  \n As indicated, the role of variable-assignments is to give\ndenotations to the free variables. We now show that\nvariable-assignments play no other role. \nTheorem 14. For any formula \\(\\theta\\), if \\(s_1\\)\nand \\(s_2\\) agree on the free variables in \\(\\theta\\), then \\(M,s_1\n\\vDash \\theta\\) if and only if \\(M,s_2 \\vDash \\theta\\).  \nProof: We proceed by induction on the complexity of\nthe formula \\(\\theta\\). The theorem clearly holds if \\(\\theta\\) is\natomic, since in those cases only the values of the\nvariable-assignments at the variables in \\(\\theta\\) figure in the\ndefinition. Assume, then, that the theorem holds for all formulas less\ncomplex than \\(\\theta\\). And suppose that \\(s_1\\) and \\(s_2\\) agree on\nthe free variables of \\(\\theta\\). Assume, first, that \\(\\theta\\) is a\nnegation, \\(\\neg \\psi\\). Then, by the induction hypothesis, \\(M,s_1\n\\vDash \\psi\\) if and only if \\(M,s_2 \\vDash \\psi\\).  So, by the clause\nfor negation, \\(M,s_1 \\vDash \\neg \\psi\\) if and only if \\(M,s_2 \\vDash\n\\neg \\psi\\). The cases where the main connective in \\(\\theta\\) is\nbinary are also straightforward. Suppose that \\(\\theta\\) is \\(\\exists\nv\\psi\\), and that \\(M,s_1 \\vDash \\exists v\\psi\\).  Then there is an\nassignment \\(s_1'\\) that agrees with \\(s_1\\) except possibly at \\(v\\)\nsuch that \\(M,s_1'\\vDash \\psi\\).  Let \\(s_2'\\) be the assignment that\nagrees with \\(s_2\\) on the free variables not in \\(\\psi\\) and agrees\nwith \\(s_1'\\) on the others. Then, by the induction hypothesis,\n\\(M,s_2'\\vDash \\psi\\).  Notice that \\(s_2'\\) agrees with \\(s_2\\) on\nevery variable except possibly \\(v\\). So \\(M,s_2 \\vDash \\exists\nv\\psi\\).  The converse is the same, and the case where \\(\\theta\\)\nbegins with a universal quantifier is similar. \nBy Theorem 14, if \\(\\theta\\) is a sentence, and \\(s_1, s_2\\), are any two\nvariable-assignments, then \\(M,s_1 \\vDash \\theta\\) if and only if\n\\(M,s_2 \\vDash \\theta\\).  So we can just write \\(M\\vDash \\theta\\) if\n\\(M,s\\vDash \\theta\\) for some, or all, variable-assignments \\(s\\). So we define   \\(M\\vDash \\theta\\) where \\(\\theta\\) is a sentence\njust in case \\(M,s\\vDash\\theta\\) for all variable assignments \\(s\\).\n \nIn this case, we call \\(M\\) a  model  of \\(\\theta\\). \nSuppose that \\(K'\\subseteq K\\) are two sets of non-logical terms. If\n\\(M = \\langle d,I\\rangle\\) is an interpretation of \\(\\LKe\\), then we\ndefine the restriction of \\(M\\) to \\(\\mathcal{L}1K'{=}\\) to be the\ninterpretation \\(M'=\\langle d,I'\\rangle\\) such that \\(I'\\) is the\nrestriction of \\(I\\) to \\(K'\\). That is, \\(M\\) and \\(M'\\) have the\nsame domain and agree on the non-logical terminology in \\(K'\\). A\nstraightforward induction establishes the following: \nTheorem 15. If \\(M'\\) is the restriction of \\(M\\) to\n\\(\\mathcal{L}1K'{=}\\), then for every sentence \\(\\theta\\) of\n\\(\\mathcal{L}1K'\\), \\(M\\vDash\\theta\\) if and only if \\(M'\\vDash \\theta\\).  \nTheorem 16. If two interpretations \\(M_1\\) and\n\\(M_2\\) have the same domain and agree on all of the non-logical\nterminology of a sentence \\(\\theta\\), then \\(M_1\\vDash\\theta\\) if and\nonly if \\(M_2\\vDash \\theta\\). \n In short, the satisfaction of a sentence \\(\\theta\\) only\ndepends on the domain of discourse and the interpretation of the\nnon-logical terminology in \\(\\theta\\). \nWe say that an argument \\(\\langle \\Gamma,\\theta \\rangle\\)\nis semantically valid, or just valid, written\n\\(\\Gamma \\vDash \\theta\\), if for every interpretation \\(M\\) of the\nlanguage, if \\(M\\vDash\\psi\\), for every member \\(\\psi\\) of \\(\\Gamma\\),\nthen \\(M\\vDash\\theta\\). If \\(\\Gamma \\vDash \\theta\\), we also say that\n\\(\\theta\\) is a logical consequence, or semantic\nconsequence, or\nmodel-theoretic consequence of \\(\\Gamma\\). The definition\ncorresponds to the informal idea that an argument is valid if it is\nnot possible for its premises to all be true and its conclusion\nfalse. Our definition of logical consequence also sanctions the common\nthesis that a valid argument is truth-preserving--to the extent that\nsatisfaction represents truth. Officially, an argument in \\(\\LKe\\) is\nvalid if its conclusion comes out true under every interpretation of\nthe language in which the premises are true. Validity is the\nmodel-theoretic counterpart to deducibility. \nA sentence \\(\\theta\\) is logically true, or\nvalid, if \\(M\\vDash \\theta\\), for every interpretation\n\\(M\\). A sentence is logically true if and only if\nit is a consequence of the empty set. If \\(\\theta\\) is logically true,\nthen for any set \\(\\Gamma\\) of sentences, \\(\\Gamma \\vDash \\theta\\).\nLogical truth is the model-theoretic counterpart of theoremhood. \nA sentence \\(\\theta\\) is satisfiable if there is an\ninterpretation \\(M\\) such\nthat \\(M\\vDash \\theta\\).  That is, \\(\\theta\\) is satisfiable if\nthere is an interpretation that satisfies it. A set\n\\(\\Gamma\\) of sentences is satisfiable if there is an interpretation\n\\(M\\) such that \\(M\\vDash\\theta\\), for every sentence \\(\\theta\\) in \\(\\Gamma\\). If \\(\\Gamma\\) is\na set of sentences and if \\(M\\vDash \\theta\\) for each sentence\n\\(\\theta\\) in \\(\\Gamma\\), then we say that \\(M\\) is a model\nof  \\(\\Gamma\\). So a set of sentences is satisfiable if it has a\nmodel. Satisfiability is the model-theoretic counterpart to\nconsistency. \nNotice that \\(\\Gamma \\vDash \\theta\\) if and only if the set\n\\(\\Gamma,\\neg \\theta\\) is not satisfiable. It follows that if a set\n\\(\\Gamma\\) is not satisfiable, then if \\(\\theta\\) is any sentence,\n\\(\\Gamma \\vDash \\theta\\).  This is a model-theoretic counterpart\nto ex falso quodlibet (see Theorem 10). We have the\nfollowing, as an analogue to Theorem 12: \nTheorem 17. Let \\(\\Gamma\\) be a set of sentences. The\nfollowing are equivalent: (a) \\(\\Gamma\\) is satisfiable; (b) there is\nno sentence \\(\\theta\\) such that both \\(\\Gamma \\vDash \\theta\\) and\n\\(\\Gamma \\vDash \\neg \\theta\\); (c) there is some sentence \\(\\psi\\) such\nthat it is not the case that \\(\\Gamma \\vDash \\psi\\). \nProof: (a)\\(\\Rightarrow\\)(b): Suppose that \\(\\Gamma\\)\nis satisfiable and let \\(\\theta\\) be any sentence. There is an\ninterpretation \\(M\\) such that \\(M\\vDash \\psi\\)\nfor every member \\(\\psi\\) of \\(\\Gamma\\). By the clause for negations,\nwe cannot have both \\(M\\vDash \\theta\\) and \\(M\\vDash \\neg\n\\theta\\).  So either \\(\\langle \\Gamma,\\theta \\rangle\\) is not valid or\nelse \\(\\langle \\Gamma,\\neg \\theta \\rangle\\) is not valid.\n(b)\\(\\Rightarrow\\)(c): This is immediate.  (c)\\(\\Rightarrow\\)(a):\nSuppose that it is not the case that \\(\\Gamma \\vDash \\psi\\). Then\nthere is an interpretation \\(M\\) such that\n\\(M\\vDash \\theta\\), for every sentence \\(\\theta\\) in \\(\\Gamma\\) and\nit is not the case that \\(M\\vDash \\psi\\).  A fortiori, \\(M\\)\nsatisfies every member of \\(\\Gamma\\), and so \\(\\Gamma\\) is\nsatisfiable. \n\nWe now present some results that relate the deductive notions to their\nmodel-theoretic counterparts. The first one is probably the most\nstraightforward. We motivated both the various rules of the deductive\nsystem \\(D\\) and the various clauses in the definition of\nsatisfaction in terms of the meaning of the English counterparts to\nthe logical terminology (more or less, with the same simplifications\nin both cases). So one would expect that an argument is deducible, or\ndeductively valid, only if it is semantically valid. \nTheorem 18. Soundness. For any sentence \\(\\theta\\) and\nset \\(\\Gamma\\) of sentences, if \\(\\Gamma \\vdash_D \\theta\\), then\n\\(\\Gamma \\vDash \\theta\\). \nProof: We proceed by induction on the number of\nclauses used to establish \\(\\Gamma \\vdash \\theta\\).  So let \\(n\\) be a\nnatural number, and assume that the theorem holds for any argument\nestablished as deductively valid with fewer than \\(n\\) steps. And\nsuppose that \\(\\Gamma \\vdash \\theta\\) was established using exactly\n\\(n\\) steps. If the last rule applied was \\((=\\)I) then \\(\\theta\\) is\na sentence in the form \\(t=t\\), and so \\(\\theta\\) is logically true. A\nfortiori, \\(\\Gamma \\vDash \\theta\\).  If the last rule applied was\n(As), then \\(\\theta\\) is a member of \\(\\Gamma\\), and so of course any\ninterpretation that satisfies every member of\n\\(\\Gamma\\) also satisfies \\(\\theta\\). Suppose the last rule applied is\n(&I). So \\(\\theta\\) has the form \\((\\phi \\amp \\psi)\\), and we have\n\\(\\Gamma_1 \\vdash \\phi\\) and \\(\\Gamma_2 \\vdash \\psi\\), with \\(\\Gamma =\n\\Gamma_1, \\Gamma_2\\). The induction hypothesis gives us \\(\\Gamma_1\n\\vDash \\phi\\) and \\(\\Gamma_2 \\vDash \\psi\\).  Suppose that \\(M\\)\nsatisfies every member of \\(\\Gamma\\). Then \\(M\\) satisfies every\nmember of \\(\\Gamma_1\\), and so \\(M\\) satisfies \\(\\phi\\).  Similarly,\n\\(M\\) satisfies every member of \\(\\Gamma_2\\), and so \\(M\\)\nsatisfies \\(\\psi\\). Thus, by the clause for “\\(\\amp\\)” in\nthe definition of satisfaction, \\(M\\) satisfies \\(\\theta\\). So\n\\(\\Gamma \\vDash \\theta\\).   \nSuppose the last clause applied was \\((\\exists\\mathrm{E})\\). So we\nhave \\(\\Gamma_1 \\vdash \\exists v\\phi\\) and \\(\\Gamma_2, \\phi(v|t)\n\\vdash \\theta\\), where \\(\\Gamma = \\Gamma_1, \\Gamma_2\\), and \\(t\\) does\nnot occur in \\(\\phi , \\theta \\), or in any member of \\(\\Gamma_2\\). \nWe need to show that \\(\\Gamma\\vDash\\theta\\). By the induction\nhypothesis, we have that \\(\\Gamma_1\\vDash\\exists v\\phi\\) and\n\\(\\Gamma_2, \\phi(v|t)\\vDash\\theta\\). Let \\(M\\) be an interpretation\nsuch that \\(M\\) makes every member of \\(\\Gamma\\) true. So, \\(M\\) makes\nevery member of \\(\\Gamma_1\\) and \\(\\Gamma_2\\) true. Then\n\\(M,s\\vDash\\exists v\\phi\\) for all variable assignments \\(s\\), so\nthere is an \\(s'\\) such that \\(M,s'\\vDash\\phi\\). Let \\(M'\\) differ\nfrom \\(M\\) only in that \\(I_{M'}(t)=s'(v)\\). Then,\n\\(M',s'\\vDash\\phi(v|t)\\) and \\(M',s'\\vDash\\Gamma_2\\) since \\(t\\) does\nnot occur in \\(\\phi\\) or \\(\\Gamma_2\\). So,\n\\(M',s'\\vDash\\theta\\). Since \\(t\\) does not occur in \\(\\theta\\) and\n\\(M'\\) differs from \\(M\\) only with respect to \\(I_{M'}(t)\\),\n\\(M,s'\\vDash\\theta\\). Since \\(\\theta\\) is a sentence, \\(s'\\) doesn't\nmatter, so \\(M\\vDash\\theta\\) as desired. Notice the role of the\nrestrictions on \\((\\exists\\)E) here. The other cases are about as\nstraightforward.  \nCorollary 19. Let \\(\\Gamma\\) be a set of sentences. If\n\\(\\Gamma\\) is satisfiable, then \\(\\Gamma\\) is consistent.   \nProof: Suppose that \\(\\Gamma\\) is satisfiable. So let\n\\(M\\) be an interpretation such that \\(M\\)\nsatisfies every member of \\(\\Gamma\\). Assume that \\(\\Gamma\\) is\ninconsistent. Then there is a sentence \\(\\theta\\) such that \\(\\Gamma\n\\vdash \\theta\\) and \\(\\Gamma \\vdash \\neg \\theta\\). By soundness\n(Theorem 18), \\(\\Gamma \\vDash \\theta\\) and \\(\\Gamma \\vDash \\neg\n\\theta\\). So we have that \\(M\\vDash \\theta\\) and \\(M\\vDash \\neg\n\\theta\\). But this is impossible, given the clause for negation in the\ndefinition of satisfaction. \n Even though the deductive system \\(D\\) and the model-theoretic\nsemantics were developed with the meanings of the logical terminology\nin mind, one should not automatically expect the converse to\nsoundness (or Corollary 19) to hold. For all we know so far, we may\nnot have included enough rules of inference to deduce every valid\nargument. The converses to soundness and Corollary 19 are among the\nmost important and influential results in mathematical logic. We begin\nwith the latter. \nTheorem 20. Completeness. Gödel [1930]. Let\n\\(\\Gamma\\) be a set of sentences. If \\(\\Gamma\\) is consistent, then\n\\(\\Gamma\\) is satisfiable. \nProof: The proof of completeness is rather\ncomplex. We only sketch it here. Let \\(\\Gamma\\) be a consistent set of\nsentences of \\(\\LKe\\). Again, we assume for simplicity that the set\n\\(K\\) of non-logical terminology is either finite or countably\ninfinite (although the theorem holds even if \\(K\\) is\nuncountable). The task at hand is to find an interpretation \\(M\\) such\nthat \\(M\\) satisfies every member of \\(\\Gamma\\). Consider the language\nobtained from \\(\\LKe\\) by adding a denumerably infinite stock of new\nindividual constants \\(c_0, c_1,\\ldots\\) We stipulate that the\nconstants, \\(c_0, c_1,\\ldots\\), are all different from each other and\nnone of them occur in \\(K\\). One interesting feature of this\nconstruction, due to Leon Henkin, is that we build an interpretation\nof the language from the language itself, using some of the constants\nas members of the domain of discourse. Let \\(\\theta_0 (x), \\theta_1\n(x),\\ldots\\) be an enumeration of the formulas of the expanded\nlanguage with at most one free variable, so that each formula with at\nmost one free variable occurs in the list eventually. Define a\nsequence \\(\\Gamma_0, \\Gamma_1,\\ldots\\) of sets of sentences (of the\nexpanded language) by recursion as follows: \\(\\Gamma_0 = \\Gamma\\); and\n\\(\\Gamma_{n+1} = \\Gamma_n,(\\exists x\\theta_n \\rightarrow\n\\theta_{n}(x|c_i))\\), where \\(c_i\\) is the first constant in the above\nlist that does not occur in \\(\\theta_n\\) or in any member of\n\\(\\Gamma_n\\). The underlying idea here is that if \\(\\exists\nx\\theta_n\\)is true, then \\(c_i\\) is to be one such \\(x\\). Let\n\\(\\Gamma\\) be the union of the sets \\(\\Gamma_n\\).  \nWe sketch a proof that \\(\\Gamma'\\) is consistent. Suppose that\n\\(\\Gamma'\\) is inconsistent. By Theorem 9, there is a finite subset of\n\\(\\Gamma\\) that is inconsistent, and so one of the sets \\(\\Gamma_m\\)\nis inconsistent. By hypothesis, \\(\\Gamma_0 = \\Gamma\\) is\nconsistent. Let \\(n\\) be the smallest number such that \\(\\Gamma_n\\) is\nconsistent, but \\(\\Gamma_{n+1} = \\Gamma_n,(\\exists x\\theta_n\n\\rightarrow \\theta_{n}(x|c_i))\\) is inconsistent. By \\((\\neg\\)I), we\nhave that  By ex falso quodlibet (Theorem 10), \\(\\Gamma_n, \\neg\n\\exists x\\theta_n, \\exists x\\theta_n \\vdash \\theta_n (x|c_i)\\).  So by\n\\((\\rightarrow\\)I), \\(\\Gamma_n, \\neg \\exists x\\theta_n \\vdash(\\exists\nx\\theta_n \\rightarrow \\theta_n (x|c_i))\\).  From this and (1), we have\n\\(\\Gamma_n \\vdash \\neg \\neg \\exists x\\theta_n\\), by \\((\\neg\\)I), and\nby (DNE) we have \nBy (As), \\(\\Gamma_n, \\theta_n (x|c_i), \\exists x\\theta_n \\vdash\n\\theta_n (x|c_i)\\).  So by \\((\\rightarrow\\)I), \\(\\Gamma_n, \\theta_n\n(x|c_i)\\vdash(\\exists x\\theta_{n} \\rightarrow\n\\theta_{n}(x|c_i))\\).  From this and (1), we have \\(\\Gamma_n \\vdash\n\\neg \\theta_n (x|c_i)\\), by \\((\\neg\\)I). Let \\(t\\) be a term that\ndoes not occur in \\(\\theta_n\\) or in any member of\n\\(\\Gamma_n\\). By uniform substitution of \\(t\\) for \\(c_i\\), we can\nturn the derivation of \\(\\Gamma_n \\vdash \\neg \\theta_n (x|c_i)\\) into\n\\(\\Gamma_n \\vdash \\neg \\theta_n (x|t)\\).  By \\((\\forall\\)I), we\nhave \nBy (As) we have \\(\\{\\forall v\\neg \\theta_n (x|v),\\theta_n\\}\\vdash\n\\theta_n\\) and by \\((\\forall\\)E) we have \\(\\{\\forall v\\neg \\theta_n\n(x|v), \\theta_n\\}\\vdash \\neg \\theta_n\\).  So \\(\\{\\forall v\\neg\n\\theta_n (x|v), \\theta_n\\}\\) is inconsistent. Let \\(\\phi\\) be any\nsentence of the language. By ex falso quodlibet (Theorem 10), we have that\n\\(\\{\\forall v\\neg \\theta_n (x|v),\\theta_n\\}\\vdash \\phi\\) and\n\\(\\{\\forall v\\neg \\theta_n (x|v), \\theta_n\\}\\vdash \\neg \\phi\\). So\nwith (2), we have that \\(\\Gamma_n, \\forall v\\neg \\theta_n (x|v)\\vdash\n\\phi\\) and \\(\\Gamma_n, \\forall v\\neg \\theta_n (x|v)\\vdash \\neg \\phi\\),\nby \\((\\exists\\)E). By Cut (Theorem 11), \\(\\Gamma_n \\vdash \\phi\\) and\n\\(\\Gamma_n \\vdash \\neg \\phi\\). So \\(\\Gamma_n\\) is inconsistent,\ncontradicting the assumption. So \\(\\Gamma'\\) is consistent.  Applying the Lindenbaum Lemma (Theorem 13), let \\(\\Gamma''\\) be a\nmaximally consistent set of sentences (of the expanded language) that\ncontains \\(\\Gamma'\\). So, of course, \\(\\Gamma''\\) contains\n\\(\\Gamma\\). We can now define an interpretation \\(M\\) such that \\(M\\) satisfies every\nmember of \\(\\Gamma''\\). \n If we did not have a sign for identity in the language, we would let\nthe domain of \\(M\\) be the collection of new constants \\(\\{c_0, c_1,\n\\ldots \\}\\). But as it is, there may be a sentence in the form\n\\(c_{i}=c_{j}\\), with \\(i\\ne j\\), in \\(\\Gamma''\\). If so, we\ncannot have both \\(c_i\\) and \\(c_j\\) in the domain of the\ninterpretation (as they are distinct constants). So we define the\ndomain \\(d\\) of \\(M\\) to be the set \\(\\{c_i\\) | there is no \\(j\\lt i\\)\nsuch that \\(c_{i}=c_{j}\\) is in \\(\\Gamma''\\}\\). In other words, a\nconstant \\(c_i\\) is in the domain of \\(M\\) if \\(\\Gamma''\\) does not\ndeclare it to be identical to an earlier constant in the list. Notice\nthat for each new constant \\(c_i\\), there is exactly one \\(j\\le i\\)\nsuch that \\(c_j\\) is in \\(d\\) and the sentence \\(c_{i}=c_{j}\\) is\nin \\(\\Gamma''\\). \nWe now define the interpretation function \\(I\\). Let \\(a\\) be any\nconstant in the expanded language. By \\((=\\)I) and \\((\\exists\\)I),\n\\(\\Gamma''\\vdash \\exists x x=a\\), and so \\(\\exists x x=a \\in\n\\Gamma''\\).  By the construction of \\(\\Gamma'\\), there is a sentence\nin the form \\((\\exists x x=a \\rightarrow c_i =a)\\) in \\(\\Gamma''\\). We\nhave that \\(c_i =a\\) is in \\(\\Gamma''\\).  As above, there is exactly\none \\(c_j\\) in \\(d\\) such that \\(c_{i}=c_{j}\\) is in\n\\(\\Gamma''\\). Let \\(I(a)=c_j\\). Notice that if \\(c_i\\) is a constant\nin the domain \\(d\\), then \\(I\\)(c\\(_i)=c_i\\).  That is each \\(c_i\\) in\n\\(d\\) denotes itself. \nLet \\(P\\) be a zero-place predicate letter in \\(K\\).  Then \\(I(P)\\) is\ntruth if \\(P\\) is in \\(\\Gamma''\\) and \\(I(P)\\) is falsehood otherwise.\nLet \\(Q\\) be a one-place predicate letter in \\(K\\). Then \\(I(Q)\\) is\nthe set of constants \\(\\{\\)c\\(_i | c_i\\) is in \\(d\\) and the sentence\n\\(Qc\\) is in \\(\\Gamma''\\}\\). Let \\(R\\) be a binary predicate letter in\n\\(K\\). Then \\(I(R)\\) is the set of pairs of constants \\(\\{\\langle\nc_i,c_j\\rangle | c_i\\) is in \\(d, c_j\\) is in \\(d\\), and the sentence\n\\(Rc_{i}c_{j}\\) is in \\(\\Gamma''\\}\\). Three-place predicates,\netc. are interpreted similarly. In effect, \\(I\\) interprets the\nnon-logical terminology as they are in \\(\\Gamma''\\). \nThe final item in this proof is a lemma that for every sentence \\(\\theta\\) in the expanded language, \\(M\\vDash \\theta\\) if and only\nif \\(\\theta\\) is in \\(\\Gamma''\\). This proceeds by induction on the\ncomplexity of \\(\\theta\\). The case where \\(\\theta\\) is atomic follows\nfrom the definitions of \\(M\\) (i.e., the domain \\(d\\) and the\ninterpretation function \\(I\\)). The\nother cases follow from the various clauses in the definition of\nsatisfaction. \nSince \\(\\Gamma \\subseteq \\Gamma''\\), we have that \\(M\\) satisfies\nevery member of \\(\\Gamma\\). By Theorem 15, the restriction of \\(M\\) to\nthe original language \\(\\LKe\\) and \\(s\\) also satisfies every member\nof \\(\\Gamma\\). Thus \\(\\Gamma\\) is satisfiable.  \nA converse to Soundness (Theorem 18) is a straightforward\ncorollary: \nTheorem 21. For any sentence \\(\\theta\\) and set\n\\(\\Gamma\\) of sentences, if \\(\\Gamma \\vDash \\theta\\), then \\(\\Gamma\n\\vdash_D \\theta\\). \nProof: Suppose that \\(\\Gamma \\vDash \\theta\\).  Then\nthere is no interpretation \\(M\\) such\nthat M satisfies every member of \\(\\Gamma\\) but does not\nsatisfy \\(\\theta\\). So the set \\(\\Gamma,\\neg \\theta\\) is not\nsatisfiable. By Completeness (Theorem 20), \\(\\Gamma,\\neg \\theta\\) is\ninconsistent. So there is a sentence \\(\\phi\\) such that \\(\\Gamma,\\neg\n\\theta \\vdash \\phi\\) and \\(\\Gamma,\\neg \\theta \\vdash \\neg \\phi\\). By\n\\((\\neg\\)I), \\(\\Gamma \\vdash \\neg \\neg \\theta\\), and by (DNE) \\(\\Gamma\n\\vdash \\theta\\). \nOur next item is a corollary of Theorem 9, Soundness (Theorem 18),\nand Completeness: \nCorollary 22. Compactness. A set \\(\\Gamma\\) of\nsentences is satisfiable if and only if every finite subset of\n\\(\\Gamma\\) is satisfiable. \nProof: If \\(M\\) satisfies every member of\n\\(\\Gamma\\), then \\(M\\) satisfies every member of each finite subset\nof \\(\\Gamma\\). For the converse, suppose that \\(\\Gamma\\) is not\nsatisfiable. Then we show that some finite subset of \\(\\Gamma\\) is not\nsatisfiable. By Completeness (Theorem 20), \\(\\Gamma\\) is\ninconsistent. By Theorem 9 (and Weakening), there is a finite subset\n\\(\\Gamma'\\subseteq \\Gamma\\) such that \\(\\Gamma'\\) is inconsistent.  By\nCorollary \\(19, \\Gamma'\\) is not satisfiable. \n Soundness and completeness together entail that an argument is\ndeducible if and only if it is valid, and a set of sentences is\nconsistent if and only if it is satisfiable. So we can go back and\nforth between model-theoretic and proof-theoretic notions,\ntransferring properties of one to the other. Compactness holds in the\nmodel theory because all derivations use only a finite number of\npremises. \n Recall that in the proof of Completeness (Theorem 20), we made the\nsimplifying assumption that the set \\(K\\) of non-logical\nconstants is either finite or denumerably infinite. The\ninterpretation we produced was itself either finite or denumerably\ninfinite. Thus, we have the following: \nCorollary 23. Löwenheim-Skolem Theorem. Let\n\\(\\Gamma\\) be a satisfiable set of sentences of the language\n\\(\\LKe\\). If \\(\\Gamma\\) is either finite or denumerably infinite, then\n\\(\\Gamma\\) has a model whose domain is either finite or denumerably\ninfinite.\n \nIn general, let \\(\\Gamma\\) be a satisfiable set of sentences of\n\\(\\LKe\\), and let \\(\\kappa\\) be the larger of the size of \\(\\Gamma\\)\nand denumerably infinite. Then \\(\\Gamma\\) has a model whose domain is\nat most size \\(\\kappa\\). \nThere is a stronger version of Corollary 23. Let \\(M_1 =\\langle\nd_1,I_1\\rangle\\) and \\(M_2 =\\langle d_2,I_2\\rangle\\) be\ninterpretations of the language \\(\\LKe\\). Define \\(M_1\\) to be\na submodel of \\(M_2\\) if \\(d_1 \\subseteq d_2, I_1 (c) = I_2\n(c)\\) for each constant \\(c\\), and \\(I_1\\) is the restriction of\n\\(I_2\\) to \\(d_1\\). For example, if \\(R\\) is a binary relation letter\nin \\(K\\), then for all \\(a,b\\) in \\(d_1\\), the pair \\(\\langle\na,b\\rangle\\) is in \\(I_1 (R)\\) if and only if \\(\\langle a,b\\rangle\\)\nis in \\(I_2 (R)\\). If we had included function letters among the\nnon-logical terminology, we would also require that \\(d_1\\) be closed\nunder their interpretations in \\(M_2\\). Notice that if \\(M_1\\) is a\nsubmodel of \\(M_2\\), then any variable-assignment on \\(M_1\\) is also a\nvariable-assignment on \\(M_2\\). \nSay that two interpretations \\(M_1 =\\langle d_1,I_1\\rangle, M_2\n=\\langle d_2,I_2\\rangle\\) are equivalent if one of them is a\nsubmodel of the other, and for any formula of the language and any\nvariable-assignment \\(s\\) on the submodel, \\(M_1,s\\vDash \\theta\\) if\nand only if \\(M_2,s\\vDash \\theta\\).  Notice that if two\ninterpretations are equivalent, then they satisfy the same\nsentences. \nTheorem 25. Downward Löwenheim-Skolem Theorem.\nLet \\(M = \\langle d,I\\rangle\\) be an interpretation of the language\n\\(\\LKe\\). Let \\(d_1\\) be any subset of \\(d\\), and let \\(\\kappa\\) be\nthe maximum of the size of \\(K\\), the size of \\(d_1\\), and denumerably\ninfinite. Then there is a submodel \\(M' = \\langle d',I'\\rangle\\) of\n\\(M\\) such that (1) \\(d'\\) is not larger than \\(\\kappa\\), and (2)\n\\(M\\) and \\(M'\\) are equivalent. In particular, if the set \\(K\\) of\nnon-logical terminology is either finite or denumerably infinite, then\nany interpretation has an equivalent submodel whose domain is either\nfinite or denumerably infinite. \nProof: Like completeness, this proof is complex, and\nwe rest content with a sketch. The downward Löwenheim-Skolem\ntheorem invokes the axiom of choice, and indeed, is equivalent to the\naxiom of choice (see the entry on\n the axiom of choice). \nSo let \\(C\\) be a choice function on the powerset of \\(d\\), so that\nfor each non-empty subset \\(e\\subseteq d, C(e)\\) is a member of\n\\(e\\). We stipulate that if \\(e\\) is the empty set, then \\(C(e)\\) is\n\\(C(d)\\). \nLet \\(s\\) be a variable-assignment on \\(M\\), let \\(\\theta\\) be a\nformula of \\(\\LKe\\), and let \\(v\\) be a variable. Define the\n\\(v\\)-witness of \\(\\theta\\) over s, written \\(w_v\n(\\theta,s)\\), as follows: Let \\(q\\) be the set of all elements \\(c\\in\nd\\) such that there is a variable-assignment \\(s'\\) on \\(M\\) that\nagrees with \\(s\\) on every variable except possibly \\(v\\), such that\n\\(M,s'\\vDash \\theta\\), and \\(s'(v)=c\\). Then \\(w_v (\\theta,s) =\nC(q)\\). Notice that if \\(M,s\\vDash \\exists v\\theta\\), then \\(q\\) is\nthe set of elements of the domain that can go for \\(v\\) in\n\\(\\theta\\). Indeed, \\(M,s\\vDash \\exists v\\theta\\) if and only if \\(q\\)\nis non-empty. So if \\(M,s\\vDash \\exists v\\theta\\), then \\(w_v\n(\\theta,s)\\) (i.e., \\(C(q))\\) is a chosen element of the domain that\ncan go for \\(v\\) in \\(\\theta\\). In a sense, it is a\n“witness” that verifies \\(M,s\\vDash \\exists v\\theta\\). \nIf \\(e\\) is a non-empty subset of the domain \\(d\\), then define a\nvariable-assignment \\(s\\) to be an \\(e\\)-assignment if for\nall variables \\(u, s(u)\\) is in \\(e\\). That is, \\(s\\) is an\n\\(e\\)-assignment if \\(s\\) assigns an element of \\(e\\) to each\nvariable. Define \\(sk(e)\\), the\nSkolem-hull of \\(e\\), to be the set: \n That is, the Skolem-Hull of \\(e\\) is the set \\(e\\) together with\nevery \\(v\\)-witness of every formula over every\n\\(e\\)-assignment. Roughly, the idea is to start with \\(e\\) and then\nthrow in enough elements to make each existentially quantified formula\ntrue. But we cannot rest content with the Skolem-hull, however. Once\nwe throw the “witnesses” into the domain, we need to deal\nwith \\(sk(e)\\) assignments. In effect, we need a set which is its own\nSkolem-hull, and also contains the given subset \\(d_1\\). \n We define a sequence of non-empty sets \\(e_0, e_1,\\ldots\\) as\nfollows: if the given subset \\(d_1\\) of \\(d\\) is empty and there are\nno constants in \\(K\\), then let \\(e_0\\) be \\(C(d)\\), the choice\nfunction applied to the entire domain; otherwise let \\(e_0\\) be the\nunion of \\(d_1\\) and the denotations under \\(I\\) of the constants in\n\\(K\\). For each natural number \\(n, e_{n+1}\\) is \\(sk(e_n)\\). Finally,\nlet \\(d'\\) be the union of the sets \\(e_n\\), and let \\(I'\\) be the\nrestriction of \\(I\\) to \\(d'\\). Our interpretation is \\(M' = \\langle\nd',I'\\rangle\\). \nClearly, \\(d_1\\) is a subset of \\(d'\\), and so \\(M'\\) is a submodel of\n\\(M\\). Let \\(\\kappa\\) be the maximum of the size of \\(K\\), the size of\n\\(d_1\\), and denumerably infinite. A calculation reveals that the size\nof \\(d'\\) is at most \\(\\kappa\\), based on the fact that there are at\nmost \\(\\kappa\\)-many formulas, and thus, at most \\(\\kappa\\)-many\nwitnesses at each stage. Notice, incidentally, that this calculation\nrelies on the fact that a denumerable union of sets of size at most\n\\(\\kappa\\) is itself at most \\(\\kappa\\). This also relies on the axiom\nof choice. \nThe final item is to show that \\(M'\\) is equivalent to \\(M\\): For\nevery formula \\(\\theta\\) and every variable-assignment \\(s\\) on\n\\(M'\\), \nThe proof proceeds by induction on the complexity of\n\\(\\theta\\). Unfortunately, space constraints require that we leave\nthis step as an exercise. \n Another corollary to Compactness (Corollary 22) is the opposite\nof the Löwenheim-Skolem theorem: \nTheorem 26. Upward Löwenheim-Skolem Theorem.\nLet \\(\\Gamma\\) be any set of sentences of \\(\\LKe,\\) such that for each\nnatural number \\(n\\), there is an interpretation \\(M_n = \\langle\nd_n,I_n\\rangle\\), such that\n\\(d_n\\) has at least \\(n\\) elements, and \\(M_n\\) satisfies every\nmember of \\(\\Gamma\\). In other words, \\(\\Gamma\\) is satisfiable and\nthere is no finite upper bound to the size of the interpretations that\nsatisfy every member of \\(\\Gamma\\). Then for any infinite cardinal\n\\(\\kappa\\), there is an interpretation \\(M=\\langle d,I\\rangle\\), such that the size of \\(d\\) is at\nleast \\(\\kappa\\) and \\(M\\) satisfies every member of\n\\(\\Gamma\\). \nProof: Add a collection of new constants\n\\(\\{c_{\\alpha} | \\alpha \\lt \\kappa \\}\\), of size \\(\\kappa\\), to the\nlanguage, so that if \\(c\\) is a constant in \\(K\\), then \\(c_{\\alpha}\\)\nis different from \\(c\\), and if \\(\\alpha \\lt \\beta \\lt \\kappa\\), then\n\\(c_{\\alpha}\\) is a different constant than \\(c_{\\beta}\\).  Consider\nthe set of sentences \\(\\Gamma'\\) consisting of \\(\\Gamma\\) together with\nthe set \\(\\{\\neg c_{\\alpha}=c_{\\beta} | \\alpha \\ne \\beta \\}\\).\nThat is, \\(\\Gamma'\\) consists of \\(\\Gamma\\) together with statements\nto the effect that any two different new constants denote different\nobjects. Let \\(\\Gamma''\\) be any finite subset of \\(\\Gamma'\\), and let\n\\(m\\) be the number of new constants that occur in \\(\\Gamma''\\).  Then\nexpand the interpretation \\(M_m\\) to an interpretation \\(M_m'\\) of the\nnew language, by interpreting each of the new constants in\n\\(\\Gamma''\\) as a different member of the domain \\(d_m\\). By\nhypothesis, there are enough members of \\(d_m\\) to do this. One can\ninterpret the other new constants at will. So \\(M_m\\) is a restriction\nof \\(M_m'\\). By hypothesis (and Theorem 15), \\(M'_m\\) satisfies\nevery member of \\(\\Gamma\\). Also \\(M'_m\\) satisfies the members of\n\\(\\{\\neg c_{\\alpha}=c_{\\beta} | \\alpha \\ne \\beta \\}\\) that are in\n\\(\\Gamma''\\). So \\(M'_m\\) satisfies every member of \\(\\Gamma''\\).\nBy compactness, there is an interpretation \\(M = \\langle d,I\\rangle\\)\nsuch that \\(M\\) satisfies every\nmember of \\(\\Gamma'\\). Since \\(\\Gamma'\\) contains every member of\n\\(\\{\\neg c_{\\alpha}=c_{\\beta} | \\alpha \\ne \\beta \\}\\), the domain\n\\(d\\) of \\(M\\) must be of size at least \\(\\kappa\\), since each of the\nnew constants must have a different denotation. By Theorem 15, the\nrestriction of \\(M\\) to the original language \\(\\LKe\\) satisfies every\nmember of \\(\\Gamma\\). \nCombined, the proofs of the downward and upward Löwenheim-Skolem\ntheorems show that for any satisfiable set \\(\\Gamma\\) of sentences, if\nthere is no finite bound on the models of \\(\\Gamma\\), then for any\ninfinite cardinal \\(\\kappa\\), there is a model of \\(\\Gamma\\) whose\ndomain has size exactly \\(\\kappa\\). Moreover, if \\(M\\) is any\ninterpretation whose domain is infinite, then for any infinite\ncardinal \\(\\kappa\\), there is an interpretation \\(M'\\) whose domain\nhas size exactly \\(\\kappa\\) such that \\(M\\) and \\(M'\\) are\nequivalent. \nThese results indicate a weakness in the expressive resources of\nfirst-order languages like \\(\\LKe\\). No satisfiable set of sentences\ncan guarantee that its models are all denumerably infinite, nor can\nany satisfiable set of sentences guarantee that its models are\nuncountable. So in a sense, first-order languages cannot express the\nnotion of “denumerably infinite”, at least not in the\nmodel theory. (See the entry on\n second-order and higher-order logic.) \n Let \\(A\\) be any set of sentences in a first-order language \\(\\LKe\\),\nwhere \\(K\\) includes terminology for arithmetic, and assume that every\nmember of \\(A\\) is true of the natural numbers. We can even let \\(A\\)\nbe the set of all sentences in \\(\\LKe\\) that are true of the natural\nnumbers. Then \\(A\\) has uncountable models, indeed models of any\ninfinite cardinality. Such interpretations are among those that are\nsometimes called unintended, or non-standard models\nof arithmetic. Let \\(B\\) be any set of first-order sentences that are\ntrue of the real numbers, and let \\(C\\) be any first-order\naxiomatization of set theory. Then if \\(B\\) and \\(C\\) are satisfiable\n(in infinite interpretations), then each of them has denumerably\ninfinite models. That is, any first-order, satisfiable set theory or\ntheory of the real numbers, has (unintended) models the size of the\nnatural numbers. This is despite the fact that a sentence (seemingly)\nstating that the universe is uncountable is provable in most\nset-theories. This situation, known as the\nSkolem paradox, has generated much discussion, but we must\nrefer the reader elsewhere for a sample of it (see the entry on\n Skolem’s paradox and\n Shapiro 1996). \nLogic and reasoning go hand in hand. We say that someone has reasoned\npoorly about something if they have not reasoned logically, or that an\nargument is bad because it is not logically valid. To date, research\nhas been devoted to exactly just what types of logical systems are\nappropriate for guiding our reasoning. Traditionally, classical logic\nhas been the logic suggested as the ideal for guiding reasoning (for\nexample, see Quine [1986], Resnik [1996] or Rumfitt [2015]). For this\nreason, classical logic has often been called “the one right\nlogic”. See Priest [2006a] for a description of how being the best\nreasoning-guiding logic could make a logic the one right logic.  \nThat classical logic has been given as the answer to which logic ought\nto guide reasoning is not unexpected. It has rules which are more or\nless intuitive, and is surprisingly simple for how strong it is. Plus,\nit is both sound and complete, which is an added bonus. There are some\nissues, though. As indicated in Section 5, there are certain\nexpressive limitations to classical logic. Thus, much literature has\nbeen written challenging this status quo. This literature in general\nstems from three positions. The first is that classical logic is not\nreason-guiding because some other single logic is. Examples of this\ntype of argument can be found in Brouwer [1949], Heyting [1956] and\nDummett [2000] who argue that intuitionistic logic is correct, and\nAnderson and Belnap [1975], who argue relevance logic is correct,\namong many others. Further, some people propose that an extension of\nclassical logic which can express the notion of “denumerably infinite”\n(see Shapiro [1991]). The second objection to the claim that classical\nlogic is the one right logic comes from a different perspective:\nlogical pluralists claim that classical logic is not the (single) one\nright logic, because more than one logic is right. See Beall and\nRestall [2006] and Shapiro [2014] for examples of this type of view\n(see also the entry on logical\npluralism). Finally, the last objection to the claim that\nclassical logic is the one right logic is that logic(s) is not\nreasoning-guiding, and so there is no one right logic.  \nSuffice it to say that, though classical logic has traditionally been\nthought of as “the one right logic”, this is not accepted by\neveryone. An interesting feature of these debates, though, is that\nthey demonstrate clearly the strengths and weaknesses of various\nlogics (including classical logic) when it comes to capturing\nreasoning.","contact.mail":"tkouri@odu.edu","contact.domain":"odu.edu"}]
