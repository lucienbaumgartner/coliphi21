[{"date.published":"2016-10-20","url":"https://plato.stanford.edu/entries/genomics/","author1":"Stephan Guttinger","author2":"John Dupré","entry":"genomics","body.text":"\n\n\nAbout 30 years ago researchers and other stakeholders started setting\nup the first genomics initiative, the Human Genome Project (HGP) (see \nthe link to All About the Human Genome Project (HGP) in the\n Other Internet Resources section below). \nWhat was conceived as an audacious plan in the 1980s turned\ninto an official multi-centre, international program in 1990 and was\nbrought to a conclusion in 2003.\n\n\nMore than a decade later genomics is still big in business (and big\nbusiness): the Obama administration announced in January 2015 that\nthey intend to sequence one million human genomes (see Precision\nMedicine Initiative in the \n Other Internet Resources section\n below; see also Reardon 2015). Craig Venter, the commercially minded\nnemesis of the publicly-funded HGP is also in the mix again, this time\ninvolved in a privately-funded collaboration that aims to sequence two\nmillion genomes over the course of the next ten years (Ledford 2016).\nAnd equally important, we see not only the same players clash again\nbut also the same promises being made, with talk of\n“groundbreaking health benefits” and “new medical\nbreakthroughs” appearing once again in press releases and other\nannouncements (see for instance Collins & Varmus 2015 or NIH\n2015).\n\n\nBut many things are also different now. For instance, China has\nemerged as a major player in the genomics field, with the BGI\n(formerly the Beijing Genomics Institute) already announcing in 2011\nthe aim to sequence one million genomes. Moreover, DNA sequencing is\nno longer the only goal of these large-scale initiatives: the new\ngenomics is of course still a genome-based effort, but it is a\ntransformed enterprise that also focuses on data about proteins, DNA\n methylation[1]\n patterns or the physiology and the environment of the people studied;\nDNA sequence data now forms only part of a much larger picture in the\npush for what is called ‘precision’ or\n‘personalised’ medicine. Developments such as these have\nled many to refer to the present as a ‘postgenomic’ age\n(Richardson & Stevens 2015). The goal of this entry is to look at\nthis constantly developing space of genomic and postgenomic research\nand outline some of the central philosophical issues it raises.\n\n\n Section 1\n will introduce and discuss several key terms, such as\n‘genome’ or ‘genomics’.\n Section 2\n will then turn to the question of what it means to read and interpret\nthe genome. What did the sequencing and the mapping of the human\ngenome entail and what philosophical issues arose in the context of\nthe human genome project? How did sequencing evolve into a much larger\n‘postgenomic’ enterprise and what issues did this\ntransformation bring about? To answer the last question\n Section 3\n will consider two different projects, perhaps newly emerging fields,\nnamely the HapMap project, and metagenomics. In the supplementary\ndocument\n The ENCODE Project and the ENCODE Controversy,\n we will look at the ENCODE project and the controversy that\nsurrounded it. These three cases will highlight key issues that come\nup again and again in the context of genomics and postgenomics.\n\n\nIt is also important to point out what this article is not\nabout. There are already a number of entries in the Stanford\nEncyclopedia of Philosophy (SEP) that deal specifically with\ngenes, genetics and also the HGP, and the present entry will not,\ntherefore, address in much detail the history of, or the philosophical\nissues surrounding, the concept of the ‘gene’ (see SEP\nentry\n gene,\n but also the entries\n molecular biology,\n molecular genetics, and\n the human genome project),\n or the history of the HGP (see SEP entry,\n the human genome project).\n Broader issues that also play a role in genomics, such as the notion\nof biological information and the issue of reductionism have also been\ndiscussed in a set of SEP entries (for more on reductionism see\n reductionism in biology;\n gene;\n HGP;\n and\n molecular genetics\n and for more on the metaphor of a ‘genetic program’ and\nbiological information see entries on\n biological information;\n gene;\n molecular genetics;\n and\n molecular biology).\n Furthermore, and probably most importantly of all, our focus here\nwill be on the epistemological, ontological and methodological issues\nraised by genomics rather than the ethical, legal and social issues\nthat the sequencing of DNA inevitably brings up (but see\n HGP\n entry for more on these topics).\n\n\nThe term ‘genomics’ derives from the term\n‘genome’, which itself derives (in part) from the term\n‘gene’. The meaning(s) of—and the relationships\nbetween—these different terms is by no means simple. \nThe term ‘gene’ was introduced in 1909 by the Danish\nbiologist Wilhelm Johannsen, who used it to refer to the (then\nuncharacterised) elements that specify the inherited characteristics\nof an organism (see\n gene\n and\n molecular genetics\n entries for an overview of the complex history of the term\n‘gene’). \nThe term ‘genome’ was introduced in 1920 by the German\nbotanist Hans Winkler (1877–1945) in his publication\n“Verbreitung und Ursache der Parthenogenesis im Pflanzen- und\nTierreiche” (Prevalence and Cause of Parthenogenesis in the\nPlant and Animal Kingdom). Winkler defined the term as follows: \nIch schlage vor, für den haploiden Chromosomensatz, der im Verein\nmit dem zugehörigen Protoplasma die materielle Grundlage der\nsystematischen Einheit darstellt, den Ausdruck: das Genom zu verwenden\n[…]. (Winkler 1920: 165) \nI propose to use the expression ‘genome’ for the haploid\nset of chromosomes that, in conjunction with the associated\nprotoplasm, represents the material foundation of the systematic unit\n[often translated as ‘species’]. (Translation by S.G.) \nThe etymology of the term is not clear but most authors and\nencyclopaedia entries assume that it is a combination of the German\nwords ‘Gen’ and ‘Chromosom’,\nleading to the composite ‘Genom’. In general, the\norigin and the different meanings of the -ome suffix are not entirely\nclear and there are now several accounts that try to bring some\nstructure and/or meaning to the ever flourishing -omes terminology in\ncontemporary life sciences (see, e.g., Lederberg & McCray 2001;\nFields & Johnston 2002; Yadav 2007; Eisen 2012: Baker 2013; for interesting/entertaining\nlists, see -omes and -omics in the \n Other Internet Resources section  below). \nThe term ‘genomics’, finally, was invented in 1986 at a\nmeeting of several scientists who were brainstorming (in a bar) to\ncome up with a name for a new journal that Frank Ruddle (Yale\nUniversity) and Victor McKusick (Johns Hopkins University) were\nsetting up. The aim of this journal was to publish data on the\nsequencing, mapping and comparison of genomes. To capture these\ndifferent activities—and in analogy to the well-established\ndiscipline of genetics—Thomas Roderick (Jackson Laboratory)\nproposed the term ‘genomics’ (Kuska 1998). Unbeknownst to\nthe people involved this was a significant moment in the history of\nthe life sciences, as it is here that the -omics suffix appears for the\nfirst time. \nLooking at the history and the etymology of a term does not, of\ncourse, necessarily tell us a lot about how it is used in the context\nof current science. So what is a genome in today’s life\nsciences? Is it the (haploid) set of chromosomes we find in the\nnucleus of a eukaryotic cell, in line with the original definition by\nWinkler? Or is it the totality of genes we find in an organism or the\ntotality of DNA present in a cell? And if so, which DNA? Most\ndefinitions that are currently in circulation are an intricate mix of\ndifferent ways of approaching the issue. This can be illustrated by\nlooking at the definitions given in several key online resources (for\nmore definitions of the term ‘genome’ see Table 1 in\nKeller 2011). \nThe term is defined on the genome.gov website glossary: \nThe genome is the entire set of genetic instructions found in a cell.\nIn humans, the genome consists of 23 pairs of chromosomes, found in\nthe nucleus, as well as a small chromosome found in the cells’\nmitochondria. Each set of 23 chromosomes contains approximately 3.1\nbillion bases of DNA sequence. (Talking Glossary: genome, in the\nOther Internet Resources) \nAnd this is how the U.S. National Library of Medicine defines it: \nA genome is an organism’s complete set of DNA, including all of\nits genes. Each genome contains all of the information needed to build\nand maintain that organism. In humans, a copy of the entire\ngenome—more than 3 billion DNA base pairs—is contained in\nall cells that have a nucleus. (NIH 2016) \nSimilarly the education portal of the journal Nature: \nA genome is the complete set of genetic information in an organism. It\nprovides all of the information the organism requires to function. In\nliving organisms, the genome is stored in long molecules of DNA called\nchromosomes. (Scitable: genome, in the Other Internet Resources) \nAll of these definitions refer both to information and to instructions\nfor the development and/or functioning of an organism. In the first\ntwo, the genome is also identified with a material entity, in the\nfirst case the chromosomes, in the second a sequence of base pairs.\nNature allows only that the information is “stored\nin” the chromosomes. \nThe combination of these two aspects is highly problematic. The\ndefinition from the U.S. National Library of Medicine implies that\n“all of the information needed to build and maintain that\norganism” is contained in the\n DNA,[2]\n which is certainly false: many environmental factors, not to mention\nfactors in the maternal cytoplasm, are required for the first task,\nand even more obviously (food, light, etc.) for the second. Moreover\nwhen, as is almost always the case, an organism requires symbiotic\npartners for its proper functioning, such a definition will imply that\nthe DNA of these symbionts is part of the genome of the first\norganism, a result that few would\n welcome.[3]\n The Nature definition commits the same error in its second\nsentence. The genome.gov definition appears to identify the\nchromosomes both with a set of instructions and a material entity,\nwhich appears rather problematically to conflate a material object\nwith an abstract entity.  \nThe problem is not hard to see. Attempting to combine aspects of the\nmaterial base of the genome and its informational content, as all\nthese definitions do, inevitably assume some simple relation between\nthese two; but in fact the relationship is extremely complex. Because\nthe informational content of the genome is dependent in multiple ways\non elements that are not, on any account, part of the genome, an\naccount in terms purely of informational content seems a hopeless\nproject. \nOne commonly held view that can be quickly dismissed, is the idea that\nthe genome is just the sum total of an organism’s genes. The\nproblem here is just that even passing over the well-known problems\nwith saying what a gene is (see Barnes & Dupré 2008; SEP\nentry on the\n gene),\n on any tenable account of genes, there is far more to the genome than\ngenes, and only a fraction of the actual DNA contained in the\nchromosomes would be part of the genome, at least in the case of\nhumans and other organisms that have a relatively large amount of\nnon-coding DNA (Barnes & Dupré 2008:\n 76).[4]\n Even if ‘gene’ is interpreted in the widest possible\nsense, including any section of the genome that has some identifiable\nfunction, no one denies that a significant amount of DNA is not\nfunctional. The rest of the DNA would not form part of the genome, an\noutcome that contradicts all definitions of the genome of which we are\naware, and makes nonsense of such familiar concepts as\n‘whole-genome sequencing’, which refers to the analysis of\nall the DNA found in the chromosomes. \nThere are, we suggest, two initially tenable approaches to the\n problem.[5]\n The first, and one that is often implicitly or explicitly assumed to\nbe correct, is to define the genome as the sequence of nucleotides.\nThis may or may not contain extranuclear DNA, as in mitochondria or\nchloroplasts; the genome.gov definition explicitly includes the\nformer. This last question figured largely in debates over the moral\npermissibility of so-called mitochondrial transplants (a designation\nthat speaks volumes, incidentally, about the almost magical importance\nattached to DNA as opposed to the remaining contents of the cell), but\nit is not one of great philosophical significance. The alternative\napproach is to understand the genome strictly as a material object,\npresumably, in most cases, the nuclear chromosomes.  \nThe problem with the first approach is that it is largely motivated by\nthe assumption that the nucleotide sequence is what contains all the\nimportant information in the genome. But in fact it has become\nincreasingly clear that this is not the case, especially as a result\nof the growing understanding of epigenetics. Epigenetics is the study\nof material modifications of the genome that affect what parts of the\ngenome sequence are or are not transcribed into RNA, the first stage\nof the process by which the genome influences the containing organism.\nThe two most well-studied classes of epigenetic modification are\nmethylation, the attachment of a methyl group (-CH3) to one\nof the four nucleotides, cytosine, and various chemical modifications\nof the histone proteins, proteins that form the core structure of the\nchromosomes, and around which the DNA double helix is wrapped\n(Bickmore & van Steensel 2013; Cutter & Hayes 2015). The\nnucleotide sequence, then, provides the (extremely large) set of\npossible transcripts that the genome can produce, but the epigenetic\nstate of the genome determines which transcripts are actually produced\n(Jones 2012). Both features of the genome (qua material\nobject) must be specified, therefore, if we want to understand the\nbiologically relevant behavior of the whole system. \nSo if the motivation for defining the genome in terms of sequence is\nto capture its informational content, the definition fails to serve\nits goal. Indeed, the definition that will come closest to this goal\nis that which identifies the genome as the material object, the set of\nchromosomes (this interpretation of the genome is defended in detail\nin Barnes & Dupré 2008). An implication of this definition\nthat is often taken to be counterintuitive by biologists is that the\ngenome will on this account encompass not only DNA, but the histone\nproteins that are material parts of the chromosomes. But of course the\npoint of the preceding discussion is that the variable chemical states\nof the histones are, in fact, essential bases for some of the\ninformation inherent in the genome. \nThe phenomenon of methylation makes a similar point in a slightly\ndifferent way. The nucleotides that comprise the familiar sequence are\ncytosine, thymine, adenine and guanine). When a methyl group attaches\nto the cytosine molecule the resultant nucleotide is not, strictly\nspeaking, cytosine, but 5-methyl cytosine. So unless one takes the\nletter ‘C’ in the standard representation of sequence to\nmean, rather counterintuitively, “cytosine or 5-methyl\ncytosine”, it is only a partially accurate representation of the\nfeature of the genome it purports to represent. More importantly, it\nis a representation that fails to capture crucial functional aspects\nof the genome. \nA final telling point is that it has recently become clear that there\nare functions of the genome, as material object, that go well beyond\neven the broadest interpretation of the genetic (Bustin & Misteli\n2016). It appears that the genome plays an essential role in a range\nof cellular processes. First, its physical arrangement into domains of\nvarying sizes plays a central role in the coordination of gene\nexpression. But much further from the genetic, it is a large object\nthe mechanical forces of which are involved in various cellular\nprocesses and cellular homeostasis, and the chromatin fiber provides a\nscaffolding for both proteins and membranes (Bustin & Misteli\n2016). Unless we are to introduce a new word to refer to this\nbiologically vital entity, only a material conception of the genome\ncan capture the full range of its activities. \nOne might be tempted to object to the argument above concerning\nmethylation, that whereas methylation is a somewhat transitory state,\nthe underlying four-letter sequence is extremely durable, lasting\nacross many generations. Richard Dawkins (1976) famously emphasized\nthe importance of this durability in arguing for the importance of\nthis stability in evolution, even going so far as to describe genes as\n“immortal”. So perhaps there is a good reason for\nunderstanding “C” as referring to a disjunction. \nThis is not the place to address the quasi-theological view of gene\nimmortality. However, this does point to a fundamental issue about the\nnature of genes. Even if genes, somehow, were unchanging immortal\nsubstances, the genome is nothing of the sort. It is an extremely\ndynamic entity, constantly changing its properties in generally\nadaptive response to it environment. Moreover even the constancy of\nits nucleotide sequence is something maintained only by the continuous\napplication of various editing and repair mechanisms. Indeed, far from\nbeing an eternal substance, we suggest it is much better seen as a\nprocess, a highly complex set of dynamic activities crucial in\nmaintaining the structural and functional stability not only of the\norganism but also, through its role in reproduction, of the lineage.\nImportantly, these relations are bi-directional and, specifically, the\norganism is also crucial to maintaining the necessary aspects of\nstability of the\n genome.[6] \nThe first genome to be sequenced was that of a virus, namely\nbacteriophage ΦX174, sequenced by Frederick Sanger in 1977 (Sanger et\nal. 1977). Up to about 1985, work on several other viruses was\ninitiated in different laboratories across the world and even the\nsequencing of model organisms such as the bacterium Escherichia\ncoli or the roundworm Caenorhabditis elegans was being\n tackled.[7] \nOf all the different sequencing efforts at the time the human genome\nproject (HGP) of course stands out. Not only is the human genome\nrelatively large (roughly 3.2 billion base pairs (bps)) and of key\ninterest to us as human beings, but the HGP itself was envisioned as a\ndiverse large-scale research project with various strands and aims.\nGetting the sequence out of this project was the one goal that got the\nmost attention in the wider media, but surely many would agree that\nother findings and practices developed within the HGP were of equal or\neven greater importance. \nIn what follows we will treat the HGP as a pivot around which genomics\ndeveloped as a field of research and as a set of techniques. For ease\nof exposition we will talk here of a pre-HGP and a post-HGP phase.\nObviously, this is a simplification; there is not just one single\ntrajectory along which the story of genomics runs and there is not one\nclear break between a pre- and a post-genome era (Richardson &\nStevens 2015). Nevertheless, as a way of structuring the discussion\nthis distinction will be a helpful tool. \nA decade after Sanger and Maxam and Gilbert published their DNA\nsequencing methods in 1975 the first concrete talk of a human genome\nproject started to appear in writing (Dulbecco 1986) and at different\nworkshops (Sinsheimer 1989; Palca 1986). The Human Genome Project\n(HGP) itself became a reality in 1990 when it was officially launched\nas a US federal program (see 1990 in a brief history and timeline\n[NHGRI] in the \n Other Internet Resources section\n below). \nIn the run-up to the HGP there were high expectations (some would say\n“hype”) developing, which inevitably also brought critics\nof the project onto the scene (Koshland 1989; Luria et al. 1989). As\nso often, the issue of funding had a key role to play. When the HGP\nwas initiated there were no ‘big science’ projects being\npursued in the life sciences. The HGP therefore was a true first for\nbiology. But pushing such a large project that absorbed a significant\nproportion of the funding allocated to the biological sciences\nencountered a lot of resistance from other scientists. \nThere were three key criticisms: 1) Some claimed that the HGP was a\nwaste of money because much useless (read: junk) DNA was sequenced;\nthe focus should be more directly on the functional parts of the\ngenome, i.e., the genes or regulatory elements, which could be\nachieved using simpler and less expensive methods (Brenner 1990;\nWeinberg 1991; Rechsteiner 1991; Lewontin 1992; Rosenberg 1994).\nOthers claimed 2) that the HGP was a waste of money as it was merely a\ndescriptive and not a hypothesis-driven project. This was an issue\nthat became much more prominent ten years after the project was\nfinished, when it became clear that big data science was here to stay\n(see, e.g., Weinberg\n 2010).[8] \nAnd last but not least there was also the critique 3) that the HGP is\nfundamentally misguided as it assumes that by using sequence knowledge\nalone we would be able to develop an understanding of how our body\nworks, how it develops disease, and that this understanding will\neventually lead to cures for many diseases (Lewontin 1992; Tauber\n& Sarkar 1992; Kitcher 1994). This more general critique of a\nnarrowly sequence-focused approach to biomedical issues also comes up\n20 years later in discussions about the use of common genetic variants\nto learn more about common diseases and traits (see\n Section 3.1.2).\n  \nIt is difficult to evaluate criticisms of the last kind. There is no\ndoubt that enthusiasm for the HGP and many other successor projects in\ngenomics has often been grounded in simplistic assumptions about the\npower of DNA and its pre-eminent role in biological systems. On the\nother hand it is arguable that many unanticipated benefits have\nderived from genomics quite independently of such assumptions. For\ninstance the ability to make very precise comparisons of genome\nsequences has led to major advances in unraveling the details of\nevolutionary history, not to mention its application to technologies\nsuch as forensic DNA testing. Moreover, it can be argued with Waters\n(2007b) that what makes\ngenomes so central to biological research is not the erroneous belief\nthat they are the ultimate causes of everything, but rather the unique\npossibilities they present for precise intervention in organisms or\ncells. \nThe main output of the HGP is usually seen as ‘the’ human\ngenome sequence. The draft human genome sequence (about 90% complete)\nwas announced in June 2000, followed in 2001 by the publication of the\ndraft sequences produced by the HGP (International Human Genome\nSequencing Consortium 2001) and the privately funded initiative\n(Venter et al. 2001). The complete (or almost complete (99%)) sequence\nof the human genome was released in 2003, which also marked the\nofficial ending of the HGP (International Human Genome Sequencing\nConsortium 2004). \nBut the view that the sequence of ‘the’ human genome was\nthe key output is wrong in several ways. First of all there is in\ngeneral no such thing as ‘the’ human genome, as each\nindividual (except for monozygotic twins) carries their own set of\nsmall and large variations in their genome (and even for twins there\nare many differences they accumulate in their genomes during their\nlifetime). The sequence that was produced in the HGP is therefore\nnothing more than an example of one particular sequence, meaning it\ncan only serve as a reference genome. Importantly, the reference\nsequences that both the HGP and Venter’s project delivered did\nnot correspond to the genome of a single person as the DNA used to\nproduce them was derived from several\n individuals.[9]\n The genomes that came out of the two sequencing efforts were\ntherefore composite reference sequences. But the HGP also produced\nmuch more than just a DNA sequence. Here we will highlight three\noutcomes or aspects of the HGP that are of particular importance, also\nfor the period that followed the completion of the project. \nOne key feature of the HGP was that it involved the sequencing of a\nrange of different model organisms, an aspect of the HGP that was\noften overlooked in discussions of the project in the philosophical\nliterature and elsewhere (Ankeny 2001; for a searchable list of\nsequenced genomes see genome information by organism in the\n Other Internet Resources section\n below). The HGP provided not only a first reference genome of\nHomo sapiens but also the first bacterial genome\n(Haemophilus influenzae, Fleischmann et al. 1995), the first\neukaryotic genome (Saccharomyces cerevisiae, Goffeau et al.\n1996), and the genomes of key model organisms (Escherichia\nColi, Blattner et al. 1997; Caenorhabditis elegans,\nC. elegans Sequencing Consortium 1998; Arabidopsis\nthaliana, Arabidopsis Genome Initiative 2000; Drosophila\nmelanogaster, Adams et al. 2000, Myers et al.\n 2000).[10] \nA further crucial output was the acceleration in technology\ndevelopment the HGP brought about. It is safe to say that without the\nHGP (and subsequent initiatives such as the Advanced Sequencing\nTechnology Awards created in 2004 by the National Human Genome\nResearch Institute (NHGRI) (NIH 2004)) there wouldn’t have been\nsuch a rapid development in next-generation sequencing (NGS)\napproaches and the cost of whole genome sequencing would not have\ndropped as quickly as it has (see Mardis 2011 for a review of the\ndevelopment of NGS). And these improvements in the sequencing\ntechnology had further consequences, for example allowing scientists\nto sample DNA in different ways and from different sources, as new\nsequencing methods could process more DNA material more quickly and\nwork with less starting material. This, finally, made possible whole\nnew sub-disciplines, such as metagenomics (see\n Section 3.2). \nA final noteworthy output of the HGP is what scientists learned about\nthe structure of the genome. Beginning with the HGP, and building on\nfurther studies, researchers have gained a much more detailed picture\nof the fine structure, the dynamics and the functioning of the human\ngenome. It was not only that there were many fewer genes present than\nexpected, but there was also much more repetitive DNA and transposable\nelements present (it is estimated that about 45% of human DNA consists\nof transposable elements or their inactive remnants). These findings\nrelate to a more general and older discussion about genome size and\ncomplexity to which we next turn. \nIt has been known since the 1950s that genome size varies greatly\nbetween different organisms (Mirsky & Ris 1951; see also Gregory\n2001), but from the very beginning it was also clear that this\ndiversity has some surprising features. One of these features is the\nabsence of correlation between the complexity of an organism and the\nsize of its genome. \nAssuming an informational account of the genome one would expect that\nthe more complex an organism is, the more DNA its genome should\ncontain (this is in fact what many biologists assumed at least until\nabout the 1960s). How to define and assess the complexity of an\norganism is a tricky issue, but intuitively it seems reasonable to\nassume that a single-celled amoeba is less complex than an onion,\nwhich in turn is less complex than a large metazoan such as a human\nbeing, both in terms of the complexity of the workings and the\nstructure of the organism. The expectation was that the DNA content of\nhuman cells should be much larger than that of onions or amoebae. As\nit turns out, however, both the onion and the amoeba have much larger\ngenomes than human beings. The onion, for instance, has a genome of\nabout 16 billion base pairs, meaning it is about five times the size\nof the human genome (Gregory 2007). The same lack of\ncorrelation between genome size and complexity can be found in many\nother instances (for an overview of different genome sizes see the\nanimal genome size database in the\n Other Internet Resources section\n below). \nIt was also found early on that very similar species in the same genus\nshow large variation in genome size, despite having similar phenotypes\nand karyotypes (i.e., number and shape of chromosomes in a genome).\nWithin the family of buttercups, for instance, DNA content varied up\nto 80-fold (Rothfels et al. 1966). Also, Holm-Hansen (1969) showed\nthat species of unicellular algae display a 2000-fold difference in\nDNA content despite all being of similar developmental complexity. It\nwas findings such as these that gave a real urgency to addressing this\ndiscrepancy that was now labelled the C-value paradox (Thomas 1971).\nThe term ‘C-value’ refers to the constant\n(‘C’) amount (‘value’) of haploid DNA per\nnucleus and is measured in picograms of DNA per nucleus. The C-value\nis a measure of the amount of DNA each genome contains (we can see\nhere Winkler’s original definition of the genome at work). \nThese discussions of genome sizes were closely related to concerns\nabout gene numbers. And this consideration of genome size vs. gene\nnumbers is what originally gave rise to the concept of ‘junk\nDNA’ (Ohno\n 1972).[11]\n The reasoning behind this concept was the following: if one assumes\na) that more complex organisms will have more DNA than less complex\norganisms and b) that gene numbers increase in proportion with genome\nsize, then the genome of the more complex organism should have more\ngenes than the less complex\n one.[12]\n Human cells, for instance, contain about 750x more DNA than E.\ncoli, meaning that they should turn out to have in the range of\n3.7 million genes, as E. coli has about 5000 genes. This is\nclearly not the case; even in the 1970s it was generally supposed that\nthe human genome might contain no more than 150,000 genes (Crollius et\nal. 2000). This discrepancy leads to the conclusion that the vast\nmajority of the DNA in our genome cannot be genes and is therefore\nwhat Ohno referred to as\n ‘junk’.[13] \nThe problem that the junk DNA discussion brings up has also been\nreferred to as the ‘G-value paradox’ (‘G’\nstands for ‘gene’), which directly concerns the\ndiscrepancy between the number of genes in an organism and its\ncomplexity (Hahn & Wray 2002). This paradox has been reinforced by\nthe findings of the HGP. As Gregory (2005) and other commentators have\npointed out, the finding that the human genome contains many fewer\ngenes than expected was one of the most surprising outcomes of the\nHGP. Initial estimates from before the project were in the range of\n50,000 to 150,000. These were reduced to about 30,000—35,000\nafter the publication of the first sequence draft in 2001 and have now\nbeen further revised to the order of 20,000 (Gregory 2001). \nSome researchers assumed that the C-value paradox was fully resolved\nby the recognition that there is non-coding DNA in genomes (Gregory\n2001). Larger genome size in ‘simpler’ organisms merely\nmeans that they have large quantities of non-coding DNA. But as\nGregory points out, the fact that the majority of DNA in our genomes\nis non-coding might make the C-value discrepancies less of a paradox,\nbut it gives rise to a whole range of further puzzles (Where does this\nextra DNA come from? What is its function? Etc.), which is why he\nproposes to talk of the C-value as an enigma rather than a\nparadox (Gregory 2001). The C-value enigma consists of many different\nand layered problems and these require a pluralistic approach to\nanswering them, or so Gregory claims. \nThe publication of the draft genome sequence in 2001 and the\nconclusion of the HGP in 2003 did not give researchers all the tools\nand insights they needed to tackle these long-standing problems. But\nafter the HGP, building on the initial sequencing effort, researchers\ncould start to go beyond the mere sequence and gain a deeper\nunderstanding of the workings of the genome. This put them in a\nposition to tackle issues such as the significance of junk DNA and the\nC-value paradox more directly (or at least from a different angle).\nThe post-HGP phase is also characterized by an intense debate about\nthe best way of doing research: the question of whether biological\nresearch should best be done on a small or a large scale has come up\nagain and again in the post-HGP era, especially with the rise of other\npost-HGP large scale projects. The next section will address two\nprojects/research fields that symbolize the various efforts and\naspirations that were characteristic of the post-HGP era and which\nwill help to illuminate some of the philosophical issues these\ndevelopments raised.  \nThe post-HGP phase is marked by a flourishing of different projects,\nclosely connected in their origins to the HGP, but going beyond it in\nmany different ways. This section discusses two such post-HGP\nprojects, namely the International HapMap project and a new field of\nresearch called ‘metagenomics’.  These examples indicate\nsome important directions in which the postgenomic era is heading and\nidentify some, though certainly not all, of the key characteristics\nand issues that mark this new period. \nThe International HapMap project was a multi-centre project launched\nin 2002 that came to an initial conclusion in 2005 (NIH \n 2002).[14]\nThe acronym ‘HapMap’ stands for ‘haplotype map’\nand (indirectly) refers to the main goal of the project, namely to map\nthe common genetic variation in the human genome. \nIt is a well-known fact that everyone’s genome is different.\nThere are, however, several ways in which genomes of individuals can\nvary from each other, ranging from the deletion, insertion or\nrearrangement of longer stretches of DNA to differences in single\nnucleotides at specific locations on a chromosome. The latter form of\nvariation was the focus of the HapMap project. If we align the DNA\nsequence of two individuals they will be identical for hundreds of\nnucleotides; the DNA of two human beings typically displays about\n99.9% sequence identity (Li & Sadler 1991; Wang et al. 1998;\nCargill et al. 1999). But the 0.1% difference means that approximately\nevery 1000 nucleotides there will be a difference in a single\nnucleotide between any two individuals. \nAny variation at a specific genomic locus is referred to as an\n‘allele’. If there are two different versions of a\nspecific gene that can be found in a population at a specific locus on\na chromosome, then that means that there are two different alleles of\nthat gene present in that\n population.[15]\n If one of these single nucleotide alleles is found in more than 1% of\na specific population it is treated as a ‘common’ variant\nand researchers speak of a ‘polymorphism’ or, more\nprecisely, a ‘single nucleotide polymorphism’ (abbreviated\n‘SNP’; pronounced ‘snip’). If a variation is\nfound in less than 1% of the population researchers simply call it a\n‘mutation’ (or also a ‘point\n mutation’).[16]\n On average there are about 3 million SNPs found in each individual\nand there is a pool of more than 10 million SNPs present in the human\npopulation as a whole (HapMap 2005). \nMany of these alleles are (or have an increased likelihood of being)\ninherited together, meaning that they do not easily become separated\nthrough recombination events during\n meiosis.[17]\n This leads to the non-random association of different alleles at two\nor more loci, a phenomenon that has been dubbed ‘linkage\ndisequilibrium’ or ‘LD’. The concept of LD is key\nfor the HapMap project as the fact that some SNPs stay associated\n(whereas the clusters themselves might get separated from each other\nover time by recombination events) explains the haplotype structure of\nthe genome (Daly et al. 2001). The term ‘haplotype’ simply\nrefers to a particular cluster of alleles (in this case SNPs) that a)\nare on the same chromosomes and b) are commonly inherited as one. The\naim of the HapMap project was to characterize human SNPs, their\nfrequency in different populations and the correlations between them\n(HapMap 2003). The first haplotype map was published in 2005,\nreporting on data from 269 samples derived from four different\npopulations (HapMap 2005). Five years later, a follow up was\npublished, now reporting on data from 1184 individuals sampled from 11\ndifferent populations (HapMap 2010). \nThe realization that the structure of genetic variation in the genome\ncan be understood in terms of haplotypes was important for at least\ntwo reasons. First it opened the door for a relatively easy and\nefficient analysis of (single nucleotide) genetic variation in\npopulations: the clustering of SNPs meant that in principle only one\nor a few of the SNPs in each cluster (so-called ‘tag\nSNPs’) would have to be tested to verify the presence of the\ncluster of variants as a whole. This made the analysis of genetic\nvariation at the level of whole genomes from a large number of\nsubjects feasible at a time when whole-genome sequencing was still too\nexpensive for such a task (HapMap 2003). The development of a\nhaplotype map was therefore a crucial step to enable what are now\ncalled ‘genome-wide association studies’ (GWAS) (see\n Section 3.1.2). \nSecondly, as the distribution of haplotypes varies between different\npopulations, the HapMap project had a strong focus on sampling DNA\nfrom different populations. This is an important aspect of this type\nof research as it brought, unwittingly perhaps, the issue of race and\nthe question of its biological basis right back into genomics. This\npoint will be revisited in\n Section 3.1.4. \nA key point driving the HapMap project was the fact that SNPs can be\nused to uncover connections between an individual’s DNA sequence\nand specific conditions or traits. At face value an SNP is simply a\ndistinguishing mark in the genome of a person. Such marks allow\nresearchers to screen groups of a population with different\nphenotypes, for instance those with a condition (e.g., high blood\npressure) and those without. Looking at the frequency of specific SNPs\nor haplotypes in either group the researchers can use statistical\nanalysis to get insight into the association between a particular SNP\nor haplotype and a trait (Cardon & Bell 2001). As mentioned above,\nthis analysis can be focused on tag SNPs that are treated as proxies\nfor a whole cluster of SNPs (if the cluster has a high LD). \nOnce a haplotype has been associated with a particular condition,\nother people can be screened for the presence of that haplotype and\ntherefore gain some understanding of the risk groups they belong to.\nAlthough the test will not tell carriers of disease-linked SNPs\nwhether they will develop the condition or not, it can nevertheless\ngive them some information about their chances. Furthermore, even\nthough the tag SNP itself might not be the genetic variation that\ncauses or contributes to the variation in phenotype, it might be\nlinked to so-called ‘causal SNPs’. Learning about SNPs\nassociated with a condition or trait therefore can give the researcher\nclues as to which genes or regulatory DNA regions might be causally\ninvolved in the development of that condition. Findings from\nassociation studies can therefore in some cases contribute to the\nanalysis of the condition itself. \nThe HapMap initially only looked for common variants (SNPs include by\ndefinition only common variants). This was in line with the so-called\ncommon disease/common variant (CD/CV) hypothesis formulated by Lander\n(1996); Cargill et al. (1999), and Chakravarti\n (1999).[18]\n This hypothesis postulates, roughly, that common conditions are\nlinked to genetic variations that are common in a population. \nThis link between common variants and common diseases also explains\nwhy the HapMap project could be promoted from the very beginning as\nthe ‘next big thing’ after the sequence of the human\ngenome had been determined: it was with the haplotype map that\ngenomics should really start to have an impact on biomedical research\nand ultimately our understanding of\n disease.[19] \nBut the HapMap project was not without its critics; indeed the\nbiologist David Botstein called it a “magnificent failure”\n(cited in Hall\n 2010).[20]\n Some commentators, for instance, were worried that the project is\nnothing more than a make-work project filling a gap that the finished\nHGP left behind, and therefore a waste of precious funds (Couzin\n2002). But more often, criticism of the HapMap project was part of\nwider debates about the way post-HGP research should be conducted. The\nHapMap project can therefore provide a useful window on some of the\nkey tendencies and disputes that marked (or marred) the post-HGP\nera. \nOne such indirect criticism of the HapMap derives from the apparent\nfailure of GWAS to lead researchers to clearer information about the\nlinks between our genetic makeup and the different conditions to which\nour bodies can succumb. In the eyes of these critics the CD/CV\nhypothesis was the key problem, as the common variants simply do not\nexplain much of the heritability of common diseases. This observation\ngave rise to the concept of ‘missing heritability’\n(Eichler et al. 2010). \nThe general focus on common variants in genomics was criticized by\nother authors who claimed that the focus of geneticists should rather\nbe on rare variants (McClellan & King 2010). These rare variants,\nthey claim, are where the missing heritability will be found. The\nproblem with the rare variants is that they cannot be picked up in\nGWAS that use SNP databases, as SNPs are by definition common\nvariants. Also, finding rare variants is a technical challenge as\nresearchers have to analyse the genomic data of a very large number of\nindividuals to do so reliably. This hunt for rare variants is a major\nreason behind the current push for the sequencing of millions (rather\nthan a couple of hundreds or thousands) of genomes. As discussed\nearlier, such large-scale approaches have become feasible in recent\nyears due to the reduced cost and increased speed of next-generation\nDNA sequencing. \nThe current shift to whole-genome sequencing will also help to address\nanother critique of the GWAS/SNP/HapMap approach, namely its strict\nfocus on single base pair changes in the genome. Other changes in the\ngenome, such as variations in the numbers of copies of repeated\nelements or rearrangements, deletions or insertions of larger chunks\nof genomic DNA, might in many cases be what is at the core of a\ndisorder, necessitating (again) a shift in focus away from point\nmutations and single genes to the genome as a whole (Lupski 1998,\n2009). \nAs one of the first follow-ups to the original HGP, the HapMap project\nwas a topic that often came up in discussions of the legacy of the\nHGP. Such discussions became especially prominent at the tenth\nanniversary of the publication of the draft genome sequence. In\ngeneral, there was an overwhelming sense of disappointment at what had\ncome out of the HPG, at least in the medical context. Given the grand\npromises that were made both around the start of the project in the\n1980s and then again in the year 2000 at the presentation at the White\n House,[21]\n it is not surprising that people were unimpressed by what had been\ndelivered by 2010/2011. Interestingly, it was not only the usual\nsuspects, such as Lewontin (2011), but also key proponents of the HGP itself who were\ncritical and pointed out the minimal medical advances that had been\nachieved in the first post-HGP decade (Collins 2010; Venter 2010). \nHowever, one thing that all critics, including the above-mentioned,\nagreed on was that even though its effect on medical practice had been\nnegligible, the HGP had transformed biological research (see for\ninstance Wade 2010; Varmus 2010; Hall 2010; Butler 2010; Green et al.\n2011). One area in which genomic research had fundamentally changed\nboth concepts and practices was in the understanding of what a gene is\nand how gene expression works and is regulated (Keller 2000; Moss\n2003; Dupré 2005; Griffiths & Stotz 2006; Stotz et al.\n2006; Check 2010). With great foresight, Evelyn Fox Keller pointed out\nalready in 2000 that the HGP was interesting not so much because of\nthe raw sequence it produced, but more because of the transformations\nit brought about in our expectations when it comes to\n‘genes’ and DNA (Keller 2000). \nAs mentioned above, HapMap’s use of samples from different\npopulations brought the concept of race into discussions of the\nproject. Studies that looked into the genetic variation between\npopulation groups (of which the HapMap was a key representative) are\namong several recent developments (Duster 2015) that reignited\ndiscussion about a) the biological reality of race and b) the question\nwhether racial classifications should be used in biomedical research\nat all. Several authors have picked up the relation between the HapMap\nproject and a renewed concern with race (see, e.g., Ossorio 2005;\nDuster 2005; Hamilton 2008). The question that dominates these\ndiscussions is whether racial classifications reflect a\n‘biological reality’. \nRace has of course been an important topic in epidemiology and\nclinical research for a long time (Witzig 1996; Stolley 1999), but it\nhas been widely perceived as a socially constructed category that has\nno biological\n basis.[22]\n And many researchers imagined that as the HGP demonstrated how highly\nsimilar any two human beings are to each other at the DNA level, any\nidea of race as serious biological concept would be disposed of once\nand for all (see, e.g., Gilbert 1992; Venter 2000). But the concept of\nbiological race was if anything rejuvenated rather than laid to rest\nby the developments in genomics (Kaufman & Cooper 2001; Foster\n& Sharp 2002; Hamilton 2008; Roberts 2011). This is exemplified by\nthe fact that more and more scientists have claimed in recent years\nthat there is a biological basis to our traditional notions\nof race, basing their claims on elaborate statistical analyses of data\non genetic variation derived from a large number of human DNA samples.\nThese developments led for many to what Troy Duster has called a\n‘post-genomic surprise’ (Duster 2015).  \nAn important point here is that linking genomics and race does not\nmean that researchers search for, or even that there are, any\n‘genes for race’, even if we consider the many different\nways in which this term can be interpreted (Dupré 2008). The discussion about the\npossible genetic basis for race is now more subtle, as it is not\nsimply concerned with the presence or absence of specific genes or DNA\nelements and hence some sort of biological essence of races, but\nrather with the variation in the frequencies of alleles in the\npopulation of interest (Gannett 2001, 2004). The question is therefore\nnot whether DNA element X is absent or present in one\npopulation or the other, but rather which variant of X is\npresent at what frequency in a population (in the context of the\nHapMap researchers will talk of SNP frequencies). \nData from population genetics shows that the global distribution of\nallele frequencies in the human population is not discontinuous (Jorde\n& Wooding 2004; Feldman & Lewontin 2008) but clinal, meaning\nthat human DNA sequences vary in a gradual manner over geographic\nspace (Livingstone 1962; Serre & Pääbo 2004; Barbujani\n& Colonna 2010). Moreover, both genetic and phenotypic traits\ndisplay what is called ‘nonconcordant’ clinal variation,\nmeaning that different traits do not necessarily co-vary with each\nother; the pattern of how trait A varies across geographic space might\nbe very different from the pattern displayed by trait B (Livingstone\n1962; Goodman 2000; Jorde & Wooding 2004). \nBut despite these widely accepted findings, it is in the discussion of\nthese distributions that the idea of a biological basis for our\ntraditional understanding of race classifications has re-emerged.\nBased on the analysis of large sets of genetic variants in samples\nderived from various locations around the globe, a number of\nresearchers have made the claim that human genetic variation displays\ngeographical clustering (see, e.g., Rosenberg et al. 2002; Edwards\n2003; Burchard et al. 2003; Bamshad et al. 2003; Leroi 2005; Tang et\nal. 2005). Importantly, these findings often also gave rise to, or\nwere interpreted to support, the claim that this geographical\ndistribution matches our traditional racial classifications. \nSuch findings also led a number of authors to claim that race still\nhas a valid place in biomedical research: since these classifications\nare supposed to describe groups that are internally genetically\nsimilar, but genetically different from other groups, they can serve\nas useful proxies in estimating, for instance, the group\nmember’s average risk of developing a particular condition (see,\ne.g., Xie et al. 2001; Wood 2001; Risch et al. 2002; Rosenberg et al.\n2002; Shiao et al. 2012). Some authors are more cautious and claim\nthat race should only serve as a loose and temporary proxy (Foster\n& Sharp 2002; Jorde & Wooding 2004) that should be abandoned\nas soon as we know the actual genetic variations that are linked to a\nparticular condition or trait (Jorde & Wooding 2004; Leroi 2005;\nDupré 2008). Such\ncritics may note that the most that these genetic studies show is that\nthere is a correlation between a person’s genetic variants and\ntheir geographical origin, if only because variants originate in a\nspecific place; and there is a loose relation between the socially\nconstructed concept of race and geographic origin. But given the\ntenuous connection that this generates between perceived or\nself-identified racial categories and genetic constitution, race is a\npoor substitute for any actually salient genetic information that may\neventually be related to disease.  \nBut there is also a significant group of researchers who are not\nconvinced by these analyses and who don’t think that there is\nany biological basis to the race concept (see, e.g., Schwartz 2001;\nDuster 2005, 2006; Krieger 2000; Ossorio 2005). All of these authors\ncriticise the above studies and the geographic clusters of genetic\nvariation they identify, mainly because of flaws in the way samples\nare collected (see, e.g., Duster 2015) and how the data is ultimately\nanalysed. The latter criticism has mainly focused on the program\n‘Structure’ that is used by a majority of the studies\nmentioned above to churn out clusters of genetic variation (Bolnick\n2008; Kalinowski, 2011; Fujimura et al. 2014). A telling criticism is\nthat while Structure can be made to report that there are five main\ngeographical clusters that show distinctive allele frequencies and\nwhich roughly match traditional notions of race (African, Asian,\nEuropean, etc.), the programme can equally be set up to report any\narbitrarily selected number of genetically different groups, as the\nuser has to specify the number of clusters they are looking for before\nthe Structure program is applied to any actual dataset. \nTwo interesting aspects of these discussions are that they a) usually\nonly deal with one way of analyzing the biological reality of race\nclassifications (as genetic) and b) adhere to a sharp distinction\nbetween race as biological reality or as social construct. Regarding\na) several philosophers of biology have come up with alternative ways\nof thinking about a biological basis for race (for instance race as\nclades (Andreasen 1998), inbred lines (Kitcher 1999), or ecotypes (Pigliucci &\nKaplan 2003)). This expansion of concepts brought with it the question\nof classificatory monism vs. pluralism, i.e., the question whether\nthere is one privileged way of classifying race that somehow captures\nthe ‘true nature’ of races (natural kinds) or whether\nthere are several ways of doing so, depending on theoretical or\npractical interests/context (Gannett 2010). As Gannet argues, however,\nthis focus on the monism/pluralism debate and on natural kinds comes\nat a cost, as it can mean that questions of practical significance are\nsystematically ignored (2010). Regarding b), Gannett points out that\ndrawing a sharp distinction between race as social construct or\nbiological reality has not only been proven meaningless by recent work\nin population genetics but can also mean that the much messier reality\nof human history and diversity on this planet (and the complex\ninteractions between scientific and social concepts of race) is being\noverlooked, leading to an impoverished analysis of the problems at\nhand (Gannett 2010). \nMetagenomics (also referred to as ‘environmental’ or\n‘community’ genomics) is a research field that aims to\nanalyse the collective genomes of microbial communities. These\ncommunities are usually extracted from environmental samples, ranging\nfrom soil to water or even air samples. A major advantage of\nmetagenomics is that it does not rely on techniques for culturing\nmicrobes. This is important because only an estimated 1%–5% of\nall microbes can be cultured at all (Amann et al. 1995), an issue that\nhas been referred to as the ‘great plate count anomaly’\n(Staley & Konopka\n 1985).[23] \nThe term ‘metagenomics’ was first coined in 1998\n(Handelsman et al. 1998). The prefix ‘meta’ in\n‘metagenomics’ can be read in at least three different\nways (O’Malley 2013): 1) As referring to the fact that\nmetagenomics transcends culturing limitations. 2) As\nemphasising the aggregate-level approach to biology that\ncharacterises metagenomics (looking beyond single entities (cells or\ngenomes)). And 3) as referring to the goal of creating an overarching\nunderstanding of the genomic diversity of the microbial realm. \nThe methodology of metagenomics can be described as a four step\nprocess, consisting of: 1) the collection of environmental samples, 2)\nthe isolation of microbial DNA from these samples, 3a) the direct\nanalysis of the DNA or 3b) the creation of a genomic DNA library by\nfragmentation and insertion of the sampled DNA into suitable vectors\n(for instance plasmids that can be propagated in laboratory bacterial\nstrains). These genomic libraries can then be used to 4a) sequence or\n4b) perform a functional screen of the sampled genomic DNA. As the\ndistinction between steps 4a) and 4b) already implies, metagenomics\ncan be divided into a sequence- and a function-based approach (Gabor\n2007; Sleator et al. 2008). In the former the collected DNA is\nsequenced so that potential genes present in the sample can be\nidentified and, if feasible, the genomes of all the microbes that were\npresent in the sample can be reconstituted. \nThe sequence-based approach is feasible due to the vastly reduced\ncosts of sequencing and the increased computing power available. The\ngoal of the approach is to get an idea of the diversity and\ndistribution of microbes present in the sample and to also get an\ninsight into their functioning (for instance by identifying\nmetabolism-related enzymes that can give clues about the metabolic\npathways active in the different microbes). This can give insights\ninto the workings of the microbial ecosystem present in the sampled\nenvironment more generally. \nIn the functional approach the fragments of DNA that are stored in the\nlibrary are used in what is often called a ‘functional\nscreen’. To perform such a screen the researchers introduce the\nlibrary plasmids into specific bacterial strains which then read and\nexpress any protein-coding sequence that might be present on the\nfragments, thereby producing the protein(s) the fragment codes\n for.[24]\n The key to a functional screen is to create conditions in which only\nthose bacteria that express a protein with the function of\ninterest can be singled out (for instance by making sure that\nonly those cells survive). Once the cells are singled out the library\nplasmid they contain can be recovered and sequenced allowing the\nresearcher to identify the protein(s) encoded by that fragment.\nFunctional metagenomics is often used to identify novel microbial\nproteins that can be used in biotechnological and pharmaceutical\ncontexts and it is not surprising that metagenomics was and still is\nof great interest to the biotechnological sector (Streit & Schmitz\n2004; Lorenz & Eck 2005; Culligan et al. 2014; Ekkers et al.\n2012). \nOne of the first actual (sequence-based) metagenomics projects was\nperformed (yet again) by one of the pioneers of genomics, Craig\nVenter. The goal of Venter and his team was to sample microbes from\nthe surface of the nutrient-poor Sargasso sea (Venter et al. 2004).\nThis particular environment was chosen for this pilot study because it\nwas expected to have a microbial community with relatively low\ndiversity. This assumption turned out to be wrong and the project\nidentified more than a million putative protein-coding sequences\nderived from at least 1800 different genomic species extracted from\nthe sea water. \nAnother early metagenomics study consisted of the analysis of an\nacidophilic biofilm with low microbial diversity from an acid mine\ndrain in California (Tyson et al. 2004). The analysed biofilm survives\nin one of the most extreme environments including a very low pH (i.e.,\nhigh acidity), relatively high temperature and high concentration of\nmetals. Importantly, this specific biofilm truly displays low\ncomplexity as it is composed of only three bacterial and two archaeal\nspecies. This simplicity greatly aided the analysis effort and allowed\nthe researchers an almost complete recovery of two of the genomes and\na partial recovery of the other three. \nThere have been many other metagenomics studies conducted since and\nthere is little point in listing them here, as the list is growing by\nthe month. One aspect of the ongoing research that is important to\npoint out, however, is that the projects are becoming increasingly\nambitious. The trend now is not just to have an integrated view on the\ngenomes but to combine metagenomics with other techniques such as\nmetabolomics (the assay of small molecules present in a system),\nmetatranscriptomics (the analysis of all RNA transcripts of a\ncommunity of microbes) and viromics (the analysis of all the viral\ngenomes present in the system of interest) (see Turnbaugh & Gordon\n2008; Bikel et al. 2015). In a sense the field is moving towards a\nhighly integrated meta-Metagenomics approach (Dupré &\nO’Malley (2007) talk of “metaorganismal\nmetagenomics”). This is also in line with the general trend\ntowards big-data and discovery-based approaches in the life sciences\n(Ankeny & Leonelli 2015; Dolinski & Troyanskaya 2015; Leonelli\n2014, 2016). \nThe rise of metagenomics is also linked to other changes in biological\nsciences more generally, especially the rise of systems biology\nstarting around the year 2000 (which is itself closely linked to the\ndevelopment of genomics since the 1990s). O’Malley and\nDupré (2005) point out that there is an important distinction\nto be made when looking at fields like systems biology, because there\nis not only a change in epistemology but also one in ontology. They\ntherefore distinguish between pragmatic and systems-theoretic\nbiologists. For the former, the idea of a ‘system’ is\nmerely an epistemic tool. For the latter, the system becomes the new\nfundamental ontological unit. Doolittle and Zhaxybayeva (2010) claim\nthat the same can be seen in metagenomics where there is a drive to\nsee the community or the ecosystem as the new fundamental unit, and\nnot the single species (see also Dupré & O’Malley\n2007). \nMoving away from a focus on single organisms or monogenomic species\nallows us to make better sense of many recent findings in microbiology\n(in which metagenomics has played a key role). Central to all of this\nare mobile DNA elements that can travel horizontally, meaning between\ndifferent members of a community (including between different kinds of\norganisms). Obtaining such mobile DNA elements can have a crucial\neffect on the survival and reproduction capacity of the recipient\ncell. Mobile DNA can therefore be a key element in the evolutionary\nprocesses as it becomes a ‘communal resource’ (McFall-Ngai\net al. 2013). Acquired antibiotic resistance is only one of many\nbenefits cells are known to obtain through acquired DNA elements. \nIt is then the composition of functional elements that the community\nas a whole contains which is preserved over evolutionary time. And the\ncommunity could be seen as an assembly of biochemical activities and\nnot of distinct microbial lineages (see for instance Turnbaugh et al.\n2009 and also Burke et al. 2011). The metagenome then becomes a\n‘genome of communities’ and not a ‘community of\ngenomes’ (Doolittle & Zhaxybayeva 2010). All of this also\nfeeds into the more general, and currently very active, discussion\nabout the problem of individuality in biology (Clarke 2010; Bouchard\n& Huneman 2013; Ereshefsky & Pedroso 2013; Guay & Pradeu\n2015; SEP entry on\n the biological notion of individual). \nApart from these issues in biological ontology, there are also\nepistemological issues raised by metagenomics, namely the discrepancy\nbetween our ability to sequence DNA and to interpret it. These\ndiscussions about the challenges of DNA sequence interpretation are\nnot just a problem for (meta)genomics and other -omics approaches, but\nalso for biomedicine more generally and its push towards a truly\npersonalised medicine. A key issue for this push is the discrepancy\nbetween the (ever-decreasing) costs of obtaining a personal genome\nsequence (Bennett et al. 2005; Mardis 2006; Check 2014a,b) and the\nhigh costs of making sure the data can be appropriately interpreted\n(Mardis 2006; Sboner et al. 2011; Phillips et al. 2015). This problem\nis related to the so-called ‘bioinformatic bottleneck’,\nthe handling and the interpretation of the large amounts of sequence\ndata that provides the main obstacle to progress (Green et al. 2011;\nDesai et al. 2012; Scholz et al. 2012; Marx 2013). In the days of\nnext-generation sequencing the sequencing step itself is no longer the\nrate-limiting step. \nGenomics is now an integral part of all of the life sciences. Not that\nevery life scientist is now a genomicist—there are still\nresearchers who focus on the biochemistry, development, or the\nmolecular networks of human cells and other organisms. But the DNA\nsequences of the human genome and the numerous model organisms that\ncame out of the HGP enter every laboratory, if not on a daily basis\nthan at least at some stage of every research project. The same\napplies to the maps of genetic variation that were discussed\nin Section 3 and to the (somewhat controversial)\ndata on functional DNA elements that the ENCODE project generated (see\nthe supplementary document\n The ENCODE Project and the ENCODE Controversy). \nAnd it is not just the quantity of data and the many new\n“-omes” that researchers now work with that have\ntransformed the science. As we have pointed out in several places,\ninsights into the genome and its functioning have transformed\nresearchers’ understanding of the entities and processes they\nare working with in the course of the last few decades. Part of this\nwas also a transformation in our understanding of what it means to do\n‘good’ science. What the HPG and its various offshoots\nhave achieved, therefore, is to change the life sciences at the\nepistemological, the ontological and also the methodological\nlevel. \nAs so often, an interesting and even pressing question is where all of\nthis is going. Predicting the future might not be possible, but there\nare trends that can be identified and which can be expected to follow\na similar trajectory in the near future. One such trend is the drive\nfor big data. ‘Big’ here refers not only to the quantity\nbut also to the different types of data collected. A derivative of\nthis big-data drive is the goal to integrate all of the diverse data\nand mould it into models that can further our understanding of\nbiological systems and the prediction of their behaviour. The\nrelatively young discipline of systems biology, which could not be\ndiscussed in detail in this entry, will certainly play a key role in\nthis endeavour.","contact.mail":"s.m.guettinger@lse.ac.uk","contact.domain":"lse.ac.uk"},{"date.published":"2016-10-20","url":"https://plato.stanford.edu/entries/genomics/","author1":"Stephan Guttinger","author2":"John Dupré","entry":"genomics","body.text":"\n\n\nAbout 30 years ago researchers and other stakeholders started setting\nup the first genomics initiative, the Human Genome Project (HGP) (see \nthe link to All About the Human Genome Project (HGP) in the\n Other Internet Resources section below). \nWhat was conceived as an audacious plan in the 1980s turned\ninto an official multi-centre, international program in 1990 and was\nbrought to a conclusion in 2003.\n\n\nMore than a decade later genomics is still big in business (and big\nbusiness): the Obama administration announced in January 2015 that\nthey intend to sequence one million human genomes (see Precision\nMedicine Initiative in the \n Other Internet Resources section\n below; see also Reardon 2015). Craig Venter, the commercially minded\nnemesis of the publicly-funded HGP is also in the mix again, this time\ninvolved in a privately-funded collaboration that aims to sequence two\nmillion genomes over the course of the next ten years (Ledford 2016).\nAnd equally important, we see not only the same players clash again\nbut also the same promises being made, with talk of\n“groundbreaking health benefits” and “new medical\nbreakthroughs” appearing once again in press releases and other\nannouncements (see for instance Collins & Varmus 2015 or NIH\n2015).\n\n\nBut many things are also different now. For instance, China has\nemerged as a major player in the genomics field, with the BGI\n(formerly the Beijing Genomics Institute) already announcing in 2011\nthe aim to sequence one million genomes. Moreover, DNA sequencing is\nno longer the only goal of these large-scale initiatives: the new\ngenomics is of course still a genome-based effort, but it is a\ntransformed enterprise that also focuses on data about proteins, DNA\n methylation[1]\n patterns or the physiology and the environment of the people studied;\nDNA sequence data now forms only part of a much larger picture in the\npush for what is called ‘precision’ or\n‘personalised’ medicine. Developments such as these have\nled many to refer to the present as a ‘postgenomic’ age\n(Richardson & Stevens 2015). The goal of this entry is to look at\nthis constantly developing space of genomic and postgenomic research\nand outline some of the central philosophical issues it raises.\n\n\n Section 1\n will introduce and discuss several key terms, such as\n‘genome’ or ‘genomics’.\n Section 2\n will then turn to the question of what it means to read and interpret\nthe genome. What did the sequencing and the mapping of the human\ngenome entail and what philosophical issues arose in the context of\nthe human genome project? How did sequencing evolve into a much larger\n‘postgenomic’ enterprise and what issues did this\ntransformation bring about? To answer the last question\n Section 3\n will consider two different projects, perhaps newly emerging fields,\nnamely the HapMap project, and metagenomics. In the supplementary\ndocument\n The ENCODE Project and the ENCODE Controversy,\n we will look at the ENCODE project and the controversy that\nsurrounded it. These three cases will highlight key issues that come\nup again and again in the context of genomics and postgenomics.\n\n\nIt is also important to point out what this article is not\nabout. There are already a number of entries in the Stanford\nEncyclopedia of Philosophy (SEP) that deal specifically with\ngenes, genetics and also the HGP, and the present entry will not,\ntherefore, address in much detail the history of, or the philosophical\nissues surrounding, the concept of the ‘gene’ (see SEP\nentry\n gene,\n but also the entries\n molecular biology,\n molecular genetics, and\n the human genome project),\n or the history of the HGP (see SEP entry,\n the human genome project).\n Broader issues that also play a role in genomics, such as the notion\nof biological information and the issue of reductionism have also been\ndiscussed in a set of SEP entries (for more on reductionism see\n reductionism in biology;\n gene;\n HGP;\n and\n molecular genetics\n and for more on the metaphor of a ‘genetic program’ and\nbiological information see entries on\n biological information;\n gene;\n molecular genetics;\n and\n molecular biology).\n Furthermore, and probably most importantly of all, our focus here\nwill be on the epistemological, ontological and methodological issues\nraised by genomics rather than the ethical, legal and social issues\nthat the sequencing of DNA inevitably brings up (but see\n HGP\n entry for more on these topics).\n\n\nThe term ‘genomics’ derives from the term\n‘genome’, which itself derives (in part) from the term\n‘gene’. The meaning(s) of—and the relationships\nbetween—these different terms is by no means simple. \nThe term ‘gene’ was introduced in 1909 by the Danish\nbiologist Wilhelm Johannsen, who used it to refer to the (then\nuncharacterised) elements that specify the inherited characteristics\nof an organism (see\n gene\n and\n molecular genetics\n entries for an overview of the complex history of the term\n‘gene’). \nThe term ‘genome’ was introduced in 1920 by the German\nbotanist Hans Winkler (1877–1945) in his publication\n“Verbreitung und Ursache der Parthenogenesis im Pflanzen- und\nTierreiche” (Prevalence and Cause of Parthenogenesis in the\nPlant and Animal Kingdom). Winkler defined the term as follows: \nIch schlage vor, für den haploiden Chromosomensatz, der im Verein\nmit dem zugehörigen Protoplasma die materielle Grundlage der\nsystematischen Einheit darstellt, den Ausdruck: das Genom zu verwenden\n[…]. (Winkler 1920: 165) \nI propose to use the expression ‘genome’ for the haploid\nset of chromosomes that, in conjunction with the associated\nprotoplasm, represents the material foundation of the systematic unit\n[often translated as ‘species’]. (Translation by S.G.) \nThe etymology of the term is not clear but most authors and\nencyclopaedia entries assume that it is a combination of the German\nwords ‘Gen’ and ‘Chromosom’,\nleading to the composite ‘Genom’. In general, the\norigin and the different meanings of the -ome suffix are not entirely\nclear and there are now several accounts that try to bring some\nstructure and/or meaning to the ever flourishing -omes terminology in\ncontemporary life sciences (see, e.g., Lederberg & McCray 2001;\nFields & Johnston 2002; Yadav 2007; Eisen 2012: Baker 2013; for interesting/entertaining\nlists, see -omes and -omics in the \n Other Internet Resources section  below). \nThe term ‘genomics’, finally, was invented in 1986 at a\nmeeting of several scientists who were brainstorming (in a bar) to\ncome up with a name for a new journal that Frank Ruddle (Yale\nUniversity) and Victor McKusick (Johns Hopkins University) were\nsetting up. The aim of this journal was to publish data on the\nsequencing, mapping and comparison of genomes. To capture these\ndifferent activities—and in analogy to the well-established\ndiscipline of genetics—Thomas Roderick (Jackson Laboratory)\nproposed the term ‘genomics’ (Kuska 1998). Unbeknownst to\nthe people involved this was a significant moment in the history of\nthe life sciences, as it is here that the -omics suffix appears for the\nfirst time. \nLooking at the history and the etymology of a term does not, of\ncourse, necessarily tell us a lot about how it is used in the context\nof current science. So what is a genome in today’s life\nsciences? Is it the (haploid) set of chromosomes we find in the\nnucleus of a eukaryotic cell, in line with the original definition by\nWinkler? Or is it the totality of genes we find in an organism or the\ntotality of DNA present in a cell? And if so, which DNA? Most\ndefinitions that are currently in circulation are an intricate mix of\ndifferent ways of approaching the issue. This can be illustrated by\nlooking at the definitions given in several key online resources (for\nmore definitions of the term ‘genome’ see Table 1 in\nKeller 2011). \nThe term is defined on the genome.gov website glossary: \nThe genome is the entire set of genetic instructions found in a cell.\nIn humans, the genome consists of 23 pairs of chromosomes, found in\nthe nucleus, as well as a small chromosome found in the cells’\nmitochondria. Each set of 23 chromosomes contains approximately 3.1\nbillion bases of DNA sequence. (Talking Glossary: genome, in the\nOther Internet Resources) \nAnd this is how the U.S. National Library of Medicine defines it: \nA genome is an organism’s complete set of DNA, including all of\nits genes. Each genome contains all of the information needed to build\nand maintain that organism. In humans, a copy of the entire\ngenome—more than 3 billion DNA base pairs—is contained in\nall cells that have a nucleus. (NIH 2016) \nSimilarly the education portal of the journal Nature: \nA genome is the complete set of genetic information in an organism. It\nprovides all of the information the organism requires to function. In\nliving organisms, the genome is stored in long molecules of DNA called\nchromosomes. (Scitable: genome, in the Other Internet Resources) \nAll of these definitions refer both to information and to instructions\nfor the development and/or functioning of an organism. In the first\ntwo, the genome is also identified with a material entity, in the\nfirst case the chromosomes, in the second a sequence of base pairs.\nNature allows only that the information is “stored\nin” the chromosomes. \nThe combination of these two aspects is highly problematic. The\ndefinition from the U.S. National Library of Medicine implies that\n“all of the information needed to build and maintain that\norganism” is contained in the\n DNA,[2]\n which is certainly false: many environmental factors, not to mention\nfactors in the maternal cytoplasm, are required for the first task,\nand even more obviously (food, light, etc.) for the second. Moreover\nwhen, as is almost always the case, an organism requires symbiotic\npartners for its proper functioning, such a definition will imply that\nthe DNA of these symbionts is part of the genome of the first\norganism, a result that few would\n welcome.[3]\n The Nature definition commits the same error in its second\nsentence. The genome.gov definition appears to identify the\nchromosomes both with a set of instructions and a material entity,\nwhich appears rather problematically to conflate a material object\nwith an abstract entity.  \nThe problem is not hard to see. Attempting to combine aspects of the\nmaterial base of the genome and its informational content, as all\nthese definitions do, inevitably assume some simple relation between\nthese two; but in fact the relationship is extremely complex. Because\nthe informational content of the genome is dependent in multiple ways\non elements that are not, on any account, part of the genome, an\naccount in terms purely of informational content seems a hopeless\nproject. \nOne commonly held view that can be quickly dismissed, is the idea that\nthe genome is just the sum total of an organism’s genes. The\nproblem here is just that even passing over the well-known problems\nwith saying what a gene is (see Barnes & Dupré 2008; SEP\nentry on the\n gene),\n on any tenable account of genes, there is far more to the genome than\ngenes, and only a fraction of the actual DNA contained in the\nchromosomes would be part of the genome, at least in the case of\nhumans and other organisms that have a relatively large amount of\nnon-coding DNA (Barnes & Dupré 2008:\n 76).[4]\n Even if ‘gene’ is interpreted in the widest possible\nsense, including any section of the genome that has some identifiable\nfunction, no one denies that a significant amount of DNA is not\nfunctional. The rest of the DNA would not form part of the genome, an\noutcome that contradicts all definitions of the genome of which we are\naware, and makes nonsense of such familiar concepts as\n‘whole-genome sequencing’, which refers to the analysis of\nall the DNA found in the chromosomes. \nThere are, we suggest, two initially tenable approaches to the\n problem.[5]\n The first, and one that is often implicitly or explicitly assumed to\nbe correct, is to define the genome as the sequence of nucleotides.\nThis may or may not contain extranuclear DNA, as in mitochondria or\nchloroplasts; the genome.gov definition explicitly includes the\nformer. This last question figured largely in debates over the moral\npermissibility of so-called mitochondrial transplants (a designation\nthat speaks volumes, incidentally, about the almost magical importance\nattached to DNA as opposed to the remaining contents of the cell), but\nit is not one of great philosophical significance. The alternative\napproach is to understand the genome strictly as a material object,\npresumably, in most cases, the nuclear chromosomes.  \nThe problem with the first approach is that it is largely motivated by\nthe assumption that the nucleotide sequence is what contains all the\nimportant information in the genome. But in fact it has become\nincreasingly clear that this is not the case, especially as a result\nof the growing understanding of epigenetics. Epigenetics is the study\nof material modifications of the genome that affect what parts of the\ngenome sequence are or are not transcribed into RNA, the first stage\nof the process by which the genome influences the containing organism.\nThe two most well-studied classes of epigenetic modification are\nmethylation, the attachment of a methyl group (-CH3) to one\nof the four nucleotides, cytosine, and various chemical modifications\nof the histone proteins, proteins that form the core structure of the\nchromosomes, and around which the DNA double helix is wrapped\n(Bickmore & van Steensel 2013; Cutter & Hayes 2015). The\nnucleotide sequence, then, provides the (extremely large) set of\npossible transcripts that the genome can produce, but the epigenetic\nstate of the genome determines which transcripts are actually produced\n(Jones 2012). Both features of the genome (qua material\nobject) must be specified, therefore, if we want to understand the\nbiologically relevant behavior of the whole system. \nSo if the motivation for defining the genome in terms of sequence is\nto capture its informational content, the definition fails to serve\nits goal. Indeed, the definition that will come closest to this goal\nis that which identifies the genome as the material object, the set of\nchromosomes (this interpretation of the genome is defended in detail\nin Barnes & Dupré 2008). An implication of this definition\nthat is often taken to be counterintuitive by biologists is that the\ngenome will on this account encompass not only DNA, but the histone\nproteins that are material parts of the chromosomes. But of course the\npoint of the preceding discussion is that the variable chemical states\nof the histones are, in fact, essential bases for some of the\ninformation inherent in the genome. \nThe phenomenon of methylation makes a similar point in a slightly\ndifferent way. The nucleotides that comprise the familiar sequence are\ncytosine, thymine, adenine and guanine). When a methyl group attaches\nto the cytosine molecule the resultant nucleotide is not, strictly\nspeaking, cytosine, but 5-methyl cytosine. So unless one takes the\nletter ‘C’ in the standard representation of sequence to\nmean, rather counterintuitively, “cytosine or 5-methyl\ncytosine”, it is only a partially accurate representation of the\nfeature of the genome it purports to represent. More importantly, it\nis a representation that fails to capture crucial functional aspects\nof the genome. \nA final telling point is that it has recently become clear that there\nare functions of the genome, as material object, that go well beyond\neven the broadest interpretation of the genetic (Bustin & Misteli\n2016). It appears that the genome plays an essential role in a range\nof cellular processes. First, its physical arrangement into domains of\nvarying sizes plays a central role in the coordination of gene\nexpression. But much further from the genetic, it is a large object\nthe mechanical forces of which are involved in various cellular\nprocesses and cellular homeostasis, and the chromatin fiber provides a\nscaffolding for both proteins and membranes (Bustin & Misteli\n2016). Unless we are to introduce a new word to refer to this\nbiologically vital entity, only a material conception of the genome\ncan capture the full range of its activities. \nOne might be tempted to object to the argument above concerning\nmethylation, that whereas methylation is a somewhat transitory state,\nthe underlying four-letter sequence is extremely durable, lasting\nacross many generations. Richard Dawkins (1976) famously emphasized\nthe importance of this durability in arguing for the importance of\nthis stability in evolution, even going so far as to describe genes as\n“immortal”. So perhaps there is a good reason for\nunderstanding “C” as referring to a disjunction. \nThis is not the place to address the quasi-theological view of gene\nimmortality. However, this does point to a fundamental issue about the\nnature of genes. Even if genes, somehow, were unchanging immortal\nsubstances, the genome is nothing of the sort. It is an extremely\ndynamic entity, constantly changing its properties in generally\nadaptive response to it environment. Moreover even the constancy of\nits nucleotide sequence is something maintained only by the continuous\napplication of various editing and repair mechanisms. Indeed, far from\nbeing an eternal substance, we suggest it is much better seen as a\nprocess, a highly complex set of dynamic activities crucial in\nmaintaining the structural and functional stability not only of the\norganism but also, through its role in reproduction, of the lineage.\nImportantly, these relations are bi-directional and, specifically, the\norganism is also crucial to maintaining the necessary aspects of\nstability of the\n genome.[6] \nThe first genome to be sequenced was that of a virus, namely\nbacteriophage ΦX174, sequenced by Frederick Sanger in 1977 (Sanger et\nal. 1977). Up to about 1985, work on several other viruses was\ninitiated in different laboratories across the world and even the\nsequencing of model organisms such as the bacterium Escherichia\ncoli or the roundworm Caenorhabditis elegans was being\n tackled.[7] \nOf all the different sequencing efforts at the time the human genome\nproject (HGP) of course stands out. Not only is the human genome\nrelatively large (roughly 3.2 billion base pairs (bps)) and of key\ninterest to us as human beings, but the HGP itself was envisioned as a\ndiverse large-scale research project with various strands and aims.\nGetting the sequence out of this project was the one goal that got the\nmost attention in the wider media, but surely many would agree that\nother findings and practices developed within the HGP were of equal or\neven greater importance. \nIn what follows we will treat the HGP as a pivot around which genomics\ndeveloped as a field of research and as a set of techniques. For ease\nof exposition we will talk here of a pre-HGP and a post-HGP phase.\nObviously, this is a simplification; there is not just one single\ntrajectory along which the story of genomics runs and there is not one\nclear break between a pre- and a post-genome era (Richardson &\nStevens 2015). Nevertheless, as a way of structuring the discussion\nthis distinction will be a helpful tool. \nA decade after Sanger and Maxam and Gilbert published their DNA\nsequencing methods in 1975 the first concrete talk of a human genome\nproject started to appear in writing (Dulbecco 1986) and at different\nworkshops (Sinsheimer 1989; Palca 1986). The Human Genome Project\n(HGP) itself became a reality in 1990 when it was officially launched\nas a US federal program (see 1990 in a brief history and timeline\n[NHGRI] in the \n Other Internet Resources section\n below). \nIn the run-up to the HGP there were high expectations (some would say\n“hype”) developing, which inevitably also brought critics\nof the project onto the scene (Koshland 1989; Luria et al. 1989). As\nso often, the issue of funding had a key role to play. When the HGP\nwas initiated there were no ‘big science’ projects being\npursued in the life sciences. The HGP therefore was a true first for\nbiology. But pushing such a large project that absorbed a significant\nproportion of the funding allocated to the biological sciences\nencountered a lot of resistance from other scientists. \nThere were three key criticisms: 1) Some claimed that the HGP was a\nwaste of money because much useless (read: junk) DNA was sequenced;\nthe focus should be more directly on the functional parts of the\ngenome, i.e., the genes or regulatory elements, which could be\nachieved using simpler and less expensive methods (Brenner 1990;\nWeinberg 1991; Rechsteiner 1991; Lewontin 1992; Rosenberg 1994).\nOthers claimed 2) that the HGP was a waste of money as it was merely a\ndescriptive and not a hypothesis-driven project. This was an issue\nthat became much more prominent ten years after the project was\nfinished, when it became clear that big data science was here to stay\n(see, e.g., Weinberg\n 2010).[8] \nAnd last but not least there was also the critique 3) that the HGP is\nfundamentally misguided as it assumes that by using sequence knowledge\nalone we would be able to develop an understanding of how our body\nworks, how it develops disease, and that this understanding will\neventually lead to cures for many diseases (Lewontin 1992; Tauber\n& Sarkar 1992; Kitcher 1994). This more general critique of a\nnarrowly sequence-focused approach to biomedical issues also comes up\n20 years later in discussions about the use of common genetic variants\nto learn more about common diseases and traits (see\n Section 3.1.2).\n  \nIt is difficult to evaluate criticisms of the last kind. There is no\ndoubt that enthusiasm for the HGP and many other successor projects in\ngenomics has often been grounded in simplistic assumptions about the\npower of DNA and its pre-eminent role in biological systems. On the\nother hand it is arguable that many unanticipated benefits have\nderived from genomics quite independently of such assumptions. For\ninstance the ability to make very precise comparisons of genome\nsequences has led to major advances in unraveling the details of\nevolutionary history, not to mention its application to technologies\nsuch as forensic DNA testing. Moreover, it can be argued with Waters\n(2007b) that what makes\ngenomes so central to biological research is not the erroneous belief\nthat they are the ultimate causes of everything, but rather the unique\npossibilities they present for precise intervention in organisms or\ncells. \nThe main output of the HGP is usually seen as ‘the’ human\ngenome sequence. The draft human genome sequence (about 90% complete)\nwas announced in June 2000, followed in 2001 by the publication of the\ndraft sequences produced by the HGP (International Human Genome\nSequencing Consortium 2001) and the privately funded initiative\n(Venter et al. 2001). The complete (or almost complete (99%)) sequence\nof the human genome was released in 2003, which also marked the\nofficial ending of the HGP (International Human Genome Sequencing\nConsortium 2004). \nBut the view that the sequence of ‘the’ human genome was\nthe key output is wrong in several ways. First of all there is in\ngeneral no such thing as ‘the’ human genome, as each\nindividual (except for monozygotic twins) carries their own set of\nsmall and large variations in their genome (and even for twins there\nare many differences they accumulate in their genomes during their\nlifetime). The sequence that was produced in the HGP is therefore\nnothing more than an example of one particular sequence, meaning it\ncan only serve as a reference genome. Importantly, the reference\nsequences that both the HGP and Venter’s project delivered did\nnot correspond to the genome of a single person as the DNA used to\nproduce them was derived from several\n individuals.[9]\n The genomes that came out of the two sequencing efforts were\ntherefore composite reference sequences. But the HGP also produced\nmuch more than just a DNA sequence. Here we will highlight three\noutcomes or aspects of the HGP that are of particular importance, also\nfor the period that followed the completion of the project. \nOne key feature of the HGP was that it involved the sequencing of a\nrange of different model organisms, an aspect of the HGP that was\noften overlooked in discussions of the project in the philosophical\nliterature and elsewhere (Ankeny 2001; for a searchable list of\nsequenced genomes see genome information by organism in the\n Other Internet Resources section\n below). The HGP provided not only a first reference genome of\nHomo sapiens but also the first bacterial genome\n(Haemophilus influenzae, Fleischmann et al. 1995), the first\neukaryotic genome (Saccharomyces cerevisiae, Goffeau et al.\n1996), and the genomes of key model organisms (Escherichia\nColi, Blattner et al. 1997; Caenorhabditis elegans,\nC. elegans Sequencing Consortium 1998; Arabidopsis\nthaliana, Arabidopsis Genome Initiative 2000; Drosophila\nmelanogaster, Adams et al. 2000, Myers et al.\n 2000).[10] \nA further crucial output was the acceleration in technology\ndevelopment the HGP brought about. It is safe to say that without the\nHGP (and subsequent initiatives such as the Advanced Sequencing\nTechnology Awards created in 2004 by the National Human Genome\nResearch Institute (NHGRI) (NIH 2004)) there wouldn’t have been\nsuch a rapid development in next-generation sequencing (NGS)\napproaches and the cost of whole genome sequencing would not have\ndropped as quickly as it has (see Mardis 2011 for a review of the\ndevelopment of NGS). And these improvements in the sequencing\ntechnology had further consequences, for example allowing scientists\nto sample DNA in different ways and from different sources, as new\nsequencing methods could process more DNA material more quickly and\nwork with less starting material. This, finally, made possible whole\nnew sub-disciplines, such as metagenomics (see\n Section 3.2). \nA final noteworthy output of the HGP is what scientists learned about\nthe structure of the genome. Beginning with the HGP, and building on\nfurther studies, researchers have gained a much more detailed picture\nof the fine structure, the dynamics and the functioning of the human\ngenome. It was not only that there were many fewer genes present than\nexpected, but there was also much more repetitive DNA and transposable\nelements present (it is estimated that about 45% of human DNA consists\nof transposable elements or their inactive remnants). These findings\nrelate to a more general and older discussion about genome size and\ncomplexity to which we next turn. \nIt has been known since the 1950s that genome size varies greatly\nbetween different organisms (Mirsky & Ris 1951; see also Gregory\n2001), but from the very beginning it was also clear that this\ndiversity has some surprising features. One of these features is the\nabsence of correlation between the complexity of an organism and the\nsize of its genome. \nAssuming an informational account of the genome one would expect that\nthe more complex an organism is, the more DNA its genome should\ncontain (this is in fact what many biologists assumed at least until\nabout the 1960s). How to define and assess the complexity of an\norganism is a tricky issue, but intuitively it seems reasonable to\nassume that a single-celled amoeba is less complex than an onion,\nwhich in turn is less complex than a large metazoan such as a human\nbeing, both in terms of the complexity of the workings and the\nstructure of the organism. The expectation was that the DNA content of\nhuman cells should be much larger than that of onions or amoebae. As\nit turns out, however, both the onion and the amoeba have much larger\ngenomes than human beings. The onion, for instance, has a genome of\nabout 16 billion base pairs, meaning it is about five times the size\nof the human genome (Gregory 2007). The same lack of\ncorrelation between genome size and complexity can be found in many\nother instances (for an overview of different genome sizes see the\nanimal genome size database in the\n Other Internet Resources section\n below). \nIt was also found early on that very similar species in the same genus\nshow large variation in genome size, despite having similar phenotypes\nand karyotypes (i.e., number and shape of chromosomes in a genome).\nWithin the family of buttercups, for instance, DNA content varied up\nto 80-fold (Rothfels et al. 1966). Also, Holm-Hansen (1969) showed\nthat species of unicellular algae display a 2000-fold difference in\nDNA content despite all being of similar developmental complexity. It\nwas findings such as these that gave a real urgency to addressing this\ndiscrepancy that was now labelled the C-value paradox (Thomas 1971).\nThe term ‘C-value’ refers to the constant\n(‘C’) amount (‘value’) of haploid DNA per\nnucleus and is measured in picograms of DNA per nucleus. The C-value\nis a measure of the amount of DNA each genome contains (we can see\nhere Winkler’s original definition of the genome at work). \nThese discussions of genome sizes were closely related to concerns\nabout gene numbers. And this consideration of genome size vs. gene\nnumbers is what originally gave rise to the concept of ‘junk\nDNA’ (Ohno\n 1972).[11]\n The reasoning behind this concept was the following: if one assumes\na) that more complex organisms will have more DNA than less complex\norganisms and b) that gene numbers increase in proportion with genome\nsize, then the genome of the more complex organism should have more\ngenes than the less complex\n one.[12]\n Human cells, for instance, contain about 750x more DNA than E.\ncoli, meaning that they should turn out to have in the range of\n3.7 million genes, as E. coli has about 5000 genes. This is\nclearly not the case; even in the 1970s it was generally supposed that\nthe human genome might contain no more than 150,000 genes (Crollius et\nal. 2000). This discrepancy leads to the conclusion that the vast\nmajority of the DNA in our genome cannot be genes and is therefore\nwhat Ohno referred to as\n ‘junk’.[13] \nThe problem that the junk DNA discussion brings up has also been\nreferred to as the ‘G-value paradox’ (‘G’\nstands for ‘gene’), which directly concerns the\ndiscrepancy between the number of genes in an organism and its\ncomplexity (Hahn & Wray 2002). This paradox has been reinforced by\nthe findings of the HGP. As Gregory (2005) and other commentators have\npointed out, the finding that the human genome contains many fewer\ngenes than expected was one of the most surprising outcomes of the\nHGP. Initial estimates from before the project were in the range of\n50,000 to 150,000. These were reduced to about 30,000—35,000\nafter the publication of the first sequence draft in 2001 and have now\nbeen further revised to the order of 20,000 (Gregory 2001). \nSome researchers assumed that the C-value paradox was fully resolved\nby the recognition that there is non-coding DNA in genomes (Gregory\n2001). Larger genome size in ‘simpler’ organisms merely\nmeans that they have large quantities of non-coding DNA. But as\nGregory points out, the fact that the majority of DNA in our genomes\nis non-coding might make the C-value discrepancies less of a paradox,\nbut it gives rise to a whole range of further puzzles (Where does this\nextra DNA come from? What is its function? Etc.), which is why he\nproposes to talk of the C-value as an enigma rather than a\nparadox (Gregory 2001). The C-value enigma consists of many different\nand layered problems and these require a pluralistic approach to\nanswering them, or so Gregory claims. \nThe publication of the draft genome sequence in 2001 and the\nconclusion of the HGP in 2003 did not give researchers all the tools\nand insights they needed to tackle these long-standing problems. But\nafter the HGP, building on the initial sequencing effort, researchers\ncould start to go beyond the mere sequence and gain a deeper\nunderstanding of the workings of the genome. This put them in a\nposition to tackle issues such as the significance of junk DNA and the\nC-value paradox more directly (or at least from a different angle).\nThe post-HGP phase is also characterized by an intense debate about\nthe best way of doing research: the question of whether biological\nresearch should best be done on a small or a large scale has come up\nagain and again in the post-HGP era, especially with the rise of other\npost-HGP large scale projects. The next section will address two\nprojects/research fields that symbolize the various efforts and\naspirations that were characteristic of the post-HGP era and which\nwill help to illuminate some of the philosophical issues these\ndevelopments raised.  \nThe post-HGP phase is marked by a flourishing of different projects,\nclosely connected in their origins to the HGP, but going beyond it in\nmany different ways. This section discusses two such post-HGP\nprojects, namely the International HapMap project and a new field of\nresearch called ‘metagenomics’.  These examples indicate\nsome important directions in which the postgenomic era is heading and\nidentify some, though certainly not all, of the key characteristics\nand issues that mark this new period. \nThe International HapMap project was a multi-centre project launched\nin 2002 that came to an initial conclusion in 2005 (NIH \n 2002).[14]\nThe acronym ‘HapMap’ stands for ‘haplotype map’\nand (indirectly) refers to the main goal of the project, namely to map\nthe common genetic variation in the human genome. \nIt is a well-known fact that everyone’s genome is different.\nThere are, however, several ways in which genomes of individuals can\nvary from each other, ranging from the deletion, insertion or\nrearrangement of longer stretches of DNA to differences in single\nnucleotides at specific locations on a chromosome. The latter form of\nvariation was the focus of the HapMap project. If we align the DNA\nsequence of two individuals they will be identical for hundreds of\nnucleotides; the DNA of two human beings typically displays about\n99.9% sequence identity (Li & Sadler 1991; Wang et al. 1998;\nCargill et al. 1999). But the 0.1% difference means that approximately\nevery 1000 nucleotides there will be a difference in a single\nnucleotide between any two individuals. \nAny variation at a specific genomic locus is referred to as an\n‘allele’. If there are two different versions of a\nspecific gene that can be found in a population at a specific locus on\na chromosome, then that means that there are two different alleles of\nthat gene present in that\n population.[15]\n If one of these single nucleotide alleles is found in more than 1% of\na specific population it is treated as a ‘common’ variant\nand researchers speak of a ‘polymorphism’ or, more\nprecisely, a ‘single nucleotide polymorphism’ (abbreviated\n‘SNP’; pronounced ‘snip’). If a variation is\nfound in less than 1% of the population researchers simply call it a\n‘mutation’ (or also a ‘point\n mutation’).[16]\n On average there are about 3 million SNPs found in each individual\nand there is a pool of more than 10 million SNPs present in the human\npopulation as a whole (HapMap 2005). \nMany of these alleles are (or have an increased likelihood of being)\ninherited together, meaning that they do not easily become separated\nthrough recombination events during\n meiosis.[17]\n This leads to the non-random association of different alleles at two\nor more loci, a phenomenon that has been dubbed ‘linkage\ndisequilibrium’ or ‘LD’. The concept of LD is key\nfor the HapMap project as the fact that some SNPs stay associated\n(whereas the clusters themselves might get separated from each other\nover time by recombination events) explains the haplotype structure of\nthe genome (Daly et al. 2001). The term ‘haplotype’ simply\nrefers to a particular cluster of alleles (in this case SNPs) that a)\nare on the same chromosomes and b) are commonly inherited as one. The\naim of the HapMap project was to characterize human SNPs, their\nfrequency in different populations and the correlations between them\n(HapMap 2003). The first haplotype map was published in 2005,\nreporting on data from 269 samples derived from four different\npopulations (HapMap 2005). Five years later, a follow up was\npublished, now reporting on data from 1184 individuals sampled from 11\ndifferent populations (HapMap 2010). \nThe realization that the structure of genetic variation in the genome\ncan be understood in terms of haplotypes was important for at least\ntwo reasons. First it opened the door for a relatively easy and\nefficient analysis of (single nucleotide) genetic variation in\npopulations: the clustering of SNPs meant that in principle only one\nor a few of the SNPs in each cluster (so-called ‘tag\nSNPs’) would have to be tested to verify the presence of the\ncluster of variants as a whole. This made the analysis of genetic\nvariation at the level of whole genomes from a large number of\nsubjects feasible at a time when whole-genome sequencing was still too\nexpensive for such a task (HapMap 2003). The development of a\nhaplotype map was therefore a crucial step to enable what are now\ncalled ‘genome-wide association studies’ (GWAS) (see\n Section 3.1.2). \nSecondly, as the distribution of haplotypes varies between different\npopulations, the HapMap project had a strong focus on sampling DNA\nfrom different populations. This is an important aspect of this type\nof research as it brought, unwittingly perhaps, the issue of race and\nthe question of its biological basis right back into genomics. This\npoint will be revisited in\n Section 3.1.4. \nA key point driving the HapMap project was the fact that SNPs can be\nused to uncover connections between an individual’s DNA sequence\nand specific conditions or traits. At face value an SNP is simply a\ndistinguishing mark in the genome of a person. Such marks allow\nresearchers to screen groups of a population with different\nphenotypes, for instance those with a condition (e.g., high blood\npressure) and those without. Looking at the frequency of specific SNPs\nor haplotypes in either group the researchers can use statistical\nanalysis to get insight into the association between a particular SNP\nor haplotype and a trait (Cardon & Bell 2001). As mentioned above,\nthis analysis can be focused on tag SNPs that are treated as proxies\nfor a whole cluster of SNPs (if the cluster has a high LD). \nOnce a haplotype has been associated with a particular condition,\nother people can be screened for the presence of that haplotype and\ntherefore gain some understanding of the risk groups they belong to.\nAlthough the test will not tell carriers of disease-linked SNPs\nwhether they will develop the condition or not, it can nevertheless\ngive them some information about their chances. Furthermore, even\nthough the tag SNP itself might not be the genetic variation that\ncauses or contributes to the variation in phenotype, it might be\nlinked to so-called ‘causal SNPs’. Learning about SNPs\nassociated with a condition or trait therefore can give the researcher\nclues as to which genes or regulatory DNA regions might be causally\ninvolved in the development of that condition. Findings from\nassociation studies can therefore in some cases contribute to the\nanalysis of the condition itself. \nThe HapMap initially only looked for common variants (SNPs include by\ndefinition only common variants). This was in line with the so-called\ncommon disease/common variant (CD/CV) hypothesis formulated by Lander\n(1996); Cargill et al. (1999), and Chakravarti\n (1999).[18]\n This hypothesis postulates, roughly, that common conditions are\nlinked to genetic variations that are common in a population. \nThis link between common variants and common diseases also explains\nwhy the HapMap project could be promoted from the very beginning as\nthe ‘next big thing’ after the sequence of the human\ngenome had been determined: it was with the haplotype map that\ngenomics should really start to have an impact on biomedical research\nand ultimately our understanding of\n disease.[19] \nBut the HapMap project was not without its critics; indeed the\nbiologist David Botstein called it a “magnificent failure”\n(cited in Hall\n 2010).[20]\n Some commentators, for instance, were worried that the project is\nnothing more than a make-work project filling a gap that the finished\nHGP left behind, and therefore a waste of precious funds (Couzin\n2002). But more often, criticism of the HapMap project was part of\nwider debates about the way post-HGP research should be conducted. The\nHapMap project can therefore provide a useful window on some of the\nkey tendencies and disputes that marked (or marred) the post-HGP\nera. \nOne such indirect criticism of the HapMap derives from the apparent\nfailure of GWAS to lead researchers to clearer information about the\nlinks between our genetic makeup and the different conditions to which\nour bodies can succumb. In the eyes of these critics the CD/CV\nhypothesis was the key problem, as the common variants simply do not\nexplain much of the heritability of common diseases. This observation\ngave rise to the concept of ‘missing heritability’\n(Eichler et al. 2010). \nThe general focus on common variants in genomics was criticized by\nother authors who claimed that the focus of geneticists should rather\nbe on rare variants (McClellan & King 2010). These rare variants,\nthey claim, are where the missing heritability will be found. The\nproblem with the rare variants is that they cannot be picked up in\nGWAS that use SNP databases, as SNPs are by definition common\nvariants. Also, finding rare variants is a technical challenge as\nresearchers have to analyse the genomic data of a very large number of\nindividuals to do so reliably. This hunt for rare variants is a major\nreason behind the current push for the sequencing of millions (rather\nthan a couple of hundreds or thousands) of genomes. As discussed\nearlier, such large-scale approaches have become feasible in recent\nyears due to the reduced cost and increased speed of next-generation\nDNA sequencing. \nThe current shift to whole-genome sequencing will also help to address\nanother critique of the GWAS/SNP/HapMap approach, namely its strict\nfocus on single base pair changes in the genome. Other changes in the\ngenome, such as variations in the numbers of copies of repeated\nelements or rearrangements, deletions or insertions of larger chunks\nof genomic DNA, might in many cases be what is at the core of a\ndisorder, necessitating (again) a shift in focus away from point\nmutations and single genes to the genome as a whole (Lupski 1998,\n2009). \nAs one of the first follow-ups to the original HGP, the HapMap project\nwas a topic that often came up in discussions of the legacy of the\nHGP. Such discussions became especially prominent at the tenth\nanniversary of the publication of the draft genome sequence. In\ngeneral, there was an overwhelming sense of disappointment at what had\ncome out of the HPG, at least in the medical context. Given the grand\npromises that were made both around the start of the project in the\n1980s and then again in the year 2000 at the presentation at the White\n House,[21]\n it is not surprising that people were unimpressed by what had been\ndelivered by 2010/2011. Interestingly, it was not only the usual\nsuspects, such as Lewontin (2011), but also key proponents of the HGP itself who were\ncritical and pointed out the minimal medical advances that had been\nachieved in the first post-HGP decade (Collins 2010; Venter 2010). \nHowever, one thing that all critics, including the above-mentioned,\nagreed on was that even though its effect on medical practice had been\nnegligible, the HGP had transformed biological research (see for\ninstance Wade 2010; Varmus 2010; Hall 2010; Butler 2010; Green et al.\n2011). One area in which genomic research had fundamentally changed\nboth concepts and practices was in the understanding of what a gene is\nand how gene expression works and is regulated (Keller 2000; Moss\n2003; Dupré 2005; Griffiths & Stotz 2006; Stotz et al.\n2006; Check 2010). With great foresight, Evelyn Fox Keller pointed out\nalready in 2000 that the HGP was interesting not so much because of\nthe raw sequence it produced, but more because of the transformations\nit brought about in our expectations when it comes to\n‘genes’ and DNA (Keller 2000). \nAs mentioned above, HapMap’s use of samples from different\npopulations brought the concept of race into discussions of the\nproject. Studies that looked into the genetic variation between\npopulation groups (of which the HapMap was a key representative) are\namong several recent developments (Duster 2015) that reignited\ndiscussion about a) the biological reality of race and b) the question\nwhether racial classifications should be used in biomedical research\nat all. Several authors have picked up the relation between the HapMap\nproject and a renewed concern with race (see, e.g., Ossorio 2005;\nDuster 2005; Hamilton 2008). The question that dominates these\ndiscussions is whether racial classifications reflect a\n‘biological reality’. \nRace has of course been an important topic in epidemiology and\nclinical research for a long time (Witzig 1996; Stolley 1999), but it\nhas been widely perceived as a socially constructed category that has\nno biological\n basis.[22]\n And many researchers imagined that as the HGP demonstrated how highly\nsimilar any two human beings are to each other at the DNA level, any\nidea of race as serious biological concept would be disposed of once\nand for all (see, e.g., Gilbert 1992; Venter 2000). But the concept of\nbiological race was if anything rejuvenated rather than laid to rest\nby the developments in genomics (Kaufman & Cooper 2001; Foster\n& Sharp 2002; Hamilton 2008; Roberts 2011). This is exemplified by\nthe fact that more and more scientists have claimed in recent years\nthat there is a biological basis to our traditional notions\nof race, basing their claims on elaborate statistical analyses of data\non genetic variation derived from a large number of human DNA samples.\nThese developments led for many to what Troy Duster has called a\n‘post-genomic surprise’ (Duster 2015).  \nAn important point here is that linking genomics and race does not\nmean that researchers search for, or even that there are, any\n‘genes for race’, even if we consider the many different\nways in which this term can be interpreted (Dupré 2008). The discussion about the\npossible genetic basis for race is now more subtle, as it is not\nsimply concerned with the presence or absence of specific genes or DNA\nelements and hence some sort of biological essence of races, but\nrather with the variation in the frequencies of alleles in the\npopulation of interest (Gannett 2001, 2004). The question is therefore\nnot whether DNA element X is absent or present in one\npopulation or the other, but rather which variant of X is\npresent at what frequency in a population (in the context of the\nHapMap researchers will talk of SNP frequencies). \nData from population genetics shows that the global distribution of\nallele frequencies in the human population is not discontinuous (Jorde\n& Wooding 2004; Feldman & Lewontin 2008) but clinal, meaning\nthat human DNA sequences vary in a gradual manner over geographic\nspace (Livingstone 1962; Serre & Pääbo 2004; Barbujani\n& Colonna 2010). Moreover, both genetic and phenotypic traits\ndisplay what is called ‘nonconcordant’ clinal variation,\nmeaning that different traits do not necessarily co-vary with each\nother; the pattern of how trait A varies across geographic space might\nbe very different from the pattern displayed by trait B (Livingstone\n1962; Goodman 2000; Jorde & Wooding 2004). \nBut despite these widely accepted findings, it is in the discussion of\nthese distributions that the idea of a biological basis for our\ntraditional understanding of race classifications has re-emerged.\nBased on the analysis of large sets of genetic variants in samples\nderived from various locations around the globe, a number of\nresearchers have made the claim that human genetic variation displays\ngeographical clustering (see, e.g., Rosenberg et al. 2002; Edwards\n2003; Burchard et al. 2003; Bamshad et al. 2003; Leroi 2005; Tang et\nal. 2005). Importantly, these findings often also gave rise to, or\nwere interpreted to support, the claim that this geographical\ndistribution matches our traditional racial classifications. \nSuch findings also led a number of authors to claim that race still\nhas a valid place in biomedical research: since these classifications\nare supposed to describe groups that are internally genetically\nsimilar, but genetically different from other groups, they can serve\nas useful proxies in estimating, for instance, the group\nmember’s average risk of developing a particular condition (see,\ne.g., Xie et al. 2001; Wood 2001; Risch et al. 2002; Rosenberg et al.\n2002; Shiao et al. 2012). Some authors are more cautious and claim\nthat race should only serve as a loose and temporary proxy (Foster\n& Sharp 2002; Jorde & Wooding 2004) that should be abandoned\nas soon as we know the actual genetic variations that are linked to a\nparticular condition or trait (Jorde & Wooding 2004; Leroi 2005;\nDupré 2008). Such\ncritics may note that the most that these genetic studies show is that\nthere is a correlation between a person’s genetic variants and\ntheir geographical origin, if only because variants originate in a\nspecific place; and there is a loose relation between the socially\nconstructed concept of race and geographic origin. But given the\ntenuous connection that this generates between perceived or\nself-identified racial categories and genetic constitution, race is a\npoor substitute for any actually salient genetic information that may\neventually be related to disease.  \nBut there is also a significant group of researchers who are not\nconvinced by these analyses and who don’t think that there is\nany biological basis to the race concept (see, e.g., Schwartz 2001;\nDuster 2005, 2006; Krieger 2000; Ossorio 2005). All of these authors\ncriticise the above studies and the geographic clusters of genetic\nvariation they identify, mainly because of flaws in the way samples\nare collected (see, e.g., Duster 2015) and how the data is ultimately\nanalysed. The latter criticism has mainly focused on the program\n‘Structure’ that is used by a majority of the studies\nmentioned above to churn out clusters of genetic variation (Bolnick\n2008; Kalinowski, 2011; Fujimura et al. 2014). A telling criticism is\nthat while Structure can be made to report that there are five main\ngeographical clusters that show distinctive allele frequencies and\nwhich roughly match traditional notions of race (African, Asian,\nEuropean, etc.), the programme can equally be set up to report any\narbitrarily selected number of genetically different groups, as the\nuser has to specify the number of clusters they are looking for before\nthe Structure program is applied to any actual dataset. \nTwo interesting aspects of these discussions are that they a) usually\nonly deal with one way of analyzing the biological reality of race\nclassifications (as genetic) and b) adhere to a sharp distinction\nbetween race as biological reality or as social construct. Regarding\na) several philosophers of biology have come up with alternative ways\nof thinking about a biological basis for race (for instance race as\nclades (Andreasen 1998), inbred lines (Kitcher 1999), or ecotypes (Pigliucci &\nKaplan 2003)). This expansion of concepts brought with it the question\nof classificatory monism vs. pluralism, i.e., the question whether\nthere is one privileged way of classifying race that somehow captures\nthe ‘true nature’ of races (natural kinds) or whether\nthere are several ways of doing so, depending on theoretical or\npractical interests/context (Gannett 2010). As Gannet argues, however,\nthis focus on the monism/pluralism debate and on natural kinds comes\nat a cost, as it can mean that questions of practical significance are\nsystematically ignored (2010). Regarding b), Gannett points out that\ndrawing a sharp distinction between race as social construct or\nbiological reality has not only been proven meaningless by recent work\nin population genetics but can also mean that the much messier reality\nof human history and diversity on this planet (and the complex\ninteractions between scientific and social concepts of race) is being\noverlooked, leading to an impoverished analysis of the problems at\nhand (Gannett 2010). \nMetagenomics (also referred to as ‘environmental’ or\n‘community’ genomics) is a research field that aims to\nanalyse the collective genomes of microbial communities. These\ncommunities are usually extracted from environmental samples, ranging\nfrom soil to water or even air samples. A major advantage of\nmetagenomics is that it does not rely on techniques for culturing\nmicrobes. This is important because only an estimated 1%–5% of\nall microbes can be cultured at all (Amann et al. 1995), an issue that\nhas been referred to as the ‘great plate count anomaly’\n(Staley & Konopka\n 1985).[23] \nThe term ‘metagenomics’ was first coined in 1998\n(Handelsman et al. 1998). The prefix ‘meta’ in\n‘metagenomics’ can be read in at least three different\nways (O’Malley 2013): 1) As referring to the fact that\nmetagenomics transcends culturing limitations. 2) As\nemphasising the aggregate-level approach to biology that\ncharacterises metagenomics (looking beyond single entities (cells or\ngenomes)). And 3) as referring to the goal of creating an overarching\nunderstanding of the genomic diversity of the microbial realm. \nThe methodology of metagenomics can be described as a four step\nprocess, consisting of: 1) the collection of environmental samples, 2)\nthe isolation of microbial DNA from these samples, 3a) the direct\nanalysis of the DNA or 3b) the creation of a genomic DNA library by\nfragmentation and insertion of the sampled DNA into suitable vectors\n(for instance plasmids that can be propagated in laboratory bacterial\nstrains). These genomic libraries can then be used to 4a) sequence or\n4b) perform a functional screen of the sampled genomic DNA. As the\ndistinction between steps 4a) and 4b) already implies, metagenomics\ncan be divided into a sequence- and a function-based approach (Gabor\n2007; Sleator et al. 2008). In the former the collected DNA is\nsequenced so that potential genes present in the sample can be\nidentified and, if feasible, the genomes of all the microbes that were\npresent in the sample can be reconstituted. \nThe sequence-based approach is feasible due to the vastly reduced\ncosts of sequencing and the increased computing power available. The\ngoal of the approach is to get an idea of the diversity and\ndistribution of microbes present in the sample and to also get an\ninsight into their functioning (for instance by identifying\nmetabolism-related enzymes that can give clues about the metabolic\npathways active in the different microbes). This can give insights\ninto the workings of the microbial ecosystem present in the sampled\nenvironment more generally. \nIn the functional approach the fragments of DNA that are stored in the\nlibrary are used in what is often called a ‘functional\nscreen’. To perform such a screen the researchers introduce the\nlibrary plasmids into specific bacterial strains which then read and\nexpress any protein-coding sequence that might be present on the\nfragments, thereby producing the protein(s) the fragment codes\n for.[24]\n The key to a functional screen is to create conditions in which only\nthose bacteria that express a protein with the function of\ninterest can be singled out (for instance by making sure that\nonly those cells survive). Once the cells are singled out the library\nplasmid they contain can be recovered and sequenced allowing the\nresearcher to identify the protein(s) encoded by that fragment.\nFunctional metagenomics is often used to identify novel microbial\nproteins that can be used in biotechnological and pharmaceutical\ncontexts and it is not surprising that metagenomics was and still is\nof great interest to the biotechnological sector (Streit & Schmitz\n2004; Lorenz & Eck 2005; Culligan et al. 2014; Ekkers et al.\n2012). \nOne of the first actual (sequence-based) metagenomics projects was\nperformed (yet again) by one of the pioneers of genomics, Craig\nVenter. The goal of Venter and his team was to sample microbes from\nthe surface of the nutrient-poor Sargasso sea (Venter et al. 2004).\nThis particular environment was chosen for this pilot study because it\nwas expected to have a microbial community with relatively low\ndiversity. This assumption turned out to be wrong and the project\nidentified more than a million putative protein-coding sequences\nderived from at least 1800 different genomic species extracted from\nthe sea water. \nAnother early metagenomics study consisted of the analysis of an\nacidophilic biofilm with low microbial diversity from an acid mine\ndrain in California (Tyson et al. 2004). The analysed biofilm survives\nin one of the most extreme environments including a very low pH (i.e.,\nhigh acidity), relatively high temperature and high concentration of\nmetals. Importantly, this specific biofilm truly displays low\ncomplexity as it is composed of only three bacterial and two archaeal\nspecies. This simplicity greatly aided the analysis effort and allowed\nthe researchers an almost complete recovery of two of the genomes and\na partial recovery of the other three. \nThere have been many other metagenomics studies conducted since and\nthere is little point in listing them here, as the list is growing by\nthe month. One aspect of the ongoing research that is important to\npoint out, however, is that the projects are becoming increasingly\nambitious. The trend now is not just to have an integrated view on the\ngenomes but to combine metagenomics with other techniques such as\nmetabolomics (the assay of small molecules present in a system),\nmetatranscriptomics (the analysis of all RNA transcripts of a\ncommunity of microbes) and viromics (the analysis of all the viral\ngenomes present in the system of interest) (see Turnbaugh & Gordon\n2008; Bikel et al. 2015). In a sense the field is moving towards a\nhighly integrated meta-Metagenomics approach (Dupré &\nO’Malley (2007) talk of “metaorganismal\nmetagenomics”). This is also in line with the general trend\ntowards big-data and discovery-based approaches in the life sciences\n(Ankeny & Leonelli 2015; Dolinski & Troyanskaya 2015; Leonelli\n2014, 2016). \nThe rise of metagenomics is also linked to other changes in biological\nsciences more generally, especially the rise of systems biology\nstarting around the year 2000 (which is itself closely linked to the\ndevelopment of genomics since the 1990s). O’Malley and\nDupré (2005) point out that there is an important distinction\nto be made when looking at fields like systems biology, because there\nis not only a change in epistemology but also one in ontology. They\ntherefore distinguish between pragmatic and systems-theoretic\nbiologists. For the former, the idea of a ‘system’ is\nmerely an epistemic tool. For the latter, the system becomes the new\nfundamental ontological unit. Doolittle and Zhaxybayeva (2010) claim\nthat the same can be seen in metagenomics where there is a drive to\nsee the community or the ecosystem as the new fundamental unit, and\nnot the single species (see also Dupré & O’Malley\n2007). \nMoving away from a focus on single organisms or monogenomic species\nallows us to make better sense of many recent findings in microbiology\n(in which metagenomics has played a key role). Central to all of this\nare mobile DNA elements that can travel horizontally, meaning between\ndifferent members of a community (including between different kinds of\norganisms). Obtaining such mobile DNA elements can have a crucial\neffect on the survival and reproduction capacity of the recipient\ncell. Mobile DNA can therefore be a key element in the evolutionary\nprocesses as it becomes a ‘communal resource’ (McFall-Ngai\net al. 2013). Acquired antibiotic resistance is only one of many\nbenefits cells are known to obtain through acquired DNA elements. \nIt is then the composition of functional elements that the community\nas a whole contains which is preserved over evolutionary time. And the\ncommunity could be seen as an assembly of biochemical activities and\nnot of distinct microbial lineages (see for instance Turnbaugh et al.\n2009 and also Burke et al. 2011). The metagenome then becomes a\n‘genome of communities’ and not a ‘community of\ngenomes’ (Doolittle & Zhaxybayeva 2010). All of this also\nfeeds into the more general, and currently very active, discussion\nabout the problem of individuality in biology (Clarke 2010; Bouchard\n& Huneman 2013; Ereshefsky & Pedroso 2013; Guay & Pradeu\n2015; SEP entry on\n the biological notion of individual). \nApart from these issues in biological ontology, there are also\nepistemological issues raised by metagenomics, namely the discrepancy\nbetween our ability to sequence DNA and to interpret it. These\ndiscussions about the challenges of DNA sequence interpretation are\nnot just a problem for (meta)genomics and other -omics approaches, but\nalso for biomedicine more generally and its push towards a truly\npersonalised medicine. A key issue for this push is the discrepancy\nbetween the (ever-decreasing) costs of obtaining a personal genome\nsequence (Bennett et al. 2005; Mardis 2006; Check 2014a,b) and the\nhigh costs of making sure the data can be appropriately interpreted\n(Mardis 2006; Sboner et al. 2011; Phillips et al. 2015). This problem\nis related to the so-called ‘bioinformatic bottleneck’,\nthe handling and the interpretation of the large amounts of sequence\ndata that provides the main obstacle to progress (Green et al. 2011;\nDesai et al. 2012; Scholz et al. 2012; Marx 2013). In the days of\nnext-generation sequencing the sequencing step itself is no longer the\nrate-limiting step. \nGenomics is now an integral part of all of the life sciences. Not that\nevery life scientist is now a genomicist—there are still\nresearchers who focus on the biochemistry, development, or the\nmolecular networks of human cells and other organisms. But the DNA\nsequences of the human genome and the numerous model organisms that\ncame out of the HGP enter every laboratory, if not on a daily basis\nthan at least at some stage of every research project. The same\napplies to the maps of genetic variation that were discussed\nin Section 3 and to the (somewhat controversial)\ndata on functional DNA elements that the ENCODE project generated (see\nthe supplementary document\n The ENCODE Project and the ENCODE Controversy). \nAnd it is not just the quantity of data and the many new\n“-omes” that researchers now work with that have\ntransformed the science. As we have pointed out in several places,\ninsights into the genome and its functioning have transformed\nresearchers’ understanding of the entities and processes they\nare working with in the course of the last few decades. Part of this\nwas also a transformation in our understanding of what it means to do\n‘good’ science. What the HPG and its various offshoots\nhave achieved, therefore, is to change the life sciences at the\nepistemological, the ontological and also the methodological\nlevel. \nAs so often, an interesting and even pressing question is where all of\nthis is going. Predicting the future might not be possible, but there\nare trends that can be identified and which can be expected to follow\na similar trajectory in the near future. One such trend is the drive\nfor big data. ‘Big’ here refers not only to the quantity\nbut also to the different types of data collected. A derivative of\nthis big-data drive is the goal to integrate all of the diverse data\nand mould it into models that can further our understanding of\nbiological systems and the prediction of their behaviour. The\nrelatively young discipline of systems biology, which could not be\ndiscussed in detail in this entry, will certainly play a key role in\nthis endeavour.","contact.mail":"J.A.Dupre@exeter.ac.uk","contact.domain":"exeter.ac.uk"}]
