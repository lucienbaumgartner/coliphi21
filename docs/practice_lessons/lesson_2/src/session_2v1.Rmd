---
title: "Practice Lesson 2:<br>Inductive Analytics"
author: "Lucien Baumgartner"
date: "6/10/2021"
mail: "lucien.baumgartner@philos.uzh.ch"
github: "lucienbaumgartner/coliphi21/tree/site"
home: "https://lucienbaumgartner.github.io/"
editor_options:
  chunk_output_type: console
output: 
  epuRate::epurate:
      toc: TRUE
      number_sections: FALSE
      code_folding: "show"
---

```{r setup, include=FALSE}
options(width = 999)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
#library(devtools)
#install_github("holtzy/epuRate")
library(epuRate)
library(rmarkdown)
```

<link rel="stylesheet" href="styles.css">

## Packages
```{r message=FALSE}
## load required libraries
library(tidyverse)
library(quanteda)
library(lexicon)
library(reshape2)
library(stringi)
library(quanteda.textplots)
library(quanteda.textmodels)
library(quanteda.textstats)
library(gridExtra)
library(seededlda)
library(ggrepel)
library(ggdendro)
library(factoextra)
library(lattice)
library(spacyr)
```

## Clean workspace and set working directory
```{r}
## clean workspace
rm(list=ls())
```
```{r eval=FALSE}
## set working directory (WD)
path <- '~/coliphi21/practice_lessons/lesson_2/src/'
## you can also set it dynamically: 
## setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
setwd(path)
```
```{r}
## check that WD is set correctly
getwd()
```

## Import data

For this tutorial you can either work with your own data, or the pre-built copora provided in the `/input`-folder for the first practice session. The `quanteda`-package also contains pre-built corpora you can use. For this session, we scraped the Stanford Encyclopedia of Philosophy and built a corpus including additional metadata.

```{r}
## relative path
load('../input/stanford-encyclopedia.RDS')
```
```{r eval=FALSE}
## absolute path
load('~/coliphi21/practice_lessons/lesson_2/input/stanford-encyclopedia.RDS')
```

### If you work with your own corpus
```{r eval=FALSE}
## either
sfe <- readtext('path/to/you/data/*')
## or
sfe <- readtext('path/to/you/data', text_field = 'name_of_text_var', docid_field = 'id_var')
## then
sfe <- corpus(sfe)
```

#### Disclaimer
Loading the data above will import a pre-built corpus object into R, which is called `sfe`.

## Inspect data
```{r}
## how does the corpus object look like?
sfe
## summary statistics
summary(sfe) %>% head
## available variables
docvars(sfe)
```

#### > Exercise
Familiarize yourself a little more with the data.

## Prep
```{r eval=FALSE}
## tokenization
toks <- tokens(sfe, what = 'word',
               remove_punct = T, remove_symbols = T, padding = F, 
               remove_numbers = T, remove_url = T)
?tokens
## to lower
toks <- tokens_tolower(toks)
## lemmatizing
toks <- tokens_replace(toks, 
                       pattern = lexicon::hash_lemmas$token, 
                       replacement = lexicon::hash_lemmas$lemma)
## remove stopwords
toks <- tokens_select(toks,  pattern = stopwords("en"), selection = "remove")
## remove noise
toks <- tokens_select(toks, pattern = '[0-9]+|^.$', valuetype = 'regex', selection = 'remove')
## create dfm
dfm_sfe <- dfm(toks) %>% 
           dfm_trim(min_termfreq = 0.8, termfreq_type = "quantile",
                    max_docfreq = 0.1, docfreq_type = "prop")
?dfm_trim
```
```{r echo=FALSE}
#save(dfm_sfe, file = '../input/steps/dfm_initial.RDS')
load('../input/steps/dfm_initial.RDS')
```
```{r}
dfm_sfe
```

#### > Exercise {.tabset .tabset-fade}
##### Task
Check whether there is still some noise in the data and remove it. Hint: Scan through the topfeatures.

##### Solution
```{r eval=FALSE}
topfeatures(dfm_sfe, n=200)
## remove additional words
toks <- tokens_select(toks, pattern = 'many|much', valuetype = 'regex', selection = 'remove')
## create dfm
dfm_sfe <- dfm(toks) %>% 
           dfm_trim(min_termfreq = 0.8, termfreq_type = "quantile",
                    max_docfreq = 0.1, docfreq_type = "prop")
```


## Scaling: correspondence analysis
```{r eval=FALSE}
## compute model
sfe_ca <- quanteda.textmodels::textmodel_ca(dfm_sfe)
```
```{r echo=FALSE}
#save(sfe_ca, file = '../input/ca/ca.RDS')
load('../input/ca/ca.RDS')
```
```{r fig.height=5, fig.width=10, fig.align='center'}
## coerce model coefficients to dataframe
str(sfe_ca)
sfe_ca <- data.frame(dim1 = coef(sfe_ca, doc_dim = 1)$coef_document, 
                     dim2 = coef(sfe_ca, doc_dim = 2)$coef_document)
str(sfe_ca)
sfe_ca$id <- gsub('\\.json.*', '', rownames(sfe_ca))
head(sfe_ca)
## plot full data with branch annotation
ggplot(sfe_ca, aes(x=dim1, y=dim2, label=id)) +
  geom_point(aes(color=dim1-dim2), alpha = 0.2) +
  # plot 0.2 of all labels, using a repel function
  geom_text_repel(data = dplyr::sample_frac(sfe_ca, 0.2), max.overlaps = 15, seed = 6734) +
  theme_bw() +
  theme(plot.title = element_text(face='bold')) +
  labs(title = 'Correspondence Analysis: Full Data')
## plot parts of the data
ggplot(sfe_ca, aes(x=dim1, y=dim2, label=id)) +
  geom_point(aes(color=dim1-dim2), alpha = 0.2) +
  # plot 0.2 of all labels, using a repel function
  geom_text_repel(data = dplyr::sample_frac(sfe_ca, 0.2), max.overlaps = 9, seed = 6734) +
  scale_y_continuous(limits=c(-2,0)) +
  scale_x_continuous(limits=c(-1,1)) +
  theme_bw() +
  theme(plot.title = element_text(face='bold')) +
  labs(title = 'Correspondence Analysis: Zoom')
```

## Unsupervised LDA
```{r echo = FALSE}
load('../input/lda/lda_k10.RDS')
#save(sfe_lda, file = '../input/lda/lda_k10.RDS')
```

```{r eval = FALSE}
## run naive unsupervised topic model with 10 topics
set.seed(123)
sfe_lda <- textmodel_lda(dfm_sfe, k = 10)
?textmodel_lda
```
```{r fig.height=5, fig.width=10}
## print top 20 terms per topic
terms(sfe_lda, 20)
## plot the topics over the correspondence analysis data
sfe_ca$topics <- topics(sfe_lda)
head(sfe_ca)
ggplot(sfe_ca, aes(x=dim1, y=dim2, color=topics)) +
  geom_point(alpha = 0.5, shape = '.') +
  geom_density_2d(alpha = 0.5) +
  theme_bw() +
  theme(plot.title = element_text(face='bold')) +
  labs(title = 'Correspondence Analysis with Topic Annotation (k=10)')
```

#### > Exercise {.tabset .tabset-fade}

##### Task
Change the names of the topics (to some meaningful description) before plotting.

##### Solution
```{r eval=F}
sfe_ca$topics <- recode(sfe_ca$topics, topic1 = "body-mind", topic2 = "biology", 
                        topic3 = "feminism/critical thinking", topic4 = "math/ai", 
                        topic5 = "physics", topic6 = "classics", topic7 = "eastern",
                        topic8 = "phenomenology", topic9 = "religion", 
                        topic10 = "middle-eastern/eastern")
```

## PoS-tagging - leaving the sandbox
```{r}
## set seed
set.seed(48621)
## draw a random sample of 20 documents
sfe_sub <- sfe[sample(1:length(sfe), 5)]
sfe_sub
## PoS-tagging
sfe_pos <- spacy_parse(sfe_sub, pos = T, tag = T, lemma = T, entity = T, dependency = T)
sfe_pos
```

## Augment your sandbox
```{r}
## aggregate tokens and pos-tags back to documents
sfe_pos <- sfe_pos %>% rowwise %>% mutate(token_pos = paste0(token,'__', pos)) 
sfe_pos
sfe_pos <- sfe_pos %>% 
  group_by(doc_id) %>% 
  summarise(text = paste0(token_pos, collapse = ' '))
sfe_pos
## import it to quanteda and add metadata
sfe_pos <- corpus(sfe_pos)
docvars(sfe_pos) <- docvars(sfe_sub)
sfe_pos
## get all the nouns preceded by the adjective 'rational'
rational_noun <- stri_match_all(sfe_pos, regex = '(?<=rational__ADJ\\s)[A-z]+__NOUN')
names(rational_noun) <- docnames(sfe_pos)
rational_noun
rational_noun <- data.frame(match = do.call(rbind, rational_noun),
       doc_id = rep(names(rational_noun), lengths(rational_noun)))
rational_noun
## count them
rational_noun %>% 
  na.omit %>% 
  group_by(doc_id, match) %>% 
  summarise(n = n()) %>% 
  arrange(doc_id, desc(n)) %>% 
  print(n=200)
```

## Additional material

### Hierarchical clustering
```{r fig.height=5, fig.width=10, fig.align='center'}
## hierarchical clustering - get distances on normalized dfm
sfe_dist_mat <- dfm_weight(dfm_sfe, scheme = "prop") %>%
    textstat_dist(method = "euclidean") %>% 
    as.dist()
## hiarchical clustering the distance object
sfe_cluster <- hclust(sfe_dist_mat, method = 'ward.D')
# label with document names
sfe_cluster$labels <- gsub('\\.json(\\.[0-9])?', '', docnames(dfm_sfe))
## determine best numbers of clusters
# fviz_nbclust(as.matrix(sfe_dist_mat), FUN = hcut, method = "wss")
## cut tree into four groups
clusters <- cutree(sfe_cluster, k = 4)
## add cluster-data to the correspondence analysis
sfe_ca_hcl <- left_join(sfe_ca, data.frame(cluster = clusters, id = names(clusters)))
## plot
ggplot(sfe_ca_hcl, aes(x=dim1, y=dim2, label=id)) +
  geom_point(aes(color=as.factor(cluster)), alpha = 0.2) +
  facet_grid(~as.factor(cluster))
## hierarchical clustering doesn't provide discrete cluster along
## the dimensions of the correspondance analysis
```

### Cosine similarities for documents
```{r echo=TRUE, results='asis'}
## subset documents about logic
logic <- dfm_subset(dfm_sfe, grepl('(?<=\\-)logic|logic(?=\\-)', docnames(dfm_sfe), perl = T))
## compute cosine similarity
logic_sim <- textstat_simil(logic, margin = 'document', method = 'cosine')
## all pairs with a cosine similarity > .4
as.data.frame(logic_sim) %>% 
  filter(cosine > .4) %>% 
  arrange(desc(cosine))
```

#### > Exercise {.tabset .tabset-fade}

##### Task
Redo the cosine similarities for another subset of documents.

##### Solution
```{r}
## subset documents about aesthetics
aesth <- dfm_subset(dfm_sfe, grepl('aesthetics', docnames(dfm_sfe), perl = T))
## compute cosine similarity
aesth <- textstat_simil(aesth, margin = 'document', method = 'cosine')
## all pairs with a cosine similarity > .2
as.data.frame(aesth) %>% 
  filter(cosine > .2) %>% 
  arrange(desc(cosine))
```


### Cosine similarities for features
```{r echo=TRUE, results='asis'}
## subset documents about feminism
fem <- dfm_subset(dfm_sfe, grepl('(?<=\\-)fem|fem.*(?=\\-)', docnames(dfm_sfe), perl = T))
## compute cosine similarities for the features 
## "empowerment", "embodiment", and "rape"
fem_sim <- textstat_simil(logic, logic[, c("empowerment", "embodiment", "rape")], 
                          margin = 'feature', method = 'cosine')
## top 5 results per feature
as.data.frame(fem_sim) %>% 
  group_by(feature2) %>% 
  arrange(feature2, desc(cosine)) %>% 
  slice_head(n=5)
```

#### > Exercise {.tabset .tabset-fade}

##### Task
Redo the cosine similarities for a different set of features.

##### Solution
```{r}
fem_sim <- textstat_simil(logic, logic[, c("feminism", "patriarchy")], 
                          margin = 'feature', method = 'cosine')
## top 5 results per feature
as.data.frame(fem_sim) %>% 
  group_by(feature2) %>% 
  arrange(feature2, desc(cosine)) %>% 
  slice_head(n=5)
```


