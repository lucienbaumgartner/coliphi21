[{"date.published":"2014-08-08","date.changed":"2019-08-15","url":"https://plato.stanford.edu/entries/rationality-normative-utility/","author1":"R. A. Briggs","author1.info":"https://philosophy.stanford.edu/people/rachael-briggs","entry":"rationality-normative-utility","body.text":"\n\n\n\nWe must often make decisions under conditions of uncertainty.\nPursuing a degree in biology may lead to lucrative employment, or to\nunemployment and crushing debt. A doctor's appointment may result in\nthe early detection and treatment of a disease, or it may be a waste\nof money. Expected utility theory is an account of how to choose\nrationally when you are not sure which outcome will result from your\nacts. Its basic slogan is: choose the act with the highest expected\nutility.\n\n\n\nThis article discusses expected utility theory as a normative\ntheory—that is, a theory of how people should make\ndecisions. In classical economics, expected utility theory is often\nused as a descriptive theory—that is, a theory of how\npeople do make decisions—or as a predictive\ntheory—that is, a theory that, while it may not accurately model\nthe psychological mechanisms of decision-making, correctly predicts\npeople's choices. Expected utility theory makes faulty predictions\nabout people's decisions in many real-life choice situations (see\nKahneman & Tversky 1982); however, this does not settle whether\npeople should make decisions on the basis of expected utility\nconsiderations.\n\n\n\nThe expected utility of an act is a weighted average of the\nutilities of each of its possible outcomes, where the utility\nof an outcome measures the extent to which that outcome is preferred,\nor preferable, to the alternatives. The utility of each outcome is\nweighted according to the probability that the act will lead to that\noutcome. Section 1 fleshes out this basic definition of expected\nutility in more rigorous terms, and discusses its relationship to\nchoice. Section 2 discusses two types of arguments for expected\nutility theory: representation theorems, and long-run statistical\narguments. Section 3 considers objections to expected utility\ntheory; section 4 discusses its applications in philosophy of\nreligion, economics, ethics, and epistemology.\n\n\n\nThe concept of expected utility is best illustrated by\nexample. Suppose I am planning a long walk, and need to decide whether\nto bring my umbrella. I would rather not tote the umbrella on a sunny\nday, but I would rather face rain with the umbrella than without\nit. There are two acts available to me: taking my umbrella, and\nleaving it at home. Which of these acts should I choose? \n\nThis informal problem description can be recast, slightly more\nformally, in terms of three sorts of entities. First, there are\noutcomes—objects of non-instrumental preferences.  In\nthe example, we might distinguish three outcomes: either I end up dry\nand unencumbered; I end up dry and encumbered by an unwieldy umbrella;\nor I end up wet. Second, there are states—things\noutside the decision-maker's control which influence the outcome of\nthe decision. In the example, there are two states: either it is\nraining, or it is not.  Finally, there are acts—objects\nof the decision-maker's instrumental preferences, and in some sense,\nthings that she can do. In the example, there are two acts: I may\neither bring the umbrella; or leave it at home. Expected utility\ntheory provides a way of ranking the acts according to how\nchoiceworthy they are: the higher the expected utility, the\nbetter it is to choose the act. (It is therefore best to choose the\nact with the highest expected utility—or one of them, in the\nevent that several acts are tied.) \n\nFollowing general convention, I will make the following assumptions\nabout the relationships between acts, states, and outcomes. \n\nSo the example of the umbrella can be depicted in the following\nmatrix, where each column corresponds to a state of the world; each row\ncorresponds to an act; and each entry corresponds to the outcome that\nresults when the act is performed in the state of the world. \n\nHaving set up the basic framework, I can now rigorously define\nexpected utility. The expected utility of an act \\(A\\) (for\ninstance, taking my umbrella) depends on two features of the\nproblem: \n\nGiven these three pieces of information, \\(A\\)'s expected\nutility is defined as: \n\nwhere \\(O\\) is is the set of outcomes,\n\\(P_{A}(o)\\) is the probability of outcome \\(o\\) conditional on \\(A\\), and \n\\(U(o)\\) is the utility of \\(o\\). \n\nThe next two subsections will unpack the conditional probability\nfunction \\(P_A\\) and the utility function \\(U\\). \n\nThe term \\(P_{A}(o)\\) represents the probability\nof \\(o\\) given \\(A\\)—roughly, how likely it is that\noutcome \\(o\\) will occur, on the supposition that the agent\nchooses act \\(A\\). (For the axioms of probability, see the entry\non \n interpretations of probability.)\n To understand what this means, we\nmust answer two questions. First, which interpretation of probability\nis appropriate? And second, what does it mean to assign a probability\non the supposition that the agent chooses act \\(A\\)? \n\nExpected utility theorists often interpret probability as measuring\nindividual degree of belief, so that a proposition \\(E\\) is likely\n(for an agent) to the extent that that agent is confident of \\(E\\)\n(see, for instance, Ramsey 1926, Savage 1972, Jeffrey 1983). But\nnothing in the formalism of expected utility theory forces this\ninterpretation on us. We could instead interpret probabilities as\nobjective chances (as in von Neumann and Morgenstern 1944), or as the\ndegrees of belief that are warranted by the evidence, if we thought\nthese were a better guide to rational action. (See the entry on\n interpretations of probability\n for discussion of these and other options.) \n\nWhat is it to have a probability on the supposition that the agent\nchooses \\(A\\)? Here, there are two basic types of answer,\ncorresponding to evidential decision theory and causal decision\ntheory. \n\nAccording to evidential decision theory, endorsed by Jeffrey (1983),\nthe relevant suppositional probability \\(P_{A}(o)\\)\nis the conditional probability \n\\(P(o \\mid A)\\), \ndefined as the ratio of two unconditional probabilities:\n\\(P(A \\amp o) / P(A)\\). \n\nAgainst Jeffrey's definition of expected utility, Spohn (1977)\nand Levi (1991) object that a decision-maker should not assign\nprobabilities to the very acts under deliberation: when freely deciding\nwhether to perform an act \\(A\\), you shouldn't take into\naccount your beliefs about whether you will perform \\(A\\). \nIf Spohn and Levi are right, then Jeffrey's ratio is undefined\n(since its denominator is undefined). \n\nNozick (1969) raises another objection: Jeffrey's definition gives\nstrange results in the Newcomb Problem. A predictor\nhands you a closed box, containing either $0 or $1 million, and offers\nyou an open box, containing an additional $1,000. You can either\nrefuse the open box (“one-box”) or take the open box\n(“two-box”). But there's a catch: the predictor\nhas predicted your choice beforehand, and all her predictions are 90%\naccurate. In other words, the probability that you one-box, given\nthat she predicts you one-box, is 90%, and the probability that you\ntwo-box, given that she predicts you two-box, is 90%. Finally,\nthe contents of the closed box depend on the prediction: if the\npredictor thought you would two-box, she put nothing in the closed box,\nwhile if she thought you would one-box, she put $1 million in the\nclosed box. The matrix for your decision looks like this: \n\nTwo-boxing dominates one-boxing: in every state, two-boxing\nyields a better outcome. Yet on Jeffrey's definition of conditional\nprobability, one-boxing has a higher expected utility than\ntwo-boxing. There is a high conditional probability of finding $1\nmillion is in the closed box, given that you one-box, so one-boxing\nhas a high expected utility. Likewise, there is a high conditional\nprobability of finding nothing in the closed box, given that you\ntwo-box, so two-boxing has a low expected utility. \n\nCausal decision theory is an\nalternative proposal that gets around these problems. It does not\nrequire (but still permits) acts to have probabilities, and it\nrecommends two-boxing in the Newcomb problem. \n\nCausal decision theory comes in many varieties, but I’ll consider a\nrepresentative version proposed by Savage (1972), which calculates\n\\(P_{A}(o)\\) by summing the probabilities of states that, when\ncombined with the act \\(A\\), lead to the outcome \\(o\\). Let\n\\(f_{A,s}(o)\\) be a of outcomes, which maps \\(o\\) to 1 if \\(o\\)\nresults from performing \\(A\\) in state s, maps \\(o\\) to 0\notherwise. Then \n\nOn Savage's proposal, two-boxing comes out with a higher\nexpected utility than one-boxing. This result holds no matter\nwhich probabilities you assign to the states prior to your\ndecision. Let \\(x\\) be the probability you assign to the\nstate that the closed box contains $1 million. According to\nSavage, the expected utilities of one-boxing and two-boxing,\nrespectively, are: \n\nand \n\nAs long as the larger monetary amounts are assigned strictly larger\nutilities, the second sum (the utility of two-boxing) is guaranteed to\nbe larger than the first (the utility of one-boxing).\n \n\nSavage assumes that each act and state are enough to uniquely\ndetermine an outcome. But there are cases where this assumption\nbreaks down. Suppose you offer to sell me the following gamble:\nyou will toss a coin; if the coin lands heads, I win $100; and if the\ncoin lands tails, I lose $100. But I refuse the gamble, and the\ncoin is never tossed. There is no outcome that would\nhave resulted, had the coin been tossed—I might have won $100,\nand I might have lost $100. \n\nWe can generalze Savage’s proposal by letting \\(f_{A,s}\\) be a\nprobability function that maps outcomes to real numbers in the \\([0,\n1]\\) interval. Lewis (1981), Skyrms (1980), and Sobel (1994) equate\n\\(f_{A,s}\\) with the objective chance that \\(o\\) would be the outcome\nif state \\(s\\) obtained and the agent chose action \\(A\\). \n\nIn some cases—most famously the Newcomb problem—the\nJeffrey definition and the Savage definition of expected utility come\napart. But whenever the following two conditions are satisfied,\nthey agree. \n\nThe term \\(U(o)\\) represents the utility of the outcome\n\\(o\\)—roughly, how valuable \\(o\\) is.  Formally, \\(U\\) is a\nfunction that assigns a real number to each of the outcomes. (The\nunits associated with \\(U\\) are typically called utiles,\nso that if \\(U(o) = 2\\), we say that \\(o\\) is worth 2 utiles.) The greater\nthe utility, the more valuable the outcome. \n\nWhat kind of value is measured in utiles? Utiles are typically\nnot taken to be units of currency, like dollars, pounds, or\nyen. Bernoulli (1738) argued that money and other goods have\ndiminishing marginal utility: as an agent gets richer, every successive\ndollar (or gold watch, or apple) is less valuable to her than the\nlast. He gives the following example: It makes rational sense for\na rich man, but not for a pauper, to pay 9,000 ducats in exchange for a\nlottery ticket that yields a 50% chance at 20,000 ducats and a 50%\nchance at nothing. Since the lottery gives the two men the same\nchance at each monetary prize, the prizes must have different values\ndepending on whether the player is poor or rich. \n\nClassic utilitarians such as Bentham (1789), Mill (1861), and Sidgwick\n(1907) interpreted utility as a measure of pleasure or happiness. For\nthese authors, to say \\(A\\) has greater utility than \\(B\\) (for an\nagent or a group of agents) is to say that \\(A\\) results in more\npleasure or happiness than \\(B\\) (for that agent or group of\nagents). \n\nOne objection to this interpretation of utility is that there may\nnot be a single good (or indeed any good) which rationality requires us\nto seek. But if we understand “utility” broadly\nenough to include all potentially desirable ends—pleasure,\nknowledge, friendship, health and so on—it's not clear that\nthere is a unique correct way to make the tradeoffs between different\ngoods so that each outcome receives a utility. There may be no\ngood answer to the question of whether the life of an ascetic monk\ncontains more or less good than the life of a happy libertine—but\nassigning utilities to these options forces us to compare them. \n\nContemporary decision theorists typically interpret utility as a\nmeasure of preference, so that to say that \\(A\\) has greater\nutility than \\(B\\) (for an agent) is simply to say that the agent\nprefers \\(A\\) to \\(B\\). It is crucial to this approach\nthat preferences hold not just between outcomes (such as amounts of\npleasure, or combinations of pleasure and knowledge), but also between\nuncertain prospects (such as a lottery that pays $1 million dollars if\na particular coin lands heads, and results in an hour of painful\nelectric shocks if the coin lands tails). Section 2 of this\narticle addresses the formal relationship between preference and choice\nin detail. \n\nExpected utility theory does not require that preferences\nbe selfish or self-interested. Someone can prefer giving money to\ncharity over spending the money on lavish dinners, or prefer\nsacrificing his own life over allowing his child to die. Sen\n(1977) suggests that each person's psychology is best represented\nusing three rankings: one representing the person's narrow\nself-interest, a second representing the person's self-interest\nconstrued more broadly to account for feelings of sympathy (e.g.,\nsuffering when watching another person suffer), and a third\nrepresenting the person's commitments, which may require her to\nact against her self-interest broadly construed. \n\nBroome (1991) interprets utilities as measuring comparisons of\nobjective betterness and worseness, rather than personal preferences:\nto say that \\(A\\) has a greater utility than \\(B\\) is to say\nthat \\(A\\) is objectively better than \\(B\\), or that a\nrational person would prefer \\(A\\) to \\(B\\). Just as\nthere is nothing in the formalism of probability theory that requires\nus to use subjective rather than objective probabilities, so there is\nnothing in the formalism of expected utility theory that requires us to\nuse subjective rather than objective values. \n\nThose who interpret utilities in terms of personal preference face a\nspecial challenge: the so-called problem of interpersonal utility\ncomparisons. When making decisions about how to distribute\nshared resources, we often want to know if our acts would make Alice\nbetter off than Bob—and if so, how much better off. But if\nutility is a measure of individual preference, there is no clear,\nmeaningful way of making these comparisons. Alice's\nutilities are constituted by Alice's preferences, Bob's\nutilities are constituted by Bob's preferences, and there are no\npreferences spanning Alice and Bob. We can't assume that\nAlice's utility 10 is equivalent to Bob's utility 10, any\nmore than we can assume that getting an A grade in differential\nequations is equivalent to getting an A grade in basket weaving. \n\nNow is a good time to consider which features of the utility\nfunction carry meaningful information. Comparisons are\ninformative: if \\(U(o_1) \\gt U(o_2)\\) (for a person), then\n\\(o_1\\) is better than (or preferred to)\n\\(o_2\\). But it is not only comparisons\nthat are informative—the utility function must carry other\ninformation, if expected utility theory is to give meaningful\nresults. \n\nTo see why, consider the umbrella example again. This time,\nI've filled in a probability for each state, and a utility for each\noutcome. \n\nThe expected utility of taking the umbrella is \n\nwhile the expected utility of leaving the umbrella is \n\nSince \\(EU(\\take) \\gt EU(\\leave)\\), expected utility theory tells me\nthat taking the umbrella is better than leaving it. \n\nBut now, suppose we change the utilities of the outcomes: instead of\nusing \\(U\\), we use \\(U'\\). \n\nThe new expected utility of taking the umbrella is \n\nwhile the new expected utility of leaving the umbrella is \n\nSince \\(EU'(\\take) \\lt EU'(\\leave)\\), expected utility theory tells me\nthat leaving the umbrella is better than taking it. \n\nThe utility functions \\(U\\) and \\(U'\\) rank the outcomes\nin exactly the same way: free, dry is best;\nencumbered, dry ranks in the middle; and wet\nis worst. Yet expected utility theory gives different advice in\nthe two versions of the problem. So there must be some\nsubstantive difference between preferences appropriately described by\n\\(U\\), and preferences appropriately described by\n\\(U'\\). Otherwise, expected utility theory is fickle, and\nliable to change its advice when fed different descriptions of the same\nproblem. \n\nWhen do two utility functions represent the same basic state of\naffairs? Measurement theory answers the question by characterizing the\nallowable transformations of a utility function—ways of\nchanging it that leave all of its meaningful features intact. If we\ncharacterize the allowable transformations of a utility function, we\nhave thereby specified which of its features are meaningful. \n\nDefenders of expected utility theory typically require that utility\nbe measured by a linear scale, where the\nallowable transformations are all and only the positive linear\ntransformations, i.e., functions \\(f\\) of the form \n\nfor real numbers \\(x \\gt 0\\) and \\(y\\). \n\nPositive linear transformations of outcome utilities will never\naffect the verdicts of expected utility theory: if \\(A\\) has\ngreater expected utility than \\(B\\) where utility is measured by\nfunction \\(U\\), then \\(A\\) will also have greater expected\nutility than \\(B\\) where utility is measured by any positive\nlinear transformation of \\(U\\).  \n\nWhy choose acts that maximize expected utility? One possible\nanswer is that expected utility theory is rational bedrock—that\nmeans-end rationality essentially involves maximizing expected\nutility. For those who find this answer unsatisfying, however,\nthere are two further sources of justification. First, there are\nlong-run arguments, which rely on evidence that expected-utility\nmaximization is a profitable policy in the long term. \nSecond, there are arguments based on representation theorems, which\nsuggest that certain rational constraints on preference entail that all\nrational agents maximize expected utility. \n\nOne reason for maximizing expected utility is that it makes for good\npolicy in the long run. Feller (1968) gives a version of this\nargument. He relies on two mathematical facts about\nprobabilities: the strong and weak laws of large\nnumbers. Both these facts concern sequences of independent,\nidentically distributed trials—the sort of setup that results\nfrom repeatedly betting the same way on a sequence of roulette spins or\ncraps games. Both the weak and strong laws of large numbers\nsay, roughly, that over the long run, the average amount of utility\ngained per trial is overwhelmingly likely to be close to the expected\nvalue of an individual trial. \n\nThe weak law of large numbers states that where each trial has an\nexpected value of \\(\\mu\\), for any arbitrarily small real numbers\n\\(\\epsilon \\gt 0\\) and \\(\\delta \\gt 0\\), there is some finite number of\ntrials \\(n\\), such that for all \\(m\\) greater than or equal\nto \\(n\\), with probability at least \\(1-\\delta\\), the gambler's\naverage gains for the first \\(m\\) trials will fall within\n\\(\\epsilon\\) of \\(\\mu\\). In other words, in a long run of similar\ngamble, the average gain per trial is highly likely to become\narbitrarily close to the gamble's expected value within a finite\namount of time. So in the finite long run, the average value\nassociated with a gamble is overwhelmingly likely to be close to its\nexpected value. \n\nThe strong law of large numbers states that where each trial has an\nexpected value of \\(\\mu\\), for any arbitrarily small real number\n\\(\\epsilon \\gt 0\\), as the number of trials increases, the probability\nthat the gambler's average winnings per trial fall within \\(\\epsilon\\) of\n\\(\\mu\\) converges to 1. In other words, as the number of repetitions of a\ngamble approaches infinity, the average gain per trial will become\narbitrarily close to the gamble's expected value with probability\n1. So in the long run, the average value associated with a gamble is\nvirtually certain to equal its expected value. \n\nThere are several objections to these long run arguments. \nFirst, many decisions cannot be repeated over indefinitely\nmany similar trials. Decisions about which career to pursue, whom\nto marry, and where to live, for instance, are made at best a small\nfinite number of times. Furthermore, where these decisions are\nmade more than once, different trials involve different possible\noutcomes, with different probabilities. It is not clear why\nlong-run considerations about repeated gambles should bear on these\nsingle-case choices. \n\nSecond, the argument relies on two independence assumptions, one or\nboth of which may fail. One assumption holds that the\nprobabilities of the different trials are independent. This is\ntrue of casino gambles, but not true of other choices where we wish to\nuse decision theory—e.g., choices about medical treatment. \nMy remaining sick after one course of antibiotics makes it more likely\nI will remain sick after the next course, since it increases the chance\nthat antibiotic-resistant bacteria will spread through my body. The\nargument also requires that the utilities of different trials be\nindependent, so that winning a prize on one trial makes the same\ncontribution to the decision-maker's overall utility no matter\nwhat she wins on other trials. But this assumption is violated in\nmany real-world cases. Due to the diminishing marginal utility of\nmoney, winning $10 million on ten games of roulette is not worth ten times as\nmuch as winning $1 million on one game of roulette. \n\nA third problem is that the strong and weak laws of large numbers are\nmodally weak. Neither law entails that if a gamble were repeated\nindefinitely (under the appropriate assumptions), the average utility\ngain per trial would be close to the game's expected\nutility. They establish only that the average utility gain per trial\nwould with high probability be close to the game's expected\nutility. But high probability—even probability 1—is not\ncertainty. (Standard probability theory rejects Cournot's\nPrinciple, which says events with low or zero probability will\nnot happen.  But see Shafer (2005) for a defense of Cournot's\nPrinciple.) For any sequence of independent, identically distributed\ntrials, it is possible for the average utility payoff per trial to\ndiverge arbitrarily far from the expected utility of an individual\ntrial.  \n\nA second type of argument for expected utility theory relies on\nso-called representation theorems. We follow Zynda's (2000) formulation\nof this argument—slightly modified to reflect the role of\nutilities as well as probabilities. The argument has three\npremises: \nThe Rationality Condition. The axioms of expected\nutility theory are the axioms of rational preference. \nRepresentability. If a person's preferences obey the\naxioms of expected utility theory, then she can be represented as\nhaving degrees of belief that obey the laws of the probability\ncalculus [and a utility function such that she prefers acts with\nhigher expected utility]. \nThe Reality Condition. If a person can be\nrepresented as having degrees of belief that obey the probability\ncalculus [and a utility function such that she prefers acts with higher\nexpected utility], then the person really has degrees of belief that\nobey the laws of the probability calculus [and really does prefer acts\nwith higher expected utility]. \n\nThese premises entail the following conclusion. \n\nIf the premises are true, the argument shows that there is something\nwrong with people whose preferences are at odds with expected utility\ntheory—they violate the axioms of rational preference. Let\nus consider each of the premises in greater detail, beginning with the\nkey premise, Representability. \n\nA probability function and a utility function together\nrepresent a set of preferences just in case the following\nformula holds for all values of \\(A\\) and \\(B\\) in the domain\nof the preference relation \n\nMathematical proofs of Representability are called\nrepresentation theorems. Section 2.1 surveys three of\nthe most influential representation theorems, each of which relies on a\ndifferent set of axioms. \n\nNo matter which set of axioms we use, the Rationality Condition is\ncontroversial. In some cases, preferences that seem rationally\npermissible—perhaps even rationally required—violate the\naxioms of expected utility theory. Section 3 discusses such cases\nin detail. \n\nThe Reality Condition is also controversial. Hampton (1994), Zynda\n(2000), and Meacham and Weisberg (2011) all point out that to be\nrepresentable using a probability and utility function is not\nto have a probability and utility function. After all,\nan agent who can be represented as an expected utility maximizer with\ndegrees of belief that obey the probability calculus, can also be\nrepresented as someone who fails to maximize expected utility with\ndegrees of belief that violate the probability calculus. \nWhy think the expected utility representation is the right one? \n\nThere are several options. Perhaps the defender of\nrepresentation theorems can stipulate that what it is to have\nparticular degrees of belief and utilities is just to have the\ncorresponding preferences. The main challenge for defenders of\nthis response is to explain why representations in terms of expected\nutility are explanatorily useful, and why they are better than\nalternative representations. Or perhaps probabilities and utilities are\na good cleaned-up theoretical substitutes for our folk notions of\nbelief and desire—precise scientific substitutes for our folk\nconcepts. Meacham and Weisberg challenge this response, arguing\nthat probabilities and utilities are poor stand-ins for our folk\nnotions. A third possibility, suggested by Zynda, is that facts\nabout degrees of belief are made true independently of the\nagent's preferences, and provide a principled way to restrict the\nrange of acceptable representations. The challenge for defenders\nof this type of response is to specify what these additional facts\nare. \n\nI now turn to consider three influential representation\ntheorems. These representation theorems differ from each other in\nthree of philosophically significant ways.  \n\nFirst, different representation theorems disagree about the objects\nof preference and utility.  Are they repeatable? Must they be wholly within the agent's control \n\nSecond, representation theorems differ in their treatment of\nprobability. They disagree about which entities have\nprobabilities, and about whether the same objects can have both\nprobabilities and utilities. \n\nThird, while every representation theorem proves that for a suitable\npreference ordering, there exist a probability and utility\nfunction representing the preference ordering, they differ how\nunique this probability and utility function are. In\nother words, they differ as to which transformations of the probability\nand utility functions are allowable. \n\nThe idea of a representation theorem for expected utility dates back\nto Ramsey (1926).  (His sketch of a representation theorem is\nsubsequently filled in by Bradley (2004) and Elliott (2017).) Ramsey\nassumes that preferences are defined over a domain of gambles, which\nyield one prize on the condition that a proposition \\(P\\) is true, and\na different prize on the condition that \\(P\\) is false.  (Examples of\ngambles: you receive a onesie if you’re having a baby and a bottle of\nscotch otherwise; you receive twenty dollars if Bojack wins the\nKentucky Derby and lose a dollar otherwise.)  \n\nRamsey calls a proposition ethically neutral when “two possible\nworlds differing only in regard to [its truth] are always of equal\nvalue”.  For an ethically neutral proposition, probability 1/2\ncan be defined in terms of preference: such a proposition has\nprobability 1/2 just in case you are indifferent as to which side of\nit you bet on.  (So if Bojack wins the Kentucky Derby is an\nethically neutral proposition, it has probability 1/2 just in case you\nare indifferent between winning twenty dollars if it’s true and losing\na dollar otherwise, and winning twenty dollars if it’s false and\nlosing a dollar otherwise.)  \n\nBy positing an ethically neutral proposition with probability 1/2,\ntogether with a rich space of prizes, Ramsey defines numerical\nutilities for prizes.  (The rough idea is that if you are indifferent\nbetween receiving a middling prize \\(m\\) for certain, and a gamble\nthat yields a better prize \\(b\\) if the ethically neutral proposition\nis true and a worse prize \\(w\\) if it is falls, then the utility of\n\\(m\\) is halfway between the utilities of \\(b\\) and \\(w\\).)  Using\nthese numerical utilities, he then exploits the definition of expected\nutility to define probabilities for all other propositions. \nThe rough idea is to exploit the richness of the space of prizes,\nwhich ensures that for any gamble \\(g\\) that yields better prize \\(b\\)\nif \\(E\\) is true and worse prize \\(w\\) if \\(E\\) is false, the agent is\nindifferent between \\(g\\) and some middling prize \\(m\\).  This means\nthat \\(EU(g) = EU(m)\\).  Using some algebra, plus the fact that\n\\(EU(g) = P(E)U(b) + (1-P(E))U(w)\\), Ramsey shows that \n\nVon Neumann and Morgenstern (1944) claim that preferences are defined\nover a domain of lotteries. Some of these lotteries are\nconstant, and yield a single prize with certainty. (Prizes\nmight include a banana, a million dollars, a million dollars' worth of\ndebt, death, or a new car.)  Lotteries can also have other lotteries\nas prizes, so that one can have a lottery with a 40% chance of\nyielding a banana, and a 60% chance of yielding a 50-50 gamble between\na million dollars and death.) The domain of lotteries is closed under\na mixing operation, so that if \\(L\\) and \\(L'\\) are lotteries and \\(x\\) is a\nreal number in the \\([0, 1]\\) interval, then there is a lottery \\(x L +\n(1-x) L'\\) that yields \\(L\\) with probability \\(x\\) and \\(L'\\) with\nprobability \\(1-x\\). They show that every preference relation obeying\ncertain axioms can be represented by the probabilities used to define\nthe lotteries, together with a utility function which is unique up to\npositive linear transformation. \n\nInstead of taking probabilities for granted, as von Neumann and\nMorgenstern do, Savage (1972) defines them in terms of preferences\nover acts.  Savage posits three separate domains. Probability attaches\nto events, which we can think of as disjunctions of states,\nwhile utility and intrinsic preference attach to outcomes.\nExpected utility and non-intrinsic preference attach\nto acts.  \n\nFor Savage, acts, states, and outcomes must satisfy certain\nconstraints.  Acts must be wholly under the agent’s control (so\npublishing my paper in Mind is not an act, since it depends\npartly on the editor’s decision, which I do not control). Outcomes\nmust have the same utility regardless of which state obtains (so \"I\nwin a fancy car\" is not an outcome, since the utility of the fancy car\nwill be greater in states where the person I most want to impress\nwishes I had a fancy car, and less in states where I lose my driver’s\nlicense). No state can rule out the performance of any act, and an act\nand a state together must determine an outcome with certainty. For\neach outcome \\(o\\), there is a constant act which yields \\(o\\) in\nevery state. (Thus, if world peace is an outcome, there is an act that\nresults in world peace, no matter what the state of the world.)\nFinally, he assumes for any two acts \\(A\\) and \\(B\\) and any event\n\\(E\\), there is a mixed act \\(A_E \\amp B_{\\sim E}\\) that yields the\nsame outcome as \\(A\\) if \\(E\\) is true, and the same outcome as \\(B\\)\notherwise. (Thus, if world peace and the end of the world are both\noutcomes, then there is a mixed act that results in world peace if a\ncertain coin lands heads, and the end of the world otherwise.) \n\nSavage postulates a preference relation over acts, and gives axioms\ngoverning that preference relation. He then defines subjective\nprobabilities, or degrees of belief, in terms of preferences. The key\nmove is to define an “at least as likely as” relation\nbetween events; I paraphrase here.  \n\nThe thought behind the definition is that the agent considers \\(E\\) at\nleast as likely as \\(F\\) just in case she would not rather\nbet on \\(F\\) than on \\(E\\)). \n\nSavage then gives axioms constraining rational preference, and shows\nthat any set of preferences satisfying those axioms yields an\n“at least as likely” relation that can be uniquely\nrepresented by a probability function. In other words, there is one\nand only one probability function \\(P\\) such that for all \\(E\\) and \\(F\\),\n\\(P(E) \\ge P(F)\\) if and only if \\(E\\) is at least as likely as \\(F\\). Every\npreference relation obeying Savage's axioms is represented by this\nprobability function \\(P\\), together with a utility function which is\nunique up to positive linear transformation. \n\nSavage's representation theorem gives strong results: starting\nwith a preference ordering alone, we can find a single probability\nfunction, and a narrow class of utility functions, which represent that\npreference ordering. The downside, however, is that Savage has to\nbuild implausibly strong assumptions about the domain of acts. \n\nLuce and Suppes (1965) point out that Savage's constant acts\nare implausible. (Recall that constant acts yield the same\noutcome and the same amount of value in every state.) Take some\nvery good outcome—total bliss for everyone. Is there really\na constant act that has this outcome in every possible state, including\nstates where the human race is wiped out by a meteor? \nSavage's reliance on a rich space of mixed acts is also\nproblematic. Savage has had to assume that any two outcomes and\nany event, there is a mixed act that yields the first outcome if the\nevent occurs, and the second outcome otherwise? Is there really an act\nthat yields total bliss if everyone is killed by an\nantibiotic-resistant plague, and total misery otherwise? Luce and\nKrantz (1971) suggest ways of reformulating Savage's\nrepresentation theorem that weaken these assumptions, but Joyce (1999)\nargues that even on the weakened assumptions, the domain of acts\nremains implausibly rich. \n\nBolker (1966) proves a general representation theorem about\nmathematical expectations, which Jeffrey (1983) uses as the basis for a\nphilosophical account of expected utility theory. Bolker's\ntheorem assumes a single domain of propositions, which are objects of\npreference, utility, and probability alike. Thus, the proposition that\nit will rain today has a utility, as well as a probability. \nJeffrey interprets this utility as the proposition's news\nvalue—a measure of how happy or disappointed I would be to\nlearn that the proposition was true. By convention, he sets the\nvalue of the necessary proposition at 0—the necessary proposition\nis no news at all! Likewise, the proposition that I take my\numbrella to work, which is an act, has a probability as well as a\nutility. Jeffrey interprets this to mean that I have degrees of\nbelief about what I will do. \n\nBolker gives axioms constraining preference, and shows that any\npreferences satisfying his axioms can be represented by a probability\nmeasure \\(P\\) and a utility measure \\(U\\). However,\nBolker's axioms do not ensure that \\(P\\) is unique, or that\n\\(U\\) is unique up to positive linear transformation. Nor do\nthey allow us to define comparative probability in terms of\npreference. Instead, where \\(P\\) and \\(U\\) jointly\nrepresent a preference ordering, Bolker shows that the pair\n\\(\\langle P, U \\rangle\\) is unique up to a fractional linear\ntransformation.  \n\nIn technical terms, where \\(U\\) is a utility function\nnormalized so that \\(U(\\Omega) = 0\\), \\(inf\\) is the\ngreatest lower bound of the values assigned by \\(U\\), \\(sup\\)\nis the least upper bound of the values assigned by by \\(U\\), and\n\\(\\lambda\\) is a parameter falling between \\(-1/inf\\) and\n\\(-1/sup\\), the fractional linear transformation\n\\(\\langle P_{\\lambda}, U_{\\lambda} \\rangle\\) of\n\\(\\langle P, U \\rangle\\) corresponding to \\(\\lambda\\) is given\nby: \n\nNotice that fractional linear transformations of a\nprobability-utility pair can disagree with the original pair about\nwhich propositions are likelier than which others. \n\nJoyce (1999) shows that with additional resources, Bolker's\ntheorem can be modified to pin down a unique \\(P\\), and a\n\\(U\\) that is unique up to positive linear transformation. \nWe need only supplement the preference ordering with a primitive\n“more likely than” relation, governed by its own set of\naxioms, and linked to belief by several additional axioms. Joyce\nmodifies Bolker's result to show that given these additional\naxioms, the “more likely than” relation is represented by a\nunique \\(P\\), and the preference ordering is represented by\n\\(P\\) together with a utility function that is unique up to\npositive linear transformation. \n\nTogether, these four representation theorems above can be summed up\nin the following table. \n\nNotice that the order of construction differs between theorems: Ramsey\nconstructs a representation of probability using utility, while von\nNeumann and Morgenstern begin with probabilities and construct a\nrepresentation of utility. Thus, although the arrows represent a\nmathematical relationship of representation, they cannot represent a\nmetaphysical relationship of grounding. The Reality Condition needs to\nbe justified independently of any representation theorem. \n\nSuitably structured ordinal probabilities (the relations picked out\nby “at least as likely as”, “more likely than”,\nand “equally likely”) stand in one-to-one correspondence\nwith the cardinal probability functions. Finally, the grey line\nfrom preferences to ordinal probabilities indicates that every\nprobability function satisfying Savage's axioms is represented by\na unique cardinal probability—but this result does not hold for\nJeffrey's axioms. \n\nNotice that it is often possible to follow the arrows in\ncircles—from preference to ordinal probability, from ordinal\nprobability to cardinal probability, from cardinal probability and\npreference to expected utility, and from expected utility back to\npreference. Thus, although the arrows represent a mathematical\nrelationship of representation, they do not represent a metaphysical\nrelationship of grounding. This fact drives home the importance\n\nof independently justifying the Reality Condition—representation\ntheorems cannot justify expected utility theory without additional\nassumptions. \n\nOught implies can, but is it humanly possible to maximize expected\nutility?  March and Simon (1958) point out that in order to compute\nexpected utilities, an agent needs a dauntingly complex understanding\nof the available acts, the possible outcomes, and the values of those\noutcomes, and that choosing the best act is much more demanding than\nchoosing an act that is merely good enough.  Similar points appear in\nLindblom (1959), Feldman (2006), and Smith (2010). \n\nMcGee (1991) argues that maximizing expected utility is not\nmathematically possible even for an ideal computer with limitless\nmemory.  In order to maximize expected utility, we would have to\naccept any bet we were offered on the truths of arithmetic, and reject\nany bet we were offered on false sentences in the language of\narithmetic.  But arithmetic is undecidable, so no Turing machine can\ndetermine whether a given arithmetical sentence is true or false. \n\nOne response to these difficulties is the\n bounded rationality approach, which\naims to replace expected utility theory with some more tractable\nrules.  Another is to argue that the demands of expected utility\ntheory are more tractable than they appear (Burch-Brown 2014; see also\nGreaves 2016), or that the relevant “ought implies can”\nprinciple is false (Srinivasan 2015). \n\nA variety of authors have given examples in which expected utility\ntheory seems to give the wrong prescriptions. Sections 3.2.1 and 3.2.2 discuss\nexamples where rationality seems to permit preferences inconsistent\nwith expected utility theory. These examples suggest that\nmaximizing expected utility is not necessary for\nrationality. Section 3.2.3 discusses examples where expected\nutility theory permits preferences that seem irrational. These\nexamples suggest that maximizing expected utility is not\nsufficient for rationality. Section 3.2.4 discusses an\nexample where expected utility theory requires preferences that seem\nrationally forbidden—a challenge to both the necessity and the\nsufficiency of expected utility for rationality. \n\nExpected utility theory implies that the structure of preferences\nmirrors the structure of the greater-than relation between real\nnumbers. Thus, according to expected utility theory, preferences\nmust be transitive: If \\(A\\) is preferred to \\(B\\)\n(so that \\(U(A) \\gt U(B)\\)), and\n\\(B\\) is preferred to \\(C\\) (so that \\(U(B)\n\\gt U(C)\\)), then \\(A\\) must be preferred to\n\\(C\\) (since it must be that \\(U(A) \\gt U(C)\\)).\nLikewise, preferences must be\ncomplete: for any two options, either one must be preferred to\nthe other, or the agent must be indifferent between them (since of\ntheir two utilities, either one must be greater or the two must be\nequal). But there are cases where rationality seems to permit (or\nperhaps even require) failures of transitivity and failures of\ncompleteness. \n\nAn example of preferences that are not transitive, but nonetheless\nseem rationally permissible, is Quinn's puzzle of the\nself-torturer (1990). The self-torturer is hooked up to a machine\nwith a dial with settings labeled 0 to 1,000, where setting 0 does\nnothing, and each successive setting delivers a slightly more powerful\nelectric shock. Setting 0 is painless, while setting 1,000 causes\nexcruciating agony, but the difference between any two adjacent\nsettings is so small as to be imperceptible. The dial is fitted\nwith a ratchet, so that it can be turned up but never down. \nSuppose that at each setting, the self-torturer is offered $10,000 to\nmove up to the next, so that for tolerating setting \\(n\\), he\nreceives a payoff of \\(n {\\cdot} {$10,000}\\). It is permissible for the\nself-torturer to prefer setting \\(n+1\\) to setting \\(n\\) for\neach \\(n\\) between 0 and 999 (since the difference in pain is\nimperceptible, while the difference in monetary payoffs is\nsignificant), but not to prefer setting 1,000 to setting 0\n(since the pain of setting 1,000 may be so unbearable that no amount of\nmoney will make up for it. \n\nIt also seems rationally permissible to have incomplete\npreferences. For some pairs of actions, an agent may have no\nconsidered view about which she prefers. Consider Jane, an\nelectrician who has never given much thought to becoming a professional\nsinger or a professional astronaut. (Perhaps both of these\noptions are infeasible, or perhaps she considers both of them much\nworse than her steady job as an electrician). It is false that\nJane prefers becoming a singer to becoming an astronaut, and it is\nfalse that she prefers becoming an astronaut to becoming a\nsinger. But it is also false that she is indifferent between\nbecoming a singer and becoming an astronaut. She prefers becoming\na singer and receiving a $100 bonus to becoming a singer, and if she\nwere indifferent between becoming a singer and becoming an astronaut,\nshe would be rationally compelled to prefer being a singer and\nreceiving a $100 bonus to becoming an astronaut. \n\nThere is one key difference between the two examples considered\nabove. Jane's preferences can be extended, by\nadding new preferences without removing any of the ones she has, in a\nway that lets us represent her as an expected utility maximizer. \nOn the other hand, there is no way of extended the\nself-torturer's preferences so that he can be represented as an\nexpected utility maximizer. Some of his preferences would have to\nbe altered. One popular response to incomplete preferences is to\nclaim that, while rational preferences need not satisfy the axioms of a\ngiven representation theorem (see section 2.2), it must be possible to\nextend them so that they satisfy the axioms. From this weaker\nrequirement on preferences—that they be extendible to a\npreference ordering that satisfies the relevant axioms—one can\nprove the existence halves of the relevant representation\ntheorems. However, one can no longer establish that each\npreference ordering has a representation which is unique up to\nallowable transformations. \n\nNo such response is available in the case of the self-torturer,\nwhose preferences cannot be extended to satisfy the axioms of expected\nutility theory. See the entry on\n preferences\n for a more extended discussion of the self-torturer case. \n\nAllais (1953) and Ellsberg (1961) propose examples of preferences that\ncannot be represented by an expected utility function, but that\nnonetheless seem rational. Both examples involve violations of\nSavage's Independence axiom: \nIndependence. Suppose that \\(A\\) and \\(A^*\\) are two\nacts that produce the same outcomes in the event that \\(E\\) is\nfalse. Then, for any act \\(B\\), one must have \n\nIn other words, if two acts have the same consequences whenever\n\\(E\\) is false, then the agent's preferences between those\ntwo acts should depend only on their consequences when \\(E\\) is\ntrue. On Savage's definition of expected utility, expected\nutility theory entails Independence. And on Jeffrey's\ndefinition, expected utility theory entails Independence in the\npresence of the assumption that the states are probabilistically\nindependent of the acts. \n\nThe first counterexample, the Allais Paradox, involves two separate\ndecision problems in which a ticket with a number between 1 and 100 is\ndrawn at random. In the first problem, the agent must choose\nbetween these two lotteries: \nIn the second decision problem, the agent must choose between these\ntwo lotteries: \n\nIt seems reasonable to prefer \\(A\\) (which offers a sure $100\nmillion) to \\(B\\) (where the added 10% chance at $500 million is\nmore than offset by the risk of getting nothing). It also seems\nreasonable to prefer \\(D\\) (an 10% chance at a $500 million prize)\nto \\(C\\) (a slightly larger 11% chance at a much smaller $100\nmillion prize). But together, these preferences (call them the\nAllais preferences) violate Independence. Lotteries\n\\(A\\) and \\(C\\) yield the same $100 million prize for\ntickets 12–100. They can be converted into lotteries \\(B\\)\nand \\(D\\) by replacing this $100 million prize with $0. \n\nBecause they violate Independence, the Allais preferences are\nincompatible with expected utility theory. This incompatibility does\nnot require any assumptions about the relative utilities of the $0,\nthe $100 million, and the $500 million. Where $500 million has\nutility \\(x\\), $100 million has utility \\(y\\), and $0 has utility \\(z\\),\nthe expected utilities of the lotteries are as follows. \n\nIt is easy to see that the condition under which \\(EU(A) \\gt EU(B)\\) is\nexactly the same as the condition under which \\(EU(C) \\gt EU(D)\\): both\ninequalities obtain just in case \\(0.11y \\gt 0.10x +\n0.01z\\)  \n\nThe Ellsberg Paradox also involves two decision problems that generate\na violation of the sure-thing principle. In each of them, a ball is\ndrawn from an urn containing 30 red balls, and 60 balls that are\neither white or yellow in unknown proportions. In the first decision\nproblem, the agent must choose between the following lotteries: \nIn the second decision problem, the agent must choose between the\nfollowing lotteries: \n\nIt seems reasonable to prefer \\(R\\) to \\(W\\), but at the same time prefer\n\\(WY\\) to \\(RY\\). (Call this combination of preferences the Ellsberg\npreferences.)  Like the Allais preferences, the Ellsberg\npreferences violate Independence. Lotteries\n\\(W\\) and \\(R\\) yield a $100 loss if a yellow ball is\ndrawn; they can be converted to lotteries \\(RY\\) and \\(WY\\)\nsimply by replacing this $100 loss with a sure $100 gain. \n\nBecause they violate independence, the Ellsberg preferences are\nincompatible with expected utility theory. Again, this incompatibility\ndoes not require any assumptions about the relative utilities of\nwinning $100 and losing $100. Nor do we need any assumptions about where\nbetween 0 and 1/3 the probability of drawing a yellow ball\nfalls. Where winning $100 has utility \\(w\\) and losing $100 has\nutility \\(l\\), \n\nIt is easy to see that the condition in which \n\\(EU(R) \\gt EU(W)\\) is exactly the same as the condition\nunder which \n\\(EU(RY) \\gt EU(WY)\\):\nboth inequalities obtain just in case \n\\(1/3\\,w + P(W)l \\gt 1/3\\,l + P(W)w\\).\n  \n\nThere are three notable responses to the Allais and Ellsberg\nparadoxes. First, one might follow Savage (101 ff) and Raiffa (1968,\n80–86), and defend expected utility theory on the grounds that\nthe Allais and Ellsberg preferences are irrational.   \n\nSecond, one might follow Buchak (2013) and claim that that the\nAllais and Ellsberg preferences are rationally permissible, so that\nexpected utility theory fails as a normative theory of\nrationality. Buchak develops an a more permissive theory of\nrationality, with an extra parameter representing the decision-maker's\nattitude toward risk. This risk parameter interacts with the utilities\nof outcomes and their conditional probabilities on acts to determine\nthe values of acts. One setting of the risk parameter yields expected\nutility theory as a special case, but other, “risk-averse”\nsettings rationalise the Allais preferences. \n\nThird, one might follow Loomes and Sugden (1986), Weirich (1986),\nand Pope (1995) and argue that the outcomes in the Allais and Ellsberg\nparadoxes can be re-described to accommodate the Allais and Ellsberg\npreferences. The alleged conflict between the Allais and Ellsberg\npreferences on the one hand, and expected utility theory on the other,\nwas based on the assumption that a given sum of money has the same\nutility no matter how it is obtained. Some authors challenge this\nassumption. Loomes and Sugden suggest that in addition to monetary\namounts, the outcomes of the gambles include feelings of disappointment\n(or elation) at getting less (or more) than expected. Pope\ndistinguishes “post-outcome” feelings of elation or\ndisappointment from “pre-outcome” feelings of excitement,\nfear, boredom, or safety, and points out that both may affect outcome\nutilities. Weirich suggests that the value of a monetary sum\ndepends partly on the risks that went into obtaining it, irrespective\nof the gambler's feelings, so that (for instance) $100 million as\nthe result of a sure bet is more than $100 million from a gamble that\nmight have paid nothing.  \n\nBroome (1991) raises a worry about this re-description\nsolution. Any preferences can be justified by\nre-describing the space of outcomes, thus rendering the axioms of\nexpected utility theory devoid of content. Broome rebuts this\nobjection by suggesting an additional constraint on preference: if\n\\(A\\) is preferred to \\(B\\), then \\(A\\) and \\(B\\)\nmust differ in some way that justifies preferring one to the\nother. An expected utility theorist can then count the Allais and\nEllsberg preferences as rational if, and only if, there is a\nnon-monetary difference that justifies placing outcomes of equal\nmonetary value at different spots in one's preference\nordering. \n\nAbove, we've seen purported examples of rational preferences\nthat violate expected utility theory. There are also\npurported examples of irrational preferences that satisfy expected\nutility theory. \n\nOn a typical understanding of expected utility theory, when two acts\nare tied for having the highest expected utility, agents are required\nto be indifferent between them. Skyrms (1980, p. 74) points out\nthat this view lets us derive strange conclusions about events with\nprobability 0. For instance, suppose you are about to throw a\npoint-sized dart at a round dartboard. Classical probability\ntheory countenances situations in which the dart has probability 0 of\nhitting any particular point. You offer me the following lousy\ndeal: if the dart hits the board at its exact center, then you will\ncharge me $100; otherwise, no money will change hands. My\ndecision problem can be captured with the following matrix: \n\nExpected utility theory says that it is permissible for me to accept\nthe deal—accepting has expected utility of 0. (This is so on\nboth the Jeffrey definition and the Savage definition, if we assume\nthat how the dart lands is probabilistically independent of how you\nbet.) But common sense says it is not permissible for me to accept the\ndeal. Refusing weakly dominates accepting: it yields a better\noutcome in some states, and a worse outcome in no state.  \n\nSkyrms suggests augmenting the laws of classical probability with an\nextra requirement that only impossibilities are assigned probability 0.\nEaswaran (2014) argues that we should instead reject the view that\nexpected utility theory commands indifference between acts with equal\nexpected utility. Instead, expected utility theory is not a\ncomplete theory of rationality: when two acts have the same expected\nutility, it does not tell us which to prefer. We can use\nnon-expected-utility considerations like weak dominance as\ntiebreakers. \n\nA utility function \\(U\\) is bounded above if there is a limit to how\ngood things can be according to \\(U\\), or more formally, if there is\nsome least natural number \\(sup\\) such that for every \\(A\\) in \\(U\\)'s\ndomain, \\(U(A) \\le sup\\). Likewise, \\(U\\) is bounded below if there is a\nlimit to how bad things can be according to \\(U\\), or more formally, if\nthere is some greatest natural number \\(inf\\) such that for every\n\\(A\\) in \\(U\\)'s domain, \\(U(A) \\ge inf\\). Expected utility\ntheory can run into trouble when utility functions are unbounded\nabove, below, or both. \n\nOne problematic example is the St. Petersburg game, originally\npublished by Bernoulli. Suppose that a coin is tossed until it lands\ntails for the first time. If it lands tails on the first toss, you win\n$2; if it lands tails on the second toss, you win $4; if it lands\ntails on the third toss, you win $8, and if it lands tails on the\n\\(n\\)th toss, you win $\\(2^n\\). Assuming each dollar is worth one\nutile, the expected value of the St Petersburg game is \n\nIt turns out that this sum diverges; the St Petersburg game has\ninfinite expected utility. Thus, according to expected utility\ntheory, you should prefer the opportunity to play the St Petersburg\ngame to any finite sum of money, no matter how large. \nFurthermore, since an infinite expected utility multiplied by any\nnonzero chance is still infinite, anything that has a positive\nprobability of yielding the St Petersburg game has infinite expected\nutility. Thus, according to expected utility theory, you should\nprefer any chance at playing the St Petersburg game, however\nslim, to any finite sum of money, however large. \n\nNover and Hájek (2004) argue that in addition to the\nSt. Petersburg game, which has infinite expected utility, there are\nother infinitary games whose expected utilities are undefined, even\nthough rationality mandates certain preferences among them. \n\nOne response to these problematic infinitary games is to argue that\nthe decision problems themselves are ill-posed (Jeffrey (1983, 154);\nanother is to adopt a modified version of expected utility theory that\nagrees with its verdicts in the ordinary case, but yields intuitively\nreasonable verdicts about the infinitary games (Thalos and Richardson\n2013) (Fine 2008) (Colyvan 2006, 2008) (Easwaran 2008). \n\nIn the 1940s and 50s, expected utility theory gained currency in the\nUS for its potential to provide a mechanism that would explain the\nbehavior of macro-economic variables.  As it became apparent that\nexpected utility theory did not accurately predict the behaviors of\nreal people, its proponents instead advanced the view that it might\nserve instead as a theory of how rational people should respond to\nuncertainty (see Herfeld 2017). \n\nExpected utility theory has a variety of applications in public\npolicy. In welfare economics, Harsanyi (1953) reasons from expected\nutility theory to the claim that the most socially just arrangement is\nthe one that maximizes total welfare distributed across a society\nsociety.  The theory of expected utility also has more direct\napplications.  Howard (1980) introduces the concept of\na micromort, or a one-in-a-million chance of death, and uses\nexpected utility calculations to gauge which mortality risks are\nacceptable.  In health policy, quality-adjusted life years, or QALYs,\nare measures of the expected utilities of different health\ninterventions used to guide health policy (see Weinstein et al\n2009). McAskill (2015) uses expected utility theory to address the\ncentral question of effective altruism: “How can I do\nthe most good?” (Utilties in these applications are most\nnaturally interpreted as measuring something like happiness or\nwellbeing, rather than subjective preference satisfaction for an\nindividual agent.) \n\nAnother area where expected utility theory finds applications is in\ninsurance sales. Like casinos, insurance companies take on\ncalculated risks with the aim of long-term financial gain, and must\ntake into account the chance of going broke in the short run. \n\nUtilitarians, along with their descendants contemporary\nconsequentialists, hold that the rightness or wrongness of an act is\ndetermined by the moral goodness or badness of its consequences.  Some\nconsequentialists, such as (Railton 1984), interpret this to mean that\nwe ought to do whatever will in fact have the best consequences. But\nit is difficult—perhaps impossible—to know the long-term\nconsequences of our acts (Lenman 2000, Howard-Snyder 2007).  In light\nof this observation, Jackson (1991) argues that the right act is the\none with the greatest expected moral value, not the one that will in\nfact yield the best consequences. \n\nAs Jackson notes, the expected moral value of an act depends on\nwhich probability function we work with. Jackson argues that,\nwhile every probability function is associated with an\n“ought”, the “ought” that matters most to\naction is the one associated with the decision-maker's degrees of\nbelief at the time of action. Other authors claim priority for\nother “oughts”: Mason (2013) favors the probability\nfunction that is most reasonable for the agent to adopt in response to\nher evidence, given her epistemic limitations, while Oddie and Menzies\n(1992) favor the objective chance function as a measure of objective\nrightness. (They appeal to a more complicated probability\nfunction to define a notion of “subjective rightness” for\ndecisionmakers who are ignorant of the objective chances.) \n\nStill others (Smart 1973, Timmons 2002) argue that even if that we\nought to do whatever will have the best consequences, expected utility\ntheory can play the role of a decision procedure when we are uncertain\nwhat consequences our acts will have. Feldman (2006) objects that\nexpected utility calculations are horribly impractical. In most\nreal life decisions, the steps required to compute expected utilities\nare beyond our ken: listing the possible outcomes of our acts,\nassigning each outcome a utility and a conditional probability given\neach act, and performing the arithmetic necessary to expected utility\ncalculations. \nThe expected-utility-maximizing version of consequentialism is not\nstrictly speaking a theory of rational choice. It is a theory\nof moral choice, but whether rationality requires us to do what is\nmorally best is up for debate.\n \n\nExpected utility theory can be used to address practical questions\nin epistemology. One such question is when to accept a\nhypothesis. In typical cases, the evidence is logically\ncompatible with multiple hypotheses, including hypotheses to which it\nlends little inductive support. Furthermore, scientists do not\ntypically accept only those hypotheses that are most probable given\ntheir data. When is a hypothesis likely enough to deserve\nacceptance? \n\nBayesians, such as Maher (1993), suggest that this decision be made\non expected utility grounds. Whether to accept a hypothesis is a\ndecision problem, with acceptance and rejection as acts. It can\nbe captured by the following decision matrix: \n\nOn Savage's definition, the expected utility of accepting the\nhypothesis is determined by the probability of the hypothesis, together\nwith the utilities of each of the four outcomes. (We can expect\nJeffrey's definition to agree with Savage's on the\nplausible assumption that, given the evidence in our possession, the\nhypothesis is probabilistically independent of whether we accept or\nreject it.) Here, the utilities can be understood as purely\nepistemic values, since it is epistemically valuable to believe\ninteresting truths, and to reject falsehoods. \n\nCritics of the Bayesian approach, such as Mayo (1996), object that\nscientific hypotheses cannot sensibly be given probabilities. \nMayo argues that in order to assign a useful probability to an event,\nwe need statistical evidence about the frequencies of similar\nevents. But scientific hypotheses are either true once and for\nall, or false once and for all—there is no population of worlds\nlike ours from which we can meaningfully draw statistics. Nor can\nwe use subjective probabilities for scientific purposes, since this\nwould be unacceptably arbitrary. Therefore, the expected\nutilities of acceptance and rejection are undefined, and we ought to\nuse the methods of traditional statistics, which rely on comparing the\nprobabilities of our evidence conditional on each of the\nhypotheses. \n\nExpected utility theory also provides guidance about when to gather\nevidence. Good (1967) argues on expected utility grounds that it is\nalways rational to gather evidence before acting, provided that\nevidence is free of cost. The act with the highest expected utility\nafter the extra evidence is in will always be always at least as good\nas the act with the highest expected utility beforehand.  \n\nIn epistemic decision theory, expected utilities are used to\nassess belief states as rational or irrational. If we think of belief\nformation as a mental act, facts about the contents of the agent's\nbeliefs as events, and closeness to truth as a desirable feature of\noutcomes, then we can use expected utility theory to evaluate degrees\nof belief in terms of their expected closeness to truth.  The entry\non epistemic utility arguments for\nprobabilism includes an overview of expected utility arguments for\na variety of epistemic norms, including conditionalization and the\nPrincipal Principle.\n \n\nKaplan (1968), argues that expected utility considerations can be used\nto fix a standard of proof in legal trials.  A jury deciding whether\nto acquit or convict faces the following decision problem: \nKaplan shows that \\(EU(convict) > EU(acquit)\\) whenever  \n\nQualitatively, this means that the standard of proof increases as the\ndisutility of convicting an innocent person\n(\\(U(\\mathrm{true~conviction})-U(\\mathrm{false~acquittal})\\))\nincreases, or as the disutility of acquitting a guilty person\n(\\(U(\\mathrm{true~acquittal})-U(\\mathrm{false~conviction})\\))\ndecreases. \n\nCritics of this decision-theoretic approach, such as Laudan (2006),\nargue that it’s difficult or impossible to bridge the gap between the\nevidence admissible in court, and the real probability of the\ndefendant’s guilt. The probability guilt depends on three factors: the\ndistribution of apparent guilt among the genuinely guilty, the\ndistribution of apparent guilt among the genuinely innocent, and the\nratio of genuinely guilty to genuinely innocent defendants who go to\ntrial (see Bell 1987).  Obstacles to calculating any of these factors\nwill block the inference from a judge or jury’s perception of apparent\nguilt to a true probability of guilt.","contact.mail":"formal.epistemology@gmail.com","contact.domain":"gmail.com"}]
