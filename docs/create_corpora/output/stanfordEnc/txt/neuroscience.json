[{"date.published":"1999-06-07","date.changed":"2019-08-06","url":"https://plato.stanford.edu/entries/neuroscience/","author1":"John Bickle","author2":"Anthony Landreth","author1.info":"http://www.philosophyandreligion.msstate.edu/faculty/bickle.php","author2.info":"http://www.petemandik.com/philosophy/philosophy.html","entry":"neuroscience","body.text":"\n\n\nOver the past four decades, philosophy of science has grown\nincreasingly “local”. Concerns have switched from general\nfeatures of scientific practice to concepts, issues, and puzzles\nspecific to particular disciplines. Philosophy of neuroscience is one\nnatural result. This emerging area was also spurred by remarkable\ngrowth in the neurosciences themselves. Cognitive and computational\nneuroscience continues to encroach directly on issues traditionally\naddressed within the humanities, including the nature of\nconsciousness, action, knowledge, and normativity. Cellular,\nmolecular, and behavioral neuroscience using animal models\nincreasingly encroaches on cognitive neuroscience’s domain.\nEmpirical discoveries about brain structure and function suggest ways\nthat “naturalistic” programs might develop in detail,\nbeyond the abstract philosophical considerations in their favor.\n\n\nThe literature has distinguished “philosophy of\nneuroscience” from “neurophilosophy” for two\ndecades. The former concerns foundational issues within the\nneurosciences. The latter concerns application of neuroscientific\nconcepts to traditional philosophical questions. Exploring various\nconcepts of representation employed in neuroscientific theories is an\nexample of the former. Examining implications of neurological\nsyndromes for the concept of a unified self is an example of the\nlatter. In this entry, we will develop this distinction further and\ndiscuss examples of both. Just as has happened in the field’s\nhistory, work in both of these areas is scattered throughout most all\nsections below. Throughout we will try to specify which area landmark\nwork falls into, when this location isn’t obvious.\n\n\nOne exciting aspect about working in philosophy of neuroscience or\nneurophilosophy is the continual element of surprise. Both fields\ndepend squarely on developments in neuroscience, and one simply has no\ninkling what’s coming down the pike in that incredibly\nfast-moving science. Last year’s speculative fiction is this\nyear’s scientific reality. But this feature makes a once-a-half\ndecade updated encyclopedia entry difficult to manage. The scientific\ndetails philosophers were reflecting on at past updates can now read\nwoefully dated. Yet one also wants to capture some history of the\nongoing fields. Our solution to this dilemma has been to keep previous\ndiscussions, to reflect that history, but to add more recent\nscientific and philosophical updates, not only to sections of this\nentry added at later times, but also peppered through the earlier\ndiscussions. It’s not always a perfect solution, but it does\npreserve something of the history of the philosophy of neuroscience\nand neurophilosophy against the continual advances in the sciences\nthese philosophical fields depend upon. \n\nHistorically, neuroscientific discoveries exerted little influence on\nthe details of materialist philosophies of mind. The\n“neuroscientific milieu” of the past half-century has made\nit harder for philosophers to adopt substantive dualisms about mind.\nBut even the “type-type” or “central state”\nidentity theories that rose to brief prominence in the late 1950s\n(Place 1956; Smart 1959) drew upon few actual details of the emerging\nneurosciences. Recall the favorite early example of a psychoneural\nidentity claim: “pain is identical to C-fiber firing”. The\n“C-fibers” turned out to be related to only a single\naspect of pain transmission (Hardcastle 1997). Early identity\ntheorists did not emphasize psychoneural identity hypotheses. Their\n“neuro” terms were admittedly placeholders for concepts\nfrom future neuroscience. Their arguments and motivations were\nphilosophical, even if the ultimate justification of the program was\nheld to be empirical. \nThe apology offered by early identity theorists for ignoring\nscientific details was that the neuroscience at that time was too\nnascent to provide any plausible identities. But potential identities\nwere afoot. David Hubel and Torsten Wiesel’s (1962)\nelectrophysiological demonstrations of the receptive field properties\nof visual neurons had been reported with great fanfare. Using their\ntechniques, neurophysiologists began discovering neurons throughout\nvisual cortex responsive to increasingly abstract features of visual\nstimuli: from edges to motion direction to colors to properties of\nfaces and hands. More notably, Donald Hebb had published The\nOrganization of Behavior (1949) more than a decade earlier. He\nhad offered detailed explanations of psychological phenomena in terms\nof neural mechanisms and anatomical circuits. His psychological\nexplananda included features of perception, learning, memory, and even\nemotional disorders. He offered these explanations as potential\nidentities. (See the Introduction to his 1949). One philosopher who\ndid take note of some available neuroscientific detail at the time was\nBarbara Von Eckardt Klein (Von Eckardt Klein 1975). She discussed the\nidentity theory with respect to sensations of touch and pressure, and\nincorporated then-current hypotheses about neural coding of sensation\nmodality, intensity, duration, and location as theorized by\nMountcastle, Libet, and Jasper. Yet she was a glaring exception. By\nand large, available neuroscience at the time was ignored by both\nphilosophical friends and foes of early identity theories. \nPhilosophical indifference to neuroscientific detail became\n“principled” with the rise and prominence of functionalism\nin the 1970s. The functionalists’ favorite argument was based on\nmultiple realizability: a given mental state or event can be realized\nin a wide variety of physical types (Putnam 1967; Fodor 1974).\nConsequently, a detailed understanding of one type of realizing\nphysical system (e.g., brains) will not shed light on the fundamental\nnature of mind. Psychology is thus autonomous from any science of one\nof its possible physical realizers (see the\n entry on multiple realizability\n in this Encyclopedia). Instead of neuroscience, scientifically-minded\nphilosophers influenced by functionalism sought evidence and\ninspiration from cognitive psychology and artificial intelligence.\nThese disciplines abstract away from underlying physical mechanisms\nand emphasize the “information-bearing” properties and\ncapacities of representations (Haugeland 1985). At this same time,\nhowever, neuroscience was delving directly into cognition, especially\nlearning and memory. For example, Eric Kandel (1976) proposed\npresynaptic mechanisms governing transmitter release rate as a\ncell-biological explanation of simple forms of associative learning.\nWith Robert Hawkins (Hawkins and Kandel 1984) he demonstrated how\ncognitivist aspects of associative learning (e.g., blocking,\nsecond-order conditioning, overshadowing) could be explained\ncell-biologically by sequences and combinations of these basic forms\nimplemented in higher neural anatomies. Working on the post-synaptic\nside, neuroscientists began unraveling the cellular mechanisms of long\nterm potentiation (LTP; Bliss and Lomo 1973). Physiological\npsychologists quickly noted its explanatory potential for various\nforms of learning and\n memory.[1]\n Yet few “materialist” philosophers paid any attention.\nWhy should they? Most were convinced functionalists. They believed\nthat the “implementation level” details might be important\nto the clinician, but were irrelevant to the theorist of mind. \nA major turning point in philosophers’ interest in neuroscience\ncame with the publication of Patricia Churchland’s\nNeurophilosophy (1986). The Churchlands (Patricia and Paul)\nwere already notorious for advocating eliminative materialism (see the\nnext section). In her (1986) book, Churchland distilled eliminativist\narguments of the past decade, unified the pieces of the philosophy of\nscience underlying them, and sandwiched the philosophy between a\nfive-chapter introduction to neuroscience and a 70-page chapter on\nthree then-current theories of brain function. She was unapologetic\nabout her intent. She was introducing philosophy of science to\nneuroscientists and neuroscience to philosophers. Nothing could be\nmore obvious, she insisted, than the relevance of empirical facts\nabout how the brain works to concerns in the philosophy of mind. Her\nterm for this interdisciplinary method was “co-evolution”\n(borrowed from biology). This method seeks resources and ideas from\nanywhere on the theory hierarchy above or below the question at issue.\nStanding on the shoulders of philosophers like Quine and Sellars,\nChurchland insisted that specifying some point where neuroscience ends\nand philosophy of science begins is hopeless because the boundaries\nare poorly defined. Neurophilosophers would pick and choose resources\nfrom both disciplines as they saw fit. \nThree themes predominated Churchland’s philosophical discussion:\ndeveloping an alternative to the logical empiricist theory of\nintertheoretic reduction; responding to property-dualistic arguments\nbased on subjectivity and sensory qualia; and responding to\nanti-reductionist multiple realizability arguments. These projects\nremained central to neurophilosophy for more than a decade after\nChurchland’s book appeared. John Bickle (1998) extended the\nprincipal insight of Clifford Hooker’s (1981a,b,c)\npost-empiricist theory of intertheoretic reduction. He quantified key\nnotions using a model-theoretic account of theory structure adapted\nfrom the structuralist program in philosophy of science (Balzer,\nMoulines, and Sneed 1987). He also made explicit a form of argument to\ndraw ontological conclusions (cross-theoretic identities, revisions,\nor eliminations) from the nature of the intertheoretic reduction\nrelations obtaining in specific cases. For example, it is routinely\nconcluded that visible light, a theoretical posit of optics, is\nelectromagnetic radiation within specified wavelengths, a theoretical\nposit of electromagnetism; in this case, a cross-theoretic ontological\nidentity. It is also routine to conclude that phlogiston does not\nexist: an elimination of a kind from our scientific ontology. Bickle\nexplicated the nature of the reduction relation in a specific case\nusing a semi-formal account of “intertheoretic\napproximation” inspired by structuralist results. \nPaul Churchland (1996) carried on the attack on property-dualistic\narguments for the irreducibility of conscious experience and sensory\nqualia. He argued that acquiring some knowledge of existing sensory\nneuroscience increases one’s ability to “imagine” or\n“conceive of” a comprehensive neurobiological explanation\nof consciousness. He defended this conclusion using a\ncharacteristically imaginative thought-experiment based on the history\nof optics and electromagnetism. \nFinally, criticisms of the multiple realizability argument\nflourish—and are challenged—to the present day. Although\nthe multiple realizability argument remains influential among\nnonreductive physicalists, it no longer commands the near-universal\nacceptance it once did. Replies to the multiple realizability argument\nbased on neuroscientific details have appeared. For example, William\nBechtel and Jennifer Mundale (1999) argue that neuroscientists use\npsychological criteria in brain mapping studies. This fact undercuts\nthe likelihood that psychological kinds are multiply realized (for a\nreview of recent developments see the\n entry on multiple realizability\n in this Encyclopedia). \nEliminative materialism (EM), in the form advocated most aggressively\nby Paul and Patricia Churchland, is the conjunction of two claims.\nFirst, our common sense “belief-desire” conception of\nmental events and processes, our “folk psychology”, is a\nfalse and misleading account of the causes of human behavior. Second,\nlike other false conceptual frameworks from both folk theory and the\nhistory of science, it will be replaced by, rather than smoothly\nreduced or incorporated into, a future neuroscience. The\nChurchlands’ characterized folk psychology as the collection of\ncommon homilies invoked (mostly implicitly) to explain human behavior\ncausally. You ask why Marica is not accompanying me this evening. I\nreply that our grandson needed sitting. You nod sympathetically. You\nunderstand my explanation because you share with me a generalization\nthat relates beliefs about taking care of grandchildren, desires to\nhelp daughters and to spend time with grandchildren compared to\nenjoying a night out, and so on. This is just one of a huge collection\nof homilies about the causes of human behavior that EM claims to be\nflawed beyond potential revision. Although this example involves only\nbeliefs and desires, folk psychology contains an extensive repertoire\nof propositional attitudes in its explanatory nexus: hopes,\nintentions, fears, imaginings, and more. EMists predict that a future,\ngenuinely scientific psychology or neuroscience will eventually eschew\nall of these, and replace them with incommensurable states and\ndynamics of neuro-cognition. \nEM is physicalist in one traditional philosophical sense. It\npostulates that some future brain science will be ultimately the\ncorrect account of (human) behavior. It is eliminative in predicting\nthe future rejection of folk psychological kinds from our\npost-neuroscientific ontology. EM proponents often employ scientific\nanalogies (Feyerabend 1963; Paul Churchland, 1981). Oxidative\nreactions as characterized within elemental chemistry bear no\nresemblance to phlogiston release. Even the “direction” of\nthe two processes differ. Oxygen is gained when an object burns (or\nrusts), phlogiston was said to be lost. The result of this theoretical\nchange was the elimination of phlogiston from our scientific ontology.\nThere is no such thing. For the same reasons, according to EM,\ncontinuing development in neuroscience will reveal that there are no\nsuch things as beliefs, desires, and the rest of the propositional\nattitudes as characterized by common sense. \nHere we focus only on the way that neuroscientific results have shaped\nthe arguments for EM. Surprisingly, only one argument has been\nstrongly influenced. (Most arguments for EM stress failures of folk\npsychology as an explanatory theory of behavior.) This argument is\nbased on a development in cognitive and computational neuroscience\nthat might provide a genuine alternative to the representations and\ncomputations implicit in folk psychological generalizations. Many\neliminative materialists assume that folk psychology is committed to\npropositional representations and computations over their contents\nthat mimic logical inferences (Paul Churchland 1981; Stich 1983;\nPatricia Churchland\n 1986).[2]\n Even though discovering an alternative to this view has been an\neliminativist goal for some time, some eliminativists hold that\nneuroscience only began delivering this alternative over the past\nthirty years. Points in and trajectories through vector spaces, as an\ninterpretation of synaptic events and neural activity patterns in\nbiological and artificial neural networks are the key features of this\nalternative. The differences between these notions of cognitive\nrepresentation and transformations, and those of the propositional\nattitudes of folk psychology, provide the basis for one argument for\nEM (Paul Churchland 1987). However, this argument will be opaque to\nthose with no background in cognitive and computational neuroscience,\nso we present a few details. With these details in place, we will\nreturn to this argument for EM (five paragraphs below). \nAt one level of analysis, the basic computational element of a neural\nnetwork, biological or artificial, is the nerve cell, or neuron.\nMathematically, neurons can be represented as simple computational\ndevices, transforming inputs into output. Both inputs and outputs\nreflect biological variables. For our discussion, we assume that\nneuronal inputs are frequencies of action potentials (neuronal\n“spikes”) in the axons whose terminal branches synapse\nonto the neuron in question, while neuronal output is the frequency of\naction potentials generated in its axon after processing the inputs. A\nneuron thereby computes its total input, usually treated\nmathematically as the sum of the products of the signal strength along\neach input line times the synaptic weight on that line. It then\ncomputes a new activation state based on its total input and current\nactivation state, and a new output state based on its new activation\nvalue. The neuron’s output state is transmitted as a signal\nstrength to whatever neurons its axon synapses on. The output state\nreflects systematically the neuron’s new activation\n state.[3] \nAnalyzed in this fashion, both biological and artificial neural\nnetworks are interpreted naturally as vector-to-vector\ntransformers. The input vector consists of values reflecting\nactivity patterns in axons synapsing on the network’s neurons\nfrom outside (e.g., from sensory transducers or other neural\nnetworks). The output vector consists of values reflecting the\nactivity patterns generated in the network’s neurons that\nproject beyond the net (e.g., to motor effectors or other neural\nnetworks). Given that each neuron’s activity depends partly upon\ntheir total input, and its total input depends partly on synaptic\nweights (e.g., presynaptic neurotransmitter release rate, number and\nefficacy of postsynaptic receptors, availability of enzymes in\nsynaptic cleft), the capacity of biological networks to change their\nsynaptic weights make them plastic vector-to-vector\ntransformers. In principle, a biological network with plastic synapses\ncan come to implement any vector-to-vector transformation that its\ncomposition permits (number of input units, output units, processing\nlayers, recurrency, cross-connections, etc.) (discussed in Paul\nChurchland, 1987, with references to the primary scientific\nliterature). Figure 1. \nThe anatomical organization of the cerebellum provides a clear example\nof a network amenable to this computational interpretation. Consider\n Figure 1.\n The cerebellum is the bulbous convoluted structure dorsal to the\nbrainstem. A variety of studies (behavioral, neuropsychological,\nsingle-cell electrophysiological) implicate this structure in motor\nintegration and fine motor coordination. Mossy fibers (axons) from\nneurons outside the cerebellum synapse on cerebellar granule cells,\nwhich in turn project to parallel fibers. Activity patterns across the\ncollection of mossy fibers (frequency of action potentials per time\nunit in each fiber projecting into the cerebellum) provide values for\nthe input vector. Parallel fibers make multiple synapses on the\ndendritic trees and cell bodies of cerebellular Purkinje neurons. Each\nPurkinje neuron “sums” its post-synaptic potentials (PSPs)\nand emits a train of action potentials down its axon based (partly) on\nits total input and previous activation state. Purkinje axons project\noutside the cerebellum. The network’s output vector is thus the\nordered values representing the pattern of activity generated in each\nPurkinje axon. Changes to the efficacy of individual synapses on the\nparallel fibers and the Purkinje neurons alter the resulting PSPs in\nPurkinje axons, generating different axonal spiking frequencies.\nComputationally, this amounts to a different output vector to the same\ninput activity\n pattern—plasticity.[4] Figure 2. \nThis interpretation puts the useful mathematical resources of\ndynamical systems into the hands of computational\nneuroscientists. Vector spaces are an example. Learning can\nthen be characterized fruitfully in terms of changes in synaptic\nweights in the network and subsequent reduction of error in network\noutput. (This approach to learning goes back to Hebb 1949, although\nthe vector-space interpretation was not part of Hebb’s account.)\nA useful representation of this account uses a synaptic\nweight-error space. One dimension represents the global error in\nthe network’s output to a given task, and all other dimensions\nrepresent the weight values of individual synapses in the network.\nConsider\n Figure 2.\n Points in this multi-dimensional state space represent the global\nperformance error correlated with each possible collection of synaptic\nweights in the network. As the weights change with each performance,\nin accordance with a biologically-inspired learning algorithm, the\nglobal error of network performance continually decreases. The\nchanging synaptic weights across the network with each training\nepisode reduces the total error of the network’s output vector,\ncompared to the desired output vector for the input vector. Learning\nis represented as synaptic weight changes correlated with a descent\nalong the error dimension in the space (Churchland and Sejnowski\n1992). Representations (concepts) can be portrayed as\npartitions in multi-dimensional vector spaces. One example is\na neuron activation vector space. See\n Figure 3.\n A graph of such a space contains one dimension for the activation\nvalue of each neuron in the network (or some specific subset of the\nnetwork’s neurons, such as those in a specific layer). A point\nin this space represents one possible pattern of activity in all\nneurons in the network. Activity patterns generated by input vectors\nthat the network has learned to group together will cluster around a\n(hyper-) point or subvolume in the activity vector space. Any input\npattern sufficiently similar to this group will produce an activity\npattern lying in geometrical proximity to this point or subvolume.\nPaul Churchland (1989) argued that this interpretation of network\nactivity provided a quantitative, neurally-inspired basis for\nprototype theories of concepts developed in late-twentieth century\ncognitive psychology. Figure 3. \nUsing this theoretical development, and in the realm of\nneurophilosophy, Paul Churchland (1987, 1989) offered a novel,\nneuroscientifically-inspired argument for EM. According to the\ninterpretation of neural networks just sketched, activity vectors are\nthe central kind of representations, and vector-to-vector\ntransformations are the central kind of computations, in the brain.\nThis contrasts sharply with the propositional representations\nand logical/semantic computations postulated by folk\npsychology. Vectorial content, an ordered sequence of real numbers, is\nunfamiliar and alien to common sense. This cross-theoretic conceptual\ndifference is at least as great as that between oxidative and\nphlogiston concepts, or kinetic-corpuscular and caloric fluid heat\nconcepts. Phlogiston and caloric fluid are two “parade”\nexamples of kinds eliminated from our scientific ontology due to the\nnature of the intertheoretic relation obtaining between the theories\nwith which they are affiliated and the theories that replaced them.\nThe structural and dynamic differences between the folk psychological\nand then-emerging cognitive neuroscientific kinds suggested that the\ntheories affiliated with the latter will likewise replace the theory\naffiliated with the former. But this claim was the key premise of the\neliminativist argument based on predicted intertheoretic relations.\nAnd with the rise of neural networks and parallel distributed\nprocessing, intertheoretic contrasts with folk-psychological\nexplanatory kinds were no longer just an eliminativist’s future\nhope. Computational and cognitive neuroscience was delivering an\nalternative kinematics for cognition, one that provided no structural\nanalogue for folk psychology’s propositional attitudes or\nlogic-like computations over propositional contents. \nCertainly the vector-space alternatives of this interpretation of\nneural networks are alien to folk psychology. But do they justify EM?\nEven if the propositional contents of folk-psychological posits find\nno analogues in one theoretical development in cognitive and\ncomputational neuroscience (that was hot three decades ago), there\nmight be other aspects of cognition that folk psychology gets right.\nWithin the scientific realism that informed early neurophilosophy,\nconcluding that a cross-theoretic identity claim is true (e.g., folk\npsychological state F is identical to neural state N) or that an\neliminativist claim is true (there is no such thing as folk\npsychological state F) depended on the nature of the intertheoretic\nreduction obtaining between the theories affiliated with the posits in\nquestion (Hooker 1981a,b,c; Churchland 1986; Bickle, 1998). But the\nunderlying account of intertheoretic reduction also recognized a\nspectrum of possible reductions, ranging from relatively\n“smooth” through “significantly revisionary”\nto “extremely\n bumpy”.[5]\n Might the reduction of folk psychology to a “vectorial”\ncomputational neuroscience occupy some middle ground between\n“smooth” and “bumpy” intertheoretic reduction\nendpoints, and hence suggest a “revisionary” conclusion?\nThe reduction of classical equilibrium thermodynamics-to-statistical\nmechanics provided a potential analogy here. John Bickle (1992, 1998,\nchapter 6) argued on empirical grounds that such an outcome is likely.\nHe specified conditions on “revisionary” reductions from\nhistorical examples and suggested that these conditions are obtaining\nbetween folk psychology and cognitive neuroscience as the latter\ndevelops. In particular, folk psychology appears to have gotten right\nthe grossly-specified functional profile of many cognitive states,\nespecially those closely related to sensory inputs and behavioral\noutputs. It also appears to get right the “intentionality”\nof many cognitive states—the object that the state is of or\nabout—even though cognitive neuroscience eschews its implicit\nlinguistic explanation of this feature. Revisionary physicalism\npredicts significant conceptual change to folk psychological\nconcepts, but denies total elimination of the caloric fluid-phlogiston\nvariety. \nThe philosophy of science is another area where vector space\ninterpretations of neural network activity patterns has impacted\nphilosophy. In the Introduction to his (1989) book, A\nNeurocomputational Perspective, Paul Churchland asserted,\ndistinctively neurophilosophically, that it will soon be impossible to\ndo serious work in the philosophy of science without drawing on\nempirical work in the brain and behavioral sciences. To justify this\nclaim, in Part II of the book he suggested neurocomputational\nreformulations of key concepts from the philosophy of science. At the\nheart of his reformulations is a neurocomputational account of the\nstructure of scientific theories (1989: chapter 9). Problems with the\northodox “sets-of-sentences” view of scientific theories\nhave been well-known since the 1960s. Churchland advocated replacing\nthe orthodox view with one inspired by the “vectorial”\ninterpretation of neural network activity. Representations implemented\nin neural networks (as sketched above) compose a system that\ncorresponds to important distinctions in the external environment, are\nnot explicitly represented as such within the input corpus, and allow\nthe trained network to respond to inputs in a fashion that continually\nreduces error. According to Churchland, these are functions of\ntheories. Churchland was bold in his assertion: an individual’s\ntheory-of-the-world is a specific point in that individual’s\nerror-synaptic weight vector space. It is a configuration of synaptic\nweights that partitions the individual’s activation vector space\ninto subdivisions that reduce future error messages to both familiar\nand novel inputs. (Consider again\n Figure 2\n and\n Figure 3.)\n This reformulation invites an objection, however. Churchland boasts\nthat his theory of theories is preferable to existing alternatives to\nthe orthodox “sets-of-sentences” account—for\nexample, the semantic view (Suppe 1974; van Fraassen\n1980)—because his is closer to the “buzzing brains”\nthat use theories. But as Bickle (1993) noted, neurocomputational models based on the\nmathematical resources described above are a long way into the realm\nof mathematical abstraction. They are little more than novel (albeit\nsuggestive) application of the mathematics of quasi-linear dynamical\nsystems to simplified schemata of brain circuitries. Neurophilosophers\nowe some account of identifications across ontological categories\n(vector representations and transformation to what?) before the\nphilosophy of science community will treat theories as points in\nhigh-dimensional state spaces implemented in biological neural\nnetworks. (There is an important methodological assumption lurking in\nBickle’s objection, however, which we will discuss toward the\nend of the next paragraph.) \nChurchland’s neurocomputational reformulations of other\nscientific and epistemological concepts build on this account of\ntheories. He sketches “neuralized” accounts of the\ntheory-ladenness of perception, the nature of concept unification, the\nvirtues of theoretical simplicity, the nature of Kuhnian paradigms,\nthe kinematics of conceptual change, the character of abduction, the\nnature of explanation, and even moral knowledge and epistemological\nnormativity. Conceptual redeployment, for example, is the activation\nof an already-existing prototype representation—the centerpoint\nor region of a partition of a high-dimensional vector space in a\ntrained neural network—by a novel type of input pattern.\nObviously, we can’t here do justice to Churchland’s many\nand varied attempts at reformulation. We urge the intrigued reader to\nexamine his suggestions in their original form. But a word about\nphilosophical methodology is in order. Churchland is not\nattempting “conceptual analysis” in anything resembling\nits traditional philosophical sense. Neither, typically, are\nneurophilosophers in any of their reformulation projects. (This is why\na discussion of neurophilosophical reformulations fits with a\ndiscussion of EM.) There are philosophers who take the\ndiscipline’s ideal analyses to be a relatively simple set of\nnecessary and sufficient conditions, expressed in non-technical\nnatural language, governing the application of important concepts\n(like justice, knowledge, theory, or explanation). These analyses\nshould square, to the extent possible, with pretheoretical usage.\nIdeally, they should preserve synonymy. Other philosophers view this\nideal as sterile, misguided, and perhaps deeply mistaken about the\nunderlying structure of human knowledge (Ramsey 1992).\nNeurophilosophers tend to reside in the latter group. Those who\ndislike philosophical speculation about the promise and potential of\ndeveloping science to reformulate\n(“reform-ulate”) traditional philosophical\nconcepts have probably already discovered that neurophilosophy is not\nfor them. But the familiar charge that neurocomputational\nreformulations of the sort Churchland attempts are\n“philosophically uninteresting” or\n“irrelevant” because they fail to provide\n“analyses” of theory, explanation, and the like will fall\non deaf ears among many contemporary “naturalistic”\nphilosophers, who have by and large given up on traditional\nphilosophical “analysis”. \nBefore we leave the topic of proposed neurophilosophical applications\nof this theoretical development from “neural\nnetworks”-style cognitive/computational neuroscience, one final\npoint of actual scientific detail bears mention. This approach did not\nremain state-of-the-art computational neuroscience for long.\nMany neural modelers quickly gave up this approach to\nmodeling the brain. Compartmental modeling enabled\ncomputational neuroscientists to mimic activity in and\ninteractions between patches of neuronal membrane (Bower and Beeman\n1995). This approach permitted modelers to control and manipulate a\nvariety of subcellular factors that determine action potentials per\ntime unit, including the topology of membrane structure in individual\nneurons, variations in ion channels across membrane patches, and field\nproperties of post-synaptic potentials depending on the location of\nthe synapse on the dendrite or soma. By the mid-1990s modelers quickly\nbegan to “custom build” the neurons in their target\ncircuitry. Increasingly powerful computer hardware still allowed them\nto study circuit properties of modeled networks. For these reasons,\nmany serious computational neuroscientists switched to working at a\nlevel of analysis that treats neurons as structured rather than simple\ncomputational devices. With compartmental modeling, vector-to-vector\ntransformations came to be far less useful in serious neurobiological\nmodels, replaced by differential equations representing ion currents\nacross patches of neural membrane. Far more biological detail came to\nbe captured in the resulting models than “connectionist”\nmodels permitted. This methodological change across computational\nneuroscience meant that a neurophilosophy guided by\n“connectionist” resources no longer drew from the state of\nthe art of the scientific field. \nPhilosophy of science and scientific epistemology were not the only\nareas where neurophilosophers urged the relevance of neuroscientific\ndiscoveries for traditionally philosophical topics. A decade after\nNeurophilosophy’s publication, Kathleen Akins (1996)\nargued that a “traditional” view of the senses underlies a\nvariety of sophisticated “naturalistic” programs about\nintentionality. (She cites the Churchlands, Daniel Dennett, Fred\nDretske, Jerry Fodor, David Papineau, Dennis Stampe, and Kim Sterelny\nas examples.) But then-recent neuroscientific work on the mechanisms\nand coding strategies implemented by sensory receptors shows that this\ntraditional view is mistaken. The traditional view holds that sensory\nsystems are “veridical” in at least three ways. (1) Each\nsignal in the system correlates with a small range of properties in\nthe external (to the body) environment. (2) The structure in the\nrelevant external relations that the receptors are sensitive to is\npreserved in the structure of the internal relations among the\nresulting sensory states. And (3) the sensory system reconstructs\nfaithfully, without fictive additions or embellishments, the external\nevents. Using then-recent neurobiological discoveries about response\nproperties of thermal receptors in the skin (i.e.,\n“thermoreceptors”) as an illustration, Akins showed that\nsensory systems are “narcissistic” rather than\n“veridical”. All three traditional assumptions are\nviolated. These neurobiological details and their philosophical\nimplications open novel questions for the philosophy of perception and\nfor the appropriate foundations for naturalistic projects about\nintentionality. Armed with the known neurophysiology of sensory\nreceptors, our “philosophy of perception” or account of\n“perceptual intentionality” will no longer focus on the\nsearch for correlations between states of sensory systems and\n“veridically detected” external properties. This\ntraditional philosophical (and scientific!) project rests upon a\nmistaken “veridicality” view of the senses.\nNeuroscientific knowledge of sensory receptor activity also shows that\nsensory experience does not serve the naturalist well as a\n“simple paradigm case” of an intentional relation between\nrepresentation and world. Once again, available scientific detail\nshowed the naivety of some traditional philosophical projects. \nFocusing on the anatomy and physiology of the pain transmission\nsystem, Valerie Hardcastle (1997) urged a similar negative implication\nfor a popular methodological assumption. Pain experiences have long\nbeen philosophers’ favorite cases for analysis and theorizing\nabout conscious experiences generally. Nevertheless, every position\nabout pain experiences has been defended: eliminativism, a variety of\nobjectivist views, relational views, and subjectivist views. Why so\nlittle agreement, despite agreement that pain experiences are the\nplace to start an analysis or theory of consciousness? Hardcastle\nurged two answers. First, philosophers tend to be uninformed about the\nneuronal complexity of our pain transmission systems, and build their\nanalyses or theories on the outcome of a single component of a\nmulti-component system. Second, even those who understand some of the\nunderlying neurobiology of pain tend to advocate gate-control\n theories.[6]\n But the best existing gate-control theories are vague about the\nneural mechanisms of the gates. Hardcastle instead proposed a\ndissociable dual system of pain transmission, consisting of a pain\nsensory system closely analogous in its neurobiological implementation\nto other sensory systems, and a descending pain inhibitory system. She\nargued that this dual system is consistent with neuroscientific\ndiscoveries and accounts for all the pain phenomena that have tempted\nphilosophers toward particular (but limited) theories of pain\nexperience. The neurobiological uniqueness of the pain inhibitory\nsystem, contrasted with the mechanisms of other sensory modalities,\nrenders pain processing atypical. In particular, the pain inhibitory\nsystem dissociates pain sensation from stimulation of nociceptors\n(pain receptors). Hardcastle concluded from the neurobiological\nuniqueness of pain transmission that pain experiences are atypical\nconscious events, and hence not a good place to start theorizing about\nor analyzing the general type. \nDeveloping and defending theories of content is a central topic in\ncontemporary philosophy of mind. A common desideratum in this debate\nis a theory of cognitive representation consistent with a physical or\nnaturalistic ontology. We’ll here describe a few contributions\nneurophilosophers have made to this project. \nWhen one perceives or remembers that he is out of coffee, his brain\nstate possesses intentionality or “aboutness”. The percept\nor memory is about one’s being out of coffee; it represents one\nas being out of coffee. The representational state has content. A\npsychosemantics seeks to explain what it is for a representational\nstate to be about something, to provide an account of how states and\nevents can have specific representational content. A physicalist\npsychosemantics seeks to do this using resources of the physical\nsciences exclusively. Neurophilosophers have contributed to two types\nof physicalist psychosemantics: the Functional Role approach and the\nInformational approach. For a description of these and other theories\nof mental content, see the entries on\n causal theories of mental content,\n mental representation, and\n teleological theories of mental content. \nThe core claim of a functional role semantics is that a representation\nhas its specific content in virtue of relations it bears to other\nrepresentations. Its paradigm application is to concepts of\ntruth-functional logic, like the conjunctive “and” or\ndisjunctive “or”. A physical event instantiates the\n“and” function just in case it maps two true inputs onto a\nsingle true output. Thus it is the relations an expression bears to\nothers that give it the semantic content of “and”.\nProponents of functional role semantics propose similar analyses for\nthe content of all representations (Block 1995). A physical event\nrepresents birds, for example, if it bears the right relations to\nevents representing feathers and others representing beaks. By\ncontrast, informational semantics ascribe content to a state depending\nupon the causal relations obtaining between the state and the object\nit represents. A physical state represents birds, for example, just in\ncase an appropriate causal relation obtains between it and birds. At\nthe heart of informational semantics is a causal account of\ninformation (Dretske 1981, 1988). Red spots on a face carry the\ninformation that one has measles because the red spots are caused by\nthe measles virus. A common criticism of informational semantics holds\nthat mere causal covariation is insufficient for representation, since\ninformation (in the causal sense) is by definition always veridical\nwhile representations can misrepresent. A popular solution to this\nchallenge invokes a teleological analysis of “function”. A\nbrain state represents X by virtue of having the function of\ncarrying information about being caused by X (Dretske 1988).\nThese two approaches do not exhaust the popular options for a\npsychosemantics, but are the ones to which neurophilosophers have most\ncontributed. \nPaul Churchland’s allegiance to functional role semantics goes\nback to his earliest views about the semantics of terms in a language.\nIn his (1979) book, he insisted that the semantic identity (content)\nof a term derives from its place in the network of sentences of the\nentire language. The functional economies envisioned by early\nfunctional role semanticists were networks with nodes corresponding to\nthe objects and properties denoted by expressions in a language. Thus\none node, appropriately connected, might represent birds, another\nfeathers, and another beaks. Activation of one of these would tend to\nspread activation to the others. As “connectionist” neural\nnetwork modeling developed (as discussed in the previous section\nabove), alternatives arose to this one-representation-per-node\n“localist” approach. By the time Churchland (1989)\nprovided a neuroscientific elaboration of functional role semantics\nfor cognitive representations generally, he too had abandoned the\n“localist” interpretation. Instead, he offered a\n“state-space semantics”. \nWe saw in the previous section how (vector) state spaces provide an\ninterpretation for activity patterns in neural networks, both\nbiological and artificial. A state-space semantics for cognitive\nrepresentations is a species of a functional role semantics because\nthe individuation of a particular state depends upon the relations\nobtaining between it and other states. A representation is a point in\nan appropriate state space, and points (or subvolumes) in a space are\nindividuated by their relations to other points (locations,\ngeometrical proximity). Paul Churchland (1989, 1995) illustrated a\nstate-space semantics for neural states by appealing to sensory\nsystems. One popular theory in sensory neuroscience of how the brain\ncodes for sensory qualities (like color) is the opponent process\naccount (Hardin 1988). Churchland (1995) describes a\nthree-dimensional activation vector state-space in which every color\nperceivable by humans is represented as a point (or subvolume). Each\ndimension corresponds to activity rates in one of three classes of\nphotoreceptors present in the human retina and their efferent paths:\nthe red-green opponent pathway, yellow-blue opponent pathway, and\nblack-white (contrast) opponent pathway. Photons striking the retina\nare transduced by photoreceptors, producing an activity rate in each\nof the segregated pathways. A represented color is hence a triplet of\nneuronal activation frequency rates. As an illustration, consider\nagain\n Figure 3.\n Each dimension in that three-dimensional space will represent average\nfrequency of action potentials in the axons of one class of ganglion\ncells projecting out of the retina. Each color perceivable by humans\nwill be a region of that space. For example, an orange stimulus\nproduces a relatively low level of activity in both the red-green and\nyellow-blue opponent pathways (x-axis and y-axis,\nrespectively), and middle-range activity in the black-white (contrast)\nopponent pathway (z-axis). Pink stimuli, on the other hand,\nproduce low activity in the red-green opponent pathway, middle-range\nactivity in the yellow-blue opponent pathway, and high activity in the\nblack-white (contrast) opponent\n pathway.[7]\n The location of each color in the space generates a “color\nsolid”. Location on the solid, and geometrical proximity between\nthese locations, reflect structural similarities between the perceived\ncolors. Human gustatory representations are points in a\nfour-dimensional state space, with each dimension coding for activity\nrates generated by gustatory stimuli in each type of taste receptor\n(sweet, salty, sour, and bitter) and their segregated efferent\npathways. When implemented in a neural network with structural\nresources, and hence computational resources as vast as the human\nbrain, the state space approach to psychosemantics generates a theory\nof content for a huge number of cognitive\n states.[8] \nJerry Fodor and Ernest LePore (1992) raised an important challenge to\nChurchland’s psychosemantics. Location in a state space alone\nseems insufficient to fix a state’s representational content.\nChurchland never explains why a point in a three-dimensional state\nspace represents a color, as opposed to any other quality,\nobject, or event that varies along three\n dimensions.[9].\n So Churchland’s account achieves its explanatory power by the\ninterpretation imposed on the dimensions. Fodor and LePore alleged\nthat Churchland never specified how a dimension comes to represent,\ne.g., degree of saltiness, as opposed to yellow-blue wavelength\nopposition. One obvious answer appeals to the stimuli that form the\n“external” inputs to the neural network in question. Then,\nfor example, the individuating conditions on neural representations of\ncolors are that opponent processing neurons receive input from a\nspecific class of photoreceptors. The latter in turn have\nelectromagnetic radiation (of a specific portion of the visible\nspectrum) as their activating stimuli. However, this appeal to\n“external” stimuli as the ultimate individuating\nconditions for representational content makes the resulting approach a\nversion of informational semantics. Is this approach consonant with\nother neurobiological details? \nThe neurobiological paradigm for informational semantics is the\nfeature detector: one or more neurons that are (i) maximally\nresponsive to a particular type of stimulus, and (ii) have the\nfunction of indicating the presence of that stimulus type. Examples of\nsuch stimulus-types for visual feature detectors include high-contrast\nedges, motion direction, and colors. A favorite feature detector among\nphilosophers is the alleged fly detector in the frog. Lettvin et al.\n(1959) identified cells in the frog retina that responded maximally to\nsmall shapes moving across the visual field. The idea that these\ncells’ activity functioned to detect flies rested upon knowledge\nof the frogs’ diet. (Bechtel 1998 provides a useful discussion.)\nUsing experimental techniques ranging from single-cell recording to\nsophisticated functional imaging, neuroscientists discovered a host of\nneurons that are maximally responsive to a variety of complex stimuli.\nHowever, establishing condition (ii) on a feature detector is much\nmore difficult. Even some paradigm examples have been called into\nquestion. David Hubel and Torsten Wiesel’s (1962) Nobel\nPrize-winning work establishing the receptive fields of neurons in\nstriate (visual) cortex is often interpreted as revealing cells whose\nfunction is edge detection. However, Lehky and Sejnowski (1988)\nchallenged this interpretation. They trained an artificial neural\nnetwork to distinguish the three-dimensional shape and orientation of\nan object from its two-dimensional shading pattern. Their network\nincorporates many features of visual neurophysiology. Nodes in the\ntrained network turned out to be maximally responsive to edge\ncontrasts, but did not appear to have the function of edge detection.\n(See Churchland and Sejnowski 1992 for a review.) \nKathleen Akins (1996) offered a different neurophilosophical challenge\nto informational semantics and its affiliated feature-detection view\nof sensory representation. We saw in the previous section that Akins\nargued that the physiology of thermoreception violates three necessary\nconditions on “veridical” representation. From this fact\nshe raised doubts about looking for feature-detecting neurons to\nground a psychosemantics generally, including for thought contents.\nHuman thoughts about flies, for example, are sensitive to numerical\ndistinctions between particular flies and the particular locations\nthey can occupy. But the ends of frog nutrition are well served\nwithout a representational system sensitive to such ontological\nniceties. Whether a fly seen now is numerically identical to one seen\na moment ago need not, and perhaps cannot, figure into the\nfrog’s feature detection repertoire. Akins’ critique cast\ndoubt on whether details of sensory transduction will scale up to\nprovide an adequate unified psychosemantics for all concepts. It also\nraised new questions for human intentionality. How do we get from\nactivity patterns in “narcissistic” sensory receptors,\nkeyed not to “objective” environmental features but rather\nonly to effects of the stimuli on the patch of tissue innervated, to\nhuman ontologies replete with enduring objects with stable\nconfigurations of properties and relations, types and their tokens (as\nthe “fly-thought” example presented above reveals), and\nthe rest? And how did the development of a stable, rich ontology\nconfer survival advantages to human ancestors? \nConsciousness re-emerged over the past three decades as a topic of\nresearch focus in philosophy of mind, and in the cognitive and brain\nsciences. Instead of ignoring it, many physicalists sought to explain\nit (Dennett 1991). Here we focus exclusively on ways that\nneuroscientific discoveries have impacted philosophical debates about\nthe nature of consciousness and its relation to physical mechanisms.\n(See links to other entries in this encyclopedia below in\n Related Entries\n for broader discussions about consciousness and physicalism.) \nThomas Nagel (1974) argued famously that conscious experience is\nsubjective, and thus permanently recalcitrant to objective scientific\nunderstanding. He invited us to ponder “what it is like to be a\nbat” and urged the intuitive judgment that no amount of\nphysical-scientific knowledge, including neuroscientific, supplies a\ncomplete answer. Nagel’s intuition pump has generated extensive\nphilosophical discussion. At least two well-known replies made direct\nappeal to neurophysiology. John Biro (1991) suggested that part of the\nintuition pumped by Nagel, that bat experience is substantially\ndifferent from human experience, presupposes systematic relations\nbetween physiology and phenomenology. Kathleen Akins (1993a) delved\ndeeper into existing knowledge of bat physiology and reports much that\nis pertinent to Nagel’s question. She argued that many of the\nquestions about bat subjective experience that we still consider open\nhinge on questions that remain unanswered about neuroscientific\ndetails. One example of the latter is the function of various cortical\nactivity profiles in the active bat. \nDavid Chalmers (1996) famously argued that any possible brain-process\naccount of consciousness will leave open an “explanatory\ngap” between the brain process and properties of the conscious\n experience.[10]\n This is because no brain-process theory can answer the\n“hard” question: Why should that particular brain process\ngive rise to that particular conscious experience? We can always\nimagine (“conceive of”) a universe populated by creatures\nhaving those brain processes but completely lacking conscious\nexperience. A theory of consciousness requires an explanation of how\nand why some brain process causes a conscious experience, replete with\nall the features we experience. The fact that the hard question\nremains unanswered shows that we will probably never get a complete\nexplanation of consciousness at the level of neural mechanism. Paul\nand Patricia Churchland (1997) offered the following diagnosis and\nreply. Chalmers offers a conceptual argument, based on our\nability to imagine creatures possessing active brains like ours but\nwholly lacking in conscious experiences. But the more one learns about\nhow the brain produces conscious experience—and such a\nliterature has emerged (for some early work, see Gazzaniga\n1995)—the harder it becomes to imagine a universe consisting of\ncreatures with brain processes like ours but lacking consciousness.\nThis is not just bare assertion. The Churchlands appeal to some\nneurobiological detail. For example, Paul Churchland (1995) develops a\nneuroscientific account of consciousness based on recurrent\nconnections between thalamic nuclei, particularly between\n“diffusely projecting” nuclei like the intralaminar nuclei\nand the\n cortex.[11]\n Churchland argues that thalamocortical recurrency accounts for the\nselective features of consciousness, for the effects of short-term\nmemory on conscious experience, for vivid dreaming during REM\n(rapid-eye movement) sleep, and other “core” features of\nconscious experience. In other words, the Churchlands are claiming\nthat when one learns about activity patterns in these recurrent\ncircuits, one can no longer “imagine” or “conceive\nof” this activity occurring without these core features of\nconscious experience occurring. (Other than just mouthing the\nexpression, “I am now imagining activity in these circuits\nwithout selective attention/the effects of short-term memory/vivid\ndreaming/…”). \nA second focus of skeptical arguments about a complete neuroscientific\nexplanation of consciousness is on sensory qualia: the\nintrospectable qualitative aspects of sensory experience, the features\nby which subjects discern similarities and differences among their\nexperiences. The colors of visual sensations are a philosopher’s\nfavorite example. One famous puzzle about color qualia is the alleged\nconceivability of spectral inversions. Many philosophers claim that it\nis conceptually possible (if perhaps physically impossible) for two\nhumans not to differ neurophysiologically, while the color that fire\nengines and tomatoes appear to have to one subject is the color that\ngrass and frogs appear to have to the other (and vice versa). A large\namount of neuroscientifically-informed philosophy has addressed this\nquestion. (C.L. Hardin 1988 and Austen Clark 1993 are noteworthy\nexamples.) A related area where neurophilosophical considerations have\nemerged concerns the metaphysics of colors themselves (rather than\ncolor experiences). A longstanding philosophical dispute is whether\ncolors are objective properties existing external to perceivers or\nrather identifiable as or dependent upon minds or nervous systems.\nSome neuroscientific work on this problem begins with characteristics\nof color experiences: for example, that color similarity judgments\nproduce color orderings that align on a circle (Clark 1993). With this\nresource, one can seek mappings of phenomenology onto environmental or\nphysiological regularities. Identifying colors with particular\nfrequencies of electromagnetic radiation does not preserve the\nstructure of the hue circle, whereas identifying colors with activity\nin opponent processing neurons does. Such a tidbit is not decisive for\nthe color objectivist-subjectivist debate, but it does convey the type\nof neurophilosophical work being done on traditional metaphysical\nissues beyond the philosophy of mind. (For more details on these\nissues, see the\n entry on color\n in this Encyclopedia.) \nWe saw in the discussion of Hardcastle (1997) two sections above that\nneurophilosophers have entered disputes about the nature and\nmethodological import of pain experiences. Two decades earlier, Dan\nDennett (1978) took up the question of whether it is possible to build\na computer that feels pain. He compares and notes tension between\nneurophysiological discoveries and common sense intuitions about pain\nexperience. He suspects that the incommensurability between scientific\nand common sense views is due to incoherence in the latter. His\nattitude is wait-and-see. But foreshadowing Churchland’s reply\nto Chalmers, Dennett favors scientific investigations over\nconceivability-based philosophical arguments. \nNeurological deficits have attracted philosophers interested in\nconsciousness. For nearly fifty years philosophers have debated\nimplications for the unity of the self of the Nobel Prize-winning\nexperiments with commissurotomy patients who, for clinical reasons,\nhad their corpus callosum surgically ablated (Nagel\n 1971).[12]\n The corpus callosum is the huge bundle of axons connecting neurons\nacross the left and right mammalian cerebral hemispheres. In carefully\ncontrolled experiments, commissurotomy patients seemingly display two\ndissociable “seats” of consciousness. Elizabeth Schechter\n(2018) has recently greatly updated philosophical treatment of the\nscientific details of these “split-brain” patients,\nincluding their own experiential reports, and has traced implications\nfor our understanding of the self.  \nIn chapter 5 of her (1986) book, Patricia Churchland extended both the\nrange and philosophical implications of neurological deficits. One\ndeficit she discusses in detail is blindsight. Some patients with\nlesions to primary visual cortex report being unable to see items in\nregions of their visual fields, yet perform far better than chance in\nforced guess trials about stimuli in those regions. A variety of\nscientific and philosophical interpretations have been offered. Ned\nBlock (1995) worried that many of these interpretations conflate\ndistinct notions of consciousness. He labels these notions\n“phenomenal consciousness” (“P-consciousness”)\nand “access consciousness”\n(“A-consciousness”). The former is the “what it is\nlike”-ness of conscious experiences. The latter is the\navailability of representational content to self-initiated action and\nspeech. Block argued that P-consciousness is not always\nrepresentational, whereas A-consciousness is. Dennett (1991, 1995) and\nTye (1993) are skeptical of non-representational analyses of\nconsciousness in general. They provide accounts of blindsight that do\nnot depend on Block’s distinction. \nWe break off our brief overview of neurophilosophical work on\nconsciousness here. Many other topics are worth neurophilosophical\npursuit. We mentioned commissurotomy and the unity of consciousness\nand the self, which continues to generate discussion. Qualia beyond\nthose of color and pain experiences quickly attracted\nneurophilosophical attention (Akins 1993a,b, 1996; Austen Clark 1993),\nas did self-consciousness (Bermúdez 1998). \nOne of the first issues to arise in neurology, as far back as the\nnineteenth century, concerned the localization of specific cognitive\nfunctions to specific brain regions. Although the\n“localization” approach had dubious origins in the\nphrenology of Gall and Spurzheim, and had been challenged strenuously\nby Flourens throughout the early nineteenth century, it re-emerged\nlate in the nineteenth century in the study of aphasia by Bouillaud,\nAuburtin, Broca, and Wernicke. These neurologists made careful studies\n(when possible) of linguistic deficits in their aphasic patients,\nfollowed by brain autopsies post\n mortem.[13]\n Broca’s initial study of twenty-two patients in the\nmid-nineteenth century confirmed that damage to the left cortical\nhemisphere was predominant, and that damage to the second and third\nfrontal convolutions was necessary to produce speech production\ndeficits. Although the anatomical coordinates Broca postulated for the\n“speech production center” do not correlate exactly with\ndamage producing production deficits, both this area of frontal cortex\nand speech production deficits still bear his name\n(“Broca’s area” and “Broca’s\naphasia”). Less than two decades later Carl Wernicke published\nevidence for a second language center. This area is anatomically\ndistinct from Broca’s area, and damage to it produced a very\ndifferent set of aphasic symptoms. The cortical area that still bears\nhis name (“Wernicke’s area”) is located around the\nfirst and second convolutions in temporal cortex, and the aphasia that\nbears his name (“Wernicke’s aphasia”) involves\ndeficits in language comprehension. Wernicke’s method, like\nBroca’s, was based on lesion studies produced by natural trauma:\na careful evaluation of the behavioral deficits, followed by post\nmortem autopsies to find the sites of tissue damage and atrophy. More\nrecent and more careful lesion studies suggest more precise\nlocalization of specific linguistic functions, and remain a\ncornerstone to this day in aphasia research. \nLesion studies have also produced evidence for the localization of\nother cognitive functions: for example, sensory processing and certain\ntypes of learning and memory. However, localization arguments for\nthese other functions invariably include studies using animal models.\nWith an animal model, one can perform careful behavioral measures in\nhighly controlled settings, then ablate specific areas of neural\ntissue (or use a variety of other techniques to block or enhance\nactivity in these areas) and re-measure performance on the same\nbehavioral tests. Since we lack widely accepted animal models for\nhuman language production and comprehension, this additional evidence\nisn’t available to the neurologist or neurolinguists. This\nlimitation makes the neurological study of language a paradigm case\nfor evaluating the logic of the lesion/deficit method of inferring\nfunctional localization. Barbara Von Eckardt (Von Eckardt Klein 1978)\nattempted to make explicit the steps of reasoning involved in this\ncommon and historically important method. Her analysis begins with\nRobert Cummins’ well-known analysis of functional explanation,\nbut she extends it into a notion of structurally adequate\nfunctional analysis. These analyses break down a complex capacity C\ninto its constituent capacities c1,\nc2,…, cn,\nwhere the constituent capacities are consistent with the underlying\nstructural details of the system. For example, human speech production\n(complex capacity C) results from formulating a speech intention, then\nselecting appropriate linguistic representations to capture the\ncontent of the speech intention, then formulating the motor commands\nto produce the appropriate sounds, then communicating these motor\ncommands to the appropriate motor pathways (all together, the\nconstituent capacitiesc1,\nc2,…, cn). A\nfunctional-localization hypothesis has the form: brain structure S in\norganism (type) O has constituent capacity\nci, where ci\nis a function of some part of O. An example might be: Broca’s\narea (S) in humans (O) formulates motor commands to produce the\nappropriate sounds (one of the constituent capacities\nci). Such hypotheses specify aspects of\nthe structural realization of a functional-component model. They are\npart of the theory of the neural realization of the functional\nmodel. \nArmed with these characterizations, Von Eckardt Klein argues that inference\nto a functional-localization hypothesis proceeds in two steps. First,\na functional deficit in a patient is hypothesized based on the\nabnormal behavior the patient exhibits. Second, localization of\nfunction in normal brains is inferred on the basis of the functional\ndeficit hypothesis plus the evidence about the site of brain damage.\nThe structurally-adequate functional analysis of the capacity connects\nthe pathological behavior to the hypothesized functional deficit. This\nconnection suggests four adequacy conditions on a functional deficit\nhypothesis. First, the pathological behavior P (e.g., the speech\ndeficits characteristic of Broca’s aphasia) must result from\nfailing to exercise some complex capacity C (human speech production).\nSecond, there must be a structurally-adequate functional analysis of\nhow people exercise capacity C that involves some constituent capacity\nci (formulating motor commands to produce\nthe appropriate sounds). Third, the operation of the steps described\nby the structurally-adequate functional analysis minus the operation\nof the component performing ci\n(Broca’s area) must result in pathological behavior P. Fourth,\nthere must not be a better available explanation for why the patient\ndoes P. Arguments to a functional deficit hypothesis on the basis of\npathological behavior is thus an instance of argument to the best\navailable explanation. When postulating a deficit in a normal\nfunctional component provides the best available explanation of the\npathological data, we are justified in drawing the inference. \nVon Eckardt Klein applies this analysis to a neurological case study\ninvolving a controversial reinterpretation of\n agnosia.[14]\n Her philosophical explication of this important neurological method\nreveals that most challenges to localization arguments either argue\nonly against the localization of a particular type of functional\ncapacity or against generalizing from localization of function in one\nindividual to all normal individuals. (She presents examples of each\nfrom the neurological literature.) Such challenges do not impugn the\nvalidity of standard arguments for functional localization from\ndeficits. It does not follow that such arguments are unproblematic.\nBut they face difficult factual and methodological problems, not\nlogical ones. Furthermore, the analysis of these arguments as\ninvolving a type of functional analysis and inference to the best\navailable explanation carries an important implication for the\nbiological study of cognitive function. Functional analyses require\nfunctional theories, and structurally adequate functional analyses\nrequire checks imposed by the lower level sciences investigating the\nunderlying physical mechanisms. Arguments to best available\nexplanation are often hampered by a lack of theoretical imagination:\nthe available alternative explanations are often severely limited. We\nmust seek theoretical inspiration from any level of investigation or\nexplanation. Hence making explicit the “logic” of this\ncommon and historically important form of neurological explanation\nreveals the necessity of joint participation from all scientific\nlevels, from cognitive psychology down to molecular neuroscience. Von\nEckardt Klein (1978) thus anticipated what came to be heralded as the\n“co-evolutionary research methodology”, which remains a\ncenterpiece of neurophilosophy to the present day (see\n section 6). \nOver the last three decades, new evidence for localizations of\ncognitive functions has come increasingly from a new source, the\ndevelopment and refinement of neuroimaging techniques. However, the\nlogical form of localization-of-function arguments appears not to have\nchanged from those employing lesion studies, as analyzed by Von\nEckardt Klein. Instead, these new neuroimaging technologies resolve some of\nthe methodological problems that plagued lesion studies. For example,\nresearchers do not need to wait until the patient dies, and in the\nmeantime probably acquires additional brain damage, to find the lesion\nsites. Two functional imaging techniques have been prominent in\nphilosophical discussions: positron emission tomography, or PET, and\nfunctional magnetic resonance imaging, or fMRI. Although these measure\ndifferent biological markers of functional activity, PET approved for\nhuman use now has spatial resolution down to the single mm range,\nwhile fMRI has resolution down to less than\n 1mm.[15]\n As these techniques increased spatial and temporal resolution of\nfunctional markers, and continued to be used with sophisticated\nbehavioral methodologies, arguments for localizing specific\npsychological functions to increasingly specific neural regions\ncontinued to grow. Stufflebeam and Bechtel provided an early and\nphilosophically useful discussion of PET. Bechtel and Richardson\n(1993) provided a general framework for “localization and\ndecomposition” arguments, which anticipated in many ways the\ncoming “new mechanistic” perspective in philosophy of\nscience and philosophy of neuroscience (see\n sections 7 and 8 below).\n Bechtel and Mundale (1999) further refined philosophical arguments\nfor localization of function specific to neuroscience. \nMore recent philosophical discussion of these functional imaging\ntechniques has tended to urge more caution resting localization claims\non their results. Roskies (2007), for example, points out the tendency\nto think of the evidential force of functional neuroimages (especially\nfMRI) on an analogy of that of photographs. Drawing on work in\naesthetics and the visual arts, Roskies argues that many of the\nfeatures that give photographs their evidential force are not present\nin functional neuroimages. So while neuroimages do serve as evidence\nfor claims about neurofunctions, and even for localization hypotheses,\ndetails of their proper interpretation are far more complicated than\nphilosophers sometimes assume. More critically, Klein (2010) argues\nthat images of “brain activity” resulting from functional\nneuroimaging, especially fMRI are poor evidence for functional\nhypotheses. For these images present the results of null hypothesis\nsignificance testing on fMRI data, and such testing alone cannot\nprovide evidence about the functional structure of a causally dense\nsystem, which the human brain is. Instead, functional neuroimages are\nproperly interpreted as indicating regions where further data and\nanalysis are warranted. But these data will typically require more\nthan simple significance testing, so skepticism about the evidential\nforce of neuroimages does not warrant skepticism more generally about\nfMRI.  \nLocalization of function remains to this day a central topic of\ndiscussion in philosophy of neuroscience. We will cover more recent\nwork in later sections. \nWhat neuroscience has now discovered about the cellular and molecular\nmechanisms of neural conductance and transmission is spectacular.\nThese results constitute one of the crowning achievements of\nscientific inquiry. (For those in doubt, simply peruse for five\nminutes a recent volume of Society for Neuroscience\nAbstracts.) Less comprehensive, yet still spectacular, are\ndiscoveries at “higher” levels of neuroscience: circuits,\nnetworks, and systems. All this is a natural outcome of increasing\nscientific specialization. We develop the technology, the experimental\ntechniques, and ultimately the experimental results-driven theories\nwithin specific disciplines to push forward our understanding. Still,\na crucial aspect of the total picture sometimes gets neglected: the\nrelationship between the levels, the “glue” that binds\nknowledge of neuron activity to subcellular and molecular mechanisms\n“below”, and to circuit, network, and systems activity\npatterns “above”. This problem is especially glaring when\nwe try to relate “cognitivist” psychological theories,\npostulating information-bearing representations and processes\noperating over their contents, to neuronal activities.\n“Co-evolution” between these explanatory levels still\nseems more a distant dream than an operative methodology guiding\nday-to-day scientific research. \nIt is here that some philosophers and neuroscientists turned to\ncomputational methods (Churchland and Sejnowski 1992). One hope was\nthat the way computational models have functioned in more developed\nsciences, like in physics, might provide a useful model. One\ncomputational resource that has usefully been applied in more\ndeveloped sciences to similar “cross-level” concerns are\ndynamical systems. Global phenomena, such as large-scale\nmeteorological patterns, have been usefully addressed as dynamical,\nnonlinear, and often chaotic interactions between lower-level physical\nphenomena. Addressing the interlocking levels of theory and\nexplanation in the mind/brain using computational resources that have\nworked to bridge levels in more mature sciences might yield comparable\nresults. This methodology is necessarily interdisciplinary, drawing on\nresources and researchers from a variety of levels, including higher\nones like experimental psychology, artificial intelligence, and\nphilosophy of science. \nThe use of computational methods in neuroscience itself is not new.\nHodgkin, Huxley, and Katz (1952) incorporated values of\nvoltage-dependent sodium and potassium conductance they had measured\nexperimentally in the squid giant axon into an equation from physics\ndescribing the time evolution of a first-order kinetic process. This\nequation enabled them to calculate best-fit curves for modeled\nconductance versus time data that reproduced the changing membrane\npotential over time when action potentials were generated. Also using\nequations borrowed from physics, Rall (1959) developed the cable model\nof dendrites. This model provided an account of how the various inputs\nfrom across the dendritic tree interact temporally and spatially to\ndetermine the input-output properties of single neurons. It remains\ninfluential today, and was incorporated into the GENESIS software\nfor programming neurally realistic networks (Bower and Beeman 1995;\nsee discussion in\n section 2 above).\n David Sparks and his colleagues showed that a vector-averaging model\nof activity in neurons of superior colliculi correctly predicts\nexperimental results about the amplitude and direction of saccadic eye\nmovements (Lee, Rohrer, and Sparks 1988). Working with a more\nsophisticated mathematical model, Apostolos Georgopoulos and his\ncolleagues predicted direction and amplitude of hand and arm movements\nbased on averaged activity of 224 cells in motor cortex. Their\npredictions were borne out under a variety of experimental tests\n(Georgopoulos, Schwartz, and Kettner 1986). We mention these\nparticular studies only because these are ones with which we are\nfamiliar. No doubt we could multiply examples of the fruitful\ninteraction of computational and experimental methods in neuroscience\neasily by one-hundred-fold. Many of these extend back before\n“computational neuroscience” was a recognized research\nendeavor. \nWe’ve already seen one example, the vector transformation\naccount of neural representation and computation, once under active\ndevelopment in cognitive neuroscience (see\n section 2 above).\n Other approaches using “cognitivist” resources were, and\ncontinue to be,\n pursued.[16]\n Some of these projects draw upon “cognitivist”\ncharacterizations of the phenomena to be explained. Some exploit\n“cognitivist” experimental techniques and methodologies.\nSome even attempt to derive “cognitivist” explanations\nfrom cell-biological processes (e.g., Hawkins and Kandel 1984). As\nStephen Kosslyn (1997) put it, cognitive neuroscientists employ the\n“information processing” view of the mind characteristic\nof cognitivism without trying to separate it from theories of brain\nmechanisms. Such an endeavor calls for an interdisciplinary community\nwilling to communicate the relevant portions of the mountain of detail\ngathered in individual disciplines with interested nonspecialists.\nThis requires more than people willing to confer with others working\nat related levels, but also researchers trained explicitly in the\nmethods and factual details of a variety of disciplines. This is a\ndaunting need, but it offers hope to philosophers wishing to\ncontribute to actual neuroscience. Thinkers trained in both the\n“synoptic vision” afforded by philosophy, and the\nscientific and experimental basis of a genuine (graduate-level)\nscience would be ideally equipped for this task. Recognition of this\npotential niche was slow to dawn on graduate programs in philosophy,\nbut a few programs have taken steps to fill it (see, e.g.,\n Other Internet Resources\n below). \nHowever, one glaring shortcoming remains. Given philosophers’\ntraining and interests, “higher-level”\nneurosciences—networks, cognitive, systems, and the fields of\ncomputational neuroscience which ally with these—tend to attract\nthe most philosophical attention. As natural as this focus might be,\nit can lead philosophers to a misleading picture of neuroscience.\nNeurobiology remains focused on cellular and molecular mechanisms of\nneuronal activity, and allies with the kind of behavioral neuroscience\nthat works with animal models. This is still how a majority of members\nof the Society for Neuroscience, now more than 37,000 members strong,\nclassify their own research; this is where the majority of grant money\nfor research goes; and these are the areas whose experimental\npublications most often appear in the most highly cited scientific\njournals. (The link to the Society for Neuroscience’s web site\nin\n Other Internet Resources below\n leads to a wealth of data on these numbers; see especially the\nPublications section.) Yet philosophers have tended not to pay much\nattention to cellular and molecular neuroscience. Fortunately this\nseems to be changing, as we will document in sections 7 and 8 below.\nStill, the preponderant attention philosophers pay to\ncognitive/systems/computational neuroscience obscures the wetlab\nexperiment-driven focus of ongoing neurobiology. \nThe distinction between “philosophy of neuroscience” and\n“neurophilosophy” came to be better clarified over the\nfirst decade of the twenty-first century, due primarily to more\nquestions being pursued in both areas. Philosophy of neuroscience\nstill tends to pose traditional questions from philosophy of science\nspecifically about neuroscience. Such questions include: What is the\nnature of neuroscientific explanation? And, what is the nature of\ndiscovery in neuroscience? Answers to these questions are pursued\neither descriptively (how does neuroscience proceed?) or normatively\n(how should neuroscience proceed)? Some normative projects in\nphilosophy of neuroscience are “deconstructive”,\ncriticizing claims about the topic made by neuroscientists. For\nexample, philosophers of neuroscience have criticized the conception\nof personhood assumed by researchers in cognitive neuroscience (cf.\nRoskies 2009). Other normative projects are constructive, proposing\nnew theories of neuronal phenomena or methods for interpreting\nneuroscientific data. Such projects often integrate smoothly with\ntheoretical neuroscience itself. For example, Chris Eliasmith and\nCharles Anderson developed an approach to constructing\nneurocomputational models in their book Neural Engineering\n(2003). In separate publications, Eliasmith has argued that the\nframework introduced in Neural Engineering provides both a\nnormative account of neural representation and a framework for\nunifying explanation in neuroscience (e.g., Eliasmith 2009). \nNeurophilosophy continued to apply findings from the neurosciences to\ntraditional, philosophical questions. Examples include: What is an\nemotion? (Prinz 2004) What is the nature of desire? (Schroeder 2004)\nHow is social cognition made possible? (Goldman 2006) What is the neural basis of\nmoral cognition? (Prinz 2007) What is the neural basis of happiness?\n(Flanagan 2009) Neurophilosophical answers to these questions are\nconstrained by what neuroscience reveals about nervous systems. For\nexample, in his book Three Faces of Desire, Timothy Schroeder\n(2004) argued that our commonsense conception of desire attributes to\nit three capacities: (1) the capacity to reinforce behavior when\nsatisfied, (2) the capacity to motivate behavior, and (3) the capacity\nto determine sources of pleasure. Based on evidence from the\nliterature on dopamine function and reinforcement learning theory,\nSchroeder argued that reward processing is the basis for all three\ncapacities. Thus, reward is the essence of desire. \nDuring the first decade of the twenty-first century a trend arose in\nneurophilosophy to look toward neuroscience for guidance in moral\nphilosophy. That should be evident from the themes we’ve just\nmentioned. Simultaneously, there was renewed interest in moralizing\nabout neuroscience and neurological treatments (see Levy 2007; Roskies\n2009). This new field, neuroethics, thus combined both\ninterest in the relevance of neuroscience data for understanding moral\ncognition, and the relevance of moral philosophy for acquiring and\nregulating the application of knowledge from neuroscience. The\nregulatory branch of neuroethics initially focused explicitly on the\nethics of treatment for people who suffer from neurological\nimpairments, the ethics of attempts to enhance human cognitive\nperformance (Schneider 2009), the ethics of applying “mind\nreading” technology to problems in forensic science (Farah and\nWolpe 2004), and the ethics of animal experimentation in neuroscience\n(Farah 2008). More recently both of these fields of neuroethics has\nseen tremendous growth. The interested reader should consult the\n neuroethics entry\n in this Encyclopedia. \nTrends during the first decade of the twenty-first century in\nphilosophy of neuroscience included renewed interest in the nature of\nmechanistic explanations. This was in keeping with a general trend in\nphilosophy of science (e.g., Machamer, Darden, and Craver 2000). The\napplication of this general approach to neuroscience isn’t\nsurprising. “Mechanism” is a widely-used term among\nneuroscientists. In his book, Explaining the Brain (2007),\nCarl Craver contended that mechanistic explanations in neuroscience\nare causal explanations, and typically multi-level. For example, the\nexplanation of the neuronal action potential involves the action\npotential itself, the cell in which it occurs, electro-chemical\ngradients, and the proteins through which ions flow across the\nmembrane. Thus we have a composite entity (a cell) causally\ninteracting with neurotransmitters at its receptors. Parts of the cell\nengage in various activities, e.g., the opening and closing of\nligand-gated and voltage-gated ion channels, to produce a pattern of\nchanges, the depolarizing current constituting the action potential. A\nmechanistic explanation of the action potential thus countenances\nentities at the cellular, molecular, and atomic levels, all of which\nare causally relevant to producing the action potential. This causal\nrelevance can be confirmed by altering any one of these variables,\ne.g., the density of ion channels in the cell membrane, to generate\nalterations in the action potential; and by verifying the consistency\nof the purported invariance between the variables. For challenges to\nCraver’s account of mechanistic explanation in neuroscience,\nspecifically concerning the action potential, see Weber 2008, and\nBogen 2005. \nAccording to epistemic norms shared implicitly by neuroscientists,\ngood explanations in neuroscience are good mechanistic explanations;\nand good mechanistic explanations are those that pick out invariant\nrelationships between mechanisms and the phenomena they control. (For\nfuller treatment of invariance in causal explanations throughout\nscience, see James Woodward 2003. Mechanists draw extensively on\nWoodward’s “interventionist” account of cause and\ncausal explanations.) Craver’s account raised questions about\nthe place of reduction in neuroscience. John Bickle (2003) suggested\nthat the working concept of reduction in the neurosciences consists of\nthe discovery of systematic relationships between interventions at\nlower levels of biological organization, as these are pursued in\ncellular and molecular neuroscience, and higher level behavioral\neffects, as they are described in psychology. Bickle called this\nperspective “reductionism-in-practice” to contrast it with\nthe concepts of intertheoretic or metaphysical reduction that have\nbeen the focus of many debates in the philosophy of science and\nphilosophy of mind. Despite Bickle’s reformulation of reduction,\nhowever, mechanists generally resist, or at least relativize, the\n“reductionist” label. Craver (2007) calls his view the\n“mosaic unity” of neuroscience. Bechtel (2009) calls his\n“mechanistic reduction(ism)”. Both Craver and Bechtel\nadvocate multi-leveled “mechanisms-within-mechanisms”,\nwith no level of mechanism epistemically privileged. This is in\ncontrast to reduction(ism), ruthless or otherwise which privileges\nlower levels. Still we can ask: Is mechanism a kind of\nreductionism-in-practice? Or does mechanism, as a position on\nneuroscientific explanation, assume some type of autonomy for\npsychology? If it assumes autonomy, reductionists might challenge\nmechanists on this assumption. On the other hand, Bickle’s\nreductionism-in-practice clearly departs from inter-theoretic\nreduction, as the latter is understood in philosophy of science. As\nBickle himself acknowledges, his latest reductionism was inspired\nheavily by mechanists’ criticisms of his earlier “new\nwave” account. Mechanists can challenge Bickle that his\ndeparture from the traditional accounts has also led to a departure\nfrom the interests that motivated those accounts. (See Polger 2004 for\na related challenge.) As we will see in\n section 8 below,\n these issues surrounding mechanistic philosophy of neuroscience have\ngrown more urgent, as mechanism has grown to dominate the field. \nThe role of temporal representation in conscious experience and the\nkinds of neural architectures sufficient to represent objects in time\ngenerated interest. In the tradition of Husserl’s phenomenology,\nDan Lloyd (2002, 2003) and Rick Grush (2001, 2009) have separately\ndrawn attention to the tripartite temporal structure of phenomenal\nconsciousness as an explanandum for neuroscience. This structure\nconsists of a subjective present, an immediate past, and an\nexpectation of the immediate future. For example, one’s\nconscious awareness of a tune is not just of a time-slice of\ntune-impression, but of a note that a moment ago was present, another\nthat is now present, and an expectation of subsequent notes in the\nimmediate future. As this experience continues, what was a moment ago\ntemporally immediate is now retained as a moment in the immediate\npast; what was expected either occurred or didn’t in what has\nnow become the experienced present; and a new expectation has formed\nof what will come. One’s experience is not static, even though\nthe experience is of a single object (the tune). These earlier works\nfound increased relevance with the rise of “predictive\ncoding” models of whole brain function, developed by\nneuroscientists including Karl Friston (2009) less than a decade\nlater, and brought to broader philosophical attention by Jakob Hohwy\n(2013) and Andy Clark (2016).  \nAccording to Lloyd, the tripartite structure of consciousness raises a\nunique problem for analyzing fMRI data and designing experiments. The\nproblem stems from the tension between the sameness in the object of\nexperience (e.g., the same tune through its progression) and the\ntemporal fluidity of experience itself (e.g., the transitions between\nheard notes). At the time Lloyd was writing, one standard means of\nanalyzing fMRI data consisted in averaging several data sets and\nsubtracting an estimate of baseline activation from the composites.\n [17]\n This is done to filter noise from the task-related hemodynamic\nresponse. But as Lloyd points out, this then-common practice ignores\nmuch of the data necessary for studying the neural correlates of\nconsciousness. It produces static images that neglect the\nrelationships between data points over the time course. Lloyd instead\napplies a multivariate approach to studying fMRI data, under the\nassumption that a recurrent network architecture underlies the\ntemporal processing that gives rise to experienced time. A simple\nrecurrent network has an input layer, an output layer, a hidden layer,\nand an additional layer that copies the prior activation state of\neither the hidden layer or the output layer. Allowing the output layer\nto represent a predicted outcome, the input layer can then represent a\ncurrent state and the additional layer a prior state. This assignment\nmimics the tripartite temporal structure of experience in a network\narchitecture. If the neuronal mechanisms underlying conscious\nexperience are approximated by recurrent network architecture, one\nprediction is that current neuronal states carry information about\nimmediate future and prior states. Applied to fMRI, the model predicts\nthat time points in an image series will carry information about prior\nand subsequent time points. The results of Lloyd’s (2002)\nanalysis of 21 subjects’ data sets, sampled from the publicly\naccessible National fMRI Data Center, support this prediction. \nGrush’s (2001, 2004) interest in temporal representation is part\nof his broader systematic project of addressing a semantic problem for\ncomputational neuroscience, namely: how do we demarcate study of the\nbrain as an information processor from the study of any other complex\ncausal process? This question leads back into the familiar territory\nof psychosemantics (see\n section 3 above),\n but now the starting point is internal to the practices of\ncomputational neuroscience. The semantic problem is thereby rendered\nan issue in philosophy of neuroscience, insofar as it asks: what does\n(or should) “computation” mean in computational\nneuroscience? \nGrush’s solution drew on concepts from modern control theory. In\naddition to a controller, a sensor, and a goal state, certain kinds of\ncontrol systems employ a process model of the actual process\nbeing controlled. A process model can facilitate a variety of\nengineering functions, including overcoming delays in feedback and\nfiltering noise. The accuracy of a process model can be assessed\nrelative to its “plug-compatibility” with the actual\nprocess. Plug-compatibility is a measure of the degree to which a\ncontroller can causally couple to a process model to produce the same\nresults it would produce by coupling with the actual process. Note\nthat plug-compatibility is not an information relation. \nTo illustrate a potential neuroscientific implementation, Grush\nconsiders a controller as some portion of the brain’s motor\nsystems (e.g., premotor cortex). The sensors are the sense organs\n(e.g., stretch receptors on the muscles). A process model of the\nmusculoskeletal system might exist in the cerebellum (see Kawato\n1999). If the controller portion of the motor system sends spike\ntrains to the cerebellum in the same way that it sends spikes to the\nmusculoskeletal system, and if in return the cerebellum receives spike\ntrains similar to real peripheral feedback, then the cerebellum\nemulates the musculoskeletal system (to the degree that the mock\nfeedback resembles real peripheral feedback). The proposed unit over\nwhich computational operations range is the neuronal realization of a\nprocess model and its components, or in Grush’s terms an\n“emulator” and its “articulants”. \nThe details of Grush’s framework are too sophisticated to\npresent in short compass. (For example, he introduces a host of\nconceptual devices to discuss the representation of external objects.)\nBut in a nutshell, he contends that understanding temporal\nrepresentation begins with understanding the emulation of the timing\nof sensorimotor contingencies. Successful sequential behavior (e.g.,\nspearing a fish) depends not just on keeping track of where one is in\nspace, but where one is in a temporal order of movements and the\ntemporal distance between the current, prior, and subsequent\nmovements. Executing a subsequent movement can depend on keeping track\nof whether a prior movement was successful and whether the current\nmovement is matching previous expectations. Grush posits\nemulators—process models in the central nervous\nsystem—that anticipate, retain, and update mock sensorimotor\nfeedback by timing their output proportionally to feedback from an\nactual process (Grush 2005). \nLloyd’s and Grush’s approaches to studying temporal\nrepresentation are varied in their emphases. But they are unified in\ntheir implicit commitment to localizing cognitive functions and\ndecomposing them into subfunctions using both top-down and bottom-up\nconstraints. (See Bechtel and Richardson 1993 for more details on this\ngeneral explanatory strategy.) As we mentioned a few paragraphs above,\nboth anticipated in important and interesting ways more recent\nneuroscientific and philosophical work on predictive coding and the\nbrain. Both developed mechanistic explanations that pay little regard\nto disciplinary boundaries. One of the principal lessons of\nBickle’s and Craver’s work is that neuroscientific\npractice in general is structured in this fashion. The ontological\nconsequences of adopting this approach continue to be debated. \nMechanism, first introduced in section 7 above, came to dominate the\nphilosophy of neuroscience throughout the second decade of the\ntwenty-first century. One much-discussed example is Gualtiero\nPiccinini and Carl Craver (2011). The authors employ two popular\nmechanistic notions. Their first is the multi-level, nested\nhierarchies of mechanisms-within-mechanisms perspective, discussed in\nsection 7 above, that traces back to Craver and Darden (2001). Their\nsecond is that of “mechanism sketch”, suggested initially\nin Machamer, Darden, and Craver (2000) and developed in detail in\nCraver (2007). Piccinini and Craver’s goal is to\n“seamlessly” situate psychology as part of an\n“integrated framework” alongside neuroscience. They\ninterpret psychology’s familiar functional analyses of cognitive\ncapacities as relatively incomplete mechanism-sketches, which leave\nout many components of the mechanisms that ultimately will fully\nexplain the system’s behavior. Neuroscience in turn fills in\nthese missing components, dynamics, and organizations, at least ones\nfound in nervous systems. This filling-in thereby turns\npsychology’s mechanism-sketches into full-blown mechanistic\nexplanations. So even though psychology proceeds via functional\nanalyses, so interpreted it is nonetheless mechanistic. Piccinini and\nCraver realize that their “integrated” account clashes\nwith classical “autonomy” claims for psychology vis-à-vis\nneuroscience. Nevertheless, they insist that their challenge to\nclassical “autonomy” does not commit them to\n“reductionism”, in either its classical or more recent\nvarieties. Their commitment to nested hierarchy of\nmechanisms-within-mechanisms to account for a system’s behavior\nacknowledges the importance of mechanisms and intralevel causation at\nall levels constituting the system, not just at lower (i.e., cellular,\nmolecular) levels. \nDavid Kaplan and Craver (2011) focus the mechanist perspective\ncritically on dynamical systems mathematical models popular in recent\nsystems and computational neuroscience. They argue that such models\nare explanatory only if there exists a “plausible mapping”\nbetween elements in the model and elements in the modeled system. At\nbottom is their Model-to-Mechanism-Mapping (3M) Constraint on\nexplanation. The variables in a genuinely explanatory model correspond\nto components, activities, or organizational features of the system\nbeing explained. And the dependencies posited among variables in the\nmodel, typically expressed mathematically in systems and computational\nneuroscience, correspond to causal relations among the system’s\ncomponents. Kaplan and Craver justify the 3M Constraint on grounds of\nexplanatory norms, common to both science and common sense. All other\nthings being equal, they insist, explanations that provide more\nrelevant details about a system’s components, activities, and\norganization, more likely will answer more questions about how the\nsystem will behave in a variety of circumstances, than will an\nexplanation that provides fewer (mechanistic) details.\n“Relevant” here pertains to the functioning of the\nspecific mechanism. Models from systems and computational neuroscience\nthat violate the 3M Constraint are thus more reasonably thought of as\nmathematical descriptions of phenomena, not explanations of some\n“non-mechanistic” variety. \nKaplan and Craver challenge their own view with one of the more\npopular dynamical/mathematical models in all of computational\nneuroscience, the Haken-Kelso-Bunz (1985) model of human bimanual\nfinger-movement coordination. They point to passages in these\nmodelers’ publications that suggest that the modelers only\nintended for their dynamical systems model to be a mathematically\ncompact description of the temporal evolution of a “purely\nbehavioral dependent variable”. The modelers interpreted none of\nthe model’s variables or parameters as mapping onto components\nor operations of any hypothetical mechanism generating the behavioral\ndata. Nor did they intend for any of the model’s mathematical\nrelations or dependencies among variables to map onto hypothesized\ncausal interactions among components or activities of any mechanism.\nAs Kaplan and Craver further point out, after publishing their\ndynamicist model, these modelers themselves then began to investigate\nhow the behavioral regularities their model described might be\nproduced by neural motor system components, activities, and\norganization. Their own follow-up research suggests that these\nmodelers saw their dynamicist model as a heuristic, to help\nneuroscientists move toward “how-possibly”, and ultimately\nto a “how-actually” mechanistic explanation.  \nAt bottom, Kaplan and Craver’s 3M constraint on explanation\npresents a dilemma for dynamicists. To the extent that dynamical\nsystems modelers intend to model hypothesized neural mechanisms for\nthe phenomenon under investigation, their explanations will need to\ncohere with the 3M Constraint (and other canons of mechanistic\nexplanation). To the extent that this is not a goal of dynamicist\nmodelers, their models do not seem genuinely explanatory, at least not\nin one sense of “explanation” prominent in the history of\nscience. Furthermore, when dynamicist models are judged to be\nsuccessful, they often prompt subsequent searches for underlying\nmechanisms, just as the 3M Constraint and the general mechanist\naccount of the move from “how-possibly” to “how\nactually” mechanisms recommends. Either horn gores dynamicists\nwho claim that their models constitute a necessary additional kind of\nexplanation in neuroscience to mechanistic explanation, beyond any\nheuristic value such models might offer toward discovering\nmechanisms. \nKaplan and Craver’s radical conclusion, that dynamicist\n“explanations” are genuine explanations only to the degree\nthat they respect the (mechanist’s) 3M Constraint, needs more\ndefense. The burden of proof always lies on those whose conclusions\nstrike at popular assumptions. More than the discussion of a couple of\nlandmark dynamicist models in neuroscience is needed (in their 2011,\nKaplan and Craver also discuss the difference-of-Gaussians model of\nreceptive field properties of mammalian visual neurons). Expectedly,\ndynamicists have taken up this challenge. Michael Silberstein and\nAnthony Chemero (2013), for example, argue that localization and\ndecomposition strategies characterize mechanistic explanation, and\nthat some explanations in systems neuroscience violate one of these\nassumptions, or both. Such violations in turn create a dilemma for\nmechanists. Either they must “stretch” their account of\nexplanation, beyond decomposition and localization, to capture these\nrecalcitrant cases, or they must accept “counterexamples”\nto the generality of mechanistic explanation, in both systems\nneuroscience and systems biology more generally. \nLauren Ross (2015) and Mazviita Chirimuuta (2014) independently appeal\nto Robert Batterman’s account of minimal model explanation as an\nimportant kind of non-mechanistic explanation in neuroscience. Minimal\nmodels were developed initially to characterize a kind of explanation\nin the physical sciences (see, e.g., Batterman and Rice 2014).\nBatterman’s account distinguishes between two different kinds of\nscientific “why-questions”: why a phenomenon manifests in\nparticular circumstances; and why a phenomenon manifests generally, or\nin a number of different circumstances. Mechanistic explanations\nanswer the first type of why-question. Here a “more details the\nbetter” (MDB) assumption (Chirimuuta 2014), akin to Kaplan and\nCraver’s “all things being equal” assumption about\nbetter explanations (mentioned above), holds force. Minimal models,\nhowever, which minimize over the presented implantation details and\nhence violate MDB, are better able to answer the second type of\nscientific why-questions. Ross (2015), quoting from computational\nneuroscientists Rinzel and Ermentrout, insists that models containing\nmore details than necessary can obscure identification of critical\nelements by leaving too many open possibilities, especially when one\nis trying to answer Batterman’s second kind of why-questions\nabout a system’s behavior. \nChirimuuta and Ross each appeal to related resources from\ncomputational neuroscience to illustrate the applicability of\nBatterman’s minimal model explanation strategy. Ross appeals to\n“canonical models”, which represent “shared\nqualitative features of a number of distinct neural systems”\n(2015: 39). Her central example is the derivation of the\nErmentrout-Kopell model of class I neuron excitability, which uses\n“mathematical abstraction techniques” to “reduce\nmodels of molecularly distinct neural systems to a single …\ncanonical model”. Such a model “explains why molecularly\ndiverse neural systems all exhibit the same qualitative\nbehavior”, (2015: 41) clearly a Batterman second-type\nwhy-question. Chirimuuta’s resource is “canonical neural\ncomputations” (CNCs):  \ncomputational modules that apply the same fundamental operations in a\nvariety of contexts … a toolbox of computational operations\nthat the brain applies in a number of different sense modalities and\nanatomic regions and which can be described at higher levels of\nabstraction from their biophysical implementation. (Chirimuuta 2014:\n138)  \nExamples include shunting inhibition, linear filtering, recurrent\namplification, and thresholding. Rather than being mechanism-sketches,\nawaiting further mechanistic details to be turned into full-blown\nhow-actually mechanisms, CNCs are invoked in a different explanatory\ncontext, namely, ones posing Batterman’s second type of\nwhy-questions. Ross concurs concerning canonical models:  \nUnderstanding the approach dynamical systems neuroscientists take in\nexplaining [system] behavior requires attending to their explanandum\nof interest and the unique modeling tools [e.g., canonical models]\ncommon in their field. (2015: 52)  \nIn short, both Chirimuuta’s and Ross’s replies to Kaplan\nand Craver’s challenge is a common one in philosophy: save a\nparticular form of explanation from collapsing into another by\nsplitting the explanandum. \nFinally, to wrap up this discussion of mechanism ascendant, an\nanalogue of Craver’s (2007) problem of accounting for\n“constitutive mechanistic relevance”, that is, for\ndetermining which active components of a system are actually part of\nthe mechanism for a given system phenomenon, has also re-emerged in\nrecent discussions. Robert Rupert (2009) suggests that\n“integration” is a key criterion for determining which set\nof causally contributing mechanisms constitute the system for a task,\nbased on the relative frequency with which sets of mechanisms\nco-contribute to causing task occurrences. He cashes frequency of\nco-contribution as the probability of the set for causing the\ncognitive task, conditional to every other co-occurring causal set.\nFelipe De Brigard (2017) challenges Rupert’s criterion, arguing\nthat it cannot account for cognitive systems displaying two features,\n“diachronic dynamicity” along with “functional\nstability”. The frequency with which a given mechanism causally\ncontributes to the same cognitive task (functional stability) can\nchange over time (diachronic dynamicity). Although De Brigard\nemphasizes the critical importance of these features for\nRupert’s integration criterion via a fanciful thought\nexperiment, he also argues that they are a widespread phenomenon in\nhuman brains. Both features are found, for example, in evidence\npertaining to the “Hemispheric Asymmetry Reduction in Older\nAdults”, in which tasks that recruit hemispherically localized\nregions of prefrontal cortex in younger adults show a reduction in\nhemispheric asymmetry in older adults. And both are found in\n“Posterior-Anterior Shift with Aging”, where a task\nincreases activity in anterior brain regions while decreasing activity\nin posterior regions in older adults, relative to activity invoked by\nthe same task in younger adults. \nTo replace Rupert’s notion of integration as a criterion for\ndetermining which sets of mechanisms constitute a cognitive system, De\nBrigard points to two promising recent developments in network\nneuroscience which potentially allow for parametrized time.\n“Scaled inclusivity” is a method for examining each node\nin a network and identifying its membership in “community\nstructures” across different iterations of the network.\n“Temporal-dynamic network analyses” is a way to quantify\nchanges in community structures or modules between networks at\ndifferent time points. Both methods thereby identify “modular\nalliances”, which convey both co-activation and dynamic change\ninformation in a single model. De Brigard suggests that these are thus\nthe candidates with which cognitive systems could be identified. \nClearly, much remains to be discussed regarding the impact mechanism\nhas come to wield in philosophy of neuroscience over the last decade.\nBut while mechanism has become the most dominant general perspective\nin the field, work in other areas continues. Michael Anderson defends\nthe relevance of cognitive neuroscience for determining\npsychology’s taxonomy, independent of any commitment to\nmechanism. The most detailed development of his approach is in his\n(2014) book, After Phrenology, based on his influential\n“neural reuse” hypothesis. Each region of the brain, as\nrecognized by the standard techniques of cognitive neuroscience\n(especially fMRI), engages in cognitive functions that are highly\nvarious, and form different “neural partnerships” with one\nanother under different circumstances. Psychological categories are\nthen to be reconceived along lines suggested by the wide-ranging\nempirical data in support of neural reuse. A genuine\n“post-phrenological” science of the mind must jettison the\nassumption that each brain region performs its own fundamental\ncomputation. In this fashion Anderson’s work explicitly\ncontinues philosophy of neuroscience’s ongoing interest in\nlocalizations of cognitive functions. \nIn shorter compass, Anderson (2015) investigates the relevance of\ncognitive neuroscience for reconceiving psychology’s basic\ncategories, starting from a consequence of his neural reuse\nhypothesis. Attempts to map cognitive processes onto specific neural\nprocesses and brain regions reveal “many-to-many”\nrelations. Not only do these relations show that combined\nanatomical-functional labels for brain regions (e.g., “fusiform\nface area”) are deceptive; they also call into question the\npossibility of deciding between alternative psychological taxonomies\nby appealing to cognitive neuroscientific data. \nFor all but the strongest proponents of psychology’s autonomy\nfrom neuroscience, these many-to-many mappings will suggest that the\npsychological taxonomy we bring to this mapping project needs\nrevision. One need not be committed to any strong sense of\npsychoneural reduction, or the epistemological superiority of\ncognitive neuroscience to psychology, to draw this conclusion. The\nmere relevance of cognitive neuroscience for psychology’s\ncategories is enough. This debate is thus “about the\nrequirements for a unified science of the mind, and the proper role of\nneurobiological evidence in the construction of such an\nontology” (2015: 70), not about the legitimacy of either. \nAnderson divides revisionary projects for psychology into three kinds,\nbased on the degree of revision each kind recommends for psychology,\nand the extent of one-to-one function-to-structure mappings the\nproposed revisions predicts will be available.\n“Conservatives” foresee little need for extensive\nrevisions of psychology’s basic taxonomy, even as more\nneuroscientific evidence is taken into account than current standard\npractices pursue. “Moderates” insist that our knowledge of\nbrain function “can (and should) act as one arbiter of the\npsychologically real” (2015: 70), principally by\n“splitting” or “merging” psychological\nconcepts that currently are in use. “Radicals” project\neven more drastic revisions, even to the most primitive concepts of\npsychology, and even after such revisions they still do not expect\nthat many one-to-one mappings between brain regions and the new\npsychological primitives will be found. Although Anderson does not\nstress this connection (eliminative materialism has not been a\nprominent concern in philosophy of mind or neuroscience for two\ndecades), readers will notice similar themes discussed in\n section 2\n above, only now with scientific, not folk psychology the target of\nthe radical revisionists. A key criterion for any satisfactory\nreformulation of a cognitive ontology is the degree to which it\nsupports two kinds of inferences: “forward inferences”,\nfrom the engagement of a specific cognitive function to the prediction\nof brain activity; and “reverse inferences”, from the\nobservation that a specific brain region or pattern occurs to the\nprediction that a specific cognitive operation is engaged. In light of\nthis explicit criterion, Anderson usefully surveys the work of a\nnumber of prominent psychologists and cognitive neuroscientists in\neach of his revisionist groups. Given his broader commitment to neural\nreuse, and the trek it invites into “evolutionarily-inspired,\necological, and enactive terms”, Anderson’s own sentiments\nlie with the “radicals”:  \nlanguage and mathematics, for instance, are best understood as\nextensions of our basic affordance processing capacities augmented\nwith public symbol systems … The psychological science that\nresults from this reappraisal may well look very different from the\none we practice today. (2015: 75) \nLandmark neuroscientific hypotheses remain a popular focus in recent\nphilosophy of neuroscience. Berit Brogaard (2012), for example, argues\nfor a reinterpretation of the standard “dissociation”\nunderstanding of Melvin Goodale and David Milner’s (1992)\ncelebrated “two visual processing streams”, a landmark,\nnow “textbook” result from late-twentieth century\nneuroscience. Two components of the standard dissociation are key. The\nfirst is that distinct brain regions compute information relevant for\nvisually guided “on-the-fly” actions, and for object\nrecognition, respectively, the dorsal stream (which runs from primary\nvisual cortex through the medial temporal region into the superior and\ninferior parietal lobules) and the ventral stream (which runs from\nprimary visual cortex through V4 and into inferior temporal cortex).\nAnd second, that only information relevant for visual object\nrecognition, processed in the ventral stream, contributes to the\ncharacter of conscious visual experiences. \nBrogaard’s concern is that this standard understanding\nchallenges psychofunctionalism, our currently most plausible\n“naturalistic” account of mental states.\nPsychofunctionalism draws its account of mind directly from our best\ncognitive psychology. If φ is some mental state type that has\ninherited the content of a visual experience, then according to\ncognitive psychology a wide range of visually guided beliefs and\ndesires, different kinds of visual memories, and so on, satisfy\nφ’s description. But by the standard\n“dissociation” account of Goodale and Milner’s two\nvisual streams, only dorsal-stream states, and not ventral-stream\nstates, represent truly egocentric visual properties, namely\n“relational properties which objects instantiate from the point\nof view of believers or perceivers”, (Brogaard 2012: 572). But\naccording to cognitive psychology, dorsal-stream states do not play\nthis wide-ranging φ-role. So according to psychofunctionalism\n“φ-mental states cannot represent egocentric\nproperties” (2012: 572). But it seems “enormously\nplausible” that some of our perceptual beliefs and visual\nmemories represent egocentric properties. So either we reject\npsychofunctionalism, and so our most plausible naturalization project\nfor determining whether a given mental state is instantiated, or we\nreject the standard dissociation interpretation of Goodale and\nMilner’s two visual streams hypothesis, despite the wealth of\nempirical evidence supporting it. Neither horn of this dilemma looks\ncomfortably graspable, although the first horn might be thought to be\nmore so, since psychofunctionalism as a general theory of mind lacks\nthe kind of strong empirical backing that the standard interpretation\nof Goodale and Milner’s hypothesis enjoys. \nNevertheless, Brogaard recommends retaining psychofunctionalism, and\ninstead rejecting “a particular formulation” of Goodale\nand Milner’s two visual stream hypothesis. The interpretation to\nreject insists that “dorsal-stream information cannot contribute\nto the potentially conscious representations computed by the ventral\nstream” (2012: 586–587). Egocentric representations of\nvisual information computed by the dorsal stream contribute to\nconscious visual stream representations “via feedback\nconnections” from dorsal- to ventral-stream neurons (2012: 586).\nThis isn’t to deny dissociation:  \nInformation about the egocentric properties of objects is processed by\nthe dorsal stream, and information about allocentric properties of\nobjects is processed by the ventral stream. (2012: 586)  \nBut this dissociation hypothesis “has no bearing on what\ninformation is passed on to parts of the brain that process\ninformation which correlated with visual awareness” (2012: 586).\nWith this re-interpretation, psychofunctionalism is rendered\nconsistent with Goodale and Milner’s two stream, dorsal and\nventral, “what” and “where/how” hypothesis and\nthe wealth of empirical evidence that supports it. According to\nBrogaard, psychofunctionalism can thereby “correctly treat\nperceptual and cognitive states that carry information processed in\nthe ventral visual stream as capable of representing egocentric\nproperties” (2012: 586). \nDespite philosophy of neuroscience’s continuing focus on\ncognitive/systems/computational-neuroscience (see the discussion in\n section 7 above),\n interest in neurobiology’s cellular/molecular mainstream\nappears to be increasing. One notable paper is Ann-Sophie Barwich and\nKarim Bschir’s (2017) historical-cum-philosophical study of\nG-protein coupled receptors (GPCRs). Work on the structure and\nfunctional significance of these proteins has dominated molecular\nneuroscience for the past forty years; their role in the mechanisms of\na variety of cognitive functions is now empirically documented beyond\nquestion. And yet one finds little interest in, or even notice of this\nshift in mainstream neuroscience among philosophers. Barwich and\nBschir’s yeoman history research on the discovery and\ndevelopment of these objects pays off philosophically. The role of\nmanipulability as a criterion for entity realism in the\nscience-in-practice of wet-lab research becomes meaningful “only\nonce scientists have decided how to conceptually coordinate measurable\neffects distinctly to a scientific object” (2017: 1317).\nScientific objects like GPCRs get assigned varying degrees of reality\nthroughout different stages of the discovery process. Such an\nobject’s role in evaluating the reality of “neighboring\nelements of enquiry” becomes a part of the criteria of its\nreality as well. \nThe impact of science-in-practice on philosophy of science generally\nhas been felt acutely in the philosophy of neuroscience, most notably\nin increased philosophical interest in neuroscientific\nexperimentation. In itself this should not surprise. Neuroscience\nrelies heavily on laboratory experimentation, especially within its\ncellular and molecular, “Society for Neuroscience”\nmainstream. So the call to understand experiment should beckon any\nphilosopher who ventures into neuroscience’s cellular/molecular\nfoundations. Two papers by Jacqueline Sullivan (2009, 2010) have been\nimportant in this new emphasis. In her (2009) Sullivan acknowledges\nboth Bickle’s (2003) and Craver’s (2007) focus on cellular\nand molecular mechanisms of long-term potentiation, and\nexperience-driven form of synaptic plasticity. But she insists that\nbroader philosophical commitments, which lead Bickle to ruthless\nreductionist and Craver to mosaic unity “global” accounts,\nobscure important aspects of real laboratory neuroscience practice.\nShe emphasizes the role of “subprotocols”, which specify\nhow data are to be gathered, in her model of “the experimental\nprocess”, and illustrates these notions with a number of\nexamples. Her analysis reveals an important underappreciated tension\namong a pair of widely-accepted experiment norms. Pursuing\n“reliability” drives experimenters more deeply into\nextensive laboratory controls. Pursuing “external\nvalidity” drives them toward enriched experimental environments\nthat more closely represent the messy natural environment beyond the\nlaboratory. These two norms commonly conflict: in order to get more of\none, scientists introduce conditions that give them less of the other.\n \nIn her (2010) Sullivan offers a detailed history of the Morris water\nmaze task, tracing her account back to Morris’s original\npublications. Philosophers of neuroscience have uncritically assumed\nthat the water maze is a widely-accepted neuroscience protocol for\nrodent spatial learning and memory, but the detailed scientific\nhistory is not so clear on this interpretation. Scientific commentary\nover time on what this task measures, including some from Morris\nhimself, reveals no clear consensus. Sullivan traces the source of\nthis scientific inconsistency back to the impact of 1980s-era\ncellular-molecular reductionism driving experimental behavioral\nneurobiology protocols like the Morris water maze. \nA different motivation drives neurobiologist Alcino Silva,\nneuroinformaticist Anthony Landreth, and philosopher of neuroscience\nJohn Bickle’s (2014) focus on experimentation. All contemporary\nsciences are growing at a vertiginous pace; but perhaps none more so\nthan neuroscience. It is no longer possible for any single scientist\nto keep up with all the relevant published literature in even his or\nher narrow research field, or fully to comprehend its implications. An\noverall lack of clarity and consensus about what is known, what\nremains doubtful, and what has been disproven creates special problems\nfor experiment planning. There is a recognized and urgent need to\ndevelop strategies and tools to address these problems. Toward this\nexplicit end, Silva, Landreth, and Bickle’s book describes a\nframework and a set of principles for organizing the published record.\nThey derive their framework and principles directly from landmark case\nstudies from the influential neuroscientific field of molecular and\ncellular cognition (MCC), and describe how their framework can be used\nto generate maps of experimental findings. Scientists armed with these\nresearch maps can then determine more efficiently what has been\naccomplished in their fields, and where the knowledge gaps still\nreside. The technology needed to automate the generation of these maps\nalready exists. Silva, Landreth, and Bickle sketch the transformative,\nrevolutionary impact these maps can have on current science. \nThree goals motivate Silva, Landreth, and Bickle’s approach.\nFirst, they derive their framework from the cellular and molecular\nneurobiology of learning and memory. This choice was due strictly to\nfamiliarity with the science. Silva was instrumental in bringing gene\ntargeting techniques applied to mammals into behavioral neuroscience,\nand Bickle’s focus on ruthlessly reductive neuroscience was\nbuilt on these and other experimental results. And while each of their\nframework’s different kinds of experiments and evidence have\nbeen recognized by others, theirs purports to be the first to\nsystematize this information explicitly toward the goal of\nfacilitating experimental planning by practicing scientists. Silva,\nLandreth and Bickle insist that important new experiments can be\nidentified and planned by methodically filling in the different forms\nof evidence recognized by their framework, and applying the different\nforms of experiments to the gaps in the experimental record revealed\nby this process. \nSecond, Silva, Landreth, and Bickle take head-on the problem of the\ngrowing amount, complexity and integration of the published literature\nfor experiment planning. They show how graphic weighted\nrepresentations of research findings can be used to guide research\ndecisions; and how to construct these. The principles for constructing\nthese maps are the principles for integrating experimental results,\nderived directly from landmark published MCC research. Using a case\nstudy from recent molecular neuroscience, they show how to generate\nsmall maps that reflect a series of experiments, and how to combine\nthese small maps to illustrate an entire field of neuroscience\nresearch. \nFinally, Silva, Landreth and Bickle begin to develop a science of\nexperiment planning. They envision the causal graphs that compose\ntheir research maps to play a role similar to that played by\nstatistics in the already-developed science of data analysis. Such a\nresource could have profound implications for further developing\ncitation indices and other impact measures for evaluating\ncontributions to a field, from those of individual scientists to those\nof entire institutions. \nMore recently Bickle and Kostko (2018) have extended Silva, Landreth\nand Bickle’s framework beyond the neurobiology of learning and\nmemory. Their case study comes from developmental and social\nneuroscience, Michael Meaney and Moshe Szyf’s work on the\nepigenetics of rodent maternal nursing behaviors on offspring stress\nresponses. Using the details of this case study they elaborate on a\nnotion that Silva, Landreth and Bickle leave underdeveloped, that of\nexperiments designed explicitly for their results, if successful, to\nbe integrated directly into an already-existing background of\nestablished results. And they argue that such experiments\n“integratable by design” with others are aimed not at\nestablishing evidence for individual causal relations among\nneuroscientific kinds, but rather at formulating entire causal\npathways connecting multiple phenomena. Their emphasis on causal paths\nrelates to that of Lauren Ross (forthcoming). Ross’s work is\nespecially interesting in this context because she uses her causal\npathway concept to address “causal selection”, which has\nto do with distinguishing between background conditions and\n“true” (triggering) causes of some outcome of interest.\nFor Silva, Landreth, and Bickle (2014), accounting for this\ndistinction is likewise crucial, and they rely on a specific kind of\nconnection experiment, “positive manipulations”, to draw\nit. Bickle and Kostko’s appeal to causal paths in a detailed\ncase study from recent developmental neurobiology might help bridge\nSilva, Landreth and Bickle’s broader work on neurobiological\nexperimentation with Ross’s work drawn from biology more\ngenerally.","contact.mail":"jb1681@msstate.edu","contact.domain":"msstate.edu"},{"date.published":"1999-06-07","date.changed":"2019-08-06","url":"https://plato.stanford.edu/entries/neuroscience/","author1":"John Bickle","author2":"Anthony Landreth","author1.info":"http://www.philosophyandreligion.msstate.edu/faculty/bickle.php","author2.info":"http://www.petemandik.com/philosophy/philosophy.html","entry":"neuroscience","body.text":"\n\n\nOver the past four decades, philosophy of science has grown\nincreasingly “local”. Concerns have switched from general\nfeatures of scientific practice to concepts, issues, and puzzles\nspecific to particular disciplines. Philosophy of neuroscience is one\nnatural result. This emerging area was also spurred by remarkable\ngrowth in the neurosciences themselves. Cognitive and computational\nneuroscience continues to encroach directly on issues traditionally\naddressed within the humanities, including the nature of\nconsciousness, action, knowledge, and normativity. Cellular,\nmolecular, and behavioral neuroscience using animal models\nincreasingly encroaches on cognitive neuroscience’s domain.\nEmpirical discoveries about brain structure and function suggest ways\nthat “naturalistic” programs might develop in detail,\nbeyond the abstract philosophical considerations in their favor.\n\n\nThe literature has distinguished “philosophy of\nneuroscience” from “neurophilosophy” for two\ndecades. The former concerns foundational issues within the\nneurosciences. The latter concerns application of neuroscientific\nconcepts to traditional philosophical questions. Exploring various\nconcepts of representation employed in neuroscientific theories is an\nexample of the former. Examining implications of neurological\nsyndromes for the concept of a unified self is an example of the\nlatter. In this entry, we will develop this distinction further and\ndiscuss examples of both. Just as has happened in the field’s\nhistory, work in both of these areas is scattered throughout most all\nsections below. Throughout we will try to specify which area landmark\nwork falls into, when this location isn’t obvious.\n\n\nOne exciting aspect about working in philosophy of neuroscience or\nneurophilosophy is the continual element of surprise. Both fields\ndepend squarely on developments in neuroscience, and one simply has no\ninkling what’s coming down the pike in that incredibly\nfast-moving science. Last year’s speculative fiction is this\nyear’s scientific reality. But this feature makes a once-a-half\ndecade updated encyclopedia entry difficult to manage. The scientific\ndetails philosophers were reflecting on at past updates can now read\nwoefully dated. Yet one also wants to capture some history of the\nongoing fields. Our solution to this dilemma has been to keep previous\ndiscussions, to reflect that history, but to add more recent\nscientific and philosophical updates, not only to sections of this\nentry added at later times, but also peppered through the earlier\ndiscussions. It’s not always a perfect solution, but it does\npreserve something of the history of the philosophy of neuroscience\nand neurophilosophy against the continual advances in the sciences\nthese philosophical fields depend upon. \n\nHistorically, neuroscientific discoveries exerted little influence on\nthe details of materialist philosophies of mind. The\n“neuroscientific milieu” of the past half-century has made\nit harder for philosophers to adopt substantive dualisms about mind.\nBut even the “type-type” or “central state”\nidentity theories that rose to brief prominence in the late 1950s\n(Place 1956; Smart 1959) drew upon few actual details of the emerging\nneurosciences. Recall the favorite early example of a psychoneural\nidentity claim: “pain is identical to C-fiber firing”. The\n“C-fibers” turned out to be related to only a single\naspect of pain transmission (Hardcastle 1997). Early identity\ntheorists did not emphasize psychoneural identity hypotheses. Their\n“neuro” terms were admittedly placeholders for concepts\nfrom future neuroscience. Their arguments and motivations were\nphilosophical, even if the ultimate justification of the program was\nheld to be empirical. \nThe apology offered by early identity theorists for ignoring\nscientific details was that the neuroscience at that time was too\nnascent to provide any plausible identities. But potential identities\nwere afoot. David Hubel and Torsten Wiesel’s (1962)\nelectrophysiological demonstrations of the receptive field properties\nof visual neurons had been reported with great fanfare. Using their\ntechniques, neurophysiologists began discovering neurons throughout\nvisual cortex responsive to increasingly abstract features of visual\nstimuli: from edges to motion direction to colors to properties of\nfaces and hands. More notably, Donald Hebb had published The\nOrganization of Behavior (1949) more than a decade earlier. He\nhad offered detailed explanations of psychological phenomena in terms\nof neural mechanisms and anatomical circuits. His psychological\nexplananda included features of perception, learning, memory, and even\nemotional disorders. He offered these explanations as potential\nidentities. (See the Introduction to his 1949). One philosopher who\ndid take note of some available neuroscientific detail at the time was\nBarbara Von Eckardt Klein (Von Eckardt Klein 1975). She discussed the\nidentity theory with respect to sensations of touch and pressure, and\nincorporated then-current hypotheses about neural coding of sensation\nmodality, intensity, duration, and location as theorized by\nMountcastle, Libet, and Jasper. Yet she was a glaring exception. By\nand large, available neuroscience at the time was ignored by both\nphilosophical friends and foes of early identity theories. \nPhilosophical indifference to neuroscientific detail became\n“principled” with the rise and prominence of functionalism\nin the 1970s. The functionalists’ favorite argument was based on\nmultiple realizability: a given mental state or event can be realized\nin a wide variety of physical types (Putnam 1967; Fodor 1974).\nConsequently, a detailed understanding of one type of realizing\nphysical system (e.g., brains) will not shed light on the fundamental\nnature of mind. Psychology is thus autonomous from any science of one\nof its possible physical realizers (see the\n entry on multiple realizability\n in this Encyclopedia). Instead of neuroscience, scientifically-minded\nphilosophers influenced by functionalism sought evidence and\ninspiration from cognitive psychology and artificial intelligence.\nThese disciplines abstract away from underlying physical mechanisms\nand emphasize the “information-bearing” properties and\ncapacities of representations (Haugeland 1985). At this same time,\nhowever, neuroscience was delving directly into cognition, especially\nlearning and memory. For example, Eric Kandel (1976) proposed\npresynaptic mechanisms governing transmitter release rate as a\ncell-biological explanation of simple forms of associative learning.\nWith Robert Hawkins (Hawkins and Kandel 1984) he demonstrated how\ncognitivist aspects of associative learning (e.g., blocking,\nsecond-order conditioning, overshadowing) could be explained\ncell-biologically by sequences and combinations of these basic forms\nimplemented in higher neural anatomies. Working on the post-synaptic\nside, neuroscientists began unraveling the cellular mechanisms of long\nterm potentiation (LTP; Bliss and Lomo 1973). Physiological\npsychologists quickly noted its explanatory potential for various\nforms of learning and\n memory.[1]\n Yet few “materialist” philosophers paid any attention.\nWhy should they? Most were convinced functionalists. They believed\nthat the “implementation level” details might be important\nto the clinician, but were irrelevant to the theorist of mind. \nA major turning point in philosophers’ interest in neuroscience\ncame with the publication of Patricia Churchland’s\nNeurophilosophy (1986). The Churchlands (Patricia and Paul)\nwere already notorious for advocating eliminative materialism (see the\nnext section). In her (1986) book, Churchland distilled eliminativist\narguments of the past decade, unified the pieces of the philosophy of\nscience underlying them, and sandwiched the philosophy between a\nfive-chapter introduction to neuroscience and a 70-page chapter on\nthree then-current theories of brain function. She was unapologetic\nabout her intent. She was introducing philosophy of science to\nneuroscientists and neuroscience to philosophers. Nothing could be\nmore obvious, she insisted, than the relevance of empirical facts\nabout how the brain works to concerns in the philosophy of mind. Her\nterm for this interdisciplinary method was “co-evolution”\n(borrowed from biology). This method seeks resources and ideas from\nanywhere on the theory hierarchy above or below the question at issue.\nStanding on the shoulders of philosophers like Quine and Sellars,\nChurchland insisted that specifying some point where neuroscience ends\nand philosophy of science begins is hopeless because the boundaries\nare poorly defined. Neurophilosophers would pick and choose resources\nfrom both disciplines as they saw fit. \nThree themes predominated Churchland’s philosophical discussion:\ndeveloping an alternative to the logical empiricist theory of\nintertheoretic reduction; responding to property-dualistic arguments\nbased on subjectivity and sensory qualia; and responding to\nanti-reductionist multiple realizability arguments. These projects\nremained central to neurophilosophy for more than a decade after\nChurchland’s book appeared. John Bickle (1998) extended the\nprincipal insight of Clifford Hooker’s (1981a,b,c)\npost-empiricist theory of intertheoretic reduction. He quantified key\nnotions using a model-theoretic account of theory structure adapted\nfrom the structuralist program in philosophy of science (Balzer,\nMoulines, and Sneed 1987). He also made explicit a form of argument to\ndraw ontological conclusions (cross-theoretic identities, revisions,\nor eliminations) from the nature of the intertheoretic reduction\nrelations obtaining in specific cases. For example, it is routinely\nconcluded that visible light, a theoretical posit of optics, is\nelectromagnetic radiation within specified wavelengths, a theoretical\nposit of electromagnetism; in this case, a cross-theoretic ontological\nidentity. It is also routine to conclude that phlogiston does not\nexist: an elimination of a kind from our scientific ontology. Bickle\nexplicated the nature of the reduction relation in a specific case\nusing a semi-formal account of “intertheoretic\napproximation” inspired by structuralist results. \nPaul Churchland (1996) carried on the attack on property-dualistic\narguments for the irreducibility of conscious experience and sensory\nqualia. He argued that acquiring some knowledge of existing sensory\nneuroscience increases one’s ability to “imagine” or\n“conceive of” a comprehensive neurobiological explanation\nof consciousness. He defended this conclusion using a\ncharacteristically imaginative thought-experiment based on the history\nof optics and electromagnetism. \nFinally, criticisms of the multiple realizability argument\nflourish—and are challenged—to the present day. Although\nthe multiple realizability argument remains influential among\nnonreductive physicalists, it no longer commands the near-universal\nacceptance it once did. Replies to the multiple realizability argument\nbased on neuroscientific details have appeared. For example, William\nBechtel and Jennifer Mundale (1999) argue that neuroscientists use\npsychological criteria in brain mapping studies. This fact undercuts\nthe likelihood that psychological kinds are multiply realized (for a\nreview of recent developments see the\n entry on multiple realizability\n in this Encyclopedia). \nEliminative materialism (EM), in the form advocated most aggressively\nby Paul and Patricia Churchland, is the conjunction of two claims.\nFirst, our common sense “belief-desire” conception of\nmental events and processes, our “folk psychology”, is a\nfalse and misleading account of the causes of human behavior. Second,\nlike other false conceptual frameworks from both folk theory and the\nhistory of science, it will be replaced by, rather than smoothly\nreduced or incorporated into, a future neuroscience. The\nChurchlands’ characterized folk psychology as the collection of\ncommon homilies invoked (mostly implicitly) to explain human behavior\ncausally. You ask why Marica is not accompanying me this evening. I\nreply that our grandson needed sitting. You nod sympathetically. You\nunderstand my explanation because you share with me a generalization\nthat relates beliefs about taking care of grandchildren, desires to\nhelp daughters and to spend time with grandchildren compared to\nenjoying a night out, and so on. This is just one of a huge collection\nof homilies about the causes of human behavior that EM claims to be\nflawed beyond potential revision. Although this example involves only\nbeliefs and desires, folk psychology contains an extensive repertoire\nof propositional attitudes in its explanatory nexus: hopes,\nintentions, fears, imaginings, and more. EMists predict that a future,\ngenuinely scientific psychology or neuroscience will eventually eschew\nall of these, and replace them with incommensurable states and\ndynamics of neuro-cognition. \nEM is physicalist in one traditional philosophical sense. It\npostulates that some future brain science will be ultimately the\ncorrect account of (human) behavior. It is eliminative in predicting\nthe future rejection of folk psychological kinds from our\npost-neuroscientific ontology. EM proponents often employ scientific\nanalogies (Feyerabend 1963; Paul Churchland, 1981). Oxidative\nreactions as characterized within elemental chemistry bear no\nresemblance to phlogiston release. Even the “direction” of\nthe two processes differ. Oxygen is gained when an object burns (or\nrusts), phlogiston was said to be lost. The result of this theoretical\nchange was the elimination of phlogiston from our scientific ontology.\nThere is no such thing. For the same reasons, according to EM,\ncontinuing development in neuroscience will reveal that there are no\nsuch things as beliefs, desires, and the rest of the propositional\nattitudes as characterized by common sense. \nHere we focus only on the way that neuroscientific results have shaped\nthe arguments for EM. Surprisingly, only one argument has been\nstrongly influenced. (Most arguments for EM stress failures of folk\npsychology as an explanatory theory of behavior.) This argument is\nbased on a development in cognitive and computational neuroscience\nthat might provide a genuine alternative to the representations and\ncomputations implicit in folk psychological generalizations. Many\neliminative materialists assume that folk psychology is committed to\npropositional representations and computations over their contents\nthat mimic logical inferences (Paul Churchland 1981; Stich 1983;\nPatricia Churchland\n 1986).[2]\n Even though discovering an alternative to this view has been an\neliminativist goal for some time, some eliminativists hold that\nneuroscience only began delivering this alternative over the past\nthirty years. Points in and trajectories through vector spaces, as an\ninterpretation of synaptic events and neural activity patterns in\nbiological and artificial neural networks are the key features of this\nalternative. The differences between these notions of cognitive\nrepresentation and transformations, and those of the propositional\nattitudes of folk psychology, provide the basis for one argument for\nEM (Paul Churchland 1987). However, this argument will be opaque to\nthose with no background in cognitive and computational neuroscience,\nso we present a few details. With these details in place, we will\nreturn to this argument for EM (five paragraphs below). \nAt one level of analysis, the basic computational element of a neural\nnetwork, biological or artificial, is the nerve cell, or neuron.\nMathematically, neurons can be represented as simple computational\ndevices, transforming inputs into output. Both inputs and outputs\nreflect biological variables. For our discussion, we assume that\nneuronal inputs are frequencies of action potentials (neuronal\n“spikes”) in the axons whose terminal branches synapse\nonto the neuron in question, while neuronal output is the frequency of\naction potentials generated in its axon after processing the inputs. A\nneuron thereby computes its total input, usually treated\nmathematically as the sum of the products of the signal strength along\neach input line times the synaptic weight on that line. It then\ncomputes a new activation state based on its total input and current\nactivation state, and a new output state based on its new activation\nvalue. The neuron’s output state is transmitted as a signal\nstrength to whatever neurons its axon synapses on. The output state\nreflects systematically the neuron’s new activation\n state.[3] \nAnalyzed in this fashion, both biological and artificial neural\nnetworks are interpreted naturally as vector-to-vector\ntransformers. The input vector consists of values reflecting\nactivity patterns in axons synapsing on the network’s neurons\nfrom outside (e.g., from sensory transducers or other neural\nnetworks). The output vector consists of values reflecting the\nactivity patterns generated in the network’s neurons that\nproject beyond the net (e.g., to motor effectors or other neural\nnetworks). Given that each neuron’s activity depends partly upon\ntheir total input, and its total input depends partly on synaptic\nweights (e.g., presynaptic neurotransmitter release rate, number and\nefficacy of postsynaptic receptors, availability of enzymes in\nsynaptic cleft), the capacity of biological networks to change their\nsynaptic weights make them plastic vector-to-vector\ntransformers. In principle, a biological network with plastic synapses\ncan come to implement any vector-to-vector transformation that its\ncomposition permits (number of input units, output units, processing\nlayers, recurrency, cross-connections, etc.) (discussed in Paul\nChurchland, 1987, with references to the primary scientific\nliterature). Figure 1. \nThe anatomical organization of the cerebellum provides a clear example\nof a network amenable to this computational interpretation. Consider\n Figure 1.\n The cerebellum is the bulbous convoluted structure dorsal to the\nbrainstem. A variety of studies (behavioral, neuropsychological,\nsingle-cell electrophysiological) implicate this structure in motor\nintegration and fine motor coordination. Mossy fibers (axons) from\nneurons outside the cerebellum synapse on cerebellar granule cells,\nwhich in turn project to parallel fibers. Activity patterns across the\ncollection of mossy fibers (frequency of action potentials per time\nunit in each fiber projecting into the cerebellum) provide values for\nthe input vector. Parallel fibers make multiple synapses on the\ndendritic trees and cell bodies of cerebellular Purkinje neurons. Each\nPurkinje neuron “sums” its post-synaptic potentials (PSPs)\nand emits a train of action potentials down its axon based (partly) on\nits total input and previous activation state. Purkinje axons project\noutside the cerebellum. The network’s output vector is thus the\nordered values representing the pattern of activity generated in each\nPurkinje axon. Changes to the efficacy of individual synapses on the\nparallel fibers and the Purkinje neurons alter the resulting PSPs in\nPurkinje axons, generating different axonal spiking frequencies.\nComputationally, this amounts to a different output vector to the same\ninput activity\n pattern—plasticity.[4] Figure 2. \nThis interpretation puts the useful mathematical resources of\ndynamical systems into the hands of computational\nneuroscientists. Vector spaces are an example. Learning can\nthen be characterized fruitfully in terms of changes in synaptic\nweights in the network and subsequent reduction of error in network\noutput. (This approach to learning goes back to Hebb 1949, although\nthe vector-space interpretation was not part of Hebb’s account.)\nA useful representation of this account uses a synaptic\nweight-error space. One dimension represents the global error in\nthe network’s output to a given task, and all other dimensions\nrepresent the weight values of individual synapses in the network.\nConsider\n Figure 2.\n Points in this multi-dimensional state space represent the global\nperformance error correlated with each possible collection of synaptic\nweights in the network. As the weights change with each performance,\nin accordance with a biologically-inspired learning algorithm, the\nglobal error of network performance continually decreases. The\nchanging synaptic weights across the network with each training\nepisode reduces the total error of the network’s output vector,\ncompared to the desired output vector for the input vector. Learning\nis represented as synaptic weight changes correlated with a descent\nalong the error dimension in the space (Churchland and Sejnowski\n1992). Representations (concepts) can be portrayed as\npartitions in multi-dimensional vector spaces. One example is\na neuron activation vector space. See\n Figure 3.\n A graph of such a space contains one dimension for the activation\nvalue of each neuron in the network (or some specific subset of the\nnetwork’s neurons, such as those in a specific layer). A point\nin this space represents one possible pattern of activity in all\nneurons in the network. Activity patterns generated by input vectors\nthat the network has learned to group together will cluster around a\n(hyper-) point or subvolume in the activity vector space. Any input\npattern sufficiently similar to this group will produce an activity\npattern lying in geometrical proximity to this point or subvolume.\nPaul Churchland (1989) argued that this interpretation of network\nactivity provided a quantitative, neurally-inspired basis for\nprototype theories of concepts developed in late-twentieth century\ncognitive psychology. Figure 3. \nUsing this theoretical development, and in the realm of\nneurophilosophy, Paul Churchland (1987, 1989) offered a novel,\nneuroscientifically-inspired argument for EM. According to the\ninterpretation of neural networks just sketched, activity vectors are\nthe central kind of representations, and vector-to-vector\ntransformations are the central kind of computations, in the brain.\nThis contrasts sharply with the propositional representations\nand logical/semantic computations postulated by folk\npsychology. Vectorial content, an ordered sequence of real numbers, is\nunfamiliar and alien to common sense. This cross-theoretic conceptual\ndifference is at least as great as that between oxidative and\nphlogiston concepts, or kinetic-corpuscular and caloric fluid heat\nconcepts. Phlogiston and caloric fluid are two “parade”\nexamples of kinds eliminated from our scientific ontology due to the\nnature of the intertheoretic relation obtaining between the theories\nwith which they are affiliated and the theories that replaced them.\nThe structural and dynamic differences between the folk psychological\nand then-emerging cognitive neuroscientific kinds suggested that the\ntheories affiliated with the latter will likewise replace the theory\naffiliated with the former. But this claim was the key premise of the\neliminativist argument based on predicted intertheoretic relations.\nAnd with the rise of neural networks and parallel distributed\nprocessing, intertheoretic contrasts with folk-psychological\nexplanatory kinds were no longer just an eliminativist’s future\nhope. Computational and cognitive neuroscience was delivering an\nalternative kinematics for cognition, one that provided no structural\nanalogue for folk psychology’s propositional attitudes or\nlogic-like computations over propositional contents. \nCertainly the vector-space alternatives of this interpretation of\nneural networks are alien to folk psychology. But do they justify EM?\nEven if the propositional contents of folk-psychological posits find\nno analogues in one theoretical development in cognitive and\ncomputational neuroscience (that was hot three decades ago), there\nmight be other aspects of cognition that folk psychology gets right.\nWithin the scientific realism that informed early neurophilosophy,\nconcluding that a cross-theoretic identity claim is true (e.g., folk\npsychological state F is identical to neural state N) or that an\neliminativist claim is true (there is no such thing as folk\npsychological state F) depended on the nature of the intertheoretic\nreduction obtaining between the theories affiliated with the posits in\nquestion (Hooker 1981a,b,c; Churchland 1986; Bickle, 1998). But the\nunderlying account of intertheoretic reduction also recognized a\nspectrum of possible reductions, ranging from relatively\n“smooth” through “significantly revisionary”\nto “extremely\n bumpy”.[5]\n Might the reduction of folk psychology to a “vectorial”\ncomputational neuroscience occupy some middle ground between\n“smooth” and “bumpy” intertheoretic reduction\nendpoints, and hence suggest a “revisionary” conclusion?\nThe reduction of classical equilibrium thermodynamics-to-statistical\nmechanics provided a potential analogy here. John Bickle (1992, 1998,\nchapter 6) argued on empirical grounds that such an outcome is likely.\nHe specified conditions on “revisionary” reductions from\nhistorical examples and suggested that these conditions are obtaining\nbetween folk psychology and cognitive neuroscience as the latter\ndevelops. In particular, folk psychology appears to have gotten right\nthe grossly-specified functional profile of many cognitive states,\nespecially those closely related to sensory inputs and behavioral\noutputs. It also appears to get right the “intentionality”\nof many cognitive states—the object that the state is of or\nabout—even though cognitive neuroscience eschews its implicit\nlinguistic explanation of this feature. Revisionary physicalism\npredicts significant conceptual change to folk psychological\nconcepts, but denies total elimination of the caloric fluid-phlogiston\nvariety. \nThe philosophy of science is another area where vector space\ninterpretations of neural network activity patterns has impacted\nphilosophy. In the Introduction to his (1989) book, A\nNeurocomputational Perspective, Paul Churchland asserted,\ndistinctively neurophilosophically, that it will soon be impossible to\ndo serious work in the philosophy of science without drawing on\nempirical work in the brain and behavioral sciences. To justify this\nclaim, in Part II of the book he suggested neurocomputational\nreformulations of key concepts from the philosophy of science. At the\nheart of his reformulations is a neurocomputational account of the\nstructure of scientific theories (1989: chapter 9). Problems with the\northodox “sets-of-sentences” view of scientific theories\nhave been well-known since the 1960s. Churchland advocated replacing\nthe orthodox view with one inspired by the “vectorial”\ninterpretation of neural network activity. Representations implemented\nin neural networks (as sketched above) compose a system that\ncorresponds to important distinctions in the external environment, are\nnot explicitly represented as such within the input corpus, and allow\nthe trained network to respond to inputs in a fashion that continually\nreduces error. According to Churchland, these are functions of\ntheories. Churchland was bold in his assertion: an individual’s\ntheory-of-the-world is a specific point in that individual’s\nerror-synaptic weight vector space. It is a configuration of synaptic\nweights that partitions the individual’s activation vector space\ninto subdivisions that reduce future error messages to both familiar\nand novel inputs. (Consider again\n Figure 2\n and\n Figure 3.)\n This reformulation invites an objection, however. Churchland boasts\nthat his theory of theories is preferable to existing alternatives to\nthe orthodox “sets-of-sentences” account—for\nexample, the semantic view (Suppe 1974; van Fraassen\n1980)—because his is closer to the “buzzing brains”\nthat use theories. But as Bickle (1993) noted, neurocomputational models based on the\nmathematical resources described above are a long way into the realm\nof mathematical abstraction. They are little more than novel (albeit\nsuggestive) application of the mathematics of quasi-linear dynamical\nsystems to simplified schemata of brain circuitries. Neurophilosophers\nowe some account of identifications across ontological categories\n(vector representations and transformation to what?) before the\nphilosophy of science community will treat theories as points in\nhigh-dimensional state spaces implemented in biological neural\nnetworks. (There is an important methodological assumption lurking in\nBickle’s objection, however, which we will discuss toward the\nend of the next paragraph.) \nChurchland’s neurocomputational reformulations of other\nscientific and epistemological concepts build on this account of\ntheories. He sketches “neuralized” accounts of the\ntheory-ladenness of perception, the nature of concept unification, the\nvirtues of theoretical simplicity, the nature of Kuhnian paradigms,\nthe kinematics of conceptual change, the character of abduction, the\nnature of explanation, and even moral knowledge and epistemological\nnormativity. Conceptual redeployment, for example, is the activation\nof an already-existing prototype representation—the centerpoint\nor region of a partition of a high-dimensional vector space in a\ntrained neural network—by a novel type of input pattern.\nObviously, we can’t here do justice to Churchland’s many\nand varied attempts at reformulation. We urge the intrigued reader to\nexamine his suggestions in their original form. But a word about\nphilosophical methodology is in order. Churchland is not\nattempting “conceptual analysis” in anything resembling\nits traditional philosophical sense. Neither, typically, are\nneurophilosophers in any of their reformulation projects. (This is why\na discussion of neurophilosophical reformulations fits with a\ndiscussion of EM.) There are philosophers who take the\ndiscipline’s ideal analyses to be a relatively simple set of\nnecessary and sufficient conditions, expressed in non-technical\nnatural language, governing the application of important concepts\n(like justice, knowledge, theory, or explanation). These analyses\nshould square, to the extent possible, with pretheoretical usage.\nIdeally, they should preserve synonymy. Other philosophers view this\nideal as sterile, misguided, and perhaps deeply mistaken about the\nunderlying structure of human knowledge (Ramsey 1992).\nNeurophilosophers tend to reside in the latter group. Those who\ndislike philosophical speculation about the promise and potential of\ndeveloping science to reformulate\n(“reform-ulate”) traditional philosophical\nconcepts have probably already discovered that neurophilosophy is not\nfor them. But the familiar charge that neurocomputational\nreformulations of the sort Churchland attempts are\n“philosophically uninteresting” or\n“irrelevant” because they fail to provide\n“analyses” of theory, explanation, and the like will fall\non deaf ears among many contemporary “naturalistic”\nphilosophers, who have by and large given up on traditional\nphilosophical “analysis”. \nBefore we leave the topic of proposed neurophilosophical applications\nof this theoretical development from “neural\nnetworks”-style cognitive/computational neuroscience, one final\npoint of actual scientific detail bears mention. This approach did not\nremain state-of-the-art computational neuroscience for long.\nMany neural modelers quickly gave up this approach to\nmodeling the brain. Compartmental modeling enabled\ncomputational neuroscientists to mimic activity in and\ninteractions between patches of neuronal membrane (Bower and Beeman\n1995). This approach permitted modelers to control and manipulate a\nvariety of subcellular factors that determine action potentials per\ntime unit, including the topology of membrane structure in individual\nneurons, variations in ion channels across membrane patches, and field\nproperties of post-synaptic potentials depending on the location of\nthe synapse on the dendrite or soma. By the mid-1990s modelers quickly\nbegan to “custom build” the neurons in their target\ncircuitry. Increasingly powerful computer hardware still allowed them\nto study circuit properties of modeled networks. For these reasons,\nmany serious computational neuroscientists switched to working at a\nlevel of analysis that treats neurons as structured rather than simple\ncomputational devices. With compartmental modeling, vector-to-vector\ntransformations came to be far less useful in serious neurobiological\nmodels, replaced by differential equations representing ion currents\nacross patches of neural membrane. Far more biological detail came to\nbe captured in the resulting models than “connectionist”\nmodels permitted. This methodological change across computational\nneuroscience meant that a neurophilosophy guided by\n“connectionist” resources no longer drew from the state of\nthe art of the scientific field. \nPhilosophy of science and scientific epistemology were not the only\nareas where neurophilosophers urged the relevance of neuroscientific\ndiscoveries for traditionally philosophical topics. A decade after\nNeurophilosophy’s publication, Kathleen Akins (1996)\nargued that a “traditional” view of the senses underlies a\nvariety of sophisticated “naturalistic” programs about\nintentionality. (She cites the Churchlands, Daniel Dennett, Fred\nDretske, Jerry Fodor, David Papineau, Dennis Stampe, and Kim Sterelny\nas examples.) But then-recent neuroscientific work on the mechanisms\nand coding strategies implemented by sensory receptors shows that this\ntraditional view is mistaken. The traditional view holds that sensory\nsystems are “veridical” in at least three ways. (1) Each\nsignal in the system correlates with a small range of properties in\nthe external (to the body) environment. (2) The structure in the\nrelevant external relations that the receptors are sensitive to is\npreserved in the structure of the internal relations among the\nresulting sensory states. And (3) the sensory system reconstructs\nfaithfully, without fictive additions or embellishments, the external\nevents. Using then-recent neurobiological discoveries about response\nproperties of thermal receptors in the skin (i.e.,\n“thermoreceptors”) as an illustration, Akins showed that\nsensory systems are “narcissistic” rather than\n“veridical”. All three traditional assumptions are\nviolated. These neurobiological details and their philosophical\nimplications open novel questions for the philosophy of perception and\nfor the appropriate foundations for naturalistic projects about\nintentionality. Armed with the known neurophysiology of sensory\nreceptors, our “philosophy of perception” or account of\n“perceptual intentionality” will no longer focus on the\nsearch for correlations between states of sensory systems and\n“veridically detected” external properties. This\ntraditional philosophical (and scientific!) project rests upon a\nmistaken “veridicality” view of the senses.\nNeuroscientific knowledge of sensory receptor activity also shows that\nsensory experience does not serve the naturalist well as a\n“simple paradigm case” of an intentional relation between\nrepresentation and world. Once again, available scientific detail\nshowed the naivety of some traditional philosophical projects. \nFocusing on the anatomy and physiology of the pain transmission\nsystem, Valerie Hardcastle (1997) urged a similar negative implication\nfor a popular methodological assumption. Pain experiences have long\nbeen philosophers’ favorite cases for analysis and theorizing\nabout conscious experiences generally. Nevertheless, every position\nabout pain experiences has been defended: eliminativism, a variety of\nobjectivist views, relational views, and subjectivist views. Why so\nlittle agreement, despite agreement that pain experiences are the\nplace to start an analysis or theory of consciousness? Hardcastle\nurged two answers. First, philosophers tend to be uninformed about the\nneuronal complexity of our pain transmission systems, and build their\nanalyses or theories on the outcome of a single component of a\nmulti-component system. Second, even those who understand some of the\nunderlying neurobiology of pain tend to advocate gate-control\n theories.[6]\n But the best existing gate-control theories are vague about the\nneural mechanisms of the gates. Hardcastle instead proposed a\ndissociable dual system of pain transmission, consisting of a pain\nsensory system closely analogous in its neurobiological implementation\nto other sensory systems, and a descending pain inhibitory system. She\nargued that this dual system is consistent with neuroscientific\ndiscoveries and accounts for all the pain phenomena that have tempted\nphilosophers toward particular (but limited) theories of pain\nexperience. The neurobiological uniqueness of the pain inhibitory\nsystem, contrasted with the mechanisms of other sensory modalities,\nrenders pain processing atypical. In particular, the pain inhibitory\nsystem dissociates pain sensation from stimulation of nociceptors\n(pain receptors). Hardcastle concluded from the neurobiological\nuniqueness of pain transmission that pain experiences are atypical\nconscious events, and hence not a good place to start theorizing about\nor analyzing the general type. \nDeveloping and defending theories of content is a central topic in\ncontemporary philosophy of mind. A common desideratum in this debate\nis a theory of cognitive representation consistent with a physical or\nnaturalistic ontology. We’ll here describe a few contributions\nneurophilosophers have made to this project. \nWhen one perceives or remembers that he is out of coffee, his brain\nstate possesses intentionality or “aboutness”. The percept\nor memory is about one’s being out of coffee; it represents one\nas being out of coffee. The representational state has content. A\npsychosemantics seeks to explain what it is for a representational\nstate to be about something, to provide an account of how states and\nevents can have specific representational content. A physicalist\npsychosemantics seeks to do this using resources of the physical\nsciences exclusively. Neurophilosophers have contributed to two types\nof physicalist psychosemantics: the Functional Role approach and the\nInformational approach. For a description of these and other theories\nof mental content, see the entries on\n causal theories of mental content,\n mental representation, and\n teleological theories of mental content. \nThe core claim of a functional role semantics is that a representation\nhas its specific content in virtue of relations it bears to other\nrepresentations. Its paradigm application is to concepts of\ntruth-functional logic, like the conjunctive “and” or\ndisjunctive “or”. A physical event instantiates the\n“and” function just in case it maps two true inputs onto a\nsingle true output. Thus it is the relations an expression bears to\nothers that give it the semantic content of “and”.\nProponents of functional role semantics propose similar analyses for\nthe content of all representations (Block 1995). A physical event\nrepresents birds, for example, if it bears the right relations to\nevents representing feathers and others representing beaks. By\ncontrast, informational semantics ascribe content to a state depending\nupon the causal relations obtaining between the state and the object\nit represents. A physical state represents birds, for example, just in\ncase an appropriate causal relation obtains between it and birds. At\nthe heart of informational semantics is a causal account of\ninformation (Dretske 1981, 1988). Red spots on a face carry the\ninformation that one has measles because the red spots are caused by\nthe measles virus. A common criticism of informational semantics holds\nthat mere causal covariation is insufficient for representation, since\ninformation (in the causal sense) is by definition always veridical\nwhile representations can misrepresent. A popular solution to this\nchallenge invokes a teleological analysis of “function”. A\nbrain state represents X by virtue of having the function of\ncarrying information about being caused by X (Dretske 1988).\nThese two approaches do not exhaust the popular options for a\npsychosemantics, but are the ones to which neurophilosophers have most\ncontributed. \nPaul Churchland’s allegiance to functional role semantics goes\nback to his earliest views about the semantics of terms in a language.\nIn his (1979) book, he insisted that the semantic identity (content)\nof a term derives from its place in the network of sentences of the\nentire language. The functional economies envisioned by early\nfunctional role semanticists were networks with nodes corresponding to\nthe objects and properties denoted by expressions in a language. Thus\none node, appropriately connected, might represent birds, another\nfeathers, and another beaks. Activation of one of these would tend to\nspread activation to the others. As “connectionist” neural\nnetwork modeling developed (as discussed in the previous section\nabove), alternatives arose to this one-representation-per-node\n“localist” approach. By the time Churchland (1989)\nprovided a neuroscientific elaboration of functional role semantics\nfor cognitive representations generally, he too had abandoned the\n“localist” interpretation. Instead, he offered a\n“state-space semantics”. \nWe saw in the previous section how (vector) state spaces provide an\ninterpretation for activity patterns in neural networks, both\nbiological and artificial. A state-space semantics for cognitive\nrepresentations is a species of a functional role semantics because\nthe individuation of a particular state depends upon the relations\nobtaining between it and other states. A representation is a point in\nan appropriate state space, and points (or subvolumes) in a space are\nindividuated by their relations to other points (locations,\ngeometrical proximity). Paul Churchland (1989, 1995) illustrated a\nstate-space semantics for neural states by appealing to sensory\nsystems. One popular theory in sensory neuroscience of how the brain\ncodes for sensory qualities (like color) is the opponent process\naccount (Hardin 1988). Churchland (1995) describes a\nthree-dimensional activation vector state-space in which every color\nperceivable by humans is represented as a point (or subvolume). Each\ndimension corresponds to activity rates in one of three classes of\nphotoreceptors present in the human retina and their efferent paths:\nthe red-green opponent pathway, yellow-blue opponent pathway, and\nblack-white (contrast) opponent pathway. Photons striking the retina\nare transduced by photoreceptors, producing an activity rate in each\nof the segregated pathways. A represented color is hence a triplet of\nneuronal activation frequency rates. As an illustration, consider\nagain\n Figure 3.\n Each dimension in that three-dimensional space will represent average\nfrequency of action potentials in the axons of one class of ganglion\ncells projecting out of the retina. Each color perceivable by humans\nwill be a region of that space. For example, an orange stimulus\nproduces a relatively low level of activity in both the red-green and\nyellow-blue opponent pathways (x-axis and y-axis,\nrespectively), and middle-range activity in the black-white (contrast)\nopponent pathway (z-axis). Pink stimuli, on the other hand,\nproduce low activity in the red-green opponent pathway, middle-range\nactivity in the yellow-blue opponent pathway, and high activity in the\nblack-white (contrast) opponent\n pathway.[7]\n The location of each color in the space generates a “color\nsolid”. Location on the solid, and geometrical proximity between\nthese locations, reflect structural similarities between the perceived\ncolors. Human gustatory representations are points in a\nfour-dimensional state space, with each dimension coding for activity\nrates generated by gustatory stimuli in each type of taste receptor\n(sweet, salty, sour, and bitter) and their segregated efferent\npathways. When implemented in a neural network with structural\nresources, and hence computational resources as vast as the human\nbrain, the state space approach to psychosemantics generates a theory\nof content for a huge number of cognitive\n states.[8] \nJerry Fodor and Ernest LePore (1992) raised an important challenge to\nChurchland’s psychosemantics. Location in a state space alone\nseems insufficient to fix a state’s representational content.\nChurchland never explains why a point in a three-dimensional state\nspace represents a color, as opposed to any other quality,\nobject, or event that varies along three\n dimensions.[9].\n So Churchland’s account achieves its explanatory power by the\ninterpretation imposed on the dimensions. Fodor and LePore alleged\nthat Churchland never specified how a dimension comes to represent,\ne.g., degree of saltiness, as opposed to yellow-blue wavelength\nopposition. One obvious answer appeals to the stimuli that form the\n“external” inputs to the neural network in question. Then,\nfor example, the individuating conditions on neural representations of\ncolors are that opponent processing neurons receive input from a\nspecific class of photoreceptors. The latter in turn have\nelectromagnetic radiation (of a specific portion of the visible\nspectrum) as their activating stimuli. However, this appeal to\n“external” stimuli as the ultimate individuating\nconditions for representational content makes the resulting approach a\nversion of informational semantics. Is this approach consonant with\nother neurobiological details? \nThe neurobiological paradigm for informational semantics is the\nfeature detector: one or more neurons that are (i) maximally\nresponsive to a particular type of stimulus, and (ii) have the\nfunction of indicating the presence of that stimulus type. Examples of\nsuch stimulus-types for visual feature detectors include high-contrast\nedges, motion direction, and colors. A favorite feature detector among\nphilosophers is the alleged fly detector in the frog. Lettvin et al.\n(1959) identified cells in the frog retina that responded maximally to\nsmall shapes moving across the visual field. The idea that these\ncells’ activity functioned to detect flies rested upon knowledge\nof the frogs’ diet. (Bechtel 1998 provides a useful discussion.)\nUsing experimental techniques ranging from single-cell recording to\nsophisticated functional imaging, neuroscientists discovered a host of\nneurons that are maximally responsive to a variety of complex stimuli.\nHowever, establishing condition (ii) on a feature detector is much\nmore difficult. Even some paradigm examples have been called into\nquestion. David Hubel and Torsten Wiesel’s (1962) Nobel\nPrize-winning work establishing the receptive fields of neurons in\nstriate (visual) cortex is often interpreted as revealing cells whose\nfunction is edge detection. However, Lehky and Sejnowski (1988)\nchallenged this interpretation. They trained an artificial neural\nnetwork to distinguish the three-dimensional shape and orientation of\nan object from its two-dimensional shading pattern. Their network\nincorporates many features of visual neurophysiology. Nodes in the\ntrained network turned out to be maximally responsive to edge\ncontrasts, but did not appear to have the function of edge detection.\n(See Churchland and Sejnowski 1992 for a review.) \nKathleen Akins (1996) offered a different neurophilosophical challenge\nto informational semantics and its affiliated feature-detection view\nof sensory representation. We saw in the previous section that Akins\nargued that the physiology of thermoreception violates three necessary\nconditions on “veridical” representation. From this fact\nshe raised doubts about looking for feature-detecting neurons to\nground a psychosemantics generally, including for thought contents.\nHuman thoughts about flies, for example, are sensitive to numerical\ndistinctions between particular flies and the particular locations\nthey can occupy. But the ends of frog nutrition are well served\nwithout a representational system sensitive to such ontological\nniceties. Whether a fly seen now is numerically identical to one seen\na moment ago need not, and perhaps cannot, figure into the\nfrog’s feature detection repertoire. Akins’ critique cast\ndoubt on whether details of sensory transduction will scale up to\nprovide an adequate unified psychosemantics for all concepts. It also\nraised new questions for human intentionality. How do we get from\nactivity patterns in “narcissistic” sensory receptors,\nkeyed not to “objective” environmental features but rather\nonly to effects of the stimuli on the patch of tissue innervated, to\nhuman ontologies replete with enduring objects with stable\nconfigurations of properties and relations, types and their tokens (as\nthe “fly-thought” example presented above reveals), and\nthe rest? And how did the development of a stable, rich ontology\nconfer survival advantages to human ancestors? \nConsciousness re-emerged over the past three decades as a topic of\nresearch focus in philosophy of mind, and in the cognitive and brain\nsciences. Instead of ignoring it, many physicalists sought to explain\nit (Dennett 1991). Here we focus exclusively on ways that\nneuroscientific discoveries have impacted philosophical debates about\nthe nature of consciousness and its relation to physical mechanisms.\n(See links to other entries in this encyclopedia below in\n Related Entries\n for broader discussions about consciousness and physicalism.) \nThomas Nagel (1974) argued famously that conscious experience is\nsubjective, and thus permanently recalcitrant to objective scientific\nunderstanding. He invited us to ponder “what it is like to be a\nbat” and urged the intuitive judgment that no amount of\nphysical-scientific knowledge, including neuroscientific, supplies a\ncomplete answer. Nagel’s intuition pump has generated extensive\nphilosophical discussion. At least two well-known replies made direct\nappeal to neurophysiology. John Biro (1991) suggested that part of the\nintuition pumped by Nagel, that bat experience is substantially\ndifferent from human experience, presupposes systematic relations\nbetween physiology and phenomenology. Kathleen Akins (1993a) delved\ndeeper into existing knowledge of bat physiology and reports much that\nis pertinent to Nagel’s question. She argued that many of the\nquestions about bat subjective experience that we still consider open\nhinge on questions that remain unanswered about neuroscientific\ndetails. One example of the latter is the function of various cortical\nactivity profiles in the active bat. \nDavid Chalmers (1996) famously argued that any possible brain-process\naccount of consciousness will leave open an “explanatory\ngap” between the brain process and properties of the conscious\n experience.[10]\n This is because no brain-process theory can answer the\n“hard” question: Why should that particular brain process\ngive rise to that particular conscious experience? We can always\nimagine (“conceive of”) a universe populated by creatures\nhaving those brain processes but completely lacking conscious\nexperience. A theory of consciousness requires an explanation of how\nand why some brain process causes a conscious experience, replete with\nall the features we experience. The fact that the hard question\nremains unanswered shows that we will probably never get a complete\nexplanation of consciousness at the level of neural mechanism. Paul\nand Patricia Churchland (1997) offered the following diagnosis and\nreply. Chalmers offers a conceptual argument, based on our\nability to imagine creatures possessing active brains like ours but\nwholly lacking in conscious experiences. But the more one learns about\nhow the brain produces conscious experience—and such a\nliterature has emerged (for some early work, see Gazzaniga\n1995)—the harder it becomes to imagine a universe consisting of\ncreatures with brain processes like ours but lacking consciousness.\nThis is not just bare assertion. The Churchlands appeal to some\nneurobiological detail. For example, Paul Churchland (1995) develops a\nneuroscientific account of consciousness based on recurrent\nconnections between thalamic nuclei, particularly between\n“diffusely projecting” nuclei like the intralaminar nuclei\nand the\n cortex.[11]\n Churchland argues that thalamocortical recurrency accounts for the\nselective features of consciousness, for the effects of short-term\nmemory on conscious experience, for vivid dreaming during REM\n(rapid-eye movement) sleep, and other “core” features of\nconscious experience. In other words, the Churchlands are claiming\nthat when one learns about activity patterns in these recurrent\ncircuits, one can no longer “imagine” or “conceive\nof” this activity occurring without these core features of\nconscious experience occurring. (Other than just mouthing the\nexpression, “I am now imagining activity in these circuits\nwithout selective attention/the effects of short-term memory/vivid\ndreaming/…”). \nA second focus of skeptical arguments about a complete neuroscientific\nexplanation of consciousness is on sensory qualia: the\nintrospectable qualitative aspects of sensory experience, the features\nby which subjects discern similarities and differences among their\nexperiences. The colors of visual sensations are a philosopher’s\nfavorite example. One famous puzzle about color qualia is the alleged\nconceivability of spectral inversions. Many philosophers claim that it\nis conceptually possible (if perhaps physically impossible) for two\nhumans not to differ neurophysiologically, while the color that fire\nengines and tomatoes appear to have to one subject is the color that\ngrass and frogs appear to have to the other (and vice versa). A large\namount of neuroscientifically-informed philosophy has addressed this\nquestion. (C.L. Hardin 1988 and Austen Clark 1993 are noteworthy\nexamples.) A related area where neurophilosophical considerations have\nemerged concerns the metaphysics of colors themselves (rather than\ncolor experiences). A longstanding philosophical dispute is whether\ncolors are objective properties existing external to perceivers or\nrather identifiable as or dependent upon minds or nervous systems.\nSome neuroscientific work on this problem begins with characteristics\nof color experiences: for example, that color similarity judgments\nproduce color orderings that align on a circle (Clark 1993). With this\nresource, one can seek mappings of phenomenology onto environmental or\nphysiological regularities. Identifying colors with particular\nfrequencies of electromagnetic radiation does not preserve the\nstructure of the hue circle, whereas identifying colors with activity\nin opponent processing neurons does. Such a tidbit is not decisive for\nthe color objectivist-subjectivist debate, but it does convey the type\nof neurophilosophical work being done on traditional metaphysical\nissues beyond the philosophy of mind. (For more details on these\nissues, see the\n entry on color\n in this Encyclopedia.) \nWe saw in the discussion of Hardcastle (1997) two sections above that\nneurophilosophers have entered disputes about the nature and\nmethodological import of pain experiences. Two decades earlier, Dan\nDennett (1978) took up the question of whether it is possible to build\na computer that feels pain. He compares and notes tension between\nneurophysiological discoveries and common sense intuitions about pain\nexperience. He suspects that the incommensurability between scientific\nand common sense views is due to incoherence in the latter. His\nattitude is wait-and-see. But foreshadowing Churchland’s reply\nto Chalmers, Dennett favors scientific investigations over\nconceivability-based philosophical arguments. \nNeurological deficits have attracted philosophers interested in\nconsciousness. For nearly fifty years philosophers have debated\nimplications for the unity of the self of the Nobel Prize-winning\nexperiments with commissurotomy patients who, for clinical reasons,\nhad their corpus callosum surgically ablated (Nagel\n 1971).[12]\n The corpus callosum is the huge bundle of axons connecting neurons\nacross the left and right mammalian cerebral hemispheres. In carefully\ncontrolled experiments, commissurotomy patients seemingly display two\ndissociable “seats” of consciousness. Elizabeth Schechter\n(2018) has recently greatly updated philosophical treatment of the\nscientific details of these “split-brain” patients,\nincluding their own experiential reports, and has traced implications\nfor our understanding of the self.  \nIn chapter 5 of her (1986) book, Patricia Churchland extended both the\nrange and philosophical implications of neurological deficits. One\ndeficit she discusses in detail is blindsight. Some patients with\nlesions to primary visual cortex report being unable to see items in\nregions of their visual fields, yet perform far better than chance in\nforced guess trials about stimuli in those regions. A variety of\nscientific and philosophical interpretations have been offered. Ned\nBlock (1995) worried that many of these interpretations conflate\ndistinct notions of consciousness. He labels these notions\n“phenomenal consciousness” (“P-consciousness”)\nand “access consciousness”\n(“A-consciousness”). The former is the “what it is\nlike”-ness of conscious experiences. The latter is the\navailability of representational content to self-initiated action and\nspeech. Block argued that P-consciousness is not always\nrepresentational, whereas A-consciousness is. Dennett (1991, 1995) and\nTye (1993) are skeptical of non-representational analyses of\nconsciousness in general. They provide accounts of blindsight that do\nnot depend on Block’s distinction. \nWe break off our brief overview of neurophilosophical work on\nconsciousness here. Many other topics are worth neurophilosophical\npursuit. We mentioned commissurotomy and the unity of consciousness\nand the self, which continues to generate discussion. Qualia beyond\nthose of color and pain experiences quickly attracted\nneurophilosophical attention (Akins 1993a,b, 1996; Austen Clark 1993),\nas did self-consciousness (Bermúdez 1998). \nOne of the first issues to arise in neurology, as far back as the\nnineteenth century, concerned the localization of specific cognitive\nfunctions to specific brain regions. Although the\n“localization” approach had dubious origins in the\nphrenology of Gall and Spurzheim, and had been challenged strenuously\nby Flourens throughout the early nineteenth century, it re-emerged\nlate in the nineteenth century in the study of aphasia by Bouillaud,\nAuburtin, Broca, and Wernicke. These neurologists made careful studies\n(when possible) of linguistic deficits in their aphasic patients,\nfollowed by brain autopsies post\n mortem.[13]\n Broca’s initial study of twenty-two patients in the\nmid-nineteenth century confirmed that damage to the left cortical\nhemisphere was predominant, and that damage to the second and third\nfrontal convolutions was necessary to produce speech production\ndeficits. Although the anatomical coordinates Broca postulated for the\n“speech production center” do not correlate exactly with\ndamage producing production deficits, both this area of frontal cortex\nand speech production deficits still bear his name\n(“Broca’s area” and “Broca’s\naphasia”). Less than two decades later Carl Wernicke published\nevidence for a second language center. This area is anatomically\ndistinct from Broca’s area, and damage to it produced a very\ndifferent set of aphasic symptoms. The cortical area that still bears\nhis name (“Wernicke’s area”) is located around the\nfirst and second convolutions in temporal cortex, and the aphasia that\nbears his name (“Wernicke’s aphasia”) involves\ndeficits in language comprehension. Wernicke’s method, like\nBroca’s, was based on lesion studies produced by natural trauma:\na careful evaluation of the behavioral deficits, followed by post\nmortem autopsies to find the sites of tissue damage and atrophy. More\nrecent and more careful lesion studies suggest more precise\nlocalization of specific linguistic functions, and remain a\ncornerstone to this day in aphasia research. \nLesion studies have also produced evidence for the localization of\nother cognitive functions: for example, sensory processing and certain\ntypes of learning and memory. However, localization arguments for\nthese other functions invariably include studies using animal models.\nWith an animal model, one can perform careful behavioral measures in\nhighly controlled settings, then ablate specific areas of neural\ntissue (or use a variety of other techniques to block or enhance\nactivity in these areas) and re-measure performance on the same\nbehavioral tests. Since we lack widely accepted animal models for\nhuman language production and comprehension, this additional evidence\nisn’t available to the neurologist or neurolinguists. This\nlimitation makes the neurological study of language a paradigm case\nfor evaluating the logic of the lesion/deficit method of inferring\nfunctional localization. Barbara Von Eckardt (Von Eckardt Klein 1978)\nattempted to make explicit the steps of reasoning involved in this\ncommon and historically important method. Her analysis begins with\nRobert Cummins’ well-known analysis of functional explanation,\nbut she extends it into a notion of structurally adequate\nfunctional analysis. These analyses break down a complex capacity C\ninto its constituent capacities c1,\nc2,…, cn,\nwhere the constituent capacities are consistent with the underlying\nstructural details of the system. For example, human speech production\n(complex capacity C) results from formulating a speech intention, then\nselecting appropriate linguistic representations to capture the\ncontent of the speech intention, then formulating the motor commands\nto produce the appropriate sounds, then communicating these motor\ncommands to the appropriate motor pathways (all together, the\nconstituent capacitiesc1,\nc2,…, cn). A\nfunctional-localization hypothesis has the form: brain structure S in\norganism (type) O has constituent capacity\nci, where ci\nis a function of some part of O. An example might be: Broca’s\narea (S) in humans (O) formulates motor commands to produce the\nappropriate sounds (one of the constituent capacities\nci). Such hypotheses specify aspects of\nthe structural realization of a functional-component model. They are\npart of the theory of the neural realization of the functional\nmodel. \nArmed with these characterizations, Von Eckardt Klein argues that inference\nto a functional-localization hypothesis proceeds in two steps. First,\na functional deficit in a patient is hypothesized based on the\nabnormal behavior the patient exhibits. Second, localization of\nfunction in normal brains is inferred on the basis of the functional\ndeficit hypothesis plus the evidence about the site of brain damage.\nThe structurally-adequate functional analysis of the capacity connects\nthe pathological behavior to the hypothesized functional deficit. This\nconnection suggests four adequacy conditions on a functional deficit\nhypothesis. First, the pathological behavior P (e.g., the speech\ndeficits characteristic of Broca’s aphasia) must result from\nfailing to exercise some complex capacity C (human speech production).\nSecond, there must be a structurally-adequate functional analysis of\nhow people exercise capacity C that involves some constituent capacity\nci (formulating motor commands to produce\nthe appropriate sounds). Third, the operation of the steps described\nby the structurally-adequate functional analysis minus the operation\nof the component performing ci\n(Broca’s area) must result in pathological behavior P. Fourth,\nthere must not be a better available explanation for why the patient\ndoes P. Arguments to a functional deficit hypothesis on the basis of\npathological behavior is thus an instance of argument to the best\navailable explanation. When postulating a deficit in a normal\nfunctional component provides the best available explanation of the\npathological data, we are justified in drawing the inference. \nVon Eckardt Klein applies this analysis to a neurological case study\ninvolving a controversial reinterpretation of\n agnosia.[14]\n Her philosophical explication of this important neurological method\nreveals that most challenges to localization arguments either argue\nonly against the localization of a particular type of functional\ncapacity or against generalizing from localization of function in one\nindividual to all normal individuals. (She presents examples of each\nfrom the neurological literature.) Such challenges do not impugn the\nvalidity of standard arguments for functional localization from\ndeficits. It does not follow that such arguments are unproblematic.\nBut they face difficult factual and methodological problems, not\nlogical ones. Furthermore, the analysis of these arguments as\ninvolving a type of functional analysis and inference to the best\navailable explanation carries an important implication for the\nbiological study of cognitive function. Functional analyses require\nfunctional theories, and structurally adequate functional analyses\nrequire checks imposed by the lower level sciences investigating the\nunderlying physical mechanisms. Arguments to best available\nexplanation are often hampered by a lack of theoretical imagination:\nthe available alternative explanations are often severely limited. We\nmust seek theoretical inspiration from any level of investigation or\nexplanation. Hence making explicit the “logic” of this\ncommon and historically important form of neurological explanation\nreveals the necessity of joint participation from all scientific\nlevels, from cognitive psychology down to molecular neuroscience. Von\nEckardt Klein (1978) thus anticipated what came to be heralded as the\n“co-evolutionary research methodology”, which remains a\ncenterpiece of neurophilosophy to the present day (see\n section 6). \nOver the last three decades, new evidence for localizations of\ncognitive functions has come increasingly from a new source, the\ndevelopment and refinement of neuroimaging techniques. However, the\nlogical form of localization-of-function arguments appears not to have\nchanged from those employing lesion studies, as analyzed by Von\nEckardt Klein. Instead, these new neuroimaging technologies resolve some of\nthe methodological problems that plagued lesion studies. For example,\nresearchers do not need to wait until the patient dies, and in the\nmeantime probably acquires additional brain damage, to find the lesion\nsites. Two functional imaging techniques have been prominent in\nphilosophical discussions: positron emission tomography, or PET, and\nfunctional magnetic resonance imaging, or fMRI. Although these measure\ndifferent biological markers of functional activity, PET approved for\nhuman use now has spatial resolution down to the single mm range,\nwhile fMRI has resolution down to less than\n 1mm.[15]\n As these techniques increased spatial and temporal resolution of\nfunctional markers, and continued to be used with sophisticated\nbehavioral methodologies, arguments for localizing specific\npsychological functions to increasingly specific neural regions\ncontinued to grow. Stufflebeam and Bechtel provided an early and\nphilosophically useful discussion of PET. Bechtel and Richardson\n(1993) provided a general framework for “localization and\ndecomposition” arguments, which anticipated in many ways the\ncoming “new mechanistic” perspective in philosophy of\nscience and philosophy of neuroscience (see\n sections 7 and 8 below).\n Bechtel and Mundale (1999) further refined philosophical arguments\nfor localization of function specific to neuroscience. \nMore recent philosophical discussion of these functional imaging\ntechniques has tended to urge more caution resting localization claims\non their results. Roskies (2007), for example, points out the tendency\nto think of the evidential force of functional neuroimages (especially\nfMRI) on an analogy of that of photographs. Drawing on work in\naesthetics and the visual arts, Roskies argues that many of the\nfeatures that give photographs their evidential force are not present\nin functional neuroimages. So while neuroimages do serve as evidence\nfor claims about neurofunctions, and even for localization hypotheses,\ndetails of their proper interpretation are far more complicated than\nphilosophers sometimes assume. More critically, Klein (2010) argues\nthat images of “brain activity” resulting from functional\nneuroimaging, especially fMRI are poor evidence for functional\nhypotheses. For these images present the results of null hypothesis\nsignificance testing on fMRI data, and such testing alone cannot\nprovide evidence about the functional structure of a causally dense\nsystem, which the human brain is. Instead, functional neuroimages are\nproperly interpreted as indicating regions where further data and\nanalysis are warranted. But these data will typically require more\nthan simple significance testing, so skepticism about the evidential\nforce of neuroimages does not warrant skepticism more generally about\nfMRI.  \nLocalization of function remains to this day a central topic of\ndiscussion in philosophy of neuroscience. We will cover more recent\nwork in later sections. \nWhat neuroscience has now discovered about the cellular and molecular\nmechanisms of neural conductance and transmission is spectacular.\nThese results constitute one of the crowning achievements of\nscientific inquiry. (For those in doubt, simply peruse for five\nminutes a recent volume of Society for Neuroscience\nAbstracts.) Less comprehensive, yet still spectacular, are\ndiscoveries at “higher” levels of neuroscience: circuits,\nnetworks, and systems. All this is a natural outcome of increasing\nscientific specialization. We develop the technology, the experimental\ntechniques, and ultimately the experimental results-driven theories\nwithin specific disciplines to push forward our understanding. Still,\na crucial aspect of the total picture sometimes gets neglected: the\nrelationship between the levels, the “glue” that binds\nknowledge of neuron activity to subcellular and molecular mechanisms\n“below”, and to circuit, network, and systems activity\npatterns “above”. This problem is especially glaring when\nwe try to relate “cognitivist” psychological theories,\npostulating information-bearing representations and processes\noperating over their contents, to neuronal activities.\n“Co-evolution” between these explanatory levels still\nseems more a distant dream than an operative methodology guiding\nday-to-day scientific research. \nIt is here that some philosophers and neuroscientists turned to\ncomputational methods (Churchland and Sejnowski 1992). One hope was\nthat the way computational models have functioned in more developed\nsciences, like in physics, might provide a useful model. One\ncomputational resource that has usefully been applied in more\ndeveloped sciences to similar “cross-level” concerns are\ndynamical systems. Global phenomena, such as large-scale\nmeteorological patterns, have been usefully addressed as dynamical,\nnonlinear, and often chaotic interactions between lower-level physical\nphenomena. Addressing the interlocking levels of theory and\nexplanation in the mind/brain using computational resources that have\nworked to bridge levels in more mature sciences might yield comparable\nresults. This methodology is necessarily interdisciplinary, drawing on\nresources and researchers from a variety of levels, including higher\nones like experimental psychology, artificial intelligence, and\nphilosophy of science. \nThe use of computational methods in neuroscience itself is not new.\nHodgkin, Huxley, and Katz (1952) incorporated values of\nvoltage-dependent sodium and potassium conductance they had measured\nexperimentally in the squid giant axon into an equation from physics\ndescribing the time evolution of a first-order kinetic process. This\nequation enabled them to calculate best-fit curves for modeled\nconductance versus time data that reproduced the changing membrane\npotential over time when action potentials were generated. Also using\nequations borrowed from physics, Rall (1959) developed the cable model\nof dendrites. This model provided an account of how the various inputs\nfrom across the dendritic tree interact temporally and spatially to\ndetermine the input-output properties of single neurons. It remains\ninfluential today, and was incorporated into the GENESIS software\nfor programming neurally realistic networks (Bower and Beeman 1995;\nsee discussion in\n section 2 above).\n David Sparks and his colleagues showed that a vector-averaging model\nof activity in neurons of superior colliculi correctly predicts\nexperimental results about the amplitude and direction of saccadic eye\nmovements (Lee, Rohrer, and Sparks 1988). Working with a more\nsophisticated mathematical model, Apostolos Georgopoulos and his\ncolleagues predicted direction and amplitude of hand and arm movements\nbased on averaged activity of 224 cells in motor cortex. Their\npredictions were borne out under a variety of experimental tests\n(Georgopoulos, Schwartz, and Kettner 1986). We mention these\nparticular studies only because these are ones with which we are\nfamiliar. No doubt we could multiply examples of the fruitful\ninteraction of computational and experimental methods in neuroscience\neasily by one-hundred-fold. Many of these extend back before\n“computational neuroscience” was a recognized research\nendeavor. \nWe’ve already seen one example, the vector transformation\naccount of neural representation and computation, once under active\ndevelopment in cognitive neuroscience (see\n section 2 above).\n Other approaches using “cognitivist” resources were, and\ncontinue to be,\n pursued.[16]\n Some of these projects draw upon “cognitivist”\ncharacterizations of the phenomena to be explained. Some exploit\n“cognitivist” experimental techniques and methodologies.\nSome even attempt to derive “cognitivist” explanations\nfrom cell-biological processes (e.g., Hawkins and Kandel 1984). As\nStephen Kosslyn (1997) put it, cognitive neuroscientists employ the\n“information processing” view of the mind characteristic\nof cognitivism without trying to separate it from theories of brain\nmechanisms. Such an endeavor calls for an interdisciplinary community\nwilling to communicate the relevant portions of the mountain of detail\ngathered in individual disciplines with interested nonspecialists.\nThis requires more than people willing to confer with others working\nat related levels, but also researchers trained explicitly in the\nmethods and factual details of a variety of disciplines. This is a\ndaunting need, but it offers hope to philosophers wishing to\ncontribute to actual neuroscience. Thinkers trained in both the\n“synoptic vision” afforded by philosophy, and the\nscientific and experimental basis of a genuine (graduate-level)\nscience would be ideally equipped for this task. Recognition of this\npotential niche was slow to dawn on graduate programs in philosophy,\nbut a few programs have taken steps to fill it (see, e.g.,\n Other Internet Resources\n below). \nHowever, one glaring shortcoming remains. Given philosophers’\ntraining and interests, “higher-level”\nneurosciences—networks, cognitive, systems, and the fields of\ncomputational neuroscience which ally with these—tend to attract\nthe most philosophical attention. As natural as this focus might be,\nit can lead philosophers to a misleading picture of neuroscience.\nNeurobiology remains focused on cellular and molecular mechanisms of\nneuronal activity, and allies with the kind of behavioral neuroscience\nthat works with animal models. This is still how a majority of members\nof the Society for Neuroscience, now more than 37,000 members strong,\nclassify their own research; this is where the majority of grant money\nfor research goes; and these are the areas whose experimental\npublications most often appear in the most highly cited scientific\njournals. (The link to the Society for Neuroscience’s web site\nin\n Other Internet Resources below\n leads to a wealth of data on these numbers; see especially the\nPublications section.) Yet philosophers have tended not to pay much\nattention to cellular and molecular neuroscience. Fortunately this\nseems to be changing, as we will document in sections 7 and 8 below.\nStill, the preponderant attention philosophers pay to\ncognitive/systems/computational neuroscience obscures the wetlab\nexperiment-driven focus of ongoing neurobiology. \nThe distinction between “philosophy of neuroscience” and\n“neurophilosophy” came to be better clarified over the\nfirst decade of the twenty-first century, due primarily to more\nquestions being pursued in both areas. Philosophy of neuroscience\nstill tends to pose traditional questions from philosophy of science\nspecifically about neuroscience. Such questions include: What is the\nnature of neuroscientific explanation? And, what is the nature of\ndiscovery in neuroscience? Answers to these questions are pursued\neither descriptively (how does neuroscience proceed?) or normatively\n(how should neuroscience proceed)? Some normative projects in\nphilosophy of neuroscience are “deconstructive”,\ncriticizing claims about the topic made by neuroscientists. For\nexample, philosophers of neuroscience have criticized the conception\nof personhood assumed by researchers in cognitive neuroscience (cf.\nRoskies 2009). Other normative projects are constructive, proposing\nnew theories of neuronal phenomena or methods for interpreting\nneuroscientific data. Such projects often integrate smoothly with\ntheoretical neuroscience itself. For example, Chris Eliasmith and\nCharles Anderson developed an approach to constructing\nneurocomputational models in their book Neural Engineering\n(2003). In separate publications, Eliasmith has argued that the\nframework introduced in Neural Engineering provides both a\nnormative account of neural representation and a framework for\nunifying explanation in neuroscience (e.g., Eliasmith 2009). \nNeurophilosophy continued to apply findings from the neurosciences to\ntraditional, philosophical questions. Examples include: What is an\nemotion? (Prinz 2004) What is the nature of desire? (Schroeder 2004)\nHow is social cognition made possible? (Goldman 2006) What is the neural basis of\nmoral cognition? (Prinz 2007) What is the neural basis of happiness?\n(Flanagan 2009) Neurophilosophical answers to these questions are\nconstrained by what neuroscience reveals about nervous systems. For\nexample, in his book Three Faces of Desire, Timothy Schroeder\n(2004) argued that our commonsense conception of desire attributes to\nit three capacities: (1) the capacity to reinforce behavior when\nsatisfied, (2) the capacity to motivate behavior, and (3) the capacity\nto determine sources of pleasure. Based on evidence from the\nliterature on dopamine function and reinforcement learning theory,\nSchroeder argued that reward processing is the basis for all three\ncapacities. Thus, reward is the essence of desire. \nDuring the first decade of the twenty-first century a trend arose in\nneurophilosophy to look toward neuroscience for guidance in moral\nphilosophy. That should be evident from the themes we’ve just\nmentioned. Simultaneously, there was renewed interest in moralizing\nabout neuroscience and neurological treatments (see Levy 2007; Roskies\n2009). This new field, neuroethics, thus combined both\ninterest in the relevance of neuroscience data for understanding moral\ncognition, and the relevance of moral philosophy for acquiring and\nregulating the application of knowledge from neuroscience. The\nregulatory branch of neuroethics initially focused explicitly on the\nethics of treatment for people who suffer from neurological\nimpairments, the ethics of attempts to enhance human cognitive\nperformance (Schneider 2009), the ethics of applying “mind\nreading” technology to problems in forensic science (Farah and\nWolpe 2004), and the ethics of animal experimentation in neuroscience\n(Farah 2008). More recently both of these fields of neuroethics has\nseen tremendous growth. The interested reader should consult the\n neuroethics entry\n in this Encyclopedia. \nTrends during the first decade of the twenty-first century in\nphilosophy of neuroscience included renewed interest in the nature of\nmechanistic explanations. This was in keeping with a general trend in\nphilosophy of science (e.g., Machamer, Darden, and Craver 2000). The\napplication of this general approach to neuroscience isn’t\nsurprising. “Mechanism” is a widely-used term among\nneuroscientists. In his book, Explaining the Brain (2007),\nCarl Craver contended that mechanistic explanations in neuroscience\nare causal explanations, and typically multi-level. For example, the\nexplanation of the neuronal action potential involves the action\npotential itself, the cell in which it occurs, electro-chemical\ngradients, and the proteins through which ions flow across the\nmembrane. Thus we have a composite entity (a cell) causally\ninteracting with neurotransmitters at its receptors. Parts of the cell\nengage in various activities, e.g., the opening and closing of\nligand-gated and voltage-gated ion channels, to produce a pattern of\nchanges, the depolarizing current constituting the action potential. A\nmechanistic explanation of the action potential thus countenances\nentities at the cellular, molecular, and atomic levels, all of which\nare causally relevant to producing the action potential. This causal\nrelevance can be confirmed by altering any one of these variables,\ne.g., the density of ion channels in the cell membrane, to generate\nalterations in the action potential; and by verifying the consistency\nof the purported invariance between the variables. For challenges to\nCraver’s account of mechanistic explanation in neuroscience,\nspecifically concerning the action potential, see Weber 2008, and\nBogen 2005. \nAccording to epistemic norms shared implicitly by neuroscientists,\ngood explanations in neuroscience are good mechanistic explanations;\nand good mechanistic explanations are those that pick out invariant\nrelationships between mechanisms and the phenomena they control. (For\nfuller treatment of invariance in causal explanations throughout\nscience, see James Woodward 2003. Mechanists draw extensively on\nWoodward’s “interventionist” account of cause and\ncausal explanations.) Craver’s account raised questions about\nthe place of reduction in neuroscience. John Bickle (2003) suggested\nthat the working concept of reduction in the neurosciences consists of\nthe discovery of systematic relationships between interventions at\nlower levels of biological organization, as these are pursued in\ncellular and molecular neuroscience, and higher level behavioral\neffects, as they are described in psychology. Bickle called this\nperspective “reductionism-in-practice” to contrast it with\nthe concepts of intertheoretic or metaphysical reduction that have\nbeen the focus of many debates in the philosophy of science and\nphilosophy of mind. Despite Bickle’s reformulation of reduction,\nhowever, mechanists generally resist, or at least relativize, the\n“reductionist” label. Craver (2007) calls his view the\n“mosaic unity” of neuroscience. Bechtel (2009) calls his\n“mechanistic reduction(ism)”. Both Craver and Bechtel\nadvocate multi-leveled “mechanisms-within-mechanisms”,\nwith no level of mechanism epistemically privileged. This is in\ncontrast to reduction(ism), ruthless or otherwise which privileges\nlower levels. Still we can ask: Is mechanism a kind of\nreductionism-in-practice? Or does mechanism, as a position on\nneuroscientific explanation, assume some type of autonomy for\npsychology? If it assumes autonomy, reductionists might challenge\nmechanists on this assumption. On the other hand, Bickle’s\nreductionism-in-practice clearly departs from inter-theoretic\nreduction, as the latter is understood in philosophy of science. As\nBickle himself acknowledges, his latest reductionism was inspired\nheavily by mechanists’ criticisms of his earlier “new\nwave” account. Mechanists can challenge Bickle that his\ndeparture from the traditional accounts has also led to a departure\nfrom the interests that motivated those accounts. (See Polger 2004 for\na related challenge.) As we will see in\n section 8 below,\n these issues surrounding mechanistic philosophy of neuroscience have\ngrown more urgent, as mechanism has grown to dominate the field. \nThe role of temporal representation in conscious experience and the\nkinds of neural architectures sufficient to represent objects in time\ngenerated interest. In the tradition of Husserl’s phenomenology,\nDan Lloyd (2002, 2003) and Rick Grush (2001, 2009) have separately\ndrawn attention to the tripartite temporal structure of phenomenal\nconsciousness as an explanandum for neuroscience. This structure\nconsists of a subjective present, an immediate past, and an\nexpectation of the immediate future. For example, one’s\nconscious awareness of a tune is not just of a time-slice of\ntune-impression, but of a note that a moment ago was present, another\nthat is now present, and an expectation of subsequent notes in the\nimmediate future. As this experience continues, what was a moment ago\ntemporally immediate is now retained as a moment in the immediate\npast; what was expected either occurred or didn’t in what has\nnow become the experienced present; and a new expectation has formed\nof what will come. One’s experience is not static, even though\nthe experience is of a single object (the tune). These earlier works\nfound increased relevance with the rise of “predictive\ncoding” models of whole brain function, developed by\nneuroscientists including Karl Friston (2009) less than a decade\nlater, and brought to broader philosophical attention by Jakob Hohwy\n(2013) and Andy Clark (2016).  \nAccording to Lloyd, the tripartite structure of consciousness raises a\nunique problem for analyzing fMRI data and designing experiments. The\nproblem stems from the tension between the sameness in the object of\nexperience (e.g., the same tune through its progression) and the\ntemporal fluidity of experience itself (e.g., the transitions between\nheard notes). At the time Lloyd was writing, one standard means of\nanalyzing fMRI data consisted in averaging several data sets and\nsubtracting an estimate of baseline activation from the composites.\n [17]\n This is done to filter noise from the task-related hemodynamic\nresponse. But as Lloyd points out, this then-common practice ignores\nmuch of the data necessary for studying the neural correlates of\nconsciousness. It produces static images that neglect the\nrelationships between data points over the time course. Lloyd instead\napplies a multivariate approach to studying fMRI data, under the\nassumption that a recurrent network architecture underlies the\ntemporal processing that gives rise to experienced time. A simple\nrecurrent network has an input layer, an output layer, a hidden layer,\nand an additional layer that copies the prior activation state of\neither the hidden layer or the output layer. Allowing the output layer\nto represent a predicted outcome, the input layer can then represent a\ncurrent state and the additional layer a prior state. This assignment\nmimics the tripartite temporal structure of experience in a network\narchitecture. If the neuronal mechanisms underlying conscious\nexperience are approximated by recurrent network architecture, one\nprediction is that current neuronal states carry information about\nimmediate future and prior states. Applied to fMRI, the model predicts\nthat time points in an image series will carry information about prior\nand subsequent time points. The results of Lloyd’s (2002)\nanalysis of 21 subjects’ data sets, sampled from the publicly\naccessible National fMRI Data Center, support this prediction. \nGrush’s (2001, 2004) interest in temporal representation is part\nof his broader systematic project of addressing a semantic problem for\ncomputational neuroscience, namely: how do we demarcate study of the\nbrain as an information processor from the study of any other complex\ncausal process? This question leads back into the familiar territory\nof psychosemantics (see\n section 3 above),\n but now the starting point is internal to the practices of\ncomputational neuroscience. The semantic problem is thereby rendered\nan issue in philosophy of neuroscience, insofar as it asks: what does\n(or should) “computation” mean in computational\nneuroscience? \nGrush’s solution drew on concepts from modern control theory. In\naddition to a controller, a sensor, and a goal state, certain kinds of\ncontrol systems employ a process model of the actual process\nbeing controlled. A process model can facilitate a variety of\nengineering functions, including overcoming delays in feedback and\nfiltering noise. The accuracy of a process model can be assessed\nrelative to its “plug-compatibility” with the actual\nprocess. Plug-compatibility is a measure of the degree to which a\ncontroller can causally couple to a process model to produce the same\nresults it would produce by coupling with the actual process. Note\nthat plug-compatibility is not an information relation. \nTo illustrate a potential neuroscientific implementation, Grush\nconsiders a controller as some portion of the brain’s motor\nsystems (e.g., premotor cortex). The sensors are the sense organs\n(e.g., stretch receptors on the muscles). A process model of the\nmusculoskeletal system might exist in the cerebellum (see Kawato\n1999). If the controller portion of the motor system sends spike\ntrains to the cerebellum in the same way that it sends spikes to the\nmusculoskeletal system, and if in return the cerebellum receives spike\ntrains similar to real peripheral feedback, then the cerebellum\nemulates the musculoskeletal system (to the degree that the mock\nfeedback resembles real peripheral feedback). The proposed unit over\nwhich computational operations range is the neuronal realization of a\nprocess model and its components, or in Grush’s terms an\n“emulator” and its “articulants”. \nThe details of Grush’s framework are too sophisticated to\npresent in short compass. (For example, he introduces a host of\nconceptual devices to discuss the representation of external objects.)\nBut in a nutshell, he contends that understanding temporal\nrepresentation begins with understanding the emulation of the timing\nof sensorimotor contingencies. Successful sequential behavior (e.g.,\nspearing a fish) depends not just on keeping track of where one is in\nspace, but where one is in a temporal order of movements and the\ntemporal distance between the current, prior, and subsequent\nmovements. Executing a subsequent movement can depend on keeping track\nof whether a prior movement was successful and whether the current\nmovement is matching previous expectations. Grush posits\nemulators—process models in the central nervous\nsystem—that anticipate, retain, and update mock sensorimotor\nfeedback by timing their output proportionally to feedback from an\nactual process (Grush 2005). \nLloyd’s and Grush’s approaches to studying temporal\nrepresentation are varied in their emphases. But they are unified in\ntheir implicit commitment to localizing cognitive functions and\ndecomposing them into subfunctions using both top-down and bottom-up\nconstraints. (See Bechtel and Richardson 1993 for more details on this\ngeneral explanatory strategy.) As we mentioned a few paragraphs above,\nboth anticipated in important and interesting ways more recent\nneuroscientific and philosophical work on predictive coding and the\nbrain. Both developed mechanistic explanations that pay little regard\nto disciplinary boundaries. One of the principal lessons of\nBickle’s and Craver’s work is that neuroscientific\npractice in general is structured in this fashion. The ontological\nconsequences of adopting this approach continue to be debated. \nMechanism, first introduced in section 7 above, came to dominate the\nphilosophy of neuroscience throughout the second decade of the\ntwenty-first century. One much-discussed example is Gualtiero\nPiccinini and Carl Craver (2011). The authors employ two popular\nmechanistic notions. Their first is the multi-level, nested\nhierarchies of mechanisms-within-mechanisms perspective, discussed in\nsection 7 above, that traces back to Craver and Darden (2001). Their\nsecond is that of “mechanism sketch”, suggested initially\nin Machamer, Darden, and Craver (2000) and developed in detail in\nCraver (2007). Piccinini and Craver’s goal is to\n“seamlessly” situate psychology as part of an\n“integrated framework” alongside neuroscience. They\ninterpret psychology’s familiar functional analyses of cognitive\ncapacities as relatively incomplete mechanism-sketches, which leave\nout many components of the mechanisms that ultimately will fully\nexplain the system’s behavior. Neuroscience in turn fills in\nthese missing components, dynamics, and organizations, at least ones\nfound in nervous systems. This filling-in thereby turns\npsychology’s mechanism-sketches into full-blown mechanistic\nexplanations. So even though psychology proceeds via functional\nanalyses, so interpreted it is nonetheless mechanistic. Piccinini and\nCraver realize that their “integrated” account clashes\nwith classical “autonomy” claims for psychology vis-à-vis\nneuroscience. Nevertheless, they insist that their challenge to\nclassical “autonomy” does not commit them to\n“reductionism”, in either its classical or more recent\nvarieties. Their commitment to nested hierarchy of\nmechanisms-within-mechanisms to account for a system’s behavior\nacknowledges the importance of mechanisms and intralevel causation at\nall levels constituting the system, not just at lower (i.e., cellular,\nmolecular) levels. \nDavid Kaplan and Craver (2011) focus the mechanist perspective\ncritically on dynamical systems mathematical models popular in recent\nsystems and computational neuroscience. They argue that such models\nare explanatory only if there exists a “plausible mapping”\nbetween elements in the model and elements in the modeled system. At\nbottom is their Model-to-Mechanism-Mapping (3M) Constraint on\nexplanation. The variables in a genuinely explanatory model correspond\nto components, activities, or organizational features of the system\nbeing explained. And the dependencies posited among variables in the\nmodel, typically expressed mathematically in systems and computational\nneuroscience, correspond to causal relations among the system’s\ncomponents. Kaplan and Craver justify the 3M Constraint on grounds of\nexplanatory norms, common to both science and common sense. All other\nthings being equal, they insist, explanations that provide more\nrelevant details about a system’s components, activities, and\norganization, more likely will answer more questions about how the\nsystem will behave in a variety of circumstances, than will an\nexplanation that provides fewer (mechanistic) details.\n“Relevant” here pertains to the functioning of the\nspecific mechanism. Models from systems and computational neuroscience\nthat violate the 3M Constraint are thus more reasonably thought of as\nmathematical descriptions of phenomena, not explanations of some\n“non-mechanistic” variety. \nKaplan and Craver challenge their own view with one of the more\npopular dynamical/mathematical models in all of computational\nneuroscience, the Haken-Kelso-Bunz (1985) model of human bimanual\nfinger-movement coordination. They point to passages in these\nmodelers’ publications that suggest that the modelers only\nintended for their dynamical systems model to be a mathematically\ncompact description of the temporal evolution of a “purely\nbehavioral dependent variable”. The modelers interpreted none of\nthe model’s variables or parameters as mapping onto components\nor operations of any hypothetical mechanism generating the behavioral\ndata. Nor did they intend for any of the model’s mathematical\nrelations or dependencies among variables to map onto hypothesized\ncausal interactions among components or activities of any mechanism.\nAs Kaplan and Craver further point out, after publishing their\ndynamicist model, these modelers themselves then began to investigate\nhow the behavioral regularities their model described might be\nproduced by neural motor system components, activities, and\norganization. Their own follow-up research suggests that these\nmodelers saw their dynamicist model as a heuristic, to help\nneuroscientists move toward “how-possibly”, and ultimately\nto a “how-actually” mechanistic explanation.  \nAt bottom, Kaplan and Craver’s 3M constraint on explanation\npresents a dilemma for dynamicists. To the extent that dynamical\nsystems modelers intend to model hypothesized neural mechanisms for\nthe phenomenon under investigation, their explanations will need to\ncohere with the 3M Constraint (and other canons of mechanistic\nexplanation). To the extent that this is not a goal of dynamicist\nmodelers, their models do not seem genuinely explanatory, at least not\nin one sense of “explanation” prominent in the history of\nscience. Furthermore, when dynamicist models are judged to be\nsuccessful, they often prompt subsequent searches for underlying\nmechanisms, just as the 3M Constraint and the general mechanist\naccount of the move from “how-possibly” to “how\nactually” mechanisms recommends. Either horn gores dynamicists\nwho claim that their models constitute a necessary additional kind of\nexplanation in neuroscience to mechanistic explanation, beyond any\nheuristic value such models might offer toward discovering\nmechanisms. \nKaplan and Craver’s radical conclusion, that dynamicist\n“explanations” are genuine explanations only to the degree\nthat they respect the (mechanist’s) 3M Constraint, needs more\ndefense. The burden of proof always lies on those whose conclusions\nstrike at popular assumptions. More than the discussion of a couple of\nlandmark dynamicist models in neuroscience is needed (in their 2011,\nKaplan and Craver also discuss the difference-of-Gaussians model of\nreceptive field properties of mammalian visual neurons). Expectedly,\ndynamicists have taken up this challenge. Michael Silberstein and\nAnthony Chemero (2013), for example, argue that localization and\ndecomposition strategies characterize mechanistic explanation, and\nthat some explanations in systems neuroscience violate one of these\nassumptions, or both. Such violations in turn create a dilemma for\nmechanists. Either they must “stretch” their account of\nexplanation, beyond decomposition and localization, to capture these\nrecalcitrant cases, or they must accept “counterexamples”\nto the generality of mechanistic explanation, in both systems\nneuroscience and systems biology more generally. \nLauren Ross (2015) and Mazviita Chirimuuta (2014) independently appeal\nto Robert Batterman’s account of minimal model explanation as an\nimportant kind of non-mechanistic explanation in neuroscience. Minimal\nmodels were developed initially to characterize a kind of explanation\nin the physical sciences (see, e.g., Batterman and Rice 2014).\nBatterman’s account distinguishes between two different kinds of\nscientific “why-questions”: why a phenomenon manifests in\nparticular circumstances; and why a phenomenon manifests generally, or\nin a number of different circumstances. Mechanistic explanations\nanswer the first type of why-question. Here a “more details the\nbetter” (MDB) assumption (Chirimuuta 2014), akin to Kaplan and\nCraver’s “all things being equal” assumption about\nbetter explanations (mentioned above), holds force. Minimal models,\nhowever, which minimize over the presented implantation details and\nhence violate MDB, are better able to answer the second type of\nscientific why-questions. Ross (2015), quoting from computational\nneuroscientists Rinzel and Ermentrout, insists that models containing\nmore details than necessary can obscure identification of critical\nelements by leaving too many open possibilities, especially when one\nis trying to answer Batterman’s second kind of why-questions\nabout a system’s behavior. \nChirimuuta and Ross each appeal to related resources from\ncomputational neuroscience to illustrate the applicability of\nBatterman’s minimal model explanation strategy. Ross appeals to\n“canonical models”, which represent “shared\nqualitative features of a number of distinct neural systems”\n(2015: 39). Her central example is the derivation of the\nErmentrout-Kopell model of class I neuron excitability, which uses\n“mathematical abstraction techniques” to “reduce\nmodels of molecularly distinct neural systems to a single …\ncanonical model”. Such a model “explains why molecularly\ndiverse neural systems all exhibit the same qualitative\nbehavior”, (2015: 41) clearly a Batterman second-type\nwhy-question. Chirimuuta’s resource is “canonical neural\ncomputations” (CNCs):  \ncomputational modules that apply the same fundamental operations in a\nvariety of contexts … a toolbox of computational operations\nthat the brain applies in a number of different sense modalities and\nanatomic regions and which can be described at higher levels of\nabstraction from their biophysical implementation. (Chirimuuta 2014:\n138)  \nExamples include shunting inhibition, linear filtering, recurrent\namplification, and thresholding. Rather than being mechanism-sketches,\nawaiting further mechanistic details to be turned into full-blown\nhow-actually mechanisms, CNCs are invoked in a different explanatory\ncontext, namely, ones posing Batterman’s second type of\nwhy-questions. Ross concurs concerning canonical models:  \nUnderstanding the approach dynamical systems neuroscientists take in\nexplaining [system] behavior requires attending to their explanandum\nof interest and the unique modeling tools [e.g., canonical models]\ncommon in their field. (2015: 52)  \nIn short, both Chirimuuta’s and Ross’s replies to Kaplan\nand Craver’s challenge is a common one in philosophy: save a\nparticular form of explanation from collapsing into another by\nsplitting the explanandum. \nFinally, to wrap up this discussion of mechanism ascendant, an\nanalogue of Craver’s (2007) problem of accounting for\n“constitutive mechanistic relevance”, that is, for\ndetermining which active components of a system are actually part of\nthe mechanism for a given system phenomenon, has also re-emerged in\nrecent discussions. Robert Rupert (2009) suggests that\n“integration” is a key criterion for determining which set\nof causally contributing mechanisms constitute the system for a task,\nbased on the relative frequency with which sets of mechanisms\nco-contribute to causing task occurrences. He cashes frequency of\nco-contribution as the probability of the set for causing the\ncognitive task, conditional to every other co-occurring causal set.\nFelipe De Brigard (2017) challenges Rupert’s criterion, arguing\nthat it cannot account for cognitive systems displaying two features,\n“diachronic dynamicity” along with “functional\nstability”. The frequency with which a given mechanism causally\ncontributes to the same cognitive task (functional stability) can\nchange over time (diachronic dynamicity). Although De Brigard\nemphasizes the critical importance of these features for\nRupert’s integration criterion via a fanciful thought\nexperiment, he also argues that they are a widespread phenomenon in\nhuman brains. Both features are found, for example, in evidence\npertaining to the “Hemispheric Asymmetry Reduction in Older\nAdults”, in which tasks that recruit hemispherically localized\nregions of prefrontal cortex in younger adults show a reduction in\nhemispheric asymmetry in older adults. And both are found in\n“Posterior-Anterior Shift with Aging”, where a task\nincreases activity in anterior brain regions while decreasing activity\nin posterior regions in older adults, relative to activity invoked by\nthe same task in younger adults. \nTo replace Rupert’s notion of integration as a criterion for\ndetermining which sets of mechanisms constitute a cognitive system, De\nBrigard points to two promising recent developments in network\nneuroscience which potentially allow for parametrized time.\n“Scaled inclusivity” is a method for examining each node\nin a network and identifying its membership in “community\nstructures” across different iterations of the network.\n“Temporal-dynamic network analyses” is a way to quantify\nchanges in community structures or modules between networks at\ndifferent time points. Both methods thereby identify “modular\nalliances”, which convey both co-activation and dynamic change\ninformation in a single model. De Brigard suggests that these are thus\nthe candidates with which cognitive systems could be identified. \nClearly, much remains to be discussed regarding the impact mechanism\nhas come to wield in philosophy of neuroscience over the last decade.\nBut while mechanism has become the most dominant general perspective\nin the field, work in other areas continues. Michael Anderson defends\nthe relevance of cognitive neuroscience for determining\npsychology’s taxonomy, independent of any commitment to\nmechanism. The most detailed development of his approach is in his\n(2014) book, After Phrenology, based on his influential\n“neural reuse” hypothesis. Each region of the brain, as\nrecognized by the standard techniques of cognitive neuroscience\n(especially fMRI), engages in cognitive functions that are highly\nvarious, and form different “neural partnerships” with one\nanother under different circumstances. Psychological categories are\nthen to be reconceived along lines suggested by the wide-ranging\nempirical data in support of neural reuse. A genuine\n“post-phrenological” science of the mind must jettison the\nassumption that each brain region performs its own fundamental\ncomputation. In this fashion Anderson’s work explicitly\ncontinues philosophy of neuroscience’s ongoing interest in\nlocalizations of cognitive functions. \nIn shorter compass, Anderson (2015) investigates the relevance of\ncognitive neuroscience for reconceiving psychology’s basic\ncategories, starting from a consequence of his neural reuse\nhypothesis. Attempts to map cognitive processes onto specific neural\nprocesses and brain regions reveal “many-to-many”\nrelations. Not only do these relations show that combined\nanatomical-functional labels for brain regions (e.g., “fusiform\nface area”) are deceptive; they also call into question the\npossibility of deciding between alternative psychological taxonomies\nby appealing to cognitive neuroscientific data. \nFor all but the strongest proponents of psychology’s autonomy\nfrom neuroscience, these many-to-many mappings will suggest that the\npsychological taxonomy we bring to this mapping project needs\nrevision. One need not be committed to any strong sense of\npsychoneural reduction, or the epistemological superiority of\ncognitive neuroscience to psychology, to draw this conclusion. The\nmere relevance of cognitive neuroscience for psychology’s\ncategories is enough. This debate is thus “about the\nrequirements for a unified science of the mind, and the proper role of\nneurobiological evidence in the construction of such an\nontology” (2015: 70), not about the legitimacy of either. \nAnderson divides revisionary projects for psychology into three kinds,\nbased on the degree of revision each kind recommends for psychology,\nand the extent of one-to-one function-to-structure mappings the\nproposed revisions predicts will be available.\n“Conservatives” foresee little need for extensive\nrevisions of psychology’s basic taxonomy, even as more\nneuroscientific evidence is taken into account than current standard\npractices pursue. “Moderates” insist that our knowledge of\nbrain function “can (and should) act as one arbiter of the\npsychologically real” (2015: 70), principally by\n“splitting” or “merging” psychological\nconcepts that currently are in use. “Radicals” project\neven more drastic revisions, even to the most primitive concepts of\npsychology, and even after such revisions they still do not expect\nthat many one-to-one mappings between brain regions and the new\npsychological primitives will be found. Although Anderson does not\nstress this connection (eliminative materialism has not been a\nprominent concern in philosophy of mind or neuroscience for two\ndecades), readers will notice similar themes discussed in\n section 2\n above, only now with scientific, not folk psychology the target of\nthe radical revisionists. A key criterion for any satisfactory\nreformulation of a cognitive ontology is the degree to which it\nsupports two kinds of inferences: “forward inferences”,\nfrom the engagement of a specific cognitive function to the prediction\nof brain activity; and “reverse inferences”, from the\nobservation that a specific brain region or pattern occurs to the\nprediction that a specific cognitive operation is engaged. In light of\nthis explicit criterion, Anderson usefully surveys the work of a\nnumber of prominent psychologists and cognitive neuroscientists in\neach of his revisionist groups. Given his broader commitment to neural\nreuse, and the trek it invites into “evolutionarily-inspired,\necological, and enactive terms”, Anderson’s own sentiments\nlie with the “radicals”:  \nlanguage and mathematics, for instance, are best understood as\nextensions of our basic affordance processing capacities augmented\nwith public symbol systems … The psychological science that\nresults from this reappraisal may well look very different from the\none we practice today. (2015: 75) \nLandmark neuroscientific hypotheses remain a popular focus in recent\nphilosophy of neuroscience. Berit Brogaard (2012), for example, argues\nfor a reinterpretation of the standard “dissociation”\nunderstanding of Melvin Goodale and David Milner’s (1992)\ncelebrated “two visual processing streams”, a landmark,\nnow “textbook” result from late-twentieth century\nneuroscience. Two components of the standard dissociation are key. The\nfirst is that distinct brain regions compute information relevant for\nvisually guided “on-the-fly” actions, and for object\nrecognition, respectively, the dorsal stream (which runs from primary\nvisual cortex through the medial temporal region into the superior and\ninferior parietal lobules) and the ventral stream (which runs from\nprimary visual cortex through V4 and into inferior temporal cortex).\nAnd second, that only information relevant for visual object\nrecognition, processed in the ventral stream, contributes to the\ncharacter of conscious visual experiences. \nBrogaard’s concern is that this standard understanding\nchallenges psychofunctionalism, our currently most plausible\n“naturalistic” account of mental states.\nPsychofunctionalism draws its account of mind directly from our best\ncognitive psychology. If φ is some mental state type that has\ninherited the content of a visual experience, then according to\ncognitive psychology a wide range of visually guided beliefs and\ndesires, different kinds of visual memories, and so on, satisfy\nφ’s description. But by the standard\n“dissociation” account of Goodale and Milner’s two\nvisual streams, only dorsal-stream states, and not ventral-stream\nstates, represent truly egocentric visual properties, namely\n“relational properties which objects instantiate from the point\nof view of believers or perceivers”, (Brogaard 2012: 572). But\naccording to cognitive psychology, dorsal-stream states do not play\nthis wide-ranging φ-role. So according to psychofunctionalism\n“φ-mental states cannot represent egocentric\nproperties” (2012: 572). But it seems “enormously\nplausible” that some of our perceptual beliefs and visual\nmemories represent egocentric properties. So either we reject\npsychofunctionalism, and so our most plausible naturalization project\nfor determining whether a given mental state is instantiated, or we\nreject the standard dissociation interpretation of Goodale and\nMilner’s two visual streams hypothesis, despite the wealth of\nempirical evidence supporting it. Neither horn of this dilemma looks\ncomfortably graspable, although the first horn might be thought to be\nmore so, since psychofunctionalism as a general theory of mind lacks\nthe kind of strong empirical backing that the standard interpretation\nof Goodale and Milner’s hypothesis enjoys. \nNevertheless, Brogaard recommends retaining psychofunctionalism, and\ninstead rejecting “a particular formulation” of Goodale\nand Milner’s two visual stream hypothesis. The interpretation to\nreject insists that “dorsal-stream information cannot contribute\nto the potentially conscious representations computed by the ventral\nstream” (2012: 586–587). Egocentric representations of\nvisual information computed by the dorsal stream contribute to\nconscious visual stream representations “via feedback\nconnections” from dorsal- to ventral-stream neurons (2012: 586).\nThis isn’t to deny dissociation:  \nInformation about the egocentric properties of objects is processed by\nthe dorsal stream, and information about allocentric properties of\nobjects is processed by the ventral stream. (2012: 586)  \nBut this dissociation hypothesis “has no bearing on what\ninformation is passed on to parts of the brain that process\ninformation which correlated with visual awareness” (2012: 586).\nWith this re-interpretation, psychofunctionalism is rendered\nconsistent with Goodale and Milner’s two stream, dorsal and\nventral, “what” and “where/how” hypothesis and\nthe wealth of empirical evidence that supports it. According to\nBrogaard, psychofunctionalism can thereby “correctly treat\nperceptual and cognitive states that carry information processed in\nthe ventral visual stream as capable of representing egocentric\nproperties” (2012: 586). \nDespite philosophy of neuroscience’s continuing focus on\ncognitive/systems/computational-neuroscience (see the discussion in\n section 7 above),\n interest in neurobiology’s cellular/molecular mainstream\nappears to be increasing. One notable paper is Ann-Sophie Barwich and\nKarim Bschir’s (2017) historical-cum-philosophical study of\nG-protein coupled receptors (GPCRs). Work on the structure and\nfunctional significance of these proteins has dominated molecular\nneuroscience for the past forty years; their role in the mechanisms of\na variety of cognitive functions is now empirically documented beyond\nquestion. And yet one finds little interest in, or even notice of this\nshift in mainstream neuroscience among philosophers. Barwich and\nBschir’s yeoman history research on the discovery and\ndevelopment of these objects pays off philosophically. The role of\nmanipulability as a criterion for entity realism in the\nscience-in-practice of wet-lab research becomes meaningful “only\nonce scientists have decided how to conceptually coordinate measurable\neffects distinctly to a scientific object” (2017: 1317).\nScientific objects like GPCRs get assigned varying degrees of reality\nthroughout different stages of the discovery process. Such an\nobject’s role in evaluating the reality of “neighboring\nelements of enquiry” becomes a part of the criteria of its\nreality as well. \nThe impact of science-in-practice on philosophy of science generally\nhas been felt acutely in the philosophy of neuroscience, most notably\nin increased philosophical interest in neuroscientific\nexperimentation. In itself this should not surprise. Neuroscience\nrelies heavily on laboratory experimentation, especially within its\ncellular and molecular, “Society for Neuroscience”\nmainstream. So the call to understand experiment should beckon any\nphilosopher who ventures into neuroscience’s cellular/molecular\nfoundations. Two papers by Jacqueline Sullivan (2009, 2010) have been\nimportant in this new emphasis. In her (2009) Sullivan acknowledges\nboth Bickle’s (2003) and Craver’s (2007) focus on cellular\nand molecular mechanisms of long-term potentiation, and\nexperience-driven form of synaptic plasticity. But she insists that\nbroader philosophical commitments, which lead Bickle to ruthless\nreductionist and Craver to mosaic unity “global” accounts,\nobscure important aspects of real laboratory neuroscience practice.\nShe emphasizes the role of “subprotocols”, which specify\nhow data are to be gathered, in her model of “the experimental\nprocess”, and illustrates these notions with a number of\nexamples. Her analysis reveals an important underappreciated tension\namong a pair of widely-accepted experiment norms. Pursuing\n“reliability” drives experimenters more deeply into\nextensive laboratory controls. Pursuing “external\nvalidity” drives them toward enriched experimental environments\nthat more closely represent the messy natural environment beyond the\nlaboratory. These two norms commonly conflict: in order to get more of\none, scientists introduce conditions that give them less of the other.\n \nIn her (2010) Sullivan offers a detailed history of the Morris water\nmaze task, tracing her account back to Morris’s original\npublications. Philosophers of neuroscience have uncritically assumed\nthat the water maze is a widely-accepted neuroscience protocol for\nrodent spatial learning and memory, but the detailed scientific\nhistory is not so clear on this interpretation. Scientific commentary\nover time on what this task measures, including some from Morris\nhimself, reveals no clear consensus. Sullivan traces the source of\nthis scientific inconsistency back to the impact of 1980s-era\ncellular-molecular reductionism driving experimental behavioral\nneurobiology protocols like the Morris water maze. \nA different motivation drives neurobiologist Alcino Silva,\nneuroinformaticist Anthony Landreth, and philosopher of neuroscience\nJohn Bickle’s (2014) focus on experimentation. All contemporary\nsciences are growing at a vertiginous pace; but perhaps none more so\nthan neuroscience. It is no longer possible for any single scientist\nto keep up with all the relevant published literature in even his or\nher narrow research field, or fully to comprehend its implications. An\noverall lack of clarity and consensus about what is known, what\nremains doubtful, and what has been disproven creates special problems\nfor experiment planning. There is a recognized and urgent need to\ndevelop strategies and tools to address these problems. Toward this\nexplicit end, Silva, Landreth, and Bickle’s book describes a\nframework and a set of principles for organizing the published record.\nThey derive their framework and principles directly from landmark case\nstudies from the influential neuroscientific field of molecular and\ncellular cognition (MCC), and describe how their framework can be used\nto generate maps of experimental findings. Scientists armed with these\nresearch maps can then determine more efficiently what has been\naccomplished in their fields, and where the knowledge gaps still\nreside. The technology needed to automate the generation of these maps\nalready exists. Silva, Landreth, and Bickle sketch the transformative,\nrevolutionary impact these maps can have on current science. \nThree goals motivate Silva, Landreth, and Bickle’s approach.\nFirst, they derive their framework from the cellular and molecular\nneurobiology of learning and memory. This choice was due strictly to\nfamiliarity with the science. Silva was instrumental in bringing gene\ntargeting techniques applied to mammals into behavioral neuroscience,\nand Bickle’s focus on ruthlessly reductive neuroscience was\nbuilt on these and other experimental results. And while each of their\nframework’s different kinds of experiments and evidence have\nbeen recognized by others, theirs purports to be the first to\nsystematize this information explicitly toward the goal of\nfacilitating experimental planning by practicing scientists. Silva,\nLandreth and Bickle insist that important new experiments can be\nidentified and planned by methodically filling in the different forms\nof evidence recognized by their framework, and applying the different\nforms of experiments to the gaps in the experimental record revealed\nby this process. \nSecond, Silva, Landreth, and Bickle take head-on the problem of the\ngrowing amount, complexity and integration of the published literature\nfor experiment planning. They show how graphic weighted\nrepresentations of research findings can be used to guide research\ndecisions; and how to construct these. The principles for constructing\nthese maps are the principles for integrating experimental results,\nderived directly from landmark published MCC research. Using a case\nstudy from recent molecular neuroscience, they show how to generate\nsmall maps that reflect a series of experiments, and how to combine\nthese small maps to illustrate an entire field of neuroscience\nresearch. \nFinally, Silva, Landreth and Bickle begin to develop a science of\nexperiment planning. They envision the causal graphs that compose\ntheir research maps to play a role similar to that played by\nstatistics in the already-developed science of data analysis. Such a\nresource could have profound implications for further developing\ncitation indices and other impact measures for evaluating\ncontributions to a field, from those of individual scientists to those\nof entire institutions. \nMore recently Bickle and Kostko (2018) have extended Silva, Landreth\nand Bickle’s framework beyond the neurobiology of learning and\nmemory. Their case study comes from developmental and social\nneuroscience, Michael Meaney and Moshe Szyf’s work on the\nepigenetics of rodent maternal nursing behaviors on offspring stress\nresponses. Using the details of this case study they elaborate on a\nnotion that Silva, Landreth and Bickle leave underdeveloped, that of\nexperiments designed explicitly for their results, if successful, to\nbe integrated directly into an already-existing background of\nestablished results. And they argue that such experiments\n“integratable by design” with others are aimed not at\nestablishing evidence for individual causal relations among\nneuroscientific kinds, but rather at formulating entire causal\npathways connecting multiple phenomena. Their emphasis on causal paths\nrelates to that of Lauren Ross (forthcoming). Ross’s work is\nespecially interesting in this context because she uses her causal\npathway concept to address “causal selection”, which has\nto do with distinguishing between background conditions and\n“true” (triggering) causes of some outcome of interest.\nFor Silva, Landreth, and Bickle (2014), accounting for this\ndistinction is likewise crucial, and they rely on a specific kind of\nconnection experiment, “positive manipulations”, to draw\nit. Bickle and Kostko’s appeal to causal paths in a detailed\ncase study from recent developmental neurobiology might help bridge\nSilva, Landreth and Bickle’s broader work on neurobiological\nexperimentation with Ross’s work drawn from biology more\ngenerally.","contact.mail":"mandikp@wpunj.edu","contact.domain":"wpunj.edu"},{"date.published":"1999-06-07","date.changed":"2019-08-06","url":"https://plato.stanford.edu/entries/neuroscience/","author1":"John Bickle","author2":"Anthony Landreth","author1.info":"http://www.philosophyandreligion.msstate.edu/faculty/bickle.php","author2.info":"http://www.petemandik.com/philosophy/philosophy.html","entry":"neuroscience","body.text":"\n\n\nOver the past four decades, philosophy of science has grown\nincreasingly “local”. Concerns have switched from general\nfeatures of scientific practice to concepts, issues, and puzzles\nspecific to particular disciplines. Philosophy of neuroscience is one\nnatural result. This emerging area was also spurred by remarkable\ngrowth in the neurosciences themselves. Cognitive and computational\nneuroscience continues to encroach directly on issues traditionally\naddressed within the humanities, including the nature of\nconsciousness, action, knowledge, and normativity. Cellular,\nmolecular, and behavioral neuroscience using animal models\nincreasingly encroaches on cognitive neuroscience’s domain.\nEmpirical discoveries about brain structure and function suggest ways\nthat “naturalistic” programs might develop in detail,\nbeyond the abstract philosophical considerations in their favor.\n\n\nThe literature has distinguished “philosophy of\nneuroscience” from “neurophilosophy” for two\ndecades. The former concerns foundational issues within the\nneurosciences. The latter concerns application of neuroscientific\nconcepts to traditional philosophical questions. Exploring various\nconcepts of representation employed in neuroscientific theories is an\nexample of the former. Examining implications of neurological\nsyndromes for the concept of a unified self is an example of the\nlatter. In this entry, we will develop this distinction further and\ndiscuss examples of both. Just as has happened in the field’s\nhistory, work in both of these areas is scattered throughout most all\nsections below. Throughout we will try to specify which area landmark\nwork falls into, when this location isn’t obvious.\n\n\nOne exciting aspect about working in philosophy of neuroscience or\nneurophilosophy is the continual element of surprise. Both fields\ndepend squarely on developments in neuroscience, and one simply has no\ninkling what’s coming down the pike in that incredibly\nfast-moving science. Last year’s speculative fiction is this\nyear’s scientific reality. But this feature makes a once-a-half\ndecade updated encyclopedia entry difficult to manage. The scientific\ndetails philosophers were reflecting on at past updates can now read\nwoefully dated. Yet one also wants to capture some history of the\nongoing fields. Our solution to this dilemma has been to keep previous\ndiscussions, to reflect that history, but to add more recent\nscientific and philosophical updates, not only to sections of this\nentry added at later times, but also peppered through the earlier\ndiscussions. It’s not always a perfect solution, but it does\npreserve something of the history of the philosophy of neuroscience\nand neurophilosophy against the continual advances in the sciences\nthese philosophical fields depend upon. \n\nHistorically, neuroscientific discoveries exerted little influence on\nthe details of materialist philosophies of mind. The\n“neuroscientific milieu” of the past half-century has made\nit harder for philosophers to adopt substantive dualisms about mind.\nBut even the “type-type” or “central state”\nidentity theories that rose to brief prominence in the late 1950s\n(Place 1956; Smart 1959) drew upon few actual details of the emerging\nneurosciences. Recall the favorite early example of a psychoneural\nidentity claim: “pain is identical to C-fiber firing”. The\n“C-fibers” turned out to be related to only a single\naspect of pain transmission (Hardcastle 1997). Early identity\ntheorists did not emphasize psychoneural identity hypotheses. Their\n“neuro” terms were admittedly placeholders for concepts\nfrom future neuroscience. Their arguments and motivations were\nphilosophical, even if the ultimate justification of the program was\nheld to be empirical. \nThe apology offered by early identity theorists for ignoring\nscientific details was that the neuroscience at that time was too\nnascent to provide any plausible identities. But potential identities\nwere afoot. David Hubel and Torsten Wiesel’s (1962)\nelectrophysiological demonstrations of the receptive field properties\nof visual neurons had been reported with great fanfare. Using their\ntechniques, neurophysiologists began discovering neurons throughout\nvisual cortex responsive to increasingly abstract features of visual\nstimuli: from edges to motion direction to colors to properties of\nfaces and hands. More notably, Donald Hebb had published The\nOrganization of Behavior (1949) more than a decade earlier. He\nhad offered detailed explanations of psychological phenomena in terms\nof neural mechanisms and anatomical circuits. His psychological\nexplananda included features of perception, learning, memory, and even\nemotional disorders. He offered these explanations as potential\nidentities. (See the Introduction to his 1949). One philosopher who\ndid take note of some available neuroscientific detail at the time was\nBarbara Von Eckardt Klein (Von Eckardt Klein 1975). She discussed the\nidentity theory with respect to sensations of touch and pressure, and\nincorporated then-current hypotheses about neural coding of sensation\nmodality, intensity, duration, and location as theorized by\nMountcastle, Libet, and Jasper. Yet she was a glaring exception. By\nand large, available neuroscience at the time was ignored by both\nphilosophical friends and foes of early identity theories. \nPhilosophical indifference to neuroscientific detail became\n“principled” with the rise and prominence of functionalism\nin the 1970s. The functionalists’ favorite argument was based on\nmultiple realizability: a given mental state or event can be realized\nin a wide variety of physical types (Putnam 1967; Fodor 1974).\nConsequently, a detailed understanding of one type of realizing\nphysical system (e.g., brains) will not shed light on the fundamental\nnature of mind. Psychology is thus autonomous from any science of one\nof its possible physical realizers (see the\n entry on multiple realizability\n in this Encyclopedia). Instead of neuroscience, scientifically-minded\nphilosophers influenced by functionalism sought evidence and\ninspiration from cognitive psychology and artificial intelligence.\nThese disciplines abstract away from underlying physical mechanisms\nand emphasize the “information-bearing” properties and\ncapacities of representations (Haugeland 1985). At this same time,\nhowever, neuroscience was delving directly into cognition, especially\nlearning and memory. For example, Eric Kandel (1976) proposed\npresynaptic mechanisms governing transmitter release rate as a\ncell-biological explanation of simple forms of associative learning.\nWith Robert Hawkins (Hawkins and Kandel 1984) he demonstrated how\ncognitivist aspects of associative learning (e.g., blocking,\nsecond-order conditioning, overshadowing) could be explained\ncell-biologically by sequences and combinations of these basic forms\nimplemented in higher neural anatomies. Working on the post-synaptic\nside, neuroscientists began unraveling the cellular mechanisms of long\nterm potentiation (LTP; Bliss and Lomo 1973). Physiological\npsychologists quickly noted its explanatory potential for various\nforms of learning and\n memory.[1]\n Yet few “materialist” philosophers paid any attention.\nWhy should they? Most were convinced functionalists. They believed\nthat the “implementation level” details might be important\nto the clinician, but were irrelevant to the theorist of mind. \nA major turning point in philosophers’ interest in neuroscience\ncame with the publication of Patricia Churchland’s\nNeurophilosophy (1986). The Churchlands (Patricia and Paul)\nwere already notorious for advocating eliminative materialism (see the\nnext section). In her (1986) book, Churchland distilled eliminativist\narguments of the past decade, unified the pieces of the philosophy of\nscience underlying them, and sandwiched the philosophy between a\nfive-chapter introduction to neuroscience and a 70-page chapter on\nthree then-current theories of brain function. She was unapologetic\nabout her intent. She was introducing philosophy of science to\nneuroscientists and neuroscience to philosophers. Nothing could be\nmore obvious, she insisted, than the relevance of empirical facts\nabout how the brain works to concerns in the philosophy of mind. Her\nterm for this interdisciplinary method was “co-evolution”\n(borrowed from biology). This method seeks resources and ideas from\nanywhere on the theory hierarchy above or below the question at issue.\nStanding on the shoulders of philosophers like Quine and Sellars,\nChurchland insisted that specifying some point where neuroscience ends\nand philosophy of science begins is hopeless because the boundaries\nare poorly defined. Neurophilosophers would pick and choose resources\nfrom both disciplines as they saw fit. \nThree themes predominated Churchland’s philosophical discussion:\ndeveloping an alternative to the logical empiricist theory of\nintertheoretic reduction; responding to property-dualistic arguments\nbased on subjectivity and sensory qualia; and responding to\nanti-reductionist multiple realizability arguments. These projects\nremained central to neurophilosophy for more than a decade after\nChurchland’s book appeared. John Bickle (1998) extended the\nprincipal insight of Clifford Hooker’s (1981a,b,c)\npost-empiricist theory of intertheoretic reduction. He quantified key\nnotions using a model-theoretic account of theory structure adapted\nfrom the structuralist program in philosophy of science (Balzer,\nMoulines, and Sneed 1987). He also made explicit a form of argument to\ndraw ontological conclusions (cross-theoretic identities, revisions,\nor eliminations) from the nature of the intertheoretic reduction\nrelations obtaining in specific cases. For example, it is routinely\nconcluded that visible light, a theoretical posit of optics, is\nelectromagnetic radiation within specified wavelengths, a theoretical\nposit of electromagnetism; in this case, a cross-theoretic ontological\nidentity. It is also routine to conclude that phlogiston does not\nexist: an elimination of a kind from our scientific ontology. Bickle\nexplicated the nature of the reduction relation in a specific case\nusing a semi-formal account of “intertheoretic\napproximation” inspired by structuralist results. \nPaul Churchland (1996) carried on the attack on property-dualistic\narguments for the irreducibility of conscious experience and sensory\nqualia. He argued that acquiring some knowledge of existing sensory\nneuroscience increases one’s ability to “imagine” or\n“conceive of” a comprehensive neurobiological explanation\nof consciousness. He defended this conclusion using a\ncharacteristically imaginative thought-experiment based on the history\nof optics and electromagnetism. \nFinally, criticisms of the multiple realizability argument\nflourish—and are challenged—to the present day. Although\nthe multiple realizability argument remains influential among\nnonreductive physicalists, it no longer commands the near-universal\nacceptance it once did. Replies to the multiple realizability argument\nbased on neuroscientific details have appeared. For example, William\nBechtel and Jennifer Mundale (1999) argue that neuroscientists use\npsychological criteria in brain mapping studies. This fact undercuts\nthe likelihood that psychological kinds are multiply realized (for a\nreview of recent developments see the\n entry on multiple realizability\n in this Encyclopedia). \nEliminative materialism (EM), in the form advocated most aggressively\nby Paul and Patricia Churchland, is the conjunction of two claims.\nFirst, our common sense “belief-desire” conception of\nmental events and processes, our “folk psychology”, is a\nfalse and misleading account of the causes of human behavior. Second,\nlike other false conceptual frameworks from both folk theory and the\nhistory of science, it will be replaced by, rather than smoothly\nreduced or incorporated into, a future neuroscience. The\nChurchlands’ characterized folk psychology as the collection of\ncommon homilies invoked (mostly implicitly) to explain human behavior\ncausally. You ask why Marica is not accompanying me this evening. I\nreply that our grandson needed sitting. You nod sympathetically. You\nunderstand my explanation because you share with me a generalization\nthat relates beliefs about taking care of grandchildren, desires to\nhelp daughters and to spend time with grandchildren compared to\nenjoying a night out, and so on. This is just one of a huge collection\nof homilies about the causes of human behavior that EM claims to be\nflawed beyond potential revision. Although this example involves only\nbeliefs and desires, folk psychology contains an extensive repertoire\nof propositional attitudes in its explanatory nexus: hopes,\nintentions, fears, imaginings, and more. EMists predict that a future,\ngenuinely scientific psychology or neuroscience will eventually eschew\nall of these, and replace them with incommensurable states and\ndynamics of neuro-cognition. \nEM is physicalist in one traditional philosophical sense. It\npostulates that some future brain science will be ultimately the\ncorrect account of (human) behavior. It is eliminative in predicting\nthe future rejection of folk psychological kinds from our\npost-neuroscientific ontology. EM proponents often employ scientific\nanalogies (Feyerabend 1963; Paul Churchland, 1981). Oxidative\nreactions as characterized within elemental chemistry bear no\nresemblance to phlogiston release. Even the “direction” of\nthe two processes differ. Oxygen is gained when an object burns (or\nrusts), phlogiston was said to be lost. The result of this theoretical\nchange was the elimination of phlogiston from our scientific ontology.\nThere is no such thing. For the same reasons, according to EM,\ncontinuing development in neuroscience will reveal that there are no\nsuch things as beliefs, desires, and the rest of the propositional\nattitudes as characterized by common sense. \nHere we focus only on the way that neuroscientific results have shaped\nthe arguments for EM. Surprisingly, only one argument has been\nstrongly influenced. (Most arguments for EM stress failures of folk\npsychology as an explanatory theory of behavior.) This argument is\nbased on a development in cognitive and computational neuroscience\nthat might provide a genuine alternative to the representations and\ncomputations implicit in folk psychological generalizations. Many\neliminative materialists assume that folk psychology is committed to\npropositional representations and computations over their contents\nthat mimic logical inferences (Paul Churchland 1981; Stich 1983;\nPatricia Churchland\n 1986).[2]\n Even though discovering an alternative to this view has been an\neliminativist goal for some time, some eliminativists hold that\nneuroscience only began delivering this alternative over the past\nthirty years. Points in and trajectories through vector spaces, as an\ninterpretation of synaptic events and neural activity patterns in\nbiological and artificial neural networks are the key features of this\nalternative. The differences between these notions of cognitive\nrepresentation and transformations, and those of the propositional\nattitudes of folk psychology, provide the basis for one argument for\nEM (Paul Churchland 1987). However, this argument will be opaque to\nthose with no background in cognitive and computational neuroscience,\nso we present a few details. With these details in place, we will\nreturn to this argument for EM (five paragraphs below). \nAt one level of analysis, the basic computational element of a neural\nnetwork, biological or artificial, is the nerve cell, or neuron.\nMathematically, neurons can be represented as simple computational\ndevices, transforming inputs into output. Both inputs and outputs\nreflect biological variables. For our discussion, we assume that\nneuronal inputs are frequencies of action potentials (neuronal\n“spikes”) in the axons whose terminal branches synapse\nonto the neuron in question, while neuronal output is the frequency of\naction potentials generated in its axon after processing the inputs. A\nneuron thereby computes its total input, usually treated\nmathematically as the sum of the products of the signal strength along\neach input line times the synaptic weight on that line. It then\ncomputes a new activation state based on its total input and current\nactivation state, and a new output state based on its new activation\nvalue. The neuron’s output state is transmitted as a signal\nstrength to whatever neurons its axon synapses on. The output state\nreflects systematically the neuron’s new activation\n state.[3] \nAnalyzed in this fashion, both biological and artificial neural\nnetworks are interpreted naturally as vector-to-vector\ntransformers. The input vector consists of values reflecting\nactivity patterns in axons synapsing on the network’s neurons\nfrom outside (e.g., from sensory transducers or other neural\nnetworks). The output vector consists of values reflecting the\nactivity patterns generated in the network’s neurons that\nproject beyond the net (e.g., to motor effectors or other neural\nnetworks). Given that each neuron’s activity depends partly upon\ntheir total input, and its total input depends partly on synaptic\nweights (e.g., presynaptic neurotransmitter release rate, number and\nefficacy of postsynaptic receptors, availability of enzymes in\nsynaptic cleft), the capacity of biological networks to change their\nsynaptic weights make them plastic vector-to-vector\ntransformers. In principle, a biological network with plastic synapses\ncan come to implement any vector-to-vector transformation that its\ncomposition permits (number of input units, output units, processing\nlayers, recurrency, cross-connections, etc.) (discussed in Paul\nChurchland, 1987, with references to the primary scientific\nliterature). Figure 1. \nThe anatomical organization of the cerebellum provides a clear example\nof a network amenable to this computational interpretation. Consider\n Figure 1.\n The cerebellum is the bulbous convoluted structure dorsal to the\nbrainstem. A variety of studies (behavioral, neuropsychological,\nsingle-cell electrophysiological) implicate this structure in motor\nintegration and fine motor coordination. Mossy fibers (axons) from\nneurons outside the cerebellum synapse on cerebellar granule cells,\nwhich in turn project to parallel fibers. Activity patterns across the\ncollection of mossy fibers (frequency of action potentials per time\nunit in each fiber projecting into the cerebellum) provide values for\nthe input vector. Parallel fibers make multiple synapses on the\ndendritic trees and cell bodies of cerebellular Purkinje neurons. Each\nPurkinje neuron “sums” its post-synaptic potentials (PSPs)\nand emits a train of action potentials down its axon based (partly) on\nits total input and previous activation state. Purkinje axons project\noutside the cerebellum. The network’s output vector is thus the\nordered values representing the pattern of activity generated in each\nPurkinje axon. Changes to the efficacy of individual synapses on the\nparallel fibers and the Purkinje neurons alter the resulting PSPs in\nPurkinje axons, generating different axonal spiking frequencies.\nComputationally, this amounts to a different output vector to the same\ninput activity\n pattern—plasticity.[4] Figure 2. \nThis interpretation puts the useful mathematical resources of\ndynamical systems into the hands of computational\nneuroscientists. Vector spaces are an example. Learning can\nthen be characterized fruitfully in terms of changes in synaptic\nweights in the network and subsequent reduction of error in network\noutput. (This approach to learning goes back to Hebb 1949, although\nthe vector-space interpretation was not part of Hebb’s account.)\nA useful representation of this account uses a synaptic\nweight-error space. One dimension represents the global error in\nthe network’s output to a given task, and all other dimensions\nrepresent the weight values of individual synapses in the network.\nConsider\n Figure 2.\n Points in this multi-dimensional state space represent the global\nperformance error correlated with each possible collection of synaptic\nweights in the network. As the weights change with each performance,\nin accordance with a biologically-inspired learning algorithm, the\nglobal error of network performance continually decreases. The\nchanging synaptic weights across the network with each training\nepisode reduces the total error of the network’s output vector,\ncompared to the desired output vector for the input vector. Learning\nis represented as synaptic weight changes correlated with a descent\nalong the error dimension in the space (Churchland and Sejnowski\n1992). Representations (concepts) can be portrayed as\npartitions in multi-dimensional vector spaces. One example is\na neuron activation vector space. See\n Figure 3.\n A graph of such a space contains one dimension for the activation\nvalue of each neuron in the network (or some specific subset of the\nnetwork’s neurons, such as those in a specific layer). A point\nin this space represents one possible pattern of activity in all\nneurons in the network. Activity patterns generated by input vectors\nthat the network has learned to group together will cluster around a\n(hyper-) point or subvolume in the activity vector space. Any input\npattern sufficiently similar to this group will produce an activity\npattern lying in geometrical proximity to this point or subvolume.\nPaul Churchland (1989) argued that this interpretation of network\nactivity provided a quantitative, neurally-inspired basis for\nprototype theories of concepts developed in late-twentieth century\ncognitive psychology. Figure 3. \nUsing this theoretical development, and in the realm of\nneurophilosophy, Paul Churchland (1987, 1989) offered a novel,\nneuroscientifically-inspired argument for EM. According to the\ninterpretation of neural networks just sketched, activity vectors are\nthe central kind of representations, and vector-to-vector\ntransformations are the central kind of computations, in the brain.\nThis contrasts sharply with the propositional representations\nand logical/semantic computations postulated by folk\npsychology. Vectorial content, an ordered sequence of real numbers, is\nunfamiliar and alien to common sense. This cross-theoretic conceptual\ndifference is at least as great as that between oxidative and\nphlogiston concepts, or kinetic-corpuscular and caloric fluid heat\nconcepts. Phlogiston and caloric fluid are two “parade”\nexamples of kinds eliminated from our scientific ontology due to the\nnature of the intertheoretic relation obtaining between the theories\nwith which they are affiliated and the theories that replaced them.\nThe structural and dynamic differences between the folk psychological\nand then-emerging cognitive neuroscientific kinds suggested that the\ntheories affiliated with the latter will likewise replace the theory\naffiliated with the former. But this claim was the key premise of the\neliminativist argument based on predicted intertheoretic relations.\nAnd with the rise of neural networks and parallel distributed\nprocessing, intertheoretic contrasts with folk-psychological\nexplanatory kinds were no longer just an eliminativist’s future\nhope. Computational and cognitive neuroscience was delivering an\nalternative kinematics for cognition, one that provided no structural\nanalogue for folk psychology’s propositional attitudes or\nlogic-like computations over propositional contents. \nCertainly the vector-space alternatives of this interpretation of\nneural networks are alien to folk psychology. But do they justify EM?\nEven if the propositional contents of folk-psychological posits find\nno analogues in one theoretical development in cognitive and\ncomputational neuroscience (that was hot three decades ago), there\nmight be other aspects of cognition that folk psychology gets right.\nWithin the scientific realism that informed early neurophilosophy,\nconcluding that a cross-theoretic identity claim is true (e.g., folk\npsychological state F is identical to neural state N) or that an\neliminativist claim is true (there is no such thing as folk\npsychological state F) depended on the nature of the intertheoretic\nreduction obtaining between the theories affiliated with the posits in\nquestion (Hooker 1981a,b,c; Churchland 1986; Bickle, 1998). But the\nunderlying account of intertheoretic reduction also recognized a\nspectrum of possible reductions, ranging from relatively\n“smooth” through “significantly revisionary”\nto “extremely\n bumpy”.[5]\n Might the reduction of folk psychology to a “vectorial”\ncomputational neuroscience occupy some middle ground between\n“smooth” and “bumpy” intertheoretic reduction\nendpoints, and hence suggest a “revisionary” conclusion?\nThe reduction of classical equilibrium thermodynamics-to-statistical\nmechanics provided a potential analogy here. John Bickle (1992, 1998,\nchapter 6) argued on empirical grounds that such an outcome is likely.\nHe specified conditions on “revisionary” reductions from\nhistorical examples and suggested that these conditions are obtaining\nbetween folk psychology and cognitive neuroscience as the latter\ndevelops. In particular, folk psychology appears to have gotten right\nthe grossly-specified functional profile of many cognitive states,\nespecially those closely related to sensory inputs and behavioral\noutputs. It also appears to get right the “intentionality”\nof many cognitive states—the object that the state is of or\nabout—even though cognitive neuroscience eschews its implicit\nlinguistic explanation of this feature. Revisionary physicalism\npredicts significant conceptual change to folk psychological\nconcepts, but denies total elimination of the caloric fluid-phlogiston\nvariety. \nThe philosophy of science is another area where vector space\ninterpretations of neural network activity patterns has impacted\nphilosophy. In the Introduction to his (1989) book, A\nNeurocomputational Perspective, Paul Churchland asserted,\ndistinctively neurophilosophically, that it will soon be impossible to\ndo serious work in the philosophy of science without drawing on\nempirical work in the brain and behavioral sciences. To justify this\nclaim, in Part II of the book he suggested neurocomputational\nreformulations of key concepts from the philosophy of science. At the\nheart of his reformulations is a neurocomputational account of the\nstructure of scientific theories (1989: chapter 9). Problems with the\northodox “sets-of-sentences” view of scientific theories\nhave been well-known since the 1960s. Churchland advocated replacing\nthe orthodox view with one inspired by the “vectorial”\ninterpretation of neural network activity. Representations implemented\nin neural networks (as sketched above) compose a system that\ncorresponds to important distinctions in the external environment, are\nnot explicitly represented as such within the input corpus, and allow\nthe trained network to respond to inputs in a fashion that continually\nreduces error. According to Churchland, these are functions of\ntheories. Churchland was bold in his assertion: an individual’s\ntheory-of-the-world is a specific point in that individual’s\nerror-synaptic weight vector space. It is a configuration of synaptic\nweights that partitions the individual’s activation vector space\ninto subdivisions that reduce future error messages to both familiar\nand novel inputs. (Consider again\n Figure 2\n and\n Figure 3.)\n This reformulation invites an objection, however. Churchland boasts\nthat his theory of theories is preferable to existing alternatives to\nthe orthodox “sets-of-sentences” account—for\nexample, the semantic view (Suppe 1974; van Fraassen\n1980)—because his is closer to the “buzzing brains”\nthat use theories. But as Bickle (1993) noted, neurocomputational models based on the\nmathematical resources described above are a long way into the realm\nof mathematical abstraction. They are little more than novel (albeit\nsuggestive) application of the mathematics of quasi-linear dynamical\nsystems to simplified schemata of brain circuitries. Neurophilosophers\nowe some account of identifications across ontological categories\n(vector representations and transformation to what?) before the\nphilosophy of science community will treat theories as points in\nhigh-dimensional state spaces implemented in biological neural\nnetworks. (There is an important methodological assumption lurking in\nBickle’s objection, however, which we will discuss toward the\nend of the next paragraph.) \nChurchland’s neurocomputational reformulations of other\nscientific and epistemological concepts build on this account of\ntheories. He sketches “neuralized” accounts of the\ntheory-ladenness of perception, the nature of concept unification, the\nvirtues of theoretical simplicity, the nature of Kuhnian paradigms,\nthe kinematics of conceptual change, the character of abduction, the\nnature of explanation, and even moral knowledge and epistemological\nnormativity. Conceptual redeployment, for example, is the activation\nof an already-existing prototype representation—the centerpoint\nor region of a partition of a high-dimensional vector space in a\ntrained neural network—by a novel type of input pattern.\nObviously, we can’t here do justice to Churchland’s many\nand varied attempts at reformulation. We urge the intrigued reader to\nexamine his suggestions in their original form. But a word about\nphilosophical methodology is in order. Churchland is not\nattempting “conceptual analysis” in anything resembling\nits traditional philosophical sense. Neither, typically, are\nneurophilosophers in any of their reformulation projects. (This is why\na discussion of neurophilosophical reformulations fits with a\ndiscussion of EM.) There are philosophers who take the\ndiscipline’s ideal analyses to be a relatively simple set of\nnecessary and sufficient conditions, expressed in non-technical\nnatural language, governing the application of important concepts\n(like justice, knowledge, theory, or explanation). These analyses\nshould square, to the extent possible, with pretheoretical usage.\nIdeally, they should preserve synonymy. Other philosophers view this\nideal as sterile, misguided, and perhaps deeply mistaken about the\nunderlying structure of human knowledge (Ramsey 1992).\nNeurophilosophers tend to reside in the latter group. Those who\ndislike philosophical speculation about the promise and potential of\ndeveloping science to reformulate\n(“reform-ulate”) traditional philosophical\nconcepts have probably already discovered that neurophilosophy is not\nfor them. But the familiar charge that neurocomputational\nreformulations of the sort Churchland attempts are\n“philosophically uninteresting” or\n“irrelevant” because they fail to provide\n“analyses” of theory, explanation, and the like will fall\non deaf ears among many contemporary “naturalistic”\nphilosophers, who have by and large given up on traditional\nphilosophical “analysis”. \nBefore we leave the topic of proposed neurophilosophical applications\nof this theoretical development from “neural\nnetworks”-style cognitive/computational neuroscience, one final\npoint of actual scientific detail bears mention. This approach did not\nremain state-of-the-art computational neuroscience for long.\nMany neural modelers quickly gave up this approach to\nmodeling the brain. Compartmental modeling enabled\ncomputational neuroscientists to mimic activity in and\ninteractions between patches of neuronal membrane (Bower and Beeman\n1995). This approach permitted modelers to control and manipulate a\nvariety of subcellular factors that determine action potentials per\ntime unit, including the topology of membrane structure in individual\nneurons, variations in ion channels across membrane patches, and field\nproperties of post-synaptic potentials depending on the location of\nthe synapse on the dendrite or soma. By the mid-1990s modelers quickly\nbegan to “custom build” the neurons in their target\ncircuitry. Increasingly powerful computer hardware still allowed them\nto study circuit properties of modeled networks. For these reasons,\nmany serious computational neuroscientists switched to working at a\nlevel of analysis that treats neurons as structured rather than simple\ncomputational devices. With compartmental modeling, vector-to-vector\ntransformations came to be far less useful in serious neurobiological\nmodels, replaced by differential equations representing ion currents\nacross patches of neural membrane. Far more biological detail came to\nbe captured in the resulting models than “connectionist”\nmodels permitted. This methodological change across computational\nneuroscience meant that a neurophilosophy guided by\n“connectionist” resources no longer drew from the state of\nthe art of the scientific field. \nPhilosophy of science and scientific epistemology were not the only\nareas where neurophilosophers urged the relevance of neuroscientific\ndiscoveries for traditionally philosophical topics. A decade after\nNeurophilosophy’s publication, Kathleen Akins (1996)\nargued that a “traditional” view of the senses underlies a\nvariety of sophisticated “naturalistic” programs about\nintentionality. (She cites the Churchlands, Daniel Dennett, Fred\nDretske, Jerry Fodor, David Papineau, Dennis Stampe, and Kim Sterelny\nas examples.) But then-recent neuroscientific work on the mechanisms\nand coding strategies implemented by sensory receptors shows that this\ntraditional view is mistaken. The traditional view holds that sensory\nsystems are “veridical” in at least three ways. (1) Each\nsignal in the system correlates with a small range of properties in\nthe external (to the body) environment. (2) The structure in the\nrelevant external relations that the receptors are sensitive to is\npreserved in the structure of the internal relations among the\nresulting sensory states. And (3) the sensory system reconstructs\nfaithfully, without fictive additions or embellishments, the external\nevents. Using then-recent neurobiological discoveries about response\nproperties of thermal receptors in the skin (i.e.,\n“thermoreceptors”) as an illustration, Akins showed that\nsensory systems are “narcissistic” rather than\n“veridical”. All three traditional assumptions are\nviolated. These neurobiological details and their philosophical\nimplications open novel questions for the philosophy of perception and\nfor the appropriate foundations for naturalistic projects about\nintentionality. Armed with the known neurophysiology of sensory\nreceptors, our “philosophy of perception” or account of\n“perceptual intentionality” will no longer focus on the\nsearch for correlations between states of sensory systems and\n“veridically detected” external properties. This\ntraditional philosophical (and scientific!) project rests upon a\nmistaken “veridicality” view of the senses.\nNeuroscientific knowledge of sensory receptor activity also shows that\nsensory experience does not serve the naturalist well as a\n“simple paradigm case” of an intentional relation between\nrepresentation and world. Once again, available scientific detail\nshowed the naivety of some traditional philosophical projects. \nFocusing on the anatomy and physiology of the pain transmission\nsystem, Valerie Hardcastle (1997) urged a similar negative implication\nfor a popular methodological assumption. Pain experiences have long\nbeen philosophers’ favorite cases for analysis and theorizing\nabout conscious experiences generally. Nevertheless, every position\nabout pain experiences has been defended: eliminativism, a variety of\nobjectivist views, relational views, and subjectivist views. Why so\nlittle agreement, despite agreement that pain experiences are the\nplace to start an analysis or theory of consciousness? Hardcastle\nurged two answers. First, philosophers tend to be uninformed about the\nneuronal complexity of our pain transmission systems, and build their\nanalyses or theories on the outcome of a single component of a\nmulti-component system. Second, even those who understand some of the\nunderlying neurobiology of pain tend to advocate gate-control\n theories.[6]\n But the best existing gate-control theories are vague about the\nneural mechanisms of the gates. Hardcastle instead proposed a\ndissociable dual system of pain transmission, consisting of a pain\nsensory system closely analogous in its neurobiological implementation\nto other sensory systems, and a descending pain inhibitory system. She\nargued that this dual system is consistent with neuroscientific\ndiscoveries and accounts for all the pain phenomena that have tempted\nphilosophers toward particular (but limited) theories of pain\nexperience. The neurobiological uniqueness of the pain inhibitory\nsystem, contrasted with the mechanisms of other sensory modalities,\nrenders pain processing atypical. In particular, the pain inhibitory\nsystem dissociates pain sensation from stimulation of nociceptors\n(pain receptors). Hardcastle concluded from the neurobiological\nuniqueness of pain transmission that pain experiences are atypical\nconscious events, and hence not a good place to start theorizing about\nor analyzing the general type. \nDeveloping and defending theories of content is a central topic in\ncontemporary philosophy of mind. A common desideratum in this debate\nis a theory of cognitive representation consistent with a physical or\nnaturalistic ontology. We’ll here describe a few contributions\nneurophilosophers have made to this project. \nWhen one perceives or remembers that he is out of coffee, his brain\nstate possesses intentionality or “aboutness”. The percept\nor memory is about one’s being out of coffee; it represents one\nas being out of coffee. The representational state has content. A\npsychosemantics seeks to explain what it is for a representational\nstate to be about something, to provide an account of how states and\nevents can have specific representational content. A physicalist\npsychosemantics seeks to do this using resources of the physical\nsciences exclusively. Neurophilosophers have contributed to two types\nof physicalist psychosemantics: the Functional Role approach and the\nInformational approach. For a description of these and other theories\nof mental content, see the entries on\n causal theories of mental content,\n mental representation, and\n teleological theories of mental content. \nThe core claim of a functional role semantics is that a representation\nhas its specific content in virtue of relations it bears to other\nrepresentations. Its paradigm application is to concepts of\ntruth-functional logic, like the conjunctive “and” or\ndisjunctive “or”. A physical event instantiates the\n“and” function just in case it maps two true inputs onto a\nsingle true output. Thus it is the relations an expression bears to\nothers that give it the semantic content of “and”.\nProponents of functional role semantics propose similar analyses for\nthe content of all representations (Block 1995). A physical event\nrepresents birds, for example, if it bears the right relations to\nevents representing feathers and others representing beaks. By\ncontrast, informational semantics ascribe content to a state depending\nupon the causal relations obtaining between the state and the object\nit represents. A physical state represents birds, for example, just in\ncase an appropriate causal relation obtains between it and birds. At\nthe heart of informational semantics is a causal account of\ninformation (Dretske 1981, 1988). Red spots on a face carry the\ninformation that one has measles because the red spots are caused by\nthe measles virus. A common criticism of informational semantics holds\nthat mere causal covariation is insufficient for representation, since\ninformation (in the causal sense) is by definition always veridical\nwhile representations can misrepresent. A popular solution to this\nchallenge invokes a teleological analysis of “function”. A\nbrain state represents X by virtue of having the function of\ncarrying information about being caused by X (Dretske 1988).\nThese two approaches do not exhaust the popular options for a\npsychosemantics, but are the ones to which neurophilosophers have most\ncontributed. \nPaul Churchland’s allegiance to functional role semantics goes\nback to his earliest views about the semantics of terms in a language.\nIn his (1979) book, he insisted that the semantic identity (content)\nof a term derives from its place in the network of sentences of the\nentire language. The functional economies envisioned by early\nfunctional role semanticists were networks with nodes corresponding to\nthe objects and properties denoted by expressions in a language. Thus\none node, appropriately connected, might represent birds, another\nfeathers, and another beaks. Activation of one of these would tend to\nspread activation to the others. As “connectionist” neural\nnetwork modeling developed (as discussed in the previous section\nabove), alternatives arose to this one-representation-per-node\n“localist” approach. By the time Churchland (1989)\nprovided a neuroscientific elaboration of functional role semantics\nfor cognitive representations generally, he too had abandoned the\n“localist” interpretation. Instead, he offered a\n“state-space semantics”. \nWe saw in the previous section how (vector) state spaces provide an\ninterpretation for activity patterns in neural networks, both\nbiological and artificial. A state-space semantics for cognitive\nrepresentations is a species of a functional role semantics because\nthe individuation of a particular state depends upon the relations\nobtaining between it and other states. A representation is a point in\nan appropriate state space, and points (or subvolumes) in a space are\nindividuated by their relations to other points (locations,\ngeometrical proximity). Paul Churchland (1989, 1995) illustrated a\nstate-space semantics for neural states by appealing to sensory\nsystems. One popular theory in sensory neuroscience of how the brain\ncodes for sensory qualities (like color) is the opponent process\naccount (Hardin 1988). Churchland (1995) describes a\nthree-dimensional activation vector state-space in which every color\nperceivable by humans is represented as a point (or subvolume). Each\ndimension corresponds to activity rates in one of three classes of\nphotoreceptors present in the human retina and their efferent paths:\nthe red-green opponent pathway, yellow-blue opponent pathway, and\nblack-white (contrast) opponent pathway. Photons striking the retina\nare transduced by photoreceptors, producing an activity rate in each\nof the segregated pathways. A represented color is hence a triplet of\nneuronal activation frequency rates. As an illustration, consider\nagain\n Figure 3.\n Each dimension in that three-dimensional space will represent average\nfrequency of action potentials in the axons of one class of ganglion\ncells projecting out of the retina. Each color perceivable by humans\nwill be a region of that space. For example, an orange stimulus\nproduces a relatively low level of activity in both the red-green and\nyellow-blue opponent pathways (x-axis and y-axis,\nrespectively), and middle-range activity in the black-white (contrast)\nopponent pathway (z-axis). Pink stimuli, on the other hand,\nproduce low activity in the red-green opponent pathway, middle-range\nactivity in the yellow-blue opponent pathway, and high activity in the\nblack-white (contrast) opponent\n pathway.[7]\n The location of each color in the space generates a “color\nsolid”. Location on the solid, and geometrical proximity between\nthese locations, reflect structural similarities between the perceived\ncolors. Human gustatory representations are points in a\nfour-dimensional state space, with each dimension coding for activity\nrates generated by gustatory stimuli in each type of taste receptor\n(sweet, salty, sour, and bitter) and their segregated efferent\npathways. When implemented in a neural network with structural\nresources, and hence computational resources as vast as the human\nbrain, the state space approach to psychosemantics generates a theory\nof content for a huge number of cognitive\n states.[8] \nJerry Fodor and Ernest LePore (1992) raised an important challenge to\nChurchland’s psychosemantics. Location in a state space alone\nseems insufficient to fix a state’s representational content.\nChurchland never explains why a point in a three-dimensional state\nspace represents a color, as opposed to any other quality,\nobject, or event that varies along three\n dimensions.[9].\n So Churchland’s account achieves its explanatory power by the\ninterpretation imposed on the dimensions. Fodor and LePore alleged\nthat Churchland never specified how a dimension comes to represent,\ne.g., degree of saltiness, as opposed to yellow-blue wavelength\nopposition. One obvious answer appeals to the stimuli that form the\n“external” inputs to the neural network in question. Then,\nfor example, the individuating conditions on neural representations of\ncolors are that opponent processing neurons receive input from a\nspecific class of photoreceptors. The latter in turn have\nelectromagnetic radiation (of a specific portion of the visible\nspectrum) as their activating stimuli. However, this appeal to\n“external” stimuli as the ultimate individuating\nconditions for representational content makes the resulting approach a\nversion of informational semantics. Is this approach consonant with\nother neurobiological details? \nThe neurobiological paradigm for informational semantics is the\nfeature detector: one or more neurons that are (i) maximally\nresponsive to a particular type of stimulus, and (ii) have the\nfunction of indicating the presence of that stimulus type. Examples of\nsuch stimulus-types for visual feature detectors include high-contrast\nedges, motion direction, and colors. A favorite feature detector among\nphilosophers is the alleged fly detector in the frog. Lettvin et al.\n(1959) identified cells in the frog retina that responded maximally to\nsmall shapes moving across the visual field. The idea that these\ncells’ activity functioned to detect flies rested upon knowledge\nof the frogs’ diet. (Bechtel 1998 provides a useful discussion.)\nUsing experimental techniques ranging from single-cell recording to\nsophisticated functional imaging, neuroscientists discovered a host of\nneurons that are maximally responsive to a variety of complex stimuli.\nHowever, establishing condition (ii) on a feature detector is much\nmore difficult. Even some paradigm examples have been called into\nquestion. David Hubel and Torsten Wiesel’s (1962) Nobel\nPrize-winning work establishing the receptive fields of neurons in\nstriate (visual) cortex is often interpreted as revealing cells whose\nfunction is edge detection. However, Lehky and Sejnowski (1988)\nchallenged this interpretation. They trained an artificial neural\nnetwork to distinguish the three-dimensional shape and orientation of\nan object from its two-dimensional shading pattern. Their network\nincorporates many features of visual neurophysiology. Nodes in the\ntrained network turned out to be maximally responsive to edge\ncontrasts, but did not appear to have the function of edge detection.\n(See Churchland and Sejnowski 1992 for a review.) \nKathleen Akins (1996) offered a different neurophilosophical challenge\nto informational semantics and its affiliated feature-detection view\nof sensory representation. We saw in the previous section that Akins\nargued that the physiology of thermoreception violates three necessary\nconditions on “veridical” representation. From this fact\nshe raised doubts about looking for feature-detecting neurons to\nground a psychosemantics generally, including for thought contents.\nHuman thoughts about flies, for example, are sensitive to numerical\ndistinctions between particular flies and the particular locations\nthey can occupy. But the ends of frog nutrition are well served\nwithout a representational system sensitive to such ontological\nniceties. Whether a fly seen now is numerically identical to one seen\na moment ago need not, and perhaps cannot, figure into the\nfrog’s feature detection repertoire. Akins’ critique cast\ndoubt on whether details of sensory transduction will scale up to\nprovide an adequate unified psychosemantics for all concepts. It also\nraised new questions for human intentionality. How do we get from\nactivity patterns in “narcissistic” sensory receptors,\nkeyed not to “objective” environmental features but rather\nonly to effects of the stimuli on the patch of tissue innervated, to\nhuman ontologies replete with enduring objects with stable\nconfigurations of properties and relations, types and their tokens (as\nthe “fly-thought” example presented above reveals), and\nthe rest? And how did the development of a stable, rich ontology\nconfer survival advantages to human ancestors? \nConsciousness re-emerged over the past three decades as a topic of\nresearch focus in philosophy of mind, and in the cognitive and brain\nsciences. Instead of ignoring it, many physicalists sought to explain\nit (Dennett 1991). Here we focus exclusively on ways that\nneuroscientific discoveries have impacted philosophical debates about\nthe nature of consciousness and its relation to physical mechanisms.\n(See links to other entries in this encyclopedia below in\n Related Entries\n for broader discussions about consciousness and physicalism.) \nThomas Nagel (1974) argued famously that conscious experience is\nsubjective, and thus permanently recalcitrant to objective scientific\nunderstanding. He invited us to ponder “what it is like to be a\nbat” and urged the intuitive judgment that no amount of\nphysical-scientific knowledge, including neuroscientific, supplies a\ncomplete answer. Nagel’s intuition pump has generated extensive\nphilosophical discussion. At least two well-known replies made direct\nappeal to neurophysiology. John Biro (1991) suggested that part of the\nintuition pumped by Nagel, that bat experience is substantially\ndifferent from human experience, presupposes systematic relations\nbetween physiology and phenomenology. Kathleen Akins (1993a) delved\ndeeper into existing knowledge of bat physiology and reports much that\nis pertinent to Nagel’s question. She argued that many of the\nquestions about bat subjective experience that we still consider open\nhinge on questions that remain unanswered about neuroscientific\ndetails. One example of the latter is the function of various cortical\nactivity profiles in the active bat. \nDavid Chalmers (1996) famously argued that any possible brain-process\naccount of consciousness will leave open an “explanatory\ngap” between the brain process and properties of the conscious\n experience.[10]\n This is because no brain-process theory can answer the\n“hard” question: Why should that particular brain process\ngive rise to that particular conscious experience? We can always\nimagine (“conceive of”) a universe populated by creatures\nhaving those brain processes but completely lacking conscious\nexperience. A theory of consciousness requires an explanation of how\nand why some brain process causes a conscious experience, replete with\nall the features we experience. The fact that the hard question\nremains unanswered shows that we will probably never get a complete\nexplanation of consciousness at the level of neural mechanism. Paul\nand Patricia Churchland (1997) offered the following diagnosis and\nreply. Chalmers offers a conceptual argument, based on our\nability to imagine creatures possessing active brains like ours but\nwholly lacking in conscious experiences. But the more one learns about\nhow the brain produces conscious experience—and such a\nliterature has emerged (for some early work, see Gazzaniga\n1995)—the harder it becomes to imagine a universe consisting of\ncreatures with brain processes like ours but lacking consciousness.\nThis is not just bare assertion. The Churchlands appeal to some\nneurobiological detail. For example, Paul Churchland (1995) develops a\nneuroscientific account of consciousness based on recurrent\nconnections between thalamic nuclei, particularly between\n“diffusely projecting” nuclei like the intralaminar nuclei\nand the\n cortex.[11]\n Churchland argues that thalamocortical recurrency accounts for the\nselective features of consciousness, for the effects of short-term\nmemory on conscious experience, for vivid dreaming during REM\n(rapid-eye movement) sleep, and other “core” features of\nconscious experience. In other words, the Churchlands are claiming\nthat when one learns about activity patterns in these recurrent\ncircuits, one can no longer “imagine” or “conceive\nof” this activity occurring without these core features of\nconscious experience occurring. (Other than just mouthing the\nexpression, “I am now imagining activity in these circuits\nwithout selective attention/the effects of short-term memory/vivid\ndreaming/…”). \nA second focus of skeptical arguments about a complete neuroscientific\nexplanation of consciousness is on sensory qualia: the\nintrospectable qualitative aspects of sensory experience, the features\nby which subjects discern similarities and differences among their\nexperiences. The colors of visual sensations are a philosopher’s\nfavorite example. One famous puzzle about color qualia is the alleged\nconceivability of spectral inversions. Many philosophers claim that it\nis conceptually possible (if perhaps physically impossible) for two\nhumans not to differ neurophysiologically, while the color that fire\nengines and tomatoes appear to have to one subject is the color that\ngrass and frogs appear to have to the other (and vice versa). A large\namount of neuroscientifically-informed philosophy has addressed this\nquestion. (C.L. Hardin 1988 and Austen Clark 1993 are noteworthy\nexamples.) A related area where neurophilosophical considerations have\nemerged concerns the metaphysics of colors themselves (rather than\ncolor experiences). A longstanding philosophical dispute is whether\ncolors are objective properties existing external to perceivers or\nrather identifiable as or dependent upon minds or nervous systems.\nSome neuroscientific work on this problem begins with characteristics\nof color experiences: for example, that color similarity judgments\nproduce color orderings that align on a circle (Clark 1993). With this\nresource, one can seek mappings of phenomenology onto environmental or\nphysiological regularities. Identifying colors with particular\nfrequencies of electromagnetic radiation does not preserve the\nstructure of the hue circle, whereas identifying colors with activity\nin opponent processing neurons does. Such a tidbit is not decisive for\nthe color objectivist-subjectivist debate, but it does convey the type\nof neurophilosophical work being done on traditional metaphysical\nissues beyond the philosophy of mind. (For more details on these\nissues, see the\n entry on color\n in this Encyclopedia.) \nWe saw in the discussion of Hardcastle (1997) two sections above that\nneurophilosophers have entered disputes about the nature and\nmethodological import of pain experiences. Two decades earlier, Dan\nDennett (1978) took up the question of whether it is possible to build\na computer that feels pain. He compares and notes tension between\nneurophysiological discoveries and common sense intuitions about pain\nexperience. He suspects that the incommensurability between scientific\nand common sense views is due to incoherence in the latter. His\nattitude is wait-and-see. But foreshadowing Churchland’s reply\nto Chalmers, Dennett favors scientific investigations over\nconceivability-based philosophical arguments. \nNeurological deficits have attracted philosophers interested in\nconsciousness. For nearly fifty years philosophers have debated\nimplications for the unity of the self of the Nobel Prize-winning\nexperiments with commissurotomy patients who, for clinical reasons,\nhad their corpus callosum surgically ablated (Nagel\n 1971).[12]\n The corpus callosum is the huge bundle of axons connecting neurons\nacross the left and right mammalian cerebral hemispheres. In carefully\ncontrolled experiments, commissurotomy patients seemingly display two\ndissociable “seats” of consciousness. Elizabeth Schechter\n(2018) has recently greatly updated philosophical treatment of the\nscientific details of these “split-brain” patients,\nincluding their own experiential reports, and has traced implications\nfor our understanding of the self.  \nIn chapter 5 of her (1986) book, Patricia Churchland extended both the\nrange and philosophical implications of neurological deficits. One\ndeficit she discusses in detail is blindsight. Some patients with\nlesions to primary visual cortex report being unable to see items in\nregions of their visual fields, yet perform far better than chance in\nforced guess trials about stimuli in those regions. A variety of\nscientific and philosophical interpretations have been offered. Ned\nBlock (1995) worried that many of these interpretations conflate\ndistinct notions of consciousness. He labels these notions\n“phenomenal consciousness” (“P-consciousness”)\nand “access consciousness”\n(“A-consciousness”). The former is the “what it is\nlike”-ness of conscious experiences. The latter is the\navailability of representational content to self-initiated action and\nspeech. Block argued that P-consciousness is not always\nrepresentational, whereas A-consciousness is. Dennett (1991, 1995) and\nTye (1993) are skeptical of non-representational analyses of\nconsciousness in general. They provide accounts of blindsight that do\nnot depend on Block’s distinction. \nWe break off our brief overview of neurophilosophical work on\nconsciousness here. Many other topics are worth neurophilosophical\npursuit. We mentioned commissurotomy and the unity of consciousness\nand the self, which continues to generate discussion. Qualia beyond\nthose of color and pain experiences quickly attracted\nneurophilosophical attention (Akins 1993a,b, 1996; Austen Clark 1993),\nas did self-consciousness (Bermúdez 1998). \nOne of the first issues to arise in neurology, as far back as the\nnineteenth century, concerned the localization of specific cognitive\nfunctions to specific brain regions. Although the\n“localization” approach had dubious origins in the\nphrenology of Gall and Spurzheim, and had been challenged strenuously\nby Flourens throughout the early nineteenth century, it re-emerged\nlate in the nineteenth century in the study of aphasia by Bouillaud,\nAuburtin, Broca, and Wernicke. These neurologists made careful studies\n(when possible) of linguistic deficits in their aphasic patients,\nfollowed by brain autopsies post\n mortem.[13]\n Broca’s initial study of twenty-two patients in the\nmid-nineteenth century confirmed that damage to the left cortical\nhemisphere was predominant, and that damage to the second and third\nfrontal convolutions was necessary to produce speech production\ndeficits. Although the anatomical coordinates Broca postulated for the\n“speech production center” do not correlate exactly with\ndamage producing production deficits, both this area of frontal cortex\nand speech production deficits still bear his name\n(“Broca’s area” and “Broca’s\naphasia”). Less than two decades later Carl Wernicke published\nevidence for a second language center. This area is anatomically\ndistinct from Broca’s area, and damage to it produced a very\ndifferent set of aphasic symptoms. The cortical area that still bears\nhis name (“Wernicke’s area”) is located around the\nfirst and second convolutions in temporal cortex, and the aphasia that\nbears his name (“Wernicke’s aphasia”) involves\ndeficits in language comprehension. Wernicke’s method, like\nBroca’s, was based on lesion studies produced by natural trauma:\na careful evaluation of the behavioral deficits, followed by post\nmortem autopsies to find the sites of tissue damage and atrophy. More\nrecent and more careful lesion studies suggest more precise\nlocalization of specific linguistic functions, and remain a\ncornerstone to this day in aphasia research. \nLesion studies have also produced evidence for the localization of\nother cognitive functions: for example, sensory processing and certain\ntypes of learning and memory. However, localization arguments for\nthese other functions invariably include studies using animal models.\nWith an animal model, one can perform careful behavioral measures in\nhighly controlled settings, then ablate specific areas of neural\ntissue (or use a variety of other techniques to block or enhance\nactivity in these areas) and re-measure performance on the same\nbehavioral tests. Since we lack widely accepted animal models for\nhuman language production and comprehension, this additional evidence\nisn’t available to the neurologist or neurolinguists. This\nlimitation makes the neurological study of language a paradigm case\nfor evaluating the logic of the lesion/deficit method of inferring\nfunctional localization. Barbara Von Eckardt (Von Eckardt Klein 1978)\nattempted to make explicit the steps of reasoning involved in this\ncommon and historically important method. Her analysis begins with\nRobert Cummins’ well-known analysis of functional explanation,\nbut she extends it into a notion of structurally adequate\nfunctional analysis. These analyses break down a complex capacity C\ninto its constituent capacities c1,\nc2,…, cn,\nwhere the constituent capacities are consistent with the underlying\nstructural details of the system. For example, human speech production\n(complex capacity C) results from formulating a speech intention, then\nselecting appropriate linguistic representations to capture the\ncontent of the speech intention, then formulating the motor commands\nto produce the appropriate sounds, then communicating these motor\ncommands to the appropriate motor pathways (all together, the\nconstituent capacitiesc1,\nc2,…, cn). A\nfunctional-localization hypothesis has the form: brain structure S in\norganism (type) O has constituent capacity\nci, where ci\nis a function of some part of O. An example might be: Broca’s\narea (S) in humans (O) formulates motor commands to produce the\nappropriate sounds (one of the constituent capacities\nci). Such hypotheses specify aspects of\nthe structural realization of a functional-component model. They are\npart of the theory of the neural realization of the functional\nmodel. \nArmed with these characterizations, Von Eckardt Klein argues that inference\nto a functional-localization hypothesis proceeds in two steps. First,\na functional deficit in a patient is hypothesized based on the\nabnormal behavior the patient exhibits. Second, localization of\nfunction in normal brains is inferred on the basis of the functional\ndeficit hypothesis plus the evidence about the site of brain damage.\nThe structurally-adequate functional analysis of the capacity connects\nthe pathological behavior to the hypothesized functional deficit. This\nconnection suggests four adequacy conditions on a functional deficit\nhypothesis. First, the pathological behavior P (e.g., the speech\ndeficits characteristic of Broca’s aphasia) must result from\nfailing to exercise some complex capacity C (human speech production).\nSecond, there must be a structurally-adequate functional analysis of\nhow people exercise capacity C that involves some constituent capacity\nci (formulating motor commands to produce\nthe appropriate sounds). Third, the operation of the steps described\nby the structurally-adequate functional analysis minus the operation\nof the component performing ci\n(Broca’s area) must result in pathological behavior P. Fourth,\nthere must not be a better available explanation for why the patient\ndoes P. Arguments to a functional deficit hypothesis on the basis of\npathological behavior is thus an instance of argument to the best\navailable explanation. When postulating a deficit in a normal\nfunctional component provides the best available explanation of the\npathological data, we are justified in drawing the inference. \nVon Eckardt Klein applies this analysis to a neurological case study\ninvolving a controversial reinterpretation of\n agnosia.[14]\n Her philosophical explication of this important neurological method\nreveals that most challenges to localization arguments either argue\nonly against the localization of a particular type of functional\ncapacity or against generalizing from localization of function in one\nindividual to all normal individuals. (She presents examples of each\nfrom the neurological literature.) Such challenges do not impugn the\nvalidity of standard arguments for functional localization from\ndeficits. It does not follow that such arguments are unproblematic.\nBut they face difficult factual and methodological problems, not\nlogical ones. Furthermore, the analysis of these arguments as\ninvolving a type of functional analysis and inference to the best\navailable explanation carries an important implication for the\nbiological study of cognitive function. Functional analyses require\nfunctional theories, and structurally adequate functional analyses\nrequire checks imposed by the lower level sciences investigating the\nunderlying physical mechanisms. Arguments to best available\nexplanation are often hampered by a lack of theoretical imagination:\nthe available alternative explanations are often severely limited. We\nmust seek theoretical inspiration from any level of investigation or\nexplanation. Hence making explicit the “logic” of this\ncommon and historically important form of neurological explanation\nreveals the necessity of joint participation from all scientific\nlevels, from cognitive psychology down to molecular neuroscience. Von\nEckardt Klein (1978) thus anticipated what came to be heralded as the\n“co-evolutionary research methodology”, which remains a\ncenterpiece of neurophilosophy to the present day (see\n section 6). \nOver the last three decades, new evidence for localizations of\ncognitive functions has come increasingly from a new source, the\ndevelopment and refinement of neuroimaging techniques. However, the\nlogical form of localization-of-function arguments appears not to have\nchanged from those employing lesion studies, as analyzed by Von\nEckardt Klein. Instead, these new neuroimaging technologies resolve some of\nthe methodological problems that plagued lesion studies. For example,\nresearchers do not need to wait until the patient dies, and in the\nmeantime probably acquires additional brain damage, to find the lesion\nsites. Two functional imaging techniques have been prominent in\nphilosophical discussions: positron emission tomography, or PET, and\nfunctional magnetic resonance imaging, or fMRI. Although these measure\ndifferent biological markers of functional activity, PET approved for\nhuman use now has spatial resolution down to the single mm range,\nwhile fMRI has resolution down to less than\n 1mm.[15]\n As these techniques increased spatial and temporal resolution of\nfunctional markers, and continued to be used with sophisticated\nbehavioral methodologies, arguments for localizing specific\npsychological functions to increasingly specific neural regions\ncontinued to grow. Stufflebeam and Bechtel provided an early and\nphilosophically useful discussion of PET. Bechtel and Richardson\n(1993) provided a general framework for “localization and\ndecomposition” arguments, which anticipated in many ways the\ncoming “new mechanistic” perspective in philosophy of\nscience and philosophy of neuroscience (see\n sections 7 and 8 below).\n Bechtel and Mundale (1999) further refined philosophical arguments\nfor localization of function specific to neuroscience. \nMore recent philosophical discussion of these functional imaging\ntechniques has tended to urge more caution resting localization claims\non their results. Roskies (2007), for example, points out the tendency\nto think of the evidential force of functional neuroimages (especially\nfMRI) on an analogy of that of photographs. Drawing on work in\naesthetics and the visual arts, Roskies argues that many of the\nfeatures that give photographs their evidential force are not present\nin functional neuroimages. So while neuroimages do serve as evidence\nfor claims about neurofunctions, and even for localization hypotheses,\ndetails of their proper interpretation are far more complicated than\nphilosophers sometimes assume. More critically, Klein (2010) argues\nthat images of “brain activity” resulting from functional\nneuroimaging, especially fMRI are poor evidence for functional\nhypotheses. For these images present the results of null hypothesis\nsignificance testing on fMRI data, and such testing alone cannot\nprovide evidence about the functional structure of a causally dense\nsystem, which the human brain is. Instead, functional neuroimages are\nproperly interpreted as indicating regions where further data and\nanalysis are warranted. But these data will typically require more\nthan simple significance testing, so skepticism about the evidential\nforce of neuroimages does not warrant skepticism more generally about\nfMRI.  \nLocalization of function remains to this day a central topic of\ndiscussion in philosophy of neuroscience. We will cover more recent\nwork in later sections. \nWhat neuroscience has now discovered about the cellular and molecular\nmechanisms of neural conductance and transmission is spectacular.\nThese results constitute one of the crowning achievements of\nscientific inquiry. (For those in doubt, simply peruse for five\nminutes a recent volume of Society for Neuroscience\nAbstracts.) Less comprehensive, yet still spectacular, are\ndiscoveries at “higher” levels of neuroscience: circuits,\nnetworks, and systems. All this is a natural outcome of increasing\nscientific specialization. We develop the technology, the experimental\ntechniques, and ultimately the experimental results-driven theories\nwithin specific disciplines to push forward our understanding. Still,\na crucial aspect of the total picture sometimes gets neglected: the\nrelationship between the levels, the “glue” that binds\nknowledge of neuron activity to subcellular and molecular mechanisms\n“below”, and to circuit, network, and systems activity\npatterns “above”. This problem is especially glaring when\nwe try to relate “cognitivist” psychological theories,\npostulating information-bearing representations and processes\noperating over their contents, to neuronal activities.\n“Co-evolution” between these explanatory levels still\nseems more a distant dream than an operative methodology guiding\nday-to-day scientific research. \nIt is here that some philosophers and neuroscientists turned to\ncomputational methods (Churchland and Sejnowski 1992). One hope was\nthat the way computational models have functioned in more developed\nsciences, like in physics, might provide a useful model. One\ncomputational resource that has usefully been applied in more\ndeveloped sciences to similar “cross-level” concerns are\ndynamical systems. Global phenomena, such as large-scale\nmeteorological patterns, have been usefully addressed as dynamical,\nnonlinear, and often chaotic interactions between lower-level physical\nphenomena. Addressing the interlocking levels of theory and\nexplanation in the mind/brain using computational resources that have\nworked to bridge levels in more mature sciences might yield comparable\nresults. This methodology is necessarily interdisciplinary, drawing on\nresources and researchers from a variety of levels, including higher\nones like experimental psychology, artificial intelligence, and\nphilosophy of science. \nThe use of computational methods in neuroscience itself is not new.\nHodgkin, Huxley, and Katz (1952) incorporated values of\nvoltage-dependent sodium and potassium conductance they had measured\nexperimentally in the squid giant axon into an equation from physics\ndescribing the time evolution of a first-order kinetic process. This\nequation enabled them to calculate best-fit curves for modeled\nconductance versus time data that reproduced the changing membrane\npotential over time when action potentials were generated. Also using\nequations borrowed from physics, Rall (1959) developed the cable model\nof dendrites. This model provided an account of how the various inputs\nfrom across the dendritic tree interact temporally and spatially to\ndetermine the input-output properties of single neurons. It remains\ninfluential today, and was incorporated into the GENESIS software\nfor programming neurally realistic networks (Bower and Beeman 1995;\nsee discussion in\n section 2 above).\n David Sparks and his colleagues showed that a vector-averaging model\nof activity in neurons of superior colliculi correctly predicts\nexperimental results about the amplitude and direction of saccadic eye\nmovements (Lee, Rohrer, and Sparks 1988). Working with a more\nsophisticated mathematical model, Apostolos Georgopoulos and his\ncolleagues predicted direction and amplitude of hand and arm movements\nbased on averaged activity of 224 cells in motor cortex. Their\npredictions were borne out under a variety of experimental tests\n(Georgopoulos, Schwartz, and Kettner 1986). We mention these\nparticular studies only because these are ones with which we are\nfamiliar. No doubt we could multiply examples of the fruitful\ninteraction of computational and experimental methods in neuroscience\neasily by one-hundred-fold. Many of these extend back before\n“computational neuroscience” was a recognized research\nendeavor. \nWe’ve already seen one example, the vector transformation\naccount of neural representation and computation, once under active\ndevelopment in cognitive neuroscience (see\n section 2 above).\n Other approaches using “cognitivist” resources were, and\ncontinue to be,\n pursued.[16]\n Some of these projects draw upon “cognitivist”\ncharacterizations of the phenomena to be explained. Some exploit\n“cognitivist” experimental techniques and methodologies.\nSome even attempt to derive “cognitivist” explanations\nfrom cell-biological processes (e.g., Hawkins and Kandel 1984). As\nStephen Kosslyn (1997) put it, cognitive neuroscientists employ the\n“information processing” view of the mind characteristic\nof cognitivism without trying to separate it from theories of brain\nmechanisms. Such an endeavor calls for an interdisciplinary community\nwilling to communicate the relevant portions of the mountain of detail\ngathered in individual disciplines with interested nonspecialists.\nThis requires more than people willing to confer with others working\nat related levels, but also researchers trained explicitly in the\nmethods and factual details of a variety of disciplines. This is a\ndaunting need, but it offers hope to philosophers wishing to\ncontribute to actual neuroscience. Thinkers trained in both the\n“synoptic vision” afforded by philosophy, and the\nscientific and experimental basis of a genuine (graduate-level)\nscience would be ideally equipped for this task. Recognition of this\npotential niche was slow to dawn on graduate programs in philosophy,\nbut a few programs have taken steps to fill it (see, e.g.,\n Other Internet Resources\n below). \nHowever, one glaring shortcoming remains. Given philosophers’\ntraining and interests, “higher-level”\nneurosciences—networks, cognitive, systems, and the fields of\ncomputational neuroscience which ally with these—tend to attract\nthe most philosophical attention. As natural as this focus might be,\nit can lead philosophers to a misleading picture of neuroscience.\nNeurobiology remains focused on cellular and molecular mechanisms of\nneuronal activity, and allies with the kind of behavioral neuroscience\nthat works with animal models. This is still how a majority of members\nof the Society for Neuroscience, now more than 37,000 members strong,\nclassify their own research; this is where the majority of grant money\nfor research goes; and these are the areas whose experimental\npublications most often appear in the most highly cited scientific\njournals. (The link to the Society for Neuroscience’s web site\nin\n Other Internet Resources below\n leads to a wealth of data on these numbers; see especially the\nPublications section.) Yet philosophers have tended not to pay much\nattention to cellular and molecular neuroscience. Fortunately this\nseems to be changing, as we will document in sections 7 and 8 below.\nStill, the preponderant attention philosophers pay to\ncognitive/systems/computational neuroscience obscures the wetlab\nexperiment-driven focus of ongoing neurobiology. \nThe distinction between “philosophy of neuroscience” and\n“neurophilosophy” came to be better clarified over the\nfirst decade of the twenty-first century, due primarily to more\nquestions being pursued in both areas. Philosophy of neuroscience\nstill tends to pose traditional questions from philosophy of science\nspecifically about neuroscience. Such questions include: What is the\nnature of neuroscientific explanation? And, what is the nature of\ndiscovery in neuroscience? Answers to these questions are pursued\neither descriptively (how does neuroscience proceed?) or normatively\n(how should neuroscience proceed)? Some normative projects in\nphilosophy of neuroscience are “deconstructive”,\ncriticizing claims about the topic made by neuroscientists. For\nexample, philosophers of neuroscience have criticized the conception\nof personhood assumed by researchers in cognitive neuroscience (cf.\nRoskies 2009). Other normative projects are constructive, proposing\nnew theories of neuronal phenomena or methods for interpreting\nneuroscientific data. Such projects often integrate smoothly with\ntheoretical neuroscience itself. For example, Chris Eliasmith and\nCharles Anderson developed an approach to constructing\nneurocomputational models in their book Neural Engineering\n(2003). In separate publications, Eliasmith has argued that the\nframework introduced in Neural Engineering provides both a\nnormative account of neural representation and a framework for\nunifying explanation in neuroscience (e.g., Eliasmith 2009). \nNeurophilosophy continued to apply findings from the neurosciences to\ntraditional, philosophical questions. Examples include: What is an\nemotion? (Prinz 2004) What is the nature of desire? (Schroeder 2004)\nHow is social cognition made possible? (Goldman 2006) What is the neural basis of\nmoral cognition? (Prinz 2007) What is the neural basis of happiness?\n(Flanagan 2009) Neurophilosophical answers to these questions are\nconstrained by what neuroscience reveals about nervous systems. For\nexample, in his book Three Faces of Desire, Timothy Schroeder\n(2004) argued that our commonsense conception of desire attributes to\nit three capacities: (1) the capacity to reinforce behavior when\nsatisfied, (2) the capacity to motivate behavior, and (3) the capacity\nto determine sources of pleasure. Based on evidence from the\nliterature on dopamine function and reinforcement learning theory,\nSchroeder argued that reward processing is the basis for all three\ncapacities. Thus, reward is the essence of desire. \nDuring the first decade of the twenty-first century a trend arose in\nneurophilosophy to look toward neuroscience for guidance in moral\nphilosophy. That should be evident from the themes we’ve just\nmentioned. Simultaneously, there was renewed interest in moralizing\nabout neuroscience and neurological treatments (see Levy 2007; Roskies\n2009). This new field, neuroethics, thus combined both\ninterest in the relevance of neuroscience data for understanding moral\ncognition, and the relevance of moral philosophy for acquiring and\nregulating the application of knowledge from neuroscience. The\nregulatory branch of neuroethics initially focused explicitly on the\nethics of treatment for people who suffer from neurological\nimpairments, the ethics of attempts to enhance human cognitive\nperformance (Schneider 2009), the ethics of applying “mind\nreading” technology to problems in forensic science (Farah and\nWolpe 2004), and the ethics of animal experimentation in neuroscience\n(Farah 2008). More recently both of these fields of neuroethics has\nseen tremendous growth. The interested reader should consult the\n neuroethics entry\n in this Encyclopedia. \nTrends during the first decade of the twenty-first century in\nphilosophy of neuroscience included renewed interest in the nature of\nmechanistic explanations. This was in keeping with a general trend in\nphilosophy of science (e.g., Machamer, Darden, and Craver 2000). The\napplication of this general approach to neuroscience isn’t\nsurprising. “Mechanism” is a widely-used term among\nneuroscientists. In his book, Explaining the Brain (2007),\nCarl Craver contended that mechanistic explanations in neuroscience\nare causal explanations, and typically multi-level. For example, the\nexplanation of the neuronal action potential involves the action\npotential itself, the cell in which it occurs, electro-chemical\ngradients, and the proteins through which ions flow across the\nmembrane. Thus we have a composite entity (a cell) causally\ninteracting with neurotransmitters at its receptors. Parts of the cell\nengage in various activities, e.g., the opening and closing of\nligand-gated and voltage-gated ion channels, to produce a pattern of\nchanges, the depolarizing current constituting the action potential. A\nmechanistic explanation of the action potential thus countenances\nentities at the cellular, molecular, and atomic levels, all of which\nare causally relevant to producing the action potential. This causal\nrelevance can be confirmed by altering any one of these variables,\ne.g., the density of ion channels in the cell membrane, to generate\nalterations in the action potential; and by verifying the consistency\nof the purported invariance between the variables. For challenges to\nCraver’s account of mechanistic explanation in neuroscience,\nspecifically concerning the action potential, see Weber 2008, and\nBogen 2005. \nAccording to epistemic norms shared implicitly by neuroscientists,\ngood explanations in neuroscience are good mechanistic explanations;\nand good mechanistic explanations are those that pick out invariant\nrelationships between mechanisms and the phenomena they control. (For\nfuller treatment of invariance in causal explanations throughout\nscience, see James Woodward 2003. Mechanists draw extensively on\nWoodward’s “interventionist” account of cause and\ncausal explanations.) Craver’s account raised questions about\nthe place of reduction in neuroscience. John Bickle (2003) suggested\nthat the working concept of reduction in the neurosciences consists of\nthe discovery of systematic relationships between interventions at\nlower levels of biological organization, as these are pursued in\ncellular and molecular neuroscience, and higher level behavioral\neffects, as they are described in psychology. Bickle called this\nperspective “reductionism-in-practice” to contrast it with\nthe concepts of intertheoretic or metaphysical reduction that have\nbeen the focus of many debates in the philosophy of science and\nphilosophy of mind. Despite Bickle’s reformulation of reduction,\nhowever, mechanists generally resist, or at least relativize, the\n“reductionist” label. Craver (2007) calls his view the\n“mosaic unity” of neuroscience. Bechtel (2009) calls his\n“mechanistic reduction(ism)”. Both Craver and Bechtel\nadvocate multi-leveled “mechanisms-within-mechanisms”,\nwith no level of mechanism epistemically privileged. This is in\ncontrast to reduction(ism), ruthless or otherwise which privileges\nlower levels. Still we can ask: Is mechanism a kind of\nreductionism-in-practice? Or does mechanism, as a position on\nneuroscientific explanation, assume some type of autonomy for\npsychology? If it assumes autonomy, reductionists might challenge\nmechanists on this assumption. On the other hand, Bickle’s\nreductionism-in-practice clearly departs from inter-theoretic\nreduction, as the latter is understood in philosophy of science. As\nBickle himself acknowledges, his latest reductionism was inspired\nheavily by mechanists’ criticisms of his earlier “new\nwave” account. Mechanists can challenge Bickle that his\ndeparture from the traditional accounts has also led to a departure\nfrom the interests that motivated those accounts. (See Polger 2004 for\na related challenge.) As we will see in\n section 8 below,\n these issues surrounding mechanistic philosophy of neuroscience have\ngrown more urgent, as mechanism has grown to dominate the field. \nThe role of temporal representation in conscious experience and the\nkinds of neural architectures sufficient to represent objects in time\ngenerated interest. In the tradition of Husserl’s phenomenology,\nDan Lloyd (2002, 2003) and Rick Grush (2001, 2009) have separately\ndrawn attention to the tripartite temporal structure of phenomenal\nconsciousness as an explanandum for neuroscience. This structure\nconsists of a subjective present, an immediate past, and an\nexpectation of the immediate future. For example, one’s\nconscious awareness of a tune is not just of a time-slice of\ntune-impression, but of a note that a moment ago was present, another\nthat is now present, and an expectation of subsequent notes in the\nimmediate future. As this experience continues, what was a moment ago\ntemporally immediate is now retained as a moment in the immediate\npast; what was expected either occurred or didn’t in what has\nnow become the experienced present; and a new expectation has formed\nof what will come. One’s experience is not static, even though\nthe experience is of a single object (the tune). These earlier works\nfound increased relevance with the rise of “predictive\ncoding” models of whole brain function, developed by\nneuroscientists including Karl Friston (2009) less than a decade\nlater, and brought to broader philosophical attention by Jakob Hohwy\n(2013) and Andy Clark (2016).  \nAccording to Lloyd, the tripartite structure of consciousness raises a\nunique problem for analyzing fMRI data and designing experiments. The\nproblem stems from the tension between the sameness in the object of\nexperience (e.g., the same tune through its progression) and the\ntemporal fluidity of experience itself (e.g., the transitions between\nheard notes). At the time Lloyd was writing, one standard means of\nanalyzing fMRI data consisted in averaging several data sets and\nsubtracting an estimate of baseline activation from the composites.\n [17]\n This is done to filter noise from the task-related hemodynamic\nresponse. But as Lloyd points out, this then-common practice ignores\nmuch of the data necessary for studying the neural correlates of\nconsciousness. It produces static images that neglect the\nrelationships between data points over the time course. Lloyd instead\napplies a multivariate approach to studying fMRI data, under the\nassumption that a recurrent network architecture underlies the\ntemporal processing that gives rise to experienced time. A simple\nrecurrent network has an input layer, an output layer, a hidden layer,\nand an additional layer that copies the prior activation state of\neither the hidden layer or the output layer. Allowing the output layer\nto represent a predicted outcome, the input layer can then represent a\ncurrent state and the additional layer a prior state. This assignment\nmimics the tripartite temporal structure of experience in a network\narchitecture. If the neuronal mechanisms underlying conscious\nexperience are approximated by recurrent network architecture, one\nprediction is that current neuronal states carry information about\nimmediate future and prior states. Applied to fMRI, the model predicts\nthat time points in an image series will carry information about prior\nand subsequent time points. The results of Lloyd’s (2002)\nanalysis of 21 subjects’ data sets, sampled from the publicly\naccessible National fMRI Data Center, support this prediction. \nGrush’s (2001, 2004) interest in temporal representation is part\nof his broader systematic project of addressing a semantic problem for\ncomputational neuroscience, namely: how do we demarcate study of the\nbrain as an information processor from the study of any other complex\ncausal process? This question leads back into the familiar territory\nof psychosemantics (see\n section 3 above),\n but now the starting point is internal to the practices of\ncomputational neuroscience. The semantic problem is thereby rendered\nan issue in philosophy of neuroscience, insofar as it asks: what does\n(or should) “computation” mean in computational\nneuroscience? \nGrush’s solution drew on concepts from modern control theory. In\naddition to a controller, a sensor, and a goal state, certain kinds of\ncontrol systems employ a process model of the actual process\nbeing controlled. A process model can facilitate a variety of\nengineering functions, including overcoming delays in feedback and\nfiltering noise. The accuracy of a process model can be assessed\nrelative to its “plug-compatibility” with the actual\nprocess. Plug-compatibility is a measure of the degree to which a\ncontroller can causally couple to a process model to produce the same\nresults it would produce by coupling with the actual process. Note\nthat plug-compatibility is not an information relation. \nTo illustrate a potential neuroscientific implementation, Grush\nconsiders a controller as some portion of the brain’s motor\nsystems (e.g., premotor cortex). The sensors are the sense organs\n(e.g., stretch receptors on the muscles). A process model of the\nmusculoskeletal system might exist in the cerebellum (see Kawato\n1999). If the controller portion of the motor system sends spike\ntrains to the cerebellum in the same way that it sends spikes to the\nmusculoskeletal system, and if in return the cerebellum receives spike\ntrains similar to real peripheral feedback, then the cerebellum\nemulates the musculoskeletal system (to the degree that the mock\nfeedback resembles real peripheral feedback). The proposed unit over\nwhich computational operations range is the neuronal realization of a\nprocess model and its components, or in Grush’s terms an\n“emulator” and its “articulants”. \nThe details of Grush’s framework are too sophisticated to\npresent in short compass. (For example, he introduces a host of\nconceptual devices to discuss the representation of external objects.)\nBut in a nutshell, he contends that understanding temporal\nrepresentation begins with understanding the emulation of the timing\nof sensorimotor contingencies. Successful sequential behavior (e.g.,\nspearing a fish) depends not just on keeping track of where one is in\nspace, but where one is in a temporal order of movements and the\ntemporal distance between the current, prior, and subsequent\nmovements. Executing a subsequent movement can depend on keeping track\nof whether a prior movement was successful and whether the current\nmovement is matching previous expectations. Grush posits\nemulators—process models in the central nervous\nsystem—that anticipate, retain, and update mock sensorimotor\nfeedback by timing their output proportionally to feedback from an\nactual process (Grush 2005). \nLloyd’s and Grush’s approaches to studying temporal\nrepresentation are varied in their emphases. But they are unified in\ntheir implicit commitment to localizing cognitive functions and\ndecomposing them into subfunctions using both top-down and bottom-up\nconstraints. (See Bechtel and Richardson 1993 for more details on this\ngeneral explanatory strategy.) As we mentioned a few paragraphs above,\nboth anticipated in important and interesting ways more recent\nneuroscientific and philosophical work on predictive coding and the\nbrain. Both developed mechanistic explanations that pay little regard\nto disciplinary boundaries. One of the principal lessons of\nBickle’s and Craver’s work is that neuroscientific\npractice in general is structured in this fashion. The ontological\nconsequences of adopting this approach continue to be debated. \nMechanism, first introduced in section 7 above, came to dominate the\nphilosophy of neuroscience throughout the second decade of the\ntwenty-first century. One much-discussed example is Gualtiero\nPiccinini and Carl Craver (2011). The authors employ two popular\nmechanistic notions. Their first is the multi-level, nested\nhierarchies of mechanisms-within-mechanisms perspective, discussed in\nsection 7 above, that traces back to Craver and Darden (2001). Their\nsecond is that of “mechanism sketch”, suggested initially\nin Machamer, Darden, and Craver (2000) and developed in detail in\nCraver (2007). Piccinini and Craver’s goal is to\n“seamlessly” situate psychology as part of an\n“integrated framework” alongside neuroscience. They\ninterpret psychology’s familiar functional analyses of cognitive\ncapacities as relatively incomplete mechanism-sketches, which leave\nout many components of the mechanisms that ultimately will fully\nexplain the system’s behavior. Neuroscience in turn fills in\nthese missing components, dynamics, and organizations, at least ones\nfound in nervous systems. This filling-in thereby turns\npsychology’s mechanism-sketches into full-blown mechanistic\nexplanations. So even though psychology proceeds via functional\nanalyses, so interpreted it is nonetheless mechanistic. Piccinini and\nCraver realize that their “integrated” account clashes\nwith classical “autonomy” claims for psychology vis-à-vis\nneuroscience. Nevertheless, they insist that their challenge to\nclassical “autonomy” does not commit them to\n“reductionism”, in either its classical or more recent\nvarieties. Their commitment to nested hierarchy of\nmechanisms-within-mechanisms to account for a system’s behavior\nacknowledges the importance of mechanisms and intralevel causation at\nall levels constituting the system, not just at lower (i.e., cellular,\nmolecular) levels. \nDavid Kaplan and Craver (2011) focus the mechanist perspective\ncritically on dynamical systems mathematical models popular in recent\nsystems and computational neuroscience. They argue that such models\nare explanatory only if there exists a “plausible mapping”\nbetween elements in the model and elements in the modeled system. At\nbottom is their Model-to-Mechanism-Mapping (3M) Constraint on\nexplanation. The variables in a genuinely explanatory model correspond\nto components, activities, or organizational features of the system\nbeing explained. And the dependencies posited among variables in the\nmodel, typically expressed mathematically in systems and computational\nneuroscience, correspond to causal relations among the system’s\ncomponents. Kaplan and Craver justify the 3M Constraint on grounds of\nexplanatory norms, common to both science and common sense. All other\nthings being equal, they insist, explanations that provide more\nrelevant details about a system’s components, activities, and\norganization, more likely will answer more questions about how the\nsystem will behave in a variety of circumstances, than will an\nexplanation that provides fewer (mechanistic) details.\n“Relevant” here pertains to the functioning of the\nspecific mechanism. Models from systems and computational neuroscience\nthat violate the 3M Constraint are thus more reasonably thought of as\nmathematical descriptions of phenomena, not explanations of some\n“non-mechanistic” variety. \nKaplan and Craver challenge their own view with one of the more\npopular dynamical/mathematical models in all of computational\nneuroscience, the Haken-Kelso-Bunz (1985) model of human bimanual\nfinger-movement coordination. They point to passages in these\nmodelers’ publications that suggest that the modelers only\nintended for their dynamical systems model to be a mathematically\ncompact description of the temporal evolution of a “purely\nbehavioral dependent variable”. The modelers interpreted none of\nthe model’s variables or parameters as mapping onto components\nor operations of any hypothetical mechanism generating the behavioral\ndata. Nor did they intend for any of the model’s mathematical\nrelations or dependencies among variables to map onto hypothesized\ncausal interactions among components or activities of any mechanism.\nAs Kaplan and Craver further point out, after publishing their\ndynamicist model, these modelers themselves then began to investigate\nhow the behavioral regularities their model described might be\nproduced by neural motor system components, activities, and\norganization. Their own follow-up research suggests that these\nmodelers saw their dynamicist model as a heuristic, to help\nneuroscientists move toward “how-possibly”, and ultimately\nto a “how-actually” mechanistic explanation.  \nAt bottom, Kaplan and Craver’s 3M constraint on explanation\npresents a dilemma for dynamicists. To the extent that dynamical\nsystems modelers intend to model hypothesized neural mechanisms for\nthe phenomenon under investigation, their explanations will need to\ncohere with the 3M Constraint (and other canons of mechanistic\nexplanation). To the extent that this is not a goal of dynamicist\nmodelers, their models do not seem genuinely explanatory, at least not\nin one sense of “explanation” prominent in the history of\nscience. Furthermore, when dynamicist models are judged to be\nsuccessful, they often prompt subsequent searches for underlying\nmechanisms, just as the 3M Constraint and the general mechanist\naccount of the move from “how-possibly” to “how\nactually” mechanisms recommends. Either horn gores dynamicists\nwho claim that their models constitute a necessary additional kind of\nexplanation in neuroscience to mechanistic explanation, beyond any\nheuristic value such models might offer toward discovering\nmechanisms. \nKaplan and Craver’s radical conclusion, that dynamicist\n“explanations” are genuine explanations only to the degree\nthat they respect the (mechanist’s) 3M Constraint, needs more\ndefense. The burden of proof always lies on those whose conclusions\nstrike at popular assumptions. More than the discussion of a couple of\nlandmark dynamicist models in neuroscience is needed (in their 2011,\nKaplan and Craver also discuss the difference-of-Gaussians model of\nreceptive field properties of mammalian visual neurons). Expectedly,\ndynamicists have taken up this challenge. Michael Silberstein and\nAnthony Chemero (2013), for example, argue that localization and\ndecomposition strategies characterize mechanistic explanation, and\nthat some explanations in systems neuroscience violate one of these\nassumptions, or both. Such violations in turn create a dilemma for\nmechanists. Either they must “stretch” their account of\nexplanation, beyond decomposition and localization, to capture these\nrecalcitrant cases, or they must accept “counterexamples”\nto the generality of mechanistic explanation, in both systems\nneuroscience and systems biology more generally. \nLauren Ross (2015) and Mazviita Chirimuuta (2014) independently appeal\nto Robert Batterman’s account of minimal model explanation as an\nimportant kind of non-mechanistic explanation in neuroscience. Minimal\nmodels were developed initially to characterize a kind of explanation\nin the physical sciences (see, e.g., Batterman and Rice 2014).\nBatterman’s account distinguishes between two different kinds of\nscientific “why-questions”: why a phenomenon manifests in\nparticular circumstances; and why a phenomenon manifests generally, or\nin a number of different circumstances. Mechanistic explanations\nanswer the first type of why-question. Here a “more details the\nbetter” (MDB) assumption (Chirimuuta 2014), akin to Kaplan and\nCraver’s “all things being equal” assumption about\nbetter explanations (mentioned above), holds force. Minimal models,\nhowever, which minimize over the presented implantation details and\nhence violate MDB, are better able to answer the second type of\nscientific why-questions. Ross (2015), quoting from computational\nneuroscientists Rinzel and Ermentrout, insists that models containing\nmore details than necessary can obscure identification of critical\nelements by leaving too many open possibilities, especially when one\nis trying to answer Batterman’s second kind of why-questions\nabout a system’s behavior. \nChirimuuta and Ross each appeal to related resources from\ncomputational neuroscience to illustrate the applicability of\nBatterman’s minimal model explanation strategy. Ross appeals to\n“canonical models”, which represent “shared\nqualitative features of a number of distinct neural systems”\n(2015: 39). Her central example is the derivation of the\nErmentrout-Kopell model of class I neuron excitability, which uses\n“mathematical abstraction techniques” to “reduce\nmodels of molecularly distinct neural systems to a single …\ncanonical model”. Such a model “explains why molecularly\ndiverse neural systems all exhibit the same qualitative\nbehavior”, (2015: 41) clearly a Batterman second-type\nwhy-question. Chirimuuta’s resource is “canonical neural\ncomputations” (CNCs):  \ncomputational modules that apply the same fundamental operations in a\nvariety of contexts … a toolbox of computational operations\nthat the brain applies in a number of different sense modalities and\nanatomic regions and which can be described at higher levels of\nabstraction from their biophysical implementation. (Chirimuuta 2014:\n138)  \nExamples include shunting inhibition, linear filtering, recurrent\namplification, and thresholding. Rather than being mechanism-sketches,\nawaiting further mechanistic details to be turned into full-blown\nhow-actually mechanisms, CNCs are invoked in a different explanatory\ncontext, namely, ones posing Batterman’s second type of\nwhy-questions. Ross concurs concerning canonical models:  \nUnderstanding the approach dynamical systems neuroscientists take in\nexplaining [system] behavior requires attending to their explanandum\nof interest and the unique modeling tools [e.g., canonical models]\ncommon in their field. (2015: 52)  \nIn short, both Chirimuuta’s and Ross’s replies to Kaplan\nand Craver’s challenge is a common one in philosophy: save a\nparticular form of explanation from collapsing into another by\nsplitting the explanandum. \nFinally, to wrap up this discussion of mechanism ascendant, an\nanalogue of Craver’s (2007) problem of accounting for\n“constitutive mechanistic relevance”, that is, for\ndetermining which active components of a system are actually part of\nthe mechanism for a given system phenomenon, has also re-emerged in\nrecent discussions. Robert Rupert (2009) suggests that\n“integration” is a key criterion for determining which set\nof causally contributing mechanisms constitute the system for a task,\nbased on the relative frequency with which sets of mechanisms\nco-contribute to causing task occurrences. He cashes frequency of\nco-contribution as the probability of the set for causing the\ncognitive task, conditional to every other co-occurring causal set.\nFelipe De Brigard (2017) challenges Rupert’s criterion, arguing\nthat it cannot account for cognitive systems displaying two features,\n“diachronic dynamicity” along with “functional\nstability”. The frequency with which a given mechanism causally\ncontributes to the same cognitive task (functional stability) can\nchange over time (diachronic dynamicity). Although De Brigard\nemphasizes the critical importance of these features for\nRupert’s integration criterion via a fanciful thought\nexperiment, he also argues that they are a widespread phenomenon in\nhuman brains. Both features are found, for example, in evidence\npertaining to the “Hemispheric Asymmetry Reduction in Older\nAdults”, in which tasks that recruit hemispherically localized\nregions of prefrontal cortex in younger adults show a reduction in\nhemispheric asymmetry in older adults. And both are found in\n“Posterior-Anterior Shift with Aging”, where a task\nincreases activity in anterior brain regions while decreasing activity\nin posterior regions in older adults, relative to activity invoked by\nthe same task in younger adults. \nTo replace Rupert’s notion of integration as a criterion for\ndetermining which sets of mechanisms constitute a cognitive system, De\nBrigard points to two promising recent developments in network\nneuroscience which potentially allow for parametrized time.\n“Scaled inclusivity” is a method for examining each node\nin a network and identifying its membership in “community\nstructures” across different iterations of the network.\n“Temporal-dynamic network analyses” is a way to quantify\nchanges in community structures or modules between networks at\ndifferent time points. Both methods thereby identify “modular\nalliances”, which convey both co-activation and dynamic change\ninformation in a single model. De Brigard suggests that these are thus\nthe candidates with which cognitive systems could be identified. \nClearly, much remains to be discussed regarding the impact mechanism\nhas come to wield in philosophy of neuroscience over the last decade.\nBut while mechanism has become the most dominant general perspective\nin the field, work in other areas continues. Michael Anderson defends\nthe relevance of cognitive neuroscience for determining\npsychology’s taxonomy, independent of any commitment to\nmechanism. The most detailed development of his approach is in his\n(2014) book, After Phrenology, based on his influential\n“neural reuse” hypothesis. Each region of the brain, as\nrecognized by the standard techniques of cognitive neuroscience\n(especially fMRI), engages in cognitive functions that are highly\nvarious, and form different “neural partnerships” with one\nanother under different circumstances. Psychological categories are\nthen to be reconceived along lines suggested by the wide-ranging\nempirical data in support of neural reuse. A genuine\n“post-phrenological” science of the mind must jettison the\nassumption that each brain region performs its own fundamental\ncomputation. In this fashion Anderson’s work explicitly\ncontinues philosophy of neuroscience’s ongoing interest in\nlocalizations of cognitive functions. \nIn shorter compass, Anderson (2015) investigates the relevance of\ncognitive neuroscience for reconceiving psychology’s basic\ncategories, starting from a consequence of his neural reuse\nhypothesis. Attempts to map cognitive processes onto specific neural\nprocesses and brain regions reveal “many-to-many”\nrelations. Not only do these relations show that combined\nanatomical-functional labels for brain regions (e.g., “fusiform\nface area”) are deceptive; they also call into question the\npossibility of deciding between alternative psychological taxonomies\nby appealing to cognitive neuroscientific data. \nFor all but the strongest proponents of psychology’s autonomy\nfrom neuroscience, these many-to-many mappings will suggest that the\npsychological taxonomy we bring to this mapping project needs\nrevision. One need not be committed to any strong sense of\npsychoneural reduction, or the epistemological superiority of\ncognitive neuroscience to psychology, to draw this conclusion. The\nmere relevance of cognitive neuroscience for psychology’s\ncategories is enough. This debate is thus “about the\nrequirements for a unified science of the mind, and the proper role of\nneurobiological evidence in the construction of such an\nontology” (2015: 70), not about the legitimacy of either. \nAnderson divides revisionary projects for psychology into three kinds,\nbased on the degree of revision each kind recommends for psychology,\nand the extent of one-to-one function-to-structure mappings the\nproposed revisions predicts will be available.\n“Conservatives” foresee little need for extensive\nrevisions of psychology’s basic taxonomy, even as more\nneuroscientific evidence is taken into account than current standard\npractices pursue. “Moderates” insist that our knowledge of\nbrain function “can (and should) act as one arbiter of the\npsychologically real” (2015: 70), principally by\n“splitting” or “merging” psychological\nconcepts that currently are in use. “Radicals” project\neven more drastic revisions, even to the most primitive concepts of\npsychology, and even after such revisions they still do not expect\nthat many one-to-one mappings between brain regions and the new\npsychological primitives will be found. Although Anderson does not\nstress this connection (eliminative materialism has not been a\nprominent concern in philosophy of mind or neuroscience for two\ndecades), readers will notice similar themes discussed in\n section 2\n above, only now with scientific, not folk psychology the target of\nthe radical revisionists. A key criterion for any satisfactory\nreformulation of a cognitive ontology is the degree to which it\nsupports two kinds of inferences: “forward inferences”,\nfrom the engagement of a specific cognitive function to the prediction\nof brain activity; and “reverse inferences”, from the\nobservation that a specific brain region or pattern occurs to the\nprediction that a specific cognitive operation is engaged. In light of\nthis explicit criterion, Anderson usefully surveys the work of a\nnumber of prominent psychologists and cognitive neuroscientists in\neach of his revisionist groups. Given his broader commitment to neural\nreuse, and the trek it invites into “evolutionarily-inspired,\necological, and enactive terms”, Anderson’s own sentiments\nlie with the “radicals”:  \nlanguage and mathematics, for instance, are best understood as\nextensions of our basic affordance processing capacities augmented\nwith public symbol systems … The psychological science that\nresults from this reappraisal may well look very different from the\none we practice today. (2015: 75) \nLandmark neuroscientific hypotheses remain a popular focus in recent\nphilosophy of neuroscience. Berit Brogaard (2012), for example, argues\nfor a reinterpretation of the standard “dissociation”\nunderstanding of Melvin Goodale and David Milner’s (1992)\ncelebrated “two visual processing streams”, a landmark,\nnow “textbook” result from late-twentieth century\nneuroscience. Two components of the standard dissociation are key. The\nfirst is that distinct brain regions compute information relevant for\nvisually guided “on-the-fly” actions, and for object\nrecognition, respectively, the dorsal stream (which runs from primary\nvisual cortex through the medial temporal region into the superior and\ninferior parietal lobules) and the ventral stream (which runs from\nprimary visual cortex through V4 and into inferior temporal cortex).\nAnd second, that only information relevant for visual object\nrecognition, processed in the ventral stream, contributes to the\ncharacter of conscious visual experiences. \nBrogaard’s concern is that this standard understanding\nchallenges psychofunctionalism, our currently most plausible\n“naturalistic” account of mental states.\nPsychofunctionalism draws its account of mind directly from our best\ncognitive psychology. If φ is some mental state type that has\ninherited the content of a visual experience, then according to\ncognitive psychology a wide range of visually guided beliefs and\ndesires, different kinds of visual memories, and so on, satisfy\nφ’s description. But by the standard\n“dissociation” account of Goodale and Milner’s two\nvisual streams, only dorsal-stream states, and not ventral-stream\nstates, represent truly egocentric visual properties, namely\n“relational properties which objects instantiate from the point\nof view of believers or perceivers”, (Brogaard 2012: 572). But\naccording to cognitive psychology, dorsal-stream states do not play\nthis wide-ranging φ-role. So according to psychofunctionalism\n“φ-mental states cannot represent egocentric\nproperties” (2012: 572). But it seems “enormously\nplausible” that some of our perceptual beliefs and visual\nmemories represent egocentric properties. So either we reject\npsychofunctionalism, and so our most plausible naturalization project\nfor determining whether a given mental state is instantiated, or we\nreject the standard dissociation interpretation of Goodale and\nMilner’s two visual streams hypothesis, despite the wealth of\nempirical evidence supporting it. Neither horn of this dilemma looks\ncomfortably graspable, although the first horn might be thought to be\nmore so, since psychofunctionalism as a general theory of mind lacks\nthe kind of strong empirical backing that the standard interpretation\nof Goodale and Milner’s hypothesis enjoys. \nNevertheless, Brogaard recommends retaining psychofunctionalism, and\ninstead rejecting “a particular formulation” of Goodale\nand Milner’s two visual stream hypothesis. The interpretation to\nreject insists that “dorsal-stream information cannot contribute\nto the potentially conscious representations computed by the ventral\nstream” (2012: 586–587). Egocentric representations of\nvisual information computed by the dorsal stream contribute to\nconscious visual stream representations “via feedback\nconnections” from dorsal- to ventral-stream neurons (2012: 586).\nThis isn’t to deny dissociation:  \nInformation about the egocentric properties of objects is processed by\nthe dorsal stream, and information about allocentric properties of\nobjects is processed by the ventral stream. (2012: 586)  \nBut this dissociation hypothesis “has no bearing on what\ninformation is passed on to parts of the brain that process\ninformation which correlated with visual awareness” (2012: 586).\nWith this re-interpretation, psychofunctionalism is rendered\nconsistent with Goodale and Milner’s two stream, dorsal and\nventral, “what” and “where/how” hypothesis and\nthe wealth of empirical evidence that supports it. According to\nBrogaard, psychofunctionalism can thereby “correctly treat\nperceptual and cognitive states that carry information processed in\nthe ventral visual stream as capable of representing egocentric\nproperties” (2012: 586). \nDespite philosophy of neuroscience’s continuing focus on\ncognitive/systems/computational-neuroscience (see the discussion in\n section 7 above),\n interest in neurobiology’s cellular/molecular mainstream\nappears to be increasing. One notable paper is Ann-Sophie Barwich and\nKarim Bschir’s (2017) historical-cum-philosophical study of\nG-protein coupled receptors (GPCRs). Work on the structure and\nfunctional significance of these proteins has dominated molecular\nneuroscience for the past forty years; their role in the mechanisms of\na variety of cognitive functions is now empirically documented beyond\nquestion. And yet one finds little interest in, or even notice of this\nshift in mainstream neuroscience among philosophers. Barwich and\nBschir’s yeoman history research on the discovery and\ndevelopment of these objects pays off philosophically. The role of\nmanipulability as a criterion for entity realism in the\nscience-in-practice of wet-lab research becomes meaningful “only\nonce scientists have decided how to conceptually coordinate measurable\neffects distinctly to a scientific object” (2017: 1317).\nScientific objects like GPCRs get assigned varying degrees of reality\nthroughout different stages of the discovery process. Such an\nobject’s role in evaluating the reality of “neighboring\nelements of enquiry” becomes a part of the criteria of its\nreality as well. \nThe impact of science-in-practice on philosophy of science generally\nhas been felt acutely in the philosophy of neuroscience, most notably\nin increased philosophical interest in neuroscientific\nexperimentation. In itself this should not surprise. Neuroscience\nrelies heavily on laboratory experimentation, especially within its\ncellular and molecular, “Society for Neuroscience”\nmainstream. So the call to understand experiment should beckon any\nphilosopher who ventures into neuroscience’s cellular/molecular\nfoundations. Two papers by Jacqueline Sullivan (2009, 2010) have been\nimportant in this new emphasis. In her (2009) Sullivan acknowledges\nboth Bickle’s (2003) and Craver’s (2007) focus on cellular\nand molecular mechanisms of long-term potentiation, and\nexperience-driven form of synaptic plasticity. But she insists that\nbroader philosophical commitments, which lead Bickle to ruthless\nreductionist and Craver to mosaic unity “global” accounts,\nobscure important aspects of real laboratory neuroscience practice.\nShe emphasizes the role of “subprotocols”, which specify\nhow data are to be gathered, in her model of “the experimental\nprocess”, and illustrates these notions with a number of\nexamples. Her analysis reveals an important underappreciated tension\namong a pair of widely-accepted experiment norms. Pursuing\n“reliability” drives experimenters more deeply into\nextensive laboratory controls. Pursuing “external\nvalidity” drives them toward enriched experimental environments\nthat more closely represent the messy natural environment beyond the\nlaboratory. These two norms commonly conflict: in order to get more of\none, scientists introduce conditions that give them less of the other.\n \nIn her (2010) Sullivan offers a detailed history of the Morris water\nmaze task, tracing her account back to Morris’s original\npublications. Philosophers of neuroscience have uncritically assumed\nthat the water maze is a widely-accepted neuroscience protocol for\nrodent spatial learning and memory, but the detailed scientific\nhistory is not so clear on this interpretation. Scientific commentary\nover time on what this task measures, including some from Morris\nhimself, reveals no clear consensus. Sullivan traces the source of\nthis scientific inconsistency back to the impact of 1980s-era\ncellular-molecular reductionism driving experimental behavioral\nneurobiology protocols like the Morris water maze. \nA different motivation drives neurobiologist Alcino Silva,\nneuroinformaticist Anthony Landreth, and philosopher of neuroscience\nJohn Bickle’s (2014) focus on experimentation. All contemporary\nsciences are growing at a vertiginous pace; but perhaps none more so\nthan neuroscience. It is no longer possible for any single scientist\nto keep up with all the relevant published literature in even his or\nher narrow research field, or fully to comprehend its implications. An\noverall lack of clarity and consensus about what is known, what\nremains doubtful, and what has been disproven creates special problems\nfor experiment planning. There is a recognized and urgent need to\ndevelop strategies and tools to address these problems. Toward this\nexplicit end, Silva, Landreth, and Bickle’s book describes a\nframework and a set of principles for organizing the published record.\nThey derive their framework and principles directly from landmark case\nstudies from the influential neuroscientific field of molecular and\ncellular cognition (MCC), and describe how their framework can be used\nto generate maps of experimental findings. Scientists armed with these\nresearch maps can then determine more efficiently what has been\naccomplished in their fields, and where the knowledge gaps still\nreside. The technology needed to automate the generation of these maps\nalready exists. Silva, Landreth, and Bickle sketch the transformative,\nrevolutionary impact these maps can have on current science. \nThree goals motivate Silva, Landreth, and Bickle’s approach.\nFirst, they derive their framework from the cellular and molecular\nneurobiology of learning and memory. This choice was due strictly to\nfamiliarity with the science. Silva was instrumental in bringing gene\ntargeting techniques applied to mammals into behavioral neuroscience,\nand Bickle’s focus on ruthlessly reductive neuroscience was\nbuilt on these and other experimental results. And while each of their\nframework’s different kinds of experiments and evidence have\nbeen recognized by others, theirs purports to be the first to\nsystematize this information explicitly toward the goal of\nfacilitating experimental planning by practicing scientists. Silva,\nLandreth and Bickle insist that important new experiments can be\nidentified and planned by methodically filling in the different forms\nof evidence recognized by their framework, and applying the different\nforms of experiments to the gaps in the experimental record revealed\nby this process. \nSecond, Silva, Landreth, and Bickle take head-on the problem of the\ngrowing amount, complexity and integration of the published literature\nfor experiment planning. They show how graphic weighted\nrepresentations of research findings can be used to guide research\ndecisions; and how to construct these. The principles for constructing\nthese maps are the principles for integrating experimental results,\nderived directly from landmark published MCC research. Using a case\nstudy from recent molecular neuroscience, they show how to generate\nsmall maps that reflect a series of experiments, and how to combine\nthese small maps to illustrate an entire field of neuroscience\nresearch. \nFinally, Silva, Landreth and Bickle begin to develop a science of\nexperiment planning. They envision the causal graphs that compose\ntheir research maps to play a role similar to that played by\nstatistics in the already-developed science of data analysis. Such a\nresource could have profound implications for further developing\ncitation indices and other impact measures for evaluating\ncontributions to a field, from those of individual scientists to those\nof entire institutions. \nMore recently Bickle and Kostko (2018) have extended Silva, Landreth\nand Bickle’s framework beyond the neurobiology of learning and\nmemory. Their case study comes from developmental and social\nneuroscience, Michael Meaney and Moshe Szyf’s work on the\nepigenetics of rodent maternal nursing behaviors on offspring stress\nresponses. Using the details of this case study they elaborate on a\nnotion that Silva, Landreth and Bickle leave underdeveloped, that of\nexperiments designed explicitly for their results, if successful, to\nbe integrated directly into an already-existing background of\nestablished results. And they argue that such experiments\n“integratable by design” with others are aimed not at\nestablishing evidence for individual causal relations among\nneuroscientific kinds, but rather at formulating entire causal\npathways connecting multiple phenomena. Their emphasis on causal paths\nrelates to that of Lauren Ross (forthcoming). Ross’s work is\nespecially interesting in this context because she uses her causal\npathway concept to address “causal selection”, which has\nto do with distinguishing between background conditions and\n“true” (triggering) causes of some outcome of interest.\nFor Silva, Landreth, and Bickle (2014), accounting for this\ndistinction is likewise crucial, and they rely on a specific kind of\nconnection experiment, “positive manipulations”, to draw\nit. Bickle and Kostko’s appeal to causal paths in a detailed\ncase study from recent developmental neurobiology might help bridge\nSilva, Landreth and Bickle’s broader work on neurobiological\nexperimentation with Ross’s work drawn from biology more\ngenerally.","contact.mail":"anthony.w.landreth@gmail.com","contact.domain":"gmail.com"}]
