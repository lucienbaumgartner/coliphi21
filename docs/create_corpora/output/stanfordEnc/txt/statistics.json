[{"date.published":"2014-08-19","url":"https://plato.stanford.edu/entries/statistics/","author1":"Jan-Willem Romeijn","author1.info":"http://www.philos.rug.nl/~romeyn/index.html","entry":"statistics","body.text":"\n\nStatistics investigates and develops specific methods for\nevaluating hypotheses in the light of empirical facts. A method is\ncalled statistical, and thus the subject of study in statistics, if it\nrelates facts and hypotheses of a particular kind: the empirical facts\nmust be codified and structured into data sets, and the hypotheses\nmust be formulated in terms of probability distributions over possible\ndata sets. The philosophy of statistics concerns the foundations and\nthe proper interpretation of statistical methods, their input, and\ntheir results.  Since statistics is relied upon in almost all\nempirical scientific research, serving to support and communicate\nscientific findings, the philosophy of statistics is of key importance\nto the philosophy of science. It has an impact on the philosophical\nappraisal of scientific method, and on the debate over the epistemic\nand ontological status of scientific theory.\n\nThe philosophy of statistics harbors a large variety of topics and\ndebates. Central to these is the \n problem of induction,\nwhich concerns the justification of inferences or procedures that\nextrapolate from data to predictions and general facts. Further\ndebates concern the\n interpretation of the probabilities\nthat are used in statistics, and the wider theoretical framework\nthat may ground and justify the correctness of statistical methods. A\ngeneral introduction to these themes is given in\n Section 1 and\n Section 2. \n Section 3 and \n Section 4 provide an account of how\nthese themes play out in the two major theories of statistical method,\nclassical and Bayesian statistics respectively.\n Section 5\ndirects attention to the notion of a statistical model, covering model\nselection and simplicity, but also discussing statistical techniques\nthat do not rely on statistical models. \n Section 6 briefly\nmentions relations between the philosophy of statistics and several\nother themes from the philosophy of science, including \n confirmation theory,\n evidence, \n causality, measurement, and scientific\nmethodology in general.\n\n\nStatistics is a mathematical and conceptual discipline that focuses on the relation\nbetween data and hypotheses. The data are recordings of\nobservations or events in a scientific study, e.g., a set of\nmeasurements of individuals from a population. The data actually\nobtained are variously called the sample, the sample\ndata, or simply the data, and all possible samples from\na study are collected in what is called a sample\nspace. The hypotheses, in turn, are general\nstatements about the target system of the scientific study, e.g.,\nexpressing some general fact about all individuals in the population.\nA statistical hypothesis is a general statement that can\nbe expressed by a probability distribution over sample space, i.e., it\ndetermines a probability for each of the possible samples.  Statistical methods provide the mathematical and conceptual means\nto evaluate statistical hypotheses in the light of a sample. To this\nend the methods employ probability theory, and incidentally generalizations\nthereof. The evaluations may determine how believable a hypothesis is,\nwhether we may rely on the hypothesis in our decisions, how strong the\nsupport is that the sample gives to the hypothesis, and so on. Good\nintroductions to statistics abound (e.g., Barnett 1999, Mood and\nGraybill 1974, Press 2002). To set the stage an example, taken from Fisher (1935), will be\nhelpful. According to the so-called null hypothesis test, such a\ndecision is warranted if the data actually obtained are included in a\nparticular region within sample space, whose total probability does\nnot exceed some specified limit, standardly set at 5%. Now consider\nwhat is achieved by the statistical test just outlined. We started\nwith a hypothesis on the actual tea tasting abilities of the lady,\nnamely, that she did not have any. On the assumption of this\nhypothesis, the sample data we obtained turned out to be surprising\nor, more precisely, highly improbable. We therefore decided that the\nhypothesis that the lady has no tea tasting abilities whatsoever can\nbe rejected. The sample points us to a negative but general conclusion\nabout what the lady can, or cannot, do. The basic pattern of a statistical analysis is thus familiar from\ninductive inference: we input the data obtained thus far, and the\nstatistical procedure outputs a verdict or evaluation that transcends\nthe data, i.e, a statement that is not entailed by the data alone. If\nthe data are indeed considered to be the only input, and if the\nstatistical procedure is understood as an inference, then statistics\nis concerned with ampliative inference: roughly speaking, we\nget out more than we have put in. And since the ampliative inferences\nof statistics pertain to future or general states of affairs, they are\ninductive.  However, the association of statistics with ampliative and\ninductive inference is contested, both because statistics is\nconsidered to be non-inferential by some (see \n Section 3) and\nnon-ampliative by others (see \n Section 4). Despite such disagreements, it is insightful to view statistics as\na response to the problem of induction (cf. Howson 2000 and the\n entry on the\n problem of induction). \nThis problem, first discussed by Hume in his Treatise of Human\nNature (Book I, part 3, section 6) but prefigured already by\nancient sceptics like Sextus Empiricus (see the entry on\n ancient skepticism), \nis that there is no proper justification for\ninferences that run from given experience to expectations about the\nfuture. Transposed to the context of statistics, it reads that there\nis no proper justification for procedures that take data as input and\nthat return a verdict, an evaluation, or some other piece of advice\nthat pertains to the future, or to general states of affairs. Arguably,\nmuch of the philosophy of statistics is about coping with this\nchallenge, by providing a foundation of the procedures that statistics\noffers, or else by reinterpreting what statistics delivers so as to\nevade the challenge. It is debatable that philosophers of statistics are ultimately\nconcerned with the delicate, even ethereal issue of the justification\nof induction. In fact, many philosophers and scientists accept the\nfallibility of statistics, and find it more important that statistical\nmethods are understood and applied correctly. As is so often the case,\nthe fundamental philosophical problem serves as a catalyst: the\nproblem of induction guides our investigations into the workings, the\ncorrectness, and the conditions of applicability of statistical\nmethods. The philosophy of statistics, understood as the general\nheader under which these investigations are carried out, is thus not\nconcerned with ephemeral issues, but presents a vital and concrete\ncontribution to the philosophy of science, and to science itself.  While there is large variation in how statistical procedures and\ninferences are organized, they all agree on the use of modern\nmeasure-theoretic probability theory (Kolmogorov ), or a near\nkin, as the means to express hypotheses and relate them to data. By\nitself, a probability function is simply a particular kind of\nmathematical function, used to express the measure of a set\n(cf. Billingsley 1995).  Let \\(W\\) be a set with elements \\(s\\), and consider an initial\ncollection of subsets of \\(W\\), e.g., the singleton sets \\(\\{ s\n\\}\\). Now consider the operation of taking the complement \\(\\bar{R}\\)\nof a given set \\(R\\): the complement \\(\\bar{R}\\) contains exactly and\nall those \\(s\\) that are not included in \\(R\\). Next consider the join\n\\(R \\cup Q\\) given sets \\(R\\) and \\(Q\\): an element \\(s\\) is a member\nof \\(R \\cup Q\\) precisely when it is a member of \\(R\\), \\(Q\\), or\nboth. The collection of sets generated by the operations of complement\nand join is called an\nalgebra, denoted \\(S\\). In statistics we interpret \\(S\\) as\nthe set of samples, and we can associate sets \\(R\\) with specific\nevents or observations. A specific sample \\(s\\) includes a record of\nthe event denoted with \\(R\\) exactly when \\(s \\in R\\). We take the\nalgebra of sets like \\(R\\) as a language for making claims about the\nsamples. A probability function is defined as an additive\nnormalized measure over the algebra: a function\n\n    \\[ P: {\\cal S} \\rightarrow [0, 1] \\]  \n\nsuch that \\(P(R \\cup Q) = P(R) + P(Q)\\) if \\(R \\cap Q = \\emptyset\\)\nand \\(P(W) = 1\\). The conditional probability \\(P(Q \\mid R)\\)\nis defined as\n\n\\[ P(Q \\mid R) \\; = \\; \\frac{P(Q \\cap R)}{P(R)} , \\]\n\nwhenever \\(P(R) > 0\\). It determines the relative size of the set\n\\(Q\\) within the set \\(R\\). It is often read as the probability of the\nevent \\(Q\\) given that the event \\(R\\) occurs. Recall that\nthe set \\(R\\) consists of all samples \\(s\\) that include a record of\nthe event associated with \\(R\\). By looking at \\(P(Q \\mid R)\\) we zoom\nin on the probability function within this set \\(R\\), i.e., we\nconsider the condition that the associated event occurs. Now what does the probability function mean? The mathematical\nnotion of probability does not provide an answer.  The function \\(P\\)\nmay be interpreted as  This distinction should not be confused with that between\nobjective and subjective probability. Both physical and epistemic\nprobability can be given an objective and subjective character, in the\nsense that both can be taken as dependent or independent of a\nknowing subject and her conceptual apparatus. For more details on the\ninterpretation of probability, the reader is invited to consult\nGalavotti (2005), Gillies (2000), Mellor (2005), von Plato (1994), the\nanthology by Eagle (2010), the handbook of Hajek and Hitchcock\n(forthcoming), or indeed the entry on \n interpretations of probability.\nIn this context the key point is that the interpretations can all be\nconnected to foundational programmes for statistical\nprocedures. Although the match is not exact, the two major types\nspecified above can be associated with the two major theories of\nstatistics, classical and Bayesian statistics, respectively. \nIn the sciences, the idea that probabilities express physical states\nof affairs, often called chances or stochastic processes, is most\nprominent. They are relative frequencies in series of events\nor, alternatively, they are tendencies or propensities in the\nsystems that realize those events. More precisely, the probability\nattached to the property of an event type can be understood as the\nfrequency or tendency with which that property manifests in a series\nof events of that type. For instance, the probability of a coin\nlanding heads is a half exactly when in a series of similar coin\ntosses, the coin lands heads half the time. Or alternatively, the\nprobability is half if there is an even tendency towards both possible\noutcomes in the setup of the coin tossing. The mathematician Venn\n(1888) and scientists like Quetelet and Maxwell (cf. von Plato 1994)\nare early proponents of this way of viewing probability. Philosophical\ntheories of propensities were first coined by Peirce (1910), and\ndeveloped by Popper (1959), Mellor (1971), Bigelow (1977), and Giere\n(1976); see Handfield (2012) for a recent overview. A rigourous theory\nof probability as frequency was first devised by von Mises (1981),\nalso defended by Reichenbach  (1938) and beautifully expounded in\nvan Lambalgen (1987). The notion of physical probability is connected to one of the major\ntheories of statistical method, which has come to be called\nclassical statistics.  It was developed roughly in the first\nhalf of the 20th century, mostly by mathematicians and working\nscientists like Fisher (1925, 1935, 1956), Wald (1939, 1950), Neyman\nand Pearson (1928, 1933, 1967), and refined by very many classical\nstatisticians of the last few decades.  The key characteristic of this\ntheory of statistics aligns naturally with viewing probabilities as\nphysical chances, hence pertaining to observable and repeatable\nevents. Physical probability cannot meaningfully be attributed to\nstatistical hypotheses, since hypotheses do not have tendencies or\nfrequencies with which they come about: they are categorically true or\nfalse, once and for all. Attributing probability to a hypothesis seems to entail that\nthe probability is read epistemically. Classical statistics is often called frequentist, owing to\nthe centrality of frequencies of events in classical procedures and\nthe prominence of the frequentist interpretation of probability\ndeveloped by von Mises. In this interpretation, chances are\nfrequencies, or proportions in a class of similar events or\nitems. They are best thought of as analogous to other physical\nquantities, like mass and energy. It deserves emphasis that\nfrequencies are thus conceptually prior to chances . In propensity\ntheory the probability of an individual event or item is viewed as a\ntendency in nature, so that the frequencies, or the proportions in a\nclass of similar events or items, manifest as a consequence of the law\nof large numbers. In the frequentist theory, by contrast, the\nproportions lay down, indeed define what the chances are. This\nleads to a central problem for frequentist probability, the\nso-called reference class problem: it is not clear what\nclass to associate with an individual event or item (cf.  Reichenbach\n1949, Hajek 2007). One may argue that the class needs to be as narrow\nas it can be, but in the extreme case of a singleton class of events,\nthe chances of course trivialize to zero or one. Since classical\nstatistics employs non-trivial probabilities that attach to the single\ncase in its procedures, a fully frequentists understanding of\nstatistics is arguably in need of a response to the reference class\nproblem.  To illustrate physical probability, we briefly consider physical probability in the\nexample of the tea tasting lady. Now say that we have found a lady for whom we reject the null\nhypothesis, i.e., a lady who passes the test. Does she have the tea\ntasting ability or not? Unfortunately this is not the sort of question\nthat can be answered by the test at hand. A good answer would\npresumably involve the proportion of ladies who indeed have the\nspecial tea tasting ability among those whose scores exceeded a\ncertain threshold, i.e., those who answered correctly on all five\ncups. But this latter proportion, namely of ladies for whom the null\nhypothesis is false among all those ladies who passed the test,\ndiffers from the proportion of ladies who passed the test among\nthose ladies for whom it is false. It will depend also on the\nproportion of ladies who have the ability in the population under\nscrutiny. The test, by contrast, only involves proportions within a\ngroup of ladies for whom the null hypothesis is true: we can only\nconsider probabilities for particular events on the assumption that\nthe events are distributed in a given way. \nThere is an alternative way of viewing the probabilities that appear\nin statistical methods: they can be seen as expressions of epistemic\nattitudes. We are again facing several interrelated options. Very\nroughly speaking, epistemic probabilities can be doxastic,\ndecision-theoretic, or logical. Probabilities may be taken to represent doxastic attitudes\nin the sense that they specify opinions about data and hypotheses of\nan idealized rational agent. The probability then expresses the strength\nor degree of belief, for instance regarding the correctness of the\nnext guess of the tea tasting lady. They may also be taken as\ndecision-theoretic, i.e., as part of a more elaborate\nrepresentation of the agent, which determines her dispositions towards\ndecisions and actions about the data and the hypotheses. Oftentimes a\ndecision-theoretic representation involves doxastic attitudes\nalongside preferential and perhaps other ones. In that case, the\nprobability may for instance express a willingness to bet on the lady\nbeing correct. Finally, the probabilities may be taken as\nlogical.  More precisely, a probabilistic model may be\ntaken as a logic, i.e., a formal representation that fixes a normative\nideal for uncertain reasoning. According to this latter option,\nprobability values over data and hypotheses have a role that is\ncomparable to the role of truth values in deductive logic: they serve\nto secure a notion of valid inference, without carrying the suggestion\nthat the numerical values refer to anything psychologically\nsalient. The epistemic view on probability came into development in the 19th\nand the first half of the 20th century, first by the hand of De Morgan\n(1847) and Boole (1854), later by Keynes (1921), Ramsey (1926) and de\nFinetti (1937), and by decision theorists, philosophers and\ninductive logicians such as Carnap (1950), Savage (1962), Levi (1980),\nand Jeffrey (1992). Important proponents of these views in statistics\nwere Jeffreys (1961), Edwards (1972), Lindley (1965), Good (1983),\nJaynes (2003) as well as very many Bayesian philosophers and\nstatisticians of the last few decades (e.g., Goldstein 2006, Kadane\n2011, Berger 2006, Dawid 2004). All of these have a view that places\nprobabilities somewhere in the realm of the epistemic rather than the\nphysical, i.e., not as part of a model of the world but rather as a\nmeans to model a representing system like the human mind.  The above division is certainly not complete and it is blurry at\nthe edges. For one, the doxastic notion of probability has mostly been\nspelled out in a behaviorist manner, with the help of decision theory.\nMany have adopted so-called Dutch book arguments to make the\ndegree of belief precise, and to show that it is indeed captured by\nthe mathematical theory of probability (cf. Jeffrey 1992). According\nto such arguments, the degree of belief in the occurrence of an event\nis given by the price of a betting contract that pays out one monetary\nunit if the event manifests. However, there are alternatives to this\nbehaviorist take on probability as doxastic attitude, using accuracy\nor proximity to the truth. Most of these are versions or extensions of\nthe arguments proposed by de Finetti (1974). Others have\ndeveloped an axiomatic approach based on natural desiderata for\ndegrees of belief (e.g., Cox 1961). Furthermore, and as alluded to above, within the doxastic\nconception of probability we can make a further subdivision\ninto subjective and objective doxastic attitudes. The defining\ncharacteristic of an objective doxastic probability is that it is\nconstrained by the demand that the beliefs are calibrated to some\nobjective fact or state of affairs, or else by further rationality\ncriteria. A subjective doxastic attitude, by contrast, is not\nconstrained in such a way: from a normative perspective, agents are\nfree to believe as they see fit, as long as they comply to the\nprobability axioms. \nFor present concerns the important point is that each of these\nepistemic interpretations of the probability calculus comes with its\nown set of foundational programs for statistics. On the whole,\nepistemic probability is most naturally associated with Bayesian\nstatistics, the second major theory of statistical methods (Press\n2002, Berger 2006, Gelman et al 2013). The key\ncharacteristic of Bayesian statistics flows directly from the\nepistemic interpretation: under this interpretation it becomes\npossible to assign probability to a statistical hypothesis and to\nrelate this probability, understood as an expression of how strongly\nwe believe the hypothesis, to the probabilities of events. Bayesian\nstatistics allows us to express how our epistemic attitudes towards a\nstatistical hypothesis, be it logical, decision-theoretic, or\ndoxastic, changes under the impact of data. To illustrate the epistemic conception of probability in Bayesian\nstatistics, we briefly return to the example of the tea tasting\nlady.  The take-home message is that the Bayesian method allows us to\nexpress our epistemic attitudes to statistical hypotheses in terms of\na probability assignment, and that the data impact on this epistemic\nattitude in a regulated fashion. It should be emphasized that Bayesian statistics is not the sole\nuser of an epistemic notion of probability. Indeed, a frequentists\nunderstanding of probabilities assigned to statistical hypotheses\nseems nonsensical. But it is perfectly possible to read the\nprobabilities of events, or elements in sample space, as epistemic,\nquite independently of the statistical method that is being used. As\nfurther explained in the next section, several philosophical\ndevelopments of classical statistics employ epistemic probability,\nmost notably fiducial probability (Fisher 1955 and 1956; see also\nSeidenfeld 1992 and Zabell 1992), likelihoodism (Hacking 1965, Edwards\n1972, Royall 1997), and evidential probability (Kyburg 1961), or\nconnect the procedures of classical statistics to inference and\nsupport in some other way. In all these developments, probabilities\nand functions over sample space are read epistemically, i.e., as\nexpressions of the strength of evidence, the degree of support, or\nsimilar. \nThe collection of procedures that may be grouped under classical\nstatistics is vast and multi-faceted. By and large, classical\nstatistical procedures share the feature that they only rely on\nprobability assignments over sample spaces. As indicated, an important\nmotivation for this is that those probabilities can be interpreted as\nfrequencies, from which the term of frequentist\nstatistics originates.  Classical statistical procedures are\ntypically defined by some function over sample space, where this\nfunction depends, often exclusively, on the distributions that the\nhypotheses under consideration assign to the sample space. For the\nrange of samples that may be obtained, the function then points to one\nof the hypotheses, or perhaps to a set of them, as being in some sense\nthe best fit with that sample. Or, conversely, it discards candidate\nhypotheses that render the sample too improbable.  In sum, classical procedures employ the data to narrow down a set\nof hypotheses. Put in such general terms, it becomes apparent that\nclassical procedures provide a response to the problem of\ninduction. The data are used to get from a weak general statement\nabout the target system to a stronger one, namely from a set of\ncandidate hypotheses to a subset of them. The central concern in the\nphilosophy of statistics is how we are to understand these procedures,\nand how we might justify them. Notice that the pattern of classical\nstatistics resembles that of eliminative induction: in view\nof the data we discard some of the candidate hypotheses. Indeed\nclassical statistics is often seen in loose association with Popper's\nfalsificationism, but this association is somewhat misleading. In\nclassical procedures statistical hypotheses are discarded when they\nrender the observed sample too improbable, which of course differs\nfrom discarding hypotheses that deem the observed sample\nimpossible. The foregoing already provided a short example and a rough sketch\nof classical statistical procedures. These are now specified in more\ndetail, on the basis of Barnett (1999) as primary source. The\nfollowing focuses on two very central procedures, hypothesis testing\nand estimation. The first has to do with the comparison of two\nstatistical hypotheses, and invokes theory developed by Neyman and\nPearson. The second concerns the choice of a hypothesis from a set,\nand employs procedures devised by Fisher. While these figures are\nrightly associated with classical statistics, their philosophical\nviews diverge. We return to this below. The procedure of Fisher's null hypothesis test was already\ndiscussed briefly in the foregoing. Let \\(h\\) be the hypothesis of\ninterest and, for the sake of simplicity, let \\(S\\) be a finite sample\nspace. The hypothesis \\(h\\) imposes a distribution over the sample\nspace, denoted \\(P_{h}\\). Every point \\(s\\) in the space represents a\npossible sample of data. We now define a function \\(F\\) on the sample\nspace that identifies when we will reject the null hypothesis by\nmarking the samples \\(s\\) that lead to rejection with \\(F(s) = 1\\), as\nfollows:\n\n \\[   F(s) =   \\begin{cases}  1 \\quad \\text{if } P_{h}(s) < r,\\\\ 0 \\quad\n\\text{otherwise.} \\end{cases} \\]\n\nNotice that the definition of the region of rejection, \\(R_{r} = \\{\ns:\\: F(s) = 1 \\}\\), hinges on the probability of the data under the\nassumption of the hypothesis, \\(P_{h}(s)\\). This expression is often\ncalled the\nlikelihood of the hypothesis on the sample \\(s\\). We can set\nthe threshold \\(r\\) for the likelihood to a suitable value, such that\nthe total probability of the region of rejection \\(R_{r}\\) is below a\ngiven level of error, for example, \\(P_{h}(R) < 0.05\\). It soon appeared that comparisons between two rival hypotheses were\nfar more informative, in particular because little can be said about\nerror rates if the null hypothesis is in fact false. Neyman and\nPearson (1928, 1933, and 1967) devised the so-called likelihood\nratio test, a test that compares the likelihoods of two rivaling\nhypotheses. Let \\(h\\) and \\(h'\\) be the null and the alternative\nhypothesis respectively. We can compare these hypotheses by the\nfollowing test function \\(F\\) over the sample space:\n\n\\[ F(s) = \\begin{cases} 1 \\quad \\text{if } \\frac{P_{h'}(s)}{P_{h}(s)}\n> r,\\\\ 0 \\quad \\text{otherwise,} \\end{cases} \\]\n\nwhere \\(P_{h}\\) and \\(P_{h'}\\) are the probability distributions over\nthe sample space determined by the statistical hypotheses \\(h\\) and\n\\(h'\\) respectively. If \\(F(s) = 1\\) we decide to reject the null\nhypothesis \\(h\\), else we accept \\(h\\) for the time being and so\ndisregard \\(h'\\). The decision to accept or reject a hypothesis is associated with\nthe so-called significance and power of the test. The\nsignificance is the probability, according to the null\nhypothesis \\(h\\), of obtaining data that leads us to falsely reject\nthis hypothesis \\(h\\):\n\n\\[ \\text{Significance}_{F} = \\alpha = P_{h}(R_{r}) = \\sum_{s \\in S}\nF(s) P_{h}(s) d s , \\]\n\nThe probability \\(\\alpha\\) is alternatively called the type-I\nerror, and it is often denoted as the\nsignificance or the p-value. The power is\nthe probability, according to the alternative hypothesis \\(h'\\), of\nobtaining data that leads us to correctly reject the null hypothesis\n\\(h\\):\n\n\\[ \\text{Power}_{F} = 1 - \\beta = P_{h'}(F_{1}) = \\sum_{s \\in S} F(s)\nP_{h'}(s) d s.  \\]\n\nThe probability \\(\\beta\\) is called the type-II error of\nfalsely accepting the null hypothesis. An optimal test is one that\nminimizes both the errors \\(\\alpha\\) and \\(\\beta\\). In their\nfundamental lemma, Neyman and Pearson proved that the decision has\noptimal significance and power for, and only for, likelihood-ratio\ntest functions \\(F\\). That is, an optimal test depends only on a\nthreshold for the ratio \\(P_{h'}(s) / P_{h}(s)\\). The example of the tea tasting lady allows for an easy illustration\nof the likelihood ratio test. The threshold of 5% significance is part of statistical convention\nand very often fixed before even considering the power. Notice that\nthe statistical procedure associates expected error rates with a\ndecision to reject or accept. Especially Neyman has become known for\ninterpreting this in a strictly behaviourist fashion. For further\ndiscussion on this point, please see \n Section 3.2.2. In this section we briefly consider parameter estimation by maximum\nlikelihood, as first devised by Fisher (1956). While in the foregoing\nwe used a finite sample space, we now employ a space with infinitely\nmany possible samples. Accordingly, a probability distribution over\nsample space is written down in terms of a so-called density\nfunction, denoted \\(P(s) ds\\), which technically speaking\nexpresses the infinitely small probability assigned to an infinitely\nsmall patch \\(ds\\) around the point \\(s\\). This probability density\nworks much like an ordinary probability function. Maximum likelihood estimation, or MLE for short, is a tool for\ndetermining the best among a set of hypotheses, often called a\nstatistical model.  Let \\(M = \\{h_{\\theta} :\\: \\theta \\in\n\\Theta \\}\\) be the model, labeled by the parameter \\(\\theta\\), let\n\\(S\\) be the sample space, and \\(P_{\\theta}\\) the distribution\nassociated with \\(h_{\\theta}\\).  Then define the maximum\nlikelihood estimator \\(\\hat{\\theta}\\) as a function over the\nsample space:\n\n\\[ \\hat{\\theta}(s) = \\left\\{ \\theta :\\: \\forall h_{\\theta'}\n\\bigl(P_{\\theta'}(s)ds \\leq P_{\\theta}(s)ds \\bigr) \\right\\}. \\]\n\nSo the estimator is a set, typically a singleton, of values of\n\\(\\theta\\) for which the likelihood of \\(h_{\\theta}\\) on the data\n\\(s\\) is maximal. The associated best hypothesis we denote with\n\\(h_{\\hat{\\theta}}\\). This can again be illustrated for the tea\ntasting lady. We suppose that the number of cups served to the lady is fixed at\n\\(t\\) so that sample space is finite again. Notice, finally, that\n\\(\\hat{\\theta}\\) is the hypothesis that makes the data most probable\nand not the hypothesis that is most probable in the light of the\ndata.  There are several requirements that we might impose on an estimator\nfunction. One is that the estimator must be consistent.  This means\nthat for larger samples the estimator function \\(\\hat{\\theta}\\)\nconverges to the parameter values associated with the distribution\n\\(\\theta^{\\star}\\) of the data generating system, or the true\nparameter values for short. Another requirement is that the estimator\nmust be unbiased, meaning that there is no discrepancy between the\nexpected value of the estimator and the true parameter values. The MLE\nprocedure is certainly not the only one used for estimating the value\nof a parameter of interest on the basis of statistical data. A simpler\ntechnique is the minimization of a particular target function, e.g.,\nthe minimizing the sum of the squares of the distances between the\nprediction of the statistical hypothesis and the data points, also\nknown as the method of least squares.  A more general\nperspective, first developed by Wald (1950), is provided by measuring\nthe discrepancy between the predictions of the hypothesis and the\nactual data in terms of a loss function. The summed squares and the\nlikelihoods may be taken as expressions of this loss. Often the estimation is coupled to a so-called confidence\ninterval (cf. Cumming 2012).  For ease of exposition, assume that\n\\(\\Theta\\) consists of the real numbers and that every sample \\(s\\) is\nlabelled with a unique \\(\\hat{\\theta}(s)\\). We define the set\n\\(R_{\\tau} = \\{ s:\\: \\hat{\\theta}(s) = \\tau \\}\\), the set of samples\nfor which the estimator function has the value \\(\\tau\\). We can now\ncollate a region in sample space within which the estimator function\n\\(\\hat{\\theta}\\) is not too far off the mark, i.e., not too far from\nthe true value \\(\\theta^{\\star}\\) of the parameter. For example,\n\n\\[ C^{\\star}_{\\Delta} = \\{ R_{\\tau} :\\: \\tau \\in [ \\theta^{\\star} -\n\\Delta , \\theta^{\\star} + \\Delta ] \\} . \\]\n\nSo this set is the union of all \\(R_{\\tau}\\) for which \\(\\tau \\in [\n\\theta^{\\star} - \\Delta , \\theta^{\\star} + \\Delta ]\\). Now we might\nset this region in such a way that it covers a large portion of the\nsample space, say \\(1 - \\alpha\\), as measured by the true distribution\n\\(P_{\\theta^{\\star}}\\). We choose \\(\\Delta\\) such that\n\n\\[ P_{\\theta^{\\star}}(C^{\\star}_{\\Delta}) = \\int_{\\theta^{\\star} -\n\\Delta}^{\\theta^{\\star} + \\Delta} P_{\\theta^{\\star}}(R_{\\tau}) d\\tau =\n1 - \\alpha .\\]\n\nStatistical folk lore typically sets \\(\\alpha\\) at a value\n5%. Relative to this number, the size of \\(\\Delta\\) says something\nabout the quality of the estimate. If we were to repeat the collection\nof the sample over and over, we would find the estimator\n\\(\\hat{\\theta}\\) within a range \\(\\Delta\\) of the true value\n\\(\\theta^{\\star}\\) in 95% of all samples. This leads us to define the\nsymmetric 95% confidence interval:\n\n\\[ CI_{95} = [ \\hat{\\theta} - \\Delta , \\hat{\\theta} + \\Delta ] \\]\n\nThe interpretation is the same as in the foregoing: with repeated\nsampling we find the true value within \\(\\Delta\\) of the estimate in\n95% of all samples.  It is crucial that we can provide an unproblematic frequentist\ninterpretation of the event that \\(\\hat{\\theta} \\in [\\theta^{\\star} -\n\\Delta, \\theta^{\\star} + \\Delta]\\), under the assumption of the true\ndistribution. In a series of estimations, the fraction of times in\nwhich the estimator \\(\\hat{\\theta}\\) is further away from\n\\(\\theta^{\\star}\\) than \\(\\Delta\\), and hence outside this interval,\nwill tend to 5%. The smaller this region, the more reliable the\nestimate.  Note that this interval is defined in terms of the unknown\ntrue value \\(\\theta^{\\star}\\). However, especially if the size of the\ninterval \\(2 \\Delta\\) is independent of the true parameter\n\\(\\theta^{\\star}\\), it is tempting to associate the 95% confidence\ninterval with the frequency with which the true value lies within a\nrange of \\(\\Delta\\) around the estimate \\(\\hat{\\theta}\\).  Below we\ncome back to this interpretation. There are of course many more procedures for estimating a variety\nof statistical targets, and there are many more expressions for the\nquality of the estimation (e.g., bootstrapping, see Efron and\nTibshirani 1993). Theories of estimation often come equipped with a\nrich catalogue of situation-specific criteria for estimators,\nreflecting the epistemic and pragmatic goals that the estimator helps\nachieving. However, in itself the estimator functions do\nnot present guidelines for belief and, importantly,\nconfidence intervals do not either.  Classical statistics is widely discussed in the philosophy of\nstatistics. In what follows two problems with the classical approach\nare outlined, to wit, its problematic interface with belief and the\nfact that it violates the so-called likelihood principle. Many more\nspecific problems can be seen to derive from these general\nones. Consider the likelihood ratio test of Neyman and Pearson. As\nindicated, the significance or p-value of a test is an error rate that\nwill manifest if data collection and testing is repeated, assuming\nthat the null hypothesis is in fact true.  Notably, the p-value does\nnot tell us anything about how probable the truth of the null\nhypothesis is. However, many scientists do use hypothesis testing in\nthis manner, and there is much debate over what can and cannot be\nderived from a p-value (cf. Berger and Sellke 1987, Casella and\nBerger 1987, Cohen 1994, Harlow et al 1997, Wagenmakers 2007,\nZiliak and McCloskey 2008, Spanos 2007, Greco 2011, Sprenger\nforthcoming-a). After all, the test leads to the advice to either\nreject the hypothesis or accept it, and this seems conceptually very\nclose to giving a verdict of truth or falsity. While the evidential value of p-values is much debated,\nmany admit that the probability of data according to a hypothesis\ncannot be used straightforwardly as an indication of how believable\nthe hypothesis is (cf. Gillies 1971, Spielman 1974 and 1978).  Such\nusage runs into the so-called base-rate fallacy. The example\nof the tea tasting lady is again instructive.  Essentially the same problem occurs if we consider the estimations\nof a parameter as direct advice on what to believe, as made clear by\nan example of Good (1983, p. 57) that is presented here in the tea\ntasting context. After observing five correct guesses, we have\n\\(\\hat{\\theta} = 1\\) as maximum likelihood estimator. But it is hardly\nbelievable that the lady will in the long run be 100% accurate. The\npoint that estimation and belief maintain complicated relations is\nalso put forward in discussions of Lindley's paradox (Lindley\n1957, Spanos 2013, Sprenger forthcoming-b). In short, it seems\nwrongheaded to turn the results of classical statistical procedures\ninto beliefs. It is a matter of debate whether any of this can be blamed on\nclassical statistics.  Initially, Neyman was emphatic that their\nprocedures could not be taken as inferences, or as in some other way\npertaining to the epistemic status of the hypotheses. Their own\nstatistical philosophy was strictly behaviorist (cf. Neyman 1957), and\nit may be argued that the problems disappear if only scientists\nabandon their faulty epistemic use of classical statistics. As\nexplained in the foregoing, we can uncontroversially associate error\nrates with classical procedures, and so with the decisions that flow\nfrom these procedures. Hence, a behavioural and error-based\nunderstanding of classical statistics seems just fine. However, both\nstatisticians and philosophers have argued that an epistemic reading\nof classical statistics is possible, and in fact preferable (e.g.,\nFisher 1955, Royall 1997). Accordingly, many have attempted to\nreinterpret or develop the theory, in order to align it with the\nepistemically oriented statistical practice of scientists (see Mayo\n1996, Mayo and Spanos 2011, Spanos 2013b). Hypothesis tests and estimations are sometimes criticised because\ntheir results generally depend on the probability functions over the\nentire sample space, and not exclusively on the probabilities of the\nobserved sample. That is, the decision to accept or reject the null\nhypothesis depends not just on the probability of what has actually\nbeen observed according to the various hypotheses, but also on the\nprobability assignments over events that could have been observed but\nwere not. A well-known illustration of this problem concerns so-called\noptional stopping (Robbins 1952, Roberts 1967, Kadane et al\n1996, Mayo 1996, Howson and Urbach 2006). Optional stopping is here illustrated for the likelihood ratio test\nof Neyman and Pearson but a similar story can be run for Fisher's null\nhypothesis test and for the determination of estimators and confidence\nintervals.  This might strike us as peculiar: statistics should tell us the\nobjective impact that the data have on a hypothesis, but here the\nimpact seems to depend on the sampling plan of the researcher\nand not just on the data themselves. As further explained in \n Section 3.2.3, \nthe results of the two researchers differ because of\ndifferences in how samples that were not observed are factored into\nthe procedure.  Some will find this dependence unacceptable: the intentions and\nplans of the researcher are irrelevant to the evidential value of the\ndata. But others argue that it is just right. They maintain that the\nimpact of data on the hypotheses should depend on the stopping\nrule or protocol that is followed in obtaining it, and not only\non the likelihoods that the hypotheses have for those data\n(e.g. Mayo 1996). The motivating intuition is that upholding the\nirrelevance of the stopping rule makes it impossible to ban\nopportunistic choices in data collection. In fact, defenders of\nclassical statistics turn the table on those who maintain that\noptional stopping is irrelevant.  They submit that it opens up the\npossibility of reasoning to a foregone conclusion by, for example,\npersistent experimentation: we might decide to cease\nexperimentation only if the preferred result is reached. However, as\nshown in Kadane et al. (1996) and further discussed in Steele\n(2012), persistent experimentation is not guaranteed to be\neffective, as long as we make sure to use the correct, in this case\nBayesian, procedures. The debate over optional stopping is eventually concerned with the\nappropriate evidential impact of data. A central concern in this wider\ndebate is the so-called likelihood principle (see Hacking\n1965 and Edwards 1972). This principle has it that the likelihoods of\nhypotheses for the observed data completely fix the evidential impact\nof those data on the hypotheses. In the formulation of Berger and\nWolpert (1984), the likelihood principle states that two samples \\(s\\)\nand \\(s'\\) are evidentially equivalent exactly when \\(P_{i}(s) =\nkP_{i}(s')\\) for all hypotheses \\(h_{i}\\) under consideration, given\nsome constant \\(k\\). Famously, Birnbaum (1962) offers a proof of the\nprinciple from more basic assumptions. This proof relies on the\nassumption of conditionality.  Say that we first toss a coin,\nfind that it lands heads, then do the experiment associated with this\noutcome, to record the sample \\(s\\). Compare this to the case where we\ndo the experiment and find \\(s\\) directly, without randomly picking\nit. The conditionality principle states that this second sample has\nthe same evidential impact as the first one: what we could have found,\nbut did not find, has no impact on the evidential value of the\nsample. Recently, Mayo (2010) has taken issue with Birnbaum's\nderivation of the likelihood principle. The classical view sketched above entails a violation of this: the\nimpact of the observed data may be different depending on the\nprobability of other samples than the observed one, because those\nother samples come into play when determining regions of acceptance\nand rejection. The Bayesian procedures discussed in \n Section 4,\non the other hand, uphold the likelihood principle: in determining the\nposterior distribution over hypotheses only the prior and the\nlikelihood of the observed data matter. In the debate over optional\nstopping and in many of the other debates between classical and\nBayesian statistics, the likelihood principle is the focal point. The view that the data reveal more, or something else,\nthan what is expressed by the likelihoods of the hypotheses at\nissue merits detailed attention. Here we investigate this issue\nfurther with reference to the controversy over optional stopping. Let us consider the analyses of the two above researchers in some\nnumerical detail by constructing the regions of rejection for both of\nthem. Determining regions of rejection\nThe diligent researcher considers all 6-tuples of success and\nfailure as the sample space, and takes their numbers as sufficient\nstatistic. The event of six successes, or six correct guesses, has a\nprobability of \\(1 / 2^{6} = 1/64\\) under the null hypothesis that the\nlady is merely guessing, against a probability of \\(3^{6} / 4^{6}\\)\nunder the alternative hypothesis. If we set \\(r < 3^{6} / 2^{6}\\),\nthen this sample is included in the region of rejection of the null\nhypothesis. Samples with five successes have a probability of \\(1/64\\)\nunder the null hypothesis too, against a probability of \\(3^5 /\n4^{6}\\) under the alternative. By lowering the likelihood ratio by a\nfactor 3, we include all these samples in the region of rejection. But\nthis will lead to a total probability of false rejection of \\(7/64\\),\nwhich is larger than 5%. So these samples cannot be included in the\nregion of rejection, and hence the diligent researcher does not reject\nthe null hypothesis upon finding five successes and one failure. \nFor the impatient researcher, on the other hand, the sample\nspace is much smaller. Apart from the sample consisting of six\nsuccesses, all samples consist of a series of successes ending with a\nfailure, differing only in the length of the series. Yet the\nprobabilities over the two samples of length six are the same as for\nthe diligent researcher. As before, the sample of six successes is\nagain included in the region of rejection. Similarly, the sequence of\nfive successes followed by one failure also has a probability of\n\\(1/64\\) under the null hypothesis, against a probability of \\(3^5 /\n4^{6}\\) according to the alternative. The difference is that lowering\nthe likelihood ratio to include this sample in the region of rejection\nleads to the inclusion of this sample only. And if we include it in\nthe region of rejection, the probability of false rejection becomes\n\\(1/32\\) and hence does not exceed 5%. Consequently, on the basis of\nthese data the laid-back researcher can reject the null hypothesis\nthat the lady is merely guessing. It is instructive to consider why exactly the impatient researcher\ncan reject the null hypothesis. In virtue of his sampling plan, the\nother samples with five successes, namely the ones which kept the\ndiligent researcher from including the observed sample in the region\nof rejection on pain of exceeding the error probability, could not\nhave been observed. This exemplifies that the results of a classical\nstatistical procedure do not only depend on the likelihoods for the\nactual data, which are indeed the same for both researchers. They also\ndepend on the likelihoods for data that we did not obtain.  In the above example, it may be considered confusing that the\nprotocol used for optional stopping depends on the data that is being\nrecorded. But the controversy over optional stopping also emerges if\nthis dependence is absent. For example, imagine a third researcher who\nsamples until the diligent researcher is done, or before that if she\nstarts to feel peckish.  Furthermore we may suppose that with each new\ncup offered to the lady, the probability of feeling peckish is\n\\(\\frac{1}{2}\\). This peckish researcher will also be able to reject\nthe null hypothesis if she completes the series of six cups. And it\ncertainly seems at variance with the objectivity of the statistical\nprocedure that this rejection depends on the physiology and the state\nof mind of the researcher: if she had not kept open the possibility of\na snack break, she would not have rejected the null hypothesis, even\nthough she did not actually take that break. As Jeffrey famously\nquipped, this is indeed a “remarkable procedure”. Yet the case is not as clear-cut as it may seem. For one, the\npeckish researcher is arguably testing two hypotheses in tandem, one\nabout the ability of the tea tasting lady and another about her own\npeckishness. Together the combined hypotheses have a different\nlikelihood for the actual sample than the simple hypothesis considered\nby the diligent researcher. The likelihood principle given above\ndictates that this difference does not affect the evidential impact of\nthe actual sample, but some retain the intuition that it\nshould. Moreover, in some cases this intuition is shared by those who\nuphold the likelihood principle, namely when the stopping rule depends\non the process being recorded in a way not already expressed by the\nhypotheses at issue (cf. Robbins 1952, Howson and Urbach 2006,\np. 365). In terms of our example, if the lady is merely guessing, then\nit may be more probable that the researcher gets peckish out of sheer\nboredom, than if the lady performs far below or above chance level. In\nsuch a case the act of stopping itself reveals something about the\nhypotheses at issue, and this should be reflected in the likelihoods\nof the hypotheses. This would make the evidential impact that the data\nhave on the hypothesis dependent on the stopping rule after all. There have been numerous responses to the above criticisms. Some of\nthose responses effectively reinterpret the classical statistical\nprocedures as pertaining only to the evidential impact of data. Other\nresponses develop the classical statistical theory to accommodate\nthe problems. Their common core is that they establish or at least\nclarify the connection between two conceptual realms: the\nstatistical procedures refer to physical probabilities, while their\nresults pertain to evidence and support, and even to the rejection or\nacceptance of hypotheses. Classical statistics is often presented as providing us with\nadvice for actions. The error probabilities do not tell us what\nepistemic attitude to take on the basis of statistical procedures,\nrather they indicate the long-run frequency of error if we live by\nthem. Specifically Neyman advocated this interpretation of classical\nprocedures. Against this, Fisher (1935a, 1955), Pearson, and\nother classical statisticians have argued for more epistemic\ninterpretations, and many more recent authors have followed suit. Central to the above discussion on classical statistics is the\nconcept of likelihood, which reflects how the data bears on the\nhypotheses at issue. In the works of Hacking (1965), Edwards (1972),\nand more recently Royall (1997), the likelihoods are taken as a\ncornerstone for statistical procedures and given an epistemic\ninterpretation. They are said to express the strength of the evidence\npresented by the data, or the comparative degree of support that the\ndata give to a hypothesis. Hacking formulates this idea in the\nso-called law of likelihood (1965, p. 59): if the sample\n\\(s\\) is more probable on the condition of \\(h_{0}\\) than on\n\\(h_{1}\\), then \\(s\\) supports \\(h_{0}\\) more than it supports\n\\(h_{1}\\). The position of likelihoodism is based on a specific combination of\nviews on probability. On the one hand, it only employs probabilities\nover sample space, and avoids putting probabilities over statistical\nhypotheses. It thereby avoids the use of probability that cannot be\ngiven a physical interpretation. On the other hand, it does interpret\nthe probabilities over sample space as components of a support\nrelation, and thereby as pertaining to the epistemic rather than the\nphysical realm. Notably, the likelihoodist approach fits well with a\nlong history in formal approaches to epistemology, in particular with\nconfirmation theory (see Fitelson 2007), in which the probability\ntheory is used to spell out confirmation relations between data\nand hypotheses. Measures of confirmation invariably take the\nlikelihoods of hypotheses as input components. They provide a\nquantitative expression of the support relations described by the law\nof likelihood. Another epistemic approach to classical statistics is presented by\nMayo (1996) and Mayo and Spanos (2011). Over the past decade or so,\nthey have done much to push the agenda of classical statistics in the\nphilosophy of science, which had become dominated by Bayesian\nstatistics. Countering the original behaviourist tendencies of Neyman,\nthe error statistical approach advances an epistemic reading\nof classical test and estimation procedures. Mayo and Spanos argue\nthat classical procedures are best understood as inferential: they\nlicense inductive inferences. But they readily admit that the\ninferences are defeasible, i.e., they could lead us\nastray. Classical procedures are always associated with particular\nerror probabilities, e.g., the probability of a false rejection or\nacceptance, or the probability of an estimator falling within a\ncertain range. In the theory of Mayo and Spanos, these error\nprobabilities obtain an epistemic role, because they are taken to\nindicate the reliability of the inferences licensed by the\nprocedures. The error statistical approach of Mayo and others comprises a\ngeneral philosophy of science as well as a particular viewpoint on the\nphilosophy statistics. We briefly focus on the latter, through a\ndiscussion of the notion of a severe test (cf. Mayo and Spanos 2006).\nThe claim is that we gain knowledge of experimental effects on the\nbasis of severely testing hypotheses, which can be\ncharacterized by the significance and power. In Mayo's definition, a\nhypothesis passes a severe test on two conditions: the data must agree\nwith the hypothesis, and the probability must be very low that\nthe data agree with the alternative hypothesis.  Ignoring potential\ncontroversy over the precise interpretation of “agree” and “low\nprobability”, we can recognize the criteria of Neyman and Pearson in\nthese requirements. The test is severe if the significance is low,\nsince the data must agree with the hypothesis, and the power is high,\nsince those data must not agree, or else have a low probability of\nagreeing, with the alternative.  Apart from re-interpretations of the classical statistical\nprocedures, numerous statisticians and philosophers have developed the\ntheory of classical statistics further in order to make good on the\nepistemic role of its results. We focus on two developments in\nparticular, to wit, fiducial and evidential probability.  The theory of evidential probability originates in Kyburg\n(1961), who developed a logical system to deal consistently with the\nresults of classical statistical analyses.  Evidential probability\nthus falls within the attempts to establish the epistemic use of\nclassical statistics. Haenni et al (2010) and Kyburg and Teng (2001)\npresent an insightful introduction to evidential probability.  The\nsystem is based on a version default reasoning: statistical hypotheses\ncome attached with a confidence level, and the logical system\norganizes how such confidence levels are propagated in inference, and\nthus advises which hypothesis to use for predictions and\ndecisions. Particular attention is devoted to the propagation of\nconfidence levels in inferences that involve multiple instances of the\nsame hypothesis tagged with different confidences, where those\nconfidences result from diverse data sets that are each associated\nwith a particular population. Evidential probability assists in\nselecting the optimal confidence level, and thus in choosing\nthe appropriate population for the case under consideration. In\nother words, evidential probability helps to resolve the reference\nclass problem alluded in the foregoing. Fiducial probability presents another way in which classical\nstatistics can be given an epistemic status. Fisher (1930, 1933,\n1935c, 1956/1973) developed the notion of fiducial\nprobability as a way of deriving a probability assignment over\nhypotheses without assuming a prior probability over statistical\nhypotheses at the outset. The fiducial argument is controversial, and\nit is generally agreed that its applicability is limited to particular\nstatistical problems. Dempster (1964), Hacking (1965), Edwards (1972),\nSeidenfeld (1996) and Zabell (1996) provide insightful\ndiscussions. Seidenfeld (1979) presents a particularly detailed study\nand a further discussion of the restricted applicability of the\nargument in cases with multiple parameters. Dawid and Stone (1982)\nargue that in order to run the fiducial argument, one has to assume\nthat the statistical problem can be captured in a functional model\nthat is smoothly invertible. Dempster (1966) provides generalizations\nof this idea for cases in which the distribution over \\(\\theta\\) is\nnot fixed uniquely but only constrained within upper and lower bounds\n(cf. Haenni et al 2011). Crucially, such constraints on the\nprobability distribution over values of \\(\\theta\\) are obtained\nwithout assuming any distribution over \\(\\theta\\) at the outset. To explain the fiducial argument we first set up a simple\nexample.  Say that we estimate the mean \\(\\theta\\) of a normal\ndistribution with unit variance over a variable \\(X\\). We collect a\nsample \\(s\\) consisting of measurements \\(X_{1}, X_{2}, \\ldots\nX_{n}\\). The maximum likelihood estimator for \\(\\theta\\) is the\naverage value of the \\(X_{i}\\), that is, \\(\\hat{\\theta}(s) = \\sum_{i}\nX_{i} / n\\).  Under an assumed true value \\(\\theta\\) we then have a\nnormal distribution for the estimator \\(\\hat{\\theta}(s)\\), centred on\nthe true value and with a variance \\(1 / \\sqrt{n}\\). Notably, this\ndistribution has the same shape for all values of \\(\\theta\\). Because\nof this, argued Fisher, we can use the distribution over the estimator\n\\(\\hat{\\theta}(s)\\) as a stand-in for the distribution over the true\nvalue \\(\\theta\\). We thus derive a probability distribution\n\\(P(\\theta)\\) on the basis of a sample \\(s\\), seemingly without\nassuming a prior probability. There are several ways to clarify this so-called fiducial argument.\nOne way employs a so-called functional model, i.e., the\nspecification of a statistical model by means of a particular\nfunction. For the above model, the function is\n\n\\[ f(\\theta, \\epsilon) = \\theta + \\epsilon = \\hat{\\theta}(s) . \\]\n\nIt relates possible parameter values \\(\\theta\\) to a quantity based on\nthe sample, in this case the estimator of the observations\n\\(\\hat{\\theta}\\). The two are related through a stochastic component\n\\(\\epsilon\\) whose distribution is known, and the same for all the\nsamples under consideration. In our case \\(\\epsilon\\) is distributed\nnormally with variance \\(1 / \\sqrt{n}\\). Importantly, the distribution\nof \\(\\epsilon\\) is the same for every value of \\(\\theta\\). The\ninterpretation of the function \\(f\\) may now be apparent. Relative to\nthe choice of a value of \\(\\theta\\), which then obtains the role of\nthe true value \\(\\theta^{\\star}\\), the distribution over \\(\\epsilon\\)\ndictates the distribution over the estimator function\n\\(\\hat{\\theta}(s)\\). The idea of the fiducial argument can now be expressed succinctly.\nIt is to project the distribution over the stochastic component back\nonto the possible parameter values. The key observation is that the\nfunctional relation \\(f(\\theta, \\epsilon)\\) is smoothly invertible,\ni.e., the function\n\n\\[ f^{-1}(\\hat{\\theta}(s), \\epsilon) = \\hat{\\theta}(s) - \\epsilon =\n\\theta \\]\n\npoints each combination of \\(\\hat{\\theta}(s)\\) and \\(\\epsilon\\) to a\nunique parameter value \\(\\theta\\). Hence, we can invert the claim of\nthe previous paragraph: relative to fixing a value for\n\\(\\hat{\\theta}\\), the distribution over \\(\\epsilon\\) fully determines\nthe distribution over \\(\\theta\\). Hence, in virtue of the inverted\nfunctional model, we can transfer the normal distribution over\n\\(\\epsilon\\) to the values \\(\\theta\\) around \\(\\hat{\\theta}(s)\\). This\nyields a so-called fiducial probability distribution over the\nparameter \\(\\theta\\). The distribution is obtained because,\nconditional on the value of the estimator, the parameters and the\nstochastic terms become perfectly correlated. A distribution over the\nlatter is then automatically applicable to the former (cf. Haenni et\nal, 52-55 and 119–122).  Another way of explaining the same idea invokes the notion of a\npivotal quantity.  Because of how the above statistical model\nis set up, we can construct the pivotal quantity \\(\\hat{\\theta}(s) -\n\\theta\\). We know the distribution of this quantity, namely normal and\nwith the aforementioned variance. Moreover, this distribution is\nindependent of the sample, and it is such that fixing the sample to\n\\(s\\), and so fixing the value of \\(\\hat{\\theta}\\), uniquely\ndetermines a distribution over the parameter values \\(\\theta\\). The\nfiducial argument thus allows us to construct a probability\ndistribution over the parameter values on the basis of the observed\nsample. The argument can be run whenever we can construct a pivotal\nquantity like that or, equivalently, whenever we can express the\nstatistical model as a functional model. A warning is in order here. As revealed in many of the above\nreferences, the fiducial argument is highly controversial. The\nmathematical results are there, but the proper interpretation of the\nresults is still up for discussion . In order to properly appreciate\nthe precise inferential move and its wobbly conceptual basis, it will\nbe instructive to consider the use of fiducial probability in\ninterpreting confidence intervals. A proper understanding of this\nrequires first reading the \n Section 3.1.2. Recall that confidence intervals, which are standardly taken to\nindicate the quality of an estimation, are often interpreted\nepistemically. The 95% confidence interval is often misunderstood as\nthe range of parameter values that includes the true value with 95%\nprobability, a so-called credal interval:\n\n\\[ P(\\theta \\in [\\hat{\\theta} - \\Delta, \\hat{\\theta} + \\Delta]) = 0.95\n. \\]\n\nThis interpretation is at odds with classical statistics but, as will\nbecome apparent, it can be motivated by an application of the fiducial\nargument. Say that we replace the integral determining the size\n\\(\\Delta\\) of the confidence interval by the following:\n\n\\[ \\int_{\\hat{\\theta}(s) - \\Delta}^{\\hat{\\theta}(s) + \\Delta}\nP_{\\theta}(R_{\\hat{\\theta}(s)}) d\\theta = 0.95 .\\]\n\nIn words, we fix the estimator \\(\\hat{\\theta}(s)\\) and then integrate\nover the parameters \\(\\theta\\) in \\(P_{\\theta}(R_{\\hat{\\theta}(s)})\\),\nrather than assuming \\(\\theta^{\\star}\\) and then integrating over the\nparameters \\(\\tau\\) in \\(R_{\\tau}\\). Sure enough we can calculate this\nintegral. But what ensures that we can treat the integral as a\nprobability? Notice that it runs over a continuum of probability\ndistributions and that, as it stands, there is no reason to think that\nthe terms \\(P_{\\theta}(R_{\\hat{\\theta}(s)})\\) add up to a proper\ndistribution in \\(\\theta\\). The assumptions of the fiducial argument, here explained in terms\nof the invertibility of the functional model, ensure that the terms\nindeed add up, and that a well-behaved distribution will surface. We\ncan choose the statistical model in such a way that the sample\nstatistic \\(\\hat{\\theta}(s)\\) and the parameter \\(\\theta\\) are related\nin the right way: relative to the parameter \\(\\theta\\), we have a\ndistribution over the statistic \\(\\hat{\\theta}\\), but by the same\ntoken we have a distribution over parameters relative to this\nstatistic. As a result, the probability function\n\\(P_{\\theta}(R_{\\hat{\\theta}(s) + \\epsilon})\\) over \\(\\epsilon\\),\nwhere \\(\\theta\\) is fixed, can be transferred to a fiducial\nprobability function \\(P_{\\theta + \\epsilon}(R_{\\hat{\\theta}(s)})\\)\nover \\(\\epsilon\\), where \\(\\hat{\\theta}(s)\\) is fixed. The function\n\\(P_{\\theta}(R_{\\hat{\\theta}})\\) of the parameter \\(\\theta\\) is thus a\nproper probability function, from which a credal interval can be\nconstructed. Even then, it is not clear why we should take this distribution as\nan appropriate expression of our belief, so that we may support the\nepistemic interpretation of confidence intervals with it. And so the\ndebate continues. In the end fiducial probability is perhaps best\nunderstood as a half-way house between the classical and the Bayesian\nview on statistics. Classical statistics grew out of a frequentist\ninterpretation of probability, and accordingly the probabilities\nappearing in the classical statistical methods are all interpreted as\nfrequencies of events. Clearly, the probability distribution over\nhypotheses that is generated by a fiducial argument cannot be\ninterpreted in this way, so that an epistemic interpretation of this\ndistribution seems the only option.  Several authors (e.g., Dempster\n1964) have noted that fiducial probability indeed makes most sense in\na Bayesian perspective. It is to this perspective that we now turn.\n Bayesian statistical methods are often presented in the form of an\ninference. The inference runs from a so-called prior\nprobability distribution over statistical hypotheses, which expresses\nthe degree of belief in the hypotheses before data has been collected,\nto a posterior probability distribution over the\nhypotheses, which expresses the beliefs after the data have been\nincorporated. The posterior distribution follows, via the axioms of\nprobability theory, from the prior distribution and the\nlikelihoods of the hypotheses for the data obtained, i.e.,\nthe probability that the hypotheses assign to the data. Bayesian\nmethods thus employ data to modulate our attitude towards\na designated set of statistical hypotheses, and in this respect\nthey achieve the same as classical statistical procedures. Both types\nof statistics present a response to the problem of induction. But\nwhereas classical procedures select or eliminate elements from the set\nof hypotheses, Bayesian methods express the impact of data in a\nposterior probability assignment over the set. This posterior is fully\ndetermined by the prior and the likelihoods of the hypotheses, via the\nformalism of probability theory. The defining characteristic of Bayesian statistics is that it\nconsiders probability distributions over statistical hypotheses as\nwell as over data. It embraces the epistemic interpretation of\nprobability whole-heartedly: probabilities over hypotheses are\ninterpreted as degrees of belief, i.e., as expressions of epistemic\nuncertainty. The philosophy of Bayesian statistics is concerned\nwith determining the appropriate interpretation of these input\ncomponents, and of the mathematical formalism of probability itself,\nultimately with the aim to justify the output. Notice that the general\npattern of a Bayesian statistical method is that of\ninductivism in the cumulative sense: under the impact of\ndata we move to more and more informed probabilistic opinions about\nthe hypotheses. However, in the following it will appear that Bayesian\nmethods may also be understood as deductivist in nature. Bayesian inference always starts from a statistical\nmodel, i.e., a set of statistical hypotheses. While the general\npattern of inference is the same, we treat models with a finite number\nand a continuum of hypotheses separately and draw parallels with\nhypothesis testing and estimation, respectively. The exposition is\nmostly based on Press 2002, Howson and Urbach 2006, Gelman et al\n2013, and Earman 1992. Central to Bayesian methods is a theorem from probability theory\nknown as Bayes' theorem.  Relative to a prior probability\ndistribution over hypotheses, and the probability distributions over\nsample space for each hypothesis, it tells us what the adequate\nposterior probability over hypotheses is. More precisely, let \\(s\\) be\nthe sample and \\(S\\) be the sample space as before, and let \\(M = \\{\nh_{\\theta} :\\: \\theta \\in \\Theta \\}\\) be the space of statistical\nhypotheses, with \\(\\Theta\\) the space of parameter values. The\nfunction \\(P\\) is a probability distribution over the entire space \\(M\n\\times S\\), meaning that every element \\(h_{\\theta}\\) is associated\nwith its own sample space \\(S\\), and its own probability distribution\nover that space. For the latter, which is fully determined by the\nlikelihoods of the hypotheses, we write the probability of the sample\nconditional on the hypothesis, \\(P(s \\mid h_{\\theta})\\). This differs\nfrom the expression \\(P_{h_{\\theta}}(s)\\), written in the context of\nclassical statistics, because in contrast to classical statisticians,\nBayesians accept \\(h_{\\theta}\\) as an argument for the probability\ndistribution.  Bayesian statistics is first introduced in the context of a finite\nset of hypotheses, after which a generalization to the infinite case\nis provided. Assume the prior probability \\(P(h_{\\theta})\\) over the\nhypotheses \\(h_{\\theta} \\in M\\). Further assume the likelihoods \\(P(s\n\\mid h_{\\theta})\\), i.e., the probability assigned to the data \\(s\\)\nconditional on the hypotheses \\(h_{\\theta}\\). Then Bayes' theorem\ndetermines that\n\n\\[ P(h_{\\theta} \\mid s) \\; = \\; \\frac{P(s \\mid h_{\\theta})}{P(s)}\nP(h_{\\theta}) . \\]\n\nBayesian statistics outputs the posterior probability assignment,\n\\(P(h_{\\theta} \\mid s)\\). This expression gets the interpretation of\nan opinion concerning \\(h_{\\theta}\\) after the sample \\(s\\) has been\nrecorded accommodated, i.e., it is a revised opinion. Further results\nfrom a Bayesian inference can all be derived from the posterior\ndistribution over the statistical hypotheses. For instance, we can use\nthe posterior to determine the most probable value for the parameter,\ni.e., picking the hypothesis \\(h_{\\theta}\\) for which \\(P(h_{\\theta}\n\\mid s)\\) is maximal. In this characterization of Bayesian statistical inference the\nprobability of the data \\(P(s)\\) is not presupposed, because it can be\ncomputed from the prior and the likelihoods by the law of total\nprobability,\n\n\\[ P(s) \\; = \\; \\sum_{\\theta \\in \\Theta} P(h_{\\theta}) P(s \\mid\nh_{\\theta}) . \\]\n\nThe result of a Bayesian statistical inference is not always reported\nas a posterior probability. Often the interest is only in comparing\nthe ratio of the posteriors of two hypotheses. By Bayes' theorem we\nhave\n\n\\[ \\frac{P(h_{\\theta} \\mid s)}{P(h_{\\theta'} \\mid s)} \\; = \\;\n\\frac{P(h_{\\theta}) P(s \\mid h_{\\theta})}{P(h_{\\theta'}) P(s \\mid\nh_{\\theta'})} , \\]\n\nand if we assume equal priors \\(P(h_{\\theta}) = P(h_{\\theta'})\\), we\ncan use the ratio of the likelihoods of the hypotheses, the so-called\nBayes factor, to compare the hypotheses. Here is a Bayesian procedure for the example of the tea tasting\nlady.  Notice that in the above exposition, the posterior probability is\nwritten as \\(P(h_{\\theta} \\mid s_{n/5})\\). Some expositions of\nBayesian inference prefer to express the revised opinion as a new\nprobability function \\(P'( \\cdot )\\), which is then equated to the old\n\\(P( \\cdot \\mid s)\\).  For the basic formal workings of Bayesian\ninference, tis distinction is inessential. But we will return to it in\n Section 4.3.3.\n In many applications the model is not a finite set of hypotheses,\nbut rather a continuum labelled by a real-valued parameter. This leads\nto some subtle changes in the definition of the distribution over\nhypotheses and the likelihoods. The prior and posterior must be\nwritten down as a so-called probability density function,\n\\(P(h_{\\theta}) d\\theta\\). The likelihoods need to be defined by a\nlimit process: the probability \\(P(h_{\\theta})\\) is infinitely small\nso that we cannot define \\(P(s \\mid h_{\\theta})\\) in the normal\nmanner. But other than that the Bayesian machinery works exactly the\nsame:\n\n\\[ P(h_{\\theta} \\mid s) d\\theta \\;\\; = \\;\\; \\frac{P(s \\mid\nh_{\\theta})}{P(s)} P(h_{\\theta}) d\\theta. \\]\n\nFinally, summations need to be replaced by integrations:\n\n\\[ P(s) \\; = \\; \\int_{\\theta \\in \\Theta} P(h_{\\theta}) P(s \\mid\nh_{\\theta}) d\\theta . \\]\n\nThis expression is often called the marginal likelihood of\nthe model: it expresses how probable the data is in the light of the\nmodel as a whole. The posterior probability density provides a basis for conclusions\nthat one might draw from the sample \\(s\\), and which are similar to\nestimations and measures for the accuracy of the estimations. For one,\nwe can derive an expectation for the parameter \\(\\theta\\), where we\nassume that \\(\\theta\\) varies continuously:\n\n\\[ \\bar{\\theta} \\;\\; = \\;\\; \\int_{\\Theta}\\, \\theta P(h_{\\theta} \\mid s)\nd\\theta. \\]\n\nIf the model is parameterized by a convex set, which it typically is,\nthen there will be a hypothesis \\(h_{\\bar{\\theta}}\\) in the\nmodel. This hypothesis can serve as a Bayesian estimation. In analogy\nto the confidence interval, we can also define a so-called\ncredal interval or credibility interval\nfrom the posterior probability distribution: an interval of size\n\\(2d\\) around the expectation value \\(\\bar{\\theta}\\), written\n\\([\\bar{\\theta} - d, \\bar{\\theta} + d]\\), such that\n\n\\[ \\int_{\\bar{\\theta} - d}^{\\bar{\\theta} + d} P(h_{\\theta} \\mid s)\nd\\theta = 1-\\epsilon .  \\]\n\nThis range of values for \\(\\theta\\) is such that the posterior\nprobability of the corresponding \\(h_{\\theta}\\) adds up to\n\\(1-\\epsilon\\) of the total posterior probability. There are many other ways of defining Bayesian estimations and\ncredal intervals for \\(\\theta\\) on the basis of the posterior\ndensity. The specific type of estimation that the Bayesian analysis\noffers can be determined by the demands of the scientist. Any Bayesian\nestimation will to some extent resemble the maximum likelihood\nestimator due to the central role of the likelihoods in the Bayesian\nformalism. However, the output will also depend on the prior\nprobability over the hypotheses, and generally speaking it will only\ntend to the maximum likelihood estimator when the sample size tends to\ninfinity. See\n Section 4.2.2 \nfor more on this so-called “washing out” of the\npriors.  Most of the controversy over the Bayesian method concerns the\nprobability assignment over hypotheses. One important set of problems\nsurrounds the interpretation of those probabilities as beliefs, as to\ndo with a willingness to act, or the like. Another set of\nproblems pertains to the determination of the prior probability\nassignment, and the criteria that might govern it. The overall question here is how we should understand the\nprobability assigned to a statistical hypothesis. Naturally the\ninterpretation will be epistemic: the probability expresses the\nstrength of belief in the hypothesis. It makes little sense to attempt\na physical interpretation since the hypothesis cannot be seen as a\nrepeatable event, or as an event that might have some tendency of\noccurring. This leaves open several interpretations of the probability\nassignment as a strength of belief.  One very influential\ninterpretation of probability as degree of belief relates probability\nto a willingness to bet against certain odds (cf.  Ramsey 1926, De\nFinetti 1937/1964, Earman 1992, Jeffrey 1992, Howson 2000).  According\nto this interpretation, assigning a probability of \\(3/4\\) to a\nproposition, for example, means that we are prepared to pay at most\n$0.75 for a betting contract that pays out $1 if the\nproposition is true, and that turns worthless if the proposition is\nfalse. The claim that degrees of belief are correctly expressed in a\nprobability assignment is then supported by a so-called Dutch book\nargument: if an agent does not comply to the axioms of\nprobability theory, a malign bookmaker can propose a set of bets that\nseems fair to the agent but that lead to a certain monetary loss, and\nthat is therefore called Dutch, presumably owing to the Dutch's\nmercantile reputation. This interpretation associates beliefs directly\nwith their behavioral consequences: believing something is the same as\nhaving the willingness to engage in a particular activity, e.g., in a\nbet.  There are several problems with this interpretation of the\nprobability assignment over hypotheses. For one, it seems to make\nlittle sense to bet on the truth of a statistical hypothesis, because\nsuch hypotheses cannot be falsified or verified.  Consequently, a\nbetting contract on them will never be cashed. More generally, it is\nnot clear that beliefs about statistical hypotheses are properly\nframed by connecting them to behavior in this way. It has been argued\n(e.g., Armendt 1993) that this way of framing probability assignments\nintroduces pragmatic considerations on beliefs, to do with navigating\nthe world successfully, into a setting that is by itself more\nconcerned with belief as a truthful representation of the world. A somewhat different problem is that the Bayesian formalism, in\nparticular its use of probability assignments over statistical\nhypotheses, suggests a remarkable closed-mindedness on the part of the\nBayesian statistician. Recall the example of the foregoing, with the\nmodel \\(M = \\{ h_{1/2}, h_{3/4} \\}\\).  The Bayesian formalism requires\nthat we assign a probability distribution over these two hypotheses,\nand further that the probability of the model is \\(P(M) = 1\\). It is\nquite a strong assumption, even of an ideally rational agent, that she\nis indeed equipped with a real-valued function that expresses her\nopinion over the hypotheses. Moreover, the probability assignment over\nhypotheses seems to entail that the Bayesian statistician is certain\nthat the true hypothesis is included in the model. This is an unduly\nstrong claim to which a Bayesian statistician will have to commit at\nthe start of her analysis. It sits badly with broadly shared\nmethodological insights (e.g., Popper 1934/1956), according to which\nscientific theory must be open to revision at all times (cf. Mayo\n1996). In this regard Bayesian statistics does not do justice to the\nnature of scientific inquiry, or so it seems. The problem just outlined obtains a mathematically more\nsophisticated form in the problem that Bayesians expect to be\nwell-calibrated.  This problem, as formulated in Dawid\n(1982), concerns a Bayesian forecaster, e.g., a weatherman who\ndetermines a daily probability for precipitation in the next day.  It\nis then shown that such a weatherman believes of himself that in\nthe long run he will converge onto the correct probability with\nprobability 1. Yet it seems reasonable to suppose that the weatherman\nrealizes something could potentially be wrong with his meteorological\nmodel, and so sets his probability for correct prediction below 1. The\nweatherman is thus led to incoherent beliefs. It seems that Bayesian\nstatistical analysis places unrealistic demands, even on an ideal\nagent. For the moment, assume that we can interpret the probability over\nhypotheses as an expression of epistemic uncertainty. Then how do we\ndetermine a prior probability? Perhaps we already have an intuitive\njudgment on the hypotheses in the model, so that we can pin down the\nprior probability on that basis. Or else we might have additional\ncriteria for choosing our prior. However, several serious problems\nattach to procedures for determining the prior. First consider the idea that the scientist who runs the Bayesian\nanalysis provides the prior probability herself. One obvious problem\nwith this idea is that the opinion of the scientist might not be\nprecise enough for a determination of a full prior distribution. It\ndoes not seem realistic to suppose that the scientist can transform\nher opinion into a single real-valued function over the model,\nespecially not if the model itself consists of a continuum of\nhypotheses. But the more pressing problem is that different scientists\nwill provide different prior distributions, and that these different\npriors will lead to different statistical results. In other words,\nBayesian statistical inference introduces an inevitable subjective\ncomponent into scientific method. It is one thing that the statistical results depend on the initial\nopinion of the scientist. But it may so happen that the scientist has\nno opinion whatsoever about the hypotheses. How is she supposed to\nassign a prior probability to the hypotheses then? The prior will have\nto express her ignorance concerning the hypotheses. The leading idea\nin expressing such ignorance is usually the principle of\nindifference: ignorance means that we are indifferent between any\npair of hypotheses.  For a finite number of hypotheses, indifference\nmeans that every hypothesis gets equal probability. For a continuum of\nhypotheses, indifference means that the probability density\nfunction must be uniform.  Nevertheless, there are different ways of applying the\nprinciple of indifference and so there are different probability\ndistributions over the hypotheses that can count as expression of\nignorance. This insight is nicely illustrated in Bertrand's paradox\n. Jaynes (1973 and 2003) provides a very insightful discussion of\nthis riddle and also argues that it may be resolved by relying on\ninvariances of the problem under certain transformations. But the\ngeneral message for now is that the principle of indifference does not\nlead to a unique choice of priors.  The point is not that ignorance\nconcerning a parameter is hard to express in a probability\ndistribution over those values. It is rather that in some cases, we do\nnot even know what parameters to use to express our ignorance\nover. In part the problem of the subjectivity of Bayesian analysis may be\nresolved by taking a different attitude to scientific theory, and by\ngiving up the ideal of absolute objectivity. Indeed, some will argue\nthat it is just right that the statistical methods accommodate\ndifferences of opinion among scientists. However, this response misses\nthe mark if the prior distribution expresses ignorance rather than\nopinion: it seems harder to defend the rationality of differences of\nopinion that stem from different ways of spelling out ignorance. Now\nthere is also a more positive answer to worries over objectivity,\nbased on so-called convergence results (e.g., Blackwell and\nDubins 1962 and Gaifman and Snir 1982). It turns out that the impact\nof prior choice diminishes with the accumulation of data, and that in\nthe limit the posterior distribution will converge to a set, possibly\na singleton, of best hypotheses, determined by the sampled data and\nhence completely independent of the prior distribution.  However, in\nthe short and medium run the influence of subjective prior choice\nremains. Summing up, it remains problematic that Bayesian statistics is\nsensitive to subjective input. The undeniable advantage of the\nclassical statistical procedures is that they do not need any such\ninput, although arguably the classical procedures are in turn\nsensitive to choices concerning the sample space (Lindley 2000).\nAgainst this, Bayesian statisticians point to the advantage of being\nable to incorporate initial opinions into the statistical\nanalysis. The philosophy of Bayesian statistics offers a wide range of\nresponses to the problems outlined above. Some Bayesians bite the\nbullet and defend the essentially subjective character of Bayesian\nmethods. Others attempt to remedy or compensate for the subjectivity,\nby providing objectively motivated means of determining the prior\nprobability or by emphasizing the objective character of the Bayesian\nformalism itself. One very influential view on Bayesian statistics buys into the\nsubjectivity of the analysis (e.g., Goldstein 2006, Kadane 2011).\nSo-called personalists or strict subjectivists argue\nthat it is just right that the statistical methods do not provide any\nobjective guidelines, pointing to radically subjective sources of any\nform of knowledge. The problems on the interpretation and choice of\nthe prior distribution are thus dissolved, at least in part: the\nBayesian statistician may choose her prior at will, and they are an\nexpression of her beliefs. However, it deserves emphasis that a\nsubjectivist view on Bayesian statistics does not mean that all\nconstraints deriving from empirical fact can be disregarded. Nobody\ndenies that if you have further knowledge that imposes constraints on\nthe model or the prior, then those constraints must be\naccommodated. For example, today's posterior probability may be used\nas tomorrow's prior, in the next statistical inference. The point is\nthat such constraints concern the rationality of belief and not the\nconsistency of the statistical inference per se. Subjectivist views are most prominent among those who interpret\nprobability assignments in a pragmatic fashion, and motivate the\nrepresentation of belief with probability assignments by the\nafore-mentioned Dutch book arguments. Central to this approach is the\nwork of Savage and De Finetti. Savage (1962) proposed to axiomatize\nstatistics in tandem with decision theory, a mathematical\ntheory about practical rationality. He argued that by themselves the\nprobability assignments do not mean anything at all, and that they can\nonly be interpreted in the context where an agent faces a choice\nbetween actions, i.e., a choice among a set of bets. In similar vein,\nDe Finetti (e.g., 1974) advocated a view on statistics in which only\nthe empirical consequences of the probabilistic beliefs, expressed in\na willingness to bet, mattered but he did not make statistical\ninference fully dependent on decision theory. Remarkably, it thus\nappears that the subjectivist view on Bayesian statistics is based on\nthe same behaviorism and empiricism that motivated Neyman and Pearson\nto develop classical statistics. Notice that all this makes one aspect of the interpretation problem\nof \n Section 4.2.1\nreappear: how will the prior distribution over hypotheses make itself\napparent in behavior, so that it can rightfully be interpreted in\nterms of belief, here understood as a willingness to act? One response\nto this question is to turn to different motivations for representing\ndegrees of beliefs by means of probability assignments.  Following\nwork by De Finetti, several authors have proposed vindications of\nprobabilistic expressions of belief that are not based on behavioral\ngoals, but rather on the epistemic goal of holding beliefs that\naccurately represent the world, e.g., Rosenkrantz (1981), Joyce\n(2001), Leitgeb and Pettigrew (2010), Easwaran (2013). A strong\ngeneralization of this idea is achieved in Schervish, Seidenfeld and\nKadane (2009), which builds on a longer tradition of using scoring\nrules for achieving statistical aims. An alternative approach is that\nany formal representation of belief must respect certain logical\nconstraints, e.g., Cox provides an argument for the expression of\nbelief in terms of probability assignments on the basis of the nature\nof partial belief per se.  However, the original subjectivist response to the issue that a\nprior over hypotheses is hard to interpret came from De Finetti's\nso-called representation theorem, which shows that every\nprior distribution can be associated with its own set of predictions,\nand hence with its own behavioral consequences.  In other words, De\nFinetti showed how priors are indeed associated with beliefs that can\ncarry a betting interpretation.\n De Finetti's representation theorem relates rules for\nprediction, as functions of the given sample data, to Bayesian\nstatistical analyses of those data, against the background of a\nstatistical model. See Festa (1996) and Suppes (2001) for useful\nintroductions. De Finetti considers a process that generates a series\nof time-indexed observations, and he then studies prediction rules\nthat take these finite segments as input and return a probability over\nfuture events, using a statistical model that can analyze such samples\nand provide the predictions. The key result of De Finetti is that a\nparticular statistical model, namely the set of all distributions in\nwhich the observations are independently and identically\ndistributed, can be equated with the class of exchangeable\nprediction rules, namely the rules whose predictions do not\ndepend on the order in which the observations come in. Let us consider the representation theorem in some more formal\ndetail. For simplicity, say that the process generates time-indexed\nbinary observations, i.e., 0's and 1's. The prediction rules take such\nbit strings of length \\(t\\), denoted \\(S_{t}\\), as input, and return a\nprobability for the event that the next bit in the string is a 1,\ndenoted \\(Q^{1}_{t+1}\\). So we write the prediction rules as partial\nprobability assignments \\(P(Q^{1}_{t+1} \\mid S_{t})\\). Exchangeable\nprediction rules are rules that deliver the same prediction\nindependently of the order of the bits in the string \\(S_{t}\\). If we\nwrite the event that the string \\(S_{t}\\) has a total of \\(n\\)\nobservations of 1's as \\(S_{n/t}\\), then exchangeable prediction rules\nare written as \\(P(Q^{1}_{t+1} \\mid S_{n/t})\\). The crucial property\nis that the value of the prediction is not affected by the order in\nwhich the 0's and 1's show up in the string \\(S_{t}\\). De Finetti relates this particular set of exchangeable prediction\nrules to a Bayesian inference over a specific type of statistical\nmodel. The model that De Finetti considers comprises the so-called\nBernoulli hypotheses \\(h_{\\theta}\\), i.e., hypotheses for\nwhich\n\n\\[ P(Q^{1}_{t+1} \\mid h_{\\theta} \\cap S_{t}) = \\theta . \\]\n\nThis likelihood does not depend on the string \\(S_{t}\\) that has gone\nbefore. The hypotheses are best thought of as determining a fixed bias\n\\(\\theta\\) for the binary process, where \\(\\theta \\in \\Theta = [0,\n1]\\).  The representation theoremstates that there is a\none-to-one mapping of priors over Bernoulli hypotheses and\nexchangeable prediction rules. That is, every prior distribution\n\\(P(h_{\\theta})\\) can be associated with exactly one exchangeable\nprediction rule \\(P(Q^{1}_{t+1} \\mid S_{n/t})\\), and conversely. Next\nto the original representation theorem derived by De Finetti, several\nother and more general representation theorems were proved, e.g., for\npartially exchangeable sequences and hypotheses on Markov processes\n(Diaconis and Freedman 1980, Skyrms 1991), for clustering predictions\nand partitioning processes (Kingman 1975 and 1978), and even for\nsequences of graphs and their generating process (Aldous 1981). Representation theorems equate a prior distribution over\nstatistical hypotheses to a prediction rule, and thus to a probability\nassignment that can be given a subjective and behavioral\ninterpretation. This removes the worry expressed above, that the prior\ndistribution over hypotheses cannot be interpreted subjectively\nbecause it cannot be related to belief as a willingness to act: priors\nrelate uniquely to particular predictions. However, for De Finetti the\nrepresentation theorem provided a reason for doing away with\nstatistical hypotheses altogether, and hence for the removal of a\nnotion of probability as anything other than subjective opinion (cf.\nHintikka 1970): hypotheses whose probabilistic claims could be taken\nto refer to intangible chancy processes are superfluous metaphysical\nbaggage. Not all subjectivists are equally dismissive of the use of\nstatistical hypotheses. Jeffrey (1992) has proposed so-called\nmixed Bayesianism in which subjectively interpreted\ndistributions over the hypotheses are combined with a physical\ninterpretation of the distributions that hypotheses define over sample\nspace. Romeijn (2003, 2005, 2006) argues that priors over hypotheses\nare an efficient and more intuitive way of determining inductive\npredictions than specifying properties of predictive systems directly. This advantage of using hypotheses seems in agreement with the practice of science,\nin which hypotheses are routinely used, and often motivated by mechanistic knowledge on the data generating process. The fact that statistical hypotheses can\nstrictly speaking be eliminated does not take away from their utility in making predictions. Despite its—seemingly inevitable—subjective character,\nthere is a sense in which Bayesian statistics might lay claim to\nobjectivity. It can be shown that the Bayesian formalism meets certain\nobjective criteria of rationality, coherence, and\ncalibration. Bayesian statistics thus answers to the requirement of\nobjectivity at a meta-level: while the opinions that it deals with\nretain a subjective aspect, the way in which it deals with these\nopinions, in particular the way in which data impacts on them, is\nobjectively correct, or so it is argued. Arguments supporting the\nBayesian way of accommodating data, namely by\nconditionalization, have been provided in a pragmatic context\nby dynamic Dutch book arguments, whereby probability is\ninterpreted as a willingness to bet (cf. Maher 1993, van Fraassen\n1989). Similar arguments have been advanced on the grounds that our\nbeliefs must accurately represent the world along the lines of De\nFinetti (1974), e.g., Greaves and Wallace (2006) and Leitgeb and\nPettigrew (2010). An important distinction must be made in arguments that support the\nBayesian way of accommodating evidence: the distinction between Bayes'\ntheorem, as a mathematical given, and Bayes' rule, as a\nprinciple of coherence over time. The theorem is simply a mathematical\nrelation among probability assignments,\n\n\\[ P(h \\mid s) \\; = \\; P(h) \\frac{P(s \\mid h)}{P(s)} , \\]\n\nand as such not subject to debate. Arguments that support the\nrepresentation of the epistemic state of an agent by means of\nprobability assignments also provide support for Bayes' theorem as a\nconstraint on degrees of belief. The conditional probability \\(P(h\n\\mid s)\\) can be interpreted as the degree of belief attached to the\nhypothesis \\(h\\) on the condition that the sample \\(s\\) is obtained,\nas integral part of the epistemic state captured by the probability\nassignment. Bayes' rule, by contrast, presents a constraint on\nprobability assignments that represent epistemic states of an agent at\ndifferent points in time. It is written as\n\n\\[ P_{s}(h) \\; = P(h \\mid s) , \\]\n\nand it determines that the new probability assignment, expressing the\nepistemic state of the agent after the sample has been obtained, is\nsystematically related to the old assignment, representing the\nepistemic state before the sample came in. In the philosophy of\nstatistics many Bayesians adopt Bayes' rule implicitly, but in what\nfollows I will only assume that Bayesian statistical inferences rely\non Bayes' theorem. Whether the focus lies on Bayes' rule or on Bayes' theorem, the\ncommon theme in the above-mentioned arguments is that they approach\nBayesian statistical inference from a logical angle, and focus on its\ninternal coherence or consistency (cf. Howson 2003). While its use in\nstatistics is undeniably inductive, Bayesian inference thereby obtains\na deductive, or at least non-ampliative character: everything that is\nconcluded in the inference is somehow already present in the\npremises. In Bayesian statistical inference, those premises are given\nby the prior over the hypotheses, \\(P(h_{\\theta})\\) for \\(\\theta \\in\n\\Theta\\), and the likelihood functions, \\(P(s \\mid h_{\\theta})\\), as\ndetermined for each hypothesis \\(h_{\\theta}\\) separately.  These\npremises fix a single probability assignment over the space \\(M \\times\nS\\) at the outset of the inference. The conclusions, in turn, are\nstraightforward consequences of this probability assignment. They can\nbe derived by applying theorems of probability theory, most notably\nBayes' theorem. Bayesian statistical inference thus becomes an\ninstance of probabilistic logic (cf. Hailperin 1986, Halpern\n2003, Haenni et al 2011). Summing up, there are several arguments showing that statistical\ninference by Bayes' theorem, or by Bayes' rule, is objectively\ncorrect. These arguments invite us to consider Bayesian statistics as\nan instance of probabilistic logic.  Such appeals to the logicality of\nBayesian statistical inference may provide a partial remedy for its\nsubjective character. Moreover, a logical approach to the statistical\ninferences avoids the problem that the formalism places\nunrealistic demands on the agents, and that it presumes the agent to\nhave certain knowledge. Much like in deductive logic, we need not\nassume that the inferences are psychologically realistic, nor that the\nagents actually believe the premises of the arguments. Rather the\narguments present the agents with a normative ideal and take the\nconditional form of consistency constraints: if you accept the\npremises, then these are the conclusions. An important instance of probabilistic logic is presented in\ninductive logic, as devised by Carnap, Hintikka and others\n(Carnap 1950 and 1952, Hintikka and Suppes 1966, Carnap and Jeffrey\n1970, Hintikka and Niiniluoto 1980, Kuipers 1978, and Paris 1994, Nix\nand Paris 2006, Paris and Waterhouse 2009). Historically, Carnapian\ninductive logic developed prior to the probabilistic logics referenced\nabove, and more or less separately from the debates in the philosophy\nof statistics. But the logical systems of Carnap can quite easily be\nplaced in the context of a logical approach to Bayesian inference, and\ndoing this is in fact quite insightful. For simplicity, we choose a setting that is similar to the one used\nin the exposition of the representation theorem, namely a binary data\ngenerating process, i.e., strings of 0's and 1's. A prediction rule\ndetermines a probability for the event, denoted \\(Q^{1}_{t+1}\\), that\nthe next bit in the string is a 1, on the basis of a given string of\nbits with length \\(t\\), denoted by \\(S_{t}\\). Carnap and followers\ndesigned specific exchangeable prediction rules, mostly variants of\nthe straight rule (Reichenbach 1938),\n\n\\[ P(Q^{1}_{t+1} \\mid S_{n/t}) = \\frac{n + 1}{t + 2} , \\]\n\nwhere \\(S_{n/t}\\) denotes a string of length \\(t\\) of which \\(n\\)\nentries are 1's. Carnap derived such rules from constraints on the\nprobability assignments over the samples. Some of these constraints\nboil down to the axioms of probability. Other constraints,\nexchangeability among them, are independently motivated, by an appeal\nto so-called logical interpretation of probability.  Under\nthis logical interpretation, the probability assignment must respect\ncertain invariances under transformations of the sample space, in\nanalogy to logical principles that constrain truth valuations over a\nlanguage in a particular way. Carnapian inductive logic is an instance of probabilistic logic,\nbecause its sequential predictions are all based on a single\nprobability assignment at the outset, and because it relies on Bayes'\ntheorem to adapt the predictions to sample data (cf. Romeijn 2011).\nOne important difference with Bayesian statistical inference is that,\nfor Carnap, the probability assignment specified at the outset only\nranges over samples and not over hypotheses. However, by De Finetti's\nrepresentation theorem Carnap's exchangeable rules can be equated to\nparticular Bayesian statistical inferences. A further difference is\nthat Carnapian inductive logic gives preferred status to particular\nexchangeable rules. In view of De Finetti's representation theorem,\nthis comes down to the choice for a particular set of preferred\npriors. As further developed below, Carnapian inductive logic is thus\nrelated to objective Bayesian statistics. It is a moot point whether\nfurther constraints on the probability assignments can be considered\nas logical, as Carnap and followers have it, or whether the title of\nlogic is best reserved for the probability formalism in isolation, as\nDe Finetti and followers argue.  A further set of responses to the subjectivity of Bayesian\nstatistical inference targets the prior distribution directly: we\nmight provide further rationality principles, with which the choice of\npriors can be chosen objectively. The literature proposes several\nobjective criteria for filling in the prior over the model. Each of\nthese lays claim to being the correct expression of complete ignorance\nconcerning the value of the model parameters, or of minimal\ninformation regarding the parameters. Three such criteria are\ndiscussed here. In the context of Bertrand's paradox we already discussed\nthe principle of indifference, according to which probability\nshould be distributed evenly over the available possibilities. A\nfurther development of this idea is presented by the requirement that\na distribution should have maximum entropy. Notably, the use of\nentropy maximization for determining degrees of beliefs finds much\nbroader application than only in statistics: similar ideas are taken\nup in diverse fields like epistemology (e.g., Shore and Johnson 1980,\nWilliams 1980, Uffink 1996, and also Williamson 2010), inductive logic\n(Paris and Vencovska 1989), statistical mechanics (Jaynes 2003)\nand decision theory (Seidenfeld 1986, Grunwald and Halpern 2004). In\nobjective Bayesian statistics, the idea is applied to the\nprior distribution over the model (cf. Berger 2006).  For a finite\nnumber of hypotheses the entropy of the distribution \\(P(h_{\\theta})\\)\nis defined as\n\n\\[ E[P] \\; = \\; \\sum_{\\theta \\in \\Theta} P(h_{\\theta}) \\log\nP(h_{\\theta}) . \\]\n\nThis requirement unequivocally leads to equiprobable\nhypotheses. However, for continuous models the maximum entropy\ndistribution depends crucially on the metric over the parameters in\nthe model. The burden of subjectivity is thereby moved to the\nparameterization, but of course it may well be that we have strong\nreasons for preferring a particular parameterization over others (cf.\nJaynes 1973). There are other approaches to the objective determination of\npriors. In view of the above problems, a particularly attractive\nmethod for choosing a prior over a continuous model is proposed by\nJeffreys (1961). The general idea of so-called Jeffreys\npriors is that the prior probability assigned to a small patch in\nthe parameter space is proportional to, what may be called, the\ndensity of the distributions within that patch. Intuitively, if a lot\nof distributions, i.e., distributions that differ quite a lot among\nthemselves, are packed together on a small patch in the parameter\nspace, this patch should be given a larger prior probability than a\nsimilar patch within which there is little variation among the\ndistributions (cf. Balasubramanian 2005). More technically, such a\ndensity is expressed by a prior distribution that is proportional to\nthe Fisher information.  A key advantage of these priors is\nthat they are invariant under reparameterizations of the parameter\nspace: a new parameterization naturally leads to an adjusted density\nof distributions. A final method of defining priors goes under the name of\nreference priors (Berger et al 2009). The proposal\nstarts from the observation that we should minimize the subjectivity\nof the results of our statistical analysis, and hence that we should\nminimize the impact of the prior probability on the posterior. The\nidea of reference priors is exactly that it will allow the sample data\na maximal say in the posterior distribution. But since at the outset\nwe do not know what sample we will obtain, the prior is chosen so as\nto maximize the expected impact of the data. The expectation must\nitself be taken with respect to some distribution over sample space,\nbut again, it may well be that we have strong reasons for this latter\ndistribution. A different response to the subjectivity of priors is to\nextend the Bayesian formalism, in order to leave the choice of prior\nto some extent open. The subjective choice of a prior is in that case\ncircumvented. Two such responses will be considered in some\ndetail. Recall that a prior probability distribution over statistical\nhypotheses expresses our uncertain opinion on which of the hypotheses\nis right. The central idea behind hierarchical Bayesian\nmodels (Gelman et al 2013) is that the same pattern of putting a\nprior over statistical hypotheses can be repeated on the level of\npriors itself. More precisely, we may be uncertain over which prior\nprobability distribution over the hypotheses is right. If we\ncharacterize possible priors by means of a set of parameters, we can\nexpress this uncertainty about prior choice in a probability\ndistribution over the parameters that characterize the shape of the\nprior. In other words, we move our uncertainty one level up in a\nhierarchy: we consider multiple priors over the statistical\nhypotheses, and compare the performance of these priors on the sample\ndata as if the priors were themselves hypotheses. The idea of hierarchical Bayesian modeling (Gelman et al 2013)\nrelates naturally to the Bayesian comparison of Carnapian prediction\nrules (e.g., Skyrms 1993 and 1996, Festa 1996), and also to the\nestimation of optimum inductive methods (Kuipers 1986, Festa 1993).\nHierarchical Bayesian modeling can also be related to another\ntool for choosing a particular prior distribution over hypotheses,\nnamely the method of empirical Bayes, which estimates the\nprior that leads to the maximal marginal likelihood of the model. In\nthe philosophy of science, hierarchical Bayesian modeling has made a\nfirst appearance due to Henderson et al (2010). There is also a response that avoids the choice of a prior\naltogether. This response starts with the same idea as hierarchical\nmodels: rather than considering a single prior over the hypotheses in\nthe model, we consider a parameterized set of them. But instead of\ndefining a distribution over this set, proponents of\ninterval-valued or imprecise probability claim that\nour epistemic state regarding the priors is better expressed by this\nset of distributions, and that sharp probability assignments must\ntherefore be replaced by lower and upper bounds to the\nassignments. Now the idea that uncertain opinion is best captured by a\nset of probability assignments, or a credal set for short,\nhas a long history and is backed by an extensive literature (e.g., De\nFinetti 1974, Levi 1980, Dempster 1967 and 1968, Shafer 1976, Walley\n1991). In light of the main debate in the philosophy of statistics,\nthe use of interval-valued priors indeed forms an attractive extension\nof Bayesian statistics: it allows us to refrain from choosing a\nspecific prior, and thereby presents a rapprochement to the classical\nview on statistics. These theoretical developments may look attractive, but the fact is\nthat they mostly enjoy a cult status among philosophers of statistics\nand that they have not moved the statistician in the street. On the\nother hand, standard Bayesian statistics has seen a steep rise in\npopularity over the past decade or so, owing to the availability of\ngood software and numerical approximation methods. And most of the\npractical use of Bayesian statistics is more or less insensitive to\nthe potentially subjective aspects of the statistical results,\nemploying uniform priors as a neutral starting point for the analysis\nand relying on the afore-mentioned convergence results to wash out the\nremaining subjectivity (cf. Gelman and Shalizi 2013). However, this\npractical attitude of scientists towards modelling should not be\nmistaken for a principled answer to the questions raised in the\nphilosophy of statistics (see Morey et al 2013). In the foregoing we have seen how classical and Bayesian statistics\ndiffer. But the two major approaches to statistics also have a lot in\ncommon. Most importantly, all statistical procedures rely on the\nassumption of a statistical model, here referring to any\nrestricted set of statistical hypotheses.  Moreover, they are both\naimed at delivering a verdict over these hypotheses.  For example, a\nclassical likelihood ratio test considers two hypotheses, \\(h\\) and\n\\(h'\\), and then offers a verdict of rejection and acceptance, while a\nBayesian comparison delivers a posterior probability over these two\nhypotheses. Whereas in Bayesian statistics the model presents a very\nstrong assumption, classical statistics does not endow the model with\na special epistemic status: they are simply the hypotheses currently\nentertained by the scientist. But across the board, the adoption of a\nmodel is absolutely central to any statistical procedure. A natural question is whether anything can be said about the\nquality of the statistical model, and whether any verdict on this\nstarting point for statistical procedures can be given. Surely some\nmodels will lead to better predictions, or be a better guide to the\ntruth, than others. The evaluation of models touches on deep issues in\nthe philosophy of science, because the statistical model often\ndetermines how the data-generating system under investigation is\nconceptualized and approached (Kieseppa 2001). Model choice thus\nresembles the choice of a theory, a conceptual scheme, or even of a\nwhole paradigm, and thereby might seem to transcend the formal\nframeworks for studying theoretical rationality (cf. Carnap 1950,\nJeffrey 1980). Despite the fact that some considerations on model\nchoice will seem extra-statistical, in the sense that they fall\noutside the scope of statistical treatment, statistics offers several\nmethods for approaching the choice of statistical models. There are in fact very many methods for evaluating statistical\nmodels (Claeskens and Hjort 2008, Wagenmakers and Waldorp 2006). In\nfirst instance, the methods occasion the comparison of statistical\nmodels, but very often they are used for selecting one model over the\nothers. In what follows we only review prominent techniques that have\nled to philosophical debate: Akaike's information criterion, the\nBayesian information criterion, and furthermore the computation of\nmarginal likelihoods and posterior model probabilities, both\nassociated with Bayesian model selection. We leave aside methods that\nuse cross-validation as they have, unduly, not received as much\nattention in the philosophical literature. Akaike's information criterion, modestly termed An\nInformation Criterion or AIC for short, is based on the classical\nstatistical procedure of estimation (see Burnham and Anderson 2002,\nKieseppa 1997). It starts from the idea that a model \\(M\\) can be\njudged by the estimate \\(\\hat{\\theta}\\) that it delivers, and more\nspecifically by the proximity of this estimate to the distribution\nwith which the data are actually generated, i.e., the true\ndistribution. This proximity is often equated with the expected\npredictive accuracy of the estimate, because if the estimate and the\ntrue distribution are closer to each other, their predictions will be\nbetter aligned to one another as well. In the derivation of the AIC,\nthe so-called relative entropy or Kullback-Leibler divergence\nof the two distributions is used as a measure of their proximity, and\nhence as a measure of the expected predictive accuracy of the\nestimate. Naturally, the true distribution is not known to the statistician\nwho is evaluating the model. If it were, then the whole statistical\nanalysis would be useless. However, it turns out that we can give an\nunbiased estimation of the divergence between the true distribution\nand the distribution estimated from a particular model,\n\n \\[ \\text{AIC}[M] = - 2 \\log P( s \\mid h_{\\hat{\\theta}(s)} ) + 2 d , \\]\n\nin which \\(s\\) is the sample data, \\(\\hat{\\theta}(s)\\) is the maximum\nlikelihood estimate (MLE) of the model \\(M\\), and \\(d = dim(\\Theta)\\)\nis the number of dimensions of the parameter space of the model. The\nMLE of the model thereby features in an expression of the model\nquality, i.e., in a role that is conceptually distinct from the\nestimator function. As can be seen from the expression above, a model with a smaller\nAIC is preferable: we want the fit to be optimal at little cost in\ncomplexity. Notice that the number of dimensions, or independent\nparameters, in the model increases the AIC and thereby lowers the\neligibility of the model: if two models achieve the same maximum\nlikelihood for the sample, then the model with fewer parameters will\nbe preferred. For this reason, statistical model selection by the AIC\ncan be seen as an independent motivation for preferring simple models\nover more complex ones (Sober and Forster 1994). But this result also\ninvites some critical remarks. For one, we might impose other criteria\nthan merely the unbiasedness on the estimation of the proximity to the\ntruth, and this will lead to different expressions for the\napproximation. Moreover, it is not always clearcut what the\ndimensions of the model under scrutiny really are. For curve fitting\nthis may seem simple, but for more complicated models or different\nconceptualizations of the space of models, things do not look so easy\n(cf. Myung et al 2001, Kieseppa 2001).  A prime example of model selection is presented in curve\nfitting.  Given a sample \\(s\\) consisting of a set of points in\nthe plane \\((x, y)\\), we are asked to choose the curve that fits these\ndata best. We assume that the models under consideration are of the\nform \\(y = f(x) + \\epsilon\\), where \\(\\epsilon\\) is a normal\ndistribution with mean 0 and a fixed standard deviation, and where\n\\(f\\) is a polynomial function. Different models are characterized by\npolynomials of different degrees that have different numbers of\nparameters. Estimations fix the parameters of these polynomials. For\nexample, for the 0-degree polynomial \\(f(x) = c_{0}\\) we estimate the\nconstant \\(\\hat{c_{0}}\\) for which the probability of the data is\nmaximal, and for the 1-degree polynomial \\(f(x) = c_{0} + c_{1}\\, x\\)\nwe estimate the slope \\(\\hat{c_{1}}\\) and the offset\n\\(\\hat{c_{0}}\\). Now notice that for a total of \\(n\\) points, we can\nalways find a polynomial of degree \\(n\\) that intersects with all\npoints exactly, resulting in a comparatively high maximum likelihood\n\\(P(s \\mid \\{\\hat{c_{0}}, \\ldots \\hat{c_{n}} \\})\\). Applying the AIC,\nhowever, we will typically find that some model with a polynomial of\ndegree \\(k < n\\) is preferable. Although \\(P(s \\mid \\{\\hat{c_{0}},\n\\ldots \\hat{c_{k}} \\})\\) will be somewhat lower, this is compensated\nfor in the AIC by the smaller number of parameters. Various other prominent model selection tools are based on methods\nfrom Bayesian statistics. They all start from the idea that the\nquality of a model is expressed in the performance of the model on the\nsample data: the model that, on the whole, makes the sampled data most\nprobable is to be preferred.  Because of this, there is a close\nconnection with the hierarchical Bayesian modelling referred to\nearlier (Gelman 2013). The central notion in the Bayesian model\nselection tools is thus the marginal likelihood of the model, i.e.,\nthe weighted average of the likelihoods over the model, using the\nprior distribution as a weighing function:\n\n\\[ P(s \\mid M_{i}) \\; = \\; \\int_{\\theta \\in \\Theta_{i}} P(h_{\\theta}) P(s\n\\mid h_{\\theta}) d\\theta . \\]\n\nHere \\(\\Theta_{i}\\) is the parameter space belonging to model\n\\(M_{i}\\).  The marginal likelihoods can be combined with a prior\nprobability over models, \\(P(M_{i})\\), to derive the\nso-called posterior model probability, using Bayes'\ntheorem. One way of evaluating models, known as Bayesian model\nselection, is by comparing the models on their marginal\nlikelihood, or else on their posteriors (cf. Kass and Raftery\n1995). Usually the marginal likelihood cannot be computed analytically.\nNumerical approximations can often be obtained, but for practical\npurposes it has proved very useful, and quite sufficient, to employ an\napproximation of the marginal likelihood. This approximation has\nbecome known as the Bayesian information criterion, or BIC\nfor short (Schwarz 1978, Raftery 1995). It turns out that this\napproximation shows remarkable similarities to the AIC:\n\n\\[ \\text{BIC}[M] \\; = \\; - 2 \\log P(s \\mid h_{\\hat{\\theta}(s)}) + d \\log\nn . \\]\n\nHere \\(\\hat{\\theta}(s)\\) is again the maximum likelihood estimate of\nthe model, \\(d = dim(M)\\) the number of independent parameters, and\n\\(n\\) is the number of data points in the sample. The latter\ndependence is the only difference with the AIC, but a major difference\nin how the model evaluation may turn out. The concurrence of the AIC and the BIC seems to give a further\nmotivation for our intuitive preference for simple models over more\ncomplex ones. Indeed, other model selection tools, like the\ndeviance information criterion (Spiegelhalter et al 2002) and\nthe approach based on minimum description length (Grunwald\n2007), also result in expressions that feature a term that penalizes\ncomplex models. However, this is not to say that the dimension term\nthat we know from the information criteria exhausts the notion of\nmodel complexity. There is ongoing debate in the philosophy of science\nconcerning the merits of model selection in explications of the notion\nof simplicity, informativeness, and the like (see, for example, Sober\n2004, Romeijn and van de Schoot 2008, Romeijn et al 2012, Sprenger\n2013). There are also statistical methods that refrain from the use of a\nparticular model, by focusing exclusively on the data or by\ngeneralizing over all possible models.  Some of these techniques are\nproperly localized in descriptive statistics: they do not concern an\ninference from data but merely serve to describe the data in a\nparticular way. Statistical methods that do not rely on an explicit\nmodel choice have unfortunately not attracted much attention in the\nphilosophy of statistics, but for completeness sake they will be\nbriefly discussed here. One set of methods, and a quite important one for many practicing\nstatisticians, is aimed at data reduction. Often the sample\ndata are very rich, e.g., consisting of a set of points in a space of\nvery many dimensions. The first step in a statistical analysis may\nthen be to pick out the salient variability in the data, in order to\nscale down the computational burden of the analysis itself. The technique of principal component analysis (PCA) is\ndesigned for this purpose (Jolliffe 2002). Given a set of points in a\nspace, it seeks out the set of vectors along which the variation in\nthe points is large. As an example, consider two points in a plane\nparameterized as \\((x, y)\\): the points \\((0, 0)\\) and \\((1, 1)\\). In\nthe \\(x\\)-direction and in the \\(y\\)-direction the variation is \\(1\\),\nbut over the diagonal the variation is maximal, namely\n\\(\\sqrt{2}\\). The vector on the diagonal is called the principal\ncomponent of the data. In richer data structures, and using a more\ngeneral measure of variation among points, we can find the first\ncomponent in a similar way. Moreover, we can repeat the procedure\nafter subtracting the variation along the last found component, by\nprojecting the data onto the plane perpendicular to that\ncomponent. This allows us to build up a set of principal components of\ndiminishing importance. PCA is only one item from a large collection of techniques that are\naimed at keeping the data manageable and finding patterns in it, a\ncollection that also includes kernel methods and support\nvector machines (e.g., Vapnik and Kotz 2006). For present\npurposes, it is important to stress that such tools should not be\nconfused with statistical analysis: they do not involve the testing or\nevaluation of distributions over sample space, even though they build\nup and evaluate models of the data. This sets them apart from, e.g.,\nconfirmatory and exploratory factor analysis (Bartholomew 2008), which\nis sometimes taken to be a close relative of PCA because both\nsets of techniques allows us to identify salient dimensions within\nsample space, along which the data show large variation.  Practicing statisticians often employ data reduction tools to\narrive at conclusions on the distributions from which the data were\nsampled. There is already a wide use for machine learning and data\nmining techniques in the sciences, and we may expect even mode usage\nof these techniques in the future, because so much data is now coming\navailable for scientific analysis. However, in the philosophy of\nstatistics there is as yet little debate over the epistemic status of\nconclusions reached by means of these techniques. Philosophers of\nstatistics would do well to direct some attention here. An entirely different approach to statistics is presented by\n formal learning theory.\nThis is again a vast area of research, primarily located in\ncomputer science and artificial intelligence. The discipline is here\nmentioned briefly, as another example of an approach to statistics\nthat avoids the choice of a statistical model altogether and merely\nidentifies patterns in the data. We leave aside the theory of\nneural networks, which also concerns predictive systems that\ndo not rely on a statistical model, and focus on the theory of\nlearning algorithms because of all these approaches they have seen\nmost philosophical attention. Pioneering work on formal learning was done by Solomonoff\n(1964). As before, the setting is one in which the data consist of\nstrings of 0's and 1's, and in which an agent is attempting to\nidentify the pattern in these data. So, for example, the data may be a\nstring of the form \\(0101010101\\ldots\\), and the challenge is to\nidentify this strings as an alternating sequence. The central idea of\nSolomonoff is that all possible computable patterns must be considered\nby the agent, and therefore that no restrictive choice on statistical\nhypotheses is warranted. Solomonoff then defined a formal system in\nwhich indeed all patterns can be taken into consideration, effectively\nusing a Bayesian analysis with a cleverly constructed prior over all\ncomputable hypotheses. This general idea can also be identified in a rather new field on\nthe intersection of Bayesian statistics and machine learning,\nBayesian nonparametrics (e.g., Orbanz and Teh\n2010, Hjort et al 2010). Rather than specifying, at the outset, a\nconfined set of distributions from which a statistical analysis is\nsupposed to choose on the basis of the data, the idea is that the data\nare confronted with a potentially infinite-dimensional space of\npossible distributions. The set of distributions taken into\nconsideration is then made relative to the data obtained: the\ncomplexity of the model grows with the sample. The result is a\npredictive system that performs an online model selection alongside a\nBayesian accommodation of the posterior over the model. Current formal learning theory is a lively field, to which\nphilosophers of statistics also contribute (e.g., Kelly 1996, Kelly et\nal 1997). Particularly salient for the present concerns is that the\nsystems of formal learning are set up to achieve some notion of\nadequate universal prediction, without confining themselves\nto a specific set of hypotheses, and hence by imposing minimal\nconstraints on the set of possible patterns in the data. It is a\nmatter of debate whether this is at all possible, and to what extent\nthe predictions of formal learning theory thereby rely on, e.g.,\nimplicit assumptions on structure of the sample space. Philosophical\nreflection on this is only in its infancy. \nThere are numerous topics in the philosophy of science that bear\ndirect relevance to the themes covered in this lemma. A few central\ntopics are mentioned here to direct the reader to related lemmas in\nthe encyclopedia.  One very important topic that is immediately adjacent to the\nphilosophy of statistics is \n confirmation theory, \nthe philosophical theory that describes and justifies\nrelations between scientific theory and empirical\nevidence. Arguably, the theory of statistics is a proper part of\nconfirmation theory, as it describes and justifies the relation that\nobtains between statistical theory and evidence in the form of\nsamples. It can be insightful to place statistical procedures in this\nwider framework of relations between evidence and theory. Zooming out\neven further, the philosophy of statistics is part of the\nphilosophical topic of methodology, i.e., the general theory on\nwhether and how science acquires knowledge. Thus conceived, statistics\nis one component in a large collection of scientific methods\ncomprising concept formation, experimental design, manipulation and\nobservation, confirmation, revision, and theorizing. There are also a fair number of specific topics from the philosophy\nof science that are spelled out in terms of statistics or that are\nlocated in close proximity to it. One of these topics is the process\nof measurement, in particular the measurement of latent variables on\nthe basis of statistical facts about manifest variables.  The\nso-called representational theory of measurement (Kranz et al\n1971) relies on statistics, in particular on factor analysis, to\nprovide a conceptual clarification of how mathematical structures\nrepresent empirical phenomena. Another important topic form the\nphilosophy of science is causation (see the entries on\n probabilistic causation\nand\n Reichenbach's common cause principle). \nPhilosophers have employed probability theory to capture causal\nrelations ever since Reichenbach (1956), but more recent work in\ncausality and statistics (e.g., Spirtes et al 2001) has given the\ntheory of probabilistic causality an enormous impulse. Here\nagain, statistics provides a basis for the conceptual analysis of\ncausal relations. And there is so much more.  Several specific statistical\ntechniques, like factor analysis and the theory of Bayesian networks,\ninvite conceptual discussion of their own accord. Numerous topics\nwithin the philosophy of science lend themselves to statistical\nelucidation, e.g., the coherence, informativeness, and surprise of\nevidence. And in turn there is a wide range of discussions in the\nphilosophy of science that inform a proper understanding of\nstatistics. Among them are debates over experimentation and\nintervention, concepts of chance, the nature of scientific models, and\ntheoretical terms. The reader is invited to consult the entries on\nthese topics to find further indications of how they relate to the\nphilosophy of statistics.","contact.mail":"j.w.romeijn@rug.nl","contact.domain":"rug.nl"}]
