[{"date.published":"2015-02-26","date.changed":"2019-07-31","url":"https://plato.stanford.edu/entries/implicit-bias/","author1":"Michael Brownstein","entry":"implicit-bias","body.text":"\n\n\n\nResearch on “implicit bias” suggests that people can act\non the basis of prejudice and stereotypes without intending to do so.\nWhile psychologists in the field of “implicit social\ncognition” study consumer products, self-esteem, food, alcohol,\npolitical values, and more, the most striking and well-known research\nhas focused on implicit biases toward members of socially stigmatized\ngroups, such as African-Americans, women, and the\nLGBTQ\n community.[1]\nFor example, imagine\nFrank, who explicitly believes that women and men are equally suited\nfor careers outside the home. Despite his explicitly egalitarian\nbelief, Frank might nevertheless behave in any number of biased ways,\nfrom distrusting feedback from female co-workers to hiring equally\nqualified men over women. Part of the reason for Frank’s\ndiscriminatory behavior might be an implicit gender bias. Psychological\nresearch on implicit bias has grown steadily (§1), raising\nmetaphysical (§2), epistemological (§3), and ethical\nquestions\n (§4).[2]\n\n\nWhile Allport’s (1954) The Nature of Prejudice\nremains a touchstone for psychological research on prejudice, the study\nof implicit social cognition has two distinct and more recent sets\nof\n roots.[3]\nThe first stems from the\ndistinction between “controlled” and\n“automatic” information processing made by cognitive\npsychologists in the 1970s (e.g., Shiffrin & Schneider 1977). While\ncontrolled processing was thought to be voluntary, attention-demanding,\nand of limited capacity, automatic processing was thought to unfold\nwithout attention, to have nearly unlimited capacity, and to be hard to\nsuppress voluntarily (Payne & Gawronski 2010; see also Bargh 1994).\nIn important early work on implicit cognition, Fazio and colleagues\nshowed that attitudes can be understood as activated by either\ncontrolled or automatic processes. In Fazio’s (1995)\n“sequential priming” task, for example, following exposure\nto social group labels (e.g., “black”, “women”,\netc.), subjects’ reaction times (or “response\nlatencies”) to stereotypic words (e.g., “lazy” or\n“nurturing”) are measured. People respond more quickly to\nconcepts closely linked together in memory, and most subjects in the\nsequential priming task are quicker to respond to words like\n“lazy” following exposure to “black” than\n“white”. Researchers standardly take this pattern to\nindicate a prejudiced automatic association between semantic concepts.\nThe broader notion embedded in this research was that subjects’\nautomatic responses were thought to be “uncontaminated” by\ncontrolled or strategic responses (Amodio & Devine 2009). \n\nWhile this first stream of research focused on automaticity, a\nsecond stream focused on (un)consciousness. Many studies demonstrated\nthat awareness of stereotypes can affect social judgment and behavior\nin relative independence from subjects’ reported attitudes\n(Devine 1989; Devine & Monteith 1999; Dovidio & Gaertner 2004;\nGreenwald & Banaji 1995; Banaji et al. 1993). These studies were\ninfluenced by theories of implicit memory (e.g., Jacoby & Dallas\n1981; Schacter 1987), leading to Greenwald & Banaji’s\noriginal definition of “implicit attitudes” as \n\nintrospectively unidentified (or inaccurately identified) traces of\npast experience that mediate favorable or unfavorable feeling, thought,\nor action toward social objects. (1995: 8) \n\nThe guiding idea here, as Dovidio and Gaertner (1986) put it, is\nthat in the modern world prejudice has been “driven\nunderground,” that is, out of conscious awareness. This idea has\nled to the common view that what makes a bias implicit is that a person\nis unwilling or unable to report it. Recent findings have challenged\nthis view, however (§3.1) \n\nWhat a person says is not necessarily a good representation of the\nwhole of what she feels and thinks, nor of how she will behave.\nArguably, the central advance of research on implicit social cognition\nis the ability to assess people’s thoughts, feelings, and\nbehavior without having to ask them directly, “what do you\nthink/feel about X?” or “what would you do in X\nsituation?”  \n\nImplicit measures, then, might be thought of as instruments that\nassess people’s thoughts, feelings, and behavior indirectly, that\nis, without relying on “self-report.” This is too quick,\nhowever. For example, a survey that asks “what do you think of\nblack people” is explicit and direct, in the sense that the\nsubject’s judgment is both explicitly reported and the subject is\nbeing directly asked about the topic of interest to the researchers.\nHowever, a survey that asks “what do you think about\nDarnell” (i.e., a person with a stereotypically black name) is\nexplicit and indirect, because the subject’s judgment is\nexplicitly reported but the content of what is being judged (i.e., the\nsubject’s attitudes toward race) is inferred by the researcher.\nThe distinction between direct and indirect measures is also relative\nrather than absolute. Even in some direct measures, such as personality\ninventories, subjects may not be completely aware of what is being\nstudied. \n\nIn the literature, “implicit” is used to refer to at\nleast four distinct things (Gawronski & Brannon 2017): (1) a\ndistinctive psychological construct, such as an “implicit\nattitude,” which is assessed by a variety of instruments; (2) a\nfamily of instruments, called “implicit measures,” that\nassess people’s thoughts and feelings in a specific way (e.g., in\na way that minimizes subjects’ reliance on introspection and\ntheir ability to respond strategically); (3) a set of cognitive and\naffective processes—“implicit processes”—that\naffect responses on a variety of measures; and (4) a kind of evaluative\nbehavior—e.g., a categorization judgment—elicited by\nspecific circumstances, such as cognitive load. In this entry, I\nwill use “implicit” in the senses of (2) and (4), unless\notherwise noted. One virtue of this approach is that it allows one to\nremain agnostic about the nature of the phenomena implicit\nmeasures\n assess.[4]\nConsider Frank again.\nHis implicit gender bias may be assessed by several different\ninstruments, such as sequential priming or the “Implicit\nAssociation Test” (IAT; Greenwald et al. 1998). The IAT—the\nmost well-known implicit test—is a reaction time measure. In a\nstandard IAT, the subject attempts to sort words or pictures into\ncategories as fast as possible while making as few errors as possible.\nIn the images below, the correct answers would be left, right, left,\nright. Image 1 Image 2 Image 3 Image 4 All images are copyright of \nProject Implicit\nand reproduced here with permission. \n\nAn IAT score is computed by comparing speed and error rates on the\n“blocks” (or trials) in which the pairing of concepts is\nconsistent with common stereotypes (images 1 and 3) to the blocks in\nwhich the pairing of the concepts is inconsistent with common\nstereotypes (images 2 and 4). If he is typical of most subjects, Frank\nwill be faster and make fewer errors on stereotype-consistent trials\nthan stereotype-inconsistent trials. While this\n“gender-career” IAT pairs concepts (e.g.,\n“male” and “career”), other IATs, such as the\n“race-evaluation” IAT, pair a concept to an evaluation\n(e.g., “black” and “bad”). Other IATs assess\nbody image, age, sexual orientation, and so on. As of 2019,\napproximately 26 million IATs have been taken (although it is unclear\nif this number represents 26 million unique participants or 26 million\ntests taken or started; Lai p.c.). One review (Nosek et al. 2007),\nwhich tested over 700,000 subjects on the race-evaluation IAT, found\nthat over 70% of white participants more easily associated black faces\nwith negative words (e.g., war, bad) and white faces with positive\nwords (e.g., peace, good). The researchers consider this an implicit\npreference for white faces over black\n faces.[5] \n\nAlthough the IAT remains the most popular implicit measure, it is\nfar from the only one. Other prominent implicit measures, many of which\nare derivations of sequential priming, are semantic priming (Banaji\n& Hardin 1996) and the Affect Misattribution Procedure (AMP; Payne\net al. 2005). Also, a “second generation” of\ncategorization-based measures (like the IAT) has been developed. For\nexample, the Go/No-go Association Task (GNAT; Nosek & Banaji 2001)\npresents subjects with one target object rather than two in order to\ndetermine whether preferences or aversions are primarily responsible\nfor scores on the standard IAT (i.e., the ease of pairing good words\nwith white faces and bad words with black faces, or the difficulty of\npairing good words with black faces and bad words with white faces;\nBrewer 1999). \n\nA notable advance in the psychometrics of implicit bias has been the\nadvent of multinomial (or formal process) models, which identify\ndistinct processes contributing to performance on implicit measures.\nFor example, elderly people tend to show greater bias on the\nrace-evaluation IAT compared with younger people, but this may be due\nto their having stronger preferences for whites or having weaker\ncontrol over their biased responding (Nosek et al. 2011). Multinomial\nmodels, like the Quadruple Process Model (Conrey et al. 2005), are used\nto tease apart these possibilities. The Quad model identifies four\ndistinct processes that contribute to responses: (1) the automatic\nactivation of an association; (2) the subject’s ability to\ndetermine a correct response (i.e., a response that reflects\none’s subjective assessment of truth); (3) the ability to\noverride automatic associations; and (4) general response biases (e.g.,\nfavoring right-handed responses). Multinomial modeling has made\nclear that implicit measures are not “process pure,” i.e.,\nthey do not tap into a single unified psychological process. \n\nWhile there is not consensus about what implicit measures capture\n(§2), it is clear that they provide at least three kinds of\ninformation (Gawronski & Hahn 2019). The first is information about\ndissociation with more explicit, direct measures. Correlations between\nimplicit and explicit measures tend to be relatively low (r =\n.2–.25; Hofmann et al. 2005; Cameron et al. 2012), although these\nrelations are significantly affected by methodological practices, such\nas comparing non-corresponding implicit and explicit measures (e.g., an\nimplicit measure of gender stereotypes and an explicit “feelings\nthermometer” toward women). It is important to note the breadth\nof research in this vein; dissociations between implicit and explicit\nmeasures are found in the study of personality (e.g., Vianello et al.\n2010), attitudes toward alcohol (e.g., de Houwer et al. 2004), phobias\n(Teachman & Woody 2003), and more. Second, implicit measures can be\nused as dependent variables in experiments. Theories about the\nformation and change of attitudes, for example, have focused on\ndifferential effects of manipulations, such as counter-attitudinal\ninformation, on implicit and explicit measures (e.g., Gawronski &\nBodenhausen 2006; Petty 2006). Third, implicit measures are used to\npredict behavior. Philosophers have been especially interested in the\nrelationship between implicit bias and discriminatory behavior,\nparticularly when the discriminatory behavior conflicts with a\nperson’s reported beliefs (as in the “Frank” case\nabove). Studies report relationships between implicit bias and behavior\nin a huge variety of social contexts, from hiring to policing to\nmedicine to teaching and more (for an incomplete list see Table 1 in\nJost et al. 2009). There is also voluminous, varied, and on-going\ndiscussion about how well implicit measures predict behavior, along\nwith several related critical assessments of the information implicit\nmeasures provide (§5). \n\n“Implicit bias” is a term of art, used in a variety of\nways. In this entry, the term is used to refer to the family of\nevaluative judgments and behavior assessed by implicit measures (e.g.,\ncategorization judgments on an IAT). These measures mimic some\nrelevant aspects of judgment and decision-making outside the lab\n(e.g., time pressure). But what do these measures measure? With some\nblurry boundaries, philosophical and psychological theories can be\ndivided into five groups. Implicit measures might provide information\nabout attitudes (§2.1), implicit processes (§2.2), beliefs\n(§2.3), traits (§2.4), or situations (§2.5).  \n\nThe idea that people’s attitudes are the cause of implicit\nbias is pervasive. The term “attitudes” tends to be used\ndifferently in psychology and philosophy, however. In psychology,\nattitudes are akin to preferences (i.e., likings and dislikings); the\nterm does not refer to propositional states per se (i.e., mental states\nthat are thought to bear a relationship to a proposition), as it does\nin philosophy. Most attitudinal theories of implicit bias use the term\nin the psychologist’s sense, although variations will be noted\nbelow. \n\n2.1.1 Dual Attitudes in Psychology \n\nEarly and influential theories posited that people hold two distinct\nattitudes in mind toward the same object, one implicit and the other\nexplicit (Greenwald & Banaji 1995; Wilson et al. 2000).\n“Explicit attitudes” are commonly identified with verbally\nreported attitudes, in this vein, while “implicit\nattitudes” are those that a person is unwilling or unable to\nreport. Evidence for theories of dual attitudes stems largely from two\nsources. The first are anecdotal reports of surprise and consternation\nthat people sometimes express after being informed of their performance\non an implicit measure (e.g., Banaji 2011; Krickel 2018). These\nexperiences suggest that people discover their putative implicit\nattitudes by taking the relevant tests, just like one learns about\none’s cholesterol by taking the relevant tests. The second source\nof evidence for dual-attitude views are dissociations between implicit\nand explicit measures (§1.2). These suggest that implicit and\nexplicit measures may be tapping into distinct representations of the\nsame attitude-object (e.g., “the elderly”). \n\nA central challenge for theories of this sort is whether people\ntruly are unaware of their implicit biases, and if so, in what way\n(e.g., if people are unaware of the source, content, or behavioral\neffects of their attitudes; §3.1). There may be reasons to posit\nunconscious representations in the human mind independent of whether\npeople are or are not aware of their implicit biases, of course. But if\npeople are aware of their implicit biases, then implicit measures are\nmost likely not assessing unconscious “dual”\nattitudes.  \n\n2.1.2 Dual Attitudes in Philosophy \n\nSome philosophers have proposed that implicit measures assess a\ndistinct kind of “action-oriented” attitude, which is\ndifferent from ordinary attitudes, but not necessarily in terms of\nbeing unconscious. The core idea here is that implicit attitudes link\nrepresentations with behavioral\n impulses.[6]\n Gendler’s (2008a,b, 2011, 2012) account\nof “alief,” a sui generis mental state comprised\nof tightly woven co-activating representational (R), affective\n(A), and behavioral (B) components, is emblematic of\nthis approach. Gendler argues that the R-A-B components of\nalief are “bundled” together or “cluster” in\nsuch a way that when an implicitly biased person sees a black face in a\nparticular context, for example, the agent’s representation will\nautomatically activate particular feelings and behaviors (i.e., an\nR–A–B cluster). This is in contrast to the\n“combinatoric” nature of ordinary beliefs and desires, that\nis, that any belief could, in principle, be combined with any desire.\nSo while the belief that “that is a black man” is not fixed\nto any particular feelings or behavior, an alief will have content\nlike, “Black man! Scary! Avoid!” \n\n “To have an alief”, Gendler writes, is \n\nAccording to Gendler, aliefs explain a wide array of otherwise\npuzzling cases of belief-behavior discordance, including not only\nimplicit bias, but also phobias, fictional emotions, and bad habits\n(2008b: 554). In fact, Gendler suggests (2008a: 663) that aliefs are\ncausally responsible for much of the “moment-by-moment\nmanagement” of human behavior, whether that behavior is\nbelief-concordant or not. \n\nCritics have raised a number of concerns about this approach, in\nparticular whether putative aliefs form a unified kind (Egan 2011;\nCurrie & Ichino 2012; Doggett 2012; Nagel 2012; Mandelbaum 2013).\nOthers have proposed alternate conceptions of action-oriented dual\nattitudes. Brownstein and Madva (2012a,b; see also Madva and Brownstein\n2018 and Brownstein 2018), for example, propose that implicit attitudes\nare comprised of F-T-B-A components: the perception of a\nsalient Feature triggers automatic low-level feelings of\naffective Tension, which are associated in turn with specific\nBehavioral responses, which either do or do not\nAlleviate the agent’s felt tension. This approach shares\nwith Gendler’s the idea that aliefs/implicit attitudes differ in\nkind from beliefs/explicit attitudes. Moreover, the difference between\nthese putative kinds of states is not necessarily the agent’s\nintrospective access to them. Gendler proposes that while paradigmatic\nbeliefs update when the agent requires new relevant information,\nparadigmatic aliefs don’t. In contrast, Brownstein and Madva\nargue that implicit attitudes do update in the face of new\ninformation—this is the feed-forward function of\n“alleviation”—and thus can automatically yet flexibly\nmodify and improve over time. Thus, for Brownstein and Madva, implicit\nattitudes are implicated not only in bias and prejudice, but also in\nskillful, intelligent, and even ethical\n action.[7]\nBut while implicit attitudes aren’t ballistic,\ninformation-insensitive reflexes, on Brownstein and Madva’s view,\nthey also don’t update in the same way as ordinary attitudes.\nBrownstein and Madva draw the distinction in terms of two key features.\nFirst, implicit attitudes are paradigmatically insensitive to the\nlogical form in which information is presented. For example, subjects\nhave been shown to form equivalent implicit attitudes on the basis of\ninformation and the negation of that information (e.g., Gawronski et\nal. 2008). Second, implicit attitudes fail to respond to the semantic\ncontents of other mental states in a systematic way; they appear to be\n“inferentially impoverished.” For example, implicit\nattitudes are implicated in behaviors for which it is difficult to give\nan inferential explanation (e.g., Dovidio et al. 1997) and implicit\nattitudes change in response to irrelevant information (e.g., Gregg et\nal. 2006; Han et al. 2006). Levy (2012, 2015)—who argues that\nimplicit attitudes are “patchy endorsements”—makes\nsimilar claims about the ways in which implicit attitudes do and do not\nupdate, although he does not argue that these kinds of states are\n“action-oriented” in the way that Gendler and Brownstein\nand Madva do. Debate about these findings is ongoing (§2.3). \n\n2.1.3 Single Attitudes \n\nSome theories posit the existence of a singular representation of\nattitude-objects. According to MODE (“Motivation and Opportunity\nas Determinants”; Fazio 1990; Fazio & Towles-Schwen 1999;\nOlson & Fazio 2009) and the related MCM (“Meta-Cognitive\nModel”; Petty 2006; Petty et al. 2007), attitudes are\nassociations between objects and “evaluative knowledge” of\nthose objects. MODE posits one singular representation underlying the\nbehavioral effects measured by implicit and explicit tests. Thus, MODE\ndenies the distinction between implicit and explicit attitudes. The\ndifference between implicit and explicit measures, then, reflects a\ndifference in the control that subjects have over the measured\nbehavior. Control is understood in terms of motivation and opportunity\nto deliberate. When an agent has low motivation or opportunity to\nengage in deliberative thought, her automatically activated\nattitudes—which might be thought of as her “true”\nattitudes—will guide her behavior and judgment. Implicit measures\nmanufacture this situation (of low control due to low motivation and/or\nopportunity to deliberate). Explicit measures, by contrast, increase\nnon-attitudinal contributions to test performance. MODE therefore\nprovides empirically-testable predictions about the conditions under\nwhich a person’s performance on implicit and explicit measures\nwill converge and diverge, as well as predictions about the conditions\nunder which implicit and explicit measures will and will not predict\nbehavior (see Gawronski & Brannon 2017 for review).  \n\nInfluenced by dual process theories of mind, RIM\n(“Reflective-Impulsive Model”; Strack & Deutsche 2004)\nand APE (“Associative-Propositional Evaluation”; Gawronski\n& Bodenhausen 2006, 2011) suggest that implicit measures assess\ndistinctive cognitive processes. The central distinction at the heart\nof both RIM and APE is between “associative” and\n“propositional” processes. Associative processes are said\nto underlie an impulsive system that functions according to classic\nassociationist principles of similarity and contiguity. Implicit\nmeasures are thought of as assessing the momentary accessibility of\nelements or nodes of a network of associations. This network produces\nspontaneous evaluative responses to stimuli. Propositional processes,\non the other hand, underlie a reflective system that validates the\ninformation provided by activated associations. Explicit measures are\nthought to capture this process of validation, which is said to operate\naccording to agents’ syllogistic reasoning and judgments of\nlogical consistency. In sum, the key distinction between associative\nand propositional processes according to RIM and APE is that\npropositional processing alone depends on an agent’s assessment\nof the truth of a given\n representation.[8]\nAPE in particular aims to explain the interactions\nbetween and mutual influences of associative and propositional\nprocesses in judgment and behavior. \n\nRIM and APE bear resemblance to the dual attitudes theories in\nphilosophy discussed above. Indeed, Bodenhausen & Gawronski (2014:\n957) write that the “distinction between associative and\npropositional evaluations is analogous to the distinction between\n‘alief’ and belief in recent philosophy of\nepistemology.” It is important to keep in mind, however, that RIM\nand APE are not attitudinal theories. APE, for example, posits two\ndistinct kinds of process—associative and propositional\nprocesses—that give rise to two kinds of evaluative responses to\nstimuli—implicit and explicit. It does not posit the existence of\ntwo distinct attitudes or two distinct co-existing representations of\nthe same entity. It is also important to note that the distinction\nbetween associative and propositional processes can be understood in at\nleast three distinct senses: as applying to the way in which\ninformation is learned, stored, or expressed (Gawronski et al. 2017).\nAt present, evidence is mixed for dissociation between associative and\npropositional processing in the learning and storage of information,\nwhile it is stronger for dissociation in the behavioral expression of\nstored information (Brownstein et al. 2019).  \n\nSome have argued that familiar notions of belief, desire, and\npretense can in fact explain what neologisms like “implicit\nattitudes” are meant to elucidate (Egan 2011; Kwong 2012;\nMandelbaum 2013). Most defend some version of what Schwitzgebel (2010)\ncalls Contradictory Belief (Egan 2008, 2011; Huebner 2009; Gertler\n2011; Huddleston 2012; Muller & Bashour 2011; Mandelbaum 2013,\n2014,\n forthcoming).[9]\nDrawing\nupon theories of the “fragmentation” of the mind (Lewis\n1982; Stalnaker 1984), Contradictory Belief holds that implicit and\nexplicit measures both reflect what a person believes, and that these\ndifferent sets of beliefs may be causally responsible for different\nbehavior in different contexts (Egan 2008). In short, if a person\nbehaves in a manner consistent with the belief that black men are\ndangerous, it is because they believe that black men are dangerous\n(notwithstanding what they say they believe). \n\nIn the psychological literature, De Houwer and colleagues defend a\nview that can be thought of as supporting Contradictory Belief\n(Mitchell et al. 2009; Hughes et al. 2011; De Houwer 2014). On this\nmodel,\n propositions[10]\nhave\nthree defining features: (1) propositions are statements about the\nworld that specify the nature of the relation between concepts (e.g.,\n“I am good” and “I want to be good” are\npropositions that involve the same two concepts—“me”\nand “good”—but differ in the way that the concepts\nare related); (2) propositions can be formed rapidly on the basis of\ninstructions or inferences; and (3) subjects are conscious of\npropositions (De Houwer 2014). On the basis of data consistent with\nthese criteria—for example, responses on implicit measures are\naffected by one-shot instruction—De Houwer (2014) argues that\nimplicit measures capture propositional states (i.e.,\n beliefs).[11]\nThis claim represents an\napplication of Mitchell and colleagues’ (2009) broader argument\nthat all learning is propositional (i.e., there is no case in\nwhich learning is the result of the automatic associative linking of\nmental representations). One reason philosophers have been interested\nin this view is due to its resonance with classic debates in the\nphilosophy of mind between empiricists and rationalists, behaviorists\nand cognitivists, and so on. \n\nAnother belief-based approach argues that implicit biases should be\nunderstood as cognitive “schemas.” Schemas are clusters of\nculturally shared concepts and beliefs. More precisely, schemas are\nabstract knowledge structures that specify the defining features and\nattributes of a target (Fiske & Linville 1980). The term\n“mother”, for example, invokes a schema that attributes a\ncollection of attributes to the person so labelled (Haslanger 2015). On\nsome accounts, schemas are “coldly” cognitive (Valian\n2005), and so in the psychologist’s sense, they are not\nattitudes. Rather, schemas are tools for social categorization, and\nwhile schemas may help to organize and interpret feelings and\nmotivations, they are themselves affectless. One advantage of focusing\non schemas is that doing so emphasizes that implicit bias is not a\nmatter of straightforward antipathy toward members of socially\nstigmatized groups. \n\nA separate version of the generic belief approach stems from recent\nwork in the philosophy of language. This approach focuses on\nstereotypes that involve generalizing extreme or horrific behavior from\na few individuals to groups. Such generalizations, such as “pit\nbulls maul children” or “Muslims are terrorists”, can\nbe thought of as a particular kind of generic statement, which Leslie\n(2017) calls a “striking property generic”. This subclass\nof generics is defined by having predicates that express properties\nthat people typically have a strong interest in avoiding. Building on\nearlier work on the cognitive structure and semantics of generics\n(Leslie 2007, 2008), Leslie notes a particularly insidious feature of\nsocial stereotyping: even if just a few members of what is perceived to\nbe an essential kind (e.g., pit bulls, Muslims) exhibit a harmful or\ndangerous property, then a generic that attributes the property to the\nkind likely will be judged to be true. This is only the case with\nstriking properties, however. As Leslie (2017) points out, it\ntakes far fewer instances of murder for one to be considered a murderer\nthan it does instances of anxiety to be considered a worrier. Striking\nproperty generics may thus illuminate some social stereotypes (e.g.,\n“black men are rapists”) better than others (e.g.,\n“black men are athletic”). Beeghly (2014), however,\nconstrues generics as expressions of cognitive schemas, which may\nbroaden the scope of explanation by way of generic statements. In all\nof these cases, generics involve an array of doxastic properties.\nGenerics involve inferences to dispositions, for example (Leslie 2017).\nThat is, generic statements about striking properties will usually be\njudged true if and only if some members of the kind possess the\nproperty and other members of the kind are judged to be disposed to\npossess it. \n\nThe most explicit defense of Contradictory Belief has been via a\ntheory of “Spinozan Belief Fixation” (SBF; Gilbert 1991;\nEgan 2008, 2011; Huebner 2009; Mandelbaum 2011, 2013, 2014, 2016).\nProponents of SBF are inspired by Spinoza’s rejection of the\nconcept of the will as a cause of free action (Huebner 2009: 68), an\nidea which is embodied in what they call the theory of “Cartesian\nBelief Fixation” (CBF). CBF holds that ordinary agents are\ncapable of evaluating the truth of an idea (or representation, or\nproposition) delivered to the mind (via sensation or imagination)\nbefore believing or disbelieving it. Agents can choose to believe or\ndisbelieve P, according to CBF, in other words, via\ndeliberation or judgment. SBF, on the other hand, holds that as soon as\nan idea is presented to the mind, it is believed. Beliefs on this view\nare understood to be unconscious propositional attitudes that are\nformed automatically as soon as an agent registers or tokens their\ncontent. For example, one cannot entertain or consider or imagine the\nproposition that “dogs are made out of paper” without\nimmediately and unavoidably believing that dogs are made out of paper,\naccording to SBF (Mandelbaum 2014). More pointedly, one cannot\nentertain or imagine the stereotype that “women are bad at\nmath” without believing that women are bad at math. As Mandelbaum\n(2014) puts it, the automaticity of believing according to SBF explains\nwhy people are likely to have many contradictory beliefs; in order to\n reject P, one must already believe P.[12] \n\nSBF is strongly revisionist with respect to the ordinary concept of\nbelief (but see Helton (forthcoming) for a similarly spirited but less\nrevisionist \n view).[13]\nNotwithstanding this, the central line of debate about SBF’s\naccount of implicit bias—as well as about belief-based accounts\nof implicit social cognition generally—focuses on the fact that\npeople’s performance on implicit measures is sometimes\nunresponsive to the kinds of reinforcement learning based interventions\nthat ought to affect associative processes and/or states; meanwhile,\nperformance on implicit measures sometimes appears to be responsive to\nthe kinds of logical and persuasion based interventions thought to\naffect doxastic states\n(e.g., de\nHouwer 2009,\n2014; Hu et al. 2017; Mann & Ferguson 2017; Van Dessel et al. 2018;\nfor additional discussion see Mandelbaum 2013, 2016; Gawronski et al.\n2017; Brownstein et al. 2019). Caution is needed in drawing strong\nconclusions about cognitive structure from these behavioral data,\nhowever (Levy 2015; Madva 2016c; Byrd forthcoming; Brownstein et al 2019). As\nnoted above (§1.2), implicit measures are not process-pure.\nModeling technique for disentangling the multiple causal contributions\nto performance on implicit measures may help to move these debates\nforward (e.g., Conrey et al. 2005; Hütter & Sweldens\n2018). \n\nAs is the case with terms like “attitude” and\n“propositional,” psychologists and philosophers tend to use\nthe term “trait” in different ways. In psychology,\ntrait-like constructs are stable over time and across situations. If\nyou have always disliked eating pork, and never eat it no matter the\ncontext, then your feelings toward pork are trait-like. If you\nsometimes decline to eat pork but sometimes indulge, depending on the\ncompany or your mood, then your feelings are more\n“state”-like. In the psychologist’s sense,\nsignificant evidence suggests that implicit bias is more state-like\nthan trait-like. Multiple longitudinal studies have found that\nindividuals’ scores on implicit measures vary significantly over\ndays, weeks, and months, much more so than individuals’ scores on\ncorresponding explicit measures (Cooley & Payne 2017; Cunningham et\nal. 2001; Devine et al. 2012; Gawronski et al. 2017). Of course, the\nsignificance of this depends on one’s theory of implicit bias. If\nimplicit measures are theorized to capture spontaneous affective\nreactions (as APE suggests; §2.2), then contextual and temporal\nvariability in performance should be predicted (because, for\nexample, one’s immediate reactions to images of women leaders\nwill likely be different after watching a documentary about Ruth Bader\nGinsburg than after watching Clueless). However, if implicit\nmeasures are meant to “diagnose” stable features of\nindividuals like political party affiliation, then far less variation\nshould be expected. Another possibility is that measurement error\ncontributes significantly to the instability of scores on implicit\nmeasures. The fact that methodological improvements have in some cases\nimproved the temporal stability of participants’ performance\nsupports this idea (e.g., Cooley and Payne 2017). \n\nIn philosophy, “trait” is used more often in the context\nof anti-representationalist, dispositional theories of mind. While\nrepresentationalists define concepts like “belief” in terms\nof internal, representational structures of the mind, dispositionalists\ndefine concepts like “belief” in terms of tendencies to\nbehave in certain ways (and perhaps also to feel and think in certain\nways). Building upon Ryle (1949/2009), Schwitzgebel (2006/2010, 2010,\n2013) advances a dispositional theory of attitudes (in the\nphilosophical sense, that is, a theory that claims that beliefs,\ndesires, hopes, etc. are dispositions). On his view, attitudes have a\nbroad (or “multitrack”) profile, including dispositions to\nfeel, think, and speak in specific ways. The dispositional profile of a\ngiven attitude is determined by the folk-psychological stereotype for\nhaving that attitude, not by what’s inside the agent’s\nmetaphoric “belief box.” For example, to establish that\nJordan believes that women make good philosophers, one would look to\nwhat Jordan says about women philosophers, to her judgments about which\nphilosophers are good and which aren’t, to her hiring practices,\nher gut feelings around men and women philosophers, etc. Agents with\nimplicit biases pose an interesting challenge to dispositionalists,\nsince these agents often match only part of the relevant\nfolk-psychological stereotypes. For example, Jordan might say that she\nbelieves that women make good philosophers but fail to read any women\nphilosophers (or, recall Frank;\n §1).\nOn Schwitzgebel’s\n“gradualist dispositionalism,” Jordan and Frank would be\n“in-between believers,” agents who partly match the\nrelevant folk-psychological stereotypes for the attitudes in\nquestion. \n\nA related trait-based approach treats the results of indirect\nmeasures as reflective of elements of attitudes, rather than\nas assessing attitudes or biases themselves (Machery 2016, 2017). On\nMachery’s view, attitudes (in the psychologist’s sense,\nthat is, preferences) are dispositions and are comprised of various\nbases, including feelings, associations, behavioral impulses, and\npropositional states like beliefs. (In contrast to Schwitzgebel,\nMachery holds a representationalist view of belief, but a\ndispositionalist view of attitudes.) To have a racist attitude, on this\npicture, is to be disposed to display the relevant mix of these bases,\nthat is, to display the feelings, associations, etc. that together\ncomprise the attitude. Implicit measures, then, are said to capture one\nof the psychological bases (e.g., her associations between concepts) of\nthe agent’s overall attitude. Explicit questionnaire measures\ncapture another psy­chological basis of the agent’s attitude,\nbehavioral measures yet another basis, and so on. Implicit measures,\nthen, do not assess “implicit attitudes,” and indeed,\nMachery denies that attitudes divide into implicit and explicit kinds.\nRather, implicit measures quantify elements of attitudes. In part, this\nproposal is meant to explain some of the key psychometric properties of\nimplicit measures, such as their instability over time and the fact\nthat some implicit measures correlate poorly with each other (§5).\nThese findings are consistent with the notion that different implicit\nmeasures quantify different psychological bases of attitudes, Machery\nargues. \n\nOne advantage of thinking of implicit biases as traits is that it is\nconsistent with the way in which personality attributions readily admit\nof vague cases. Just as we might say that Frank is partly\nagreeable if he extols the virtues of compassion yet sometimes\ntreats strangers rudely, we might say that Frank is partly\nprejudiced. Dispositional theories capture this intuition. On the\nother hand, trait-based theories of implicit bias face long-standing\nchallenges to dispositionalism in the philosophy of mind. One such\nchallenge is that traits are explanatory as generalizations, not as\ntoken causes of judgment and behavior\n(Carruthers 2013). Another is the specter of circularity arising from the simultaneous use of an agent’s behavior to both define her disposition and to point to what her disposition predicts (Bandura,\n1971; Cervone et al. 2015; Mischel 1968; Payne et al. 2017). In both\ncases, the question for dispositionalism is whether it truly helps to\nexplain the data, or merely repackages outwardly observed\npatterns in new terms. \n\nThe most common way people think and write about implicit biases is\nas attributes of persons. Another possibility, though, is that\nimplicit biases are attributes of situations. Although\npsychologists have been debating person-based and situation-based\nexplanations throughout the history of implicit social cognition\nresearch (Payne & Gawronski 2010; Murphy & Walton 2013; Murphy\net al. 2018), the situationist approach has gained steam due to Payne\nand colleagues’ (2017) “bias of crowds” model.\nBorrowing from the concept of the “wisdom of crowds,” this\napproach suggests that differences between situations explains the\nvariance of scores on implicit measures, rather than differences\nbetween individuals. A helpful metaphor used by Payne and colleagues is\ndoing “the wave” at a baseball game. Where a person is\nsitting in the bleachers, in combination with where the wave is at a\ngiven time, is likely to outperform most individual differences (e.g.,\nimplicit or explicit feelings about the wave) in predicting whether a\nperson sits or stands. Likewise, what predicts implicit bias are\nfeatures of people’s situations, not features of their\npersonality. For example, living in a highly residentially segregated\nneighborhood might be expected to outpredict racial implicit bias\ncompared to individual-level factors, such as beliefs and\npersonality. \n\nThe bias of crowds model is aimed at making sense of five features\nof implicit bias which are otherwise challenging to make sense of\ntogether, namely: (1) average group-level scores of implicit bias are\nvery robust and stable; (2) children’s average scores of implicit\nbias are nearly identical to adults’ average scores; (3)\naggregate levels of implicit bias at the population level (e.g.,\nregions, states, and countries) are both highly stable and strongly\nassociated with discriminatory outcomes and group-based disparities;\nyet, (4) individual differences in implicit bias have small-to-medium\nzero-order correlations with discriminatory behavior; and (5)\nindividual test-retest reliability is low over weeks and months. (See\nPayne et al. 2017 for references.) Another advantage of the bias of\ncrowds model is that it coalesces well with calls in philosophy for\nfocusing more on “structural” or “systemic”\nbias, rather than on the biases in the heads of individuals\n(§5). \n\nOne challenge for the bias of crowds model is explaining how\nsystemic biases interact with and affect the minds of individuals,\nhowever. Payne and colleagues appeal to the idea of the\n“accessibility” of concepts in individuals’ minds,\nthat is, the “likelihood that a thought, evaluation, stereotype,\ntrait, or other piece of information” becomes activated and\npoised to influence behavior. The lion’s share of evidence, they\nargue, suggests that the concepts related to implicit bias are\nactivated due to situational causes. This may be, but it does not\nexplain (a) how situations activate concepts in individuals’\nminds (Payne and colleagues are explicitly agnostic about the format of\ncognitive representations that underlie implicit bias); and (b) how\nsituational factors interact with individual factors to give rise to\nbiased actions (Gawronski & Bodenhausen 2017; Brownstein et al.\n2019). \n\nPhilosophical work on the epistemology of implicit bias has focused\non three related\n questions.[14]\nFirst, do we have knowledge of our own implicit biases, and if so, how?\nSecond, do the emerging data on implicit bias demand that we become\nskeptics about our perceptual beliefs or our overall status as\nepistemic agents? And third, are we faced with a dilemma between our\nepistemic and ethical values due to the pervasive nature of implicit\nbias? \n\nImplicit bias is typically thought of as unconscious (§2.1.1),\nbut what exactly does this mean? There are several possibilities: there\nmight be no phenomenology associated with the relevant mental states or\ndispositions; agents might be unaware of the content of the\nrepresentations underlying their performance on implicit measures, or\nthey might be unaware of the source of their implicit biases or the\neffects those biases have on their behavior; agents might be unaware of\nthe relations between their relevant states (e.g., that their implicit\nand explicit evaluations of a given target conflict); and agents might\nhave different modes of awareness of their own minds (e.g.,\n“access” vs. “phenomenal” awareness; Block\n1995). Gawronski and colleagues (2006) argue that agents typically lack\n“source” and “impact” awareness of their\nimplicit biases, but typically have\n“content”\n awareness.[15]\nEvidence for\ncontent awareness stems from “bogus pipeline” experiments\n(e.g., Nier 2005) in which participants are led to believe that\ninaccurate self-reports will be detected by the experimenter. In these\nexperiments, participants’ scores on implicit and explicit\nmeasures come to be more closely correlated, suggesting that\nparticipants are aware of the content of those judgments detected by\nimplicit measures and shift their reports when they believe that the\nexperimenter will notice discrepancies. Additional evidence for content\nawareness is found in studies in which experimenters bring implicit\nmeasures and self-reports into conceptual alignment (e.g., Banse et al.\n2001) and studies in which agents are asked to predict their own\nimplicit biases (Hahn et al. 2014). Indeed, Hahn and colleagues (2014)\nand Hahn and Gawronski (2019) have found that people are good at\npredicting their own IAT scores regardless of how the test is\ndescribed, how much experience they have taking the test, and how much\nexplanation they are given about the test before taking it. Moreover,\npeople have unique insight into how they will do on the test, insight\nwhich is not explained by their beliefs about how people in general\nwill perform. \n\nHahn and colleagues’ data do not determine, however, whether\nagents come to be aware of the content of their implicit biases through\nintrospection, by drawing inferences from their own behavior, or from\nsome other source (see Berger forthcoming for discussion). This is important\nfor determining whether the awareness agents have of their implicit\nbiases constitutes self-knowledge. If our awareness of the content of\nour implicit biases derives from inferences we make based on (for\nexample) our behavior, then the question is whether these inferences\nare justified, assuming knowledge entails justified true belief. Some\nhave suggested that the facts about implicit bias warrant a\n“global” skepticism toward our capacities as epistemic\nagents (Saul 2012; see\n §3.2.2).\nIf this is\nright, then we ought to worry that our inferences about the content of\nour implicit biases, from all the ways we behave on a day-to-day basis,\nare likely to be unjustified. Others, however, have argued that people\nare typically very good interpreters of their own minds (e.g.,\nCarruthers 2009; Levy 2012), in which case it may be more likely that\nour inferences about the content of our implicit biases are\nwell-justified. But whether the inferences we make about our own minds\nare well-justified would be moot if it were shown that we have direct\nintrospective access to our biases. \n\nOne sort of skeptical worry stems from research on the effects of\nimplicit bias on perception\n (§3.2.1).\nThis\nleads to a worry about the status of our perceptual beliefs. A second\nkind of skeptical worry focuses on what implicit bias may tell us about\nour capacities as epistemic agents in general\n (§3.2.2). \n\nCompared with participants who were first shown pictures of white\nfaces, those who were primed with black faces in Payne (2001) were\nfaster to identify pictures of guns as guns and were more likely to\nmisidentify pictures of tools as guns. This finding has been directly\nand conceptually replicated (e.g., Payne et al. 2002; Conrey et al.\n2005) and is an instance of a broader set of findings about the effects\nof attitudes and beliefs on perception (e.g., Barrick et al. 2002;\nProffitt 2006). Payne’s findings are chilling particularly in\nlight of police shootings of unarmed black men in recent years, such as\nAmadou Diallo and Oscar Grant. The findings suggest that agents’\nimplicit associations between “black men” and\n“guns” may affect their judgment and behavior by affecting\nwhat they see. In addition to the moral implications, this may be cause\nfor a particular kind of epistemic concern. As Siegel (2012, 2017,\nforthcoming) puts it, the worry is that implicit bias introduces a\ncircular structure into belief formation. If an agent believes that\nblack men are more likely than white men to have or use guns, and this\nbelief causes the agent to more readily see ambiguous objects in the\nhands of black men as guns, then when the agent relies upon visual\nperception as evidence to confirm her beliefs, she will have moved in a\nvicious circle. \n\nWhether implicit biases are cause for this sort of epistemic concern\ndepends on what sort of causal influence social attitudes have on\nvisual perception. Payne’s weapons bias findings would be a case\nof “cognitive penetration” if the black primes make the\nimages of tools look like images of guns, via an effect on perceptual\nexperience itself (Siegel 2012, 2017, forthcoming). This would\ncertainly introduce a circular structure in belief formation. Other\nscenarios raise the possibility of illicit belief formation without\ngenuine cognitive penetration. Consider what Siegel calls\n“perceptual bypass”: the black primes do not cause the\ntools to look like guns (i.e., the prime does not cause a change in\nperceptual experience), yet some state in the agent, such as a\nheightened state of anxiety, is affected by the black prime and causes\nthe agent to make a classification error. This will count as a case of\nillicit belief formation inasmuch as the agent’s social attitudes\ncause her to be insensitive to her visual stimuli in a way that\nconfirms her antecedent attitudes (Siegel 2012). Other scenarios might\nallay the worry about illicit belief formation. For example, what\nSiegel calls “disowned behavior” proposes the same route to\nthe classification error as “perceptual bypass,” except\nthat the agent antecedently regards her error as an error. Empirical\nevidence can help to sort through these possibilities, though perhaps\nnot settle between them conclusively (e.g., Correll et al. 2015). \n\nA broader worry is that research on implicit bias should cause\nagents to mistrust their knowledge-seeking faculties in general.\n“Bias-related doubt” (Saul 2012) is stronger than\ntraditional forms of skepticism (e.g., external world skepticism) in\nthe sense that it suggests that our epistemic judgments are not just\npossibly but often likely mistaken. Implicit biases are likely\nto degrade our judgments across many domains, e.g., professors’\njudgments about student grades, journal submissions, and\njob\n candidates.[16]\nMoreover, as\nFricker (2007) points out, the testimony of members of stigmatized\ngroups is likely to be discounted due to implicit bias, which, Saul\nsuggests, can magnify these epistemic failures as well as create\nothers, such as failing to recognize certain questions as relevant for\ninquiry (Hookway 2010). The key point about these examples is that our\njudgments are likely to be affected by implicit biases even when\n“we think we’re making judgments of scientific or\nargumentative merit” (Saul 2012: 249; see also Welpinghus forthcoming).\nMoreover, unlike errors of probabilistic reasoning, these effects\ngeneralize across many areas of day-to-day life. We should be worried,\nSaul argues, \n\nwhenever we consider a claim, an argument, a suggestion, a question,\netc from a person whose apparent social group we’re in a position\nto recognize. (Saul 2012: 250). \n\nBias-related doubt may be diminished if successful interventions can\nbe developed to correct for epistemic errors caused by implicit bias.\nIn some cases, the fix may be simple, such as anonymous review of job\ncandidate dossiers. But other contexts will certainly be\nmore\n challenging.[17]\nMore generally,\nSaul’s account of bias-related doubt takes a strongly pessimistic\nstance toward the normativity of our unreflective habits. “It is\ndifficult to see”, she writes, “how we could ever properly\ntrust [our habits] again once we have reflected on implicit bias”\n(2012: 254). Others, however, have stressed the ways in which\nunreflective habits can have epistemic virtues (e.g., Arpaly 2004;\nRailton 2014; Brownstein & Madva 2012a,b; Nagel 2012; Antony 2016).\nSquaring the reasons for pessimism about the epistemic status of our\nhabits with these streams of thought will be important in future\nresearch. \n\nGendler (2011) and Egan (2011) argue that implicit bias creates a\nconflict between our ethical and epistemic aims. Concern about\nethical/epistemic dilemmas is at least as old as Pascal, as Egan points\nout, but is also incarnated in contemporary research on the value of\npositive illusions (i.e., beliefs like “I am brilliant!”\nwhich may promote well-being despite being false; e.g., Taylor &\nBrown 1988). The dilemma surrounding implicit bias stems from the\napparent unavoidability of stereotyping, which Gendler traces to the\nway in which social categorization is fundamental to our\ncognitive\n capacities.[18]\nFor agents who\ndisavow common social stereotypes for ethical reasons, this creates a\nconflict between what we know and what we value. As Gendler puts\nit, \n\nif you live in a society structured by racial categories that you\ndisavow, either you must pay the epistemic cost of failing to encode\ncertain sorts of base-rate or background information about cultural\ncategories, or you must expend epistemic energy regulating the\ninevitable associations to which that information—encoded in ways\nto guarantee availability—gives rise. (2011: 37) \n\nGender considers forbidden base rates, for example, which are useful\nstatistical generalizations that utilize problematic social knowledge.\nPeople who are asked to set insurance premiums for hypothetical\nneighborhoods will accept actuarial risk as a justification for setting\nhigher premiums for particular neighborhoods but will not do so if they\nare told that actuarial risk is correlated with the racial composition\nof that neighborhood (Tetlock et al. 2000). This “epistemic\nself-censorship on non-epistemic grounds” makes it putatively\nimpossible for agents to be both rational and equitable (Gendler 2011:\n55, 57). \n\nEgan (2011) raises problems for intuitive ways of diffusing this\ndilemma, settling instead on the idea that making epistemic sacrifices\nfor our ethical values may simply be worth it. Others have been more\nunwilling to accept that implicit bias does in fact create an\nunavoidable ethical-epistemic dilemma (Mugg 2013; Beeghly 2014; Madva\n2016b; Lassiter & Ballantyne 2017; Puddifoot 2017). One way of\ndiffusing the dilemma, for example, is to suggest that it is not social\nknowledge per se that has costs, but rather that the\naccessibility of social knowledge in the wrong circumstances has\ncognitive costs (Madva 2016b). The solution to the dilemma, then, is\nnot ignorance, but the situation-specific regulation of stereotype\naccessibility. For example, the accessibility of social knowledge can\nbe regulated by agents’ goals and habits (Moskowitz & Li\n2011). Readers interested in ethical-epistemic dilemmas due to implicit\nbias should also consider related scholarship on “moral\nencroachment” (e.g., Basu & Schroeder 2018; Gardiner\n2018). \n\nMost philosophical writing on the ethics of implicit bias has\nfocused on two distinct (but related) questions. First, are agents\nmorally responsible for their implicit biases\n (§4.1)?\nSecond, can agents change their implicit\nbiases or control their effects on their judgments and\nbehavior\n (§4.2)? \n\nResearchers working on moral responsibility for implicit bias often\nmake two key distinctions. First, they distinguish responsibility for\nattitudes from responsibility for judgments and behavior. One can, that\nis, ask whether agents are responsible for their putative (§2)\nimplicit attitudes as such, or whether agents are responsible for the\neffects of their implicit attitudes on their judgments and behavior.\nMost have focused on the latter question, as will I. A second important\ndistinction is between being responsible and holding\nresponsible. This distinction can be glossed in a number of different\nbut related ways. It can be glossed as a distinction between\nblameworthiness and actual expressions of blame; between backward- and\nforward-looking responsibility (i.e., responsibility for things one has\ndone in the past versus responsibility for doing certain things in the\nfuture); and between responsibility as a form of judgment versus\nresponsibility as a form of sanction. Most have focused on the former\nof these disjuncts (being responsible, blameworthiness, etc.) via three\nkinds of approaches: arguments from the importance of awareness or\nknowledge of one’s implicit biases\n (§4.1.1);\narguments from the importance of control\nover the impact of one’s implicit biases on one’s judgment\nand behavior\n (§4.1.2);\nand arguments from\n“attributionist” and “Deep Self”\nconsiderations\n (§4.1.3;\nsee Holroyd et al. 2017 for a\nmore in-depth review of theories of moral responsibility and implicit\nbias). \n\nIt is plausible that conscious awareness of our implicit biases is a\nnecessary condition for moral responsibility for those biases. Saul\narticulates the intuitive idea, suggesting that we \n\nabandon the view that all biases against stigmatised groups are\nblameworthy … [because a] person should not be blamed\nfor an implicit bias that they are completely unaware of, which results\nsolely from the fact that they live in a sexist culture. (2013: 55,\nemphasis in original) \n\nSaul’s claim appears to be in keeping with folk psychological\nattitudes about blameworthiness and implicit bias. Cameron and\ncolleagues (2010) found that subjects were considerably more willing to\nascribe moral responsibility to “John” when he was\ndescribed as acting in discriminatory ways against black people despite\n“thinking that people should be treated equally, regardless of\nrace” compared to when he was described as acting in\ndiscriminatory ways despite having a “sub-conscious dislike for\nAfrican Americans” that he is “unaware of\nhaving”. \n\nRecalling the evidence that people often do have awareness of their\nimplicit biases\n (§3.1),\nit would seem that\ntypical agents are responsible for those biases on the basis of the\nargument from awareness. However, if the question is whether agents are\nblameworthy for behaviors affected by implicit biases (rather than for\nhaving biases themselves), then perhaps impact awareness is what\nmatters most (Holroyd 2012). That said, lacking impact awareness of the\neffects of implicit bias on our behavior may not exculpate agents from\nresponsibility even in principle. One possibility is that implicit\nbiases are analogous to moods in the sense that being in an\nintrospectively unnoticed bad mood can cause one to act badly (Madva\n2018). There is debate about whether unnoticed moods are exculpatory\n(e.g., Korsgaard 1997; Levy 2011). One possibility is that bad moods\nand implicit biases both diminish blameworthiness, but do not undermine\nit as such. This claim depends in part on moral responsibility\nadmitting of degrees. \n\nOne problem with focusing on impact awareness, however, as Holroyd\n(2012) points out, is that we may be unaware of the impact of a great\nmany cognitive states on our behavior. The focus on impact awareness\nmay lead to a global skepticism about moral responsibility, in other\nwords. This suggests that impact awareness may not serve as a good\ncriterion for distinguishing responsibility for implicit biases from\nresponsibility for other cognitive states, notwithstanding whether\nglobal skepticism about moral responsibility is defensible. \n\nA second way to unpack the argument from awareness is to focus on\nwhat agents ought to know about implicit bias, rather than\nwhat they do know. This approach indexes moral responsibility\nto one’s social and epistemic environment. For example, Kelly\n& Roedder (2008) argue that a “savvy grader” is\nresponsible for adjusting her grades to compensate for her likely\nbiases because she ought to be aware of and compelled by research on\nimplicit bias. In a similar spirit, Washington & Kelly (2016)\ncompare two hypothetical egalitarians with equivalent psychological\nprofiles, the only difference between them being that the “Old\nSchool Egalitarian” is evaluating résumés in 1980\nand the “New Egalitarian” is doing so in 2014. While\nneither has heard of implicit bias, Washington & Kelly argue that\nthe New Egalitarian is morally culpable in a way that the Old School\nEgalitarian isn’t. Only the New Egalitarian could have, and ought\nto have, known about his likely implicit biases, given the comparative\nstates of art of psychological research in 1980 and 2014. The\nunderlying intuition here is that assessments of responsibility change\nwith changes in an agent’s social and epistemic environment. \n\nA third way of unpacking the argument from awareness is to focus on\nthe way in which an attitude does or does not integrate with a variety\nof the agent’s other attitudes once it becomes conscious (Levy\n2012; see\n §2.1).\nOn this view, attitudes\nthat cause responsible behavior are available to a broad range of\ncognitive systems. For example, in cognitive dissonance experiments\n(e.g., Festinger 1956), agents attribute confabulatory reasons to\nthemselves and then tend to act in accord with those self-attributed\nreasons. The self-attribution of reasons in this case, according to\nLevy (2012), has an integrating effect on behavior, and thus can be\nthought of as underwriting the sort of agency required for moral\nresponsibility. Crucially, it is when the agent becomes conscious of\nher self-attributed reasons that they have this integrating effect.\nThis provides grounds for claiming that attitudes for which agents are\nresponsible are those that integrate behavior when the agent becomes\naware of the content of those attitudes. Implicit attitudes are not\nlike this, according to Levy. What’s morally important is\nthat \n\nawareness of the content of our implicit attitudes fails to\nintegrate them into our person level concerns in the manner required\nfor direct moral responsibility. (Levy 2012: 9). \n\nThe fact that implicit processes are often defined in contrast to\n“controlled” cognitive processes (§2.2) implies that\nthey may affect behavior in a way that bypasses a person’s\nagential capacities. The fact that implicit biases seem to\n“rebound” in response to intentional efforts to suppress\nthem supports this interpretation (Huebner 2009; Follenfant & Ric\n2010). Early research suggesting that implicit biases reflect mere\nawareness of stereotypes, rather than personal attitudes, also implies\nthat these states reflect processes that “happen to”\nagents. More recently, however, philosophers have questioned the\nramifications of these and other data for the notion of control\nrelevant to moral responsibility. \n\nPerhaps the most familiar way of understanding control in the\nresponsibility literature is in terms of a psychological mechanism that\nwould allow an agent to act differently than she otherwise would act\nwhen there is sufficient reason to do so (Fischer & Ravizza 2000).\nThe question facing this sort of reasons-responsiveness view of control\nis whether automatized behaviors—which unfold in the absence of\nexplicit reasoning—should be thought of as under an agent’s\ncontrol. Some have argued that automaticity and control are not\nmutually exclusive. Holroyd & Kelly (2016) advance a notion of\n“ecological control”, and Suhler and Churchland (2009)\noffer an account of nonconscious control that underwrites automaticity\nitself, yet is ostensibly sufficient for underwriting responsibility.\nOthers have distinguished between automaticity and automatisms (e.g.,\nsleepwalking); in this sense, the relevant moral distinction might be\ndrawn in terms of agents’ ability to “pre-program”\ntheir automatic actions (but not automatistic actions) via previous\ncontrolled choices (e.g., Wigley 2007); it might be drawn in terms of\nagents’ ability to consciously monitor their automatic actions\n(e.g., Levy & Bayne, 2004); or it might simply be the case that\nputative implicit attitudes are not automatic because they are readily\nchangeable (e.g., Buckwalter\n forthcoming).[19]\nOthers still have distinguished between\n“indirect” and “direct” control over\none’s attitudes or behavior (e.g., Holroyd 2012; Levy &\nMandelbaum 2014; Sie & Voorst Vader-Bours 2016). Holroyd (2012)\nargues that there are many things over which we do not hold direct and\nimmediate control, yet for which we are commonly held responsible, such\nas learning a skill, speaking a foreign language, and even holding\ncertain beliefs. None of these abilities or states can be had by fiat\nof will; rather, they take time and effort to obtain. This suggests\nthat we can be held responsible for attitudes or behaviors over which\nwe only have indirect long-range control. The question, then, of\ncourse, is whether agents can exercise indirect long-range control over\ntheir implicit biases. Mounting evidence suggests that we can\n (§4.2). \n\n“Attributionist” and Deep Self theories of moral\nresponsibility represent an alternative to arguments from awareness and\ncontrol. According to these theories, for an agent to be responsible\nfor an action is for that action to “reflect upon” the\nagent “herself”. A common way of speaking is to say that\nresponsibility-bearing actions are attributable to agents in virtue of\nreflecting upon the agent’s “deep self”, where the\ndeep self represents the person’s fundamental evaluative stance\n(Sripada 2016). Although there is much disagreement in the literature\nabout what the deep self really is, as well as what it means for an\nattitude or action to reflect upon it, attributionists agree that\npeople can be morally responsible for actions that are non-conscious\n(e.g., “failure to notice” cases), non-voluntary (e.g.,\nactions stemming from strong emotional reactions), or otherwise\ndivergent from an agent’s will (Frankfurt 1971; Watson 1975,\n1996; Scanlon 1998; A. Smith 2005, 2008, 2012; Hieronymi 2008; Sher\n2009; and H. Smith 2011). \n\nOne influential view developed in recent years is that agents are\nresponsible for just those actions or attitudes that stem from, or are\nsusceptible to modification by, the agent’s\n“evaluative” or “rational” judgments, which are\njudgments for which it is appropriate (in principle) to ask the agent\nher reasons (in a justifying sense) for holding (Scanlon 1998; A. Smith\n2005, 2008, 2012). A. Smith suggests that implicit biases stem from\nrational judgments, because \n\na person’s explicitly avowed beliefs do not settle the\nquestion of what she regards as a justifying consideration. (2012:\n581–582, fn 10) \n\nAn alternative approach sees the source of the “deep\nself” in an agent’s “cares” rather than in her\nrational judgments (Shoemaker 2003, 2011; Jaworska 2007; Sripada 2016).\nCares have been described in different ways, but in this context are\nthought of as psychological states with motivational, affective, and\nevaluative dispositional properties. It is an open question whether\nimplicit biases are reflective of an agent’s cares (Brownstein\n2016a, 2018). It is also possible that even in cases in which an\nimplicit bias is not attributable to an agent’s deep self, it may\nstill be appropriate to hold the agent responsible for\nviolating some duty or obligation she holds due to her implicit biases\n(Zheng 2016). Glasgow (2016) similarly argues for responsibility for\nimplicit biases that may not be attributable to agents. His view\nunfolds in terms of responsibility for actions from which agents are\nnevertheless alienated. Glasgow defends this view on the basis of\n“Content-Sensitive Variantism” and “Harm-Sensitive\nVariantism”, a pair of views according to which alienation\nexculpates depending on extra-agential features of an action, such as\nthe content of the action or the kind of harm it creates. These\nvariantist views are fairly strongly revisionist with respect to\ntraditional conceptions of responsibility in the 20th\ncentury philosophical literature. Some have argued that research on\nimplicit bias calls for revisionism of this sort (Vargas 2005; Faucher\n2016). \n\nResearchers working in applied ethics may be less concerned with\nquestions about in-principle culpability and more concerned with\ninvestigating how to change or control our implicit biases. Of course,\nanyone committed to fighting against prejudice and discrimination will\nlikely share this interest. Policymakers and workplace managers may\nalso be concerned with finding effective interventions, given that they\nare already directing tremendous public and private resources toward\nanti-discrimination programs in workplaces, universities, and other\ndomains affected by intergroup conflict. Yet as Paluck and Green (2009)\nsuggest, the effectiveness of many of the strategies commonly used\nremains unclear. Most studies on prejudice reduction are\nnon-experimental (lacking random assignment), are performed without\ncontrol groups, focus on self-report surveys, and gather primarily\nqualitative (rather than quantitative) data. \n\nAn emerging body of laboratory-based research suggests that\nstrategies are available for regulating implicit biases, however. One\nway to class these strategies is in terms of those that purport to\nchange the apparent associations underlying agents’\nimplicit biases, compared with those that purport to leave implicit\nassociations intact but enable agents to control the effects\nof their biases on their judgment and behavior (Stewart & Payne\n2008; Mendoza et al. 2010; Lai et al. 2013). For example, a\n“change-based” strategy might reduce individuals’\nautomatic associations of “white” with “good”\nwhile a “control-based” strategy might enable individuals\nto prevent that association from affecting their behavior. Below, I\nbriefly describe some of these interventions. For comparison of the\ndata on their effectiveness, see Lai and colleagues (2014, 2016), and\nfor discussion of their significance for theories of the metaphysics of\nimplicit bias, including a helpful appendix listing\n“debiasing” experiments, see Byrd (forthcoming). \n\nIntergroup contact (Aberson et al. 2008; Dasgupta &\nRivera 2008; Anderson 2010 for discussion): long studied for its\neffects on explicit prejudice (e.g., Allport 1954; Pettigrew &\nTropp 2006), interaction between members of different social groups\nappears to diminish implicit bias as well, albeit under some\nmoderating conditions (e.g., equal status interaction) and not under\nothers. \n\nApproach training (Kawakami et al. 2007, 2008; Phills et\nal. 2011): participants repeatedly “negate” stereotypes\nand “affirm” counter-stereotypes by pressing a button\nlabelled “NO!” when they see stereotype-consistent images\n(e.g., of a black face paired with the word “athletic”) or\n“YES!” when they see stereotype-inconsistent images (e.g.,\nof a white face paired with the word “athletic”). Other\nexperimental scenarios have had participants push a joystick away from\nthemselves to “negate” stereotypes and pull the joystick\ntoward themselves to “affirm” counter-stereotypes. \n\nEvaluative conditioning (Olson & Fazio 2006; De Houwer\n2011): a widely used technique whereby an attitude object (e.g., a picture\nof a black face) is paired with another valenced attitude object (e.g.,\nthe word “genius”), which shifts the valence of the first\nobject in the direction of the second. \n\nCounter-stereotype exposure (Blair et al. 2001; Dasgupta\n& Greenwald 2001): increasing individuals’ exposure to\nimages, film clips, or even mental imagery depicting members of\nstigmatized groups acting in stereotype-discordant ways (e.g., images\nof female scientists). \n\nImplementation intentions (Gollwitzer & Sheeran 2006;\nStewart & Payne 2008; Mendoza et al. 2010; Webb et al. 2012):\n“if-then” plans that specify a goal-directed response\nthat an individual plans to perform on encountering an anticipated cue.\nFor example, in a “Shooter Bias” test, where participants\nare given the goal to “shoot” all and only those\nindividuals shown holding guns in a computer simulation, participants\nmay be asked to adopt the plan, “if I see a black face, I will\nthink\n ‘safe!’”[20] \n\n“Cues for control” (Monteith 1993; Monteith et\nal. 2002): techniques for noticing prejudiced responses, in particular the\naffective discomfort caused by the inconsistency of those responses\nwith participants’ egalitarian goals. \n\nPriming goals, moods, and motivations (Huntsinger et al.\n2010; Moskowitz & Li 2011; Mann & Kawakami 2012): \npriming egalitarian goals, multicultural ideologies, or particular\nmoods can lower scores of prejudice on implicit measures. \n\nThere is some doubt about this way of categorizing interventions, as\nsome control-based interventions may also change agents’\nunderlying associations and some association-based interventions may\nalso promote control (Stewart & Payne 2008; Mendoza et al. 2010).\nMore significant though are concerns about the efficacy of these\ninterventions over time (Lai et al. 2016), their practical feasibility\n(Bargh 1999; Schneider 2004), and the possibility that they may\ndistract from broader problems of economic and institutional forms of\ninjustice (Anderson 2010; Dixon et al. 2012; see\n §5).\nOf course, most of the research on\ninterventions like these is recent, so it is simply not clear yet which\nstrategies, or combination of strategies (Devine et al. 2012), will or\nwon’t be effective. Some have voiced optimism about the role\nlab-based interventions like these can play as elements of broader\nefforts to combat prejudice and discrimination (e.g., Kelly et al.\n2010a; Madva 2017). Research on implicit bias has been criticized in several ways.\nBelow are brief descriptions of, and discussion about, prominent lines\nof\n critique.[21]\nI leave aside\ncritical assessments of specific implicit measures. \n\nResearch on implicit bias has received a lot of attention, not only\nin philosophy and psychology, but in politics, journalism,\njurisprudence, business, and medicine as well. Some have worried that\nthis attention is excessive, such that the explanatory power of\nresearch on implicit bias has been overstated (e.g., Singal 2017;\nJussim 2018 (Other Internet Resources); Blanton & Ikizer 2019). \n\nWhile the difficulty of public science communication is pervasive\n(i.e., not limited to implicit bias research), and the most egregious\ncases are found in the popular press, it is true that some researchers\nhave overhyped the importance of implicit bias for explaining social\nphenomena. Hype can have disastrous consequences, such as creating\npublic distrust in science. One important point to bear in mind,\nhowever, is that the challenges facing science communication and the\nchallenges facing a body of research are distinct. That is, one\nquestion is whether the science is strong, and it is a separate\nquestion whether the strength of the science, such as it is, is\naccurately communicated to the public. Overhyped research may create\nincentives for scientists to do flashy but weak work—and this is\na problem—but problems with hype are nevertheless distinct from\nproblems with the science itself.  Some have argued that explicit bias can explain much of what\nimplicit bias purports to explain (e.g., Hermanson 2017a,b, 2018\n(Other Internet Resources); Singal 2017; Buckwalter 2018). Jesse\nSingal (2017), for example, denies that implicit bias is more\nimportant than explicit bias, pointing to the United States Department\nof Justice’s findings about intentional race-based\ndiscrimination in Ferguson, MO and to the fact that the United States\nelected a relatively explicitly racist President in 2016. \n\nSingal and others are surely right that explicit bias and outright\nprejudice are persistent and, in some places, pervasive. It is,\nhowever, unclear who, if anyone, thinks that implicit bias is more\nimportant than explicit bias. Philosophers in particular have been\ninterested in implicit bias because, despite the persistence and\npervasiveness of explicit bias, there are many people—presumably\nmany of those reading this article—who aim to think and act in\nunprejudiced ways, and yet are susceptible to the kinds of biased\nbehavior implicit bias researchers have studied. This is not only an\nimportant phenomenon in its own right, but also may contribute causally\nto the mainstream complacence toward the very outrageous instances of\nbigotry Singal discusses. Implicit bias may also contribute causally to\nexplicit bias, particularly in environments suffused with prejudiced\nnorms (Madva 2019). A related worry is that there is not agreement in the literature\nabout what “implicit” means. Arguably the most common\nunderstanding is that “implicit” means\n“unconscious.” But whatever is assessed by implicit\nmeasures is arguably not unconscious (§3.1). It is true that there is no widespread agreement about the meaning\nof “implicit,” and it is also true that no theory of\nimplicit social cognition is consistent with all the current data. To\nwhat extent this is a problem depends on background theories about how\nscience progresses. It is also crucial to recognize that implicit\nmeasures are not high-fidelity assessments of any one distinct\n“part” of the mind. They are not process pure (§1.2).\nThis means that they capture a mix of various cognitive and affective\nprocesses. Included in this mix are people’s beliefs and explicit\nattitudes. Indeed, researchers have known for some time that the best\nway to predict a person’s scores on an implicit measure like the\nIAT is to ask them their opinions about the IAT’s targets. This\ndoes not mean that implicit measures lack “discriminant\nvalidity,” however (i.e., that they are redundant with existing\nmeasures). By analogy, you are likely to find that people who say that\ncilantro is disgusting are likely to have aversive reactions to it, but\nthis doesn’t mean that their aversive reactions are an invalid\nconstruct. Indeed, one of the leading theories of the dynamics and\nprocesses of implicit social cognition since 2006—APE\n(§2.2)—is based on a set of predictions about this process\nimpurity (i.e., about the interactions of implicit and explicit\nevaluative processes). \n\nSeveral meta-analyses have found that, according to standard\nconventions, the correlation between implicit measures and behavior is\nsmall to medium. Average correlations have ranged from approximately\n.14 to .37\n(Cameron et al. 2012; Greenwald et al. 2009; Oswald et al. 2013; Kurdi et al. 2019).\nThis variety is due\nto several factors, including the type of measures, type of attitudes\nmeasured (e.g., attitudes in general vs. intergroup attitudes in\nparticular), inclusion criteria for meta-analyses, and statistical\nmeta-analytic techniques. From these data, critics have concluded that\nimplicit measures are poor predictors of behavior. Oswald and\ncolleagues write, “the IAT provides little insight into who will\ndiscriminate against whom, and provides no more insight than explicit\nmeasures of bias” (2013, 18). Focusing on implicit bias research\nmore broadly, Buckwalter suggests that a review of the evidence\n“casts doubt on the claim that implicit attitudes will be found\nto be significant causes of behavior” (2018, 11). \n\nSeveral background questions must be considered in order to assess\nthese claims. Should implicit measures be expected to have small,\nmedium, or large unconditional (or “zero-order”)\ncorrelations with behavior? Zero-order correlations are those that\nobtain between two variables when no additional variable has been\ncontrolled for. Since the 1970s, research on self-reported attitudes\nhas largely focused on when—under what\nconditions—attitudes predict behavior, not whether\nattitudes predict behavior just as such. For example, attitudes better\npredict behavior when there is clear correspondence between the\nattitude object and the behavior in question\n(Ajzen\n& Fishbein 1977). While generic attitudes\ntoward the environment do not predict recycling behavior very well, for\ninstance, specific attitudes toward recycling do\n(Oskamp\net al. 1991). In the 1970s and 1980s, a\nconsensus emerged that attitude-behavior relations depend in general on\nthe particular behavior being measured (e.g., political judgments vs.\nracial judgments), the conditions under which the behavior is performed\n(e.g., under time pressure or not), and the person who is performing\nthe behavior (e.g., personality;\nZanna\n&\nFazio 1982). A wealth of theoretical models of attitude-behavior\nrelations take these facts into account to make principled predictions\nabout when attitudes do and do not predict behavior (e.g.,\nFazio 1990).\nSimilar work is underway focusing on\nimplicit social cognition (for review see Gawronski & Hahn 2019 and\nBrownstein et al. ms). \n\nIn a related vein, it is also important to keep in mind that large\nzero-order correlations are rarely found in social science, let alone\nin attitude research. Large zero-order correlations should not be\nexpected to be found in implicit bias research, either\n(Gawronski, forthcoming).\nIndeed, the zero-order\ncorrelations between other familiar constructs and outcome measures is\ncomparable to what has been found in meta-analyses of implicit\nmeasures: beliefs and stereotypes about outgroups and behavior\n(r = .12;\nTalaska\net al. 2008); IQ\nand income (r = .2–.3; Strenze 2007); SAT scores and freshman\ngrades in college (r = .24; Wolfe and Johnson 1995);\nparents’ and their children’s socioeconomic status\n(r = .2–.3; Strenze 2007). The fact that no meta-analysis of\nimplicit measures has reported nonsignificant correlations close to\nzero or negative correlations with behavior further supports the\nconclusion that the relationship between implicit bias and behavior\nfalls within the “zone” of the relationship between these\nmore familiar constructs and relevant kinds of behavior. Whether this\ncommon pattern of findings in social science—of weak to moderate\nunconditional relations with behavior—is succor for supporters of\nimplicit bias research or cause for concern about the social sciences\nin general is an important and open question (see, e.g., Greenwald et\nal. 2015; Oswald et al. 2015; Jost 2019;\nGawronski\n forthcoming).[22]\nBut note that\nthe consistent finding of meta-analyses of implicit measures\ndistinguishes this body of research from those that have been\nswept up in the social sciences’ ongoing “replication\ncrisis.” That people, on average, display biases on implicit\nmeasures is one of the most stable and replicated findings in recent\npsychological\n science.[23]\nThe\ndebate described in this section pertains to interpreting the\nsignificance of this finding. \n\nSo-called “structuralist” critics (e.g.,\nBanks & Ford 2009; Anderson 2010;\nHaslanger 2015;\nAyala 2016, 2018; Mallon ms) have argued that researchers ought to pay\nmore attention to systemic and institutional causes of\ninjustice—such as poverty, housing segregation, economic\ninequality, etc.—rather than focusing on the biases inside the\nminds of individuals. One way to express the structuralist idea is that\nwhat happens in the minds of individuals, including their biases, is\nthe product of social inequities rather than an\nexplanation for them. Structuralists then tend to argue that\nour efforts to combat discrimination and inequity ought to focus on\nchanging social structures themselves, rather than trying to change\nindividual’s biases directly. For example, Ayala argues that\n“agents’ mental states [are] … not necessary to\nunderstand and explain” when considering social injustice (2016,\n9).\nLikewise, in her call to combat segregation in the contemporary United States, Anderson (2010)\nis critical of\nwhat she sees as a distracting focus on the psychology of bias. \n\nA strong version of the structuralist critique—that research\non the psychology of prejudice is entirely useless, distracting, or\neven dangerous—is hard to defend. Large-scale demographic\nresearch makes clear that psychological prejudice is a key driver of\n(for example) economic inequality (e.g.,\nChetty et al. 2018)\nand inequities in the criminal justice\nsystem\n(Center for Policing Equity 2016).\nMore\nbroadly, no matter how autonomously certain social structures operate,\npeople must choose to accept or reject those structures, to vote for\npoliticians who speak for or against them, and so on. How people assess\nthese options is at least in part a psychological question. \n\nA weaker version of the structuralist critique calls for needed\nattention to the ways in which psychological and structural phenomena\ninteract to produce and entrench discrimination and inequity. This\n“interactionism” seeks to understand how bias operates\ndifferently in different contexts. If you wanted to combat housing\nsegregation, for example, you would want to consider not only\nproblematic institutional practices, such as “redlining”\ncertain neighborhoods within which banks will not give mortgage loans,\nand not only psychological factors, such as the propensity to perceive\nlow-income people as untrustworthy, but the interaction of the\ntwo. A low-income person from a redlined neighborhood might not\nbe perceived as untrustworthy when they are interviewing for a job as a\nnanny, but might be perceived as untrustworthy when they are\ninterviewing for a loan. Adopting the view that bias and structure\ninteract to produce unequal outcomes does not mean that researchers\nmust always account for both. Sometimes it makes sense to\nemphasize one kind of cause or the other.  \n\nAn interactionist version of structuralism can incorporate research\non prejudice into a wider understanding of inequity, rather than eschew\nit. One way to do so is to identify ways in which psychological biases\n(whether implicit or explicit) might be key contributors to\nsocial-structural phenomena. For example, structuralists sometimes\npoint to the drug laws and sentencing guidelines that contribute to the\nmass incarceration of black men in the USA as examples of systemic\nbiases. Sometimes, however, when these laws and policies change,\ndiscrimination persists. While arrests have declined for all racial\ngroups in states that have decriminalized marijuana, black people\ncontinue to be arrested for marijuana-related offenses at a rate of\nabout 10 times that of white people\n(Drug Policy Alliance 2018).\nThis suggests that psychological biases (belonging\nto officers, policy makers, or voters) are an ineliminable part of\nsystemic inequity. Such interactionism is just one approach for\nblending individual and institutional approaches to intergroup\ndiscrimination (see, e.g.,\nMadva\n2016a, 2017;\nDavidson & Kelly forthcoming). Another idea is to incorporate research\nspecifically on implicit bias into a wider understanding of the\nstructural sources of inequity by using implicit measures to assess\nbroad social patterns (rather than to assess the differences between\nindividuals). The “Bias of Crowds” model (§2.5) argues\nthat implicit bias is a feature of cultures and communities. For\nexample, average scores on implicit measures of prejudice and\nstereotypes, when aggregated at the level of cities within the United\nStates, predict racial disparities of shootings of citizens by police\nin those cities\n(Hehman\net al. 2017). Thus,\nwhile it is certainly true that most of the relevant literature and\ndiscussion conceptualizes implicit bias as way of differentiating\nbetween individuals, structuralists might utilize the data for\ndifferentiating regions, cultures, and so on. \n\nNosek and colleagues (2011) suggest that the second generation of\nresearch on implicit social cognition will come to be known as the\n“Age of Mechanism”. Several metaphysical questions fall\nunder this label. One question crucial to the metaphysics of implicit\nbias is whether the relevant psychological constructs should be thought\nof as stable, trait-like features of a person’s identity or as\nmomentary, state-like features of their current mindset or situation\n(§2.4). While current data suggest that implicit biases are more\nstate-like than trait-like, methodological improvements may generate\nmore stable, dispositional results on implicit measures. Ongoing\nresearch on additional psychometric properties of implicit\nmeasures—such as their discriminant validity and capacity to\npredict behavior—will also strengthen support for some theories\nof the metaphysics of implicit bias and weaken support for others.\nAnother open metaphysical question is whether the mechanisms underlying\ndifferent forms of implicit bias (e.g., implicit racial biases vs.\nimplicit gender biases) are heterogeneous. Some have already begun to\ncarve implicit social attitudes into kinds (Amodio & Devine 2006;\nHolroyd & Sweetman 2016; Del Pinal et al. 2017; Del Pinal &\nSpaulding 2018; Madva & Brownstein 2018). Future research on\nimplicit bias in particular domains of social life may also help to\nilluminate this issue, such as research on implicit bias in legal\npractices (e.g., Lane et al. 2007; Kang 2009) and in medicine (e.g.,\nGreen et al. 2007; Penner et al. 2010), on the development of implicit\nbias in children (e.g., Dunham et al. 2013b), on implicit intergroup\nbias toward non-black racial minorities, such as Asians and Latinos\n(Dasgupta 2004), and cross-cultural research on implicit bias in\nnon-Western countries (e.g., Dunham et al. 2013a). \n\nFuture research on epistemology and implicit bias may tackle a\nnumber of questions, for example: does the testimony of social and\npersonality psychologists about statistical regularities justify\nbelieving that you are biased? What can developments in vision\nscience tell us about illicit belief formation due to implicit bias? In\nwhat ways is implicit bias depicted and discussed outside academia\n(e.g., in stand-up comedy focusing on social attitudes)? Also germane\nare future methodological questions, such as how research on implicit\nsocial cognition may interface with large-scale correlational\nsociological studies on social attitudes and discrimination (Lee 2016).\nAnother crucial methodological question is whether and how theories of\nimplicit bias—and more generally psychological approaches to\nunderstanding social phenomena—can come to be integrated with\nbroader social theories focusing on race, gender, class, disability,\netc. Important discussions have begun (e.g., Valian 2005; Kelly &\nRoedder 2008; Faucher & Machery 2009; Anderson 2010; Machery et al.\n2010; Madva 2017), but there is no doubt that more connections must be\ndrawn to relevant work on identity (e.g., Appiah 2005), critical theory\n(e.g., Delgado & Stefancic 2012), feminist epistemology (Grasswick\n2013), and race and political theory (e.g., Mills 1999). \n\nAs with all of the above, questions in theoretical ethics about\nmoral responsibility for implicit bias will certainly be influenced by\nfuture empirical research. One noteworthy intersection of theoretical\nethics with forthcoming empirical research will focus on the\ninterpersonal effects of blaming and judgments about blameworthiness\nfor implicit\n bias.[24]\nThis\nresearch aims to have practical ramifications for mitigating intergroup\nconflict as well, of course. On this front, arguably the most pressing\nquestion, however, is about the durability of psychological\ninterventions once agents leave the lab. How long will shifts in biased\nresponding last? Will individuals inevitably “relearn”\ntheir biases (cf. Madva 2017)? Is it possible to leverage the lessons\nof “situationism” in reverse, such that shifts in\nindividuals’ attitudes create environments that provoke more\negalitarian behaviors in others (Sarkissian 2010; Brownstein 2016b)?\nMoreover, what has (or has not) changed in people’s feelings,\njudgments, and actions now that research on implicit bias has received\nconsiderable public attention (e.g., Charlesworth & Banaji\n2019)?","contact.mail":"msbrownstein@gmail.com","contact.domain":"gmail.com"}]
