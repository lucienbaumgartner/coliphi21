[{"date.published":"2004-10-29","date.changed":"2016-12-20","url":"https://plato.stanford.edu/entries/simplicity/","author1":"Alan Baker","entry":"simplicity","body.text":"\n\n\nMost philosophers believe that, other things being equal, simpler\ntheories are better. But what exactly does theoretical simplicity\namount to? Syntactic simplicity, or elegance, measures the number and\nconciseness of the theory's basic principles. Ontological simplicity,\nor parsimony, measures the number of kinds of entities postulated by\nthe theory. One issue concerns how these two forms of simplicity\nrelate to one another. There is also an issue concerning the\njustification of principles, such as Occam's Razor, which favor simple\ntheories. The history of philosophy has seen many approaches to\ndefending Occam's Razor, from the theological justifications of the\nEarly Modern period, to contemporary justifications employing results\nfrom probability theory and statistics.\n\nThere is a widespread philosophical presumption that simplicity is a\ntheoretical virtue. This presumption that simpler theories are\npreferable appears in many guises. Often it remains implicit;\nsometimes it is invoked as a primitive, self-evident proposition;\nother times it is elevated to the status of a ‘Principle’\nand labeled as such (for example, the ‘Principle of\nParsimony’). However, it is perhaps best known by the name\n‘Occam's (or Ockham's) Razor.’ Simplicity principles have\nbeen proposed in various forms by theologians, philosophers, and\nscientists, from ancient through medieval to modern times. Thus\nAristotle writes in his Posterior Analytics, \nMoving to the medieval period, Aquinas writes: \nKant—in the Critique of Pure Reason—supports the\nmaxim that “rudiments or principles must not be unnecessarily\nmultiplied (entia praeter necessitatem non esse\nmultiplicanda)” and argues that this is a regulative idea\nof pure reason which underlies scientists' theorizing about nature\n(Kant, 1781/1787, pp. 538–9). Both\nGalileo and Newton accepted versions of Occam's Razor. Indeed Newton\nincludes a principle of parsimony as one of his three ‘Rules of\nReasoning in Philosophy’ at the beginning of Book III of\nPrincipia Mathematica (1687): \nNewton goes on to remark that “Nature is pleased with\nsimplicity, and affects not the pomp of superfluous causes”\n(Newton 1687, p. 398). Galileo, in the course of making a detailed\ncomparison of the Ptolemaic and Copernican models of the solar system,\nmaintains that “Nature does not multiply things unnecessarily;\nthat she makes use of the easiest and simplest means for producing her\neffects; that she does nothing in vain, and the like” (Galileo\n1632, p. 397). Nor are scientific advocates of simplicity principles\nrestricted to the ranks of physicists and astronomers. Here is the\nchemist Lavoisier writing in the late 18th Century \nCompare this to the following passage from Einstein, writing 150 years\nlater. \nEditors of a recent volume on simplicity sent out surveys to 25 recent\nNobel laureates in economics. Almost all replied that simplicity\nplayed a role in their research, and that simplicity is a desirable\nfeature of economic theories (Zellner et al. 2001, p.2). Riesch (2010)\ninterviewed 40 scientists and found a range of attitudes towards the\nnature and role of simplicity principles in science. \nWithin philosophy, Occam's Razor (OR) is often wielded against\nmetaphysical theories which involve allegedly superfluous ontological\napparatus. Thus materialists about the mind may use OR against\ndualism, on the grounds that dualism postulates an extra ontological\ncategory for mental phenomena. Similarly, nominalists about abstract\nobjects may use OR against their platonist opponents, taking them to\ntask for committing to an uncountably vast realm of abstract\nmathematical entities. The aim of appeals to simplicity in such\ncontexts seem to be more about shifting the burden of proof, and less\nabout refuting the less simple theory outright. \nThe philosophical issues surrounding the notion of simplicity are\nnumerous and somewhat tangled. The topic has been studied in piecemeal\nfashion by scientists, philosophers, and statisticians (though for an\ninvaluable book-length philosophical treatment see Sober 2015). The\napparent familiarity of the notion of simplicity means that it is\noften left unanalyzed, while its vagueness and multiplicity of\nmeanings contributes to the challenge of pinning the notion down\n precisely.[2]\n A distinction is often made between two fundamentally distinct senses\nof simplicity: syntactic simplicity (roughly, the number and\ncomplexity of hypotheses), and ontological simplicity (roughly, the\nnumber and complexity of things\n postulated).[3]\n These two facets of simplicity are often referred to as\nelegance and parsimony respectively. For the\npurposes of the present overview we shall follow this usage and\nreserve ‘parsimony’ specifically for simplicity in the\nontological sense. It should be noted, however, that the terms\n‘parsimony’ and ‘simplicity’ are used\nvirtually interchangeably in much of the philosophical literature. \nPhilosophical interest in these two notions of simplicity may be\norganized around answers to three basic questions; \n(i) How is simplicity to be defined? [Definition] \n(ii) What is the role of simplicity principles in different areas of\ninquiry? [Usage] \n(iii) Is there a rational justification for such simplicity\nprinciples? [Justification] \nAs we shall see, answering the definitional question, (i), is more\nstraightforward for parsimony than for elegance. Conversely, more\nprogress on the issue, (iii), of rational justification has been made\nfor elegance than for parsimony. It should also be noted that the\nabove questions can be raised for simplicity principles both within\nphilosophy itself and in application to other areas of theorizing,\nespecially empirical science. \nWith respect to question (ii), there is an important distinction to be\nmade between two sorts of simplicity principle. Occam's Razor may be\nformulated as an epistemic principle: if theory T is\nsimpler than theory T*, then it is rational (other things\nbeing equal) to believe T rather than T*. Or it may\nbe formulated as a methodological principle: if T is\nsimpler than T* then it is rational to adopt T as\none's working theory for scientific purposes. These two conceptions of\nOccam's Razor require different sorts of justification in answer to\nquestion (iii). \nIn analyzing simplicity, it can be difficult to keep its two\nfacets—elegance and parsimony—apart. Principles such as\nOccam's Razor are frequently stated in a way which is ambiguous\nbetween the two notions, for example, “Don't multiply\npostulations beyond necessity.” Here it is unclear whether\n‘postulation’ refers to the entities being postulated, or\nthe hypotheses which are doing the postulating, or both. The first\nreading corresponds to parsimony, the second to elegance. Examples of\nboth sorts of simplicity principle can be found in the quotations\ngiven earlier in this section. \nWhile these two facets of simplicity are frequently conflated, it is\nimportant to treat them as distinct. One reason for doing so is that\nconsiderations of parsimony and of elegance typically pull in\ndifferent directions. Postulating extra entities may allow a theory to\nbe formulated more simply, while reducing the ontology of a theory may\nonly be possible at the price of making it syntactically more complex.\nFor example the postulation of Neptune, at the time not directly\nobservable, allowed the perturbations in the orbits of other observed\nplanets to be explained without complicating the laws of celestial\nmechanics. There is typically a trade-off between ontology and\nideology—to use the terminology favored by Quine—in which\ncontraction in one domain requires expansion in the other. This points\nto another way of characterizing the elegance/parsimony distinction,\nin terms of simplicity of theory versus simplicity of world\n respectively.[4]\n Sober (2001) argues that both these facets of simplicity can be\ninterpreted in terms of minimization. In the (atypical) case of\ntheoretically idle entities, both forms of minimization pull in the\nsame direction; postulating the existence of such entities makes both\nour theories (of the world) and the world (as represented by our\ntheories) less simple than they might be. \nPerhaps the most common formulation of the ontological form of Occam's\nRazor is the following: \nIt should be noted that modern formulations of Occam's Razor are\nconnected only very tenuously to the 14th-century figure\nWilliam of Ockham. We are not here interested in the exegetical\nquestion of how Ockham intended his ‘Razor’ to function,\nnor in the uses to which it was put in the context of medieval\n metaphysics.[5]\n Contemporary philosophers have tended to reinterpret OR as a\nprinciple of theory choice: OR implies that—other things being\nequal—it is rational to prefer theories which commit us to\nsmaller ontologies. This suggests the following paraphrase of OR: \nWhat does it mean to say that one theory is more ontologically\nparsimonious than another? The basic notion of ontological parsimony\nis quite straightforward, and is standardly cashed out in terms of\nQuine's concept of ontological commitment. A theory,\nT, is ontologically committed to Fs if and only if\nT entails that F's exist (Quine 1981, pp.\n144–4). If two theories, T1 and\nT2, have the same ontological commitments except\nthat T2 is ontologically committed to Fs\nand T1 is not, then T1 is more\nparsimonious than T2. More generally, a sufficient\ncondition for T1 being more parsimonious than\nT2 is for the ontological commitments of\nT1 to be a proper subset of those of\nT2. Note that OR1 is considerably\nweaker than the informal version of Occam's Razor, OR, with which we\nstarted. OR stipulates only that entities should not be multiplied\nbeyond necessity. OR1, by contrast, states that\nentities should not be multiplied other things being equal,\nand this is compatible with parsimony being a comparatively weak\ntheoretical virtue. \nOne ‘easy’ case where OR1 can be\nstraightforwardly applied is when a theory, T, postulates\nentities which are explanatorily idle. Excising these entities from\nT produces a second theory, T*, which has the same\ntheoretical virtues as T but a smaller set of ontological\ncommitments. Hence, according to OR1, it is rational to\npick T* over T. (As previously noted, terminology\nsuch as ‘pick’ and ‘prefer’ is crucially\nambiguous between epistemic and methodological versions of Occam's\nRazor. For the purposes of defining ontological parsimony, it is not\nnecessary to resolve this ambiguity.) However, such cases are\npresumably rare, and this points to a more general worry concerning\nthe narrowness of application of OR1. First, how often does\nit actually happen that we have two (or more) competing theories for\nwhich ‘other things are equal’? As biologist Kent\nHolsinger remarks, \nSecond, how often are one candidate theory's ontological commitments a\nproper subset of another's? Much more common are situations where\nontologies of competing theories overlap, but each theory has\npostulates which are not made by the other. Straightforward\ncomparisons of ontological parsimony are not possible in such\ncases. \nBefore setting aside the definitional question for ontological\nparsimony, one further distinction should be mentioned. This\ndistinction is between qualitative parsimony (roughly, the\nnumber of types (or kinds) of thing postulated) and\nquantitative parsimony (roughly, the number of individual\nthings\n postulated).[6]\n The default reading of Occam's Razor in the bulk of the philosophical\nliterature is as a principle of qualitative parsimony. Thus Cartesian\ndualism, for example, is less qualitatively parsimonious than\nmaterialism because it is committed to two broad kinds of entity\n(mental and physical) rather than one. Section 6.1 contains a brief\ndiscussion of quantitative parsimony; apart from this the focus will\nbe on the qualitative notion. It should be noted that interpreting\nOccam's Razor in terms of kinds of entity brings with it some\nextra philosophical baggage of its own. In particular, judgments of\nparsimony become dependent on how the world is sliced up into kinds.\nNor is guidance from extra-philosophical usage—and in particular\nfrom science—always clearcut. For example, is a previously\nundiscovered subatomic particle made up of a novel rearrangement of\nalready discovered sub-particles a new ‘kind’? What about\na biological species, which presumably does not contain any novel\nbasic constituents? Also, ought more weight to be given to broad and\nseemingly fundamental divisions of kind—for example between the\nmental and physical—than between more parochial divisions?\nIntuitively, the postulation of a new kind of matter would seem to\nrequire much more extensive and solid justification than the\npostulation of a new sub-species of\n spider.[7] \nThe third and final question from Section 1 concerns potential\njustifications for principles of ontological parsimony such as Occam's\nRazor. The demand for justification of such principles can be\nunderstood in two importantly distinct ways, corresponding to the\ndistinction between epistemic principles and methodological principles\nmade at the end of Section 1. Justifying an epistemic principle\nrequires answering an epistemic question: why are parsimonious\ntheories more likely to be true? Justifying a methodological principle\nrequires answering a pragmatic question: why does it make practical\nsense for theorists to adopt parsimonious\n theories?[8]\n Most attention in the literature has centered on the first, epistemic\nquestion. It is easy to see how syntactic elegance in a theory can\nbring with it pragmatic advantages such as being more perspicuous,\nbeing easier to use and manipulate, and so on. But the case is more\ndifficult to make for ontological\n parsimony.[9]\n It is unclear what particular pragmatic disadvantages accrue to\ntheories which postulate extra kinds of entities; indeed—as was\nmentioned in the previous section—such postulations can often\nbring with them striking syntactic simplification. \nBefore looking at approaches to answering the epistemic justification\nquestion, mention should be made of two positions in the literature\nwhich do not fall squarely into either the pragmatic or epistemic\ncamp. The first position, associated primarily with Quine, argues that\nparsimony carries with it pragmatic advantages and that pragmatic\nconsiderations themselves provide rational grounds for discriminating\nbetween competing theories (Quine 1966, Walsh 1979). The Quinean\nposition bases an answer to the second question on the answer to the\nfirst, thus blurring the boundary between pragmatic and epistemic\njustification. The second position, due to Sober, rejects the implicit\nassumption in both the above questions that some global justification\nof parsimony can be found (Sober 1988, 1994). Instead Sober argues\nthat appeals to parsimony always depend on local background\nassumptions for their rational justification. Thus Sober writes: \nPhilosophers who reject these arguments of Quine and Sober, and thus\ntake the demand for a global, epistemic justification seriously, have\ndeveloped a variety of approaches to justifying parsimony. Most of\nthese approaches can be collected under two broad headings: \n(B) Naturalistic justifications, based on appeal to scientific\npractice. \nAs we shall see, the contrast between these two sorts of approach\nmirrors a broader divide between the rival traditions of rationalism\nand empiricism in philosophy as a whole. \nAs well as parsimony, the question of rational justification can also\nbe raised for principles based on elegance, the second facet of\nsimplicity distinguished in Section 1. Approaches to justifying\nelegance along the lines of (A) and (B) are possible, but much of the\nrecent work falls under a third category; \nThe next three sections examine these three modes of justification of\nsimplicity principles. The a priori justifications in\ncategory (A) concern simplicity in both its parsimony and elegance\nforms. The justifications falling under category (B) pertain mostly to\nparsimony, while those falling under category (C) pertain mostly to\nelegance. \nThe role of simplicity as a theoretical virtue seems so widespread,\nfundamental, and implicit that many philosophers, scientists, and\ntheologians have sought a justification for principles such as Occam's\nRazor on similarly broad and basic grounds. This rationalist approach\nis connected to the view that making a priori simplicity\nassumptions is the only way to get around the underdetermination of\ntheory by data. Until the second half of the 20th Century\nthis was probably the predominant approach to the issue of simplicity.\nMore recently, the rise of empiricism within analytic philosophy led\nmany philosophers to argue disparagingly that a priori\njustifications keep simplicity in the realm of metaphysics (see\nZellner et al. 2001, p.1). Despite its changing fortunes, the\nrationalist approach to simplicity still has its adherents. For\nexample, Richard Swinburne writes: \n(i) Theological Justifications \nThe post-medieval period coincided with a gradual transition from\ntheology to science as the predominant means of revealing the workings\nof nature. In many cases, espoused principles of parsimony continued\nto wear their theological origins on their sleeves, as with Leibniz's\nthesis that God has created the best and most complete of all possible\nworlds, and his linking of this thesis to simplifying principles such\nas light always taking the (time-wise) shortest path. A similar\nattitude—and rhetoric—is shared by scientists through the\nearly modern and modern period, including Kepler, Newton, and\nMaxwell. \nSome of this rhetoric has survived to the present day, especially\namong theoretical physicists and cosmologists such as Einstein and\n Hawking.[10]\n Yet there are clear dangers with relying on a theological\njustification of simplicity principles. Firstly, many—probably\nmost—contemporary scientists are reluctant to link\nmethodological principles to religious belief in this way. Secondly,\neven those scientists who do talk of ‘God’ often turn out\nto be using the term metaphorically, and not necessarily as referring\nto the personal and intentional Being of monotheistic religions.\nThirdly, even if there is a tendency to justify simplicity principles\nvia some literal belief in the existence of God, such justification is\nonly rational to the extent that rational arguments can be given for\nthe existence of\n God.[11] \nFor these reasons, few philosophers today are content to rest with a\ntheological justification of simplicity principles. Yet there is no\ndoubting the influence such justifications have had on past and\npresent attitudes to simplicity. As Smart (1994) writes: \n(ii) Metaphysical Justifications \nOne approach to justifying simplicity principles is to embed such\nprinciples in some more general metaphysical framework. Perhaps the\nclearest historical example of systematic metaphysics of this sort is\nthe work of Leibniz. The leading contemporary example of this\napproach—and in one sense a direct descendent of Leibniz's\nmethodology—is the possible worlds framework of David Lewis. In\none of his earlier works, Lewis writes, \nLewis has been attacked for not saying more about what exactly he\ntakes simplicity to be (see Woodward 2003). However, what is clear is\nthat simplicity plays a key role in underpinning his metaphysical\nframework, and is also taken to be a prima facie theoretical\nvirtue. \nThough Occam's Razor has arguably been a longstanding and important\ntool in the rise of analytic metaphysics, it has only been\ncomparatively recently that there has been much debate among\nmetaphysicians concerning the principle itself. Cameron (2010),\nSchaffer (2010), and Sider (2013) each argue for a version of Occam's\nRazor that focuses specifically on fundamental entities.\nSchaffer (2015, p. 647) dubs this version \"The Laser\" and formulates\nit as an injunction not to multiply fundamental entities beyond\nnecessity, together with the implicit understanding that there is no\nsuch injunction against multiplying derivative entities. Baron and\nTallant (forthcoming) attack 'razor-revisers' such as Schaffer,\narguing that principles such as The Laser fail to mesh with actual\npatterns of theory-choice in science and are also not vindicated by\nsome of the lines of justification for Occam's Razor.  \n(iii) ‘Intrinsic Value’\nJustifications \nSome philosophers have approached the issue of justifying simplicity\nprinciples by arguing that simplicity has intrinsic value as a\ntheoretical goal. Sober, for example, writes: \nSuch intrinsic value may be ‘primitive’ in some sense, or\nit may be analyzable as one aspect of some broader value. For those\nwho favor the second approach, a popular candidate for this broader\nvalue is aesthetic. Derkse (1992) is a book-length development of this\nidea, and echoes can be found in Quine's remarks—in connection\nwith his defense of Occam's Razor—concerning his taste for\n“clear skies” and “desert landscapes.” In\ngeneral, forging a connection between aesthetic virtue and simplicity\nprinciples seems better suited to defending methodological rather than\nepistemic principles. \n(iv) Justifications via Principles of Rationality \nAnother approach is to try to show how simplicity principles follow\nfrom other better established or better understood principles of\n rationality.[12]\n For example, some philosophers just stipulate that they will take\n‘simplicity’ as shorthand for whatever package of\ntheoretical virtues is (or ought to be) characteristic of rational\ninquiry. A more substantive alternative is to link simplicity to some\nparticular theoretical goal, for example unification (see Friedman\n1983). While this approach might work for elegance, it is less clear\nhow it can be maintained for ontological parsimony. Conversely, a line\nof argument which seems better suited to defending parsimony than to\ndefending elegance is to appeal to a principle of epistemological\nconservatism. Parsimony in a theory can be viewed as minimizing the\nnumber of ‘new’ kinds of entities and mechanisms which are\npostulated. This preference for old mechanisms may in turn be\njustified by a more general epistemological caution, or conservatism,\nwhich is characteristic of rational inquiry. \nNote that the above style of approach can be given both a rationalist\nand an empiricist gloss. If unification, or epistemological\nconservatism, are themselves a priori rational principles,\nthen simplicity principles stand to inherit this feature if this\napproach can be carried out successfully. However, philosophers with\nempiricist sympathies may also pursue analysis of this sort, and then\njustify the base principles either inductively from past success or\nnaturalistically from the fact that such principles are in fact used\nin science. \nTo summarize, the main problem with a priori justifications\nof simplicity principles is that it can be difficult to distinguish\nbetween an a priori defense and no defense(!).\nSometimes the theoretical virtue of simplicity is invoked as a\nprimitive, self-evident proposition that cannot be further justified\nor elaborated upon. (One example is the beginning of Goodman and\nQuine's 1947 paper, where they state that their refusal to admit\nabstract objects into their ontology is “based on a\nphilosophical intuition that cannot be justified by appeal to anything\nmore ultimate.”) (Goodman & Quine 1947, p. 174). It is\nunclear where leverage for persuading skeptics of the validity of such\nprinciples can come from, especially if the grounds provided are not\nthemselves to beg further questions. Misgivings of this sort have led\nto a shift away from justifications rooted in ‘first\nphilosophy’ towards approaches which engage to a greater degree\nwith the details of actual practice, both scientific and statistical.\nThese other approaches will be discussed in the next two sections. \nThe rise of naturalized epistemology as a movement within analytic\nphilosophy in the second half of the 20th Century has\nlargely sidelined the rationalist style of approach. From the\nnaturalistic perspective, philosophy is conceived of as continuous\nwith science, and not as having some independently privileged status.\nThe perspective of the naturalistic philosopher may be broader, but\nher concerns and methods are not fundamentally different from those of\nthe scientist. The conclusion is that science neither needs—nor\ncan legitimately be given—external philosophical justification.\nIt is against this broadly naturalistic background that some\nphilosophers have sought to provide an epistemic justification of\nsimplicity principles, and in particular principles of ontological\nparsimony such as Occam's Razor. \nThe main empirical evidence bearing on this issue consists of the\npatterns of acceptance and rejection of competing theories by working\nscientists. Einstein's development of Special Relativity—and its\nimpact on the hypothesis of the existence of the electromagnetic\nether—is one of the episodes most often cited (by both\nphilosophers and scientists) as an example of Occam's Razor in action\n(see Sober 1981, p. 153). The ether is by hypothesis a fixed medium\nand reference frame for the propagation of light (and other\nelectromagnetic waves). The Special Theory of Relativity includes the\nradical postulate that the speed of a light ray through a vacuum is\nconstant relative to an observer no matter what the state of motion of\nthe observer. Given this assumption, the notion of a universal\nreference frame is incoherent. Hence Special Relativity implies that\nthe ether does not exist. \nThis episode can be viewed as the replacement of an empirically\nadequate theory (the Lorentz-Poincaré theory) by a more\nontologically parsimonious alternative (Special Relativity). Hence it\nis often taken to be an example of Occam's Razor in action. The\nproblem with using this example as evidence for Occam's Razor is that\nSpecial Relativity (SR) has several other theoretical advantages over\nthe Lorentz-Poincaré (LP) theory in addition to being more\nontologically parsimonious. Firstly, SR is a simpler and more unified\ntheory than LP, since in order to ‘save the phenomena’ a\nnumber of ad hoc and physically unmotivated patches had been\nadded to LP. Secondly, LP raises doubts about the physical meaning of\ndistance measurements. According to LP, a rod moving with velocity,\nv, contracts by a factor of (1 −\nv2/c2)1/2. Thus\nonly distance measurements that are made in a frame at rest relative\nto the ether are valid without modification by a correction factor.\nHowever, LP also implies that motion relative to the ether is in\nprinciple undetectable. So how is distance to be measured? In other\nwords, the issue here is complicated by the fact that—according\nto LP—the ether is not just an extra piece of ontology but an\nundetectable extra piece. Given these advantages of SR over\nLP, it seems clear that the ether example is not merely a case of\nontological parsimony making up for an otherwise inferior theory. \nA genuine test-case for Occam's Razor must involve an ontologically\nparsimonious theory which is not clearly superior to its rivals in\nother respects. An instructive example is the following historical\nepisode from biogeography, a scientific subdiscipline which originated\ntowards the end of the 18th Century, and whose central\npurpose was to explain the geographical distribution of plant and\nanimal\n species.[13]\n In 1761, the French naturalist Buffon proposed the following law; \nThere were also known exceptions to Buffon's Law, for example remote\nislands which share (so-called) ‘cosmopolitan’ species\nwith continental regions a large distance away. \nTwo rival theories were developed to explain Buffon's Law and its\noccasional exceptions. According to the first theory, due to Darwin\nand Wallace, both facts can be explained by the combined effects of\ntwo causal mechanisms—dispersal, and evolution by natural\nselection. The explanation for Buffon's Law is as follows. Species\ngradually migrate into new areas, a process which Darwin calls\n“dispersal.” As natural selection acts over time on the\ncontingent initial distribution of species in different areas,\ncompletely distinct species eventually evolve. The existence of\ncosmopolitan species is explained by “improbable\ndispersal,” Darwin's term for dispersal across seemingly\nimpenetrable barriers by “occasional means of transport”\nsuch as ocean currents, winds, and floating ice. Cosmopolitan species\nare explained as the result of improbable dispersal in the relatively\nrecent past. \nIn the 1950's, Croizat proposed an alternative to the Darwin-Wallace\ntheory which rejects their presupposition of geographical stability.\nCroizat argues that tectonic change, not dispersal, is the principal\ncausal mechanism which underlies Buffon's Law. Forces such as\ncontinental drift, the submerging of ocean floors, and the formation\nof mountain ranges have acted within the time frame of evolutionary\nhistory to create natural barriers between species where at previous\ntimes there were none. Croizat's theory was the sophisticated\nculmination of a theoretical tradition which stretched back to the\nlate 17th Century. Followers of this so-called\n“extensionist” tradition had postulated the existence of\nancient land bridges to account for anomalies in the geographical\ndistribution of plants and\n animals.[14] \nExtensionist theories are clearly less ontologically parsimonious than\nDispersal Theories, since the former are committed to extra entities\nsuch as land bridges or movable tectonic plates. Moreover,\nExtensionist theories were (given the evidence then available) not\nmanifestly superior in other respects. Darwin was an early critic of\nExtensionist theories, arguing that they went beyond the\n“legitimate deductions of science.” Another critic of\nExtensionist theories pointed to their “dependence on ad hoc\nhypotheses, such as land bridges and continental extensions of vast\nextent, to meet each new distributional anomaly” (Fichman 1977,\np. 62) The debate over the more parsimonious Dispersal theories\ncentered on whether the mechanism of dispersal is sufficient on its\nown to explain the known facts about species distribution, without\npostulating any extra geographical or tectonic entities. \nThe criticisms leveled at the Extensionist and Dispersal theories\nfollow a pattern that is characteristic of situations in which one\ntheory is more ontologically parsimonious than its rivals. In such\nsituations the debate is typically over whether the extra ontology is\nreally necessary in order to explain the observed phenomena. The less\nparsimonious theories are condemned for profligacy, and lack of direct\nevidential support. The more parsimonious theories are condemned for\ntheir inadequacy to explain the observed facts. This illustrates a\nrecurring theme in discussions of simplicity—both inside and\noutside philosophy—namely, how the correct balance between\nsimplicity and goodness of fit ought to be struck. This theme takes\ncenter stage in the statistical approaches to simplicity discussed in\nSection 5. \nLess work has been done on describing episodes in science where\nelegance—as opposed to parsimony—has been (or may have\nbeen) the crucial factor. This may just reflect the fact that\nconsiderations linked to elegance are so pervasive in scientific\ntheory choice as to be unremarkable as a topic for special study. A\nnotable exception to this general neglect is the area of celestial\nmechanics, where the transition from Ptolemy to Copernicus to Kepler\nto Newton is an oft-cited example of simplicity considerations in\naction, and a case study which makes much more sense when seen through\nthe lens of elegance rather than of\n parsimony.[15] \nNaturalism depends on a number of presuppositions which are open to\ndebate. But even if these presuppositions are granted, the\nnaturalistic project of looking to science for methodological guidance\nwithin philosophy faces a major difficulty, namely how to ‘read\noff’ from actual scientific practice what the underlying\nmethodological principles are supposed to be. Burgess, for example,\nargues that what the patterns of scientific behavior show is not a\nconcern with multiplying entities per se, but a concern more\nspecifically with multiplying ‘causal mechanisms’ (Burgess\n1998). And Sober considers the debate in psychology over psychological\negoism versus motivational pluralism, arguing that the former theory\npostulates fewer types of ultimate desire but a larger number of\ncausal beliefs, and hence that comparing the parsimony of these two\ntheories depends on what is counted and how (Sober 2001, pp.\n14–5). Some of the concerns raised in Sections 1 and 2 also\nreappear in this context; for example, how the world is sliced up into\nkinds effects the extent to which a given theory\n‘multiplies’ kinds of entity. Justifying a particular way\nof slicing becomes more difficult once the epistemological naturalist\nleaves behind the a priori, metaphysical presuppositions of\nthe rationalist approach. \nOne philosophical debate where these worries over naturalism become\nparticularly acute is the issue of the application of parsimony\nprinciples to abstract objects. The scientific data\nis—in an important sense—ambiguous. Applications of\nOccam's Razor in science are always to concrete, causally efficacious\nentities, whether land-bridges, unicorns, or the luminiferous ether.\nPerhaps scientists apply an unrestricted version of Occam's Razor to\nthat portion of reality in which they are interested, namely the\nconcrete, causal, spatiotemporal world. Or perhaps scientists apply a\n‘concretized’ version of Occam's Razor unrestrictedly.\nWhich is the case? The answer determines which general philosophical\nprinciple we end up with: ought we to avoid the multiplication of\nobjects of whatever kind, or merely the multiplication of concrete\nobjects? The distinction here is crucial for a number of central\nphilosophical debates. Unrestricted Occam's Razor favors monism over\ndualism, and nominalism over platonism. By contrast,\n‘concretized’ Occam's Razor has no bearing on these\ndebates, since the extra entities in each case are not concrete. \nThe two approaches discussed in Sections 3 and 4—a\npriori rationalism and naturalized empiricism—are both in\nsome sense extreme. Simplicity principles are taken either to have no\nempirical grounding, or to have solely empirical grounding. Perhaps as\na result, both these approaches yield vague answers to certain key\nquestions about simplicity. In particular, neither seems equipped to\nanswer how exactly simplicity ought to be balanced against empirical\nadequacy. Simple but wildly inaccurate theories are not hard to come\nup with. Nor are accurate theories which are highly complex. But how\nmuch accuracy should be sacrificed for a gain in simplicity? The\nblack-and-white boundaries of the rationalism/empiricism divide may\nnot provide appropriate tools for analyzing this question. In\nresponse, philosophers have recently turned to the mathematical\nframework of probability theory and statistics, hoping in the process\nto combine sensitivity to actual practice with the\n‘trans-empirical’ strength of mathematics. \nPhilosophically influential early work in this direction was done by\nJeffreys and by Popper, both of whom tried to analyze simplicity in\nprobabilistic terms. Jeffreys argued that “the simpler laws have\nthe greater prior probability,” and went on to provide an\noperational measure of simplicity, according to which the prior\nprobability of a law is 2−k, where\nk = order + degree + absolute values of the coefficients,\nwhen the law is expressed as a differential equation (Jeffreys 1961,\np. 47). A generalization of Jeffreys' approach is to look not at\nspecific equations, but at families of equations. For\nexample, one might compare the family, LIN, of linear equations (of\nthe form y = a + bx) with the family, PAR,\nof parabolic equations (of the form y = a +\nbx + cx2). Since PAR is of higher degree\nthan LIN, Jeffreys' proposal assigns higher probability to LIN. Laws\nof this form are intuitively simpler (in the sense of being more\nelegant). \nPopper (1959) points out that Jeffreys' proposal, as it stands,\ncontradicts the axioms of probability. Every member of LIN is also a\nmember of PAR, where the coefficient, c, is set to 0. Hence\n‘Law, L, is a member of LIN’ entails ‘Law,\nL, is a member of PAR.’ Jeffreys' approach assigns\nhigher probability to the former than the latter. But it follows from\nthe axioms of probability that when A entails B, the\nprobability of B is greater than or equal to the probability\nof A. Popper argues, in contrast to Jeffreys, that LIN has\nlower prior probability than PAR. Hence LIN is—in\nPopper's sense—more falsifiable, and hence should be preferred\nas the default hypothesis. One response to Popper's objection is to\namend Jeffrey's proposal and restrict members of PAR to equations\nwhere c ≠ 0. \nMore recent work on the issue of simplicity has borrowed tools from\nstatistics as well as from probability theory. It should be noted that\nthe literature on this topic tends to use the terms\n‘simplicity’ and ‘parsimony’ more-or-less\ninterchangeably (see Sober 2003). But, whichever term is preferred,\nthere is general agreement among those working in this area that\nsimplicity is to be cashed out in terms of the number of free (or\n‘adjustable’) parameters of competing hypotheses. Thus the\nfocus here is totally at the level of theory. Philosophers who have\nmade important contributions to this approach include Forster and\nSober (1994) and Lange (1995). \nThe standard case in the statistical literature on parsimony concerns\n curve-fitting.[16]\n We imagine a situation in which we have a set of discrete data points\nand are looking for the curve (i.e. function) which has generated\nthem. The issue of what family of curves the answer belongs in (e.g.\nin LIN or in PAR) is often referred to as model-selection.\nThe basic idea is that there are two competing criteria for model\nselection—parsimony and goodness of fit. The possibility of\nmeasurement error and ‘noise’ in the data means that the\ncorrect curve may not go through every data point. Indeed, if goodness\nof fit were the only criterion then there would be a danger of\n‘overfitting’ the model to accidental discrepancies\nunrepresentative of the broader regularity. Parsimony acts as a\ncounterbalance to such overfitting, since a curve passing through\nevery data point is likely to be very convoluted and hence have many\nadjusted parameters. \nIf proponents of the statistical approach are in general agreement\nthat simplicity should be cashed out in terms of number of parameters,\nthere is less unanimity over what the goal of simplicity principles\nought to be. This is partly because the goal is often not made\nexplicit. (An analogous issue arises in the case of Occam's Razor.\n‘Entities are not to be multiplied beyond necessity.’ But\nnecessity for what, exactly?) Forster distinguishes two\npotential goals of model selection, namely probable truth and\npredictive accuracy, and claims that these are importantly distinct\n(Forster 2001, p. 95). Forster argues that predictive accuracy tends\nto be what scientists care about most. They care less about the\nprobability of an hypothesis being exactly right than they do about it\nhaving a high degree of accuracy. \nOne reason for investigating statistical approaches to simplicity is a\ndissatisfaction with the vagaries of the a priori and\nnaturalistic approaches. Statisticians have come up with a variety of\nnumerically specific proposals for the trade-off between simplicity\nand goodness of fit. However, these alternative proposals disagree\nabout the ‘cost’ associated with more complex hypotheses.\nTwo leading contenders in the recent literature on model selection are\nthe Akaike Information Criterion [AIC] and the Bayesian Information\nCriterion [BIC]. AIC directs theorists to choose the model with the\nhighest value of {log\nL(Θk)/n} −\nk/n, where Θk is the best-fitting\nmember of the class of curves of polynomial degree k, log\nL is log-likelihood, and n is the sample size. By\ncontrast, BIC maximizes the value of {log\nL(Θk)/n} −\nklog[n]/2n. In effect, BIC gives an extra\npositive weighting to simplicity by a factor of log[n]/2\n(where n is the size of the\n sample).[17] \nExtreme answers to the trade-off problem seem to be obviously\ninadequate. Always picking the model with the best fit to the data,\nregardless of its complexity, faces the prospect (mentioned earlier)\nof ‘overfitting’ error and noise in the data. Always\npicking the simplest model, regardless of its fit to the data, cuts\nthe model free from any link to observation or experiment. Forster\nassociates the ‘Always Complex’ and the ‘Always\nSimple’ rule with empiricism and rationalism\n respectively.[18]\n All the candidate rules that are seriously discussed by statisticians\nfall in between these two extremes. Yet they differ in their answers\nover how much weight to give simplicity in its trade-off against\ngoodness of fit. In addition to AIC and BIC, other rules include\nNeyman-Pearson hypothesis testing, and the minimum description length\n(MDL) criterion. \nThere are at least three possible responses to the varying answers to\nthe trade-off problem provided by different criteria. One response,\nfavored by Forster and by Sober, is to argue that there is no genuine\nconflict here because the different criteria have different aims. Thus\nAIC and BIC might both be optimal criteria, if AIC is aiming to\nmaximize predictive accuracy whereas BIC is aiming to maximize\nprobable truth. Another difference that may influence the choice of\ncriterion is whether the goal of the model is to extrapolate beyond\ngiven data or interpolate between known data points. A second\nresponse, typically favored by statisticians, is to argue that the\nconflict is genuine but that it has the potential to be resolved by\nanalyzing (using both mathematical and empirical methods) which\ncriterion performs best over the widest class of possible situations.\nA third, more pessimistic, response is to argue that the conflict is\ngenuine but is unresolvable. Kuhn (1977) takes this line, claiming\nthat how much weight individual scientists give a particular\ntheoretical virtue, such as simplicity, is solely a matter of taste,\nand is not open to rational resolution. McAllister (2007) draws\nontological morals from a similar conclusion, arguing that sets of\ndata typically exhibit multiple patterns, and that different patterns\nmay be highlighted by different quantitative techniques. \nAside from this issue of conflicting criteria, there are other\nproblems with the statistical approach to simplicity. One problem,\nwhich afflicts any approach emphasizing the elegance aspect of\nsimplicity, is language relativity. Crudely put, hypotheses which are\nsyntactically very complex in one language may be syntactically very\nsimple in another. The traditional philosophical illustration of this\nproblem is Goodman's ‘grue’ challenge to induction. Are\nstatistical approaches to the measurement of simplicity similarly\nlanguage relative, and—if so—what justifies choosing one\nlanguage over another? It turns out that the statistical approach has\nthe resources to at least partially deflect the charge of language\nrelativity. Borrowing techniques from information theory, it can be\nshown that certain syntactic measures of simplicity are asymptotically\nindependent of choice of measurement\n language.[19] \nA second problem for the statistical approach is whether it can\naccount not only for our preference for small numbers over large\nnumbers (when it comes to picking values for coefficients or exponents\nin model equations), but also our preference for whole numbers and\nsimple fractions over other values. In Gregor Mendel's original\nexperiments on the hybridization of garden peas, he crossed pea\nvarieties with different specific traits, such as tall versus short or\ngreen seeds versus yellow seeds, and then self-pollinated the hybrids\nfor one or more\n generations.[20]\n In each case one trait was present in all the first-generation\nhybrids, but both traits were present in subsequent generations.\nAcross his experiments with seven different such traits, the ratio of\ndominant trait to recessive trait averaged 2.98 : 1. On this\nbasis, Mendel hypothesized that the true ratio is 3 : 1.\nThis ‘rounding’ was made prior to the formulation of any\nexplanatory model, hence it cannot have been driven by any\ntheory-specific consideration. This raises two related questions.\nFirst, in what sense is the 3 : 1 ratio hypothesis\nsimpler than the 2.98 : 1 ratio hypothesis? Second,\ncan this choice be justified within the framework of the statistical\napproach to simplicity? The more general worry lying behind these\nquestions is whether the statistical approach, in defining simplicity\nin terms of number of adjustable parameters, is replacing the broad\nissue of simplicity with a more narrowly—and perhaps\narbitrarily—defined set of issues. \nA third problem with the statistical approach concerns whether it can\nshed any light on the specific issue of ontological parsimony. At\nfirst glance, one might think that the postulation of extra entities\ncan be attacked on probabilistic grounds. For example, quantum\nmechanics together with the postulation ‘There exist\nunicorns’ is less probable than quantum mechanics alone, since\nthe former logically entails the latter. However, as Sober has pointed\nout, it is important here to distinguish between agnostic\nOccam's Razor and atheistic Occam's Razor. Atheistic OR\ndirects theorists to claim that unicorns do not exist, in the\nabsence of any compelling evidence in their favor. And there is no\nrelation of logical entailment between {QM + there exist\nunicorns} and {QM + there do not exist unicorns}. This also\nlinks back to the terminological issue. Models involving circular\norbits are more parsimonious—in the statisticians' sense of\n‘parsimonious’—than models involving elliptical\norbits, but the latter models do not postulate the existence of any\nmore things in the world. \nThis section addresses three distinct issues concerning simplicity and\nits relation to other methodological issues. These issues concern\nquantitative parsimony, plenitude, and induction. \nTheorists tend to be frugal in their postulation of new entities. When\na trace is observed in a cloud-chamber, physicists may seek to explain\nit in terms of the influence of a hitherto unobserved particle. But,\nif possible, they will postulate one such unobserved particle, not\ntwo, or twenty, or 207 of them. This desire to minimize the number of\nindividual new entities postulated is often referred to as\nquantitative parsimony. David Lewis articulates the attitude\nof many philosophers when he writes: \nIs the initial assumption that one particle is acting to cause the\nobserved trace more rational than the assumption that 207 particles\nare so acting? Or is it merely the product of wishful thinking,\naesthetic bias, or some other non-rational influence? \nNolan (1997) examines these questions in the context of the discovery\nof the\n neutrino.[21]\n Physicists in the 1930's were puzzled by certain anomalies arising\nfrom experiments in which radioactive atoms emit electrons during\nso-called Beta decay. In these experiments the total spin of the\nparticles in the system before decay exceeds by\n1/2 the total spin of the (observed) emitted\nparticles. Physicists' response was to posit a ‘new’\nfundamental particle, the neutrino, with spin\n1/2 and to hypothesize that exactly one neutrino\nis emitted by each electron during Beta decay. \nNote that there is a wide range of very similar neutrino theories\nwhich can also account for the missing spin. \nH1: 1 neutrino with a spin of 1/2 is\nemitted in each case of Beta decay. \nH2: 2 neutrinos, each with a spin of\n1/4 are emitted in each case of Beta decay. \nand, more generally, for any positive integer n, \nEach of these hypotheses adequately explains the observation of a\nmissing 1/2-spin following Beta decay. Yet the\nmost quantitatively parsimonious hypothesis, H1, is the\nobvious default\n choice.[22] \nOne promising approach is to focus on the relative explanatory power\nof the alternative hypotheses, H1, H2, …\nHn. When neutrinos were first postulated in the 1930's,\nnumerous experimental set-ups were being devised to explore the\nproducts of various kinds of particle decay. In none of these\nexperiments had cases of ‘missing’\n1/3-spin, or 1/4-spin, or\n1/100-spin been found. The absence of these\nsmaller fractional spins was a phenomenon which competing neutrino\nhypotheses might potentially help to explain. \nConsider the following two competing neutrino hypotheses: \nH1: 1 neutrino with a spin of 1/2 is\nemitted in each case of Beta decay. \nH10: 10 neutrinos, each with a spin of\n1/20, are emitted in each case of Beta\ndecay. \nWhy has no experimental set-up yielded a ‘missing’\nspin-value of 1/20? H1 allows a\nbetter answer to this question than H10 does, for\nH1 is consistent with a simple and parsimonious\nexplanation, namely that there exist no particles with spin\n1/20 (or less). In the case of H10,\nthis potential explanation is ruled out because H10\nexplicitly postulates particles with spin 1/20.\nOf course, H10 is consistent with other hypotheses\nwhich explain the non-occurrence of missing\n1/20-spin. For example, one might conjoin to\nH10 the law that neutrinos are always emitted in groups of\nten. However, this would make the overall explanation less\nsyntactically simple, and hence less virtuous in other respects. In\nthis case, quantitative parsimony brings greater explanatory power.\nLess quantitatively parsimonious hypotheses can match this power only\nby adding auxiliary claims which decrease their syntactic simplicity.\nThus the preference for quantitatively parsimonious hypotheses emerges\nas one facet of a more general preference for hypotheses with greater\nexplanatory power. \nOne distinctive feature of the neutrino example is that it is\n‘additive.’ It involves postulating the existence of a\ncollection of qualitatively identical objects which collectively\nexplain the observed phenomenon. The explanation is additive in the\nsense that the overall phenomenon is explained by summing the\nindividual positive contributions of each\n object.[23]\n Whether the above approach can be extended to non-additive cases\ninvolving quantitative parsimony is an interesting question. Jansson\nand Tallant (forthcoming) argue that it can, and they offer a\nprobabilistic analysis that aims to bring together a variety of\ndifferent cases where quantitative parsimony plays a role in\nhypothesis selection. Consider a case in which the aberrations of a\nplanet's orbit can be explained by postulating a single unobserved\nplanet, or it can be explained by postulating two or more unobserved\nplanets. In order for the latter situation to be actual, the multiple\nplanets must orbit in certain restricted ways so as to match the\neffects of a single planet. Prima facie this is unlikely, and\nthis counts against the less quantitatively parsimonious\nhypothesis. \nRanged against the principles of parsimony discussed in previous\nsections is an equally firmly rooted (though less well-known)\ntradition of what might be termed “principles of explanatory\n sufficiency.”[24]\n These principles have their origins in the same medieval\ncontroversies that spawned Occam's Razor. Ockham's contemporary,\nWalter of Chatton, proposed the following counter-principle to Occam's\nRazor: \nA related counter-principle was later defended by Kant: \nThere is no inconsistency in the coexistence of these two families of\nprinciples, for they are not in direct conflict with each other.\nConsiderations of parsimony and of explanatory sufficiency function as\nmutual counter-balances, penalizing theories which stray into\nexplanatory inadequacy or ontological\n excess.[25]\n What we see here is an historical echo of the contemporary debate\namong statisticians concerning the proper trade-off between simplicity\nand goodness of fit. \nThere is, however, a second family of principles which do appear\ndirectly to conflict with Occam's Razor. These are so-called\n‘principles of plenitude.’ Perhaps the best-known version\nis associated with Leibniz, according to whom God created the best of\nall possible worlds with the greatest number of possible entities.\nMore generally, a principle of plenitude claims that if it is\npossible for an object to exist then that object\nactually exists. Principles of plenitude conflict with\nOccam's Razor over the existence of physically possible but\nexplanatorily idle objects. Our best current theories presumably do\nnot rule out the existence of unicorns, but nor do they provide any\nsupport for their existence. According to Occam's Razor we ought\nnot to postulate the existence of unicorns. According to a\nprinciple of plenitude we ought to postulate their\nexistence. \nThe rise of particle physics and quantum mechanics in the\n20th Century led to various principles of plenitude being\nappealed to by scientists as an integral part of their theoretical\nframework. A particularly clear-cut example of such an appeal is the\ncase of magnetic\n monopoles.[26]\n The 19th-century theory of electromagnetism postulated\nnumerous analogies between electric charge and magnetic charge. One\ntheoretical difference is that magnetic charges must always come in\noppositely-charged pairs, called “dipoles” (as in the\nNorth and South poles of a bar magnet), whereas single electric\ncharges, or “monopoles,” can exist in isolation. However,\nno actual magnetic monopole had ever been observed. Physicists began\nto wonder whether there was some theoretical reason why monopoles\ncould not exist. It was initially thought that the newly developed\ntheory of quantum mechanics ruled out the possibility of magnetic\nmonopoles, and this is why none had ever been detected. However, in\n1931 the physicist Paul Dirac showed that the existence of monopoles\nis consistent with quantum mechanics, although it is not required by\nit. Dirac went on to assert the existence of monopoles, arguing that\ntheir existence is not ruled out by theory and that “under these\ncircumstances one would be surprised if Nature had made no use of\nit” (Dirac 1930, p. 71, note 5). This appeal to plenitude was\nwidely—though not universally—accepted by other\nphysicists. \nOthers have been less impressed by Dirac's line of argument: \nIt is difficult to know how to interpret these principles of\nplenitude. Quantum mechanics diverges from classical physics by\nreplacing of a deterministic model of the universe with a model based\non objective probabilities. According to this probabilistic model,\nthere are numerous ways the universe could have evolved from its\ninitial state, each with a certain probability of occurring that is\nfixed by the laws of nature. Consider some kind of object, say\nunicorns, whose existence is not ruled out by the initial conditions\nplus the laws of nature. Then one can distinguish between a weak and a\nstrong version of the principle of plenitude. According to the weak\nprinciple, if there is a small finite probability of unicorns existing\nthen given enough time and space unicorns will exist. According to the\nstrong principle, it follows from the theory of quantum mechanics that\nif it is possible for unicorns to exist then they do exist. One way in\nwhich this latter principle may be cashed out is in the\n‘many-worlds’ interpretation of quantum mechanics,\naccording to which reality has a branching structure in which every\npossible outcome is realized. \nThe problem of induction is closely linked to the issue of simplicity.\nOne obvious link is between the curve-fitting problem and the\ninductive problem of predicting future outcomes from observed data.\nLess obviously, Schulte (1999) argues for a connection between\ninduction and ontological parsimony. Schulte frames the problem of\ninduction in information-theoretic terms: given a data-stream of\nobservations of non-unicorns (for example), what general conclusion\nshould be drawn? He argues for two constraints on potential rules.\nFirst, the rule should converge on the truth in the long run (so if no\nunicorns exist then it should yield this conclusion). Second, the rule\nshould minimize the maximum number of changes of hypothesis, given\ndifferent possible future observations. Schulte argues that the\n‘Occam Rule’—conjecture that Ω does not exist\nuntil it has been detected in an experiment—is optimal relative\nto these constraints. An alternative rule—for example,\nconjecturing that Ω exists until 1 million negative results have\nbeen obtained—may result in two changes of hypothesis if, say,\nΩ's are not detected until the 2 millionth experiment. The Occam\nRule leads to at most one change of hypothesis (when an Ω is\nfirst detected). (See also Kelly 2004, 2007.) Schulte (2008) applies\nthis approach to the problem of discovering conservation laws in\nparticle physics. The analysis has been criticized by Fitzpatrick\n(2013), who raises doubts about why long-run convergence to the truth\nshould matter when it comes to predicting the outcome of the very next\nexperiment. \nWith respect to the justification question, arguments have been made\nin both directions. Scientists are often inclined to justify\nsimplicity principles on broadly inductive grounds. According to this\nargument, scientists select new hypotheses based partly on criteria\nthat have been generated inductively from previous cases of theory\nchoice. Choosing the most parsimonious of the acceptable alternative\nhypotheses has tended to work in the past. Hence scientists continue\nto use this as a rule of thumb, and are justified in so doing on\ninductive grounds. One might try to bolster this point of view by\nconsidering a counterfactual world in which all the fundamental\nconstituents of the universe exist in pairs. In such a\n‘pairwise’ world, scientists might well prefer pairwise\nhypotheses in general to their more parsimonious rivals. This line of\nargument has a couple of significant weaknesses. Firstly, one might\nlegitimately wonder just how successful the choice of parsimonious\nhypotheses has been; examples from chemistry spring to mind, such as\noxygen molecules containing two atoms rather than one. Secondly, and\nmore importantly, there remains the issue of explaining why\nthe preference for parsimonious hypotheses in science has been as\nsuccessful as it has been. \nMaking the justificatory argument in the reverse direction, from\nsimplicity to induction, has a strong historical precedent in\nphilosophical approaches to the problem of induction, from Hume\nonwards. Justifying the ‘straight rule’ of induction by\nappeal to some general Principle of Uniformity is an initially\nappealing response to the skeptical challenge. However, in the absence\nof a defense of the underlying Principle itself (and one which does\nnot, on pain of circularity, depend inductively on past success), it\nis unclear how much progress this represents. There have also been\nattempts (see e.g. Steel 2009) to use simplicity considerations to\nrespond to Nelson Goodman's ‘new riddle of induction.’","contact.mail":"abaker1@swarthmore.edu","contact.domain":"swarthmore.edu"}]
