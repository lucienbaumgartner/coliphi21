[{"date.published":"2015-06-02","date.changed":"2019-08-09","url":"https://plato.stanford.edu/entries/word-meaning/","author1":"Luca Gasparri","author2":"Diego Marconi","author1.info":"https://lcgasparri.github.io/","entry":"word-meaning","body.text":"\n\n\nWord meaning has played a somewhat marginal role in early contemporary\nphilosophy of language, which was primarily concerned with the\nstructural features of sentence meaning and showed less interest in\nthe nature of the word-level input to compositional processes.\nNowadays, it is well-established that the study of word meaning is\ncrucial to the inquiry into the fundamental properties of human\nlanguage. This entry provides an overview of the way issues related to\nword meaning have been explored in analytic philosophy and a summary\nof relevant research on the subject in neighboring scientific domains.\nThough the main focus will be on philosophical problems, contributions\nfrom linguistics, psychology, neuroscience and artificial intelligence\nwill also be considered, since research on word meaning is highly\ninterdisciplinary.\n\nThe notions of word and word meaning are problematic\nto pin down, and this is reflected in the difficulties one encounters\nin defining the basic terminology of lexical semantics. In part, this\ndepends on the fact that the term ‘word’ itself is highly\npolysemous (see, e.g., Matthews 1991; Booij 2007; Lieber 2010). For\nexample, in ordinary parlance ‘word’ is ambiguous between\na type-level reading (as in “Color and colour\nare spellings of the same word”), an occurrence-level reading\n(as in “there are thirteen words in the tongue-twister How\nmuch wood would a woodchuck chuck if a woodchuck could chuck\nwood?”), and a token-level reading (as in “John\nerased the last two words on the blackboard”). Before proceeding\nfurther, let us then elucidate the notion of word in more detail\n (Section 1.1),\n and lay out the key questions that will guide our discussion of word\nmeaning in the rest of the entry\n (Section 1.2). \nWe can distinguish two fundamental approaches to the notion of word.\nOn one side, we have linguistic approaches, which\ncharacterize the notion of word by reflecting on its explanatory role\nin linguistic research (for a survey on explanation in linguistics,\nsee Egré 2015). These approaches often end up splitting the\nnotion of word into a number of more fine-grained and theoretically\nmanageable notions, but still tend to regard ‘word’ as a\nterm that zeroes in on a scientifically respectable concept (e.g., Di\nSciullo & Williams 1987). For example, words are the primary locus\nof stress and tone assignment, the basic domain of morphological\nconditions on affixation, clitization, compounding, and the theme of\nphonological and morphological processes of assimilation, vowel shift,\nmetathesis, and reduplication (Bromberger 2011). \nOn the other side, we have metaphysical approaches, which\nattempt to pin down the notion of word by inquiring into the\nmetaphysical nature of words. These approaches typically deal with\nsuch questions as “what are words?”, “how should\nwords be individuated?”, and “on what conditions two\nutterances count as utterances of the same word?”. For example,\nKaplan (1990, 2011) has proposed to replace the orthodox type-token\naccount of the relation between words and word tokens with a\n“common currency” view on which words relate to their\ntokens as continuants relate to stages in four-dimensionalist\nmetaphysics (see the entries on\n types and tokens\n and\n identity over time).\n Other contributions to this debate can be found, a.o., in McCulloch\n(1991), Cappelen (1999), Alward (2005), Hawthorne & Lepore (2011),\nSainsbury & Tye (2012), Gasparri (2016), and Irmak\n(forthcoming). \nFor the purposes of this entry, we can rely on the following\nstipulation. Every natural language has a lexicon organized\ninto lexical entries, which contain information about word\ntypes or lexemes. These are the smallest linguistic\nexpressions that are conventionally associated with a\nnon-compositional meaning and can be articulated in isolation to\nconvey semantic content. Word types relate to word tokens and\noccurrences just like phonemes relate to phones in phonological\ntheory. To understand the parallelism, think of the variations in the\nplace of articulation of the phoneme /n/, which is pronounced as the\nvoiced bilabial nasal [m] in “ten bags” and as the voiced\nvelar nasal [ŋ] in “ten gates”. Just as phonemes are\nabstract representations of sets of phones (each defining one way the\nphoneme can be instantiated in speech), lexemes can be defined as\nabstract representations of sets of words (each defining one way the\nlexeme can be instantiated in sentences). Thus, ‘do’,\n‘does’, ‘done’ and ‘doing’ are\nmorphologically and graphically marked realizations of the same\nabstract word type do. To wrap everything into a single\nformula, we can say that the lexical entries listed in a\nlexicon set the parameters defining the instantiation\npotential of word types in sentences, utterances and inscriptions (cf.\nMurphy 2010). In what follows, unless otherwise indicated, our talk of\n“word meaning” should be understood as talk of “word\ntype meaning” or “lexeme meaning”, in the sense we\njust illustrated. \nAs with general theories of meaning (see the entry on\n theories of meaning),\n two kinds of theory of word meaning can be distinguished. The first\nkind, which we can label a semantic theory of word meaning,\nis a theory interested in clarifying what meaning-determining\ninformation is encoded by the words of a natural language. A framework\nestablishing that the word ‘bachelor’ encodes the lexical\nconcept adult unmarried\nmale would be an example of a semantic theory of word meaning.\nThe second kind, which we can label a foundational theory of\nword meaning, is a theory interested in elucidating the facts in\nvirtue of which words come to have the semantic properties they have\nfor their users. A framework investigating the dynamics of semantic\nchange and social coordination in virtue of which the word\n‘bachelor’ is assigned the function of expressing the\nlexical concept adult\nunmarried male would be an example of a foundational theory of\nword meaning. Likewise, it would be the job of a foundational theory\nof word meaning to determine whether words have the semantic\nproperties they have in virtue of social conventions, or whether\nsocial conventions do not provide explanatory purchase on the facts\nthat ground word meaning (see the entry on\n convention). \nObviously, the endorsement of a given semantic theory is bound to\nplace important constraints on the claims one might propose about the\nfoundational attributes of word meaning, and vice versa.\nSemantic and foundational concerns are often interdependent, and it is\ndifficult to find theories of word meaning which are either purely\nsemantic or purely foundational. According to Ludlow (2014), for\nexample, the fact that word meaning is systematically underdetermined\n(a semantic matter) can be explained in part by looking at the\nprocesses of linguistic negotiation whereby discourse partners\nconverge on the assignment of shared meanings to the words of their\nlanguage (a foundational matter). However, semantic and foundational\ntheories remain in principle different and designed to answer partly\nnon-overlapping sets of questions. \nOur focus in this entry will be on semantic theories of word\nmeaning, i.e., on theories that try to provide an answer to such\nquestions as “what is the nature of word meaning?”,\n“what do we know when we know the meaning of a word?”, and\n“what (kind of) information must a speaker associate to the\nwords of a language in order to be a competent user of its\nlexicon?”. However, we will engage in foundational\nconsiderations whenever necessary to clarify how a given framework\naddresses issues in the domain of a semantic theory of word\nmeaning. \nThe study of word meaning became a mature academic enterprise in the\n19th century, with the birth of historical-philological\nsemantics\n (Section 2.2).\n Yet, matters related to word meaning had been the subject of much\ndebate in earlier times. We can distinguish three major classical\napproaches to word meaning: speculative etymology, rhetoric, and\nclassical lexicography (Meier-Oeser 2011; Geeraerts 2013). We describe\nthem briefly in\n Section 2.1. \nThe prototypical example of speculative etymology is perhaps the\nCratylus (383a-d), where Plato presents his well-known\nnaturalist thesis about word meaning. According to Plato, natural kind\nterms express the essence of the objects they denote and words are\nappropriate to their referents insofar as they implicitly describe the\nproperties of their referents (see the entry on\n Plato’s Cratylus).\n For example, the Greek word ‘anthrôpos’\ncan be broken down into anathrôn ha opôpe, which\ntranslates as “one who reflects on what he has seen”: the\nword used to denote humans reflects their being the only animal\nspecies which possesses the combination of vision and intelligence.\nFor speculative etymology, there is a natural or non-arbitrary\nrelation between words and their meaning, and the task of the theorist\nis to make this relation explicit through an analysis of the\ndescriptive, often phonoiconic mechanisms underlying the genesis of\nwords. More on speculative etymology in Malkiel (1993), Fumaroli\n(1999), and Del Bello (2007). \nThe primary aim of the rhetorical tradition was the study of\nfigures of speech. Some of these concern sentence-level variables such\nas the linear order of the words occurring in a sentence (e.g.,\nparallelism, climax, anastrophe); others are lexical in nature and\ndepend on using words in a way not intended by their normal or literal\nmeaning (e.g., metaphor, metonymy, synecdoche). Although originated\nfor stylistic and literary purposes, the identification of regular\npatterns in the figurative use of words initiated by the rhetorical\ntradition provided a first organized framework to investigate the\nsemantic flexibility of words, and laid the groundwork for further\ninquiry into our ability to use lexical expressions beyond the\nboundaries of their literal meaning. More on the rhetorical tradition\nin Kennedy (1994), Herrick (2004), and Toye (2013). \nFinally, classical lexicography and the practice of writing\ndictionaries played an important role in systematizing the descriptive\ndata on which later inquiry would rely to illuminate the relationship\nbetween words and their meaning. Putnam’s (1970) claim that it\nwas the phenomenon of writing (and needing) dictionaries that gave\nrise to the idea of a semantic theory is probably an overstatement.\nBut the inception of lexicography certainly had an impact on the\ndevelopment of modern theories of word meaning. The practice of\nseparating dictionary entries via lemmatization and defining them\nthrough a combination of semantically simpler elements provided a\nstylistic and methodological paradigm for much subsequent research on\nlexical phenomena, such as decompositional theories of word meaning.\nMore on classical lexicography in Béjoint (2000), Jackson\n(2002), and Hanks (2013). \nHistorical-philological semantics incorporated elements from all the\nabove classical traditions and dominated the linguistic scene roughly\nfrom 1870 to 1930, with the work of scholars such as Michel\nBréal, Hermann Paul, and Arsène Darmesteter (Gordon\n1982). In particular, it absorbed from speculative etymology an\ninterest in the conceptual mechanisms underlying the formation of word\nmeaning, it acquired from rhetorical analysis a taxonomic toolkit for\nthe classification of lexical phenomena, and it assimilated from\nlexicography and textual philology the empirical basis of descriptive\ndata that subsequent theories of word meaning would have to account\nfor (Geeraerts 2013). \nOn the methodological side, the key features of the approach to word\nmeaning introduced by historical-philological semantics can be\nsummarized as follows. First, it had a diachronic and pragmatic\norientation. That is, it was primarily concerned with the historical\nevolution of word meaning rather than with word meaning statically\nunderstood, and attributed great importance to the contextual\nflexibility of word meaning. Witness Paul’s (1920 [1880])\ndistinction between usuelle Bedeutung and okkasionelle\nBedeutung, or Bréal’s (1924 [1897]) account of\npolysemy as a byproduct of semantic change. Second, it looked at word\nmeaning primarily as a psychological phenomenon. It assumed that the\nsemantic properties of words should be defined in mentalistic terms\n(i.e., words signify “concepts” or “ideas” in\na broad sense), and that the dynamics of sense modulation, extension,\nand contraction that underlie lexical change correspond to broader\npatterns of conceptual activity in the human mind. Interestingly,\nwhile the classical rhetorical tradition had conceived of tropes as\nmarginal linguistic phenomena whose investigation, albeit important,\nwas primarily motivated by stylistic concerns, for\nhistorical-philological semantics the psychological mechanisms\nunderlying the production and the comprehension of figures of speech\nwere part of the ordinary life of languages, and engines of the\nevolution of all aspects of lexical systems (Nerlich 1992). \nThe contribution made by historical-philological semantics to the\nstudy of word meaning had a long-lasting influence. First, with its\nemphasis on the principles of semantic change, historical-philological\nsemantics was the first systematic framework to focus on the dynamic\nnature of word meaning, and established contextual flexibility as the\nprimary explanandum for a theory of word meaning (Nerlich & Clarke\n1996, 2007). This feature of historical-philological semantics is a\nclear precursor of the emphasis placed on context-sensitivity by many\nsubsequent approaches to word meaning, both in philosophy (see\n Section 3)\n and in linguistics (see\n Section 4).\n Second, the psychologistic approach to word meaning fostered by\nhistorical philological-semantics added to the agenda of linguistic\nresearch the question of how word meaning relates to cognition at\nlarge. If word meaning is essentially a psychological phenomenon, what\npsychological categories should be used to characterize it? What is\nthe dividing line separating the aspects of our mental life that\nconstitute knowledge of word meaning from those that do not? As we\nshall see, this question will constitute a central concern for\ncognitive theories of word meaning (see\n Section 5). \nIn this section we shall review some semantic and metasemantic\ntheories in analytic philosophy that bear on how lexical meaning\nshould be conceived and described. We shall follow a roughly\nchronological order. Some of these theories, such as Carnap’s\ntheory of meaning postulates and Putnam’s theory of stereotypes,\nhave a strong focus on lexical meaning, whereas others, such as\nMontague semantics, regard it as a side issue. However, such negative\nviews form an equally integral part of the philosophical debate on\nword meaning. \nBy taking the connection of thoughts and truth as the basic issue of\nsemantics and regarding sentences as “the proper means of\nexpression for a thought” (Frege 1979a [1897]), Frege paved the\nway for the 20th century priority of sentential meaning\nover lexical meaning: the semantic properties of subsentential\nexpressions such as individual words were regarded as derivative, and\nidentified with their contribution to sentential meaning. Sentential\nmeaning was in turn identified with truth conditions, most explicitly\nin Wittgenstein’s Tractatus logico-philosophicus\n(1922). However, Frege never lost interest in the “building\nblocks of thoughts” (Frege 1979b [1914]), i.e., in the semantic\nproperties of subsentential expressions. Indeed, his theory of sense\nand reference for names and predicates may be counted as the inaugural\ncontribution to lexical semantics within the analytic tradition (see\nthe entry on\n Gottlob Frege).\n It should be noted that Frege did not attribute semantic properties\nto lexical units as such, but to what he regarded as a\nsentence’s logical constituents: e.g., not to the word\n‘dog’ but to the predicate ‘is a dog’. In\nlater work this distinction was obliterated and Frege’s semantic\nnotions came to be applied to lexical units. \nPossibly because of lack of clarity affecting the notion of sense, and\nsurely because of Russell’s (1905) authoritative criticism of\nFregean semantics, word meaning disappeared from the philosophical\nscene during the 1920s and 1930s. In Wittgenstein’s\nTractatus the “real” lexical units, i.e., the\nconstituents of a completely analyzed sentence, are just names, whose\nsemantic properties are exhausted by their reference. In\nTarski’s (1933) work on formal languages, which was taken as\ndefinitional of the very field of semantics for some time, lexical\nunits are semantically categorized into different classes (individual\nconstants, predicative constants, functional constants) depending on\nthe logical type of their reference, i.e., according to whether they\ndesignate individuals in a domain of interpretation, classes of\nindividuals (or of n-tuples of individuals), or functions\ndefined over the domain. However, Tarski made no attempt nor felt any\nneed to represent semantic differences among expressions belonging to\nthe same logical type (e.g., between one-place predicates such as\n‘dog’ and ‘run’, or between two-place\npredicates such as ‘love’ and ‘left of’). See\nthe entry on\n Alfred Tarski. \nQuine (1943) and Church (1951) rehabilitated Frege’s distinction\nof sense and reference. Non-designating words such as\n‘Pegasus’ cannot be meaningless: it is precisely the\nmeaning of ‘Pegasus’ that allows speakers to establish\nthat the word lacks reference. Moreover, as Frege (1892) had argued,\ntrue factual identities such as “Morning Star = Evening\nStar” do not state synonymies; if they did, any competent\nspeaker of the language would be aware of their truth. Along these\nlines, Carnap (1947) proposed a new formulation of the sense/reference\ndichotomy, which was translated into the distinction between\nintension and extension. The notion of intension was\nintended to be an explicatum of Frege’s\n“obscure” notion of sense: two expressions have the same\nintension if and only if they have the same extension in every\npossible world or, in Carnap’s terminology, in every state\ndescription (i.e., in every maximal consistent set of atomic\nsentences and negations of atomic sentences). Thus,\n‘round’ and ‘spherical’ have the same\nintension (i.e., they express the same function from possible worlds\nto extensions) because they apply to the same objects in every\npossible world. Carnap later suggested that intensions could be\nregarded as the content of lexical semantic competence: to know the\nmeaning of a word is to know its intension, “the general\nconditions which an object must fulfill in order to be denoted by\n[that] word” (Carnap 1955). However, such general conditions\nwere not spelled out by Carnap (1947). Consequently, his system did\nnot account, any more than Tarski’s, for semantic differences\nand relations among words belonging to the same semantic category:\nthere were possible worlds in which the same individual a\ncould be both a married man and a bachelor, as no constraints were\nplaced on either word’s intension. One consequence, as Quine\n(1951) pointed out, was that Carnap’s system, which was supposed\nto single out analytic truths as true in every possible world,\n“Bachelors are unmarried”—intuitively, a\nparadigmatic analytic truth—turned out to be synthetic rather\nthan analytic. \nTo remedy what he agreed was an unsatisfactory feature of his system,\nCarnap (1952) introduced meaning postulates, i.e.,\nstipulations on the relations among the extensions of lexical items.\nFor example, the meaning postulate \nstipulates that any individual that is in the extension of\n‘bachelor’ is not in the extension of\n‘married’. Meaning postulates can be seen either as\nrestrictions on possible worlds or as relativizing analyticity to\npossible worlds. On the former option we shall say that “If Paul\nis a bachelor then Paul is unmarried” holds in every\nadmissible possible world, while on the latter we shall say\nthat it holds in every possible world in which (MP) holds.\nCarnap regarded the two options as equivalent; nowadays, the former is\nusually preferred. Carnap (1952) also thought that meaning postulates\nexpressed the semanticist’s “intentions” with\nrespect to the meanings of the descriptive constants, which may or may\nnot reflect linguistic usage; again, today postulates are usually\nunderstood as expressing semantic relations (synonymy, analytic\nentailment, etc.) among lexical items as currently used by competent\nspeakers. \nIn the late 1960s and early 1970s, Montague (1974) and other\nphilosophers and linguists (Kaplan, Kamp, Partee, and D. Lewis among\nothers) set out to apply to the analysis of natural language the\nnotions and techniques that had been introduced by Tarski and Carnap\nand further developed in Kripke’s possible worlds semantics (see\nthe entry on\n Montague semantics).\n Montague semantics can be represented as aiming to capture the\ninferential structure of a natural language: every inference that a\ncompetent speaker would regard as valid should be derivable in the\ntheory. Some such inferences depend for their validity on syntactic\nstructure and on the logical properties of logical words, like the\ninference from “Every man is mortal and Socrates is a man”\nto “Socrates is mortal”. Other inferences depend on\nproperties of non-logical words that are usually regarded as semantic,\nlike the inference from “Kim is pregnant” to “Kim is\nnot a man”. In Montague semantics, such inferences are taken\ncare of by supplementing the theory with suitable Carnapian meaning\npostulates. Yet, some followers of Montague regarded such additions as\nspurious: the aims of semantics, they said, should be distinguished\nfrom those of lexicography. The description of the meaning of\nnon-logical words requires considerable world knowledge: for example,\nthe inference from “Kim is pregnant” to “Kim is not\na man” is based on a “biological” rather than on a\n“logical” generalization. Hence, we should not expect a\nsemantic theory to furnish an account of how any two expressions\nbelonging to the same syntactic category differ in meaning (Thomason\n1974). From such a viewpoint, Montague semantics would not differ\nsignificantly from Tarskian semantics in its account of lexical\nmeaning. But not all later work within Montague’s program shared\nsuch a skepticism about representing aspects of lexical meaning within\na semantic theory, using either componential analysis (Dowty 1979) or\nmeaning postulates (Chierchia & McConnell-Ginet 2000). \nFor those who believe that meaning postulates can exhaust lexical\nmeaning, the issue arises of how to choose them, i.e., of\nhow—and whether—to delimit the set of meaning-relevant\ntruths with respect to the set of all true statements in which a given\nword occurs. As we just saw, Carnap himself thought that the choice\ncould only be the expression of the semanticist’s intentions.\nHowever, we seem to share intuitions of analyticity, i.e., we\nseem to regard some, but not all sentences of a natural language as\ntrue by virtue of the meaning of the occurring words. Such intuitions\nare taken to reflect objective semantic properties of the language,\nthat the semanticist should describe rather than impose at will. Quine\n(1951) did not challenge the existence of such intuitions, but he\nargued that they could not be cashed out in the form of a\nscientifically respectable criterion separating analytic truths\n(“Bachelors are unmarried”) from synthetic truths\n(“Aldo’s uncle is a bachelor”), whose truth does not\ndepend on meaning alone. Though Quine’s arguments were often\ncriticized (for recent criticisms, see Williamson 2007), and in spite\nof Chomsky’s constant endorsement of analyticity (see e.g. 2000:\n47, 61–2), within philosophy the analytic/synthetic distinction\nwas never fully vindicated (for an exception, see Russell 2008).\nHence, it was widely believed that lexical meaning could not be\nadequately described by meaning postulates. Fodor and Lepore (1992)\nargued that this left semantics with two options: lexical meanings\nwere either atomic (i.e., they could not be specified by\ndescriptions involving other meanings) or they were holistic,\ni.e., only the set of all true sentences of the language could count\nas fixing them. \nNeither alternative looked promising. Holism incurred in objections\nconnected with the acquisition and the understanding of language: how\ncould individual words be acquired by children, if grasping their\nmeaning involved, somehow, semantic competence on the whole language?\nAnd how could individual sentences be understood if the information\nrequired to understand them exceeded the capacity of human working\nmemory? (For an influential criticism of several varieties of holism,\nsee Dummett 1991; for a review, Pagin 2006). Atomism, in turn, ran\nagainst strong intuitions of (at least some) relations among words\nbeing part of a language’s semantics: it is because of what\n‘bachelor’ means that it doesn’t make sense to\nsuppose we could discover that some bachelors are married. Fodor\n(1998) countered this objection by reinterpreting allegedly semantic\nrelations as metaphysically necessary connections among extensions of\nwords. However, sentences that are usually regarded as analytic, such\nas “Bachelors are unmarried”, are not easily seen as just\nmetaphysically necessary truths like “Water is\nH2O”. If water is H2O, then its\nmetaphysical essence consists in being H2O (whether we know\nit or not); but there is no such thing as a metaphysical essence that\nall bachelors share—an essence that could be hidden to us, even\nthough we use the word ‘bachelor’ competently. On the\ncontrary, on acquiring the word ‘bachelor’ we acquire the\nbelief that bachelors are unmarried (Quine 1986); by contrast, many\nspeakers that have ‘water’ in their lexical repertoire do\nnot know that water is H2O. The difficulties of atomism and\nholism opened the way to vindications of molecularism (e.g., Perry\n1994; Marconi 1997), the view on which only some relations among words\nmatter for acquisition and understanding (see the entry on\n meaning holism). \nWhile mainstream formal semantics went with Carnap and Montague,\nsupplementing the Tarskian apparatus with the possible worlds\nmachinery and defining meanings as intensions, Davidson (1967, 1984)\nput forth an alternative suggestion. Tarski had shown how to provide a\ndefinition of the truth predicate for a (formal) language L:\nsuch a definition is materially adequate (i.e., it is a definition of\ntruth, rather than of some other property of sentences of\nL) if and only if it entails every biconditional of the\nform \nwhere S is a sentence of L and p is its\ntranslation into the metalanguage of L in which the definition\nis formulated. Thus, Tarski’s account of truth presupposes that\nthe semantics of both L and its metalanguage is fixed\n(otherwise it would be undetermined whether S translates into\np). On Tarski’s view, each biconditional of form (T)\ncounts as a “partial definition” of the truth predicate\nfor sentences of L (see the entry on\n Tarski’s truth definitions).\n By contrast, Davidson suggested that if one took the notion of truth\nfor granted, then T-biconditionals could be read as collectively\nconstituting a theory of meaning for L, i.e., as stating truth\nconditions for the sentences of L. For example, \nstates the truth conditions of the English sentence “If the\nweather is bad then Sharon is sad”. Of course, (W) is\nintelligible only if one understands the language in which it is\nphrased, including the predicate ‘true in English’.\nDavidson thought that the recursive machinery of Tarski’s\ndefinition of truth could be transferred to the suggested semantic\nreading, with extensions to take care of the forms of natural language\ncomposition that Tarski had neglected because they had no analogue in\nthe formal languages he was dealing with. Unfortunately, few of such\nextensions were ever spelled out by Davidson or his followers.\nMoreover, it is difficult to see how, giving up possible worlds and\nintensions in favor of a purely extensional theory, the Davidsonian\nprogram could account for the semantics of propositional attitude\nascriptions of the form “A believes (hopes, imagines, etc.) that\np”. \nConstrued as theorems of a semantic theory, T-biconditionals were\noften accused of being uninformative (Putnam 1975; Dummett 1976): to\nunderstand them, one has to already possess the information they are\nsupposed to provide. This is particularly striking in the case of\nlexical axioms such as the following: \n(To be read, respectively, as “the predicate ‘man’\napplies to x if and only if x is a man” and\n“the predicate ‘know’ applies to the pair \\(\\langle\nx, y\\rangle\\) if and only if x knows y”). Here\nit is apparent that in order to understand (V1) one must know what\n‘man’ means, which is just the information that (V1) is\nsupposed to convey (as the theory, being purely extensional,\nidentifies meaning with reference). Some Davidsonians, though\nadmitting that statements such as (V1) and (V2) are in a sense\n“uninformative”, insist that what (V1) and (V2) state is\nno less “substantive” (Larson & Segal 1995). To prove\ntheir point, they appeal to non-homophonic versions of lexical axioms,\ni.e., to the axioms of a semantic theory for a language that does not\ncoincide with the (meta)language in which the theory itself is\nphrased. Such would be, e.g., \n(V3), they argue, is clearly substantive, yet what it says is exactly\nwhat (V1) says, namely, that the word ‘man’ applies to a\ncertain category of objects. Therefore, if (V3) is substantive, so is\n(V1). But this is beside the point. The issue is not whether (V1)\nexpresses a proposition; it clearly does, and it is, in this sense,\n“substantive”. But what is relevant here is informative\npower: to one who understands the metalanguage of (V3), i.e., French,\n(V3) may communicate new information, whereas there is no circumstance\nin which (V1) would communicate new information to one who understands\nEnglish. \nIn the mid-1970s, Dummett raised the issue of the proper place of\nlexical meaning in a semantic theory. If the job of a theory of\nmeaning is to make the content of semantic competence\nexplicit—so that one could acquire semantic competence in a\nlanguage L by learning an adequate theory of meaning for\nL—then the theory ought to reflect a competent\nspeaker’s knowledge of circumstances in which she would assert a\nsentence of L, such as “The horse is in the barn”,\nas distinct from circumstances in which she would assert “The\ncat is on the mat”. This, in turn, appears to require that the\ntheory yields explicit information about the use of\n‘horse’, ‘barn’, etc., or, in other words,\nthat it includes information which goes beyond the logical type of\nlexical units. Dummett identified such information with a word’s\nFregean sense. However, he did not specify the format in which word\nsenses should be expressed in a semantic theory, except for words that\ncould be defined (e.g., ‘aunt’ = “sister of a\nparent”): in such cases, the definiens specifies what a\nspeaker must understand in order to understand the word (Dummett\n1991). But of course, not all words are of this kind. For other words,\nthe theory should specify what it is for a speaker to know them,\nthough we are not told how exactly this should be done. Similarly,\nGrandy (1974) pointed out that by identifying the meaning of a word\nsuch as ‘wise’ as a function from possible worlds to the\nsets of wise people in those worlds, Montague semantics only specifies\na formal structure and eludes the question of whether there is some\npossible description for the functions which are claimed to be the\nmeanings of words. Lacking such descriptions, possible worlds\nsemantics is not really a theory of meaning but a theory of logical\nform or logical validity. Again, aside from suggesting that “one\nwould like the functions to be given in terms of computation\nprocedures, in some sense”, Grandy had little to say about the\nform of lexical descriptions. \nIn a similar vein, Partee (1981) argued that Montague semantics, like\nevery compositional or structural semantics, does not\nuniquely fix the intensional interpretation of words. The addition of\nmeaning postulates does rule out some interpretations (e.g.,\ninterpretations on which the extension of ‘bachelor’ and\nthe extension of ‘married’ may intersect in some possible\nworld). However, it does not reduce them to the unique,\n“intended” or, in Montague’s words,\n“actual” interpretation (Montague 1974). Hence, standard\nmodel-theoretic semantics does not capture the whole content of a\nspeaker’s semantic competence, but only its structural aspects.\nFixing “the actual interpretation function” requires more\nthan language-to-language connections as encoded by, e.g., meaning\npostulates: it requires some “language-to-world\ngrounding”. Arguments to the same effect were developed\nby Bonomi (1983) and Harnad (1990). In particular, Harnad had in mind\nthe simulation of human semantic competence in artificial systems: he\nsuggested that symbol grounding could be implemented, in part, by\n“feature detectors” picking out “invariant features\nof objects and event categories from their sensory projections”\n(for recent developments see, e.g., Steels & Hild 2012). Such a\ncognitively oriented conception of grounding differs from\nPartee’s Putnam-inspired view, on which the semantic grounding\nof lexical items depends on the speakers’ objective interactions\nwith the external world in addition to their narrow psychological\nproperties. \nA resolutely cognitive approach characterizes Marconi’s (1997)\naccount of lexical semantic competence. In his view, lexical\ncompetence has two aspects: an inferential aspect, underlying\nperformances such as semantically based inference and the command of\nsynonymy, hyponymy and other semantic relations; and a\nreferential aspect, which is in charge of performances such\nas naming (e.g., calling a horse ‘horse’) and application\n(e.g., answering the question “Are there any spoons in the\ndrawer?”). Language users typically possess both aspects of\nlexical competence, though in different degrees for different words: a\nzoologist’s inferential competence on ‘manatee’ is\nusually richer than a layman’s, though a layman who spent her\nlife among manatees may be more competent, referentially, than a\n“bookish” scientist. However, the two aspects are\nindependent of each another, and neuropsychological evidence appears\nto show that they can be dissociated: there are patients whose\nreferential competence is impaired or lost while their inferential\ncompetence is intact, and vice versa (see\n Section 5.3).\n Being a theory of individual competence, Marconi’s account does\nnot deal directly with lexical meanings in a public language:\ncommunication depends both on the uniformity of cognitive interactions\nwith the external world and on communal norms concerning the use of\nlanguage, together with speakers’ deferential attitude toward\nsemantic authorities. \nSince the early 1970s, views on lexical meaning were revolutionized by\nsemantic externalism. Initially, externalism was limited to proper\nnames and natural kind words such as ‘gold’ or\n‘lemon’. In slightly different ways, both Kripke (1972)\nand Putnam (1970, 1975) argued that the reference of such words was\nnot determined by any description that a competent speaker associated\nwith the word; more generally, and contrary to what Frege may have\nthought, it was not determined by any cognitive content associated\nwith it in a speaker’s mind (for arguments to that effect, see\nthe entry on\n names).\n Instead, reference is determined, at least in part, by objective\n(“causal”) relations between a speaker and the external\nworld. For example, a speaker refers to Aristotle when she utters the\nsentence “Aristotle was a great warrior”—so that her\nassertion expresses a false proposition about Aristotle, not a true\nproposition about some great warrior she may “have in\nmind”—thanks to her connection with Aristotle himself. In\nthis case, the connection is constituted by a historical chain of\nspeakers going back to the initial users of the name\n‘Aristotle’, or its Greek equivalent, in baptism-like\ncircumstances. To belong to the chain, speakers (including present-day\nspeakers) are not required to possess any precise knowledge of\nAristotle’s life and deeds; they are, however, required to\nintend to use the name as it is used by the speakers they are picking\nup the name from, i.e., to refer to the individual those speakers\nintend to refer to. \nIn the case of most natural kind names, it may be argued, baptisms are\nhard to identify or even conjecture. In Putnam’s view, for such\nwords reference is determined by speakers’ causal interaction\nwith portions of matter or biological individuals in their\nenvironment: ‘water’, for example, refers to this\nliquid stuff, stuff that is normally found in our rivers,\nlakes, etc. The indexical component (this liquid,\nour rivers) is crucial to reference determination: it\nwouldn’t do to identify the referent of ‘water’ by\nway of some description (“liquid, transparent, quenches thirst,\nboils at 100°C, etc.”), for something might fit the\ndescription yet fail to be water, as in Putnam’s (1973, 1975) famous Twin\nEarth thought experiment (see the entry on\n reference).\n It might be remarked that, thanks to modern chemistry, we now possess\na description that is sure to apply to water and only to water:\n“being H2O” (Millikan 2005). However, even if\nour chemistry were badly mistaken (as in principle it could turn out\nto be) and water were not, in fact, H2O,\n‘water’ would still refer to whatever has the same nature\nas this liquid. Something belongs to the extension of\n‘water’ if and only if it is the same substance as this\nliquid, which we identify—correctly, as we believe—as\nbeing H2O. \nLet it be noted that in Putnam’s original proposal, reference\ndetermination is utterly independent of speakers’ cognition:\n‘water’ on Twin Earth refers to XYZ (not to\nH2O) even though the difference between the two substances\nis cognitively inert, so that before chemistry was created nobody on\neither Earth or Twin Earth could have told them apart. However, the\nlabel ‘externalism’ has been occasionally used for weaker\nviews: a semantic account may be regarded as externalist if it takes\nsemantic content to depend in one way or another on relations a\ncomputational system bears to things outside itself (Rey 2005; Borg\n2012), irrespective of whether such relations affect the\nsystem’s cognitive state. Weak externalism is hard to\ndistinguish from forms of internalism on which a word’s\nreference is determined by information stored in a speaker’s\ncognitive system—information of which the speaker may or may not\nbe aware (Evans 1982). Be that as it may, in what follows\n‘externalism’ will be used to mean strong, or Putnamian,\nexternalism. \nDoes externalism apply to other lexical categories besides proper\nnames and natural kind words? Putnam (1975) extended it to artifactual\nwords, claiming that ‘pencil’ would refer to\npencils—those objects—even if they turned out not\nto fit the description by which we normally identify them (e.g., if\nthey were discovered to be organisms, not artifacts). Schwartz (1978,\n1980) pointed out, among many objections, that even in such a case we\ncould make objects fitting the original description; we would\nthen regard the pencil-like organisms as impostors, not as\n“genuine” pencils. Others sided with Putnam and the\nexternalist account: for example, Kornblith (1980) pointed out that\nartifactual kinds from an ancient civilization could be re-baptized in\ntotal ignorance of their function. The new artifactual word would then\nrefer to the kind those objects belong to independently of\nany beliefs about them, true or false. Against such externalist\naccounts, Thomasson (2007) argued that artifactual terms cannot refer\nto artifactual kinds independently of all beliefs and concepts about\nthe nature of the kind, for the concept of the kind’s creator(s)\nis constitutive of the nature of the kind. Whether artifactual words\nare liable to an externalist account is still an open issue (for\nrecent discussions see Marconi 2013; Bahr, Carrara & Jansen 2019;\nsee also the entry on\n artifacts),\n as is, more generally, the scope of application of externalist\nsemantics. \nThere is another form of externalism that does apply to all or most\nwords of a language: social externalism (Burge 1979), the\nview on which the meaning of a word as used by an individual speaker\ndepends on the semantic standards of the linguistic community the\nspeaker belongs to. In our community the word ‘arthritis’\nrefers to arthritis—an affliction of the joints—even when\nused by a speaker who believes that it can afflict the muscles as well\nand uses the word accordingly. If the community the speaker belongs to\napplied ‘arthritis’ to rheumatoids ailments in general,\nwhether or not they afflict the joints, the same word form would not\nmean arthritis and\nwould not refer to arthritis. Hence, a speaker’s mental\ncontents, such as the meanings associated with the words she uses,\ndepend on something external to her, namely the uses and the standards\nof use of the linguistic community she belongs to. Thus, social\nexternalism eliminates the notion of idiolect: words only have the\nmeanings conferred upon them by the linguistic community\n(“public” meanings); discounting radical incompetence,\nthere is no such thing as individual semantic deviance, there are only\nfalse beliefs (for criticisms, see Bilgrami 1992, Marconi 1997; see\nalso the entry on\n idiolects). \nThough both forms of externalism focus on reference, neither is a\ncomplete reduction of lexical meaning to reference. Both Putnam and\nBurge make it a necessary condition of semantic competence on a word\nthat a speaker commands information that other semantic views would\nregard as part of the word’s sense. For example, if a speaker\nbelieves that manatees are a kind of household appliance, she would\nnot count as competent on the word ‘manatee’, nor would\nshe refer to manatees by using it (Putnam 1975; Burge 1993). Beyond\nthat, it is not easy for externalists to provide a satisfactory\naccount of lexical semantic competence, as they are committed to\nregarding speakers’ beliefs and abilities (e.g., recognitional\nabilities) as essentially irrelevant to reference determination, hence\nto meaning. Two main solutions have been proposed. Putnam (1970, 1975)\nsuggested that a speaker’s semantic competence consists in her\nknowledge of stereotypes associated with words. A stereotype\nis an oversimplified theory of a word’s extension: the\nstereotype associated with ‘tiger’ describes tigers as\ncat-like, striped, carnivorous, fierce, living in the jungle, etc.\nStereotypes are not meanings, as they do not determine reference in\nthe right way: there are albino tigers and tigers that live in zoos.\nWhat the ‘tiger’-stereotype describes is (what the\ncommunity takes to be) the typical tiger. Knowledge of\nstereotypes is necessary to be regarded as a competent speaker,\nand—one surmises—it can also be considered sufficient for\nthe purposes of ordinary communication. Thus, Putnam’s account\ndoes provide some content for semantic competence, though it\ndissociates it from knowledge of meaning. \nOn an alternative view (Devitt 1983), competence on\n‘tiger’ does not consist in entertaining propositional\nbeliefs such as “tigers are striped”, but rather in being\nappropriately linked to a network of causal chains for\n‘tiger’ involving other people’s abilities,\ngroundings, and reference borrowings. In order to understand the\nEnglish word ‘tiger’ and use it in a competent fashion, a\nsubject must be able to combine ‘tiger’ appropriately with\nother words to form sentences, to have thoughts which those sentences\nexpress, and to ground these thoughts in tigers. Devitt’s\naccount appears to make some room for a speaker’s ability to,\ne.g., recognize a tiger when she sees one; however, the respective\nweights of individual abilities (and beliefs) and objective grounding\nare not clearly specified. Suppose a speaker A belongs to a\ncommunity C that is familiar with tigers; unfortunately,\nA has no knowledge of the typical appearance of a tiger and\nis unable to tell a tiger from a leopard. Should A be\nregarded as a competent user ‘tiger’ on account of her\nbeing “part of C” and therefore linked to a\nnetwork of causal chains for ‘tiger’? \nSome philosophers (e.g., Loar 1981; McGinn 1982; Block 1986) objected\nto the reduction of lexical meaning to reference, or to\nnon-psychological factors that are alleged to determine reference. In\ntheir view, there are two aspects of meaning (more generally, of\ncontent): the narrow aspect, that captures the intuition that\n‘water’ has the same meaning in both Earthian and\nTwin-Earthian English, and the wide aspect, that captures the\nexternalist intuition that ‘water’ picks out different\nsubstances in the two worlds. The wide notion is required to account\nfor the difference in reference between English and Twin-English\n‘water’; the narrow notion is needed, first and foremost,\nto account for the relation between a subject’s beliefs and her\nbehavior. The idea is that how an object of reference is\ndescribed (not just which object one refers to) can make a difference\nin determining behavior. Oedipus married Jocasta because he thought he\nwas marrying the queen of Thebes, not his mother, though as a matter\nof fact Jocasta was his mother. This applies to words of all\ncategories: someone may believe that water quenches thirst without\nbelieving that H2O does; Lois Lane believed that Superman\nwas a superhero but she definitely did not believe the same of her\ncolleague Clark Kent, so she behaved one way to the man she identified\nas Superman and another way to the man she identified as Clark Kent\n(though they were the same man). Theorists that countenance these two\ncomponents of meaning and content usually identify the narrow aspect\nwith the inferential or conceptual role of an\nexpression e, i.e., with the aspect of e that\ncontributes to determine the inferential relations between sentences\ncontaining an occurrence of e and other sentences. Crucially,\nthe two aspects are independent: neither determines the other. The\nstress on the independence of the two factors also characterizes more\nrecent versions of so-called “dual aspect” theories, such\nas Chalmers (1996, 2002). \nWhile dual theorists agree with Putnam’s claim that some aspects\nof meaning are not “in the head”, others have opted for\nplain internalism. For example, Segal (2000) rejected the intuitions\nthat are usually associated with the Twin-Earth cases by arguing that\nmeaning (and content in general) “locally supervenes” on a\nsubject’s intrinsic physical properties. But the most\ninfluential critic of externalism has undoubtedly been Chomsky (2000).\nFirst, he argued that much of the alleged support for externalism\ncomes in fact from “intuitions” about words’\nreference in this or that circumstance. But ‘reference’\n(and the verb ‘refer’ as used by philosophers) is a\ntechnical term, not an ordinary word, hence we have no more intuitions\nabout reference than we have about tensors or c-command. Second, if we\nlook at how words such as ‘water’ are applied in ordinary\ncircumstances, we find that speakers may call ‘water’\nliquids that contain a smaller proportion of H2O than other\nliquids they do not call ‘water’ (e.g., tea): our use of\n‘water’ does not appear to be governed by hypotheses about\nmicrostructure. According to Chomsky, it may well be that progress in\nthe scientific study of the language faculty will allow us to\nunderstand in what respects one’s picture of the world is framed\nin terms of things selected and individuated by properties of the\nlexicon, or involves entities and relationships describable by the\nresources of the language faculty. Some semantic properties\ndo appear to be integrated with other aspects of language. However,\nso-called “natural kind words” (which in fact have little\nto do with kinds in nature, Chomsky claims) may do little more than\nindicating “positions in belief systems”: studying them\nmay be of some interest for “ethnoscience”, surely not for\na science of language. Along similar lines, others have maintained\nthat the genuine semantic properties of linguistic expressions should\nbe regarded as part of syntax, and that they constrain but do not\ndetermine truth conditions (e.g., Pietroski 2005, 2010). Hence, the\nconnection between meaning and truth conditions (and reference) may be\nsignificantly looser than assumed by many philosophers. \n“Ordinary language” philosophers of the 1950s and 1960s\nregarded work in formal semantics as essentially irrelevant to issues\nof meaning in natural language. Following Austin and the later\nWittgenstein, they identified meaning with use and were prone to\nconsider the different patterns of use of individual expressions as\noriginating different meanings of the word. Grice (1975) argued that\nsuch a proliferation of meanings could be avoided by distinguishing\nbetween what is asserted by a sentence (to be identified with its\ntruth conditions) and what is communicated by it in a given context\n(or in every “normal” context). For example, consider the\nfollowing exchange: \nAlthough B does not literally assert that Kim had breakfast on that\nparticular day (see, however, Partee 1973), she does communicate as\nmuch. More precisely, A could infer the communicated content by\nnoticing that the asserted sentence, taken literally (“Kim had\nbreakfast at least once in her life”), would be less informative\nthan required in the context: thus, it would violate one or more\nprinciples of conversation (“maxims”) whereas there is no\nreason to suppose that the speaker intended to opt out of\nconversational cooperation (see the entries on\n Paul Grice\n and\n pragmatics).\n If the interlocutor assumes that the speaker intended him to infer\nthe communicated content—i.e., that Kim had breakfast that\nmorning, so presumably she would not be hungry at\n11—cooperation is preserved. Such non-asserted content, called\n‘implicature’, need not be an addition to the overtly\nasserted content: e.g., in irony asserted content is negated rather\nthan expanded by the implicature (think of a speaker uttering\n“Paul is a fine friend” to implicate that Paul has\nwickedly betrayed her). \nGrice’s theory of conversation and implicatures was interpreted\nby many (including Grice himself) as a convincing way of accounting\nfor the variety of contextually specific communicative contents while\npreserving the uniqueness of a sentence’s “literal”\nmeaning, which was identified with truth conditions and regarded as\ndetermined by syntax and the conventional meanings of the occurring\nwords, as in formal semantics. The only semantic role context was\nallowed to play was in determining the content of indexical words\n(such as ‘I’, ‘now’, ‘here’, etc.)\nand the effect of context-sensitive structures (such as tense) on a\nsentence’s truth conditions. However, in about the same years\nTravis (1975) and Searle (1979, 1980) pointed out that the semantic\nrelevance of context might be much more pervasive, if not universal:\nintuitively, the same sentence type could have very different truth\nconditions in different contexts, though no indexical expression or\nstructure appeared to be involved. Take the sentence “There is\nmilk in the fridge”: in the context of morning breakfast it will\nbe considered true if there is a carton of milk in the fridge and\nfalse if there is a patch of milk on a tray in the fridge, whereas in\nthe context of cleaning up the kitchen truth conditions are reversed.\nExamples can be multiplied indefinitely, as indefinitely many factors\ncan turn out to be relevant to the truth or falsity of a sentence as\nuttered in a particular context. Such variety cannot be plausibly\nreduced to traditional polysemy such as the polysemy of\n‘property’ (meaning quality or real estate), nor can it be\ndescribed in terms of Gricean implicatures: implicatures are supposed\nnot to affect a sentence’s truth conditions, whereas here it is\nprecisely the sentence’s truth conditions that are seen as\nvarying with context. \nThe traditionalist could object by challenging the\ncontextualist’s intuitions about truth conditions. “There\nis milk in the fridge”, she could argue, is true if and only if\nthere is a certain amount (a few molecules will do) of a certain\norganic substance in the relevant fridge (for versions of this\nobjection, Cappelen & Lepore 2005). So the sentence is true both\nin the carton case and in the patch case; it would be false only if\nthe fridge did not contain any amount of any kind of milk (whether cow\nmilk or goat milk or elephant milk). The contextualist’s reply\nis that, in fact, neither the speaker nor the interpreter is aware of\nsuch alleged literal content (the point is challenged by Fodor 1983,\nCarston 2002); but “what is said” must be intuitively\naccessible to the conversational participants (Availability\nPrinciple, Recanati 1989). If truth conditions are associated\nwith what is said—as the traditionalist would agree they\nare—then in many cases a sentence’s literal content, if\nthere is such a thing, does not determine a complete, evaluable\nproposition. For a genuine proposition to arise, a sentence\ntype’s literal content (as determined by syntax and conventional\nword meaning) must be enriched or otherwise modified by primary\npragmatic processes based on the speakers’ background\nknowledge relative to each particular context of use of the sentence.\nSuch processes differ from Gricean implicature-generating processes in\nthat they come into play at the sub-propositional level; moreover,\nthey are not limited to saturation of indexicals but may\ninclude the replacement of a constituent with another. These tenets\ndefine contextualism (Recanati 1993; Bezuidenhout 2002; Carston 2002;\nrelevance theory (Sperber & Wilson 1986) is in some respects a\nprecursor of such views). Contextualists take different stands on\nnature of the semantic contribution made by words to sentences, though\nthey typically agree that it is insufficient to fix truth conditions\n(Stojanovic 2008). See Del Pinal (2018) for an argument that radical\ncontextualism (in particular, truth-conditional pragmatics) should\ninstead commit to rich lexical items which, in certain conditions, do\nsuffice to fix truth conditions. \nEven if sentence types have no definite truth conditions, it does not\nfollow that lexical types do not make definite or predictable\ncontributions to the truth conditions of sentences (think of indexical\nwords). It does follow, however, that conventional word meanings are\nnot the final constituents of complete propositions (see Allot &\nTextor 2012). Does this imply that there are no such things as lexical\nmeanings understood as features of a language? If so, how should we\naccount for word acquisition and lexical competence in general?\nRecanati (2004) does not think that contextualism as such is committed\nto meaning eliminativism, the view on which words as types have no\nmeaning; nevertheless, he regards it as defensible. Words could be\nsaid to have, rather than “meaning”, a semantic\npotential, defined as the collection of past uses of a word\nw on the basis of which similarities can be established\nbetween source situations (i.e., the circumstances in which a speaker\nhas used w) and target situations (i.e., candidate occasions\nof application of w). It is natural to object that even\nadmitting that long-term memory could encompass such an immense amount\nof information (think of the number of times ‘table’ or\n‘woman’ are used by an average speaker in the course of\nher life), surely working memory could not review such information to\nmake sense of new uses. On the other hand, if words were associated\nwith “more abstract schemata corresponding to types of\nsituations”, as Recanati suggests as a less radical alternative\nto meaning eliminativism, one wonders what the difference would be\nwith respect to traditional accounts in terms of polysemy. \nOther conceptions of “what is said” make more room for the\nsemantic contribution of conventional word meanings. Bach (1994)\nagrees with contextualists that the linguistic meaning of words (plus\nsyntax and after saturation) does not always determine complete,\ntruth-evaluable propositions; however, he maintains that they do\nprovide some minimal semantic information, a so-called\n‘propositional radical’, that allows pragmatic processes\nto issue in one or more propositions. Bach identifies “what is\nsaid” with this minimal information. However, many have objected\nthat minimal content is extremely hard to isolate (Recanati 2004;\nStanley 2007). Suppose it is identified with the content that all the\nutterances of a sentence type share; unfortunately, no such content\ncan be attributed to a sentence such as “Every bottle is in the\nfridge”, for there is no proposition that is stably asserted by\nevery utterance of it (surely not the proposition that every bottle in\nthe universe is in the fridge, which is never asserted).\nStanley’s (2007) indexicalism rejects the notion of\nminimal proposition and any distinction between semantic content and\ncommunicated content: communicated content can be entirely captured by\nmeans of consciously accessible, linguistically controlled content\n(content that results from semantic value together with the provision\nof values to free variables in syntax, or semantic value together with\nthe provision of arguments to functions from semantic types to\npropositions) together with general conversational norms. Accordingly,\nStanley generalizes contextual saturation processes that are usually\nregarded as characteristic of indexicals, tense, and a few other\nstructures; moreover, he requires that the relevant variables be\nlinguistically encoded, either syntactically or lexically. It remains\nto be seen whether such solutions apply (in a non-ad hoc way)\nto all the examples of content modulation that have been presented in\nthe literature. \nFinally, minimalism (Borg 2004, 2012; Cappelen & Lepore\n2005) is the view that appears (and intends) to be closest to the\nFrege-Montague tradition. The task of a semantic theory is said to be\nminimal in that it is supposed to account only for the literal meaning\nof sentences: context does not affect literal semantic content but\n“what the speaker says” as opposed to “what the\nsentence means” (Borg 2012). In this sense, semantics is not\nanother name for the theory of meaning, because not all\nmeaning-related properties are semantic properties (Borg 2004).\nContrary to contextualism and Bach’s theory, minimalism holds\nthat lexicon and syntax together determine complete truth-evaluable\npropositions. Indeed, this is definitional for lexical meaning: word\nmeanings are the kind of things which, if one puts enough of them\ntogether in the right sort of way, then what one gets is propositional\ncontent (Borg 2012). Borg believes that, in order to be\ntruth-evaluable, propositional contents must be “about the\nworld”, and that this entails some form of semantic externalism.\nHowever, the identification of lexical meaning with reference makes it\nhard to account for semantic relations such as synonymy, analytic\nentailment or the difference between ambiguity and polysemy, and\nsyntactically relevant properties: the difference between “John\nis easy to please” and “John is eager to please”\ncannot be explained by the fact that ‘easy’ means the\nproperty easy (see the\nentry on\n ambiguity).\n To account for semantically based syntactic properties, words may\ncome with “instructions” that are not, however,\nconstitutive of a word’s meaning like meaning postulates (which\nBorg rejects), though awareness of them is part of a speaker’s\ncompetence. Once more, lexical semantic competence is divorced from\ngrasp of word meaning. In conclusion, some information counts as\nlexical if it is either perceived as such in “firm, type-level\nlexical intuitions” or capable of affecting the word’s\nsyntactic behavior. Borg concedes that even such an extended\nconception of lexical content will not capture, e.g., analytic\nentailments such as the relation between ‘bachelor’ and\n‘unmarried’. \nThe emergence of modern linguistic theories of word meaning is usually\nplaced at the transition from historical-philological semantics\n (Section 2.2)\n to structuralist semantics, the linguistics movement started at the\nbreak of the 20th century by Ferdinand de Saussure with his\nCours de Linguistique Générale (1995\n[1916]). \nThe advances introduced by the structuralist conception of word\nmeaning are best appreciated by contrasting its basic assumptions with\nthose of historical-philological semantics. Let us recall the three\nmost important differences (Lepschy 1970; Matthews 2001). \nThe account of lexical phenomena popularized by structuralism gave\nrise to a variety of descriptive approaches to word meaning. We can\ngroup them in three categories (Lipka 1992; Murphy 2003; Geeraerts\n2006). \nThe componential current of structuralism was the first to produce an\nimportant innovation in theories of word meaning: Katzian semantics\n(Katz & Fodor 1963; Katz 1972, 1987). Katzian semantics combined\ncomponential analysis with a mentalistic conception of word meaning\nand developed a method for the description of lexical phenomena in the\ncontext of a formal grammar. The mentalistic component of Katzian\nsemantics is twofold. First, word meanings are defined as aggregates\nof simpler conceptual features inherited from our general\ncategorization abilities. Second, the proper subject matter of the\ntheory is no longer identified with the “structure of the\nlanguage” but, following Chomsky (1957, 1965), with speakers’\nability to competently interpret the words and sentences of their\nlanguage. In Katzian semantics, word meanings are structured entities\nwhose representations are called semantic markers. A semantic\nmarker is a hierarchical tree with labeled nodes whose structure\nreproduces the structure of the represented meaning, and whose labels\nidentify the word’s conceptual components. For example, the\nfigure below illustrates the sense of ‘chase’ (simplified\nfrom Katz 1987). \nKatz (1987) claimed that this approach was superior in both\ntransparency and richness to the analysis of word meaning that could\nbe provided via meaning postulates. For example, in Katzian semantics\nthe validation of conditionals such as \\(\\forall x\\forall y\n(\\textrm{chase}(x, y) \\to \\textrm{follow}(x,y))\\) could be reduced to\na matter of inspection: one had simply to check whether the semantic\nmarker of ‘follow’ was a subtree of the semantic marker of\n‘chase’. Furthermore, the method incorporated syntagmatic\nrelations in the representation of word meanings (witness the\ngrammatical tags ‘NP’, ‘VP’ and\n‘S’ attached to the conceptual components above). Katzian\nsemantics was favorably received by the Generative Semantics movement\n(Fodor 1977; Newmeyer 1980) and boosted an interest in the formal\nrepresentation of word meaning that would dominate the linguistic\nscene for decades to come (Harris 1993). Nonetheless, it was\neventually abandoned. As subsequent commentators noted, Katzian\nsemantics suffered from three important drawbacks. First, the theory\ndid not provide any clear model of how the complex conceptual\ninformation represented by semantic markers contributed to the truth\nconditions of sentences (Lewis 1972). Second, some aspects of word\nmeaning that could be easily represented with meaning postulates could\nnot be expressed through semantic markers, such as the symmetry and\nthe transitivity of predicates\n\n\n\n\n(e.g., \\(\\forall x\\forall y (\\textrm{sibling}(x, y) \\to\n\\textrm{sibling}(y, x))\\) or \\(\\forall x\\forall y\\forall z\n(\\textrm{louder}(x, y) \\mathbin{\\&} \\textrm{louder}(y, z) \\to\n\\textrm{louder}(x, z))\\); see Dowty 1979).\n\n\nThird, Katz’s arguments for the view that word meanings are\nintrinsically structured turned out to be vulnerable to objections\nfrom proponents of atomistic views of word meaning (see, most notably,\nFodor & Lepore 1992). \nAfter Katzian semantics, the landscape of linguistic theories of word\nmeaning bifurcated. On one side, we find a group of theories advancing\nthe decompositional agenda established by Katz. On the other\nside, we find a group of theories fostering the relational\napproach originated by Lexical Field Theory and relational semantics.\nFollowing Geeraerts (2010), we will briefly characterize the following\nones. \nThe basic idea of the Natural Semantic Metalanguage approach\n(henceforth, NSM; Wierzbicka 1972, 1996; Goddard & Wierzbicka\n2002) is that word meaning is best described through the combination\nof a small set of elementary conceptual particles, known as\nsemantic primes. Semantic primes are primitive (i.e., not\ndecomposable into further conceptual parts), innate (i.e., not\nlearned), and universal (i.e., explicitly lexicalized in all natural\nlanguages, whether in the form of a word, a morpheme, a phraseme, and\nso forth). According to NSM, the meaning of any word in any natural\nlanguage can be defined by appropriately combining these fundamental\nconceptual particles. Wierzbicka (1996) proposed a catalogue of about\n60 semantic primes, designed to analyze word meanings within so-called\nreductive paraphrases. For example, the reductive paraphrase for\n‘top’ is a part of\nsomething; this part is above all the other parts of this\nsomething. NSM has produced interesting applications in\ncomparative linguistics (Peeters 2006), language teaching (Goddard\n& Wierzbicka 2007), and lexical typology (Goddard 2012). However,\nthe approach has been criticized on various grounds. First, it has\nbeen argued that the method followed by NSM in the identification of\nsemantic primes is insufficiently clear (e.g., Matthewson 2003).\nSecond, some have observed that reductive paraphrases are too vague to\nbe considered adequate representations of word meanings, since they\nfail to account for fine-grained differences between semantically\nneighboring words. For example, the reductive paraphrase provided by\nWierzbicka for ‘sad’ (i.e., x feels\nsomething; sometimes a person\nthinks something like this: something bad happened; if i didn’t\nknow that it happened i would say: i don’t want it to happen; i\ndon’t say this now because i know: i can’t do anything;\nbecause of this, this person feels something bad; x\nfeels something like\nthis) seems to apply equally well to ‘unhappy’,\n‘distressed’, ‘frustrated’,\n‘upset’, and ‘annoyed’ (e.g., Aitchison 2012).\nThird, there is no consensus on what items should ultimately feature\nin the list of semantic primes available to reductive paraphrases: the\ncontent of the list is debated and varies considerably between\nversions of NSM. Fourth, some purported semantic primes appear to fail\nto comply with the universality requirement and are not explicitly\nlexicalized in all known languages (Bohnemeyer 2003; Von Fintel &\nMatthewson 2008). See Goddard (1998) for some replies and Riemer\n(2006) for further objections. \nFor NSM, word meanings can be exhaustively represented with a\nmetalanguage appealing exclusively to the combination of primitive\nlinguistic particles. Conceptual Semantics (Jackendoff 1983, 1990,\n2002) proposes a more open-ended approach. According to Conceptual\nSemantics, word meanings are essentially an interface phenomenon\nbetween a specialized body of linguistic knowledge (e.g.,\nmorphosyntactic knowledge) and core non-linguistic cognition. Word\nmeanings are thus modeled as hybrid semantic representations combining\nlinguistic features (e.g., syntactic tags) and conceptual elements\ngrounded in perceptual knowledge and motor schemas. For example, here\nis the semantic representation of ‘drink’ according to\nJackendoff. \nSyntactic tags represent the grammatical properties of the word under\nanalysis, while the items in subscript are picked from a core set of\nperceptually grounded primitives (e.g., event,\nstate, thing, path, place, property,\namount) which are assumed to be innate, cross-modal and\nuniversal categories of the human mind. The decompositional machinery\nof Conceptual Semantics has a number of attractive features. Most\nnotably, its representations take into account grammatical class and\nword-level syntax, which are plausibly an integral aspect of our\nknowledge of the meaning of words. However, some of its claims about\nthe interplay between language and conceptual structure appear more\nproblematic. To begin with, it has been observed that speakers tend to\nuse causative predicates (e.g., ‘drink’) and the\nparaphrases expressing their decompositional structure (e.g.,\n“cause a liquid to go into someone or something’s\nmouth”) in different and sometimes non-interchangeable ways\n(e.g., Wolff 2003), which raises concerns about the hypothesis that\ndecompositional analyses à la Jackendoff may be regarded as faithful\nrepresentations of word meanings. In addition, Conceptual Semantics is\nsomewhat unclear as to what exact method should be followed in the\nidentification of the motor-perceptual primitives that can feed\ndescriptions of word meanings (Pulman 2005). Finally, the restriction\nplaced by Conceptual Semantics on the type of conceptual material that\ncan inform definitions of word meaning (low-level primitives grounded\nin perceptual knowledge and motor schemas) appears to affect the\nexplanatory power of the framework. For example, how can one account\nfor the difference in meaning between ‘jog’ and\n‘run’ without ut taking into account higher-level,\narguably non-perceptual knowledge about the social characteristics of\njogging, which typically implies a certain leisure setting, the\nintention to contribute to physical wellbeing, and so on? See Taylor\n(1996), Deane (1996). \nThe neat dividing line drawn between word meanings and general world\nknowledge by Conceptual Semantics does not tell us much about the\ndynamic interaction of the two in language use. The Two-Level\nSemantics of Bierwisch (1983a,b) and Lang (Bierwisch & Lang 1989;\nLang 1993) aims to provide such a dynamic account. Two-Level Semantics\nviews word meaning as the result of the interaction between two\nsystems: semantic form (SF) and conceptual structure\n(CS). SF is a formalized representation of the basic features of a\nword. It contains grammatical information that specifies, e.g., the\nadmissible syntactic distribution of the word, plus a set of variables\nand semantic parameters whose value is determined by the interaction\nwith CS. By contrast, CS consists of language-independent systems of\nknowledge (including general world knowledge) that mediate between\nlanguage and the world (Lang & Maienborn 2011). According to\nTwo-Level Semantics, for example, polysemous words can express\nvariable meanings by virtue of having a stable underspecified SF which\ncan be flexibly manipulated by CS. By way of example, consider the\nword ‘university’, which can be read as referring either\nto an institution (as in “the university selected John’s\napplication”) or to a building (as in “the university is\nlocated on the North side of the river”). Simplifying a bit,\nTwo-Level Semantics explains the dynamics governing the selection of\nthese readings as follows. \nTwo-Level Semantics shares Jackendoff’s and Wierzbicka’s\ncommitment to a descriptive paradigm that anchors word meaning to a\nstable decompositional template, all the while avoiding the immediate\ncomplications arising from a restrictive characterization of the type\nof conceptual factors that can modulate such stable decompositional\ntemplates in contexts. But there are, once again, a few significant\nissues. A first problem is definitional accuracy: defining the SF of\n‘university’ as \\(\\lambda x [\\textrm{purpose} [x w]\n\\mathbin{\\&} \\textit{advanced study and teaching} [w]]\\) seems too\nloose to reflect the subtle differences in meaning among\n‘university’ and related terms designating institutions\nfor higher education, such as ‘college’ or\n‘academy’. Furthermore, the apparatus of Two-Level\nSemantics relies heavily on lambda expressions, which, as some\ncommentators have noted (e.g., Taylor 1994, 1995), appears ill-suited\nto represent the complex forms of world knowledge we often rely on to\nfix the meaning of highly polysemous words. See also Wunderlich (1991,\n1993). \nThe Generative Lexicon theory (GL; Pustejovsky 1995) takes a different\napproach. Instead of explaining the contextual flexibility of word\nmeaning by appealing to rich conceptual operations applied on\nsemantically thin lexical entries, this approach postulates lexical\nentries rich in conceptual information and knowledge of worldly facts.\nAccording to classical GL, the informational resources encoded in the\nlexical entry for a typical word w consist of the following\nfour levels. \nIn particular, qualia structure specifies the conceptual relations\nthat speakers associate to the real-world referents of a word and\nimpact on the way the word is used in the language (Pustejovsky 1998).\nFor example, our knowledge that bread is something that is brought\nabout through baking is considered a Quale of the word\n‘bread’, and this knowledge is responsible for our\nunderstanding that, e.g., “fresh bread” means “bread\nwhich has been baked recently”. GL distinguishes four types of\nqualia: \nTake together, these qualia form the “qualia structure” of\na word. For example, the qualia structure of the noun\n‘sandwich’ will feature information about the composition\nof sandwiches, their nature of physical artifacts, their being\nintended to be eaten, and our knowledge about the operations typically\ninvolved in the preparation of sandwiches. The notation is as\nfollows. \nsandwich(x)\nconst = {bread, …}\nform = physobj(x)\ntel = eat(P, g, x)\nagent = artifact(x) \nQualia structure is the primary explanatory device by which GL\naccounts for polysemy. The sentence “Mary finished the\nsandwich” receives the default interpretation “Mary\nfinished eating the sandwich” because the argument\nstructure of ‘finish’ requires an action as direct object,\nand the qualia structure of ‘sandwich’ allows the\ngeneration of the appropriate sense via type coercion (Pustejovsky\n2006). GL is an ongoing research program (Pustejovsky et al. 2012)\nthat has led to significant applications in computational linguistics\n(e.g., Pustejovsky & Jezek 2008; Pustejovsky & Rumshisky\n2008). But like the theories mentioned so far, it has been subject to\ncriticisms. A first general criticism is that the decompositional\nassumptions underlying GL are unwarranted and should be replaced by an\natomist view of word meaning (Fodor & Lepore 1998; see Pustejovsky\n1998 for a reply). A second criticism is that GL’s focus on\nvariations in word meaning which depend on sentential context and\nqualia structure is too narrow, since since contextual variations in\nword meaning often depend on more complex factors, such as the ability\nto keep track of coherence relations in a discourse (e.g., Asher &\nLascarides 1995; Lascarides & Copestake 1998; Kehler 2002; Asher\n2011). Finally, the empirical adequacy of the framework has been\ncalled into question. It has been argued that the formal apparatus of\nGL leads to incorrect predictions, that qualia structure sometimes\novergenerates or undergenerates interpretations, and that the rich\nlexical entries postulated by GL are psychologically implausible\n(e.g., Jayez 2001; Blutner 2002). \nTo conclude this section, we will briefly mention some contemporary\napproaches to word meaning that, in different ways, pursue the\ntheoretical agenda of the relational current of the structuralist\nparadigm. For pedagogical convenience, we can group them into two\ncategories. On the one hand, we have network approaches,\nwhich formalize knowledge of word meaning within models where the\nlexicon is seen as a structured system of entries interconnected by\nsense relations such as synonymy, antonymy, and meronymy. On the\nother, we have statistical approaches, whose primary aim is\nto investigate the patterns of co-occurrence among words in linguistic\ncorpora. \nThe main example of network approaches is perhaps Collins and\nQuillian’s (1969) hierarchical network model, in which words are\nrepresented as entries in a network of nodes, each comprising a set of\nconceptual features defining the conventional meaning of the word in\nquestion, and connected to other nodes in the network through semantic\nrelations (more in Lehman 1992). Subsequent developments of the\nhierarchical network model include the Semantic Feature Model (Smith,\nShoben & Rips 1974), the Spreading Activation Model (Collins &\nLoftus 1975; Bock & Levelt 1994), the WordNet database (Fellbaum\n1998), as well as the connectionist models of Seidenberg &\nMcClelland (1989), Hinton & Shallice (1991), and Plaut &\nShallice (1993). More on this in the entry on\n connectionism. \nFinally, statistical analysis investigates word meaning by examining\nthrough computational means the distribution of words in linguistic\ncorpora. The main idea is to use quantitative data about the frequency\nof co-occurrence of sets of lexical items to identify their semantic\nproperties and differentiate their different senses (for overviews,\nsee Atkins & Zampolli 1994; Manning & Schütze 1999;\nStubbs 2002; Sinclair 2004). Notice that while symbolic networks are\nmodels of the architecture of the lexicon that seek to be\npsychologically adequate (i.e., to reveal how knowledge of word\nmeaning is stored and organized in the mind/brain of human speakers),\nstatistical approaches to word meaning are not necessarily interested\nin psychological adequacy, and may have completely different goals,\nsuch as building a machine translation service able to mimic human\nperformance (a goal that can obviously be achieved without reproducing\nthe cognitive mechanisms underlying translation in humans). More on\nthis in the entry on\n computational linguistics. \nAs we have seen, most theories of word meaning in linguistics face, at\nsome point, the difficulties involved in drawing a plausible dividing\nline between word knowledge and world knowledge, and the various ways\nthey attempt to meet this challenge display some recurrent features.\nFor example, they assume that the lexicon, though richly interfaced\nwith world knowledge and non-linguistic cognition, remains an\nautonomous representational system encoding a specialized body of\nlinguistic knowledge. In this section, we survey a group of empirical\napproaches that adopt a different stance on word meaning. The focus is\nonce again psychological, which means that the overall goal of these\napproaches is to provide a cognitively realistic account of the\nrepresentational repertoire underlying knowledge of word meaning.\nUnlike the approaches surveyed in\n Section 4,\n however, these theories tend to encourage a view on which the\ndistinction between the semantic and pragmatic aspects of word meaning\nis highly unstable (or even impossible to draw), where lexical\nknowledge and knowledge of worldly facts are aspects of a continuum,\nand where the lexicon is permeated by our general inferential\nabilities (Evans 2010).\n Section 5.1\n will briefly illustrate the central assumptions underlying the study\nof word meaning in cognitive linguistics.\n Section 5.2\n will turn to the study of word meaning in psycholinguistics.\n Section 5.3\n will conclude with some references to neurolinguistics. \nAt the beginning of the 1970s, Eleanor Rosch put forth a new theory of\nthe mental representation of categories. Concepts such as furniture\nor bird,\nshe claimed, are not\nrepresented just as sets of criterial features with clear-cut\nboundaries, so that an item can be conceived as falling or not falling\nunder the concept based on whether or not it meets the relevant\ncriteria. Rather, items within categories can be considered more or\nless representative of the category itself (Rosch 1975; Rosch &\nMervis 1975; Mervis & Rosch 1981). Several experiments seemed to\nshow that the application of concepts is no simple yes-or-no business:\nsome items (the “good examples”) are more easily\nidentified as falling under a concept than others (the “poor\nexamples”). An automobile is perceived as a better example of\nvehicle than a rowboat,\nand much better than an elevator; a carrot is more readily identified\nas an example of the concept vegetable\nthan a pumpkin. If the concepts speakers\nassociate to category words (such as ‘vehicle’ and\n‘vegetable’) were mere bundles of criterial features,\nthese preferences would be inexplicable, since they rank items that\nmeet the criteria equally well. It is thus plausible to assume that\nthe concepts associated to category words are have a center-periphery\narchitecture centered on the most representative examples of the\ncategory: a robin is perceived as a more “birdish” bird\nthan an ostrich or, as people would say, closer to the\nprototype of a bird or to the prototypical bird (see\nthe entry on\n concepts). \nAlthough nothing in Rosch’s experiments licensed the conclusion\nthat prototypical rankings should be reified and treated as the\ncontent of concepts (what her experiments did support was merely that\na theory of the mental representation of categories should be\nconsistent with the existence of prototype effects), the\nstudy of prototypes revolutionized the existing approaches to category\nconcepts (Murphy 2002) and was a leading force behind the birth of\ncognitive linguistics. Prototypes were central to the development of\nthe Radial Network Theory of Brugman (1988 [1981]) and Lakoff (Brugman\n& Lakoff 1988), which proposed to model the sense network of words\nby introducing in the architecture of word meanings the\ncenter-periphery relation at the heart of Rosch’s seminal work.\nAccording to Brugman, word meanings can typically be modeled as radial\ncomplexes where a dominant sense is related to less typical senses by\nmeans of semantic relations such as metaphor and metonymy. For\nexample, the sense network of ‘fruit’ features product\nof plant growth at\nits center and a more abstract outcome\nat its periphery, and the two are\nconnected by a metaphorical relation). On a similar note, the\nConceptual Metaphor Theory of Lakoff & Johnson (1980; Lakoff 1987)\nand the Mental Spaces Approach of Fauconnier (1994; Fauconnier &\nTurner 1998) combined the assumption that word meanings typically have\nan internal structure arranging multiple related senses in a radial\nfashion, with the further claim that our use of words is governed by\nhard-wired mapping mechanisms that catalyze the integration of word\nmeanings across conceptual domains. For example, it is in virtue of\nthese mechanisms that the expressions “love is war”,\n“life is a journey”) are so widespread across cultures and\nsound so natural to our ears. On the proposed view, these associations\nare creative, perceptually grounded, systematic, cross-culturally\nuniform, and grounded on pre-linguistic patterns of conceptual\nactivity which correlate with core elements of human embodied\nexperience (see the entries on\n metaphor\n and\n embodied cognition).\n More in Kövecses (2002), Gibbs (2008), and Dancygier &\nSweetser (2014). \nAnother major innovation introduced by cognitive linguistics is the\ndevelopment of a resolutely “encyclopedic” approach to\nword meaning, best exemplified by Frame Semantics (Fillmore 1975,\n1982) and by the Theory of Domains (Langacker 1987). Approximating a\nbit, an approach to word meaning can be defined\n“encyclopedic” insofar as it characterizes knowledge of\nworldly facts as the primary constitutive force of word meaning. While\nthe Mental Spaces Approach and Conceptual Metaphor Theory regarded\nword meaning mainly as the product of associative patterns between\nconcepts, Fillmore and Langacker turned their attention to the\nrelation between word meaning and the body of encyclopedic knowledge\npossessed by typical speakers. Our ability to use and interpret the\nverb ‘buy’, for example, is closely intertwined with our\nbackground knowledge of the social nature of commercial transfer,\nwhich involves a seller, a buyer, goods, money, the relation between\nthe money and the goods, and so forth. However, knowledge structures\nof this kind cannot be modeled as standard concept-like\nrepresentations. Here is how Frame Semantics attempts to meet the\nchallenge. First, words are construed as pairs of phonographic forms\nwith highly schematic concepts which are internally organized as\nradial categories and function as access sites to encyclopedic\nknowledge. Second, an account of the representational organization of\nencyclopedic knowledge is provided. According to Fillmore,\nencyclopedic knowledge is represented in long-term memory in the form\nof frames, i.e., schematic conceptual scenarios that specify\nthe prototypical features and functions of a denotatum, along with its\ninteractions with the objects and the events typically associated with\nit. Frames provide thus a schematic representation of the elements and\nentities associated with a particular domain of experience and convey\nthe information required to use and interpret the words employed to\ntalk about it. For example, according to Fillmore & Atkins (1992)\nthe use of the verb ‘bet’ is governed by the risk\nframe, which is as\nfollows: \nIn the same vein as Frame Semantics (more on the parallels in Clausner\n& Croft 1999), Langacker’s Theory of Domains argues that our\nunderstanding of word meaning depends on our access to larger\nknowledge structures called domains. To illustrate the notion\nof a domain, consider the word ‘diameter’. The meaning of\nthis word cannot be grasped independently of a prior understanding of\nthe notion of a circle. According to Langacker, word meaning is\nprecisely a matter of “profile-domain” organization: the\nprofile corresponds to a substructural element designated within a\nrelevant macrostructure, whereas the domain corresponds to the\nmacrostructure providing the background information against which the\nprofile can be interpreted (Taylor 2002). In the diameter/circle\nexample, ‘diameter’ designates a profile in the circle\ndomain. Similarly,\nexpressions like ‘hot’, ‘cold’, and\n‘warm’ designate properties in the temperature\ndomain. Langacker\nargues that domains are typically structured into hierarchies that\nreflect meronymic relations and provide a basic conceptual ontology\nfor language use. For example, the meaning of ‘elbow’ is\nunderstood with respect to the arm\ndomain, while the meaning of ‘arm’\nis situated within the body\ndomain. Importantly, individual profiles\ntypically inhere to different domains, and this is one of the factors\nresponsible for the ubiquity of polysemy in natural language. For\nexample, the profile associated to the word ‘love’ inheres\nboth to the domains of embodied experience and to the abstract domains\nof social activities such as marriage ceremonies. \nDevelopments of the approach to word meaning fostered by cognitive\nlinguistics include Construction Grammar (Goldberg 1995), Embodied\nConstruction Grammar (Bergen & Chang 2005), Invited Inferencing\nTheory (Traugott & Dasher 2001), and LCCM Theory (Evans 2009). The\nnotion of a frame has become popular in cognitive psychology to model\nthe dynamics of ad hoc categorization (e.g., Barsalou 1983,\n1992, 1999; more in\n Section 5.2).\n General information about the study of word meaning in cognitive\nlinguistics can be found in Talmy (2000a,b), Croft & Cruse (2004),\nand Evans & Green (2006). \nIn psycholinguistics, the study of word meaning is understood as the\ninvestigation of the mental lexicon, the cognitive system\nthat underlies the capacity for conscious and unconscious lexical\nactivity (Jarema & Libben 2007). Simply put, the mental lexicon is\nthe long-term representational inventory storing the body of\nlinguistic knowledge speakers are required to master in order to make\ncompetent use of the lexical elements of a language; as such, it can\nbe equated with the lexical component of an individual’s\nlanguage capacity. Research on the mental lexicon is concerned with a\nvariety of problems (for surveys, see, e.g., Traxler & Gernsbacher\n2006, Spivey, McRae & Joanisse 2012, Harley 2014), that center\naround the following tasks: \nFrom a functional point of view, the mental lexicon is usually\nunderstood as a system of lexical entries, each containing\nthe information related to a word mastered by a speaker (Rapp 2001). A\nlexical entry for a word w is typically modeled as a complex\nrepresentation made up of the following components (Levelt 1989,\n2001): \nFrom this standpoint, a theory of word meaning translates into an\naccount of the information stored in the semantic form of lexical\nentries. A crucial part of the task consists in determining exactly\nwhat kind of information is stored in lexical semantic forms as\nopposed to, e.g., bits of information that fall under the scope of\nepisodic memory or general factual knowledge. Recall the example we\nmade in\n Section 3.3:\n how much of the information that a competent zoologist can associate\nto tigers is part of her knowledge of the meaning of the word\n‘tiger’? Not surprisingly, even in psycholinguistics\ntracing a neat functional separation between word processing and\ngeneral-purpose cognition has proven a problematic task. The general\nconsensus among psycholinguists seems to be that lexical\nrepresentations and conceptual representations are richly interfaced,\nthough functionally distinct (e.g., Gleitman & Papafragou 2013).\nFor example, in clinical research it is standard practice to\ndistinguish between amodal deficits involving an inability to\nprocess information at both the conceptual and the lexical level, and\nmodal deficits specifically restricted to one of the two\nspheres (Saffran & Schwartz 1994; Rapp & Goldrick 2006;\nJefferies & Lambon Ralph 2006; more in more in\n Section 5.3).\n On the resulting view, lexical activity in humans is the output of\nthe interaction between two functionally neighboring systems, one\nbroadly in charge of the storage and processing of\nconceptual-encyclopedic knowledge, the other coinciding with the\nmental lexicon. The role of lexical entries is essentially to make\nthese two systems communicate with one another through semantic forms\n(see Denes 2009). Contrary to the folk notion of a mental lexicon\nwhere words are associated to fully specified meanings or senses which\nare simply retrieved from the lexicon for the purpose of language\nprocessing, in these models lexical semantic forms are seen as highly\nschematic representations whose primary function is to supervise the\nrecruitment of the extra-linguistic information required to interpret\nword occurrences in language use. In recent years, appeals to\n“ultra-thin” lexical entries have taken an eliminativist\nturn. It has been suggested that psycholinguistic accounts of the\nrepresentational underpinnigs of lexical competence should dispose of\nthe largely metaphorical notion of an “internal word\nstore”, and there is no such thing as a mental lexicon in the\nhuman mind (e.g., Elman 2004, 2009; Dilkina, McClelland & Plaut\n2010). \nIn addition to these approaches, in a number of prominent\npsychological accounts emerged over the last two decades, the study of\nword meaning is essentially considered a chapter of theories of the\nmental realization of concepts (see the entry on\n concepts).\n Lexical units are seen either as ingredients of conceptual networks\nor as (auditory or visual) stimuli providing access to conceptual\nnetworks. A flow of neuroscientific results has shown that\nunderstanding of (certain categories of) words correlates with neural\nactivations corresponding to the semantic content of the processed\nwords. For example, it has been shown that listening to sentences that\ndescribe actions performed with the mouth, hand, or leg activates the\nvisuomotor circuits which subserve execution and observation of such\nactions (Tettamanti et al. 2005); that reading words denoting specific\nactions of the tongue (‘lick’), fingers\n(‘pick’), and leg (‘kick’) differentially\nactivate areas of the premotor cortex that are active when the\ncorresponding movements are actually performed (Hauk et al. 2004);\nthat reading odor-related words (‘jasmine’,\n‘garlic’, ‘cinnamon’) differentially activates\nthe primary olfactory cortex (Gonzales et al. 2006); and that color\nwords (such as ‘red’) activate areas in the fusiform gyrus\nthat have been associated with color perception (Chao et al. 1999,\nSimmons et al. 2007; for a survey of results on visual activations in\nlanguage processing, see Martin 2007). \nThis body of research originated so-called simulationist (or\nenactivist) accounts of conceptual competence, on which\n“understanding is imagination” and “imagining is a\nform of simulation” (Gallese & Lakoff 2005). In these\naccounts, conceptual (often called “semantic”) competence\nis seen as the ability to simulate or re-enact perceptual (including\nproprioceptive and introspective) experiences of the states of affairs\nthat language describes, by manipulating memory traces of such\nexperiences or fragments of them. In Barsalou’s theory of\nperceptual symbol systems (1999), language understanding (and\ncognition in general) is based on perceptual experience and memory of\nit. The central claim is that “sensory-motor systems represent\nnot only perceived entities but also conceptualizations of them in\ntheir absence”. Perception generates mostly unconscious\n“neural representations in sensory-motor areas of the\nbrain”, which represent schematic components of perceptual\nexperience. Such perceptual symbols are not holistic copies of\nexperiences but selections of information isolated by attention.\nRelated perceptual symbols are integrated into a simulator\nthat produces limitless simulations of a perceptual component, such as\nred or lift. Simulators are located in long-term\nmemory and play the roles traditionally attributed to concepts: they\ngenerate inferences and can be combined recursively to implement\nproductivity. A concept is not “a static amodal structure”\nas in traditional, computationally-oriented cognitive science, but\n“the ability to simulate a kind of thing perceptually”.\nLinguistic symbols (i.e., auditory or visual memories of words) get to\nbe associated with simulators; perceptual recognition of a word\nactivates the relevant simulator, which simulates a referent for the\nword; syntax provides instructions for building integrated perceptual\nsimulations, which “constitute semantic\ninterpretations”. \nThough popular among researchers interested in the conceptual\nunderpinnings of semantic competence, the simulationist paradigm faces\nimportant challenges. Three are worth mentioning. First, it appears\nthat imulations do not always capture the intuitive truth conditions\nof sentences: listeners may enact the same simulation upon exposure to\nsentences that have different truth conditions (e.g., “The man\nstood on the corner” vs. “The man waited on the\ncorner”; see Weiskopf 2010). Moreover, simulations may\noverconstrain truth conditions. For example, even though in the\nsimulations listeners typically associate to the sentence “There\nare three pencils and four pens in Anna’s mug”, the pens\nand the pencils are in vertical position, the sentence would be true\neven if they were lying horizontally in the mug. Second, the framework\ndoes not sit well with pathological data. For example, no general\nimpairment with auditory-related words is reported in patients with\nlesions in the auditory association cortex (e.g., auditory agnosia\npatients); analogously, patients with damage to the motor cortex seem\nto have no difficulties in linguistic performance, and specifically in\ninferential processing with motor-related words (for a survey of these\nresults, see Calzavarini, to appear; for a defense of the embodied\nparadigm, Pulvermüller 2013). Finally, the theory has\ndifficulties accounting for the meaning of abstract words (e.g.,\n‘beauty’, ‘pride’, ‘kindness’),\nwhich does not appear to hinge on sensory-motor simulation (see Dove\n2016 for a discussion). \nBeginning in the mid-1970s, neuropsychological research on cognitive\ndeficits related to brain lesions has produced a considerable amount\nof findings related to the neural correlates of lexical semantic\ninformation and processing. More recently, the development of\nneuroimaging techniques such as PET, fMRI and ERP has provided further\nmeans to adjudicate hypotheses about lexical semantic processes in the\nbrain (Vigneau et al. 2006). Here we do not intend to provide a\ncomplete overview of such results (for a survey, see Faust 2012). We\nshall just mention three topics of neurolinguistic research that\nappear to bear on issues in the study of word meaning: the partition\nof the lexicon into categories, the representation of common nouns vs.\nproper names, and the distinction between the inferential and the\nreferential aspects of lexical competence. \nTwo preliminary considerations should be kept in mind. First, a\ndistinction must be drawn between the neural realization of word\nforms, i.e., traces of acoustic, articulatory, graphic, and motor\nconfigurations (‘peripheral lexicons’), and the neural\ncorrelates of lexical meanings (‘concepts’). A patient can\nunderstand what is the object represented by a picture shown to her\n(and give evidence of her understanding, e.g., by miming the\nobject’s function) while being unable to retrieve the relevant\nphonological form from her output lexicon (Warrington 1985; Shallice\n1988). Second, there appears to be wide consensus about the\nirrelevance to brain processing of any distinction between strictly\nsemantic and factual or encyclopedic information (e.g., Tulving 1972;\nSartori et al. 1994). Whatever information is relevant to such\nprocesses as object recognition or confrontation naming is standardly\ncharacterized as ‘semantic’. This may be taken as a\nstipulation—it is just how neuroscientists use the word\n‘semantic’—or as deriving from lack of evidence for\nany segregation between the domains of semantic and encyclopedic\ninformation (see Binder et al. 2009). Be that as it may, in\npresent-day neuroscience there seems to be no room for a correlate of\nthe analytic/synthetic distinction. Moreover, in the literature\n‘semantic’ and ‘conceptual’ are often used\nsynonymously; hence, no distinction is drawn between lexical semantic\nand conceptual knowledge. Finally, the focus of neuroscientific\nresearch on “semantics” is on information structures\nroughly corresponding to word-level meanings, not to sentence-level\nmeanings: hence, so far neuroscientific research has had little to say\nabout the compositional mechanisms that have been the focus (and,\noften, the entire content) of theories of meaning as pursued within\nformal semantics and philosophy of language. \nLet us start with the partition of the semantic lexicon into\ncategories. Neuropsychological research indicates that the ability to\nname objects or to answer simple questions involving such nouns can be\nselectively lost or preserved: subjects can perform much better in\nnaming living entities than in naming artifacts, or in naming animate\nliving entities than in naming fruits and vegetables (Shallice 1988).\nDifferent patterns of brain activation may correspond to such\ndissociations between performances: e.g., Damasio et al. (1996) found\nthat retrieval of names of animals and of tools activate different\nregions in the left temporal lobe. However, the details of this\npartition have been interpreted in different ways. Warrington &\nMcCarthy (1983) and Warrington & Shallice (1984) explained the\nliving vs. artifactual dissociation by taking the category distinction\nto be an effect of the difference among features that are crucial in\nthe identification of living entities and artifacts: while living\nentities are identified mainly on the basis of perceptual features,\nartifacts are identified by their function. A later theory (Caramazza\n& Shelton 1998) claimed that animate and inanimate objects are\ntreated by different knowledge systems separated by evolutionary\npressure: domains of features pertaining to the recognition of living\nthings, human faces, and perhaps tools may have been singled out as\nrecognition of such entities had survival value for humans. Finally,\nDevlin et al. (1998) proposed to view the partition as the consequence\nof a difference in how recognition-relevant features are connected\nwith one another: in the case of artifactual kinds, an object is\nrecognized thanks to a characteristic coupling of form and function,\nwhereas no such coupling individuates kinds of living things (e.g.,\neyes go with seeing in many animal species). For non-neutral surveys,\nsee Caramazza & Mahon (2006) and Shallice & Cooper (2011). \nOn the other hand, it is also known that “semantic” (i.e.,\nconceptual) competence may be lost in its entirety (though often\ngradually). This is what typically happens in semantic dementia.\nEmpirical evidence has motivated theories of the neural realization of\nconceptual competence that are meant to account for both\nmodality-specific deficits and pathologies that involve impairment\nacross all modalities. The former may involve a difficulty or\nimpossibility to categorize a visually exhibited object which,\nhowever, can be correctly categorized in other modalities (e.g., if\nthe object is touched) or verbally described on the basis of the\nobject’s name (i.e., on the basis of the lexical item supposedly\nassociated with the category). The original “hub and\nspokes” model of the brain representation of concepts (Rogers et\nal. 2004, Patterson et al. 2007) accounted for both sets of findings\nby postulating that the semantic network is composed of a series of\n“spokes”, i.e., cortical areas distributed across the\nbrain processing modality-specific (visual, auditory, motor, as well\nas verbal) sources of information, and that the spokes are two-ways\nconnected to a transmodal “hub”. While damage to the\nspokes accounts for modality-specific deficits, damage to the hub and\nits connections explains the overall impairment of semantic\ncompetence. On this model, the hub is supposed to be located in the\nanterior temporal lobe (ATL), since semantic dementia had been found\nto be associated with degeneration of the anterior ventral and polar\nregions of both temporal poles (Guo et al. 2013). According to more\nrecent, “graded” versions of the model (Lambon Ralph et\nal. 2017), the contribution of the hub units may vary depending on\ndifferent patterns of connectivity to the spokes, to account for\nevidence of graded variation of function across subregions of ATL. It\nshould be noted that while many researchers converge on a distributed\nview of semantic representation and on the role of domain-specific\nparts of the neural network (depending on differential patterns of\nfunctional connectivity), not everybody agrees on the need to\npostulate a transmodal hub (see, e.g., Mahon & Caramazza\n2011). \nLet us now turn to common nouns and proper names. As we have seen, in\nthe philosophy of language of the last decades, proper names (of\npeople, landmarks, countries, etc.) have being regarded as\nsemantically different from common nouns. Neuroscientific research on\nthe processing of proper names and common nouns concurs, to some\nextent. To begin with, the retrieval of proper names is doubly\ndissociated from the retrieval of common nouns. Some patients proved\ncompetent with common nouns but unable to associate names to pictures\nof famous people, or buildings, or brands (Ellis, Young &\nCritchley 1989); in other cases, people’s names were\nspecifically affected (McKenna & Warrington 1980). Other patients\nhad the complementary deficit. The patient described in Semenza &\nSgaramella (1993) could name no objects at all (with or without\nphonemic cues) but he was able to name 10 out of 10 familiar people,\nand 18 out of 22 famous people with a phonemic cue. Martins &\nFarrayota‘s (2007) patient ACB also presented impaired object\nnaming but spared retrieval of proper names. Such findings suggest\ndistinct neural pathways for the retrieval of proper names and common\nnouns (Semenza 2006). The study of lesions and neuroimaging research\nboth initially converged in identifying the left temporal pole as\nplaying a crucial role in the retrieval of proper names, from both\nvisual stimuli (Damasio et al. 1996) and the presentation of speaker\nvoices (Waldron et al. 2014) (though in at least one case damage to\nthe left temporal pole was associated with selective sparing of proper\nnames; see Martins & Farrajota 2007). In addition, recent research\nhas found a role for the uncinate fasciculus (UF). In patients\nundergoing surgical removal of UF, retrieval of common nouns was\nrecovered while retrieval of proper names remained impaired (Papagno\net al. 2016). The present consensus appears to be that “the\nproduction of proper names recruits a network that involves at least\nthe left anterior temporal lobe and the left orbitofrontal cortex\nconnected together by the UF” (Brédart 2017). \nFurthermore, a few neuropsychological studies have described patients\nwhose competence on geographical names was preserved while names of\npeople were lost: one patient had preserved country names, though he\nhad lost virtually every other linguistic ability (McKenna &\nWarrington 1978; see Semenza 2006 for other cases of selective\npreservation of geographical names). Other behavioral experiments seem\nto show that country names are closer to common nouns than to other\nproper names such as people and landmark names in that the\nconnectivity between the word and the conceptual system is likely to\nrequire diffuse multiple connections, as with common nouns (Hollis\n& Valentine 2001). If these results were confirmed, it would turn\nout that the linguistic category of proper names is not homogeneous in\nterms of neural processing. Studies have also demonstrated that the\nretrieval of proper names from memory is typically a more difficult\ncognitive task than the retrieval of common nouns. For example, it is\nharder to name faces (of famous people) than to name objects;\nmoreover, it is easier to remember a person’s occupation than\nher or his name. Interestingly, the same difference does not\nmaterialize in definition naming, i.e., in tasks where names and\ncommon nouns are to be retrieved from definitions (Hanley 2011).\nThough several hypotheses about the source of this difference have\nbeen proposed (see Brédart 2017 for a survey), no consensus has\nbeen reached on how to explain this phenomenon. \nFinally, a few words on the distinction between the inferential and\nthe referential component of lexical competence. As we have seen in\n Section 3.2,\n Marconi (1997) suggested that processing of lexical meaning might be\ndistributed between two subsystems, an inferential and a referential\none. Beginning with Warrington (1975), many patients had been\ndescribed that were more or less severely impaired in referential\ntasks such as naming from vision (and other perceptual modalities as\nwell), while their inferential competence was more or less intact. The\ncomplementary pattern (i.e., the preservation of referential abilities\nwith loss of inferential competence) is definitely less common. Still,\na number of cases have been reported, beginning with a stroke patient\nof Heilman et al. (1976), who, while unable to perform any task\nrequiring inferential processing, performed well in referential naming\ntasks with visually presented objects (he could name 23 of 25 common\nobjects). In subsequent years, further cases were described. For\nexample, in a study of 61 patients with lesions affecting linguistic\nabilities, Kemmerer et al. (2012) found 14 cases in which referential\nabilities were better preserved than inferential abilities. More\nrecently, Pandey & Heilman (2014), while describing one more case\nof preserved (referential) naming from vision with severely impaired\n(inferential) naming from definition, hypothesized that “these\ntwo naming tasks may, at least in part, be mediated by two independent\nneuronal networks”. Thus, while double dissociation between\ninferential processes and naming from vision is well attested, it is\nnot equally clear that it involves referential processes in general.\nOn the other hand, evidence from neuroimaging is, so far, limited and\noverall inconclusive. Some neuroimaging studies (e.g.,\nTomaszewski-Farias et al. 2005, Marconi et al. 2013), as well as TMS\nmapping experiments (Hamberger et al. 2001, Hamberger & Seidel\n2009) did find different patterns of activation for inferential vs.\nreferential performances. However, the results are not entirely\nconsistent and are liable to different interpretations. For example,\nthe selective activation of the anterior left temporal lobe in\ninferential performances may well reflect additional syntactic demands\ninvolved in definition naming, rather than be due to inferential\nprocessing as such (see Calzavarini 2017 for a discussion).","contact.mail":"luca.gasparri@protonmail.com","contact.domain":"protonmail.com"},{"date.published":"2015-06-02","date.changed":"2019-08-09","url":"https://plato.stanford.edu/entries/word-meaning/","author1":"Luca Gasparri","author2":"Diego Marconi","author1.info":"https://lcgasparri.github.io/","entry":"word-meaning","body.text":"\n\n\nWord meaning has played a somewhat marginal role in early contemporary\nphilosophy of language, which was primarily concerned with the\nstructural features of sentence meaning and showed less interest in\nthe nature of the word-level input to compositional processes.\nNowadays, it is well-established that the study of word meaning is\ncrucial to the inquiry into the fundamental properties of human\nlanguage. This entry provides an overview of the way issues related to\nword meaning have been explored in analytic philosophy and a summary\nof relevant research on the subject in neighboring scientific domains.\nThough the main focus will be on philosophical problems, contributions\nfrom linguistics, psychology, neuroscience and artificial intelligence\nwill also be considered, since research on word meaning is highly\ninterdisciplinary.\n\nThe notions of word and word meaning are problematic\nto pin down, and this is reflected in the difficulties one encounters\nin defining the basic terminology of lexical semantics. In part, this\ndepends on the fact that the term ‘word’ itself is highly\npolysemous (see, e.g., Matthews 1991; Booij 2007; Lieber 2010). For\nexample, in ordinary parlance ‘word’ is ambiguous between\na type-level reading (as in “Color and colour\nare spellings of the same word”), an occurrence-level reading\n(as in “there are thirteen words in the tongue-twister How\nmuch wood would a woodchuck chuck if a woodchuck could chuck\nwood?”), and a token-level reading (as in “John\nerased the last two words on the blackboard”). Before proceeding\nfurther, let us then elucidate the notion of word in more detail\n (Section 1.1),\n and lay out the key questions that will guide our discussion of word\nmeaning in the rest of the entry\n (Section 1.2). \nWe can distinguish two fundamental approaches to the notion of word.\nOn one side, we have linguistic approaches, which\ncharacterize the notion of word by reflecting on its explanatory role\nin linguistic research (for a survey on explanation in linguistics,\nsee Egré 2015). These approaches often end up splitting the\nnotion of word into a number of more fine-grained and theoretically\nmanageable notions, but still tend to regard ‘word’ as a\nterm that zeroes in on a scientifically respectable concept (e.g., Di\nSciullo & Williams 1987). For example, words are the primary locus\nof stress and tone assignment, the basic domain of morphological\nconditions on affixation, clitization, compounding, and the theme of\nphonological and morphological processes of assimilation, vowel shift,\nmetathesis, and reduplication (Bromberger 2011). \nOn the other side, we have metaphysical approaches, which\nattempt to pin down the notion of word by inquiring into the\nmetaphysical nature of words. These approaches typically deal with\nsuch questions as “what are words?”, “how should\nwords be individuated?”, and “on what conditions two\nutterances count as utterances of the same word?”. For example,\nKaplan (1990, 2011) has proposed to replace the orthodox type-token\naccount of the relation between words and word tokens with a\n“common currency” view on which words relate to their\ntokens as continuants relate to stages in four-dimensionalist\nmetaphysics (see the entries on\n types and tokens\n and\n identity over time).\n Other contributions to this debate can be found, a.o., in McCulloch\n(1991), Cappelen (1999), Alward (2005), Hawthorne & Lepore (2011),\nSainsbury & Tye (2012), Gasparri (2016), and Irmak\n(forthcoming). \nFor the purposes of this entry, we can rely on the following\nstipulation. Every natural language has a lexicon organized\ninto lexical entries, which contain information about word\ntypes or lexemes. These are the smallest linguistic\nexpressions that are conventionally associated with a\nnon-compositional meaning and can be articulated in isolation to\nconvey semantic content. Word types relate to word tokens and\noccurrences just like phonemes relate to phones in phonological\ntheory. To understand the parallelism, think of the variations in the\nplace of articulation of the phoneme /n/, which is pronounced as the\nvoiced bilabial nasal [m] in “ten bags” and as the voiced\nvelar nasal [ŋ] in “ten gates”. Just as phonemes are\nabstract representations of sets of phones (each defining one way the\nphoneme can be instantiated in speech), lexemes can be defined as\nabstract representations of sets of words (each defining one way the\nlexeme can be instantiated in sentences). Thus, ‘do’,\n‘does’, ‘done’ and ‘doing’ are\nmorphologically and graphically marked realizations of the same\nabstract word type do. To wrap everything into a single\nformula, we can say that the lexical entries listed in a\nlexicon set the parameters defining the instantiation\npotential of word types in sentences, utterances and inscriptions (cf.\nMurphy 2010). In what follows, unless otherwise indicated, our talk of\n“word meaning” should be understood as talk of “word\ntype meaning” or “lexeme meaning”, in the sense we\njust illustrated. \nAs with general theories of meaning (see the entry on\n theories of meaning),\n two kinds of theory of word meaning can be distinguished. The first\nkind, which we can label a semantic theory of word meaning,\nis a theory interested in clarifying what meaning-determining\ninformation is encoded by the words of a natural language. A framework\nestablishing that the word ‘bachelor’ encodes the lexical\nconcept adult unmarried\nmale would be an example of a semantic theory of word meaning.\nThe second kind, which we can label a foundational theory of\nword meaning, is a theory interested in elucidating the facts in\nvirtue of which words come to have the semantic properties they have\nfor their users. A framework investigating the dynamics of semantic\nchange and social coordination in virtue of which the word\n‘bachelor’ is assigned the function of expressing the\nlexical concept adult\nunmarried male would be an example of a foundational theory of\nword meaning. Likewise, it would be the job of a foundational theory\nof word meaning to determine whether words have the semantic\nproperties they have in virtue of social conventions, or whether\nsocial conventions do not provide explanatory purchase on the facts\nthat ground word meaning (see the entry on\n convention). \nObviously, the endorsement of a given semantic theory is bound to\nplace important constraints on the claims one might propose about the\nfoundational attributes of word meaning, and vice versa.\nSemantic and foundational concerns are often interdependent, and it is\ndifficult to find theories of word meaning which are either purely\nsemantic or purely foundational. According to Ludlow (2014), for\nexample, the fact that word meaning is systematically underdetermined\n(a semantic matter) can be explained in part by looking at the\nprocesses of linguistic negotiation whereby discourse partners\nconverge on the assignment of shared meanings to the words of their\nlanguage (a foundational matter). However, semantic and foundational\ntheories remain in principle different and designed to answer partly\nnon-overlapping sets of questions. \nOur focus in this entry will be on semantic theories of word\nmeaning, i.e., on theories that try to provide an answer to such\nquestions as “what is the nature of word meaning?”,\n“what do we know when we know the meaning of a word?”, and\n“what (kind of) information must a speaker associate to the\nwords of a language in order to be a competent user of its\nlexicon?”. However, we will engage in foundational\nconsiderations whenever necessary to clarify how a given framework\naddresses issues in the domain of a semantic theory of word\nmeaning. \nThe study of word meaning became a mature academic enterprise in the\n19th century, with the birth of historical-philological\nsemantics\n (Section 2.2).\n Yet, matters related to word meaning had been the subject of much\ndebate in earlier times. We can distinguish three major classical\napproaches to word meaning: speculative etymology, rhetoric, and\nclassical lexicography (Meier-Oeser 2011; Geeraerts 2013). We describe\nthem briefly in\n Section 2.1. \nThe prototypical example of speculative etymology is perhaps the\nCratylus (383a-d), where Plato presents his well-known\nnaturalist thesis about word meaning. According to Plato, natural kind\nterms express the essence of the objects they denote and words are\nappropriate to their referents insofar as they implicitly describe the\nproperties of their referents (see the entry on\n Plato’s Cratylus).\n For example, the Greek word ‘anthrôpos’\ncan be broken down into anathrôn ha opôpe, which\ntranslates as “one who reflects on what he has seen”: the\nword used to denote humans reflects their being the only animal\nspecies which possesses the combination of vision and intelligence.\nFor speculative etymology, there is a natural or non-arbitrary\nrelation between words and their meaning, and the task of the theorist\nis to make this relation explicit through an analysis of the\ndescriptive, often phonoiconic mechanisms underlying the genesis of\nwords. More on speculative etymology in Malkiel (1993), Fumaroli\n(1999), and Del Bello (2007). \nThe primary aim of the rhetorical tradition was the study of\nfigures of speech. Some of these concern sentence-level variables such\nas the linear order of the words occurring in a sentence (e.g.,\nparallelism, climax, anastrophe); others are lexical in nature and\ndepend on using words in a way not intended by their normal or literal\nmeaning (e.g., metaphor, metonymy, synecdoche). Although originated\nfor stylistic and literary purposes, the identification of regular\npatterns in the figurative use of words initiated by the rhetorical\ntradition provided a first organized framework to investigate the\nsemantic flexibility of words, and laid the groundwork for further\ninquiry into our ability to use lexical expressions beyond the\nboundaries of their literal meaning. More on the rhetorical tradition\nin Kennedy (1994), Herrick (2004), and Toye (2013). \nFinally, classical lexicography and the practice of writing\ndictionaries played an important role in systematizing the descriptive\ndata on which later inquiry would rely to illuminate the relationship\nbetween words and their meaning. Putnam’s (1970) claim that it\nwas the phenomenon of writing (and needing) dictionaries that gave\nrise to the idea of a semantic theory is probably an overstatement.\nBut the inception of lexicography certainly had an impact on the\ndevelopment of modern theories of word meaning. The practice of\nseparating dictionary entries via lemmatization and defining them\nthrough a combination of semantically simpler elements provided a\nstylistic and methodological paradigm for much subsequent research on\nlexical phenomena, such as decompositional theories of word meaning.\nMore on classical lexicography in Béjoint (2000), Jackson\n(2002), and Hanks (2013). \nHistorical-philological semantics incorporated elements from all the\nabove classical traditions and dominated the linguistic scene roughly\nfrom 1870 to 1930, with the work of scholars such as Michel\nBréal, Hermann Paul, and Arsène Darmesteter (Gordon\n1982). In particular, it absorbed from speculative etymology an\ninterest in the conceptual mechanisms underlying the formation of word\nmeaning, it acquired from rhetorical analysis a taxonomic toolkit for\nthe classification of lexical phenomena, and it assimilated from\nlexicography and textual philology the empirical basis of descriptive\ndata that subsequent theories of word meaning would have to account\nfor (Geeraerts 2013). \nOn the methodological side, the key features of the approach to word\nmeaning introduced by historical-philological semantics can be\nsummarized as follows. First, it had a diachronic and pragmatic\norientation. That is, it was primarily concerned with the historical\nevolution of word meaning rather than with word meaning statically\nunderstood, and attributed great importance to the contextual\nflexibility of word meaning. Witness Paul’s (1920 [1880])\ndistinction between usuelle Bedeutung and okkasionelle\nBedeutung, or Bréal’s (1924 [1897]) account of\npolysemy as a byproduct of semantic change. Second, it looked at word\nmeaning primarily as a psychological phenomenon. It assumed that the\nsemantic properties of words should be defined in mentalistic terms\n(i.e., words signify “concepts” or “ideas” in\na broad sense), and that the dynamics of sense modulation, extension,\nand contraction that underlie lexical change correspond to broader\npatterns of conceptual activity in the human mind. Interestingly,\nwhile the classical rhetorical tradition had conceived of tropes as\nmarginal linguistic phenomena whose investigation, albeit important,\nwas primarily motivated by stylistic concerns, for\nhistorical-philological semantics the psychological mechanisms\nunderlying the production and the comprehension of figures of speech\nwere part of the ordinary life of languages, and engines of the\nevolution of all aspects of lexical systems (Nerlich 1992). \nThe contribution made by historical-philological semantics to the\nstudy of word meaning had a long-lasting influence. First, with its\nemphasis on the principles of semantic change, historical-philological\nsemantics was the first systematic framework to focus on the dynamic\nnature of word meaning, and established contextual flexibility as the\nprimary explanandum for a theory of word meaning (Nerlich & Clarke\n1996, 2007). This feature of historical-philological semantics is a\nclear precursor of the emphasis placed on context-sensitivity by many\nsubsequent approaches to word meaning, both in philosophy (see\n Section 3)\n and in linguistics (see\n Section 4).\n Second, the psychologistic approach to word meaning fostered by\nhistorical philological-semantics added to the agenda of linguistic\nresearch the question of how word meaning relates to cognition at\nlarge. If word meaning is essentially a psychological phenomenon, what\npsychological categories should be used to characterize it? What is\nthe dividing line separating the aspects of our mental life that\nconstitute knowledge of word meaning from those that do not? As we\nshall see, this question will constitute a central concern for\ncognitive theories of word meaning (see\n Section 5). \nIn this section we shall review some semantic and metasemantic\ntheories in analytic philosophy that bear on how lexical meaning\nshould be conceived and described. We shall follow a roughly\nchronological order. Some of these theories, such as Carnap’s\ntheory of meaning postulates and Putnam’s theory of stereotypes,\nhave a strong focus on lexical meaning, whereas others, such as\nMontague semantics, regard it as a side issue. However, such negative\nviews form an equally integral part of the philosophical debate on\nword meaning. \nBy taking the connection of thoughts and truth as the basic issue of\nsemantics and regarding sentences as “the proper means of\nexpression for a thought” (Frege 1979a [1897]), Frege paved the\nway for the 20th century priority of sentential meaning\nover lexical meaning: the semantic properties of subsentential\nexpressions such as individual words were regarded as derivative, and\nidentified with their contribution to sentential meaning. Sentential\nmeaning was in turn identified with truth conditions, most explicitly\nin Wittgenstein’s Tractatus logico-philosophicus\n(1922). However, Frege never lost interest in the “building\nblocks of thoughts” (Frege 1979b [1914]), i.e., in the semantic\nproperties of subsentential expressions. Indeed, his theory of sense\nand reference for names and predicates may be counted as the inaugural\ncontribution to lexical semantics within the analytic tradition (see\nthe entry on\n Gottlob Frege).\n It should be noted that Frege did not attribute semantic properties\nto lexical units as such, but to what he regarded as a\nsentence’s logical constituents: e.g., not to the word\n‘dog’ but to the predicate ‘is a dog’. In\nlater work this distinction was obliterated and Frege’s semantic\nnotions came to be applied to lexical units. \nPossibly because of lack of clarity affecting the notion of sense, and\nsurely because of Russell’s (1905) authoritative criticism of\nFregean semantics, word meaning disappeared from the philosophical\nscene during the 1920s and 1930s. In Wittgenstein’s\nTractatus the “real” lexical units, i.e., the\nconstituents of a completely analyzed sentence, are just names, whose\nsemantic properties are exhausted by their reference. In\nTarski’s (1933) work on formal languages, which was taken as\ndefinitional of the very field of semantics for some time, lexical\nunits are semantically categorized into different classes (individual\nconstants, predicative constants, functional constants) depending on\nthe logical type of their reference, i.e., according to whether they\ndesignate individuals in a domain of interpretation, classes of\nindividuals (or of n-tuples of individuals), or functions\ndefined over the domain. However, Tarski made no attempt nor felt any\nneed to represent semantic differences among expressions belonging to\nthe same logical type (e.g., between one-place predicates such as\n‘dog’ and ‘run’, or between two-place\npredicates such as ‘love’ and ‘left of’). See\nthe entry on\n Alfred Tarski. \nQuine (1943) and Church (1951) rehabilitated Frege’s distinction\nof sense and reference. Non-designating words such as\n‘Pegasus’ cannot be meaningless: it is precisely the\nmeaning of ‘Pegasus’ that allows speakers to establish\nthat the word lacks reference. Moreover, as Frege (1892) had argued,\ntrue factual identities such as “Morning Star = Evening\nStar” do not state synonymies; if they did, any competent\nspeaker of the language would be aware of their truth. Along these\nlines, Carnap (1947) proposed a new formulation of the sense/reference\ndichotomy, which was translated into the distinction between\nintension and extension. The notion of intension was\nintended to be an explicatum of Frege’s\n“obscure” notion of sense: two expressions have the same\nintension if and only if they have the same extension in every\npossible world or, in Carnap’s terminology, in every state\ndescription (i.e., in every maximal consistent set of atomic\nsentences and negations of atomic sentences). Thus,\n‘round’ and ‘spherical’ have the same\nintension (i.e., they express the same function from possible worlds\nto extensions) because they apply to the same objects in every\npossible world. Carnap later suggested that intensions could be\nregarded as the content of lexical semantic competence: to know the\nmeaning of a word is to know its intension, “the general\nconditions which an object must fulfill in order to be denoted by\n[that] word” (Carnap 1955). However, such general conditions\nwere not spelled out by Carnap (1947). Consequently, his system did\nnot account, any more than Tarski’s, for semantic differences\nand relations among words belonging to the same semantic category:\nthere were possible worlds in which the same individual a\ncould be both a married man and a bachelor, as no constraints were\nplaced on either word’s intension. One consequence, as Quine\n(1951) pointed out, was that Carnap’s system, which was supposed\nto single out analytic truths as true in every possible world,\n“Bachelors are unmarried”—intuitively, a\nparadigmatic analytic truth—turned out to be synthetic rather\nthan analytic. \nTo remedy what he agreed was an unsatisfactory feature of his system,\nCarnap (1952) introduced meaning postulates, i.e.,\nstipulations on the relations among the extensions of lexical items.\nFor example, the meaning postulate \nstipulates that any individual that is in the extension of\n‘bachelor’ is not in the extension of\n‘married’. Meaning postulates can be seen either as\nrestrictions on possible worlds or as relativizing analyticity to\npossible worlds. On the former option we shall say that “If Paul\nis a bachelor then Paul is unmarried” holds in every\nadmissible possible world, while on the latter we shall say\nthat it holds in every possible world in which (MP) holds.\nCarnap regarded the two options as equivalent; nowadays, the former is\nusually preferred. Carnap (1952) also thought that meaning postulates\nexpressed the semanticist’s “intentions” with\nrespect to the meanings of the descriptive constants, which may or may\nnot reflect linguistic usage; again, today postulates are usually\nunderstood as expressing semantic relations (synonymy, analytic\nentailment, etc.) among lexical items as currently used by competent\nspeakers. \nIn the late 1960s and early 1970s, Montague (1974) and other\nphilosophers and linguists (Kaplan, Kamp, Partee, and D. Lewis among\nothers) set out to apply to the analysis of natural language the\nnotions and techniques that had been introduced by Tarski and Carnap\nand further developed in Kripke’s possible worlds semantics (see\nthe entry on\n Montague semantics).\n Montague semantics can be represented as aiming to capture the\ninferential structure of a natural language: every inference that a\ncompetent speaker would regard as valid should be derivable in the\ntheory. Some such inferences depend for their validity on syntactic\nstructure and on the logical properties of logical words, like the\ninference from “Every man is mortal and Socrates is a man”\nto “Socrates is mortal”. Other inferences depend on\nproperties of non-logical words that are usually regarded as semantic,\nlike the inference from “Kim is pregnant” to “Kim is\nnot a man”. In Montague semantics, such inferences are taken\ncare of by supplementing the theory with suitable Carnapian meaning\npostulates. Yet, some followers of Montague regarded such additions as\nspurious: the aims of semantics, they said, should be distinguished\nfrom those of lexicography. The description of the meaning of\nnon-logical words requires considerable world knowledge: for example,\nthe inference from “Kim is pregnant” to “Kim is not\na man” is based on a “biological” rather than on a\n“logical” generalization. Hence, we should not expect a\nsemantic theory to furnish an account of how any two expressions\nbelonging to the same syntactic category differ in meaning (Thomason\n1974). From such a viewpoint, Montague semantics would not differ\nsignificantly from Tarskian semantics in its account of lexical\nmeaning. But not all later work within Montague’s program shared\nsuch a skepticism about representing aspects of lexical meaning within\na semantic theory, using either componential analysis (Dowty 1979) or\nmeaning postulates (Chierchia & McConnell-Ginet 2000). \nFor those who believe that meaning postulates can exhaust lexical\nmeaning, the issue arises of how to choose them, i.e., of\nhow—and whether—to delimit the set of meaning-relevant\ntruths with respect to the set of all true statements in which a given\nword occurs. As we just saw, Carnap himself thought that the choice\ncould only be the expression of the semanticist’s intentions.\nHowever, we seem to share intuitions of analyticity, i.e., we\nseem to regard some, but not all sentences of a natural language as\ntrue by virtue of the meaning of the occurring words. Such intuitions\nare taken to reflect objective semantic properties of the language,\nthat the semanticist should describe rather than impose at will. Quine\n(1951) did not challenge the existence of such intuitions, but he\nargued that they could not be cashed out in the form of a\nscientifically respectable criterion separating analytic truths\n(“Bachelors are unmarried”) from synthetic truths\n(“Aldo’s uncle is a bachelor”), whose truth does not\ndepend on meaning alone. Though Quine’s arguments were often\ncriticized (for recent criticisms, see Williamson 2007), and in spite\nof Chomsky’s constant endorsement of analyticity (see e.g. 2000:\n47, 61–2), within philosophy the analytic/synthetic distinction\nwas never fully vindicated (for an exception, see Russell 2008).\nHence, it was widely believed that lexical meaning could not be\nadequately described by meaning postulates. Fodor and Lepore (1992)\nargued that this left semantics with two options: lexical meanings\nwere either atomic (i.e., they could not be specified by\ndescriptions involving other meanings) or they were holistic,\ni.e., only the set of all true sentences of the language could count\nas fixing them. \nNeither alternative looked promising. Holism incurred in objections\nconnected with the acquisition and the understanding of language: how\ncould individual words be acquired by children, if grasping their\nmeaning involved, somehow, semantic competence on the whole language?\nAnd how could individual sentences be understood if the information\nrequired to understand them exceeded the capacity of human working\nmemory? (For an influential criticism of several varieties of holism,\nsee Dummett 1991; for a review, Pagin 2006). Atomism, in turn, ran\nagainst strong intuitions of (at least some) relations among words\nbeing part of a language’s semantics: it is because of what\n‘bachelor’ means that it doesn’t make sense to\nsuppose we could discover that some bachelors are married. Fodor\n(1998) countered this objection by reinterpreting allegedly semantic\nrelations as metaphysically necessary connections among extensions of\nwords. However, sentences that are usually regarded as analytic, such\nas “Bachelors are unmarried”, are not easily seen as just\nmetaphysically necessary truths like “Water is\nH2O”. If water is H2O, then its\nmetaphysical essence consists in being H2O (whether we know\nit or not); but there is no such thing as a metaphysical essence that\nall bachelors share—an essence that could be hidden to us, even\nthough we use the word ‘bachelor’ competently. On the\ncontrary, on acquiring the word ‘bachelor’ we acquire the\nbelief that bachelors are unmarried (Quine 1986); by contrast, many\nspeakers that have ‘water’ in their lexical repertoire do\nnot know that water is H2O. The difficulties of atomism and\nholism opened the way to vindications of molecularism (e.g., Perry\n1994; Marconi 1997), the view on which only some relations among words\nmatter for acquisition and understanding (see the entry on\n meaning holism). \nWhile mainstream formal semantics went with Carnap and Montague,\nsupplementing the Tarskian apparatus with the possible worlds\nmachinery and defining meanings as intensions, Davidson (1967, 1984)\nput forth an alternative suggestion. Tarski had shown how to provide a\ndefinition of the truth predicate for a (formal) language L:\nsuch a definition is materially adequate (i.e., it is a definition of\ntruth, rather than of some other property of sentences of\nL) if and only if it entails every biconditional of the\nform \nwhere S is a sentence of L and p is its\ntranslation into the metalanguage of L in which the definition\nis formulated. Thus, Tarski’s account of truth presupposes that\nthe semantics of both L and its metalanguage is fixed\n(otherwise it would be undetermined whether S translates into\np). On Tarski’s view, each biconditional of form (T)\ncounts as a “partial definition” of the truth predicate\nfor sentences of L (see the entry on\n Tarski’s truth definitions).\n By contrast, Davidson suggested that if one took the notion of truth\nfor granted, then T-biconditionals could be read as collectively\nconstituting a theory of meaning for L, i.e., as stating truth\nconditions for the sentences of L. For example, \nstates the truth conditions of the English sentence “If the\nweather is bad then Sharon is sad”. Of course, (W) is\nintelligible only if one understands the language in which it is\nphrased, including the predicate ‘true in English’.\nDavidson thought that the recursive machinery of Tarski’s\ndefinition of truth could be transferred to the suggested semantic\nreading, with extensions to take care of the forms of natural language\ncomposition that Tarski had neglected because they had no analogue in\nthe formal languages he was dealing with. Unfortunately, few of such\nextensions were ever spelled out by Davidson or his followers.\nMoreover, it is difficult to see how, giving up possible worlds and\nintensions in favor of a purely extensional theory, the Davidsonian\nprogram could account for the semantics of propositional attitude\nascriptions of the form “A believes (hopes, imagines, etc.) that\np”. \nConstrued as theorems of a semantic theory, T-biconditionals were\noften accused of being uninformative (Putnam 1975; Dummett 1976): to\nunderstand them, one has to already possess the information they are\nsupposed to provide. This is particularly striking in the case of\nlexical axioms such as the following: \n(To be read, respectively, as “the predicate ‘man’\napplies to x if and only if x is a man” and\n“the predicate ‘know’ applies to the pair \\(\\langle\nx, y\\rangle\\) if and only if x knows y”). Here\nit is apparent that in order to understand (V1) one must know what\n‘man’ means, which is just the information that (V1) is\nsupposed to convey (as the theory, being purely extensional,\nidentifies meaning with reference). Some Davidsonians, though\nadmitting that statements such as (V1) and (V2) are in a sense\n“uninformative”, insist that what (V1) and (V2) state is\nno less “substantive” (Larson & Segal 1995). To prove\ntheir point, they appeal to non-homophonic versions of lexical axioms,\ni.e., to the axioms of a semantic theory for a language that does not\ncoincide with the (meta)language in which the theory itself is\nphrased. Such would be, e.g., \n(V3), they argue, is clearly substantive, yet what it says is exactly\nwhat (V1) says, namely, that the word ‘man’ applies to a\ncertain category of objects. Therefore, if (V3) is substantive, so is\n(V1). But this is beside the point. The issue is not whether (V1)\nexpresses a proposition; it clearly does, and it is, in this sense,\n“substantive”. But what is relevant here is informative\npower: to one who understands the metalanguage of (V3), i.e., French,\n(V3) may communicate new information, whereas there is no circumstance\nin which (V1) would communicate new information to one who understands\nEnglish. \nIn the mid-1970s, Dummett raised the issue of the proper place of\nlexical meaning in a semantic theory. If the job of a theory of\nmeaning is to make the content of semantic competence\nexplicit—so that one could acquire semantic competence in a\nlanguage L by learning an adequate theory of meaning for\nL—then the theory ought to reflect a competent\nspeaker’s knowledge of circumstances in which she would assert a\nsentence of L, such as “The horse is in the barn”,\nas distinct from circumstances in which she would assert “The\ncat is on the mat”. This, in turn, appears to require that the\ntheory yields explicit information about the use of\n‘horse’, ‘barn’, etc., or, in other words,\nthat it includes information which goes beyond the logical type of\nlexical units. Dummett identified such information with a word’s\nFregean sense. However, he did not specify the format in which word\nsenses should be expressed in a semantic theory, except for words that\ncould be defined (e.g., ‘aunt’ = “sister of a\nparent”): in such cases, the definiens specifies what a\nspeaker must understand in order to understand the word (Dummett\n1991). But of course, not all words are of this kind. For other words,\nthe theory should specify what it is for a speaker to know them,\nthough we are not told how exactly this should be done. Similarly,\nGrandy (1974) pointed out that by identifying the meaning of a word\nsuch as ‘wise’ as a function from possible worlds to the\nsets of wise people in those worlds, Montague semantics only specifies\na formal structure and eludes the question of whether there is some\npossible description for the functions which are claimed to be the\nmeanings of words. Lacking such descriptions, possible worlds\nsemantics is not really a theory of meaning but a theory of logical\nform or logical validity. Again, aside from suggesting that “one\nwould like the functions to be given in terms of computation\nprocedures, in some sense”, Grandy had little to say about the\nform of lexical descriptions. \nIn a similar vein, Partee (1981) argued that Montague semantics, like\nevery compositional or structural semantics, does not\nuniquely fix the intensional interpretation of words. The addition of\nmeaning postulates does rule out some interpretations (e.g.,\ninterpretations on which the extension of ‘bachelor’ and\nthe extension of ‘married’ may intersect in some possible\nworld). However, it does not reduce them to the unique,\n“intended” or, in Montague’s words,\n“actual” interpretation (Montague 1974). Hence, standard\nmodel-theoretic semantics does not capture the whole content of a\nspeaker’s semantic competence, but only its structural aspects.\nFixing “the actual interpretation function” requires more\nthan language-to-language connections as encoded by, e.g., meaning\npostulates: it requires some “language-to-world\ngrounding”. Arguments to the same effect were developed\nby Bonomi (1983) and Harnad (1990). In particular, Harnad had in mind\nthe simulation of human semantic competence in artificial systems: he\nsuggested that symbol grounding could be implemented, in part, by\n“feature detectors” picking out “invariant features\nof objects and event categories from their sensory projections”\n(for recent developments see, e.g., Steels & Hild 2012). Such a\ncognitively oriented conception of grounding differs from\nPartee’s Putnam-inspired view, on which the semantic grounding\nof lexical items depends on the speakers’ objective interactions\nwith the external world in addition to their narrow psychological\nproperties. \nA resolutely cognitive approach characterizes Marconi’s (1997)\naccount of lexical semantic competence. In his view, lexical\ncompetence has two aspects: an inferential aspect, underlying\nperformances such as semantically based inference and the command of\nsynonymy, hyponymy and other semantic relations; and a\nreferential aspect, which is in charge of performances such\nas naming (e.g., calling a horse ‘horse’) and application\n(e.g., answering the question “Are there any spoons in the\ndrawer?”). Language users typically possess both aspects of\nlexical competence, though in different degrees for different words: a\nzoologist’s inferential competence on ‘manatee’ is\nusually richer than a layman’s, though a layman who spent her\nlife among manatees may be more competent, referentially, than a\n“bookish” scientist. However, the two aspects are\nindependent of each another, and neuropsychological evidence appears\nto show that they can be dissociated: there are patients whose\nreferential competence is impaired or lost while their inferential\ncompetence is intact, and vice versa (see\n Section 5.3).\n Being a theory of individual competence, Marconi’s account does\nnot deal directly with lexical meanings in a public language:\ncommunication depends both on the uniformity of cognitive interactions\nwith the external world and on communal norms concerning the use of\nlanguage, together with speakers’ deferential attitude toward\nsemantic authorities. \nSince the early 1970s, views on lexical meaning were revolutionized by\nsemantic externalism. Initially, externalism was limited to proper\nnames and natural kind words such as ‘gold’ or\n‘lemon’. In slightly different ways, both Kripke (1972)\nand Putnam (1970, 1975) argued that the reference of such words was\nnot determined by any description that a competent speaker associated\nwith the word; more generally, and contrary to what Frege may have\nthought, it was not determined by any cognitive content associated\nwith it in a speaker’s mind (for arguments to that effect, see\nthe entry on\n names).\n Instead, reference is determined, at least in part, by objective\n(“causal”) relations between a speaker and the external\nworld. For example, a speaker refers to Aristotle when she utters the\nsentence “Aristotle was a great warrior”—so that her\nassertion expresses a false proposition about Aristotle, not a true\nproposition about some great warrior she may “have in\nmind”—thanks to her connection with Aristotle himself. In\nthis case, the connection is constituted by a historical chain of\nspeakers going back to the initial users of the name\n‘Aristotle’, or its Greek equivalent, in baptism-like\ncircumstances. To belong to the chain, speakers (including present-day\nspeakers) are not required to possess any precise knowledge of\nAristotle’s life and deeds; they are, however, required to\nintend to use the name as it is used by the speakers they are picking\nup the name from, i.e., to refer to the individual those speakers\nintend to refer to. \nIn the case of most natural kind names, it may be argued, baptisms are\nhard to identify or even conjecture. In Putnam’s view, for such\nwords reference is determined by speakers’ causal interaction\nwith portions of matter or biological individuals in their\nenvironment: ‘water’, for example, refers to this\nliquid stuff, stuff that is normally found in our rivers,\nlakes, etc. The indexical component (this liquid,\nour rivers) is crucial to reference determination: it\nwouldn’t do to identify the referent of ‘water’ by\nway of some description (“liquid, transparent, quenches thirst,\nboils at 100°C, etc.”), for something might fit the\ndescription yet fail to be water, as in Putnam’s (1973, 1975) famous Twin\nEarth thought experiment (see the entry on\n reference).\n It might be remarked that, thanks to modern chemistry, we now possess\na description that is sure to apply to water and only to water:\n“being H2O” (Millikan 2005). However, even if\nour chemistry were badly mistaken (as in principle it could turn out\nto be) and water were not, in fact, H2O,\n‘water’ would still refer to whatever has the same nature\nas this liquid. Something belongs to the extension of\n‘water’ if and only if it is the same substance as this\nliquid, which we identify—correctly, as we believe—as\nbeing H2O. \nLet it be noted that in Putnam’s original proposal, reference\ndetermination is utterly independent of speakers’ cognition:\n‘water’ on Twin Earth refers to XYZ (not to\nH2O) even though the difference between the two substances\nis cognitively inert, so that before chemistry was created nobody on\neither Earth or Twin Earth could have told them apart. However, the\nlabel ‘externalism’ has been occasionally used for weaker\nviews: a semantic account may be regarded as externalist if it takes\nsemantic content to depend in one way or another on relations a\ncomputational system bears to things outside itself (Rey 2005; Borg\n2012), irrespective of whether such relations affect the\nsystem’s cognitive state. Weak externalism is hard to\ndistinguish from forms of internalism on which a word’s\nreference is determined by information stored in a speaker’s\ncognitive system—information of which the speaker may or may not\nbe aware (Evans 1982). Be that as it may, in what follows\n‘externalism’ will be used to mean strong, or Putnamian,\nexternalism. \nDoes externalism apply to other lexical categories besides proper\nnames and natural kind words? Putnam (1975) extended it to artifactual\nwords, claiming that ‘pencil’ would refer to\npencils—those objects—even if they turned out not\nto fit the description by which we normally identify them (e.g., if\nthey were discovered to be organisms, not artifacts). Schwartz (1978,\n1980) pointed out, among many objections, that even in such a case we\ncould make objects fitting the original description; we would\nthen regard the pencil-like organisms as impostors, not as\n“genuine” pencils. Others sided with Putnam and the\nexternalist account: for example, Kornblith (1980) pointed out that\nartifactual kinds from an ancient civilization could be re-baptized in\ntotal ignorance of their function. The new artifactual word would then\nrefer to the kind those objects belong to independently of\nany beliefs about them, true or false. Against such externalist\naccounts, Thomasson (2007) argued that artifactual terms cannot refer\nto artifactual kinds independently of all beliefs and concepts about\nthe nature of the kind, for the concept of the kind’s creator(s)\nis constitutive of the nature of the kind. Whether artifactual words\nare liable to an externalist account is still an open issue (for\nrecent discussions see Marconi 2013; Bahr, Carrara & Jansen 2019;\nsee also the entry on\n artifacts),\n as is, more generally, the scope of application of externalist\nsemantics. \nThere is another form of externalism that does apply to all or most\nwords of a language: social externalism (Burge 1979), the\nview on which the meaning of a word as used by an individual speaker\ndepends on the semantic standards of the linguistic community the\nspeaker belongs to. In our community the word ‘arthritis’\nrefers to arthritis—an affliction of the joints—even when\nused by a speaker who believes that it can afflict the muscles as well\nand uses the word accordingly. If the community the speaker belongs to\napplied ‘arthritis’ to rheumatoids ailments in general,\nwhether or not they afflict the joints, the same word form would not\nmean arthritis and\nwould not refer to arthritis. Hence, a speaker’s mental\ncontents, such as the meanings associated with the words she uses,\ndepend on something external to her, namely the uses and the standards\nof use of the linguistic community she belongs to. Thus, social\nexternalism eliminates the notion of idiolect: words only have the\nmeanings conferred upon them by the linguistic community\n(“public” meanings); discounting radical incompetence,\nthere is no such thing as individual semantic deviance, there are only\nfalse beliefs (for criticisms, see Bilgrami 1992, Marconi 1997; see\nalso the entry on\n idiolects). \nThough both forms of externalism focus on reference, neither is a\ncomplete reduction of lexical meaning to reference. Both Putnam and\nBurge make it a necessary condition of semantic competence on a word\nthat a speaker commands information that other semantic views would\nregard as part of the word’s sense. For example, if a speaker\nbelieves that manatees are a kind of household appliance, she would\nnot count as competent on the word ‘manatee’, nor would\nshe refer to manatees by using it (Putnam 1975; Burge 1993). Beyond\nthat, it is not easy for externalists to provide a satisfactory\naccount of lexical semantic competence, as they are committed to\nregarding speakers’ beliefs and abilities (e.g., recognitional\nabilities) as essentially irrelevant to reference determination, hence\nto meaning. Two main solutions have been proposed. Putnam (1970, 1975)\nsuggested that a speaker’s semantic competence consists in her\nknowledge of stereotypes associated with words. A stereotype\nis an oversimplified theory of a word’s extension: the\nstereotype associated with ‘tiger’ describes tigers as\ncat-like, striped, carnivorous, fierce, living in the jungle, etc.\nStereotypes are not meanings, as they do not determine reference in\nthe right way: there are albino tigers and tigers that live in zoos.\nWhat the ‘tiger’-stereotype describes is (what the\ncommunity takes to be) the typical tiger. Knowledge of\nstereotypes is necessary to be regarded as a competent speaker,\nand—one surmises—it can also be considered sufficient for\nthe purposes of ordinary communication. Thus, Putnam’s account\ndoes provide some content for semantic competence, though it\ndissociates it from knowledge of meaning. \nOn an alternative view (Devitt 1983), competence on\n‘tiger’ does not consist in entertaining propositional\nbeliefs such as “tigers are striped”, but rather in being\nappropriately linked to a network of causal chains for\n‘tiger’ involving other people’s abilities,\ngroundings, and reference borrowings. In order to understand the\nEnglish word ‘tiger’ and use it in a competent fashion, a\nsubject must be able to combine ‘tiger’ appropriately with\nother words to form sentences, to have thoughts which those sentences\nexpress, and to ground these thoughts in tigers. Devitt’s\naccount appears to make some room for a speaker’s ability to,\ne.g., recognize a tiger when she sees one; however, the respective\nweights of individual abilities (and beliefs) and objective grounding\nare not clearly specified. Suppose a speaker A belongs to a\ncommunity C that is familiar with tigers; unfortunately,\nA has no knowledge of the typical appearance of a tiger and\nis unable to tell a tiger from a leopard. Should A be\nregarded as a competent user ‘tiger’ on account of her\nbeing “part of C” and therefore linked to a\nnetwork of causal chains for ‘tiger’? \nSome philosophers (e.g., Loar 1981; McGinn 1982; Block 1986) objected\nto the reduction of lexical meaning to reference, or to\nnon-psychological factors that are alleged to determine reference. In\ntheir view, there are two aspects of meaning (more generally, of\ncontent): the narrow aspect, that captures the intuition that\n‘water’ has the same meaning in both Earthian and\nTwin-Earthian English, and the wide aspect, that captures the\nexternalist intuition that ‘water’ picks out different\nsubstances in the two worlds. The wide notion is required to account\nfor the difference in reference between English and Twin-English\n‘water’; the narrow notion is needed, first and foremost,\nto account for the relation between a subject’s beliefs and her\nbehavior. The idea is that how an object of reference is\ndescribed (not just which object one refers to) can make a difference\nin determining behavior. Oedipus married Jocasta because he thought he\nwas marrying the queen of Thebes, not his mother, though as a matter\nof fact Jocasta was his mother. This applies to words of all\ncategories: someone may believe that water quenches thirst without\nbelieving that H2O does; Lois Lane believed that Superman\nwas a superhero but she definitely did not believe the same of her\ncolleague Clark Kent, so she behaved one way to the man she identified\nas Superman and another way to the man she identified as Clark Kent\n(though they were the same man). Theorists that countenance these two\ncomponents of meaning and content usually identify the narrow aspect\nwith the inferential or conceptual role of an\nexpression e, i.e., with the aspect of e that\ncontributes to determine the inferential relations between sentences\ncontaining an occurrence of e and other sentences. Crucially,\nthe two aspects are independent: neither determines the other. The\nstress on the independence of the two factors also characterizes more\nrecent versions of so-called “dual aspect” theories, such\nas Chalmers (1996, 2002). \nWhile dual theorists agree with Putnam’s claim that some aspects\nof meaning are not “in the head”, others have opted for\nplain internalism. For example, Segal (2000) rejected the intuitions\nthat are usually associated with the Twin-Earth cases by arguing that\nmeaning (and content in general) “locally supervenes” on a\nsubject’s intrinsic physical properties. But the most\ninfluential critic of externalism has undoubtedly been Chomsky (2000).\nFirst, he argued that much of the alleged support for externalism\ncomes in fact from “intuitions” about words’\nreference in this or that circumstance. But ‘reference’\n(and the verb ‘refer’ as used by philosophers) is a\ntechnical term, not an ordinary word, hence we have no more intuitions\nabout reference than we have about tensors or c-command. Second, if we\nlook at how words such as ‘water’ are applied in ordinary\ncircumstances, we find that speakers may call ‘water’\nliquids that contain a smaller proportion of H2O than other\nliquids they do not call ‘water’ (e.g., tea): our use of\n‘water’ does not appear to be governed by hypotheses about\nmicrostructure. According to Chomsky, it may well be that progress in\nthe scientific study of the language faculty will allow us to\nunderstand in what respects one’s picture of the world is framed\nin terms of things selected and individuated by properties of the\nlexicon, or involves entities and relationships describable by the\nresources of the language faculty. Some semantic properties\ndo appear to be integrated with other aspects of language. However,\nso-called “natural kind words” (which in fact have little\nto do with kinds in nature, Chomsky claims) may do little more than\nindicating “positions in belief systems”: studying them\nmay be of some interest for “ethnoscience”, surely not for\na science of language. Along similar lines, others have maintained\nthat the genuine semantic properties of linguistic expressions should\nbe regarded as part of syntax, and that they constrain but do not\ndetermine truth conditions (e.g., Pietroski 2005, 2010). Hence, the\nconnection between meaning and truth conditions (and reference) may be\nsignificantly looser than assumed by many philosophers. \n“Ordinary language” philosophers of the 1950s and 1960s\nregarded work in formal semantics as essentially irrelevant to issues\nof meaning in natural language. Following Austin and the later\nWittgenstein, they identified meaning with use and were prone to\nconsider the different patterns of use of individual expressions as\noriginating different meanings of the word. Grice (1975) argued that\nsuch a proliferation of meanings could be avoided by distinguishing\nbetween what is asserted by a sentence (to be identified with its\ntruth conditions) and what is communicated by it in a given context\n(or in every “normal” context). For example, consider the\nfollowing exchange: \nAlthough B does not literally assert that Kim had breakfast on that\nparticular day (see, however, Partee 1973), she does communicate as\nmuch. More precisely, A could infer the communicated content by\nnoticing that the asserted sentence, taken literally (“Kim had\nbreakfast at least once in her life”), would be less informative\nthan required in the context: thus, it would violate one or more\nprinciples of conversation (“maxims”) whereas there is no\nreason to suppose that the speaker intended to opt out of\nconversational cooperation (see the entries on\n Paul Grice\n and\n pragmatics).\n If the interlocutor assumes that the speaker intended him to infer\nthe communicated content—i.e., that Kim had breakfast that\nmorning, so presumably she would not be hungry at\n11—cooperation is preserved. Such non-asserted content, called\n‘implicature’, need not be an addition to the overtly\nasserted content: e.g., in irony asserted content is negated rather\nthan expanded by the implicature (think of a speaker uttering\n“Paul is a fine friend” to implicate that Paul has\nwickedly betrayed her). \nGrice’s theory of conversation and implicatures was interpreted\nby many (including Grice himself) as a convincing way of accounting\nfor the variety of contextually specific communicative contents while\npreserving the uniqueness of a sentence’s “literal”\nmeaning, which was identified with truth conditions and regarded as\ndetermined by syntax and the conventional meanings of the occurring\nwords, as in formal semantics. The only semantic role context was\nallowed to play was in determining the content of indexical words\n(such as ‘I’, ‘now’, ‘here’, etc.)\nand the effect of context-sensitive structures (such as tense) on a\nsentence’s truth conditions. However, in about the same years\nTravis (1975) and Searle (1979, 1980) pointed out that the semantic\nrelevance of context might be much more pervasive, if not universal:\nintuitively, the same sentence type could have very different truth\nconditions in different contexts, though no indexical expression or\nstructure appeared to be involved. Take the sentence “There is\nmilk in the fridge”: in the context of morning breakfast it will\nbe considered true if there is a carton of milk in the fridge and\nfalse if there is a patch of milk on a tray in the fridge, whereas in\nthe context of cleaning up the kitchen truth conditions are reversed.\nExamples can be multiplied indefinitely, as indefinitely many factors\ncan turn out to be relevant to the truth or falsity of a sentence as\nuttered in a particular context. Such variety cannot be plausibly\nreduced to traditional polysemy such as the polysemy of\n‘property’ (meaning quality or real estate), nor can it be\ndescribed in terms of Gricean implicatures: implicatures are supposed\nnot to affect a sentence’s truth conditions, whereas here it is\nprecisely the sentence’s truth conditions that are seen as\nvarying with context. \nThe traditionalist could object by challenging the\ncontextualist’s intuitions about truth conditions. “There\nis milk in the fridge”, she could argue, is true if and only if\nthere is a certain amount (a few molecules will do) of a certain\norganic substance in the relevant fridge (for versions of this\nobjection, Cappelen & Lepore 2005). So the sentence is true both\nin the carton case and in the patch case; it would be false only if\nthe fridge did not contain any amount of any kind of milk (whether cow\nmilk or goat milk or elephant milk). The contextualist’s reply\nis that, in fact, neither the speaker nor the interpreter is aware of\nsuch alleged literal content (the point is challenged by Fodor 1983,\nCarston 2002); but “what is said” must be intuitively\naccessible to the conversational participants (Availability\nPrinciple, Recanati 1989). If truth conditions are associated\nwith what is said—as the traditionalist would agree they\nare—then in many cases a sentence’s literal content, if\nthere is such a thing, does not determine a complete, evaluable\nproposition. For a genuine proposition to arise, a sentence\ntype’s literal content (as determined by syntax and conventional\nword meaning) must be enriched or otherwise modified by primary\npragmatic processes based on the speakers’ background\nknowledge relative to each particular context of use of the sentence.\nSuch processes differ from Gricean implicature-generating processes in\nthat they come into play at the sub-propositional level; moreover,\nthey are not limited to saturation of indexicals but may\ninclude the replacement of a constituent with another. These tenets\ndefine contextualism (Recanati 1993; Bezuidenhout 2002; Carston 2002;\nrelevance theory (Sperber & Wilson 1986) is in some respects a\nprecursor of such views). Contextualists take different stands on\nnature of the semantic contribution made by words to sentences, though\nthey typically agree that it is insufficient to fix truth conditions\n(Stojanovic 2008). See Del Pinal (2018) for an argument that radical\ncontextualism (in particular, truth-conditional pragmatics) should\ninstead commit to rich lexical items which, in certain conditions, do\nsuffice to fix truth conditions. \nEven if sentence types have no definite truth conditions, it does not\nfollow that lexical types do not make definite or predictable\ncontributions to the truth conditions of sentences (think of indexical\nwords). It does follow, however, that conventional word meanings are\nnot the final constituents of complete propositions (see Allot &\nTextor 2012). Does this imply that there are no such things as lexical\nmeanings understood as features of a language? If so, how should we\naccount for word acquisition and lexical competence in general?\nRecanati (2004) does not think that contextualism as such is committed\nto meaning eliminativism, the view on which words as types have no\nmeaning; nevertheless, he regards it as defensible. Words could be\nsaid to have, rather than “meaning”, a semantic\npotential, defined as the collection of past uses of a word\nw on the basis of which similarities can be established\nbetween source situations (i.e., the circumstances in which a speaker\nhas used w) and target situations (i.e., candidate occasions\nof application of w). It is natural to object that even\nadmitting that long-term memory could encompass such an immense amount\nof information (think of the number of times ‘table’ or\n‘woman’ are used by an average speaker in the course of\nher life), surely working memory could not review such information to\nmake sense of new uses. On the other hand, if words were associated\nwith “more abstract schemata corresponding to types of\nsituations”, as Recanati suggests as a less radical alternative\nto meaning eliminativism, one wonders what the difference would be\nwith respect to traditional accounts in terms of polysemy. \nOther conceptions of “what is said” make more room for the\nsemantic contribution of conventional word meanings. Bach (1994)\nagrees with contextualists that the linguistic meaning of words (plus\nsyntax and after saturation) does not always determine complete,\ntruth-evaluable propositions; however, he maintains that they do\nprovide some minimal semantic information, a so-called\n‘propositional radical’, that allows pragmatic processes\nto issue in one or more propositions. Bach identifies “what is\nsaid” with this minimal information. However, many have objected\nthat minimal content is extremely hard to isolate (Recanati 2004;\nStanley 2007). Suppose it is identified with the content that all the\nutterances of a sentence type share; unfortunately, no such content\ncan be attributed to a sentence such as “Every bottle is in the\nfridge”, for there is no proposition that is stably asserted by\nevery utterance of it (surely not the proposition that every bottle in\nthe universe is in the fridge, which is never asserted).\nStanley’s (2007) indexicalism rejects the notion of\nminimal proposition and any distinction between semantic content and\ncommunicated content: communicated content can be entirely captured by\nmeans of consciously accessible, linguistically controlled content\n(content that results from semantic value together with the provision\nof values to free variables in syntax, or semantic value together with\nthe provision of arguments to functions from semantic types to\npropositions) together with general conversational norms. Accordingly,\nStanley generalizes contextual saturation processes that are usually\nregarded as characteristic of indexicals, tense, and a few other\nstructures; moreover, he requires that the relevant variables be\nlinguistically encoded, either syntactically or lexically. It remains\nto be seen whether such solutions apply (in a non-ad hoc way)\nto all the examples of content modulation that have been presented in\nthe literature. \nFinally, minimalism (Borg 2004, 2012; Cappelen & Lepore\n2005) is the view that appears (and intends) to be closest to the\nFrege-Montague tradition. The task of a semantic theory is said to be\nminimal in that it is supposed to account only for the literal meaning\nof sentences: context does not affect literal semantic content but\n“what the speaker says” as opposed to “what the\nsentence means” (Borg 2012). In this sense, semantics is not\nanother name for the theory of meaning, because not all\nmeaning-related properties are semantic properties (Borg 2004).\nContrary to contextualism and Bach’s theory, minimalism holds\nthat lexicon and syntax together determine complete truth-evaluable\npropositions. Indeed, this is definitional for lexical meaning: word\nmeanings are the kind of things which, if one puts enough of them\ntogether in the right sort of way, then what one gets is propositional\ncontent (Borg 2012). Borg believes that, in order to be\ntruth-evaluable, propositional contents must be “about the\nworld”, and that this entails some form of semantic externalism.\nHowever, the identification of lexical meaning with reference makes it\nhard to account for semantic relations such as synonymy, analytic\nentailment or the difference between ambiguity and polysemy, and\nsyntactically relevant properties: the difference between “John\nis easy to please” and “John is eager to please”\ncannot be explained by the fact that ‘easy’ means the\nproperty easy (see the\nentry on\n ambiguity).\n To account for semantically based syntactic properties, words may\ncome with “instructions” that are not, however,\nconstitutive of a word’s meaning like meaning postulates (which\nBorg rejects), though awareness of them is part of a speaker’s\ncompetence. Once more, lexical semantic competence is divorced from\ngrasp of word meaning. In conclusion, some information counts as\nlexical if it is either perceived as such in “firm, type-level\nlexical intuitions” or capable of affecting the word’s\nsyntactic behavior. Borg concedes that even such an extended\nconception of lexical content will not capture, e.g., analytic\nentailments such as the relation between ‘bachelor’ and\n‘unmarried’. \nThe emergence of modern linguistic theories of word meaning is usually\nplaced at the transition from historical-philological semantics\n (Section 2.2)\n to structuralist semantics, the linguistics movement started at the\nbreak of the 20th century by Ferdinand de Saussure with his\nCours de Linguistique Générale (1995\n[1916]). \nThe advances introduced by the structuralist conception of word\nmeaning are best appreciated by contrasting its basic assumptions with\nthose of historical-philological semantics. Let us recall the three\nmost important differences (Lepschy 1970; Matthews 2001). \nThe account of lexical phenomena popularized by structuralism gave\nrise to a variety of descriptive approaches to word meaning. We can\ngroup them in three categories (Lipka 1992; Murphy 2003; Geeraerts\n2006). \nThe componential current of structuralism was the first to produce an\nimportant innovation in theories of word meaning: Katzian semantics\n(Katz & Fodor 1963; Katz 1972, 1987). Katzian semantics combined\ncomponential analysis with a mentalistic conception of word meaning\nand developed a method for the description of lexical phenomena in the\ncontext of a formal grammar. The mentalistic component of Katzian\nsemantics is twofold. First, word meanings are defined as aggregates\nof simpler conceptual features inherited from our general\ncategorization abilities. Second, the proper subject matter of the\ntheory is no longer identified with the “structure of the\nlanguage” but, following Chomsky (1957, 1965), with speakers’\nability to competently interpret the words and sentences of their\nlanguage. In Katzian semantics, word meanings are structured entities\nwhose representations are called semantic markers. A semantic\nmarker is a hierarchical tree with labeled nodes whose structure\nreproduces the structure of the represented meaning, and whose labels\nidentify the word’s conceptual components. For example, the\nfigure below illustrates the sense of ‘chase’ (simplified\nfrom Katz 1987). \nKatz (1987) claimed that this approach was superior in both\ntransparency and richness to the analysis of word meaning that could\nbe provided via meaning postulates. For example, in Katzian semantics\nthe validation of conditionals such as \\(\\forall x\\forall y\n(\\textrm{chase}(x, y) \\to \\textrm{follow}(x,y))\\) could be reduced to\na matter of inspection: one had simply to check whether the semantic\nmarker of ‘follow’ was a subtree of the semantic marker of\n‘chase’. Furthermore, the method incorporated syntagmatic\nrelations in the representation of word meanings (witness the\ngrammatical tags ‘NP’, ‘VP’ and\n‘S’ attached to the conceptual components above). Katzian\nsemantics was favorably received by the Generative Semantics movement\n(Fodor 1977; Newmeyer 1980) and boosted an interest in the formal\nrepresentation of word meaning that would dominate the linguistic\nscene for decades to come (Harris 1993). Nonetheless, it was\neventually abandoned. As subsequent commentators noted, Katzian\nsemantics suffered from three important drawbacks. First, the theory\ndid not provide any clear model of how the complex conceptual\ninformation represented by semantic markers contributed to the truth\nconditions of sentences (Lewis 1972). Second, some aspects of word\nmeaning that could be easily represented with meaning postulates could\nnot be expressed through semantic markers, such as the symmetry and\nthe transitivity of predicates\n\n\n\n\n(e.g., \\(\\forall x\\forall y (\\textrm{sibling}(x, y) \\to\n\\textrm{sibling}(y, x))\\) or \\(\\forall x\\forall y\\forall z\n(\\textrm{louder}(x, y) \\mathbin{\\&} \\textrm{louder}(y, z) \\to\n\\textrm{louder}(x, z))\\); see Dowty 1979).\n\n\nThird, Katz’s arguments for the view that word meanings are\nintrinsically structured turned out to be vulnerable to objections\nfrom proponents of atomistic views of word meaning (see, most notably,\nFodor & Lepore 1992). \nAfter Katzian semantics, the landscape of linguistic theories of word\nmeaning bifurcated. On one side, we find a group of theories advancing\nthe decompositional agenda established by Katz. On the other\nside, we find a group of theories fostering the relational\napproach originated by Lexical Field Theory and relational semantics.\nFollowing Geeraerts (2010), we will briefly characterize the following\nones. \nThe basic idea of the Natural Semantic Metalanguage approach\n(henceforth, NSM; Wierzbicka 1972, 1996; Goddard & Wierzbicka\n2002) is that word meaning is best described through the combination\nof a small set of elementary conceptual particles, known as\nsemantic primes. Semantic primes are primitive (i.e., not\ndecomposable into further conceptual parts), innate (i.e., not\nlearned), and universal (i.e., explicitly lexicalized in all natural\nlanguages, whether in the form of a word, a morpheme, a phraseme, and\nso forth). According to NSM, the meaning of any word in any natural\nlanguage can be defined by appropriately combining these fundamental\nconceptual particles. Wierzbicka (1996) proposed a catalogue of about\n60 semantic primes, designed to analyze word meanings within so-called\nreductive paraphrases. For example, the reductive paraphrase for\n‘top’ is a part of\nsomething; this part is above all the other parts of this\nsomething. NSM has produced interesting applications in\ncomparative linguistics (Peeters 2006), language teaching (Goddard\n& Wierzbicka 2007), and lexical typology (Goddard 2012). However,\nthe approach has been criticized on various grounds. First, it has\nbeen argued that the method followed by NSM in the identification of\nsemantic primes is insufficiently clear (e.g., Matthewson 2003).\nSecond, some have observed that reductive paraphrases are too vague to\nbe considered adequate representations of word meanings, since they\nfail to account for fine-grained differences between semantically\nneighboring words. For example, the reductive paraphrase provided by\nWierzbicka for ‘sad’ (i.e., x feels\nsomething; sometimes a person\nthinks something like this: something bad happened; if i didn’t\nknow that it happened i would say: i don’t want it to happen; i\ndon’t say this now because i know: i can’t do anything;\nbecause of this, this person feels something bad; x\nfeels something like\nthis) seems to apply equally well to ‘unhappy’,\n‘distressed’, ‘frustrated’,\n‘upset’, and ‘annoyed’ (e.g., Aitchison 2012).\nThird, there is no consensus on what items should ultimately feature\nin the list of semantic primes available to reductive paraphrases: the\ncontent of the list is debated and varies considerably between\nversions of NSM. Fourth, some purported semantic primes appear to fail\nto comply with the universality requirement and are not explicitly\nlexicalized in all known languages (Bohnemeyer 2003; Von Fintel &\nMatthewson 2008). See Goddard (1998) for some replies and Riemer\n(2006) for further objections. \nFor NSM, word meanings can be exhaustively represented with a\nmetalanguage appealing exclusively to the combination of primitive\nlinguistic particles. Conceptual Semantics (Jackendoff 1983, 1990,\n2002) proposes a more open-ended approach. According to Conceptual\nSemantics, word meanings are essentially an interface phenomenon\nbetween a specialized body of linguistic knowledge (e.g.,\nmorphosyntactic knowledge) and core non-linguistic cognition. Word\nmeanings are thus modeled as hybrid semantic representations combining\nlinguistic features (e.g., syntactic tags) and conceptual elements\ngrounded in perceptual knowledge and motor schemas. For example, here\nis the semantic representation of ‘drink’ according to\nJackendoff. \nSyntactic tags represent the grammatical properties of the word under\nanalysis, while the items in subscript are picked from a core set of\nperceptually grounded primitives (e.g., event,\nstate, thing, path, place, property,\namount) which are assumed to be innate, cross-modal and\nuniversal categories of the human mind. The decompositional machinery\nof Conceptual Semantics has a number of attractive features. Most\nnotably, its representations take into account grammatical class and\nword-level syntax, which are plausibly an integral aspect of our\nknowledge of the meaning of words. However, some of its claims about\nthe interplay between language and conceptual structure appear more\nproblematic. To begin with, it has been observed that speakers tend to\nuse causative predicates (e.g., ‘drink’) and the\nparaphrases expressing their decompositional structure (e.g.,\n“cause a liquid to go into someone or something’s\nmouth”) in different and sometimes non-interchangeable ways\n(e.g., Wolff 2003), which raises concerns about the hypothesis that\ndecompositional analyses à la Jackendoff may be regarded as faithful\nrepresentations of word meanings. In addition, Conceptual Semantics is\nsomewhat unclear as to what exact method should be followed in the\nidentification of the motor-perceptual primitives that can feed\ndescriptions of word meanings (Pulman 2005). Finally, the restriction\nplaced by Conceptual Semantics on the type of conceptual material that\ncan inform definitions of word meaning (low-level primitives grounded\nin perceptual knowledge and motor schemas) appears to affect the\nexplanatory power of the framework. For example, how can one account\nfor the difference in meaning between ‘jog’ and\n‘run’ without ut taking into account higher-level,\narguably non-perceptual knowledge about the social characteristics of\njogging, which typically implies a certain leisure setting, the\nintention to contribute to physical wellbeing, and so on? See Taylor\n(1996), Deane (1996). \nThe neat dividing line drawn between word meanings and general world\nknowledge by Conceptual Semantics does not tell us much about the\ndynamic interaction of the two in language use. The Two-Level\nSemantics of Bierwisch (1983a,b) and Lang (Bierwisch & Lang 1989;\nLang 1993) aims to provide such a dynamic account. Two-Level Semantics\nviews word meaning as the result of the interaction between two\nsystems: semantic form (SF) and conceptual structure\n(CS). SF is a formalized representation of the basic features of a\nword. It contains grammatical information that specifies, e.g., the\nadmissible syntactic distribution of the word, plus a set of variables\nand semantic parameters whose value is determined by the interaction\nwith CS. By contrast, CS consists of language-independent systems of\nknowledge (including general world knowledge) that mediate between\nlanguage and the world (Lang & Maienborn 2011). According to\nTwo-Level Semantics, for example, polysemous words can express\nvariable meanings by virtue of having a stable underspecified SF which\ncan be flexibly manipulated by CS. By way of example, consider the\nword ‘university’, which can be read as referring either\nto an institution (as in “the university selected John’s\napplication”) or to a building (as in “the university is\nlocated on the North side of the river”). Simplifying a bit,\nTwo-Level Semantics explains the dynamics governing the selection of\nthese readings as follows. \nTwo-Level Semantics shares Jackendoff’s and Wierzbicka’s\ncommitment to a descriptive paradigm that anchors word meaning to a\nstable decompositional template, all the while avoiding the immediate\ncomplications arising from a restrictive characterization of the type\nof conceptual factors that can modulate such stable decompositional\ntemplates in contexts. But there are, once again, a few significant\nissues. A first problem is definitional accuracy: defining the SF of\n‘university’ as \\(\\lambda x [\\textrm{purpose} [x w]\n\\mathbin{\\&} \\textit{advanced study and teaching} [w]]\\) seems too\nloose to reflect the subtle differences in meaning among\n‘university’ and related terms designating institutions\nfor higher education, such as ‘college’ or\n‘academy’. Furthermore, the apparatus of Two-Level\nSemantics relies heavily on lambda expressions, which, as some\ncommentators have noted (e.g., Taylor 1994, 1995), appears ill-suited\nto represent the complex forms of world knowledge we often rely on to\nfix the meaning of highly polysemous words. See also Wunderlich (1991,\n1993). \nThe Generative Lexicon theory (GL; Pustejovsky 1995) takes a different\napproach. Instead of explaining the contextual flexibility of word\nmeaning by appealing to rich conceptual operations applied on\nsemantically thin lexical entries, this approach postulates lexical\nentries rich in conceptual information and knowledge of worldly facts.\nAccording to classical GL, the informational resources encoded in the\nlexical entry for a typical word w consist of the following\nfour levels. \nIn particular, qualia structure specifies the conceptual relations\nthat speakers associate to the real-world referents of a word and\nimpact on the way the word is used in the language (Pustejovsky 1998).\nFor example, our knowledge that bread is something that is brought\nabout through baking is considered a Quale of the word\n‘bread’, and this knowledge is responsible for our\nunderstanding that, e.g., “fresh bread” means “bread\nwhich has been baked recently”. GL distinguishes four types of\nqualia: \nTake together, these qualia form the “qualia structure” of\na word. For example, the qualia structure of the noun\n‘sandwich’ will feature information about the composition\nof sandwiches, their nature of physical artifacts, their being\nintended to be eaten, and our knowledge about the operations typically\ninvolved in the preparation of sandwiches. The notation is as\nfollows. \nsandwich(x)\nconst = {bread, …}\nform = physobj(x)\ntel = eat(P, g, x)\nagent = artifact(x) \nQualia structure is the primary explanatory device by which GL\naccounts for polysemy. The sentence “Mary finished the\nsandwich” receives the default interpretation “Mary\nfinished eating the sandwich” because the argument\nstructure of ‘finish’ requires an action as direct object,\nand the qualia structure of ‘sandwich’ allows the\ngeneration of the appropriate sense via type coercion (Pustejovsky\n2006). GL is an ongoing research program (Pustejovsky et al. 2012)\nthat has led to significant applications in computational linguistics\n(e.g., Pustejovsky & Jezek 2008; Pustejovsky & Rumshisky\n2008). But like the theories mentioned so far, it has been subject to\ncriticisms. A first general criticism is that the decompositional\nassumptions underlying GL are unwarranted and should be replaced by an\natomist view of word meaning (Fodor & Lepore 1998; see Pustejovsky\n1998 for a reply). A second criticism is that GL’s focus on\nvariations in word meaning which depend on sentential context and\nqualia structure is too narrow, since since contextual variations in\nword meaning often depend on more complex factors, such as the ability\nto keep track of coherence relations in a discourse (e.g., Asher &\nLascarides 1995; Lascarides & Copestake 1998; Kehler 2002; Asher\n2011). Finally, the empirical adequacy of the framework has been\ncalled into question. It has been argued that the formal apparatus of\nGL leads to incorrect predictions, that qualia structure sometimes\novergenerates or undergenerates interpretations, and that the rich\nlexical entries postulated by GL are psychologically implausible\n(e.g., Jayez 2001; Blutner 2002). \nTo conclude this section, we will briefly mention some contemporary\napproaches to word meaning that, in different ways, pursue the\ntheoretical agenda of the relational current of the structuralist\nparadigm. For pedagogical convenience, we can group them into two\ncategories. On the one hand, we have network approaches,\nwhich formalize knowledge of word meaning within models where the\nlexicon is seen as a structured system of entries interconnected by\nsense relations such as synonymy, antonymy, and meronymy. On the\nother, we have statistical approaches, whose primary aim is\nto investigate the patterns of co-occurrence among words in linguistic\ncorpora. \nThe main example of network approaches is perhaps Collins and\nQuillian’s (1969) hierarchical network model, in which words are\nrepresented as entries in a network of nodes, each comprising a set of\nconceptual features defining the conventional meaning of the word in\nquestion, and connected to other nodes in the network through semantic\nrelations (more in Lehman 1992). Subsequent developments of the\nhierarchical network model include the Semantic Feature Model (Smith,\nShoben & Rips 1974), the Spreading Activation Model (Collins &\nLoftus 1975; Bock & Levelt 1994), the WordNet database (Fellbaum\n1998), as well as the connectionist models of Seidenberg &\nMcClelland (1989), Hinton & Shallice (1991), and Plaut &\nShallice (1993). More on this in the entry on\n connectionism. \nFinally, statistical analysis investigates word meaning by examining\nthrough computational means the distribution of words in linguistic\ncorpora. The main idea is to use quantitative data about the frequency\nof co-occurrence of sets of lexical items to identify their semantic\nproperties and differentiate their different senses (for overviews,\nsee Atkins & Zampolli 1994; Manning & Schütze 1999;\nStubbs 2002; Sinclair 2004). Notice that while symbolic networks are\nmodels of the architecture of the lexicon that seek to be\npsychologically adequate (i.e., to reveal how knowledge of word\nmeaning is stored and organized in the mind/brain of human speakers),\nstatistical approaches to word meaning are not necessarily interested\nin psychological adequacy, and may have completely different goals,\nsuch as building a machine translation service able to mimic human\nperformance (a goal that can obviously be achieved without reproducing\nthe cognitive mechanisms underlying translation in humans). More on\nthis in the entry on\n computational linguistics. \nAs we have seen, most theories of word meaning in linguistics face, at\nsome point, the difficulties involved in drawing a plausible dividing\nline between word knowledge and world knowledge, and the various ways\nthey attempt to meet this challenge display some recurrent features.\nFor example, they assume that the lexicon, though richly interfaced\nwith world knowledge and non-linguistic cognition, remains an\nautonomous representational system encoding a specialized body of\nlinguistic knowledge. In this section, we survey a group of empirical\napproaches that adopt a different stance on word meaning. The focus is\nonce again psychological, which means that the overall goal of these\napproaches is to provide a cognitively realistic account of the\nrepresentational repertoire underlying knowledge of word meaning.\nUnlike the approaches surveyed in\n Section 4,\n however, these theories tend to encourage a view on which the\ndistinction between the semantic and pragmatic aspects of word meaning\nis highly unstable (or even impossible to draw), where lexical\nknowledge and knowledge of worldly facts are aspects of a continuum,\nand where the lexicon is permeated by our general inferential\nabilities (Evans 2010).\n Section 5.1\n will briefly illustrate the central assumptions underlying the study\nof word meaning in cognitive linguistics.\n Section 5.2\n will turn to the study of word meaning in psycholinguistics.\n Section 5.3\n will conclude with some references to neurolinguistics. \nAt the beginning of the 1970s, Eleanor Rosch put forth a new theory of\nthe mental representation of categories. Concepts such as furniture\nor bird,\nshe claimed, are not\nrepresented just as sets of criterial features with clear-cut\nboundaries, so that an item can be conceived as falling or not falling\nunder the concept based on whether or not it meets the relevant\ncriteria. Rather, items within categories can be considered more or\nless representative of the category itself (Rosch 1975; Rosch &\nMervis 1975; Mervis & Rosch 1981). Several experiments seemed to\nshow that the application of concepts is no simple yes-or-no business:\nsome items (the “good examples”) are more easily\nidentified as falling under a concept than others (the “poor\nexamples”). An automobile is perceived as a better example of\nvehicle than a rowboat,\nand much better than an elevator; a carrot is more readily identified\nas an example of the concept vegetable\nthan a pumpkin. If the concepts speakers\nassociate to category words (such as ‘vehicle’ and\n‘vegetable’) were mere bundles of criterial features,\nthese preferences would be inexplicable, since they rank items that\nmeet the criteria equally well. It is thus plausible to assume that\nthe concepts associated to category words are have a center-periphery\narchitecture centered on the most representative examples of the\ncategory: a robin is perceived as a more “birdish” bird\nthan an ostrich or, as people would say, closer to the\nprototype of a bird or to the prototypical bird (see\nthe entry on\n concepts). \nAlthough nothing in Rosch’s experiments licensed the conclusion\nthat prototypical rankings should be reified and treated as the\ncontent of concepts (what her experiments did support was merely that\na theory of the mental representation of categories should be\nconsistent with the existence of prototype effects), the\nstudy of prototypes revolutionized the existing approaches to category\nconcepts (Murphy 2002) and was a leading force behind the birth of\ncognitive linguistics. Prototypes were central to the development of\nthe Radial Network Theory of Brugman (1988 [1981]) and Lakoff (Brugman\n& Lakoff 1988), which proposed to model the sense network of words\nby introducing in the architecture of word meanings the\ncenter-periphery relation at the heart of Rosch’s seminal work.\nAccording to Brugman, word meanings can typically be modeled as radial\ncomplexes where a dominant sense is related to less typical senses by\nmeans of semantic relations such as metaphor and metonymy. For\nexample, the sense network of ‘fruit’ features product\nof plant growth at\nits center and a more abstract outcome\nat its periphery, and the two are\nconnected by a metaphorical relation). On a similar note, the\nConceptual Metaphor Theory of Lakoff & Johnson (1980; Lakoff 1987)\nand the Mental Spaces Approach of Fauconnier (1994; Fauconnier &\nTurner 1998) combined the assumption that word meanings typically have\nan internal structure arranging multiple related senses in a radial\nfashion, with the further claim that our use of words is governed by\nhard-wired mapping mechanisms that catalyze the integration of word\nmeanings across conceptual domains. For example, it is in virtue of\nthese mechanisms that the expressions “love is war”,\n“life is a journey”) are so widespread across cultures and\nsound so natural to our ears. On the proposed view, these associations\nare creative, perceptually grounded, systematic, cross-culturally\nuniform, and grounded on pre-linguistic patterns of conceptual\nactivity which correlate with core elements of human embodied\nexperience (see the entries on\n metaphor\n and\n embodied cognition).\n More in Kövecses (2002), Gibbs (2008), and Dancygier &\nSweetser (2014). \nAnother major innovation introduced by cognitive linguistics is the\ndevelopment of a resolutely “encyclopedic” approach to\nword meaning, best exemplified by Frame Semantics (Fillmore 1975,\n1982) and by the Theory of Domains (Langacker 1987). Approximating a\nbit, an approach to word meaning can be defined\n“encyclopedic” insofar as it characterizes knowledge of\nworldly facts as the primary constitutive force of word meaning. While\nthe Mental Spaces Approach and Conceptual Metaphor Theory regarded\nword meaning mainly as the product of associative patterns between\nconcepts, Fillmore and Langacker turned their attention to the\nrelation between word meaning and the body of encyclopedic knowledge\npossessed by typical speakers. Our ability to use and interpret the\nverb ‘buy’, for example, is closely intertwined with our\nbackground knowledge of the social nature of commercial transfer,\nwhich involves a seller, a buyer, goods, money, the relation between\nthe money and the goods, and so forth. However, knowledge structures\nof this kind cannot be modeled as standard concept-like\nrepresentations. Here is how Frame Semantics attempts to meet the\nchallenge. First, words are construed as pairs of phonographic forms\nwith highly schematic concepts which are internally organized as\nradial categories and function as access sites to encyclopedic\nknowledge. Second, an account of the representational organization of\nencyclopedic knowledge is provided. According to Fillmore,\nencyclopedic knowledge is represented in long-term memory in the form\nof frames, i.e., schematic conceptual scenarios that specify\nthe prototypical features and functions of a denotatum, along with its\ninteractions with the objects and the events typically associated with\nit. Frames provide thus a schematic representation of the elements and\nentities associated with a particular domain of experience and convey\nthe information required to use and interpret the words employed to\ntalk about it. For example, according to Fillmore & Atkins (1992)\nthe use of the verb ‘bet’ is governed by the risk\nframe, which is as\nfollows: \nIn the same vein as Frame Semantics (more on the parallels in Clausner\n& Croft 1999), Langacker’s Theory of Domains argues that our\nunderstanding of word meaning depends on our access to larger\nknowledge structures called domains. To illustrate the notion\nof a domain, consider the word ‘diameter’. The meaning of\nthis word cannot be grasped independently of a prior understanding of\nthe notion of a circle. According to Langacker, word meaning is\nprecisely a matter of “profile-domain” organization: the\nprofile corresponds to a substructural element designated within a\nrelevant macrostructure, whereas the domain corresponds to the\nmacrostructure providing the background information against which the\nprofile can be interpreted (Taylor 2002). In the diameter/circle\nexample, ‘diameter’ designates a profile in the circle\ndomain. Similarly,\nexpressions like ‘hot’, ‘cold’, and\n‘warm’ designate properties in the temperature\ndomain. Langacker\nargues that domains are typically structured into hierarchies that\nreflect meronymic relations and provide a basic conceptual ontology\nfor language use. For example, the meaning of ‘elbow’ is\nunderstood with respect to the arm\ndomain, while the meaning of ‘arm’\nis situated within the body\ndomain. Importantly, individual profiles\ntypically inhere to different domains, and this is one of the factors\nresponsible for the ubiquity of polysemy in natural language. For\nexample, the profile associated to the word ‘love’ inheres\nboth to the domains of embodied experience and to the abstract domains\nof social activities such as marriage ceremonies. \nDevelopments of the approach to word meaning fostered by cognitive\nlinguistics include Construction Grammar (Goldberg 1995), Embodied\nConstruction Grammar (Bergen & Chang 2005), Invited Inferencing\nTheory (Traugott & Dasher 2001), and LCCM Theory (Evans 2009). The\nnotion of a frame has become popular in cognitive psychology to model\nthe dynamics of ad hoc categorization (e.g., Barsalou 1983,\n1992, 1999; more in\n Section 5.2).\n General information about the study of word meaning in cognitive\nlinguistics can be found in Talmy (2000a,b), Croft & Cruse (2004),\nand Evans & Green (2006). \nIn psycholinguistics, the study of word meaning is understood as the\ninvestigation of the mental lexicon, the cognitive system\nthat underlies the capacity for conscious and unconscious lexical\nactivity (Jarema & Libben 2007). Simply put, the mental lexicon is\nthe long-term representational inventory storing the body of\nlinguistic knowledge speakers are required to master in order to make\ncompetent use of the lexical elements of a language; as such, it can\nbe equated with the lexical component of an individual’s\nlanguage capacity. Research on the mental lexicon is concerned with a\nvariety of problems (for surveys, see, e.g., Traxler & Gernsbacher\n2006, Spivey, McRae & Joanisse 2012, Harley 2014), that center\naround the following tasks: \nFrom a functional point of view, the mental lexicon is usually\nunderstood as a system of lexical entries, each containing\nthe information related to a word mastered by a speaker (Rapp 2001). A\nlexical entry for a word w is typically modeled as a complex\nrepresentation made up of the following components (Levelt 1989,\n2001): \nFrom this standpoint, a theory of word meaning translates into an\naccount of the information stored in the semantic form of lexical\nentries. A crucial part of the task consists in determining exactly\nwhat kind of information is stored in lexical semantic forms as\nopposed to, e.g., bits of information that fall under the scope of\nepisodic memory or general factual knowledge. Recall the example we\nmade in\n Section 3.3:\n how much of the information that a competent zoologist can associate\nto tigers is part of her knowledge of the meaning of the word\n‘tiger’? Not surprisingly, even in psycholinguistics\ntracing a neat functional separation between word processing and\ngeneral-purpose cognition has proven a problematic task. The general\nconsensus among psycholinguists seems to be that lexical\nrepresentations and conceptual representations are richly interfaced,\nthough functionally distinct (e.g., Gleitman & Papafragou 2013).\nFor example, in clinical research it is standard practice to\ndistinguish between amodal deficits involving an inability to\nprocess information at both the conceptual and the lexical level, and\nmodal deficits specifically restricted to one of the two\nspheres (Saffran & Schwartz 1994; Rapp & Goldrick 2006;\nJefferies & Lambon Ralph 2006; more in more in\n Section 5.3).\n On the resulting view, lexical activity in humans is the output of\nthe interaction between two functionally neighboring systems, one\nbroadly in charge of the storage and processing of\nconceptual-encyclopedic knowledge, the other coinciding with the\nmental lexicon. The role of lexical entries is essentially to make\nthese two systems communicate with one another through semantic forms\n(see Denes 2009). Contrary to the folk notion of a mental lexicon\nwhere words are associated to fully specified meanings or senses which\nare simply retrieved from the lexicon for the purpose of language\nprocessing, in these models lexical semantic forms are seen as highly\nschematic representations whose primary function is to supervise the\nrecruitment of the extra-linguistic information required to interpret\nword occurrences in language use. In recent years, appeals to\n“ultra-thin” lexical entries have taken an eliminativist\nturn. It has been suggested that psycholinguistic accounts of the\nrepresentational underpinnigs of lexical competence should dispose of\nthe largely metaphorical notion of an “internal word\nstore”, and there is no such thing as a mental lexicon in the\nhuman mind (e.g., Elman 2004, 2009; Dilkina, McClelland & Plaut\n2010). \nIn addition to these approaches, in a number of prominent\npsychological accounts emerged over the last two decades, the study of\nword meaning is essentially considered a chapter of theories of the\nmental realization of concepts (see the entry on\n concepts).\n Lexical units are seen either as ingredients of conceptual networks\nor as (auditory or visual) stimuli providing access to conceptual\nnetworks. A flow of neuroscientific results has shown that\nunderstanding of (certain categories of) words correlates with neural\nactivations corresponding to the semantic content of the processed\nwords. For example, it has been shown that listening to sentences that\ndescribe actions performed with the mouth, hand, or leg activates the\nvisuomotor circuits which subserve execution and observation of such\nactions (Tettamanti et al. 2005); that reading words denoting specific\nactions of the tongue (‘lick’), fingers\n(‘pick’), and leg (‘kick’) differentially\nactivate areas of the premotor cortex that are active when the\ncorresponding movements are actually performed (Hauk et al. 2004);\nthat reading odor-related words (‘jasmine’,\n‘garlic’, ‘cinnamon’) differentially activates\nthe primary olfactory cortex (Gonzales et al. 2006); and that color\nwords (such as ‘red’) activate areas in the fusiform gyrus\nthat have been associated with color perception (Chao et al. 1999,\nSimmons et al. 2007; for a survey of results on visual activations in\nlanguage processing, see Martin 2007). \nThis body of research originated so-called simulationist (or\nenactivist) accounts of conceptual competence, on which\n“understanding is imagination” and “imagining is a\nform of simulation” (Gallese & Lakoff 2005). In these\naccounts, conceptual (often called “semantic”) competence\nis seen as the ability to simulate or re-enact perceptual (including\nproprioceptive and introspective) experiences of the states of affairs\nthat language describes, by manipulating memory traces of such\nexperiences or fragments of them. In Barsalou’s theory of\nperceptual symbol systems (1999), language understanding (and\ncognition in general) is based on perceptual experience and memory of\nit. The central claim is that “sensory-motor systems represent\nnot only perceived entities but also conceptualizations of them in\ntheir absence”. Perception generates mostly unconscious\n“neural representations in sensory-motor areas of the\nbrain”, which represent schematic components of perceptual\nexperience. Such perceptual symbols are not holistic copies of\nexperiences but selections of information isolated by attention.\nRelated perceptual symbols are integrated into a simulator\nthat produces limitless simulations of a perceptual component, such as\nred or lift. Simulators are located in long-term\nmemory and play the roles traditionally attributed to concepts: they\ngenerate inferences and can be combined recursively to implement\nproductivity. A concept is not “a static amodal structure”\nas in traditional, computationally-oriented cognitive science, but\n“the ability to simulate a kind of thing perceptually”.\nLinguistic symbols (i.e., auditory or visual memories of words) get to\nbe associated with simulators; perceptual recognition of a word\nactivates the relevant simulator, which simulates a referent for the\nword; syntax provides instructions for building integrated perceptual\nsimulations, which “constitute semantic\ninterpretations”. \nThough popular among researchers interested in the conceptual\nunderpinnings of semantic competence, the simulationist paradigm faces\nimportant challenges. Three are worth mentioning. First, it appears\nthat imulations do not always capture the intuitive truth conditions\nof sentences: listeners may enact the same simulation upon exposure to\nsentences that have different truth conditions (e.g., “The man\nstood on the corner” vs. “The man waited on the\ncorner”; see Weiskopf 2010). Moreover, simulations may\noverconstrain truth conditions. For example, even though in the\nsimulations listeners typically associate to the sentence “There\nare three pencils and four pens in Anna’s mug”, the pens\nand the pencils are in vertical position, the sentence would be true\neven if they were lying horizontally in the mug. Second, the framework\ndoes not sit well with pathological data. For example, no general\nimpairment with auditory-related words is reported in patients with\nlesions in the auditory association cortex (e.g., auditory agnosia\npatients); analogously, patients with damage to the motor cortex seem\nto have no difficulties in linguistic performance, and specifically in\ninferential processing with motor-related words (for a survey of these\nresults, see Calzavarini, to appear; for a defense of the embodied\nparadigm, Pulvermüller 2013). Finally, the theory has\ndifficulties accounting for the meaning of abstract words (e.g.,\n‘beauty’, ‘pride’, ‘kindness’),\nwhich does not appear to hinge on sensory-motor simulation (see Dove\n2016 for a discussion). \nBeginning in the mid-1970s, neuropsychological research on cognitive\ndeficits related to brain lesions has produced a considerable amount\nof findings related to the neural correlates of lexical semantic\ninformation and processing. More recently, the development of\nneuroimaging techniques such as PET, fMRI and ERP has provided further\nmeans to adjudicate hypotheses about lexical semantic processes in the\nbrain (Vigneau et al. 2006). Here we do not intend to provide a\ncomplete overview of such results (for a survey, see Faust 2012). We\nshall just mention three topics of neurolinguistic research that\nappear to bear on issues in the study of word meaning: the partition\nof the lexicon into categories, the representation of common nouns vs.\nproper names, and the distinction between the inferential and the\nreferential aspects of lexical competence. \nTwo preliminary considerations should be kept in mind. First, a\ndistinction must be drawn between the neural realization of word\nforms, i.e., traces of acoustic, articulatory, graphic, and motor\nconfigurations (‘peripheral lexicons’), and the neural\ncorrelates of lexical meanings (‘concepts’). A patient can\nunderstand what is the object represented by a picture shown to her\n(and give evidence of her understanding, e.g., by miming the\nobject’s function) while being unable to retrieve the relevant\nphonological form from her output lexicon (Warrington 1985; Shallice\n1988). Second, there appears to be wide consensus about the\nirrelevance to brain processing of any distinction between strictly\nsemantic and factual or encyclopedic information (e.g., Tulving 1972;\nSartori et al. 1994). Whatever information is relevant to such\nprocesses as object recognition or confrontation naming is standardly\ncharacterized as ‘semantic’. This may be taken as a\nstipulation—it is just how neuroscientists use the word\n‘semantic’—or as deriving from lack of evidence for\nany segregation between the domains of semantic and encyclopedic\ninformation (see Binder et al. 2009). Be that as it may, in\npresent-day neuroscience there seems to be no room for a correlate of\nthe analytic/synthetic distinction. Moreover, in the literature\n‘semantic’ and ‘conceptual’ are often used\nsynonymously; hence, no distinction is drawn between lexical semantic\nand conceptual knowledge. Finally, the focus of neuroscientific\nresearch on “semantics” is on information structures\nroughly corresponding to word-level meanings, not to sentence-level\nmeanings: hence, so far neuroscientific research has had little to say\nabout the compositional mechanisms that have been the focus (and,\noften, the entire content) of theories of meaning as pursued within\nformal semantics and philosophy of language. \nLet us start with the partition of the semantic lexicon into\ncategories. Neuropsychological research indicates that the ability to\nname objects or to answer simple questions involving such nouns can be\nselectively lost or preserved: subjects can perform much better in\nnaming living entities than in naming artifacts, or in naming animate\nliving entities than in naming fruits and vegetables (Shallice 1988).\nDifferent patterns of brain activation may correspond to such\ndissociations between performances: e.g., Damasio et al. (1996) found\nthat retrieval of names of animals and of tools activate different\nregions in the left temporal lobe. However, the details of this\npartition have been interpreted in different ways. Warrington &\nMcCarthy (1983) and Warrington & Shallice (1984) explained the\nliving vs. artifactual dissociation by taking the category distinction\nto be an effect of the difference among features that are crucial in\nthe identification of living entities and artifacts: while living\nentities are identified mainly on the basis of perceptual features,\nartifacts are identified by their function. A later theory (Caramazza\n& Shelton 1998) claimed that animate and inanimate objects are\ntreated by different knowledge systems separated by evolutionary\npressure: domains of features pertaining to the recognition of living\nthings, human faces, and perhaps tools may have been singled out as\nrecognition of such entities had survival value for humans. Finally,\nDevlin et al. (1998) proposed to view the partition as the consequence\nof a difference in how recognition-relevant features are connected\nwith one another: in the case of artifactual kinds, an object is\nrecognized thanks to a characteristic coupling of form and function,\nwhereas no such coupling individuates kinds of living things (e.g.,\neyes go with seeing in many animal species). For non-neutral surveys,\nsee Caramazza & Mahon (2006) and Shallice & Cooper (2011). \nOn the other hand, it is also known that “semantic” (i.e.,\nconceptual) competence may be lost in its entirety (though often\ngradually). This is what typically happens in semantic dementia.\nEmpirical evidence has motivated theories of the neural realization of\nconceptual competence that are meant to account for both\nmodality-specific deficits and pathologies that involve impairment\nacross all modalities. The former may involve a difficulty or\nimpossibility to categorize a visually exhibited object which,\nhowever, can be correctly categorized in other modalities (e.g., if\nthe object is touched) or verbally described on the basis of the\nobject’s name (i.e., on the basis of the lexical item supposedly\nassociated with the category). The original “hub and\nspokes” model of the brain representation of concepts (Rogers et\nal. 2004, Patterson et al. 2007) accounted for both sets of findings\nby postulating that the semantic network is composed of a series of\n“spokes”, i.e., cortical areas distributed across the\nbrain processing modality-specific (visual, auditory, motor, as well\nas verbal) sources of information, and that the spokes are two-ways\nconnected to a transmodal “hub”. While damage to the\nspokes accounts for modality-specific deficits, damage to the hub and\nits connections explains the overall impairment of semantic\ncompetence. On this model, the hub is supposed to be located in the\nanterior temporal lobe (ATL), since semantic dementia had been found\nto be associated with degeneration of the anterior ventral and polar\nregions of both temporal poles (Guo et al. 2013). According to more\nrecent, “graded” versions of the model (Lambon Ralph et\nal. 2017), the contribution of the hub units may vary depending on\ndifferent patterns of connectivity to the spokes, to account for\nevidence of graded variation of function across subregions of ATL. It\nshould be noted that while many researchers converge on a distributed\nview of semantic representation and on the role of domain-specific\nparts of the neural network (depending on differential patterns of\nfunctional connectivity), not everybody agrees on the need to\npostulate a transmodal hub (see, e.g., Mahon & Caramazza\n2011). \nLet us now turn to common nouns and proper names. As we have seen, in\nthe philosophy of language of the last decades, proper names (of\npeople, landmarks, countries, etc.) have being regarded as\nsemantically different from common nouns. Neuroscientific research on\nthe processing of proper names and common nouns concurs, to some\nextent. To begin with, the retrieval of proper names is doubly\ndissociated from the retrieval of common nouns. Some patients proved\ncompetent with common nouns but unable to associate names to pictures\nof famous people, or buildings, or brands (Ellis, Young &\nCritchley 1989); in other cases, people’s names were\nspecifically affected (McKenna & Warrington 1980). Other patients\nhad the complementary deficit. The patient described in Semenza &\nSgaramella (1993) could name no objects at all (with or without\nphonemic cues) but he was able to name 10 out of 10 familiar people,\nand 18 out of 22 famous people with a phonemic cue. Martins &\nFarrayota‘s (2007) patient ACB also presented impaired object\nnaming but spared retrieval of proper names. Such findings suggest\ndistinct neural pathways for the retrieval of proper names and common\nnouns (Semenza 2006). The study of lesions and neuroimaging research\nboth initially converged in identifying the left temporal pole as\nplaying a crucial role in the retrieval of proper names, from both\nvisual stimuli (Damasio et al. 1996) and the presentation of speaker\nvoices (Waldron et al. 2014) (though in at least one case damage to\nthe left temporal pole was associated with selective sparing of proper\nnames; see Martins & Farrajota 2007). In addition, recent research\nhas found a role for the uncinate fasciculus (UF). In patients\nundergoing surgical removal of UF, retrieval of common nouns was\nrecovered while retrieval of proper names remained impaired (Papagno\net al. 2016). The present consensus appears to be that “the\nproduction of proper names recruits a network that involves at least\nthe left anterior temporal lobe and the left orbitofrontal cortex\nconnected together by the UF” (Brédart 2017). \nFurthermore, a few neuropsychological studies have described patients\nwhose competence on geographical names was preserved while names of\npeople were lost: one patient had preserved country names, though he\nhad lost virtually every other linguistic ability (McKenna &\nWarrington 1978; see Semenza 2006 for other cases of selective\npreservation of geographical names). Other behavioral experiments seem\nto show that country names are closer to common nouns than to other\nproper names such as people and landmark names in that the\nconnectivity between the word and the conceptual system is likely to\nrequire diffuse multiple connections, as with common nouns (Hollis\n& Valentine 2001). If these results were confirmed, it would turn\nout that the linguistic category of proper names is not homogeneous in\nterms of neural processing. Studies have also demonstrated that the\nretrieval of proper names from memory is typically a more difficult\ncognitive task than the retrieval of common nouns. For example, it is\nharder to name faces (of famous people) than to name objects;\nmoreover, it is easier to remember a person’s occupation than\nher or his name. Interestingly, the same difference does not\nmaterialize in definition naming, i.e., in tasks where names and\ncommon nouns are to be retrieved from definitions (Hanley 2011).\nThough several hypotheses about the source of this difference have\nbeen proposed (see Brédart 2017 for a survey), no consensus has\nbeen reached on how to explain this phenomenon. \nFinally, a few words on the distinction between the inferential and\nthe referential component of lexical competence. As we have seen in\n Section 3.2,\n Marconi (1997) suggested that processing of lexical meaning might be\ndistributed between two subsystems, an inferential and a referential\none. Beginning with Warrington (1975), many patients had been\ndescribed that were more or less severely impaired in referential\ntasks such as naming from vision (and other perceptual modalities as\nwell), while their inferential competence was more or less intact. The\ncomplementary pattern (i.e., the preservation of referential abilities\nwith loss of inferential competence) is definitely less common. Still,\na number of cases have been reported, beginning with a stroke patient\nof Heilman et al. (1976), who, while unable to perform any task\nrequiring inferential processing, performed well in referential naming\ntasks with visually presented objects (he could name 23 of 25 common\nobjects). In subsequent years, further cases were described. For\nexample, in a study of 61 patients with lesions affecting linguistic\nabilities, Kemmerer et al. (2012) found 14 cases in which referential\nabilities were better preserved than inferential abilities. More\nrecently, Pandey & Heilman (2014), while describing one more case\nof preserved (referential) naming from vision with severely impaired\n(inferential) naming from definition, hypothesized that “these\ntwo naming tasks may, at least in part, be mediated by two independent\nneuronal networks”. Thus, while double dissociation between\ninferential processes and naming from vision is well attested, it is\nnot equally clear that it involves referential processes in general.\nOn the other hand, evidence from neuroimaging is, so far, limited and\noverall inconclusive. Some neuroimaging studies (e.g.,\nTomaszewski-Farias et al. 2005, Marconi et al. 2013), as well as TMS\nmapping experiments (Hamberger et al. 2001, Hamberger & Seidel\n2009) did find different patterns of activation for inferential vs.\nreferential performances. However, the results are not entirely\nconsistent and are liable to different interpretations. For example,\nthe selective activation of the anterior left temporal lobe in\ninferential performances may well reflect additional syntactic demands\ninvolved in definition naming, rather than be due to inferential\nprocessing as such (see Calzavarini 2017 for a discussion).","contact.mail":"diego_marconi@fastwebnet.it","contact.domain":"fastwebnet.it"}]
