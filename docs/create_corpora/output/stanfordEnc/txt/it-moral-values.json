[{"date.published":"2012-06-12","date.changed":"2018-11-09","url":"https://plato.stanford.edu/entries/it-moral-values/","author1":"John Sullins","author1.info":"http://sonoma.academia.edu/JohnSullins","entry":"it-moral-values","body.text":"\n\n\nEvery action we take leaves a trail of information that could, in\nprinciple, be recorded and stored for future use. For instance, one\nmight use the older forms of information technologies of pen and paper\nand keep a detailed diary listing all the things one did and thought\nduring the day. It might be a daunting task to record all this\ninformation this way but there are a growing list of technologies and\nsoftware applications that can help us collect all manner of data,\nwhich in principle, and in practice, can be aggregated together for\nuse in building a data profile about you, a digital diary with\nmillions of entries. Some examples of which might be: a detailed\nlisting of all of your economic transactions; a GPS generated plot of\nwhere you traveled; a list of all the web addresses you visited and\nthe details of each search you initiated online; a listing of all your\nvital signs such as blood pressure and heart rate; all of your dietary\nintakes for the day; and any other kind of data that can be\nmeasured. As you go through this thought experiment you begin to see\nthe complex trail of data that you generate each and every day and how\nthat same data might be efficiently collected and stored though the\nuse of information technologies. It is here we can begin to see how\ninformation technology can impact moral values. As this data gathering\nbecomes more automated and ever-present, we must ask who is in control\nof collecting this data and what is done with it once it has been\ncollected and stored? Which bits of information should be made public,\nwhich held private, and which should be allowed to become the property\nof third parties like corporations? Questions of the production,\naccess, and control of information will be at the heart of moral\nchallenges surrounding the use of information technology.\n\n\nOne might argue that the situation just described is no different from\nthe moral issues revolving around the production, access, and control\nof any basic necessity of life. If one party has the privilege of the\nexclusive production, access, and/or control of some natural resource,\nthen that by necessity prohibits others from using this resource\nwithout the consent of the exclusive owner. This is not necessarily so\nwith digital information. Digital information is nonexclusory, meaning\nwe can all, at least theoretically, possess the same digital\ninformation without excluding its use from others. This is because\ncopying digital information from one source to another does not\nrequire eliminating the previous copy. Unlike a physical object,\ntheoretically, we can all possess the same digital object as it can be\ncopied indefinitely with no loss of fidelity. Since making these\ncopies is often so cheap that it is almost without cost, there is no\ntechnical obstacle to the spread of all information as long as there\nare people willing to copy it and distribute it. Only appeals to\nmorality, or economic justice might prevent the distribution of\ncertain forms of information. For example, digital entertainment\nmedia, such as songs or video, has been a recurring battleground as\nusers and producers of the digital media fight to either curtail or\nextend the free distribution of this material. Therefore,\nunderstanding the role of moral values in information technology is\nindispensable to the design and use of these technologies (Johnson,\n1985, Moore, 1985, Nissenbaum, 1998, Spinello, 2001). It should be\nnoted that this entry will not directly address the phenomenological\napproach to the ethics of information technology since there is a\ndetailed entry on this subject available (see the entry on\n phenomenological approaches to ethics and information technology).\n\n\nInformation technology is ubiquitous in the lives of people across the\nglobe. These technologies take many forms such as personal computers,\nsmart phones, internet technologies, as well as AI and robotics. In\nfact, the list is growing constantly and new forms of these\ntechnologies are working their way into every aspect of daily life.\nThey all have some form of computation at their core and human users\ninterface with them mostly through applications and other software\noperating systems. In some cases, such as massive multiplayer online\ngames (see section 3.1.1 below), these\ntechnologies are even opening up new ways for humans to interacting\nwith each other. Information technologies are used to record,\ncommunicate, synthesize or organize information through the use of\ncomputer technologies. Information itself can be understood as any\nuseful data, instructions, or meaningful message content. The word\nliterally means to “give form to” or to shape one’s\nthoughts. A basic type of information technology might be the\nproverbial string tied around one’s finger that is used to remind, or\ninform, someone that they have some specific task to accomplish that\nday. Here the string stands in for a more complex proposition such as\n“buy groceries before you come home.” The string itself is\nnot the information, it merely symbolizes the information and\ntherefore this symbol must be correctly interpreted for it to be\nuseful. Which raises the question, what is information itself?  Unfortunately there is not a completely satisfying and\nphilosophically rigorous definition available, though there are at\nleast two very good starting points. For those troubled by the\nontological questions regarding information, we might want to simply\nfocus on the symbols and define information as any meaningfully\nordered set of symbols.  Mathematicians and engineers prefer to focus\non this aspect of information, which is called “syntax”\nand leave the meaningfulness of information or its\n“semantics” for others to figure out. Claude E. Shannon\nworking at Bell Labs in the forties produced a landmark mathematical\ntheory of communication (1948).  In this work he utilized his\nexperiences in cryptography and telephone technologies to work out a\nmathematical formulation describing how syntactical information can be\nturned into a signal that is transmitted in such a way as to mitigate\nnoise or other extraneous signals which can then be decoded by the\ndesired receiver of the message (Shannon 1948; Shannon and Weaver\n1949). The concepts described by Shannon, (along with additional\nimportant innovations made by others who are too many to list),\nexplain the way that information technology works, but we still have\nthe deeper questions to resolve if we want to thoroughly trace the\nimpact of information technologies on moral values. Some philosophers\nnoted the fact that information technologies had highlighted the\ndistinction between syntax and semantics, and have been vocal critics\nabout the inability of technologies to bridge the gap between the two\nconcepts.  Meaning that while information technologies might be adept\nat manipulating syntax, they would be incapable of ever understanding\nthe semantics, or meanings, of the information upon which they\nworked. One famous example can be found in the “Chinese Room\nArgument” (Searle 1980) in which the philosopher John Searle\nargued that even if one were to build a machine that could take\nstories written in Chinese as input and then output coherent answers\nto questions about those stories, it would not prove that the machine\nitself actually understood what it was doing.  The argument rests on\nthe claim that if you replaced the workings of the machine with a\nperson who was not a native Chinese speaker who would then\npainstakingly follow a set of rules to transform the set of Chinese\nlogograms input into other output symbols.  The claim is that that\nperson would not understand the input and also would not know what the\nsystem is saying as its output, it is all meaningless symbol\nmanipulation to them.  The conclusion is that this admittedly strange\nsystem could skillfully use the syntax of the language and story while\nthe person inside would have no ability to understand the semantics,\nor meaning, of the stories (Searle 1980).  Replace the person with\nelectronics and it follows that the electronics also have no\nunderstanding of the symbols they are processing. This\nargument, while provocative is not universally accepted and has lead\nto decades worth of argument and rebuttal (see the entry on\n the Chinese room argument).   Information technology has also had a lasting impression on the philosophical\nstudy of logic and information. In this field logic is used as a way to\nunderstand information as well as using information science as a way\nto build the foundations of logic itself (see the entry on \nlogic and information). The issues just discussed are fascinating but they are separate\narguments that do not necessarily have to be resolved before we can\nenter a discussion on information technology and moral values.  Even\npurely syntactical machines can still impact many important ethical\nconcerns even if they are completely oblivious to the semantic meaning\nof the information that they compute.  \nThe second starting point is to explore the more metaphysical role that\ninformation might play in philosophy. If we were to begin with the claim\nthat information either constitutes or is closely correlated with what\nconstitutes our existence and the existence of everything around us,\nthen this claim means that information plays an important ontological\nrole in the manner in which the universe operates. Adopting this\nstandpoint places information as a core concern for philosophy and\ngives rise to the fields \n philosophy of information\n and\n information ethics.\n In this entry, we will not limit our exploration to just the theory\nof information but instead look more closely at the actual moral and\nethical impacts that information technologies are already having on our\nsocieties. Philosophy of Information will not be addressed in detail\nhere but the interested reader can begin with Floridi (2010b, 2011b)\nfor an introduction. Some of the most important aspects of Information\nEthics will be outlined in more detail below. \nThe move from one set of dominant information technologies to another\nis always morally contentious. Socrates lived during the long\ntransition from a largely oral tradition to a newer information\ntechnology consisting of writing down words and information and\ncollecting those writings into scrolls and books. Famously Socrates\nwas somewhat antagonistic to writing and scholars claim that he never\nwrote anything down himself. Ironically, we only know about\nSocrates’ argument against writing because his student Plato\nignored his teacher and wrote them down in a dialogue called\n“Phaedrus” (Plato). Towards the end of this dialogue\nSocrates discusses with his friend Phaedrus the\n“…conditions which make it (writing) proper or\nimproper” (section 274b–479c). Socrates tells a fable of\nan Egyptian God he names Theuth who gives the gift of writing to a\nking named Thamus. Thamus is not pleased with the gift and\nreplies, \nIf men learn this, it will implant forgetfulness in their souls; they\nwill cease to exercise memory because they rely on that which is\nwritten, calling things to remembrance no longer from within\nthemselves, but by means of external marks. (Phaedrus, section\n275a) \nSocrates, who was adept at quoting lines from poems and epics and\nplacing them into his conversations, fears that those who rely on\nwriting will never be able to truly understand and live by these\nwords. For Socrates there is something immoral or false about writing.\nBooks can provide information but they cannot, by themselves, give you\nthe wisdom you need to use or deeply understand that information.\nConversely, in an oral tradition you do not simply consult a library,\nyou are the library, a living manifestation of the information you\nknow by heart. For Socrates, reading a book is nowhere near as\ninsightful as talking with its author. Written words,  \n…seem to talk to you as though they were intelligent, but if\nyou ask them anything about what they say, from a desire to be\ninstructed, they go on telling you the same thing forever. (Phaedrus,\nsection 275d). \nHis criticism of writing at first glance may seem humorous but the\ntemptation to use recall and call it memory is getting more and more\nprevalent in modern information technologies. Why learn anything when\ninformation is just an Internet search away? In order to avoid\nSocrates’ worry, information technologies should do more than just\nprovide access to information; they should also help foster wisdom and\nunderstanding as well. \nEarly in the information technology revolution Richard Mason suggested\nthat the coming changes in information technologies, such as their\nroles in education and economic impacts, would necessitate rethinking\nthe social contract (Mason 1986). He worried that they would challenge\nprivacy, accuracy, property and accessibility (PAPA) and to protect\nour society we “… must formulate a new social contract,\none that insures everyone the right to fulfill his or her own human\npotential” (Mason 1986 P. 11) What he could not have known then\nwas how often we would have to update the social contract as these\ntechnologies rapidly change. Information technologies change quickly\nand move in and out of fashion at a bewildering pace. This makes it\ndifficult to try to list them all and catalog the moral impacts of\neach. The very fact that this change is so rapid and momentous has\ncaused some to argue that we need to deeply question the ethics of the\nprocess of developing emerging technologies (Moor 2008).  It has also\nbeen argued that the ever morphing nature of information technology is\nchanging our ability to even fully understand moral values as they\nchange. Lorenzo Magnani claims that acquiring knowledge of how that\nchange confounds our ability to reason morally “…has\nbecome a duty in our technological world” (Magnani 2007,\n93). The legal theorist Larry Lessig warns that the pace of change in\ninformation technology is so rapid that it leaves the slow and\ndeliberative process of law and political policy behind and in effect\nthese technologies become lawless, or extralegal. This is due to the\nfact that by the time a law is written to curtail, for instance, some\nform of copyright infringement facilitated by a particular file\nsharing technology, that technology has become out of date and users\nare on to something else that facilitates even more copyright\ninfringement (Lessig 1999). But even given this rapid pace of change,\nit remains the case that information technologies or applications can\nall be categorized into at least three different types – each of\nwhich we will look at below.  \nAll information technologies record (store), transmit (communicate),\norganize and/or synthesize information. For example, a book is a\nrecord of information, a telephone is used to communicate information,\nand the Dewey decimal system organizes information. Many information\ntechnologies can accomplish more than one of the above functions and,\nmost notably, the computer can accomplish all of them since it can be\ndescribed as a universal machine (see the entry on\n computability and complexity),\n so it can be programmed to emulate any form of information\ntechnology. In\n section 2\n we will look at some specific example technologies and applications\nfrom each of the three types of information technology listed above\nand track the moral challenges that arise out of the use and design of\nthese particular technologies. In addition to the above we will need\nto address the growing use of information environments such as massive\nmultiplayer games, which are environments completely composed of\ninformation where people can develop alternate lives filled with\nvarious forms of social activities (see\n section 3.3).\n Finally we will look at not only how information technology impacts\nour moral intuitions but also how it might be changing the very nature\nof moral reasoning. In\n section 4,\n we will look at information as a technology of morality and how we\nmight program applications and robots to interact with us in a more\nmorally acceptable manner. \nThe control of information is power, and in an information economy\nsuch as we find ourselves today, it may be the ultimate form of\npolitical power. We live in a world rich in data and the technology to\nproduce, record, and store vast amounts of this data has developed\nrapidly. The primary moral concern here is that when we collect,\nstore, and/or access information, it is vital that this be done in a\njust manner that can reasonably be seen as fair and in the best\ninterests of all parties involved. As was mentioned above, each of us\nproduces a vast amount of information every day that could be recorded\nand stored as useful data to be accessed later when needed. But moral\nconundrums arise when that collection, storage and use of our\ninformation is done by third parties without our knowledge or done\nwith only our tacit consent. The social institutions that have\ntraditionally exercised this power are things like, religious\norganizations, universities, libraries, healthcare officials,\ngovernment agencies, banks and corporations. These entities have\naccess to stored information that gives them a certain amount of power\nover their customers and constituencies. Today each citizen has access\nto more and more of that stored information without the necessity of\nutilizing the traditional mediators of that information and therefore\na greater individual share of social power (see Lessig 1999).  \nOne of the great values of modern information technology is that it\nmakes the recording of information easy and almost automatic. Today, a\ngrowing number of people enter biometric data such as blood pressure,\ncalorie intake, exercise patterns, etc. into applications designed to\nhelp them achieve a healthier lifestyle. This type of data collection\ncould become almost fully automated in the near future. Through the\nuse of smart watches or technologies such as the “Fitbit,” or\ngathered through a users smartphone, such as applications that use the\nGPS tracking to track the length and duration of a user’s walk or run.\nHow long until a smartphone collects a running data stream of your\nblood pressure throughout the day perhaps tagged with geolocation\nmarkers of particularly high or low readings? In one sense this could\nbe immensely powerful data that could lead to much healthier lifestyle\nchoices. But it could also be a serious breach in privacy if the\ninformation got into the wrong hands, which could be easily\naccomplished, since third parties have access to information collected\non smartphones and online applications. In the next section\n (2.1.2)\n we will look at some theories on how best to ethically communicate\nthis recorded information to preserve privacy. But here we must\naddress a more subtle privacy breach – the collection and\nrecording of data about a users without their knowledge or\nconsent. When searching on the Internet, browser software records all\nmanner of data about our visits to various websites which can, for\nexample, make webpages load faster next time you visit them. Even the\nwebsites themselves use various means to record information when your\ncomputer has accessed them and they may leave bits of information on\nyour computer which the site can use the next time you visit. Some\nwebsites are able to detect which other sites you have visited or\nwhich pages on the website you spend the most time on. If someone were\nfollowing you around a library noting down this kind of information,\nyou might find it uncomfortable or hostile, but online this kind of\nbehavior takes place behind the scenes and is barely noticed by the\ncasual user.  \nAccording to some professionals, information technology has all but\neliminated the private sphere and that it has been this way for\ndecades. Scott McNealy of Sun Microsystems famously announced in 1999:\n“You have zero privacy anyway. Get over it” (Sprenger,\n1999). Helen Nissenbaum observes that, \n[w]here previously, physical barriers and inconvenience might have\ndiscouraged all but the most tenacious from ferreting out information,\ntechnology makes this available at the click of a button or for a few\ndollars (Nissenbaum 1997)  \nand since the time when she wrote this the gathering of personal data\nhas become more automated and cheaper. Clearly, earlier theories of\nprivacy that assumed the inviolability of physical walls no longer\napply but as Nissenbaum argues, personal autonomy and intimacy require\nus to protect privacy nonetheless (Nissenbaum 1997).  \nA final concern in this section is that information technologies are\nnow storing user data in “the cloud” meaning that the data\nis stored on a device remotely located from the user and not owned or\noperated by that user, but the data is then available from anywhere\nthe user happens to be, on any device they happen to be using. This\nease of access has the result of also making the relationship one has\nto one’s own data more tenuous because of the uncertainty about the\nphysical location of that data. Since personal data is crucially\nimportant to protect, the third parties that offer “cloud”\nservices need to understand the responsibility of the trust the user\nis placing in them. If you load all the photographs of your life to a\nservice like Flickr and they were to somehow lose or delete them, this\nwould be a tragic mistake that might not be impossible to repair. \nInformation technology has forced us to rethink earlier notions of\nprivacy that were based on print technologies, such as letters, notes,\nbooks, pamphlets, newspapers, etc. The moral values that coalesced\naround these earlier technologies have been sorely stretched by the\neasy way that information can be shared and altered using digital\ninformation technologies and this has required the rapid development\nof new moral theories that recognize both the benefits and risks of\ncommunicating all manner of information using modern information\ntechnologies. The primary moral values that seem to be under pressure\nfrom these changes are privacy, confidentiality, ownership, trust, and\nthe veracity of the information being communicated in these new\nways. \nWho has the final say whether or not some information about a user is\ncommunicated or not? Who is allowed to sell your medical records, your\nfinancial records, your email, your browser history, etc.? If you do\nnot have control over this process, then how can you enforce your own\nmoral right to privacy? For instance Alan Westin argued in the very\nearly decades of the advance of digital information technologies that\ncontrol of access to one’s personal information was the key to\nmaintaining privacy (Westin, 1967). It follows that if we care about\nprivacy, then we should give all the control of access to personal\ninformation to the individual. Most corporate entities resist this\nnotion for the simple reason that information about users has become a\nprimary commodity in the digital world boosting the vast fortunes of\ncorporations like Google or Facebook. Indeed, there is a great deal of\nutility each of us gains from the services provided by internet search\ncompanies like Google and social networks such as Facebook. It might\nbe argued that it is actually a fair exchange we receive since they\nprovide search results and other applications for free and they offset\nthe cost of creating those valuable serviced by collecting data from\nindividual user behavior that can be monetized in various lucrative\nways. A major component of the profit model for these companies is\nbased on directed advertising where the information collected on the\nuser is used to help identify advertising that will be most effective\non a particular user based on his or her search history and other\nonline behaviors. Simply by using the free applications offered, each\nuser tacitly agrees to give up some amount of privacy that varies with\nthe applications they are using. Even if we were to agree that there\nis some utility to the services users receive in this exchange, there\nare still many potential moral problems with this arrangement. If we\nfollow the argument raised by Westin earlier that privacy is\nequivalent to information control (ibid.), then we do seem to be\nceding our privacy away little by little given that we have almost no\ncontrol or even much understanding of the vast amounts of digital\ninformation that is collected about us. \nThere is a counterargument to this. Herman Tavani and James Moor\n(2004) argue that in some cases giving the user more control of their\ninformation may actually result in greater loss of privacy. Their\nprimary argument is that no one can actually control all of the\ninformation about oneself that is produced every day by our\nactivities. If we focus only on the fraction of it that we can\ncontrol, we lose sight of the vast mountains of data we cannot (Tavani\nand Moor, 2004). Tavani and Moor argue that privacy must be recognized\nby the third parties that do control your information and only if\nthose parties have a commitment to protecting user privacy, will we\nactually acquire any privacy worth having. Towards this end, they\nsuggest that we think in terms of restricted access to information\nrather than strict personal control of information (ibid). \nInformation security is another important moral value that impacts the\ncommunication and access of user information. If we grant the control\nof our information to third parties in exchange for the services they\nprovide, then these entities must also be responsible for restricting\nthe access to that information by others who might use it to harm us\n(See Epstein 2007; Magnani 2007; Tavani 2007). With enough\ninformation, a person’s entire identity can be stolen and used to\nfacilitate fraud and larceny. This type of crime has grown rapidly\nsince the advent of digital information technologies. The victims of\nthese crimes can have their lives ruined as they try to rebuild such\nthings as their credit rating and bank accounts. This has led to the\ndesign of computer systems that are more difficult to access and the\ngrowth of a new industry dedicated to securing computer systems.  Even\nwith these efforts the economic and social impact of cybercrime is\ngrowing at a staggering rate.  In February of 2018 the cyber-security\ncompany McAfee released a report that estimated the world cost in\ncybercrime was up from $445 billion in 2014 to $608 billion dollars or\n0.8 of the global GDP in 2018, and that is not counting the hidden\ncosts of increased friction and productivity loss in time spent trying\nto fight cybercrime (McAfee 2018).  \nThe difficulty in obtaining complete digital security rests on the\nfact that the moral value of security can be in conflict with the\nmoral values of sharing and openness, and it is these later values\nthat guided many of the early builders of information technology.\nSteven Levy (1984) describes in his book, “Hackers: Heroes of\nthe Computer Revolution,” a kind of “Hacker ethic,”\nthat includes the idea that computers should be freely accessible and\ndecentralized in order to facilitate “world improvement”\nand further social justice (Levy 1984; see also Markoff 2005). So it\nseems that information technology has a strong dissonance created in\nthe competing values of security and openness that is worked right\ninto the design of these technologies and this is all based on the\ncompeting moral values held by the various people who designed the\ntechnologies themselves.  \nThis conflict in values has been debated by philosophers. While many\nof the hackers interviewed by Levy argue that hacking is not as\ndangerous as it seems and that it is mostly about gaining access to\nhidden knowledge of how information technology systems work, Eugene\nSpafford counters that no computer break-in is entirely harmless and\nthat the harm precludes the possibility of ethical hacking except in\nthe most extreme cases (Spafford 2007). Kenneth Himma largely agrees\nthat the activity of computer hacking is unethical but that\npolitically motivated hacking or “Hacktivism” may have\nsome moral justification, though he is hesitant to give his complete\nendorsement of the practice due to the largely anonymous nature of the\nspeech entailed by the hacktivist protests (Himma 2007b). Mark Manion\nand Abby Goodrum agree that hacktivism could be a special case of\nethical hacking but warn that it should proceed in accordance to the\nmoral norms set by the acts of civil disobedience that marked the\ntwentieth century or risk being classified as online terrorism (Manion\nand Goodrum 2007).  \nA very similar value split plays out in other areas as well,\nparticularly in the field of intellectual property rights (see entry\non\n intellectual property/)\n and pornography and censorship (see entry on\n pornography and censorship).\n What information technology adds to these long standing moral debates\nis the nearly effortless access to information that others might want\nto control such as intellectual property, dangerous information and\npornography (Floridi 1999), as well as providing technological\nanonymity for both the user and those providing access to the\ninformation in question (Nissenbaum 1999; Sullins 2010). For example,\neven though cases of bullying and stalking occur regularly, the\nanonymous and remote actions of cyber-bullying and cyberstalking make\nthese behaviors much easier and the perpetrator less likely to be\ncaught. Given that information technologies can make these unethical\nbehaviors more likely, then it can be argued that the design of\ncyberspace itself tacitly promotes unethical behavior (Adams 2002;\nGrodzinsky and Tavani 2002). Since the very design capabilities of\ninformation technology influence the lives of their users, the moral\ncommitments of the designers of these technologies may dictate the\ncourse society will take and our commitments to certain moral values\nwill then be determined by technologists (Brey 2010; Bynum 2000; Ess\n2009; Johnson 1985; Magnani 2007; Moor 1985; Spinello 2001; Sullins\n2010). \nAssuming we are justified in granting access to some store of\ninformation that we may be in control of, there is a duty to ensure\nthat that information is truthful, accurate, and useful. A simple\nexperiment will show that information technologies might have some\ndeep problems in this regard. Load a number of different search\nengines and then type the same search terms in each of them, each will\npresent different results and some of these searches will vary widely\nfrom one another. This shows that each of these services uses a\ndifferent proprietary algorithm for presenting the user with results\nfrom their search. It follows then that not all searches are equal and\nthe truthfulness, accuracy, and usefulness of the results will depend\ngreatly on which search provider you are using and how much user\ninformation is shared with this provider. All searches are filtered by\nvarious algorithms in order to ensure that the information the search\nprovider believes is most important to the user is listed first. Since\nthese algorithms are not made public and are closely held trade\nsecrets, users are placing a great deal of trust in this filtering\nprocess. The hope is that these filtering decisions are morally\njustifiable but it is difficult to know. A simple example is found in\n“clickjacking.” If we are told a link will take us to one location on\nthe web yet when we click it we are taken to some other place, the\nuser may feel that this is a breach of trust. This is often called\n“clickjacking” and malicious software can clickjack a\nbrowser by taking the user to some other site than what is expected;\nit will usually be rife with other links that pay the clickjacker for\nbringing traffic to them (Hansen and Grossman, 2008). Again the\nanonymity and ease of use that information technology provides can\nfacilitate deceitful practices such as clickjacking. Pettit (2009)\nsuggests that this should cause us to reevaluate the role that moral\nvalues such as trust and reliance play in a world of information\ntechnology. Anonymity and the ability to hide the authors of news\nreports online has contributed to the raise of “fake news” or\npropaganda of various sorts posing as legitimate news. This is a\nsignificant problem and will be discussed in section (2.2.3) below \nLastly in this section we must address the impact that the access to\ninformation has on social justice. Information technology was largely\ndeveloped in the Western industrial societies during the twentieth\ncentury. But even today the benefits of this technology have not\nspread evenly around the world and to all socioeconomic demographics.\nCertain societies and social classes have little to no access to the\ninformation easily available to those in more well off and in\ndeveloped nations, and some of those who have some access have that\naccess heavily censored by their own governments. This situation has\ncome to be called the “digital divide,” and despite\nefforts to address this gap it may be growing wider. It is worth\nnoting that as the cost of smart phones decreases these technologies\nare giving some access to the global internet to communities that have\nbeen shut out before (Poushter 2016).  While much of this gap is\ndriven by economics (see Warschauer 2003), Charles Ess notes that\nthere is also a problem with the forces of a new kind of cyber enabled\ncolonialism and ethnocentrism that can limit the desire of those\noutside the industrial West to participate in this new “Global\nMetropolis” (Ess 2009). John Weckert also notes that cultural\ndifferences in giving and taking offence play a role in the design of\nmore egalitarian information technologies (Weckert 2007). Others argue\nthat basic moral concerns like privacy are weighed differently in\nAsian cultures (Hongladarom 2008; Lü 2005). \nIn addition to storing and communicating information, many information\ntechnologies automate the organizing of information as well as\nsynthesizing or mechanically authoring or acting on new information.\nNorbert Wiener first developed a theory of automated information\nsynthesis which he called Cybernetics (Wiener 1961 [1948]).\nWiener realized that a machine could be designed to gather information\nabout the world, derive logical conclusions about that information\nwhich would imply certain actions, which the machine could then\nimplement, all without any direct input form a human agent. Wiener\nquickly saw that if his vision of cybernetics was realized, there\nwould be tremendous moral concerns raised by such machines and he\noutlined some of them in his book the Human Use of Human\nBeings (Wiener 1950). Wiener argued that, while this sort of\ntechnology could have drastic moral impacts, it was still possible to\nbe proactive and guide the technology in ways that would increase the\nmoral reasoning capabilities of both humans and machines (Bynum\n2008). \nMachines make decisions that have moral impacts. Wendell Wallach and\nColin Allen tell an anecdote in their book “Moral\nMachines” (2008). One of the authors left on a vacation and when\nhe arrived overseas his credit card stopped working, perplexed, he\ncalled the bank and learned that an automatic anti-theft program had\ndecided that there was a high probability that the charges he was\ntrying to make were from someone stealing his card and that in order\nto protect him the machine had denied his credit card transactions.\nHere we have a situation where a piece of information technology was\nmaking decisions about the probability of nefarious activity happening\nthat resulted in a small amount of harm to the person that it was\ntrying to help. Increasingly, machines make important life changing\nfinancial decisions about people without much oversight from human\nagents. Whether or not you will be given a credit card, mortgage loan,\nthe price you will have to pay for insurance, etc., is very often\ndetermined by a machine. For instance if you apply for a credit card,\nthe machine will look for certain data points, like your salary, your\ncredit record, the economic condition of the area you reside in, etc.,\nand then calculate the probability that you will default on your\ncredit card. That probability will either pass a threshold of\nacceptance or not and determine whether or not you are given the card.\nThe machine can typically learn to make better judgments given the\nresults of earlier decisions it has made. This kind of machine\nlearning and prediction is based on complex logic and mathematics (see\nfor example, Russell and Norvig 2010), this complexity may result in\nslightly humorous examples of mistaken predictions as told in the\nanecdote above, or it might be more eventful. For example, the program\nmay interpret the data regarding the identity of one’s friends and\nacquaintances, his or her recent purchases, and other readily\navailable social data, which might result in the mistaken\nclassification of that person as a potential terrorist, thus altering\nthat person’s life in a powerfully negative way (Sullins 2010). It all\ndepends on the design of the learning and prediction algorithm,\nsomething that is typically kept secret, so that it is hard to justify\nthe veracity of the prediction. \nSeveral of the issues raised above result from the moral paradox of\nInformation technologies. Many users want information to be quickly\naccessible and easy to use and desire that it should come at as low a\ncost as possible, preferably free. But users also want important and\nsensitive information to be secure, stable and reliable. Maximizing\nour value of quick and low cost minimizes our ability to provide\nsecure and high quality information and the reverse is true also. Thus\nthe designers of information technologies are constantly faced with\nmaking uncomfortable compromises. The early web pioneer Stewart Brand\nsums this up well in his famous quote: \nIn fall 1984, at the first Hackers’ Conference, I said in one\ndiscussion session: “On the one hand information wants to be\nexpensive, because it’s so valuable. The right information in the\nright place just changes your life. On the other hand, information\nwants to be free, because the cost of getting it out is getting lower\nand lower all the time. So you have these two fighting against each\nother” (Clarke 2000—see Other Internet\n Resources)[1] \nSince these competing moral values are essentially impossible to\nreconcile, they are likely to continue to be at the heart of moral\ndebates in the use and design of information technologies for the\nforeseeable future. \nIn the section above, the focus was on the moral impacts of\ninformation technologies on the individual user. In this section, the\nfocus will be on how these technologies shape the moral landscape at\nthe societal level. At the turn of the twentieth century the term\n“web 2.0” began to surface and it referred to the new way\nthat the world wide web was being used as a medium for information\nsharing and collaboration as well as a change in the mindset of web\ndesigners to include more interoperability and user-centered\nexperiences on their websites. This term has also become associated\nwith “social media” and “social networking.”\nWhile the original design of the World Wide Web in 1989 by its creator\nTim Berners-Lee was always one that included notions of meeting others\nand collaborating with them online, users were finally ready to fully\nexploit those capabilities by 2004 when the first Web 2.0 conference\nwas held by O’Reilly Media (O’Reilly 2007 [2005]). \nThis change has meant that a growing number of people have\nbegun to spend significant portions of their lives online with other\nusers experiencing a new unprecedented lifestyle. Social networking is\nan important part of many people’s lives worldwide. Vast numbers of\npeople congregate on sites like Facebook and interact with friends old\nand new, real and virtual. The Internet offers the immersive\nexperience of interacting with others in virtual worlds where\nenvironments are constructed entirely out of information. Just now,\nemerging onto the scene are technologies that will allow us to merge\nthe real and the virtual. This new form of “augmented\nreality” is facilitated by the fact that many people now carry\nGPS enabled smart phones and other portable computers with them upon\nwhich they can run applications that let them interact with their\nsurroundings and their computers at the same time, perhaps looking at\nan item though the camera in their device and the “app”\ncalling up information about that entity and displaying it in a bubble\nabove the item. Each of these technologies comes with their own suite\nof new moral challenges some of which will be discussed below. \nSocial networking is a term given to sites and applications which\nfacilitate online social interactions that typically focus on sharing\ninformation with other users referred to as “friends.” The\nmost famous of these sites today is Facebook but there are many\nothers, such as Instagram, Twitter, Snapchat, to name just a few.\nThere are a number of moral values that these sites call into\nquestion. Shannon Vallor (2011, 2016) has reflected on how sites like\nFacebook change or even challenge our notion of friendship. Her\nanalysis is based on the Aristotelian theory of friendship (see entry\non\n Aristotle’s ethics).\nAristotle argued that humans realize a good and true life though\nvirtuous friendships. Vallor notes that four key dimensions of\nAristotle’s ‘virtuous friendship,’ namely:\nreciprocity, empathy, self-knowledge and the shared life, and that the\nfirst three are found in online social media in ways that can\nsometimes strengthen friendship (Vallor 2011, 2016). Yet she argues\nthat social media is not yet up to the task of facilitating what\nAristotle calls ‘the shared life.’ Meaning that social\nmedia can give us shared activities but not the close intimate\nfriendship that shared daily lives can give. (Here is a more complete\ndiscussion of Aristotelian friendship).\nThus these media cannot fully support the Aristotelian notion of\ncomplete and virtuous friendship by themselves (Vallor 2011). Vallor\nalso has a similar analysis of other Aristotelian virtues such as\npatience, honesty, and empathy and their problematic application in\nsocial media (Vallor 2010). Vallor has gone on to argue that both the\nusers and designers of information technologies need to develop a new\nvirtue that she terms “technomoral wisdom” which can help\nus foster better online communities and friendships (Vallor,\n2016). \nJohnny Hartz Søraker (2012) argues for a nuanced understanding\nof online friendship rather than a rush to normative judgement on the\nvirtues of virtual friends. \nPrivacy issues abound in the use of social media. James Parrish\nfollowing Mason (1986) recommends four policies that a user of social\nmedia should follow to ensure proper ethical concern for other’s\nprivacy: \nThese systems are not normally designed to explicitly infringe on\nindividual privacy, but since these services are typically free there\nis a strong economic drive for the service providers to harvest at\nleast some information about their user’s activities on the site in\norder to sell that information to advertisers for directed marketing.\nThis marketing can be done with the provider just selling access to\nusers’ data that has been made anonymous, so that the advertiser knows\nthat the user may be likely to buy a pair of jeans but they do not be\ngiven the exact identity of that person. In this way a social network\nprovider can try to maintain the moral value of privacy for its users\nwhile still profiting off of linking them with advertisers. \nThe first moral impact one encounters when contemplating online games\nis the tendency for these games to portray violence, sexism, and\nsexual violence. There are many news stories that claim a cause and\neffect relationship between violence in computer games and real\nviolence. The claim that violence in video games has a causal\nconnection to actual violence has been strongly critiqued by the\nsocial scientist Christopher J. Ferguson (Ferguson 2007). However,\nMark Coeckelbergh argues that since this relationship is tenuous at\nbest and that the real issue at hand is the effect these games have on\none’s moral character (Coeckelbergh 2007).  But Coeckelbergh\ngoes on to claim that computer games could be designed to facilitate\nvirtues like empathy and cosmopolitan moral development, thus he is\nnot arguing against all games just those where the violence inhibits\nmoral growth (Coeckelbergh 2007). A good example of this might be the\nvirtual reality experience that was designed by Planned Parenthood in\n2017, “…which focuses on the experience of accessing abortion in\nAmerica, positively influences the way viewers feel about the\nharassment that many patients, providers, and health center staff\nexperience from opponents of safe, legal abortion” (Planned\nParenthood, 2017). \nMarcus Schulzke (2010) defends the depiction of violence in video\ngames. Schulzke’s main claim is that actions in a virtual world are\nvery different from actions in the real world. Although a player may\n“kill” another player in a virtual world, the offended\nplayer is instantly back in the game and the two will almost certainly\nremain friends in the real world. Thus virtual violence is very\ndifferent from real violence, a distinction that gamers are\ncomfortable with (Schulzke 2010). While virtual violence may seem\npalatable to some, Morgan Luck (2009) seeks a moral theory that might\nbe able to allow the acceptance of virtual murder but that will not\nextend to other immoral acts such as pedophilia. Christopher Bartel\n(2011) is less worried about the distinction Luck attempts to draw;\nBartel argues that virtual pedophilia is real child pornography, which\nis already morally reprehensible and illegal across the globe. \nWhile violence is easy to see in online games, there is a much more\nsubstantial moral value at play and that is the politics of virtual\nworlds. Peter Ludlow and Mark Wallace describe the initial moves to\nonline political culture in their book, The Second Life Herald:\nThe Virtual Tabloid that Witnessed the Dawn of the Metaverse\n(2007). Ludlow and Wallace chronicle how the players in massive online\nworlds have begun to form groups and guilds that often confound the\ndesigners of the game and are at times in conflict with those that\nmake the game. Their contention is that designers rarely realize that\nthey are creating a space where people intended to live large portions\nof their lives and engage in real economic and social activity and\nthus the designers have the moral duties somewhat equivalent to those\nwho may write a political constitution (Ludlow and Wallace 2007).\nAccording to Purcell (2008), there is little commitment to democracy\nor egalitarianism by those who create and own online games and this needs to be discussed, if more and\nmore of us are going to spend time living in these virtual societies. \nA persistent concern about the use of computers and especially\ncomputer games is that this could result in anti-social behavior and\nisolation. Yet studies might not support these hypotheses (Gibba, et\nal. 1983). With the advent of massively multiplayer games as well as\nvideo games designed for families the social isolation hypothesis is\neven harder to believe. These games do, however, raise gender equality\nissues. James Ivory used online reviews of games to complete a study\nthat shows that male characters outnumber female characters in games\nand those female images that are in games tend to be overly sexualized\n(Ivory 2006). Soukup (2007) suggests that gameplay in these virtual\nworlds is most often based on gameplay that is oriented to masculine\nstyles of play thus potentially alienating women players. And those\nwomen that do participate in game play at the highest level play roles\nin gaming culture that are very different from those the largely\nheterosexual white male gamers, often leveraging their sexuality to\ngain acceptance (Taylor et al. 2009). Additionally, Joan M. McMahon\nand Ronnie Cohen have studied how gender plays a role in the making of\nethical decisions in the virtual online world, with women more likely\nto judge a questionable act as unethical then men (2009). Marcus\nJohansson suggests that we may be able to mitigate virtual immorality\nby punishing virtual crimes with virtual penalties in order to foster\nmore ethical virtual communities (Johansson 2009).  \nThe media has raised moral concerns about the way that childhood has\nbeen altered by the use of information technology (see for example\nJones 2011). Many applications are now designed specifically for\nbabies and toddlers with educational applications or just\nentertainment to help keep the children occupied while their parents\nare busy.  This encourages children to interact with computers from as\nearly an age as possible. Since children may be susceptible to media\nmanipulation such as advertising we have to ask if this practice is\nmorally acceptable or not. Depending on the particular application\nbeing used, it may encourage solitary play that may lead to isolation\nbut others are more engaging with both the parents and the children\nplaying (Siraj-Blatchford 2010). It should also be noted that\npediatricians have advised that there are no known benefits to early\nmedia use amongst young children but there potential risks (Christakis\n2009). Studies have shown that from 1998 to 2008, sedentary lifestyles\namongst children in England have resulted in the first measured\ndecline in strength since World War Two (Cohen et al. 2011). It is not\nclear if this decline is directly attributable to information\ntechnology use but it may be a contributing factor. In 2018 the\nAmerican Academy of Pediatrics released some simple guidelines for\nparents who may be trying to set realistic limits on this activity\n  (Tips from the American Academy of Pediatrics). \nOne may wonder why social media services tend to be free to use, but\nnone the less, often make fabulous profits for the private companies\nthat offer these services. It is no deep secret that the way these\ncompanies make profit is through the selling of information that the\nusers are uploading to the system as they interact with it. The more\nusers, and the more information that they provide, the greater the\nvalue that aggregating that information becomes. Mark Zuckerberg\nstated his philosophical commitment to the social value of this in his\nletter to shareholders from February 1, 2012:  \nAt Facebook, we build tools to help people connect with the people\nthey want and share what they want, and by doing this we are extending\npeople’s capacity to build and maintain relationships. People\nsharing more – even if just with their close friends or families\n– creates a more open culture and leads to a better\nunderstanding of the lives and perspectives of others. We believe that\nthis creates a greater number of stronger relationships between\npeople, and that it helps people get exposed to a greater number of\ndiverse perspectives. By helping people form these connections, we\nhope to rewire the way people spread and consume information. We think\nthe world’s information infrastructure should resemble the social\ngraph “a network built from the bottom up or peer-to-peer,\nrather than the monolithic, top-down structure that has existed to\ndate. We also believe that giving people control over what they share\nis a fundamental principle of this rewiring” (Facebook, Inc.,\n2012).  \nThe social value of perusing this is debatable, but the economic value\nhas been undeniable. At the time this was written, Mark Zuckerberg has\nbeen constantly listed in the top ten richest billionaires by Forbes\nMagazine where he is typically in the top five of that rarefied group.\nAn achievement built on providing a free service to the world. What\ncompanies like Facebook do charge for are services, such as directed\nadvertising, which allow third party companies to access information\nthat users have provided to the social media applications. The result\nis that ads bought on an application such as Facebook are more likely\nto be seen as useful to viewers who are much more likely to click on\nthese ads and buy the advertised products. The more detailed and\npersonal the information shared, the more valuable it will be to the\ncompanies that it is shared with. This radical transparency of sharing\ndeeply personal information with companies like Facebook is\nencouraged. Those who do use social networking technologies do receive\nvalue as evidenced by the rapid growth of this technology. Statista\nreports that in 2019 there will be 2.77 billion users of social media\nworldwide and it will grow to 3.02 by 2021 (Statista, 2018). The\nquestion here is, what do we give up in order to receive this\n“free” service? In 2011, back when there were less than a billion\nsocial media users the technology critic Andrew Keen warned that,\n“sharing is a trap,” and that there was a kind of cult of radical\ntransparency developing that clouded our ability to think critically\nabout the kind of power we were giving these companies (Keen, 2011).\nEven before companies like Facebook were making huge profits, there\nwere those warning of the dangers of the cult of transparency with\nwarning such as:  \n…it is not surprising that public distrust has grown in the very\nyears in which openness and transparency have been so avidly pursued.\nTransparency destroys secrecy: but it may not limit the deception and\ndeliberate misinformation that undermine relations of trust. If we\nwant to restore trust we need to reduce deception and lies, rather\nthan secrecy. (O’Neill, 2002)  \nIn the case of Facebook we can see that some of the warnings of the\ncritics were prescient. In April of 2018, Mark Zuckerberg was called\nbefore congress where he apologized for the actions of his corporation\nin a scandal that involved divulging a treasure trove of information\nabout his users to an independent researcher, who then sold it to\nCambridge Analytica, which was a company involved in political data\nanalysis. This data was then used to target political ads to the users\nof Facebook. Many of which were fake ads created by Russian\nintelligence to disrupt the US election in 2016 (Au-Yeung, 2018). \nThe philosopher Shannon Vallor critiques the cult of transparency as a\nversion of what she calls the “Technological Transparency\nParadox” (Vallor, 2016). She notes that those in favor of\ndeveloping technologies to promote radically transparent societies, do\nso under the premise that this openness will increase accountability\nand democratic ideals. But the paradox is that this cult of\ntransparency often achieves just the opposite with large unaccountable\norganizations that are not democratically chosen holding information\nthat can be used to weaken democratic societies. This is due to the\nasymmetrical relationship between the user and the companies with whom\nshe shares all the data of her life. The user is, indeed radically\nopen and transparent to the company, but the algorithms used to mine\nthe data and the 3rd parties that this data is shared with is opaque\nand not subject to accountability. We, the users of these\ntechnologies, are forced to be transparent but the companies profiting\noff our information are not required to be equally transparent.  \nMalware and computer virus threats continue to grow at an astonishing\nrate. Security industry professionals report that while certain types\nof malware attacks such as spam are falling out of fashion, newer\ntypes of attacks such as Ransomware and other methods focused on\nmobile computing devices, cryptocurrency, and the hacking of cloud\ncomputing infrastructure are on the rise outstripping any small relief\nseen in the slowing down of older forms of attack (Cisco Systems\n2018; Kaspersky Lab 2017, McAfee 2018, Symantec 2018). What is clear is\nthat this type of activity will be with us for the foreseeable\nfuture. In addition to the largely criminal activity of malware\nproduction, we must also consider the related but more morally\nambiguous activities of hacking, hacktivism, commercial spyware, and\ninformational warfare. Each of these topics has its own suite of\nsubtle moral ambiguities. We will now explore some of them here.  \nWhile there may be wide agreement that the conscious spreading of\nmalware is of questionable morality there is an interesting question\nas to the morality of malware protection and anti-virus software. With\nthe rise in malicious software there has been a corresponding growth\nin the security industry which is now a multibillion dollar market.\nEven with all the money spent on security software there seems to be\nno slowdown in virus production, in fact quite the opposite has\noccurred. This raises an interesting business ethics concern; what\nvalue are customers receiving for their money from the security\nindustry? The massive proliferation of malware has been shown to be\nlargely beyond the ability of anti-virus software to completely\nmitigate. There is an important lag in the time between when a new\npiece of malware is detected by the security community and the\neventual release of the security patch and malware removal tools. \nThe anti-virus modus operandi of receiving a sample, analyzing the\nsample, adding detection for the sample, performing quality assurance,\ncreating an update, and finally sending the update to their users\nleaves a huge window of opportunity for the adversary … even\nassuming that anti-virus users update regularly. (Aycock and Sullins\n2010) \nIn the past most malware creation was motivated by hobbyists and\namateurs, but this has changed and now much of this activity is\ncriminal in nature (Cisco Systems 2018; Kaspersky Lab 2017, McAfee\n2018, Symantec 2018). Aycock and Sullins (2010) argue that relying on\na strong defense is not enough and the situation requires a\ncounteroffensive reply as well and they propose an ethically motivated\nmalware research and creation program. This is not an entirely new\nidea and it was originally suggested by the Computer Scientist George\nLedin in his editorial for the Communications of the ACM,\n“Not Teaching Viruses and Worms is Harmful” (2005). This\nidea does run counter to the majority opinion regarding the ethics of\nlearning and deploying malware. Many computer scientists and\nresearchers in information ethics agree that all malware is unethical\n(Edgar 2003; Himma 2007a; Neumann 2004; Spafford 1992; Spinello\n2001). According to Aycock and Sullins, these worries can be mitigated\nby open research into understanding how malware is created in order to\nbetter fight this threat (2010). \nWhen malware and spyware is created by state actors, we enter the\nworld of informational warfare and a new set of moral concerns. Every\ndeveloped country in the world experiences daily cyber-attacks, with\nthe major target being the United States that experiences a purported\n1.8 billion attacks a month in 2010 (Lovely 2010) and 80 billion\nmalicious scans world wide in 2017 (McAfee 2018). The majority of\nthese attacks seem to be just probing for weaknesses but they can\ndevastate a countries internet such as the cyber-attacks on Estonia in\n2007 and those in Georgia which occurred in 2008. While the Estonian\nand Georgian attacks were largely designed to obfuscate communication\nwithin the target countries more recently informational warfare has\nbeen used to facilitate remote sabotage. The famous Stuxnet virus used\nto attack Iranian nuclear centrifuges is perhaps the first example of\nweaponized software capable of creating remotely damaging physical\nfacilities (Cisco Systems 2018). The coming decades will likely see\nmany more cyber weapons deployed by state actors along well-known\npolitical fault lines such as those between Israel-America-western\nEurope vs Iran, and America-Western Europe vs China (Kaspersky Lab\n2018). The moral challenge here is to determine when these attacks are\nconsidered a severe enough challenge to the sovereignty of a nation to\njustify military reactions and to react in a justified and ethical\nmanner to them (Arquilla 2010; Denning 2008, Kaspersky Lab 2018). \nThe primary moral challenge of informational warfare is determining\nhow to use weaponized information technologies in a way that honors\nour commitments to just and legal warfare. Since warfare is already a\nmorally questionable endeavor it would be preferable if information\ntechnologies could be leveraged to lessen violent combat. For\ninstance, one might argue that the Stuxnet virus used undetected from\n2005 to 2010 did damage to Iranian nuclear weapons programs that in\ngenerations before might have only been accomplished by an air raid or\nother kinetic military action that would have incurred significant\ncivilian casualties—and that so far there have been no reported\nhuman casualties resulting from Stuxnet. Thus malware might lessen the\namount of civilian casualties in conflict. The malware known as\n“Flame” is an interesting case of malware that evidence\nsuggests was designed to aid in espionage.  One might argue that more\naccurate information given to decision makers during wartime should\nhelp them make better decisions on the battlefield. On the other hand,\nthese new informational warfare capabilities might allow states to\nengage in continual low level conflict eschewing efforts for\npeacemaking which might require political compromise. \nAs was mentioned in the introduction above, information technologies\nare in a constant state of change and innovation. The internet\ntechnologies that have brought about so much social change were\nscarcely imaginable just decades before they appeared. Even though we\nmay not be able to foresee all possible future information\ntechnologies, it is important to try to imagine the changes we are\nlikely to see in emerging technologies. James Moor argues that moral\nphilosophers need to pay particular attention to emerging technologies\nand help influence the design of these technologies early on to\nencourage beneficial moral outcomes (Moor 2005). The following\nsections contain some potential technological concerns. \nAn information technology has an interesting growth pattern that has\nbeen observed since the founding of the industry. Intel engineer\nGordon E. Moore noticed that the number of components that could be\ninstalled on an integrated circuit doubled every year for a minimal\neconomic cost and he thought it might continue that way for another\ndecade or so from the time he noticed it in 1965 (Moore 1965). History\nhas shown his predictions were rather conservative. This doubling of\nspeed and capabilities along with a halving of costs to produce it has\nroughly continued every eighteen months since 1965 and is likely to\ncontinue. This phenomenon is not limited to computer chips and can\nalso be found in many different forms of information technologies. The\npotential power of this accelerating change has captured the\nimagination of the noted inventor and futurist Ray Kurzweil. He has\nfamously predicted that if this doubling of capabilities continues and\nmore and more technologies become information technologies, then there\nwill come a point in time where the change from one generation of\ninformation technology to the next will become so massive that it will\nchange everything about what it means to be human. Kurzweil has named\nthis potential event “the Singularity” at which time he\npredicts that our technology will allow us to become a new post human\nspecies (2006). If this is correct, there could be no more profound\nchange to our moral values. There has been some support for this\nthesis from the technology community with institutes such as \n \nthe Acceleration Studies Foundation, Future of Humanity Institute, and\n H+.[2]\n Reaction to this hypothesis from philosophers has been mixed but\nlargely critical. For example Mary Midgley (1992) argues that the\nbelief that science and technology will bring us immortality and\nbodily transcendence is based on pseudoscientific beliefs and a deep\nfear of death. In a similar vein Sullins (2000) argues that there is often a\nquasi-religious aspect to the acceptance of transhumanism that is\ncommitted to certain outcomes such as uploading of human consciousness\ninto computers as a way to achieve immortality, and that the\nacceptance of the transhumanist hypothesis influences the values\nembedded in computer technologies, which can be dismissive or hostile\nto the human body.  \nThere are other cogent critiques of this argument but none as simple\nas the realization that: \n…there is, after all, a limit to how small things can get before\nthey simply melt. Moore’s Law no longer holds. Just because\nsomething grows exponentially for some time, does not mean that it\nwill continue to do so forever… (Floridi, 2016).  \nWhile many ethical systems place a primary moral value on preserving\nand protecting nature and the natural given world, transhumanists do\nnot see any intrinsic value in defining what is natural and what is not and\nconsider arguments to preserve some perceived natural state of the\nhuman body as an unjustifiable obstacle to progress. Not all philosophers\nare critical of transhumanism, as an example Nick Bostrom (2008) of\nthe Future of Humanity Institute at Oxford University argues that\nputting aside the feasibility argument, we must conclude that there\nare forms of posthumanism that would lead to long and worthwhile lives\nand that it would be overall a very good thing for humans to become\nposthuman if it is at all possible (Bostrom, 2008).  \nArtificial Intelligence (AI) refers to the many longstanding research\nprojects directed at building information technologies that exhibit\nsome or all aspects of human level intelligence and problem solving.\nArtificial Life (ALife) is a project that is not as old as AI and is\nfocused on developing information technologies and or synthetic\nbiological technologies that exhibit life functions typically found\nonly in biological entities. A more complete description of logic and\nAI can be found in the entry on\n logic and artificial intelligence.\n ALife essentially sees biology as a kind of naturally occurring\ninformation technology that may be reverse engineered and synthesized\nin other kinds of technologies. Both AI and ALife are vast research\nprojects that defy simple explanation. Instead the focus here is on\nthe moral values that these technologies impact and the way some of\nthese technologies are programmed to affect emotion and moral\nconcern. \nAlan Turing is credited with defining the research project that would\ncome to be known as Artificial Intelligence in his seminal 1950 paper\n“Computing Machinery and Intelligence.” He described the\n“imitation game,” where a computer attempts to fool a\nhuman interlocutor that it is not a computer but another human (Turing\n1948, 1950). In 1950, he made the now famous claim that \nI believe that in about fifty years’ time…. one will be able to\nspeak of machines thinking without expecting to be contradicted. \nA description of the test and its implications to philosophy outside\nof moral values can be found here (see entry on\n the Turing test).\nTuring’s prediction may have been overly ambitious and in fact\nsome have argued that we are nowhere near the completion of\nTuring’s dream.  For example, Luciano Floridi (2011a) argues\nthat while AI has been very successful as a means of augmenting our\nown intelligence, but as a branch of cognitive science interested in\nintelligence production, AI has been a dismal disappointment.  The\nopposite opinion has also been argued and some claim that the Turing\nTest has already been passed or at least that programmers are on the\nverge of doing so. For instance it was reported by the BBC in 2014\nthat the Turing Test had been passed by a program that could convince\nthe judges that it was a 13 year old Ukrainian boy, but even so, many\nexperts remain skeptical (BBC 2014).  \nFor argument’s sake, assume Turing is correct even if he is off in his\nestimation of when AI will succeed in creating a machine that can\nconverse with you. Yale professor David Gelernter worries that that\nthere would be certain uncomfortable moral issues raised. “You\nwould have no grounds for treating it as a being toward which you have\nmoral duties rather than as a tool to be used as you like”\n(Gelernter 2007). Gelernter suggests that consciousness is a\nrequirement for moral agency and that we may treat anything without it\nin any way that we want without moral regard. Sullins (2006) counters\nthis argument by noting that consciousness is not required for moral\nagency. For instance, nonhuman animals and the other living and\nnonliving things in our environment must be accorded certain moral\nrights, and indeed, any Turing capable AI would also have moral duties\nas well as rights, regardless of its status as a conscious being\n(Sullins 2006).  \nAI is certainly capable of creating machines that can converse\neffectively in simple ways with with human beings as evidenced by\nApple Siri, Amazon Alexa, OK Goolge, etc. along with the many systems\nthat businesses use to automate customer service, but these are still\na ways away form having the natural kinds of unscripted conversations\nhumans have with one another.  But that may not matter when it comes\nto assessing the moral impact of these technologies.  In addition,\nthere are still many other applications that use AI technology. Nearly\nall of the information technologies we discussed above such as,\nsearch, computer games, data mining, malware filtering, robotics,\netc., all utilize AI programming techniques. Thus AI will grow to be a\nprimary location for the moral impacts of information technologies.\nMany governments and professional associations are now developing\nethical guidelines and standards to help shape this important\ntechnology, on e good example is the Global Initiative on the Ethics\nof Intelligent and Autonomous Systems (IEEE 2018). \nArtificial Life (ALife) is an outgrowth of AI and refers to the use of\ninformation technology to simulate or synthesize life functions. The\nproblem of defining life has been an interest in philosophy since its\nfounding. See the entry on\n life\n for a look at the concept of life and its philosophical\nramifications. If scientists and technologists were to succeed in\ndiscovering the necessary and sufficient conditions for life and then\nsuccessfully synthesize it in a machine or through synthetic biology,\nthen we would be treading on territory that has significant moral\nimpact. Mark Bedau has been tracing the philosophical implications of\nALife for some time now and argues that there are two distinct forms\nof ALife and each would thus have different moral effects if and when\nwe succeed in realizing these separate research agendas (Bedau 2004;\nBedau and Parke 2009). One form of ALife is completely computational\nand is in fact the earliest form of ALife studied. ALife is inspired\nby the work of the mathematician John von Neumann on self-replicating\ncellular automata, which von Neumann believed would lead to a\ncomputational understanding of biology and the life sciences (1966).\nThe computer scientist Christopher Langton simplified von Neumann’s\nmodel greatly and produced a simple cellular automata called\n“Loops” in the early eighties and helped get the field off\nthe ground by organizing the first few conferences on Artificial Life\n(1989). Artificial Life programs are quite different from AI programs.\nWhere AI is intent on creating or enhancing intelligence, ALife is\ncontent with very simple minded programs that display life functions\nrather than intelligence. The primary moral concern here is that these\nprograms are designed to self-reproduce and in that way resemble\ncomputer viruses and indeed successful ALife programs could become as\nmalware vectors. The second form of ALife is much more morally\ncharged. This form of ALife is based on manipulating actual biological\nand biochemical processes in such a way as to produce novel life forms\nnot seen in nature. \nScientists at the J. Craig Venter institute were able to synthesize an\nartificial bacterium called JCVI-syn1.0 in May of 2010. While media\npaid attention to this breakthrough, they tended to focus on the\npotential ethical and social impacts of the creation of artificial\nbacteria. Craig Venter himself launched a public relations campaign\ntrying to steer the conversation about issues relating to creating\nlife. This first episode in the synthesis of life gives us a taste of\nthe excitement and controversy that will be generated when more viable\nand robust artificial protocells are synthesized. The ethical concerns\nraised by Wet ALife, as this kind of research is called, are more\nproperly the jurisdiction of bioethics (see entry on\n theory and bioethics).\n But it does have some concern for us here in that Wet ALife is part\nof the process of turning theories from the life sciences into\ninformation technologies. This will tend to blur the boundaries\nbetween bioethics and information ethics. Just as software ALife might\nlead to dangerous malware, so too might Wet ALife lead to dangerous\nbacteria or other disease agents. Critics suggest that there are\nstrong moral arguments against pursuing this technology and that we\nshould apply the precautionary principle here which states that if\nthere is any chance at a technology causing catastrophic harm, and\nthere is no scientific consensus suggesting that the harm will not\noccur, then those who wish to develop that technology or pursue that\nresearch must prove it to be harmless first (see Epstein 1980). Mark\nBedau and Mark Traint argue against a too strong adherence to the\nprecautionary principle by suggesting that instead we should opt for\nmoral courage in pursuing such an important step in human\nunderstanding of life (2009). They appeal to the Aristotelian notion\nof courage, not a headlong and foolhardy rush into the unknown, but a\nresolute and careful step forward into the possibilities offered by\nthis research. \nInformation technologies have not been content to remain confined to\nvirtual worlds and software implementations. These technologies are\nalso interacting directly with us through robotics applications.\nRobotics is an emerging technology but it has already produced a\nnumber of applications that have important moral implications.\nTechnologies such as military robotics, medical robotics, personal\nrobotics and the world of sex robots are just some of the already\nexistent uses of robotics that impact on and express our moral\ncommitments (see, Anderson and Anderson 2011; Capurro and Nagenborg\n2009; Lin et al. 2012, 2017). \nThere have already been a number of valuable contributions to the\ngrowing fields of machine morality and robot ethics (roboethics). For\nexample, in Wallach and Allen’s book Moral Machines:\nTeaching Robots Right from Wrong (2010), the authors present\nideas for the design and programming of machines that can functionally\nreason on moral questions as well as examples from the field of\nrobotics where engineers are trying to create machines that can behave\nin a morally defensible way. The introduction of semi and fully\nautonomous machines, (meaning machines that make decisions with little\nor no human intervention), into public life will not be\nsimple. Towards this end, Wallach (2011) has also contributed to the\ndiscussion on the role of philosophy in helping to design public\npolicy on the use and regulation of robotics.\n \nMilitary robotics has proven to be one of the most ethically charged\nrobotics applications (Lin et al. 2008, 2013, Lin 2010; Strawser,\n2013). Today these machines are largely remotely operated (telerobots)\nor semi-autonomous, but over time these machines are likely to become\nmore and more autonomous due to the necessities of modern warfare\n(Singer 2009). In the first decades of war in the 21st\ncentury robotic weaponry has been involved in numerous killings of\nboth soldiers and noncombatants (Plaw 2013), and this fact alone is of\ndeep moral concern. Gerhard Dabringer has conducted numerous\ninterviews with ethicists and technologists regarding the implications\nof automated warfare (Dabringer 2010). Many ethicists are cautious in\ntheir acceptance of automated warfare with the provision that the\ntechnology is used to enhance ethical conduct in war, for instance by\nreducing civilian and military casualties or helping warfighters\nfollow International Humanitarian Law and other legal and ethical\ncodes of conduct in war (see Lin et al. 2008, 2013; Sullins 2009b),\nbut others have been highly skeptical of the prospects of an ethical\nautonomous war due to issues like the risk to civilians and the ease\nin which wars might be declared given that robots will be taking most\nof the risk (Asaro 2008; Sharkey 2011). \nA key development in the realm of information technologies is that\nthey are not only the object of moral deliberations but they are also\nbeginning to be used as a tool in moral deliberation itself. Since\nartificial intelligence technologies and applications are a kind of\nautomated problem solvers, and moral deliberations are a kind of\nproblem, it was only a matter of time before automated moral reasoning\ntechnologies would emerge. This is still only an emerging technology\nbut it has a number of very interesting moral implications which will\nbe outlined below. The coming decades are likely to see a number of\nadvances in this area and ethicists need to pay close attention to\nthese developments as they happen. Susan and Michael Anderson have\ncollected a number of articles regarding this topic in their book,\nMachine Ethics (2011), and Rocci Luppicini has a section of\nhis anthology devoted to this topic in the Handbook of Research on\nTechnoethics (2009). \nPatrick Grim has been a longtime proponent of the idea that philosophy\nshould utilize information technologies to automate and illustrate\nphilosophical thought experiments (Grim et al. 1998; Grim 2004). Peter\nDanielson (1998) has also written extensively on this subject\nbeginning with his book Modeling Rationality, Morality, and\nEvolution with much of the early research in the computational\ntheory of morality centered on using computer models to elucidate the\nemergence of cooperation between simple software AI or ALife agents\n(Sullins 2005).  \nLuciano Floridi and J. W. Sanders argue that information as it is used\nin the theory of computation can serve as a powerful idea that can\nhelp resolve some of the famous moral conundrums in philosophy such as\nthe nature of evil (1999, 2001). The propose that along with moral\nevil and natural evil, both concepts familiar to philosophy (see entry\non\n the problem of evil);\n we add a third concept they call artificial evil (2001). Floridi and\nSanders contend that if we do this then we can see that the actions of\nartificial agents \n…to be morally good or evil can be determined even in the\nabsence of biologically sentient participants and thus allows\nartificial agents not only to perpetrate evil (and for that matter\ngood) but conversely to ‘receive’ or ‘suffer\nfrom’ it. (Floridi and Sanders 2001) \nEvil can then be equated with something like information dissolution,\nwhere the irretrievable loss of information is bad and the\npreservation of information is good (Floridi and Sanders 2001). This\nidea can move us closer to a way of measuring the moral impacts of any\ngiven action in an information environment. \nEarly in the twentieth century the American philosopher John Dewey\n(see entry on\n John Dewey)\n proposed a theory of inquiry based on the instrumental uses of\ntechnology. Dewey had an expansive definition of technology which\nincluded not only common tools and machines but information systems\nsuch as logic, laws and even language as well (Hickman 1990). Dewey\nargued that we are in a ‘transactional’ relationship with\nall of these technologies within which we discover and construct our\nworld (Hickman 1990). This is a helpful standpoint to take as it\nallows us to advance the idea that an information technology of\nmorality and ethics is not impossible. As well as allowing us to take\nseriously the idea that the relations and transactions between human\nagents and those that exist between humans and their artifacts have\nimportant ontological similarities. While Dewey could only dimly\nperceive the coming revolutions in information technologies, his\ntheory is useful to us still because he proposed that ethics was not\nonly a theory but a practice and solving problems in ethics is like\nsolving problems in algebra (Hickman 1990). If he is right, then an\ninteresting possibility arises, namely the possibility that ethics and\nmorality are computable problems and therefore it should be possible\nto create an information technology that can embody moral systems of\nthought.  \nIn 1974 the philosopher Mario Bunge proposed that we take the notion\nof a ‘technoethics’ seriously arguing that moral\nphilosophers should emulate the way engineers approach a problem.\nEngineers do not argue in terms of reasoning by categorical\nimperatives but instead they use: \n… the forms If A produces B, and you value\nB, chose to do A, and If A produces\nB and C produces D, and you prefer\nB to D, choose A rather than C. In\nshort, the rules he comes up with are based on fact and value, I\nsubmit that this is the way moral rules ought to be fashioned, namely\nas rules of conduct deriving from scientific statements and value\njudgments. In short ethics could be conceived as a branch of\ntechnology. (Bunge 1977, 103) \nTaking this view seriously implies that the very act of building\ninformation technologies is also the act of creating specific moral\nsystems within which human and artificial agents will, at least\noccasionally, interact through moral transactions. Information\ntechnologists may therefore be in the business of creating moral\nsystems whether they know it or not and whether or not they want that\nresponsibility. \nThe most comprehensive literature that argues in favor of the prospect\nof using information technology to create artificial moral agents is\nthat of Luciano Floridi (1999, 2002, 2003, 2010b, 2011b), and Floridi\nwith Jeff W. Sanders (1999, 2001, 2004). Floridi (1999) recognizes\nthat issues raised by the ethical impacts of information technologies\nstrain our traditional moral theories. To relieve this friction he\nargues that what is needed is a broader philosophy of information\n(2002). After making this move, Floridi (2003) claims that information\nis a legitimate environment of its own and that has its own intrinsic\nvalue that is in some ways similar to the natural environment and in\nother ways radically foreign but either way the result is that\ninformation is on its own a thing that is worthy of ethical concern.\nFloridi (2003) uses these ideas to create a theoretical model of moral\naction using the logic of object oriented programming.  \nHis model has seven components; 1) the moral agent a, 2) the moral\npatient p (or more appropriately, reagent), 3) the\ninteractions of these agents, 4) the agent’s frame of\ninformation, 5) the factual information available to the agent\nconcerning the situation that agent is attempting to navigate, 6) the\nenvironment the interaction is occurring in, and 7) the situation in\nwhich the interaction occurs (Floridi 2003, 3). Note that there is no\nassumption of the ontology of the agents concerned in the moral\nrelationship modeled and these agents can be any mixture or artificial\nor natural in origin (Sullins 2009a). \nThere is additional literature which critiques arguments such as\nFloridi’s with the hope of expanding the idea of automated moral\nreasoning so that one can speak of many different types of automated\nmoral technologies form simple applications all the way to full moral\nagents with rights and responsibilities similar to humans (Adam 2008;\nAnderson and Anderson 2011; Johnson and Powers 2008; Schmidt 2007;\nWallach and Allen 2010). \nWhile scholars recognize that we are still some time from creating\ninformation technology that would be unequivocally recognized as an\nartificial moral agent, there are strong theoretical arguments in\nsuggesting that automated moral reasoning is an eventual possibility\nand therefore it is an appropriate area of study for those interested\nin the moral impacts of information technologies.","contact.mail":"john.sullins@sonoma.edu","contact.domain":"sonoma.edu"}]
