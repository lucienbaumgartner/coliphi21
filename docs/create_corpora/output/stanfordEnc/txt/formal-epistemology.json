[{"date.published":"2015-03-02","date.changed":"2021-03-15","url":"https://plato.stanford.edu/entries/formal-epistemology/","author1":"Jonathan Weisberg","author1.info":"http://www.philosophy.utoronto.ca/directory/jonathan-weisberg/","entry":"formal-epistemology","body.text":"\n\n\nFormal epistemology explores knowledge and reasoning using\n“formal” tools, tools from math and logic. For example, a\nformal epistemologist might use probability theory to explain how\nscientific reasoning works. Or she might use modal logic to defend a\nparticular theory of knowledge.\n\n\nThe questions that drive formal epistemology are often the same as\nthose that drive “informal” epistemology. What is\nknowledge, and how is it different from mere opinion? What separates\nscience from pseudoscience? When is a belief justified? What justifies\nmy belief that the sun will rise tomorrow, or that the external world\nis real and not an illusion induced by Descartes’ demon?\n\n\nAnd yet, the tools formal epistemologists apply to these questions\nshare much history and interest with other fields, both inside and\noutside philosophy. So formal epistemologists often ask questions that\naren’t part of the usual epistemological core, questions about\ndecision-making\n (§5.1)\n or the meaning of hypothetical language\n (§5.3),\n for example.\n\n\nPerhaps the best way to get a feel for formal epistemology is to look\nat concrete examples. We’ll take a few classic epistemological\nquestions and look at popular formal approaches to them, to see what\nformal tools bring to the table. We’ll also look at some\napplications of these formal methods outside epistemology.\n\nHow does scientific reasoning work? In the early 20th\ncentury, large swaths of mathematics were successfully reconstructed\nusing first-order logic. Many philosophers sought a similar\nsystematization of the reasoning in empirical sciences, like biology,\npsychology, and physics. Though empirical sciences rely heavily on\nnon-deductive reasoning, the tools of deductive logic still offer a\npromising starting point. \nConsider a hypothesis like All electrons have negative\ncharge, which in first-order logic is rendered \\(\\forall x (Ex\n\\supset Nx)\\). Having identified some object \\(a\\) as an electron,\nthis hypothesis deductively entails a prediction, \\(Na\\), that \\(a\\)\nhas negative charge: \\[ \\begin{array}{l} \\forall x (Ex \\supset Nx)\\\\ Ea\\\\ \\hline Na\n\\end{array} \\] \nIf we test this prediction and observe that, indeed, \\(Na\\), this\nwould seem to support the hypothesis. \nScientific hypothesis-testing thus appears to work something like\n“deduction in reverse” (Goodman 1954). If we swap the\nhypothesis and the predicted datum in the above deduction, we get an\nexample of confirmation: \\[ \\begin{array}{l} Ea\\\\ Na\\\\ \\overline{\\overline{\\forall x (Ex\n\\supset Nx)}} \\end{array} \\] \nHere the double-line represents non-deductive inference. The inference\nis very weak in this case, since the hypothesis has only been verified\nin one instance, \\(a\\). But as we add further instances \\(b\\), \\(c\\),\netc., it becomes stronger (provided we discover no counter-instances,\nof course). \nThese observations suggest a proposal due to Nicod (1930) and famously\nexamined by Hempel (1945): \nNicod’s Criterion\n\nA universal generalization is confirmed by its positive instances (as\nlong as no counter-instances are discovered): \\(\\forall x(Fx \\supset\nGx)\\) is confirmed by \\(Fa \\wedge Ga\\), by \\(Fb \\wedge Gb\\), etc. \nThe general idea is that hypotheses are confirmed when their\npredictions are borne out. To capture this idea formally in deductive\nlogic, we’re equating prediction with logical entailment. When\nan object is \\(F\\), the hypothesis \\(\\forall x(Fx \\supset Gx)\\)\nentails/predicts that the object is \\(G\\). So any discovery of an\nobject that is both \\(F\\) and \\(G\\) confirms the hypothesis. \nOne classic challenge for Nicod’s criterion is the notorious\nraven paradox. Suppose we want to test the hypothesis that\nall ravens are black, which we formalize \\(\\forall x(Rx \\supset Bx)\\).\nThat’s logically equivalent to \\(\\forall x(\\neg Bx \\supset \\neg\nRx)\\), by contraposition. And Nicod’s Criterion says this latter\nhypothesis is confirmed by the discovery of any object that is not\nblack and not a raven—a red shirt, for example, or a pair of\nblue underpants (Hempel 1937, 1945). But walking the halls of my\ndepartment noting non-black non-ravens hardly seems a reasonable way\nto verify that all ravens are black. How can “indoor\nornithology” (Goodman 1954) be good science?! \nA second, more general challenge for the prediction-as-deduction\napproach is posed by statistical hypotheses. Suppose we want to test\nthe theory that only 50% of ravens are black. This hypothesis entails\nnothing about the color of an individual raven; it might be one of the\nblack ones, it might not. In fact, even a very large survey of ravens,\nall of which turn out to be black, does not contradict this\nhypothesis. It’s always possible that the 50% of ravens that\naren’t black weren’t caught up in the survey. (Maybe\nnon-black ravens are exceptionally skilled at evasion.) \nThis challenge suggests some important lessons. First, we need a laxer\nnotion of prediction than deductive entailment. The 50% hypothesis may\nnot entail that a large survey of ravens will have some\nnon-black ravens, but it does suggest this prediction pretty strongly.\nSecond, as a sort of corollary, confirmation is quantitative: it comes\nin degrees. A single, black raven doesn’t do much to support the\nhypothesis that 50% of ravens are black, but a large sample of roughly\nhalf black, half white ravens would. Third and finally, degrees of\nconfirmation should be understood in terms of probability. The 50%\nhypothesis doesn’t make it very probable that a single raven\nwill be black, but it makes it highly probable that a much larger\ncollection will be roughly half black, half non-black. And the\nall-black hypothesis predicts that any sample of ravens will be\nentirely black with \\(100\\)% probability. \nA quantitative approach also promises to help resolve the raven\nparadox. The most popular resolution says that observing a red shirt\ndoes confirm that all ravens are black, just by a very minuscule\namount. The raven paradox is thus an illusion: we mistake a minuscule\namount of confirmation for none at all (Hosiasson-Lindenbaum 1940).\nBut to make this response convincing, we need a proper, quantitative\ntheory of confirmation that explains how a red shirt could be relevant\nto a hypothesis about ravens, but only just slightly relevant. \nLet’s start with the idea that to confirm a hypothesis is to\nmake it more probable. The more a piece of evidence increases the\nprobability of a hypothesis, the more it confirms the hypothesis. \nWhat we need then is a theory of probability. The standard theory\nbegins with a function, \\(p\\), which takes in a proposition and\nreturns a number, \\(x\\), the probability of that proposition:\n\\(p(A)=x\\). To qualify as a probability function, \\(p\\) must\nsatisfy three axioms: \nThe first axiom sets the scale of probability, from 0 to 1, which we\ncan think of as running from 0% probability to 100%\n probability.[2]\n The second axiom places tautologies at the top of this scale: nothing\nis more probable than a\n tautology.[3]\n And finally, the third axiom tells us how to figure out the\nprobability of a hypothesis by breaking it into parts. For example,\nthe probability that an American country will be the first to develop\na cure for Alzheimer’s can be figured by adding the probability\nthat a North American country will be first to the probability that a\nSouth American country will\n be.[4] \nWhat about conditional probabilities, like the probability of\ndoing well in your next philosophy class given that you’ve done\nwell in previous ones? So far we’ve only formalized the notion\nof absolute probability, \\(p(A)=x\\). Let’s introduce conditional\nprobability by definition: \nDefinition. The conditional probability of\n\\(B\\) given \\(A\\) is written \\(p(B\\mid A)\\), and is defined:\n\n\\[p(B\\mid A) = \\frac{p(B \\wedge A)}{p(A)}.\\]\n\n  \nWhy this definition? A helpful heuristic is to think of the\nprobability of \\(B\\) given \\(A\\) as something like the portion of the\n\\(A\\)-possibilities that are also \\(B\\)-possibilities. For example,\nthe probability of rolling a high number (4, 5, or 6) on a six-sided\ndie given that the roll is even is 2/3. Why? There are 3 even\npossibilities (2, 4, 6), so \\(p(A) = 3/6\\). Of those 3 possibilities,\n2 are also high numbers (4, 6), so \\(p(B \\wedge A) = 2/6\\). Thus\n\n\\[p(B\\mid A) = \\frac{p(B \\wedge A)}{p(A)} = \\frac{2/6}{3/6} = 2/3.\\]\n\n Generalizing this idea, we start with the quantity of\n\\(A\\)-possibilities as a sort of baseline by putting \\(p(A)\\) in the\ndenominator. Then we consider how many of those are also\n\\(B\\)-possibilities by putting \\(p(B \\wedge A)\\) in the numerator. \nNotice, by the way, that \\(p(B\\mid A)\\) is undefined when \\(p(A) =\n0\\). This might seem fine at first. Why worry about the probability of\n\\(B\\) when \\(A\\) is true if there’s no chance \\(A\\) is true? In\nfact there are deep problems lurking here (Hájek\nm.s., Other Internet Resources), though we won’t\nstop to explore them. \nInstead, let’s take advantage of the groundwork we’ve laid\nto state our formal definition of quantitative confirmation. Our\nguiding idea is that evidence confirms a hypothesis to the extent that\nit increases its probability. So we are comparing \\(p(H\\mid E)\\) to\n\\(p(H)\\) by looking at the difference between them: \nDefinition. The degree to which \\(E\\) confirms \\(H\\),\ncalled the degree of confirmation, is written \\(c(H,E)\\) and\nis defined: \n\n\\[c(H,E) = p(H\\mid E) - p(H).\\]\n\n  \nWhen \\(c(H,E)\\) is negative, \\(E\\) actually decreases the probability\nof \\(H\\), and we say that \\(E\\) disconfirms \\(H\\). When\n\\(c(H,E)\\) is 0, we say that \\(E\\) is neutral with respect to\n\\(H\\). \nMinimal as they are, these simple axioms and definitions are enough to\nderive many interesting claims about probability and confirmation. The\nfollowing two subsections introduce some elementary, yet promising\nresults. See the\n technical supplement\n for proofs. \nLet’s start with some elementary theorems that illustrate how\nprobability interacts with deductive logic: \nTheorem (No Chance for Contradictions). When \\(A\\) is\na contradiction, \\(p(A) = 0\\). \nTheorem (Complementarity for Contradictories). For\nany \\(A\\), \\(p(A) = 1 - p(\\neg A)\\). \nTheorem (Equality for Equivalents). When \\(A\\) and\n\\(B\\) are logically equivalent, \\(p(A) = p(B)\\). \nTheorem (Conditional Certainty for Logical\nConsequences) When \\(A\\) logically entails \\(B\\), \\(p(B\\mid\nA)=1\\). \nThe next three theorems go a bit deeper, and are useful for building\nup more interesting results: \nTheorem (Conjunction Costs Probability). For any\n\\(A\\) and \\(B\\), \\(p(A) > p(A \\wedge B)\\) unless \\(p(A \\wedge \\neg\nB)=0\\), in which case \\(p(A) = p(A \\wedge B)\\). \nOne way of thinking about what Conjunction Costs Probability says is\nthat the stronger a statement is, the greater the risk of falsehood.\nIf we strengthen \\(A\\) by adding \\(B\\) to it, the resulting, stronger\nstatement is less probable. Unless, that is, there was no chance of\n\\(A\\) being true without \\(B\\) to begin with. In that case, adding\n\\(B\\) to \\(A\\) doesn’t change the risk of falsehood, because\nthere was no chance of \\(A\\) being true without \\(B\\) anyway. \nTheorem (The Conjunction Rule). For any \\(A\\) and\n\\(B\\) such that \\(p(B) \\neq 0\\), \\(p(A \\wedge B) = p(A\\mid\nB)p(B)\\). \nThis says we can calculate how likely two statements \\(A\\) and \\(B\\)\nare to be true together by temporarily taking \\(B\\) for granted,\nassessing the probability of \\(A\\) in that light, and then giving the\nresult as much weight as \\(B\\)’s probability on its own\nmerits. \nTheorem (The Law of Total Probability). For any\n\\(A\\), and any \\(B\\) whose probability is neither \\(0\\) nor 1:\n\n\\[p(A) = p(A\\mid B)p(B) + p(A\\mid \\neg B)p(\\neg B).\\]\n\n  \nThe Law of Total Probability basically says that we can calculate the\nprobability of \\(A\\) by breaking it down into two possible cases:\n\\(B\\) and \\(\\neg B\\). We consider how likely \\(A\\) is if \\(B\\) is true\nand how likely it is if \\(B\\) is false. We then give each case\nappropriate “weight”, by multiplying it against the\nprobability that it holds, then adding together the results. For this\nto work, \\(p(A\\mid B)\\) and \\(p(A\\mid \\neg B)\\) have to be\nwell-defined, so \\(p(B)\\) can’t be 0 or 1. \nThis classic theorem relates a conditional probability \\(p(H\\mid E)\\)\nto the unconditional probability, \\(p(H)\\): \n\n\\[ p(H\\mid E) = p(H)\\frac{p(E\\mid H)}{p(E)}\\] \nThe theorem is philosophically important, as we’ll see in a\nmoment. But it’s also useful as a tool for calculating \\(p(H\\mid\nE)\\), because the three terms on the right hand side can often be\ninferred from available statistics. \nConsider, for example, whether a student at University X having\nhigh grades (\\(E\\)) says anything about the likelihood of her taking a\nclass in philosophy (\\(H\\)). The registrar tells us that 35% of\nstudents take a philosophy class at some point, so \\(p(H) = 35/100\\).\nThey also tell us that only 20% of students campus-wide have high\ngrades (defined as a GPA of 3.5 or above), so \\(p(E) = 20/100\\). But\nthey don’t keep track of any more detailed information. Luckily,\nthe philosophy department can tell us that 25% of students who take\ntheir classes have high grades, so \\(p(E\\mid H) = 25/100\\).\nThat’s everything we need to apply Bayes’ theorem:\n\n\\[\\begin{split} p(H\\mid E) &=\np(H)\\frac{p(E\\mid H)}{p(E)}\\\\ &= 35/100 \\times\n\\frac{25/100}{20/100}\\\\ &= 7/16\\end{split}\\] \nThat’s higher than \\(p(H) = 20/100\\), so we can also see that a\nstudent’s having high grades confirms the hypothesis that she\nwill take a philosophy class. \nWhat’s the philosophical significance of Bayes’ theorem?\nIt unifies a number of influential ideas about confirmation and\nscientific methodology, binding them together in a single, simple\nequation. Let’s see how. \nTheoretical Fit. It’s a truism that the better a theory\nfits the evidence, the more the evidence supports it. But what does it\nmean for a theory to fit the evidence? \nWhen \\(H\\) entails \\(E\\), the theory says the evidence must be true,\nso the discovery of the evidence fits the theory perfectly. Our\nformalism vindicates the truism in this special case as follows. When\n\\(H\\) entails \\(E\\), Conditional Certainty for Logical Consequences\ntells us that \\(p(E\\mid H)=1\\), so Bayes’ theorem becomes:\n\n\\[p(H\\mid E) = p(H)\\frac{1}{p(E)}\\] \nProvided \\(p(E)\\) is less than 1, this amounts to multiplying \\(p(H)\\)\nby a ratio greater than 1, which means \\(p(H\\mid E)\\) comes out larger\nthan \\(p(H)\\). Moreover, since 1 is the greatest quantity that can\nappear in the numerator, the case where \\(H\\) entails \\(E\\) and thus\n\\(p(E\\mid H)=1\\) gives the greatest possible boost to the probability\nof \\(H\\). In other words, confirmation is greatest when the theory\nfits the evidence as well as possible. \n(What if \\(p(E) = 1\\), though? Then \\(H\\) may fit \\(E\\), but so may\n\\(\\neg H\\). If \\(p(E)=1\\), we can prove that \\(p(E\\mid H)=1\\) and\n\\(p(E\\mid \\neg H)=1\\) (hint: combine The Law of Total Probability with\nComplementarity for Contradictories). In other words, \\(E\\) fits both\n\\(H\\) and its negation perfectly. So it shouldn’t be able to\ndiscriminate between these two hypotheses. And, indeed, in this case\n\\(p(H\\mid E)\\) comes out the same as \\(p(H)\\), so \\(c(H,E)=0\\).) \nWhat about when the theory fits the evidence less than perfectly? If\nwe think of fit as the certainty with which \\(H\\) predicts \\(E\\),\n\\(p(E\\mid H)\\), then the previous analysis generalizes nicely. Suppose\n\\(H\\) predicts \\(E\\) strongly, but not with absolute certainty:\n\\(p(E\\mid H) = 1 - \\varepsilon\\), for some small number\n\\(\\varepsilon\\). Applying Bayes’ theorem again, we have:\n\n\\[ p(H\\mid E) = p(H)\\frac{1-\\varepsilon}{p(E)}\\] \nThis again amounts to multiplying \\(p(H)\\) by a ratio larger than 1,\nprovided \\(p(E)\\) isn’t close to 1. So \\(p(H\\mid E)\\) will come\nout larger than \\(p(H)\\). Of course, the larger \\(\\varepsilon\\) gets,\nthe weaker the confirmation becomes, befitting the weakness with which\n\\(H\\) then predicts \\(E\\). \nNovel Prediction. Another truism is that novel predictions\ncount more. When a theory predicts something we wouldn’t\notherwise expect, it’s confirmed especially strongly if the\nprediction is borne out. For example, Poisson derided the theory that\nlight is a wave because it predicted a bright spot should appear at\nthe center of certain shadows. No one had previously observed such\nbright spots, making it a novel prediction. When the presence of these\nbright spots was then verified, it was a boon for the wave theory. \nOnce again, our formalization vindicates the truism. Suppose as before\nthat \\(H\\) predicts \\(E\\) and thus \\(p(E\\mid H) = 1\\), or nearly so. A\nnovel prediction is one where \\(p(E)\\) is low, or at least not very\nhigh. It’s a prediction one wouldn’t expect. Our previous\nanalysis exposed that, in such circumstances, we multiply \\(p(H)\\) by\na large ratio in Bayes’ theorem. Thus \\(p(H\\mid E)\\) comes out\nsignificantly larger than \\(p(H)\\), making \\(c(H,E)\\) large. So novel\npredictions turn out especially confirmatory. \nPrior Plausibility. A final truism: new evidence for a theory\nhas to be weighed against the theory’s prior plausibility. Maybe\nthe theory is inherently implausible, being convoluted or\nmetaphysically fraught. Or maybe the theory had become implausible\nbecause it clashed with earlier evidence. Or maybe the theory was\nalready pretty plausible, being elegant and fitting well with previous\nevidence. In any case, the new evidence has to be evaluated in light\nof these prior considerations. \nOnce again, Bayes’ theorem vindicates this truism. \\(p(H\\mid\nE)\\) is calculated by multiplying \\(p(H)\\) by the factor \\(p(E\\mid\nH)/p(E)\\). We can think of the factor \\(p(E\\mid H)/p(E)\\) as capturing\nthe extent to which the evidence counts for \\(H\\) (or against it, if\n\\(p(E\\mid H)/p(E)\\) is less than 1), which we then multiply against\nthe previous probability of \\(H\\), \\(p(H)\\), in order to obtain\n\\(H\\)’s new, all-things-considered plausibility. If \\(H\\) was\nalready implausible, \\(p(H)\\) will be low and the result of this\nmultiplication will be smaller than it would be if \\(H\\) had already\nbeen plausible, and \\(p(H)\\) had thus been high. \nLet’s pause to summarize. Bayes’ theorem isn’t just\na useful calculational tool. It also vindicates three truisms about\nconfirmation, unifying them in a single equation. Each truism\ncorresponds to a term in Bayes’ theorem: \n\\(p(E\\mid H)\\) corresponds to theoretical fit. The better the\nhypothesis fits the evidence, the greater this quantity will be. Since\nthis term appears in the numerator in Bayes’ theorem, better fit\nmeans a larger value for \\(p(H\\mid E)\\). \n\\(p(E)\\) corresponds to predictive novelty, or rather the\nlack of it. The more novel the prediction is, the less we expect \\(E\\)\nto be true, and thus the smaller \\(p(E)\\) is. Since this term appears\nin the denominator of Bayes’ theorem, more novelty means a\nlarger value for \\(p(H\\mid E)\\). \n\\(p(H)\\) corresponds to prior plausibility. The more\nplausible \\(H\\) is before the discovery of \\(E\\), the greater this\nquantity will be, and thus the greater \\(p(H\\mid E)\\) will\nbe. \nBut what about the raven paradox? \nRecall the raven paradox: the hypothesis that all ravens are black is\nlogically equivalent to the hypothesis that all non-black things are\nnon-ravens. Yet the latter would seem to be confirmed with each\ndiscovery of a non-black, non-raven…red shirts, blue\nunderpants, etc. Yet examining the contents of your neighbor’s\nclothesline doesn’t seem a good way to research an\nornithological hypothesis. (Nor does it seem a good way to treat your\nneighbor.) \nThe classic, quantitative solution originates with\nHosiasson-Lindenbaum (1940). It holds that the discovery of blue\nunderpants does confirm the hypothesis that all ravens are black, just\nby so little that we overlook it. How could blue underpants be\nrelevant to the hypothesis that all ravens are black? Informally, the\nidea is that an object which turns out to be a blue pair of underpants\ncould instead have turned out to be a white raven. When it turns out\nnot to be such a counterexample, our hypothesis passes a weak sort of\ntest. Does our formal theory of confirmation vindicate this informal\nline of thinking? The answer is, “yes, but…”. \nThe ‘but…’ will prove crucial to the fate of\nNicod’s Criterion (spoiler: outlook not good). But let’s\nstart with the ‘yes’. \nWe vindicate the ‘yes’ with a theorem: discovering an\nobject to be a non-raven that isn’t black, \\(\\neg R \\wedge \\neg\nB\\), just slightly boosts the probability of the hypothesis that all\nravens are black, \\(H\\), if we make certain assumptions. Here\nis the theorem (see the\n technical supplement\n for a proof): \nTheorem (Raven Theorem). If (i) \\(p(\\neg R \\mid \\neg\nB)\\) is very high and (ii) \\(p(\\neg B\\mid H)=p(\\neg B)\\), then\n\\(p(H\\mid \\neg R \\wedge \\neg B)\\) is just slightly larger than\n\\(p(H)\\). \nThe first assumption, that \\(p(\\neg R \\mid \\neg B)\\) is very high,\nseems pretty sensible. With all the non-ravens in the world, the\nprobability that a given object will be a non-raven is quite high,\nespecially if it’s not black. The second assumption is that\n\\(p(\\neg B\\mid H)=p(\\neg B)\\). In other words, assuming that all\nravens are black doesn’t change the probability that a given\nobject will not be black. This assumption is more controversial\n(Vranas 2004). If all the ravens are black, then some of the things\nthat might have been black aren’t, namely the ravens. In that\ncase shouldn’t \\(p(\\neg B\\mid H) < p(\\neg B)\\) instead? On\nthe other hand, maybe all the ravens being black doesn’t reduce\nthe number of black things in the universe. Maybe it just means that\nother kinds of things are black slightly more often. Luckily, it turns\nout we can replace (ii) with less dubious assumptions (Fitelson 2006;\nFitelson and Hawthorne 2010; Rinard 2014). But we can’t do with\nno assumptions at all, which brings us to two crucial points about\nconfirmation and probability. \nThe first point is that Nicod’s Criterion fails. Assumptions\nlike (i) and (ii) of the Raven Theorem don’t always hold. In\nfact, in some situations, discovering a black raven would actually\nlower the probability that all ravens are black. How could\nthis be? The trick is to imagine a situation where the very discovery\nof a raven is bad news for the hypothesis that all ravens are black.\nThis would happen if the only way for all the ravens to be black is\nfor there to be very few of them. Then stumbling across a raven would\nsuggest that ravens are actually plentiful, in which case they\naren’t all black. Good (1967) offers the following, concrete\nillustration. Suppose there are only two possibilities: \nAll ravens are black, though there are only \\(100\\) ravens and a\nmillion other things. \nThere is one non-black raven out of \\(1,000\\) ravens, and there are a\nmillion other things. \nIn this case, happening upon a raven favors \\(\\neg H\\) because \\(\\neg\nH\\) makes ravens ten times less exotic. That the raven is black fits\nslightly better with \\(H\\), but not enough to outweigh the first\neffect: black ravens are hardly a rarity on \\(\\neg H\\). This is the\n‘but…’ to go with our earlier\n‘yes’. \nThe second point is a far-reaching moral: that the fates of claims\nabout confirmation often turn crucially on what assumptions we make\nabout the values of \\(p\\). Nicod’s criterion fails in situations\nlike Good’s, where \\(p\\) assigns a lower value to \\(p(R \\wedge\nB\\mid H)\\) than to \\(p(R \\wedge B\\mid \\neg H)\\). But in another\nsituation, where things are reversed, Nicod’s Criterion does\napply. Likewise, a diagnosis of the raven paradox like the standard\none only applies given certain assumptions about \\(p\\), like\nassumptions (i) and (ii) of the Raven Theorem. The probability axioms\nalone generally aren’t enough to tell us when Nicod’s\nCriterion applies, or when confirmation is small or large, positive or\nnegative. \nThis last point is a very general, very important phenomenon. Like the\naxioms of first-order logic, the axioms of probability are quite weak\n(Howson and Urbach 1993; Christensen 2004). Unless \\(H\\) is a\ntautology or contradiction, the axioms only tell us that its\nprobability is somewhere between \\(0\\) and 1. If we can express \\(H\\)\nas a disjunction of two logically incompatible sub-hypotheses, \\(H_1\\)\nand \\(H_2\\), and we know the probabilities of these sub-hypotheses,\nthen the third axiom lets us compute \\(p(H) = p(H_1)+p(H_2)\\). But\nthis just pushes things back a step, since the axioms by themselves\nonly tell us that \\(p(H_1)\\) and \\(p(H_2)\\) must themselves lie\nbetween \\(0\\) and 1. \nThis weakness of the probability axioms generates the famous\nproblem of the priors, the problem of saying where initial\nprobabilities come from. Are they always based on evidence previously\ncollected? If so, how does scientific inquiry get started? If instead\nthey’re not based on previous evidence but are a\npriori, what principles govern this a priori reasoning?\nFormal epistemologists are split on this question. The so-called\nobjectivists see the probability axioms as incomplete,\nwaiting to be supplemented by additional postulates that determine the\nprobabilities with which inquiry should begin. (The Principle of\nIndifference (PoI) is the leading candidate here. See the entry on the\n interpretation of probability.)\n The so-called subjectivists think instead that there is no\nsingle, correct probability function \\(p\\) with which inquiry should\nbegin. Different inquirers may begin with different values for \\(p\\),\nand none of them is thereby more or less scientific or rational than\nthe others. \nIn later sections the problem of the priors will return several times,\nillustrating its importance and ubiquity. \nWe’ve seen that formalizing confirmation using probability\ntheory yields an account that succeeds in several significant ways: it\nvindicates several truisms about confirmation, it unifies those\ntruisms in a single equation, and it resolves a classic paradox (not\nto mention others we didn’t discuss (Crupi and Tentori\n2010)). \nWe also saw that it raises a problem though, the problem of priors,\nwhich formal epistemologists are divided on how to resolve. And there\nare other problems we didn’t explore, most notably the problems\nof\n logical omniscience\n and\n old evidence\n (see subsections of entry on\n Bayesian epistemology). \nThese and other problems have led to the exploration and development\nof other approaches to scientific reasoning, and reasoning in general.\nSome stick to the probabilistic framework but develop different\nmethodologies within it (Fisher 1925; Neyman and Pearson 1928a,b;\nRoyall 1997; Mayo 1996; Mayo and Spanos 2011; see entry on the\n philosophy of statistics).\n Others depart from standard probability theory, like\n Dempster-Shafer theory\n (Shafer 1976; see entry on\n formal representations of belief),\n a variant of probability theory meant to solve the problem of the\npriors and make other improvements.\n Ranking theory\n (Spohn 1988, 2012; again see entry on\n formal representations of belief)\n also bears some resemblance to probability theory but draws much\ninspiration from\n possible-world semantics for conditionals\n (see entry on\n indicative conditionals).\n Bootstrapping theory (Glymour 1980; Douven and Meijs 2006) leaves the\nprobabilistic framework behind entirely, drawing inspiration instead\nfrom the deduction-based approach we began with. Still other\napproaches develop\n non-monotonic logics (see entry),\n logics for making not only deductive inferences, but also defeasible,\ninductive inferences (Pollock 1995, 2008; Horty 2012).\n Formal learning theory\n provides a framework for studying the long-run consequences of a wide\nrange of methodologies. \nFor the next two sections we’ll build on the probabilistic\napproach introduced here, since it’s currently the most popular\nand influential approach to formal epistemology. But it’s\nimportant to remember that there is a rich and variegated range of\nalternative approaches, and that this one has its problems, some\nconsequences of which we’ll soon encounter. \nA lot of our reasoning seems to involve projecting observed patterns\nonto unobserved instances. For example, suppose I don’t know\nwhether the coin I’m holding is biased or fair. If I flip it 9\ntimes and it lands tails every time, I’ll expect the\n10th toss to come up tails too. What justifies this kind of\nreasoning? Hume famously argued that nothing can justify it. In modern\nform, Hume’s challenge is essentially this: a justification for\nsuch reasoning must appeal to either an inductive argument or a\ndeductive one. Appealing to an inductive argument would be\nunacceptably circular. While a deductive argument would have to show\nthat unobserved instances will resemble observed ones, which is not a\nnecessary truth, and hence not demonstrable by any valid argument. So\nno argument can justify projecting observed patterns onto unobserved\ncases. (Russell and Restall (2010) offer a formal development. Haack\n(1976) discusses the supposed asymmetry between induction and\ndeduction here.) \nCan probability come to the rescue here? What if instead of deducing\nthat unobserved instances will resemble observed ones we just deduce\nthat they’ll probably resemble the observed ones? If we\ncan deduce from the probability axioms that the next toss is likely to\ncome up tails given that it landed tails 9 out of 9 times so far, that\nwould seem to solve Hume’s problem. \nUnfortunately, no such deduction is possible: the probability axioms\nsimply don’t entail the conclusion we want. How can that be?\nConsider all the different sequences of heads (\\(\\mathsf{H}\\)) and\ntails (\\(\\mathsf{T}\\)) we might get in the course of 10 tosses: \nThere are 1024 possible sequences, so the probability of each possible\nsequence would seem to be \\(1/1024\\). Of course, only two of them\nbegin with 9 tails in a row, namely the last two. So, once we’ve\nnarrowed things down to a sequence that begins with 9 out of 9 tails,\nthe probability of tails on the 10th toss is \\(1/2\\), same\nas heads. More formally, applying the definition conditional\nprobability gives us: \\[\\begin{align} p(T_{10} \\mid T_{1\\ldots9})\n&= \\frac{p(T_{10} \\wedge T_{1\\ldots9})}{p(T_{1\\ldots9})}\\\\ &=\n\\frac{1/1024}{2/1024}\\\\ &= \\frac{1}{2}\\end{align}\\] \nSo it looks like the axioms of probability entail that the\nfirst 9 tosses tell us nothing about the 10th toss. \nIn fact, though, the axioms of probability don’t even entail\nthat—they don’t actually say anything about\n\\(p(T_{10} \\mid T_{1\\ldots9})\\). In the previous paragraph, we assumed\nthat each possible sequence of tosses was equally probable, with\n\\(p(\\ldots)=1/1024\\) the same for each sequence. But the probability\naxioms don’t require this “uniform” assignment. As\nwe saw earlier when we encountered the problem of the priors\n (1.4),\n the probability axioms only tell us that tautologies have probability\n1 (and contradictions probability \\(0\\)). Contingent propositions can\nhave any probability from \\(0\\) to 1, and this includes the\nproposition that the sequence of tosses will be\n\\(\\mathsf{HHHHHHHTHT}\\), or any other sequence of \\(\\mathsf{H}\\)s and\n\\(\\mathsf{T}\\)s. \nWe can exploit this freedom and get more sensible, induction-friendly\nresults if we assign prior probabilities using a different scheme\nadvocated by Carnap (1950). Suppose instead of assigning each possible\nsequence the same probability, we assign each possible number of\n\\(\\mathsf{T}\\)s the same probability. We could get anywhere from 0 to\n10 \\(\\mathsf{T}\\)s, so each possible number of \\(\\mathsf{T}\\)s has\nprobability 1/11. Now, there’s just one way of getting 0\n\\(\\mathsf{T}\\)s: \nSo \\(p(H_{1\\ldots10})=1/11\\). But there are 10 ways of getting 1\n\\(\\mathsf{T}\\): \\[\\begin{array}{c} \\mathsf{HHHHHHHHHT}\\\\\n\\mathsf{HHHHHHHHTH}\\\\ \\mathsf{HHHHHHHTHH}\\\\ \\vdots\\\\\n\\mathsf{THHHHHHHHH}\\end{array}\\] \nSo this possibility’s probability of \\(1/11\\) is divided 10\nways, yielding probability \\(1/110\\) for each subpossibility, e.g.,\n\\(p(\\mathsf{HHHHHHHTHH})=1/110\\). And then there are 45 ways of\ngetting 2 \\(\\mathsf{T}\\)s: \\[\\begin{array}{c} \\mathsf{HHHHHHHHTT}\\\\\n\\mathsf{HHHHHHHTHT}\\\\ \\mathsf{HHHHHHTHHT}\\\\ \\vdots\\\\\n\\mathsf{TTHHHHHHHH}\\end{array}\\] \nSo here the probability of \\(1/11\\) is divided \\(45\\) ways, yielding a\nprobability of \\(1/495\\) for each subpossibility, e.g.,\n\\(p(\\mathsf{HTHHHHHTHH})=1/495\\). And so on. \nWhat then becomes of \\(p(T_{10} \\mid T_{1\\ldots9})\\)? \\[\\begin{align} p(T_{10} \\mid T_{1\\ldots9})\n&= \\frac{p(T_{10} \\wedge T_{1\\ldots9})}{p(T_{1\\ldots9})}\\\\ &=\n\\frac{p(T_{1\\ldots10})}{p(T_{1\\ldots10} \\vee [T_{1\\ldots9} \\wedge\nH_{10}])}\\\\ &= \\frac{p(T_{1\\ldots10})}{p(T_{1\\ldots10}) +\np(T_{1\\ldots9} \\wedge H_{10})}\\\\ &= \\frac{1/11}{1/11 + 1/110}\\\\\n&= \\frac{10}{11}\\end{align}\\] \nSo we get a much more reasonable result when we assign prior\nprobabilities according to Carnap’s two-stage scheme. However,\nthis scheme is not mandated by the axioms of probability. \nOne thing this teaches us is that the probability axioms are silent on\nHume’s problem. Inductive reasoning is compatible with the\naxioms, since Carnap’s way of constructing the prior\nprobabilities makes a 10th \\(\\mathsf{T}\\) quite likely\ngiven an initial string of \\(9\\) \\(\\mathsf{T}\\)s. But the axioms are\nalso compatible with skepticism about induction. On the first way of\nconstructing the prior probabilities, a string of \\(\\mathsf{T}\\)s\nnever makes the next toss any more likely to be a \\(\\mathsf{T}\\), no\nmatter how long the string gets! In fact, there are further ways of\nconstructing the prior probabilities that yield\n“anti-induction”, where the more \\(\\mathsf{T}\\)s we\nobserve, the less likely the next toss is to be a\n\\(\\mathsf{T}\\). \nWe also learn something else though, something more constructive: that\nHume’s problem is a close cousin of the problem of the priors.\nIf we could justify Carnap’s way of assigning prior\nprobabilities, we would be well on our way to solving Hume’s\nproblem. (Why only on our way? More on that in a moment, but very\nbriefly: because we’d still have to justify using conditional\nprobabilities as our guide to the new, unconditional probabilities.)\nCan we justify Carnap’s two-stage scheme? This brings us to a\nclassic debate in formal epistemology. \nIf you had to bet on a horserace without knowing anything about any of\nthe horses, which one would you bet on? It probably wouldn’t\nmatter to you: each horse is as likely to win as the others, so\nyou’d be indifferent between the available wagers. If there are\n3 horses in the race, each has a 1/3 chance of winning; if there are\n5, each has a 1/5 chance; etc. This kind of reasoning is common and is\noften attributed to the Principle of\n Indifference:[5] \nThe Principle of Indifference (PoI)\n\nGiven \\(n\\) mutually exclusive and jointly exhaustive possibilities,\nnone of which is favored over the others by the available evidence,\nthe probability of each is \\(1/n\\). \nPoI looks quite plausible at first, and may even have the flavor of a\nconceptual truth. How could one possibility be more probable than\nanother if the evidence doesn’t favor it? And yet, the PoI faces\na classic and recalcitrant challenge. \nConsider the first horse listed in the race, Athena. There are two\npossibilities, that she will win and that she will lose. Our evidence\n(or lack thereof) favors neither possibility, so the PoI says the\nprobability that she’ll win is \\(1/2\\). But suppose there are\nthree horses in the race: Athena, Beatrice, and Cecil. Since our\nevidence favors none of them over any other, the PoI requires that we\nassign probability \\(1/3\\) to each, which contradicts our earlier\nconclusion that Athena’s probability of winning is \\(1/2\\). \nThe source of the trouble is that possibilities can be subdivided into\nfurther subpossibilities. The possibility of Athena losing can be\nsubdivided into two subpossibilities, one where Beatrice wins and\nanother where Cecil wins. Because we lack any relevant evidence, the\navailable evidence doesn’t seem to favor the coarser\npossibilities over the finer subpossibilities, leading to\ncontradictory probability assignments. What we need, it seems, is some\nway of choosing a single, privileged way of dividing up the space of\npossibilities so that we can apply the PoI consistently. \nIt’s natural to think we should use the more fine-grained\ndivision of possibilities, the three-way division in the case of\nAthena, Beatrice, and Cecil. But we can actually divide things\nfurther—infinitely further in fact. For example, Athena might\nwin by a full length, by half a length, by a quarter of a length, etc.\nSo the possibility that she wins is actually infinitely divisible. We\ncan extend the PoI to handle such infinite divisions of possibilities\nin a natural way by saying that, if Athena wins, the probability that\nshe’ll win by between 1 and 2 lengths is twice the probability\nthat she’ll win by between \\(1/2\\) and 1 length. But the same\nproblem we were trying to solve still persists, in the form of the\nnotorious Bertrand paradox (Bertrand 2007 [1888]). \nThe paradox is nicely illustrated by the following example from van\nFraassen (1989). Suppose a factory cuts iron cubes with edge-lengths\nranging from \\(0\\) cm to 2 cm. What is the probability that the next\ncube to come off the line will have edges between \\(0\\) cm and 1 cm in\nlength? Without further information about how the factory goes about\nproducing cubes, the PoI would seem to say the probability is \\(1/2\\).\nThe range from \\(0\\) to 1 covers \\(1/2\\) the full range of\npossibilities from \\(0\\) to 2. But now consider this question: what is\nthe probability that the next cube to come off the line will have\nvolume between \\(0\\) cubic cm and 1 cubic cm? Here the PoI seems to\nsay the probability is \\(1/8\\). For the range from \\(0\\) to 1 covers\nonly \\(1/8\\) the full range of possible volumes from \\(0\\) to \\(8\\)\ncubic cm. So we have two different probabilities for equivalent\npropositions: a cube has edge-length between \\(0\\) and 1 cm if and\nonly if it has a volume between \\(0\\) cubic cm and 1 cubic cm. Once\nagain, the probabilities given by the PoI seem to depend on how we\ndescribe the range of possible outcomes. Described in terms of length,\nwe get one answer; described in terms of volume, we get another. \nImportantly, Bertrand’s paradox applies quite generally. Whether\nwe’re interested in the size of a cube, the distance by which a\nhorse will win, or any other parameter measured in real numbers, we\ncan always redescribe the space of possible outcomes so that the\nprobabilities assigned by the PoI come out differently. Even an\ninfinitely fine division of the space of possibilities doesn’t\nfix the problem: the probabilities assigned by the PoI still depend on\nhow we describe the space of possibilities. \nWe face essentially this problem when we frame the problem of\ninduction in probabilistic terms. Earlier we saw two competing ways of\nassigning prior probabilities to sequences of coin tosses. One way\ndivides the possible outcomes according to the exact sequence in which\n\\(\\mathsf{H}\\) and \\(\\mathsf{T}\\) occur. The PoI assigns each possible\nsequence a probability of \\(1/1024\\), with the result that the first 9\ntosses tell us nothing about the 10th toss. The second,\nCarnapian way instead divides the possible outcomes according to the\nnumber of \\(\\mathsf{T}\\)s, regardless of where they occur in the\nsequence. The PoI then assigns each possible number of \\(\\mathsf{T}\\)s\nthe same probability, \\(1/11\\). The result then is that the first 9\ntosses tell us a lot about the 10th toss: if the first 9\ntosses are tails, the 10th toss has a \\(10/11\\) chance of\ncoming up tails too. \nSo one way of applying the PoI leads to inductive skepticism, the\nother yields the inductive optimism that seems so indispensable to\nscience and daily life. If we could clarify how the PoI should be\napplied, and justify its use, we would have our answer to Hume’s\nproblem (or at least the first half—we still have to address the\nissue of using conditional probabilities as a guide to new,\nunconditional probabilities). Can it be clarified and justified? \nHere again we run up against one of the deepest and oldest divides in\nformal epistemology, that between subjectivists and objectivists. The\nsubjectivists hold that any assignment of probabilities is a\nlegitimate, reasonable way to start one’s inquiry. One need only\nconform to the three probability axioms to be reasonable. They take\nthis view largely because they despair of clarifying the PoI. They see\nno reason, for example, that we should follow Carnap in first dividing\naccording to the number of \\(\\mathsf{T}\\)s, and only then subdividing\naccording to where in sequence those \\(\\mathsf{T}\\)s appear. Closely\nrelated to this skepticism is a skepticism about the prospects for\njustifying the PoI, even once clarified, in a way that would put it on\na par with the three axioms of probability. We haven’t yet\ntouched on how the three axioms are supposed to be justified. But the\nclassic story is this: a family of\n theorems—Dutch book theorems (see entry)\n and\n representation theorems (see entry)—are\n taken to show that any deviation from the three axioms of probability\nleads to irrational decision-making. For example, if you deviate from\nthe axioms, you will accept a set of bets that is bound to lose money,\neven though you can see that losing money is inevitable a\npriori. These theorems don’t extend to violations of the\nPoI though, however it’s clarified. So subjectivists conclude\nthat violating the PoI is not irrational. \nSubjectivists aren’t thereby entirely helpless in the face of\nthe problem of induction, though. According to them, any initial\nassignment of probabilities is reasonable, including Carnap’s.\nSo if you do happen to start out with a Carnap-esque assignment, you\nwill be an inductive optimist, and reasonably so. It’s just that\nyou don’t have to start out that way. You could instead start\nout treating each possible sequence of \\(\\mathsf{H}\\)s and\n\\(\\mathsf{T}\\)s as equally probable, in which case you’ll end up\nan inductive skeptic. That’s reasonable too. According to\nsubjectivism, induction is perfectly rational, it just isn’t the\nonly rational way to reason. \nObjectivists hold instead that there’s just one way to assign\ninitial probabilities (though some allow a bit of flexibility (Maher\n1996)). These initial probabilities are given by the PoI, according to\northodox objectivism. As for the PoI’s conflicting probability\nassignments depending on how possibilities are divided up, some\nobjectivists propose restricting it to avoid these inconsistencies\n(Castell 1998). Others argue that it’s actually appropriate for\nprobability assignments to depend on the way possibilities are divvied\nup, since this reflects the language in which we conceive the\nsituation, and our language reflects knowledge we bring to the matter\n(Williamson 2007). Still others argue that the PoI’s assignments\ndon’t actually depend on the way possibilities are divided\nup—it’s just hard to tell sometimes when the evidence\nfavors one possibility over another (White 2009). \nWhat about justifying the PoI though? Subjectivists have traditionally\njustified the three axioms of probability by appeal to one of the\naforementioned theorems: the Dutch book theorem or some form of\nrepresentation theorem. But as we noted earlier, these theorems\ndon’t extend to the PoI. \nRecently, a different sort of justification has been gaining favor,\none that may extend to the PoI. Arguments that rely on Dutch book or\nrepresentation theorems have long been suspect because of their\npragmatic character. They aim to show that deviating from the\nprobability axioms leads to irrational choices, which seems to show at\nbest that obeying the probability axioms is part of pragmatic\nrationality, as opposed to epistemic irrationality. (But see\nChristensen (1996, 2001) and Vineberg (1997, 2001) for replies.)\nPreferring a more properly epistemic approach, Joyce (1998, 2009)\nargues that deviating from the probability axioms takes one\nunnecessarily far from the truth, no matter what the truth turns out\nto be. Pettigrew (2016) adapts this approach to the PoI, showing that\nviolations of the PoI increase one’s risk of being further from\nthe truth. (But see Carr (2017) for a critical perspective on this\ngeneral approach.) \nWhether we prefer the subjectivist’s response to Hume’s\nproblem or the objectivist’s, a crucial element is still\nmissing. Earlier we noted that justifying a Carnapian assignment of\nprior probabilities only gets us half way to a solution. We still have\nto turn these prior probabilities into posterior\nprobabilities: initially, the probability of tails on the tenth toss\nwas \\(1/2\\), but after observing the first 9 tosses come out tails,\nit’s supposed to be \\(10/11\\). Having justified our initial\nassignment of probabilities—whether the subjectivist way or the\nobjectivist way—we can prove that \\(p(T_{10}\\mid\nT_{1\\ldots9})=10/11\\) compared to \\(p(T_{10})=1/2\\). But that\ndoesn’t mean the new probability of \\(T_{10}\\) is\n\\(10/11\\). Remember, the symbolism \\(p(T_{10}\\mid T_{1\\ldots9})\\)\nis just shorthand for the fraction \\(p(T_{10} \\wedge\nT_{1\\ldots9})/p(T_{1\\ldots9})\\). So the fact that \\(p(T_{10}\\mid\nT_{1\\ldots9})=10/11\\) just means that this ratio is \\(10/11\\), which\nis still just a fact about the initial, prior\nprobabilities. \nTo appreciate the problem, it helps to forget probabilities for a\nmoment and think in simple, folksy terms. Suppose you aren’t\nsure whether \\(A\\) is true, but you believe that if it is true, then\nso is \\(B\\). If you then learn that \\(A\\) is in fact true, you then\nhave two options. You might conclude that \\(B\\) is true, but you might\ninstead decide that you were wrong at the outset to think \\(B\\) is\ntrue if \\(A\\) is. Faced with the prospect of accepting \\(B\\), you\nmight find it too implausible to accept, and thus abandon your\ninitial, conditional belief that \\(B\\) is true if \\(A\\) is (Harman\n1986). \nLikewise, we might start out unsure whether the first \\(9\\) tosses\nwill come up tails, but believe that if they do, then the probability\nof the \\(10\\)th toss coming up tails is \\(10/11\\). Then, when we see\nthe first \\(9\\) tosses come up tails, we might conclude that the\n\\(10\\)th toss has a \\(10/11\\) chance of coming up tails, or,\nwe might instead decide we were wrong at the outset to think it had a\n\\(10/11\\) chance of coming up tails on the \\(10\\)th toss if it came up\ntails on the first \\(9\\) tosses. \nThe task is to justify taking the first route rather than the second:\nsticking to our conditional belief that, if \\(T_{1\\ldots9}\\), then\n\\(T_{10}\\) has probability \\(10/11\\), even once we’ve learned\nthat indeed \\(T_{1\\ldots9}\\). Standing by one’s conditional\nprobabilities in this way is known as “conditionalizing”,\nbecause one thereby turns the old conditional probabilities into new,\nunconditional probabilities. To see why sticking by your old\nconditional probabilities amounts to turning them into unconditional\nprobabilities, let’s keep using \\(p\\) to represent the prior\nprobabilities, and let’s introduce \\(p'\\) to stand for the new,\nposterior probabilities after we learn that \\(T_{1\\ldots9}\\). If we\nstand by our prior conditional probabilities, then \n\n\\[p'(T_{10}\\mid T_{1\\ldots9}) = p(T_{10}\\mid T_{1\\ldots9})=10/11.\\]\n\n And\nsince we now know that \\(T_{1\\ldots9}\\), \\(p'(T_{1\\ldots9})=1\\). It\nthen follows that \\(p'(T_{10})=10/11\\): \\[\\begin{align} p'(T_{10}\\mid T_{1\\ldots9})\n&= \\frac{p'(T_{10} \\wedge T_{1\\ldots9})}{p'(T_{1\\ldots9})}\\\\\n&= p'(T_{10} \\wedge T_{1\\ldots9})\\\\ &= p'(T_{10})\\\\ &=\n10/11\\end{align}\\] \nThe first line follows from the definition of conditional probability.\nThe second follows from the fact that \\(p'(T_{1\\ldots9})=1\\), since\nwe’ve seen how the first \\(9\\) tosses go. The third line follows\nfrom an elementary theorem of the probability axioms: conjoining \\(A\\)\nwith another proposition \\(B\\) that has probability 1 results in the\nsame probability, i.e., \\(p(A \\wedge B)=p(A)\\) when \\(p(B)=1\\).\n(Deriving this theorem is left as an exercise for the reader.)\nFinally, the last line just follows from our assumption that\n\n\\[p'(T_{10}\\mid T_{1\\ldots9}) = p(T_{10}\\mid T_{1\\ldots9})=10/11.\\]\n\n The thesis that we should generally update probabilities\nin this fashion is known as conditionalization.  \nConditionalization\n\nGiven the prior probability assignment \\(p(H\\mid E)\\), the new,\nunconditional probability assignment to \\(H\\) upon learning \\(E\\)\nshould be \\(p'(H)=p(H\\mid E)\\). \nA number of arguments have been given for this principle, many of them\nparallel to the previously mentioned arguments for the axioms of\nprobability. Some appeal to Dutch books (Teller 1973; Lewis 1999),\nothers to the pursuit of cognitive values (Greaves and Wallace 2006),\nespecially closeness to the truth (Leitgeb and Pettigrew 2010a,b), and\nstill others to the idea that one should generally revise one’s\nbeliefs as little as possible when accommodating new information\n(Williams 1980). \nThe details of these arguments can get very technical, so we\nwon’t examine them here. The important thing for the moment is\nto appreciate that (i) inductive inference is a dynamic process, since\nit involves changing our beliefs over time, but (ii) the general\nprobability axioms, and particular assignments of prior probabilities\nlike Carnap’s, are static, concerning only the initial\nprobabilities. Thus (iii) a full theory of inference that answers\nHume’s challenge must appeal to additional, dynamic principles\nlike Conditionalization. So (iv) we need to justify these additional,\ndynamic principles in order to justify a proper theory of inference\nand answer Hume’s challenge. \nImportantly, the morals summarized in (i)–(iv) are extremely\ngeneral. They don’t just apply to formal epistemologies based in\nprobability theory. They also apply to a wide range of theories based\nin other formalisms, like\n Dempster-Shafer theory,\n ranking theory,\n belief-revision theory,\n and\n non-monotonic logics.\n One way of viewing the takeaway here, then, is as follows. \nFormal epistemology gives us precise ways of stating how induction\nworks. But these precise formulations do not themselves solve a\nproblem like Hume’s, for they rely on assumptions like the\nprobability axioms, Carnap’s assignment of prior probabilities,\nand Conditionalization. Still, they do help us isolate and clarify\nthese assumptions, and then formulate various arguments in their\ndefense. Whether formal epistemology thereby aids in the solution of\nHume’s problem depends on whether these formulations and\njustifications are plausible, which is controversial. \nThe problem of induction challenges our inferences from the observed\nto the unobserved. The regress problem challenges our knowledge at an\neven more fundamental level, questioning our ability to know anything\nby observation in the first place (see Weintraub 1995 for a critical\nanalysis of this distinction). \nTo know something, it seems you must have some justification for\nbelieving it. For example, your knowledge that Socrates taught Plato\nis based on testimony and textual sources handed down through the\nyears. But how do you know these testimonies and texts are reliable\nsources? Presumably this knowledge is itself based on some further\njustification—various experiences with these sources, their\nagreement with each other, with other things you’ve observed\nindependently, and so on. But the basis of this knowledge too can be\nchallenged. How do you know that these sources even say what you think\nthey say, or that they even exist—maybe every experience\nyou’ve had reading The Apology has been a mirage or a\ndelusion. \nThe famous Agrippan trilemma identifies three possible ways this\nregress of justification might ultimately unfold. First, it could go\non forever, with \\(A\\) justified by \\(B\\) justified by \\(C\\) justified\nby …, ad infinitum. Second, it it could cycle back on\nitself at some point, with \\(A\\) justified by \\(B\\) justified by \\(C\\)\njustified by…justified by \\(B\\), for example. Third and\nfinally, the regress might stop at some point, with \\(A\\) justified by\n\\(B\\) justified by \\(C\\) justified by…justified by \\(N\\), which\nis not justified by any further belief. \nThese three possibilities correspond to three classic responses to\nthis regress of justification. Infinitists hold that the regress goes\non forever, coherentists that it cycles back on itself, and\nfoundationalists that it ultimately terminates. The proponents of each\nview reject the alternatives as unacceptable. Infinitism looks\npsychologically unrealistic, requiring an infinite tree of beliefs\nthat finite minds like ours could not accommodate. Coherentism seems\nto make justification unacceptably circular, and thus too easy to\nachieve. And foundationalism seems to make justification arbitrary,\nsince the beliefs at the end of the regress apparently have no\njustification. \nThe proponents of each view have long striven to answer the concerns\nabout their own view, and to show that the concerns about the\nalternatives cannot be adequately answered. Recently, methods from\nformal epistemology have begun to be recruited to examine the adequacy\nof these answers. We’ll look at some work that’s been done\non coherentism and foundationalism, since these have been the focus of\nboth informal and formal work. (For work on infinitism, see Turri and\nKlein 2014. See Haack (1993) for a hybrid option,\n“foundherentism”.) \nThe immediate concern about coherentism is that it makes justification\ncircular. How can a belief be justified by other beliefs which are,\nultimately, justified by the first belief in question? If cycles of\njustification are allowed, what’s to stop one from believing\nanything one likes, and appealing to it as a justification for\nitself? \nCoherentists usually respond that justification doesn’t actually\ngo in cycles. In fact, it isn’t even really a relationship\nbetween individual beliefs. Rather, a belief is justified by being\npart of a larger body of beliefs that fit together well, that\ncohere. Justification is thus global, or\nholistic. It is a feature of an entire body of beliefs first,\nand only of individual beliefs second, in virtue of their being part\nof the coherent whole. When we trace the justification for a belief\nback and back and back until we come full circle, we aren’t\nexposing the path by which it’s justified. Rather, we are\nexposing the various interconnections that make the whole web\njustified as a unit. That these connections can be traced in a circle\nmerely exposes how interconnected the web is, being connected in both\ndirections, from \\(A\\) to \\(B\\) to …to \\(N\\), and then from\n\\(N\\) all the way back to \\(A\\) again. \nStill, arbitrariness remains a worry: you can still believe just about\nanything, provided you also believe many other things that fit well\nwith it. If I want to believe in ghosts, can I just adopt a larger\nworld view on which supernatural and paranormal phenomena are rife?\nThis worry leads to a further one, a worry about truth: given that\nalmost any belief can be embedded in a larger, just-so story that\nmakes sense of it, why expect a coherent body of beliefs to be true?\nThere are many coherent stories one can tell, the vast majority of\nwhich will be massively false. If coherence is no indication of truth,\nhow can it provide justification? \nThis is where formal methods come in: what does probability theory\ntell us about the connection between coherence and truth? Are more\ncoherent bodies of belief more likely to be true? Less likely? \nKlein and Warfield (1994) argue that coherence often\ndecreases probability. Why? Increases in coherence often come\nfrom new beliefs that make sense of our existing beliefs. A detective\ninvestigating a crime may be puzzled by conflicting testimony until\nshe learns that the suspect has an identical twin, which explains why\nsome witnesses report seeing the suspect in another city the day of\nthe crime. And yet, adding the fact about the identical twin to her\nbody of beliefs actually decreases its probability. This follows from\na theorem of the probability axioms we noted earlier\n (§1.2),\n Conjunction Costs Probability, which says that conjoining \\(A\\) with\n\\(B\\) generally yields a lower probability than for \\(A\\) alone\n(unless \\(p(A \\wedge \\neg B)=0\\)). Intuitively, the more things you\nbelieve the more risks you take with the truth. But making sense of\nthings often requires believing more. \nMerricks (1995) replies that it’s only the probability of the\nentire belief corpus that goes down when beliefs are added. But the\nindividual probabilities of the beliefs it contains are what’s\nat issue. And from the detective’s point of view, her individual\nbeliefs do become more probable when made sense of by the additional\ninformation that the suspect has an identical twin. Shogenji (1999)\ndiffers: coherence of the whole cannot influence probability of the\nparts. Coherence is for the parts to stand or fall together, so just\nas coherence makes all the members more likely to be true together, it\nmakes it more likely that they are all false (at the expense of the\npossibility that some will turn out true and others false). \nInstead, Shogenji prefers to answer Klein & Warfield at the\ncollective level, the level of the whole belief corpus. He argues that\nthe corpora Klein & Warfield compare differ in probability because\nthey are of different strengths. The more beliefs a corpus\ncontains, or the more specific its beliefs are, the stronger it is. In\nthe case of the detective, adding the information about the twin\nincreases the strength of her beliefs. And, in general, increasing\nstrength decreases probability, since as we’ve seen, \\(p(A\n\\wedge B) \\leq p(A)\\). Thus the increase in the coherence of the\ndetective’s beliefs is accompanied by an increase in strength.\nThe net effect, argues Shogenji, is negative: the probability of the\ncorpus goes down because the increase in strength outweighs the\nincrease in coherence. \nTo vindicate this diagnosis, Shogenji appeals to a formula for\nmeasuring the coherence of a belief-set in probabilistic terms, which\nwe’ll label coh: \\[ \\textit{coh}(A_1,\\ldots,A_n) = \\frac{p(A_1\n\\wedge \\ldots \\wedge A_n)}{p(A_1) \\times \\ldots \\times\np(A_n)}\\] \nTo see the rationale behind this formula, consider the simple case of\njust two beliefs: \\[\\begin{align} \\textit{coh}(A,B) &=\n\\frac{p(A \\wedge B)}{p(A) \\times p(B)}\\\\ &= \\frac{p(A \\mid\nB)}{p(A)}\\end{align}\\] \nWhen \\(B\\) has no bearing on \\(A\\), \\(p(A\\mid B)=p(A)\\), and this\nratio just comes out 1, which is our neutral point. If instead \\(B\\)\nraises the probability of \\(A\\), this ratio comes out larger than 1;\nand if \\(B\\) lowers the probability of \\(A\\), it comes out smaller\nthan 1. So \\(\\textit{coh}(A,B)\\) measures the extent to which \\(A\\)\nand \\(B\\) are related. Shogenji’s formula\n\\(\\textit{coh}(A_1,\\ldots,A_n)\\) generalizes this idea for larger\ncollections of propositions. \nHow does measuring coherence this way vindicate Shogenji’s reply\nto Klein & Warfield, that the increase in the detective’s\ncoherence is outweighed by an increase in the strength of her beliefs?\nThe denominator in the formula for \\(\\textit{coh}\\) tracks strength:\nthe more propositions there are, and the more specific they are, the\nsmaller this denominator will be. So if we compare two belief-sets\nwith the same strength, their denominators will be the same. Thus, if\none is more coherent than the other, it must be because its numerator\nis greater. Thus coherence increases with overall probability,\nprovided strength is held constant. Since in the detective’s\ncase overall probability does not increase despite the increase in\ncoherence, it must be because the strength of her commitments had an\neven stronger influence. \nShogenji’s measure of coherence is criticized by other authors,\nmany of whom offer their own, preferred measures (Akiba 2000; Olsson\n2002, 2005; Glass 2002; Bovens & Hartmann 2003; Fitelson 2003;\nDouven and Meijs 2007). Which measure is correct, if any, remains\ncontroversial, as does the fate of Klein & Warfield’s\nargument against coherentism. Another line of probabilistic attack on\ncoherentism, which we won’t explore here, comes from Huemer\n(1997) and is endorsed by Olsson (2005). Huemer (2011) later retracts\nthe argument though, on the grounds that it foists unnecessary\ncommitments on the coherentist. More details are available in\n the entry on coherentism. \nFoundationalists hold that some beliefs are justified without being\njustified by other beliefs. Which beliefs have this special,\nfoundational status? Foundationalists usually identify either beliefs\nabout perceived or remembered matters, like “there’s a\ndoor in front of me” or “I had eggs yesterday”, or\nelse beliefs about how things seem to us, like “there appears to\nbe a door in front of me” or “I seem to remember having\neggs yesterday”. Either way, the challenge is to say how these\nbeliefs can be justified if they are not justified by any other\nbeliefs. \nOne view is that these beliefs are justified by our perceptual and\nmemorial states. When it looks like there’s a door in front of\nme, this perceptual state justifies me in believing that there is a\ndoor there, provided I have no reason to distrust this appearance. Or,\nat least, I am justified in believing that there appears to\nbe a door there. So foundational beliefs are not arbitrary, they are\njustified by closely related perceptual and memorial states. Still,\nthe regress ends there, because it makes no sense to ask what\njustifies a state of perception or memory. These states are outside\nthe domain of epistemic normativity. \nA classic criticism of foundationalism now arises, a version of the\ninfamous\n Sellarsian dilemma.\n Must you know that your (say) vision is reliable to be justified in\nbelieving that there’s a door in front of you on the basis of\nits looking that way? If so, we face the first horn of the dilemma:\nthe regress of justification is revived. For what justifies your\nbelief that your vision is reliable? Appealing to previous cases where\nyour vision proved reliable just pushes things back a step, since the\nsame problem now arises for the reliability of your memory. Could we\nsay instead that the appearance of a door is enough by itself to\njustify your belief in the door? Then we face the second horn: such a\nbelief would seem to be arbitrary, formed on the basis of a source you\nhave no reason to trust, namely your vision (Sellars 1956; Bonjour\n1985; Cohen 2002). \nThis second horn is sharpened by White (2006), who formalizes it in\nprobabilistic terms. Let \\(A(D)\\) be the proposition that there\nappears to be a door before you, and \\(D\\) the proposition that there\nreally is a door there. The conjunction \\(A(D) \\wedge \\neg D\\)\nrepresents the possibility that appearances are misleading in this\ncase. It says there appears to be a door but isn’t really. Using\nthe probability axioms, we can prove that \\(p(D\\mid A(D)) \\leq p(\\neg\n(A(D) \\wedge \\neg D))\\) (see\n technical supplement §3).\n In other words, the probability that there really is a door given\nthat there appears to be one cannot exceed the initial probability\nthat appearances are not misleading in this case. So it seems that any\njustification \\(A(D)\\) lends to belief in \\(D\\) must be preceded by\nsome justification for believing that appearances are not misleading,\ni.e., \\(\\neg (A(D) \\wedge \\neg D)\\). Apparently then, you must know\n(or have reason to believe) your sources are reliable before you can\ntrust them. (Pryor 2013 elucidates some tacit assumptions in this\nargument.) \nLying in wait at the other horn of the Sellarsian dilemma is the\nPrinciple of Indifference (PoI). What is the initial probability that\nthe appearance as of a door is misleading, according to the PoI? On\none way of thinking about it, your vision can be anywhere from 100%\nreliable to 0% reliable. That is, the way things appear to us might be\naccurate all the time, none of the time, or anywhere in between. If we\nregard every degree of reliability from 0% to 100% as equally\nprobable, the effect is the same as if we just assumed experience to\nbe 50% reliable. The PoI will then assign \\(p(D\\mid A(D))=1/2\\). This\nresult effectively embraces skepticism, since we remain agnostic about\nthe presence of the door despite appearances. \nWe saw earlier\n (§2.1)\n that the PoI assigns different probabilities depending on how we\ndivide up the space of possibilities. What if we divide things up this\nway instead: \nOnce again, we get the skeptical, agnostic result that \\(p(D\\mid\nA(D))=1/2\\). Other ways of dividing up the space of possibilities will\nsurely deliver better, anti-skeptical results. But then some argument\nfor preferring those ways of dividing things up will be wanted,\nlaunching the regress of justification all over again. \nSubjectivists, who reject the PoI and allow any assignment of initial\nprobabilities as long as it obeys the probability axioms, may respond\nthat it’s perfectly permissible to assign a high initial\nprobability to the hypothesis that our senses are (say) 95% reliable.\nBut they must also admit that it is permissible to assign a high\ninitial probability to the hypothesis that our senses are 0% reliable,\ni.e., wrong all the time. Subjectivists can say that belief in the\nexternal world is justified, but they must allow that skepticism is\njustified too. Some foundationalists may be able to live with this\nresult, but many seek to understand how experience justifies external\nworld beliefs in a stronger sense—in a way that can be used to\ncombat skeptics, rather than merely agreeing to disagree with\nthem. \nSo far we’ve used just one formal tool, probability theory. We\ncan get many similar results in the above applications using other\ntools, like\n Dempster-Shafer theory\n or\n ranking theory.\n But let’s move to a new application, and a new tool.\nLet’s use modal logic to explore the limits of knowledge. \nThe language of modal logic is the same as ordinary, classical logic,\nbut with an additional sentential operator, \\(\\Box\\), thrown in to\nrepresent necessity. If a sentence \\(\\phi\\) isn’t just true, but\nnecessarily true, we write \\(\\Box \\phi\\). \nThere are many kinds of necessity, though. Some things are logically\nnecessary, like tautologies. Others may not be logically necessary,\nbut still metaphysically necessary. (That Hesperus and Phosphorus are\nidentical is a popular example; more controversial candidates are\nGod’s existence or facts about parental origin, e.g., the fact\nthat Ada Lovelace’s father was Lord Byron.) \nBut the kind of necessity that concerns us here is epistemic\nnecessity, the necessity of things that must be true given what\nwe know. For example, it is epistemically necessary for you that the\nauthor of this sentence is human. If you didn’t know that\nalready (maybe you hadn’t considered the question), it had to be\ntrue given other things you did know: that humans are the only beings\non Earth capable of constructing coherent surveys of formal\nepistemology, and that this is such a survey (I hope). \nIn epistemic modal logic then, it makes sense to write \\(K \\phi\\)\ninstead of \\(\\Box \\phi\\), where \\(K \\phi\\) means that \\(\\phi\\) is\nknown to be true, or at least follows from what is known to be true.\nKnown by whom? That depends on the application. Let’s assume we\nare talking about your knowledge unless specified otherwise. \nWhat axioms should epistemic modal logic include? Well, any tautology\nof propositional logic should be a theorem, like \\(\\phi \\supset\n\\phi\\). For that matter, formulas with the \\(K\\) operator that are\nsimilarly truth-table valid, like \\(K \\phi \\supset K \\phi\\), should be\ntheorems too. So we’ll just go ahead and make all these formulas\ntheorems in the crudest way possible, by making them all axioms: \nAdopting P immediately makes our list of axioms\ninfinite. But they’re all easily identified by the truth-table\nmethod, so we won’t worry about it. \nMoving beyond classical logic, all so-called “normal”\nmodal logics share an axiom that looks pretty sensible for epistemic\napplications: \nIf you know that \\(\\phi \\supset \\psi\\) is true, then if you also know\n\\(\\phi\\), you also know \\(\\psi\\). Or at least, \\(\\psi\\) follows from\nwhat you know if \\(\\phi \\supset \\psi\\) and \\(\\phi\\) do. (The\n‘K’ here stands for ‘Kripke’\nby the way, not for ‘knowledge’.) Another common axiom\nshared by all “alethic” modal logics also looks good: \nIf you know \\(\\phi\\), it must be true. (Note: K and\nT are actually axiom schemas, since any sentence of\nthese forms is an axiom. So each of these schemas actually adds\ninfinitely many axioms, all of the same general form.) \nTo these axioms we’ll add two inference rules. The first,\nfamiliar from classical logic, states that from \\(\\phi \\supset \\psi\\)\nand \\(\\phi\\), one may derive \\(\\psi\\). Formally: \nThe second rule is specific to modal logic and states that from\n\\(\\phi\\) one can infer \\(K \\phi\\). Formally: \nThe NEC rule looks immediately suspect: doesn’t\nit make everything true known? Actually, no: our logic only admits\naxioms and things that follow from them by MP. So\nonly logical truths will be subject to the NEC rule,\nand these are epistemically necessary: they’re either known, or\nthey follow from what we know, because they follow given no\nassumptions at all. (NEC stands for\n‘necessary’, epistemically necessary in the present\nsystem.) \nThe three axiom schemas P, K, and\nT, together with the derivation rules\nMP and NEC, complete our minimal\nepistemic modal logic. They allow us to derive some basic theorems,\none of which we’ll use in the next section: \nTheorem (\\(\\bwedge\\)-distribution). \\(K(\\phi \\wedge\n\\psi) \\supset (K \\phi \\wedge K \\psi)\\)  \n(See the\n technical supplement\n for a proof). This theorem says roughly that if you know a\nconjunction, then you know each conjunct. At least, each conjunct\nfollows from what you know (I’ll be leaving this qualifier\nimplicit from now on), which seems pretty sensible. \nCan we prove anything more interesting? With some tweaks here and\nthere, we can derive some quite striking results about the limits of\nour knowledge. \nCan everything that is true be known? Or are there some truths that\ncould never be known, even in principle? A famous argument popularized\nby Fitch (1963) and originally due to Alonzo Church (Salerno 2009)\nsuggests not: some truths are unknowable. For if all truths were\nknowable in principle, we could derive that all truths are actually\nknown already, which would be absurd. \nThe argument requires a slight extension of our epistemic logic, to\naccommodate the notion of knowability. For us, \\(K\\) means known (or\nentailed by the known), whereas knowability adds an extra modal layer:\nwhat it’s possible to know. So we’ll need a\nsentential operator \\(\\Diamond\\) in our language to represent\nmetaphysical possibility. Thus \\(\\Diamond \\phi\\) means\n“it’s metaphysically possible for \\(\\phi\\) to be\ntrue”. In fact, \\(\\Diamond \\phi\\) is just short for \\(\\neg \\Box\n\\neg \\phi\\), since what doesn’t have to be false can be true. So\nwe can actually add the \\(\\Box\\) instead and assume that, like the\n\\(K\\) operator, it obeys the NEC rule. (As with the\nNEC rule for the \\(K\\) operator, it’s okay that\nwe can always derive \\(\\Box \\phi\\) from \\(\\phi\\), because we can only\nderive \\(\\phi\\) in the first place when \\(\\phi\\) is a logical truth.)\n\\(\\Diamond\\) is then just \\(\\neg \\Box \\neg\\) by definition. \nWith this addition to our language in place, we can derive the\nfollowing lemma (see the\n technical supplement\n for the derivation): \nLemma (Unknowns are Unknowable). \\( \\neg \\Diamond\nK(\\phi \\wedge \\neg K \\phi)\\) \nThis lemma basically says you can’t know a fact of the sort,\n“\\(\\phi\\) is true but I don’t know it’s true”,\nwhich seems pretty sensible. If you knew such a conjunction, the\nsecond conjunct would have to be true, which conflicts with your\nknowing the first conjunct. (This is where\n\\(\\bwedge\\)-distribution proves useful.) \nYet this plausible looking lemma leads almost immediately to the\nunknowability of some truths. Suppose for reductio that\neverything true could be known, at least in principle. That is,\nsuppose we took as an axiom: \nKnowledge Without Limits\n\n\\(\\phi \\supset \\Diamond K \\phi\\)  \nWe would then be able to derive in just a few lines that everything\ntrue is actually known, i.e., \\(\\phi \\supset K \\phi\\). \nIf \\(K\\) represents what God knows, this would be fine. But if \\(K\\)\nrepresents what you or I know, it seems absurd! Not only are there\ntruths we don’t know, most truths don’t even follow from\nwhat we know. Knowledge Without Limits appears to be\nthe culprit here, so it seems there are some things we could not know,\neven in principle. But see\n the entry on Fitch’s paradox of knowability\n for more discussion. \nEven if we can’t know some things, might we at least have\nunlimited access to our own knowledge? Are we at least always able to\ndiscern whether we know something? A popular axiom in the logic of\nmetaphysical necessity is the so-called S4 axiom: \\(\\Box \\phi \\supset\n\\Box \\Box \\phi\\). This says that whatever is necessary had to\nbe necessary. In epistemic logic, the corresponding formula is: \nThis says roughly that whenever we know something, we know that we\nknow it. Hintikka (1962) famously advocates including\nKK as an axiom of epistemic logic. But an influential\nargument due to Williamson (2000) suggests otherwise. \nThe argument hinges on the idea that knowledge can’t be had by\nluck. Specifically, to know something, it must be that you\ncouldn’t have been wrong very easily. Otherwise, though you\nmight be right, it’s only by luck. For example, you might\ncorrectly guess that there are exactly 967 jellybeans in the jar on my\ndesk, but even though you’re right, you just got lucky. You\ndidn’t know there were 967 jellybeans, because there could\neasily have been 968 jellybeans without you noticing the\ndifference. \nTo formalize this “no-luck” idea, let the propositions\n\\(\\phi_1, \\phi_2\\), etc. say that the number of jellybeans is at least\n1, at least 2, etc. We’ll assume you’re eyeballing the\nnumber of jellybeans in the jar, not counting them carefully. Because\nyou’re an imperfect estimator of large quantities of jellybeans,\nyou can’t know that there are at least 967 jellybeans in the\njar. If you think there are at least 967 jellybeans, you could easily\nmake the mistake of thinking there are at least 968, in which case\nyou’d be wrong. So we can formalize the “not easily\nwrong” idea in this scenario as follows: \nSafety\n\n\\(K \\phi_i \\supset \\phi_{i+1}\\) when \\(i\\) is large (at least \\(100\\)\nlet’s say).  \nThe idea is that knowledge requires a margin for error, a margin of at\nleast one jellybean in our example. Presumably more than one\njellybean, but at least one. Within one jellybean of the true number,\nyou can’t discern truth from falsehood. (See Nozick (1981) for a\ndifferent conception of a “no luck” requirement on\nknowledge, which Roush (2005; 2009) formalizes in probabilistic\nterms.) \nHaving explained all this to you though, here’s something else\nyou now know: that the Safety thesis is true. So we\nalso have: \nKnowledge of Safety\n\n\\(K(K \\phi_i \\supset \\phi_{i+1})\\) when \\(i\\) is large.  \nAnd combining Knowledge of Safety with\nKK yields an absurd result: \nGiven the assumption on line (1), that you know there are at least\n\\(100\\) jellybeans in the jar (which you can plainly see), we can show\nthat there are more jellybeans in the jar than stars in the galaxy.\nSet \\(n\\) high enough and the jellybeans even outnumber the particles\nin the universe! (Notice that we don’t rely on\nNEC anywhere in this derivation, so it’s okay\nto use non-logical assumptions like line (1) and Knowledge of\nSafety.) \nWhat’s the philosophical payoff if we join Williamson in\nrejecting KK on these grounds? Skeptical arguments\nthat rely on KK might be disarmed. For example, a\nskeptic might argue that to know something, you must be able to rule\nout any competing alternatives. For example, to know the external\nworld is real, you must be able to rule out the possibility that you\nare being deceived by Descartes’ demon (Stroud 1984). But then\nyou must also be able to rule out the possibility that you\ndon’t know the external world is real, since this is\nplainly an alternative to your knowing it is real. That is,\nyou must \\(K \\neg\\neg K\\phi\\), and thus \\(KK\\phi\\) (Greco 2014). So\nthe driving premise of this skeptical argument entails the\nKK thesis, which we’ve seen reason to\nreject. \nOther skeptical arguments don’t rely on KK, of\ncourse. For example, a different skeptical tack begins with the\npremise that a victim of Descartes’ demon has exactly the same\nevidence as a person in the real world, since their experiential\nstates are indistinguishable. But if our evidence is the same in the\ntwo scenarios, we have no justification for believing we are in one\nrather than the other. Williamson (2000: ch. 8) deploys an argument\nsimilar to his reductio of KK against the\npremise that the evidence is the same in the real world and the demon\nworld. The gist is that we don’t always know what evidence we\nhave in a given scenario, much as we don’t always know what we\nknow. Indeed, Williamson argues that any interesting feature of our\nown minds is subject to a similar argument, including that it appears\nto us that \\(\\phi\\): \\(A\\phi \\supset KA\\phi\\) faces a similar\nreductio to that for \\(K\\phi \\supset KK \\phi\\). For further\nanalysis and criticism, see Hawthorne (2005), Mahtani (2008),\nRamachandran (2009), Cresto (2012), and Greco (2014). \nInteresting things happen when we study whole communities, not just\nisolated individuals. Here we’ll look at information-sharing\nbetween researchers, and find two interesting results. First, sharing\ninformation freely can actually hurt a community’s ability to\ndiscover the truth. Second, mistrust between members of the community\ncan lead to a kind of polarization. \nWe’ll also introduce a new tool in the process: computer\nsimulation. The python code to reproduce this section’s results\ncan be \n downloaded from GitHub. \nImagine there are two treatments for some medical condition. One\ntreatment is old, and its efficacy is well known: it has a .5 chance\nof curing the condition in any given case. The other treatment is new,\nand might be slightly better or slightly worse: a .501 chance of\nsuccess, or else .499. Researchers aren’t sure yet which it\nis. \nAt present, some doctors are wary of the new treatment, others are\nmore optimistic. So some try it on their patients while others stick\nto the old ways. As it happens the optimists are right: the new\ntreatment is superior: it has a .501 chance of success. \nSo, will the new treatment’s superiority eventually emerge as a\nconsensus within the community? As data on its performance are\ngathered and shared, shouldn’t it become clear over time that\nthe new treatment is slightly better? \nNot necessarily. It’s possible that those trying the new\ntreatment will hit a string of bad luck. Initial studies may get a run\nof less-than-stellar results, which don’t accurately reflect the\nnew treatment’s superiority. After all, it’s only slightly\nbetter than the traditional treatment. So it might not show its mettle\nright away. And if it doesn’t, the optimists may abandon it\nbefore it has a chance to prove itself. \nOne way to mitigate this danger is to limit the flow of information in\nthe medical community. Following Zollman (2007), let’s\ndemonstrate this by simulation. \nWe’ll create a network of “doctors,” each with their\nown initial credence that the new treatment is superior. Those with\ncredence above .5 will try the new treatment, others will stick to the\nold one. Doctors connected by a line share their results with each\nother, and everyone then updates on whatever results they see using\nBayes’ theorem (§1.2.2). \nWe’ll consider networks of different sizes, from 3 to 10\ndoctors. And we’ll try three different network\n“shapes,” either a complete network, a wheel, or a\ncycle: Three network configurations, illustrated here with 6 doctors each \nOur conjecture is that the cycle will prove most reliable. A doctor\nwho gets an unlucky string of misleading results will do the least\ndamage there. Sharing their results might discourage their two\nneighbours from learning the truth. But the others in the network may\nkeep investigating, and ultimately learn the truth about the new\ntreatment’s superiority. The wheel should be more vulnerable to\naccidental misinformation, however, and the complete network most\nvulnerable. \nHere are the details. Initially, each doctor is assigned a random\ncredence that the new treatment is superior, chosen uniformly from the\n[0, 1] interval. Those with credence above .5 will then try the new\ntreatment on 1,000 patients. The number of successes will be randomly\ndetermined by performing 1,000 “flips” of a virtual coin\nwith probability .501 of heads (successful treatment). \nEach doctor then shares their results with their neighbours, and\nupdates by Bayes’ theorem on all data available to them. Then we\ndo another round of experimenting, sharing, and updating, followed by\nanother, and so on until the community reaches a consensus. \nConsensus can be achieved in either of two ways. Either everyone\nlearns the truth, that the new treatment is superior, by achieving\nhigh credence in it (above .99 we’ll say). Alternatively,\neveryone might reach credence .5 or lower in the new treatment. Then\nno one experiments with it further, so it’s impossible for it to\nmake a comeback. \nHere’s what happens when we run each simulation 10,000 times.\nBoth the shape of the network and the number of doctors affect how\noften the community finds the truth. The first factor is the Zollman\neffect: the less connected the network, the more likely they’ll\nfind the truth. Probability of discovering the truth\ndepends on network configuration and number of doctors. \nBut notice that a bigger community is more likely to find the truth\ntoo. Why? Because bigger, less connected networks are better insulated\nagainst misleading results. Some doctors are bound to get data that\ndon’t reflect the true character of the new treatment once in a\nwhile. And when that happens, their misleading results risk polluting\nthe community with misinformation, discouraging others from\nexperimenting with the new treatment. But the more people in the\nnetwork, the more likely the misleading results will be swamped by\naccurate, representative results from others. And the fewer people see\nthe misleading results, the fewer people will be misled. \nHere’s an animated pair of simulations to illustrate the first\neffect. Here I’ve set the six doctors’ starting credences\nto the same, even spread in both networks: .3, .4, .5, .6, .7, and .8.\nI also gave them the same sequence of random data. Only the\nconnections in the networks are different, and in this case it makes\nall the difference. Only the cycle learns the truth. The complete\nnetwork goes dark very early, abandoning the novel treatment entirely\nafter just 26 iterations. Two networks with identical priors encounter\n identical evidence, but only one discovers the truth.\n [Alternative link to video] \nWhat saves the cycle network is the doctor who starts with .8 credence\n(bottom left). They start out optimistic enough to keep going after\nthe group encounters an initial string of dismaying results. In the\ncomplete network, however, they receive so much negative evidence\nearly on that they give up almost right away. Their optimism is\noverwhelmed by the negative findings of their many neighbours. Whereas\nthe cycle exposes them to less of this discouraging evidence, giving\nthem time to keep experimenting with the novel treatment, ultimately\nwinning over their neighbours. \nAs Rosenstock, Bruner, and O’Connor (2017) put it: sometimes\nless is more, when it comes to sharing the results of scientific\ninquiry. But how important is this effect? How often is it present,\nand is it big enough to worry about in actual practice? \nRosenstock, Bruner, and O’Connor argue that the Zollman effect\nonly afflicts epistemically “hard” problems. It’s\nonly because the difference between our two treatments is so hard to\ndiscern from the data that the Zollman effect is a concern. If the new\ntreatment were much more noticeably superior to the old one, say a .7\nchance of success rather than the .501 we imagined above,\nwouldn’t there be little chance of its superiority going\nunnoticed? \nSo Rosenstock, Bruner, and O’Connor rerun the simulations with\ndifferent values for “epsilon,” the increase in\nprobability of success afforded by the new treatment. Before we held\nepsilon fixed at .001 = .501 − .5. But now we’ll let it\nvary up to .1. For simplicity we’ll only consider a complete\nnetwork versus a cycle this time, and we’ll hold the number of\ndoctors fixed at 10. (The number of trials each round continues to be\n1,000.) The Zollman effect vanishes as the difference in efficacy between the two treatments increases\n \nObserve how the Zollman effect shrinks as epsilon grows. In fact\nit’s only visible up to about .025 in these simulations. \nRosenstock, Bruner, and O’Connor also run other variations to\nshow that if our medical community is much larger, or if each doctor\ngathers a much larger sample before sharing, the Zollman effect\nvanishes. It becomes very unlikely that an unrepresentative sample\nwill arise and discourage the whole community. So there’s no\nreal harm in sharing data freely. \nA natural question then is: how often do real-world research\ncommunities face the kind of “hard” problem where the\nZollman effect is a real concern? Rosenstock, Bruner, and\nO’Connor acknowledge that some laboratory experiments have found\nsimilar effects, where limiting communication between subjects leads\nto improved epistemic outcomes. But they also stress that the Zollman\neffect is not “robust,” requiring fairly specific\ncircumstances to arise (small epsilon, a small research community, and\nnot-too-large sample sizes). Since the above model is both simple and\nidealized, this lack of robustness should give us pause, they argue,\nabout its likely applicability in real-world scenarios. \nLet’s switch now to a different use of these epistemic network\nmodels. So far our doctors updated on each other’s data as if it\nwere their own. But what if they mistrust one another? It’s\nnatural to have less than full faith in those whose opinions differ\nfrom your own. They seem to have gone astray somewhere, after all. And\neven if not, their views may have illicitly influenced their\nresearch. \nSo maybe our doctors won’t take the data shared by others at\nface value. Suppose instead they discount it, especially when the\nsource’s viewpoint differs greatly from their own.\nO’Connor & Weatherall (2018) and Weatherall &\nO’Connor (forthcoming) explore this possibility, and find that\nit can lead to polarization. Instead of the community reaching a\nconsensus, some doctors in the community may abandon the new\ntreatment, even while others conclude that it’s superior. \nIn the example animated below, doctors in blue have credence above .5,\nso they experiment with the new treatment, sharing the results with\neveryone. Doctors in green have credence .5 or below, but are still\npersuadable. They still trust the blue doctors enough to update on\ntheir results—though they discount these results more the\ngreater their difference of opinion with the doctor who generated\nthem. Finally, red doctors ignore results entirely. They’re so\nfar from all the blue doctors that they don’t trust them at\nall. Example of polarization in the O’Connor-Weatherall model \n[Alternative link to video] \nIn this simulation, we reach a point where there are no more green\ndoctors, only unpersuadable skeptics in red and highly confident\nbelievers in blue. And the blues have become so confident,\nthey’re unlikely to ever move close enough to any of the reds to\nget their ear. So we’ve reached a stable state of\npolarization. \nHow often does such polarization occur? It depends on the size of the\ncommunity, and on the “rate of mistrust.” To program this\nmodel, we have to decide how much one doctor discounts another’s\ndata, given their difference of opinion. This \"rate of mistrust\" is an\nadjustable parameter in the model. \nHere’s how these two factors—community size and rate of\nmistrust—affect the probability of polarization. (Note that we\nonly consider complete networks here.) Probability of polarization depends on community size and rate of mistrust. \nSo, the more doctors are inclined to mistrust one another, the more\nlikely they are to end up polarized. No surprise there. But larger\ncommunities are also more disposed to polarize. Why? \nAs O’Connor & Weatherall explain, the more doctors there\nare, the more likely it is that strong skeptics will be present at the\nstart of inquiry: doctors with credence well below .5. These doctors\nwill tend to ignore the reports of the optimists experimenting with\nthe new treatment. So they anchor a skeptical segment of the\npopulation. \nSo far we’ve glossed over an important detail of O’Connor\n& Weatherall’s model. How does the discounting work, and how\ndo doctor’s update on discounted evidence? When Dr. X\nreports data \\(E\\) to Dr. Y, Y doesn’t simply\nconditionalize on \\(E\\). That would mean they take X’s report at\nface value. So what do they do? \nTo compute their updated credence \\(p'(H)\\) in the new\ntreatment’s superiority, Y takes a weighted average of\n\\(p(H \\mid E)\\) and \\(p(H \\mid \\neg E)\\). This\nprocedure is a famous variation on conditionalization known as\n Jeffrey Conditionalization: \nJeffrey Conditionalization\n\nGiven the prior probability assignments \\(p(H\\mid E)\\) and \\(p(H\\mid\n\\neg E)\\), the new, unconditional probability assignment to \\(H\\) upon\nlearning \\(E\\) with level of certainty \\(p'(E)\\) should be:\n\n\\[p'(H) = p(H \\mid E)p'(E) + p(H \\mid \\neg E)p'(\\neg E).\\]\n\n  \nThis formula looks a lot like the law of total probability\n(§1.2.1), but there’s a crucial difference. The weights in\nthis weighted average are not \\(p(E)\\) and \\(p(\\neg E)\\). They are\ninstead \\(p'(E)\\) and \\(p'(\\neg E)\\). They are the updated,\nalready-discounted probabilities Y assigns to X’s report and its\nnegation. \nO’Connor & Weatherall (2018) suggest a natural formula for\ncomputing \\(p'(E)\\) and \\(p'(\\neg E)\\), which we won’t go into\nhere. We’ll just note that the choice of formula is crucial to the\npolarization effect. Mistrust doesn’t necessarily introduce the\npossibility of polarization; the mistrust has to be sufficiently\nstrong (greater than 1.0 in the above figure). There has to be a point\nat which agents won’t trust each other at all because their difference\nof opinion is so great. Otherwise the skeptics would never ignore\ntheir optimistic colleagues entirely, so they’d eventually be won over\nby their encouraging reports. \nThis illustrates a general issue with update rules like Jeffrey\nConditionalization: to apply them, we first need to determine the new\nprobabilities to assign to the evidence. From there we can determine\nthe new probabilities of other propositions. But this essential bit of\ninput is something for which we don’t have a rule; it’s a\nsort of loose end in the formal system, something that’s left up\nto us as users of the model. For some discussion of the\nepistemological significance of this point, see Field (1978) and\nChristensen (1992). \nFor a different formal approach to polarization, see Dorst (2020, \nOther Internet Resources). For\nother work on network epistemology see Zollman (2013) and\n §4.3 of the entry on social epistemology,\n and the references therein. \nOther formal projects in social epistemology include work on the\nrelationship between social and individual rationality (Mayo-Wilson,\nZollman, and Danks 2011); on judgment aggregation/opinion pooling\n(Genest and Zidek 1986; List and Pettit 2002; Russell, Hawthorne, and\nBuchak 2015); on learning from the beliefs of others (Easwaran et\nal 2016; Bradley 2018); and on the social benefits of competing\nupdate rules, such as Conditionalization vs. Inference to the\nBest Explanation (Douven and Wenmackers 2017; Pettigrew m.s., \nOther Internet Resources). \nTools like probability theory and epistemic logic have numerous uses\nin many areas of philosophy besides epistemology. Here we’ll\nlook briefly at just a few examples: how to make decisions, whether\nGod exists, and what hypothetical discourses like\n‘if…then …’ mean. \nShould you keep reading this section, or should you stop here and go\ndo something else? That all depends: what might you gain by continuing\nreading, and what are the odds those gains will surpass the gains of\ndoing something else instead? Decision theory weighs these\nconsiderations to determine which choice is best. \nTo see how the weighing works, let’s start with a very simple\nexample: betting on the outcome of a die-roll. In particular,\nlet’s suppose a 5 or 6 will win you $19, while any other outcome\nloses you $10. Should you take this bet? We can represent the choice\nyou face in the form of a table: \nSo far, taking the bet looks pretty good: you stand to gain almost\ntwice as much as you stand to lose. What the table doesn’t show,\nhowever, is that you’re twice as likely to lose as to win:\n\\(2/3\\) vs. \\(1/3\\). So let’s add this information in: \nNow we can see that the potential downside of betting, namely losing\n$10, isn’t outweighed by the potential upside. What you stand to\nwin isn’t quite twice what you’d lose, but the probability\nof losing is twice as much. Formally, we can express this\nline of thinking as follows: \\[ (-10 \\times 2/3) + (19 \\times 1/3) = -1/3\n< 0\\] \nIn other words, when the potential losses and gains are weighed\nagainst their respective probabilities, their sum total fails to\nexceed 0. But $0 is what you can expect if you don’t bet. So\nbetting doesn’t quite measure up to abstaining in this\nexample. \nThat’s the basic idea at the core of decision theory, but\nit’s still a long way from being satisfactory. For one thing,\nthis calculation assumes money is everything, which it surely\nisn’t. Suppose you need exactly $29 to get a bus home for the\nnight, and all you have is the $10 bill in your pocket, which on its\nown is no use (even the cheapest drink at the casino bar is $11). So\nlosing your $10 isn’t really much worse than keeping\nit—you might as well be broke either way. But gaining\n$19, now that’s worth a lot to you. If you can just get the bus\nback home, you won’t have to sleep rough for the night. \nSo we have to consider how much various dollar-amounts are worth\nto you. Losing $10 is worth about the same to you as losing $0,\nthough gaining $19 is much, much more valuable. To capture these\nfacts, we introduce a function, \\(u\\), which represents the\nutility of various possible outcomes. For you, \\(u(-$10)\n\\approx u(-$0)\\), but \\(u(+$19) \\gg u(-$0)\\). \nExactly how much is gaining $19 worth to you? What is\n\\(u(+$19)=\\ldots\\), exactly? We can actually answer this question if\nwe just set a scale first. For example, suppose we want to know\nexactly how much you value a gain of $19 on a scale that ranges from\ngaining nothing to gaining $100. Then we set \\(u(+$0)=0\\) and\n\\(u(+$100)=1\\), so that our scale ranges from 0 to 1. Then we can\ncalculate \\(u(+$19)\\) by asking how much you would be willing to risk\nto gain $100 instead of just $19. That is, suppose you had a choice\nbetween just being handed $19 with no strings attached vs. being\noffered a (free) gamble that pays $100 if you win, but nothing\notherwise. How high would the probability of winning that $100 have to\nbe for you to take a chance on it instead of the guaranteed $19? Given\nwhat’s at stake—making it home for the night vs. sleeping\nrough—you probably wouldn’t accept much risk for the\nchance at the full $100 instead of the guaranteed $19. Let’s say\nyou’d accept at most .01 risk, i.e., the chance of winning the\nfull $100 would have to be at least .99 for you to trade the\nguaranteed $19 for the chance at the full $100. Well, then, on a scale\nfrom gaining $0 to gaining $100, you value gaining $19 quite highly:\n.99 out of 1. (This method of measuring utility was discovered and\npopularized by von Neumann and Morgenstern (1944), though\nessentially the same idea was previously discovered by Ramsey (1964\n[1926]).) \nOur full decision theory relies on two functions then, \\(p\\) and\n\\(u\\). The probability function \\(p\\) reflects how likely you think\nthe various possible outcomes of an action are to obtain, while \\(u\\)\nrepresents how desirable each outcome is. Faced with a choice between\ntwo possible courses of action, \\(A\\) and \\(\\neg A\\), with two\npossible states the world might be in, \\(S\\) and \\(\\neg S\\), there are\nfour possible outcomes, \\(O_1,\\ldots,O_4\\). For example, if you bet $1\non a coin-flip coming up heads and it does comes up heads, outcome\n\\(O_1\\) obtains and you win $1; if instead it comes up tails, outcome\n\\(O_2\\) obtains and you lose $1. The general shape of such situations\nis thus: \nTo weigh the probabilities and the utilities against each other, we\nthen define the notion of expected utility: \nDefinition. The expected utility of act\n\\(A\\), \\(EU(A)\\), is defined: \n\n\\[ EU(A) = p(S)u(O_1) + p(\\neg S)u(O_2).\\]\n\n The expected\nutility of act \\(\\neg A\\), \\(EU(\\neg A)\\), is likewise:\n\n\\[ EU(\\neg A) = p(S)u(O_3) + p(\\neg S)u(O_4).\\]\n\n  \n(Why “expected” utility? If you faced the same decision\nproblem over and over again, and each time you chose option \\(A\\), in\nthe long run you could expect your average utility to be approximately\n\\(EU(A)\\).) The same idea extends to cases with more than two ways\nthings could turn out simply by adding columns to the table and\nmultiplying/summing all the way across. When there are more than two\npossible actions, we just add more rows and do the same. \nFinally, our decision theory culminates in the following norm: \nExpected Utility Maximization\n\nChoose the option with the highest expected utility. (In case of a\ntie, either option is acceptable.) \nWe haven’t given much of an argument for this rule, except that\nit “weighs” the desirability of each possible outcome\nagainst the probability that it will obtain. There are various ways\none might develop this weighing idea, however. The one elaborated here\nis due to Savage (1954). It is considered the classic/orthodox\napproach in social sciences like economics and psychology.\nPhilosophers, however, tend to prefer variations on Savage’s\nbasic approach: either the “evidential” decision theory\ndeveloped by Jeffrey (1965) or some form of\n “causal” decision theory (see entry)\n (Gibbard and Harper 1978; Skyrms 1980; Lewis 1981; Joyce 1999). \nThese approaches all agree on the broad idea that the correct decision\nrule weighs probabilities and utilities in linear fashion: multiply\nthen add (see\n the entry on expected utility).\n A different approach recently pioneered by Buchak (2013, 2014) holds\nthat (in)tolerance for risk throws a non-linear wrench into this\nequation, however (see also Steele 2007). And taking account of\npeople’s cognitive limitations has long been thought to require\nfurther departures from the traditional, linear model (Kahneman and\nTversky 1979; Payne, Bettman, and Johnson 1993; Gigerenzer, Todd, and\nGroup 1999; Weirich 2004). \nThe mathematical theories of probability and decision emerged together\nin correspondence between Blaise Pascale and Pierre de Fermat in the\nmid-17th Century. Pascal went on to apply them to\ntheological questions, developing his famous “wager”\nargument\n (see entry on Pascal’s Wager)\n for belief in God. Probability theory now commonly appears in\ndiscussions of other arguments for and against theism, especially the\nargument from design. Though Darwin is generally thought to have\ntoppled theistic appeals to biological design, newer findings in\ncosmology and physics seem to support a new probabilistic argument for\nGod’s existence. \nThe development of the universe from the Big Bang to its present form\ndepended on two factors: the laws of physics and the initial\nconditions at the time of the Big Bang. Both factors appear to have\nbeen carefully arranged so that the universe would be capable of\nsupporting life. Had certain constants in the physical laws been\nslightly different, intelligent life would never have been able to\nevolve. For example, had the forces that bind the nuclei of atoms\ntogether been slightly stronger or weaker, only hydrogen would exist.\nThere would be no carbon, oxygen, or other elements available to form\ncomplex molecules or organisms. Similarly, had the expansion speed of\nthe Big Bang been slightly different, the universe would have either\nsimply collapsed back in on itself soon after the Big Bang, or else\ndispersed into diffuse dust. Stars and planets would never have formed\n(Rees 1999). \nThese findings point to a new kind of design argument, one untouched\nby the advent of evolutionary theory. Evolution might explain the\ndesigns we find in the organic world, but what explains the fact that\nour cosmos appears to be “fine-tuned” to allow the\nexistence of (intelligent) life? Apparently, the cosmos actually was\nfine-tuned, by a creator who deliberately designed it so that it would\ncontain (intelligent) life. If there were no such designer, the\nfine-tuning of the cosmos would be a massively improbable\ncoincidence. \nTo make this argument rigorous, it’s often formulated in\nprobabilistic terms. Following Sober (2005), we’ll adopt a\nsimple, modest formulation. Let \\(F\\) be the evidence that our\nuniverse is fine-tuned, as just described, and let \\(D\\) be the\n“design hypothesis”, the hypothesis that the universe was\ncreated by an intelligent designer with the aim of creating\n(intelligent) life. The argument then runs: \n\\(p(F\\mid D) > p(F\\mid \\neg D)\\) \nIn general, when \\(p(E\\mid H) > p(E\\mid \\neg H)\\), then \\(E\\)\nsupports \\(H\\) over \\(\\neg H\\). \nSo \\(F\\) supports \\(D\\) over \\(\\neg D\\). \nThe argument is plainly valid, so discussion focuses on the\npremises. \nThe rationale behind (1) is that \\(p(F\\mid \\neg D)\\) is quite small,\nsince there are so many ways the physical laws and initial constants\ncould have been, almost all of which would have yielded a universe\ninhospitable to life. Without a designer to ensure hospitable\nconstants and conditions, a hospitable outcome would have been\nmassively improbable. But \\(p(F\\mid D)\\), on the other hand, is fairly\nhigh: the envisioned designer’s aim in creating the universe was\nto create life, after all. \nTo see the rationale for (2), recall our discussion of confirmation\ntheory\n (§1.2).\n According to our definition of confirmation, evidence confirms a\nhypothesis just in case \\(p(H\\mid E)>p(H)\\), which Bayes’\ntheorem tells us is equivalent to \\(p(E\\mid H) > p(E)\\). Likewise,\n\\(E\\) disconfirms \\(\\neg H\\) just in case \\(p(E) > p(E\\mid\n\\neg H)\\). Now, we can prove that if \\(p(E\\mid H) > p(E)\\), then\n\\(p(E) > p(E\\mid \\neg H)\\). So if \\(E\\) confirms \\(H\\), it\ndisconfirms \\(\\neg H\\), which amounts to \\(E\\) supporting \\(H\\) over\n\\(\\neg H\\). \nIt’s crucial to note, however, that \\(E\\) supporting \\(H\\) over\n\\(\\neg H\\) does not mean that, once we learn \\(E\\), \\(H\\)\nbecomes more probable than \\(\\neg H\\). It just means that \\(E\\) raises\nthe probability of \\(H\\) and decreases the probability of \\(\\neg H\\).\nIf \\(H\\) was very improbable to begin with, then \\(E\\) might not\nincrease its probability enough to make it more probable than \\(\\neg\nH\\). This is why our formulation of the argument is so modest. It only\naims to show that \\(F\\) is evidence for \\(D\\) and against \\(\\neg D\\).\nIt makes no claims about how strong the evidence is, or whether it\nshould leave us theists or atheists in the end (Sober 2005). Yet\ncritics argue that even this modest argument is unsound. We’ll\nconsider four such lines of criticism. \nOne line of criticism appeals to so-called “anthropic”\nconsiderations. The idea is that some findings are a consequence of\nour nature as observers, and thus reflect something about us rather\nthan the phenomena under discussion. For example, I might notice that\nwhenever I observe a physical object, the observation happens while I\nam awake. But I shouldn’t conclude from this that physical\nobjects only exist when I am awake. This feature of my observations\njust reflects something about me: I have to be awake to make these\nobservations. Likewise, these critics argue, we can only observe a\ncosmos that has the features necessary to support (intelligent) life.\nSo our discovery that our universe is fine-tuned only reflects a\nlimitation in us, that we could not observe the opposite (McMullin\n1993; Sober 2005). \nProponents of the fine-tuning argument respond that our inability to\nobserve something does not render observations to the contrary\nuninformative. For example, Leslie (1989) notes that someone put\nbefore an expert firing squad cannot observe that they do not survive,\nsince they won’t be alive to make the observation. Yet in the\nunlikely event that they do survive, that’s strong evidence that\nthe squad missed by design. Expert firing squads rarely miss by\naccident. Sober (2005) responds that a firing-squad survivor does\nindeed have evidence, but on a different basis, one that isn’t\navailable to proponents of the design argument. See Monton (2006) and\nSober (2009) for further discussion. \nA different line of criticism objects that \\(p(F\\mid \\neg D)\\)\nisn’t low after all: even without a designer, the fine-tuning\ndiscovery was “inevitable” because our universe is just\none in an infinite sequence of universes, oscillating from bang to\ncrunch and back to bang again, with a new set of constants and initial\nconditions emerging at each bang (Wheeler 1973; Leslie 1989). Sooner\nor later, this endless cycle of universal reboots is bound to hit upon\na life-supporting configuration of constants and initial conditions,\nso \\(p(F\\mid \\neg D)\\) may even equal 1, contra premise (1). (How we\ncould know about this endless cycle of universes is a tricky question.\nThe crucial piece of evidence might be that it explains why our\nuniverse is fine-tuned. But then, the same may be true of the design\nhypothesis, \\(D.)\\) \nHacking (1987) counters that these “oscillating universes”\nonly ensure that some universe at some point in the sequence\nis capable of supporting life. But they make it no more likely that\nthis universe would. At the time of our Big Bang, there were\nstill innumerably life-unfriendly ways things could have started off,\nall equally likely if there was no designer to ensure a life-friendly\nbeginning. Just as rolling a pair of dice over and over again ensures\nthat snake-eyes (both dice coming up 1) will turn up at some point,\nwhatever roll they do turn up on was still extremely unlikely to turn\nout that way. If the 53rd roll comes up snake-eyes, this\nwas hardly inevitable; in fact, it was quite improbable, only a 1 in\n36 chance. Hacking suggests that a different sort of “multiple\nuniverses” hypothesis escapes this problem: Carter’s\n(1974) hypothesis that all the possible Big Bang-type universes exist\n“side by side”, rather than in an oscillating sequence.\nThen, Hacking suggests, it follows deductively that our universe had\nto exist, so \\(p(F\\mid \\neg D)\\) comes out 1 after all. But White\n(2000) counters that the fallacy in the appeal to Wheeler’s\nmodel afflicts the appeal to Carter’s model too. Even with the\nmultitude of universes existing “side by side”,\nthis one didn’t have to be one of the few with\nlife-friendly parameters. \nA third line of criticism attacks the rationale for assigning a low\nnumber to \\(p(F\\mid \\neg D)\\). The complaint is that the rationale\nactually makes \\(p(F\\mid \\neg D)=0\\), and also assigns probability 0\nto many other, intuitively much more probable, ways the universe might\nhave turned out. How so? The rationale for a low \\(p(F\\mid \\neg D)\\)\ngoes something like this: take an apparently fine-tuned parameter of\nour universe, like its expansion speed. This speed had to be exactly\nbetween 9 and 10 km/sc, let’s pretend, for the universe to be\nable to support life. But given that it could have been any speed from\n0 km/sc to 100 km/sc to \\(1,000,000\\) km/sc to…that it would\nend up in the narrow 9–10 km/sc window was extremely unlikely to\nhappen without divine guidance. But, the objection goes, the same\ncould be said of much larger ranges, like a \\(10^1\\)–\\(10^{10}\\)\nkm/sc window. Even that large range is a drop in the infinite bucket\nof speeds that could have obtained, from 0 through the entire positive\nreal line. In fact, any finite range is effectively 0% of\ninfinity—indeed, it really is \\(0\\%\\) on the standard ways of\nmeasuring these things (Colyvan, Garfield, and Priest 2005). So even\nif our universe only needed “coarse tuning” to support\nlife, i.e., even if it would have supported life given any of a\nmassively broad yet finite range of conditions, a parallel premise to\n(1) could be justified by this rationale, and a corresponding\n“coarse-tuning argument” for design offered (McGrew,\nMcGrew, and Vestrup 2001). \nCollins (2009) points out an uncomfortable consequence of this\nobjection, that the fine-tuning argument would be compelling if only\n\\(\\neg D\\) were more life-friendly. Imagine that the laws of\nphysics only permitted a finite range of possible expansion speeds,\nsay 0–100 km/s, with a speed of 9–10 km/s required to\nsupport life. Then premise (1) would hold and the fine-tuning argument\nwould succeed: \\(p(F\\mid \\neg D)=1/100\\), with \\(p(F\\mid D)\\)\npresumably much higher, maybe even 1. Now imagine the possible range\nto be much larger, say 0–\\(10^{10}\\) km/s. The argument then\nbecomes even stronger, with \\(p(F\\mid \\neg D)=1/10^{10}\\). As the\nupper limit on possible expansion speeds increases, the argument\nbecomes stronger and stronger…until the limit becomes infinite,\nat which point the argument fails, according to the present\nobjection. \nHypothetical discourses have a puzzling connection to reality. Suppose\nI assert, “If the GDP continues to decline, unemployment will\nrise”, but the GDP does not continue to decline, instead holding\nsteady. Is what I said true or false? It’s not obvious, since my\nstatement has not been tested by the world in the obvious way. If the\nGDP had continued to decline yet unemployment had fallen, my statement\nwould have been tested, and it would have failed. But GDP held steady,\nso what test can my assertion be put to? \nWhen working with propositional logic, we often translate ordinary\n‘If …then …’ statements using the material\nconditional, \\(\\supset\\). But the probability of a\n\\(\\supset\\)-statement often exceeds that of the corresponding\n‘If …then …’ statement. For example,\nit’s very improbable that I’ll win an Olympic gold medal\nin diving (\\(G\\)) if I train five hours a day (\\(T\\)). Olympic divers\nretire by the time they’re my age. Yet \\(p(T \\supset G)\\) is\nquite high, for the simple reason that \\(T \\supset G\\) is equivalent\nto \\(\\neg T \\vee G\\) and \\(\\neg T\\) is very probable. I won’t be\ntraining for Olympic diving one minute a day, much less five hours. I\ndon’t even swim. So it’s hard to accept \\(\\supset\\) as a\ngood model of ‘If …then …’, though some\nphilosophers do nevertheless think it’s correct (Grice 1989;\nJackson 1987). \nCould we introduce a new connective with a different semantics than\n\\(\\supset\\) that would do better? A striking theorem discovered by\nLewis (1976) suggests not. The theorem relies on an assumption posited\nby Stalnaker (1970): that the probability of “If \\(A\\) then\n\\(B\\)” is the same as the conditional probability, \\(p(B\\mid\nA)\\). Let’s use \\(A \\rightarrow B\\) as shorthand for the\nEnglish, “If \\(A\\) then \\(B\\)”: \nStalnaker’s Hypothesis\n\n\\(p(A \\rightarrow B) = p(B\\mid A)\\), for any propositions \\(A\\) and\n\\(B\\) and probability function \\(p\\) such that \\(p(A) \\neq 0.\\) \nStalnaker’s Hypothesis might seem obvious at first, even\ntautological. Isn’t \\(p(B\\mid A)\\) just the probability of the\nproposition \\(B\\mid A\\), which is just shorthand for “\\(B\\) is\ntrue if \\(A\\) is”? This is a common misconception for newcomers\nto probability theory, one that Lewis shows leads to disastrous\nresults. If we think of \\(B\\mid A\\) as a complex proposition built out\nof the sentences \\(A\\) and \\(B\\) with a connective \\(\\mid \\),\nprobability theory goes to pot (see the\n technical supplement\n for a proof): \nTheorem (Lewis’ Triviality Theorem). If\nStalnaker’s Hypothesis is true, then \\(p(B\\mid A)=p(B)\\) for all\npropositions \\(A\\) and \\(B\\) such that \\(p(A) \\neq 0\\) and \\(1 >\np(B) > 0\\). \nApparently, no propositional connective \\(\\rightarrow\\) can obey\nStalnaker’s Hypothesis. If one did, every proposition would be\nindependent of every other (except where things are absolutely\ncertain). But surely some facts are relevant to some others. \nOne thing this tells us is that the right way to read \\(p(B\\mid A)\\)\nis not as the probability of some sentence, \\(B\\mid A\\), but instead\nas a two-place function. The syntax \\(p(B\\mid A)\\) is misleading, and\nmight be more clearly written \\(p(B,A)\\), the standard notation for a\ntwo-place function like \\(f(x,y)=x^2+y^2\\). \nBut a more troubling lesson is that we face an uncomfortable choice:\neither there is no such thing as the proposition \\(A \\rightarrow B\\),\nor the probability of the proposition \\(A \\rightarrow B\\)\ndoesn’t always match \\(p(B\\mid A)\\). The first option would seem\nto make assertions of the form “If …then …”\na peculiar exception to the compositionality of natural language\nsemantics (but see Edgington 2000). The second option is\ncounterintuitive, and also apparently counter to empirical evidence\nthat people ordinarily do take \\(p(A \\rightarrow B)\\) to be the same\nas \\(p(B\\mid A)\\) (Douven and Dietz 2011). \nA particularly striking thing about this problem is how robust it is.\nNot only have many related theorems been proved using probability\ntheory (Hájek 1989; Edgington 1995; Bradley 2000), but similar\nresults have also emerged in a completely independent formal\nframework:\n the theory of belief revision. \nBelief revision theory represents beliefs with sentences of\npropositional logic: \\(A\\), \\(A \\supset B\\), \\(\\neg (A \\wedge \\neg\nB)\\), and so on. Your full corpus of beliefs is a set of such\nsentences we call \\(K\\) (not to be confused with the sentential\noperator \\(K\\) from epistemic logic\n (§4.1)).\n Importantly, we assume that \\(K\\) contains everything entailed by\nyour beliefs: if \\(A\\) and \\(A \\supset B\\) are in \\(K\\), then so is\n\\(B\\), for example. \nOf course, real people don’t believe everything their beliefs\nentail, but it helps keep things simple to make this assumption. You\ncan think of it as an idealization: we’re theorizing about what\nyour beliefs should look like if you were a perfect logician. (Notice\nthat probability theory has a similar feature encoded in axiom (2),\nand epistemic logic’s K axiom and\nNEC rule together have a similar effect.) \nThe main aim of belief revision theory is to say how you should revise\nyour beliefs when you learn new information. Suppose you learn about\nthe existence of a new planet, Algernon. How should \\(K\\) change when\nyou learn this new fact, \\(A\\)? As long as \\(A\\) doesn’t\ncontradict your existing beliefs, the standard view is that you should\njust add \\(A\\) to \\(K\\), along with everything that follows logically\nfrom the members of \\(K\\) and \\(A\\) together. We call the new set of\nbeliefs \\(K + A\\): add \\(A\\) to \\(K\\) along with all that follows\nlogically (Alchourrón, Gärdenfors, and Makinson 1985). \nWhat if \\(A\\) does contradict your existing beliefs? Then \\(K + A\\)\nwouldn’t do, since it would be inconsistent. We’d have to\nremove some of your existing beliefs to make room for \\(A\\). Luckily,\nfor our purposes here we don’t have to worry about how this\nworks. We’ll only consider cases where \\(A\\) is consistent with\n\\(K\\), in which case \\(K + A\\) will do. \nNow, suppose we want to add a new connective \\(\\rightarrow\\) to our\nlanguage to represent ‘If …then …’. When\nshould you believe a sentence of the form \\(A \\rightarrow B\\)? The\nclassic answer comes from an idea of Ramsey’s: that we decide\nwhether to accept \\(A \\rightarrow B\\) by temporarily adding \\(A\\) to\nour stock of beliefs and then seeing whether \\(B\\) follows (Ramsey\n1990 [1929]). This idea yields a principle called the Ramsey\nTest: \nRamsey Test\n\n\\(K\\) contains \\(A \\rightarrow B\\) if \\(K + A\\) contains \\(B\\); and\n\\(K\\) contains \\(\\neg (A \\rightarrow B)\\) if \\(K + A\\) contains \\(\\neg\nB\\). \nIn other words, you accept \\(A \\rightarrow B\\) if adding \\(A\\) to your\nstock of beliefs brings \\(B\\) with it. If instead adding \\(A\\) brings\n\\(\\neg B\\) with it, you reject this conditional (Etlin 2009). \nPlausible as the Ramsey Test is, Gärdenfors (1986) shows that it\ncannot hold unless your beliefs are absurdly opinionated. We’ll\nstate this result somewhat informally (see the\n technical supplement\n for a somewhat informal proof): \nTheorem (Gärdenfors’ Triviality Theorem).\nAs long as there are two propositions \\(A\\) and \\(B\\) such that \\(K\\)\nis agnostic about \\(A\\), \\(A \\supset B\\), and \\(A \\supset \\neg B\\),\nthe Ramsey Test cannot hold. \nApparently, much as no propositional connective \\(\\rightarrow\\) can\nobey Stalnaker’s Hypothesis in probability theory, none can obey\nThe Ramsey Test in belief revision theory either. Whether we approach\nepistemology using probabilities or flat-out beliefs, the same problem\narises. Should we conclude that conditionals have no factual content?\nIt’s a hotly contested question, on which\n the entry on conditionals\n has more.","contact.mail":"jonathan.weisberg@utoronto.ca","contact.domain":"utoronto.ca"}]
