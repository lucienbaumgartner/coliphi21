[{"date.published":"2015-12-18","date.changed":"2020-09-11","url":"https://plato.stanford.edu/entries/justice-moral-psych/","author1":"Christian B. Miller","author1.info":"http://www.wfu.edu/~millerc","entry":"justice-moral-psych","body.text":"\n\nWhether and to what extent people are motivated by considerations\nof justice is a central topic in a number of fields including\neconomics, psychology, and business. The implications of this topic\nextend broadly, from the psychology of negotiations, to the motives\ncitizens have to pay taxes, to what considerations influence\nhealthcare allocation decisions. Given all the possible topics which\ncould be explored, this essay adopts the following parameters to help\nnarrow the focus:\n\n\nWe will look at the moral psychology of individuals as opposed to\nfirms, societies, or other collective entities.\n\nOur focus will be on empirical results, rather than armchair\nconsiderations of what the psychology of people might be like or what\nit should be like. The SEP entry on the virtue of justice discusses\nthe moral psychology of a just person (LeBar 2020). But whether people\nactually are motivated by justice or have the virtue of justice is an\nempirical question.\n\nThe kind of justice that will be our focus here is distributive\njustice, rather than retributive, international, transitional, or\nother kinds.\n\n\nMore specifically, we will examine the distributive motives and\nbehavior of individuals by consulting the empirical literature on\neconomic games. Other topics, including the relationship between\nempirical results and specific theories of distributive justice such\nas John Rawls’s or Robert Nozick’s, will have to be\naddressed on another occasion (Lamont and Favor 2014).\n\nWhy economic games? The psychologists Kun Zhao and Luke Smillie\nprovide a nice answer:\n\n  Economic games have become widely adopted within the\npsychological sciences, where they are used to model complex social\ninteractions and allow for rigorous empirical investigations. As\nbehavioral paradigms, they are well-controlled, manipulable, and\nreplicable…making them ideal for bridging the gap between\ntheory and naturalistic data.  In personality research, they provide\nbehavioral paradigms that may complement and help validate self-report\nmeasures, and offer sharp operationalizations of somewhat slippery\nconcepts. (2015: 277–278, see also Fetchenhauer and Huang 2004:\n1018)\n\nThere are two general kinds of economic games. Social\ndilemmas typically juxtapose short-term self-interest with\nlong-term group interests, and include the prisoner’s dilemma\nand the public goods game. Bargaining games typically involve\ntwo players distributing a specific payoff (usually money), and will\nbe our focus here, as they are especially helpful for examining the\nmoral psychology of justice. Examples include the ultimatum game and\ndictator game. We will also look at a novel twist on the dictator game\nby the psychologist Daniel Batson, which has fostered a large\nexperimental literature on what he calls “moral\nhypocrisy”. Finally we will connect this discussion of economic\ngames to the virtue of justice and to other personality traits such as\nagreeableness, honesty-humility, and justice sensitivity.\n\nBefore diving in, it is worth saying something very briefly about\nwhy empirical results might be of interest to philosophers as they\ntheorize about distributive justice. One might think, after all, that\nphilosophers tend to focus less on what people actually tend to do and\nbe like, as opposed to what they should do and be like. Nevertheless,\nin recent years philosophers in general have paid increasing attention\nto empirical research, and with respect to matters of distributive\njustice, a few important reasons for doing so are the following:\n\nEthical egoism claims that a person’s central goal ought to\nbe the promotion of his or her long-term self-interest.  Most\nphilosophers reject ethical egoism. But is it psychologically\nrealistic to expect most people to act on motives other than\nself-interested ones, such as motives of fairness and justice?\nEmpirical results, such as those reviewed in the first three sections\nof this article, would bear on this question.\n\nRelatedly, moral philosophers have developed rich conceptions of\nthe virtue of justice and what a just person would do. But are these\nconceptions empirically adequate and psychologically realistic for\nhuman beings like us? Empirical results would bear on this question,\nand if the answer is no, then perhaps those conceptions are\nproblematized (Doris 2002; Miller 2014).\n\nPhilosophers might come up with normative criteria for when\nactions and institutions are just or not. But to actually apply those\ncriteria to real world concerns, they need empirical data about how\npeople and institutions are in fact behaving.\n\nEmpirical data is also highly relevant to philosophers interested\nin developing strategies for improvement, so that just actions, just\ninstitutions, and just character are increasingly promoted.\n\nThe entry is structured as follows:\n\nUltimatum games have the following setup. Call one person the\n“offerer” and the other the “responder”, The\nofferer is informed that she can allocate a certain amount of a good.\nSuppose the good is money, such as $10, since that is the usual good\nchosen by experimenters in the literature. Next the offerer is\ninstructed to make some offer to the responder—in our example,\nthe offer could range between $0 and $10, inclusive. So the offerer\ncan give away all the money, none of it, or something in-between. The\nonly other piece of information that the offerer knows, standardly, is\nthat the responder’s reaction matters when he is presented with\nher offer. If the responder accepts the offer, then both parties keep\nwhatever the offer says (unless it is $0, in which case only one side\nkeeps any money). But if the responder rejects the offer, then neither\nside gets any money at all, and the game is done. Hence if the offer\nis to keep $8 and give $2, and the responder accepts this offer, then\nthat is in fact the amount of money with which each side walks\naway. But if the responder rejects the offer, then each side walks\naway with $0.  A couple of observations are important here. First, it is not\nessential to the game that the offerer know anything about the\nresponder—he or she could be a complete stranger. Second and, as\nwe will see, very importantly, it is not an essential feature of the\ngame that the responder know how much money the offerer has to work\nwith in the first place. So when the offer comes in at $2, the\nresponder might have no idea whether that is all the money that the\nofferer has to allocate, or whether it is just a small amount out of\n$100, $10,000, or even higher.  In many versions of the ultimatum\ngame, the responder is told what the offerer has\navailable. But the point to stress here is that this is an optional\nfeature of the game. Finally, it is worth noting that the power\ndynamic in this game favors the offerer. The responder cannot make any\ncounteroffers back in the hope of getting a bigger offer. He only has\nthe threat of veto power. But if he does veto the offer, then he comes\naway with nothing himself. That is not a great negotiating position to\nbe in. Given this last thought, a natural prediction to make about how\nultimatum games will go is the following. Offerers will want to\nmaximize their self-interest, and so maximize their take-home payment.\nThey might think that, from a responder’s perspective, any money\nis better than no money. So we might predict that the offer itself\nwill be as low as possible, say 1 cent or 5 cents in our example\ninvolving $10. For the responder to reject this offer would be\nirrational, since he would come away with nothing, whereas accepting\nthe offer would make him better off than he would otherwise have\nbeen. This is, indeed, what many economists in the early days of research\non the ultimatum game predicted would happen, on the basis of standard\ngame theoretic assumptions (Kahneman et al. 1986: S285–286;\nPillutla and Murnighan 1995: 1409; Güth 1995: 329; Camerer and Thaler 1995: 210). In other words,\ngiven a concern to promote one’s own self-interest, the\nprediction is that minimal offers will be made in ultimatum games to\nmaximize one’s take-home amount—which is the subgame\nperfect Nash equilibrium (Kahneman et al. 1986: S289; Forsythe et\nal. 1994: 348; Güth 1995: 331; Pillutla and Murnighan 1995: 1409,\n2003: 248; Straub and Murnighan 1995: 345–346). But something surprising happened. Beginning with Werner\nGüth’s famous paper in 1982 (Güth et al. 1982), the first\nempirical studies that were actually conducted to see what people\nwould do in this situation found a rather different result. Robert\nForsythe and his colleagues, for instance, found that not one offerer\nkept the entire amount of $10, and that offers were instead\ndistributed around the 50/50 split amount with 75% offering at least\nan equal amount (1994: 362). As we noted, this behavior by offerers is\nnot what standard game theory would have predicted. Similarly, in a\nwell-known paper Daniel Kahneman and his colleagues found that out of\n$10, the mean amount offered was $4.76 with 81% making equal split\noffers (1986: S291, see also Güth 1995). Surprising results were found for responders too. Kahneman also\nreports, for instance, that the mean of minimally acceptable (i.e.,\nnot rejected) offers was $2.59, with 58% demanding more than $1.50\n(Kahneman et al. 1986: S291). To explain these puzzling results, several researchers began to\nmove beyond standard game theoretic assumptions and appeal to a\njustice motive. With offerers, this could be a desire to be fair in\nallocating goods, together with a belief that the fair allocation in\nthe game is an equal split. With responders, it could be a desire to\nnot be treated unfairly. This could lead to different explanations of\nrejection behavior, such as a desire to not participate in unfair\ndeals or a desire to punish someone who is behaving unfairly (Kahneman\net al.  1986: S290, see also Forsythe et al. 1994; Güth et al. 1982;\nGüth 1995; Camerer and Thaler 1995).  But this is not the end of the story about the research done to try\nto explain and predict ultimatum game behavior. In the 1990s, a wave\nof new studies began to cast doubt on these fairness\nexplanations. Here are some of the interesting results from this\nresearch: Paul Straub and J. Keith Murnighan (1995) varied the amount of\ninformation given to offerers and responders. Some offerers were told\nthat responders would know the amount that the offerer had to work\nwith, while other offerers were told that responders would not have\nthis piece of information at their disposal when presented with the\noffer. If the fairness hypothesis is correct, this change of\ninformation should not matter. The average offer should be roughly the\nsame in both conditions. But it wasn’t. In the scenario where\nthe starting amount is $10, the mean offer made to responders was\n$4.05 in the complete information condition, but $3.14 in the partial\ninformation condition. When the amount was $80, the means were $30.73\nand $23.00 respectively (Straub and Murnighan 1995: 353). A similar disparity emerged when participants were responders, some\nof whom were told the amount that the offerer had to work with, and\nsome of whom were kept in the dark about this. For the ignorant group,\nthe mean lowest acceptable offer was $1.04, and 29 out of 45\nparticipants accepted an offer of $0.01. For the fully informed $10\ngroup, it was $1.92. For the fully informed $80 group, it was $17.43\n(1995: 351–352). This result, though, need not be out of line\nwith what the fairness models above predict. The troublesome results\nfor fairness models arise here with respect to the offerers, not the\nresponders. Madan Pillutla and J. Keith Murnighan (1995) also varied\npartial versus complete information for offerers, and again found a\nsignificant difference. When the starting amount to be divided was\n$10, the mean offer given partial information was $3.54, whereas for\ncomplete information it was $4.66 (1995: 1415). The two new wrinkles\nwere (i) whether offers would come with a label “this is\nfair” or not, and (ii) whether an independent third party would\nevaluate offers and decide if they were fair or not. Pillutla and\nMurnighan reasoned that if offerers really were motivated\npredominantly by fairness, then these variations should not matter\nsignificantly. But they did. For instance, in the partial information\ncondition, it was already noted that the mean offer was $3.54, but it\ndropped to $2.61 in the fair label variation (1995: 1415). Hence,\n  [t]hey acted as if labeling an offer as fair would\nlead respondents to accept smaller offers—even when the latter\nhad complete information. (1995: 1417)  On the other hand, the mean offer increased all\nthe way to $4.67 in the third-party label variation, with 72% making\n50–50 offers (1995: 1415–1416).  These are effects that\nare easier to explain given a self-interested motivational story,\nthan they are on a fairness motivational story. As the researchers\nnote, “It seems that offerers only made equal offers when it\nwas worth their while to appear fair” (1995: 1416). For responders, Pillutla and Murnighan found that again small\noffers were rejected much more frequently in complete information\nversus partial information conditions. The rejection rate increased\neven more when the information was complete and the offer was labeled\nas unfair by a third party (1995: 1420). Pillutla and Murnighan (1996) and Terry Boles and his\ncolleagues (2000) were among several researchers who introduced the\nvariation of giving outside options to responders, or options which\nthey knew they would receive if they rejected an offer. For instance,\na responder might reject an offer of $1 if he knows that he has an\noutside option of receiving $2 when rejecting an offer. With this new\ntwist, researchers can then introduce additional conditions in which\nofferers know or do not know whether there is an outside option for\nresponders, or what the value of that offer is, or what the range of\nthe offer could be, and so forth. Without getting into all the various\npermutations, one important result that Boles found was that offerers\nmade lower offers when they knew the size of the outside option, which\ndoes not seem to be what fairness would predict (Boles et al. 2000:\n247). Furthermore, Boles introduced a wrinkle whereby offerers could\nsend a message with their offer, allowing them to be deceptive about\nthe size of the allocation they had available to them in partial\ninformation conditions. The game was repeated with the same\nparticipants as offerers and responders for four rounds, and after\neach round, any deceit that offerers used was revealed to\nresponders. It turned out that offerers were deceptive 13.6% of the\ntime (2000: 247). Boles also discovered that when responders felt\ndeceived, in the next round they were much more likely to reject a new\noffer, even at the expense of their self-interest. In other words,\nthey wanted to punish the offender for deceiving them (Boles et\nal. 2000: 249–250). Given these results, a diverse array of more sophisticated\nmotivational accounts has been offered in the recent literature (For\nadditional results and discussion, see also Roth 1995; Kagel et al.\n1996; Pillutla and Murnighan 2003; Camerer 2003; Murnighan and Wang 2016.). One thing they have in common\nis that they are egoistic accounts, appealing in some way to what\nwould advance a person’s self-interest. In the case of offerers,\nhere are some of the motivational proposals that have been made: In the case of responders, here is an example of a recent egoistic\nproposal: \nSmall offers are rejected because of wounded pride. Although\nresponders may appeal to considerations of fairness, this only serves\nas post-hoc confabulation (Straub and Murnighan 1995: 360–361;\nPillutla and Murnighan 2003: 253–254). So we can see a trend here (Roth 1995; Pillutla and Murnighan 1995:\n1424, 2003: 250). Initial hypotheses about participants in ultimatum\ngames relied on relatively simple egoistic motives. Those hypotheses\nwere allegedly disconfirmed by the evidence. New hypotheses were\noffered which appealed to the role of fairness motives. But then these\nhypotheses also were allegedly disconfirmed by additional evidence. So\neven newer hypotheses have now been offered that involve more\nelaborate egoistic motives. Future work will have to establish whether\nthese proposals turn out to be any more plausible in the long run. It turns out that this trend is not specific to ultimatum\ngames—research on dictator games went through the same\nevolution. We said that ultimatum games are relatively simple two\nperson games with an offerer and a responder. In the standard dictator\nsetup, things are even simpler. Call the two people involved the\n“dictator” and the “recipient”, Using our $10\nexample, the dictator is told that she can give any amount between $0\nand $10, inclusive, to another person. And whatever the dictator\ndecides, that is the end of the story. So if the dictator wants to\nkeep $8 and give $2 to the recipient, then that is what each person\nwalks away with. Beginning with the traditional game-theoretic framework, the\nprediction for what participants serving as dictators would do is\nstraightforward—keep all the money (Forsythe et al. 1994: 348;\nDana et al.  2006: 193). Since there is no threat of rejection by the\nrecipient, and since all that matters is promoting one’s\nself-interest, there is every reason to keep all of the money. Or so\none might think. But once again, that is not what most participants actually did. In\nthe same Forsythe study, while 21% gave nothing to the recipient, the\nremaining 79% gave something, with 21% giving an equal amount of the\n$10 (Forsythe et al. 1994: 362). Kahneman also ran a version of a\ndictator game where students in a psychology course could divide $20\nwith an anonymous fellow student, or keep $18 for themselves and give\n$2 to the other student. Strikingly, 76% choose the even division\n(Kahneman et al. 1986: S291). In addition, 74% of participants were\nsubsequently willing to pay $1 to punish an unfair dictator and in the\nprocess reward a fair one, even though this would reduce their own\nmonetary payment in the process (1986: S291). Van Dijk and\nVermunt (2000) found that information asymmetries were not as\nprevalent as in ultimatum games. Specifically, there wasn’t a\nsignificant effect observed based on whether dictators knew that\nrecipients were aware of the total amount dictators had to\nallocate. As they conclude,\n  whereas the dictator game evokes a true concern\nfor fairness, allocators in the ultimatum game only appear to behave\nfairly…. (2000: 19; for a review of results in dictator games,\nsee Camerer 2003; Murnighan and Wang 2016)\n  Overall, according one review of the literature,\ngiving in dictator games amounts to roughly 15–20% of what a\nparticipant receives in the first place (Camerer 2003: 57–58), which\nis less than fair but more than completely self-interested. As a quick\naside, it is worth noting the implicit assumption in much of this\nliterature that a 50/50 split is the fair distribution, whereas\nkeeping most of the money for oneself would be considered\nunfair. Philosophically this might be a contested matter, and there\ncould also be some variability in terms of which cultures tend to\naccept this assumption more so than others. If the same trend holds that we saw in the previous section, then\nwe know to expect that alternative egoistic stories about motivation\nin dictator games will have emerged as a result of more recent results\nchallenging the fairness motivational hypothesis. Here are two\nexamples of such results: a. In the baseline condition of John List’s study (2007), both\npeople in the dictator game setup were given $5 to start with. Then\nthe dictator received another $5 and was given the chance to allocate\nanywhere from $0 to all $5 to the other person. Three additional\nconditions had interesting twists. In the Take ($1) condition, the\nsetup was the same including the chance to allocate money, but now\ninstead there was an option to take $1 from the other person. The Take\n($5) condition, as its name suggests, allowed up to $5 to be taken\nfrom the other person (or $5 allocated, or nothing taken or\nallocated). Finally, in the Earnings condition, everything was the\nsame as Take ($5) except the dictator earned the $10 for performing a\n30-minute task beforehand. Here were the results (List 2007: 487): \nStrikingly, when given the chance to take money from a stranger,\ndictators would less frequently allocate money to the stranger, and\ntheir average allocation would be smaller. Plus if they felt like they\nearned the money as opposed to just being given it by the\nexperimenter, then almost none of the dictators would be willing to\npart with their money, being more inclined to take rather than to\ngive. b. Jason Dana and his colleagues (2006) had dictators make their\nallocation decision using $10. They were told that recipients would be\ninformed about what the dictator knew when making his choice. But\nbefore recipients actually were told anything, dictators were given\nanother choice. They could keep $9 for themselves, $0 would go to the\nrecipient, and crucially, the recipient would not learn anything about\nthe dictator game at all. This exit option would thereby allow dictators to\nkeep a large payoff while also knowing that they had not disappointed\nthe recipient. And the dictator would not have to worry about\nappearing selfish by keeping most of the money (Dana et al. 2006:\n196). Eleven out of 40 dictators (28%) ended up taking this exit option,\nwith 2 of them initially planning to keep the $10 (2006: 197). The\nremaining 9 participants had planned on giving some money to the\nrecipient, and now did not have to. 28%, while an interesting result, is still not that large. The\nmajority of participants both did not exit and gave some of the $10 to\nthe recipient. However, in a second study, Dana did two things. First\nhe replicated the results of his first study, and this time 43% exited\n(2006: 199). But in a new condition, dictators were told at the very\nbeginning that recipients would be oblivious to the dictator\ngame—they would just receive a note that said, “Thank you\nfor your time today. An additional payment of $__ is attached to this\nnote” (2006: 198; Dana did not explain what would happen if the\namount was $0). Thus dictators knew that they could make any offer\nthey wanted to without worrying about how it would be viewed by the\nreceiver. So they chose their allocation, and then were given the $9\nexit option.  And here is the interesting result. Now only 4% of\nparticipants took the exit option, and 54% chose to give $0 and to not\nexit (2006: 199; see also Dana et al. 2007 for additional related\nstudies). As with ultimatum games, a diverse array of more sophisticated\negoistic motivational explanations have been proposed to account for\nthe behavior of dictators. Here are examples of a few of them: A desire to feel good by giving. A desire to not appear selfish, or to appear to be fair. A desire to give what others expect one to give (or not to\ngive). (for an overview, see Dana et al. 2006: 194–195, 200; Dana et al. 2007: 68–69, 78) For what it’s worth, my view is that it is unlikely any one\nof the proposed motives in the ultimatum and dictator literatures is\ngoing to be sufficient by itself to explain the observed patterns of\nbehavior. Nor, to be fair, are they usually intended by researchers to\nbe sufficient either. Rather, motives concerned with good (monetary)\noutcomes for the self are no doubt part of the story. But so too are\nmotives concerned with fairness. And so too are one or more additional\negoistic motives, such as not wanting to appear selfish. A complex,\nmulti-motive model is what is needed going forward (see also Forsythe\net al. 1994: 362; Roth 1995; Güth 1995; Kagel et al. 1996: 102;\nPillutla and Murnighan 2003: 254; Dana et al. 2006: 195; Dana et al. 2007: 78; Murnighan and Wang 2016: 87). Let me end with a cautious note. Recently there have been some doubts raised about the external validity of dictator games, or the extent to which the results generated in these lab studies carry over to non-artificial or natural settings. For instance, Jeffrey Winking and Nicholas Mizer (2013) had a confederate approach an individual at a bus stop in Las Vegas, and “pretended to notice chips in his pocket, stopped briefly and claimed to the participant that he was late for a ride to the airport and asked the individual if he/she wanted the casino chips, which he did not have time to cash in,” and which were worth $20 (2013: 290). Ten feet away with his back turned to the participant was another confederate. The confederate with the chips would also add, “I don’t know, you can split it with that guy however you want” while pointing towards the second confederate (2013: 290). However, in stark contrast to the results earlier, here every single participant kept all $20 for themselves (2013: 291). Several writers have claimed that external validity might be limited because the laboratory results for dictator games could be heavily influenced by experimenter demand effects (Zizzo 2010, 2013, Winking and Mizer 2013. For the past twenty years, the psychologist Daniel Batson and his\ncolleagues have developed a novel experimental approach which closely\nresembles the dictator game. Because this approach has been explored\nusing a number of interesting modifications by the same researcher in\nthe course of constructing a sophisticated motivational story about\nresource allocation, and because his findings have attracted a great\ndeal of interest and attention, it is worth delving into them in some\ndetail (For the studies, see Batson et al. 1997, 1999, 2002, 2003. For\nreviews, see Batson and Thompson 2001 and Batson 2008. For related\nstudies and discussion, see Valdesolo and DeSteno 2007, 2008 and\nWatson and Sheikh 2008. For Batson’s exploration of the implications of this research for moral psychology, see Batson 2015.). Here is the setup Batson typically used. Participants were told\nthat they were part of a task assignment study. Individually, they\nwere given the choice of whether to assign a positive consequences\ntask or a neutral consequences task to either themselves or another\nparticipant, who (they were told) would simply assume the assignment\nwas made by chance. The positive consequences task was such that for\neach correct response, the participant would receive one ticket for a\nraffle with a prize of $30 at the store of his or her choice. In the\nneutral consequences task, there would be no consequences for correct\nor incorrect responses but, “most participants assigned to the\nneutral consequences task find it rather dull and boring”\n(Batson et al. 1997: 1339). After making the assignment privately and\nanonymously, participants were asked about what was the morally right\nway to assign the task consequences, and to rate on a 9-point scale\nwhether they thought the way they had actually made the task\nassignment was morally right. We can see that one of the differences between Batson’s setup\nand a typical dictator game is that here the recipient would not know\nwhere the allocation came from, nor that there was another possible\nallocation that could have been made to him instead. Now such\ninformation is not an essential feature of dictator games; in fact we\njust saw a similar setup in one of Dana’s studies. But in the\nusual configuration dictators typically know that recipients will know\nhow they received their allocation. What ends up happening when participants are placed into this\nsituation? Well, they tend to assign themselves the positive task. Out\nof twenty participants, Batson found (Batson et al. 1997: 1340): Furthermore, only 1 out of the 16 said that assigning oneself to\nthe positive task was morally correct. Yet even these 16 participants\nrated the morality of their assignment in the middle of the 9-point\nscale (4.38), which was significantly lower than the 8.25 rating for\nthe 4 participants who assigned the other participant to the positive\nconsequences task (Batson et al. 1997: 1340). From here Batson performed many additional studies which were\nvariations of this initial setup. Suppose, for instance, that we make\nthe moral norm at work here salient to participants just before they\nmake their assignment by including a statement that,  Most participants feel that giving both people an\nequal chance—by, for example, flipping a coin—is the\nfairest way to assign themselves and the other participant to the\ntasks (we have provided a coin for you to flip if you wish). But the\ndecision is entirely up to you. (Batson et al. 1997: 1341)\n 10 participants flipped and 10 did not. 8 out of 10 in the first\ngroup said flipping was the morally right procedure, and 6 out of 10\nsaid so in the second group. Most importantly, Batson found (Batson et\nal. 1997: 1342): This second result is grossly out of line with what the random\nflipping of a coin would have predicted. At least some of the\nparticipants in the second group must have flipped in a way that went\nin favor of the other person, but still assigned themselves\nthe positive task. Self-interest seemed to have crept into their\ndecision making process in a significant way. Yet, and perhaps most\nsurprisingly, those who flipped rated what they had done much more\nmorally right (7.30 on a 1–9 scale) than those who did not flip\n(4.00) (Batson et al. 1997: 1341). Here is another wrinkle—suppose participants are now given\nthe option to have the experimenter assign them one of the two tasks,\nwhile also knowing what that assignment is going to be ahead of\ntime. We get the following (Batson et al. 1997: 1343): So in this experiment, only 22.5% of participants ended up with the\nneutral task. Yet for the 17 participants in the first group, on\naverage they felt they had acted just as morally right (7.06 on a\n9-point scale) as the 11 participants in the second group (7.91)\n(Batson et al. 1997: 1343). What these results have suggested to Batson is that most of us (at\nleast in these kinds of situations) are disposed towards a kind\nof moral hypocrisy, or appearing to be moral to oneself and\nothers but avoiding the costs of actually being so if one can (try to)\nget away with it. After all, many participants were typically eager to\nflip the coin and report after the fact that this was the morally\nright course of action, but then distorted the process so that the\nresults came out in their favor. Note that it is not the mere fact\nthat they choose the positive consequences task for themselves that is\nhypocritical by itself—there could be cases where a person\nthinks of acts like this as being in line with her self-interest, and\nis not even aware of their moral ramifications. Nor is their hypocrisy\ncaptured here by the additional fact that the participants also seemed\nto believe in the moral principle that flipping the coin is the\nmorally fair way to assign the task consequences. For then we would\njust have a perfectly familiar case of weakness of will, in which you\nbelieve that something is right but fail to be sufficiently motivated\nto do it.  Rather, their hypocrisy arises when they (i) choose the\npositive consequences for themselves, while (ii) seeming to believe\nthat flipping the coin and following what it indicates is morally\ncorrect, and (iii) still claiming to themselves and others to\nhave made the morally right task assignment (For more on\ncharacterizing moral hypocrisy, see Batson et al. 1997:\n1335–1336, 1999: 525–526, 2002: 330; Batson and Thompson\n2001; and Batson 2008, 2011: 222–224.). How is it really possible for these participants to pull off this\ncombination? In particular, it is one thing to appear to be acting\nmorally to others. But these participants are also appearing to be\nmoral to themselves too. How are they able\nto downplay the costs associated with guilt, regret, and hypocritical\nbehavior for acting immorally by going against what they know to be\nright, and also experience the self-rewards for moral behavior? One\npossibility is that the participants in question have come to deny any\nmoral responsibility in this situation, thereby deactivating the moral\nnorm. Or perhaps they have come to deny that they are able to carry\nout the task, or that they are no longer aware of the likely\nconsequences of their actions. But there is no evidence to suggest\nthat these background conditions have failed to apply in these\nparticular cases (Batson 2008: 57). Another possibility is that these\nparticipants have come to think that their behavior is in line with\ntheir moral standards, thereby allowing them to not feel guilt and to\neven perhaps take pride in their behavior. This could be because they\nhave revised the content of the moral principle to create an exception\nclause for this one kind of situation. Or it might be that they lie to\nor deceive themselves about what the principle says in the first\nplace. But the results already cited above cast doubt on these hypotheses\nas well (see in particular Batson et al. 1999: 526; this is not to say\nthat these hypotheses are not accurate in other cases, but the focus\nhere is only on understanding the data generated by Batson’s\nstudies). Batson also introduced another variety of the setup whereby\nthe moral principle is made salient, a coin is provided, and the coin\nis clearly marked on one side with “SELF to POS[ITIVE]”\nand “OTHER to POS” for the opposite side (Batson et\nal. 1999: 527). In those cases when the coin lands on OTHER, it seems\nvery hard to think, in spite of what the moral principle and the coin\nboth say, that a person who still assigns himself to the positive\nconsequences task would take that to be morally\nacceptable. Furthermore, only 2 out of 40 participants reported\nafterwards that the most morally right thing to do is to assign\nthemselves to the positive consequences task (Batson et al. 1999:\n529). Yet of the 28 who chose to flip, Batson found (Batson et\nal. 1999: 528): Furthermore, those who assigned themselves to the positive\nconsequences task after flipping the coin again thought that they were\nbeing highly moral (7.42 on a 9-point scale), while those who made the\nsame assignment without flipping the coin did not (3.90) (Batson et\nal.  1999: 529). So marking the coin and thereby reducing the\nambiguity as to what the fair assignment should have been, did nothing\nto undermine moral hypocrisy. We are still without an explanation for\nhow it seems to work—more on that in a moment. A natural thought here is that this last result does not\ndistinguish between those participants who flip, win, and then rate\nthe morality of their action, versus those who flip, lose, change the\ntask assignment to favor themselves, and then rate the morality of\ntheir action. It could be that the former group (rightly) rates the\nmorality of their action highly, since they are following the fair\nprocedure, whereas the second group rates it low, perhaps around 4.0\nas do those who do not even bother flipping the coin and just assign\nthemselves the positive task. If the results came out this way, then\nwe would have evidence for moral weakness of will, to be sure, but not\nfor moral hypocrisy, since the second group would be honest about the\nmoral failure of their action. In a later study, Batson tested this possibility using secret\nobservations of how each participant who flipped ended up having his\nor her coin fall (Batson et al. 2002: 334–338). Of the 32 who\nflipped the coin, 16 got OTHER to POS or otherwise fiddled the coin\nflip so that it came out in their favor (i.e., repeated flipping). For\nthis group, the mean rated morality of their action was 5.56, which\nwas significantly higher than the 3.89 for the group which assigned\nitself to the positive task without using the coin. On the other hand,\nit was also lower than the 7.45 for the group which flipped, won SELF\nto POS, and assigned themselves to the positive task (2002: 336). The\nupshot is that,  even though the coin had no more effect on their\ndecision than it had on the decision of those who did not claim to use\nthe coin at all, the fiddlers still said they thought the way they\nmade the task assignment decision was more moral. Their sham reference\nto use of the coin seems to have provided sufficient appearance of\nmorality that they could claim to have acted, if not totally morally,\nat least moderately so. (2002: 337) To summarize, the studies mentioned above suggest that participants\ncould have had their relevant fairness norm activated—in this\ncase, the norm that flipping a coin and conforming to its results is\nthe morally right way to behave—and even be motivated to some\nextent to do so. But this motivation must have been so weak that, once\nthe cost/benefit analysis was done on an alternative action of\nsecretly ignoring the coin when it went against the person and instead\nassigning oneself to the positive task, it became fairly easy to\noutweigh or undercut the motivation to do the right thing. So our\nfairness norms look to have little enhancing power after all, or at\nleast in these kinds of cases. But all is not lost for fairness norms. Batson was able to discover\ntwo variables which separately helped to ensure that motivation to be\nfair did win out. The first variable was the introduction of a\nmirror. The setup was as usual with no labeling of the coin, but now a\nmirror was propped up against a wall on the only table where\nparticipants could fill out their forms. For one group of\nparticipants, the mirror was facing them; for the other, it was turned\nto the wall. This ended up making a significant difference (Batson et\nal. 1999: 530): Here for the first time we see results in line with what fairness\nrequires—for those who flipped the coin and were facing the\nmirror, the overall results were as chance would predict. What is the best explanation for the contribution made by the\nmirror?  Batson appeals to research on the psychology of\nself-awareness, in which objects like mirrors can,  heighten awareness of discrepancies between\nbehavior and salient personal standards, creating pressure to act in\naccord with standards. (Batson et al. 1999: 529; for more on the\npsychology of self-awareness, see Wicklund 1975)  In this case, the mirror served to highlight to the person the\ndifference between what he believed was morally correct in this\nsituation, and the opposing temptation to act out of self-interest.\nThis heightened awareness seemed to either create extra motivation to\ncomply with the fairness norm or lessen motivation to do the\nself-interested thing (or perhaps both). Thus the mirror can be said\nto increase the salience of the self’s own personal standards of\nevaluation (Another hypothesis, though, is that it increases the\nsalience of standards of social evaluation, i.e., how others might\njudge him. Batson tested this possibility and did not find support for\nit (2002: 331–334)). Self-awareness also provides a clue about where to find a plausible\nexplanation for how moral hypocrisy is possible. That clue has to do\nwith a particular form of self-deception (Batson et al. 1997: 1336,\n1346, 1999: 526–527.). Rather than thinking that participants\nsimply revised their fairness standards to make their behavior look\nacceptable in their own eyes, perhaps instead they were engaging\n(often unconsciously) in an act of self-deception whereby they avoided\ncomparing their behavior to the relevant moral standards. If the two\nare kept apart from each other, that mitigates the perceived costs of\nnot acting fairly while doing nothing to mitigate the perceived\nbenefits of acting self-interestedly. But with increased\nself-awareness, the discrepancy between the fairness norm and the\nself-interested option was made especially salient so that it became\npsychologically difficult for many participants to employ this\nparticular form of self-deception (Batson et al. 1999: 527, 529,\n531–532, 2002: 331 and Batson and Thompson 2001: 55.). The other variable which Batson found to increase motivation\nassociated with the fairness norms had to do with perspective\ntaking. Take the usual setup, but with the caveat that the default\nstarting point is for the participant to be awarded two raffle tickets\nfor every correct response, while the other participant gets zero\ntickets. Then the task assignment becomes whether the participant is\nwilling to change the assignment so that it is symmetrical with both\npeople receiving one ticket each. For controls, 38% changed the task\nconsequences to symmetrical. However, for the experimental group, 83%\ndid (Batson et al. 2003: 1199). The difference? This group was\ninstructed to adopt the perspective of the other\nperson—“we would like for you to imagine yourself in the\nplace of the other participant” (Batson et al. 2003: 1198,\nemphasis deleted). This is different from imagining what the other\nperson is feeling or experiencing—the instructions here are very\nmuch in line with the Biblical mandate to, “Do unto others as\nyou would have them do unto you” (Matthew 7:12.). There is one additional implication from Batson’s work which\nI want to highlight here. Consider again those participants who are\nabout to flip the coin. At that point before they see what the outcome\nis, what is the nature of their motivation? Perhaps for the moment at\nleast some of them want to do the morally right thing (follow the\ndictate of the coin, however it ends up landing) for its own sake. If\nthe coin lands in their favor, then the outcome also aligns with their\nself-interest, which is so much the better. But if it lands in the\nother person’s favor, then they might see what being fair would\ncost them, and their self-interested motives end up outweighing their\ninitial moral motivation. Or perhaps this is all\nfanciful—perhaps when they are flipping the coin, all they want\nis what they think is ultimately in their self-interest, which at the\nmoment is just flipping the coin so as to benefit from appearing to be\nmoral. One way to test these hypotheses, Batson reasoned, is to see if\nparticipants cared about whether the flipping of the coin and so the\ntask assignment was done by themselves or by the experimenter. If the\negoistic hypothesis is correct, then they should want to flip the coin\nthemselves so that they can rig the outcome. If the other hypothesis\ninvolving an ultimate desire to be moral is correct, however, it\nshould not matter who flips. When the experiment was actually run with\nthis choice option, 80% of those who used a coin wanted the\nexperimenter to flip it. This initial evidence thus favors postulating\na motive to follow the fairness norm (and perhaps moral norms more\ngenerally) for its own sake (Batson and Thompson 2001:\n55–56.). Suppose this hypothesis about motivation is correct. How strong and\npsychologically powerful of a force does it typically seem to be in\ncases involving fairness? The evidence suggests, at least given the\ncurrent state of research, that for many of us it is only weak in\nstrength. We can already see this from the studies cited above when so\nmany participants do not actually follow their moral principle about\nwhat is a fair task assignment. In addition, Batson varied the\nprevious setup so that the assignment was between a positive and\na negative task, where the latter involved receiving\n“mild but uncomfortable” electric shocks for each\nincorrect response.  With this change, only 25% of participants\noffered to let the experimenter flip the coin, and another 25% flipped\nthemselves, with 91% choosing the positive task. The remaining 50% of\nparticipants simply bypassed the pretense of the coin flip and gave\nthemselves the positive task while readily admitting that this was not\nmorally right (Batson and Thompson 2001: 56.). The implication is that\nmoral motivation caused by our fairness norms seems to be weak and\nhighly susceptible to being outweighed in cases of this type where the\nperson’s self-interest is at stake. Having reviewed this extensive research project by Batson, one\nnatural direction some philosophers might go in next is to examine\nwhat it suggests about people’s moral character. This is\nespecially true for philosophers engaged in recent discussions of\nsituationism and the empirical reality of virtue and vice (Harman\n1999, 2000; Doris 1998, 2002; Snow 2010; Miller 2014). Given the focus of this essay, we will only look at the virtue of\nfairness. It seems clear that Batson’s results do not inspire\nmuch confidence in this virtue being widely held. For instance,\nsomething like the following seems plausible: But time and again in Batson’s studies, the participants\nrigged things so that they received the better outcome. Admittedly, to\ncount against (1), we have to assume that this was an unfair\ndistribution, which may be controversial. But most are likely to grant\nthat assumption, and the participants themselves did not appear to\nbelieve that what they did was the fair thing. Similarly, this seems to be true of the virtue of fairness: To clarify, this is not a point about being honest rather than\nlying about one’s behavior. Rather, the point with this criteria\nis that a fair person would likely not be deceived or be of two minds\nabout the unfairness of his behavior in such instances. He would\nclearly recognize and acknowledge that his behavior was unfair. But\nagain the participants did not tend to live up to this fairness\nstandard. As we saw, in one study those who flipped and assigned\nthemselves the positive consequence task rated what they had done as\nhighly moral (7.30 on a 1 to 9 scale), and their rating was much\nhigher than for those who assigned themselves to this task without\nflipping (4.00) (Batson et al. 1997: 1341). There are other respects in which participants failed to live up to\nthe standards of fairness, but the picture is not all bleak. In fact,\nthere are many ways in which participants did not conform to the\nstandards of the vice of unfairness either. Here is one: But we saw that Batson’s participants did seem to genuinely\nbelieve that the fair thing to do was to flip the coin and make the\nappropriate assignment. True, this belief did not have a significant\nmotivational role to play in some contexts, but in others it did, such\nas when taking the perspective of the other person. This suggests another respect in which most people, if they are\nlike Batson’s participants, would fall short of being vicious in\nthis context: Yet the experimental manipulations Batson introduced involving a\nmirror and the instructions to imagine yourself in the place of the\nother person, seemed to eliminate the unfair behavior altogether. To note one more interesting conflict with unfairness, consider the\nmotivational picture which emerged in the last few studies that were\nreviewed concerning fairness norms. Batson’s results suggested\nthat participants were motivated to do the morally right/fair thing,\nbut when the flip of the coin went against them, this dutiful\nmotivation would often be outweighed by self-interested\nmotivation. Yet on a traditional Aristotelian picture of the\nvices: This might be too strong a characterization of an unfair person in an ordinary sense, but again we are focusing on the viciously unfair person. Yet it turned out that the participants seemed to be experiencing motivation to do the right thing. Furthermore, if they were motivationally conflicted but gave in to self-interested motivation, they would be weak of will or incontinent on the Aristotelian picture, rather than vicious. So one could argue Batson’s results do not sit comfortably\neither with the widespread possession of the virtue of fairness or the vice of\nunfairness. Instead they suggest that we have beliefs and desires like\nthese: While certainly not an exhaustive list, these mental states do seem\nlike a mixed bag, morally speaking. Some of them are quite morally\nadmirable, such as those in the first set, and by themselves could\ngive rise to positive moral behavior. Others, of course, are not\nmorally admirable, and they can help to explain the unfair behavior we\nsee in Batson’s studies (For the development of a Mixed Trait\napproach to character along these lines, see Miller 2013, 2014,\n2015). Here is a general observation about the results we have seen in\nthis essay. For each of the studies, there was rarely uniformity in\nbehavior by all the participants in a particular situation. Indeed, in\nsome cases there were striking differences. For instance, we saw that\nin the Forsythe study of dictator games, 21% gave nothing and 21% gave\nthe equal amount of $10, leaving 58% somewhere in-between (Forsythe et\nal.  1994: 362). We also saw that many people would pay $9 to take an\nexit option in Dana’s dictator studies, but many people also\nwould not (Dana et al. 2006). Or in Batson’s studies, many\nparticipants would assign themselves to the positive consequence task,\nbut rarely would everyone do this. So there appear to be important individual differences in behavior\namong participants in these game environments. Since the situation a\ngiven group of participants is confronting is the same, it is only\nnatural to think that the ensuing differences in behavior might be\nexplained, at least in part, by differences in their underlying\npersonality traits. If this turns out to be the case, then if we learn\nsomething about their personality, we could more accurately predict\nhow they would subsequently behave in these games, as well as in other\nrelevant situations. In other words, individual differences in the\nmoral psychology of a person’s justice relevant traits, can\ntranslate into individual differences in justice relevant\nbehavior. And knowing something about the former can help us predict\nthe latter. It turns out that there is in fact some evidence linking certain\npersonality traits with behavior in these games. Here I briefly\nmention three such instances where evidence has been found. Spending a\nmoment on each of them is also worthwhile because of what we can learn\nmore generally about the moral psychology of justice beyond what\neconomic games tell us (for a massive meta-analysis of 60 years of research on the relationship between personality traits and behavior in six economic games, including the dictator and ultimatum games, see Thielmann et al. 2020). The Big Five. The Big Five personality traits (or\nFive-Factor model) have come to dominate the field of personality,\nwith thousands of relevant papers appearing in just the past five\nyears (for an overview, see John et al. 2008). Results from these\nstudies have repeatedly pointed in the direction of five basic\ndimensions of personality with the following most commonly used\nlabels: The idea, then, is that in a typical group there will be people who\ndiffer in their ratings on each of these five dimensions. Some, for\ninstance, might be high on extraversion, which can be interpreted as\ninvolving an energetic approach towards social interaction manifested\nin, for instance, the behavior of attending more parties and\nintroducing themselves to strangers (John et al. 2008: 120). Others\nmight be quite introverted instead. Advocates of this approach typically have hierarchical models of\npersonality traits in mind, where the Big Five are subdivided into\ndifferent “facets” that are less broad and so are claimed\nto have increased accuracy (for details, see Paunonen 1998). To cite\none example, here are the 30 facets from Robert McCrae and Paul\nCosta’s version of the Five-Factor Model (Costa and McCrae 1995:\n28): In their 240 item survey instrument, the NEO-PI-R, 8 items are\ndesigned to measure each of these facets. For instance, “I keep\nmy belongings neat and clean” and “I like to keep\neverything in its place so I know just where it is” are two\nitems for the consciousness facet of order (Costa and McCrae 1992:\n73). It is noteworthy that there is nothing on the list of Big Five\ntraits or their facets that seems directly related to justice. But in\nrecent years connections have been established between performance on\neconomic games and the Big Five (for a review, see Zhao and Smillie\n2015). The most pronounced finding among the Big Five traits is that\nhigher agreeableness is correlated with higher allocations to the\nrecipient in a dictator game. In one meta-analysis, the sample\nweighted average correlation was \\(r_{wa} = .18\\). A negative\ncorrelation of \\(r_{wa} = -.10\\) was also found for rejecting fewer\noffers in ultimatum games (Zhao and Smillie 2015:\n288). Conscientiousness has also been linked to lower allocations in\ndictator games (Ben-Ner et al.  2004). Studies on the other Big Five\ntraits have been more inconsistent in their results, and are also few\nand far between. The HEXACO. Michael Ashton and Kibeom Lee have proposed a\nsix-factor model of personality (Ashton & Lee 2001, 2005 and Lee\n& Ashton 2004). Five of the factors carry over with minor\nmodifications from the Big Five taxonomy. The key addition is a sixth\nfactor which they call honesty-humility, and which they describe this\nway:  Persons with very high scores on the Honesty-Humility\nscale avoid manipulating others for personal gain, feel little\ntemptation to break rules, are uninterested in lavish wealth and\nluxuries, and feel no special entitlement to elevated social\nstatus. Conversely, persons with very low scores on this scale will\nflatter others to get what they want, are inclined to break rules for\npersonal profit, are motivated by material gain, and feel a strong\nsense of self-importance. (Lee & Ashton 2015, Other Internet\nResources). Honesty-humility in turn has four facets: sincerity, fairness,\ngreed avoidance, and modesty. Most relevant for our purposes is\nnaturally the fairness facet. As part of the 100-item HEXACO-PI-R\ninventory, the following are the items which are labeled under\nfairness: Now one might wonder about whether these are really best understood\nas fairness items. They might seem to relate at the\nconceptual level more to honesty than to justice. But let’s\nleave that terminological point to one side. For it turns out that significant statistical relationships have\nbeen found with behavior in bargaining games (for a review, see Zhao\nand Smillie 2015). In the same meta-analysis, for instance, the sample\nweighted average correlation was \\(r_{wa} = .24\\) between dictator\nallocation amount and honesty-humility (291). Another meta-analysis\nreported a correlation of \\(r_{wa} = .29\\) (Hilbig et al. 2015:\n92). So too was there a negative correlation between HEXACO\nagreeableness and rejecting offers in ultimatum games (\\(r_{wa}=\n-.16\\)) (Zhao and Smillie 2015: 288). Furthermore, one study found\nthat of those who were high in honesty-humility, 64.4% chose the 50/50\nallocation in the dictator game, while only 34.6% did who were low on\nthis dimension (Hilbig et al. 2015: 92). Benjamin Hilbig and Ingo Zettler (2009) provide a nice experimental\ndemonstration of this relationship. Participants completed the\nHEXACO-PI and played a dictator game and an ultimatum game with an\ninitial allocation of 100 points for each game. As expected, those who\nwere higher in honesty-humility gave fewer points to themselves in\nboth games (518). More interesting was the consistency in\nallocations between the two games. Those lowest in\nhonesty-humility allocated an almost equal amount of points to\nthemselves and the other person in the ultimatum game. But in the\ndictator game, where there was no threat of rejection by the other\nperson, the average allocation for participants to keep was almost 80\npoints. By way of contrast, those highest in honesty-humility showed\nno significant difference in their allocations in the two games. In\nother words, they were consistently fair, whereas the other group of\nparticipants was only being fair when it was to their advantage\n(518–519). Justice Sensitivity. A relatively new and particularly\nexciting development in personality psychology has been the study of\nwhat is being called “justice sensitivity”. The basic idea\nis that individuals differ in general on this trait, which can be\nunderstood at its heart as a motive for justice (Stavrova and\nSchlösser 2015: 3). After several refinements, consensus seems to be\nemerging around four facets for this trait (Schmitt et al. 2010;\nStavrova and Schlösser 2015): JSVictim: Sensitivity to becoming a victim of\ninjustice. JSObserver: Sensitivity to witnessing injustice. JSBeneficiary: Sensitivity to passively benefiting from\ninjustice. JSPerpetrator: Sensitivity to actively committing\ninjustice.  (Stavrova and Schlösser 2015: 3) Three of these—observer, beneficiary, and perpetrator\nsensitivity—are other-focused aspects of justice sensitivity,\nwhereas victim sensitivity is self-focused on one’s being\ntreated fairly (Stavrova and Schlösser 2015: 3; Schlösser et al. 2018: 76–77). This makes a\ndifference statistically. For instance, victim sensitivity tends to\ncorrelate with anti-social personality traits—such as\nMachiavellianism—in a way that the first three do not. But all four of these facets are indeed statistically related to\neach other, and at the same time are distinguishable pieces of a\nstable trait of justice sensitivity. While they do correlate with\nother personality traits such as the Big Five, the correlations are\ntypically on the low side, which can be taken as evidence that justice\nsensitivity itself is a distinct trait (for a review, see Schmitt et\nal. 2010).  A well validated measure of justice sensitivity has been developed\nby Manfred Schmitt and his colleagues, with 10 items for each of the\nfour facets (Schmitt et al. 2010). For instance, here are the items\nfor beneficiary sensitivity: Participants complete these items using a scale which ranges from 0\n(not at all) to 5 (exactly). Using this measure of justice sensitivity, some interesting work is\nbeing done. For instance, Gollwitzer and his colleagues (2005)\nexamined the willingness of West Germans to contribute to East Germany\nto help improve living conditions, as there is a large disparity even\nyears after the wall came down. Some particular proposals included an\nautomatic deduction from the salaries of West Germans that would be\nused for job creation in East Germany, and the implementation of\naffirmative action hiring policies which would favor East over West\nGermans for new positions. It turned out that beneficiary sensitivity,\nfor instance, was found to predict solidarity with East Germans,\nwhereas victim sensitivity did not (Gollwitzer et al. 2005: 191). In another study, Thomas Schlösser and his colleagues (2018) examined whether justice sensitivity predicts wealth distribution. They used a version of the welfare state game in which participants were assigned to be an upper, middle, or lower class subject in a hypothetical society which they got to help shape. Specifically they chose between a society with lower inequality (person A received €5, B received €4, and C received €3) versus higher overall wealth but greater inequality (A received €10, B received €6, and C received €1). As expected, participants who were high on JSVictim (sensitivity to becoming a victim of injustice) were more likely to go with greater inequality if they were A or B, but greater equality if they were C. Participants who were high on the other three dimensions of justice sensitivity were more likely to choose less inequality regardless of whether they were A, B, or C (Schlösser et al. 2018: 79). Most relevant for our purposes, participants who are high on victim\nsensitivity are less likely to allocate 50/50 in dictator games,\nwhereas the opposite is true for the other facets of justice\nsensitivity. Hence Detlef Fetchenhauer and Xu Huang (2004) found that\nobserver sensitivity correlated .21 with an equal allocation, and\nperpetrator sensitivity correlated .30. On the other hand, victim\nsensitivity was negatively correlated at −.18 (Fetchenhauer and\nHuang 2004: 1024). Further research in this area of the moral psychology of justice\nlooks to be especially promising in the years to come (for additional\nnoteworthy studies, see Dalbert and Umlauft 2009; Lotz et al 2013;\nEdele et al. 2013). At this point, it is tempting to speculate about what some of the\nbroader implications might be of these empirical findings. For\ninstance, what can they tell us about how to design a just society? If\nwe should assume that most of us suffer from moral hypocrisy to enough\nof an extent that we are not fair people, does that support more\nactive monitoring by the state to detect unfair behavior? How might we\npractically implement the findings about mirrors (should they be\nstrategically placed around the public square to encourage fair\nbehavior?) and empathy (should there be regular public reminders to\nimagine what others are going through who are less fortunate?)? And\nwhat other lessons can we extrapolate from this research to help us\nimprove society? This is not the place to explore these questions, but\nthey are a natural next step. Of course, allocation decisions in real life tend to be much more complicated\nthan simple dictator or ultimatum games. But as we have seen, these\ngames and their many different variations can shed important light on\nthe moral psychology of justice. Clearly no simple story about our\nmotives to be just or fair is going to be plausible. We are more\ncomplex creatures than that.","contact.mail":"millerc@wfu.edu","contact.domain":"wfu.edu"}]
