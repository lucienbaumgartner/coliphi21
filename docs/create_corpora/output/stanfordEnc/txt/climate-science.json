[{"date.published":"2018-05-11","url":"https://plato.stanford.edu/entries/climate-science/","author1":"Wendy Parker","entry":"climate-science","body.text":"\n\n\nClimate science investigates the structure and dynamics of\nearth’s climate system. It seeks to understand how global,\nregional and local climates are maintained as well as the processes by\nwhich they change over time. In doing so, it employs observations and\ntheory from a variety of domains, including meteorology, oceanography,\nphysics, chemistry and more. These resources also inform the\ndevelopment of computer models of the climate system, which are a\nmainstay of climate research today. This entry provides an overview of\nsome of the core concepts and practices of contemporary climate\nscience as well as philosophical work that engages with them. The\nfocus is primarily on epistemological and methodological issues that\narise when producing climate datasets and when constructing, using and\nevaluating climate models. Some key questions and findings about\nanthropogenic climate change are also discussed. \n\nThe field of climate science emerged in the second half of the\ntwentieth century. Though it is sometimes also referred to as\n“climatology”, it differs markedly from the field of\nclimatology that came before. That climatology, which existed from the\nlate-nineteenth century (if not earlier), was an inductive science, in\nmany ways more akin to geography than to physics; it developed systems\nfor classifying climates based on empirical criteria and, by the\nmid-twentieth century, was increasingly focused on the calculation of\nstatistics from weather observations (Nebeker 1995; Edwards 2010;\nWeart 2008 [2017, Other Internet Resources]; Heymann & Achermann\nforthcoming). Climate science, by contrast, aims to explain and\npredict the workings of a global climate system—encompassing the\natmosphere, oceans, land surface, ice sheets and more—and it\nmakes extensive use of both theoretical knowledge and mathematical\nmodeling. In fact, the emergence of climate science is closely linked\nto the rise of digital computing, which made it possible to simulate\nthe large-scale motions of the atmosphere and oceans using fluid\ndynamical equations that were otherwise intractable; these motions\ntransport mass, heat, moisture and other quantities that shape\nparadigmatic climate variables, such as average surface temperature\nand rainfall. Today, complex computer models that represent a wide\nrange of climate system processes are a mainstay of climate\nresearch. \nThe emergence of climate science is also linked to the issue of\nanthropogenic climate change. In recent decades, growing concern about\nclimate change has brought a substantial influx of funding for climate\nresearch. It is a misconception, however, that climate science\njust is the study of anthropogenic climate change. On the\ncontrary, there has been, and continues to be, a significant body of\nresearch within climate science that addresses fundamental questions\nabout the workings of the climate system. This includes questions\nabout how energy flows in the system, about the roles of particular\nphysical processes in shaping climates, about the interactions that\noccur among climate system components, about natural oscillations\nwithin the system, about climate system feedbacks, about the\npredictability of the climate system, and much more. \nThis entry provides an overview of some of the core concepts and\npractices of contemporary climate science, along with philosophical\nwork that engages with them. Thus far, most of this philosophical work\nhas focused on the epistemology of climate modeling and\nclosely-related topics.\n Section 2\n introduces a few basic concepts: climate system, climate and climate\nchange.\n Section 3\n turns to climate data, highlighting some of the complexities and\nchallenges that arise when producing three important types of climate\ndataset.\n Section 4\n focuses on climate modeling, briefly describing some of the main types\nof climate model employed today and then considering in more detail\nthe construction, use and evaluation of complex global climate models.\nFinally,\n Section 5\n discusses research in climate science that addresses the issue of\nanthropogenic climate change, as well as several recent controversies\nrelated to this research; it also provides some pointers to the large\nliterature that has emerged on ethics and climate change. \nOn a standard characterization, Earth’s climate system\nis the complex, interactive system consisting of the atmosphere,\nhydrosphere, cryosphere, lithosphere and biosphere (IPCC-glossary:\n1451). Some definitions appear to be narrower, limiting the scope of\nthe climate system to those aspects of the atmosphere, hydrosphere,\netc. that jointly determine the values of paradigmatic climate\nvariables, such as average surface temperature and rainfall, in\nresponse to external influences (see, e.g., American Meteorological\nSociety 2017a). In either case, it would seem that some human\nactivities, including those that release greenhouse gases to the\natmosphere and those that change the land surface, are part of the\nclimate system. In practice, however, climate scientists often\nclassify such human activities as external influences, along with\nvolcanic eruptions (which eject reflective aerosols and other material\ninto the atmosphere) and solar output (which is the primary source of\nenergy for the system) (see IPCC-glossary: 1454). Classifying human\nactivities as external to the climate system seems to be a pragmatic\nchoice—it is easier, and a reasonable first approximation, to\nrepresent anthropogenic greenhouse gas emissions as exogenous\nvariables in climate models—though it may also reflect a deeper\nambivalence about whether humans are part of nature.  \nA climate is a property of a climate system, but there are\ndifferent views on what sort of property it is. According to what might be\ncalled actualist views, a climate is defined by actual\nconditions in the climate system. Narrower actualist definitions,\nwhich have their roots in climatology (see\n Section 1),\n reference weather conditions only. For example, the climate of a\nregion is often defined as its average weather conditions, or the\nstatistical distribution of those conditions, when long time periods\nare considered (Houghton 2015: 2; IPCC-glossary: 1450). The standard\nperiod of analysis, set by the World Meteorological Organization, is\n30 years, defining a “climate normal” for a region. Given\nrecent rates of change in climate system conditions, however, some\nclimate scientists suggest that climate normals should now be defined\nover much shorter periods (Arguez & Vose 2011). Broader actualist\ndefinitions of climate reference conditions throughout the climate\nsystem. The Intergovernmental Panel on Climate Change (IPCC), for\nexample, gives a broader definition of climate as “the state,\nincluding a statistical description, of the climate system”\n(IPCC-glossary: 1450). These broader definitions emerged in the second\nhalf of the twentieth century in connection with increased efforts to\nunderstand, with the help of physical theory, how climates in the\nnarrower sense are maintained and changed via atmospheric, oceanic and\nother processes. These efforts in effect marked the emergence of\nclimate science itself (see\n Section 1). \nA number of other definitions of “climate” are what Werndl\n(2016) calls model-immanent: they reference conditions\nindicated by a model of the climate system, rather than actual\nconditions. For example, taking a dynamical systems perspective,\nclimate is often identified with an attractor of the climate system,\nthat is, with the conditions represented by an attractor of a\n(perfect) mathematical model of the climate system (Palmer 1999; Smith\n2002). Roughly speaking, an attractor is a set of points to which the\nvalues of variables in a dynamical model tend to evolve from a wide\nrange of starting values. Actual conditions in the climate system over\na finite period are then understood to be a single realization from\nthe set of possibilities represented by the attractor. A practical\ndrawback of this way of thinking about climate is that the\ndistribution of conditions in a given finite time period need not\nresemble those represented by an (infinite-time) attractor (Smith\n2002; Werndl 2016). More fundamentally, standard (autonomous)\ndynamical systems theory does not apply to models of the climate\nsystem in which external factors like solar output and greenhouse gas\nconcentrations are changing significantly with time, as they are for\nthe real climate system; how to apply the resources of non-autonomous\ndynamical systems theory to the climate system is a current area of\nresearch (e.g., Chekroun et al. 2011; Drόtos et al. 2015). \nWerndl (2016) provides a critical, philosophical examination of\nseveral definitions of climate and proposes a novel, model-immanent\ndefinition on which a climate is a distribution of values of climate\nvariables over a suitably-long but finite time period, under a regime\nof external conditions and starting from a particular initial\nstate. Roughly speaking, a regime is a pattern of variation in\nexternal conditions that has an approximately constant mean value over\ndifferent sub-periods of a chosen time period. This definition is\nmodel-immanent because it refers to the distribution of conditions\nthat would obtain (i.e., the conditions that a perfect model of the\nclimate system would indicate) if the climate system were subjected to\na regime for a suitably-long time period; this distribution defines\nthe climate of periods in which that regime actually obtains, even if\nthose periods are relatively short. Werndl’s definition avoids\nmany of the problems that she finds with other definitions, but still\nsome questions remain, including how to employ the regime concept when\nexternal conditions are changing rapidly. \nDefinitions of climate change are closely related to\ndefinitions of climate. For instance, scientists subscribing to the\nclimate-as-attractor view might characterize climate change as the\ndifference between two attractors, one associated with a set of\nexternal conditions obtaining at an earlier time and one associated\nwith a different set obtaining at a later time. By contrast, a\ndefinition of climate change associated with a narrower, actualist\nview is:  \nany systematic change in the long-term statistics of climate elements\n(such as temperature, pressure, or winds) sustained over several\ndecades or longer. (American Meteorological Society 2017b)  \nThe latter definition, unlike the former, allows that climate change\nmight occur even in the absence of any changes in external conditions,\nas a result of natural processes internal to the climate system (e.g.,\nslowly-evolving ocean circulations). That is, the latter definition\nallows that climate change can be a manifestation of internal\nvariability in the climate system. \nThe notion of internal variability, and various other concepts in\nclimate science, also raise interesting questions and problems, both\nconceptual and empirical (Katzav and Parker forthcoming). Thus far, however, the foundations of\nclimate science have remained largely unexplored by philosophers. \nThe sources and types of observational data employed in climate\nscience are tremendously varied. Data are collected not only at\nland-based stations, but also on ships and buoys in the ocean, on\nairplanes, on satellites that orbit the earth, by drilling into\nancient ice at earth’s poles, by examining tree rings and ocean\nsediments, and in other ways. Many challenges arise as climate\nscientists attempt to use these varied data to answer questions about\nclimate and climate change. These challenges stem both from the\ncharacter of the data—they are gappy in space and time, and they\nare obtained from instruments and other sources that have limited\nlifespans and that vary in quality and resolution—and from the\nnature of the questions that climate scientists seek to address, which\ninclude questions about long-term changes on a global scale. \nTo try to overcome these challenges, climate scientists employ a rich\ncollection of data modeling practices (Edwards 2010; Frigg et\nal. 2015b). These are practices that take available observations and\napply various procedures for quality control, correction, synthesis\nand transformation. Some of these practices will be highlighted below,\nin the course of introducing three important types of climate dataset:\nstation-based datasets\n (Section 3.1),\n reanalyses\n (Section 3.2),\n and paleoclimate reconstructions\n (Section 3.3).\n Because of the extensive data modeling involved in their production,\nthese datasets are often referred to as data products. As the\ndiscussion below will suggest, an interesting feature of data modeling\npractices in climate science is that they tend to be dynamic and\niterative: data models for nearly the same set of historical\nobservations are further and further refined, to address limitations\nof previous efforts. \nThe weather and climate conditions that matter most immediately to\npeople are those near the earth’s surface, where they live.\nCoordinated networks of land-based observing stations—measuring\nnear-surface temperature, pressure, precipitation, humidity and\nsometimes other variables—began to emerge in the mid-nineteenth\ncentury and expanded rapidly in the twentieth century (Fleming 1998:\nCh.3). Today, there are thousands of stations around the world making\ndaily observations of these conditions, often overseen by national\nmeteorological services. In recent decades, there have been major\nefforts to bring together records of past surface observations in\norder to produce long-term global datasets that are useful for climate\nchange research (e.g., Menne et al. 2012; Rennie et al. 2014). These\nongoing efforts involve international cooperation as well as\nsignificant “data rescue” activities, including imaging\nand digitizing of paper records, in some cases with the help of the\npublic. \nObtaining digitized station data, however, is just the first step. As\nEdwards (2010: 321) emphasizes, “…if you want global\ndata, you have to make them”. To construct global temperature\ndatasets that are useful for climate research, thousands of station\nrecords, amounting to millions of individual observational records,\nare merged, subjected to quality control, homogenized and transformed\nto a grid. Records come from multiple sources, and merging\naims to avoid redundancies while maximizing comprehensiveness in\nstation coverage (Rennie et al. 2014). Procedures for quality\ncontrol seek to identify and remove erroneous data. For example,\nproduction of the Global Historical Climate Network—Daily\n(GCHN-Daily) database involves 19 automated quality assurance tests\ndesigned to detect duplicate data, climatological outliers and\nspatial, temporal and internal inconsistencies (Durre et al. 2010).\nHomogenization attempts to remove jumps and trends in station\ntime series that are due to non-climatic factors, e.g., because an\ninstrument is replaced with a new one, a building is constructed\nnearby, or the timing of observations changes. Homogenization methods\nrely on station metadata, when it is available, as well as physical\nunderstanding and statistical techniques, such as change-point\nanalysis (Costa & Soares 2009). Finally, for many purposes, it is\nuseful to have datasets that are gridded, providing\ntemperature values at points on a grid, where each point is associated\nwith a spatial region (e.g., 2° latitude × 2°\nlongitude). Transforming from a collection of stations to a grid\ninvolves further methodological choices: which stations should\ninfluence the value assigned to a given grid point, what to do if the\nassociated region contains no reporting stations over a period, etc.\nIn practice, scientific groups make different methodological choices\n(Hartmann et al. 2013). \nGridded station-based datasets for temperature, precipitation and\nother variables have been developed (e.g., Harris et al. 2014).\nSurface temperature datasets have attracted particular attention,\nbecause of their role in efforts to quantify the extent of recent\nglobal warming. Three prominent sources for such temperature datasets\nare NASA’s Goddard Institute for Space Studies (GISS), the\nUniversity of East Anglia’s Climatic Research Unit (CRU) and the\nU.S. National Centers for Environmental Information (NCEI).\nPeriodically, these groups develop new versions of their datasets\nreflecting both the acquisition of additional data as well as\nmethodological innovation, e.g., addressing additional sources of\ninhomogeneity (see Hansen et al. 2010; Jones et al. 2012; Lawrimore et\nal. 2011). Despite the many different methodological choices involved\nin producing these datasets, there is good agreement among analyses of\ntwentieth century global land surface temperature changes derived from\nthem, especially for the second half of the twentieth century.\nNevertheless, climate contrarians have expressed concern that these\nanalyses exaggerate late twentieth-century warming. This recently\nmotivated a fourth, independent analysis by the non-governmental\norganization Berkeley Earth; drawing on a much larger set of station\nrecords and using a quite different geostatistical methodology for\nhandling inhomogeneities, their analysis confirmed the twentieth\ncentury global warming seen in other datasets (Rohde et al. 2013). \nIn situ observations of conditions away from earth’s\nsurface are much less plentiful than surface observations.\nRadiosondes, which are balloon-borne instrument packages that ascend\nthrough the atmosphere measuring pressure, temperature, humidity and\nother variables, are now launched twice daily at hundreds of sites\naround the world, but they do not provide even, global coverage\n(Ingleby et al. 2016). Satellite-borne instruments can provide global\ncoverage—and are of significant value in the study of weather\nand climate—but they do not directly measure vertical profiles\nof key climatological variables like temperature; approximate values\nof these variables must be inferred in a complex way from radiance\nmeasurements. A recently-established system of several thousand ocean\nfloats, known as Argo, takes local profiles of temperature\nand salinity in the top 2000 meters of the ocean roughly every 10 days\n(Riser et al. 2016). None of these data sources existed a century ago,\nthough observations of conditions away from earth’s surface were\nsometimes made (e.g., using thermometers attached to kites). \nOne way to remedy spatial and temporal gaps in observations is to\nperform statistical interpolation. Beginning in the 1990s, however,\nclimate scientists also began to employ a different type of\nmethodology, known as data assimilation (Kalnay 2003). First\ndeveloped in the context of weather forecasting, data assimilation\nestimates the three-dimensional state of the atmosphere or ocean using\nnot only available observations but also one or more forecasts from a\nphysics-based simulation model. The forecast constitutes a first-guess\nestimate of the atmosphere or ocean state at the time of interest;\nthis is updated in light of observational data collected around that\ntime. In daily weather forecasting, the resulting best estimate is\nknown as the analysis, and it provides initial conditions for\nweather prediction models. To produce long-term datasets for climate\nresearch, data assimilation is performed iteratively for a sequence of\npast times (e.g., every 12 hours over several decades), producing a\nretrospective analysis or reanalysis for each time in the\nsequence (Bengtsson & Shukla 1988; Edwards 2010). \nAtmospheric reanalysis datasets are in heavy use in climate research,\nbecause they provide complete gridded data at regular time steps over\nlong time periods, both at the surface and above, for a wide range of\nvariables, including ones that are difficult to measure with\ninstruments. Many reanalysis datasets cover a few decades of the\ntwentieth century and are produced using as much of the available\nobservational data as is feasible (see Dee et al. 2016 for an\noverview); when satellite data are available, this can amount to\nmillions of observations for each analysis period in the sequence. An\ninteresting exception is NOAA’s 20th Century\nReanalysis (20CR) project: it covers the entire twentieth century and\nassimilates only surface pressure observations and observed monthly\nsea surface temperatures and sea ice distribution (Compo et al. 2011).\n20CR is not informed by any temperature observations over land, yet\nits results indicate twentieth century global warming over land\nsimilar to that seen in station-based temperature datasets. \n20CR has been described as providing independent\n“observational” confirmation of the twentieth century\nwarming seen in station-based datasets (Compo et al. 2013). More generally, despite the\ncentral role of computer forecasts in reanalysis, many climate\nscientists refer to reanalysis data as “observations” and\nuse them as such: to investigate climate system dynamics, to evaluate\nclimate models, to find evidence of the causes of recent climate\nchange, and so on. Other climate scientists, however, emphasize that\nreanalyses should not be confused with “real” observations\n(e.g., Schmidt 2011). Parker (2017) argues that differences between\ndata assimilation and traditional observation and measurement are not\nas great as one might think; she suggests that data assimilation can\nbe understood as a complex measuring procedure that is still under\ndevelopment. \nClimate scientists are also interested in climates of the distant\npast, before the advent of meteorological instruments. These\npaleoclimatic investigations rely on proxies: aspects of the\nnatural environment that are “interpreted, using physical and\nbiophysical principles, to represent some combination of\nclimate-related variations back in time” (IPCC-glossary: 1460).\nFor example, variations in the ratio of different isotopes of oxygen\nin deep ice cores and in the fossilized shells of tiny animals serve\nas proxies for variations in temperature. Additional proxies for\nclimate-related variables come via tree rings, corals, lake sediments,\nboreholes and other sources (PAGES-2K-Consortium 2013; Masson-Delmotte\net al. 2013). \nProducing proxy-based paleoclimate reconstructions, especially on\nhemispheric or global scales, involves a host of methodological\nchallenges. Just a few will be mentioned here. First, as the\ncharacterization above suggests, proxies often reflect the influence\nof multiple environmental factors at once. For example, tree rings can\nbe influenced not only by temperature but also by precipitation, soil\nquality, cloud cover, etc., which makes it more difficult to\nconfidently infer a single variable of interest, such as temperature.\nSecond, proxies often must be calibrated using recent observations\nmade with meteorological instruments, but the instrumental record\ncovers only a very short period in earth’s history, and factors\nshaping the proxy in the distant past may be somewhat different.\nThird, proxies of a given type can have limited geographical coverage:\nice cores are found only at the poles, tree rings are absent in\nlocations where there is no distinct growing season, and so on.\nFourth, the temporal resolution of different types of proxy can differ\nmarkedly—from a single year to a century or more—adding a\nlayer of complexity when attempting to use multiple proxies together.\nFor these reasons and others, quantifying the uncertainties associated\nwith proxy reconstructions is also a significant challenge (Frank et\nal. 2010). \nDespite these challenges, climate scientists have produced\npaleoclimatic reconstructions covering various regions, time periods\nand climate-related variables, including temperature, precipitation,\nstreamflow, vegetation and more. Temperature reconstructions in\nparticular have been a source of controversy (see\n Section 5.3),\n in part because they underwrite conclusions like this one, from the\nIntergovernmental Panel on Climate Change (IPCC) Fifth Assessment\nReport: the period 1983–2012 was very likely (i.e.,\nprobability >0.95) the warmest 30-year period of the last 800 years\n(Masson-Delmotte et al. 2013: 386). Vezér (2016a) suggests that\ninferences to such conclusions can be characterized as a form of\nvariety-of-evidence reasoning, insofar as they involve multiple\ntemperature reconstructions informed by different proxies,\nmethodological assumptions and statistical techniques (see also\nOreskes 2007). \nModels of the climate system, especially computer simulation models,\nhave come to occupy a central place in both theoretical and applied\nresearch in climate science. After providing an overview of some\ncommon types of climate model\n (Section 4.1),\n this section focuses primarily on atmosphere-ocean general\ncirculation models and earth system models, discussing some noteworthy\nfeatures of their construction\n (Section 4.2),\n some of their most important uses\n (Section 4.3),\n and how they are evaluated\n (Section 4.4).\n The focus is on these complex climate models both because they have\nattracted the most attention from philosophers and, relatedly, because\nthey play particularly important roles in climate research today,\nincluding climate change research. \nClimate scientists often speak of a “hierarchy” or\n“spectrum” of climate models, ranging from simple to\ncomplex. The complexity of climate models increases with: the number\nof spatial dimensions represented; the resolution at which those\ndimensions are represented; the range of climate system components and\nprocesses “included” in the model; and the extent to which\nthose processes are given realistic rather than simplified\nrepresentations. As a consequence, more complex models tend to be more\ncomputationally demanding as well. The discussion below introduces a\nfew types of model, illustrating different levels of complexity (for\nother types see, e.g., Stocker 2011; McGuffie & Henderson-Sellars\n2014). The types discussed here are physics-based, in the sense that\nthey are grounded to a significant extent in physical theory.\nData-driven or “empirical” climate models also have been\ndeveloped for some purposes but are less common and will be mentioned\nonly in passing. \nAmong the simplest climate models are energy balance models\n(EBMs), designed to represent earth’s surface energy budget in a\nhighly aggregate way (McGuffie & Henderson-Sellars 2014: Ch.3).\nThese models, which often have surface temperature as their sole\ndependent variable, are constructed using both physical theory (e.g.,\nthe Stefan-Boltzmann equation) and empirical parameters (e.g.,\nrepresenting the albedo of the earth and the emissivity of the\natmosphere). Zero-dimensional EBMs represent the entire climate system\nas a single point. They can be used to calculate by hand an estimate\nof the globally-averaged surface temperature when the system is in\nradiative equilibrium. One-dimensional (1-D) and two-dimensional (2-D)\nEBMs take a similar energy-budget approach but represent average\ntemperature at different latitudes and/or longitudes and account in a\nrough way for the transport of heat between them (see Sellers 1969 for\nan early example). The equations of 1-D and 2-D EBMs are usually\nsolved with the help of digital\n computers.[1] \nEarth system models of intermediate complexity (EMICs) come\nin a range of forms but tend to be both comprehensive and highly\nidealized (Claussen et al. 2002). An EMIC might incorporate not only representations of the atmosphere, ocean, land surface and sea ice, but also representations\nof some biospheric processes, ice sheets and ocean sediment\nprocesses. These representations, however, are often relatively simple or coarse. The atmosphere component of an EMIC, for\ninstance, might be a two-dimensional enhanced version of an EBM known\nas an energy moisture balance model. The ocean might be simulated\nexplicitly in three dimensions using fluid dynamical equations, but\nwith low spatiotemporal resolution. (See Flato et al. 2013: Table 9.2\nand 9.A.2 for examples and further details.) The relative simplicity\nand coarseness of EMICs makes them computationally efficient, however, which in\nturn makes it possible to use them to simulate the climate system on\nmillennial time scales. \nAt the complex end of the spectrum, coupled ocean-atmosphere\ngeneral circulation models (GCMs) simulate atmospheric and\noceanic motions in three spatial dimensions, often at as high a\nresolution as available supercomputing power allows. They also\nincorporate representations of the land surface and sea ice, and they\nattempt to account for important interactions among all of these\ncomponents. GCMs evolved from atmosphere-only general circulation\nmodels, which in turn were inspired by early weather forecasting\nmodels (Edwards 2000; Weart 2010). The latest generation of climate\nmodels, earth system models (ESMs), extend GCMs by\nincorporating additional model components related to atmospheric\nchemistry, aerosols and/or ocean biogeochemistry (Flato et al. 2013:\n747). In both GCMs and ESMs, numerical methods are used to estimate\nsolutions to discretized versions of fluid dynamical equations at a\nset of points on a three-dimensional\n grid.[2]\n With increased computing power in recent decades, the horizontal\nspacing of these grid points for the atmosphere has reduced from\nseveral hundred kilometres to ~100 km, and the number of vertical\nlayers has increased from ~10 to ~50, with a time step of 10–30\nminutes (McGuffie & Henderson-Sellers 2014: 282). Despite this\nincreased resolution, many important processes—including the\nformation of clouds and precipitation, the transfer of radiation, and\nchemical reactions—still occur at sub-grid scales; accounting\nfor the effects of these processes is a major challenge, as discussed\nin the next section. \nAnother important type of climate model is the regional climate\nmodel (RCM). Like GCMs and ESMs, RCMs are designed to be\ncomprehensive and to incorporate realistic, theory-based\nrepresentations of climate system processes. Unlike GCMs and ESMs,\nhowever, RCMs represent only a portion of the globe (e.g., a continent\nor country), which allows them to have a higher spatiotemporal\nresolution without exceeding available computing power. With this\nhigher resolution, RCMs can explicitly simulate smaller-scale\nprocesses, and they also have the potential to reveal spatial\nvariations in conditions—due to complex topography, for\ninstance—that cannot be resolved by GCMs/ESMs. These features\nmake RCMs an attractive tool for studies of regional climate change\n(e.g., Mearns et al. 2013). A challenge, however, is specifying\nconditions at the horizontal boundaries of the modelled region in a\nphysically-consistent way; in practice, these boundary conditions\noften are informed by results from GCM/ESM simulations. \nCurrently, there are a few dozen state-of-the-art GCMs/ESMs housed at\nmodeling centers around the world (see Flato et al. 2013: Table 9.1).\nThese are huge models, in some cases involving more than a million\nlines of computer code; it takes considerable time to produce simulations\nof interest with them, even on today’s supercomputers. To construct a GCM/ESM\nfrom scratch requires a vast range of knowledge and expertise (see\nLahsen 2005). Consequently, many of today’s GCMs and ESMs have\nbeen built upon the foundations of earlier generations of models\n(Knutti et al. 2013). Such models often have a layered history, with\nsome parts of their code originally developed years or even decades\nago—sometimes by scientists who are no longer working at the\nmodeling center—and other parts just added or upgraded. Even\nmodels at different modeling centers sometimes have pieces of computer\ncode in common, whether shared directly or borrowed independently from\nan earlier model. \nMany of today’s GCMs/ESMs have an ostensibly modular design:\nthey consist of several component models corresponding to different\nparts of the climate system—atmosphere, ocean, land surface,\netc.—as well as a “coupler” that passes information\nbetween them where the spatial boundaries of the component systems meet and\nthat must account for any differences in the component models’\nspatiotemporal resolution (Alexander & Easterbrook 2015). Often\nthese component models are modified versions of named, stand-alone\nmodels. For example, the Community Earth System Model version 1.2\n(CESM1.2) housed at the U.S. National Center for Atmospheric Research\nincorporates the Community Atmosphere Model (CAM) and an extension of\nthe Parallel Ocean Program (POP). Such modularity is intended to allow\nfor easier reconfiguration of a GCM/ESM for different modeling\nstudies, e.g., replacing the ocean model with a simpler version when\nthis will suffice. Lenhard and Winsberg (2010), however, argue that\ncomplex climate models in fact exhibit only a “fuzzy”\nmodularity: in order for the model as a whole to work well, some of\nthe details of the component models will be adjusted to mesh with the\nparticular features of other components and to try to compensate for\ntheir particular limitations. \nEach component model in a GCM/ESM in turn incorporates representations\nof a number of important processes operating within that part of the\nclimate system. For atmosphere models, for example, it is common to\ndistinguish the “dynamics” and the “physics”\n(see, e.g., Neale et al. 2010). The dynamics (or “dynamical\ncore”) is a set of fluid dynamical and thermodynamic equations\nreflecting conservation of momentum, mass and energy, as well as an\nequation of state. These equations are used to simulate large-scale\natmospheric motions, which transport heat, mass and moisture; the\nscale of the motions that can be resolved depends on the grid mesh of\nthe model. The “physics” encompasses representations of\nsub-grid processes that impact grid-scale conditions in a significant\nway: radiative transfer, cloud formation, precipitation and more.\nThese processes are parameterized, i.e., represented as a\nfunction of the grid-scale variables that are calculated explicitly in\nthe dynamical core (e.g., temperature, pressure, winds, humidity).\nParameterization is necessary in other components of climate models as\nwell, whenever sub-grid processes significantly influence\nresolved-scale variables. \nConstructing a parameterization is akin to an engineering problem,\nwhere the goal is to find an “adequate substitute” (Katzav\n2013a) for the explicit simulation of a sub-grid process, using a\nlimited set of ingredients. This is a challenging task. For given\nvalues of the grid-scale variables, there are usually many possible\nrealizations of the sub-grid conditions. The standard approach to\nparameterization has been a deterministic one, aiming to estimate the\ncontribution that the sub-grid process would make on average, over\nmany possible realizations that are physically consistent with a given\nset of grid-scale conditions (McFarlane 2011). An alternative approach\ngaining popularity is stochastic parameterization,\nwhich aims to estimate the contribution made by a single,\nrandomly-selected member of the set of realizations physically\nconsistent with the grid-scale conditions (Berner et al. 2017).\nTypically, parameterizations of either type are informed by physical\nunderstanding but also incorporate elements derived at least in part\nfrom observations (see also Sundberg 2007; Guillemot 2010); this is\nwhy parameterizations are often described as\n“semi-empirical”. Because climate models incorporate both\naccepted physical theory and such semi-empirical, engineered elements,\nthey are sometimes described as having a hybrid\nrealist-instrumentalist status (Parker 2006; Katzav 2013a; see also\nGoodwin 2015). \nParameterization also gives rise to fuzzy modularity within\ncomponent models of GCMs/ESMs: which parameterizations work best with\na given climate model depends to some extent on how other sub-grid\nprocesses have already been represented and the errors that those other\nrepresentations introduce (see also Lenhard & Winsberg 2010 on\n“generative entrenchment”). Even the best-available\nparameterizations, however, usually have significant limitations.\nIndeed, uncertainty about how to adequately parameterize sub-grid\nprocesses remains a major source of uncertainty in climate modeling.\nDifferences in parameterizations—especially for cloud\nprocesses—account for much of the spread among simulations of\nthe climate system’s response to increasing greenhouse gas\nconcentrations (Flato et al. 2013: 743). This is one motivation for a\nnew approach to parameterization: super-parameterization\ninvolves explicitly simulating a sub-grid process in a simplified way,\nby coupling a 1-D or 2-D model of the process (e.g., cloud formation)\nto each GCM/ESM grid point (Randall et al. 2013; Gramelsberger 2010).\nThis is a multi-scale modeling approach that requires significant\nadditional computing power, relative to traditional\nparameterization. \nConstruction of climate models also inevitably involves some\ntuning, or calibration, which involves the (often ad\nhoc) adjustment of parameter values or other elements of the model in\norder to improve model performance, usually measured by fit with\nobservations. The observations that are targeted vary from case to\ncase; they might relate to individual processes or to aggregate,\nsystem-level variables, such as global mean surface temperature (see\nMauritsen et al. 2012; Hourdin et al. 2017). Which parameter values are\nadjusted depends on the observations with which a better fit is\nsought, but often adjustments are made within parameterizations, to\nparameters whose best values are significantly\nuncertain. The extent to which a model is tuned is usually not\ndocumented or reported in detail (but see Mauritsen et al. 2012 for an\ninstructive example). Recently, there have been calls for more\ntransparency in reporting tuning strategies and targets (Hourdin et\nal. 2017), in part because information about tuning is relevant to\nmodel evaluation (see\n Section 4.4).\n An interesting question, however, is exactly what counts as tuning\nsince, even when formal quantitative comparison with a set of\nobservational data is not performed, modelers can be familiar with\nthose data and may well make choices in model\ndevelopment—choices which could reasonably have been somewhat\ndifferent—with the expectation that they will improve the\nmodel’s performance with respect to those already-seen data. \nA further issue concerns the role of social and ethical values in\nclimate model construction. Winsberg (2010, 2012; Biddle &\nWinsberg 2009) argues that such values influence climate model\nconstruction (and thereby results) both by shaping priorities in model\ndevelopment and via inductive risk considerations, e.g., when deciding\nto represent a climate system process in one way rather than another,\nin order to reduce the risk that the model’s results err in a\nway that would have particularly negative non-epistemic consequences.\nHe contends that social and ethical values operate in climate model\nconstruction whenever there are no decisive, purely epistemic grounds\nfor considering one model-building option to be the best available.\nParker (2014a) disputes this, pointing to the importance of pragmatic\nfactors, such as ease of implementation, local expertise and\ncomputational demands. Intemann (2015) argues that social and ethical\nvalues can legitimately influence climate model construction,\nincluding via the routes identified by Winsberg, when this promotes\ndemocratically-endorsed social and epistemic aims of research.  \nClimate models are used for many types of purpose; just a few will be\nmentioned here (see also Petersen 2012: Ch.5). One important use is in\ncharacterizing features of the climate system that are\ndifficult to learn about via available observations. For example, the\ninternal variability of the climate system is often estimated from the\nvariability seen in long GCM/ESM simulations in which external\nconditions are held constant at pre-industrial levels (Bindoff et al.\n2013). Internal variability is difficult to estimate from the\ninstrumental record, because the record is relatively short and\nreflects the influence not just of natural internal processes but also\nof changing external conditions, such as rising greenhouse gas\nconcentrations. Estimates of internal variability in turn play an\nimportant role in studies that seek to detect climate change in\nobservations (see\n Section 5.1). \nClimate models are also used as scientists seek explanations\nand understanding. Typically, explanations sought in climate\nscience are causal; climate scientists seek accurate accounts of how\ncombinations of climate system processes, conditions and features\ntogether bring about climate phenomena of interest, including climate\nchange. Parker (2014b) identifies several ways in which climate models\nhave facilitated such explanation: by serving as a surrogate for\nobservations, the analysis of which can suggest explanatory hypotheses\nand help to fill in gaps in how-possibly/plausibly explanations; by\nallowing scientists to test hypotheses relevant to explanation, e.g.,\nthat a set of causal factors represented in the model is sufficient to\nproduce a phenomenon (see also Lorenz 1970); and by serving as\nexperimental systems that can be manipulated and studied in order to\ngain insight into their workings, which can inform thinking about how\nthe climate system itself works. In connection with the latter, Held\n(2005) calls for increased efforts to develop and systematically study\n“hierarchies of lasting value”—sets of models,\nranging from the highly idealized to the very complex, that are\nconstructed such that they stand in known relations to one another so\nthat the sources of differences in their behavior can be more readily\ndiagnosed; he contends that the study of such hierarchies is essential\nto the development of climate theory in the twenty-first century. \nIn addition, climate models are used to make predictions. It\nmight be thought that, due to chaos, climate prediction is impossible.\nBut chaos is an obstacle to precisely predicting the trajectory of a\nsystem—the time-ordered sequence of its states over a\nperiod—not (necessarily) for predicting the statistical\nproperties of one or more trajectories; climate prediction is\nconcerned with the latter. Short-term predictions of climate for\nperiods ranging from a year to a decade or so are made with both\nphysics-based and empirical models (e.g., Meehl et al. 2014; Krueger\n& von Storch 2011). For these predictions, assumptions are made\nabout what external conditions actually will be like over the forecast\nperiod, and the forecasts begin from an observationally-based estimate\nof the recent state of the climate system. Better known, however, is\nthe use of climate models to make longer-term conditional predictions,\nknown as projections. These are predictions of what future\nclimate would be like under particular external condition\nscenarios, without assuming that any of those scenarios will\nactually occur, and they are often launched from an initial state that\nis meant to be representative of the climate at the start of the\nsimulated period, though not estimated directly from observations (see\n Section 4.4).\n Climate change projections have become a major focus of climate\nresearch and will be discussed further in\n Section 5.2\n below. \nRecently there have been calls for discipline-level changes in the\npractice of climate modeling, in order to better serve some of the\npurposes just mentioned. To improve climate change projections, for\ninstance, Shukla et al. (2009) call for a “revolution” in\nclimate modeling, such that both expertise and computing power are\nconsolidated in a few international modeling centers, allowing for\nmuch-higher-resolution simulations. Katzav and Parker (2015)\ncritically examine this and other leading proposals and reflect on\ntheir potential benefits and costs. \nEvaluation of climate models occurs throughout the model development\nprocess. Parameterizations and component models are often tested\nindividually “off-line” to see how they perform (Randall\net al. 2007), and adjustments are made to try to improve them when\nthey fall short of expectations. Likewise, there is testing and\nadjustment as the different model components are coupled. This\nevaluation can have both qualitative and quantitative dimensions. For\ninstance, it might be checked that the model’s climate is\nbasically realistic-looking, with salient features (e.g., of the\natmospheric circulation) in roughly the right place. Quantitative\nevaluation produces scores on performance metrics. These can\ninclude measures of conservation and stability—how close does\nthe model come to achieving a top-of-atmosphere energy\nbalance?—as well measures of fit between model output and\nobservational data. The evaluation that occurs in the course of model\ndevelopment, however, is rarely reported in publications. What is\nreported instead are features of the model, including how it performs,\nonce it is fully constructed, tuned and ready to be released for\nscientific use. \nClimate model evaluation can have many different goals. A basic\nmotivation is simply to learn about how climate models relate to the\nclimate system. Borrowing the language of Giere (2004), we might say\nthat a goal is to learn about the respects in which and degrees to\nwhich climate models are similar to the climate system (Lloyd\n2010). A climate scientist might investigate, for example, whether a\nparticular GCM exhibits a seasonal cycle in temperature for North\nAmerica that roughly matches that which currently occurs in the real\nsystem. Evaluation efforts can also be directed at understanding\nsimilarities at the level of the processes and mechanisms underlying\nbehaviors. In this regard, Lloyd (2009, 2010, 2015) points to fit\nbetween model results and observations on a variety of metrics,\nindependent support for model components, and robustness of findings\nas considerations that are relevant. A second common goal of climate\nmodel evaluation, closely related to the first, is to learn whether a\nmodel is adequate for particular purposes (Parker 2009; Baumberger et al. 2017). As discussed in\n Section 4.3,\n scientists use climate models for various purposes: to characterize\nclimate system properties (e.g., internal variability), to make\nprojections of future climate change under particular scenarios, and\nso on. Many evaluation activities are intended to inform conclusions\nabout climate models’ adequacy for these purposes\nand, ipso facto, about the levels of confidence that can be had in the\nmodels’ estimates of internal variability, their projections of\nfuture climate change, etc. \nWhat is known about the ingredients of a climate model—e.g.,\nthat they closely approximate accepted theory or involve massive\nsimplifications—can provide reasons to think that the model is\nsimilar (or not) to the climate system in particular ways or adequate\n(or not) for particular purposes (Baumberger et al. 2017). But climate scientists also seek\nevidence regarding similarity and adequacy by comparing model results\nto observations (see also Guillemot 2010). Such comparisons can be\nqualitative or quantitative, as noted above. In some cases, it is\nrelatively straightforward to gauge whether model-data comparisons\nprovide evidence for or against hypotheses about model-system\nsimilarities or about the adequacy of climate models for\nparticular purposes. Prime candidates include cases in which what is\nhypothesized entails just a rough fit between model results and\nwell-observed climate system behaviors. Thus, for example, the\nIntergovernmental Panel on Climate Change (IPCC) is able to report\n \nvery high confidence that models reproduce the general features of the\nglobal-scale annual mean surface temperature increase over the\nhistorical period, including the more rapid warming in the second half\nof the 20th century, and the cooling immediately following\nlarge volcanic eruptions. (Flato et al. 2013: 743)  \nIn many other cases, however, it is more difficult to gauge the extent\nto which model-data comparisons provide evidence of a model's similarity to the climate system or its\nadequacy for particular purposes. There are a number of complicating\nfactors.  \nFirst, differences between modeling results and observational data\nmight not be due to shortcomings of the climate model. Climate\ndatasets can contain errors or have uncertainties that are\nunderestimated or not reported. Indeed, in more than one\ncase, it has turned out that model-data conflicts were resolved\nlargely in favor of the models (see, e.g., Lloyd 2012). There also\nhave been cases where a poor fit between modeling results and data was judged to\nbe due in part to errors in assumptions about the external conditions\n(e.g., aerosol concentrations) that obtained during the simulated\nperiod (e.g., Santer et al. 2014, 2017). More fundamental is an issue\nrelated to the initial conditions of climate simulations (see also\nSchmidt & Sherwood 2015). Oftentimes, a simulation for a\nparticular period is launched from a representative state that is\narrived at after letting the climate model run for a while under\nexternal conditions like those assumed to hold at the start of the\nperiod of interest; this “spin up” of the model is needed to\nallow its various components to come into balance with one another.\nSince this representative state inevitably differs somewhat from the\nactual conditions at the start of the period of interest (e.g., on Jan\n1, 1900), and since the climate system is believed to be chaotic, the\nsimulation cannot be expected to closely track observations throughout\nthe period, even if the model perfectly represents all climate system\nprocesses; the goal is for differences between climate simulations and\nobservations to be no larger than would result when running a perfect\nmodel from other initial states that are consistent with the external\nconditions at the start of the period of interest (i.e., no larger than\ndifferences stemming from internal variability). \nInterpretation can be difficult even when there is relatively good fit\nbetween modeling results and observational data. One reason is that,\nto varying degrees, today’s climate models have been tuned to\ntwentieth century observations. Tuning sometimes delivers an improved\nscore on a performance metric by compensating for errors elsewhere in\nthe model (Petersen 2000), and the stability of errors is not\nguaranteed as external conditions change, e.g., as greenhouse gas\nconcentrations continue to rise (Stainforth et al. 2007; Knutti 2008).\nSome climate scientists suggest that data used in tuning cannot be\nused subsequently in model evaluation (e.g., Randall et al. 2007: 596;\nFlato et al. 2013: 750). Steele and Werndl (2013, 2016), however,\nargue that in some circumstances it is legitimate to use data for both\ncalibration (i.e., tuning) and confirmation, something they illustrate\nwith both Bayesian and frequentist methodologies. Frisch (2015)\nadvocates what he calls a moderate predictivism in this\ncontext: if a climate model is tuned to achieve a good fit with data\nfor a particular variable, this provides less support for the\nhypothesis that the model can deliver the same performance with\nrespect to other predictions for that variable (e.g., future values),\ncompared to the case when the good fit is achieved without tuning (see\nalso Stainforth et al. 2007; Katzav et al. 2012; Mauritsen et al.\n2012; Schmidt & Sherwood 2015). Frisch defends this view on the\ngrounds that successful simulation without tuning gives us more reason\nto think that a climate model has accurately represented key processes\nshaping the variable of interest; if we were already confident of the\nlatter, then successful simulation without tuning would not have such\nan advantage. In any case, to take account of tuning in model\nevaluation, it is important to know what tuning has been done, and\nthis information is often not readily available (see\n Section 4.2). \nA further complication is that, in many cases, the\n“observations” used in climate model evaluation are\nreanalysis datasets whose contents are determined in part by model-based\nweather forecasts (Edwards 1999,\n2010; Flato et al. 2013: Table 9.3; see also\n Section 3.2\n above). Since weather forecasting models and climate models can\nincorporate some similar idealizations and simplifications, the worry\narises that fit between reanalyses and climate modeling results might\nbe artificially inflated in some respects, due to shared imperfections. Identifying such artificial inflation of fit can be\nchallenging, given limited availability of independent observations,\ni.e., ones not used in producing the reanalysis. Leuschner (2015:\n370), however, suggests that a focus on the shared assumptions\nthemselves might be a promising way forward; the adequacy of shared\nassumptions about the radiative effects of aerosols, for instance,\nmight be established via comparison with the results of laboratory\nexperiments (see also Frigg et al. 2015b: 957). \nLastly, when it comes to evaluating the adequacy of climate models for\nsome purposes, especially predictive purposes, it can be difficult to\nknow what sort of fit with past climate conditions would even count as\nevidence of a climate model’s adequacy-for-purpose (Parker\n2009). How accurately should a climate model simulate various features\nof past climate if it is adequate for, say, predicting future changes\nin global mean surface temperature to within some specified margin of\nerror? This can be very difficult to answer, not only because there is\nmore than one way to achieve such predictive accuracy (e.g., by\ndistributing error differently across contributing variables), but\nalso because simplifications and idealizations in the model, as well\nas tuned components, might work better under some external conditions\nthan others, e.g., under those of the recent past rather than those\nassumed in future scenarios (see ibid.). Katzav (2014) suggests that,\nin practice, attempts to determine what sort of fit with past\nconditions would count as evidence of a climate model’s\nadequacy-for-purpose will often rely on results from climate models\nthemselves, in a way that is question-begging. \nGiven these challenges, climate model evaluation today often focuses\nprimarily on describing the extent to which modeling results fit with\nobservations and on characterizing how that fit has improved from one\ngeneration of models to the next; it tends to reach only relatively\nmodest conclusions about the adequacy of climate models for\nparticular purposes. For example, recent Intergovernmental Panel on\nClimate Change (IPCC) reports have concluded that today’s\nclimate models provide “credible quantitative estimates”\nof climate system conditions in support of climate change detection,\nattribution and projection (see\n Section 5),\n where this credibility is underwritten both by how the models are\nconstructed—i.e., their grounding in basic physical\nprinciples—and by how they perform on a wide range of metrics\n(see Randall et al. 2007; Flato et al. 2013; see also Knutti 2008).\nThe credibility of results is judged to be greater for larger spatial\nscales (i.e., for quantities averaged over global or continental\nscales) and for variables that are less spatially heterogeneous (e.g.,\nfor temperature and pressure as opposed to precipitation), but the\nexpected accuracy of modeling results used in detection, attribution\nand projection studies is rarely explicitly quantified. To begin to move the discussion forward, Baumberger et al. (2017) draw on both philosophical and scientific resources to outline a conceptual framework intended to aid evaluation of the adequacy of climate models for particular purposes; they point to empirical accuracy, robustness, and coherence with background knowledge as relevant considerations (see also Knutti (2018: 11.4), highlighting the importance of \"process understanding\"). \nFinally, it is worth noting that a significant motivation for model\nevaluation studies is model improvement: model evaluation activities\ncan be directed toward understanding why a model’s results\nexhibit particular errors, in the hope of improving the model in\nfuture. Lenhard and Winsberg (2010), however, suggest that such\n“analytical understanding” of climate model performance is\nlargely out of reach. They argue that features of climate models,\nincluding their complexity, their fuzzy modularity (see\n Section 4.2)\n and their incorporation of “kludges”—unprincipled\nfixes applied in model development to make the model as a whole work\nbetter—will often make it very difficult to apportion blame for\npoor simulation performance to different parts of climate models;\nclimate modeling, they argue, faces a particularly challenging variety\nof confirmational holism (see also Petersen 2000). As\nempirical support, they point to the limited success of model\nintercomparison projects (e.g., Taylor et al. 2012) in identifying the\nsources of disagreement in climate model simulations. However, there\nhave been some notable successes in localizing sources of error and\ndisagreement in climate simulations. For example, differences in the\nrepresentation of cloud processes have been identified as a major\nsource of disagreement among estimates of climate sensitivity (Flato\net al. 2013: 743). Likewise, a growing body of work on “emergent\nconstraints” has uncovered sources of spread in various aspects\nof climate change projections (see, e.g., Qu & Hall 2007 and\nsubsequent work; Frigg et al. 2015c). Thus, while Lenhard and Winsberg\nare surely right that it is difficult to achieve analytical\nunderstanding of climate model performance, just how limited the\nprospects are seems an open question. \nThe idea that humans could change Earth’s climate by emitting\nlarge quantities of carbon dioxide and other gases is not a new one\n(Fleming 1998; Weart 2008 [2017, Other Internet Resources]). In the\nlate nineteenth century, Swedish chemist Svante Arrhenius calculated\nthat doubling the levels of carbonic acid (i.e., carbon dioxide) in\nthe atmosphere would warm Earth’s average surface temperature by\nseveral degrees Celsius (Arrhenius 1896). By the mid-twentieth\ncentury, oceanographer Roger Revelle and colleagues concluded:  \nBy the year 2000, the increase in atmospheric CO2 …\nmay be sufficient to produce measurable and perhaps marked change in\nclimate. (quoted in Oreskes 2007: 83–84)  \nIn 1988, climate scientist James Hansen famously testified to the U.S.\nCongress that global warming was already happening. The same year, the\nWorld Meteorological Organization and the United Nations Environment\nProgram established the Intergovernmental Panel on Climate Change\n(IPCC)  \nto provide policymakers with regular assessments of the scientific\nbasis of climate change, its impacts and future risks, and options for\nadaptation and mitigation. (IPCC 2013a)  \nDrawing on the expertise of the international climate science\ncommunity, the IPCC has delivered assessment reports roughly every\nfive years since\n 1990.[3] \nBecause IPCC reports usefully synthesize a huge body of scientific\nresearch, they will be referenced frequently in what follows. The discussion\nwill focus primarily on questions about the physical climate system,\nrather than questions about the impacts of climate change on societies\nor about climate policy options; the latter, which are also addressed\nby the IPCC, call for expertise that extends well beyond climate\nscience (see IPCC 2014).\n Section 5.1\n discusses detection and attribution research, which is concerned with\nthe following questions: In what ways has Earth’s climate\nchanged since pre-industrial times? How much have human activities,\nespecially human emissions of greenhouse gases, contributed to the\nchanges?\n Section 5.2\n considers the projection of future climate change, with special\nattention to debates about the interpretation of model-based\nprojections.\n Section 5.3\n provides a brief overview of several recent controversies related to\nthe methods and conclusions of climate change research. Finally,\n Section 5.4\n notes some of the ethical questions and challenges raised by the\nissue of anthropogenic climate change. \nThe IPCC defines detection of climate change as  \nthe process of demonstrating that climate or a system affected by\nclimate has changed in some defined statistical sense, without\nproviding a reason for that change. (IPCC-glossary: 1452)  \nThey imply that they are interested in climate change due to external\nfactors, rather than natural processes internal to the climate system,\nwhen they add:  \nAn identified change is detected in observations if the likelihood of\noccurrence by chance due to internal variability alone is determined\nto be small, for example <10%. (ibid.)  \nDetection thus requires a statistical estimate of how much a quantity\nor field of interest might fluctuate due to internal variability. It\nis challenging to estimate this from the instrumental record.\nEstimates have been extracted from paleoclimate data for some\nquantities (Schurer et al. 2013) but, as noted in\n Section 4.3,\n often they are obtained from long GCM/ESM simulations in which\nexternal conditions are held constant. \nIn its periodic assessments, the IPCC has reached increasingly strong\nconclusions about the detection of climate change in observations. The\nIPCC’s Fifth Assessment Report concluded that it is\n“virtually certain” (i.e., probability >0.99) that the\nincrease in global mean surface temperature seen since 1950 is not due\nto internal variability alone (Bindoff et al. 2013: 885). That is, the\nprobability that the warming is due to internal variability alone was\nassessed by the scientists, on the basis of available evidence and\nexpert judgment, to be less than\n 1%.[4]\n Indeed, it was noted that, even if internal variability were three\ntimes larger than estimates from simulations, a change would still be\ndetected (ibid.: 881, citing Knutson et al. 2013). Changes have been\nformally detected in many other aspects of the climate system as well,\nnot just in the atmosphere but also in the oceans and the\ncryosphere. \nThe process of attribution seeks to identify the cause(s) of\nan observed change in climate. It employs both basic physical\nreasoning—observed changes are qualitatively consistent (or not)\nwith the expected effects of a potential cause—as well as\nquantitative studies. The latter include fingerprint studies, in which\nclimate models are used to simulate what would occur over a period if\na particular causal factor (or set of factors) were changing, while\nother potential causal factors were held constant; the resulting\nsimulated pattern of change in the target variable or\nfield—often a spatiotemporal pattern—is the\n“fingerprint” of that factor (or set of factors).\nScientists then perform a regression-style analysis, looking for the\nlinear combination of the fingerprints that best fits the observations\nand checking whether the residual is statistically consistent with\ninternal variability (see, e.g., Bindoff et al. 2013: Box 10.1). If\nso, then an estimate of the contributions of the different causal\nfactors that were considered can be extracted from the analysis.\nBecause of uncertainties associated with observational data, the\nresponse patterns and various methodological choices, attribution\nstudies produce a range of estimated contributions for each\n factor.[5] \nAs with detection, IPCC conclusions related to attribution have grown\nstronger over time and have come to encompass more variables and\nfields. In their Fifth Assessment Report, the IPCC concluded that it\nis “extremely likely” (i.e., probability >0.95) that\nmore than half of the increase in global mean surface temperature\nsince 1950 was due to anthropogenic greenhouse gas emissions and other\nanthropogenic forcings (Bindoff et al. 2013: 869). This conclusion was\ninformed by multiple fingerprint studies, as well as physical\nreasoning and expert judgment. The latter underwrite key assumptions\nof the attribution exercise (e.g., that no significant causes have\nbeen overlooked) and, relatedly, play a role in deciding the extent to\nwhich face-value probabilities produced in fingerprint studies should\nbe downgraded (e.g., from “virtually certain” to\n“extremely likely”) in light of remaining uncertainties\nand recognized limitations of those studies. For various other\nchanges, including increases in global mean sea level since the 1970s\nand retreat of glaciers since the 1960s, the IPCC concludes that it is\nat least “likely” (i.e., probability >0.66) that human\ninfluence played a role (ibid.: 869–871). \nThese conclusions about detection and attribution align with, if not\nconstitute, the “consensus” position regarding the reality\nand causes of recent climate change. The extent to which there is a\nscientific consensus on these matters, however, has itself become a\ntopic of debate. It is often reported, for example, that 97% (or more)\nof climate scientists agree that anthropogenic emissions of greenhouse\ngases are causing global climate change (Cook et al. 2016). Analyses\nof the scientific literature, as well as surveys of scientists, have\nbeen conducted to demonstrate (or challenge) the existence of this\nconsensus. This focus on consensus can seem misguided: what matters is\nthat there is good evidence for a scientific claim, not merely that\nsome percentage of scientists endorses it (see also Intemann 2017).\nYet consensus among experts can serve as indirect evidence for a\nscientific claim—an indication that the scientific evidence\nfavors or even strongly supports the claim—at least if the\nconsensus is “produced in the right manner” (Odenbaugh\n2012). A consensus emerging from a process that gives free rein to\nshared bias or groupthink (Ranalli 2012), or that does not require\nthat those surveyed have relevant expertise, might have little\nevidential value. By contrast, a “hard won”\nconsensus—one that emerges despite the reluctance of\nindependent-minded and critical parties, after “vigorous debate\nand a thorough examination of the range of alternative\nexplanations” (ibid.: 187) might be significant; the indirect\nevidence provided by such a consensus could be useful to non-experts\nwho are not able to evaluate the scientific evidence themselves.\nRanalli (2012) suggests that parties on opposite sides of debates\nabout the reality and causes of climate change generally hold similar\nviews about what makes for a reliable scientific consensus, but\ndisagree over the extent to which the climate consensus meets the\nstandard. \nIn support of the consensus position regarding detection and\nattribution, Oreskes (2007) argues that research underpinning it\naccords with various models of scientific reliability: it has a strong\ninductive base, it has made nontrivial correct predictions, it has\nsurvived persistent efforts at falsification, it has uncovered a body\nof evidence that is consilient, and—in light of all of\nthis—climate scientists have concluded that the best explanation\nof recent observed warming includes a significant anthropogenic\ncomponent. She also emphasizes that the core idea that increased\natmospheric concentrations of carbon dioxide would warm the climate\nemerged first from basic physical understanding, not from computer\nmodels of the climate system. In a complementary analysis, Lloyd\n(2010, 2013) offers a defense of the use of climate models in\nattribution research and argues that studies involving multiple\nclimate models—which incorporate similar assumptions about the\nradiative properties of greenhouse gases but differ somewhat in their\nempirically-supported representations of other climate system\nprocesses—provide enhanced support for the conclusion that\ngreenhouse gases are (part of) a good explanation of observed recent\nwarming; she argues that such studies illustrate a distinctive kind of\nmodel robustness, which incorporates variety-of-evidence\nconsiderations, and that such robustness is a “confirmatory\nvirtue” (see also Vezér 2016b). Robustness and\nvariety-of-evidence considerations are also highlighted by Parker\n(2010), in a different way; she suggests that they have been\nimportant for justifying detection and attribution claims, given that\nformal studies, such as fingerprint studies, often rely on contentious\nor false assumptions. Winsberg (2018: Ch.11&12) critically\nexamines and synthesizes much of this recent work on robustness in\nclimate science, drawing on Schupbach’s (2016) account of\nrobustness analysis as explanatory reasoning. \nKatzav (2013b), however, considers another perspective on\nevidence—Mayo’s (1996) severe testing framework—and\nargues that the claim that more than half of the increase in global\nmean surface temperature since 1950 was due to anthropogenic forcing\nhas not passed a severe test in Mayo’s sense, even when\nvariety-of-evidence considerations are factored in; from a severe\ntesting perspective, he contends, we still lack good evidence for this\nconclusion. He suggests that some weaker attribution claims—such\nas the claim that anthropogenic forcing had some role in\npost-1950 warming—might be ones for which there is good evidence\nin Mayo’s demanding sense.  \nClimate scientists also seek to understand how climate would change in\nthe future if greenhouse gas emissions and other anthropogenic forcing\nfactors were to evolve in particular ways. Such information can be of\nsignificant interest to policy makers, who might choose to implement\npolicies that push toward some future scenarios and away from others,\nand to many other decision makers as well (e.g., insurance companies\ndeciding which policies to offer). Climate models have emerged as an\nimportant tool for making projections of future conditions. For a\ngiven scenario, it is common to make an ensemble of\nprojections using different climate models or model versions; this is\nmotivated in part by uncertainty about how to construct a climate\nmodel that can deliver highly accurate projections (Parker 2006; Betz 2009). This uncertainty in turn stems both from limited theoretical\nunderstanding of some processes operating within the climate system as\nwell as from limited computing power, which constrains how existing\nknowledge can be implemented in models (see the discussion of\nparameterization in\n Section 4.2). \nDifferent types of ensemble study explore the consequences of\ndifferent sources of uncertainty in modeling. A\nmulti-model ensemble (MME) study probes structural\nuncertainty, which is uncertainty about the form that modeling\nequations should take as well as how those equations should be solved.\nAn example of an MME study is the Coupled Model Intercomparison\nProject Version 5 (CMIP5), which produced projections using a few\ndozen climate models developed at different modeling centers around\nthe world (see Flato et al. 2013: Table 9.1). A perturbed-physics\nensemble (PPE) consists of multiple versions of the same climate\nmodel, where the versions differ in the numerical values assigned to\none or more parameters within the model. Examples of studies employing\nPPEs include the climateprediction.net project (Stainforth et al.\n2005) and the UK Climate Projections (UKCP09) project (Murphy et al.\n2009). PPEs probe parameter uncertainty, i.e., uncertainty\nabout the numerical values that should be assigned to parameters\nwithin a given model structure. In an initial condition\nensemble (ICE), the same model or model version is assigned\ndifferent initial conditions. ICEs can probe initial condition\nuncertainty, which is uncertainty about the initial model state\nfrom which simulations should be launched, and they are often used in\ncombination with one of the other two types of ensemble (see Werndl forthcoming for further discussion). \nWhen it comes to interpreting ensemble projections, a range of views\nand methodologies have emerged. At one end of the spectrum, there are\nstatistical methodologies used to infer probabilities (of future\nchanges in climate) from ensemble results. For instance, some MME\nstudies have adopted what amounts to a “truth-plus-error”\nframework: probabilities are estimated by treating ensemble members as\nrandom draws from a distribution of possible models centered on truth\n(e.g., Tebaldi et al. 2005). The performance of today’s MMEs on\npast data, however, suggests that they are not centered on truth\n(Knutti et al. 2010a). An alternative\n“statistically-indistinguishable” framework allows for the\nestimation of probabilities by assuming that ensemble members and\ntruth (i.e., perfect observations of future conditions) are drawn from\nthe same statistical distribution. Examining past data, Annan and\nHargreaves (2010) find this assumption to roughly hold for one\nprominent MME (the CMIP3 ensemble), at least for some variables.\nBishop and Abramowitz (2013), however, argue that results from\ntoday’s MMEs lack some statistical properties that should be\npresent if the assumption of indistinguishability is accurate. They\npropose an alternative “replicate-Earth” framework, which\ninvolves post-processing MME projections so that they display those\ndesired statistical properties; Bishop and Abramowitz do not seem to\ninsist, however, that the post-processed distribution be considered a\nprobability distribution. Bayesian methodologies have also been\ndeveloped. The UKCP09 project, for example, used a Bayesian PPE\nmethodology to produce probabilistic climate change projections out to\n2100 at a fine spatiotemporal resolution for the United Kingdom\n(Murphy et al. 2009). That methodology treated parameter uncertainty\nas uncertainty about which model was the best (on given metrics of\nperformance) in a given model class, and attempted to account for the\ndifference between the best model and a perfect model with the help of\nresults from an MME (Rougier 2007; Sexton et al. 2012).  \nScientists and philosophers have raised a number of criticisms of\nmethods that infer probabilities from ensemble results. As implied\nabove, some lines of criticism cite evidence that the statistical\nassumptions underlying the studies are not met. Others argue that\nuncertainty about future climate change is deeper than precise\nprobabilities imply, either because of challenges associated with\nquantifying structural model uncertainty (e.g., Stainforth et al.\n2007; \nHillerbrand 2014; Parker 2014a) or, relatedly, because the possible future states of\nthe climate system (i.e., the outcome space) is itself uncertain\n(Katzav et al. 2012). In other words, these critics contend that\nprobabilistic uncertainty estimates have a false precision and, in that sense, are misleading about the actual\nstate of knowledge. Frigg et al. (2013, 2014, 2015a) argue that, for\nPPE studies like UKCP09, such probabilities are also likely to be\nmisleading in a different sense, namely, because they differ markedly\nfrom the probabilities that would be obtained from a model with no\nstructural error. They illustrate that, for nonlinear systems, even a\nsmall amount of structural model error can give rise to very\nmisleading probabilities—a consequence they dub the\n“hawkmoth effect” (taking inspiration from the\n“butterfly effect”, which relates to initial condition\nerror; see also Mayo-Wilson 2015, Lawhead forthcoming). Winsberg and\nGoodwin (2016), however, contend that more work is needed to establish\nthat the hawkmoth effect is as devastating for probabilistic climate\nprojection as Frigg et al. claim (see also Goodwin & Winsberg\n2016). \nSome of these critics of probabilistic approaches advocate a very\ndifferent interpretation of results from today’s ensemble\nprojection studies, according to which the projections indicate\nchanges that are (merely) plausible or possible in\nlight of current understanding. Stainforth et al. (2007), for example,\nsuggest that the range of projections produced using state-of-the-art\nclimate models constitutes a “non-discountable envelope”\nof future changes, a set of possible changes that should not be\ndisregarded; this does not imply that changes outside the envelope are\ndiscountable. Likewise, Katzav (2014) argues that climate change\nprojections can often be interpreted as indicating real\npossibilities:  \na state of affairs in a target domain is…a real possibility\nrelative to time t if and only if (a) its realisation is\ncompatible with the basic way things are in the target domain over the\nperiod during which it might be realised and (b) our knowledge at\nt does not exclude its realisation over that period. (2014:\n236)  \nBetz (2015), however, argues that even a possibilistic interpretation\nof ensemble results may be hard to justify, since it is difficult to\nshow that contrary-to-fact assumptions employed in climate models do\nnot lead to projections that are inconsistent with background\nknowledge; on his view, though not on Katzav’s, this consistency\nis necessary for projections to indicate serious possibilities. Part\nof the disagreement here thus hinges on what it takes for something to\nbe a serious/real possibility. \nThere are also intermediate views. One such view sees today’s\nensemble projection studies as imperfect investigations of\nuncertainty, whose results should be considered by experts alongside\nall other available information when reaching conclusions about future\nclimate change; those conclusions might take various forms, depending\non the variable under consideration (see, e.g., Kandlikar et al. 2005;\nMastrandrea et al. 2010). For example, in its recent\nassessment reports, the IPCC relied on ensemble modeling results,\nbackground knowledge and expert judgment to arrive at\n“likely” ranges for changes in near-surface global mean\ntemperature under different scenarios; the experts judged it to be\nlikely (i.e., probability >0.66) that the temperature\nchange would fall within the 5% to 95% range of the CMIP5 ensemble\nresults (Collins et al. 2013: 1031). For the moderate emission\nscenario known as RCP6.0, for instance, the conclusion was that the\nglobal mean temperature during 2081–2100 would likely\nbe between 1.4 and 3.1 degrees Celsius warmer than during\n1986–2005 (ibid.). The IPCC thus asserted more than that the\nchanges projected by the CMIP5 ensemble were possibilities, but they\ndid not go so far as to assign a single, full probability density\nfunction over temperature change values. \nIntertwined with the issue of ensemble interpretation is the issue of weighting, i.e., \nwhether to assign greater weight to some projections than others.\nThis is part and parcel of some methods, but in MME studies it has\nbeen common to give equal weight to participating models, as the IPCC approach\nillustrates. This is sometimes referred to as “model\ndemocracy” or “one-model one-vote” (Knutti 2010).\nPart of the original motivation for model democracy was the difficulty\nof determining which of the state-of-the-art models included in MMEs\nwould give the most accurate projections of future conditions (see\n Section 4.4).\n This difficulty notwithstanding, recently weighting has been\nadvocated on the grounds that, for at least some projected variables,\nthere is good reason to think that some models are more skillful than\nothers and, moreover, the one-model one-vote approach fails to take\naccount of dependence among models, i.e., of the fact that an MME can\ninclude several models that are variants of one another (Knutti et al.\n2017). A key challenge, however, is to select and combine\nrelevant metrics of performance and other criteria to assign appropriate\nweights for a given projected variable (Gleckler et al. 2008; Knutti et al. 2010b; Weigel et al. 2010). The basic\nissue of interpretation discussed above also remains, i.e., how to\ninterpret the weighted ensemble results. \nThere are many other philosophically-interesting questions related to\nensemble climate projection (Frame et al. 2007); just a few will be\nmentioned here. First, how can ensemble studies be designed so that\nthey probe uncertainty in desired ways? Today’s MME studies do\nnot probe structural uncertainty systematically, and it is unclear\nwhat it would mean to do so; there are questions about how to define\nthe space of relevant models and how to sample it (Murphy et al. 2007). Second, how significant are robust results? Climate\nscientists often assume that agreement among projections has special\nepistemic significance—e.g., that we can be substantially more\nconfident in those projected changes (Pirtle et al. 2010)—but\nParker (2011) argues that it is difficult to make the case for this.\nThere are also questions about how to quantify the extent of agreement\nor robustness (see Collins et al. 2013: Box 12.1) and about what it\nmeans for ensemble members to be independent, an important factor in\ninterpreting the significance of robustness (Annan and Hargreaves\n2017). Finally, to what extent do non-epistemic values influence\nensemble results? Winsberg (2010, 2012; see also Biddle & Winsberg\n2009) identifies pathways of influence via model construction (see\n Section 3.2),\n which in turn affect probabilistic uncertainty estimates produced\nfrom ensemble results, though not necessarily in a way that raises\nconcerns about “wishful thinking”. Parker (2014a) suggests that this influence\nmight be dampened by representing uncertainty in coarser ways that\nalso better reflect the extent of actual uncertainty, e.g., by using\nthe IPCC imprecise probability intervals; the extent to which the value influence is escapable in practice, however, remains unclear. \nClimate “contrarians” challenge key conclusions of\n“mainstream” climate science, such as those of the IPCC,\nin a host of public venues: blogs, newspaper op-ed pieces, television\nand radio interviews, Congressional hearings and, occasionally,\nscientific journals. Those considered contrarians come in many\nvarieties, from climate scientists who consider headline attribution\nclaims to be insufficiently justified—they are usually described\nas climate “skeptics”—to individuals and groups from\noutside of climate science whose primary motivation is to block\nclimate policy, in some cases by “manufacturing doubt”\nabout the reality or seriousness of anthropogenic climate change; they\nare commonly labelled climate “deniers” (see, e.g.,\nOreskes & Conway 2010; Ranalli 2012). (The use of these labels\nvaries, however.) Contrarians have played a role in creating or\nsustaining a number of public controversies related to climate\nscience. Four of these are discussed very briefly below, along with\nsome recent philosophical work reflecting on the impact of contrarian\ndissent. \nThe Tropospheric Temperature Controversy. Climate model\nsimulations indicate that rising greenhouse gas concentrations will\ninduce warming not only at earth’s surface but also in the layer\nof atmosphere extending 8–12 km above the surface, known as the\ntroposphere. Satellites and radiosondes are the primary means\nof monitoring temperatures in this layer. Analyses of these data in\nthe early 1990s, when only about a decade of satellite data was\navailable, indicated a lack of warming in the troposphere (Spencer\n& Christy 1990). Prima facie, this presented a challenge to\nclimate science, and it became a key piece of evidence in contrarian\ndismissals of the threat of anthropogenic climate change. Over time,\nadditional research uncovered numerous problems with the satellite and\nradiosonde data used to estimate tropospheric temperature trends, with\nmany of these problems related to homogenization (NRC 2000; Karl et\nal. 2006). More recent observational estimates agree that the\ntroposphere has been warming and, although observed trends still tend\nto be somewhat smaller than those in simulations, the mainstream view\nis that there is “no fundamental discrepancy” between\nobservations and models, given the significant uncertainties involved\n(Thorne et al. 2011). Nevertheless, the debate continues (see, e.g.,\nChristy et al. 2010; Santer et al. 2017). Lloyd (2012) suggests that\nthis controversy in part involves a conflict between a “direct\nempiricist” view that sees observational data as a naked\nreflection of reality, taking priority over models, and a more nuanced\n“complex empiricist” view (see also Edwards 2010:\n417). \nThe Hockey Stick Controversy. The hockey stick controversy\nfocused on some of the first millennial-scale paleoclimate\nreconstructions of the evolution of Northern Hemisphere mean\nnear-surface temperature. These reconstructions, when joined with the\ninstrumental temperature record, indicate a long, slow decline in\ntemperature followed by a sharp rise beginning around 1900; their\nshape is reminiscent of a hockey stick (e.g., Mann et al. 1998, 1999).\nSuch reconstructions featured prominently in the IPCC’s Third\nAssessment Report (Albritton et al. 2001) and constituted part of the\nevidence for the IPCC conclusion that “the 1990s are likely to\nhave been the warmest decade of the millennium” (Folland et al.\n2001: 102). Contrarian criticism in the published literature followed\ntwo main lines. One argued that there were problems with the data and\nstatistical methods used in producing the reconstructions (McIntyre\n& McKitrick 2003, 2005), while a second appealed to additional\nproxy data and alternative methods for their interpretation in order\nto challenge the uniqueness of late-twentieth century warming (Soon et\nal. 2003). Mainstream climate scientists offered direct replies to\nthese challenges (e.g., Mann et al. 2003; Wahl & Ammann 2007), and\nadditional research has since produced new and longer reconstructions\nusing a variety of proxies, which also support the conclusion that the\nlate twentieth century was anomalously warm (Masson-Delmotte et al.\n2013).\n [6]\n Contrarians, however, continue to criticize temperature\nreconstructions and the conclusions drawn from them, on various\ngrounds; a recent target was Marcott et al. 2013. Book length accounts\nof the controversy from opposing sides include Montford 2010 and Mann\n2012. \nThe Climategate Controversy. In 2009, a large number of\nemails were taken from the University of East Anglia’s Climatic\nResearch Unit and made public without authorization. Authors of the\nemails included climate scientists at a variety of institutions around\nthe world. Focusing primarily on a few passages, contrarians claimed\nthat the emails revealed that climate scientists had manipulated data\nto support the consensus position on anthropogenic climate change and\nhad suppressed legitimate dissenting research in various ways (e.g.,\nby preventing its publication or by refusing to share data). A number\nof independent investigations were subsequently conducted, all\nexonerating climate scientists of the charges of scientific fraud and\nmisconduct that contrarians had alleged (e.g., Russell et al. 2010).\nSome of the investigations, however, did find that climate scientists\nhad failed to be sufficiently transparent, especially in their response to\ncontrarian requests for station data used to estimate changes in\nglobal temperature (ibid.: 11). Despite the exonerations,\nsurveys found that in some countries this “Climategate”\nepisode—as it became known in popular media—significantly\nreduced public trust in climate science (Leiserowitz et al. 2013). \nThe Hiatus Controversy. Global mean near-surface temperature\nincreased significantly during the 1990s but then showed little\nincrease between the late 1990s and the early 2010s. By the mid-2000s,\ncontrarians began to claim that global warming had stopped and that\nclimate models (and climate science) were thus fundamentally flawed,\nsince they had projected more warming. Part of the problem here was\ncommunication: graphs shared with policymakers and the public often\nhighlighted the average of climate model projections, which smoothed\nout the significant variability seen in individual simulations and\nsuggested a relatively steady warming; in fact, the observed rate of\nwarming was not so different from that seen in some of the model\nprojections (see Schmidt 2012; Risbey et al. 2014). Moreover,\nsignificant variations in temperature on decadal time scales, akin to\nthat seen in the observations, were not entirely unexpected, given\ninternal variability in the climate system (Easterling & Wehner\n2009). Nevertheless, by the early 2010s, pressure was mounting for\nclimate scientists to give a detailed explanation of the apparent\nslowdown in surface warming, by then referred to as the\n“hiatus” or “pause”, and to explain why most\nmodels projected more warming. A preliminary analysis was given in the\nIPCC Fifth Assessment Report (see Flato et al. 2013: Box 9.2). A host\nof potential explanatory factors were identified—related to\nexternal forcing, internal variability, ocean heat uptake and errors in\nobservational data—which contrarians portrayed as excuses.\nSubsequent investigation found evidence for actual contributions from\nmost of the hypothesized factors, though discussion of their relative\nimportance continues (see Medhaug et al. 2017 for an overview;\na large literature on the topic has emerged). In the meantime, since\n2014, global mean temperature has once again shown a sharp increase,\nin part due to a strong El Niño event in 2015/16. \nContrarian dissent has impacted the practice of climate science in\nvarious ways. Most obviously, research is sometimes directed (at least in part) at rebutting contrarian claims and arguments. For example, a recent research\npaper related to tropospheric temperature trends (Santer et al. 2017)\nis explicitly framed as a response to contrarian claims made in the\ncourse of testimony to the U.S. Senate (see also Lewandowsky et al. 2015 on \"seepage\" and the hiatus controversy). In addition, Brysse et al.\n(2013) contend that pressure from contrarians and the risk of being\naccused of alarmism may be part of the explanation for climate\nscientists’ tendency to err on the side of caution in their\npredictions related to climate change. Drawing these threads together,\nBiddle and Leuschner (2015) suggest that contrarian dissent has\nimpeded scientific progress in at least two ways:  \nby (1) forcing scientists to respond to a seemingly endless wave of\nunnecessary and unhelpful objections and demands and (2) creating an\natmosphere in which scientists fear to address certain topics and/or\nto defend hypotheses as forcefully as they believe is appropriate.\n(Biddle & Leuschner 2015: 269)  \nThey argue that, while dissent is science is often epistemically\nfruitful, the dissent expressed by climate contrarians has tended to\nbe epistemically detrimental (see also Biddle et al. 2017; Leuschner\n2018). \nThe issue of anthropogenic climate change raises a host of challenging\nethical questions. Most of these are beyond the scope of this entry on\nclimate science. A very brief discussion is provided here\nnevertheless, because the questions are important and because a full\nentry on the topic is not yet available.  \nThe basic ethical question is: What ought to be done about\nanthropogenic climate change, and by whom? The question arises because\nthere is good evidence that climate change is already having harmful\nimpacts on both humans and non-human nature, and because continued\nhigh rates of greenhouse gas emission can be expected to bring\nadditional and more devastating harms in the future (Field et al.\n2014). Attempting to address this basic ethical question, however,\nleads to further, complex questions of global and intergenerational\njustice, as well as to questions regarding our ethical obligations to\nnon-human nature. Here are just a few examples: Do some nations,\nincluding those that have emitted large quantities of greenhouse gases\nin the past, have an obligation to bear more of the costs of climate\nchange mitigation and adaptation than other nations? (See, e.g., Baer\net al. 2009; Caney 2011; Singer 2016: Ch.2.) When considering actions\nto mitigate climate change, how should the harms and benefits to\nfuture generations be weighed against those affecting people today?\n(See, e.g., Broome 2008; Gardiner 2006.) How should impacts of climate\nchange on non-human nature, including loss of biodiversity, be taken\ninto account? (See, e.g., Palmer 2011.) Are there circumstances in\nwhich proposed geoengineering solutions—such as injecting\nsulfate aerosols into the stratosphere or seeding the oceans with\ncarbon-absorbing phytoplankton—are ethically acceptable? (See,\ne.g., Gardiner 2010; Preston 2013.) There is a large and growing\nphilosophical literature engaging with these and related questions;\nsome anthologies and book-length works include Arnold 2011; Broome\n2012; Gardiner 2011; Gardiner et al. 2010; Garvey 2008; Maltais and\nMcKinnon 2015, Preston 2016; Shue 2014; Singer 2016.","contact.mail":"wendy.parker@durham.ac.uk","contact.domain":"durham.ac.uk"}]
