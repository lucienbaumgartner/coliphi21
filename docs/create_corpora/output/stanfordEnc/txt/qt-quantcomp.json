[{"date.published":"2006-12-03","date.changed":"2019-09-30","url":"https://plato.stanford.edu/entries/qt-quantcomp/","author1":"Amit Hagar","author2":"Michael Cuffaro","author1.info":"https://hagara.pages.iu.edu/","author2.info":"http://www.michaelcuffaro.com/","entry":"qt-quantcomp","body.text":"\n\n\nCombining physics, mathematics and computer science, quantum computing\nand its sister discipline of\n quantum information\n have developed in the past few decades from visionary ideas to two of\nthe most fascinating areas of quantum theory. General interest and\nexcitement in quantum computing was initially triggered by Peter Shor\n(1994) who showed how a quantum algorithm could exponentially\n“speed-up” classical computation and factor large numbers\ninto primes far more efficiently than any (known) classical algorithm.\nShor’s algorithm was soon followed by several other algorithms\nthat aimed to solve combinatorial and algebraic problems, and in the\nyears since theoretical study of quantum systems serving as\ncomputational devices has achieved tremendous progress. Common belief\nhas it that the implementation of Shor’s algorithm on a large\nscale quantum computer would have devastating consequences for current\ncryptography protocols which rely on the premise that all known\nclassical worst-case algorithms for factoring take time\nexponential in the length of their input (see, e.g., Preskill 2005).\nConsequently, experimentalists around the world are engaged in\nattempts to tackle the technological difficulties that prevent the\nrealisation of a large scale quantum computer. But regardless whether\nthese technological problems can be overcome (Unruh 1995; Ekert and\nJozsa 1996; Haroche and Raimond 1996), it is noteworthy that no proof\nexists yet for the general superiority of quantum computers\nover their classical counterparts.\n\n\nThe philosophical interest in quantum computing is manifold. From a\nsocial-historical perspective, quantum computing is a domain where\nexperimentalists find themselves ahead of their fellow theorists.\nIndeed, quantum mysteries such as\n entanglement\n and\n nonlocality\n were historically considered a philosophical quibble, until\nphysicists discovered that these mysteries might be harnessed to\ndevise new efficient algorithms. But while the technology for\nharnessing the power of 50–100 qubits (the basic unit of information\nin the quantum computer) is now within reach (Preskill 2018), only a\nhandful of quantum algorithms exist, and the question of whether these\ncan truly outperform any conceivable classical alternative is still\nopen. From a more philosophical perspective, advances in quantum\ncomputing may yield foundational benefits. For example, it may turn\nout that the technological capabilities that allow us to isolate\nquantum systems by shielding them from the effects of\n decoherence\n for a period of time long enough to manipulate them will also allow\nus to make progress in some fundamental problems in the foundations of\nquantum theory itself. Indeed, the development and the implementation\nof efficient quantum algorithms may help us understand better the\nborder between classical and quantum physics (Cuffaro 2017, 2018a; cf.\nPitowsky 1994, 100), and perhaps even illuminate fundamental concepts\nsuch as\n measurement\n and\n causality.\n Finally, the idea that abstract mathematical concepts such as\ncomputability and complexity may not only be\ntranslated into physics, but also re-written by physics bears\ndirectly on the autonomous character of computer science and the\nstatus of its theoretical entities—the so-called\n“computational kinds”. As such it is also relevant to the\nlong-standing philosophical debate on the relationship between\nmathematics and the physical world.\n\nThe mathematical model for a “universal” computer was\ndefined long before the invention of computers and is called the\n Turing machine\n (Turing 1936). A Turing machine consists of an unbounded tape, a head\ncapable of reading from and writing to it which can occupy one of a\npotentially infinite number of possible states, and an instruction\ntable (i.e. a transition function). This table, given the head’s\ninitial state and the input it reads from the tape in that state,\ndetermines (a) the symbol that the head will write on the tape, (b)\nthe internal state it will occupy, and (c) the displacement of the\nhead on the tape. In 1936 Turing showed that since one can encode the\ninstruction table of a Turing machine \\(T\\) and express it as a binary\nnumber \\(\\#(T)\\), there exists a universal Turing machine \\(U\\) that\ncan simulate the instruction table of any Turing machine on any given\ninput. That the Turing machine model captures the concept of\ncomputability in its entirety is the essence of the\n Church-Turing thesis,\n according to which any effectively calculable function can be\ncomputed using a Turing machine. Admittedly, no counterexample to this\nthesis (which is the result of convergent ideas of Turing, Post,\nKleene and Church) has yet been found. But since it identifies the\nclass of computable functions with the class of those functions which\nare computable using a Turing machine, this thesis involves both a\nprecise mathematical notion and an informal and intuitive notion,\nhence cannot be proved or disproved. Simple cardinality considerations\nshow, however, that not all functions are Turing-computable (the set\nof all Turing machines is countable, while the set of all functions\nfrom the natural numbers to the natural numbers is not), and the\ndiscovery of this fact came as a complete surprise in the 1930s (Davis\n1958). \nComputability, or the question whether a function can be computed, is\nnot the only question that interests computer scientists. Beginning\nespecially in the 1960s (Cobham 1965; Edmonds 1965; Hartmanis and\nStearns 1965), the question of the cost of computing a\nfunction (which was to some extent already anticipated in 1956 by\nGödel) also came to be of great importance. This cost, also known\nas\n computational complexity,\n is measured naturally in the physical resources (e.g., time, space,\nenergy) invested in order to solve the computational problem at hand.\nComputer scientists classify computational problems according to the\nway their cost function behaves as a function of their input size,\n\\(n\\) (the number of bits required to store the input) and in\nparticular, whether it increases exponentially or polynomially with\n\\(n\\). Tractable problems are those which can be solved in\npolynomial cost, while intractable problems are those which\ncan only be solved with exponential cost (the former solutions are\ncommonly regarded as efficient although an exponential-time\nalgorithm could turn out to be more efficient than a polynomial-time\nalgorithm for some range of input sizes). \nSo far, the Turing machines we have been discussing have been\ndeterministic; for such machines, their behaviour at any given time is\nwholly determined by their state plus whatever their input happens to\nbe. In other words such machines have a unique “instruction\ntable” (i.e. transition function). We can generalise the Turing\nmodel, however, by allowing a machine to instantiate more than one\ntransition function simultaneously. A nondeterministic Turing\nmachine (NTM), upon being presented with a given input in a given\nstate, is allowed to ‘choose’ which of its transition\nfunctions to follow, and we say that it solves a given problem\nwhenever, given some input, there exists at least one path through its\nstate space leading to a solution. Exactly how an NTM\n“chooses” whether to follow one transition function rather\nthan another is left undefined (in his 1936 paper, Turing originally\nconceived these choices as those of an external operator). In\nparticular, we do not assume that any probabilities are attached to\nthese choices. In a probabilistic Turing machine (PTM), on\nthe other hand, we characterise the computer’s choices by\nassociating a particular probability with each of its possible\ntransitions. \nProbabilistic and deterministic Turing machines (DTMs) have different\nsuccess criteria. A successful deterministic algorithm for a given\nproblem is guaranteed to yield the correct answer given its input. Of\na successful probabilistic algorithm, on the other hand, we only\ndemand that it yield a correct answer with “high”\nprobability (minimally, we demand that it be strictly greater than\n1/2). It was believed, until relatively recently, that for some\nproblems (see, e.g. Rabin 1976) probabilistic algorithms are\ndramatically more efficient than any deterministic alternatives; in\nother words that the set or “class” of problems\nefficiently solvable by PTM is larger than the class of problems\nefficiently solvable by DTM. Fascinatingly, evidence has been mounting\nin recent years (e.g. Agrawal, Kayal, and Saxena 2004) that this is\nnot the case, and it is now believed that the PTM model in fact does\nnot offer a computational advantage in this sense over the DTM model\n(Arora and Barak 2009 Ch. 20). Probabilistic (Turing) computation is\nnevertheless interesting to consider, because abstractly a\nquantum computer is just a variation on the PTM which\ndoes appear to offer computational advantages over\ndeterministic computation, although as already mentioned this\nconjecture still awaits a proof. See Hagar (2007) and Cuffaro (2018b)\nfor divergent opinions over what this purported quantum computational\nadvantage tells us about the theory of computational complexity as a\nwhole. \nThe class \\(\\mathbf{P}\\) (for Polynomial) is the class containing all\nthe computational decision problems that can be solved by a DTM in\npolynomial time. The class NP (for Non-deterministic\nPolynomial) is the class containing all the computational decision\nproblems that can be solved by an NTM in polynomial\n time.[1]\n The most famous problems in NP are called\n“NP-complete”, where\n“complete” designates the fact that these problems stand\nor fall together: Either they are all tractable, or none of them is!\nIf we knew how to solve an NP-complete problem\nefficiently (i.e., with polynomial cost) we could use it to\nefficiently solve any other problem in NP (Cook\n1971). Today we know of hundreds of examples of\nNP-complete problems (Garey and Johnson 1979), all of\nwhich are reducible one to another with polynomial slowdown, and since\nthe best known algorithm for any of these problems is exponential, the\nwidely believed conjecture is that there is no polynomial algorithm\nthat can solve them. Clearly \\(\\mathbf{P} \\subseteq \\mathbf{NP}\\).\nProving or disproving the conjecture that \\(\\mathbf{P} \\ne\n\\mathbf{NP}\\), however, remains perhaps one of the most important open\nquestions in computer science and complexity theory. \nAlthough the original Church-Turing thesis involves the\nabstract mathematical notion of computability, physicists as\nwell as computer scientists often interpret it as saying something\nabout the scope and limitations of physical computing\nmachines. Wolfram (1985) claims that any physical system can be\nsimulated (to any degree of approximation) by a universal Turing\nmachine, and that complexity bounds on Turing machine simulations have\nphysical significance. For example, if the computation of the minimum\nenergy of some system of \\(n\\) particles requires at least an\nexponentially increasing number of steps in \\(n\\), then the actual\nrelaxation of this system to its minimum energy state will also take\nexponential time. Aharonov (1999) strengthens this thesis (in the\ncontext of showing its putative incompatibility with quantum\nmechanics) when she says that a PTM can simulate any reasonable\nphysical device at polynomial cost. In order for the physical\nChurch-Turing thesis to make sense we have to relate physical space\nand time parameters to their computational counterparts: memory\ncapacity and number of computation steps, respectively. There are\nvarious ways to do that, leading to different formulations of the\nthesis (Pitowsky 1990). For example, one can encode the set of\ninstructions of a universal Turing machine and the state of its\ninfinite tape in the binary development of the position coordinates of\na single particle. Consequently, one can physically\n‘realise’ a universal Turing machine as a billiard ball\nwith hyperbolic mirrors (Moore 1990; Pitowsky 1996). For the most\nintuitive connection between abstract Turing machines and physical\ndevices see the pioneering work of Gandy (1980), simplified later by\nSieg and Byrnes (1999), and discussed, for example, in Copeland\n(2018). It should be stressed that there is no relation between the\noriginal Church-Turing thesis and its physical version (Pitowsky and\nShagrir 2003), and while the former concerns the concept of\ncomputation that is relevant to logic (since it is strongly tied to\nthe notion of proof which requires validation), it\ndoes not analytically entail that all computations should be\nsubject to validation. Indeed, there is a long historical tradition of\nanalog computations which use continuous physical processes (Dewdney\n1984), and the output of these computations is validated either by\nrepetitive “runs” or by validating the physical theory\nthat presumably governs the behaviour of the analog computer. \nDo physical processes exist which contradict the physical\nChurch-Turing thesis? Apart from analog computation, there exist at\nleast two main kinds of example purporting to show that the notion of\nrecursion, or Turing-computability, is not a natural physical\nproperty (Pour-el and Richards 1981; Pitowsky 1990; Hogarth 1994).\nAlthough the physical systems involved (a specific initial condition\nfor the wave equation in three dimensions and an exotic solution to\nEinstein’s field equations, respectively) are somewhat\ncontrived, a thriving school of “hypercomputation” that\naspires to extend the limited examples of physical\n“hypercomputers” and in so doing to physically\n“compute” the non-Turing-computable has nevertheless\nemerged (for a review see Copeland (2002); for a criticism: Davis\n(2003); for a recent proposal and response to criticisms see\nAndréka et al. (2018)). Quantum hypercomputation is\nrarely discussed in the literature (see, e.g., Adamyan, Calude, and\nPavlov 2004), but the most concrete attempt to harness quantum theory\nto compute the non-computable is the suggestion to use the quantum\nadiabatic algorithm (see below) to solve Hilbert’s Tenth Problem\n(Kieu 2002, 2004)—a Turing-undecidable problem equivalent to the\nhalting problem. Criticism, however, has exposed the unphysical\ncharacter of the alleged quantum adiabatic hypercomputer (see Hodges\n2005; Hagar and Korolev 2007). \nSetting aside “hypercomputers”, even if we restrict\nourselves only to Turing-computable functions, one can still find many\nproposals in the literature that purport to display\n“short-cuts” in computational resources. Consider, e.g.,\nthe DNA model of computation that was claimed (Adleman 1994; Lipton\n1995) to solve NP-complete problems in polynomial\ntime. A closer inspection shows that the cost of the computation in\nthis model is still exponential since the number of molecules in the\nphysical system grows exponentially with the size of the problem. Or\ntake an allegedly instantaneous solution to another\nNP-complete problem using a construction of rods and\nballs (Vergis, Steiglitz, and Dickinson 1986) that unfortunately\nignores the accumulating time-delays in the rigid rods that result in\nan exponential overall slowdown. It appears that these and other\nsimilar models cannot serve as counter-examples to the physical\nChurch-Turing thesis (as far as complexity is concerned) since they\nall require some exponential physical resource. Note, however, that\nall these models are based on classical physics, hence the unavoidable\nquestion: Can the shift to quantum physics allow us to find\n“short-cuts” in computational resources? The quest for the\nquantum computer began with the possibility of giving a positive\nanswer to this question. \nThe idea of a computational device based on quantum mechanics was\nexplored already in the 1970s by physicists and computer scientists.\nAs early as 1969 Steven Wiesner suggested quantum information\nprocessing as a possible way to better accomplish cryptologic tasks.\nBut the first four published papers on quantum information (Wiesner\npublished his only in 1983), belong to Alexander Holevo (1973), R. P.\nPoplavskii (1975), Roman Ingarden (1976), and Yuri Manin (1980).\nBetter known are contributions made in the early 1980s by Charles H.\nBennett of the IBM Thomas J. Watson Research Center, Paul A. Benioff\nof Argonne National Laboratory in Illinois, David Deutsch of the\nUniversity of Oxford, and Richard P. Feynman of the California\nInstitute of Technology. The idea emerged when scientists were\ninvestigating the fundamental physical limits of computation. If\ntechnology continued to abide by “Moore’s Law” (the\nobservation made in 1965 by Gordon Moore, co-founder of Intel, that\nthe number of transistors per square inch on integrated circuits had\ndoubled every 18 months since the integrated circuit was invented),\nthen the continually shrinking size of circuitry packed onto silicon\nchips would eventually reach a point where individual elements would\nbe no larger than a few atoms. But since the physical laws that govern\nthe behaviour and properties of the putative circuit at the atomic\nscale are inherently quantum mechanical in nature, not classical, the\nnatural question arose whether a new kind of computer could be devised\nbased on the principles of quantum physics. \nInspired by Ed Fredkin’s ideas on reversible computation (see\nHagar 2016), Feynman was among the first to attempt to provide an\nanswer to this question by producing an abstract model in 1982 that\nshowed how a quantum system could be used to do computations. He also\nexplained how such a machine would be able to act as a simulator for\nquantum physics, conjecturing that any classical computer could do the\nsame task only inefficiently. In 1985 David Deutsch proposed the first\nuniversal quantum Turing machine and paved the way to the quantum\ncircuit model (Deutsch 1989). The young and thriving domain also\nattracted philosophers’ attention. In 1983 David Albert showed\nhow a quantum mechanical automaton behaves remarkably differently from\na classical automaton, and in 1990 Itamar Pitowsky raised the question\nof whether the superposition principle may allow quantum computers to\nefficiently solve NP-complete problems. He also\nstressed that although one could in principle ‘squeeze’\ninformation of exponential complexity into polynomially many quantum\nstates, the real problem lay in the efficient retrieval of this\ninformation. \nProgress in quantum algorithms began in the 1990s, with the discovery\nof the Deutsch-Josza algorithm (1992) and of Simon’s algorithm\n(1994). The latter supplied the basis for Shor’s algorithm for\nfactoring. Published in 1994, this algorithm marked a\n‘phase transition’ in the development of quantum computing\nand sparked a tremendous interest even outside the physics community.\nIn that year the first experimental realisation of the quantum\nCNOT gate with trapped ions was proposed by Cirac and Zoller\n(1995). In 1995, Peter Shor and Andrew Steane proposed (independently)\nthe first scheme for quantum error-correction. In that same year the\nfirst realisation of a quantum logic gate was done in Boulder,\nColorado, following Cirac and Zoller’s proposal. In 1996, Lov\nGrover from Bell Labs invented a quantum search algorithm which yields\na provable (though only quadratic) “speed-up” compared to\nits classical counterparts. A year later the first model for quantum\ncomputation based on nuclear magnetic resonance (NMR) techniques was\nproposed. This technique was realised in 1998 with a 2-qubit register,\nand was scaled up to 7 qubits in the Los Alamos National Lab in\n2000. \nSince 2000 the field has seen tremendous growth. New paradigms of\nquantum algorithms have appeared, such as adiabatic algorithms,\nmeasurement-based algorithms, and\ntopological-quantum-field-theory-based algorithms, as well as new\nphysical models for realising a large scale quantum computer with cold\nion traps, quantum optics (using photons and optical cavity),\ncondensed matter systems and solid state physics (meanwhile, the first\nNMR model had turned out to be a dead-end with respect to scaling; see\nDiVincenzo (2000)). The basic questions, however, remain open even\ntoday: (1) theoretically, can quantum algorithms efficiently solve\nclassically intractable problems? (2) operationally, can we actually\nrealise a large scale quantum computer to run these algorithms? \nIn this section we review the basic paradigm for quantum algorithms,\nnamely the quantum circuit model, which is composed of the basic\nquantum units of information (qubits) and the basic logical\nmanipulations thereof (quantum gates). For more detailed introductions\nsee Nielsen and Chuang (2000) and Mermin (2007). \nThe qubit is the quantum analogue of the bit, the classical\nfundamental unit of information. It is a mathematical object with\nspecific properties that can be realised in an actual physical system\nin many different ways. Just as the classical bit has a state\n(either 0 or 1), a qubit also has a state. Yet contrary to the\nclassical bit, \\(\\lvert 0\\rangle\\) and \\(\\lvert 1\\rangle\\) are but two\npossible states of the qubit, and any linear combination\n(superposition) thereof is also physically possible. In\ngeneral, thus, the physical state of a qubit is the superposition\n\\(\\lvert\\psi \\rangle = \\alpha \\lvert 0\\rangle + \\beta \\lvert\n1\\rangle\\) (where \\(\\alpha\\) and \\(\\beta\\) are complex numbers). The\nstate of a qubit can be described as a vector in a two-dimensional\nHilbert space, a complex vector space (see the entry on\n quantum mechanics).\n The special states \\(\\lvert 0\\rangle\\) and \\(\\lvert 1\\rangle\\) are\nknown as the computational basis states, and form an\northonormal basis for this vector space. According to quantum theory,\nwhen we try to measure the qubit in this basis in order to determine\nits state, we get either \\(\\lvert 0\\rangle\\) with probability \\(\\lvert\n\\alpha\\rvert^2\\) or \\(\\lvert 1\\rangle\\) with probability \\(\\lvert\n\\beta\\rvert^2\\). Since \\(\\lvert \\alpha\\rvert^2 + \\lvert\\beta\\rvert^2 =\n1\\) (i.e., the qubit is a unit vector in the aforementioned\ntwo-dimensional Hilbert space), we may (ignoring the overall phase\nfactor) effectively write its state as \\(\\lvert \\psi \\rangle =\\)\ncos\\((\\theta)\\lvert 0\\rangle + e^{i\\phi}\\)sin\\((\\theta)\\lvert\n1\\rangle\\), where the numbers \\(\\theta\\) and \\(\\phi\\) define a point\non the unit three-dimensional sphere, as shown here. This sphere is\noften called the Bloch sphere, and it provides a useful means\nto visualise the state of a single qubit. \nThe Bloch Sphere \nSince \\(\\alpha\\) and \\(\\beta\\) are complex and therefore continuous\nvariables one might think that a single qubit is capable of storing an\ninfinite amount of information. When measured, however, it yields only\nthe classical result (0 or 1) with certain probabilities specified by\nthe quantum state. In other words, the measurement changes\nthe state of the qubit, “collapsing” it from a\nsuperposition to one of its terms. In fact one can prove (Holevo 1973)\nthat the amount of information actually retrievable from a single\nqubit (what Timpson (2013, 47ff.) calls its “accessible\ninformation”) is no more than one bit. If the qubit is\nnot measured, however, the amount of “hidden”\ninformation it “stores” (what Timpson calls its\n“specification information”) is conserved under its\n(unitary) dynamical evolution. This feature of quantum mechanics\nallows one to manipulate the information stored in unmeasured qubits\nwith quantum gates (i.e. unitary transformations), and is one of the\nsources for the putative power of quantum computers. \nTo see why, let us suppose we have two qubits at our disposal. If\nthese were classical bits, then they could be in four possible states\n(00, 01, 10, 11). Correspondingly, a pair of qubits has\nfour computational basis states (\\(\\lvert 00\\rangle\\),\n\\(\\lvert 01\\rangle\\), \\(\\lvert 10\\rangle\\), \\(\\lvert 11\\rangle)\\). But\nwhile a single classical two-bit register can store these numbers\nonly one at a time, a pair of qubits can also exist in a\nsuperposition of these four basis states, each with its own complex\ncoefficient (whose mod square, being interpreted as a probability, is\nnormalised). For example, using a “Hadamard\ngate”—which unitarily transforms a single qubit to the\nstate \\(\\frac{\\lvert 0\\rangle + \\lvert 1\\rangle}{\\sqrt 2}\\) whenever\nit is in the state \\(\\lvert 0\\rangle\\), and to the state\n\\(\\frac{\\lvert 0\\rangle - \\lvert 1\\rangle}{\\sqrt 2}\\) whenever it is\nin the state \\(\\lvert 1\\rangle\\)—we can transform the\n\\(n\\)-qubit state \\(\\lvert 0...01 \\rangle\\) as\nfollows: \nwhere \\(\\lvert - \\rangle =_{df} \\frac{| 0 \\rangle - | 1 \\rangle}{\\sqrt\n2}\\). The resulting state is a superposition of \\(2^n\\) terms and can\nbe imagined to “store” that many bits of (specification)\ninformation. The difficult task, however, is to use this information\nefficiently in light of the bound on the state’s accessible\ninformation. \nClassical computational gates are Boolean logic gates that manipulate\ninformation stored in bits. In quantum computing such gates are\nrepresented by matrices, and can be visualised as rotations over the\nBloch sphere. This visualisation represents the fact that quantum\ngates are unitary operators, i.e., they preserve the norm of the\nquantum state (if \\(U\\) is a matrix describing a single qubit gate,\nthen \\(U^{\\dagger}U=I\\), where \\(U^{\\dagger}\\) is the adjoint\nof \\(U\\), obtained by transposing and then complex-conjugating \\(U)\\).\nIn classical computing some gates are “universal”. For\nexample the NAND gate is a gate that evaluates the function\n“not both A and B” over its two inputs. By stringing\ntogether a number of NAND gates it is possible to compute any\ncomputable function. Another universal gate is the NOR gate,\nwhich evaluates the function “not (A or B)”. In the\ncontext of quantum computing it was shown (DiVincenzo 1995) that\ntwo-qubit gates (i.e. which transform two qubits) are sufficient to\nrealise a general quantum circuit, in the sense that a circuit\ncomposed exclusively from a small set of one- and two-qubit gates can\napproximate to arbitrary accuracy any unitary transformation of \\(n\\)\nqubits. Barenco et. al. (1995) showed in particular that any\nmultiple qubit logic gate may be composed in this sense from a\ncombination of single-qubit gates and the two-qubit controlled-not\n(CNOT) gate, which either flips or preserves its\n“target” input bit depending on the state of its\n“control” input bit (specifically: in a CNOT gate\nthe output state of the target qubit is the result of an operation\nanalogous to the classical exclusive-OR (XOR) gate on the\ninputs). One general feature of quantum gates that distinguishes them\nfrom classical gates is that they are always reversible: the inverse\nof a unitary matrix is also a unitary matrix, and thus a quantum gate\ncan always be inverted by another quantum gate. \nThe CNOT Gate \nUnitary gates manipulate information stored in the “quantum\nregister”—a quantum system—and in this sense\nordinary (unitary) quantum evolution can be regarded as a computation.\nIn order to read the result of this computation, however, the quantum\nregister must be measured. The measurement gate is a non-unitary gate\nthat “collapses” the quantum superposition in the register\nonto one of its terms with a probability corresponding to its complex\ncoefficient. Usually this measurement is done in the computational\nbasis (see the previous section), but since quantum mechanics allows\none to express an arbitrary state as a linear combination of basis\nstates, provided that the states are orthonormal (a condition that\nensures normalisation) one can in principle measure the\nregister in any arbitrary orthonormal basis. This, however,\ndoesn’t mean that measurements in different bases are equivalent\ncomplexity-wise. Indeed, one of the difficulties in constructing\nefficient quantum algorithms stems exactly from the fact that\nmeasurement collapses the state, and some measurements are much more\ncomplicated than others. \nQuantum circuits are similar to classical computer circuits in that\nthey consist of wires and logical gates. The wires\nare used to carry the information, while the gates manipulate it (note\nthat the wires are abstract and do not necessarily correspond to\nphysical wires; they may correspond to a physical particle, e.g. a\nphoton, moving from one location to another in space, or even to\ntime-evolution). Conventionally, the input of the quantum circuit is\nassumed to be a number of qubits each initialised to a computational\nbasis state (typically \\(\\lvert 0\\rangle\\)). The output state of the\ncircuit is then measured in the computational basis, or in any other\narbitrary orthonormal basis. The first quantum algorithms (i.e.\nDeutsch-Jozsa, Simon, Shor and Grover) were constructed in this\nparadigm. Additional paradigms for quantum computing exist today that\ndiffer from the quantum circuit model in many interesting ways. So\nfar, however, they all have been demonstrated to be computationally\nequivalent to the circuit model (see below), in the sense that any\ncomputational problem that can be solved by the circuit model can be\nsolved by these new models with only a polynomial overhead in\ncomputational resources. This is analogous to the fact that in\nclassical computation every “reasonable” model can be\nefficiently simulated by any other. For discussion see Cuffaro (2018b,\n274). \nAlgorithm design is a highly complicated task, and in quantum\ncomputing, delicately leveraging the features of quantum mechanics in\norder to make our algorithms more efficient makes the task even more\ncomplicated. But before discussing this aspect of quantum algorithm\ndesign, let us first convince ourselves that quantum computers can be\nharnessed to perform standard, classical, computation without any\ncomputational speed-up. In some sense this is obvious, given the\nbelief in the universal character of quantum mechanics, and the\nobservation that any quantum computation that is diagonal in the\ncomputational basis, i.e., that involves no interference between the\nqubits, is effectively classical. Yet the demonstration that quantum\ncircuits can be used to simulate classical circuits is not\nstraightforward (recall that the former are always reversible while\nthe latter use gates which are in general irreversible). Indeed,\nquantum circuits cannot be used directly to simulate\nclassical computation, but the latter can still be simulated on a\nquantum computer using an intermediate gate, namely the\nToffoli gate. This universal classical gate has three input\nbits and three output bits. Two of the input bits are control bits,\nunaffected by the action of the gate. The third input bit is a target\nbit that is flipped if both control bits are set to 1, and otherwise\nis left alone. This gate is reversible (its inverse is itself), and by\nstringing a number of such gates together one can simulate any\nclassical irreversible circuit. \nConsequently, using the quantum version of the Toffoli gate\n(which by definition permutes the computational basis states similarly\nto the classical Toffoli gate) one can simulate, although\nrather tediously, irreversible classical logic gates with quantum\nreversible ones. Quantum computers are thus capable of performing any\ncomputation which a classical deterministic computer can do. \nWhat about probabilistic computation? Not surprisingly, a quantum\ncomputer can also simulate this type of computation by using another\nfamous quantum gate, namely the Hadamard gate, a single-qubit gate\nwhich receives as input the state \\(\\lvert 0\\rangle\\) and produces the\nstate \\(\\frac{\\lvert 0\\rangle + \\lvert 1\\rangle}{\\sqrt{2}}\\).\nMeasuring this output state yields \\(\\lvert 0\\rangle\\) or \\(\\lvert\n1\\rangle\\) with 50/50 probability, which can be used to simulate a\nfair coin toss. \nThe Hadamard Gate \nObviously, if quantum algorithms could be used only to simulate\nclassical algorithms, then the technological advancement in\ninformation storage and manipulation, encapsulated in\n“Moore’s law”, would have only trivial consequences\non computational complexity theory, leaving the latter unaffected by\nthe physical world. But while some computational problems will always\nresist quantum “speed-up” (in these problems the\ncomputation time depends on the input, and this feature will lead to a\nviolation of unitarity hence to an effectively classical computation\neven on a quantum computer—see Myers (1997) and Linden and\nPopescu (1998)), the hope is, nonetheless, that quantum algorithms may\nnot only simulate classical ones, but that they will actually\noutperform the latter in some cases, and in so doing help to\nre-define the abstract notions of tractability and intractability and\nviolate the physical Church-Turing thesis, at least as far as\ncomputational complexity is concerned. \nThe first quantum algorithms were designed to solve problems which\nessentially involve the use of an “oracle”, so let us\nbegin by explaining this term. Oracles are used by computer scientists\nas conceptual aids in the complexity-theoretic analysis of algorithms.\nWe can think of an oracle as a kind of imaginary magic black box\n(Arora and Barak (2009, 72–73); Aaronson (2013a, 29ff.)) to\nwhich, like the famous oracle at Delphi, one poses (yes or no)\nquestions. Unlike that ancient oracle, the oracles considered in\ncomputer science always return an answer in a single time\nstep. For example, we can imagine an oracle to determine whether a\ngiven Boolean formula is satisfiable or not: Given as input the\ndescription of a particular propositional formula, the oracle\noutputs—in a single time step—a single bit indicating\nwhether or not there is a truth-value assignment satisfying that\nformula. Obviously such a machine does not really\nexist—SAT is an NP-complete problem—but\nthat is not the point. The point of using such imaginary devices is to\nabstract away from certain “implementational details”\nwhich are for whatever reason deemed unimportant for the\ncomplexity-theoretic analysis of a given problem. For example,\nSimon’s problem (Simon 1994, see below) is that of determining\nthe period of a given function \\(f\\) that is periodic under bit-wise\nmodulo-2 addition. Relative to Simon’s problem, we judge the\ninternal complexity of \\(f\\) to be unimportant, and so abstract away\nfrom it by imagining that we have an oracle to evaluate it in a single\nstep. As useful as these conceptual devices are, however, their\nusefulness has limitations. To take one example, there are oracles\nrelative to which P = NP, as well as\noracles relative to which P \\(\\not =\\)\nNP. This (and many other) questions are not clarified\nby oracles (see Fortnow 1994). \nDeutsch (1989) asks the following question: Suppose we have a function\n\\(f\\) which can be either constant—i.e. such that it produces\nthe same output value for each of its possible inputs, or\nbalanced—i.e. such that the output of one half of its possible\ninputs is the opposite of the output of the other half. The particular\nexample considered is the function \\(f : \\{0,1\\} \\rightarrow\n\\{0,1\\}\\), which is constant if \\(f\\)(0) \\(= f\\)(1) and balanced if\n\\(f\\)(0) \\(\\ne f\\)(1). Classically it would take two\nevaluations of the function to tell whether it is one or the other.\nQuantumly, we can answer this question in one evaluation. For\nDeutsch, the explanation for this complexity reduction involves an\nappeal to “many computational worlds” (see section 5.1.1).\n Arguably, however, a fully satisfactory answer appeals only to the\nsuperposition principle and entanglement (Bub 2010). \nAfter initially preparing the first and second qubits of the computer\nin the state \\(\\lvert 0\\rangle\\lvert 0\\rangle\\), one then\n“flips” the second qubit using a “NOT” gate\n(i.e. a Pauli X operation) to \\(\\lvert 1 \\rangle\\),\nand then subjects each qubit to a Hadamard gate. We now send the two\nqubits through an oracle or ‘black box’ which we imagine\nas a unitary gate, \\(\\mathbf{U}_f\\), representative of the function\nwhose character (of being either constant or balanced) we wish to\ndetermine. We define \\(\\mathbf{U}_f\\) so that it takes inputs like\n\\(\\lvert x,y\\rangle\\) to \\(\\lvert x, y\\oplus f (x)\\rangle\\), where\n\\(\\oplus\\) is addition modulo two (i.e. exclusive-or). The first qubit\nis then fed into a further Hadamard gate, and the final output of the\nalgorithm (prior to measurement) is the state: \\[\\pm\\lvert f(0)\\oplus\nf(1)\\rangle~\\lvert - \\rangle,\\] where \\(\\lvert - \\rangle =_{df}\n\\frac{| 0 \\rangle - | 1 \\rangle}{\\sqrt 2}\\). Since \\(f\\)(0)\\(\\oplus\nf\\)(1) is 0 if the function is constant and 1 if the function is\nbalanced, a single measurement of the first qubit suffices to retrieve\nthe answer to our original question regarding the function’s\nnature. And since there are two possible constant functions and two\npossible balanced functions from \\(f : \\{0,1\\} \\rightarrow \\{0,1\\}\\),\nwe can characterise the algorithm as distinguishing, using only one\noracle call, between two quantum disjunctions without finding out the\ntruth values of the disjuncts themselves, i.e. without\ndetermining which balanced or which constant\nfunction \\(f\\) is (Bub 2010). \nA generalisation of Deutsch’s problem, called the Deutsch-Jozsa\nproblem (Deutsch and Jozsa 1992), enlarges the class of functions\nunder consideration so as to include all of the functions\n\\(f:\\{0,1\\}^n\\to\\{0,1\\}\\), i.e. rather than only considering \\(n =\n1\\). The best deterministic classical algorithm for determining\nwhether a given such function is constant or balanced requires\n\\(\\frac{2^{n}}{2}+1\\) queries to an oracle in order to solve this\nproblem. In a quantum computer, however, we can answer the question\nusing one oracle call. Generalising our conclusion regarding\nthe Deutsch algorithm, we may say that the Deutsch-Jozsa algorithm\nallows one to evaluate a global property of the function in one\nmeasurement because the output state is a superposition of balanced\nand constant states such that the balanced states all lie in a\nsubspace orthogonal to the constant states and can therefore be\ndistinguished from the latter in a single measurement (Bub 2006a) \nSuppose we have a Boolean function \\(f\\) on \\(n\\) bits that is 2-to-1,\ni.e. that takes \\(n\\) bits to \\(n-1\\) bits in such a way that for\nevery \\(n\\)-bit integer \\(x_1\\) there is an \\(n\\)-bit integer \\(x_2\\)\nfor which \\(f (x_{1}) = f (x_{2})\\). The function is moreover periodic\nin the sense that \\(f(x_1)\\) = \\(f(x_2)\\) if and only if \\(x_1 = x_2\n\\oplus a\\), where \\(\\oplus\\) designates bit-wise modulo 2 addition and\n\\(a\\) is an \\(n\\)-bit nonzero number called the period of\n\\(f\\). Simon’s problem is the problem to find \\(a\\) given \\(f\\).\nRelative to an oracle \\(U_f\\) which evaluates \\(f\\) in a single step,\nSimon’s quantum algorithm (Simon 1994) finds the period of \\(f\\)\nin a number of oracle calls that grows only linearly with the length\nof \\(n\\), while the best known classical algorithm requires an\nexponentially greater number of oracle calls. Simon’s algorithm\nreduces to Deutsch’s algorithm when \\(n=2\\), and can be regarded\nas an extension of the latter, in the sense that in both cases a\nglobal property of a function is evaluated in no more than a\n(sub-)polynomial number of oracle invocations, owing to the fact that\nthe output state of the computer just before the final measurement is\ndecomposed into orthogonal subspaces, only one of which contains the\nproblem’s solution. Note that one important difference between\nDeutsch’s and Simon’s algorithms is that the former yields\na solution with certainty, whereas the latter only yields a solution\nwith probability very close to 1. For more on the logical analysis of\nthese first quantum-circuit-based algorithms see Bub (2006a) and Bub\n(2010). \nThe algorithms just described, although demonstrating the potential\nsuperiority of quantum computers over their classical counterparts,\nnevertheless deal with apparently unimportant computational problems.\nMoreover the speed-ups in each of them are only relative to their\nrespective oracles. It is doubtful whether research into quantum\ncomputing would have attracted so much attention and evolved to its\ncurrent status if its merit could be demonstrated only with these\nproblems. But in 1994, Peter Shor realised that Simon’s\nalgorithm could be harnessed to solve a much more interesting and\ncrucial problem, namely factoring, which lies at the heart of\ncurrent cryptographic protocols such as RSA (Rivest, Shamir, and\nAdleman 1978). Shor’s algorithm has turned quantum computing\ninto one of the most exciting research domains in quantum\nmechanics. \nShor’s algorithm exploits the ingenious number theoretic\nargument that two prime factors \\(p,q\\) of a positive integer \\(N=pq\\)\ncan be found by determining the period of a function \\(f(x) = y^x\n\\textrm{mod} N,\\) for any \\(y < N\\) which has no common factors\nwith \\(N\\) other than 1 (Nielsen and Chuang 2000 App. 4). The period\n\\(r\\) of \\(f(x)\\) depends on \\(y\\) and \\(N\\). Once one knows it, one\ncan factor \\(N\\) if \\(r\\) is even and \\(y^{\\,\\frac{r}{2}} \\neq -1\\)\nmod \\(N\\), which will be jointly the case with probability greater\nthan \\(\\frac{1}{2}\\) for any \\(y\\) chosen randomly (if not, one\nchooses another value of \\(y\\) and tries again). The factors of \\(N\\)\nare the greatest common divisors of \\(y^{\\,\\frac{r}{2}} \\pm 1\\) and\n\\(N\\), which can be found in polynomial time using the well known\nEuclidean algorithm. In other words, Shor’s remarkable result\nrests on the discovery that the problem of factoring reduces\nto the problem of finding the period of a certain periodic function\n\\(f: Z_{n} \\rightarrow Z_{N}\\), where \\(Z_{n}\\) is the additive group\nof integers mod \\(n\\) (Note that \\(f(x) = y^{x}\\ \\textrm{mod}\\ N\\) so\nthat \\(f(x+r) = f(x)\\) if \\(x+r \\le n\\). The function is periodic if\n\\(r\\) divides \\(n\\) exactly, otherwise it is almost periodic). That\nthis problem can be solved efficiently by a quantum computer is hinted\nat by Simon’s algorithm, which considers the more restricted\ncase of functions periodic under bit-wise modulo-2 addition as opposed\nto the periodic functions under ordinary addition considered here. \nShor’s result is the most dramatic example so far of quantum\n“speed-up” of computation, notwithstanding the fact that\nfactoring is believed to be only in NP and\nnot in NP-complete (see Aaronson 2013a, 64–66).\nTo verify whether \\(n\\) is prime takes a number of steps which is a\npolynomial in \\(\\log_{2}n\\) (the binary encoding of a natural number\n\\(n\\) requires \\(\\log_{2}n\\) resources). But nobody knows how to\nfactor numbers into primes in polynomial time, and the best classical\nalgorithms we have for this problem are sub-exponential. This is yet\nanother open problem in the theory of computational complexity. Modern\ncryptography and Internet security protocols are based on these facts\n(Giblin 1993): It is easy to find large prime numbers fast, and it is\nhard to factor large composite numbers in any reasonable amount of\ntime. The discovery that quantum computers can solve\nfactoring in polynomial time has had, therefore, a dramatic\neffect. The implementation of the algorithm on a physical machine\nwould have economic, as well as scientific consequences (Alléaume\net al. 2014). \nIn a brilliant undercover operation, Agent 13 has managed to secure\ntwo crucial bits of information concerning the whereabouts of the\narch-villain Siegfried: the phone number of the secret hideout from\nwhich he intends to begin carrying out KAOS’s plans for world\ndomination, and the fact that the number is a listed one (apparently\nan oversight on Siegfried’s part). Unfortunately you and your\ncolleagues at CONTROL have no other information besides this. Can you\nfind Siegfried’s hideout using only this number and a phone\ndirectory? In theoretical computer science this task is known as an\nunstructured search. In the worst case, if there are \\(n\\) entries in\nthe directory, the computational resources required to find the entry\nwill be linear in \\(n\\). Grover (1996) showed how this task could be\ndone with a quantum algorithm using computational resources on the\norder of only \\(\\sqrt{n}\\). Agreed, this “speed-up” is\nmore modest than Shor’s since unstructured search belongs to the\nclass \\(\\mathbf{P}\\), but contrary to Shor’s case, where the\nclassical complexity of factoring is still unknown, here the\nsuperiority of the quantum algorithm, however modest, is definitely\nprovable. That this quadratic “speed-up” is also\nthe optimal quantum “speed-up” possible for this\nproblem was proved by Bennett, Bernstein, Brassard, & Vazirani\n(1997). \nAlthough the purpose of Grover’s algorithm is usually described\nas “searching a database”, it may be more accurate to\ndescribe it as “inverting a function”. Roughly speaking,\nif we have a function \\(y=f(x)\\) that can be evaluated on a quantum\ncomputer, Grover’s algorithm allows us to calculate \\(x\\) given\n\\(y\\). Inverting a function is related to searching a database because\nwe could come up with a function that produces a particular value of\n\\(y\\) if \\(x\\) matches a desired entry in a database, and another\nvalue of \\(y\\) for other values of \\(x\\). The applications of this\nalgorithm are far-reaching (even more so than foiling\nSiegfried’s plans for world domination). For example, it can be\nused to determine efficiently the number of solutions to an \\(N\\)-item\nsearch problem, hence to perform exhaustive searches on a class of\nsolutions to an NP-complete problem and substantially\nreduce the computational resources required for solving it. \nMany decades have passed since the discovery of the first quantum\nalgorithm, but so far little progress has been made with respect to\nthe “Holy Grail” of solving an\nNP-complete problem with a quantum-circuit. In 2000 a\ngroup of physicists from MIT and Northeastern University (Farhi et al.\n2000) proposed a novel paradigm for quantum computing that differs\nfrom the circuit model in several interesting ways. Their goal was to\ntry to solve with this algorithm an instance of the\nsatisfiability problem (see above), one of the most famous\nNP-complete problems (Cook 1971). \nAccording to the adiabatic theorem (e.g. Messiah 1961) and given\ncertain specific conditions, a quantum system remains in its lowest\nenergy state, known as the ground state, along an adiabatic\ntransformation in which the system is deformed slowly and smoothly\nfrom an initial Hamiltonian to a final Hamiltonian (as an\nillustration, think of moving a sleeping baby in a cradle from the\nliving room to the bedroom. If the transition is done slowly and\nsmoothly enough, and if the baby is a sound sleeper, then it will\nremain asleep during the whole transition). The most important\ncondition in this theorem is the energy gap between the ground state\nand the next excited state (in our analogy, this gap reflects how\nsound asleep the baby is). Being inversely proportional to the\nevolution time \\(T\\), this gap controls the latter. If this gap exists\nduring the entire evolution (i.e., there is no level crossing between\nthe energy states of the system), the theorem dictates that in the\nadiabatic limit (when \\(T\\rightarrow \\infty)\\) the system will remain\nin its ground state. In practice, of course, \\(T\\) is always finite,\nbut the longer it is, the less likely it is that the system will\ndeviate from its ground state during the time evolution. \nThe crux of the quantum adiabatic algorithm which rests on this\ntheorem lies in the possibility of encoding a specific instance of a\ngiven decision problem in a certain Hamiltonian (this can be done by\ncapitalising on the well-known fact that any decision problem can be\nderived from an optimisation problem by incorporating into it a\nnumerical bound as an additional parameter). One then starts the\nsystem in a ground state of another Hamiltonian which is easy to\nconstruct, and slowly evolves the system in time, deforming it towards\nthe desired Hamiltonian. According to the quantum adiabatic theorem\nand given the gap condition, the result of such a physical process is\nanother energy ground state that encodes the solution to the desired\ndecision problem. The adiabatic algorithm is thus a rather ‘laid\nback’ algorithm: one needs only to start the system in its\nground state, deform it adiabatically, and measure its final ground\nstate in order to retrieve the desired result. But whether or not this\nalgorithm yields the desired “speed-up” depends crucially\non the behaviour of the energy gap as the number of degrees of freedom\nin the system increases. If this gap decreases exponentially with the\nsize of the input, then the evolution time of the algorithm will\nincrease exponentially; if the gap decreases polynomially, the\ndecision problem so encoded could be solved efficiently in polynomial\ntime. Although physicists have been studying spectral gaps for almost\na century, they have never done so with quantum computing in mind. How\nthis gap behaves in general remains thus far an open\nempirical question. \nThe quantum adiabatic algorithm holds much promise (Farhi et al.\n2001). It has been shown (Aharonov et al. 2008) to be polynomially\nequivalent to the circuit model (that is, each model can simulate the\nother with only polynomial overhead in the number of qubits and\ncomputational steps), but the caveat that is sometimes left\nunmentioned is that its application to an intractable computational\nproblem may sometimes require solving another, as intractable a task\n(this general worry was first raised by a philosopher; see Pitowsky\n(1990)). Indeed, Reichardt (2004) has shown that there are simple\nproblems for which the algorithm will get stuck in a local minimum, in\nwhich there are exponentially many eigenvalues all exponentially close\nto the ground state energy, so applying the adiabatic theorem, even\nfor these simple problems, will take exponential time, and we are back\nto square one. \nMeasurement-based algorithms differ from circuit algorithms in that\ninstead of employing unitary evolution as the basic mechanism for the\nmanipulation of information, these algorithms essentially make use of\nnon-unitary measurements in the course of a computation. They are\nespecially interesting from a foundational perspective because they\nhave no evident classical analogues and because they offer new insight\non the role of\n entanglement\n in quantum computing (Jozsa 2006). They may also have interesting\nengineering-related consequences, suggesting a different kind of\ncomputer architecture which is more fault tolerant (Nielsen and Dawson\n2005). \nMeasurement-based algorithms fall into two categories. The first is\nteleportation quantum computing (based on an idea of Gottesman and\nChuang (1999), and developed into a computational model by Nielsen\n(2003) and Leung (2004)). The second is the “one way quantum\ncomputer”, known also as the “cluster state” model\n(Raussendorf and Briegel 2002). The interesting feature of these\nmodels is that they are able to simulate arbitrary quantum dynamics,\nincluding unitary dynamics, using basic non-unitary measurements. The\nmeasurements are performed on a pool of highly entangled states and\nare adaptive, i.e., each measurement is done in a different basis\nwhich is calculated classically, given the result of earlier\nmeasurements. \nExotic models such as these might seem redundant, especially since\nthey have been shown to be polynomially equivalent to the standard\ncircuit model in terms of computational complexity (Raussendorf,\nBrowne, and Briegel 2003). Their merit, however, lies in the\nfoundational lessons they drive home: with these models the separation\nbetween the classical (i.e., the calculation of the next\nmeasurement-basis) and quantum (i.e., measurements on the entangled\nqubits) parts of the computation becomes evident, hence it may be\neasier to pinpoint the quantum resources that are responsible for the\nputative “speed-up”. \nAnother exotic model for quantum computing which has attracted a lot\nof attention, especially from Microsoft inc. (Freedman 1998), is the\nTopological Quantum Field Theory model. In contrast to the easily\nvisualisable circuit model, this model resides in the most abstract\nreaches of theoretical physics. The exotic physical systems TQFT\ndescribes are topological states of matter. That the formalism of TQFT\ncan be applied to computational problems was shown by Witten (1989)\nand the idea was later developed by others. The model has been proved\nto be efficiently simulatable on a standard quantum computer\n(Freedman, Kitaev, and Wang 2002; Aharonov, Jones, and Landau 2009).\nIts main merit lies in its high tolerance to the errors which are\ninevitably introduced in the implementation of a large scale quantum\ncomputer (see below). Topology is especially helpful here because many\nglobal topological properties are, by definition, invariant under\ndeformation, and given that most errors are local, information encoded\nin topological properties is robust against them. \nThe quantum computer might be the theoretician’s dream, but as\nfar as experimentalists are concerned, its realisation is a nightmare.\nThe problem is that while some prototypes of the simplest elements\nneeded to build a quantum computer have already been implemented in\nthe laboratory, it is still an open question how to combine these\nelements into scalable systems (see Van Meter and Horsman 2013).\nShor’s algorithm may break RSA encryption, but it will remain an\nanecdote if the largest number that it can factor is 15. In the\ncircuit-based model the problem is to achieve a scalable quantum\nsystem that at the same time will allow one to (1) robustly represent\nquantum information with (2) a time to decoherence significantly\nlonger than the length of the computation, (3) implement a universal\nfamily of unitary transformations, (4) prepare a fiducial initial\nstate, and (5) measure the output result (these are DiVincenzo\n(2000)’s five criteria). Alternative paradigms may trade some of\nthese requirements with others, but the gist will remain the same,\ni.e., one would have to achieve control of one’s quantum system\nin such a way that the system will remain “quantum” albeit\nmacroscopic or at least mesoscopic in its dimensions. \nIn order to deal with these challenges, several ingenious solutions\nhave been devised, including quantum error correction codes and fault\ntolerant computation (Shor 1995; Shor and DiVincenzo 1996; Aharonov\nand Ben-Or 1997; Raussendorf, Harrington, and Goyal 2008; Horsman et\nal. 2012; De Beaudrap and Horsman 2019) which can dramatically reduce\nthe spread of errors during a ‘noisy’ quantum computation.\nAn important criticism of these active error correction schemes,\nhowever, is that they are devised for a very unrealistic noise model\nwhich treats the computer as quantum and the environment as classical\n(Alicki, Lidar, and Zinardi 2006). Once a more realistic noise model\nis allowed, the feasibility of large scale, fault tolerant and\ncomputationally superior quantum computers is less clear (Hagar 2009;\nTabakin 2017). \nIn the near term, a promising avenue for realising a quantum advantage\nin a limited number of problem domains is the Noisy Intermediate-Scale\nQuantum (NISQ) paradigm (Preskill 2018). The NISQ paradigm does not\nemploy any error correction mechanisms (postponing the problem to\nimplement scalable versions of these to the future) but rather focuses\non building computational components, and on tackling computational\nproblems, which are inherently more resilient to noise. These include,\nfor example, certain classes of optimisation problems, quantum\nsemidefinite programming, and digital quantum simulation (Tacchino et\nal. 2019). A caveat here is that as the resiliency to noise of a\ncircuit increases, the more classically it behaves. Nevertheless,\nresearch into NISQ computing is believed to be on track to realise a\n50–100 qubit machine—large enough to achieve a quantum advantage\nover known classical alternatives for the envisioned\napplications—within the next 5–10 years. \nAs mentioned, one of the envisioned applications of NISQ computing is\nfor digital quantum simulation (i.e. simulation using a gate-based\nprogrammable quantum computer). There is an older tradition of\nanalog quantum simulation, however, wherein one utilises a\nquantum system whose dynamics resemble the dynamics of a particular\ntarget system of interest. Although it is believed that digital\nquantum simulation will eventually supersede it, the field of analog\nquantum simulation has progressed substantially in the years since it\nwas first proposed, and analog quantum simulators have already been\nused to study quantum dynamics in regimes thought to be beyond the\nreach of classical simulators (see, e.g., Bernien et al. (2017); for\nfurther discussion of the philosophical issues involved, see\nHangleiter, Carolan, and Thébault (2017)). \nNotwithstanding the excitement around the discovery of Shor’s\nalgorithm, and putting aside the presently insurmountable problem of\npractically realising and implementing a large scale quantum computer,\na crucial theoretical question remains open: What physical resources\nare responsible for quantum computing’s putative power? Put\nanother way, what are the essential features of quantum mechanics that\nwould in principle allow one to solve problems or simulate certain\nsystems more efficiently than on a classical computer? A number of\ncandidates have been put forward. Fortnow (2003) posits interference\nas the key, though it has been suggested that this is not truly a\nquantum phenomenon (Spekkens 2007). Jozsa (1997) and many others point\nto entanglement, although there are purported counter-examples to this\nthesis (see, e.g., Linden and Popescu (1999), Gottesman (1999), Biham\net al. (2004), and finally see Cuffaro (2017) for a philosophical\ndiscussion). Howard et al. (2014) appeal to quantum contextuality. For\nBub (2010) the answer lies in the logical structure of quantum\nmechanics (cf. Pitowsky 1989). Duwell (2018) argues for quantum\nparallelism, and for Deutsch (1997) and Hewitt-Horsman (2009) it is\n“parallel worlds” which are the resource. \nSpeculative as it may seem, the question “what is\nquantum in quantum computing?” has significant\npractical consequences. One of the embarrassments of quantum computing\nis the paucity of quantum algorithms which have actually been\ndiscovered. It is almost certain that one of the reasons for this is\nthe lack of a full understanding of what makes a quantum computer\nquantum (see also Preskill (1998) and Shor (2004)). As an ultimate\nanswer to this question one would like to have something similar to\nBell’s famous\n theorem,\n i.e., a succinct crisp statement of the fundamental difference\nbetween quantum and classical systems. Quantum computers,\nunfortunately, do not seem to allow such a simple characterisation\n(see Cuffaro 2017, 2018a). Quantum computing skeptics (Levin 2003)\nhappily capitalise on this puzzle: If no one knows why\nquantum computers are superior to classical ones, how can we be sure\nthat they are, indeed, superior? \nThe answer that has tended to dominate the popular literature on\nquantum computing is motivated by evolutions such as:  \nwhich were common to many early quantum algorithms. Note the\nappearance that \\(f\\) is evaluated for each of its possible inputs\nsimultaneously. The idea that we should take this at face\nvalue—that quantum computers actually do compute a\nfunction for many different input values simultaneously—is what\nDuwell (2018) calls the Quantum Parallelism Thesis\n(QPT). For Deutsch, who accepts it as true, the only reasonable\nexplanation for the QPT is that the many\nworlds interpretation (MWI) of quantum mechanics is also true. For\nDeutsch, a quantum computer in superposition, like any other quantum\nsystem, exists in some sense in many classical universes\nsimultaneously. These provide the physical arena within which the\ncomputer effects its parallel computations. This conclusion is\ndefended by Hewitt-Horsman (2009) and by Wallace (2012). Wallace\nnotes, however, that the QPT—and hence the explanatory need for\nmany worlds—may not be true of all or even most quantum\nalgorithms. \nFor Steane (2003), in contrast, quantum computers are not well\ndescribed in terms of many worlds or even quantum parallelism. Among\nother things, Steane argues that the motivation for the QPT is at\nleast partly due to misleading aspects of the standard quantum\nformalism. Additionally, comparing the information actually produced\nby quantum and classical algorithms (state collapse entails that only\none evaluation instance in (2) is ever accessible, while a classical\ncomputer must actually produce every instance) suggests that quantum\nalgorithms perform not more but fewer, cleverer, computations than\nclassical algorithms (see, also, section 5.1.2  below). \nAnother critic is Duwell, who (contra Steane) accepts the QPT (Duwell\n2018a), but nevertheless denies that it uniquely supports the MWI\n(Duwell 2007). Considering the phase relations between the terms in a\nsuperposition such as (2) is crucially important when evaluating a\nquantum algorithm’s computational efficiency. Phase relations,\nhowever, are global properties of a state. Thus a quantum computation,\nDuwell argues, does not consist solely of local parallel\ncomputations. But in this case, the QPT does not uniquely support the\nMWI over other explanations. \nDefending the MWI, Hewitt-Horsman (2009) argues (contra Steane) that\nto state that quantum computers do not actually generate each of the\nevaluation instances represented in (2) is false according to the\nview: on the MWI such information could be extracted in principle\ngiven sufficiently advanced technology. Further, Hewitt-Horsman\nemphasises that the MWI is not motivated simply by a suggestive\nmathematical representation. Worlds on the MWI are defined according\nto their explanatory usefulness, manifested in particular by their\nstability and independence over the time scales relevant to the\ncomputation. Wallace (2012) argues similarly. \nCuffaro (2012) and Aaronson (2013b) point out that the Many Worlds\nExplanation of Quantum Computing (MWQC) and the MWI are not actually\nidentical. The latter employs\n decoherence\n as a criterion for distinguishing macroscopic worlds from one\nanother. Quantum circuit model algorithms, however, utilise\ncoherent superpositions. To distinguish computational worlds,\ntherefore, one must weaken the decoherence criterion, but Cuffaro\nargues that this move is ad hoc. Further, Cuffaro argues that the MWQC\nis for all practical purposes incompatible with\n measurement based computation,\n for even granting a weakened world identification criterion, there is\nno natural way in this model to identify worlds that are stable and\nindependent in the way required. \nEven if we could rule out the MWQC, the problem of finding the\nphysical resource(s) responsible for quantum “speed-up”\nwould remain a difficult one. Consider a solution of a decision\nproblem, say satisfiability, with a quantum algorithm based\non the circuit model. What we are given here as input is a proposition\nin the propositional calculus and we have to decide whether it has a\nsatisfying truth assignment. As Pitowsky (2002) shows, the quantum\nalgorithm appears to solve this problem by testing all \\(2^{n}\\)\nassignments “at once” as suggested by (2), yet this\nquantum ‘miracle’ helps us very little since, as\npreviously mentioned, any measurement performed on the output state\ncollapses it, and if there is one possible truth assignment that\nsolves this decision problem, the probability of retrieving it is\n\\(2^{-n}\\), just as in the case of a classical probabilistic Turing\nmachine which guesses the solution and then checks it.\nPitowsky’s conclusion (echoed, as we saw, by Steane (2003) and\nDuwell (2007)) is that in order to enhance computation with quantum\nmechanics we must construct ‘clever’ superpositions that\nincrease the probability of successfully retrieving the result far\nmore than that of a pure guess. Shor’s algorithm and the class\nof algorithms that evaluate a global property of a function\n(this class is known as the hidden subgroup class of\nalgorithms) are (so far) a unique example of both a construction of\nsuch ‘clever’ superpositions and a retrieval of the\nsolution in polynomial time. The quantum adiabatic algorithm may give\nus similar results, contingent upon the existence of an energy gap\nthat decreases polynomially with the input. \nThis question also raises important issues about how to measure the\ncomplexity of a given quantum algorithm. The answer differs, of\ncourse, according to the particular model at hand. In the adiabatic\nmodel, for example, one needs only to estimate the energy gap\nbehaviour and its relation to the input size (encoded in the number of\ndegrees of freedom of the Hamiltonian of the system). In the\nmeasurement-based model, one counts the number of measurements needed\nto reveal the solution that is hidden in the input cluster state\n(since the preparation of the cluster state is a polynomial process,\nit does not add to the complexity of the computation). But in the\ncircuit model things are not as straightforward. After all, the whole\nof the quantum-circuit-based computation can be be simply represented\nas a single unitary transformation from the input state to\nthe output state. \nThis feature of the quantum circuit model supports the conjecture that\nthe power of quantum computers, if any, lies not in quantum dynamics\n(i.e., in the Schrödinger equation), but rather in the quantum\nstate, or the wave function. Another argument in favour of this\nconjecture is that the Hilbert subspace “visited” during a\nquantum computational process is, at any moment, a linear space\nspanned by all of the vectors in the total Hilbert space which have\nbeen created by the computational process up to that moment. But this\nHilbert subspace is thus a subspace spanned by a polynomial number of\nvectors and is thus at most a polynomial subspace of the total Hilbert\nspace. A classical simulation of the quantum evolution on a Hilbert\nspace with polynomial number of dimensions (that is, a Hilbert space\nspanned by a number of basis vectors which is polynomial in the number\nof qubits involved in the computation), however, can be carried out in\na polynomial number of classical computations. Were quantum\ndynamics the sole ingredient responsible to the efficiency of\nquantum computing, the latter could be mimicked in a polynomial number\nof steps with a classical computer (see, e.g. Vidal 2003). \nThis is not to say that quantum computation is no more powerful than\nclassical computation. The key point, of course, is that one does not\nend a quantum computation with an arbitrary superposition, but aims\nfor a very special, ‘clever’ state—to use\nPitowsky’s term. Quantum computations may not always be\nmimicked with a classical computer because the characterisation of the\ncomputational subspace of certain quantum states is difficult, and it\nseems that these special, ‘clever’, quantum states cannot\nbe classically represented as vectors derivable via a quantum\ncomputation in an optimal basis, or at least that one cannot do so in\nsuch way that would allow one to calculate the outcome of the final\nmeasurement made on these states. \nConsequently, in the quantum circuit model one should count the number\nof computational steps in the computation not by counting the number\nof transformations of the state, but by counting the number of one- or\ntwo-qubit local transformations that are required to create the\n‘clever’ superposition that ensures the desired\n“speed-up”. (Note that Shor’s algorithm, for\nexample, involves three major steps in this context: First, one\ncreates the ‘clever’ entangled state with a set of unitary\ntransformations. The result of the computation—a global property\nof a function—is now ‘hidden’ in this state; second,\nin order to retrieve this result, one projects it on a subspace of the\nHilbert space, and finally one performs another set of unitary\ntransformations in order to make the result measurable in the original\ncomputational basis. All these steps count as\ncomputational steps as far as the efficiency of the algorithm\nis concerned. See also Bub (2006b).) The trick is to perform these\nlocal one- or two-qubit transformations in polynomial time, and it is\nlikely that it is here where the physical power of quantum computing\nmay be found. \nThe quantum information revolution has prompted several physicists and\nphilosophers to claim that new insights can be gained from the rising\nnew science into conceptual problems in the foundations of quantum\nmechanics (see, e.g., Bub (2016), Chiribella and Spekkens (2016); for\nresponses and commentaries, see, e.g., Myrvold (2010), Timpson (2013),\nFelline (2016), Cuffaro (forthcoming), Duwell (forthcoming), Felline\n(forthcoming-a), Henderson (forthcoming), Koberinski and Müller\n(2018)). Yet while one of the most famous foundational problems in\nquantum mechanics, namely\n the quantum measurement problem,\n remains unsolved even within quantum information theory (see Hagar\n (2003), Hagar and Hemmo (2006), and Felline (forthcoming-b) for a\n critique of the quantum information theoretic approach to the\n foundations of quantum mechanics and the role of the quantum\n measurement problem in this context), some quantum information\n theorists dismiss it as a philosophical quibble (Fuchs 2002). Indeed,\n in quantum information theory the concept of\n “measurement” is taken as a primitive, a “black\n box” which remains unanalysed. The measurement problem itself,\n furthermore, is regarded as a misunderstanding of quantum theory. But\n recent advances in the realisation of a large scale quantum computer\n may eventually prove quantum information theorists wrong: Rather than\n supporting the dismissal of the quantum measurement problem, these\n advances may surprisingly lead to its empirical solution. \nThe speculative idea is the following. As it turns\nout, collapse theories—one form of\nalternatives to quantum theory which aim to solve the measurement\nproblem—modify Schrödinger’s equation and give\ndifferent predictions from quantum theory in certain specific\ncircumstances. These circumstances can be realised, moreover,\nif decoherence effects can be\nsuppressed (Bassi, Adler, & Ippoliti 2004). Now one of the most\ndifficult obstacles that await the construction of a large scale\nquantum computer is its robustness against decoherence effects (Unruh\n1995). It thus appears that the technological capabilities required\nfor the realisation of a large scale quantum computer are potentially\nrelated to those upon which the distinction between “true”\nand “false” collapse (Pearle 1997), i.e., between collapse\ntheories and environmentally induced decoherence, is\ncontingent. Consequently the physical realisation of a large-scale\nquantum computer, if it were of the right architecture, could\npotentially shed light on one of the long standing conceptual problems\nin the foundations of the theory, and if so this would serve as yet\nanother example of experimental metaphysics (the term was coined by\nAbner Shimony to designate the chain of events that led\nfrom the EPR argument\nvia Bell’s theorem to\nAspect’s experiments). Note, however, that as just mentioned,\none would need to consider the computer’s architecture before\nmaking any metaphysical conclusions. The computer architecture is\nimportant because while dynamical collapse theories tend to collapse\nsuperpositions involving the positions of macroscopic quantities of\nmass, they tend not to collapse large complicated superpositions of\nphoton polarisation or spin. \nIs quantum mechanics compatible with the principle of causality? This\nis an old question—indeed one of the very first interpretational\nquestions confronted by the early commentators on the theory (Hermann\n2017; Schlick 1961, 1962). The contemporary literature continues to\nexhibit considerable skepticism in regards to the prospects of\nexplaining quantum phenomena causally (Hausman & Woodward 1999;\nVan Fraassen 1982; Woodward 2007), or at any rate locally\ncausally, especially in the wake\nof Bell’s theorem (Myrvold\n2016). As a result of some fascinating theoretical work (Allen,\nBarrett, Horsman, Lee, & Spekkens 2017; Costa & Shrapnel 2016;\nShrapnel 2017), however, it seems that the prospects for a locally\ncausal explanation of quantum phenomena are not quite as hopeless as\nthey may initially have seemed, at least in the context of\nan interventionist theory of\ncausation. This is not to say that decades of physical and\nphilosophical investigations into the consequences of Bell’s\ntheorem have all been mistaken, of course. For one thing, the\ninterventionist frameworks utilised in this new work are\noperationalist, thus the relevance of this work to so-called hidden\nvariables theories of quantum mechanics is unclear. Second, the\ninterventionist frameworks utilised are not classical, and neither is\nthe kind of causality they explicate. Indeed, in regard to the latter\npoint, it is arguably the key insight emerging from this work that the\nframeworks previously utilised for analysing interventionist causation\nin the quantum context are inappropriate to that context. In contrast\nto a classical interventionist framework in which events are thought\nof as primitive (i.e. as not further analysable), events in these\ngeneralised frameworks are characterised as processes with\nassociated inputs and outputs. Specifically, one characterises quantum\nevents using a concept from quantum computation and information theory\ncalled a quantum channel. And within this generalised\ninterventionist framework, causal models of quantum phenomena can be\ngiven which do not need to posit non-local causal influences, and\nwhich satisfy certain other desiderata typically required in a causal\nmodel (in particular that such a model respect the causal Markov\ncondition and that it not require ‘fine-tuning’; see\nShrapnel (2017)). \nPhysics is traditionally conceived as a primarily\n“theoretical” activity, in the sense that it is generally\nthought to be the goal of physics to tell us, even if only indirectly\n(Fuchs (2002), pp. 5–6, Fuchs (2010), pp. 22–3), what the world is\nlike independently of ourselves. This is not the case with every\nscience. Chemistry, for example, is arguably best thought of as a\n“practically” oriented discipline concerned with the ways\nin which systems can be manipulated for particular purposes\n(Bensaude-Vincent (2009)). Even within physics, there are\nsub-disciplines which are best construed in this way (Myrvold 2011;\nWallace 2014; Ladyman 2018), and indeed, some (though at present these\nare still a minority) have even sought to (re-)characterise physics as\na whole in something like this way, i.e. as a science of possible as\nopposed to impossible transformations (Deutsch 2013). \nElaborating upon ideas which one can glean from Pitowsky’s work\n(1990, 1996, 2002), Cuffaro argues at length that quantum computation\nand information theory (QCIT) are practical sciences in this sense, as\nopposed to the “theoretical sciences” exemplified by\nphysics under its traditional characterisation; further that\nrecognising this distinction illuminates both areas of activity. On\nthe one hand (Cuffaro 2017), practical investigators attempting to\nisolate and/or quantify the computational resources made available by\nquantum computers are in danger of conceptual confusion if they are\nnot cognisant of the differences between practical and traditional\nsciences. On the other hand (Cuffaro 2018a), one should be wary of the\nsignificance of classical computer simulations of quantum mechanical\nphenomena for the purposes of a foundational analysis of the latter.\nFor example, certain mathematical results can legitimately be thought\nof as no-go theorems for the purposes of foundational analysis, and\nyet are not really relevant for the purpose of characterising the\nclass of efficiently simulable quantum phenomena. \nThe Church-Turing thesis, which asserts that every function naturally\nregarded as computable is Turing-computable, is argued by Deutsch to\npresuppose a physical principle, namely that: \n[DP]: Every finitely realisable physical system can be perfectly\nsimulated by a universal model computing machine operating by finite\nmeans. (Deutsch 1985) \nSince no machine operating by finite means can simulate classical\nphysics’ continuity of states and dynamics, Deutsch argues that\nDP is false in a classical world. He argues that it is true for\nquantum physics, however, owing to the existence of the universal\nquantum Turing machine he introduces in the same paper, which thus\nproves both DP and the Church-Turing thesis it underlies to be sound.\nThis idea—that the Church-Turing thesis requires a physical\ngrounding—is set into historical context by Lupacchini (2018),\nwho traces its roots in the thought of Gödel, Post, and Gandy. It\nis criticised by Timpson (2013), who views it as methodologically\nfruitful, but as nevertheless resting on a confusion regarding the\nmeaning of the Church-Turing thesis, which in itself has nothing to do\nwith physics. \nIn the general philosophy of science literature on\n scientific explanation\n there is a distinction between so-called “how-actually”\nand “how-possibly” explanation, where the former aims to\nconvey how a particular outcome actually came about, and the latter\naims to convey how the occurrence of an event can have been possible.\nThat how-actually explanation actually explains is uncontroversial,\nbut the merit (if any) of how-possibly explanation has been debated.\nWhile some view how-possibly explanation as genuinely explanatory,\nothers have argued that how-possibly ‘explanation’ is\nbetter thought of as, at best, a merely heuristically useful\nexercise. \nIt turns out that the science of quantum computation is able to\nilluminate this debate. Cuffaro (2015) argues that when one examines\nthe question of the source of quantum “speed-up”, one sees\nthat to answer this question is to compare algorithmic processes of\nvarious kinds, and in so doing to describe the possibility spaces\nassociated with these processes. By doing so one explains how it is\npossible for one process to outperform its rival. Further, Cuffaro\nargues that in examples like this, once one has answered the\nhow-possibly question, nothing is actually gained by subsequently\nasking a how-actually question. \nFinally, another philosophical implication of the realisation of a\nlarge scale quantum computer regards the long-standing debate in the\nphilosophy of mind on the autonomy of computational theories of the\nmind (Fodor 1974). In the shift from strong to weak artificial\nintelligence, the advocates of this view tried to impose constraints\non computer programs before they could qualify as theories of\ncognitive science (Pylyshyn 1984). These constraints include, for\nexample, the nature of physical realisations of symbols and the\nrelations between abstract symbolic computations and the physical\ncausal processes that execute them. The search for the computational\nfeature of these theories, i.e., for what makes them\ncomputational theories of the mind, involved isolating some\nfeatures of the computer as such. In other words, the\nadvocates of weak AI were looking for computational properties, or\nkinds, that would be machine independent, at least in the\nsense that they would not be associated with the physical constitution\nof the computer, nor with the specific machine model that was being\nused. These features were thought to be instrumental in debates within\ncognitive science, e.g., the debate between functionalism and\nconnectionism (Fodor and Pylyshyn 1988). \nNote, however, that once the physical Church-Turing thesis is\nviolated, arguably some computational notions cease to be autonomous.\nIn other words, given that quantum computers may be able to\nefficiently solve classically intractable problems, hence re-describe\nthe abstract space of computational complexity (Bernstein and Vazirani\n1997), computational concepts and even computational kinds such as\n‘an efficient algorithm’ or ‘the class NP’,\nbecome machine-dependent, and recourse to ‘hardware’\nbecomes inevitable in any analysis thereof (Hagar 2007). \nAdvances in quantum computing may thus militate against the\nfunctionalist view about the unphysical character of the\ntypes and properties that are used in computer science. In fact, these\ntypes and categories may become physical as a result of this\nnatural development in physics (e.g., quantum computing, chaos\ntheory). Consequently, efficient quantum algorithms may also serve as\ncounterexamples to a-priori arguments against reductionism (Pitowsky\n1996).","contact.mail":"mike@michaelcuffaro.com","contact.domain":"michaelcuffaro.com"}]
