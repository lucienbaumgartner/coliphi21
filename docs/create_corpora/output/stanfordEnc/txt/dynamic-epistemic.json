[{"date.published":"2016-06-24","url":"https://plato.stanford.edu/entries/dynamic-epistemic/","author1":"Alexandru Baltag","author1.info":"http://bryan.renne.org/","entry":"dynamic-epistemic","body.text":"\n\n\nDynamic Epistemic Logic is the study of modal logics of model change.\nDEL (pronounced “dell”) is a highly active area of applied\nlogic that touches on topics in many areas, including Formal and\nSocial Epistemology, Epistemic and Doxastic Logic, Belief Revision,\nmulti-agent and distributed systems, Artificial Intelligence,\nDefeasible and Non-monotonic Reasoning, and Epistemic Game Theory.\nThis article surveys DEL, identifying along the way a number of open\nquestions and natural directions for further research.\n\nDynamic Epistemic Logic is the study of a family of modal logics, each\nof which is obtained from a given logical language by adding one or\nmore modal operators that describe model-transforming actions. If\n\\([A]\\) is such a modality, then new formulas of the form \\([A]F\\) are\nused to express the statement that F is true after the\noccurrence of action A. To determine whether \\([A]F\\) is true\nat a pointed Kripke model \\((M,w)\\) (see\n Appendix A\n for definitions), we transform the current Kripke model M\naccording to the prescription of action A and we obtain a new\npointed Kripke model \\((M',w')\\) at which we then investigate whether\nF is true. If it is true there, then we say that original\nformula \\([A]F\\) is true in our starting situation \\((M,w)\\). If\nF is not true in the newly produced situation \\((M',w')\\), then\nwe conclude the opposite: \\([A]F\\) is not true in our starting\nsituation \\((M,w)\\). In this way, we obtain the meaning of \\([A]F\\)\nnot by the analysis of what obtains in a single Kripke model but by\nthe analysis of what obtains as a result of a specific\nmodality-specified Kripke model transformation. This is a shift from a\nstatic semantics of truth that takes place in an individual\nKripke model to a dynamic semantics of truth that takes place\nacross modality-specified Kripke model transformations. The advantage\nof the dynamic perspective is that we can analyze the epistemic and\ndoxastic consequences of actions such as public and private\nannouncements without having to “hard wire” the results\ninto the model from the start. Furthermore, we may look at the\nconsequences of different sequences of actions simply by changing the\nsequence of action-describing modalities.  \nIn the following sections, we will look at the many model-changing\nactions that have been studied in Dynamic Epistemic Logic. Many\nnatural applications and questions arise as part of this study, and we\nwill see some of the results obtained in this work. Along the way it\nwill be convenient to consider many variations of the general formal\nsetup described above. Despite these differences, at the core is the\nsame basic idea: new modalities describing certain\napplication-specific model-transforming operations are added to an\nexisting logical language and the study proceeds from there.\nProceeding now ourselves, we begin with what is perhaps the\nquintessential and most basic model-transforming operation: the public\nannouncement.  \nPublic Announcement Logic (PAL) is the modal logic study of knowledge,\nbelief, and public communication. PAL (pronounced “pal”)\nis used to reason about knowledge and belief and the changes brought\nabout in knowledge and belief as per the occurrence of completely\ntrustworthy, truthful announcements. PAL’s most common\nmotivational examples include the Muddy Children Puzzle and\nthe Sum and Product Puzzle (see, e.g., Plaza 1989, 2007). The\nCheryl’s Birthday problem, which became a sensation on\nthe Internet in April 2015, can also be addressed using PAL. Here we\npresent a version of the Cheryl’s Birthday problem due to Chang\n(2015, 15 April) and a three-child version of the Muddy Children\nPuzzle (Fagin et al. 1995). Instead of presenting the traditional Sum\nand Product Puzzle (see Plaza (1989, 2007) for details), we present\nour own simplification that we call the Sum and Least Common\nMultiple Problem.  \nCheryl’s Birthday (version of Chang (2015, 15\nApril)). Albert and Bernard just met Cheryl.\n“When’s your birthday?” Albert asked Cheryl. \nCheryl thought a second and said, “I’m not going to tell\nyou, but I’ll give you some clues”. She wrote down a list\nof 10 dates: \n“My birthday is one of these”, she said. \nThen Cheryl whispered in Albert’s ear the month—and only\nthe month—of her birthday. To Bernard, she whispered the day,\nand only the day. \n“Can you figure it out now?” she asked Albert. \nWhen is Cheryl’s birthday? \nThe Muddy Children Puzzle. Three children are playing\nin the mud. Father calls the children to the house, arranging them in\na semicircle so that each child can clearly see every other child.\n“At least one of you has mud on your forehead”, says\nFather. The children look around, each examining every other\nchild’s forehead. Of course, no child can examine his or her\nown. Father continues, “If you know whether your forehead is\ndirty, then step forward now”. No child steps forward. Father\nrepeats himself a second time, “If you know whether your\nforehead is dirty, then step forward now”. Some but not all of\nthe children step forward. Father repeats himself a third time,\n“If you know whether your forehead is dirty, then step forward\nnow”. All of the remaining children step forward. How many\nchildren have muddy foreheads? \nThe Sum and Least Common Multiple Puzzle. Referee\nreminds Mr. S and Mr. L that the least common multiple\n(“\\(\\text{lcm}\\)”) of two positive integers x and\ny is the smallest positive integer that is divisible without\nany remainder by both x and y (e.g.,\n\\(\\text{lcm}(3,6)=6\\) and \\(\\text{lcm}(5,7)=35\\)). Referee then says,\n \nAmong the integers ranging from \\(2\\) to \\(7\\), including \\(2\\) and\n\\(7\\) themselves, I will choose two different numbers. I will whisper\nthe sum to Mr. S and the least common multiple to Mr. L.  \nReferee then does as promised. The following dialogue then takes\nplace: \nWhat are the numbers? \nThe Sum and Product Puzzle is like the Sum and Least Common Multiple\nPuzzle except that the allowable integers are taken in the range\n\\(2,\\dots,100\\) (inclusive), Mr. L is told the product of the two\nnumbers (instead of their least common multiple), and the dialogue is\naltered slightly (L: “I don’t know the numbers”, S:\n“I knew you didn’t know them”, L: “Ah, but now\nI do know them”, S: “And now so do I!”). These\nchanges result in a substantially more difficult problem. See Plaza\n(1989, 2007) for details.  \nThe reader is advised to try solving the puzzles himself or herself\nand to read more about PAL below before looking at the PAL-based\nsolutions found in a\n Appendix B.\n Later, after the requisite basics of PAL have been presented, the\nauthors will again point the reader to this appendix.  \nThere are many variations of these puzzles, some of which motivate\nlogics that can handle more than just public communication.\nRestricting attention to the variations above, we note that a formal\nlogic for reasoning about these puzzles must be able to represent\nvarious agents’ knowledge along with changes in this knowledge\nthat are brought about as a result of public announcements. One\nimportant thing to note is that the announcements in the puzzles are\nall truthful and completely trustworthy: so that we\ncan solve the puzzles, we tacitly assume (among other things) that\neverything that is announced is in fact true and that all agents\naccept the content of a public announcement without question. These\nassumptions are of course unrealistic in many everyday situations,\nand, to be sure, there are more sophisticated Dynamic Epistemic Logics\nthat can address more complicated and nuanced attitudes agents may\nhave with respect to the information they receive. Nevertheless, in an\nappropriately restricted situation, Public Announcement Logic provides\na basic framework for reasoning about truthful, completely trustworthy\npublic announcements.  \nGiven a nonempty set \\(\\sP\\) of propositional letters and a finite\nnonempty set \\(\\sA\\) of agents, the basic modal language \\eqref{ML} is\ndefined as follows:  \nFormulas \\([a]F\\) are assigned a reading that is doxastic\n(“agent a believes F”) or epistemic\n(“agent a knows F”), with the particular\nreading depending on the application one has in mind. In this article\nwe will use both readings interchangeably, choosing whichever is more\nconvenient in a given context. In the language \\eqref{ML}, Boolean\nconnectives other than negation \\(\\lnot\\) and conjunction \\(\\land\\)\nare taken as abbreviations in terms of negation in conjunction as is\nfamiliar from any elementary Logic textbook. See\n Appendix A\n for further details on \\eqref{ML} and its Kripke semantics.  \nThe language \\(\\eqref{PAL}\\) of Public Announcement Logic extends the\nbasic modal language \\eqref{ML} by adding formulas \\([F!]G\\) to\nexpress that “after the public announcement of F, formula\nG is true”:  \nSemantically, the formula \\([F!]G\\) is interpreted in a Kripke model\nas follows: to say that \\([F!]G\\) is true means that, whenever\nF is true, G is true after we eliminate all not-F\npossibilities (and all arrows to and from these possibilities). This\nmakes sense: since the public announcement of F is completely\ntrustworthy, all agents respond by collectively eliminating all\nnon-F possibilities from consideration. So to see what obtains\nafter a public announcement of F occurs, we eliminate the\nnon-F worlds and then see what is true in the resulting\nsituation. Formally, \\(\\eqref{PAL}\\)-formulas are evaluated as an\nextension of the binary truth relation \\(\\models\\) between pointed\nKripke models and \\eqref{ML}-formulas (defined in\n Appendix A)\n as follows: given a Kripke model \\(M=(W,R,V)\\) and a world \\(w\\in\nW\\),  \nNote that the formula \\([F!]G\\) is vacuously true if F is\nfalse: the announcement of a false formula is inconsistent with our\nassumption of truthful announcements, and hence every formula follows\nafter a falsehood is announced (ex falso quodlibet). It is\nworth remarking that the dual announcement operator \\(\\may{F!}\\)\ndefined by  \ngives the formula \\(\\may{F!} G\\) the following meaning: F is\ntrue and, after F is announced, G is also true. In\nparticular, we observe that the announcement formula \\(\\may{F!} G\\) is\nfalse whenever F is false.  \nOften one wishes to restrict attention to a class of Kripke models\nwhose relations \\(R_a\\) satisfy certain desirable properties such as\nreflexivity, transitivity, Euclideanness, or seriality. Reflexivity\ntells us that agent knowledge is truthful, transitivity tells us that\nagents know what they know, Euclideanness tells us that agents know\nwhat they do not know, and seriality tells us that agent knowledge is\nconsistent. (A belief reading is also possible.) In order to study\npublic announcements over such classes, we must be certain that the\npublic announcement of a formula F does not transform a given\nKripke model M into a new model \\(M[F!]\\) that falls outside of\nthe class. The following theorem indicates when it is that a given\nclass of Kripke models is “closed” under public\nannouncements (meaning a public announcement performed on a model in\nthe class always yields another model in the class).  \nSee\n Appendix C\n for the definition of reflexivity, transitivity, Euclideanness,\nseriality, and other important relational properties.  \nPublic Announcement Closure Theorem. Let\n\\(M=(W,R,V)\\) be a Kripke model and F be a formula true at at\nleast one world in W. \nThe Public Announcement Closure Theorem tells us that reflexivity,\ntransitivity, and Euclideanness are always closed under the public\nannouncement operation. Seriality is in general not; however, if\nseriality comes with Euclideanness, then public announcements of\nformulas of the form \\(F\\land\\bigwedge_{x\\in\\sA}\\may{x} F\\) (read,\n“F is true and consistent with each agent’s\nknowledge”) preserve both seriality and Euclideanness.\nTherefore, if we wish to study classes of models that are serial,\nthen, to make use of the above theorem, we will need to further\nrestrict to models that are both serial and Euclidean and we will need\nto restrict the language of public announcements so that all\nannouncement formulas have this form. (One could also restrict to\nanother form, so long as public announcements of this form preserve\nseriality over some class \\(\\sC\\) of serial models.) Restricting the\nlanguage \\eqref{PAL} by requiring that public announcements have the\nform \\(F\\land\\bigwedge_{x\\in\\sA}\\may{x} F\\) leads to the language\n\\eqref{sPAL} of serial Public Announcement Logic, which we\nmay use when interested in serial and Euclidean Kripke models.  \nGiven a class of Kripke models satisfying certain properties and a\nmodal logic \\(\\L\\) in the language \\eqref{ML} that can reason about\nthat class, we would like to construct a Public Announcement Logic\nwhose soundness and completeness are straightforwardly proved. To do\nthis, we would like to know in advance that \\(\\L\\) is sound and\ncomplete with respect to the class of models in question, that some\npublic announcement extension \\(\\LPAL\\) of the language \\eqref{ML}\n(e.g., the language \\eqref{sPAL} or maybe even \\eqref{PAL} itself)\nwill include announcements that do not spoil closure, and that there\nis an easy way for us to determine the truth of \\(\\LPAL\\)-formulas by\nlooking only the underlying modal language \\eqref{ML}. This way, we\ncan “reduce” completeness of the public announcement\ntheory to the completeness of the basic modal theory \\(\\L\\). We call\nsuch theories for which this is possible PAL-friendly.  \nPAL-friendly theory. To say that a logic \\(\\L\\) is\nPAL-friendly means we have the following: \nSee\n Appendix D\n for the exact meaning of the first component of a PAL-friendly\ntheory.  \nExamples of PAL-friendly theories include the common “logic of\nbelief” (multi-modal \\(\\mathsf{KD45}\\)), the common “logic\nof knowledge” (multi-modal \\(\\mathsf{S5}\\)), multi-modal\n\\(\\mathsf{K}\\), multi-modal \\(\\mathsf{T}\\), multi-modal\n\\(\\mathsf{S4}\\), and certain logics that mix modal operators of the\npreviously mentioned types (e.g., \\(\\mathsf{S5}\\) for \\([a]\\) and\n\\(\\mathsf{T}\\) for all other agent modal operators \\([b]\\)). Fixing a\nPAL-friendly theory \\(\\L\\), we easily obtain an axiomatic theory of\npublic announcement logic based on \\(\\L\\) as follows.  \nThe axiomatic theory \\(\\PAL\\). \nThe reduction axioms characterize truth of an announcement formula\n\\([F!]G\\) in terms of the truth of other announcement formulas\n\\([F!]H\\) whose post-announcement formula H is less complex\nthan the original post-announcement formula G. In the case\nwhere G is just a propositional letter p, Reduction\nAxiom 1 says that the truth of \\([F!]p\\) can be reduced to a formula\nnot containing any announcements of F. So we see that the\nreduction axioms “reduce” statements of truth of\ncomplicated announcements to statements of truth of simpler and\nsimpler announcements until the mention of announcements is not\nnecessary. For example, writing the reduction axiom used in a\nparenthetical subscript, we have the following sequence of provable\nequivalences:  \nNotice that the last formula does not contain public announcements.\nHence we see that the reduction axioms allow us to express the truth\nof the announcement-containing formula \\([[b]p!](p\\land[a]p)\\) in\nterms of a provably equivalent announcement-free formula. This is true\nin general.  \n\\(\\PAL\\) Reduction Theorem. Given a PAL-friendly\ntheory \\(\\L\\), every F in the language \\(\\LPAL\\) of Public\nAnnouncement Logic (without common knowledge) is \\(\\PAL\\)-provably\nequivalent to a formula \\(F^\\circ\\) coming from the announcement-free\nfragment of \\(\\LPAL\\). \nThe Reduction Theorem makes proving completeness of the axiomatic\ntheory with respect to the appropriate class of pointed Kripke models\neasy: since every \\(\\LPAL\\)-formula can all be expressed using a\nprovably equivalent announcement-free \\eqref{ML}-formula, completeness\nof the theory \\(\\PAL\\) follows by the Reduction Theorem, the soundness\nof \\(\\PAL\\), and the known completeness of the underlying modal theory\n\\(\\L\\).  \n\\(\\PAL\\) Soundness and Completeness. \\(\\PAL\\) is\nsound and complete with respect to the collection \\(\\sC_*\\) of pointed\nKripke models for which the underlying PAL-friendly theory \\(\\L\\) is\nsound and complete. That is, for each \\(\\LPAL\\)-formula F, we\nhave that \\(\\PAL\\vdash F\\) if and only if \\(\\sC_*\\models F\\). \nOne interesting \\(\\PAL\\)-derivable scheme (available if allowed by the\nlanguage \\(\\LPAL\\)) is the following:  \nThis says that two consecutive announcements can be combined into a\nsingle announcement: to announce that F is true and then to\nannounce that G is true will have the same result as announcing\nthe single statement that “F is true and, after F\nis announced, G is true”.  \nWe conclude with a few complexity results for Public Announcement\nLogic.  \nPAL Complexity. Let \\(\\sC\\) be the class of all\nKripke models. Let \\(\\sC_{\\mathsf{S5}}\\) be the class of Kripke models\nsuch that each binary accessibility relation is reflexive, transitive,\nand symmetric. \nOne thing to note about the theory \\(\\PAL\\) as presented above is that\nit is parameterized on a PAL-friendly logic \\(\\L\\). Therefore,\n“Public Announcement Logic” as an area of study in fact\nencompasses a wide-ranging family individual Public Announcement\nLogics, one for each instance of \\(\\L\\). Unless otherwise noted, the\nresults and concepts we present apply to all logics within this\nfamily.  \nIn\n Appendix E,\n we detail further aspects of Public Announcement Logic: schematic\nvalidity, expressivity and succinctness, Gerbrandy–Groeneveld\nannouncements, consistency-preserving announcements and Arrow Update\nLogic, and quantification over public announcements in Arbitrary\nPublic Announcement Logic.  \nWhile iterated public announcements seem like a natural operation to\nconsider (motivated by, e.g., the Muddy Children Puzzle), Miller and\nMoss (2005) showed that a logic of such a language cannot be\nrecursively axiomatized.  \nFinally, PAL-based solutions to the Cheryl’s Birthday, Muddy\nChildren, and Sum and Least Common Multiple Puzzles are presented in\n Appendix B.\n  \nTo reason about common knowledge and public announcements, we add the\ncommon knowledge operators \\([B*]\\) to the language for each group of\nagents \\(B\\subseteq\\sA\\). The formula \\([B*]F\\) is read, ”it is\ncommon knowledge among the group B that F is\ntrue”. We define the language \\eqref{PAL+C} of public\nannouncement logic with common knowledge as follows:  \nThe semantics of this language over pointed Kripke models is defined\nin\n Appendix A.\n We recall two key defined expressions:  \n\\([B]F\\) denotes \\(\\bigwedge_{a\\in B}[a]F\\) — “everyone in\ngroup B knows (or believes) F”; \n\\([C]F\\) denotes \\([\\sA*]F\\) — “it is common knowledge (or\nbelief) that F is true.” \nFor convenience in what follows, we will adopt the epistemic (i.e.,\nknowledge) reading of formulas in the remainder of this subsection. In\nparticular, using the language \\eqref{PAL+C}, we are able to provide a\nformal sense in which public announcements bring about common\nknowledge.  \nTheorem. For each pointed Kripke model \\((M,w)\\), we\nhave: \nWe now examine the axiomatic theory of public announcement logic with\ncommon knowledge.  \nThe axiomatic theory \\(\\PALC\\). \n\\(\\PALC\\) Soundness and Completeness (Baltag, Moss, and\nSolecki 1998, 1999; see also van Ditmarsch, van der Hoek, and Kooi\n2007). \\(\\PALC\\) is sound and complete with respect to the\ncollection \\(\\sC_*\\) of pointed Kripke models for which the underlying\npublic announcement logic \\(\\PAL\\) is sound and complete. That is, for\neach \\eqref{PAL+C}-formula F, we have that \\(\\PALC\\vdash F\\) if\nand only if \\(\\sC_*\\models F\\). \nUnlike the proof of completeness for the logic \\(\\PAL\\) without common\nknowledge, the proof for the logic \\(\\PALC\\) with common knowledge\ndoes not proceed by way of a reduction theorem. This is because adding\ncommon knowledge to the language strictly increases the expressivity.\n \nTheorem (Baltag, Moss, and Solecki 1998, 1999; see also van\nDitmarsch, van der Hoek, and Kooi 2007). Over the class of\nall pointed Kripke models, the language \\eqref{PAL+C} of public\nannouncement logic with common knowledge is strictly more expressive\nthan language \\eqref{PAL} without common knowledge. In particular, the\n\\eqref{PAL+C}-formula \\([p!][C]q\\) cannot be expressed in \\eqref{PAL}\nwith respect to the class of all pointed Kripke models: for every\n\\eqref{PAL}-formula F there exists a pointed Kripke model\n\\((M,w)\\) such that \\(M,w\\not\\models F\\leftrightarrow[p!][C]q\\). \nThis result rules out the possibility of a reduction theorem for\n\\(\\PALC\\): we cannot find a public announcement-free equivalent of\nevery \\eqref{PAL+C}-formula. This led van Benthem, van Eijck, and Kooi\n(2006) to develop a common knowledge-like operator for which a\nreduction theorem does hold. The result is the binary relativized\ncommon knowledge operator \\([B*](F|G)\\), which is read,\n“F is common knowledge among group B relative to\nthe information that G is true”. The language \\eqref{RCK}\nof relativized common knowledge is given by the following grammar:\n \nand the language \\eqref{RCK+P} of relativized common knowledge with\npublic announcements is obtained by adding public announcements to\n\\eqref{RCK}:  \nThe semantics of \\eqref{RCK} is an extension of the semantics of\n\\eqref{ML}, and the semantics of \\eqref{RCK+P} is an extension of the\nsemantics of \\eqref{PAL}. In each case, the extension is obtained by\nadding the following inductive truth clause:  \nHere we recall that \\(R[G!]\\) is the function that obtains after the\npublic announcement of G; that is, we have \\(xR[G!]_ay\\) if and\nonly if x and y are in the model after the announcement\nof G (i.e., \\(M,w\\models G\\) and \\(M,y\\models G\\)) and there is\nan a-arrow from x to y in the original model\n(i.e., \\(xR_ay\\)). The relation \\(R[G!]_B\\) is then the union of the\nrelations for those agents in B; that is, we have \\(xR[G!]_By\\)\nif and only if there is an \\(a\\in B\\) with \\(xR[G!]_ay\\). Finally,\n\\((R[G!]_B)^*\\) is the reflexive-transitive closure of the relation\n\\(R[G!]_B\\); that is, we have \\(x(R[G!]_B)^*y\\) if and only if \\(x=y\\)\nor there is a finite sequence  \nof \\(R[G!]_B\\)-arrows connecting x to y. So, all\ntogether, the formula \\([B*](F|G)\\) is true at w if and only if\nan F-world is at the end of every finite path (of length zero\nor greater) that begins at w, contains only G-worlds,\nand uses only arrows for agents in B. Intuitively, this says\nthat if the agents in B commonly assume G is true in\njointly entertaining possible alternatives to the given state of\naffairs w, then, relative to this assumption, F is\ncommon knowledge among those in B.  \nAs observed by van Benthem, van Eijck, and Kooi (2006), relativized\ncommon knowledge is not the same as non-relativized common knowledge\nafter an announcement. For example, over the collection of all pointed\nKripke models, the following formulas are not equivalent:  \nIn particular, in the pointed model \\((M,w)\\) pictured in Figure 1,\nthe formula \\(\\lnot[\\{a,b\\}*]([a]p\\mid p)\\) is true because there is a\npath that begins at w, contains only p-worlds, uses only\narrows in \\(\\{a,b\\}\\), and ends on the \\(\\lnot[a]p\\)-world u.\n \nFigure 1: The pointed Kripke model\n\\((M,w)\\). \nHowever, the formula \\([p!]\\lnot[\\{a,b\\}*][a]p\\) is false at \\((M,w)\\)\nbecause, after the announcement of p, the model \\(M[p!]\\)\npictured in Figure 2 obtains, and all worlds in this model are\n\\([a]p\\)-worlds. In fact, whenever p is true, the formula\n\\([p!]\\lnot[\\{a,b\\}*][a]p\\) is always false: after the\nannouncement of p, all that remains are p-worlds, and\ntherefore every world is an \\([a]p\\)-world.  \nFigure 2: The pointed Kripke model\n\\((M[p!],w)\\). \nThe axiomatic theories of relativized common knowledge with and\nwithout public announcements along with expressivity results for the\ncorresponding languages are detailed in\n Appendix F.\n  \nWe now state two complexity results for the languages of this\nsubsection.  \n\\eqref{PAL+C} and \\eqref{RCK} Complexity. Let \\(\\sC\\)\nbe the class of all Kripke models. Let \\(\\sC_{\\mathsf{S5}}\\) be the\nclass of Kripke models such that each binary accessibility relation is\nreflexive, transitive, and symmetric. \nIn the remainder of the article, unless otherwise stated, we will\ngenerally assume that we are working with languages that do not\ncontain common knowledge or relativized common knowledge.  \nAnother notion of group knowledge is distributed knowledge\n(Fagin et al. 1995). Intuitively, a group B of agents has\ndistributed knowledge that F is true if and only if, were they\nto pool together all that they know, they would then know F. As\nan example, if agents a and b are going to visit a\nmutual friend, a knows that the friend is at home or at work,\nand b knows that the friend is at work or at the cafe, then\na and b have distributed knowledge that the friend is at\nwork: after they pool together what they know, they will each know the\nlocation of the friend. Distributed knowledge and public announcements\nhave been studied by Wáng and Ågotnes (2011). Related to\nthis is the study of whether a notion of group knowledge (such as\ndistributed knowledge) satisfies the property that something known by\nthe group can be established via communication; see Roelofsen (2007)\nfor details.  \nIt may seem as though public announcements always\n“succeed”, by which we mean that after something is\nannounced, we are guaranteed that that it is true. After all, this is\noften the purpose of an announcement: by making the announcement, we\nwish to inform everyone of its truth. However, it is not hard to come\nup with announcements that are true when announced but false\nafterward; that is, not all announcements are successful. Here are a\nfew everyday examples in plain English.  \nThe opposite of unsuccessful formulas are the “successful”\nones: these are the formulas that are true after they are announced.\nHere one should distinguish between “performative\nannouncements” that bring about truth by their very occurrence\n(e.g., a judge says, “The objection is overruled”, which\nhas the effect of making the objection overruled) and\n“informative announcements” that simply inform their\nlisteners of truth (e.g., our mutual friend says, “I live on\n207th Street”, which has the effect of informing us of something\nthat is already true). Performative announcements are best addressed\nin a Dynamic Epistemic Logic setting using factual changes, a\ntopic discussed in\n Appendix G.\n For now our concern will be with informative announcements.  \nThe phenomena of (un)successfulness of announcements was noted early\non by Hintikka (1962) but was not studied in detail until the advent\nof Dynamic Epistemic Logic. In DEL, the explicit language for public\nannouncements provides for an explicit syntactic definition of\n(un)successfulness.  \n(Un)successful formula (van Ditmarsch and Kooi 2006; see also\nGerbrandy 1999). Let F be a formula in a language with\npublic announcements. \nAs we have seen, the Moore formula  \nis unsuccessful: if (MF) is true, then its announcement eliminates all\n\\(\\lnot p\\)-worlds, thereby falsifying \\(\\lnot[a]p\\) (since the truth\nof \\(\\lnot[a]p\\) requires the existence of an a-arrow leading\nto a \\(\\lnot p\\)-world).  \nAn example of a successful formula is a propositional letter p.\nIn particular, after an announcement of p, it is clear that\np still holds (since the propositional valuation does not\nchange); that is, \\([p!]p\\). Moreover, as the reader can easily\nverify, the formula \\([a]p\\) is also successful.  \nIn considering (un)successful formulas, a natural question arises: can\nwe provide an syntactic characterization of the formulas that are\n(un)successful? That is, is there a way for us know whether a formula\nis (un)successful simply by looking at its form? Building off of the\nwork of Visser et al. (1994) and Andréka, Németi, and van\nBenthem (1998), van Ditmarsch and Kooi (2006) provide one\ncharacterization of some of the successful \\eqref{PAL+C}-formulas.\n \nTheorem (van Ditmarsch and Kooi 2006). The\npreserved formulas are formed by the following grammar.\n\n\\[\\begin{gather*}\nF \\ccoloneqq p \\mid \\lnot p \\mid F\\land F \\mid F\\lor F\\mid [a]F\\mid [\\lnot F!]F \\mid [B*]F\n\\\\\n\\small p\\in\\sP,\\; a\\in\\sA,\\; B\\subseteq\\sA\n\\end{gather*}\\]\n\n Every preserved formula is successful. \nUsing a slightly different notion of successfulness wherein a formula\nF is said to be successful if and only if we have that\n\\(M,w\\models F\\land \\may{a}F\\) implies \\(M[F!],w\\models F\\) for each\npointed Kripke model \\((M,w)\\) coming from a given class \\(\\sC\\),\nHolliday and Icard (2010) provide a comprehensive analysis of\n(un)successfulness with respect to the class of single-agent\n\\(\\mathsf{S5}\\) Kripke models and with respect to the class of\nsingle-agent \\(\\mathsf{KD45}\\) Kripke models. In particular, they\nprovide a syntactic characterization of the successful formulas over\nthese classes of Kripke models. This analysis was extended in part to\na multi-agent setting by Saraf and Sourabh (2012). The highly\ntechnical details of these works are beyond the scope of the present\narticle.  \nFor more on Moore sentences, we refer the reader to\n Section 5.3\n of the Stanford Encyclopedia of Philosophy entry on\n Epistemic Paradoxes\n (Sorensen 2011).  \nIn the previous section, we focused on one kind of model-transforming\naction: the public announcement. In this section, we look at the\npopular “action model” generalization of public\nannouncements due to Baltag, Moss, and Solecki (Baltag, Moss, and\nSolecki 1998), together referred to as “BMS”. Action\nmodels are simple relational structures that can be used to describe a\nvariety of informational actions, from public announcements to more\nsubtle communications that may contain degrees of privacy,\nmisdirection, deception, and suspicion, to name just a few\npossibilities.  \nTo begin, let us consider a specific example of a more complex\ncommunicative action: a completely private announcement. The idea of\nthis action is that one agent, let us call her a, is to receive\na message in complete privacy. Accordingly, no other agent should\nlearn the contents of this message, and, furthermore, no other agent\nshould even consider the possibility that agent a received the\nmessage in the first place. (Think of agent a traveling\nunnoticed to a secret and secure location, finding and reading a coded\nmessage only she can decode, and then destroying the message then and\nthere.) One way to think about this action is as follows: there are\ntwo possible events that might occur. One of these, let us call it\nevent e, is the announcement that p is true; this is the\nsecret message to a. The other event, let us call it f,\nis the announcement that the propositional constant \\(\\top\\) for truth\nis true, an action that conveys no new propositional information\n(since \\(\\top\\) is a tautology). Agent a should know that the\nmessage is p and hence that the event that is in fact occurring\nis e. All other agents should mistakenly believe that it is\ncommon knowledge that the message is \\(\\top\\) and not even consider\nthe possibility that the message is p. Accordingly, other\nagents should consider event f the one and only possibility and\nmistakenly believe that this is common knowledge. We picture a\ndiagrammatic representation of this setup in Figure 3.  \nFigure 3: The pointed action model\n\\((\\Pri_a(p),e)\\) for the completely private announcement of p\nto agent a. \nIn the figure, our two events e and f are pictured as\nrectangles (to distinguish these from the circled worlds of a Kripke\nmodel). The formula appearing inside an event’s rectangle is\nwhat is announced when the event occurs. So event e represents\nthe announcement of p, and event f represents the\nannouncement of \\(\\top\\). The event that actually occurs, called the\n“point”, is indicated using a double rectangle; in this\ncase, the point is e. The only event that a considers\npossible is e because the only a-arrow leaving e\nloops right back to e. But all of the agents in our agent set\n\\(\\sA\\) other than a mistakenly consider the alternative event\nf as the only possibility: all non–a-arrows\nleaving e point to f. Furthermore, from the perspective\nof event f, it is common knowledge that event f (and its\nannouncement of \\(\\top\\)) is the only event that occurs: every agent\nhas exactly one arrow leaving f and this arrow loops right back\nto f. Accordingly, the structure pictured above describes the\nfollowing action: p is to be announced, agent a is to\nknow this, and all other agents are to mistakenly believe it is common\nknowledge that \\(\\top\\) is announced. Structures like those pictured\nin Figure 3 are called action models.  \nAction model (Baltag, Moss, and Solecki 1998, 1999; see also\nBaltag and Moss 2004). Other names in the literature:\n“event model” or “update model”. Given a set\nof formulas \\(\\Lang\\) and a finite nonempty set \\(\\sA\\) of agents, an\naction model is a structure \n\n\\[ A=(E,R,\\pre) \\]\n\n consisting of \nNotation: if A is an action model, then adding a superscript\nA to a symbol in \\(\\{E,R,\\pre\\}\\) is used to denote a component\nof the triple that makes up A in such a way that\n\\((E^A,R^A,\\pre^A)=A\\). We define a pointed action model,\nsometimes also called an action, to be a pair \\((A,e)\\)\nconsisting of an action model A and an event \\(e\\in E^A\\) that\nis called the \\(point\\). In drawing action models, events are drawn as\nrectangles, and a point (if any) is indicated with a double rectangle.\nWe use many of the same drawing and terminological conventions for\naction models that we use for (pointed) Kripke models (see\n Appendix A). \n\\((\\Pri_a(p),e)\\) is the action pictured in\n Figure 3.\n Given an initial pointed Kripke model \\((M,w)\\) at which p is\ntrue, we determine the model-transforming effect of the action\n\\((\\Pri_a(p),e)\\) by constructing a new pointed Kripke model\n\n\\[ (M[\\Pri_a(p)],(w,e)). \\]\n\n The construction of the Kripke model \\(M[\\Pri_a(p)]\\) is\ngiven by the BMS “product update”.  \nProduct update (Baltag, Moss, and Solecki 1998, 1999; see also\nBaltag and Moss 2004). Let \\((M,w)\\) be a pointed Kripke\nmodel and \\((A,e)\\) be a pointed action model. Let \\(\\models\\) be a\nbinary satisfaction relation defined between \\((M,w)\\) and formulas in\nthe language \\(\\Lang\\) of the precondition function\n\\(\\pre^A:E^A\\to\\Lang\\) of the action model A. If\n\\(M,w\\models\\pre^A(e)\\), then the Kripke model \n\n\\[\nM[A]=(W[A],R[A],V[A])\n\\]\n\n is defined\nvia the product update operation \\(M\\mapsto M[A]\\) given as\nfollows: \nAn action \\((A,e)\\) operates on an initial situation \\((M,w)\\)\nsatisfying \\(M,w\\models\\pre^A(e)\\) via the product update to produce\nthe resultant situation \\((M[A],(w,e))\\).  \nIn this definition, the worlds of \\(M[A]\\) are obtained by making\nmultiple copies of the worlds of M, one copy per event \\(f\\in\nE^A\\). The event-f copy of a world v in M is\nrepresented by the pair \\((v,f)\\). Such a pair is to be included in\nthe worlds of \\(M[A]\\) if and only if \\((M,v)\\) satisfies the\nprecondition \\(\\pre^A(f)\\) of event f. The term “product\nupdate” comes from the fact that the set \\(W[A]\\) of worlds of\n\\(M[A]\\) is specified by restricting the full Cartesian product\n\\(W^M\\times E^A\\) to those pairs \\((v,f)\\) whose indicated world\nv satisfies the precondition \\(\\pre^A(f)\\) of the indicated\nevent f; that is, the “product update” is based on\na restricted Cartesian product, hence the name.  \nAccording to the product update, we insert an a-arrow\n\\((v_1,f_1)\\to_a (v_2,f_2)\\) in \\(M[A]\\) if and only if there is an\na-arrow \\(v_1\\to_a v_2\\) in M and an a-arrow\n\\(f_1\\to_a f_2\\) in A. In this way, agent a’s\nuncertainty in the resultant model \\(M[A]\\) comes from two sources:\nher initial uncertainty in M (represented by \\(R^M_a\\)) as to\nwhich is the actual world and her uncertainty in A (represented\nby \\(R^A_a\\)) as to which is the actual event. Finally, the valuation\nat the copy \\((v,f)\\) in \\(M[A]\\) is just the same as it was at the\noriginal world v in M.  \nFor an example of the product update in action, consider the following\npointed Kripke model \\((M,w)\\):  \nThe action model \\(\\Pri_a(p)\\) from\n Figure 3\n operates on \\((M,w)\\) via the product update to produce the resultant\nsituation \\((M[\\Pri_a(p)],(w,e))\\) pictured as follows:  \nIndeed, to produce \\(M[\\Pri_a(p)]\\) from M via the product\nupdate with the action model \\(\\Pri_a(p)\\):  \nWe therefore obtain the model \\(M[\\Pri_a(p)]\\) as pictured above. We\nnote that the product update-induced mapping \n\n\\[\n(M,w) \\mapsto\n(M[A],(w,e))\n\\]\n\n from the\ninitial situation \\((M,w)\\) to the resultant situation\n\\((M[A],(w,e))\\) has the following effect: we go from an initial\nsituation \\((M,w)\\) in which neither agent knows whether p is\ntrue to a resultant situation \\((M[A],(w,e))\\) in which a knows\np is true but b mistakenly believes everyone’s\nknowledge is unchanged. This is of course just what we want of the\nprivate announcement of p to agent a.  \nWe now take a moment to comment on the similarities and differences\nbetween action models and Kripke models. To begin, both are labeled\ndirected graphs (consisting of labeled nodes and labeled edges\npointing between the nodes). A node of a Kripke model (a\n“world”) is labeled by the propositional letters that are\ntrue at the world; in contrast, a node of an action model (an\n“event”) is labeled by a single formula that is to be\nannounced if the event occurs. However, in both cases, agent\nuncertainty is represented using the same “considered\npossibilities” approach. In the case of Kripke models, an agent\nconsiders various possibilities for the world that might be actual; in\nthe case of action models, an agent considers various possibilities\nfor the event that might actually occur. The key insight behind action\nmodels, as put forward by Baltag, Moss, and Solecki (1998), is that\nthese two uncertainties can be represented using similar\ngraph-theoretic structures. We can therefore leverage our experience\nworking with Kripke models when we need to devise new action models\nthat describe complex communicative actions. In particular, to\nconstruct an action model for a given action, all we must do is break\nup the action into a number of simple announcement events and then\ndescribe the agents’ respective uncertainties among these events\nin the appropriate way so as to obtain the desired action. The\ndifficulty, of course, is in determining the exact uncertainty\nrelationships. However, this determination amounts to inserting the\nappropriate agent arrows between possible events, and doing this\nrequires the same kind of reasoning as that which we used in the\nconstruction of Kripke models meeting certain basic or higher-order\nknowledge constraints. We demonstrate this now by way of example,\nconstructing a few important action models along the way.  \nWe saw the example of a completely private announcement in\n Figure 3,\n a complex action in which one agent learns something without the\nother agents even suspecting that this is so. Before devising an\naction model for another similarly complicated action, let us return\nto our most basic action: the public announcement of p. The\nidea of this action is that all agents receive the information that\np is true, and this is common knowledge. So to construct an\naction model for this action, we need only one event e that\nconveys the announcement that p is true, and the occurrence of\nthis event should be common knowledge. This leads us immediately to\nthe action model \\(\\Pub(p)\\) pictured in Figure 4.  \nFigure 4: The pointed action model\n\\((\\Pub(p),e)\\) for the public announcement of p.  \nIt is not difficult to see that \\(\\Pub(p)\\) is just what we want:\nevent e conveys the desired announcement and the reflexive\narrows for each agent make it so that this event is common knowledge.\nIt is important to note that in virtue of the fact that we can\nconstruct an action model for public announcements, it follows that\naction models are a generalization of public announcements.  \nWe now turn to a more complicated action: the semi-private\nannouncement of p to agent a (sometimes called the\n“semi-public announcement” of p to agent a).\nThe idea of this action is that agent a is told that p\nis true, the other agents know that a is told the truth value\nof p, but these other agents do not know what it is exactly\nthat a is told. This suggests an action model with two events,\none for each thing that a might be told: an event e that\nannounces p and event f that announces \\(\\lnot p\\).\nAgent a is to know which event occurs, whereas all other agents\nare to be uncertain as to which event occurs. This leads us to the\naction model \\(\\frac12\\Pri_a(p)\\) pictured in Figure 5.  \nFigure 5: The pointed action model\n\\((\\frac12\\Pri_a(p),e)\\) for the semi-private announcement of p\nto agent a.  \nWe see that \\(\\frac12\\Pri_a(p)\\) satisfies just what we want: the\nactual event that occurs is the point e (the announcement of\nthe precondition p), agent a knows this, but all other\nagents consider it possible that either e (the announcement of\np) or f (the announcement of \\(\\lnot p\\)) occurred.\nFurthermore, the other agents know that a knows which event was\nthe case (since at each of the events e and f that they\nconsider possible, agent a knows the event that occurs). This\nis just what we want of a semi-private announcement.  \nFinally, let us consider a much more challenging action: the\nmisleading private announcement of p to agent a. The\nidea of this action is that agent a is told p in a\ncompletely private manner but all other agents are misled into\nbelieving that a received the private announcement of \\(\\lnot\np\\) instead. So to construct an action model for this, we need a few\nelements: events for the private announcement of \\(\\lnot p\\) to\na that the non-a agents mistakenly believe occurs and an\nevent for the actual announcement of p that only a knows\noccurs. As for the events for the private announcement of \\(\\lnot p\\),\nit follows by a simple modification of\n Figure 3\n that the private announcement of \\(\\lnot p\\) to agent a is the\naction \\((\\Pri_a(\\lnot p),e)\\) pictured as follows:  \nSince the other agents are to believe that the above action occurs,\nthey should believe it is event e that occurs. However, they\nare mistaken: what actually does occur is a new event g that\nconveys to a the private information that p is true.\nTaken together, we obtain the action \\((\\MPri_a(p),g)\\) pictured in\nFigure 6.  \nFigure 6: The pointed action model\n\\((\\MPri_a(p),g)\\) for the misleading private announcement of p\nto agent a.  \nLooking at \\(\\MPri_a(p)\\), we see that if we were to to delete event\ng (and all arrows to and from g), then we would obtain\n\\(\\Pri_a(\\lnot p)\\). So events e and f in \\(\\MPri_a(p)\\)\nplay the role of representing the “misdirection” the\nnon-a agents experience: the private announcement of \\(\\lnot\np\\) to agent a. However, it is event g that actually\noccurs: this event conveys to a that p is true while\nmisleading the other agents into believing that it is event e,\nthe event corresponding to the private announcement of \\(\\lnot p\\) to\na, that occurs. In sum, a receives the information that\np is true while the other agents are mislead into believing\nthat a received the private announcement of \\(\\lnot p\\). One\nconsequence of this is that non-a agents come to hold the\nfollowing beliefs: \\(\\lnot p\\) is true, agent a knows this, and\nagent a believes the others believe that no new propositional\ninformation was provided. These beliefs are all incorrect. The\nnon-a agents are therefore highly mislead.  \nNow that we have seen a number of action models, we turn to the formal\nsyntax and semantics of the language \\eqref{EAL} of Epistemic\nAction Logic (a.k.a., the Logic of Epistemic Actions).\nWe define the language \\eqref{EAL} along with the set \\(\\AM_*\\) of\npointed action models with preconditions in the language \\eqref{EAL}\naccording to the following recursive grammar:  \nTo be clear: in the language \\eqref{EAL}, the precondition\n\\(\\pre^A(e)\\) of an action model A may be a formula that\nincludes an action model modality \\([A',e']\\) for some other action\n\\((A',e')\\in\\AM_*\\). For full technical details on how this works,\nplease see\n Appendix H.\n  \nFor convenience, we let \\(\\AM\\) denote the set of all action models\nwhose preconditions are all in the language \\eqref{EAL}. As we saw in\nthe previous two subsections, the set \\(\\AM_*\\) contains pointed\naction models for public announcements\n (Figure 4),\n private announcements\n (Figure 3),\n semi-private announcements\n (Figure 5),\n and misleading private announcements\n (Figure 6),\n along with many others. The satisfaction relation \\(\\models\\) between\npointed Kripke models and formulas of \\eqref{EAL} is the smallest\nextension of the relation \\(\\models\\) for \\eqref{ML} (see\n Appendix A)\n satisfying the following:  \nNote that the formula \\([A,e]G\\) is vacuously true if the precondition\n\\(\\pre(e)\\) of event e is false. Accordingly, the action model\nsemantics retains the assumption of truthfulness that we had for\npublic announcements. That is, for an event to actually occur, its\nprecondition must be true. As a consequence, the occurrence of an\nevent e implies that its precondition \\(\\pre(e)\\) was true, and\nhence the occurrence of an event conveys its precondition formula as a\nmessage. If an event can occur at a given world, then we say that the\nevent is executable at that world.  \nExecutable events and action models. To say that a\npointed action model \\((A,e)\\) is executable at a pointed\nKripke model \\((M,w)\\) means that \\(M,w\\models\\pre(e)\\). To say that\nan event f in an action model A is executable\nmeans that \\((A,f)\\) is executable. To say that an action model\nA is executable in a Kripke model M means there\nis an event f in A and a world v in M such\nthat f is executable at \\((M,v)\\). \nAs was the case for PAL, one often wishes to restrict attention to\nKripke models whose relations \\(R_a\\) satisfy certain desirable\nproperties such as reflexivity, transitivity, Euclideanness, and\nseriality. In order to study actions over such classes, we must be\ncertain that the actions do not transform a Kripke model in the class\ninto a new Kripke model not in the class; that is, we must ensure that\nthe class of Kripke models is “closed” under actions. The\nfollowing theorem provides some sufficient conditions that guarantee\nclosure.  \nAction Model Closure Theorem. Let \\(M=(W^M,R^M,V)\\)\nbe a Kripke model and \\(A=(W^A,R^A,\\pre)\\) be an action model\nexecutable in M. \nThis theorem, like the analogous theorem for Public Announcement\nLogic, is used in providing simple sound and complete theories for the\nLogic of Epistemic Actions based on appropriate\n“action-friendly” logics.  \nAction-friendly logic. To say that a logic \\(\\L\\) is\naction-friendly means we have the following: \nThe various axiomatic theories of modal logic with action models\n(without common knowledge) are obtained based on the choice of an\nunderlying action-friendly logic \\(\\L\\).  \nThe axiomatic theory \\(\\EAL\\). Other names in the\nliterature: \\(\\DEL\\) or \\(\\AM\\) (for “action model”; see\nvan Ditmarsch, van der Hoek, and Kooi 2007). \nThe first three reduction axioms are nearly identical to the\ncorresponding reduction axioms for \\(\\PAL\\), except that the first and\nthird \\(\\EAL\\) reduction axioms check the truth of a precondition in\nthe place where the \\(\\PAL\\) reduction axioms would check the truth of\nthe formula to be announced. This is actually the same kind of check:\nfor an event, the precondition must hold in order for the event to be\nexecutable; for a public announcement, the formula must be true in\norder for the public announcement to occur (and hence for the public\nannouncement event in question to be “executable”). The\nmajor difference between the \\(\\PAL\\) and \\(\\EAL\\) reduction axioms is\nin the fourth \\(\\EAL\\) reduction axiom. This axiom specifies the\nconditions under which an agent has belief (or knowledge) of something\nafter the occurrence of an action. In particular, adopting a doxastic\nreading for this discussion, the axiom says that agent a\nbelieves G after the occurrence of action \\((A,e)\\) if and only\nif the formula \n\n\\[\n\\textstyle\n\\pre(e)\\to\\bigwedge_{e R_af}[a][A,f]G\n\\]\n\n is true. This formula, in turn, says that\nif the precondition is true—and therefore the action is\nexecutable—then, for each of the possible events the agent\nentertains, she believes that G is true if the event in\nquestion occurs. This makes sense: a cannot be sure which of\nthe events has occurred, and so for her to believe something after the\naction has occurred, she must be sure that this something is true no\nmatter which of her entertained events might have been the actual one.\nFor example, if a sees her friend b become elated as he\nlistens to something he hears on the other side of a private phone\ncall, then the a may not know exactly what it is that b\nis being told; nevertheless, a has reason to believe that\nb is receiving good news because, no matter what it is exactly\nthat he is being told (i.e., no matter which of the events she thinks\nthat he may be hearing), she knows from his reaction that he must be\nreceiving good news.  \nAs was the case for \\(\\PAL\\), the \\(\\EAL\\) reduction axioms allow us\nto “reduce” each formula containing action models to a\nprovably equivalent formula whose action model modalities appear\nbefore formulas of lesser complexity, allowing us to eliminate action\nmodel modalities completely via a sequence of provable equivalences.\nAs a consequence, we have the following.  \n\\(\\EAL\\) Reduction Theorem (Baltag, Moss, and Solecki 1998,\n1999; see also Baltag and Moss 2004). Given an\naction-friendly logic \\(\\L\\), every F in the language \\(\\LEAL\\)\nof Epistemic Action Logic (without common knowledge) is\n\\(\\EAL\\)-provably equivalent to a formula \\(F^\\circ\\) coming from the\naction model-free modal language \\eqref{ML}. \nOnce we have proved \\(\\EAL\\) is sound, the Reduction Theorem leads us\nto axiomatic completeness via the known completeness of the underlying\nmodal theory.  \n\\(\\EAL\\) Soundness and Completeness (Baltag, Moss, and Solecki\n1998, 1999; see also Baltag and Moss 2004). \\(\\EAL\\) is sound\nand complete with respect to the collection \\(\\sC_*\\) of pointed\nKripke models for which the underlying action-friendly logic \\(\\L\\) is\nsound and complete. That is, for each \\(\\LEAL\\)-formula F, we\nhave that \\(\\EAL\\vdash F\\) if and only if \\(\\sC_*\\models F\\). \nWe saw above that for \\(\\PAL\\) it was possible to combine two\nconsecutive announcements into a single announcement via the schematic\nvalidity \n\n\\[ [F!][G!]H\\leftrightarrow[F\\land[F!]G!]H.  \\]\n\n Something similar is available for action\nmodels.  \nAction model composition. The composition\n\\(A\\circ B=(E,R,\\pre)\\) of action models \\(A=(E^A,R^A,\\pre^A)\\) and\n\\(B=(E^B,R^B,\\pre^B)\\) is defined as follows: \nComposition Theorem. Each instance of the following\nschemes is \\(\\EAL\\)-derivable (so long as they are permitted in the\nlanguage \\(\\LEAL\\)). \nWe conclude this subsection with two complexity results for\n\\eqref{EAL}.  \nEAL Complexity (Aucher and Schwarzentruber 2013). Let\n\\(\\sC\\) be the class of all Kripke models. \n\n Appendix G\n provides information on action model equivalence (including the\nnotions of action model bisimulation and emulation), studies a simple\nmodification that enables action models to change the truth value of\npropositional letters (permitting so-called “factual\nchanges”), and shows how to add common knowledge to \\(\\EAL\\).\n \nIn this section, we mention some variants of the action model approach\nto Kripke model transformation.  \nMore on these variants to the action model approach may be found in\n Appendix I.\n  \nUp to this point, the logics we have developed all have one key\nlimitation: an agent cannot meaningfully assimilate information that\ncontradicts her knowledge or beliefs; that is, incoming information\nthat is inconsistent with an agent’s knowledge or\nbelief leads to difficulties. For example, if agent a believes\np, then announcing that p is false brings about a state\nin which the agent’s beliefs are trivialized (in the sense that\nshe comes to believe every sentence):  \nNote that in the above, we may replace F by a contradiction\nsuch as the propositional constant \\(\\bot\\) for falsehood.\nAccordingly, an agent who initially believes p is lead by an\nannouncement that p is false to an inconsistent state in which\nshe believes everything, including falsehoods. This\ntrivialization occurs whenever something is announced that contradicts\nthe agent’s beliefs; in particular, it occurs if a contradiction\nsuch as \\(\\bot\\) is itself announced:  \nIn everyday life, the announcement of a contradiction, when recognized\nas such, is generally not informative; at best, a listener who\nrealizes she is hearing a contradiction learns that there is some\nproblem with the announcer or the announced information itself.\nHowever, the announcement of something that is not intrinsically\ncontradictory but merely contradicts existing beliefs is an everyday\noccurrence of great importance: upon receipt of trustworthy\ninformation that our belief about something is wrong, a rational\nresponse is to adjust our beliefs in an appropriate way. Part of this\nadjustment requires a determination of our attitude toward the general\nreliability or trustworthiness of the incoming information: perhaps we\ntrust it completely, like a young child trusts her parents. Or maybe\nour attitude is more nuanced: we are willing to trust the information\nfor now, but we still allow for the possibility that it might be\nwrong, perhaps leading us to later revise our beliefs if and when we\nlearn that it is incorrect. Or maybe we are much more skeptical: we\ndistrust the information for now, but we do not completely disregard\nthe possibility, however seemingly remote, that it might turn out to\nbe true.  \nWhat is needed is an adaptation of the above-developed frameworks that\ncan handle incoming information that may contradict existing beliefs\nand that does so in a way that accounts for the many nuanced attitudes\nan agent may have with respect to the general reliability or\ntrustworthiness of the information. This has been a focus of much\nrecent activity in the DEL literature.  \nBelief Revision is the study of belief change brought about by the\nacceptance of incoming information that may contradict initial beliefs\n(Gärdenfors 2003; Ove Hansson 2012; Peppas 2008). The seminal\nwork in this area is due to Alchourrón, Gärdenfors, and\nMackinson, or “AGM” (1985). The AGM approach to belief\nrevision characterizes belief change using a number of postulates.\nEach postulate provides a qualitative account of the belief revision\nprocess by saying what must obtain with respect to the agent’s\nbeliefs after revision by an incoming formula F. For example,\nthe AGM Success postulate says that the formulas the agent\nbelieves after revision by F must include F itself; that\nis, the revision always “succeeds” in causing the agent to\ncome to believe the incoming information F.  \nBelief Revision has traditionally restricted attention to\nsingle-agent, “ontic” belief change: the beliefs in\nquestion all belong to a single agent, and the beliefs themselves\nconcern only the “facts” of the world and not, in\nparticular, higher-order beliefs (i.e., beliefs about beliefs).\nFurther, as a result of the Success postulate, the incoming formula\nF that brings about the belief change is assumed to be\ncompletely trustworthy: the agent accepts without question\nthe incoming information F and incorporates it into her set of\nbeliefs as per the belief change process.  \nWork on belief change in Dynamic Epistemic Logic incorporates key\nideas from Belief Revision Theory but removes three key restrictions.\nFirst, belief change in DEL can can involve higher-order beliefs (and\nnot just “ontic” information). Second, DEL can be used in\nmulti-agent scenarios. Third, the DEL approach permits agents to have\nmore nuanced attitudes with respect to the incoming information.  \nThe literature on belief change in Dynamic Epistemic Logic makes an\nimportant distinction between “static” and\n“dynamic” belief change (van Ditmarsch 2005; Baltag and\nSmets 2008b; van Benthem 2007).  \nTo better explain and illustrate the difference, let us consider the\nresult of a belief change brought about by the Moore formula  \ninformally read, “p is true but agent a does not\nbelieve it”. Let us suppose that this formula is true; that is,\np is true and, indeed, agent a does not believe that\np is true. Now suppose that agent a receives the formula\n\\eqref{MF} from a completely trustworthy source and is supposed to\nchange her beliefs to take into account the information this formula\nprovides. In a dynamic belief change, she will accept the formula\n\\eqref{MF} and hence, in particular, she will come to believe that\np is true. But then the formula \\eqref{MF} becomes false: she\nnow believes p and therefore the formula \\(\\lnot[a]p\\)\n(“agent a does not believe p”) is false. So\nwe see that this belief change is indeed dynamic: in revising her\nbeliefs based on the incoming true formula \\eqref{MF}, the truth of\nthe formula \\eqref{MF} was itself changed. That is, the\n“situation”, which involves the truth of p and the\nagent’s beliefs about this truth, changed as per the belief\nchange brought about by the agent learning that \\eqref{MF} is true.\n(As an aside, this example shows that for dynamic belief\nchange, the AGM Success postulate is violated and so must be dropped.)\n \nPerhaps surprisingly, it is also possible to undergo a static\nbelief change upon receipt of the true formula \\eqref{MF} from a\ncompletely trustworthy source. For this to happen, we must think of\nthe “situation” with regard to the truth of p and\nthe agent’s beliefs about this truth as completely static, like\na “snapshot in time”. We then look at how the\nagent’s beliefs about that static snapshot might change upon\nreceipt of the completely trustworthy information that \\eqref{MF} was\ntrue in the moment of that snapshot. To make sense of this, it might\nbe helpful to think of it this way: the agent learns something in\nthe present about what was true of her situation in the\npast. So her present views about her past beliefs change, but the\npast beliefs remain fixed. It is as though the agent studies a\nphotograph of herself from the past: her “present self”\nchanges her beliefs about that “past self” pictured in the\nphotograph, fixed forever in time. In a certain respect, the\n“past self” might as well be a different person:  \nNow that I have been told \\eqref{MF} is true at the moment pictured in\nthe photograph, what can I say about the situation in the picture and\nabout the person in that situation?  \nSo to perform a static belief change upon receipt of the\nincoming formula F, the agent is to change her present belief\nbased on the information that F was true in the state of\naffairs that existed before she was told about F.\nAccordingly, in performing a static belief change upon receipt of\n\\eqref{MF}, the agent will come to accept that, just before she was\ntold \\eqref{MF}, the letter p was true but she did not believe\nthat p was true. But most importantly, this will not cause her\nto believe that \\eqref{MF} is true afterward: she is only\nchanging her beliefs about what was true in the past; she has\nnot been provided with information that bears on the present. In\nparticular, while she will change her belief about the truth of\np in the moment that existed just before she was informed of\n\\eqref{MF}, she will leave her present belief about p as it is\n(i.e., she still will not know that p is true). Therefore, upon\nstatic belief revision by \\eqref{MF}, it is still the case that\n\\eqref{MF} is true! (As an aside, this shows that for static\nbelief change, the AGM Success postulate is satisfied.)  \nStatic belief change occurs in everyday life when we receive\ninformation about something that can quickly change, so that the\ninformation can become “stale” (i.e., incorrect) just\nafter we receive it. This happens, for example, with our knowledge of\nthe price of a high-volume, high-volatility stock during trading\nhours: if we check the price and then look away for the rest of the\nday, we only know the price at the given moment in the past and cannot\nguarantee that the price remains the same, even right after we checked\nit. Therefore, we only know the price of the stock in the\npast—not in the present—even though for practical reasons\nwe sometimes operate under the fiction that the price remains constant\nafter we checked it and therefore speak as though we know it (even\nthough we really do not).  \nDynamic belief change is more common in everyday life. It happens\nwhenever we receive information whose truth cannot rapidly become\n“stale”: we are given the information and this information\nbears directly on our present situation.  \nWe note that the distinction between static and dynamic belief change\nmay raise a dilemma that bears on the problem of skepticism in\nEpistemology (see, e.g., entry on\n Epistemology):\n our “dynamic belief change skeptic” might claim that\nall belief changes must be static because we cannot really\nknow that then information we have received has not become stale. To\nthe authors’ knowledge, this topic has not yet been explored.\n \nIn the DEL study of belief change, situations involving the beliefs of\nmultiple agents are represented using a variation of basic Kripke\nmodels called plausibility models. Static belief change is\ninterpreted as conditionalization in these models: without changing\nthe model (i.e., the situation), we see what the agent would believe\nconditional on the incoming information. This will be explained in\ndetail in a moment. Dynamic belief change involves transforming\nplausibility models: after introducing plausibility model-compatible\naction models, we use model operators defined from these\n“plausibility action models” to describe changes in the\nplausibility model (i.e., the situation) itself.  \nOur presentation of the DEL approach to belief change will follow\nBaltag and Smets (2008b), so all theorems and definitions in the\nremainder of Section 4 are due to them unless otherwise noted. Their\nwork is closely linked with the work of van Benthem (2007), Board\n(2004), Grove (1988), and others. For an alternative approach based on\nPropositional Dynamic Logic, we refer the reader to van Eijck and Wang\n(2008).  \nPlausibility models are used to represent more nuanced versions of\nknowledge and belief. These models are also used to reason about\nstatic belief changes. The idea behind plausibility models is\nsimilar to that for our basic Kripke models: each agent considers\nvarious worlds as possible candidates for the actual one. However,\nthere is a key difference: among any two worlds w and v\nthat an agent a considers possible, she imposes a relative\nplausibility order. The plausibility order for agent a\nis denoted by \\(\\geq_a\\). We write  \nNote that if we think of \\(\\geq_a\\) as a “greater than or equal\nto” sign, it is the “smaller” world that is either\nmore plausible or else of equal plausibility. The reason for\nordering things in this way comes from an idea due to Grove (1988): we\nthink of each world as positioned on the surface of exactly one of a\nseries of concentric spheres (of non-equal radii), with a more\nplausible world located on a sphere of smaller radius and a less\nplausible world located on a sphere of greater radius. Consider the\nfollowing illustration:  \nIn this diagram, the black concentric circles indicate spheres, the\nblue points on the smallest (i.e., innermost) sphere are the most\nplausible worlds overall, the red points on the second-smallest (i.e.,\nmiddle) sphere are the second-most plausible worlds, and the green\npoints on the largest sphere are the least plausible worlds overall.\n \nWe write \\(\\leq_a\\) (“no less plausible than”) for the\nconverse plausibility relation: \\(w\\leq_a v\\) means that\n\\(v\\geq_a w\\). Also, we define the strict plausibility\nrelation \\(\\gt_a\\) (“strictly more plausible than”)\nin the usual way: \\(w\\gt_a v\\) means that we have \\(w\\geq_a v\\) and\n\\(v\\ngeq_a w\\). (A slash through the relation means the relation does\nnot hold.) The strict converse plausibility relation\n\\(\\lt_a\\) (“strictly less plausible than”) is defined as\nexpected: \\(w\\lt_a v\\) means that \\(v\\gt_a w\\). Finally, we define the\nequi-plausibility relation \\(\\simeq_a\\) (“equally\nplausible”) as follows: \\(w\\simeq_a v\\) means that we have\n\\(w\\geq_a v\\) and \\(v\\geq_a w\\).  \nWe draw plausibility models much like our basic Kripke models from\nbefore except that we use dashed arrows (instead of solid ones) in\norder to indicate the plausibility relations and also to indicate that\nthe picture in question is one of a plausibility model. We adopt the\nfollowing conventions for drawing plausibility models.  \nAn absence of a drawn or implied a-arrow from v to\nw indicates \\(v\\not\\geq_a w\\):  \nThe picture above indicates there is no a-arrow from v\nto w that is either drawn or implied. So we conclude that\n\\(v\\not\\geq_a w\\) only after we have taken into account all drawn and\nimplied a-arrows and determined that no a-arrow from\nv to w is indicated.  \nWorlds in the same connected component are said to be\ninformationally equivalent.  \nInformational equivalence. Worlds v and\nw are said to be informationally equivalent (for agent\na) if and only if \\(\\cc_a(w)=\\cc_a(v)\\). Notice that we\nhave \\(\\cc_a(w)=\\cc_a(v)\\) if and only if \\(v\\in\\cc_a(w)\\) if and only\nif \\(w\\in\\cc_a(v)\\). \nThe idea is that if w is the actual world, then agent a\nhas the information that the actual world must be one of those in her\nconnected component \\(\\cc_a(w)\\). Thus the set \\(\\cc_a(w)\\) makes up\nthe worlds agent a considers to be possible whenever w\nis the actual world. And since \\(w\\in\\cc_a(w)\\), agent a will\nalways consider the actual world to be possible. Local connectivity\nthen guarantees that the agent always has an opinion as to the\nrelative plausibility of any two worlds among those in \\(\\cc_a(w)\\)\nthat she considers possible.  \nOne consequence of local connectivity is that informationally\nequivalent states can be stratified according to Grove's idea (Grove\n1988) of concentric spheres: the most plausible worlds overall are\npositioned on the innermost sphere, the next-most-plausible worlds are\npositioned on the next-most-larger sphere, and so on, all the way out\nto the positioning of the least-most-plausible worlds on the largest\nsphere overall. (The number of worlds in our pictures of plausibility\nmodels will always be finite—otherwise we could not draw them\naccording to our above-specified conventions—so it is always\npossible to organize the worlds in our pictures into concentric\nspheres in this way.)  \nGrove spheres (Grove 1988) also suggest a natural method for static\nbelief revision in plausibility models: if the agent is told by a\ncompletely trustworthy source that the actual world is among some\nnonempty subset \\(S\\subseteq \\cc_a(w)\\) of her informationally\nequivalent worlds, then she will restrict her attention to the worlds\nin S. The most plausible worlds in S will be the worlds\nshe then considers to be most plausible overall, the\nnext-most-plausible worlds in S will be the worlds she then\nconsiders to be next-most-plausible overall, and so on. That is, she\nwill “reposition” her system of spheres around the set\nS.  \nTo see how all of this works, let us consider a simple example\nscenario in which our two agents a and b are discussing\nthe truth of two statements p and q. In the course of\nthe conversation, it becomes common knowledge that neither agent has\nany information about q and hence neither knows whether\nq is true, though, as it turns out, q happens to be\ntrue. However, it is common knowledge that agent b is an expert\nabout an area of study whose body of work encompasses the question of\nwhether p is true. Further, agent b publicly delivers\nhis expert opinion: p is true. Agent a trusts agent\nb’s expertise and so she (agent a) comes to\nbelieve that p is true. But her trust is not absolute: a\nstill maintains the possibility that agent b is wrong or\ndeceitful; hence she is willing to concede that her belief of p\nis incorrect. Nevertheless, she does trust b for now and comes\nto believe p. Unfortunately, her trust is misplaced: agent\nb has knowingly lied; p is actually false. We picture\nthis scenario in Figure 8.  \nFigure 8: The pointed plausibility model\n\\((N,w_1)\\).  \nIt is easy to see that the pointed plausibility model \\((N,w_1)\\)\nclearly satisfies the property of local connectedness, so this is an\nallowable picture. To see that this picture reasonably represents the\nabove-describe example scenario, first notice that we have one world\nfor each of the four possible truth assignments to the two letters\np and q. At the actual world \\(w_1\\), the letter\np is false and the letter q is true. Agent a\nconsiders each of the four worlds to be informationally equivalent\n(since she does not know with certainty which world is the actual\none); however, she considers the p-worlds to be strictly more\nplausible than the \\(\\lnot p\\)-worlds. This represents her belief that\np is true: each of the worlds she considers to be most\nplausible overall satisfies p. Further, if she is told that\np is in fact false, she will restrict her attention to the\nnext-most-plausible \\(\\lnot p\\)-worlds, thereby statically revising\nher belief. It is in this sense that she trusts b (and so\nbelieves p is true) but does not completely rule out the\npossibility that he is incorrect or deceptive. Since a has no\ninformation about q, each of her spheres—the inner\np-sphere and the outer \\(\\lnot p\\)-sphere—contains both a\nworld at which q is true and a world at which q is\nfalse.  \nNow let us look at the attitudes of agent b. First, we see that\nb has two connected components, one consisting of the\np-worlds and the other consisting of the \\(\\lnot p\\)-worlds,\nand these two components are not informationally equivalent. That is,\nno p-world is informationally equivalent to a \\(\\lnot p\\)-world\nin the eyes of agent b. This tells us that b\nconclusively knows whether p is true. Further, a knows\nthis is so (since each of a’s informationally equivalent\nworlds is one in which b knows whether p is true). Since\nthe actual world is a \\(\\lnot p\\)-world, agent b in fact knows\np is false. Finally, we see that b knows that a\nmistakenly believes that p is true: at each of b’s\ninformationally equivalent worlds \\(w_1\\) and \\(w_2\\), agent a\nbelieves that p is true (since a’s most plausible\nworlds overall, \\(w_3\\) and \\(w_4\\), both satisfy p).  \nWe are now ready for the formal definition of plausibility models.\nThis definition summarizes what we have seen so far.  \nPlausibility model. Given a nonempty set \\(\\sP\\) of\npropositional letters and a finite nonempty set \\(\\sA\\) of agents, a\nplausibility model is a structure \n\n\\[\nM=(W,\\geq,V)\n\\]\n\n consisting\nof \nFor each world w in W and agent a, we define the\nconnected component of w, also called the\na-connected component if emphasizing a is\nimportant, as follows: \n\n\\[\n\\cc_a(w) \\coloneqq \\{ v\\in W \\mid w({\\geq_a}\\cup{\\leq_a})^*v\\} .\n\\]\n\n If \\(\\cc_a(w)=\\cc_a(w)\\), then we\nsay that w and v are informationally equivalent\n(or that they are a-informationally equivalent). The\nrelation \\(\\geq_a\\) must satisfy the property of\nPlausibility, which consists of the following three\nitems: \nA pointed plausibility model, sometimes called a\nscenario or a situation, is a pair \\((M,w)\\)\nconsisting of a plausibility model M and a world w\n(called the point) that designates the state of affairs that\nwe (the formal modelers) currently assume to be actual.  \nIntuitively, \\(w \\geq_a v\\) means that w is no more plausible\nthan v according to agent a. Therefore, it is the\n“smaller” worlds that are more plausible, so that\n\\(\\min_a(\\cc_w(w))\\) is the set of worlds that agent a\nconsiders to be most plausible of all worlds that are\ninformationally equivalent with w.   Local connectivity, as we have seen, ensures that the agent has an opinion as to the relative plausibility of informationally equivalent worlds. Converse well-foundedness guarantees that the agent can always stratify informationally equivalent worlds in such a way that some worlds are the most plausible overall. As a result, we cannot have a situation where agent a has some sequence \\[ w_1\\gt_a w_2\\gt_a w_3 \\gt_a \\cdots \\] of worlds of strictly increasing plausibility, a circumstance in which it would be impossible to find “the most plausible worlds”. By forbidding such a circumstance, converse well-foundedness guarantees that the notion of “the most plausible worlds” is always well-defined.   \nThe collection of formulas interpreted on pointed plausibility models\ngenerally contains at least the formulas coming from the language\n\\eqref{KBox} defined by the following grammar:  \nThe satisfaction relation \\(\\models\\) between pointed plausibility\nmodels and formulas of \\eqref{KBox} is defined as follows.  \nFor each \\eqref{KBox}-formula F and plausibility model \\(M =\n(W, \\geq, V)\\), we define the set  \nof worlds at which F is true. If M is fixed, we may\nsimply write \\(\\sem{F}\\) without the subscript M.  \n\\(K_aF\\) is assigned the reading “agent a has information\nthat F is true”. One may consider \\(K_a\\) as a kind of\nknowledge, though not the kind usually possessed by actual, real-life\nagents (because it satisfies properties such as closure under logical\nconsequence that are typically not satisfied in practice).\nIntuitively, possession of information of F is belief in\nF that persists upon receipt of any further\ninformation, even information that is not true. This kind of\nknowledge is therefore infallible and indefeasible.  \nWe assign \\(\\Box_aF\\) the reading “agent a defeasibly\nknows F”. This is a weak notion of knowledge studied by\nLehrer and Paxson (1969) and by Lehrer (1990, 2000) and formalized by\nStalnaker (1980, 2006). Intuitively, defeasible knowledge of F\nis belief in F that persists upon receipt of any further true\ninformation: the agent believes F and, if told any further\ntrue information, she will continue to believe F.\nDefeasible knowledge is sometimes also called “safe\nbelief”.  \nThe dual form of information possession \\(K_aF\\), written \\(\\hat K_a\nF\\), denotes informational consistency:  \nwhich has the meaning that F is consistent with agent\na’s information. We use this to define a notion of\nconditional belief:  \nwhich is assigned the reading “agent a believes F\nconditional on G”. Sometimes \\(B_a^GF\\) is abbreviated by\n\\(B_a(F|G)\\). Though the meaning of \\(B_a^GF\\) can be derived from the\nabove definitions, the following provides a more intuitive\ninterpretation.  \nTheorem. For each pointed plausibility model\n\\((M,w)\\), we have: \n\n \\[ \\textstyle M,w\\models B_a^GF \\quad\\text{iff}\\quad \\min_a(\\sem{G}_M\\cap\\cc_a(w))\\subseteq\\sem{F}_M ; \\] \n\n that is, agent a believes\nF conditional on G at world w if and only if\nF is true at the most plausible G-worlds that are\nconsistent with a’s information.  \nThis theorem tell us that to see what an agent believes conditional on\nG, all we need to do is look at the agent’s most\nplausible G-worlds. In this way, conditional belief has the\nagent “recenter” her system of spheres over the set of all\nworlds at which G is true. Conditional belief thereby\nimplements static belief revision: to see what agent a believes\nafter statically revising her beliefs by G we simply see what\nit is she believes conditional on G. Thus \\(B_a^GF\\) says that\nagent a believes F after statically revising her beliefs\nby G.  \nThe notion of conditional belief allows us to connect the notions\nknowledge possession \\(K_a\\) and defeasible knowledge \\(\\Box_a\\) with\nthe defeasibility analysis of knowledge, as indicated by the following\nresult.  \nTheorem. For each pointed plausibility model\n\\((M,w)\\), we have each of the following. \nConditional belief gives rise to a notion of unconditional\nbelief obtained by taking the trivial condition \\(\\top\\) (i.e.,\nthe propositional constant for truth) as the condition:  \nSo to see what the agent believes unconditionally, we simply\nconditionalize her beliefs on the trivial condition \\(\\top\\), which is\ntrue everywhere. It is then easy to see that we have the following.\n \nTheorem. For each pointed plausibility model\n\\((M,w)\\), we have: \n\n\\[\n\\textstyle\nM,w\\models B_aF \\quad\\text{iff}\\quad \\min_a(\\cc_a(w))\\subseteq\\sem{F}_M;\n\\]\n\n that is, agent a believes\nF (unconditionally) at world w if and only if F\nis true at the most plausible worlds that are consistent with\na’s information.  \nWe conclude this section with the axiomatic theory characterizing\nthose formulas that are valid in all plausibility models. Since we can\nexpress conditional belief (and since conditional belief describes\nstatic belief revision), what we obtain is a theory of defeasible\nknowledge, possession of information, conditional belief,\nunconditional belief, and static belief revision.  \nThe axiomatic theory \\(\\KBox\\). \n\\(\\KBox\\) Soundness and Completeness. \\(\\KBox\\) is\nsound and complete with respect to the collection \\(\\sC_*\\) of pointed\nplausibility models. That is, for each \\eqref{KBox}-formula F,\nwe have that \\(\\KBox\\vdash F\\) if and only if \\(\\sC_*\\models F\\). \nInstead of taking information possession \\(K_a\\) and defeasible\nknowledge \\(\\Box_a\\) as the basic propositional attitudes, one may\ninstead choose conditional belief statements \\(B_a^GF\\). This choice\ngives the theory \\(\\CDL\\) of Conditional Doxastic Logic. See\n Appendix J\n for details.  \nWe may define a number of additional propositional attitudes beyond\nconditional belief \\(B_a^GF\\), defeasible knowledge \\(\\Box_aF\\), and\ninformation possession \\(K_aF\\). We take a brief look at a two of\nthese that have important connections with the Belief Revision\nliterature.  \nThe theories and operators we have seen so far all concern\nstatic belief change. We now wish to turn to dynamic\nbelief change. For this the approach follows the typical pattern in\nDynamic Epistemic Logic: we take a given static theory (in this case\n\\(\\KBox\\)) and we add action model-style modalities to create the\ndynamic theory. When we did this before in the case of basic\nmulti-modal epistemic and doxastic logic, the relational structure of\nthe added action models matched the relational structure of the models\nof the theory—Kripke models. The structural match between action\nmodels and finite Kripke models is not accidental: the semantics of\naction model modalities (as explained by the\n BMS product update)\n uses the same Kripke model-based notion of agent uncertainty over\nobjects (i.e., the “worlds”) to describe agent uncertainty\nover action model objects (i.e., the “events”). Both\nuncertainties are represented using the same kind of structure: the\nbinary possibility relation \\(R_a\\).  \nFor the present theory of conditional belief \\(B_a^FG\\), defeasible\nknowledge \\(\\Box_aF\\), and information possession \\(K_aF\\), we take a\nsimilar approach: we define plausibility action models, which\nare action model-type objects whose relational structure matches the\nrelational structure of the models of this theory—plausibility\nmodels. Since a finite plausibility model has the form \\((W,\\geq,V)\\),\nour intuition from the Kripke model case suggests that plausibility\naction models should have the form \\((E,\\geq,\\pre)\\), with E a\nfinite nonempty set of events, \\(\\geq\\) a function giving a\nplausibility relation \\(\\geq_a\\) for each agent a, and \\(\\pre\\)\na precondition function as before.  \nPlausibility action model. Given a set of formulas\n\\(\\Lang\\) and a finite nonempty set \\(\\sA\\) of agents, a\nplausibility action model is a structure \n\n\\[ A=(E,\\geq,\\pre) \\]\n\nconsisting of \nA pointed plausibility action model, sometimes also called an\naction, is a pair \\((A,e)\\) consisting of a plausibility\naction model A and an event e in A that is called\nthe point. In drawing plausibility action models, events are\ndrawn as rectangles, a point (if any) is indicated with a double\nrectangle, and arrows are drawn using dashes (as for plausibility\nmodels). We use many of the same drawing and terminological\nconventions for plausibility action models that we use for (pointed)\nplausibility models. \nAs expected, the main difference between plausibility action models\nand basic action models is that the agent-specific component (i.e.,\nthe function \\(\\geq\\) giving the agent-specific relation \\(\\geq_a\\)).\nIn constructing new plausibility models based on plausibility action\nmodels, we may follow a construction similar to the\n product update.\n To make this work, our main task is to describe how the plausibility\nrelation \\(\\geq_a\\) in the resultant plausibility model \\(M[A]\\) is to\nbe determined in terms of the plausibility relations coming from the\ngiven initial plausibility model M and the plausibility action\nmodel A. For this it will be helpful to consider an example.\n \nFigure 9: The pointed plausibility\naction model \\((\\rPub(q),e)\\) for the revisable public announcement of\nq (also called the “lexicographic upgrade by\nq” by van Benthem 2007).  \nFigure 9 depicts \\((\\rPub(q),e)\\), a pointed plausibility action model\nconsisting of two events: the event f in which \\(\\lnot q\\) is\nannounced and the event e in which q is announced. Event\ne is the event that actually occurs. For each agent a\n(coming from the full set of agents \\(\\sA\\)), event e is\nstrictly more plausible. We adopt the\n same drawing conventions\n for plausibility action models that we did for plausibility models:\none- and two-way arrows, reflexive and transitive closures, and the\nrequirement of local connectedness. (Well-foundedness follows because\nthe set E of events is finite.) Accordingly, Figure 9\nimplicitly contains reflexive dashed arrows for each agent at each\nevent.  \n\\((\\rPub(q),e)\\) has the following intuitive effect: the public\nannouncement of q (i.e., event e) occurs and this is\ncommon knowledge; however, the agents still maintain the possibility\nthat the negation \\(\\lnot q\\) was announced (i.e., event f\noccurred). In effect, the agents will come to believe q\n(because the announcement of this was most plausible), but they will\nnevertheless maintain the less plausible possibility that q is\nfalse. This allows the agents to accept the announced formula q\nbut with some caution: they can still revise their beliefs if they\nlater learn that q is false.  \nThe “action-priority update” is the analog of the product\nupdate for plausibility models.  \nAction-priority update (Baltag and Smets 2008b). Let\n\\((M,w)\\) be a pointed plausibility model and \\((A,e)\\) be a pointed\nplausibility action model. Let \\(\\models\\) be a binary satisfaction\nrelation defined between \\((M,w)\\) and formulas in the language\n\\(\\Lang\\) of the precondition function \\(\\pre^A:E^A\\to\\Lang\\) of the\nplausibility action model A. If \\(M,w\\models\\pre^A(e)\\), then\nthe plausibility model \n\n\\[ M[A]=(W[A],{\\geq}[A],V[A]) \\]\n\n is defined via the\naction-priority update operation \\(M\\mapsto M[A]\\) given as\nfollows: \nAn action \\((A,e)\\) operates on an initial situation \\((M,w)\\)\nsatisfying \\(M,w\\models\\pre^A(e)\\) via the action-priority update to\nproduce the resultant situation \\((M[A],(w,e))\\). Note that we may\nwrite the plausibility relation \\(\\mathrel{{\\geq}[A]_a}\\) for agent\na after the action-priority update by A simply as\n\\(\\geq_a\\) when the meaning is clear from context. \nWe now turn to Action-Priority Update Logic (a.k.a., the\nLogic of Doxastic Actions). To begin, we define the language\n\\eqref{APUL} of Action-Priority Update Logic along with the set\n\\(\\PAM_*\\) of pointed plausibility action models having preconditions\nin the language \\eqref{APUL} according to the following recursive\ngrammar:  \nThe satisfaction relation \\(\\models\\) between pointed plausibility\nmodels and formulas of \\eqref{APUL} is the smallest extension of the\nabove-defined satisfaction relation \\(\\models\\) for \\eqref{KBox}\nsatisfying the following:  \nIn addition to the revisable public announcement\n (Figure 9),\n there are a number of interesting pointed plausibility action models.\n \nFigure 11: The pointed plausibility\naction model \\((\\rPri_G(q),e)\\) for the private announcement of\nq to the group of agents G.  \nFigure 11 depicts the private announcement of q to a group of\nagents G. This consists of two events: the event e in\nwhich q is announced and the event f in which the\npropositional constant for truth \\(\\top\\) is announced. For agents\noutside the group G, the most plausible event is the one in\nwhich the \\(\\top\\) is announced; for agents in the group, the most\nplausible event is the one in which q is announced. In reality,\nthe announcement of q (i.e., event e) occurs. Since the\npropositional constant for truth \\(\\top\\) is uninformative, the agents\noutside of G will come to believe that the situation is as it\nwas before. The agents inside G, however, will come to believe\nq.  \nThe plausibility action model version of the private announcement\n(Figure 11) is almost identical to the action model version of the\nprivate announcement\n (Figure 3).\n This is because action models are easily converted into plausibility\naction models: simply change the arrows to dashed arrows. In this way,\nwe readily obtain plausibility action models from our existing action\nmodels. In particular, we can obtain plausibility actions for a public\nannouncement by converting\n Figure 4,\n for a semi-private announcement by converting\n Figure 5,\n and for a misleading private announcement by converting\n Figure 6.\n  \nFinally, van Benthem (2007) studied two important operations on\nmulti-agent plausibility models that are representable using the\naction-priority update.  \nWe now study the axiomatic theory of Action-Priority Update Logic.\n \nThe axiomatic theory \\(\\APUL\\). \nThe first three reduction axioms are identical to the corresponding\nreduction axioms for \\(\\EAL\\). The fourth \\(\\APUL\\) reduction axiom is\nalmost identical to the fourth \\(\\EAL\\) reduction axiom. In\nparticular, the fourth \\(\\EAL\\) reduction axiom, which reads  \ndiffers only in the conjunction on the right-hand side: the \\(\\EAL\\)\naxiom has its conjunction over events related to e via the\nKripke model-style relation \\(R_a\\), whereas the \\(\\APUL\\) axiom has\nits conjunction over events related to e via the plausibility\nmodel-style relation \\(\\simeq_a\\).  \nThe fifth \\(\\APUL\\) reduction axiom is new. This axiom captures the\nessence of the action-priority update: for an agent to have defeasible\nknowledge after an action, she must have information about what\nhappens as a result of more plausible actions and, further, she must\nhave defeasible knowledge about the outcome of equi-plausible actions.\nThe reason for this follows from the definition of the resulting\nplausibility relation \\({\\geq}[A]_a\\). As a reminder, this is defined\nby setting \\((v_1,f_1)\\mathrel{{\\geq}[A]_a}(v_2,f_2)\\) if and only if\nwe have one of the following:  \nLooking to the fifth \\(\\APUL\\) reduction axiom, the conjunct\n\\(\\bigwedge_{e\\gt_a f}K_a[A,f]G\\) says that G is true whenever\nan event of plausibility strictly greater than e is applied to\na world within a’s current connected component. This\ntells us that G is true at worlds having greater plausibility\nin light of the first bulleted item above. The other conjunct\n\\(\\bigwedge_{e\\simeq_a f}\\Box_a[A,f]G)\\) of the fifth \\(\\APUL\\)\nreduction axiom says that G is true whenever an event\nequi-plausible with e is applied to world of equal or greater\nplausibility within a’s current connected component. This\ntells us that G is true at worlds having greater or equal\nplausibility in light of the second bulleted item above. Taken\ntogether, since these two bulleted items define when it is that a\nworld has equal or greater plausibility in the resultant model\n\\(M[A]\\), the truth of these two conjuncts at an initial situation\n\\((M,w)\\) at which \\((A,e)\\) is executable implies that G is\ntrue at all worlds of equal or greater plausibility than the actual\nworld \\((w,e)\\) of the resultant model \\(M[A]\\). That is, we have\n\\(M[A],(w,e)\\models\\Box_a G\\) and therefore that \\(M,w\\models[A,e]G\\).\nThis explains the right-to-left direction of the fifth \\(\\APUL\\)\nreduction axiom. The left-to-right direction is explained similarly.\n \nAs was the case for \\(\\EAL\\), the \\(\\APUL\\) reduction axioms allow us\nto “reduce” each formula containing plausibility action\nmodels to a provably equivalent formula whose plausibility action\nmodel modalities appear before formulas of lesser complexity, allowing\nus to eliminate plausibility action model modalities completely via a\nsequence of provable equivalences. As a consequence, we have the\nfollowing.  \nAPUL Reduction Theorem. Every F in the\nlanguage \\eqref{APUL} is \\(\\APUL\\)-provably equivalent to a formula\n\\(F^\\circ\\) coming from the plausibility action model-free modal\nlanguage \\eqref{KBox}. \nOnce we have proved \\(\\APUL\\) is sound, the Reduction Theorem leads us\nto axiomatic completeness via the known completeness of the underlying\nmodal theory \\(\\KBox\\).  \n\\(\\APUL\\) Soundness and Completeness. \\(\\APUL\\) is\nsound and complete with respect to the collection \\(\\sC_*\\) of pointed\nplausibility action models. That is, for each \\eqref{APUL}-formula\nF, we have that \\(\\APUL\\vdash F\\) if and only if \\(\\sC_*\\models\nF\\). \nAs is the case for \\(\\EAL\\), it is possible to combine two consecutive\nactions into a single action. All that is required is an appropriate\nnotion of plausibility action model composition.  \nPlausibility action model composition. The\ncomposition \\(A\\circ B=(E,\\geq,\\pre)\\) of plausibility action\nmodels \\(A=(E^A,\\geq^A,\\pre^A)\\) and \\(B=(E^B,\\geq^B,\\pre^B)\\) is\ndefined as follows: \nComposition Theorem. Each instance of the following\nschemes is \\(\\APUL\\)-derivable. \nIt is also possible to add valuation-changing substitutions (i.e.,\n“factual changes”) to plausibility action models. This is\ndone exactly as it is done for action models proper: substitutions are\nadded to plausibility action models, the action-priority update is\nmodified to account for substitutions in the semantics, and the first\nreduction axiom is changed to account for substitutions in the\naxiomatics. See\n Appendix G\n for details.  \nOne development in DEL is work aimed toward building logics of\nevidence, belief, and knowledge for use in Formal Epistemology.  \nWe refer the reader to\n Appendix K\n for further details.  \nDynamic Epistemic Logics that incorporate probability have been\nstudied by a number of authors. Van Benthem (2003), Kooi (2003),\nBaltag and Smets (2008a), and van Benthem, Gerbrandy, and Kooi (2009b)\nstudied logics of finite probability spaces. Sack (2009) extended the\nwork of Kooi (2003) and van Benthem, Gerbrandy, and Kooi (2009b) to\nfull probability spaces (based on σ-algebras of events). Of\nthese, we mention two in particular:  \nWe refer the reader to\n Appendix L\n for further details.  \nDEL-style model-changing operators have been applied by a number of\nresearchers to the study of preferences, preference change, and\nrelated notions. We refer the reader to\n Appendix M\n for further information, which mentions the work of van Benthem et\nal. (2009), van Benthem and Liu (2007), Liu (2008), Yamada (2007a,b,\n2008), van Eijck (2008), van Eijck and Sietsma (2010), van Benthem,\nGirard, and Roy (2009c), and Liu (2011).  \nAction model-style modalities \\([A,e]\\) of Dynamic Epistemic Logic\nhave a temporally suggestive reading: “after action\n\\((A,e)\\), formula F is true”. This\n“before-after” reading suggests, naturally enough, that\ntime passes as actions occur. The semantics of action models supports\nthis suggestion: determining the truth of an action model formula\n\\([A,e]F\\) in a model—the model “before” the\naction—requires us to apply the model-transforming operation\ninduced by the action \\((A,e)\\) and then see whether F holds in\nthe model that results “after” the action. Channeling\nParikh and Ramanujam (2003) some DEL authors further this suggestion\nby using the temporally charged word “history” to refer to\na sequence of pointed Kripke models brought about by the occurrence of\na sequence of model-transforming operations. All of this seems to\npoint to the existence of a direct relationship between the occurrence\nof model-transforming actions and the passage of time: time passes as\nthese actions occur. However, the formal languages introduced so far\ndo not have a built-in means for directly expressing the passage of\ntime, and so, as a consequence, the axiomatic theories developed above\nare silent on the relationship between the flow of time and the\noccurrence of model-changing actions. This leaves open the possibility\nthat, within the context of these theories, the passage of time and\nthe occurrence of actions need not necessarily relate as we might\notherwise suspect.  \nFor more on this, we refer the interested reader to\n Appendix N,\n which mentions a number of studies that bring some method of\ntime-keeping within the scope of the Dynamic Epistemic Logic approach:\nthe work of Sack (2007, 2008, 2010), Yap (2006, 2011), Hoshi (2009),\nHoshi and Yap (2009), van Benthem, Gerbrandy, and Pacuit (2007), van\nBenthem et al. (2009a), Dégremont, Löwe, and Witzel\n(2011), and Renne, Sack, and Yap (2009, 2015).  \nA number of works utilize tools and techniques from Dynamic Epistemic\nLogic for formal reasoning on topics in mainstream Epistemology.  \nWe have surveyed the literature of Dynamic Epistemic Logic, from its\nearly development in the Public Announcement Logic to the generalized\ncommunication operations of action models, work on qualitative and\nquantitative belief revision, and applications in a variety of areas.\nDynamic Epistemic Logic is an active and expanding area, and we have\nhighlighted a number of open problems and directions for further\nresearch. ","contact.mail":"albaltag@yahoo.com","contact.domain":"yahoo.com"},{"date.published":"2016-06-24","url":"https://plato.stanford.edu/entries/dynamic-epistemic/","author1":"Alexandru Baltag","author1.info":"http://bryan.renne.org/","entry":"dynamic-epistemic","body.text":"\n\n\nDynamic Epistemic Logic is the study of modal logics of model change.\nDEL (pronounced “dell”) is a highly active area of applied\nlogic that touches on topics in many areas, including Formal and\nSocial Epistemology, Epistemic and Doxastic Logic, Belief Revision,\nmulti-agent and distributed systems, Artificial Intelligence,\nDefeasible and Non-monotonic Reasoning, and Epistemic Game Theory.\nThis article surveys DEL, identifying along the way a number of open\nquestions and natural directions for further research.\n\nDynamic Epistemic Logic is the study of a family of modal logics, each\nof which is obtained from a given logical language by adding one or\nmore modal operators that describe model-transforming actions. If\n\\([A]\\) is such a modality, then new formulas of the form \\([A]F\\) are\nused to express the statement that F is true after the\noccurrence of action A. To determine whether \\([A]F\\) is true\nat a pointed Kripke model \\((M,w)\\) (see\n Appendix A\n for definitions), we transform the current Kripke model M\naccording to the prescription of action A and we obtain a new\npointed Kripke model \\((M',w')\\) at which we then investigate whether\nF is true. If it is true there, then we say that original\nformula \\([A]F\\) is true in our starting situation \\((M,w)\\). If\nF is not true in the newly produced situation \\((M',w')\\), then\nwe conclude the opposite: \\([A]F\\) is not true in our starting\nsituation \\((M,w)\\). In this way, we obtain the meaning of \\([A]F\\)\nnot by the analysis of what obtains in a single Kripke model but by\nthe analysis of what obtains as a result of a specific\nmodality-specified Kripke model transformation. This is a shift from a\nstatic semantics of truth that takes place in an individual\nKripke model to a dynamic semantics of truth that takes place\nacross modality-specified Kripke model transformations. The advantage\nof the dynamic perspective is that we can analyze the epistemic and\ndoxastic consequences of actions such as public and private\nannouncements without having to “hard wire” the results\ninto the model from the start. Furthermore, we may look at the\nconsequences of different sequences of actions simply by changing the\nsequence of action-describing modalities.  \nIn the following sections, we will look at the many model-changing\nactions that have been studied in Dynamic Epistemic Logic. Many\nnatural applications and questions arise as part of this study, and we\nwill see some of the results obtained in this work. Along the way it\nwill be convenient to consider many variations of the general formal\nsetup described above. Despite these differences, at the core is the\nsame basic idea: new modalities describing certain\napplication-specific model-transforming operations are added to an\nexisting logical language and the study proceeds from there.\nProceeding now ourselves, we begin with what is perhaps the\nquintessential and most basic model-transforming operation: the public\nannouncement.  \nPublic Announcement Logic (PAL) is the modal logic study of knowledge,\nbelief, and public communication. PAL (pronounced “pal”)\nis used to reason about knowledge and belief and the changes brought\nabout in knowledge and belief as per the occurrence of completely\ntrustworthy, truthful announcements. PAL’s most common\nmotivational examples include the Muddy Children Puzzle and\nthe Sum and Product Puzzle (see, e.g., Plaza 1989, 2007). The\nCheryl’s Birthday problem, which became a sensation on\nthe Internet in April 2015, can also be addressed using PAL. Here we\npresent a version of the Cheryl’s Birthday problem due to Chang\n(2015, 15 April) and a three-child version of the Muddy Children\nPuzzle (Fagin et al. 1995). Instead of presenting the traditional Sum\nand Product Puzzle (see Plaza (1989, 2007) for details), we present\nour own simplification that we call the Sum and Least Common\nMultiple Problem.  \nCheryl’s Birthday (version of Chang (2015, 15\nApril)). Albert and Bernard just met Cheryl.\n“When’s your birthday?” Albert asked Cheryl. \nCheryl thought a second and said, “I’m not going to tell\nyou, but I’ll give you some clues”. She wrote down a list\nof 10 dates: \n“My birthday is one of these”, she said. \nThen Cheryl whispered in Albert’s ear the month—and only\nthe month—of her birthday. To Bernard, she whispered the day,\nand only the day. \n“Can you figure it out now?” she asked Albert. \nWhen is Cheryl’s birthday? \nThe Muddy Children Puzzle. Three children are playing\nin the mud. Father calls the children to the house, arranging them in\na semicircle so that each child can clearly see every other child.\n“At least one of you has mud on your forehead”, says\nFather. The children look around, each examining every other\nchild’s forehead. Of course, no child can examine his or her\nown. Father continues, “If you know whether your forehead is\ndirty, then step forward now”. No child steps forward. Father\nrepeats himself a second time, “If you know whether your\nforehead is dirty, then step forward now”. Some but not all of\nthe children step forward. Father repeats himself a third time,\n“If you know whether your forehead is dirty, then step forward\nnow”. All of the remaining children step forward. How many\nchildren have muddy foreheads? \nThe Sum and Least Common Multiple Puzzle. Referee\nreminds Mr. S and Mr. L that the least common multiple\n(“\\(\\text{lcm}\\)”) of two positive integers x and\ny is the smallest positive integer that is divisible without\nany remainder by both x and y (e.g.,\n\\(\\text{lcm}(3,6)=6\\) and \\(\\text{lcm}(5,7)=35\\)). Referee then says,\n \nAmong the integers ranging from \\(2\\) to \\(7\\), including \\(2\\) and\n\\(7\\) themselves, I will choose two different numbers. I will whisper\nthe sum to Mr. S and the least common multiple to Mr. L.  \nReferee then does as promised. The following dialogue then takes\nplace: \nWhat are the numbers? \nThe Sum and Product Puzzle is like the Sum and Least Common Multiple\nPuzzle except that the allowable integers are taken in the range\n\\(2,\\dots,100\\) (inclusive), Mr. L is told the product of the two\nnumbers (instead of their least common multiple), and the dialogue is\naltered slightly (L: “I don’t know the numbers”, S:\n“I knew you didn’t know them”, L: “Ah, but now\nI do know them”, S: “And now so do I!”). These\nchanges result in a substantially more difficult problem. See Plaza\n(1989, 2007) for details.  \nThe reader is advised to try solving the puzzles himself or herself\nand to read more about PAL below before looking at the PAL-based\nsolutions found in a\n Appendix B.\n Later, after the requisite basics of PAL have been presented, the\nauthors will again point the reader to this appendix.  \nThere are many variations of these puzzles, some of which motivate\nlogics that can handle more than just public communication.\nRestricting attention to the variations above, we note that a formal\nlogic for reasoning about these puzzles must be able to represent\nvarious agents’ knowledge along with changes in this knowledge\nthat are brought about as a result of public announcements. One\nimportant thing to note is that the announcements in the puzzles are\nall truthful and completely trustworthy: so that we\ncan solve the puzzles, we tacitly assume (among other things) that\neverything that is announced is in fact true and that all agents\naccept the content of a public announcement without question. These\nassumptions are of course unrealistic in many everyday situations,\nand, to be sure, there are more sophisticated Dynamic Epistemic Logics\nthat can address more complicated and nuanced attitudes agents may\nhave with respect to the information they receive. Nevertheless, in an\nappropriately restricted situation, Public Announcement Logic provides\na basic framework for reasoning about truthful, completely trustworthy\npublic announcements.  \nGiven a nonempty set \\(\\sP\\) of propositional letters and a finite\nnonempty set \\(\\sA\\) of agents, the basic modal language \\eqref{ML} is\ndefined as follows:  \nFormulas \\([a]F\\) are assigned a reading that is doxastic\n(“agent a believes F”) or epistemic\n(“agent a knows F”), with the particular\nreading depending on the application one has in mind. In this article\nwe will use both readings interchangeably, choosing whichever is more\nconvenient in a given context. In the language \\eqref{ML}, Boolean\nconnectives other than negation \\(\\lnot\\) and conjunction \\(\\land\\)\nare taken as abbreviations in terms of negation in conjunction as is\nfamiliar from any elementary Logic textbook. See\n Appendix A\n for further details on \\eqref{ML} and its Kripke semantics.  \nThe language \\(\\eqref{PAL}\\) of Public Announcement Logic extends the\nbasic modal language \\eqref{ML} by adding formulas \\([F!]G\\) to\nexpress that “after the public announcement of F, formula\nG is true”:  \nSemantically, the formula \\([F!]G\\) is interpreted in a Kripke model\nas follows: to say that \\([F!]G\\) is true means that, whenever\nF is true, G is true after we eliminate all not-F\npossibilities (and all arrows to and from these possibilities). This\nmakes sense: since the public announcement of F is completely\ntrustworthy, all agents respond by collectively eliminating all\nnon-F possibilities from consideration. So to see what obtains\nafter a public announcement of F occurs, we eliminate the\nnon-F worlds and then see what is true in the resulting\nsituation. Formally, \\(\\eqref{PAL}\\)-formulas are evaluated as an\nextension of the binary truth relation \\(\\models\\) between pointed\nKripke models and \\eqref{ML}-formulas (defined in\n Appendix A)\n as follows: given a Kripke model \\(M=(W,R,V)\\) and a world \\(w\\in\nW\\),  \nNote that the formula \\([F!]G\\) is vacuously true if F is\nfalse: the announcement of a false formula is inconsistent with our\nassumption of truthful announcements, and hence every formula follows\nafter a falsehood is announced (ex falso quodlibet). It is\nworth remarking that the dual announcement operator \\(\\may{F!}\\)\ndefined by  \ngives the formula \\(\\may{F!} G\\) the following meaning: F is\ntrue and, after F is announced, G is also true. In\nparticular, we observe that the announcement formula \\(\\may{F!} G\\) is\nfalse whenever F is false.  \nOften one wishes to restrict attention to a class of Kripke models\nwhose relations \\(R_a\\) satisfy certain desirable properties such as\nreflexivity, transitivity, Euclideanness, or seriality. Reflexivity\ntells us that agent knowledge is truthful, transitivity tells us that\nagents know what they know, Euclideanness tells us that agents know\nwhat they do not know, and seriality tells us that agent knowledge is\nconsistent. (A belief reading is also possible.) In order to study\npublic announcements over such classes, we must be certain that the\npublic announcement of a formula F does not transform a given\nKripke model M into a new model \\(M[F!]\\) that falls outside of\nthe class. The following theorem indicates when it is that a given\nclass of Kripke models is “closed” under public\nannouncements (meaning a public announcement performed on a model in\nthe class always yields another model in the class).  \nSee\n Appendix C\n for the definition of reflexivity, transitivity, Euclideanness,\nseriality, and other important relational properties.  \nPublic Announcement Closure Theorem. Let\n\\(M=(W,R,V)\\) be a Kripke model and F be a formula true at at\nleast one world in W. \nThe Public Announcement Closure Theorem tells us that reflexivity,\ntransitivity, and Euclideanness are always closed under the public\nannouncement operation. Seriality is in general not; however, if\nseriality comes with Euclideanness, then public announcements of\nformulas of the form \\(F\\land\\bigwedge_{x\\in\\sA}\\may{x} F\\) (read,\n“F is true and consistent with each agent’s\nknowledge”) preserve both seriality and Euclideanness.\nTherefore, if we wish to study classes of models that are serial,\nthen, to make use of the above theorem, we will need to further\nrestrict to models that are both serial and Euclidean and we will need\nto restrict the language of public announcements so that all\nannouncement formulas have this form. (One could also restrict to\nanother form, so long as public announcements of this form preserve\nseriality over some class \\(\\sC\\) of serial models.) Restricting the\nlanguage \\eqref{PAL} by requiring that public announcements have the\nform \\(F\\land\\bigwedge_{x\\in\\sA}\\may{x} F\\) leads to the language\n\\eqref{sPAL} of serial Public Announcement Logic, which we\nmay use when interested in serial and Euclidean Kripke models.  \nGiven a class of Kripke models satisfying certain properties and a\nmodal logic \\(\\L\\) in the language \\eqref{ML} that can reason about\nthat class, we would like to construct a Public Announcement Logic\nwhose soundness and completeness are straightforwardly proved. To do\nthis, we would like to know in advance that \\(\\L\\) is sound and\ncomplete with respect to the class of models in question, that some\npublic announcement extension \\(\\LPAL\\) of the language \\eqref{ML}\n(e.g., the language \\eqref{sPAL} or maybe even \\eqref{PAL} itself)\nwill include announcements that do not spoil closure, and that there\nis an easy way for us to determine the truth of \\(\\LPAL\\)-formulas by\nlooking only the underlying modal language \\eqref{ML}. This way, we\ncan “reduce” completeness of the public announcement\ntheory to the completeness of the basic modal theory \\(\\L\\). We call\nsuch theories for which this is possible PAL-friendly.  \nPAL-friendly theory. To say that a logic \\(\\L\\) is\nPAL-friendly means we have the following: \nSee\n Appendix D\n for the exact meaning of the first component of a PAL-friendly\ntheory.  \nExamples of PAL-friendly theories include the common “logic of\nbelief” (multi-modal \\(\\mathsf{KD45}\\)), the common “logic\nof knowledge” (multi-modal \\(\\mathsf{S5}\\)), multi-modal\n\\(\\mathsf{K}\\), multi-modal \\(\\mathsf{T}\\), multi-modal\n\\(\\mathsf{S4}\\), and certain logics that mix modal operators of the\npreviously mentioned types (e.g., \\(\\mathsf{S5}\\) for \\([a]\\) and\n\\(\\mathsf{T}\\) for all other agent modal operators \\([b]\\)). Fixing a\nPAL-friendly theory \\(\\L\\), we easily obtain an axiomatic theory of\npublic announcement logic based on \\(\\L\\) as follows.  \nThe axiomatic theory \\(\\PAL\\). \nThe reduction axioms characterize truth of an announcement formula\n\\([F!]G\\) in terms of the truth of other announcement formulas\n\\([F!]H\\) whose post-announcement formula H is less complex\nthan the original post-announcement formula G. In the case\nwhere G is just a propositional letter p, Reduction\nAxiom 1 says that the truth of \\([F!]p\\) can be reduced to a formula\nnot containing any announcements of F. So we see that the\nreduction axioms “reduce” statements of truth of\ncomplicated announcements to statements of truth of simpler and\nsimpler announcements until the mention of announcements is not\nnecessary. For example, writing the reduction axiom used in a\nparenthetical subscript, we have the following sequence of provable\nequivalences:  \nNotice that the last formula does not contain public announcements.\nHence we see that the reduction axioms allow us to express the truth\nof the announcement-containing formula \\([[b]p!](p\\land[a]p)\\) in\nterms of a provably equivalent announcement-free formula. This is true\nin general.  \n\\(\\PAL\\) Reduction Theorem. Given a PAL-friendly\ntheory \\(\\L\\), every F in the language \\(\\LPAL\\) of Public\nAnnouncement Logic (without common knowledge) is \\(\\PAL\\)-provably\nequivalent to a formula \\(F^\\circ\\) coming from the announcement-free\nfragment of \\(\\LPAL\\). \nThe Reduction Theorem makes proving completeness of the axiomatic\ntheory with respect to the appropriate class of pointed Kripke models\neasy: since every \\(\\LPAL\\)-formula can all be expressed using a\nprovably equivalent announcement-free \\eqref{ML}-formula, completeness\nof the theory \\(\\PAL\\) follows by the Reduction Theorem, the soundness\nof \\(\\PAL\\), and the known completeness of the underlying modal theory\n\\(\\L\\).  \n\\(\\PAL\\) Soundness and Completeness. \\(\\PAL\\) is\nsound and complete with respect to the collection \\(\\sC_*\\) of pointed\nKripke models for which the underlying PAL-friendly theory \\(\\L\\) is\nsound and complete. That is, for each \\(\\LPAL\\)-formula F, we\nhave that \\(\\PAL\\vdash F\\) if and only if \\(\\sC_*\\models F\\). \nOne interesting \\(\\PAL\\)-derivable scheme (available if allowed by the\nlanguage \\(\\LPAL\\)) is the following:  \nThis says that two consecutive announcements can be combined into a\nsingle announcement: to announce that F is true and then to\nannounce that G is true will have the same result as announcing\nthe single statement that “F is true and, after F\nis announced, G is true”.  \nWe conclude with a few complexity results for Public Announcement\nLogic.  \nPAL Complexity. Let \\(\\sC\\) be the class of all\nKripke models. Let \\(\\sC_{\\mathsf{S5}}\\) be the class of Kripke models\nsuch that each binary accessibility relation is reflexive, transitive,\nand symmetric. \nOne thing to note about the theory \\(\\PAL\\) as presented above is that\nit is parameterized on a PAL-friendly logic \\(\\L\\). Therefore,\n“Public Announcement Logic” as an area of study in fact\nencompasses a wide-ranging family individual Public Announcement\nLogics, one for each instance of \\(\\L\\). Unless otherwise noted, the\nresults and concepts we present apply to all logics within this\nfamily.  \nIn\n Appendix E,\n we detail further aspects of Public Announcement Logic: schematic\nvalidity, expressivity and succinctness, Gerbrandy–Groeneveld\nannouncements, consistency-preserving announcements and Arrow Update\nLogic, and quantification over public announcements in Arbitrary\nPublic Announcement Logic.  \nWhile iterated public announcements seem like a natural operation to\nconsider (motivated by, e.g., the Muddy Children Puzzle), Miller and\nMoss (2005) showed that a logic of such a language cannot be\nrecursively axiomatized.  \nFinally, PAL-based solutions to the Cheryl’s Birthday, Muddy\nChildren, and Sum and Least Common Multiple Puzzles are presented in\n Appendix B.\n  \nTo reason about common knowledge and public announcements, we add the\ncommon knowledge operators \\([B*]\\) to the language for each group of\nagents \\(B\\subseteq\\sA\\). The formula \\([B*]F\\) is read, ”it is\ncommon knowledge among the group B that F is\ntrue”. We define the language \\eqref{PAL+C} of public\nannouncement logic with common knowledge as follows:  \nThe semantics of this language over pointed Kripke models is defined\nin\n Appendix A.\n We recall two key defined expressions:  \n\\([B]F\\) denotes \\(\\bigwedge_{a\\in B}[a]F\\) — “everyone in\ngroup B knows (or believes) F”; \n\\([C]F\\) denotes \\([\\sA*]F\\) — “it is common knowledge (or\nbelief) that F is true.” \nFor convenience in what follows, we will adopt the epistemic (i.e.,\nknowledge) reading of formulas in the remainder of this subsection. In\nparticular, using the language \\eqref{PAL+C}, we are able to provide a\nformal sense in which public announcements bring about common\nknowledge.  \nTheorem. For each pointed Kripke model \\((M,w)\\), we\nhave: \nWe now examine the axiomatic theory of public announcement logic with\ncommon knowledge.  \nThe axiomatic theory \\(\\PALC\\). \n\\(\\PALC\\) Soundness and Completeness (Baltag, Moss, and\nSolecki 1998, 1999; see also van Ditmarsch, van der Hoek, and Kooi\n2007). \\(\\PALC\\) is sound and complete with respect to the\ncollection \\(\\sC_*\\) of pointed Kripke models for which the underlying\npublic announcement logic \\(\\PAL\\) is sound and complete. That is, for\neach \\eqref{PAL+C}-formula F, we have that \\(\\PALC\\vdash F\\) if\nand only if \\(\\sC_*\\models F\\). \nUnlike the proof of completeness for the logic \\(\\PAL\\) without common\nknowledge, the proof for the logic \\(\\PALC\\) with common knowledge\ndoes not proceed by way of a reduction theorem. This is because adding\ncommon knowledge to the language strictly increases the expressivity.\n \nTheorem (Baltag, Moss, and Solecki 1998, 1999; see also van\nDitmarsch, van der Hoek, and Kooi 2007). Over the class of\nall pointed Kripke models, the language \\eqref{PAL+C} of public\nannouncement logic with common knowledge is strictly more expressive\nthan language \\eqref{PAL} without common knowledge. In particular, the\n\\eqref{PAL+C}-formula \\([p!][C]q\\) cannot be expressed in \\eqref{PAL}\nwith respect to the class of all pointed Kripke models: for every\n\\eqref{PAL}-formula F there exists a pointed Kripke model\n\\((M,w)\\) such that \\(M,w\\not\\models F\\leftrightarrow[p!][C]q\\). \nThis result rules out the possibility of a reduction theorem for\n\\(\\PALC\\): we cannot find a public announcement-free equivalent of\nevery \\eqref{PAL+C}-formula. This led van Benthem, van Eijck, and Kooi\n(2006) to develop a common knowledge-like operator for which a\nreduction theorem does hold. The result is the binary relativized\ncommon knowledge operator \\([B*](F|G)\\), which is read,\n“F is common knowledge among group B relative to\nthe information that G is true”. The language \\eqref{RCK}\nof relativized common knowledge is given by the following grammar:\n \nand the language \\eqref{RCK+P} of relativized common knowledge with\npublic announcements is obtained by adding public announcements to\n\\eqref{RCK}:  \nThe semantics of \\eqref{RCK} is an extension of the semantics of\n\\eqref{ML}, and the semantics of \\eqref{RCK+P} is an extension of the\nsemantics of \\eqref{PAL}. In each case, the extension is obtained by\nadding the following inductive truth clause:  \nHere we recall that \\(R[G!]\\) is the function that obtains after the\npublic announcement of G; that is, we have \\(xR[G!]_ay\\) if and\nonly if x and y are in the model after the announcement\nof G (i.e., \\(M,w\\models G\\) and \\(M,y\\models G\\)) and there is\nan a-arrow from x to y in the original model\n(i.e., \\(xR_ay\\)). The relation \\(R[G!]_B\\) is then the union of the\nrelations for those agents in B; that is, we have \\(xR[G!]_By\\)\nif and only if there is an \\(a\\in B\\) with \\(xR[G!]_ay\\). Finally,\n\\((R[G!]_B)^*\\) is the reflexive-transitive closure of the relation\n\\(R[G!]_B\\); that is, we have \\(x(R[G!]_B)^*y\\) if and only if \\(x=y\\)\nor there is a finite sequence  \nof \\(R[G!]_B\\)-arrows connecting x to y. So, all\ntogether, the formula \\([B*](F|G)\\) is true at w if and only if\nan F-world is at the end of every finite path (of length zero\nor greater) that begins at w, contains only G-worlds,\nand uses only arrows for agents in B. Intuitively, this says\nthat if the agents in B commonly assume G is true in\njointly entertaining possible alternatives to the given state of\naffairs w, then, relative to this assumption, F is\ncommon knowledge among those in B.  \nAs observed by van Benthem, van Eijck, and Kooi (2006), relativized\ncommon knowledge is not the same as non-relativized common knowledge\nafter an announcement. For example, over the collection of all pointed\nKripke models, the following formulas are not equivalent:  \nIn particular, in the pointed model \\((M,w)\\) pictured in Figure 1,\nthe formula \\(\\lnot[\\{a,b\\}*]([a]p\\mid p)\\) is true because there is a\npath that begins at w, contains only p-worlds, uses only\narrows in \\(\\{a,b\\}\\), and ends on the \\(\\lnot[a]p\\)-world u.\n \nFigure 1: The pointed Kripke model\n\\((M,w)\\). \nHowever, the formula \\([p!]\\lnot[\\{a,b\\}*][a]p\\) is false at \\((M,w)\\)\nbecause, after the announcement of p, the model \\(M[p!]\\)\npictured in Figure 2 obtains, and all worlds in this model are\n\\([a]p\\)-worlds. In fact, whenever p is true, the formula\n\\([p!]\\lnot[\\{a,b\\}*][a]p\\) is always false: after the\nannouncement of p, all that remains are p-worlds, and\ntherefore every world is an \\([a]p\\)-world.  \nFigure 2: The pointed Kripke model\n\\((M[p!],w)\\). \nThe axiomatic theories of relativized common knowledge with and\nwithout public announcements along with expressivity results for the\ncorresponding languages are detailed in\n Appendix F.\n  \nWe now state two complexity results for the languages of this\nsubsection.  \n\\eqref{PAL+C} and \\eqref{RCK} Complexity. Let \\(\\sC\\)\nbe the class of all Kripke models. Let \\(\\sC_{\\mathsf{S5}}\\) be the\nclass of Kripke models such that each binary accessibility relation is\nreflexive, transitive, and symmetric. \nIn the remainder of the article, unless otherwise stated, we will\ngenerally assume that we are working with languages that do not\ncontain common knowledge or relativized common knowledge.  \nAnother notion of group knowledge is distributed knowledge\n(Fagin et al. 1995). Intuitively, a group B of agents has\ndistributed knowledge that F is true if and only if, were they\nto pool together all that they know, they would then know F. As\nan example, if agents a and b are going to visit a\nmutual friend, a knows that the friend is at home or at work,\nand b knows that the friend is at work or at the cafe, then\na and b have distributed knowledge that the friend is at\nwork: after they pool together what they know, they will each know the\nlocation of the friend. Distributed knowledge and public announcements\nhave been studied by Wáng and Ågotnes (2011). Related to\nthis is the study of whether a notion of group knowledge (such as\ndistributed knowledge) satisfies the property that something known by\nthe group can be established via communication; see Roelofsen (2007)\nfor details.  \nIt may seem as though public announcements always\n“succeed”, by which we mean that after something is\nannounced, we are guaranteed that that it is true. After all, this is\noften the purpose of an announcement: by making the announcement, we\nwish to inform everyone of its truth. However, it is not hard to come\nup with announcements that are true when announced but false\nafterward; that is, not all announcements are successful. Here are a\nfew everyday examples in plain English.  \nThe opposite of unsuccessful formulas are the “successful”\nones: these are the formulas that are true after they are announced.\nHere one should distinguish between “performative\nannouncements” that bring about truth by their very occurrence\n(e.g., a judge says, “The objection is overruled”, which\nhas the effect of making the objection overruled) and\n“informative announcements” that simply inform their\nlisteners of truth (e.g., our mutual friend says, “I live on\n207th Street”, which has the effect of informing us of something\nthat is already true). Performative announcements are best addressed\nin a Dynamic Epistemic Logic setting using factual changes, a\ntopic discussed in\n Appendix G.\n For now our concern will be with informative announcements.  \nThe phenomena of (un)successfulness of announcements was noted early\non by Hintikka (1962) but was not studied in detail until the advent\nof Dynamic Epistemic Logic. In DEL, the explicit language for public\nannouncements provides for an explicit syntactic definition of\n(un)successfulness.  \n(Un)successful formula (van Ditmarsch and Kooi 2006; see also\nGerbrandy 1999). Let F be a formula in a language with\npublic announcements. \nAs we have seen, the Moore formula  \nis unsuccessful: if (MF) is true, then its announcement eliminates all\n\\(\\lnot p\\)-worlds, thereby falsifying \\(\\lnot[a]p\\) (since the truth\nof \\(\\lnot[a]p\\) requires the existence of an a-arrow leading\nto a \\(\\lnot p\\)-world).  \nAn example of a successful formula is a propositional letter p.\nIn particular, after an announcement of p, it is clear that\np still holds (since the propositional valuation does not\nchange); that is, \\([p!]p\\). Moreover, as the reader can easily\nverify, the formula \\([a]p\\) is also successful.  \nIn considering (un)successful formulas, a natural question arises: can\nwe provide an syntactic characterization of the formulas that are\n(un)successful? That is, is there a way for us know whether a formula\nis (un)successful simply by looking at its form? Building off of the\nwork of Visser et al. (1994) and Andréka, Németi, and van\nBenthem (1998), van Ditmarsch and Kooi (2006) provide one\ncharacterization of some of the successful \\eqref{PAL+C}-formulas.\n \nTheorem (van Ditmarsch and Kooi 2006). The\npreserved formulas are formed by the following grammar.\n\n\\[\\begin{gather*}\nF \\ccoloneqq p \\mid \\lnot p \\mid F\\land F \\mid F\\lor F\\mid [a]F\\mid [\\lnot F!]F \\mid [B*]F\n\\\\\n\\small p\\in\\sP,\\; a\\in\\sA,\\; B\\subseteq\\sA\n\\end{gather*}\\]\n\n Every preserved formula is successful. \nUsing a slightly different notion of successfulness wherein a formula\nF is said to be successful if and only if we have that\n\\(M,w\\models F\\land \\may{a}F\\) implies \\(M[F!],w\\models F\\) for each\npointed Kripke model \\((M,w)\\) coming from a given class \\(\\sC\\),\nHolliday and Icard (2010) provide a comprehensive analysis of\n(un)successfulness with respect to the class of single-agent\n\\(\\mathsf{S5}\\) Kripke models and with respect to the class of\nsingle-agent \\(\\mathsf{KD45}\\) Kripke models. In particular, they\nprovide a syntactic characterization of the successful formulas over\nthese classes of Kripke models. This analysis was extended in part to\na multi-agent setting by Saraf and Sourabh (2012). The highly\ntechnical details of these works are beyond the scope of the present\narticle.  \nFor more on Moore sentences, we refer the reader to\n Section 5.3\n of the Stanford Encyclopedia of Philosophy entry on\n Epistemic Paradoxes\n (Sorensen 2011).  \nIn the previous section, we focused on one kind of model-transforming\naction: the public announcement. In this section, we look at the\npopular “action model” generalization of public\nannouncements due to Baltag, Moss, and Solecki (Baltag, Moss, and\nSolecki 1998), together referred to as “BMS”. Action\nmodels are simple relational structures that can be used to describe a\nvariety of informational actions, from public announcements to more\nsubtle communications that may contain degrees of privacy,\nmisdirection, deception, and suspicion, to name just a few\npossibilities.  \nTo begin, let us consider a specific example of a more complex\ncommunicative action: a completely private announcement. The idea of\nthis action is that one agent, let us call her a, is to receive\na message in complete privacy. Accordingly, no other agent should\nlearn the contents of this message, and, furthermore, no other agent\nshould even consider the possibility that agent a received the\nmessage in the first place. (Think of agent a traveling\nunnoticed to a secret and secure location, finding and reading a coded\nmessage only she can decode, and then destroying the message then and\nthere.) One way to think about this action is as follows: there are\ntwo possible events that might occur. One of these, let us call it\nevent e, is the announcement that p is true; this is the\nsecret message to a. The other event, let us call it f,\nis the announcement that the propositional constant \\(\\top\\) for truth\nis true, an action that conveys no new propositional information\n(since \\(\\top\\) is a tautology). Agent a should know that the\nmessage is p and hence that the event that is in fact occurring\nis e. All other agents should mistakenly believe that it is\ncommon knowledge that the message is \\(\\top\\) and not even consider\nthe possibility that the message is p. Accordingly, other\nagents should consider event f the one and only possibility and\nmistakenly believe that this is common knowledge. We picture a\ndiagrammatic representation of this setup in Figure 3.  \nFigure 3: The pointed action model\n\\((\\Pri_a(p),e)\\) for the completely private announcement of p\nto agent a. \nIn the figure, our two events e and f are pictured as\nrectangles (to distinguish these from the circled worlds of a Kripke\nmodel). The formula appearing inside an event’s rectangle is\nwhat is announced when the event occurs. So event e represents\nthe announcement of p, and event f represents the\nannouncement of \\(\\top\\). The event that actually occurs, called the\n“point”, is indicated using a double rectangle; in this\ncase, the point is e. The only event that a considers\npossible is e because the only a-arrow leaving e\nloops right back to e. But all of the agents in our agent set\n\\(\\sA\\) other than a mistakenly consider the alternative event\nf as the only possibility: all non–a-arrows\nleaving e point to f. Furthermore, from the perspective\nof event f, it is common knowledge that event f (and its\nannouncement of \\(\\top\\)) is the only event that occurs: every agent\nhas exactly one arrow leaving f and this arrow loops right back\nto f. Accordingly, the structure pictured above describes the\nfollowing action: p is to be announced, agent a is to\nknow this, and all other agents are to mistakenly believe it is common\nknowledge that \\(\\top\\) is announced. Structures like those pictured\nin Figure 3 are called action models.  \nAction model (Baltag, Moss, and Solecki 1998, 1999; see also\nBaltag and Moss 2004). Other names in the literature:\n“event model” or “update model”. Given a set\nof formulas \\(\\Lang\\) and a finite nonempty set \\(\\sA\\) of agents, an\naction model is a structure \n\n\\[ A=(E,R,\\pre) \\]\n\n consisting of \nNotation: if A is an action model, then adding a superscript\nA to a symbol in \\(\\{E,R,\\pre\\}\\) is used to denote a component\nof the triple that makes up A in such a way that\n\\((E^A,R^A,\\pre^A)=A\\). We define a pointed action model,\nsometimes also called an action, to be a pair \\((A,e)\\)\nconsisting of an action model A and an event \\(e\\in E^A\\) that\nis called the \\(point\\). In drawing action models, events are drawn as\nrectangles, and a point (if any) is indicated with a double rectangle.\nWe use many of the same drawing and terminological conventions for\naction models that we use for (pointed) Kripke models (see\n Appendix A). \n\\((\\Pri_a(p),e)\\) is the action pictured in\n Figure 3.\n Given an initial pointed Kripke model \\((M,w)\\) at which p is\ntrue, we determine the model-transforming effect of the action\n\\((\\Pri_a(p),e)\\) by constructing a new pointed Kripke model\n\n\\[ (M[\\Pri_a(p)],(w,e)). \\]\n\n The construction of the Kripke model \\(M[\\Pri_a(p)]\\) is\ngiven by the BMS “product update”.  \nProduct update (Baltag, Moss, and Solecki 1998, 1999; see also\nBaltag and Moss 2004). Let \\((M,w)\\) be a pointed Kripke\nmodel and \\((A,e)\\) be a pointed action model. Let \\(\\models\\) be a\nbinary satisfaction relation defined between \\((M,w)\\) and formulas in\nthe language \\(\\Lang\\) of the precondition function\n\\(\\pre^A:E^A\\to\\Lang\\) of the action model A. If\n\\(M,w\\models\\pre^A(e)\\), then the Kripke model \n\n\\[\nM[A]=(W[A],R[A],V[A])\n\\]\n\n is defined\nvia the product update operation \\(M\\mapsto M[A]\\) given as\nfollows: \nAn action \\((A,e)\\) operates on an initial situation \\((M,w)\\)\nsatisfying \\(M,w\\models\\pre^A(e)\\) via the product update to produce\nthe resultant situation \\((M[A],(w,e))\\).  \nIn this definition, the worlds of \\(M[A]\\) are obtained by making\nmultiple copies of the worlds of M, one copy per event \\(f\\in\nE^A\\). The event-f copy of a world v in M is\nrepresented by the pair \\((v,f)\\). Such a pair is to be included in\nthe worlds of \\(M[A]\\) if and only if \\((M,v)\\) satisfies the\nprecondition \\(\\pre^A(f)\\) of event f. The term “product\nupdate” comes from the fact that the set \\(W[A]\\) of worlds of\n\\(M[A]\\) is specified by restricting the full Cartesian product\n\\(W^M\\times E^A\\) to those pairs \\((v,f)\\) whose indicated world\nv satisfies the precondition \\(\\pre^A(f)\\) of the indicated\nevent f; that is, the “product update” is based on\na restricted Cartesian product, hence the name.  \nAccording to the product update, we insert an a-arrow\n\\((v_1,f_1)\\to_a (v_2,f_2)\\) in \\(M[A]\\) if and only if there is an\na-arrow \\(v_1\\to_a v_2\\) in M and an a-arrow\n\\(f_1\\to_a f_2\\) in A. In this way, agent a’s\nuncertainty in the resultant model \\(M[A]\\) comes from two sources:\nher initial uncertainty in M (represented by \\(R^M_a\\)) as to\nwhich is the actual world and her uncertainty in A (represented\nby \\(R^A_a\\)) as to which is the actual event. Finally, the valuation\nat the copy \\((v,f)\\) in \\(M[A]\\) is just the same as it was at the\noriginal world v in M.  \nFor an example of the product update in action, consider the following\npointed Kripke model \\((M,w)\\):  \nThe action model \\(\\Pri_a(p)\\) from\n Figure 3\n operates on \\((M,w)\\) via the product update to produce the resultant\nsituation \\((M[\\Pri_a(p)],(w,e))\\) pictured as follows:  \nIndeed, to produce \\(M[\\Pri_a(p)]\\) from M via the product\nupdate with the action model \\(\\Pri_a(p)\\):  \nWe therefore obtain the model \\(M[\\Pri_a(p)]\\) as pictured above. We\nnote that the product update-induced mapping \n\n\\[\n(M,w) \\mapsto\n(M[A],(w,e))\n\\]\n\n from the\ninitial situation \\((M,w)\\) to the resultant situation\n\\((M[A],(w,e))\\) has the following effect: we go from an initial\nsituation \\((M,w)\\) in which neither agent knows whether p is\ntrue to a resultant situation \\((M[A],(w,e))\\) in which a knows\np is true but b mistakenly believes everyone’s\nknowledge is unchanged. This is of course just what we want of the\nprivate announcement of p to agent a.  \nWe now take a moment to comment on the similarities and differences\nbetween action models and Kripke models. To begin, both are labeled\ndirected graphs (consisting of labeled nodes and labeled edges\npointing between the nodes). A node of a Kripke model (a\n“world”) is labeled by the propositional letters that are\ntrue at the world; in contrast, a node of an action model (an\n“event”) is labeled by a single formula that is to be\nannounced if the event occurs. However, in both cases, agent\nuncertainty is represented using the same “considered\npossibilities” approach. In the case of Kripke models, an agent\nconsiders various possibilities for the world that might be actual; in\nthe case of action models, an agent considers various possibilities\nfor the event that might actually occur. The key insight behind action\nmodels, as put forward by Baltag, Moss, and Solecki (1998), is that\nthese two uncertainties can be represented using similar\ngraph-theoretic structures. We can therefore leverage our experience\nworking with Kripke models when we need to devise new action models\nthat describe complex communicative actions. In particular, to\nconstruct an action model for a given action, all we must do is break\nup the action into a number of simple announcement events and then\ndescribe the agents’ respective uncertainties among these events\nin the appropriate way so as to obtain the desired action. The\ndifficulty, of course, is in determining the exact uncertainty\nrelationships. However, this determination amounts to inserting the\nappropriate agent arrows between possible events, and doing this\nrequires the same kind of reasoning as that which we used in the\nconstruction of Kripke models meeting certain basic or higher-order\nknowledge constraints. We demonstrate this now by way of example,\nconstructing a few important action models along the way.  \nWe saw the example of a completely private announcement in\n Figure 3,\n a complex action in which one agent learns something without the\nother agents even suspecting that this is so. Before devising an\naction model for another similarly complicated action, let us return\nto our most basic action: the public announcement of p. The\nidea of this action is that all agents receive the information that\np is true, and this is common knowledge. So to construct an\naction model for this action, we need only one event e that\nconveys the announcement that p is true, and the occurrence of\nthis event should be common knowledge. This leads us immediately to\nthe action model \\(\\Pub(p)\\) pictured in Figure 4.  \nFigure 4: The pointed action model\n\\((\\Pub(p),e)\\) for the public announcement of p.  \nIt is not difficult to see that \\(\\Pub(p)\\) is just what we want:\nevent e conveys the desired announcement and the reflexive\narrows for each agent make it so that this event is common knowledge.\nIt is important to note that in virtue of the fact that we can\nconstruct an action model for public announcements, it follows that\naction models are a generalization of public announcements.  \nWe now turn to a more complicated action: the semi-private\nannouncement of p to agent a (sometimes called the\n“semi-public announcement” of p to agent a).\nThe idea of this action is that agent a is told that p\nis true, the other agents know that a is told the truth value\nof p, but these other agents do not know what it is exactly\nthat a is told. This suggests an action model with two events,\none for each thing that a might be told: an event e that\nannounces p and event f that announces \\(\\lnot p\\).\nAgent a is to know which event occurs, whereas all other agents\nare to be uncertain as to which event occurs. This leads us to the\naction model \\(\\frac12\\Pri_a(p)\\) pictured in Figure 5.  \nFigure 5: The pointed action model\n\\((\\frac12\\Pri_a(p),e)\\) for the semi-private announcement of p\nto agent a.  \nWe see that \\(\\frac12\\Pri_a(p)\\) satisfies just what we want: the\nactual event that occurs is the point e (the announcement of\nthe precondition p), agent a knows this, but all other\nagents consider it possible that either e (the announcement of\np) or f (the announcement of \\(\\lnot p\\)) occurred.\nFurthermore, the other agents know that a knows which event was\nthe case (since at each of the events e and f that they\nconsider possible, agent a knows the event that occurs). This\nis just what we want of a semi-private announcement.  \nFinally, let us consider a much more challenging action: the\nmisleading private announcement of p to agent a. The\nidea of this action is that agent a is told p in a\ncompletely private manner but all other agents are misled into\nbelieving that a received the private announcement of \\(\\lnot\np\\) instead. So to construct an action model for this, we need a few\nelements: events for the private announcement of \\(\\lnot p\\) to\na that the non-a agents mistakenly believe occurs and an\nevent for the actual announcement of p that only a knows\noccurs. As for the events for the private announcement of \\(\\lnot p\\),\nit follows by a simple modification of\n Figure 3\n that the private announcement of \\(\\lnot p\\) to agent a is the\naction \\((\\Pri_a(\\lnot p),e)\\) pictured as follows:  \nSince the other agents are to believe that the above action occurs,\nthey should believe it is event e that occurs. However, they\nare mistaken: what actually does occur is a new event g that\nconveys to a the private information that p is true.\nTaken together, we obtain the action \\((\\MPri_a(p),g)\\) pictured in\nFigure 6.  \nFigure 6: The pointed action model\n\\((\\MPri_a(p),g)\\) for the misleading private announcement of p\nto agent a.  \nLooking at \\(\\MPri_a(p)\\), we see that if we were to to delete event\ng (and all arrows to and from g), then we would obtain\n\\(\\Pri_a(\\lnot p)\\). So events e and f in \\(\\MPri_a(p)\\)\nplay the role of representing the “misdirection” the\nnon-a agents experience: the private announcement of \\(\\lnot\np\\) to agent a. However, it is event g that actually\noccurs: this event conveys to a that p is true while\nmisleading the other agents into believing that it is event e,\nthe event corresponding to the private announcement of \\(\\lnot p\\) to\na, that occurs. In sum, a receives the information that\np is true while the other agents are mislead into believing\nthat a received the private announcement of \\(\\lnot p\\). One\nconsequence of this is that non-a agents come to hold the\nfollowing beliefs: \\(\\lnot p\\) is true, agent a knows this, and\nagent a believes the others believe that no new propositional\ninformation was provided. These beliefs are all incorrect. The\nnon-a agents are therefore highly mislead.  \nNow that we have seen a number of action models, we turn to the formal\nsyntax and semantics of the language \\eqref{EAL} of Epistemic\nAction Logic (a.k.a., the Logic of Epistemic Actions).\nWe define the language \\eqref{EAL} along with the set \\(\\AM_*\\) of\npointed action models with preconditions in the language \\eqref{EAL}\naccording to the following recursive grammar:  \nTo be clear: in the language \\eqref{EAL}, the precondition\n\\(\\pre^A(e)\\) of an action model A may be a formula that\nincludes an action model modality \\([A',e']\\) for some other action\n\\((A',e')\\in\\AM_*\\). For full technical details on how this works,\nplease see\n Appendix H.\n  \nFor convenience, we let \\(\\AM\\) denote the set of all action models\nwhose preconditions are all in the language \\eqref{EAL}. As we saw in\nthe previous two subsections, the set \\(\\AM_*\\) contains pointed\naction models for public announcements\n (Figure 4),\n private announcements\n (Figure 3),\n semi-private announcements\n (Figure 5),\n and misleading private announcements\n (Figure 6),\n along with many others. The satisfaction relation \\(\\models\\) between\npointed Kripke models and formulas of \\eqref{EAL} is the smallest\nextension of the relation \\(\\models\\) for \\eqref{ML} (see\n Appendix A)\n satisfying the following:  \nNote that the formula \\([A,e]G\\) is vacuously true if the precondition\n\\(\\pre(e)\\) of event e is false. Accordingly, the action model\nsemantics retains the assumption of truthfulness that we had for\npublic announcements. That is, for an event to actually occur, its\nprecondition must be true. As a consequence, the occurrence of an\nevent e implies that its precondition \\(\\pre(e)\\) was true, and\nhence the occurrence of an event conveys its precondition formula as a\nmessage. If an event can occur at a given world, then we say that the\nevent is executable at that world.  \nExecutable events and action models. To say that a\npointed action model \\((A,e)\\) is executable at a pointed\nKripke model \\((M,w)\\) means that \\(M,w\\models\\pre(e)\\). To say that\nan event f in an action model A is executable\nmeans that \\((A,f)\\) is executable. To say that an action model\nA is executable in a Kripke model M means there\nis an event f in A and a world v in M such\nthat f is executable at \\((M,v)\\). \nAs was the case for PAL, one often wishes to restrict attention to\nKripke models whose relations \\(R_a\\) satisfy certain desirable\nproperties such as reflexivity, transitivity, Euclideanness, and\nseriality. In order to study actions over such classes, we must be\ncertain that the actions do not transform a Kripke model in the class\ninto a new Kripke model not in the class; that is, we must ensure that\nthe class of Kripke models is “closed” under actions. The\nfollowing theorem provides some sufficient conditions that guarantee\nclosure.  \nAction Model Closure Theorem. Let \\(M=(W^M,R^M,V)\\)\nbe a Kripke model and \\(A=(W^A,R^A,\\pre)\\) be an action model\nexecutable in M. \nThis theorem, like the analogous theorem for Public Announcement\nLogic, is used in providing simple sound and complete theories for the\nLogic of Epistemic Actions based on appropriate\n“action-friendly” logics.  \nAction-friendly logic. To say that a logic \\(\\L\\) is\naction-friendly means we have the following: \nThe various axiomatic theories of modal logic with action models\n(without common knowledge) are obtained based on the choice of an\nunderlying action-friendly logic \\(\\L\\).  \nThe axiomatic theory \\(\\EAL\\). Other names in the\nliterature: \\(\\DEL\\) or \\(\\AM\\) (for “action model”; see\nvan Ditmarsch, van der Hoek, and Kooi 2007). \nThe first three reduction axioms are nearly identical to the\ncorresponding reduction axioms for \\(\\PAL\\), except that the first and\nthird \\(\\EAL\\) reduction axioms check the truth of a precondition in\nthe place where the \\(\\PAL\\) reduction axioms would check the truth of\nthe formula to be announced. This is actually the same kind of check:\nfor an event, the precondition must hold in order for the event to be\nexecutable; for a public announcement, the formula must be true in\norder for the public announcement to occur (and hence for the public\nannouncement event in question to be “executable”). The\nmajor difference between the \\(\\PAL\\) and \\(\\EAL\\) reduction axioms is\nin the fourth \\(\\EAL\\) reduction axiom. This axiom specifies the\nconditions under which an agent has belief (or knowledge) of something\nafter the occurrence of an action. In particular, adopting a doxastic\nreading for this discussion, the axiom says that agent a\nbelieves G after the occurrence of action \\((A,e)\\) if and only\nif the formula \n\n\\[\n\\textstyle\n\\pre(e)\\to\\bigwedge_{e R_af}[a][A,f]G\n\\]\n\n is true. This formula, in turn, says that\nif the precondition is true—and therefore the action is\nexecutable—then, for each of the possible events the agent\nentertains, she believes that G is true if the event in\nquestion occurs. This makes sense: a cannot be sure which of\nthe events has occurred, and so for her to believe something after the\naction has occurred, she must be sure that this something is true no\nmatter which of her entertained events might have been the actual one.\nFor example, if a sees her friend b become elated as he\nlistens to something he hears on the other side of a private phone\ncall, then the a may not know exactly what it is that b\nis being told; nevertheless, a has reason to believe that\nb is receiving good news because, no matter what it is exactly\nthat he is being told (i.e., no matter which of the events she thinks\nthat he may be hearing), she knows from his reaction that he must be\nreceiving good news.  \nAs was the case for \\(\\PAL\\), the \\(\\EAL\\) reduction axioms allow us\nto “reduce” each formula containing action models to a\nprovably equivalent formula whose action model modalities appear\nbefore formulas of lesser complexity, allowing us to eliminate action\nmodel modalities completely via a sequence of provable equivalences.\nAs a consequence, we have the following.  \n\\(\\EAL\\) Reduction Theorem (Baltag, Moss, and Solecki 1998,\n1999; see also Baltag and Moss 2004). Given an\naction-friendly logic \\(\\L\\), every F in the language \\(\\LEAL\\)\nof Epistemic Action Logic (without common knowledge) is\n\\(\\EAL\\)-provably equivalent to a formula \\(F^\\circ\\) coming from the\naction model-free modal language \\eqref{ML}. \nOnce we have proved \\(\\EAL\\) is sound, the Reduction Theorem leads us\nto axiomatic completeness via the known completeness of the underlying\nmodal theory.  \n\\(\\EAL\\) Soundness and Completeness (Baltag, Moss, and Solecki\n1998, 1999; see also Baltag and Moss 2004). \\(\\EAL\\) is sound\nand complete with respect to the collection \\(\\sC_*\\) of pointed\nKripke models for which the underlying action-friendly logic \\(\\L\\) is\nsound and complete. That is, for each \\(\\LEAL\\)-formula F, we\nhave that \\(\\EAL\\vdash F\\) if and only if \\(\\sC_*\\models F\\). \nWe saw above that for \\(\\PAL\\) it was possible to combine two\nconsecutive announcements into a single announcement via the schematic\nvalidity \n\n\\[ [F!][G!]H\\leftrightarrow[F\\land[F!]G!]H.  \\]\n\n Something similar is available for action\nmodels.  \nAction model composition. The composition\n\\(A\\circ B=(E,R,\\pre)\\) of action models \\(A=(E^A,R^A,\\pre^A)\\) and\n\\(B=(E^B,R^B,\\pre^B)\\) is defined as follows: \nComposition Theorem. Each instance of the following\nschemes is \\(\\EAL\\)-derivable (so long as they are permitted in the\nlanguage \\(\\LEAL\\)). \nWe conclude this subsection with two complexity results for\n\\eqref{EAL}.  \nEAL Complexity (Aucher and Schwarzentruber 2013). Let\n\\(\\sC\\) be the class of all Kripke models. \n\n Appendix G\n provides information on action model equivalence (including the\nnotions of action model bisimulation and emulation), studies a simple\nmodification that enables action models to change the truth value of\npropositional letters (permitting so-called “factual\nchanges”), and shows how to add common knowledge to \\(\\EAL\\).\n \nIn this section, we mention some variants of the action model approach\nto Kripke model transformation.  \nMore on these variants to the action model approach may be found in\n Appendix I.\n  \nUp to this point, the logics we have developed all have one key\nlimitation: an agent cannot meaningfully assimilate information that\ncontradicts her knowledge or beliefs; that is, incoming information\nthat is inconsistent with an agent’s knowledge or\nbelief leads to difficulties. For example, if agent a believes\np, then announcing that p is false brings about a state\nin which the agent’s beliefs are trivialized (in the sense that\nshe comes to believe every sentence):  \nNote that in the above, we may replace F by a contradiction\nsuch as the propositional constant \\(\\bot\\) for falsehood.\nAccordingly, an agent who initially believes p is lead by an\nannouncement that p is false to an inconsistent state in which\nshe believes everything, including falsehoods. This\ntrivialization occurs whenever something is announced that contradicts\nthe agent’s beliefs; in particular, it occurs if a contradiction\nsuch as \\(\\bot\\) is itself announced:  \nIn everyday life, the announcement of a contradiction, when recognized\nas such, is generally not informative; at best, a listener who\nrealizes she is hearing a contradiction learns that there is some\nproblem with the announcer or the announced information itself.\nHowever, the announcement of something that is not intrinsically\ncontradictory but merely contradicts existing beliefs is an everyday\noccurrence of great importance: upon receipt of trustworthy\ninformation that our belief about something is wrong, a rational\nresponse is to adjust our beliefs in an appropriate way. Part of this\nadjustment requires a determination of our attitude toward the general\nreliability or trustworthiness of the incoming information: perhaps we\ntrust it completely, like a young child trusts her parents. Or maybe\nour attitude is more nuanced: we are willing to trust the information\nfor now, but we still allow for the possibility that it might be\nwrong, perhaps leading us to later revise our beliefs if and when we\nlearn that it is incorrect. Or maybe we are much more skeptical: we\ndistrust the information for now, but we do not completely disregard\nthe possibility, however seemingly remote, that it might turn out to\nbe true.  \nWhat is needed is an adaptation of the above-developed frameworks that\ncan handle incoming information that may contradict existing beliefs\nand that does so in a way that accounts for the many nuanced attitudes\nan agent may have with respect to the general reliability or\ntrustworthiness of the information. This has been a focus of much\nrecent activity in the DEL literature.  \nBelief Revision is the study of belief change brought about by the\nacceptance of incoming information that may contradict initial beliefs\n(Gärdenfors 2003; Ove Hansson 2012; Peppas 2008). The seminal\nwork in this area is due to Alchourrón, Gärdenfors, and\nMackinson, or “AGM” (1985). The AGM approach to belief\nrevision characterizes belief change using a number of postulates.\nEach postulate provides a qualitative account of the belief revision\nprocess by saying what must obtain with respect to the agent’s\nbeliefs after revision by an incoming formula F. For example,\nthe AGM Success postulate says that the formulas the agent\nbelieves after revision by F must include F itself; that\nis, the revision always “succeeds” in causing the agent to\ncome to believe the incoming information F.  \nBelief Revision has traditionally restricted attention to\nsingle-agent, “ontic” belief change: the beliefs in\nquestion all belong to a single agent, and the beliefs themselves\nconcern only the “facts” of the world and not, in\nparticular, higher-order beliefs (i.e., beliefs about beliefs).\nFurther, as a result of the Success postulate, the incoming formula\nF that brings about the belief change is assumed to be\ncompletely trustworthy: the agent accepts without question\nthe incoming information F and incorporates it into her set of\nbeliefs as per the belief change process.  \nWork on belief change in Dynamic Epistemic Logic incorporates key\nideas from Belief Revision Theory but removes three key restrictions.\nFirst, belief change in DEL can can involve higher-order beliefs (and\nnot just “ontic” information). Second, DEL can be used in\nmulti-agent scenarios. Third, the DEL approach permits agents to have\nmore nuanced attitudes with respect to the incoming information.  \nThe literature on belief change in Dynamic Epistemic Logic makes an\nimportant distinction between “static” and\n“dynamic” belief change (van Ditmarsch 2005; Baltag and\nSmets 2008b; van Benthem 2007).  \nTo better explain and illustrate the difference, let us consider the\nresult of a belief change brought about by the Moore formula  \ninformally read, “p is true but agent a does not\nbelieve it”. Let us suppose that this formula is true; that is,\np is true and, indeed, agent a does not believe that\np is true. Now suppose that agent a receives the formula\n\\eqref{MF} from a completely trustworthy source and is supposed to\nchange her beliefs to take into account the information this formula\nprovides. In a dynamic belief change, she will accept the formula\n\\eqref{MF} and hence, in particular, she will come to believe that\np is true. But then the formula \\eqref{MF} becomes false: she\nnow believes p and therefore the formula \\(\\lnot[a]p\\)\n(“agent a does not believe p”) is false. So\nwe see that this belief change is indeed dynamic: in revising her\nbeliefs based on the incoming true formula \\eqref{MF}, the truth of\nthe formula \\eqref{MF} was itself changed. That is, the\n“situation”, which involves the truth of p and the\nagent’s beliefs about this truth, changed as per the belief\nchange brought about by the agent learning that \\eqref{MF} is true.\n(As an aside, this example shows that for dynamic belief\nchange, the AGM Success postulate is violated and so must be dropped.)\n \nPerhaps surprisingly, it is also possible to undergo a static\nbelief change upon receipt of the true formula \\eqref{MF} from a\ncompletely trustworthy source. For this to happen, we must think of\nthe “situation” with regard to the truth of p and\nthe agent’s beliefs about this truth as completely static, like\na “snapshot in time”. We then look at how the\nagent’s beliefs about that static snapshot might change upon\nreceipt of the completely trustworthy information that \\eqref{MF} was\ntrue in the moment of that snapshot. To make sense of this, it might\nbe helpful to think of it this way: the agent learns something in\nthe present about what was true of her situation in the\npast. So her present views about her past beliefs change, but the\npast beliefs remain fixed. It is as though the agent studies a\nphotograph of herself from the past: her “present self”\nchanges her beliefs about that “past self” pictured in the\nphotograph, fixed forever in time. In a certain respect, the\n“past self” might as well be a different person:  \nNow that I have been told \\eqref{MF} is true at the moment pictured in\nthe photograph, what can I say about the situation in the picture and\nabout the person in that situation?  \nSo to perform a static belief change upon receipt of the\nincoming formula F, the agent is to change her present belief\nbased on the information that F was true in the state of\naffairs that existed before she was told about F.\nAccordingly, in performing a static belief change upon receipt of\n\\eqref{MF}, the agent will come to accept that, just before she was\ntold \\eqref{MF}, the letter p was true but she did not believe\nthat p was true. But most importantly, this will not cause her\nto believe that \\eqref{MF} is true afterward: she is only\nchanging her beliefs about what was true in the past; she has\nnot been provided with information that bears on the present. In\nparticular, while she will change her belief about the truth of\np in the moment that existed just before she was informed of\n\\eqref{MF}, she will leave her present belief about p as it is\n(i.e., she still will not know that p is true). Therefore, upon\nstatic belief revision by \\eqref{MF}, it is still the case that\n\\eqref{MF} is true! (As an aside, this shows that for static\nbelief change, the AGM Success postulate is satisfied.)  \nStatic belief change occurs in everyday life when we receive\ninformation about something that can quickly change, so that the\ninformation can become “stale” (i.e., incorrect) just\nafter we receive it. This happens, for example, with our knowledge of\nthe price of a high-volume, high-volatility stock during trading\nhours: if we check the price and then look away for the rest of the\nday, we only know the price at the given moment in the past and cannot\nguarantee that the price remains the same, even right after we checked\nit. Therefore, we only know the price of the stock in the\npast—not in the present—even though for practical reasons\nwe sometimes operate under the fiction that the price remains constant\nafter we checked it and therefore speak as though we know it (even\nthough we really do not).  \nDynamic belief change is more common in everyday life. It happens\nwhenever we receive information whose truth cannot rapidly become\n“stale”: we are given the information and this information\nbears directly on our present situation.  \nWe note that the distinction between static and dynamic belief change\nmay raise a dilemma that bears on the problem of skepticism in\nEpistemology (see, e.g., entry on\n Epistemology):\n our “dynamic belief change skeptic” might claim that\nall belief changes must be static because we cannot really\nknow that then information we have received has not become stale. To\nthe authors’ knowledge, this topic has not yet been explored.\n \nIn the DEL study of belief change, situations involving the beliefs of\nmultiple agents are represented using a variation of basic Kripke\nmodels called plausibility models. Static belief change is\ninterpreted as conditionalization in these models: without changing\nthe model (i.e., the situation), we see what the agent would believe\nconditional on the incoming information. This will be explained in\ndetail in a moment. Dynamic belief change involves transforming\nplausibility models: after introducing plausibility model-compatible\naction models, we use model operators defined from these\n“plausibility action models” to describe changes in the\nplausibility model (i.e., the situation) itself.  \nOur presentation of the DEL approach to belief change will follow\nBaltag and Smets (2008b), so all theorems and definitions in the\nremainder of Section 4 are due to them unless otherwise noted. Their\nwork is closely linked with the work of van Benthem (2007), Board\n(2004), Grove (1988), and others. For an alternative approach based on\nPropositional Dynamic Logic, we refer the reader to van Eijck and Wang\n(2008).  \nPlausibility models are used to represent more nuanced versions of\nknowledge and belief. These models are also used to reason about\nstatic belief changes. The idea behind plausibility models is\nsimilar to that for our basic Kripke models: each agent considers\nvarious worlds as possible candidates for the actual one. However,\nthere is a key difference: among any two worlds w and v\nthat an agent a considers possible, she imposes a relative\nplausibility order. The plausibility order for agent a\nis denoted by \\(\\geq_a\\). We write  \nNote that if we think of \\(\\geq_a\\) as a “greater than or equal\nto” sign, it is the “smaller” world that is either\nmore plausible or else of equal plausibility. The reason for\nordering things in this way comes from an idea due to Grove (1988): we\nthink of each world as positioned on the surface of exactly one of a\nseries of concentric spheres (of non-equal radii), with a more\nplausible world located on a sphere of smaller radius and a less\nplausible world located on a sphere of greater radius. Consider the\nfollowing illustration:  \nIn this diagram, the black concentric circles indicate spheres, the\nblue points on the smallest (i.e., innermost) sphere are the most\nplausible worlds overall, the red points on the second-smallest (i.e.,\nmiddle) sphere are the second-most plausible worlds, and the green\npoints on the largest sphere are the least plausible worlds overall.\n \nWe write \\(\\leq_a\\) (“no less plausible than”) for the\nconverse plausibility relation: \\(w\\leq_a v\\) means that\n\\(v\\geq_a w\\). Also, we define the strict plausibility\nrelation \\(\\gt_a\\) (“strictly more plausible than”)\nin the usual way: \\(w\\gt_a v\\) means that we have \\(w\\geq_a v\\) and\n\\(v\\ngeq_a w\\). (A slash through the relation means the relation does\nnot hold.) The strict converse plausibility relation\n\\(\\lt_a\\) (“strictly less plausible than”) is defined as\nexpected: \\(w\\lt_a v\\) means that \\(v\\gt_a w\\). Finally, we define the\nequi-plausibility relation \\(\\simeq_a\\) (“equally\nplausible”) as follows: \\(w\\simeq_a v\\) means that we have\n\\(w\\geq_a v\\) and \\(v\\geq_a w\\).  \nWe draw plausibility models much like our basic Kripke models from\nbefore except that we use dashed arrows (instead of solid ones) in\norder to indicate the plausibility relations and also to indicate that\nthe picture in question is one of a plausibility model. We adopt the\nfollowing conventions for drawing plausibility models.  \nAn absence of a drawn or implied a-arrow from v to\nw indicates \\(v\\not\\geq_a w\\):  \nThe picture above indicates there is no a-arrow from v\nto w that is either drawn or implied. So we conclude that\n\\(v\\not\\geq_a w\\) only after we have taken into account all drawn and\nimplied a-arrows and determined that no a-arrow from\nv to w is indicated.  \nWorlds in the same connected component are said to be\ninformationally equivalent.  \nInformational equivalence. Worlds v and\nw are said to be informationally equivalent (for agent\na) if and only if \\(\\cc_a(w)=\\cc_a(v)\\). Notice that we\nhave \\(\\cc_a(w)=\\cc_a(v)\\) if and only if \\(v\\in\\cc_a(w)\\) if and only\nif \\(w\\in\\cc_a(v)\\). \nThe idea is that if w is the actual world, then agent a\nhas the information that the actual world must be one of those in her\nconnected component \\(\\cc_a(w)\\). Thus the set \\(\\cc_a(w)\\) makes up\nthe worlds agent a considers to be possible whenever w\nis the actual world. And since \\(w\\in\\cc_a(w)\\), agent a will\nalways consider the actual world to be possible. Local connectivity\nthen guarantees that the agent always has an opinion as to the\nrelative plausibility of any two worlds among those in \\(\\cc_a(w)\\)\nthat she considers possible.  \nOne consequence of local connectivity is that informationally\nequivalent states can be stratified according to Grove's idea (Grove\n1988) of concentric spheres: the most plausible worlds overall are\npositioned on the innermost sphere, the next-most-plausible worlds are\npositioned on the next-most-larger sphere, and so on, all the way out\nto the positioning of the least-most-plausible worlds on the largest\nsphere overall. (The number of worlds in our pictures of plausibility\nmodels will always be finite—otherwise we could not draw them\naccording to our above-specified conventions—so it is always\npossible to organize the worlds in our pictures into concentric\nspheres in this way.)  \nGrove spheres (Grove 1988) also suggest a natural method for static\nbelief revision in plausibility models: if the agent is told by a\ncompletely trustworthy source that the actual world is among some\nnonempty subset \\(S\\subseteq \\cc_a(w)\\) of her informationally\nequivalent worlds, then she will restrict her attention to the worlds\nin S. The most plausible worlds in S will be the worlds\nshe then considers to be most plausible overall, the\nnext-most-plausible worlds in S will be the worlds she then\nconsiders to be next-most-plausible overall, and so on. That is, she\nwill “reposition” her system of spheres around the set\nS.  \nTo see how all of this works, let us consider a simple example\nscenario in which our two agents a and b are discussing\nthe truth of two statements p and q. In the course of\nthe conversation, it becomes common knowledge that neither agent has\nany information about q and hence neither knows whether\nq is true, though, as it turns out, q happens to be\ntrue. However, it is common knowledge that agent b is an expert\nabout an area of study whose body of work encompasses the question of\nwhether p is true. Further, agent b publicly delivers\nhis expert opinion: p is true. Agent a trusts agent\nb’s expertise and so she (agent a) comes to\nbelieve that p is true. But her trust is not absolute: a\nstill maintains the possibility that agent b is wrong or\ndeceitful; hence she is willing to concede that her belief of p\nis incorrect. Nevertheless, she does trust b for now and comes\nto believe p. Unfortunately, her trust is misplaced: agent\nb has knowingly lied; p is actually false. We picture\nthis scenario in Figure 8.  \nFigure 8: The pointed plausibility model\n\\((N,w_1)\\).  \nIt is easy to see that the pointed plausibility model \\((N,w_1)\\)\nclearly satisfies the property of local connectedness, so this is an\nallowable picture. To see that this picture reasonably represents the\nabove-describe example scenario, first notice that we have one world\nfor each of the four possible truth assignments to the two letters\np and q. At the actual world \\(w_1\\), the letter\np is false and the letter q is true. Agent a\nconsiders each of the four worlds to be informationally equivalent\n(since she does not know with certainty which world is the actual\none); however, she considers the p-worlds to be strictly more\nplausible than the \\(\\lnot p\\)-worlds. This represents her belief that\np is true: each of the worlds she considers to be most\nplausible overall satisfies p. Further, if she is told that\np is in fact false, she will restrict her attention to the\nnext-most-plausible \\(\\lnot p\\)-worlds, thereby statically revising\nher belief. It is in this sense that she trusts b (and so\nbelieves p is true) but does not completely rule out the\npossibility that he is incorrect or deceptive. Since a has no\ninformation about q, each of her spheres—the inner\np-sphere and the outer \\(\\lnot p\\)-sphere—contains both a\nworld at which q is true and a world at which q is\nfalse.  \nNow let us look at the attitudes of agent b. First, we see that\nb has two connected components, one consisting of the\np-worlds and the other consisting of the \\(\\lnot p\\)-worlds,\nand these two components are not informationally equivalent. That is,\nno p-world is informationally equivalent to a \\(\\lnot p\\)-world\nin the eyes of agent b. This tells us that b\nconclusively knows whether p is true. Further, a knows\nthis is so (since each of a’s informationally equivalent\nworlds is one in which b knows whether p is true). Since\nthe actual world is a \\(\\lnot p\\)-world, agent b in fact knows\np is false. Finally, we see that b knows that a\nmistakenly believes that p is true: at each of b’s\ninformationally equivalent worlds \\(w_1\\) and \\(w_2\\), agent a\nbelieves that p is true (since a’s most plausible\nworlds overall, \\(w_3\\) and \\(w_4\\), both satisfy p).  \nWe are now ready for the formal definition of plausibility models.\nThis definition summarizes what we have seen so far.  \nPlausibility model. Given a nonempty set \\(\\sP\\) of\npropositional letters and a finite nonempty set \\(\\sA\\) of agents, a\nplausibility model is a structure \n\n\\[\nM=(W,\\geq,V)\n\\]\n\n consisting\nof \nFor each world w in W and agent a, we define the\nconnected component of w, also called the\na-connected component if emphasizing a is\nimportant, as follows: \n\n\\[\n\\cc_a(w) \\coloneqq \\{ v\\in W \\mid w({\\geq_a}\\cup{\\leq_a})^*v\\} .\n\\]\n\n If \\(\\cc_a(w)=\\cc_a(w)\\), then we\nsay that w and v are informationally equivalent\n(or that they are a-informationally equivalent). The\nrelation \\(\\geq_a\\) must satisfy the property of\nPlausibility, which consists of the following three\nitems: \nA pointed plausibility model, sometimes called a\nscenario or a situation, is a pair \\((M,w)\\)\nconsisting of a plausibility model M and a world w\n(called the point) that designates the state of affairs that\nwe (the formal modelers) currently assume to be actual.  \nIntuitively, \\(w \\geq_a v\\) means that w is no more plausible\nthan v according to agent a. Therefore, it is the\n“smaller” worlds that are more plausible, so that\n\\(\\min_a(\\cc_w(w))\\) is the set of worlds that agent a\nconsiders to be most plausible of all worlds that are\ninformationally equivalent with w.   Local connectivity, as we have seen, ensures that the agent has an opinion as to the relative plausibility of informationally equivalent worlds. Converse well-foundedness guarantees that the agent can always stratify informationally equivalent worlds in such a way that some worlds are the most plausible overall. As a result, we cannot have a situation where agent a has some sequence \\[ w_1\\gt_a w_2\\gt_a w_3 \\gt_a \\cdots \\] of worlds of strictly increasing plausibility, a circumstance in which it would be impossible to find “the most plausible worlds”. By forbidding such a circumstance, converse well-foundedness guarantees that the notion of “the most plausible worlds” is always well-defined.   \nThe collection of formulas interpreted on pointed plausibility models\ngenerally contains at least the formulas coming from the language\n\\eqref{KBox} defined by the following grammar:  \nThe satisfaction relation \\(\\models\\) between pointed plausibility\nmodels and formulas of \\eqref{KBox} is defined as follows.  \nFor each \\eqref{KBox}-formula F and plausibility model \\(M =\n(W, \\geq, V)\\), we define the set  \nof worlds at which F is true. If M is fixed, we may\nsimply write \\(\\sem{F}\\) without the subscript M.  \n\\(K_aF\\) is assigned the reading “agent a has information\nthat F is true”. One may consider \\(K_a\\) as a kind of\nknowledge, though not the kind usually possessed by actual, real-life\nagents (because it satisfies properties such as closure under logical\nconsequence that are typically not satisfied in practice).\nIntuitively, possession of information of F is belief in\nF that persists upon receipt of any further\ninformation, even information that is not true. This kind of\nknowledge is therefore infallible and indefeasible.  \nWe assign \\(\\Box_aF\\) the reading “agent a defeasibly\nknows F”. This is a weak notion of knowledge studied by\nLehrer and Paxson (1969) and by Lehrer (1990, 2000) and formalized by\nStalnaker (1980, 2006). Intuitively, defeasible knowledge of F\nis belief in F that persists upon receipt of any further true\ninformation: the agent believes F and, if told any further\ntrue information, she will continue to believe F.\nDefeasible knowledge is sometimes also called “safe\nbelief”.  \nThe dual form of information possession \\(K_aF\\), written \\(\\hat K_a\nF\\), denotes informational consistency:  \nwhich has the meaning that F is consistent with agent\na’s information. We use this to define a notion of\nconditional belief:  \nwhich is assigned the reading “agent a believes F\nconditional on G”. Sometimes \\(B_a^GF\\) is abbreviated by\n\\(B_a(F|G)\\). Though the meaning of \\(B_a^GF\\) can be derived from the\nabove definitions, the following provides a more intuitive\ninterpretation.  \nTheorem. For each pointed plausibility model\n\\((M,w)\\), we have: \n\n \\[ \\textstyle M,w\\models B_a^GF \\quad\\text{iff}\\quad \\min_a(\\sem{G}_M\\cap\\cc_a(w))\\subseteq\\sem{F}_M ; \\] \n\n that is, agent a believes\nF conditional on G at world w if and only if\nF is true at the most plausible G-worlds that are\nconsistent with a’s information.  \nThis theorem tell us that to see what an agent believes conditional on\nG, all we need to do is look at the agent’s most\nplausible G-worlds. In this way, conditional belief has the\nagent “recenter” her system of spheres over the set of all\nworlds at which G is true. Conditional belief thereby\nimplements static belief revision: to see what agent a believes\nafter statically revising her beliefs by G we simply see what\nit is she believes conditional on G. Thus \\(B_a^GF\\) says that\nagent a believes F after statically revising her beliefs\nby G.  \nThe notion of conditional belief allows us to connect the notions\nknowledge possession \\(K_a\\) and defeasible knowledge \\(\\Box_a\\) with\nthe defeasibility analysis of knowledge, as indicated by the following\nresult.  \nTheorem. For each pointed plausibility model\n\\((M,w)\\), we have each of the following. \nConditional belief gives rise to a notion of unconditional\nbelief obtained by taking the trivial condition \\(\\top\\) (i.e.,\nthe propositional constant for truth) as the condition:  \nSo to see what the agent believes unconditionally, we simply\nconditionalize her beliefs on the trivial condition \\(\\top\\), which is\ntrue everywhere. It is then easy to see that we have the following.\n \nTheorem. For each pointed plausibility model\n\\((M,w)\\), we have: \n\n\\[\n\\textstyle\nM,w\\models B_aF \\quad\\text{iff}\\quad \\min_a(\\cc_a(w))\\subseteq\\sem{F}_M;\n\\]\n\n that is, agent a believes\nF (unconditionally) at world w if and only if F\nis true at the most plausible worlds that are consistent with\na’s information.  \nWe conclude this section with the axiomatic theory characterizing\nthose formulas that are valid in all plausibility models. Since we can\nexpress conditional belief (and since conditional belief describes\nstatic belief revision), what we obtain is a theory of defeasible\nknowledge, possession of information, conditional belief,\nunconditional belief, and static belief revision.  \nThe axiomatic theory \\(\\KBox\\). \n\\(\\KBox\\) Soundness and Completeness. \\(\\KBox\\) is\nsound and complete with respect to the collection \\(\\sC_*\\) of pointed\nplausibility models. That is, for each \\eqref{KBox}-formula F,\nwe have that \\(\\KBox\\vdash F\\) if and only if \\(\\sC_*\\models F\\). \nInstead of taking information possession \\(K_a\\) and defeasible\nknowledge \\(\\Box_a\\) as the basic propositional attitudes, one may\ninstead choose conditional belief statements \\(B_a^GF\\). This choice\ngives the theory \\(\\CDL\\) of Conditional Doxastic Logic. See\n Appendix J\n for details.  \nWe may define a number of additional propositional attitudes beyond\nconditional belief \\(B_a^GF\\), defeasible knowledge \\(\\Box_aF\\), and\ninformation possession \\(K_aF\\). We take a brief look at a two of\nthese that have important connections with the Belief Revision\nliterature.  \nThe theories and operators we have seen so far all concern\nstatic belief change. We now wish to turn to dynamic\nbelief change. For this the approach follows the typical pattern in\nDynamic Epistemic Logic: we take a given static theory (in this case\n\\(\\KBox\\)) and we add action model-style modalities to create the\ndynamic theory. When we did this before in the case of basic\nmulti-modal epistemic and doxastic logic, the relational structure of\nthe added action models matched the relational structure of the models\nof the theory—Kripke models. The structural match between action\nmodels and finite Kripke models is not accidental: the semantics of\naction model modalities (as explained by the\n BMS product update)\n uses the same Kripke model-based notion of agent uncertainty over\nobjects (i.e., the “worlds”) to describe agent uncertainty\nover action model objects (i.e., the “events”). Both\nuncertainties are represented using the same kind of structure: the\nbinary possibility relation \\(R_a\\).  \nFor the present theory of conditional belief \\(B_a^FG\\), defeasible\nknowledge \\(\\Box_aF\\), and information possession \\(K_aF\\), we take a\nsimilar approach: we define plausibility action models, which\nare action model-type objects whose relational structure matches the\nrelational structure of the models of this theory—plausibility\nmodels. Since a finite plausibility model has the form \\((W,\\geq,V)\\),\nour intuition from the Kripke model case suggests that plausibility\naction models should have the form \\((E,\\geq,\\pre)\\), with E a\nfinite nonempty set of events, \\(\\geq\\) a function giving a\nplausibility relation \\(\\geq_a\\) for each agent a, and \\(\\pre\\)\na precondition function as before.  \nPlausibility action model. Given a set of formulas\n\\(\\Lang\\) and a finite nonempty set \\(\\sA\\) of agents, a\nplausibility action model is a structure \n\n\\[ A=(E,\\geq,\\pre) \\]\n\nconsisting of \nA pointed plausibility action model, sometimes also called an\naction, is a pair \\((A,e)\\) consisting of a plausibility\naction model A and an event e in A that is called\nthe point. In drawing plausibility action models, events are\ndrawn as rectangles, a point (if any) is indicated with a double\nrectangle, and arrows are drawn using dashes (as for plausibility\nmodels). We use many of the same drawing and terminological\nconventions for plausibility action models that we use for (pointed)\nplausibility models. \nAs expected, the main difference between plausibility action models\nand basic action models is that the agent-specific component (i.e.,\nthe function \\(\\geq\\) giving the agent-specific relation \\(\\geq_a\\)).\nIn constructing new plausibility models based on plausibility action\nmodels, we may follow a construction similar to the\n product update.\n To make this work, our main task is to describe how the plausibility\nrelation \\(\\geq_a\\) in the resultant plausibility model \\(M[A]\\) is to\nbe determined in terms of the plausibility relations coming from the\ngiven initial plausibility model M and the plausibility action\nmodel A. For this it will be helpful to consider an example.\n \nFigure 9: The pointed plausibility\naction model \\((\\rPub(q),e)\\) for the revisable public announcement of\nq (also called the “lexicographic upgrade by\nq” by van Benthem 2007).  \nFigure 9 depicts \\((\\rPub(q),e)\\), a pointed plausibility action model\nconsisting of two events: the event f in which \\(\\lnot q\\) is\nannounced and the event e in which q is announced. Event\ne is the event that actually occurs. For each agent a\n(coming from the full set of agents \\(\\sA\\)), event e is\nstrictly more plausible. We adopt the\n same drawing conventions\n for plausibility action models that we did for plausibility models:\none- and two-way arrows, reflexive and transitive closures, and the\nrequirement of local connectedness. (Well-foundedness follows because\nthe set E of events is finite.) Accordingly, Figure 9\nimplicitly contains reflexive dashed arrows for each agent at each\nevent.  \n\\((\\rPub(q),e)\\) has the following intuitive effect: the public\nannouncement of q (i.e., event e) occurs and this is\ncommon knowledge; however, the agents still maintain the possibility\nthat the negation \\(\\lnot q\\) was announced (i.e., event f\noccurred). In effect, the agents will come to believe q\n(because the announcement of this was most plausible), but they will\nnevertheless maintain the less plausible possibility that q is\nfalse. This allows the agents to accept the announced formula q\nbut with some caution: they can still revise their beliefs if they\nlater learn that q is false.  \nThe “action-priority update” is the analog of the product\nupdate for plausibility models.  \nAction-priority update (Baltag and Smets 2008b). Let\n\\((M,w)\\) be a pointed plausibility model and \\((A,e)\\) be a pointed\nplausibility action model. Let \\(\\models\\) be a binary satisfaction\nrelation defined between \\((M,w)\\) and formulas in the language\n\\(\\Lang\\) of the precondition function \\(\\pre^A:E^A\\to\\Lang\\) of the\nplausibility action model A. If \\(M,w\\models\\pre^A(e)\\), then\nthe plausibility model \n\n\\[ M[A]=(W[A],{\\geq}[A],V[A]) \\]\n\n is defined via the\naction-priority update operation \\(M\\mapsto M[A]\\) given as\nfollows: \nAn action \\((A,e)\\) operates on an initial situation \\((M,w)\\)\nsatisfying \\(M,w\\models\\pre^A(e)\\) via the action-priority update to\nproduce the resultant situation \\((M[A],(w,e))\\). Note that we may\nwrite the plausibility relation \\(\\mathrel{{\\geq}[A]_a}\\) for agent\na after the action-priority update by A simply as\n\\(\\geq_a\\) when the meaning is clear from context. \nWe now turn to Action-Priority Update Logic (a.k.a., the\nLogic of Doxastic Actions). To begin, we define the language\n\\eqref{APUL} of Action-Priority Update Logic along with the set\n\\(\\PAM_*\\) of pointed plausibility action models having preconditions\nin the language \\eqref{APUL} according to the following recursive\ngrammar:  \nThe satisfaction relation \\(\\models\\) between pointed plausibility\nmodels and formulas of \\eqref{APUL} is the smallest extension of the\nabove-defined satisfaction relation \\(\\models\\) for \\eqref{KBox}\nsatisfying the following:  \nIn addition to the revisable public announcement\n (Figure 9),\n there are a number of interesting pointed plausibility action models.\n \nFigure 11: The pointed plausibility\naction model \\((\\rPri_G(q),e)\\) for the private announcement of\nq to the group of agents G.  \nFigure 11 depicts the private announcement of q to a group of\nagents G. This consists of two events: the event e in\nwhich q is announced and the event f in which the\npropositional constant for truth \\(\\top\\) is announced. For agents\noutside the group G, the most plausible event is the one in\nwhich the \\(\\top\\) is announced; for agents in the group, the most\nplausible event is the one in which q is announced. In reality,\nthe announcement of q (i.e., event e) occurs. Since the\npropositional constant for truth \\(\\top\\) is uninformative, the agents\noutside of G will come to believe that the situation is as it\nwas before. The agents inside G, however, will come to believe\nq.  \nThe plausibility action model version of the private announcement\n(Figure 11) is almost identical to the action model version of the\nprivate announcement\n (Figure 3).\n This is because action models are easily converted into plausibility\naction models: simply change the arrows to dashed arrows. In this way,\nwe readily obtain plausibility action models from our existing action\nmodels. In particular, we can obtain plausibility actions for a public\nannouncement by converting\n Figure 4,\n for a semi-private announcement by converting\n Figure 5,\n and for a misleading private announcement by converting\n Figure 6.\n  \nFinally, van Benthem (2007) studied two important operations on\nmulti-agent plausibility models that are representable using the\naction-priority update.  \nWe now study the axiomatic theory of Action-Priority Update Logic.\n \nThe axiomatic theory \\(\\APUL\\). \nThe first three reduction axioms are identical to the corresponding\nreduction axioms for \\(\\EAL\\). The fourth \\(\\APUL\\) reduction axiom is\nalmost identical to the fourth \\(\\EAL\\) reduction axiom. In\nparticular, the fourth \\(\\EAL\\) reduction axiom, which reads  \ndiffers only in the conjunction on the right-hand side: the \\(\\EAL\\)\naxiom has its conjunction over events related to e via the\nKripke model-style relation \\(R_a\\), whereas the \\(\\APUL\\) axiom has\nits conjunction over events related to e via the plausibility\nmodel-style relation \\(\\simeq_a\\).  \nThe fifth \\(\\APUL\\) reduction axiom is new. This axiom captures the\nessence of the action-priority update: for an agent to have defeasible\nknowledge after an action, she must have information about what\nhappens as a result of more plausible actions and, further, she must\nhave defeasible knowledge about the outcome of equi-plausible actions.\nThe reason for this follows from the definition of the resulting\nplausibility relation \\({\\geq}[A]_a\\). As a reminder, this is defined\nby setting \\((v_1,f_1)\\mathrel{{\\geq}[A]_a}(v_2,f_2)\\) if and only if\nwe have one of the following:  \nLooking to the fifth \\(\\APUL\\) reduction axiom, the conjunct\n\\(\\bigwedge_{e\\gt_a f}K_a[A,f]G\\) says that G is true whenever\nan event of plausibility strictly greater than e is applied to\na world within a’s current connected component. This\ntells us that G is true at worlds having greater plausibility\nin light of the first bulleted item above. The other conjunct\n\\(\\bigwedge_{e\\simeq_a f}\\Box_a[A,f]G)\\) of the fifth \\(\\APUL\\)\nreduction axiom says that G is true whenever an event\nequi-plausible with e is applied to world of equal or greater\nplausibility within a’s current connected component. This\ntells us that G is true at worlds having greater or equal\nplausibility in light of the second bulleted item above. Taken\ntogether, since these two bulleted items define when it is that a\nworld has equal or greater plausibility in the resultant model\n\\(M[A]\\), the truth of these two conjuncts at an initial situation\n\\((M,w)\\) at which \\((A,e)\\) is executable implies that G is\ntrue at all worlds of equal or greater plausibility than the actual\nworld \\((w,e)\\) of the resultant model \\(M[A]\\). That is, we have\n\\(M[A],(w,e)\\models\\Box_a G\\) and therefore that \\(M,w\\models[A,e]G\\).\nThis explains the right-to-left direction of the fifth \\(\\APUL\\)\nreduction axiom. The left-to-right direction is explained similarly.\n \nAs was the case for \\(\\EAL\\), the \\(\\APUL\\) reduction axioms allow us\nto “reduce” each formula containing plausibility action\nmodels to a provably equivalent formula whose plausibility action\nmodel modalities appear before formulas of lesser complexity, allowing\nus to eliminate plausibility action model modalities completely via a\nsequence of provable equivalences. As a consequence, we have the\nfollowing.  \nAPUL Reduction Theorem. Every F in the\nlanguage \\eqref{APUL} is \\(\\APUL\\)-provably equivalent to a formula\n\\(F^\\circ\\) coming from the plausibility action model-free modal\nlanguage \\eqref{KBox}. \nOnce we have proved \\(\\APUL\\) is sound, the Reduction Theorem leads us\nto axiomatic completeness via the known completeness of the underlying\nmodal theory \\(\\KBox\\).  \n\\(\\APUL\\) Soundness and Completeness. \\(\\APUL\\) is\nsound and complete with respect to the collection \\(\\sC_*\\) of pointed\nplausibility action models. That is, for each \\eqref{APUL}-formula\nF, we have that \\(\\APUL\\vdash F\\) if and only if \\(\\sC_*\\models\nF\\). \nAs is the case for \\(\\EAL\\), it is possible to combine two consecutive\nactions into a single action. All that is required is an appropriate\nnotion of plausibility action model composition.  \nPlausibility action model composition. The\ncomposition \\(A\\circ B=(E,\\geq,\\pre)\\) of plausibility action\nmodels \\(A=(E^A,\\geq^A,\\pre^A)\\) and \\(B=(E^B,\\geq^B,\\pre^B)\\) is\ndefined as follows: \nComposition Theorem. Each instance of the following\nschemes is \\(\\APUL\\)-derivable. \nIt is also possible to add valuation-changing substitutions (i.e.,\n“factual changes”) to plausibility action models. This is\ndone exactly as it is done for action models proper: substitutions are\nadded to plausibility action models, the action-priority update is\nmodified to account for substitutions in the semantics, and the first\nreduction axiom is changed to account for substitutions in the\naxiomatics. See\n Appendix G\n for details.  \nOne development in DEL is work aimed toward building logics of\nevidence, belief, and knowledge for use in Formal Epistemology.  \nWe refer the reader to\n Appendix K\n for further details.  \nDynamic Epistemic Logics that incorporate probability have been\nstudied by a number of authors. Van Benthem (2003), Kooi (2003),\nBaltag and Smets (2008a), and van Benthem, Gerbrandy, and Kooi (2009b)\nstudied logics of finite probability spaces. Sack (2009) extended the\nwork of Kooi (2003) and van Benthem, Gerbrandy, and Kooi (2009b) to\nfull probability spaces (based on σ-algebras of events). Of\nthese, we mention two in particular:  \nWe refer the reader to\n Appendix L\n for further details.  \nDEL-style model-changing operators have been applied by a number of\nresearchers to the study of preferences, preference change, and\nrelated notions. We refer the reader to\n Appendix M\n for further information, which mentions the work of van Benthem et\nal. (2009), van Benthem and Liu (2007), Liu (2008), Yamada (2007a,b,\n2008), van Eijck (2008), van Eijck and Sietsma (2010), van Benthem,\nGirard, and Roy (2009c), and Liu (2011).  \nAction model-style modalities \\([A,e]\\) of Dynamic Epistemic Logic\nhave a temporally suggestive reading: “after action\n\\((A,e)\\), formula F is true”. This\n“before-after” reading suggests, naturally enough, that\ntime passes as actions occur. The semantics of action models supports\nthis suggestion: determining the truth of an action model formula\n\\([A,e]F\\) in a model—the model “before” the\naction—requires us to apply the model-transforming operation\ninduced by the action \\((A,e)\\) and then see whether F holds in\nthe model that results “after” the action. Channeling\nParikh and Ramanujam (2003) some DEL authors further this suggestion\nby using the temporally charged word “history” to refer to\na sequence of pointed Kripke models brought about by the occurrence of\na sequence of model-transforming operations. All of this seems to\npoint to the existence of a direct relationship between the occurrence\nof model-transforming actions and the passage of time: time passes as\nthese actions occur. However, the formal languages introduced so far\ndo not have a built-in means for directly expressing the passage of\ntime, and so, as a consequence, the axiomatic theories developed above\nare silent on the relationship between the flow of time and the\noccurrence of model-changing actions. This leaves open the possibility\nthat, within the context of these theories, the passage of time and\nthe occurrence of actions need not necessarily relate as we might\notherwise suspect.  \nFor more on this, we refer the interested reader to\n Appendix N,\n which mentions a number of studies that bring some method of\ntime-keeping within the scope of the Dynamic Epistemic Logic approach:\nthe work of Sack (2007, 2008, 2010), Yap (2006, 2011), Hoshi (2009),\nHoshi and Yap (2009), van Benthem, Gerbrandy, and Pacuit (2007), van\nBenthem et al. (2009a), Dégremont, Löwe, and Witzel\n(2011), and Renne, Sack, and Yap (2009, 2015).  \nA number of works utilize tools and techniques from Dynamic Epistemic\nLogic for formal reasoning on topics in mainstream Epistemology.  \nWe have surveyed the literature of Dynamic Epistemic Logic, from its\nearly development in the Public Announcement Logic to the generalized\ncommunication operations of action models, work on qualitative and\nquantitative belief revision, and applications in a variety of areas.\nDynamic Epistemic Logic is an active and expanding area, and we have\nhighlighted a number of open problems and directions for further\nresearch. ","contact.mail":"bryan@renne.org","contact.domain":"renne.org"}]
