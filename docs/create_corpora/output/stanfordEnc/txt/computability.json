[{"date.published":"2004-06-24","date.changed":"2015-09-02","url":"https://plato.stanford.edu/entries/computability/","author1":"Neil Immerman","author1.info":"http://www.cs.umass.edu/~immerman/","entry":"computability","body.text":"\n\n\n\nA mathematical problem is computable if it can be solved in\nprinciple by a computing device. Some common synonyms for\n“computable” are “solvable”,\n“decidable”, and “recursive”. Hilbert believed\nthat all mathematical problems were solvable, but in the 1930’s\nGödel, Turing, and Church showed that this is not the case. There\nis an extensive study and classification of which mathematical\nproblems are computable and which are not. In addition, there is an\nextensive classification of computable problems into computational\ncomplexity classes according to how much computation—as a\nfunction of the size of the problem instance—is needed to\nanswer that instance. It is striking how clearly, elegantly, and\nprecisely these classifications have been drawn. \n\n\n\nIn the 1930’s, well before there were computers, various\nmathematicians from around the world invented precise, independent\ndefinitions of what it means to be computable. Alonzo Church defined\nthe Lambda calculus, Kurt Gödel defined Recursive functions,\nStephen Kleene defined Formal systems, Markov defined what became known\nas Markov algorithms, Emil Post and Alan Turing defined abstract\nmachines now known as Post machines and Turing machines. \n\nSurprisingly, all of these models are exactly equivalent: anything\ncomputable in the lambda calculus is computable by a Turing machine and\nsimilarly for any other pairs of the above computational systems. After\nthis was proved, Church expressed the belief that the intuitive notion\nof “computable in principle” is identical to the above precise notions.\nThis belief, now called the “Church-Turing Thesis”, is uniformly\naccepted by mathematicians. \n\nPart of the impetus for the drive to codify what is computable came\nfrom the mathematician David Hilbert. Hilbert believed that all of\nmathematics could be precisely axiomatized. He felt that once this was\ndone, there would be an “effective procedure”, i.e., an algorithm that\nwould take as input any precise mathematical statement, and, after a\nfinite number of steps, decide whether the statement was true or false.\nHilbert was asking for what would now be called a decision\nprocedure for all of mathematics. \n\nAs a special case of this decision problem, Hilbert considered the\nvalidity problem for first-order logic. First-order logic is a\nmathematical language in which most mathematical statements can be\nformulated. Every statement in first-order logic has a precise meaning\nin every appropriate logical structure, i.e., it is true or false in\neach such structure. Those statements that are true in every\nappropriate structure are called valid. Those statements that\nare true in some structure are called satisfiable. Notice that\na formula, \\(\\varphi\\), is valid iff its negation, \\(\\neg \\varphi\\), is not\nsatisfiable. \n\nHilbert called the validity problem for first-order logic, the\nentscheidungsproblem. In a textbook, Principles of\nMathematical Logic by Hilbert and Ackermann, the authors wrote,\n“The Entscheidungsproblem is solved when we know a procedure that\nallows for any given logical expression to decide by finitely many\noperations its validity or satisfiability.… The\nentscheidungsproblem must be considered the main problem of\nmathematical logic.” (Böerger, Grädel, & Gurevich\n1997). \n\nIn his 1930 Ph.D. thesis, Gödel presented a complete\naxiomatization of first-order logic, based on the Principia\nMathematica by Whitehead and Russell (Gödel 1930). Gödel proved his\nCompleteness Theorem, namely that a formula is provable from the axioms\nif and only if it is valid. Gödel’s Completeness theorem was a\nstep towards the resolution of Hilbert’s\nentscheidungsproblem. \n\nIn particular, since the axioms are easily recognizable, and rules\nof inference very simple, there is a mechanical procedure that can list\nout all proofs. Note that each line in a proof is either an axiom, or\nfollows from previous lines by one of the simple rules. For any given\nstring of characters, we can tell if it is a proof. Thus we can\nsystematically list all strings of characters of length 1, 2, 3, and so\non, and check whether each of these is a proof. If so, then we can add\nthe proof’s last line to our list of theorems. In this way, we can list\nout all theorems, i.e., exactly all the valid formulas of first-order\nlogic, can be listed out by a simple mechanical procedure. More\nprecisely, the set of valid formulas is the range of a computable\nfunction. In modern terminology we say that the set of valid formulas\nof first-order logic is recursively enumerable (r.e.). \n\nGödel’s Completeness theorem was not sufficient, however, to\ngive a positive solution to the entscheidungsproblem. Given a\nformula, \\(\\varphi\\), if \\(\\varphi\\) is valid then the above procedure would\neventually list it out and thus could answer, “Yes, \\(\\varphi\\) is valid.”\nHowever, if \\(\\varphi\\) were not valid then we might never find this fact\nout. What was missing was a procedure to list out all the non-valid\nformulas, or equivalently to list out all satisfiable formulas. \n\nA year later, in 1931, Gödel shocked the mathematical world by\nproving his Incompleteness Theorem: there is no complete and computable\naxiomatization of the first-order theory of the natural numbers. That\nis, there is no reasonable list of axioms from which we can prove\nexactly all true statements of number theory (Gödel 1931). \n\nA few years later, Church and Turing independently proved that the\nentscheidungsproblem is unsolvable. Church did this by using\nthe methods of Gödel’s Incompleteness Theorem to show that the set\nof satisfiable formulas of first-order logic is not r.e., i.e., they\ncannot be systematically listed out by a function computable by the\nlambda calculus. Turing introduced his machines and proved many\ninteresting theorems some of which we will discuss in the next section.\nIn particular, he proved the unsolvability of the halting\nproblem. He obtained the unsolvability of the\nentscheidungsproblem as a corollary. \n\nHilbert was very disappointed because his program towards a decision\nprocedure for all of mathematics was proved impossible. However, as we\nwill see in more detail in the rest of this article, a vast amount was\nlearned about the fundamental nature of computation. \n\nIn his 1936 paper, “On Computable Numbers, with an Application to\nthe Entscheidungsproblem”, Alan Turing introduced his machines and\nestablished their basic properties. \n\nHe thought clearly and abstractly about what it would mean for a\nmachine to perform a computational task. Turing defined his machines to\nconsist of the following: \n\nThe linear nature of its memory tape, as opposed to random access\nmemory, is a limitation on computation speed but not power: a Turing\nmachine can find any memory location, i.e., tape cell, but this may be\ntime consuming because it has to move its head step by step along its\ntape. \n\nThe beauty of Turing machines is that the model is extremely simple,\nyet nonetheless, extremely powerful. A Turing machine has potentially\ninfinite work space so that it can process arbitrarily large inputs,\ne.g., multiply two huge numbers, but it can only read or write a\nbounded amount of information, i.e., one symbol, per step. Even before\nTuring machines and all the other mathematical models of computation\nwere proved equivalent, and before any statement of the Church-Turing\nthesis, Turing argued convincingly that his machines were as powerful\nas any possible computing device. \n\nEach Turing machine can be uniquely described by its transition\ntable: for each state, \\(q\\), and each symbol, \\(\\sigma ,\n\\delta(q,\\sigma)\\) is the new state, the new symbol, and the\nhead displacement. These transition tables, can be written as a finite\nstring of symbols, giving the complete set of instructions of each\nTuring machine. Furthermore, these strings of symbols can be listed in\nlexicographic order as follows: \\(M_{1},\nM_{2}, M_{3},\\ldots\\), where\n\\(M_{i}\\) is the transition table, i.e., the complete set\nof instructions, for Turing machine number \\(i\\). The transition\ntable for \\(M_{i}\\) is the program for Turing machine\n\\(i\\), or more simply, the \\(i\\)th program. \n\nTuring showed that he could build a Turing machine, \\(U\\), that\nwas universal, in the sense that it could run the program of\nany other Turing machine. More explicitly, for any \\(i\\), and any\ninput \\(w, U\\) on inputs \\(i\\) and \\(w\\) would\ndo exactly what \\(M_{i}\\) would do on input \\(w\\), in\nsymbols, \n\nTuring’s construction of a universal machine gives the most\nfundamental insight into computation: one machine can run any program\nwhatsoever. No matter what computational tasks we may need to perform\nin the future, a single machine can perform them all. This is the\ninsight that makes it feasible to build and sell computers. One\ncomputer can run any program. We don’t need to buy a new computer every\ntime we have a new problem to solve. Of course, in the age of personal\ncomputers, this fact is such a basic assumption that it may be\ndifficult to step back and appreciate it. \n\nBecause they were designed to embody all possible computations,\nTuring machines have an inescapable flaw: some Turing machines on\ncertain inputs never halt. Some Turing machines do not halt for silly\nreasons, for example, we can mis-program a Turing machine so that it\ngets into a tight loop, for example, in state 17 looking at a 1 it\nmight go to state 17, write a 1 and displace its head by 0. Slightly\nless silly, we can reach a blank symbol, having only blank symbols to\nthe right, and yet keep staying in the same state, moving one step to\nthe right, and looking for a “1”. Both of those cases of non-halting\ncould be easily detected and repaired by a decent compiler. However,\nconsider the Turing machine \\(M_{F}\\), which on\ninput “0”, systematically searches for the first counter-example to\nFermat’s last theorem, and upon finding it outputs the counter-example\nand halts. Until Andrew Wiles relatively recently proved Fermat’s Last\nTheorem, all the mathematicians in the world, working for over three\ncenturies, were unable to decide whether or not\n\\(M_{F}\\) on input “0” eventually halts. Now we\nknow that it never does. \n\nSince a Turing machine might not halt on certain inputs, we have to\nbe careful in how we define functions computable by Turing machines.\nLet the natural numbers, \\(\\mathbf{N}\\), be the set\n\\(\\{0,1,2,\\ldots \\}\\) and let us consider Turing machines as partial\nfunctions from \\(\\mathbf{N}\\) to \\(\\mathbf{N}\\). \n\nLet \\(M\\) be a Turing machine and \\(n\\) a natural number.\nWe say that \\(M\\)’s tape contains the number\n\\(n\\), if \\(M\\)’s tape begins with a binary\nrepresentation of the number \\(n\\) (with no unnecessary leading\n0’s) followed by just blank symbols from there on. \n\nIf we start the Turing machine \\(M\\) on a tape containing\n\\(n\\) and it eventually halts with its tape containing \\(m\\),\nthen we say that \\(M\\) on input \\(n\\), computes \\(m:\nM(n) = m\\). If, when we start \\(M\\) on\ninput \\(n\\), it either never halts, or when it halts, its tape\ndoes not contain a natural number, e.g., because it has leading 0’s, or\ndigits interspersed with blank symbols, then we say that\n\\(M(n)\\) is undefined, in symbols: \\(M(n)\n=\\nearrow\\).\n We can thus\nassociate with each Turing machine, \\(M\\), a partial\nfunction, \\(M: \\mathbf{N} \\rightarrow \\mathbf{N} \\cup \\{\\nearrow\\}\\). We say that\nthe function \\(M\\) is total if for all\n\\(n\\in \\mathbf{N}\\), \\(M(n) \\in \\mathbf{N}\\), i.e., M\\((n)\\) is always defined. \n\nNow we can formally define what it means for a set to be\nrecursively enumerable (r.e.) which we earlier described\ninformally. Let \\(S \\subseteq \\mathbf{N}\\). Then \\(S\\)\nis r.e. if and only if there is some Turing machine, \\(M\\), such\nthat \\(S\\) is the image of the function computed by \\(M\\), in\nsymbols, \n\nThus, \\(S\\) is r.e. just if it can be listed out by some Turing\nmachine. Suppose that \\(S\\) is r.e. and its elements are\nenumerated by Turing machine \\(M\\) as above. We can then describe\nanother Turing machine, \\(P\\), which, on input \\(n\\), runs\n\\(M\\) in a round-robin fashion on all its possible inputs until\neventually \\(M\\) outputs \\(n\\). If this happens then\n\\(P\\) halts and outputs “1”, i.e., \\(P(n)=1\\). If\n\\(n \\not\\in S\\), then\n\\(M\\) will never output \\(n\\), so \\(P(n)\\) will\nnever halt, i.e., \\(P(n)=\\nearrow\\).\n \n\nLet the notation \\(P(n)\\downarrow\\) mean that Turing\nmachine \\(P\\) on input \\(n\\) eventually halts. For a Turing\nmachine, \\(P\\), define \\(L(P)\\), the set\naccepted by \\(P\\), to be those numbers \\(n\\) such that\n\\(P\\) on input \\(n\\) eventually halts, \n\nThe above argument shows that if a set \\(S\\) is r.e. then it is\naccepted by some Turing machine, \\(P\\), i.e., \\(S = L(P)\\). The\nconverse of this statement holds as well.  That is, \\(S\\) is r.e. if\nand only if it is accepted by some Turing machine, \\(P\\). \n\nWe say that a set, \\(S\\), is decidable if and only if there\nis a total Turing machine, \\(M\\), that decides for all \\(n \\in\n\\mathbf{N}\\) whether or not \\(n \\in S\\). Think of “1” as\n“yes” and “0” as “no”. For all\n\\(n\\in \\mathbf{N}\\), if \\(n \\in S\\), then \\(M(n) = 1\\), i.e., \\(M\\) on\ninput \\(n\\) eventually halts and outputs “yes”, whereas if\n\\(n \\not\\in S\\), then \\(M(n) = 0\\), i.e., \\(M\\) on input \\(n\\)\neventually halts and outputs “no”. Synonyms for decidable\nare:\ncomputable, solvable, and recursive. \n\nFor \\(S \\subseteq \\mathbf{N}\\), the complement of\n\\(S\\) is \\(\\mathbf{N} - S\\), i.e., the set of all\nnatural numbers not in \\(S\\). We say that the set \\(S\\) is\nco-r.e. if and only if its complement is r.e. If a set,\n\\(S\\), is r.e. and co-r.e. then we can list out all of its\nelements in one column and we can list out all of its non-elements in a\nsecond column. In this way we can decide whether or not a given\nelement, \\(n\\), is in \\(S\\): just scan the two columns and\nwait for \\(n\\) to show up. If it shows up in the first column then\n\\(n \\in S\\). Otherwise it will show up in the second\ncolumn and \\(n \\not\\in S\\).\nIn fact, a set is recursive iff it is r.e. and co-r.e. \n\nTuring asked whether every set of natural numbers is decidable. It\nis easy to see that the answer is, “no”, by the following counting\nargument. There are uncountably many subsets of \\(\\mathbf{N}\\), but\nsince there are only countably many Turing machines, there can be only\ncountably many decidable sets. Thus almost all sets are\nundecidable. \n\nTuring actually constructed a non-decidable set. As we will see, he\ndid this using a diagonal argument. The diagonal argument goes back to\nGeorg Cantor who used it to show that the real numbers are uncountable.\nGödel used a similar diagonal argument in his proof of the\nIncompleteness Theorem in which he constructed a sentence, \\(J\\),\nin number theory whose meaning could be understood to be, “\\(J\\)\nis not a theorem.” \n\nTuring constructed a diagonal halting set, \\(K\\), as\nfollows: \n\nThat is, \\(K\\) consists of those Turing machines that\neventually halt when input their own programs. \n\nIt is not hard to see that \\(K\\) is r.e. Suppose for the sake\nof a contradiction that \\(K\\) is also co-r.e., and let \\(d\\)\nbe the number of a Turing machine that accepts the complement of\n\\(K\\). That is, for any \\(n\\), \n\nBut consider what happens when we substitute \\(d\\) for\n\\(n\\) in the above equation: \n\nHowever, the definition of \\(K\\) tells us that: \n\nThus we have that \n\nwhich is a contradiction. Thus our assumption that \\(K\\) is\nco-r.e. is false. Thus \\(K\\) is not recursive. It follows that it\nis not a computable problem to be given a Turing machine and its input\nand to decide whether or not the Turing machine will eventually halt on\nthat input, i.e., the halting problem is unsolvable. \n\nWe next define the class of Primitive Recursive Functions.\nThis is a very interesting class of functions \ndescribed in paper by Skolem (1923) and used by\nGödel in\nhis proof of the Incompleteness Theorem. We are interested in\nfunctions \\(f\\) from \\(\\mathbf{N}^{r}\\) to\n\\(\\mathbf{N}\\), for \\(r = 0, 1, 2,\\ldots\\) . Here\n\\(r\\) is called the arity of the function \\(f\\), i.e., the\nnumber of arguments that it takes. Gödel started with three very\nsimple functions, the initial functions, and two natural closure\noperations, composition and primitive recursion, each of which take\nsome already defined functions and use them to define a new one. We\nnext explain his definitions in detail. This section is technical and\ncan be safely skipped. The important idea is that the primitive\nrecursive functions comprise a very large and powerful class of\ncomputable functions, all generated in an extremely simple way. \n\nWe begin with the three initial primitive recursive\nfunctions: \n\nNow consider the following two operations: \n\nwhere each \\(w_{i}\\) is a list of \\(r_{i}\\) arguments, perhaps with\nrepetition, from \\(x_{1}, \\ldots ,x_{k}\\); and, \n\nHere composition is the natural way to combine functions, and\nprimitive recursion is a restricted kind of recursion in which\n\\(h\\) with first argument \\(n+1\\) is defined in terms of\n\\(h\\) with first argument \\(n\\), and all the other arguments\nunchanged.  \n\nDefine the primitive recursive functions to be the smallest\nclass of functions that contains the Initial functions and is closed\nunder Composition and Primitive Recursion. The set of primitive\nrecursive functions is equal to the set of functions computed using\nbounded iteration (Meyer & Ritchie 1967), i.e. the set of\nfunctions definable in the language Bloop from (Hofstadter 1979). \n\nThe primitive recursive functions have a very simple definition and\nyet they are extremely powerful. Gödel proved inductively that\nevery primitive recursive function can be simply represented in\nfirst-order number theory. He then used the primitive recursive\nfunctions to encode formulas and even sequences of formulas by numbers.\nHe finally used the primitive recursive functions to compute properties\nof the represented formulas including that a formula was well formed, a\nsequence of formulas was a proof, and that a formula was a theorem. \n\nIt takes a long series of lemmas to show how powerful the primitive\nrecursive functions are. The following are a few examples showing that\naddition, multiplication, and exponentiation are primitive\nrecursive. \n\nDefine the addition function, \\(P(x,y)\\), as\nfollows: \n\n(Note that this fits into the definition of primitive recursion\nbecause the function \\(g(x_{1},x_{2},x_{3}) = \\eta(\\sigma(x_{1}))\\) is\ndefinable from the initial functions \\(\\eta\\) and \\(\\sigma\\) by\ncomposition.) \n\nNext, define the multiplication function,\n\\(T(x,y)\\), as follows: \n\nNext, we define the exponential function,\n\\(E(x,y)\\). (Usually \\(0^{0}\\) is\nconsidered undefined, but since primitive recursive functions must be\ntotal, we define \\(E\\)(0,0) to be 1.) Since\nprimitive recursion only allows us to recurse on the first argument,\nwe use two steps to define the exponential function: \n\nFinally we can define \\(E(x,y) = R(\\eta(y),\\eta(x))\\) by\ncomposition. (Recall that \\(\\eta\\) is the identity function so this\ncould be more simply written as \\(E(x,y) = R(y,x)\\).) \n\nThe exponential function, \\(E\\), grows very rapidly, for\nexample, \\(E\\)(10,10) is ten billion, and \\(E\\)(50,50) is\nover \\(10^{84}\\) (and thus significantly more than the estimated\nnumber of atoms in the universe). However, there are much faster\ngrowing primitive recursive functions. As we saw, \\(E\\) was\ndefined from the slowly growing function, \\(\\sigma\\), using three\napplications of primitive recursion: one for addition, one for\nmultiplication, and then one more for exponentiation. We can continue\nto apply primitive recursion to build a series of unimaginably fast\ngrowing functions. Let’s do just one more step in the series defining\nthe hyper-exponential function, \\(H(n,m)\\) as 2\nto the 2 to the 2 to the … to the \\(m\\), with a tower of\n\\(n\\) 2s. \\(H\\) is primitive recursive because it can be\ndefined from \\(E\\) using one more application of primitive\nrecursion: \n\nThus \\(H(2,2) = 2^{4} = 16, H(3,3) = 2^{256}\\) is more than\n\\(10^{77}\\) and comparable to the number of atoms in the universe. If\nthat’s not big enough for you then consider \\(H(4,4)\\). To write\nthis number in decimal notation we would need a one, followed by more\nzero’s than the number of particles in the universe. \n\nThe set of primitive recursive functions is a huge class of\ncomputable functions. In fact, they can be characterized as the set of\nfunctions computable in time that is some primitive recursive function\nof \\(n\\), where \\(n\\) is the length of the input. For\nexample, since \\(H(n,n)\\) is a primitive\nrecursive function, the primitive recursive functions include all of\nTIME[\\(H(n,n)\\)]. (See the next section for a\ndiscussion of computational complexity, including TIME.) Thus, the\nprimitive recursive functions include all functions that are feasibly\ncomputable by any conceivable measure of feasible, and much beyond\nthat. \n\nHowever, the primitive recursive functions do not include all\nfunctions computable in principle. To see this, we can again use\ndiagonalization. We can systematically encode all definitions of\nprimitive recursive functions of arity 1, calling them\n\\(p_{1}, p_{2}, p_{3}\\),\nand so on. \n\nWe can then build a Turing machine to compute the value of the\nfollowing diagonal function, \\(D(n) = p_{n}(n) + 1\\). \n\nNotice that \\(D\\) is a total, computable function from\n\\(\\mathbf{N}\\) to \\(\\mathbf{N}\\), but it is not primitive\nrecursive. Why? Suppose for the sake of a contradiction that \\(D\\)\nwere primitive recursive. Then \\(D\\) would be equal to\n\\(p_{d}\\) for some \\(d\\in \\mathbf{N}\\). But it would then follow that \n\nwhich is a contradiction. Therefore, \\(D\\) is not primitive\nrecursive. \n\nAlas, the above diagonal argument works on any class of total\nfunctions that could be considered a candidate for the class of all\ncomputable functions. The only way around this, if we want all\nfunctions computable in principle, not just in practice, is to add some\nkind of unbounded search operation. This is what Gödel did to\nextend the primitive recursive functions to the recursive\nfunctions. \n\nDefine the unbounded minimization operator, \\(\\mu\\), as follows. Let\n\\(f\\) be a perhaps partial function of arity \\(k+1\\). Then\n\\(\\mu[f\\)] is defined as the following function of arity k. On\ninput \\(x_{1}, \\ldots ,x_{k}\\) do the\nfollowing: For \\(i = 0\\) to \\(\\infty\\) do \\(\\{\\) if \\(f(i,x_{1},\\ldots ,x_{k}) = 1\\), then output \\(i\\)\n \\(\\}\\) \n\nThus if \\(f(i,x_{1},\\ldots ,x_{k}) = 1\\), and for all \\(j \\lt i,\nf(j,x_{1},\\ldots ,x_{k})\\) is defined, but not equal to 1, then \\(\\mu\n[f](x_{1}, \\ldots ,x_{k}) = i\\). Otherwise \\(\\mu[f](x_{1}, \\ldots\n,x_{k})\\) is undefined. \n\nGödel defined the set of Recursive functions to be the\nclosure of the initial primitive recursive functions under composition,\nprimitive recursion, and \\(\\mu\\) . With this definition, the Recursive\nfunctions are exactly the same as the set of partial functions\ncomputable by the Lambda calculus, by Kleene Formal systems, by Markov\nalgorithms, by Post machines, and by Turing machines. \n\nDuring World War II, Turing helped design and build a specialized\ncomputing device called the Bombe at Bletchley Park. He used the Bombe\nto crack the German “Enigma” code, greatly aiding the\nAllied cause [Hodges, 1992]. By the 1960’s computers were widely\navailable in industry and at universities. As algorithms were\ndeveloped to solve myriad problems, some mathematicians and scientists\nbegan to classify algorithms according to their efficiency and to\nsearch for best algorithms for certain problems. This was the\nbeginning of the modern theory of computation. \n\nIn this section we are dealing with complexity instead of\ncomputability, and all the Turing machines that we consider will halt\non all their inputs. Rather than accepting by halting, we will assume\nthat a Turing machine accepts by outputting “1” and rejects by\noutputting “0”, thus we redefine the set accepted by a total machine,\n\\(M\\), \n\nThe time that an algorithm takes depends on the input and the\nmachine on which it is run. The first important insight in complexity\ntheory is that a good measure of the complexity of an algorithm is its\nasymptotic worst-case complexity as a function of the size of the\ninput, \\(n\\). \n\nFor an input, \\(w\\), let \\(n = \\lvert w\\rvert\\) be the length of\n\\(w\\). Following [Hartmanis, 1989] we say that a Turing machine\n\\(M\\) runs in time \\(T(n)\\) if for all \\(w\\) of length \\(n,\nM(w)\\) takes at most \\(T(n)\\) steps and then halts. This is called\nworst-case complexity because \\(T(n)\\) must be as large as the time\ntaken by any input of length \\(n\\). \n\nFor any function \\(T : \\mathbf{N} \\rightarrow \\mathbf{N}\\), let \n\nAlan Cobham and Jack Edmonds identified the complexity class, \\(\\P\\), of\nproblems recognizable in some polynomial amount of time, as being an\nexcellent mathematical wrapper of the class of feasible \nproblems—those problems all of whose moderately-sized instances can be\nfeasibly recognized, \n\nAny problem not in \\(\\P\\) is certainly not feasible. On the other hand,\nnatural problems that have algorithms in \\(\\P\\), tend to eventually have\nalgorithms discovered for them that are actually feasible. \n\nMany important complexity classes besides P have been defined and\nstudied; a few of these are \\(\\NP\\), \\(\\PSPACE\\), and \\(\\EXPTIME\\).\n\\(\\PSPACE\\) consists of those problems solvable using some polynomial\namount of memory space. \\(\\EXPTIME\\) is the set of problems solvable in\ntime \\(2^{p(n)}\\) for some polynomial, \\(p\\). \n\nPerhaps the most interesting of the above classes is NP:\nnondeterministic polynomial time. The definition comes not from a real\nmachine, but rather a mathematical abstraction. A nondeterministic\nTuring machine, \\(N\\), makes a choice (guess) of one of two\npossible actions at each step. If, on input \\(w\\), some sequence\nof these choices leads to acceptance, then we say that\n\\(\\mathbf{N}\\) accepts \\(w\\), and we say the\nnondeterministic time taken by \\(\\mathbf{N}\\) on input\n\\(w\\), is just the number of steps taken in the sequence that\nleads to acceptance. A nondeterministic machine is not charged for all\nthe other possible choices it might have made, just the single sequence\nof correct choices. \n\nNP is sometimes described as the set of problems, \\(S\\), that\nhave short proofs of membership. For example, suppose we are given a\nlist of \\(m\\) large natural numbers: \\(a_{1},\n\\ldots ,a_{m}\\), and a target number,\n\\(t\\). This is an instance of the Subset Sum problem: is there a\nsubset of the \\(m\\) numbers whose sum is exactly \\(t\\)? This\nproblem is easy to solve in nondeterministic linear time: for each\n\\(i\\), we guess whether or not to take \\(a_{i}\\).\nNext we add up all the numbers we decided to take and if the sum is\nequal to \\(t\\) then accept. Thus the nondeterministic time is\nlinear, i.e., some constant times the length of the input, \\(n\\).\nHowever there is no known (deterministic) way to solve this problem in\ntime less than exponential in \\(n\\). \n\nThere has been a large study of algorithms and the complexity of\nmany important problems is well understood. In particular reductions\nbetween problems have been defined and used to compare the relative\ndifficulty of two problems. Intuitively, we say that \\(A\\) is\nreducible to \\(B\\)  \\((A \\le B)\\) if there\nis a simple transformation, \\(\\tau\\), that maps instances of \\(A\\) to\ninstances of \\(B\\) in a way that preserves membership, i.e.,\n\\(\\tau\\)(w) \\(\\in B \\Leftrightarrow\\) w \\(\\in A\\). \n\nRemarkably, a high percentage of naturally occurring computational\nproblems turn out to be complete for one of the above classes. (A\nproblem, \\(A\\), is complete for a complexity class \\(C\\) if\n\\(A\\) is a member of \\(C\\) and all other problems \\(B\\)\nin \\(C\\) are no harder than \\(A\\), i.e., \\(B \\le A\\). Two complete problems for the same class have equivalent\ncomplexity.) \n\nThe reason for this completeness phenomenon has not been adequately\nexplained. One plausible explanation is that natural computational\nproblems tend to be universal in the sense of Turing’s universal\nmachine. A universal problem in a certain complexity class can simulate\nany other problem in that class. The reason that the class NP is so\nwell studied is that a large number of important practical problems are\nNP complete, including Subset Sum. None of these problems is known to\nhave an algorithm that is faster than exponential time, although some\nNP-complete problems admit feasible approximations to their\nsolutions. \n\nA great deal remains open about computational complexity. We know that\nstrictly more of a particular computational resource lets us solve\nstrictly harder problems, e.g. \\(\\TIME[n]\\) is strictly contained in\n\\(\\TIME[n^{1.01}]\\) and similarly for \\(\\SPACE\\) and other\nmeasures. However, the trade-offs between different computational\nresources is still quite poorly understood. It is obvious that \\(\\P\\)\nis contained in \\(\\NP\\). Furthermore, \\(\\NP\\) is contained in\n\\(\\PSPACE\\) because in \\(\\PSPACE\\) we can systematically try every\nsingle branch of an \\(\\NP\\) computation, reusing space for the\nsuccessive branches, and accepting if any of these branches lead to\nacceptance. \\(\\PSPACE\\) is contained in \\(\\EXPTIME\\) because if a\n\\(\\PSPACE\\) machine takes more than exponential time, then it has\nexactly repeated some configuration so it must be in an infinite\nloop. The following are the known relationships between the above\nclasses: \n\nHowever, while it seems clear that \\(\\P\\) is strictly contained in\n\\(\\NP\\), that \\(\\NP\\) is strictly contained in \\(\\PSPACE\\), and that\n\\(\\PSPACE\\) is strictly contained in \\(\\EXPTIME\\), none of these\ninequalities has been proved. In fact, it is not even known that\n\\(\\P\\) is different from \\(\\PSPACE\\), nor that \\(\\NP\\) is different\nfrom \\(\\EXPTIME\\). The only known proper inclusion from the above is\nthat \\(\\P\\) is strictly contained in \\(\\EXPTIME\\). The remaining\nquestions concerning the relative power of different computational\nresources are fundamental unsolved problems in the theory of\ncomputation. \n\nThere is an extensive theory of computational complexity.  This entry\nbriefly describes the area, putting it into the context of the\nquestion of what is computable in principle versus in practice.  For\nreaders interested in learning more about complexity, there are\nexcellent books, for example, [Papadimitriou, 1994] and [Arora and\nBarak, 2009]. There is also the entry on\n Computational Complexity Theory.\n\n \n\nThe following diagram  maps out all the\ncomplexity classes we have discussed and a few more as well.  The diagram comes from work in\nDescriptive Complexity [Immerman, 1999] which \nshows that all important complexity \nclasses have descriptive characterizations.  Fagin began this field by proving that NP = \nSO\\(\\exists\\), i.e., a property is in NP iff it is expressible in second-order existential logic\n[Fagin, 1974]. \n\nVardi and the author of this entry later independently proved that P =\nFO(LFP): a property is in P iff it is expressible in first-order logic\nplus a least fixed-point operator (LFP) which formalizes the power to\ndefine new relations by induction.  A captivating corollary of this is\nthat P = NP iff SO = FO(LFP).  That is, P is equal to NP iff every\nproperty expressible in second order logic is already expressible in\nfirst-order logic plus inductive definitions. (The languages in\nquestion are over finite ordered input structures.  See [Immerman,\n1999] for details.) The World of Computability and Complexity \n\nThe top right of the diagram shows the recursively enumerable (r.e.)\nproblems; this includes r.e.-complete problems such as the halting\nproblem (Halt). On the left is the set of co-r.e. problems including\nthe co-r.e.-complete problem \\(\\overline{{\\rm Halt}}\\) -- the set of\nTuring Machines that never halt on a given input.  We mentioned at the\nend of Section 2.3 that the intersection of the set of r.e problems\nand the set of co-r.e problems is equal to the set of Recursive\nproblems.  The set of Primitive Recursive problems is a strict subset\nof the Recursive problems. \n\nMoving toward the bottom of the diagram, there is a region marked with\na green dotted line labelled “truly feasible”.  Note that\nthis is not a mathematically defined class, but rather an intuitive\nnotion of those problems that can be solved exactly, for all the\ninstances of reasonable size, within a reasonable amount of time,\nusing a computer that we can afford. (Interestingly, as the speed of\ncomputers has dramatically increased over the years, our expectation\nof how large an instance we should be able to handle has increased\naccordingly.  Thus, the boundary of what is “truly\nfeasible” changes more slowly than the increase of computer\nspeed might suggest.) \n\nAs mentioned before, P is a good mathematical wrapper for the set of\nfeasible problems.  There are problems in P requiring \\(n^{1,000}\\)\ntime for problems of size \\(n\\) and thus not feasible.  Nature appears\nto be our friend here, which is to say naturally occurring problems in\nP favor relatively simple algorithms, and “natural”\nproblems tend to be feasible. The number of steps required for\nproblems of size \\(n\\) tends to be less than \\(c n^k\\) with small\nmultiplicative constants \\(c\\), and very small exponents, \\(k\\), i.e.,\n\\(k\\leq 2\\). \n\nIn practice the asympototic complexity of naturally occurring problems\ntends to be the key issue determining whether or not they are\nfeasible.  A problem with complexity \\(17n\\) can be handled in under a\nminute on modern computers, for every instance of size a\nbillion.  On the other hand, a problem with worst-case complexity\n\\(2^n\\) cannot be handled in our lifetimes for some instance of\nsize a hundred. \n\nRemarkably, natural problems tend to be complete for important\ncomplexity classes, namely the ones in the diagram and only a very few\nothers.  This fascinating phenomenon means that algorithms and\ncomplexity are more than abstract concepts; they are important at a\npractical level.  We have had remarkable success in proving that our\nproblem of interest is complete for a well-known complexity class.  If\nthe class is contained in P, then we can usually just look up a known\nefficient algorithm.  Otherwise, we must look at simplifications or\napproximations of our problem which may be feasible. \n\nThere is a rich theory of the approximability of NP optimization\nproblems (See [Arora & Barak, 2009]).  For example, the Subset Sum\nproblem mentioned above is an NP-complete problem.  Most likely it\nrequires exponential time to tell whether a given Subset Sum problem\nhas an exact solution.  However, if we only want to see if we can\nreach the target up to a fixed number of digits of accuracy, then the\nproblem is quite easy, i.e., Subset Sum is hard, but very easy to\napproximate. \n\nEven the r.e.-complete Halting problem has many important feasible\nsubproblems.  Given a program, it is in general not possible to figure\nout what it does and whether or not it eventually halts.  However,\nmost programs written by programmers or students can be automatically\nanalyzed, optimized and even corrected by modern compilers and model\ncheckers. \n\nThe class NP is very important practically and philosophically.  It is\nthe class of problems, \\(S\\), such that any input \\(w\\) is in \\(S\\)\niff there is a proof, \\(p(w)\\), that \\(w\\in S\\) and \\(p(w)\\) is not\nmuch larger than \\(w\\).  Thus, very informally, we can think of NP has\nthe set of intellectual endeavors that may be in reach: if we find the\nanswer to whether \\(w \\in S\\), we can convince others that we have\ndone so. \n\nThe boolean satisfiability problem, SAT, was the first problem proved\nNP complete [Cook, 1971], i.e., it is a hardest NP problem.  The fact\nthat SAT is NP complete means that all problems in NP are reducible to\nSAT.  Over the years, researchers have built very efficient SAT\nsolvers which can quickly solve many SAT instances – i.e., find a\nsatisfying assignment or prove that there is none\n-- even for instances with millions of variables.  Thus, SAT solvers are being used as general purpose\nproblem solvers.  On the other hand, there are known classes of small instances for which current\nSAT solvers fail.  Thus part of the P versus NP question concerns the practical and theoretical\ncomplexity of SAT [Nordström, 2015].","contact.mail":"immerman@cs.umass.edu","contact.domain":"cs.umass.edu"}]
