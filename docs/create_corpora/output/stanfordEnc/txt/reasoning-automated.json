[{"date.published":"2001-07-18","date.changed":"2019-03-02","url":"https://plato.stanford.edu/entries/reasoning-automated/","author1":"Frederic Portoraro","author1.info":"http://www.symlog.ca/Profile/Fred.htm","entry":"reasoning-automated","body.text":"\n\n\nReasoning is the ability to make inferences, and automated reasoning\nis concerned with the building of computing systems that automate this\nprocess. Although the overall goal is to mechanize different forms of\nreasoning, the term has largely been identified with valid deductive\nreasoning as practiced in mathematics and formal logic. In this\nrespect, automated reasoning is akin to mechanical theorem proving.\nBuilding an automated reasoning program means providing an algorithmic\ndescription to a formal calculus so that it can be implemented on a\ncomputer to prove theorems of the calculus in an efficient manner.\nImportant aspects of this exercise involve defining the class of\nproblems the program will be required to solve, deciding what language\nwill be used by the program to represent the information given to it\nas well as new information inferred by the program, specifying the\nmechanism that the program will use to conduct deductive inferences,\nand figuring out how to perform all these computations efficiently.\nWhile basic research work continues in order to provide the necessary\ntheoretical framework, the field has reached a point where automated\nreasoning programs are being used by researchers to attack open\nquestions in mathematics and logic, provide important applications in\ncomputing science, solve problems in engineering, and find novel\napproaches to questions in exact philosophy. \n\nA problem being presented to an automated reasoning program consists\nof two main items, namely a statement expressing the particular\nquestion being asked called the problem’s conclusion,\nand a collection of statements expressing all the relevant information\navailable to the program—the problem’s\nassumptions. Solving a problem means proving the conclusion\nfrom the given assumptions by the systematic application of rules of\ndeduction embedded within the reasoning program. The problem solving\nprocess ends when one such proof is found, when the program is able to\ndetect the non-existence of a proof, or when it simply runs out of\nresources. \nA first important consideration in the design of an automated\nreasoning program is to delineate the class of problems that the\nprogram will be required to solve—the problem\ndomain. The domain can be very large, as would be the case\nfor a general-purpose theorem prover for first-order logic, or be more\nrestricted in scope as in a special-purpose theorem prover for\nTarski’s geometry, or the modal logic K. A typical approach in the\ndesign of an automated reasoning program is to provide it first with\nsufficient logical power (e.g., first-order logic) and then further\ndemarcate its scope to the particular domain of interest defined by a\nset of domain axioms. To illustrate, EQP, a\ntheorem-proving program for equational logic, was used to solve an\nopen question in Robbins algebra (McCune 1997): Are all Robbins\nalgebras Boolean? For this, the program was provided with the\naxioms defining a Robbins algebra: \nThe program was then used to show that a characterization of Boolean\nalgebra that uses Huntington’s equation, \nfollows from the axioms. We should remark that this problem is\nnon-trivial since deciding whether a finite set of equations provides\na basis for Boolean algebra is undecidable, that is, it does not\npermit an algorithmic representation; also, the problem was attacked\nby Robbins, Huntington, Tarski and many of his students with no\nsuccess. The key step was to establish that all Robbins algebras\nsatisfy  \nsince it was known that this formula is a sufficient condition for a\nRobbins algebra to be Boolean. When EQP was supplied with this piece\nof information, the program provided invaluable assistance by\ncompleting the proof automatically.  \nA special-purpose theorem prover does not draw its main benefit by\nrestricting its attention to the domain axioms but from the fact that\nthe domain may enjoy particular theorem-proving techniques which can\nbe hardwired—coded—within the reasoning program itself and\nwhich may result in a more efficient logic implementation. Much of\nEQP’s success at settling the Robbins question can be attributed to\nits built-in associative-commutative inference mechanisms. \nA second important consideration in the building of an automated\nreasoning program is to decide (1) how problems in its domain will be\npresented to the reasoning program; (2) how they will actually be\nrepresented internally within the program; and, (3) how the solutions\nfound—completed proofs—will be displayed back to the user.\nThere are several formalisms available for this, and the choice is\ndependent on the problem domain and the underlying deduction calculus\nused by the reasoning program. The most commonly used formalisms\ninclude standard first-order logic, typed λ-calculus, and\nclausal logic. We take up clausal logic here and assume that the\nreader is familiar with the rudiments of first-order logic; for the\ntyped λ-calculus the reader may want to check Church 1940.\nClausal logic is a quantifier-free variation of first-order logic and\nhas been the most widely used notation within the automated reasoning\ncommunity. Some definitions are in order: A term is a\nconstant, a variable, or a function whose arguments are themselves\nterms. For example, a, x, f(x),\nand h(c,f(z),y) are all\nterms. A literal is either an atomic formula, e.g.\nF(x), or the negation of an atomic formula, e.g.\n~R(x,f(a)). Two literals are\ncomplementary if one is the negation of the other. A\nclause is a (possibly empty) finite disjunction of\nliterals l1∨ … ∨\nln where no literal appears more than once in the\nclause (that is, clauses can be alternatively treated as sets of\nliterals). Ground terms, ground literals, and ground\nclauses have no variables. The empty clause,\n[ ], is the clause having no literals and, hence, is\nunsatisfiable—false under any interpretation. Some examples:\n~R(a,b), and F(a) ∨\n~R(f(x),b) ∨\nF(z) are both examples of clauses but only the\nformer is ground. The general idea is to be able to express a\nproblem’s formulation as a set of clauses or, equivalently, as a\nformula in conjunctive normal form (CNF), that is, as\na conjunction of clauses. \nFor formulas already expressed in standard logic notation, there is a\nsystematic two-step procedure for transforming them into conjunctive\nnormal form. The first step consists in re-expressing a formula into a\nsemantically equivalent formula in prenex normal\nform,\n(Θx1)…(Θxn)α(x1,…,xn), consisting of a\nstring of quantifiers\n(Θx1)…(Θxn)\nfollowed by a quantifier-free expression\nα(x1,…,xn) called\nthe matrix. The second step in the transformation\nfirst converts the matrix into conjunctive normal form by using\nwell-known logical equivalences such as DeMorgan’s laws, distribution,\ndouble-negation, and others; then, the quantifiers in front of the\nmatrix, which is now in conjunctive normal form, are dropped according\nto certain rules. In the presence of existential quantifiers, this\nlatter step does not always preserve equivalence and requires the\nintroduction of Skolem functions whose role is to\n“simulate” the behaviour of existentially quantified\nvariables. For example, applying the skolemizing process to the\nformula \nrequires the introduction of a one-place and two-place Skolem\nfunctions, f and g respectively, resulting in the\nformula \nThe universal quantifiers can then be removed to obtain the final\nclause, R(x,f(x),v) ∨\n~K(x,z,g(x,z),v)\nin our example. The Skolemizing process may not preserve equivalence\nbut maintains satisfiability, which is enough for clause-based\nautomated reasoning. \nAlthough clausal form provides a more uniform and economical\nnotation—there are no quantifiers and all formulas are\ndisjunctions—it has certain disadvantages. One drawback is the\nincrease in the size of the resulting formula when transformed from\nstandard logic notation into clausal form. The increase in size is\naccompanied by an increase in cognitive complexity that makes it\nharder for humans to read proofs written with clauses. Another\ndisadvantage is that the syntactic structure of a formula in standard\nlogic notation can be used to guide the construction of a proof but\nthis information is completely lost in the transformation into clausal\nform. \nA third important consideration in the building of an automated\nreasoning program is the selection of the actual deduction calculus\nthat will be used by the program to perform its inferences. As\nindicated before, the choice is highly dependent on the nature of the\nproblem domain and there is a fair range of options available:\nGeneral-purpose theorem proving and problem solving (first-order\nlogic, simple type theory), program verification (first-order logic),\ndistributed and concurrent systems (modal and temporal logics),\nprogram specification (intuitionistic logic), hardware verification\n(higher-order logic), logic programming (Horn logic), constraint\nsatisfaction (propositional clausal logic), computational metaphysics\n(higher-order modal logic), and others. \nA deduction calculus consists of a set of logical axioms and a\ncollection of deduction rules for deriving new formulas from\npreviously derived formulas. Solving a problem in the program’s\nproblem domain then really means establishing a particular formula\nα—the problem’s conclusion—from the extended set\nΓ consisting of the logical axioms, the domain axioms, and the\nproblem assumptions. That is, the program needs to determine if\nΓ entails α, Γ⊨ α. How the program\ngoes about establishing this semantic fact depends, of course, on the\ncalculus it implements. Some programs may take a very\ndirect route and attempt to establish that\nΓ ⊨ α by actually constructing a\nstep-by-step proof of α from Γ. If successful, this shows\nof course that Γ derives—proves—α, a fact we\ndenote by writing Γ ⊢ α. Other reasoning\nprograms may instead opt for a more indirect approach\nand try to establish that Γ ⊨ α by showing\nthat Γ ∪ {~α} is inconsistent which, in turn, is shown\nby deriving a contradiction, ⊥, from the set Γ ∪\n{~α}. Automated systems that implement the former approach\ninclude natural deduction systems; the latter approach is used by\nsystems based on resolution, sequent deduction, and matrix connection\nmethods. \nSoundness and completeness are two (metatheoretical) properties of a\ncalculus that are particularly important for automated deduction.\nSoundness states that the rules of the calculus are\ntruth-preserving. For a direct calculus this means that if\nΓ ⊢ α then\nΓ ⊨ α. For indirect calculi, soundness\nmeans that if Γ∪{~α} ⊢ ⊥ then\nΓ ⊨ α. Completeness in a\ndirect calculus states that if Γ ⊨ α then\nΓ ⊢ α. For indirect calculi, the\ncompleteness property is expressed in terms of\nrefutations since one establishes that\nΓ ⊨ α by showing the existence of a proof,\nnot of α from Γ, but of ⊥ from\nΓ∪{~α}. Thus, an indirect calculus is\nrefutation complete if Γ ⊨ α\nimplies Γ∪{~α} ⊢ ⊥. Of the two\nproperties, soundness is the most desirable. An incomplete calculus\nindicates that there are entailment relations that cannot be\nestablished within the calculus. For an automated reasoning program\nthis means, informally, that there are true statements that the\nprogram cannot prove. Incompleteness may be an unfortunate affair but\nlack of soundness is a truly problematic situation since an unsound\nreasoning program would be able to generate false conclusions from\nperfectly true information. \nIt is important to appreciate the difference between a logical\ncalculus and its corresponding implementation in a reasoning program.\nThe implementation of a calculus invariably involves making some\nmodifications to the calculus and this results, strictly speaking, in\na new calculus. The most important modification to the original\ncalculus is the “mechanization” of its deduction rules,\nthat is, the specification of the systematic way in which the rules\nare to be applied. In the process of doing so, one must exercise care\nto preserve the metatheoretical properties of the original\ncalculus. \nTwo other metatheoretical properties of importance to automated\ndeduction are decidability and complexity. A calculus is\ndecidable if it admits an algorithmic representation,\nthat is, if there is an algorithm that, for any given Γ and\nα, it can determine in a finite amount of time the answer,\n“Yes” or “No”, to the question “Does\nΓ ⊨ α?” A calculus may be\nundecidable in which case one needs to determine which decidable\nfragment to implement. The time-space complexity of a\ncalculus specifies how efficient its algorithmic representation is.\nAutomated reasoning is made the more challenging because many calculi\nof interest are not decidable and have poor complexity measures\nforcing researchers to seek tradeoffs between deductive power versus\nalgorithmic efficiency. \nOf the many calculi used in the implementation of reasoning programs,\nthe ones based on the resolution principle have been\nthe most popular. Resolution is modeled after the chain rule (of which\nModus Ponens is a special case) and essentially states that from\np ∨ q and ~q ∨ r one can\ninfer p ∨ r. More formally, let C\n− l denote the clause C with the literal\nl removed. Assume that C1 and\nC2 are ground clauses containing, respectively, a\npositive literal l1 and a negative literal\n~l2 such that l1 and\n~l2 are complementary. Then, the rule of\nground resolution states that, as a result of\nresolving C1 and\nC2, one can infer (C1 −\nl1) ∨ (C2 −\n~l2): \nHerbrand’s theorem (Herbrand 1930) assures us that\nthe non-satisfiability of any set of clauses, ground or not,\ncan be established by using ground resolution. This is a very\nsignificant result for automated deduction since it tells us that if a\nset Γ is not satisfied by any of the infinitely many\ninterpretations, this fact can be determined in finitely many\nsteps. Unfortunately, a direct implementation of ground resolution\nusing Herbrand’s theorem requires the generation of a vast number of\nground terms making this approach hopelessly inefficient. This issue\nwas effectively addressed by generalizing the ground resolution rule\nto binary resolution and by introducing the notion of\nunification (Robinson 1965a). Unification allows resolution proofs to\nbe “lifted” and be conducted at a more general level;\nclauses only need to be instantiated at the moment where they are to\nbe resolved. Moreover, the clauses resulting from the instantiation\nprocess do not have to be ground instances and may still contain\nvariables. The introduction of binary resolution and unification is\nconsidered one of the most important developments in the field of\nautomated reasoning. \nA unifier of two expressions—terms or\nclauses—is a substitution that when applied to the expressions\nmakes them equal. For example, the substitution σ given by \nis a unifier for  \nsince when applied to both expressions it makes them equal:  \nA most general unifier (mgu) produces the most\ngeneral instance shared by two unifiable expressions. In the previous\nexample, the substitution {x ← b, y\n← b, z ←\nf(a,b)} is a unifier but not an mgu;\nhowever, {x ← b, z ←\nf(a,y)} is an mgu. Note that unification\nattempts to “match” two expressions and this fundamental\nprocess has become a central component of most automated deduction\nprograms, resolution-based and otherwise.\nTheory-unification is an extension of the unification\nmechanism that includes built-in inference capabilities. For example,\nthe clauses R(g(a,b),x)\nand R(g(b,a),d) do not\nunify but they AC-unify, where AC-unification is unification with\nbuilt-in associative and commutative rules such as\ng(a,b) = g(b,a).\nShifting inference capabilities into the unification mechanism adds\npower but at a price: The existence of an mgu for two unifiable\nexpressions may not be unique (there could actually be infinitely\nmany), and the unification process becomes undecidable in general.\n \nLet C1 and C2 be two clauses\ncontaining, respectively, a positive literal l1\nand a negative literal ~l2 such that\nl1 and l2 unify with mgu\nθ. Then,  \nby binary resolution; the clause (C1θ\n− l1θ) ∨\n(C2θ − ~l2θ)\nis called a binary resolvent of\nC1 and C2.  \nIf two or more literals occurring in a clause C share an mgu\nθ then Cθ is a factor of\nC. For example, in R(x,a) ∨\n~K(f(x),b) ∨\nR(c,y) the literals\nR(x,a) and\nR(c,y) unify with mgu {x ←\nc, y ← a} and, hence,\nR(c,a) ∨\n~K(f(c),b) is a factor of the\noriginal clause.  \nLet C1and C2 be two clauses.\nThen, a resolvent obtained by\nresolution from C1 and\nC2 is defined as: (a) a binary resolvent of\nC1 and C2; (b) a binary\nresolvent of C1 and a factor of\nC2; (c) a binary resolvent of a factor of\nC1 and C2; or, (d) a binary\nresolvent of a factor of C1 and a factor of\nC2.  \nResolution proofs, more precisely refutations, are constructed by\nderiving the empty clause [ ] from Γ ∪ {~α} using\nresolution; this will always be possible if Γ ∪ {~α}\nis unsatisfiable since resolution is refutation complete (Robinson\n1965a). As an example of a resolution proof, we show that the set\n{∀x(P(x) ∨\nQ(x)), ∀x(P(x)\n⊃\nR(x)),∀x(Q(x)\n⊃ R(x))}, denoted by Γ, entails the\nformula ∃xR(x). The first step is to\nfind the clausal form of Γ ∪\n{~∃xR(x)}; the resulting clause set,\ndenoted by S0, is shown in steps 1 to 4 in the\nrefutation below. The refutation is constructed by using a\nlevel-saturation method: Compute all the resolvents of the initial\nset, S0, add them to the set and repeat the\nprocess until the empty clause is derived. (This produces the sequence\nof increasingly larger sets: S0,\nS1, S2,…) The only\nconstraint that we impose is that we do not resolve the same two\nclauses more than once. \nAlthough the resolution proof is successful in deriving [ ], it\nhas some significant drawbacks. To start with, the refutation is too\nlong as it takes 21 steps to reach the contradiction, [ ]. This\nis due to the naïve brute-force nature of the implementation. The\napproach not only generates too many formulas but some are clearly\nredundant. Note how R(a) is derived six times; also,\nR(x) has more “information content” than\nR(a) and one should keep the former and disregard\nthe latter. Resolution, like all other automated deduction methods,\nmust be supplemented by strategies aimed at improving the efficiency\nof the deduction process. The above sample derivation has 21 steps but\nresearch-type problems command derivations with thousands or hundreds\nof thousands of steps. \nThe successful implementation of a deduction calculus in an automated\nreasoning program requires the integration of search strategies that\nreduce the search space by pruning unnecessary deduction paths. Some\nstrategies remove redundant clauses or tautologies as soon as they\nappear in a derivation. Another strategy is to remove more specific\nclauses in the presence of more general ones by a process known as\nsubsumption (Robinson 1965a). Unrestricted\nsubsumption, however, does not preserve the refutation completeness of\nresolution and, hence, there is a need to restrict its applicability\n(Loveland 1978). Model elimination (Loveland 1969)\ncan discard a sentence by showing that it is false in some model of\nthe axioms. The subject of model generation has received much\nattention as a complementary process to theorem proving. The method\nhas been used successfully by automated reasoning programs to show the\nindependence of axioms sets and to determine the existence of discrete\nmathematical structures meeting some given criteria.  \nInstead of removing redundant clauses, some strategies prevent the\ngeneration of useless clauses in the first place. The\nset-of-support strategy (Wos, Carson & Robinson\n1965) is one of the most powerful strategies of this kind. A subset\nT of the set S, where S is initially\nΓ ∪ {~α}, is called a set of support\nof S iff S − T is satisfiable.\nSet-of-support resolution dictates that the resolved clauses are not\nboth from S − T. The motivation behind\nset-of-support is that since the set Γ is usually satisfiable it\nmight be wise not to resolve two clauses from Γ against each\nother. Hyperresolution (Robinson 1965b) reduces the\nnumber of intermediate resolvents by combining several resolution\nsteps into a single inference step. \nIndependently co-discovered, linear resolution\n(Loveland 1970, Luckham 1970) always resolves a clause against the\nmost recently derived resolvent. This gives the deduction a simple\n“linear” structure affording a straightforward\nimplementation; yet, linear resolution preserves refutation\ncompleteness. Using linear resolution we can derive the empty clause\nin the above example in only eight steps: \nWith the exception of unrestricted subsumption, all the strategies\nmentioned so far preserve refutation completeness. Efficiency is an\nimportant consideration in automated reasoning and one may sometimes\nbe willing to trade completeness for speed. Unit\nresolution and input resolution are two such\nrefinements of linear resolution. In the former, one of the resolved\nclauses is always a literal; in the latter, one of the resolved\nclauses is always selected from the original set to be refuted. Albeit\nefficient, neither strategy is complete. Ordering strategies impose\nsome form of partial ordering on the predicate symbols, terms,\nliterals, or clauses occurring in the deduction. Ordered\nresolution treats clauses not as sets of literals but as\nsequences—linear orders—of literals. Ordered resolution is\nextremely efficient but, like unit and input resolution, is not\nrefutation complete. To end, it must be noted that some strategies\nimprove certain aspects of the deduction process at the expense of\nothers. For instance, a strategy may reduce the size of the proof\nsearch space at the expense of increasing, say, the length of the\nshortest refutations. A taxonomy and detailed presentation of\ntheorem-proving strategies can be found in (Bonacina 1999). \nThere are several automated reasoning programs that are based on\nresolution, or refinements of resolution. Otter (succeeded by Prover4)\nwas a driving force in the development of automated reasoning (Wos,\nOverbeek, Lusk & Boyle 1984) but it has been superseded by more\ncapable programs like Vampire (Voronkov 1995, Kovács &\nVoronkov 2013). Resolution also provides the underlying\nlogico-computational mechanism for the popular logic programming\nlanguage Prolog (Clocksin & Mellish 1981). \nHilbert-style calculi (Hilbert and Ackermann 1928) have been\ntraditionally used to characterize logic systems. These calculi\nusually consist of a few axiom schemata and a small number of rules\nthat typically include modus ponens and the rule of substitution.\nAlthough they meet the required theoretical requisites (soundness,\ncompleteness, etc.) the approach at proof construction in these\ncalculi is difficult and does not reflect standard practice. It was\nGentzen’s goal “to set up a formalism that reflects as\naccurately as possible the actual logical reasoning involved in\nmathematical proofs” (Gentzen 1935). To carry out this task,\nGentzen analyzed the proof-construction process and then devised two\ndeduction calculi for classical logic: the natural deduction calculus,\nNK, and the sequent calculus, LK.\n(Gentzen actually designed NK first and then introduced LK to pursue\nmetatheoretical investigations). The calculi met his goal to a large\nextent while at the same time managing to secure soundness and\ncompleteness. Both calculi are characterized by a relatively larger\nnumber of deduction rules and a simple axiom schema. Of the two\ncalculi, LK is the one that has been most widely used in\nimplementations of automated reasoning programs, and it is the one\nthat we will discuss first; NK will be discussed in the next\nsection. \nAlthough the application of the LK rules affect logic formulas, the\nrules are seen as manipulating not logic formulas themselves but\nsequents. Sequents are expressions of the form\nΓ → Δ, where both Γ and Δ are (possibly\nempty) sets of formulas. Γ is the sequent’s\nantecedent and Δ its\nsuccedent. Sequents can be interpreted thus: Let\nI be an interpretation. Then, \nIn other words,  \nIf Γ or Δ are empty then they are respectively valid or\nunsatisfiable. An axiom of LK is a sequent Γ\n→ Δ where Γ ∩ Δ ≠ ∅. Thus, the\nrequirement that the same formula occurs at each side of the →\nsign means that the axioms of LK are valid, for no interpretation can\nthen make all the formulas in Γ true and, simultaneously, make\nall those in Δ false. LK has two rules per logical connective,\nplus one extra rule: the cut rule.  \nThe sequents above a rule’s line are called the rule’s\npremises and the sequent below the line is the rule’s\nconclusion. The quantification rules ∃→ and\n→∀ have an eigenvariable condition that restricts their\napplicability, namely that a must not occur in Γ,\nΔ or in the quantified sentence. The purpose of this restriction\nis to ensure that the choice of parameter, a, used in the\nsubstitution process is completely “arbitrary”. \nProofs in LK are represented as trees where each node in the tree is\nlabeled with a sequent, and where the original sequent sits at the\nroot of the tree. The children of a node are the premises of the rule\nbeing applied at that node. The leaves of the tree are labeled with\naxioms. Here is the LK-proof of\n∃xR(x) from the set\n{∀x(P(x) ∨\nQ(x)), ∀x(P(x)\n⊃\nR(x)),∀x(Q(x)\n⊃ R(x))}. In the tree below, Γ stands for\nthis set: \nIn our example, all the leaves in the proof tree are labeled with\naxioms. This establishes the validity of Γ →\n∃xR(x) and, hence, the fact that\nΓ ⊨ ∃xR(x). LK\ntakes an indirect approach at proving the conclusion and this is an\nimportant difference between LK and NK. While NK constructs an actual\nproof (of the conclusion from the given assumptions), LK instead\nconstructs a proof that proves the existence of a proof (of the\nconclusion from the assumptions). For instance, to prove that α\nis entailed by Γ, NK constructs a step-by-step proof of α\nfrom Γ (assuming that one exists); in contrast, LK first\nconstructs the sequent Γ → α which then attempts to\nprove valid by showing that it cannot be made false. This is done by\nsearching for a counterexample that makes (all the sentences in)\nΓ true and makes α false: If the search fails then a\ncounterexample does not exist and the sequent is therefore valid. In\nthis respect, proof trees in LK are actually refutation proofs. Like\nresolution, LK is refutation complete: If\nΓ ⊨ α then the sequent Γ →\nα has a proof tree. \nAs it stands, LK is unsuitable for automated deduction and there are\nsome obstacles that must be overcome before it can be efficiently\nimplemented. The reason is, of course, that the statement of the\ncompleteness of LK only has to assert, for each entailment relation,\nthe existence of a proof tree but a reasoning program has the more\ndifficult task of actually having to construct one. Some of the main\nobstacles: First, LK does not specify the order in which the rules\nmust be applied in the construction of a proof tree. Second, and as a\nparticular case of the first problem, the premises in the rules\n∀→ and →∃ rules inherit the quantificational\nformula to which the rule is applied, meaning that the rules can be\napplied repeatedly to the same formula sending the proof search into\nan endless loop. Third, LK does not indicate which formula must be\nselected next in the application of a rule. Fourth, the quantifier\nrules provide no indication as to what terms or free variables must be\nused in their deployment. Fifth, and as a particular case of the\nprevious problem, the application of a quantifier rule can lead into\nan infinitely long tree branch because the proper term to be used in\nthe instantiation never gets chosen. Fortunately, as we will hint at\nbelow each of these problems can be successfully addressed. \nAxiom sequents in LK are valid, and the conclusion of a rule is valid\niff its premises are. This fact allows us to apply the LK rules in\neither direction, forwards from axioms to conclusion, or backwards\nfrom conclusion to axioms. Also, with the exception of the cut rule,\nall the rules’ premises are subformulas of their respective\nconclusions. For the purposes of automated deduction this is a\nsignificant fact and we would want to dispense with the cut rule;\nfortunately, the cut-free version of LK preserves its refutation\ncompleteness (Gentzen 1935). These results provide a strong case for\nconstructing proof trees in a backwards fashion; indeed, by working\nthis way a refutation in cut-free LK gets increasingly simpler as it\nprogresses since subformulas are simpler than their parent formulas.\nMoreover, and as far as propositional rules go, the new subformulas\nentered into the tree are completely dictated by the cut-free LK\nrules. Furthermore, and assuming the proof tree can be brought to\ncompletion, branches eventually end up with atoms and the presence of\naxioms can be quickly determined. Another reason for working backwards\nis that the truth-functional fragment of cut-free LK is\nconfluent in the sense that the order in which the\nnon-quantifier rules are applied is irrelevant: If there is a proof,\nregardless of what you do, you will run into it! To bring the\nquantifier rules into the picture, things can be arranged so that all\nrules have a fair chance of being deployed: Apply, as far as possible,\nall the non-quantifier rules before applying any of the quantifier\nrules. This takes care of the first and second obstacles, and it is no\ntoo difficult to see how the third one would now be handled. The\nfourth and fifth obstacles can be addressed by requiring that the\nterms to be used in the substitutions be suitably selected from the\nHerbrand universe (Herbrand 1930). \nThe use of sequent-type calculi in automated theorem proving was\ninitiated by efforts to mechanize mathematics (Wang 1960). At the\ntime, resolution captured most of the attention of the automated\nreasoning community but during the 1970s some researchers started to\nfurther investigate non-resolution methods (Bledsoe 1977), prompting a\nfrutiful and sustained effort to develop more human-oriented theorem\nproving systems (Bledsoe 1975, Nevins 1974). Eventually, sequent-type\ndeduction gained momentum again, particularly in its re-incarnation as\nanalytic tableaux (Fitting 1990). The method of\ndeduction used in tableaux is essentially cut-free LK’s with sets used\nin lieu of sequents. \nAlthough LK and NK are both commonly labeled as “natural\ndeduction” systems, it is the latter which better deserves the\ntitle due to its more natural, human-like, approach to proof\nconstruction. The rules of NK are typically presented as acting on\nstandard logic formulas in an implicitly understood context, but they\nare also commonly given in the literature as acting more explicitly on\n“judgements”, that is, expressions of the form\nΓ ⊢ α where Γ is a set of formulas\nand α is a formula. This form is typically understood as making\nthe metastatement that there is a proof of α from Γ\n(Kleene 1962). Following Gentzen 1935 and Prawitz 1965 here we take\nthe former approach. The system NK has no logical axioms and provides\ntwo introduction-elimination rules for each logical connective: \nA few remarks: First, the expression\n[α — γ] represents the fact that α\nis an auxiliary assumption in the proof of γ that eventually\ngets discharged, i.e. discarded. For example, ∃E tells us that\nif in the process of constructing a proof one has already derived\n∃xα(x) and also β with\nα(a/x) as an auxiliary assumption then the\ninference to β is allowed. Second, the eigenparameter,\na, in ∃E and ∀I must be foreign to the premises,\nundischarged—“active”—assumptions, to the\nrule’s conclusion and, in the case of ∃E, to\n∃xα(x). Third, ⊥ is shorthand for\ntwo contradictory formulas, β and ~β. Finally, NK is\ncomplete: If Γ  ⊨  α then there is a\nproof of α from Γ using the rules of NK. \nAs in LK, proofs constructed in NK are represented as trees with the\nproof’s conclusion sitting at the root of the tree, and the problem’s\nassumptions sitting at the leaves. (Proofs are also typically given as\nsequences of judgements, Γ ⊢ α, running\nfrom the top to the bottom of the printed page.) Here is a natural\ndeduction proof tree of ∃xR(x) from\n∀x(P(x) ∨\nQ(x)), ∀x(P(x)\n⊃ R(x)) and\n∀x(Q(x) ⊃\nR(x)): \nAs in LK, a forward-chaining strategy for proof construction is not\nwell focused. So, although proofs are read forwards, that is,\nfrom leaves to root or, logically speaking, from assumptions to\nconclusion, that is not the way in which they are typically\nconstructed. A backward-chaining strategy implemented by\napplying the rules in reverse order is more effective. Many of the\nobstacles that were discussed above in the implementation of sequent\ndeduction are applicable to natural deduction as well. These issues\ncan be handled in a similar way, but natural deduction introduces some\nissues of its own. For example, as suggested by the ⊃-Introduction\nrule, to prove a goal of the form α ⊃ β one could\nattempt to prove β on the assumption that α. But note that\nalthough the goal α ⊃ β does not match the conclusion\nof any other introduction rule, it matches the conclusion of all\nelimination rules and the reasoning program would need to\nconsider those routes too. Similarly to forward-chaining, here there\nis the risk of setting goals that are irrelevant to the proof and that\ncould lead the program astray. To wit: What prevents a program from\nentering the never-ending process of building, say, larger and larger\nconjunctions? Or, what is there to prevent an uncontrolled chain of\nbackward applications of, say, ⊃-Elimination? Fortunately, NK\nenjoys the subformula property in the sense that each\nformula entering into a natural deduction proof can be restricted to\nbeing a subformula of Γ ∪ Δ ∪ {α}, where\nΔ is the set of auxiliary assumptions made by the ~-Elimination\nrule. By exploiting the subformula property a natural deduction\nautomated theorem prover can drastically reduce its search space and\nbring the backward application of the elimination rules under control\n(Portoraro 1998, Sieg & Byrnes 1996). Further gains can be realized\nif one is willing to restrict the scope of NK’s logic to its\nintuitionistic fragment where every proof has a normal form in the\nsense that no formula is obtained by an introduction rule and then is\neliminated by an elimination rule (Prawitz 1965). \nImplementations of automated theorem proving systems using NK\ndeduction have been motivated by the desire to have the program reason\nwith precisely the same proof format and methods employed by the human\nuser. This has been particularly true in the area of education where\nthe student is engaged in the interactive construction of formal\nproofs in an NK-like calculus working under the guidance of a theorem\nprover ready to provide assistance when needed (Portoraro 1994, Suppes\n1981). Other, research-oriented, theorem provers true to the spirit of\nNK exist (Pelletier 1998) but are rare. \nThe name of the matrix connection method (Bibel 1981) is indicative of\nthe way it operates. The term “matrix” refers to the form\nin which the set of logic formulas expressing the problem is\nrepresented; the term “connection” refers to the way the\nmethod operates on these formulas. To illustrate the method at work,\nwe will use an example from propositional logic and show that\nR is entailed by P ∨ Q, P ⊃\nR and Q ⊃ R. This is done by\nestablishing that the formula \nis unsatisfiable. To do this, we begin by transforming it into\nconjunctive normal form:  \nThis formula is then represented as a matrix, one conjunct per row\nand, within a row, one disjunct per column: \nThe idea now is to explore all the possible vertical paths running\nthrough this matrix. A vertical path is a set of\nliterals selected from each row in the matrix such that each literal\ncomes from a different row. The vertical paths: \nA path is complementary if it contains two literals\nwhich are complementary. For example, Path 2 is complementary since it\nhas both P and ~P but so is Path 6 since it contains\nboth R and ~R. Note that as soon as a path includes\ntwo complementary literals there is no point in pursuing the path\nsince it has itself become complementary. This typically allows for a\nlarge reduction in the number of paths to be inspected. In any event,\nall the paths in the above matrix are complementary and this fact\nestablishes the unsatisfiability of the original formula. This is the\nessence of the matrix connection method. The method can be extended to\npredicate logic but this demands additional logical apparatus:\nSkolemnization, variable renaming, quantifier duplication,\ncomplementarity of paths via unification, and simultaneous\nsubstitution across all matrix paths (Bibel 1981, Andrews 1981).\nVariations of the method have been implemented in reasoning programs\nin higher-order logic (Andrews 1981) and non-classical logics (Wallen\n1990). \nEquality is an important logical relation whose behavior within\nautomated deduction deserves its own separate treatment.\nEquational logic and, more generally, term\nrewriting treat equality-like equations as rewrite\nrules, also known as reduction or\ndemodulation rules. An equality statement like\nf(a)= a allows the simplification of a term\nlike g(c,f(a)) to\ng(c,a). However, the same equation also has\nthe potential to generate an unboundedly large term:\ng(c,f(a)),\ng(c,f(f(a))),\ng(c,f(f(f(a)))),\n… . What distinguishes term rewriting from equational logic is\nthat in term rewriting equations are used as unidirectional reduction\nrules as opposed to equality which works in both directions. Rewrite\nrules have the form t1 ⇒\nt2 and the basic idea is to look for terms\nt occurring in expressions e such that t\nunifies with t1 with unifier θ so that the\noccurrence t1θ in eθ can be\nreplaced by t2θ. For example, the rewrite\nrule x + 0 ⇒ x allows the rewriting of\nsucc(succ(0) + 0) as\nsucc(succ(0)). \nTo illustrate the main ideas in term rewriting, let us explore an\nexample involving symbolic differentiation (the example and ensuing\ndiscussion are adapted from Chapter 1 of Baader & Nipkow 1998). Let\nder denote the derivative respect to x, let\ny be a variable different from x, and let u\nand v be variables ranging over expressions. We define the\nrewrite system: \nAgain, the symbol ⇒ indicates that a term matching the left-hand\nside of a rewrite rule should be replaced by the rule’s right-hand\nside. To see the differentiation system at work, let us compute the\nderivative of x × x respect to x,\nder(x × x): \nAt this point, since none of the rules (R1)–(R4) applies, no\nfurther reduction is possible and the rewriting process ends. The\nfinal expression obtained is called a normal form,\nand its existence motivates the following question: Is there an\nexpression whose reduction process will never terminate when applying\nthe rules (R1)–(R4)? Or, more generally: Under what conditions a\nset of rewrite rules will always stop, for any given expression, at a\nnormal form after finitely many applications of the rules? This\nfundamental question is called the termination\nproblem of a rewrite system, and we state without proof that the\nsystem (R1)–(R4) meets the termination condition. \nThere is the possibility that when reducing an expression, the set of\nrules of a rewrite system could be applied in more than one way. This\nis actually the case in the system (R1)–(R4) where in the\nreduction of der(x × x) we could have\napplied R1 first to the second sub-expression in (x ×\nder(x)) + (der(x) ×\nx), as shown below: \nFollowing this alternative course of action, the reduction terminates\nwith the same normal form as in the previous case. This fact, however,\nshould not be taken for granted: A rewriting system is said to be\n(globally) confluent if and only if independently of\nthe order in which its rules are applied every expression always ends\nup being reduced to its one and only normal form. It can be shown that\n(R1)–(R4) is confluent and, hence, we are entitled to say:\n“Compute the derivative of an expression” (as\nopposed to simply “a” derivative). Adding more\nrules to a system in an effort to make it more practical can have\nundesired consequences. For example, if we add the rule \nto (R1)–(R4) then we will be able to further reduce certain\nexpressions but at the price of losing confluency. The following\nreductions show that der(x + 0) now has two normal\nforms: the computation \ngives one normal form, and \ngives another. Adding the rule \nwould allow the further reduction of 1 + der(0) to 1 + 0 and\nthen, by R5, to 1. Although the presence of this new rule actually\nincreases the number of alternative\npaths—der(x + 0) can now be reduced in four\npossible ways—they all end up with the same normal form, namely\n1. This is no coincidence as it can be shown that (R6) actually\nrestores confluency. This motivates another fundamental question:\nUnder what conditions can a non-confluent system be made into an\nequivalent confluent one? The Knuth-Bendix completion\nalgorithm (Knuth & Bendix 1970) gives a partial answer to this\nquestion. \nTerm rewriting, like any other automated deduction method, needs\nstrategies to direct its application. Rippling (Bundy, Stevens &\nHarmelen 1993, Basin & Walsh 1996) is a heuristic that has its\norigins in inductive theorem-proving that uses annotations to\nselectively restrict the rewriting process. The superposition calculus\nis a calculus of equational first-order logic that combines notions\nfrom first-order resolution and Knuth-Bendix ordering equality.\nSuperposition is refutation complete (Bachmair & Ganzinger 1994) and\nis at the heart of a number of theorem provers, most notably the E\nequational theorem prover (Schulz 2004) and Vampire (Voronkov\n1995). \nMathematical induction is a very important technique of theorem\nproving in mathematics and computer science. Problems stated in terms\nof objects or structures that involve recursive definitions or some\nform of repetition invariably require mathematical induction for their\nsolving. In particular, reasoning about the correctness of computer\nsystems requires induction and an automated reasoning program that\neffectively implements induction will have important applications. \nTo illustrate the need for mathematical induction, assume that a\nproperty φ is true of the number zero and also that if true of a\nnumber then is true of its successor. Then, with our deductive\nsystems, we can deduce that for any given number n, φ is\ntrue of it, φ(n). But we cannot deduce that φ is true\nof all numbers, ∀xφ(x); this inference\nstep requires the rule of mathematical induction: \nIn other words, to prove that ∀xα(x)\none proves that α(0) is the case, and that\nα(succ(n)) follows from the assumption that\nα(n). The implementation of induction in a reasoning\nsystem presents very challenging search control problems. The most\nimportant of these is the ability to determine the particular way in\nwhich induction will be applied during the proof, that is, finding the\nappropriate induction schema. Related issues include selecting the\nproper variable of induction, and recognizing all the possible cases\nfor the base and the inductive steps. \nNqthm (Boyer & Moore 1979) has been one of the most successful\nimplementations of automated inductive theorem proving. In the spirit\nof Gentzen, Boyer and Moore were interested in how people prove\ntheorems by induction. Their theorem prover is written in the\nfunctional programming language Lisp which is also the language in\nwhich theorems are represented. For instance, to express the\ncommutativity of addition the user would enter the Lisp expression\n(EQUAL (PLUS X Y) (PLUS Y X)). Everything\ndefined in the system is a functional term, including its basic\n“predicates”: T, F,\nEQUAL X Y, IF\nX Y Z, AND, NOT,\netc. The program operates largely as a black\nbox, that is, the inner working details are hidden from the user;\nproofs are conducted by rewriting terms that posses recursive\ndefinitions, ultimately reducing the conclusion’s statement to the\nT predicate. The Boyer-Moore theorem prover\nhas been used to check the proofs of some quite deep theorems (Boyer,\nKaufmann & Moore 1995). Lemma caching, problem statement\ngeneralization, and proof planning are techniques particularly useful\nin inductive theorem proving (Bundy, Harmelen & Hesketh 1991). \nHigher-order logic differs from first-order logic in that\nquantification over functions and predicates is allowed. The statement\n“Any two people are related to each other in one way or\nanother” can be legally expressed in higher-order logic as\n∀x∀y∃RR(x,y)\nbut not in first-order logic. Higher-order logic is inherently more\nexpressive than first-order logic and is closer in spirit to actual\nmathematical reasoning. For example, the notion of set finiteness\ncannot be expressed as a first-order concept. Due to its richer\nexpressiveness, it should not come as a surprise that implementing an\nautomated theorem prover for higher-order logic is more challenging\nthan for first-order logic. This is largely due to the fact that\nunification in higher-order logic is more complex than in the\nfirst-order case: unifiable terms do not always posess a most general\nunifier, and higher-order unification is itself undecidable. Finally,\ngiven that higher-order logic is incomplete, there are always proofs\nthat will be entirely out of reach for any automated reasoning\nprogram. \nMethods used to automate first-order deduction can be adapted to\nhigher-order logic. TPS (Andrews et al. 1996, Andrews et\nal. 2006) is a theorem proving system for higher-order logic that\nuses Church’s typed λ-calculus as its logical representation\nlanguage and is based on a connection-type deduction mechanism that\nincorporates Huet’s unification algorithm (Huet 1975). As a sample of\nthe capabilities of TPS, the program has proved automatically that a\nsubset of a finite set is finite, the equivalence among several\nformulations of the Axiom of Choice, and Cantor’s Theorem that a set\nhas more subsets than members. The latter was proved by the program by\nasserting that there is no onto function from individuals to sets of\nindividuals, with the proof proceeding by a diagonal argument. HOL\n(Gordon & Melham 1993) is another higher-order proof development\nsystem primarily used as an aid in the development of hardware and\nsoftware safety-critical systems. HOL is based on the LCF approach to\ninteractive theorem proving (Gordon, Milner & Wadsworth 1979), and\nit is built on the strongly typed functional programming language ML.\nHOL, like TPS, can operate in automatic and interactive mode.\nAvailability of the latter mode is welcomed since the most useful\nautomated reasoning systems may well be those which place an emphasis\non interactive theorem proving (Farmer, Guttman & Thayer 1993) and\ncan be used as assistants operating under human guidance. (Harrison\n2000) discusses the verification of floating-point algorithms and the\nnon-trivial mathematical properties that are proved by HOL Light under\nthe guidance of the user. Isabelle (Paulson 1994) is a generic,\nhigher-order, framework for rapid prototyping of deductive systems.\nObject logics can be formulated within Isabelle’s metalogic by using\nits many syntactic and deductive tools. Isabelle also provides some\nready-made theorem proving environments, including Isabelle/HOL,\nIsabelle/ZF and Isabelle/FOL, which can be used as starting points for\napplications and further development by the user (Paulson 1990, Nipkow\n& Paulson 2002). Isabelle/ZF has been used to prove equivalent\nformulations of the Axiom of Choice, formulations of the Well-Ordering\nPrinciple, as well as the key result about cardinal arithmetic that,\nfor any infinite cardinal κ, κ · κ = κ\n(Paulson & Grabczewski 1996). \nTo help prove higher-order theorems and discharge goals arising in\ninteractive proofs, the user can ask Isabelle/HOL to invoke external\nfirst-order provers through Sledgehammer (Paulson 2010), a subsystem\naimed at combining the complementary capabilities of automated\nreasoning systems of different types, including SMT solvers (see\n4.2 SAT Solvers, in this article; Blanchette et al. 2013).\nLEO-II (Benzmüller et al. 2015) is also a resolution-based\nautomated theorem prover for higher-order logic that has been applied\nin a wide array of problems, most notably in the automation of\nGödel’s ontological proof of God’s existence (see 4.6 Logic\nand Philosophy, in this article).  \nNon-classical logics (Haack 1978) such as modal logics, intuitionsitic\nlogic, multi-valued logics, autoepistemic logics, non-monotonic\nreasoning, commonsense and default reasoning, relevance logic,\nparaconsistent logic, and so on, have been increasingly gaining the\nattention of the automated reasoning community. One of the reasons has\nbeen the natural desire to extend automated deduction techniques to\nnew domains of logic. Another reason has been the need to mechanize\nnon-classical logics as an attempt to provide a suitable foundation\nfor artificial intelligence. A third reason has been the desire to\nattack some problems that are combinatorially too large to be handled\nby paper and pencil. Indeed, some of the work in automated\nnon-classical logic provides a prime example of automated reasoning\nprograms at work. To illustrate, the Ackerman Constant Problem asks\nfor the number of non-equivalent formulas in the relevance logic R.\nThere are actually 3,088 such formulas (Slaney 1984) and the number\nwas found by “sandwiching” it between a lower and an upper\nlimit, a task that involved constraining a vast universe of\n20400 20-element models in search of those models that\nrejected non-theorems in R. It is safe to say that this result would\nhave been impossible to obtain without the assistance of an automated\nreasoning program. \nThere have been three basic approaches to automate the solving of\nproblems in non-classical logic (McRobie 1991). One approach has been,\nof course, to try to mechanize the non-classical deductive calculi.\nAnother has been to simply provide an equivalent formulation of the\nproblem in first-order logic and let a classical theorem prover handle\nit. A third approach has been to formulate the semantics of the\nnon-classical logic in a first-order framework where resolution or\nconnection-matrix methods would apply. (Pelletier et al.\n2017) describes an automated reasoning system for a paraconsistent\nlogic that takes both “indirect” approaches, the\ntranslational and the truth-value approach, to prove its theorems. \nModal logics find extensive use in computing science as logics of\nknowledge and belief, logics of programs, and in the specification of\ndistributed and concurrent systems. Thus, a program that automates\nreasoning in a modal logic such as K, K4, T, S4, or S5 would have\nimportant applications. With the exception of S5, these logics share\nsome of the important metatheoretical results of classical logic, such\nas cut-elimination, and hence cut-free (modal) sequent calculi can be\nprovided for them, along with techniques for their automation.\nConnection methods (Andrews 1981, Bibel 1981) have played an important\nrole in helping to understand the source of redundancies in the search\nspace induced by these modal sequent calculi and have provided a\nunifying framework not only for modal logics but also for\nintuitionistic and classical logic as well (Wallen 1990). Current\nefforts to automate modal logic reasoning revolve around the\ntranslational approach mentioned above, namely to embed modal logic\ninto classical logic and then use an existing automated reasoning\nsystem for the latter to prove theorems of the former.\n(Benzmüller & Paulson 2013) shows how to embed quantified modal\nlogic into simple type theory, proves the soundness and completeness\nof the embedding, and demonstrates with simple experiments how\nexisting higher-order theorem provers can be used to automate proofs\nin modal logic. The approach can be extended to higher-order modal\nlogic as well (Benzmüller & Paleo 2015). \nThere are different ways in which intuitionsitic logic can be\nautomated. One is to directly implement the intuitionistic versions of\nGentzen’s sequent and natural deduction calculi, LJ and NJ\nrespectively. This approach inherits the stronger normalization\nresults enjoyed by these calculi allowing for a more compact\nmechanization than their classical counterparts. Another approach at\nmechanizing intuitionistic logic is to exploit its semantic\nsimilarities with the modal logic S4 and piggy back on an automated\nimplementation of S4. Automating intuitionistic logic has applications\nin software development since writing a program that meets a\nspecification corresponds to the problem of proving the specification\nwithin an intuitionistic logic (Martin-Löf 1982). A system that\nautomated the proof construction process would have important\napplications in algorithm design but also in constructive mathematics.\nNuprl (Constable et al. 1986) is a computer system supporting\na particular mathematical theory, namely constructive type theory, and\nwhose aim is to provide assistance in the proof development process.\nThe focus is on logic-based tools to support programming and on\nimplementing formal computational mathematics. Over the years the\nscope of the Nuprl project has expanded from\n“proofs-as-programs” to “systems-as-theories”.\nSimilar in spirit and based on the Curry-Howard isomorphism, the Coq\nsystem formalizes its proofs in the Calculus of Inductive\nConstructions, a λ-calculus with a rich system of types\nincluding dependent types (Coquand & Huet 1988, Coquand &\nPaulin-Mohring 1988). Like Nuprl, Coq is designed to assist in the\ndevelopment of mathematical proofs as well as computer programs from\ntheir formal specifications. \nLogic programming, particularly represented by the language\nProlog (Colmerauer et al. 1973), is probably\nthe most important and widespread application of automated theorem\nproving. During the early 1970s, it was discovered that logic could be\nused as a programming language (Kowalski 1974). What distinguishes\nlogic programming from other traditional forms of programming is that\nlogic programs, in order to solve a problem, do not explicitly state\nhow a specific computation is to be performed; instead, a\nlogic program states what the problem is and then delegates\nthe task of actually solving it to an underlying theorem prover. In\nProlog, the theorem prover is based on a refinement of resolution\nknown as SLD-resolution. SLD-resolution is a\nvariation of linear input resolution that incorporates a special rule\nfor selecting the next literal to be resolved upon; SLD-resolution\nalso takes into consideration the fact that, in the computer’s memory,\nthe literals in a clause are actually ordered, that is, they form a\nsequence as opposed to a set. A Prolog program\nconsists of clauses stating known facts and rules. For example, the\nfollowing clauses make some assertions about flight connections: \nThe clause flight(toronto, london) is a fact while\nflight(X,Y) :– flight(X,Z)\n,\nflight(Z,Y) is a rule, written by convention as a\nreversed conditional (the symbol “:–” means\n“if”; the comma means “and”; terms starting in\nuppercase are variables). The former states that there is flight\nconnection between Toronto and London; the latter states that there is\na flight between cities X and Y if, for some city\nZ, there is a flight between X and Z and\none between Z and Y. Clauses in Prolog programs are\na special type of Horn clauses having precisely one positive literal:\nFacts are program clauses with no negative literals\nwhile rules have at least one negative literal. (Note\nthat in standard clause notation the program rule in the previous\nexample would be written as flight(X,Y) ∨\n~flight(X,Z) ∨ ~flight(Z,Y).)\nThe specific form of the program rules is to effectively express\nstatements of the form: “If these conditions over here are\njointly met then this other fact will follow”. Finally, a\ngoal is a Horn clause with no positive literals. The\nidea is that, once a Prolog program Π has been written, we can then\ntry to determine if a new clause γ, the goal, is entailed by\nΠ, Π ⊨ γ; the Prolog prover does this by attempting\nto derive a contradiction from Π ∪ {~γ}. We should remark\nthat program facts and rules alone cannot produce a contradiction; a\ngoal must enter into the process. Like input resolution,\nSLD-resolution is not refutation complete for first-order logic but it\nis complete for the Horn logic of Prolog programs. The fundamental\ntheorem: If Π is a Prolog program and γ is the goal clause\nthen Π ⊨ γ iff Π ∪ {~γ} ⊢\n[ ] by SLD-resolution (Lloyd 1984). \nFor instance, to find out if there is a flight from Toronto to Rome\none asks the Prolog prover to see if the clause flight(toronto, rome)\nfollows from the given program. To do this, the prover adds\n~flight(toronto,rome) to the program clauses and attempts to derive\nthe empty clause, [ ], by SLD-resolution: \nThe conditional form of rules in Prolog programs adds to their\nreadability and also allows reasoning about the underlying refutations\nin a more friendly way: To prove that there is a flight between\nToronto and Rome, flight(toronto,rome), unify this clause with the\nconsequent flight(X,Y) of the fourth clause in the\nprogram which itself becomes provable if both\nflight(toronto,Z) and flight(Z,rome) can be proved.\nThis can be seen to be the case under the substitution {Z\n← london} since both flight(toronto,london) and\nflight(london,rome) are themselves provable. Note that the theorem\nprover not only establishes that there is a flight between Toronto and\nRome but it can also come up with an actual itinerary,\nToronto-London-Rome, by extracting it from the unifications used in\nthe proof. \nThere are at least two broad problems that Prolog must address in\norder to achieve the ideal of a logic programming language. Logic\nprograms consist of facts and rules describing what is true; anything\nthat is not provable from a program is deemed to be false. In regards\nto our previous example, flight(toronto,\nboston) is not true since this literal cannot be deduced from\nthe program. The identification of falsity with non-provability is\nfurther exploited in most Prolog implementations by incorporating an\noperator, not, that allows programmers to explicitly\nexpress the negation of literals (or even subclauses) within a\nprogram. By definition, not l succeeds if the literal\nl itself fails to be deduced. This mechanism, known as\nnegation-by-failure, has been the target of\ncriticism. Negation-by-failure does not fully capture the standard\nnotion of negation and there are significant logical differences\nbetween the two. Standard logic, including Horn logic, is monotonic\nwhich means that enlarging an axiom set by adding new axioms simply\nenlarges the set of theorems derivable from it; negation-by-failure,\nhowever, is non-monotonic and the addition of new\nprogram clauses to an existing Prolog program may cause some goals to\ncease from being theorems. A second issue is the control\nproblem. Currently, programmers need to provide a fair amount\nof control information if a program is to achieve acceptable levels of\nefficiency. For example, a programmer must be careful with the order\nin which the clauses are listed within a program, or how the literals\nare ordered within a clause. Failure to do a proper job can result in\nan inefficient or, worse, non-terminating program. Programmers must\nalso embed hints within the program clauses to prevent the prover from\nrevisiting certain paths in the search space (by using the\ncut operator) or to prune them altogether (by using\nfail). Last but not least, in order to improve their\nefficiency, many implementations of Prolog do not implement\nunification fully and bypass a time-consuming yet critical\ntest—the so-called\noccurs-check—responsible for checking the\nsuitability of the unifiers being computed. This results in an unsound\ncalculus and may cause a goal to be entailed by a Prolog program (from\na computational point of view) when in fact it should not (from a\nlogical point of view). \nThere are variations of Prolog intended to extend its scope. By\nimplementing a model elimination procedure, the Prolog Technology\nTheorem Prover (PTTP) (Stickel 1992) extends Prolog into full\nfirst-order logic. The implementation achieves both soundness and\ncompleteness. Moving beyond first-order logic, λProlog (Miller\n& Nadathur 1988) bases the language on higher-order constructive\nlogic. \nThe problem of determining the satisfiability of logic formulas has\nreceived much attention by the automated reasoning community due to\nits important applicability in industry. A propositional formula is\nsatisfiable if there is an assignment of truth-values\nto its variables that makes the formula true. For example, the\nassignment (P ← true, Q ← true, R\n← false) does not make (P ∨R) &\n~Q true but (P ← true, Q ← false,\nR ← false) does and, hence, the formula is satisfiable.\nDetermining whether a formula is satisfiable or not is called the\nBoolean Satisfiability Problem—SAT for\nshort—and for a formula with n variables SAT can be\nsettled thus: Inspect each of the 2n possible\nassignments to see if there is at least one assignment that satisfies\nthe formula, i.e. makes it true. This method is clearly complete: If\nthe original formula is satisfiable then we will eventually find one\nsuch satisfying assignment; but if the formula is contradictory (i.e.\nnon-satisfiable), we will be able to determine this too. Just as\nclearly, and particularly in this latter case, this search takes an\nexponential amount of time, and the desire to conceive more efficient\nalgorithms is well justified particularly because many computing\nproblems of great practical importance such as graph-theoretic\nproblems, network design, storage and retrieval, scheduling, program\noptimization, and many others (Garey & Johnson 1979) can be\nexpressed as SAT instances, i.e. as the SAT question of some\npropositional formula representing the problem. Given that SAT is\nNP-complete (Cook 1971) it is very unlikely that a polynomial\nalgorithm exists for it; however, this does not preclude the existence\nof sufficiently efficient algorithms for particular cases of SAT\nproblems. \nThe Davis-Putnam-Logemann-Loveland (DPLL) algorithm\nwas one of the first SAT search algorithms (Davis & Putnam 1960;\nDavis, Logemman & Loveland 1962) and is still considered one of the\nbest complete SAT solvers; many of the complete SAT procedures in\nexistence today can be considered optimizations and generalizations of\nDPLL. In essence, DPLL search procedures proceed by considering ways\nin which assignments can be chosen to make the original formula true.\nFor example, consider the formula in CNF \nSince P is a conjunct, but also a unit clause, P\nmust be true if the entire formula is to be true. Moreover, the value\nof ~P does not contribute to the truth of ~P ∨\nQ ∨ R and P ∨ ~S is true\nregardless of S. Thus, the whole formula reduces to \nSimilarly, ~Q must be true and the formula further reduces\nto \nwhich forces R to be true. From this process we can recover\nthe assignment (P ← true, Q ← false,\nR ← true, S ← false) proving that the\noriginal formula is satisfiable. A formula may cause the algorithm to\nbranch; the search through a branch reaches a dead end the moment a\nclause is deemed false—a conflicting\nclause—and all variations of the assignment that has\nbeen partially constructed up to this point can be discarded. To\nillustrate: \nHence, the formula is satisfiable by the existence of (P\n← false, Q ← true, R ← true). DPLL\nalgorithms are made more efficient by strategies such as term\nindexing (ordering of the formula variables in an\nadvantageous way), chronological backtracking\n(undoing work to a previous branching point if the process leads to a\nconflicting clause), and conflict-driven learning\n(determining the information to keep and where to backtrack). The\ncombination of these strategies results in a large prune of the search\nspace; for a more extensive discussion the interested reader is\ndirected to Zhang & Malik 2002. \nA quick back-envelope calculation reveals the staggering computing\ntimes of (algorithms for) SAT-type problems represented by formulas\nwith as little as, say, 60 variables. To wit: A problem represented as\na Boolean formula with 10 variables that affords a linear solution\ntaking one hundredth of a second to complete would take just four\nhundredths and six hundredths of a second to complete if the formula\nhad instead 40 and 60 variables respectively. In dramatic contrast, if\nthe solution to the problem were exponential (say\n2n) then the times to complete the job for 10, 40\nand 60 variables would be respectively one thousandth of a second, 13\ndays, and 365 centuries. It is a true testament to the ingenuity of\nthe automated reasoning community and the power of current SAT-based\nsearch algorithms that real-world problems with thousands of variables\ncan be handled with reasonable efficency. Küchlin & Sinz 2000\ndiscuss a SAT application in the realm of industrial automotive\nproduct data management where 18,000 (elementary) Boolean formulas and\n17,000 variables are used to express constraints on orders placed by\ncustomers. As another example, Massacci & Marraro 2000 discuss\nan application in logical cryptanalysis, that is, the verification of\nproperties of cryptographic algorithms expressed as SAT problems. They\ndemonstrate how finding a key with a cryptographic attack is analogous\nto finding a model—assignment—for a Boolean formula; the\nformula in their application encodes the commercial version of the U.S\nData Encryption Standard (DES) with the encoding requiring 60,000\nclauses and 10,000 variables. \nAlthough SAT is conceptually very simple, its inner nature is not well\nunderstood—there are no criteria that can be generally applied\nto answer as to why one SAT problem is harder than another. It should\nthen come as no surprise that algorithms that tend to do well on some\nSAT instances do not perform so well on others, and efforts are being\nspent in designing hybrid algorithmic solutions that combine the\nstrength of complementary approaches—see Prasad, Biere &\nGupta 2005 for an application of this hybrid approach in the\nverification of hardware design. \nRecent advances in SAT hybrid strategies coupled with supercomputing\npower has allowed a team of three computing scientists to solve the\nBoolean Pythagorean Triples Problem, a long-standing open question in\nRamsey Theory: Can the set {1, 2,...} of natural numbers be divided\ninto two parts with no part containing a triple\n(a, b, c) such that a2\n+ b2 = c2? Heule, Kullmann\n& Marek 2016 proved that this cannot be done by showing that the\nset {1, 2, … , n} can be so partitioned for n\n= 7824 but that this is impossible for n ≥\n7825. Expressing this deceptively simple question as a SAT problem\nrequired close to 38,000 clauses and 13,000 variables with about half\nof these going to represent that the problem is satisfiable when n =\n7824 and the other half to represent that it is not when n = 7825; of\nthe two, proving the latter was far more challenging since it demanded\na proof of unsatisfiability, i.e. that no such partition exists. A\nnaïve brute-force approach considering all 27825\npossible two-part partitions was clearly out of the question and the\nproblem was attacked by using “clever” algorithms within a\nmulti-stage SAT-based framework for solving hard problems in\ncombinatorics, consisting of five phases: Encode (encoding\nthe problem as SAT formulas), Transform (optimizing the\nencoding using clause elimination and symmetry breaking\ntechniques), Split (dividing the problem effectively into\nsubproblems using splitting heuristics), Solve (searching for\nsatisfying assignments or their lack thereof using fast processing),\nand Validate (validating the results of the earlier\nphases). Of special importance was the application\nof cube-and-conquer, a hybrid SAT strategy\nparticularly effective for hard combinatorial problems. The strategy\ncombines look-ahead with conflict-driven\nclause-learning (CDCL), with the former\naiming to construct small binary search trees using global heuristics\nand the latter aiming to find short refutations using local\nheuristics. \nAfter splitting the problem into 106 hard subproblems\n(known as “cubes”), these were handed down to 800 cores\nworking in parallel on a Stampede supercomputer which, after\n2 days of further splitting and CDCL clause-crunching, settled the\nquestion and delivered a 200-terabyte proof validating the work. After\ndeservedly celebrating this significant accomplishment of automated\nreasoning, and after entertaining all the new applications that the\nenhanced SAT method would afford (particularly in the areas of\nhardware and software verification), we should then ask some questions\nthat are of especial importance to mathematicians: Is there a more\ninsightful way to establish this result that would involve more\ntraditional and intellectually satisfying mathematical proof methods?\nOr, as far as increasing our understanding of a given field\n(combinatorics in this case), what is the value of settling a question\nwhen no human can inspect the proof and hence get no insight from it?\nEven the team responsible for the result admits that “the proofs\nof unsatisfiability coming from SAT solvers are, from a human point of\nview, a giant heap of random information (no direct understanding is\ninvolved)”. The conjecture has been settled but we basically\nhave no underlying idea what makes 7825 so special. Perhaps the real\nvalue to be drawn from these considerations is that they lead us to\nthink about the deeper question: What is it about the structure of a\nspecific problem that makes it amenable to standard mathematical\ntreatment as opposed to requiring a mindless brute-force approach?\nWhile this question is being contemplated, SAT may provide the best line of attack on certain mathematical problems. \nThe DPLL search procedure has been extended to quantified logic. MACE\nis a popular program based on the DPLL algorithm that searches for\nfinite models of first-order formulas with equality. As an example\n(McCune 2001), to show that not all groups are commutative one can\ndirect MACE to look for a model of the group axioms that also\nfalsifies the commutation law or, equivalently, to look for a model\nof: \nMACE finds a six-element model of these axioms, where · is\ndefined as: \nand where i are defined as: \nThis example also illustrates, once again, the benefits of using an automated\ndeduction system: How long would have taken the human researcher to\ncome up with the above or a similar model? For more challenging\nproblems, the program is being used as a practical complement to the\nresolution-based theorem prover Prover9 (formerly Otter), with Prover9\nsearching for proofs and MACE jointly looking for (counter) models. To\nfind such models, MACE converts the first-order problem into a set of\n\"flattened\" clauses which, for increasing model sizes, are\ninstantiated into propositional clauses and solved as a SAT problem.\nThe method has been implemented in other automated reasoning systems\nas well, most notably in the Paradox model finder where the MACE-style\napproach has been enhanced by four additional techniques resulting in\nsome significant efficiency improvements (Claessen & Sörensson\n2003): term definitions (to reduce the number of variables in\nflattened clauses), static symmetric reduction (to reduce the number\nof isomorphic models), sort inference (to apply symmetric reduction at\na finer level) and incremental SAT (to reuse search information\nbetween consecutive model sizes). The strategy of pairing the\ncomplementary capabilities of separate automated reasoning systems has\nbeen applied to higher-order logic too as exemplified by Nitpick, a\ncounterexample generator for Isabelle/HOL (Blanchette & Nipkow\n2010). Brown 2013 describes a theorem proving procedure for\nhigher-order logic that uses SAT-solving to do most of the work; the\nprocedure is a complete, cut-free, ground refutation calculus that\nincorporates restrictions on instantiations and has been implemented\nin the Satallax theorem prover (Brown 2012). \nAn approach of great interest at solving SAT problems in first-order\nlogic is Satisfiability Modulo Theory\n(SMT) where the interpretation of symbols in the\nproblem’s formulation is constrained by a background\ntheory. For example, in linear arithmetic the function\nsymbols are restricted to + and −. As another example, in the\nextensional theory of arrays (McCarthy 1962) the array function\nread(a, i) returns the value of the array\na at index i, and write(a,\ni, x) returns the array identical to a but\nwhere the value of a at i is x. More\nformally, \nIn the context of these axioms, an SMT solver would attempt to\nestablish the satisfiability (or, dually, the validity) of a given\nfirst-order formula, or thousands of formulas for that matter, such\nas \n(Ganzinger et al. 2004) discusses an approach to SMT called\nDPLL(T) consisting of a general\nDPLL(X) engine that works in conjunction with a solver\nSolverT for background theory T. Bofill\net al. (2008) present the approach in the setting of the theory of\narrays, where the DPLL engine is responsible for enumerating\npropositional models for the given formula whereas\nSolverT checks whether these models are consistent\nwith the theory of arrays. Their approach is sound and complete, and\ncan be smoothly extended to multidimensional arrays. \nSMT is particularly successful in verification applications, most\nnotably software verification. Having improved the efficiency of SAT\nsolvers with SMT, the effort is now on designing more efficient SMT\nsolvers (de Moura 2007). \nTo prove automatically even the simplest mathematical facts requires a\nsignificant amount of domain knowledge. As a rule, automated theorem\nprovers lack such rich knowledge and attempt to construct proofs from\nfirst principles by the application of elementary deduction rules.\nThis approach results in very lengthy proofs (assuming a proof is\nfound) with each step being justified at a most basic logical level.\nLarger inference steps and a significant improvement in mathematical\nreasoning capability can be obtained, however, by having a theorem\nprover interact with a computer algebra system, also known as a\nsymbolic computation system. A computer algebra\nsystem is a computer program that assists the user with the\nsymbolic manipulation and numeric evaluation of mathematical\nexpressions. For example, when asked to compute the improper\nintegral \na competent computer algebra system would quickly reply with the\nanswer \nEssentially, the computer algebra system operates by taking the input\nexpression entered by the user and successively applies to it a series\nof transformation rules until the result no longer changes (see the\nsection Term Rewriting in this article for more details).\nThese transformation rules encode a significant amount of domain\n(mathematical) knowledge making symbolic systems powerful tools in the\nhands of applied mathematicians, scientists, and engineers trying to\nattack problems in a wide variety of fields ranging from calculus and\nthe solving of equations to combinatorics and number theory. \nProblem solving in mathematics involves the interplay of deduction and\ncalculation, with decision procedures being a reminder of the fuzzy\ndivision between the two; hence, the integration of deductive and\nsymbolic systems, which we coin here as Deductive Computer\nAlgebra (DCA), is bound to be a fruitful combination.\nAnalytica (Bauer, Clarke & Zhao 1998) is a theorem prover built on\ntop of Mathematica, a powerful and popular computer algebra system.\nBesides supplying the deductive engine, Analytica also extends\nMathematica’s capabilities by defining a number of rewrite\nrules—more precisely, identities about summations and\ninequalities—that are missing in the system, as well as\nproviding an implementation of Gosper’s algorithm for finding closed\nforms of indefinite hypergeometric summations. Equipped with this\nextended knowledge, Analytica can prove semi-automatically some\nnontrivial theorems from real analysis, including a series of lemmas\ndirectly leading to a proof of the Bernstein Approximation Theorem.\nHere is the statement of the theorem simply to give the reader a sense\nof the level of the mathematical richness we are dealing with: \nTo be frank, the program is supplied with key information to establish\nthe lemmas that lead to this theorem but the amount and type of\ndeductive work done by the program is certainly nontrivial. (Clarke\n& Zhao 1994) provides examples of fully automated proofs using\nproblems in Chapter 2 of Ramanujan’s Notebooks (Berndt 1985)\nincluding the following example that the reader is invited to try.\nShow that: \nwhere A0=1,\nAn+1=3An+1 and\nφ(x,n) is Ramanujan’s abbreviation for \nAnalytica’s proof of this identity proceeds by simplifying both the\nleft- and right-hand sides of the equality and showing that both sides\nreduce to the same expression, −Hn +\nHAr. The simplification uses the added summation\nidentities mentioned before as well as some elementary properties of\nthe harmonic numbers, \nThe resulting proof has 28 steps (some of which are nontrivial) taking\nabout 2 minutes to find. \nKerber, Kohlhase & Sorge 1998 use the Ωmega planning\nsystem as the overall way to integrate theorem proving and symbolic\ncomputation. In Harrison & Théry 1998, we find an example\nof the integration of a higher-order logic theorem proving system\n(HOL) with a computer algebra system (Maple). \nTheir great power notwithstanding, symbolic algebra systems do not\nenforce the same level of rigor and formality that is the essence of\nautomated deduction systems. In fact, the mathematical semantics of\nsome of the knowledge rules in most algebra systems is not entirely\nclear and are, in cases, logically unsound (Harrison & Théry\n1998). The main reason for this is an over-aggressiveness to provide\nthe user with an answer in a timely fashion at whatever cost,\nbypassing the checking of required assumptions even if it means\nsacrificing the soundness of the calculation. (This is strongly\nreminiscent of most Prolog implementations that bypass the so-called\n“occurs-check” also abandoning logical soundness in the\nname of efficiency.) This serious problem opens the opportunity for a\ndeduction system to provide a service to the computer algebra system:\nUse its deductive capabilities to verify that the computer algebra’s\ncomputational steps meet the required assumptions. There is a catch in\nthis, however: For sufficiently large calculation steps, verifying is\ntantamount to proving and, to check these steps, the deduction system\nmay well need the assistance of the very same system that is in need\nof verification! The solution to the soundness problem may then well\nrequire an extensive modification of the chosen symbolic algebra\nsystem to make it sound; an alternative approach is to develop a new\nsystem, entirely from scratch, in conjunction with the development of\nthe automated theorem prover. In either case, the resulting combined\ndeductive computer algebra system should display a much improved\nability for automated mathematical reasoning. \nAutomated reasoning has reached the level of maturity where theorem\nproving systems and techniques are being used for industrial-strength\napplications. One such application area is the formal verification of\nhardware and software systems. The cost of defects in hardware can\neasily run into the millions. In 1994, the Pentium processor was\nshipped with a defect in its floating-point unit and the subsequent\noffer by Intel to replace the flawed chip (which was taken up only by\na small fraction of all Pentium owners) cost the company close to $500\nmillion. To guard against situations like this, the practice of\ntesting chip designs is now considered insufficient and more formal\nmethods of verification have not only gained large attention in the\nmicroprocessor industry but have become a necessity. The idea behind\nformal verification is to rigorously prove with mathematical certainty\nthat the system functions as specified. Common applications to\nhardware design include formally establish that the system functions\ncorrectly on all inputs, or that two different circuits are\nfunctionally equivalent. \nDepending on the task at hand, one can draw from a number of automated\nformal verification techniques, including SAT solvers in propositional\nlogic, symbolic simulation using binary decision diagrams (BDDs),\nmodel checking in temporal logic, or conducting proofs in higher-order\nlogic. In the latter case, using an automated theorem prover like\nHOL—see Section 10—has shown to be invaluable in practice.\nProof construction in a system like HOL proceeds semi-automatically\nwith the user providing a fair amount of guidance as to how the proof\nshould proceed: The user tries to find a proof while being assisted by\nthe theorem prover which, on request, can either automatically fill in\na proof segment or verify proof steps given to it. Although some of\nthe techniques mentioned above provide decision procedures which\nhigher-order logic lacks, higher-order logic has the advantage of\nbeing very expressive. The tradeoff is justified since proving facts\nabout floating-point arithmetic requires the\nformalization of a large body of real analysis, including many\nelementary statements such as: \nThis statement from Harrison 2000 written in HOL says that if a\nfunction f is differentiable with derivative\nf′ in an interval [a, b] then a\nsufficient condition for f(x) ≤ K\nthroughout the interval is that f(x) ≤ K\nat the endpoints a, b and at all points of zero\nderivative. The result is used to determine error bounds when\napproximating transcendental functions by truncated power series.\nConducting proofs in such a “painstakingly foundational\nsystem” (Harrison 2006) has some significant benefits. First,\none achieves a high degree of assurance that the proofs are valid\nsince (admitedly lengthy) they are composed of small error-free\ndeductive steps. Second, the formalization of these elementary\nstatements and intermediate results can be reused in other tasks or\nprojects. For example, a library of formal statements and proven\nresults in floating-point division can be reused when proving other\nresults of floating-point algorithms for square roots or\ntranscendental functions. To further illustrate, different versions of\nthe square root algorithm for the Intel Itanium share many\nsimilarities and the proof of correctness for one version of the\nalgorithm can be carried over to another version after minor tweaking\nof the proof. A third benefit of using a prover like HOL is, of\ncourse, that such lengthy proofs are carried out mechanically and are\ndeductively certain; the likelihood of introducing a human error if\nthey were carried out manually would be just as certain. \nSociety is becoming increasingly dependent on software systems for\ncritical services such as safety and security. Serious adverse effects\nof malfunctioning software include loss of human life, threats to\nsecurity, unauthorized access to sensitive information, large\nfinancial losses, denial of critical services, and risk to safety. One\nway to increase the quality of critical software is to supplement\ntraditional methods of testing and validation with techniques of\nformal verification. The basic approach to formal verification is to\ngenerate a number of conditions that the software must meet and to\nverify—establish—them by mathematical proof. As with\nhardware, automated formal verification (simply formal verification,\nhereafter) is concerned with discharging these proof obligations using\nan automated theorem prover. \nThe formal verification of security protocols is an\nalmost ideal application of automated theorem proving in industry.\nSecurity protocols are small distributed programs aimed at ensuring\nthat transactions take place securely over public networks. The\nspecification of a security protocol is relatively small and well\ndefined but its verification is certainly non-trivial. We have already\nmentioned in a previous section the use of SAT-based theorem provers\nin the verification of the U.S Data Encryption Standard (DES). As\nanother example, the Mondex “electronic purse” is a smart\ncard electronic cash system that was originally developed by National\nWestminster Bank and subsequently sold to MasterCard International.\nSchmitt & Tonin 2007 describe a Java Card implementation of the\nMondex protocol for which the security properties were reformulated in\nthe Java Modeling Language (JML) following closely the original Z\nspecification. Proof of correctness was conducted using the KeY tool\n(Beckert, Hanle & Schmitt 2007), an interactive theorem proving\nenvironment for first-order dynamic logic that allows the user to\nprove properties of imperative and object-oriented sequential\nprograms. This application of automated reasoning demonstrates, in the\nwords of the authors, that “it is possible to bridge the gap\nbetween specification and implementation ensuring a fully verified\nresult”. \nDenney, Fischer & Schumann 2004 describe a system to automate the\ncertification of safety properties of data-analysis aerospace\nsoftware at NASA. Using Hoare-style program verification\ntechniques, their system generates proof obligations that are then\nhandled by an automated theorem prover. The process is not fully\nautomated, however, since many of the obligations must be simplified\nfirst in order to improve the ability of the theorem prover to solve\nthe proof tasks. For example, one such class of obligations makes a\nstatement about a matrix, r, that needs to remain symmetric\nafter updates along its diagonal have been made, and has the form: \nOriginal form:\nsymm(r) →\nsymm(diag-updates(r)) \nSimplified form (when r is 2x2): \nEven after the simplification, current theorem provers find the proof\ntask challenging. The task becomes intractable for larger matrices and\nnumber of updates (e.g. a 6×6 matrix with 36 updates) and\nfurther preprocessing and simplification on the obligation is required\nbefore the task eventually falls within the reach of state-of-art\ntheorem provers. But it is worth remarking that proofs are found\nwithout using any specific features or configuration parameters of the\ntheorem provers which would improve their chances at completing the\nproofs.  This is important since the everyday application of theorem\nprovers in industry cannot presuppose such deep knowledge of the\nprover from their users. The formal verification of software remains a\ndemanding task but it is difficult to see how the certification of\nproperties could happen without the assistance of automated deduction\nwhen one faces the humanly impossible task of establishing thousands\nof such obligations. \nIn the field of nuclear engineering, techniques of\nautomated reasoning are deemed mature enough to assist in the formal\nverification of the safety-critical software responsible for\ncontrolling a nuclear power plant’s reactor prevention systems (RPS).\nThe RPS component of the digital control system of the APR-1400\nnuclear reactor is specified using NuSCR, a formal specification\nlanguage customized for nuclear applications (Yoo, Jee & Cha 2009).\nModel checking in computation tree logic is used to check the\nspecifications for completeness and consistency. After this, nuclear\nengineers generate function block designs via a process of automatic\nsynthesis and formally verify the designs also using techniques of\nmodel checking in linear temporal logic; the techniques are also used\nto verify the equivalence of the multiple revisions and releases of\nthe design. These model-checking tools were implemented to make their\nuse as easy and intuitive as possible, in a way that did not require a\ndeep knowledge of the techniques, and used notations familiar to\nnuclear engineers. The use of automated reasoning tools not only helps\nthe design engineers to establish the desired results but it also\nraises the confidence of the government’s regulatory personnel that\nneed to approve the RPS software before the reactor can be certified\nfor operation. \nIn the spirit of Wos, Overbeek, Lusk & Boyle 1992, we pose the\nquestion: What do the following statements about different systems of\nformal logic and exact philosophy have in common? \nWe ask again, what do these results have in common? The answer is that\neach has been proved with the help of an automated reasoning program.\nHaving disclosed the answer to this question prompts a new one: How\nmuch longer would have taken to settle these open problems without the\napplication of such an automated reasoning tool?  \nThe strict implicational fragments of the logical systems S4 and S5 of\nmodal logic are known as C4 and C5, respectively, and\ntheir Hilbert-style axiomatizations presuppose condensed detachment as\ntheir sole rule of inference. With insight from Kripke’s work,\nAnderson & Belnap (1962) published the first axiomatization of C4\nusing the following 3-axiom basis, where the Polish notation\n‘Cpq’ stands for ‘p\n→ q’.  \nA question was posed sometime after: Is there a shorter such\naxiomatization for C4, using a 2-axiom basis or even a single axiom?\nUsing the automated reasoning program Otter, the authors Ernst,\nFitelson, Harris & Wos (2001) settled both questions in the\naffirmative. In fact, several 2-axiom bases were discovered of which\nthe following turned out to be shortest:  \nFurther rounds of automated reasoning work were rewarded with the\ndiscovery of a single axiom for C4; the axiom is 21 symbols long and\nit was also proved that it is the shortest such axiom:  \nTo show that each of (2) and (3) is necessary and sufficient for (1),\na circle of proofs was produced using the automated reasoning tool:\n(1) ⇒ (3) ⇒ (2) ⇒ (1). As for C5, its axiomatization\nwas originally published in a paper by Lemmon, A. Meredith, D. Meredith,\nPrior & Thomas (1957) giving several 4-, 3-, 2- and 1-axiom bases\nfor C5, including the following 3-axiom basis:  \nThe publication also included the shortest known 2-axiom bases for C5\n(actually two of them, containing 20 symbols each) but the shortest\nsingle axiom for C5 was later discovered by (Meredith and Prior 1964)\nand having 21 symbols:  \nApplying automated reasoning strategies again, Ernst, Fitelson,\nHarris & Wos 2001) discovered several new bases, including the\nfollowing 2-axiom basis of length 18 and six 1-axiom bases matching\nMeredith’s length of 21 (only one of these is given below):  \nTo show that each of (6) and (7) is necessary and sufficient for (4),\na circle of proofs was also produced with the theorem prover: (6)\n⇒ (4) ⇒ (7) ⇒ (6).  \nA charming foray into combinatory logic is presented\nin Smullyan 1985 and Glickfeld & Overbeek 1986, where we learn\nabout a certain enchanted forest inhabited by talking birds. Given any\nbirds A and B, if the name of bird B is\nspoken to bird A then A will respond with the name\nof some bird in the forest, AB, and this response to\nB from A will always be the same. Here are some\ndefinitions about enchanted birds:  \nAnd here are two facts about this enchanted forest:  \nThere have been rumors that every bird in the forest is fond of at\nleast one bird, and also that there is at least one bird that is not\nfond of any bird. The challenge to the reader now is, of course, to\nsettle these rumors using only F1 and F2, and the given definitions\n(B1)–(B3). Glickfeld & Overbeek 1986 do this in mere\nseconds with an automated reasoning system using paramodulation,\ndemodulation and subsumption. For a more challenging problem, consider\nthe additional definitions:  \nSmullyan challenges us to prove a most surprising thing about larks:\nSuppose we are not given any other information except that the forest\ncontains a lark. Then, show that at least one bird in the forest must\nbe egocentric! Below we give the salient steps in the proof found by\nthe automated reasoning system, where ‘S(x,\ny)’ stands for ‘xy’ and where\nclauses (2) and (3) are, respectively, the definition of a lark and\nthe denial of the theorem; numbers on the right are applications of\nparamodulation:  \nCloser inspection of the left and right hand sides of (18) under the\napplication of unification revealed the discovery of a 10-L\nbird, i.e. a 10-symbol bird expressed solely in terms of larks, which\nwas a strong candidate for egocentricity. This discovery was exciting\nbecause the shortest egocentric L-bird known to Smullyan was\nof length 12. A subsequent run of the automated reasoning system\nproduced a proof of this fact as well as another new significant bird:\nA possible egocentric 8-L bird! A few more runs of the system\neventually produced a 22-line proof (with terms with as many as 50\nsymbols, excluding commas and parentheses) of the fact that\n((LL)(L(LL)))(L(LL)) is\nindeed egocentric. The natural questions to ask next are, of course,\nwhether there are other 8-L egocentric birds and whether\nthere are shorter ones. The reader may want to attempt this with paper\nand pencil but, given that there are 429 such birds, it may be wiser\nto try it instead (or in conjunction) with an automated reasoning\nprogram; both approaches are explored in Glickfeld & Overbeek\n1986. For a more formal, but admittedly less colorful, introduction\nto combinatory logic and lambda-conversion the reader is referred to\nHindley & Seldin 1986.  \nFormulas in the classical equivalential calculus are\nwritten using sentential variables and a two-place function symbol,\ne, for equivalence. The calculus has two rules of inference,\ndetachment (modus ponens) and substitution; the rules can be combined\ninto the single rule of condensed detachment: Obtain tθ\nfrom e(s,t) and r where\nsθ = rθ with mgu θ. The calculus\ncan be axiomatized with the formulas:  \nWe can dispense with reflexivity since it is derivable from the other\ntwo formulas. This brings the number of axioms down to two and a\nnatural question to ask is whether there is a single axiom for the\nequivalential calculus. In 1933, Łukasiewicz found three formulas\nof length eleven that each could act as a single axiom for the\ncalculus—here’s one of them:\ne(e(x,y),e(e(z,y),e(x,z)))—and\nhe also showed that no shorter single axiom existed. Over time, other\nsingle axioms also of length eleven were found and the list kept\ngrowing with additions by Meredith, Kalman and Peterson to a total of\n14 formulas of which 13 were known to be single axioms and one formula\nwith a yet undetermined status: the formula XCB =\ne(x, e(e(e(x,\ny), e(z, y)), z)).\n(Actually, the list grew to 18 formulas but Wos, Winker, Veroff,\nSmith & Henschen 1983 reduced it to 14.) Resisting the intense\nstudy of various researchers, it remained as an open question for many\nyears whether the 14th formula, XCB, was a single axiom for\nthe equivalential calculus (Peterson 1977). One way to answer the\nquestion in the affirmative would be to show that at least one of the\n13 known single axioms is derivable from XCB alone; another\napproach would be to derive from XCB the 3-axiom set\n(E1)–(E3). While Wos, Ulrich & Fitelson 2002 take shots at\nthe former, their line of attack concentrates on the latter with the\nmost challenging task being the proving of symmetry. Working with the\nassistance of a powerful automated reasoning program, Otter, they\nconducted a concerted, persistent and very aggressive assault on the\nopen question. (Their article sometimes reads like a military briefing\nfrom the front lines!) For simpler problems, proofs can be found by\nthe reasoning program automatically; deeper and more challenging ones\nlike the one at hand require the guidance of the user. The relentless\napplication of the reasoning tool involved much guidance in the\nsetting of lemmas as targets and the deployment of an arsenal of\nstrategies, including the set of support, forward and backward\nsubsumption, lemma adjunction, formula complexity, hints strategy,\nratio strategy, term avoidance, level saturation, and others. After\nmuch effort and CPU time, the open question finally succumbed to the\ncombined effort of man and machine and a 61-step proof of symmetry was\nfound, followed by one for transitivity after 10 more applications of\ncondensed detachment. Subsequent runs of the theorem prover using\ndemodulation blocking and the so-called cramming strategy delivered\nshorter proofs. Here are the last lines of their 25-step proof which\nin this case proves transitivity first followed by symmetry:  \nWith an effective methodology and a strategy that included the\nassistance of an automated reasoning program in a crucial way, the\nsearch for shortest single axioms for the equivalent calculus came to\nan end.  \nFitelson & Zalta 2007, Oppenheimer & Zalta 2011, and Alama,\nOppenheimer, & Zalta 2015 describe several applications of\nautomated reasoning in computational metaphysics. By\nrepresenting formal metaphysical claims as axioms and premises in an\nautomated reasoning environment using programs like Prover9, Mace4,\nthe E-prover system and Paradox, the logical status of metaphysical\narguments is investigated. After the suitable formalization of axioms\nand premises, the model finder program Mace4 is used to help verify\ntheir consistency. Then, using Prover9, proofs are automatically\ngenerated for a number of theorems of the Theory of Plato’s Forms,\ntwenty five fundamental theorems of the Theory of Possible Worlds, the\ntheorems described in Leibniz’s unpublished paper of 1690 and in his\nmodal metaphysics, and a fully automated construction of Saint\nAnselm’s Ontological Argument. In the latter application, Saint Anselm\nis understood in Oppenheimer & Zalta 2011 as having found a way of\ninferring God’s existence from His mere being as opposed to inferring\nGod’s actuality from His mere possibility. This allows for a\nformalization that is free of modal operators, involving an underlying\nlogic of descriptions, three non-logical premises, and a definition of\nGod. Here are two key definitions in the formalization, as inputted\ninto Prover9, that helped express the concept of God:  \nPart of the challenge when representing in Prover9 these and other\nstatements from axiomatic metaphysics was to circumvent some of the\nprover’s linguistic limitations. For example, Prover9 does not have\ndefinite descriptions so statements of this kind as well as\nsecond-order concepts had to be expressed in terms of Prover9’s\nexisting first-order logic. But the return is worth the investment\nsince Prover9 not only delivered a proof\nof Ex1(e,g)—there is one and only one\nGod—but does so with an added bonus. A close inspection of the\noutput provides yet another example of an automated theorem prover\n\"outreasoning\" its users, revealing that some of the logical machinery\nis actually redundant: The proof can be constructed only using two of\nthe logical theorems of the theory of descriptions (called \"Theorem 2\"\nand \"Theorem 3\" in their article), one of the non-logical premises\n(called \"Premise 2\"), and the definition of God. We cannot help but to\ninclude here Prover9’s shorter proof, written in the more elegant\nnotation of standard logic (from Oppenheimer & Zalta 2011):  \nIn the same tradition as St. Anselm’s, Gödel also provided an\nontological proof of God’s existence (Gödel 1970, Scott 1972). An\nimportant difference between the two is Gödel’s use of modal\noperators to represent metaphysical possibility and necessity and, of\ncourse, his use of symbolic logic for added reasoning precision. In\nhis proof, Gödel begins by framing the concept of “positive\nproperty” using two axioms, and he introduces a definition\nstating that “A God-like being possesses all positive\nproperties”. This is enough logical machinery to prove as a\ntheorem the possibility of God’s existence,\n◊∃xG(x); three more axioms and\ntwo additional definitions allow Gödel to further his proof to\nestablish not only that God exists,\n∃xG(x), but that this is so by\nnecessity, □∃xG(x).\nGödel’s proof is in the formalism of higher-order modal logic\n(HOML) using modal operators and quantification over properties.\nGödel never published his proof but he shared it with Dana Scott\nwho produced the version presented below, which is taken from\n(Benzmüller & Paleo 2014) along with its English annotation to\naid the reader with its intended interpretation:  \nAxiom A1\n\n∀ϕ[P(~ϕ) ≡ ~P(ϕ)]\n\n  Either a property or its negation is positive, but not\nboth) \nAxiom A2\n\n∀ϕ∀ψ[(P(ϕ) ∧ □∀x[ϕ(x) → ψ(x)]) ⊃ P(ψ)]\n\n  A property necessarily implied by a positive property\nis positive \nTheorem T1\n\n∀ϕ[P(ϕ) ⊃ ◊∃xϕ(x)]\n\n  Positive properties are possibly exemplified \nDefinition D1\nG(x) ≡ ∀ϕ[P(ϕ) ⊃ ϕ(x)]\n\n  A God-like being possesses all positive\nproperties \nAxiom A3\nP(G)\n\n  The property of being God-like is positive \nCorollary C\n\n◊∃xG(x)\n\n  Possibly, God exists \nAxiom A4\n\n∀ϕ[P(ϕ) ⊃ □P(ϕ)]\n\n  Positive properties are necessarily positive \nDefinition D2\nϕ ess\nx  ≡  ϕ(x) ∧\n∀ψ(ψ(x) ⊃\n□∀y(ϕ(y) ⊃\nψ(y)))\n\n  An essence of an individual is a property possessed by\nit and\n\n  necessarily implying any of its properties \nTheorem T2\n\n∀x[G(x) ⊃ G ess\nx]\n\n  Being God-like is an essence of any God-like\nbeing \nDefinition D3\nNE(x) ≡ ∀ϕ[ϕ\ness x ⊃ □∃yϕ(y)]\n\n  Necessary existence of an individual is the necessary\n\n   exemplification of all its essences \nAxiom A5\nP(NE)\n\n  Necessary existence is a positive property \nTheorem T3\n\n□∃xG(x)\n\n  Necessarily, God exists \nThe proof has recently been analysed to an unprecedented degree of\ndetail and precision by Benzmüller & Paleo 2014 with the help\nof automated theorem provers. A major challenge faced by these authors\nwas the lack of a HOML-based theorem prover that could carry out the\nwork but this was circumvented by embedding the logic into the\nclassical higher-order logic (HOL) already offered by existing theorem\nprovers like LEO-II, Satallax and the countermodel finder Nitpick.\nDetails of the syntactic and semantic embedding are given in their\npaper and it consists of encoding HOML formulas as HOL predicates via\nmappings, expansions, and βη-conversions. The\nmapping associates HOML types α, terms\nsα, and logical operators θ with\ncorresponding HOL “raised” types\n⌈α⌉, type-raised terms\n⌈sα⌉, and type-raised logical\noperators θ•. If μ\nand ο are, respectively, the types of individuals and\nBooleans then ⌈μ⌉ = μ and\n⌈ο⌉ = σ\nwhere σ is shorthand\nfor ι → ο\nwith ι as the type of possible worlds; as for function\ntypes, ⌈β→γ⌉ =\n⌈β⌉→⌈γ⌉. For\ntype-raised terms, ⌈sα⌉\nis defined inductively on the structure\nof sα as the following example\nillustrates: \nType-raised logical connectives, θ•, are defined below where r is a new constant symbol in HOL associated with the accessibility relation of HOML:\n \nThe other connectives can be defined in the usual way. Validity is\nexpressed as a λ-term,\nλsι→ο∀wι sw,\nthat when applied to a term sσ we write as\n[sσ]. For example, under the embedding,\nproving in HOML the possibility of God’s existence,\n◊ο→ο∃(μ→ο)→ο Xμ . gμ→ο X,\nis tantamount to proving its validity in HOL:\n[◊•σ→σ∃•(μ→σ)→σ Xμ . gμ→σ X]μ→ο.\nTo prove so, the type-raised HOL expression\n[◊•∃•Xμ . gμ→σ X]\nis then encoded in the so-called THF0 syntax (Sutcliffe &\nBenzmüller 2010) prior to being fed, along with the above set of\nequality rules, to the provers that were used in completing the proof:\n \nThe proof in Benzmüller & Paleo 2014 is presented here,\nincluding the axioms and definitions as well as the derivation of its\nfour main results—T1, C, T2, T3—all written in the\ntype-decorated type-raised higher-order logic notation resulting from\nthe embedding. The proof steps are not fully expanded—note the\npresence of type-raised connectives—and the inferential moves\nare not broken down to lower levels of detail. Borrowing a phrase from\nBertrand Russell (Urquhart 1994), this was done to spare the reader of\nthe “kind of nausea” that the fully detailed automated\nproof would cause:  \nBesides helping in the completion of the proof, the automated theorem\nprovers were also very instrumental in the finding of some novel\nresults. First, Gödel’s set of original assumptions was shown to\nbe inconsistent by LEO-II by proving that self-difference becomes an\nessential property of every entity; a re-formulation of the definition\nof essence due to Dana Scott—this involved the addition of a\nmissing conjunct, ϕX, in the definition—was shown\nby Nitpick to be consistent. Second, LEO-II and Satallax managed to\nprove C, T1 and T2 using only the logic system K and, moreover,\nNitpick found a counter-model for T3 in K thus showing that more\nlogical power is required to complete the rest of the proof. Third,\nusing LEO-II and Satallax, it is shown that the logic system KB\n(system K with the Brower axiom) is sufficient to establish the\nnecessity of God’s existence,\n□•∃•Xμ . gμ→σ X,\nwhich is a double-win for automated reasoning: a gain in logical\neconomy, and the deeper philosophical result of having effectively\ndismissed a major criticism against Gödel’s proof, namely his use\nof the stronger logic system S5. Fourth, the authors also prove in KB\nthat: \nas well as: \nthat is, that God is flawless and that monotheism holds, respectively.\nAt this point, it would be fair to say that any of these results would\nbe enough to vindicate the application of automated reasoning in exact\nphilosophy. Now, for the bad news followed by good news: Fifth, the\nformula\nsσ ⊃• □•sσ\ncan also be formally derived which is unfortunate since it implies\nthat there are no contingent truths and that everything is determined,\ni.e. there is no free will. However, the issue has been addressed by\nfollow-up work based on Fitting’s and Anderson’s variants of the\nontological argument (Fuenmayor & Benzmüller 2017, Fitting\n2002, Anderson 1990).  \nLeibniz’s dream was to have a charateristica universalis and\ncalculus ratiocinator that would allow us to reason in\nmetaphysics and morals in much the same way as we do in geometry and\nanalysis; that is to say, to settle disputes between philosophers as\naccountants do: “To take pen in hand, sit down at the abacus\nand, having called in a friend if they want, say to each other: Let us\ncalculate!” From the above applications of automated reasoning,\none would agree with the researchers when they imply that these\nresults achieve, to some extent, Leibniz’s goal of a computational\nmetaphysics (Fitelson & Zalta 2007, Benzmüller & Paleo\n2014).\n \nA nonmonotonic theorem prover can provide the basis for a\n“computational laboratory” in which to explore and\nexperiment with different models of artificial rationality; the\ntheorem prover can be used to equip an artificial rational agent with\nan inference engine to reason and gain information about the world. In\nsuch procedural epistemology, a rational agent is\ndefeasible (i.e. nonmonotonic) in the sense that new reasoning leads\nto the acceptance of new beliefs but also to the retraction of\npreviously held beliefs in the presence of new information. At any\ngiven point in time, the agent holds a set of justified beliefs but\nthis set is open to revision and is in a continuous set of flux as\nfurther reasoning is conducted. This model better reflects our\naccepted notion of rationality than a model in which all the beliefs\nare warranted, i.e. beliefs that once are attained are never\nretracted. Actually, a set of warranted beliefs can be seen as\njustified beliefs “in the limit”, that is, as the ultimate\nepistemic goal in the agent’s search for true knowledge about its\nworld. (Pollock 1995) offers the following definition:  \nA set is defeasible enumerable iff there is an effective computable\nfunction f such that for each n,\nf(n) is a recursive set and the following two\nconditions hold  \nTo compare the concepts, if A is recursively enumerable then\nthere is a sequence of recursive sets Ai such that\neach Ai is a subset of A with each\nAi growing monotonically, approaching A\nin the limit. But if A is only defeasibly enumerable then the\nAi’s still approach A in the limit but\nmay not be subsets of A and approach A\nintermittently from above and below. The goal of the OSCAR Project\n(Pollock 1989) is to construct a general theory of rationality and\nimplement it in an artificial computer-based rational agent. As such,\nthe system uses a defeasible automated reasoner that operates\naccording to the maxim that the set of warranted beliefs should be\ndefeasible enumerable. OSCAR has been in the making for some time and\nthe application of automated nonmonotonic reasoning has also been used\nto extend its capabilities to reason defeasibly about perception and\ntime, causation, and decision-theoretic planning (Pollock 2006).  \nOne of the main goals of automated reasoning has been the automation\nof mathematics. An early attempt at this was Automath (de Bruijn 1968)\nwhich was the first computer system used to check the correctness of\nproofs and whole books of mathematics, including Landau’s\nGrundlagen der Analysis (van Benthem Jutting 1977). Automath\nhas been superseded by more modern and capable systems, most notably\nMizar. The Mizar system (Trybulec 1979, Muzalewski 1993) is based on\nTarski-Grothendieck set theory and, like Automath, consists of a\nformal language which is used to write mathematical theorems and their\nproofs. Once a proof is written in the language, it can be checked\nautomatically by Mizar for correctness. Mizar proofs are formal but\nquite readable, can refer to definitions and previously proved\ntheorems and, once formally checked, can be added to the growing Mizar\nMathematical Library (MML) (Bancerek & Rudnicki 2003, Bancerek\net al. 2018). As of June 2018, MML contained about 12,000\ndefinitions and 59,000 theorems. The Mizar language is a subset of\nstandard English as used in mathematical texts and is highly\nstructured to ensure the production of rigorous and semantically\nunambiguous texts. Here’s a sample proof in Mizar of the existence of\na rational number xy where x and\ny are irrational:  \nExamples of proofs that have been checked by Mizar include the\nHahn-Banach theorem, the Brower fixed-point theorem, Konig’s lemma,\nthe Jordan curve theorem, and Gödel’s completeness theorem.\nRudnicki (2004) discusses the challenges of formalizing Witt’s proof\nof the Wedderburn theorem: Every finite division ring is commutative.\nThe theorem was formulated easily using the existing formalizations\navailable in MML but the proof demanded further entries into the\nlibrary to formalize notions and facts from algebra, complex numbers,\nintegers, roots of unity, cyclotomic polynomials, and polynomials in\ngeneral. It took several months of effort to supply the missing\nmaterial to the MML library but, once in place, the proof was\nformalized and checked correct in a matter of days. Clearly, a\nrepository of formalized mathematical facts and definitions is a\nprerequisite for more advanced applications. The QED Manifesto (Boyer\nat al. 1994, Wiedijk 2007) has such aim in mind and there is\nmuch work to do: Mizar has the largest such repository but even after\n30 years of work “it is miniscule with respect to the body of\nestablished mathematics” (Rudnicki 2004). This last remark\nshould be construed as a call to increase the effort toward this\nimportant aspect in the automation of mathematics.  \nMizar’s goal is to assist the practitioner in the formalization of\nproofs and to help check their correctness; other systems aim at\nfinding the proofs themselves. Geometry has been a target of early\nautomated proof-finding efforts. Chou (1987) proves over 500 geometry\ntheorems using the algebraic approach offered by Wu’s method and the\nGröbner basis method by representing hypotheses and conclusions\nas polynomial equations. Quaife (1992) provides another early effort\nto find proofs in first-order mathematics: over 400 theorems in\nNeumann-Bernays-Gödel set theory, over 1,000 theorems in\narithmetic, a number of theorems in Euclidian geometry, and\nGödel’s incompleteness theorems. The approach is best described\nas semi-automatic or “interactive” with the user providing\na significant amount of input to guide the theorem-proving effort.\nThis is no surprise since, as one applies automated reasoning systems\ninto richer areas of mathematics, the systems take more on the role of\nproof assistants than theorem provers. This is because in richer\nmathematical domains the systems need to reason about theories and\nhigher-order objects which in general takes them deeper into the\nundecidable. Interactive theorem proving is arguably\nthe “killer” application of automated reasoning in\nmathematics and much effort is being expended in the building of\nincreasingly capable reasoning systems that can act as assistants to\nprofessional mathematicians. The proof assistant Isabelle/HOL provides\nthe user with an environment in which to conduct proofs expressed in a\nstructured, yet human-readable, higher-order logic language and which\nincorporates a number of facilities that increase the user’s\nproductivity, automates proof-verification and proof-finding tasks,\nand provides a modular way for the user to build and manage theory\nhierarchies (Ballarin 2014).  \nDifferent proof assistants offer different capabilities measured by\ntheir power at automating reasoning tasks, supported logic, object\ntyping, size of mathematical library, and readability of input and\noutput. A “canonical” proof which is not too trivial but\nnot too complex either can be used as a baseline for system\ncomparison, as done in (Wiedijk 2006) where the authors of seventeen\nreasoning systems are tasked with establishing the irrationality of\n√2. The systems discussed are certainly more capable than this\nand some have been used to assist in the formalization of far more\nadvanced proofs such as Erdös-Selberg’s proof of the Prime Number\nTheorem (about 30,000 lines in Isabelle), the formalization of the\nFour Color Theorem (60,000 lines in Coq), and the Jordan Curve Theorem\n(75,000 lines in HOL Light). A milestone in interactive theorem\nproving was reached in 2012 when, after six-years of effort and using\nthe Coq proof assistant, George Gonthier and his team completed the\nformal verification of the 255-page proof of the Feit-Thompson\ntheorem, also known as the Odd Order Theorem, a major step in the\nclassification of finite simple groups.  \nThe above notwithstanding, automated reasoning has had a small impact\non the practice of doing mathematics and there is a number of reasons\ngiven for this. One reason is that automated theorem provers are not\nsufficiently powerful to attempt the kind of problems mathematicians\ntypically deal with; that their current power is, at best, at the\nlevel of first-year undergraduate mathematics and still far from\nleading edge mathematical research. While it is true that current\nsystems cannot prove completely on their own problems at this level of\ndifficulty we should remember that the goal is to build reasoning\nsystems so that “eventually machines are to be an aid to\nmathematical research and not a substitute for it” (Wang 1960).\nWith this in mind, and while the automated reasoning community\ncontinues to try to meet the grand challenge of building increasingly\npowerful theorem provers, mathematicians can draw now some of the\nbenefits offered by current systems, including assistance in\ncompleting proof gaps or formalizing and checking the correctness of\nproposed proofs. Indeed, the latter may be an application that could\nhelp address some real issues currently being faced by the\nmathematical community. Consider the announcement by Daniel Goldston\nand Cem Yildrim of a proof of the Twin Prime Conjecture where,\nalthough experts initially agreed that the proof was correct, an\ninsurmountable error was found shortly after. Or, think about the case\nof Hales’ proof of the Kepler Conjecture which asserts that no packing\nof congruent balls in Euclidean 3-space has density greater than the\nface-centered cubic packing. Hales’ proof consists of about 300 pages\nof text and a large number of computer calculations. After four years\nof hard work, the 12-person panel assigned by Annals of\nMathematics to the task of verifying the proof still had genuine\ndoubts about its correctness. Thomas Hales, for one, took upon himself\nto formalize his proof and have it checked by an automated proof\nassistant with the aim of convincing others of its correctness (Hales\n2005b, in Other Internet Resources). His task was admittedly heavy but\nthe outcome is potentially very significant to both the mathematical\nand automated reasoning communities. All eyes were on Hales and his\nformal proof as he announced the completion of the Flyspeck\nproject (Hales 2014, in Other Internet Resources; Hales 2015) having\nconstructed a formal proof of the conjecture using the Isabelle and\nHOL Light automated proof assistants: “In truth, my motivations\nfor the project are far more complex than a simple hope of removing\nresidual doubt from the minds of few referees. Indeed, I see formal\nmethods as fundamental to the long-term growth of mathematics.”\n(Hales 2006). \nChurch 1936a, 1936b and Turing 1936 imply the existence of\ntheorems whose shortest proof is very large, and the proof of the Four\nColor Theorem in (Appel & Haken 1977), the Classification of Simple\nGroups in (Gorenstein 1982), and the proof of the Kepler Conjecture in\n(Hales 2005a) may well be just samples of what is yet to come. As\n(Bundy 2011) puts it: “As important theorems requiring larger\nand larger proofs emerge, mathematics faces a dilemma: either these\ntheorems must be ignored or computers must be used to assist with\ntheir proofs.” \nThe above remarks also counter another argument given for not using\nautomated theorem provers: Mathematicians enjoy proving theorems, so\nwhy let machines take away the fun? The answer to this is, of course,\nthat mathematicians can have even more fun by letting the machine do\nthe more tedious and menial tasks: “It is unworthy of excellent\nmen to lose hours like slaves in the labour of calculation which could\nsafely be relegated to anyone else if machines were used” (G. W.\nLeibniz, New Essays Concerning Human Understanding). If still\nnot convinced, just consider the sobering prospect of having to\nmanually check the 23,000 inequalities used in Hales’ proof! \nAnother reason that is given for the weak acceptance of automated\nreasoning by the mathematical community is that the programs are not\nto be trusted since they may contain bugs—software\ndefects—and hence may produce erroneous results. Formally\nverifying automated reasoning programs will help ameliorate this,\nparticularly in the case of proof checkers. Proving programs correct\nis no easy task but the same is true about proving theorems in\nadvanced mathematics: Gonthier proved correct the programs used in the\nformalization of his proof of the Four Color Theorem, but he spent far\nmore effort formalizing all the graph theory that was part of the\nproof. So ironically enough, it turns out that at least in this case,\nand surely there are others, “it is actually easier to verify\nthe correctness of the program than to verify the correctness of the\npen-and-paper mathematics” (Wiedijk 2006). For theorem provers\nand model finders, a complementary strategy would be to verify the\nprograms’ results as opposed to the programs themselves. Paraphrasing\n(Slaney 1994): It does not matter to the mathematician how many\ndefects a program may have as long as the proof (or model) it outputs\nis correct. So, the onus is in the verification of results, whether\nproduced by machine or man, and checking them by independent parties\n(where of course the effort may well use automated checkers) should\nincrease the confidence on the validity of the proofs.  \nIt is often argued that automated proofs are too long and detailed.\nThat a proof can be expressed in more elementary steps is in principle\nvery beneficial since this allows a mathematician to request a proof\nassistant justify its steps in terms of simpler ones. But proof\nassistants should also allow the opposite, namely to abstract detail\nand present results and their justifications using the higher-level\nconcepts, language, and notation mathematicians are accustomed to.\nExploiting the hierarchical structure of proofs as done in (Denney\n2006) is a step in this direction but more work along these lines is\nneeded. Having the proof assistant work at the desired level of\ngranularity provides more opportunity for insight during the proof\ndiscovery process. This is an important consideration since\nmathematicians are equally interested in gaining understanding from\ntheir proofs as in establishing facts.  \n(Bundy 2011) alludes to a deadlock that is preventing the wider\nadoption of theorem provers by the mathematical community: On the one\nhand, the mathematicians need to use the proof assistants to build a\nlarge formal library of mathematical results. But, on the other hand,\nthey do not want to use the provers since there is no such library of\npreviously proved results they can build upon. To break the impasse, a\nnumber of applications are proposed of which assisting the\nmathematician in the search of previously proved theorems is of\nparticular promise. During its history, mathematics has accumulated a\nhuge number of theorems and the number of mathematical results\ncontinues to grow dramatically. In 2010, Zentralblatt MATH\ncovered about 120,000 new publications (Wegner 2011). Clearly, no\nindividual researcher can be acquainted with all this mathematical\nknowledge and it will be increasingly difficult to cope with one’s\never-growing area of specialty unless assisted with automated\ntheorem-proving tools that can search in intelligent ways for\npreviously proved results of interest. An alternative approach to this\nproblem is for mathematicians to tap into each other’s knowledge as\nenabled in computational social systems like polymath and\nmathoverflow. The integration of automated reasoning tools\ninto such social systems would increase the effectiveness of their\ncollective intelligence by supporting “the combination of\nprecise formal deductions and the more informal loose interaction seen\nin mathematical practice” (Martin & Pease 2013, in Other\nInternet Resources).  \nDue to real pressing needs from industry, some applications of\nautomated reasoning in pure and applied mathematics are more of\nnecessity than choice. After having worked on the formalization of\nsome elementary real analysis to verify hardware-based floating point\ntrigonometric functions, (Harrison 2006, Harrison 2000) mentions the\nfurther need to formalize more pure mathematics—italics\nare his—to extend his formalization to power series for\ntrigonometric functions and basic theorems about Diophantine\napproximations. Harrison finds it surprising that “such\nextensive mathematical developments are used simply to verify that a\nfloating point tangent function satisfies a certain error bound”\nand, from this remark, one would expect there are other industrial\napplications that will demand more extensive formalizations. \nAlbeit not at the rate originally anticipated, automated reasoning is\nfinding applications in mathematics. Of these, formal verification of\nproofs is of special significance since it not only provides a viable\nmechanism to check proofs that humans alone could not but it also has,\nas a side effect, the potential to redefine what it would take for a\nproof to be accepted as such. As the use of automated reasoning\nassistants becomes more widespread one can envision their use\nfollowing a certain methodical order: First, automated reasoning tools\nare used for theory exploration and discovery. Then, having identified\nsome target problem, the practitioner works interactively with an\nautomated assistant to find proofs and establish facts. Finally, an\nautomated proof checker is used to check the correctness of all final\nproofs prior to being submitted for publication and being made\navailable to the rest of the mathematical community via the creation\nof new entries in a repository of formalized mathematics. It is indeed\na matter of time before the application of automated proof assistants\nbecomes an everyday affair in the life of the mathematician; it is the\ngrand challenge of the automated reasoning community to make it happen\nsooner than later. \nSince its inception, the field of automated theorem proving has had\nimportant applications in the larger field of artificial intelligence\n(AI). Automated deduction is at the heart of AI applications like\nlogic programming (see section 4.1 Logic Programming, in this\narticle) where computation is equated with deduction; robotics and\nproblem solving (Green 1969) where the steps to achieve goals are\nsteps extracted from proofs; deductive databases (Minker et\nal. 2014) where factual knowledge is expressed as atomic clauses\nand inference rules, and new facts are inferred by deduction; expert\nsystems (Giarratano & Riley 2004) where human expertise in a given\ndomain (e.g. blood infections) is captured as a collection of IF-THEN\ndeduction rules and where conclusions (e.g. diagnoses) are obtained by\nthe application of the inference rules; and many others. An\napplication of automated reasoning in AI which is bound to have deep\nphilosophical implications is the increased use of BDI computational\nlogics for describing the beliefs, desires, and intentions of\nintelligent agents and multi-agent systems (Meyer 2014) and, in\nparticular, endowing future intelligent systems, such as\ndecision-support systems or robots, with legal and ethical behaviour.\nDeontic logic can be automated for the task (Furbach et al.\n2014) but given that there is no agreement on a universal system of\ndeontic logic, ethics “code designers” need a way to\nexperiment with the different deontic systems (i.e., to lay out axioms\nand see what conclusions follow from them) to help them identify the\ndesired ethic code for the specific application at hand;\n(Benzmüller et al. 2018) discusses an environment for this. If\nactual, physical, robots were to be used in these experiments, the\nterm “deontic laboratory” would be quite descriptive\nalbeit somewhat eerie. \nRestricting the proof search space has always been a key consideration\nin the implementation of automated deduction, and traditional\nAI-approaches to search have been an integral part of theorem provers.\nThe main idea is to prevent the prover from pursuing unfruitful\nreasoning paths. A dual aspect of search is to try to look for a\npreviously proved result that could be useful in the completion of the\ncurrent proof. Automatically identifying those results is no easy task\nand it becomes less easy as the size of the problem domain, and the\nnumber of already established results, grows. This is not a happy\nsituation particularly in light of the growing trend to build large\nlibraries of theorems such as the Mizar Problems for Theorem Proving\n(MPTP) (Urban et al. 2010, Bancerek & Rudnicki 2003) or the\nIsabelle/HOL mathematical library (Meng & Paulson 2008), so\ndeveloping techniques for the discovery, evaluation, and selection of\nexisting suitable definitions, premises and lemmas in large libraries\nof formal mathematics as discussed in (Kühlwein et al.\n2012) is an important line of research.  \nAmong many other methods, and in stark contrast to automated provers,\nmathematicians combine induction heuristics with deductive techniques\nwhen attacking a problem. The former helps them guide the\nproof-finding effort while the latter allows them to close proof gaps.\nAnd of course all this happens in the presence of the very large body\nof knowledge that the human possesses. For an automated prover, the\nanalogous counterpart to the mathematician’s body of knowledge is a\nlarge library like MPTP. An analogous approach to using inductive\nheuristics would be to endow the theorem prover with inductive,\ndata-driven, machine learning abilities. Urban & Vyskocil 2012\nrun a number of experiments to determine any gains that may result\nfrom such an approach. For this, they use MPTP and theorem provers\nlike E and SPASS enhanced with symbol-based machine learning\nmechanisms. A detailed presentation and statistical results can be\nfound in the above reference but in summary, and quoting the authors,\n“this experiment demonstrates a very real and quite unique\nbenefit of large formal mathematical libraries for conducting novel\nintegration of AI methods. As the machine learner is trained on\nprevious proofs, it recommends relevant premises from the large\nlibrary that (according to the past experience) should be useful for\nproving new conjectures.” Urban 2007 discusses MaLARea (a\nMachine Learner for Automated Reasoning), a meta-system that also\ncombines inductive and deductive reasoning methods. MaLARea is\nintended to be used in large theories, i.e. problems with a large\nnumber of symbols, definitions, premises, lemmas, and theorems. The\nsystem works in cycles where results proved deductively in a given\niteration are then used by the inductive machine-learning component to\nplace restrictions in the search space for the next theorem-proving\ncycle. Albeit simple in design, the first version of MaLARea solved\n142 problems out of 252 in the MPTP Challenge, outperforming the more\nseasoned provers E (89 problems solved) and SPASS (81 problems\nsolved). \nBesides using large mathematical libraries, tapping into web-based\nsemantic ontologies is another possible source of knowledge. Pease\n& Sutcliffe 2007 discuss ways for making the SUMO ontology\nsuitable for first-order theorem proving, and describes work on\ntranslating SUMO into TPTP. An added benefit of successfully reasoning\nover large semantic ontologies is that this promotes the application\nof automated reasoning into other fields of science. Tapping into its\nfull potential, however, will require a closer alignment of methods\nfrom automated reasoning and artificial intelligence. \nAutomated reasoning is a growing field that provides a healthy\ninterplay between basic research and application. Automated deduction\nis being conducted using a multiplicity of theorem-proving methods,\nincluding resolution, sequent calculi, natural deduction, matrix\nconnection methods, term rewriting, mathematical induction, and\nothers. These methods are implemented using a variety of logic\nformalisms such as first-order logic, type theory and higher-order\nlogic, clause and Horn logic, non-classical logics, and so on.\nAutomated reasoning programs are being applied to solve a growing\nnumber of problems in formal logic, mathematics and computer science,\nlogic programming, software and hardware verification, circuit design,\nexact philosophy, and many others. One of the results of this variety\nof formalisms and automated deduction methods has been the\nproliferation of a large number of theorem proving programs. To test\nthe capabilities of these different programs, selections of problems\nhave been proposed against which their performance can be measured\n(McCharen, Overbeek & Wos 1976, Pelletier 1986). The TPTP\n(Sutcliffe & Suttner 1998) is a library of such problems that is\nupdated on a regular basis. There is also a competition among\nautomated theorem provers held regularly at the CADE conference\n(Pelletier, Sutcliffe & Suttner 2002; Sutcliffe 2016, in Other\nInternet Resources); the problems for the competition are selected\nfrom the TPTP library. There is a similar library and competition for\nSMT solvers (Barret et al. 2013). \nInitially, computers were used to aid scientists with their complex\nand often tedious numerical calculations. The power of the machines\nwas then extended from the numeric into the symbolic domain where\ninfinite-precision computations performed by computer algebra programs\nhave become an everyday affair. The goal of automated reasoning has\nbeen to further extend the machine’s reach into the realm of deduction\nwhere they can be used as reasoning assistants in helping their users\nestablish truth through proof.","contact.mail":"fred.portoraro@symlog.ca","contact.domain":"symlog.ca"}]
