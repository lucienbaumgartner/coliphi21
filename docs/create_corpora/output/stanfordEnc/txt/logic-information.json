[{"date.published":"2014-02-03","date.changed":"2018-05-30","url":"https://plato.stanford.edu/entries/logic-information/","author1":"Maricarmen Martinez","author2":"Sebastian Sequoiah-Grayson","entry":"logic-information","body.text":"\n\n\nAt their most basic, logic is the study of consequence, and\ninformation is a commodity. Given this, the interrelationship between\nlogic and information will centre on the informational consequences of\nlogical actions or operations conceived broadly. The explicit\ninclusion of the notion of information as an object of\nlogical study is a recent development. It was by the beginning of the\npresent century that a sizable body of existing technical and\nphilosophical work (with precursors that can be traced back to the\n1930s) coalesced into the new emerging field of logic and information\n(see Dunn 2001). This entry is organised thematically, rather than\nchronologically. We survey major logical approaches to the study of\ninformation, as well as informational understandings of logics\nthemselves. We proceed via three interrelated and complementary\nstances: information-as-range, information-as-correlation, and\ninformation-as-code. \n\n\nThe core intuition motivating the Information-as-range\nstance, is that an\ninformational state may be characterised by the range of possibilities\nor configurations that are compatible with the information available\nat that state. Acquiring new information corresponds to a reduction of\nthat range, thus reducing uncertainty about the actual configuration\nof affairs. With this understanding, the setting of possible-world\nsemantics for epistemic modal logics proves to be rewarding for the\nstudy of various semantic aspects of information. A prominent\nphenomenon here is information update, which may occur in\nboth individual and social settings, due to the interaction between\nboth agents and their environment via different types of epistemic\nactions. We will see that an epistemic action is any action that\nfacilitates the flow of information, hence we will return to epistemic\nactions themselves throughout.\n\n\nThe Information-as-correlation stance focuses\non information flow as it is licensed within structured systems formed\nby systematically correlated components. For example: the number of\nrings of a tree trunk can give you information about the time when the\ntree was born, in virtue of certain regularities of nature that\n‘connect’ the past and present of trees. Central themes of\nthis stance include the aboutness, situatedness, and accessibility\nof information in structured information environments. \n\n\nThe key concern of the third stance, Information-as-code,\nis the syntax-like\nstructure of information pieces (their encoding) and the\ninference and computation processes that are licensed by\nvirtue (among other things) of that structure. A most natural logical\nsetting to study these informational aspects is the algebraic proof\ntheory underpinned by a range of substructural logics.\nSubstructural logics have always been a natural home for informational\nanalysis, and the recent developments in the area enrich the\ninformation-as-code stance. \n\n\nThe three stances are by no means incompatible, but neither are they\nnecessarily reducible to each other. This will be expanded on later in\nthe entry, and some further topics of research will be illustrated,\nbut for a preview of how the three stances can live together, take the\ncase of a structured information system composed of several parts.\nFirstly, the correlations between the parts naturally allow for\n‘information flow’ in the sense of the\ninformation-as-correlation stance. Secondly, they also give rise to a\nlocal ranges of possibilities, since the local information available\nat one part will be compatible with a certain range of global states\nof the system. Thirdly, the combinatorial, syntax-like,\nproof-theoretical aspects of information can be brought to this\nsetting in various ways. One of them is treating the correlational\nflow of information as a sort of combinatorial system by which local\ninformation states are combined in syntactic-like ways, fitting a\nparticular interpretation of substructural logic. One could also add\ncode-like-structure to the modelling explicitly, for example by\nassigning local deductive calculi to either the components or local\nstates of the system. We begin however with information as\nrange\n\nThe understanding of information as range has its origins in\nBar-Hillel and Carnap’s theory of semantic information,\nBar-Hillel and Carnap (1952). It is here that the inverse range\nprinciple is given its first articulation with regard to the\ninformational content of a proposition. The inverse range principle\nstates that there is an inverse relationship between the information\ncontained by a proposition on the one hand, and the likelihood of that\nproposition being true on the other. That is, the more information\ncarried by a proposition, the less likely it is that the proposition\nis true. Similarly, the more likely the truth of a proposition, the\nless information it carries.  \nThe likelihood of the truth of a proposition connects with information\nas range via a possible worlds semantics. For any contingent\nproposition, it will be supported by some possibilities (those where\nit is true) and not supported by others (those where it is false).\nHence a proposition will be supported by a range of possibilities, an\n“information range”. Now suppose that there is a\nprobability distribution across the space of possibilities, and for\nthe sake of simplicity suppose that the distribution is uniform. In\nthis case, the more worlds that support a proposition, the likelier\nthe proposition’s truth, and, via the inverse relationship\nprinciple, the less information it carries. Although information as\nrange has its origins in quantitative information theory, its role in\ncontemporary qualitative logics of information cannot be overstated.\n \nConsider the following example due to Johan van Benthem (2011). A\nwaiter in a cafe receives an order for your table—an espresso\nand a soda. When the waiter arrives at your table, he asks “For\nwhom is the soda?”. After your telling him that the soda is for\nyou and his giving you your soda, the waiter does not need to ask\nabout the espresso, he can just give it to your cafe-partner. This is\nbecause the information gained by the waiter from your telling him\nthat you ordered the soda allows him to eliminate certain open\npossibilities from the total range of possibilities such that only one\nis left—your friend ordered the espresso.  \nLogics of information distinguish regularly between hard\ninformation and soft information. The terminology is a\nslight misnomer, as this distinction is not one between different\ntypes of information per se. Rather it is one between\ndifferent types of information storage. Hard information is\nfactive, and unrevisable. Hard information is often taken to\ncorrespond to knowledge. In contrast to hard information,\nsoft information is non-necessarily-factive, hence revisable\nin the presence of new information. Soft information, in virtue of its\nrevisability, corresponds very closely to belief. The terms\nknowledge and belief are conventional, but on the context of\ninformation flow, the hard/soft information reading is convenient on\naccount of it bringing the informational phenomena to the foreground.\nAt the very least the terminology is increasingly popular, so being\nclear on the distinction being one between types of information\nstorage as opposed to types of information is important. Although both\nhard and soft information are important for our epistemic and doxastic\nsuccess, in this section we will concentrate mainly on logics of hard\ninformation flow.  \nIn\n section 1.1\n we will see how it is that classic epistemic logics exemplify the\nflow of hard information within the information as range framework. In\n section 1.2\n we will extend our exposition from logics of hard information-gain to\nlogics of the actions that facilitate the gain of such hard\ninformation, dynamic epistemic logics. At the end of Section 1.2, we\nwill expound the important phenomenon of private information,\nbefore examining how it is that information as range is captured in\nvarious quantitative frameworks.  \nIn this section we will explore how it is that the elimination of\npossibilities corresponding to information-gain is the starting point\nfor research on logics of knowledge and belief that fall under the\nheading of epistemic logics. We will begin with classic\nsingle-agent epistemic logic, before exploring multi-agent epistemic\nlogics. In both cases, since we will be concentrating on logics of\nknowledge as opposed to logics of belief, the information-gained will\nbe hard information.  \nConsider the waiter example in more detail. Before receiving the hard\ninformation that the soda is for you (and for the sake of the example\nwe are assuming that the waiting is dealing with hard information\nhere), the waiter’s knowledge-base is modelled by a pair of\nworlds (hereafter information states) \\(x\\) and \\(y\\)\nsuch that in \\(x\\) you ordered the soda and your friend the\nespresso, and in \\(y\\) you ordered the espresso and your friend the\nsoda. After receiving the hard information that the soda is for you,\n\\(y\\) is eliminated from the waiter’s knowledge-base, leaving\nonly \\(x\\). As such, the reduction of the range of possibilities\ncorresponds to an information-gain for the waiter. Consider the truth\ncondition for agent \\(\\alpha\\) knows that \\(\\phi\\),\nwritten \\(K_{\\alpha}\\phi\\): \nThe accessibility relation \\(R_{\\alpha}\\) is an\nequivalence relation connecting \\(x\\) to all information states\n\\(y\\) such that \\(y\\) is indistinguishable from \\(x\\),\ngiven \\(\\alpha\\)’s hard information at that state\n\\(x\\). That is, given what the waiter knows when he is in that\nstate. So, if \\(x\\) was the waiter’s information state before\nbeing informed that you ordered the soda, \\(y\\) would have included\nthe information that you ordered the espresso, as each option was as\ngood as the other until the waiter was informed otherwise. There is an\nimplicit assumption at work here—that some state \\(z\\) say,\nwhere you ordered both the soda and the espresso, is not in the\nwaiter’s information-range. That is, the waiter knows that\n\\(z\\) is not a possibility. Once informed however, the information\nstates supporting your ordering the espresso are eliminated from the\nrange of information corresponding to the waiter’s knowledge.\n \nBasic modal logic extends propositional formulas with modal operators\nsuch as \\(K_{\\alpha}\\). If \\(\\mathbf{K}\\) is the set\nof all Kripke models then we have the following:  \nIn hard information terms, (A1) states that hard information is closed\nunder (known) implications. Since the first conjunct states that all\nstates accessible by \\(\\alpha\\) are \\(\\phi\\) states, \\(\\alpha\\) possesses the\nhard information that \\(\\phi\\), hence \\(\\alpha\\) also possesses the hard\ninformation that \\(\\psi\\). (A2) states that if \\(\\phi\\) holds in the set of\nall models, then \\(\\alpha\\) possesses the hard information that \\(\\phi\\). In\nother words, (A2) states that all tautologies are known/hard stored by\nthe agent, and (A1) states that \\(\\alpha\\) knows the logical consequences\nof all propositions that \\(\\alpha\\) knows (be they tautologies or\notherwise). That is, the axioms state that the agent is logical\nomniscient, or an ideal reasoner, a property of agents that\nwe will return to in detail in the sections\n below.[1] \nThe framework explored so far concerns single-agent epistemic logic,\nbut reasoning and information flow are very often multi-agent\naffairs. Consider again the waiter example. Importantly, the\nwaiter is only able to execute the relevant reasoning procedure\ncorresponding to a restriction of the range of information states\non account of your announcement to him with regard to the\nespresso. That is, it is the verbal interaction between several agents\nthat facilitates the information flow that enabled the logical\nreasoning to be undertaken.  \nIt is at this point that multi-agent epistemic logic raises new\nquestions regarding the information in a group. “Everybody in\n\\(G\\) possesses the hard information that \\(\\phi\\)” (where\n\\(G\\) is any group of agents from a finite set of agents\n\\(G*)\\) written as \\(E_G\\phi .\nE_G\\) is defined for each \\(G \\subseteq G^*\\) in the following manner:  \nGroup knowledge is importantly different from common\nknowledge (Lewis 1969; Fagin et al. 1995). Common knowledge is\nthe condition of the group where everybody knows that everybody\nknows that everybody knows … that \\(\\phi\\). In other words,\ncommon knowledge concerns the hard information that each agent in the\ngroup possesses about the hard information possessed by the other\nmembers of the group. That everybody in \\(G\\) possesses the hard\ninformation that \\(\\phi\\) does not imply that \\(\\phi\\) is common knowledge.\nWith group knowledge each agent in the group may possess the same hard\ninformation (hence achieving group knowledge) without necessarily\npossessing hard information about the hard information possessed by\nthe other agents in the group. As noted by van Ditmarsh, van der Hoek,\nand Kooi (2008: 30), “the number of iterations of the\n\\(E\\)-operator makes a real difference in practice”.\n\\(C_G\\phi\\)—the common knowledge\nthat \\(\\phi\\) for members of \\(G\\), is defined as follows:\n \nTo appreciate the difference between \\(E\\) and \\(C\\), consider\nthe following “spy example” (originally Barwise 1988 with\nthe envelope details due to Johan van Benthem).  \nThere are a group of competing spies at a formal dinner. All of them\nare tasked with the mission of acquiring some secret information from\ninside the restaurant. Furthermore, it is common knowledge amongst\nthem that they want the information. Given this much, compare the\nfollowing:  \nVery obviously, the two scenarios will elicit very different types of\nbehaviour from the spies. The first would be relatively subtle, the\nlatter dramatically less so. See Vanderschraaf and Sillari (2009) for\nfurther details.  \nA still more fine-grained use of S5 based epistemic logics is that of\nZhou (2016). Zhou demonstrates that S5 based epistemic logic may be\nused to model the epistemic states of the agent from the perspective\nof the agent themselves. Hence Zhou refers to such an epistemic logic\nas internally epistemic. Zhou then uses a multi-valued logic\nto model the relationship between the agent’s internal knowledge\nbase and their external informational environment.  \nSee the full entry on\n Dynamic Epistemic Logic.\n As noted above, the waiter example from the beginning of this section\nis as much about information-gain via announcements, epistemic\nactions, as it is about information structures. In this section,\nwe will outline how it is that the expressive power of multi-agent\nepistemic logic can be extended so as to capture epistemic actions.\n \nHard information flow, that is, the flow of information between the\nknowledge states of two or more agents, can be facilitated by more\nthan one epistemic action. Two canonical examples are\nannouncements and observations. When\n“announcement” is restricted to true and public\nannouncement, its result on the receiving agent’s\nknowledge-base is similar to that of an observation (on the assumption\nthat the agent believes the content of the announcement). The public\nannouncement that \\(\\phi\\) will restrict the model of the agent’s\nknowledge-base to the information states where \\(\\phi\\) is true, hence\n“announce \\(\\phi\\)” is an epistemic state\ntransformer in the sense that it transforms the epistemic states\nof the agents in the group, (see van Ditmarsh, van der Hoek, and Kooi\n2008:\n 74).[2] \nDynamic epistemic logics extend the language of non-dynamic epistemic\nlogics with dynamic operators. In particular, public announcement\nlogic (PAL) extends the language of epistemic logics with the\ndynamic announcement operator [\\(\\phi\\)], where [\\(\\phi]\\psi\\) is read\n“after announcement \\(\\phi\\), it is the case that \\(\\psi\\)”. The\nkey reduction axioms of PAL are as follows:  \nRA1–RA5 capture the properties of the announcement operator by\nconnecting what is true before the announcement with what is true\nafter the announcement. The axioms are named ‘reduction’\naxioms because the left-to-right hand direction reduces either the\nnumber of announcement operators or the complexity of the formulas\nwithin their scope. For an in depth discussion see Pacuit (2011). RA1\nstates that announcements are truthful. RA5 specifies the\nepistemic-state-transforming properties of the announcement operator.\nIt states that \\(\\alpha\\) knows that \\(\\psi\\) after the announcement that\n\\(\\phi\\) iff \\(\\phi\\) implies that \\(\\alpha\\) knows that \\(\\psi\\) will be\ntrue after \\(\\phi\\) is announced in all \\(\\phi\\)-states. The “after\n\\(\\phi\\) is announced” condition is there to account for the fact\nthat \\(\\psi\\) might change its truth-value after the announcement. The\ninteraction between the dynamic announcement operator and the\nknowledge operator is described completely by RA5 (see van Benthem,\nvan Eijck, and Kooi 2006).  \nJust as adding the common knowledge operator \\(C\\) to\nmulti-agent epistemic logic extends the expressive capabilities of\nmulti-agent epistemic logic, adding \\(C\\) to PAL results in the\nmore expressive public announcement logic with common\nknowledge, (PAC). The exact relationship between public\nannouncements and common knowledge is captured by the announcement\nand common knowledge rule of the logic PAC as the following:  \nAgain, PAC is the dynamic logic of hard information. The epistemic\nlogics dealing with soft information fall within the scope of\nbelief revision theory (van Benthem 2004; Segerberg 1998).\nRecall that hard and soft information are not distinct types of\ninformation per se, rather they are distinct types of\ninformation storage. Hard-stored information is unrevisable,\nwhereas soft-stored information is revisable. Variants of PAL that\nmodel soft information augment their models with\nplausibility-orderings on information-states (Baltag and Smets 2008).\nThese orderings are known as preferential models in\nnon-monotonic logic and belief-revision theory. The logics can be made\ndynamic in virtue of the orderings changing in the face of new\ninformation (which is the mark of soft information as opposed to hard\ninformation). Such plausibility-orderings may be modelled\nqualitatively via partial orders etc., or modelled quantitatively via\nprobability-measures. Such quantitative measures provide a connection\nto a broader family of quantitative approaches to semantic information\nthat we will examine below. Recent work by Allo (2017) ties the soft\ninformation of dynamic epistemic logic to non-monotonic logics. This\nis an intuitive move. Soft information is information that has been\nstored in a revisable way, hence the revisable nature of conclusions\nin non-monotonic arguments makes non-monotonic logics a natural fit.\nOn this very topic, see also Chapter 13.7 of van Benthem (2011).  \nPrivate information. Private information is\nan equally important aspect of our social interaction. Consider\nscenarios where the announcing agent is aware of the private\ncommunication whilst other members of the group are not, such as\nemails in Bcc. Consider also scenarios where the sending agent is\nnot aware of the private communication, such as a\nsurveillance operation. The system of dynamic epistemic logic\n(DEL) models events that turn on private (and public) information by\nmodelling the agents’ information concerning the events\ntaking place in a given communicative scenario (see Baltag et\nal. 2008; van Ditmarsh et al. 2008; and Pacuit 2011). For an excellent\noverview and integration of all of the issues above, see the recent\nwork of van Benthem (2016), where the author discusses multiple\ninterrelated levels of logical dynamics, one level of update, and\nanother of representation. For an extensive collection of papers\nextending this and related approaches, see Baltag and Smets (2014) \nThe modal information theory approach to multi-agent information flow\nis the subject of a great amount of research. The semantics is not\nalways carried out in relational terms (i.e., with Kripke Frames) but\nis done often algebraically (see Blackburn et al. 2001 for details of\nthe algebraic approach to modal logic). For more details on algebraic\nas well as type-theoretic approaches, see the subsection on algebraic\nand other approaches to modal information theory in the supplementary\ndocument\n Abstract Approaches to Information Structure. \nQuantitative approaches to information as range also have\ntheir origins in the inverse relationship principle. To\nrestate—the motivation being that the less likely the truth of a\nproposition as expressed in a logical language with respect to a\nparticular domain, the greater the amount of information encoded by\nthe relevant formula. This is in contrast to the information measures\nin the mathematical theory of communication (Shannon 1953\n[1950]) where such measures are gotten via an inverse relationship on\nthe expectation of the receiver \\(R\\) of the receipt of a signal\nfrom some source \\(S\\).  \nAnother important aspect of the classical theory of information, is\nthat it is an entirely static theory—it is concerned\nwith the informational content and measure of particular formulas, and\nnot with information flow in any way at all.  \nThe formal details of classical information theory turn on the\nprobability calculus. These details may be left aside here, as the\nobvious conceptual point is that logical truths have a\ntruth-likelihood of 1, and therefore an information measure of 0.\nBar-Hillel and Carnap did not take this to mean that logical truths,\nor deductions, were without information yield, only that their theory\nof semantic information was not designed to capture such a property.\nThey coined the term psychological information for the\nproperty involved. See Floridi (2013) for further details.  \nA quantitative attempt at specifying the information yield of\ndeductions was undertaken by Jaakko Hintikka with his theory of\nsurface information and depth information (Hintikka\n1970, 1973). The theory of surface and depth information extends\nBar-Hillel and Carnap’s theory of semantic information from the\nmonadic predicate calculus all the way up to the full polyadic\npredicate calculus. This itself is a considerable achievement, but\nalthough technically astounding, a serious restriction of this\napproach is that it is only a fragment of the deductions carried out\nwithin full first-order logic that yield a non-zero information\nmeasure. The rest of the deductions in the full polyadic predicate\ncalculus, as well as all of those in the monadic predicate calculus\nand propositional calculus, measure 0, (see Sequoiah-Grayson 2008).\n \nThe obvious inverse situation with the theory of classical semantic\ninformation is that logical contradictions, having a truth-likelihood\nof 0, will deliver a maximal information measure of 1. Referred to in\nthe literature as the Bar-Hillel-Carnap Semantic Paradox, the\nmost developed quantitative approach to addressing it is the theory of\nstrongly semantic information (Floridi 2004). The conceptual\nmotivation behind strongly semantic information is that for a\nstatement to yield information, it must help us to narrow down the set\nof possible worlds. That is, it must assist us in the search for the\nactual world, so to speak (Sequoiah-Grayson 2007). Such a\ncontingency requirement on informativeness is violated by\nboth logical truths and logical contradictions, both of which measure\n0 on the theory of strongly semantic information. See Floridi (2013)\nfor further details. See also Brady (2016) for recent work on the\nrelationship between quantitative accounts of information and\nanalyticity. For a new approach to connecting quantitative and\nqualitative measures of information, see Harrison-Trainor et\nal. (2018) \nThe correlational take on information looks at how the existence of\nsystematic connections between the parts of a structured\ninformation environment permits that one part may carry\ninformation about another. For example: the pattern of pixels\nthat appear on the screen of a computer gives information (not\nnecessarily complete) about the sequence of keys that were\npressed by the person who is typing a document, and even a partial\nsnapshot of the clear starred sky your friend is looking at now will\ngive you information about his possible locations on Earth at\nthis moment. The focus on structured environments and the aboutness of\ninformation goes hand in hand with a third main topic of the\ninformation-as correlation approach, namely the situatedness of\ninformation, that is, its dependence on the particular setting on\nwhich an informational signal occurs. Take the starry sky as an\nexample again: the same pattern of stars, at different moments in time\nand locations in space will in general convey different information\nabout the location of your friend.  \nHistorically, the first paradigmatic setting of correlated information\nwas Shannon’s work on communication (1948), which we already\nmentioned in the last section. Shannon considered a communication\nsystem formed by two information sites, a source and a receiver,\nconnected via a noisy channel. He gave conclusive and extremely useful\nanswers to questions having to do with the construction of\ncommunication codes that help maximising the effectiveness of\ncommunication (in terms of bits of information that can be\ntransmitted) while minimizing the possibility of errors caused by\nchannel noise. As we previously said, Shannon’s concern was\npurely quantitative. The logical approach to information as\ncorrelation builds on Shannon’s ideas, but is concerned with\nqualitative aspects of information flow , like the ones we highlighted\nbefore: what information about a\n‘remote’ site (remote in terms of space, time,\nperspective, etc.) can be drawn out of information that is directly\navailable at a ‘proximal’ site? \nSituation theory (Barwise and Perry 1983; Devlin 1991) is the\nmajor logical framework so far that has made these ideas its starting\npoint for an analysis of information. Its origin and some of its\ncentral insights can be found in the project of naturalization of mind\nand the possibility of knowledge initiated by Fred Dretske (1981),\nwhich soon influenced the inception of situation semantics in the\ncontext of natural language (see Kratzer 2011).  \nTechnically, there are two kinds of developments in situation theory:\n \nThe next three subsections survey some of the basic notions from this\ntradition: the basic sites of information in situation theory (called\nsituations), the basic notion of information flow based on\ncorrelations between situations, and the mathematical theory of\nclassifications and channels mentioned in (b).  \nThe ontologies in (a) span a wide spectrum of entities. They are meant\nto reflect a particular way in which an agent may carve up a system.\nHere “a system” can be the world, or a part or aspect of\nit, while the agent (or kind of agent) can be an animal species, a\ndevice, a theorist, etc. The list of basic entities includes\nindividuals, relations (which come with roles attached to them),\ntemporal and spacial locations, and various other things. Distinctive\namong them are the situations and infons.  \nRoughly speaking, situations are highly structured parts of a system,\nsuch as a class session, a scene as seen from a certain perspective, a\nwar, etc. Situations are the basic supporters of information. Infons,\non the other hand, are the informational issues that situations may or\nmay not support. The simplest kind of informational issue is whether\nsome entities \\(a_1 , \\ldots ,a_n\\) stand (or do not stand) in a relation\n\\(R\\) when playing the roles \\(r_1 , \\ldots ,r_n\\), respectively. Such basic infon is usually\ndenoted as  \nwhere \\(i\\) is 1 or 0, according to whether the issue is positive\nor negative.  \nInfons are not intrinsic bearers of truth, and they are not claims\neither. They are simply informational issues that may or may not be\nsupported by particular situations. We’ll write \\(s \\models \\sigma\\) to mean that the situation\n\\(s\\) supports the infon \\(\\sigma\\). As an example, a successful\ntransaction whereby Mary bought a piece of cheese in the local market\nis a situation that supports the infon  \nThis situation does not support the infon  \nbecause Mary did buy cheese. Nor does the situation support the infon\n \nbecause Armstrong is not part of the situation in question at all.\n \nThe discrimination or individuation of a situation by an agent does\nnot entail that the agent has full information about it: when we\nwonder whether the local market is open, we have individuated a\nsituation about which we actually lack some information. See\nTextor (2012) for a detailed discussion on the nature of\nsituation-like entities and their relation with other ontological\ncategories such as the possible worlds used in modal logic.  \nBesides individuals, relations, locations, situations and basic\ninfons, there are usually various kinds of parametric and abstract\nentities. For example, there is a mechanism of type\nabstraction. According to it, if \\(y\\) is a parameter for\nsituations, then  \nis the type of situations where somebody buys cheese. There will be\nsome basic types in an ontology, and many other types obtained via\nabstraction, as just described.  \nThe collection of ontology entities also includes propositions and\nconstraints. They are key in the formulation of the basic principles\nof information content in situation theory, to be introduced next.\n \nThe following are typical statements about “information\nflow” as studied in situation theory:  \nThe general scheme has the form  \nwhere \\(s : T\\) is notation for “\\(s\\) is of type\n\\(T\\)”. The idea is that it is concrete parts of the world\nthat act as carriers of information (the concrete dot in the radar or\nthe footprints in Zhucheng), and that they do so by virtue of being of\na certain type (the dot moving upward or the footprints showing a\ncertain pattern). What each of these concrete instances indicates is a\nfact about another correlated part of the world. For the issues to be\ndiscussed below it will suffice to consider cases where the indicated\nfact— \\(p\\) in the formulation of\n[IC]—is of the form \\(s' : T '\\), as in the\nradar example.  \nThe conditions needed to verify informational signalling in the sense\nof [\\(\\mathbf{IC}\\)] rely on the existence of law-like\nconstraints such as natural laws, necessary laws such as\nthose of math, or conventions, thanks to which (in part) one situation\nmay serve as carrier of information about another one. Constraints\nspecify the correlations that exist between situations of\nvarious types, in the following sense: if two types \\(T\\) and \\(T '\\)\nare subject to the constraint \\(T \\Rightarrow T '\\), then for every\nsituation \\(s\\) of type \\(T\\) there is a relevantly connected\nsituation \\(s'\\) of type \\(T '\\). In the radar example, the relevant\ncorrelation would be captured by the\nconstraint GoingUpward\n\\(\\Rightarrow\\) GoingNorth, which says that\neach situation where a radar point moves upward is connected with\nanother situation where a plane is moving to the north. It is the\nexistence of this constraint that allows a particular situation where\nthe dot moves to indicate something about the connected plane\nsituation.  \nWith this background, the verification principle for information\nsignalling in situation theory can be formulated as follows:  \n[IS Verification] \\(s : T\\) indicates that \\(s' :\nT'\\) if \\(T \\Rightarrow T '\\) and \\(s\\) is relevantly\nconnected to \\(s'\\). \nThe relation \\(\\Rightarrow\\) is transitive. This ensures that Dretske’s\nXerox principle holds in this account of information transfer, that\nis, there can be no loss of semantic information through information\ntransfer chains.  \n[Xerox Principle]: If \\(s_1 : T_1\\) indicates that\n\\(s_2 : T_2\\) and \\(s_2 : T_2\\) indicates that \\(s_3 : T_3\\), then\n\\(s_1 : T_1\\) indicates that \\(s_3 : T_3\\). \nThe [IS Verification] principle deals with\ninformation that in principle could be acquired by an agent.\nThe access to some of this information will be blocked, for example,\nif the agent is oblivious to the correlation that exists between two\nkinds of situations. In addition, most correlations are not absolute,\nthey admit exceptions. Thus, for the signalling described in\n[E1] to be really informational, the extra\ncondition that the radar system is working properly must be\nmet. Conditional versions of the [IS Verification]\nprinciple may be used to insist that the carrier situation must meet\ncertain background conditions. The inability of an agent to keep track\nof changes on these background conditions may lead to errors. So, if\nthe radar is broken, the dot on the screen may end up moving upward\nwhile the plane is moving south. Unless the air controller is able to\nrecognise the problem, that is, unless she realises that the\nbackground conditions have changed, she may end up giving absurd\ninstructions to the pilot. Now, instructions are tied to actions. For\na treatment of actions from the situation-theoretical view, we refer\nthe reader to Israel and Perry (1991).  \nThe basic notion of information flow sketched in the previous section\ncan be lifted to a more abstract setting in which the supporters of\ninformation are not necessarily situations as concrete parts of the\nworld, but rather any entity which, as in the case of situations, can\nbe classified as being of or not of certain types. The mathematical\ntheory of distributed systems (Barwise and Seligman 1997) to be\ndescribed next takes this abstract approach by studying information\ntransfer within distributed systems in general.  \nA model of a distributed system in this framework will actually be a\nmodel of a kind of distributed system, hence the model of the\nradar-airplane system that we will use as a running example here will\nactually be a model of radar-airplane systems (in plural).\nSetting such a model requires describing the architecture of the\nsystem in terms of its parts and the way they are put together into a\nwhole. Once that is done, one can proceed to see how that architecture\nenables the flow of information among its parts.  \nA part of a system (again, really its kind) is modelled by saying how\nparticular instances of it are classified according to a given set of\ntypes. In other words, for each part of a system one has a\nclassification  \nwhere \\(\\models\\) is a binary relation such that \\(a \\models T\\) if\nthe instance \\(a\\) is of type \\(T\\).  In a simplistic analysis of the\nradar example, one could posit at least three classifications, one for\nthe monitor screen, one for the flying plane, and one for the whole\nmonitoring system:  \nA general version of a ‘part-of’ relation between\nclassifications is needed in order to model the way parts of a system\nare assembled together. Consider the case of the monitoring systems.\nThat each one of them has a screen as one of its parts means that\nthere is a function that assigns to each instance of the\nclassification MonitSit an instance of\nScreens. On the other hand, all the ways in which a\nscreen can be classified (the types of Screens)\nintuitively correspond to ways in which the whole screening system\ncould be classified: if a screen is part of a monitoring system and\nthe screen is blinking, say, then the whole monitoring situation is\nintuitively one of the type ‘its screen is blinking’.\nAccordingly, a generalised ‘part-of’ relation between any\ntwo arbitrary classifications \\(\\mathbf{A}, \\mathbf{C}\\)\ncan be modelled via two functions  \nthe first of which takes every type in \\(\\mathbf{A}\\) to its\ncounterpart in \\(\\mathbf{C}\\), and the second of which takes every\ninstance \\(c\\) of \\(\\mathbf{C}\\) to its\n \\(\\mathbf{A}\\)-component.[3] \nIf \\(f : \\mathbf{A} \\rightarrow \\mathbf{C}\\) is shortcut\nnotation for the existence of the two functions above (the pair\n\\(f\\) of functions is called an infomorphism), then an\narbitrary distributed system will consist of various classifications\nrelated by infomorphisms. For our purposes, it will suffice here to\nconsider three classifications \\(\\mathbf{A}, \\mathbf{B},\n\\mathbf{C}\\) together with two infomorphisms  \nThen a simple way to model the radar monitoring system would consist\nof the pair  \nThe common codomain in these cases \\((\\mathbf{C}\\) in the general\ncase and MonitSit in the example) works as a the core\nof a channel that connects two parts of the system. The core\ndetermines the correlations that obtain between the two parts, thus\nenabling information flow of the kind discussed in\n section 2.2.\n This is achieved via two kinds of links. On the one hand, two\ninstances \\(a\\) from \\(\\mathbf{A}\\) and \\(b\\) from\n\\(\\mathbf{B}\\) can be thought to be connected via the channel if\nthey are components of the same instance in \\(\\mathbf{C}\\), so the\ninstances of \\(\\mathbf{C}\\) act as connections between components.\nThus, in the radar example a particular screen will be connected to a\nparticular plane if they belong to the same monitoring situation.  \nOn the other hand, suppose that every instance in \\(\\mathbf{C}\\)\nverifies some relation between types that happen to be counterparts of\ntypes from \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\). Then such\nrelation captures a constraint on how the parts of the system\nare correlated. In the radar example, the theory of the core\nclassification MonitSit will include constraints such\nas PlainMovingNorth \\(\\Rightarrow\\) DotGoingUp.\nThis regularity of monitoring\nsituations, which act as connections between radar screen-shots and\nplanes, reveals a way in which radar screens and monitored planes\ncorrelate with each other. All this affords the following version of\ninformation transfer.  Channel-enabled signalling: Suppose that  \nThen instance \\(a\\) being of type \\(T\\) in \\(\\mathbf{A}\\) indicates\nthat instance \\(b\\) is of type \\(T'\\) in \\(\\mathbf{C}\\) if \\(a\\) and\n\\(b\\) are connected by a instance from \\(\\mathbf{C}\\) and the relation\n\\(f^{\\wedge}(T) \\Rightarrow g^{\\wedge}(T')\\) between the counterpart\ninterpreted types is satisfied by all instances of \\(\\mathbf{C}\\). \nNow, for each classification \\(\\mathbf{A}\\), the collection  \nformed by all the global constraints of the classification\ncan be thought of as a logic that is intrinsic to \\(\\mathbf{A}\\).\nThen a distributed system consisting of various classifications and\ninfomorphisms will have a logic of constraints attached to each part\nof\n it,[4]\n and more sophisticated questions about information flow within the\nsystem can be formulated.  \nFor example, suppose an infomorfism \\(f : \\mathbf{A} \\rightarrow\n\\mathbf{C}\\) is part of the distributed system under study. Then \\(f\\)\nnaturally transforms each global constraint \\(T\n\\Rightarrow T'\\) of \\(L_{\\mathbf{A}}\\) into \\(f^{\\wedge}(T)\n\\Rightarrow f^{\\wedge}(T')\\), which can always be shown to be an\nelement of \\(L_{\\mathbf{C}}\\). This means that one can reason within\n\\(\\mathbf{A}\\) and then reliably draw conclusions about\n\\(\\mathbf{C}\\). On the other hand, it can be shown that using\npreimages under \\(f^{\\wedge}\\) in order to translate global\nconstraints of \\(\\mathbf{C}\\) does not always guarantee the\nresult to be a global constraint of \\(\\mathbf{A}\\). It is then\ndesirable to identify extra conditions under which the reliability of\nthe inverse translation can be guaranteed, or at least improved. In a\nsense, these questions are qualitatively close to the concerns Shannon\noriginally had about noise and reliability.  \nAnother issue one may want to model is reasoning about a system from\nthe perspective of an agent that has only partial knowledge\nabout the parts of a system. For a running example, think of a plane\ncontroller who has only worked with ACME monitors and knows nothing\nabout electronics. The logic such an agent might use to reason about\npart \\(\\mathbf{A}\\) of a system (actually part\nScreens in the case of the controller) will in\ngeneral consist of some constraints that may not even be global, but\nsatisfied only by some subset of instances (the ACME monitors). The\nagent’s logic may be incomplete in the sense that it\nmight miss some of the global constraints of the classification (like\nthe ones involving inner components of the monitor). The agent’s\nlogic may also be unsound, in the sense that there might be\ninstances out of the awareness of the agent (say monitors of\nunfamiliar brands) that falsify some of the agent’s constraints\n(which do hold of all ACME monitors). A local logic \\(L\\) in\n\\(\\mathbf{A}\\) can be “moved” along an infomorphism \\(f :\n\\mathbf{A} \\rightarrow \\mathbf{C}\\) in the expected way, that\nis, its constraints are transformed via \\(f^{\\wedge}\\),\nwhile its instances are transformed via \\(f^{\\vee}\\).\nNatural questions studied in channel theory concerning these notions\ninclude the preservation (or not), under translation, of some\ndesirable properties of local logics, such as soundness.  \nA recent development in channel theory (Seligman 2014) uses a more\ngeneral definition of local logic, in which not all instances in the\nlogic need to satisfy all its constraints. This version of channel\ntheory is put to use in two important ways. Firstly, by using local\nlogics to stand for situations, and with a natural interpretation of\nwhat an infon should then be, a reconstruction is produced of the core\nmachinery of situation theory (barely presented in\n sections 2.1\n and\n section 2.2).\n Secondly, it is shown that this version of channel theory can deal\nwith probabilistic constraints. The rough idea is that any pair\nof a classification plus a probability measure over the set of\ninstances induces an extended classification with the same set of\ntypes, and where a constraint holds if and only if the set of\ncounterexample instances has measure 0. Notice that this set of\ncounterexamples might not be empty. Having probabilistic constraints\nis a crucial step towards the effort of formally relating channel\ntheory to Shannon’s theory of communication.  \nFor an extensive development of the theory of channels sketched here,\nplus several explorations towards applications, see Barwise and\nSeligman (1997). See van Benthem (2000) for a study of conditions\nunder which constraint satisfiability is preserved under\ninfomorphisms, and Allo (2009) for an application of this framework to\nan analysis of the distinction between cognitive states and\ncognitive commodities. Finally, it must be mentioned that the\nnotion of classification has been around for some years now in the\nliterature, having being independently studied and introduced under\nnames such as Chu spaces (Pratt 1995) or Formal Contexts (Ganter and Wille 1999).  \nFor information to be computed, it must be handled by the\ncomputational mechanism in question, and for such a handling to take\nplace, the information must be encoded. Information as\ncode is a stance that takes this encoding-condition very\nseriously. The result is the development of fine-grained models of\ninformation flow that turn on the syntactic properties of the encoding\nitself.  \nTo see how this is so, consider again cases involving information flow\nvia observations. Such observations are informative because we are not\nomniscient in the normal, God-like sense of the term. We have to go\nand observe that the cat is on the mat, for example, precisely because\nwe are not automatically aware of every fact in the universe.\nInferences work in an analogous manner. Deductions are informative for\nus precisely because we are not logically omniscient. We have\nto reason about matters, sometimes at great length, because we are not\nautomatically aware of the logical consequences of the body of\ninformation with which we are reasoning.  \nTo come full circle—reasoning explicitly with information\nrequires handling it, where in this case such handling is cognitive\nact. Hence the information in question is encoded in some manner,\nhence Information as code underpins the development of fine-grained\nmodels of information flow that turn on the syntactic properties of\nthe encoding itself, as well as the properties of the actions that\nunderpin the various information-processing contexts involved.  \nSuch information-processing contexts are not restricted to explicit\nacts of inferential reasoning by human agents, but include\nautomated reasoning and theorem proving, as well as\nmachine-based computational procedures in general. Approaches to\nmodelling the properties of these latter information-processing\nscenarios fall under algorithmic information theory.  \nIn\n section 3.1,\n we will explore a major approach to modelling the properties of\ninformation-processing within the information as code framework via\ncategorial information theory. In\n section 3.2,\n we will examine the more general approach to modelling information as\ncode of which categorial information theory is an instance, the\nmodelling of information as code via substructural logics. In\n section 3.3\n we will lay out the details of several other notable examples of\nlogics of information flow motivated by the information as code\napproach.  \nCategorial information theory is a theory of fine-grained\ninformation flow whose models are based upon those specified by the\ncategorial grammars underpinned by the Lambek Calculi, due originally\nto Lambek (1958, 1961). The motivation for categorial information\ntheory is to provide a logical framework for modelling the properties\nof the very cognitive procedures that underpin deductive reasoning.\n \nThe conceptual origin of categorial information theory is found in van\nBenthem (1995: 186). Understanding van Benthem’s use of\n“procedural” to be synonymous with “dynamic”:\n \n[I]t turns out that, in particular, the Lambek Calculus itself permits\nof procedural re-interpretation, and thus, categorial calculi may turn\nout to describe cognitive procedures just as much as the syntactic or\nsemantic structures which provided their original motivation. \nThe motivation for categorial information theory is to model the\ncognitive procedures constituting deductive reasoning. Consider as an\nanalogy the following example. You arrive home from IKEA with an\nunassembled table that is still flat-packed in its box. Now the\nquestion here is this, do you have your table? Well, there is a sense\nin which you do, and a sense in which you do not. You have your table\nin the sense that you have all of the pieces required to construct or\ngenerate the table, but this is not to say that you have the table in\nthe sense that you are able to use it. That is, you do not\nhave the table in any useful form, you have merely pieces of a table.\nIndeed, getting these table-pieces into their useful form, namely a\ntable, may be a long and arduous process…  \nThe analogy between the table-example above and deductive reasoning is\nthis. It is said often that the information encoded by (or\n“contained in” or “expressed by”) the\nconclusion of a deductive argument is encoded by the premises. So,\nwhen you possess the information encoded by the premises of some\ninstance of deductive reasoning, do you possess the information\nencoded by the conclusion? Just as with the table-pieces, you do not\npossess the information encoded by the conclusion in any useful form,\nnot until you have put the “information-pieces”\nconstituting the premises together in the correct manner. To be sure,\nwhen you possess the information-pieces encoded by the premises, you\npossess some of the information required for the construction or\ngeneration of the information encoded by the conclusion. As with the\ntable-pieces however, getting the information encoded by the\nconclusion from the information encoded by the premises may be a long\nand arduous process. You need also the instructional information that\ntells you how to combine the information encoded by the premises in\nthe right way. This information-generation via deductive inference may\nbe thought of also as the movement of information from implicit to\nexplicit storage in the mind of the reasoning agent, and it is the\ncognitive procedures facilitating this storage transfer that motivate\ncategorial information theory.  \nCategorial information theory is a theory of dynamic information\nprocessing based on the merge/fusion \\((\\otimes)\\) and typed\nfunction \\((\\rightarrow , \\leftarrow)\\) operations from categorial grammar. The\nconceptual motivation is to understand the information in the mind of\nan agent as the agent reasons deductively to be a database in much the\nsame way as a natural language lexicon is a database (see\nSequoiah-Grayson (2013), (2016)). In this case, a grammar\nwill be understood as a set of processing constraints so imposed as to\nguarantee information flow, or well-formed strings as outputs. Recent\nresearch on proofs as events from a very similar conceptual\nstarting point may by found in Stefaneas and Vandoulakis\n(forthcoming).  \nCategorial information theory is strongly algebraic in flavour. Fusion\n‘\\(\\otimes\\)’ corresponds to the binary composition operator\n‘.’, and ‘\\(\\vdash\\)’ to the partial order\n‘\\(\\le\\)’ (see Dunn 1993). The merge and function operations\nare related to each other via the familiar residuation\nconditions:  \nIn general, applications for directional function application will be\nrestricted to algebraic analyses of grammatical structures, where\ncommuted lexical items will result in non-well-formed strings.  \nDespite its algebraic nature, the operations can be given their\nevaluation conditions via “informationalised” Kripke\nframes (Kripke 1963, 1965). An information frame (Restall 1994)\n\\(\\mathbf{F}\\) is a triple \\(\\langle S, \\sqsubseteq, \\bullet\\rangle\\).\n\\(S\\) is a set of information states \\(x, y, z\\ldots\\) . \\(\\sqsubseteq\\)\nis a partial order of informational development/inclusion such that\n\\(x \\sqsubseteq y\\) is taken to mean that the information carried by\n\\(y\\) is a development of the information carried by \\(x\\), and \\(\\bullet\\)\nis an operation for combining information states. In other words, we\nhave a domain with a combination operation. The operation of\ninformation combination and the partial order of information inclusion\ninterrelate as follows:\n \nReading \\(x \\Vdash A\\) as state \\(x\\) carries\ninformation of type \\(A\\), we have it that:  \nAt the syntactic level, we read \\(X \\vdash A\\) as\nprocessing on \\(X\\) generates information of type A. In\nthis case we are understanding \\(\\vdash\\) as an information processing\nmechanism as suggested by Wansing (1993: 16), such that \\(\\vdash\\)\nencodes not just the output of an information processing procedure,\nbut the properties of the procedure itself. Just what this processing\nconsists of will depend on the processing constraints that we set up\non our database. These processing constraints will be imposed in order\nto guarantee an output from the processing itself, or to put this\nanother way, in order to preserve information flow. Such processing\nconstraints are fixed by the presence or absence of various\nstructural rules, and structural rules are the business of\nsubstructural logics.  \nCategorial information theory is precipitated by giving the Lambek\ncalculi an informational semantics. At a suitable level of\nabstraction, the Lambek calculi is seen to be a highly expressive\nsubstructural logic. Unsurprisingly, by giving an\ninformational semantics for substructural logics in general, we get a\nfamily of logics that exemplify the information as code approach. This\nlogical family is organised by expressive power, with the expressive\npower of the logics in question being captured by the presence of\nvarious structural rules.  \nA structural rule is of the following general form:  \nWe may read (11) as any information generated by processing on\n\\(X\\) is generated by processing on \\(Y\\) also. Hence the\nlong-form of (11) is as follows:  \nHence \\(X\\) is a structured body of information, or “data\nstructure” as Gabbay (1996: 423) puts it, where the actual\narrangement of the information plays a crucial role. The\nstructural rules will fix the structure of the information encoded by\n\\(X\\), and as such impact upon the granularity of the information\nbeing processed.  \nConsider Weakening, the most familiar of the structural rules\n(followed by its corresponding frame condition:  \nWith Weakening present, we loose track of which pieces of information\nwere actually used in an inference. This is precisely why it is that\nthe rejection of Weakening is the mark of relevant logics, where the\npreservation of bodies of information relevant to the derivation of\nthe conclusion is the motivation. By rejecting Weakening, we highlight\na certain type of informational taxonomy, in the sense that\nwe know which bodies of information were used. To preserve\nmore structural detail than simply which bodies of information were\nused, we need to consider rejecting further structural rules. \nSuppose that we want to record not only which pieces of information\nwere used in an inference, but also how often they were used. In this\ncase we would reject Contraction:  \nContraction allows the multiple use, without restriction, of a piece\nof information. So if keeping a record of the “informational\ncost” of the execution of some information processing is a\nconcern, Contraction will be rejected. The rejection of Contraction is\nthe mark of linear logics, which were designed for modelling just such\nprocessing costs (see Troelstra 1992).  \nIf we wish to preserve the order of use of pieces of\ninformation, then we will reject the structural rule of Commutation:\n \nInformation-order will be of particular concern in temporal settings\n(consider action-composition) and natural language semantics (Lambek\n1958), where non-commuting logics first appeared. Commutation comes\nalso in a more familiar strong form:  \nThe strong form of Commutation results from its combination with the\nstructural rule of\n Association:[5] \nRejecting Association will preserve the precise fine-grained\nproperties of the combination of pieces of information.\nNon-associative logics were introduced originally to capture the\ncombinatorial properties of language syntax (see Lambek 1961).  \nIn the presence of Commutation, a double implication pair \\((\\rightarrow ,\n\\leftarrow)\\) collapses into single implication \\(\\rightarrow\\). In the presence of\nall of the structural rules, fusion, \\(\\otimes\\), collapses into Boolean\nconjunction, \\(\\wedge\\). In this case, the residuation conditions outlined\nin (5) and (6) collapse into a mono-directional function.  \nThe choice of which structural rules to retain obviously depends on\njust what informational phenomena is being modelled, so there is a\nstrong pluralism at work. By rejecting Weakening say, we are\nspeaking of which data were relevant to the process, but are\nsaying nothing about its multiplicity (in which case we would reject\nContraction), its order (in which case we would reject Commutation),\nor the actual patterns of use (in which case we would reject\nAssociation). By allowing Association, Commutation, and Contraction,\nwe have the taxonomy locked down. We might not know the order or\nmultiplicity of the data that were used, but we do know what types,\nand exactly what types, were relevant to the successful processing.\nThe canonical contemporary exposition of such an information-based\ninterpretation of propositional relevant logic is Mares (2004). Such\nan interpretation allows for an elegant treatment of the\ncontradictions encoded by relevant logics. By distinguishing between\ntruth conditions and information conditions, we\nallow for an interpretation of \\(x \\Vdash A \\wedge \\neg A\\) as \\(x\\) carries the\ninformation that \\(A\\) and not \\(A\\). For an exploration of\nthe distinction between truth-conditions and information-conditions\nwithin quantified relevant logic, see Mares (2009).  \nAt such a stage, things are still fairly static. By shifting\nour attention from static bodies of information, to the manipulation\nof these bodies, we will reject structural rules beyond\nWeakening, arriving ultimately at categorial information theory, as it\nis encoded by the very weakest substructural logics. Hence the weaker\nwe go, the more “procedural” the flavour of the logics\ninvolved. From a dynamic/procedural perspective, linear logics might\nbe thought of as a “half way point” between static\nclassical logic, and fully procedural categorial information theory.\nFor a detailed exposition of the relationship between linear logic and\nother formal frameworks in the context of modelling information flow,\nsee Abramsky (2008).  \nRecent important work by Dunn (2015) ties\nsubstructural logics and structural rules together with\ninformational relevance in the following way. Dunn makes a\ndistinction between  programs and data, with the\nformer being dynamic and the latter static. We may think of programs\nas conditional statements of the form \\(A \\rightarrow B\\), and of\ndata as atomic propositions \\(A, B\\) etc. Given these two\ntypes of information artefacts, we have three possible combinations,\nprogram to data combination, program to program combination, and data\nto data combination. For program to data combination, commutation will\nhold whilst weakening and association will fail, and contraction not\napplying. For program to program combination association will hold,\nwhilst commutation, weakening fail. As demonstrated in\nSequoiah-Grayson (2016), the case of contraction for program to\nprogram combination is more complicated. The exact properties of data\nto data combination remain an interesting open issue. The connection\nwith informational relevance is made by interpreting the partial order\nrelation \\(\\sqsubseteq\\) as marking information relevance itself. In this\ncase, \\(x \\sqsubseteq y\\) is read as the information x\nis relevant to the information y. To what it is exactly that\ninformational relevance amounts will depend on the precise context of\ninformation processing in question. Sequoiah-Grayson (2016) extends\nthe framework about to contexts of information processing by an agent\nas the agent reasons explicitly. Given that the combination of\ninformation states \\(x \\bullet y\\) may sit on the left hand\nside of the partial order relation, the extension is an account of the\nepistemic relevance of epistemic actions. For a collection of recent\npapers exploring the information as code approach in depth, see Bimbo\n(2016). \nThe information as code approach is a very natural perspective on\ninformation flow, hence there are a number of related frameworks that\nexemplify it.  \nOne such approach to analysing information as code is to carry out\nsuch an analysis in terms of the computational complexity of various\npropositional logics. Such an approach may propose a hierarchy of\npropositional logics that are all decidable in polynomial time, with\nthis hierarchy being structured by the increasing computational\nresources required for the proofs in the various logics.\nD’Agostino and Floridi (2009) carry out just such an analysis,\nwith their central claim being that this hierarchy may be used to\nrepresent the increasing levels of informativeness of propositional\ndeductive reasoning.  \nGabbay’s (1993, 1996) framework of labelled deductive\nsystems exemplifies the information as code approach in manner\nvery similar to the informationalised substructural logics of\n section 3.1.\n An item of data (note that Gabbay refers to both atomic and\nconditional information as data, in contrast to Dunn and\nSequoiah-Grayson in the section above) is given as a pair of the form\n\\(x : A\\), where \\(A\\) is a piece of declarative\ninformation, and \\(x\\) is a label for \\(A. x\\) is a\nrepresentation of information that is needed operate on or alter the\ninformation encoded by \\(A\\). Suppose that we have also the\ndata-pair \\(y : A \\rightarrow B\\). We may apply \\(x\\)\nto \\(y\\), resulting in the data-pair \\(x + y : B\\)\nIn this case, a database is a configuration of labelled formulas, or\ndata-pairs (Gabbay 1993: 72). The labels and their corresponding\napplication operation are organised by an algebra, and the properties\nof this algebra will impose constraints on the applications operation.\nDifferent constraints, of “meta-conditions” as Gabbay\ncalls them (Gabbay 1993: 77), will correspond to different logics. For\nexample, if we were to ignore the labels, then we would have classical\nlogic, if we were to accept only the derivations which used all of the\nlabelled assumptions, then we would have relevance logic, and if we\naccepted only the derivations which used the labelled assumptions\nexactly once, then we would have linear logic. Labels are behaving\nvery much like possible worlds here, and the short step from possible\nworlds to information states makes it obvious how it is that the\nmeta-conditions on labels may be captured by structural rules.  \nArtemov’s (2008) framework of justification logic\nshares many surface similarities with Gabbay’s system of\nlabelled deduction. The logic is composed of justification\nassertions of the form \\(x : A\\), read as \\(x\\)\nis a justification for \\(A\\). Justifications themselves are\nevidential bases of varying sorts that will vary depending on the\ncontext. They might be mathematical proofs, sets of causes or\ncounterfactuals, or something else that fulfils the justificatory\nrole. What it means for \\(x\\) to justify \\(A\\) is not analysed\ndirectly in justification logic. Rather, attempts are made to\ncharacterise the justification relation \\(x : A\\) itself,\nvia various operations and their axioms. The application operation,\n‘.’ mimics the application operation ‘+’ from\nlabelled deduction, or the fusion ‘\\(\\otimes\\)’ operation\nfrom categorial information theory. In justification logic, the symbol\n‘+’ is reserved for the representation of joint evidence.\nHence ‘\\(x + y\\)’ is read as ‘the\njoint evidence of \\(x\\) and \\(y\\)’. Application and\njoin are characterised in justification logic by the following axioms\nrespectively:  \nThe latter axiom characterises the monotonicity of joint evidential\nbases. Apart from the commutativity of +, the structural properties of\nthe justification operations are currently unexplored, although the\npotential for such an exploration is exciting. Justification logic is\nused to analyse notoriously difficult epistemic problems such as the\nGettier cases and more. If we take our epistemology to be\ninformationalised, then the constitution of evidential bases as\ninformation states places justification logics within the information\nas code approach in a straightforward manner. For further details, see\nArtemov and Fitting (2012).  \nZalta’s work on object theory (Zalta 1983, 1993) provides a\ndifferent way to analyse informational content—understood as\npropositional content—and its structure. Motivated by\nmetaphysical considerations, object theory starts by proposing a\ntheory of objects and relations (usually formulated in a second order\nquantified modal language). This theory can then be used to define and\ncharacterise states of affairs, propositions, situations, possible\nworlds, and other related notions. The resulting picture is one where\nall these things have internal structure, their algebraic properties\nare axiomatized, and one can therefore reason about them in a\nclassical proof-theoretical way.  \nA philosophical point touched by this approach concerns the link\nbetween the propositional content (information) expressed by sentences\nand the idea of predication. Relevant to this entry is Zalta’s\n(1993) development of a version of situation theory that follows this\napproach, and where a key element is the usage of two forms of\npredication. Briefly, the formula ‘\\(Px\\)’ corresponds\nto the usual form of predication by exemplification (as in\n“Obama is American”), while ‘\\(xP\\)’\ncorresponds to predication via encoding. Abstract objects are\nthen defined to be (essentially) encodings of properties, in\ncombinations which might not even be made factual. These provisions\nenable the existence of information about abstract, possible, or\nfictional entities. For details on the tradition to which object\ntheory belongs see Textor (2012), McGrath (2012), and King (2012).\n \nWhile the three approaches discussed above (range, correlations, code)\ndiffer in that they emphasise different informational themes, the\nunderlying notion they aim to clarify is the same (information). It is\nthen natural to find that the similarities and synergies between the\napproaches invite the exploration of ways to combine them. Each one of\nthe next subsections illustrates how one could bring together two out\nof the three approaches.\n Section 4.1\n exemplifies the interface between the info-as-range and\ninfo-as-correlation views. Sections\n 4.2\n and\n 4.3\n do the same with the other two pairs of combinations, namely code and\ncorrelations, and code and ranges.  \nA central intuition in the information-as-range view is the\ncorrespondence that exists between information at hand (where this can\nbe qualified in various ways) and the range of possibilities which are\ncompatible with such information. On the other hand, a key feature of\nthe correlational approach to information is its reliance on a\nstructured information system formed by components that are\nsystematically connected. In general, many properties of a structured\nsystem will actually be local properties, in that they are\ndetermined by only some of the components (the fact that there is a\ndot moving upwards in a radar can be determined only by looking at the\nscreen, even if this behaviour is correlated with the motion of a\nremote plane, which is another component of the system). If one has\naccess to information pertaining to only a few of the many components\nof a system, a natural notion of range of possibilities arises,\nconsisting of all the possible global configurations of the system\nthat are compatible with such local information. This subsection\nexpands on this particular way to link the two approaches, but as it\nwill be noted at the end, this is not the only one and the search for\nother ways lies ahead as an open area of inquiry.  \nFormally, the link between ranges and correlations described above may\nbe approached by using a restricted product state space as a\nmodel of the architecture of the system (van Benthem 2006, van Benthem\nand Martinez 2008). The basic structures are constraint\nmodels, versions of which have been around in the literature for\nsome years (for example Fagin et al. 1995 in the study of epistemic\nlogic, and Ghidini and Giunchiglia 2001 in the study of context\ndependent reasoning). Constraint models have the form  \nHere, the basic component spaces are indexed by Comp, the\nstates of each component are taken from States (with\ndifferent components using maybe only a few of the elements of\nStates), and the global states of the system are global\nvaluations, that is, functions that assign a state to each basic\ncomponent Comp. Not all such functions are allowed, only\nthose in \\(C\\). Finally, Pred is a labelled family of\npredicates (sets of global states).  \nTo see how this fits with the information-as-correlation view,\nconsider again the example of planes being monitored by radars. As\nbefore, each monitoring situation will be modelled as having only two\nparts, now indexed by the members of \\(Comp = \\{ screen,\nplane\\}\\). The actual instances of screening situations would\ncorrespond to global states, which in this case — where we have\nonly two components — can be thought of as pairs \\((s, b)\\) where\n\\(s\\) is a particular screen and \\(b\\) a particular plane. Hence,\nglobal states connect instances of parts, so representing instances of\na whole system. But then a crucial restriction comes into play,\nbecause not all screens are connected with all planes, only with those\nbelonging to the same monitoring situation. The set \\(C\\) selects only\nsuch permissible pairs, thus playing a role similar to that of a\nchannel in\n section 1.\n Finally, Pred classifies global states into types, similar to\nthe classification relations of\n section 2.3.\n  \nAs we said before, some properties of systems are local properties,\nwith only some of the components of the systems being relevant in\ndetermining whether they hold or not. That a monitoring situation is\none where the plane is moving North depends only on the plane, not on\nthe screen. In general, if a property is completely determined by\nsubset of components \\(\\mathbf{x}\\) then, in what concerns that\nproperty, any two global states that agree on \\(\\mathbf{x}\\)\nshould be indistinguishable. In fact, each such \\(\\mathbf{x}\\)\ninduces an equivalence relation of local property determination so\nthat for every two global states \\(\\mathbf{s}, \\mathbf{t}\\):  \n\\(\\mathbf{s} \\sim_{\\mathbf{x}}\\mathbf{t}\\) if and only if\nthe values of \\(\\mathbf{s}\\) and \\(\\mathbf{t}\\) at each one of\nthe components in \\(\\mathbf{x}\\) are the same. \nIn this way one gets not only a conceptual but also formal link to the\ninformation-as-range approach, because constraint models can be used\nto interpret a basic modal language with atomic formulas of the form\n\\(P\\)—where \\(P\\) is one of the labels of predicates in\nPred—and with complex formulas of the form \\(\\neg \\phi,\n\\phi \\vee \\psi, U\\phi\\), and \\(\\Box_{\\mathbf{x}}\\phi\\), where\n\\(\\mathbf{x}\\) is a partial tuple of components and \\(U\\) is the\nuniversal modality.  More concretely, given a constraint model\n\\(\\mathscr{M}\\) and a global state \\(s\\), the crucial satisfaction\nconditions are given by:  \nThe resulting logic is axiomatised by the fusion of\n\\(S_5\\) modal logics for the universal modality \\(U\\)\nand each one of the \\(\\Box_{\\mathbf{x}}\\) modalities,\nplus the addition of axioms of the form \\(U \\phi \\rightarrow \\Box_{\\mathbf{x}}\\phi\\), and\n\\(\\Box_{\\mathbf{x}}\\phi \\rightarrow \\Box_{\\mathbf{y}}\\phi\\) whenever\n\\(\\sim_{\\mathbf{y}} \\subseteq \\sim_{\\mathbf{x}}\\).  \nThe information-as-range research agenda includes other topics, such\nas agency and the dynamics of information update, which can in\nprinciple be incorporated to the constraint models setting. For\nexample, in the case of agency, to the architectural structure of a\nstate system captured by a constraint model one could add epistemic\naccessibility relations for a group of agents \\(\\mathcal{A}\\),\nso to obtain epistemic constraint\nmodels of the form  \nwhere \\(\\approx_a\\) is the equivalence accessibility\nrelation of agent \\(a\\). Here one could refine the planes and radar\nexample above by adding some agents, say the controller and the pilot.\nBy relying only on the controls each agent can see, the controller\nwill not be able to distinguish states that agree on the direction of\nthe plane but differ, say, on the metereological conditions around the\nplane. Those states will be related by the controller’s relation\nin the model, but not by the pilot’s relation. In principle,\nthis merge of modal epistemic models and constraint models allows one\nto study, in a single setting, aspects of both the\ninformation-as-range and information-as-correlation points of view.\nThe corresponding logical language for epistemic constraint models is\nthe same as for basic constraint models, expanded with the\n\\(K_i\\) modal operators, one per agent. The logic\nis the fusion of the constraint logic from above and a\n\\(S_5\\) logic per each agent \\(a\\).  \nThere are some newer, different approaches to information modelling\nthat sit at the intersection of the information as range and\ninformation as correlation perspectives. One is van Benthem’s\nwork on information tracking (van Benthem 2016). Tracking is a new\nperspective that addresses both the connections between different\nrepresentations of information on the one hand, and the updates on\nthese connections on the other.  \nAnother recent development (Baltag 2016) comes from a line of work\nthat studies how to capture, in the style of epistemic logics such as\nthose described in\n section 1,\n the properties and dynamics of knowledge de re (Wang and Fan\n2014). Identifying this kind of knowledge with knowledge of the value\nof a variable, Baltag’s insight is to add, to the language of\nbasic epistemic logic, the usual first-order resources for\nconstructing terms and basic formulas (that is, symbols of constants,\nfunctions, relations, and variables), plus, crucially, a generalised\nconditional knowledge operator \\(K_{a}^{t_1 ,\\ldots ,t_n}\\).  The\nextended language has now formulas \\(K_{a}^{t_1 ,\\ldots ,t_n} t\\) and\n\\(K_{a}^{t_1 ,\\ldots ,t_n} \\phi\\), with the intended meaning that\nagent \\(a\\) knows the value of term \\(t\\) (or knows that \\(\\phi\\), for\nthe second formula), provided it knows the values of terms \\(t_1\n,\\ldots ,t_n\\). To be able to capture this idea on the semantic side,\nKripke models are enriched so that, in addition to the usual set of\ninformation states, interpretations for propositional letters, and\nagents relations, we will also have a domain of objects over which\nterms and basic relational formulas are locally interpreted at each\nstate (that is, the interpretations can vary from state to state, but\nthe underlying domain is the same across states). A sound and complete\naxiomatisation exists, and the resulting logical system is a sort of a\ngeneral, yet decidable, dependence logic where information about\ncorrelations can be captured via the conditional knowledge\noperators. Dynamic versions are also obtained where, in addition to\nthe public announcement operator \\([\\phi]\\), one has value\nannouncement operators \\([t_1,\\ldots ,t_n]\\), with formula\n\\([t_1,\\ldots ,t_n] \\phi\\) being read as “after the simultaneous\nannouncement of the values of terms \\(t_1,\\ldots ,t_n\\), it is the\ncase that \\(\\phi\\)”.  \nYet other links between the approaches have also be found, which are\nmotivated by other kind of questions and use formalisms that are\ncloser to the situation-theoretic ones. For example, consider a\nsetting in which agents have incomplete information about an\nintended subset of a set of epistemic states. How can a relation of\naccessibility arise from such a setting? (Notice that this is\ndifferent to the setting of epistemic constraint models described\nabove, where agents do have complete information about what holds true\nof all the epistemically accessible worlds). One way to address this\nquestion (Barwise 1997) is to consider a fixed classification\n\\(A\\), the instances of which are the epistemic states, plus a\nlocal logic per agent attached to each state. For some states these\nlocal logics may be incomplete (see\n section 2.3),\n so agents may not have information about everything that holds true\nof the intended range of states. Then, roughly, the states accessible\nfrom a given state \\(s\\) and agent \\(a\\) will be those whose\nproperties (types) do not contradict the local logic of \\(a\\) in\n\\(s\\). With these epistemic relations in place, classification\n\\(A\\) can be used to interpret a basic modal language.  \nLogical frameworks that crossover information as code and information\nas correlation get their most explicit representation in work that\ndoes just this—model the crossover between the two frameworks.\nRestall (1994) and Mares (1996) give independent proofs of the\nrepresentability of Barwise’s information as correlation\nchannel-theoretic framework within the information as code approach as\nexemplified by the substructural logics framework. In this section we\nwill trace the motivations and the main details of the proof, before\ndemonstrating the connection with category theory.  \nThe basic steps are these—if we understand information channels\nto be information states of a special sort, namely the sort of\ninformation state that carries information of conditional types, then\nthere is an obvious meeting point between information as correlation\nas exemplified by channel theory, and information as code as\nexemplified by informationalised substructural logics. The\nintermediate step is to reveal the connection between channel\nsemantics for conditional types, and the frame semantics for\nconditionals given by relevance logics.  \nStarting with the channel theoretic analysis of conditionals, as noted\nalready, the running motivation behind Barwise’s\nchannel-theoretic framework is that information flow is underpinned by\nan information channel. Barwise understood conditionals as\nconstraints in the sense that \\(A \\rightarrow B\\) is a\nconstraint from \\(A\\) to \\(B\\) in the sense of \\(A \\Rightarrow B\\) from\n section 2.2\n above. If the information that \\(A\\) is combined with the\ninformation encoded by the constraint, then the result or output is\nthe information that \\(B\\).  \nThe information that \\(A\\) and that \\(B\\) is carried by the\nsituations \\(s_1, s_2\\ldots\\). and the\ninformation encoded by the constraint is carried by an information\nchannel \\(c\\). Given this, Barwise’s evaluation condition for\na constraint is as follows (the condition is given here in\nBarwise’s notation from his later work on conditionals, although\nin earlier writings such conditions appeared in the notation given in\n section 2.2\n above):  \nwhere \\(s_1 \\stackrel{c}{\\mapsto} s_2\\) is read as \nthe information carried by the channel \\(c\\), when combined with the\ninformation carried by the situation \\(s_1\\), results in the\ninformation carried by the situation \\(s_2\\). \nObviously enough, this is very close in spirit to (9) in the section\non information as code above.  \nAs noted above, the intermediate step concerns the ternary relation\n\\(R\\) from the early semantics for relevance logic. The semantic\nclause for the conditional from relevance logic is:  \n\\(Rxyz\\) is, by itself, simply an abstract mathematical entity. One\nway or reading it, the way that became popular in relevance logic\ncircles, is \n\\(Rxyz\\) iff the result of combining \\(x\\) with \\(y\\) is true\nat \\(z\\). \nGiven that the points of evaluation in relevance logics were\nunderstood originally as impossible situations (since they may be both\ninconsistent and incomplete), the main conceptual move was to\nunderstand channels to be special types of situations. The full proofs\nmay be found in Restall (1994) and Mares (1996), and these demonstrate\nthat the expressive power of Barwise’s system may be captured by\nthe frame semantics of relevance logic. What it is that such\n“combining” of \\(x\\) and \\(y\\) amounts to depends\non, of course, which structural rules are operating on the frame in\nquestion. As explained in the previous section above, the choice of\nwhich rules to include will depend on the properties of the phenomena\nbeing modelled.  \nThe final step required for locating the meeting point between\ninformation as code and information as correlation is as follows.\nContemporary approaches to relevance and other substructural logics\nunderstand the points of evaluation (impossible situations) to be\ninformation states. There is certainly no constraint on information\nthat it be complete or consistent, so the expressibility of impossible\nsituations it not sacrificed. Such an informational reading (Paoli\n2002; Restall 2000; Mares 2004) lends itself to multiple applications\nof various substructural frameworks, and also does away with the\nontological baggage brought by questions like “what are\nimpossible situations?” in the “What are possible\nworlds?” spirit. An information-state reading of \\(Rxyz\\)\nwill be something like \nthe result of combining the information carried by \\(x\\) and\n\\(y\\) generates the informations carried by \\(z\\). \nMaking this explicit results in \\(Rxyz\\) being written down as \\(x\n\\bullet y \\sqsubseteq z\\), in which case (15) is, via (16), equivalent\nto (9).  \nAn important structural rule for the composition operation on\ninformation channels, that is, on information states that carry\ninformation of conditional types, is that it is associative. What this\nmeans is that:  \nWhere \\(z \\Vdash A\\) and \\(w \\Vdash D\\), this will be the case for all\n\\(x, y, v\\) s.t. \\(x \\Vdash A \\rightarrow\\), \\(y \\Vdash B \\rightarrow\nC\\), \\(v \\Vdash C \\rightarrow D\\). This is just the first step\nrequired to demonstrate that channel theory, and its underlying\nsubstructural logic, form a category.  \nCategory theory is an extremely powerful tool in its own right. For a\nthorough introduction see Awodey (2006). For more work on the\nrelationship between various substructural logics and channel theory,\nsee Restall (1994a, 1997, 2006). Further category-theoretic work on\ninformation flow may be found in Goguen (2004—see Other Internet\nResources). Recent important work on category-theoretic frameworks for\ninformation flow that extend to quantifiable/probabilistic\nframeworks is due to Seligman (2009). Perhaps the most in depth\ntreatment of information flow in category theoretic terms is to be\nfound in the work of Samson Abramsky, and an excellent overview may be\nfound in his “Information, Processes, and Games” (2008).\nRecent work on the intersection between information as code and\ninformation as correlation uses substructural logics (relevance and\nlinear logics in particular) to model logical proofs as information\nsources themselves. A proof is a source of information par\nexcellence, and the contributions in the area by Mares (2016) are\nvital. \nExcitingly, there has been a recent surge in the recent development of\ninformation logics that combine the flexibility of categorial\ninformation theory with the subject matter of dynamic epistemic logics\nin order to design  substructural epistemic logics. Sedlar\n(2015) combines the modal epistemic logics of implicit knowledge and\nbelief with substructural logics in order to capture the availability\nof evidence for the agent. Aucher (2015, 2014) redefines dynamic\nepistemic logic as a substructural logic corresponding to the Lambek\nCalculi of categorial information theory. Aucher shows also that the\nsemantics for DEL can be understood as providing a conceptual\nfoundation for the semantics of substructural logics in general. See\nHjortland and Roy (2016) for an extension of Aucher’s approach\nto soft information \nOther logical frameworks that model information as code and range\nalong with information about encoding have been developed by\nVelázquez-Quesada (2009), Liu (2009), Jago (2006), and others.\nThe key element to all of these approaches is the introduction of some\nsyntactic code to the conceptual architecture of the information as\nrange approach.  \nTaking Velázquez-Quesada (2009) as a working example, start\nwith a modal-access model \\(M =\\langle S, R, V, Y, Z\\rangle\\)\nwhere \\(\\langle S, R, V \\rangle\\) is a Kripke Model, \\(Y\\) is\nthe access set function, and \\(Z\\) is the rule set\nfunction s.t.  (where \\(I\\) is the set of classical propositional\nlanguage based on a set of atomic propositions):  \nA modal-access model is a member of the class of modal access models\n\\(\\mathbf{MA}\\) iff it satisfies truth for formulas and truth\npreservation for rules. \\(\\mathbf{MA}_k\\) models\nare those \\(\\mathbf{MA}\\) models such that \\(R\\) is an\nequivalence relation.  \nFrom here, inference is represented as a modal operation adding the\nrule’s conclusion to the access set of information states of the\nof the agent such that the agent can access both the rule and its\npremises. Where \\(Y(x)\\) is the access set at \\(x\\), and\n\\(Z(x)\\) is the rule set at \\(x\\):  \nInference on knowledge: Where \\(M = \\langle S, R,\nV, Y, Z\\rangle \\in \\mathbf{MA}_k\\), and \\(\\sigma\\) is a rule,\n\\(M_k\\sigma = \\langle S, R, V, Y', Z\\rangle\\) differs from \\(M\\) in\n\\(Y'\\), given by \\(Y'(x) := Y(x) \\cup \\{\\)conc\\((\\sigma)\\}\\) if\n\\(\\text{prem}(\\sigma) \\subseteq Y(x)\\) and \\(\\sigma \\in Z(x)\\), and by\n\\(Y'(x) := Y(x)\\) otherwise. \nThe dynamic logic for inference on knowledge then incorporates the\nability to represent “there is a knowledge inference\nwith \\(\\sigma\\) after which \\(\\phi\\) holds”\n(Velázquez-Quesada 2009). It is in just this sense that such\nmodal information theoretical approaches model the outputs of\ninferential processes, as opposed to the properties of the inferential\nprocesses that generate such outputs (see the section on\ncategorial information theory for models of such dynamic\nproperties).  \nJago (2009) proposes a rather different approach based upon the\nelimination of worlds considered possible by the agent as the\nagent reasons deductively. Such epistemic (doxastic) possibilities\nstructure an epistemic (doxastic) space under bounded rationality. The\nconnection with information as code is that the modal space is\nindividuated syntactically, with the worlds corresponding to possible\nresults of step-wise rule-governed inferences. The connection with\ninformation as range is that the rules that he agent does or does not\nhave access to will impact upon the range of discrimination for the\nagent. For example, if the agent’s epistemic-base contains two\nworlds, a \\(\\neg \\phi\\) world and a \\(\\phi \\vee \\psi\\) world say, then can\nrefine their epistemic base only if they have access to the\ndisjunctive syllogism rule.  \nA subtle but important contribution of Jago’s is the following:\nthe modal space in question will contain only those epistemic options\nwhich are not obviously impossible. However, what is or is\nnot obviously impossible will vary from both agent to agent, as well\nas for a single agent over time as that agent refines its logical\nacumen. This being the case, the modal space in question has\nfuzzy boundaries.  \nThere is a varied list of special topics pertaining to the logical\napproach to information. This section briefly illustrates just a\ncouple of them, which are important regardless of the particular\nstance one takes (information as range, as correlation, as code). The\nfirst topic is the issue of informational equivalence: when are two\nstructures in the logical approach one is using indistinguishable in\nterms of the information they are meant to encode, convey, or carry?\nThe second topic concerns the various ways in which the idea of\nnegative information can be understood conceptually, and properly\ndealt with formally.  \nEvery logical approach to information comes with its own kind of\ninformation structures. Depending on the particular stance and the\naspect of information to be stressed, these structures may stand for\ninformational states, structured syntactic representations, pieces of\ninformation understood as commodities, or global structures made up\nfrom local interrelated informational states or stages. Under which\nconditions can two informational structures be considered to be\ninformationally equivalent?  \nAddressing this question brings out the need to have it clear at which\nlevel of granularity one is testing for equivalence. Take for example\nthe classical extensional notion of logical equivalence. This is a\ncoarse equivalence, in that informationally different claims such as 2\nis even and 2 is prime cannot be distinguished, as\ntheir extensions will coincide. Take instead an equivalence given by\nidentity at the level of representations (say syntactic equality).\nThis is on the contrary too fine-grained in some cases: to a bilingual\nspeaker, the information that the shop is closed would be equally\nconveyed by a sign saying “Closed” as by a sign saying\n“Geschlossen”, even if the two words are\ndifferent.  \nAn intermediate notion of equivalence that has proved central to the\nrange, correlational, and code views on information is the relation of\nbisimulation between structures. A bisimulation relation between two\ngraphs \\(G\\) and \\(H\\) (where both the arrows and nodes of the\ngraphs are labelled) is a binary relation \\(R\\) between the nodes\nof the graphs with the property that whenever a node \\(g\\) of\n\\(G\\) is related to a node \\(h\\) of \\(H\\), then:  \nA simple example would be the relation between the following two\ngraphs (empty set of labels) that relates the point \\(x\\) with\n\\(a\\) and the point \\(y\\) with the points \\(b, c, d\\).  \nBisimulation is naturally a central notion for the\ninformation-as-range perspective because the Kripke models of\n section 1\n are precisely labelled graphs. It is a classical result of modal\nlogic that if two states of two models are related by a bisimulation,\nthen the states will satisfy exactly the same modal formulas, and in\naddition a first order property of states is definable in the basic\nmodal language if and only if the property is preserved under\nbisimulation.  \nAs for the correlational stance, in situation theory bisimulation\nturns out to be the right notion in determining whether two infons\nthat might look structurally different are actually the same as pieces\nof information. For example, one possible analysis of Liar-like claims\nleads to infons that are nested in themselves, such as  \nOne can naturally depict the structure of \\(\\sigma\\) as a labelled graph,\nwhich will be bisimilar to the graph associated with the apparently\ndifferent infon  \nThe notion of bisimulation appeared independently in computer science,\nso it so no surprise that it also features in matters related to the\ninformation-as-code approach, with its focus on representation and\ncomputation. In particular, several versions of bisimulation have been\napplied to classes of automata to determine when two of them are\nbehaviourally equivalent, and data encodings such as  \nboth of which represent the same object (an infinite list of zeroes),\ncan be identified as such by noticing that the graphs that depict the\nstructure of these two expressions are bisimilar. See Aczel (1988),\nBarwise and Moss (1996), and Moss (2009) for more information about\nbisimulation an circularity, connections with modal logic, data\nstructures, and coalgebras.  \nThis entry has focused mostly on positive information.\nFormally speaking, negative information is simply the\nextension-via-negation of the positive fragment of any logic built\naround information-states. Different negation-types will constrain the\nbehaviour of negative information in various ways. Informally,\nnegative information may be thought of variously as what is\ncanonically expressed with sentential negation, process exclusion\n(both propositional and sub-propositional) and more. Even when we\nrestrict ourselves to a single conceptual notion, there may be\nvigorous philosophical debate as to which formal construction best\ncaptures the notion in question. In this section, we run though\nseveral formal analyses of negative information, we examine some of\nthe philosophical debates surrounding the suitability of various\nformal constructions with respect to particular applications, and\nexamine the related topic of failure of information flow in the\nsituation-theoretic sense, which may give raise to misinformation or\nlack of information in particular settings.  \nNon-constructive intuitionistic negation, is aimed towards accounting\nfor negative information in the context of information flow via\nobservation. For more details on this point, see the subsection\nintuitionistic logics and Beth and Kripke models in the supplementary\ndocument:\n Abstract Approaches to Information Structure. \nWorking with the frames from\n section 3.1,\n non-constructive intuitionistic negation is defined in terms of the\nconstructive implication, (21), which is combined with bottom,\n\\(\\mathbf{0}\\), which holds nowhere, as specified by its frame\ncondition:  \nHence intuitionistic negation is defined as follows:  \nHence the frame condition for \\(-A\\) is as follows:  \n(20) states that if \\(x\\) carries the information that\n\\(-A\\), then there no state \\(y\\) such that \\(y\\) is an\ninformational development of \\(x\\) where \\(y\\) carries the\ninformation that \\(A\\).  \nThe definition of \\(-A\\) in terms of \\(A \\rightarrow \\mathbf{0}\\) throws up an\nasymmetry between positive and negative information. In an information\nmodel \\(-A\\) holds at \\(x \\in \\mathbf{F}\\) iff \\(A\\) does not hold at any \\(y \\in \\mathbf{F}\\) such that \\(x \\sqsubseteq y\\). Whilst the verification\nof \\(A\\) at \\(x \\in \\mathbf{F}\\) only involves\nchecking \\(x\\), verifying \\(-A\\) at \\(x \\in \\mathbf{F}\\) involves checking all \\(y \\in \\mathbf{F}\\) such that \\(x \\sqsubseteq y\\). According to Gurevich (1977) and Wansing\n(1993), this asymmetry means that intuitionistic logic does not\nprovide an adequate treatment of negative information, since, unlike\nthe verification of \\(A\\), there is no way of verifying\n\\(-A\\) “on the spot” so to speak. Gurevich and\nWansing’s objection to this asymmetry is a critical response to\nGrzegorczyk (1964). For arguments in support of Grzegorczyk’s\nasymmetry between positive and negative information, see\nSequoiah-Grayson (2009). A fully constructive negation that allows for\nfalsification “on the spot” is known also as Nelson\nNegation on account of it being embedded within Nelson’s\nconstructive systems (Nelson 1949, 1959). For a contemporary\ndevelopment of these constructive systems, see section 2.4.1 of\nWansing (1993).  \nIn a static logic setting, negation is, at the very least, used to\nrule out truth (if not to express explicit falsity). In a dynamic\nsetting, negation will be used to rule out particular\nprocesses. For a development negative information as process\nexclusion in the context of categorial information theory see\nSequoiah-Grayson (2013). This idea has its origins in the Dynamic\nPredicate Logic of Groenendijk and Stokhof (1991), in particular with\ntheir development of negative information via negation as\ntest-failure. For an exploration between the conceptions of\nnegative information as process exclusion and test-failure, see\nSequoiah-Grayson (2010).  \nIn any logic for negation as process-exclusion, the process-exclusion\nwill be non-directional if the logic in question is\ncommutative. Directional process-exclusion will result when we remove\nthe structural rule of commutation. For a discussion of the\nrelationship between the formalisation of directional process\nexclusion as commutation-failure along with symmetry-failure on\ncompatibility and incompatibility relations on information states, see\nSequoiah-Grayson (2011). For an extended discussion of negative\ninformation in the context of categorial grammars, see Buszkowski\n(1995).  \nThere is a bi-directional relation between logic and information. On\nthe one hand, information underlies the intuitive understanding of\nstandard logical notions such as inference (which may be thought of as\nthe process that turns implicit information into explicit informaiton)\nand computation. On the other hand, logic provides a formal framework\nfor the study of information itself.  \nThe logical study of information focuses on some of the most\nfundamental qualitative aspects of information. Different stances on\ninformation naturally highlight some of these aspects more than\nothers. Thus, the information-as-range stance most naturally\nhighlights agency and the dynamics of information in settings with\nmultiple agents that can interact with each other. The aboutness of\ninformation (information is always about something) is a central theme\nin the information-as-correlation stance. The topic of encoding\ninformation and its processing (as in the case of formal inference) is\nat the core of the information-as-code stance. None of these\nqualitative aspects of information is exclusive to just one of the\nstances, even if some stress certain topics more than others. Some\nthemes such as the structure of information and its relation with\ninformation content are equally pertinent regardless of the stance.\nThe ways in which information is studied in this entry differs from\nother important formal frameworks that study information\nquantitatively. For example, Shannon’s statistical theory of\ninformation is concerned with things such as optimizing the amount of\ndata that can be transmitted via a noisy channel, and the\nKolmogorov’s complexity theory quantifies the informational\ncomplexity of a string as the length of the shortest program that\noutputs it when executed by a fixed universal Turing machine.  \nThe logical analysis of information includes fruitful\nreinterpretations of known logical systems (such as epistemic logic or\nrelevance logic), and new systems that result from attempts to capture\nfurther aspects of informational. Still other logical approaches to\nthe analysis of information result from combining aspects of two\ndifferent stances, as with the constraint systems of\n section 4.\n New frameworks (situation theory in the 80s) have also resulted from\nexploring from scratch what sort of inferences—including those\nthat are novel and non-classical—one should allow in order to\nmodel certain aspects of information.  \nLooking for interfaces between the three stances is a nascent\ndirection of inquiry, discussed here in\n section 4.\n A complementary issue is whether the stances can be unified. There\nare several formal frameworks that, beyond serving as potential\nsettings for exploring the issue of unification, are abstract\nmathematical theories of information in their own right. Each of these\ngoes well beyond the scope of this entry:  \nThe logical study of information resembles in spirit other more\ntraditional endeavours, such as the logical study of the concept of\ntruth or computation: in all these cases the object of logical study\nplays a central role in the intuitive understanding of logic itself.\nThe three perspectives on qualitative information presented in this\nentry (ranges, correlations, and code) portrait the diverse state of\nthe art in this field, where many directions of research are open,\nboth as a way of searching for unifying or interfacing settings for\nthe different stances, and of deepening the understanding of the main\nqualitative features of information (dynamics, aboutness, encoding,\ninteraction, etc.) within each stance itself.  \nInterested readers may wish to pursue the topics in the supplementary\ndocument \nwhich covers the topics\n intuitionistic logic, Beth and Kripke models,\n and\n algebraic and other approaches to modal information theory and related areas.","contact.mail":"m.martinez@uniandes.edu.co","contact.domain":"uniandes.edu.co"},{"date.published":"2014-02-03","date.changed":"2018-05-30","url":"https://plato.stanford.edu/entries/logic-information/","author1":"Maricarmen Martinez","author2":"Sebastian Sequoiah-Grayson","entry":"logic-information","body.text":"\n\n\nAt their most basic, logic is the study of consequence, and\ninformation is a commodity. Given this, the interrelationship between\nlogic and information will centre on the informational consequences of\nlogical actions or operations conceived broadly. The explicit\ninclusion of the notion of information as an object of\nlogical study is a recent development. It was by the beginning of the\npresent century that a sizable body of existing technical and\nphilosophical work (with precursors that can be traced back to the\n1930s) coalesced into the new emerging field of logic and information\n(see Dunn 2001). This entry is organised thematically, rather than\nchronologically. We survey major logical approaches to the study of\ninformation, as well as informational understandings of logics\nthemselves. We proceed via three interrelated and complementary\nstances: information-as-range, information-as-correlation, and\ninformation-as-code. \n\n\nThe core intuition motivating the Information-as-range\nstance, is that an\ninformational state may be characterised by the range of possibilities\nor configurations that are compatible with the information available\nat that state. Acquiring new information corresponds to a reduction of\nthat range, thus reducing uncertainty about the actual configuration\nof affairs. With this understanding, the setting of possible-world\nsemantics for epistemic modal logics proves to be rewarding for the\nstudy of various semantic aspects of information. A prominent\nphenomenon here is information update, which may occur in\nboth individual and social settings, due to the interaction between\nboth agents and their environment via different types of epistemic\nactions. We will see that an epistemic action is any action that\nfacilitates the flow of information, hence we will return to epistemic\nactions themselves throughout.\n\n\nThe Information-as-correlation stance focuses\non information flow as it is licensed within structured systems formed\nby systematically correlated components. For example: the number of\nrings of a tree trunk can give you information about the time when the\ntree was born, in virtue of certain regularities of nature that\n‘connect’ the past and present of trees. Central themes of\nthis stance include the aboutness, situatedness, and accessibility\nof information in structured information environments. \n\n\nThe key concern of the third stance, Information-as-code,\nis the syntax-like\nstructure of information pieces (their encoding) and the\ninference and computation processes that are licensed by\nvirtue (among other things) of that structure. A most natural logical\nsetting to study these informational aspects is the algebraic proof\ntheory underpinned by a range of substructural logics.\nSubstructural logics have always been a natural home for informational\nanalysis, and the recent developments in the area enrich the\ninformation-as-code stance. \n\n\nThe three stances are by no means incompatible, but neither are they\nnecessarily reducible to each other. This will be expanded on later in\nthe entry, and some further topics of research will be illustrated,\nbut for a preview of how the three stances can live together, take the\ncase of a structured information system composed of several parts.\nFirstly, the correlations between the parts naturally allow for\n‘information flow’ in the sense of the\ninformation-as-correlation stance. Secondly, they also give rise to a\nlocal ranges of possibilities, since the local information available\nat one part will be compatible with a certain range of global states\nof the system. Thirdly, the combinatorial, syntax-like,\nproof-theoretical aspects of information can be brought to this\nsetting in various ways. One of them is treating the correlational\nflow of information as a sort of combinatorial system by which local\ninformation states are combined in syntactic-like ways, fitting a\nparticular interpretation of substructural logic. One could also add\ncode-like-structure to the modelling explicitly, for example by\nassigning local deductive calculi to either the components or local\nstates of the system. We begin however with information as\nrange\n\nThe understanding of information as range has its origins in\nBar-Hillel and Carnap’s theory of semantic information,\nBar-Hillel and Carnap (1952). It is here that the inverse range\nprinciple is given its first articulation with regard to the\ninformational content of a proposition. The inverse range principle\nstates that there is an inverse relationship between the information\ncontained by a proposition on the one hand, and the likelihood of that\nproposition being true on the other. That is, the more information\ncarried by a proposition, the less likely it is that the proposition\nis true. Similarly, the more likely the truth of a proposition, the\nless information it carries.  \nThe likelihood of the truth of a proposition connects with information\nas range via a possible worlds semantics. For any contingent\nproposition, it will be supported by some possibilities (those where\nit is true) and not supported by others (those where it is false).\nHence a proposition will be supported by a range of possibilities, an\n“information range”. Now suppose that there is a\nprobability distribution across the space of possibilities, and for\nthe sake of simplicity suppose that the distribution is uniform. In\nthis case, the more worlds that support a proposition, the likelier\nthe proposition’s truth, and, via the inverse relationship\nprinciple, the less information it carries. Although information as\nrange has its origins in quantitative information theory, its role in\ncontemporary qualitative logics of information cannot be overstated.\n \nConsider the following example due to Johan van Benthem (2011). A\nwaiter in a cafe receives an order for your table—an espresso\nand a soda. When the waiter arrives at your table, he asks “For\nwhom is the soda?”. After your telling him that the soda is for\nyou and his giving you your soda, the waiter does not need to ask\nabout the espresso, he can just give it to your cafe-partner. This is\nbecause the information gained by the waiter from your telling him\nthat you ordered the soda allows him to eliminate certain open\npossibilities from the total range of possibilities such that only one\nis left—your friend ordered the espresso.  \nLogics of information distinguish regularly between hard\ninformation and soft information. The terminology is a\nslight misnomer, as this distinction is not one between different\ntypes of information per se. Rather it is one between\ndifferent types of information storage. Hard information is\nfactive, and unrevisable. Hard information is often taken to\ncorrespond to knowledge. In contrast to hard information,\nsoft information is non-necessarily-factive, hence revisable\nin the presence of new information. Soft information, in virtue of its\nrevisability, corresponds very closely to belief. The terms\nknowledge and belief are conventional, but on the context of\ninformation flow, the hard/soft information reading is convenient on\naccount of it bringing the informational phenomena to the foreground.\nAt the very least the terminology is increasingly popular, so being\nclear on the distinction being one between types of information\nstorage as opposed to types of information is important. Although both\nhard and soft information are important for our epistemic and doxastic\nsuccess, in this section we will concentrate mainly on logics of hard\ninformation flow.  \nIn\n section 1.1\n we will see how it is that classic epistemic logics exemplify the\nflow of hard information within the information as range framework. In\n section 1.2\n we will extend our exposition from logics of hard information-gain to\nlogics of the actions that facilitate the gain of such hard\ninformation, dynamic epistemic logics. At the end of Section 1.2, we\nwill expound the important phenomenon of private information,\nbefore examining how it is that information as range is captured in\nvarious quantitative frameworks.  \nIn this section we will explore how it is that the elimination of\npossibilities corresponding to information-gain is the starting point\nfor research on logics of knowledge and belief that fall under the\nheading of epistemic logics. We will begin with classic\nsingle-agent epistemic logic, before exploring multi-agent epistemic\nlogics. In both cases, since we will be concentrating on logics of\nknowledge as opposed to logics of belief, the information-gained will\nbe hard information.  \nConsider the waiter example in more detail. Before receiving the hard\ninformation that the soda is for you (and for the sake of the example\nwe are assuming that the waiting is dealing with hard information\nhere), the waiter’s knowledge-base is modelled by a pair of\nworlds (hereafter information states) \\(x\\) and \\(y\\)\nsuch that in \\(x\\) you ordered the soda and your friend the\nespresso, and in \\(y\\) you ordered the espresso and your friend the\nsoda. After receiving the hard information that the soda is for you,\n\\(y\\) is eliminated from the waiter’s knowledge-base, leaving\nonly \\(x\\). As such, the reduction of the range of possibilities\ncorresponds to an information-gain for the waiter. Consider the truth\ncondition for agent \\(\\alpha\\) knows that \\(\\phi\\),\nwritten \\(K_{\\alpha}\\phi\\): \nThe accessibility relation \\(R_{\\alpha}\\) is an\nequivalence relation connecting \\(x\\) to all information states\n\\(y\\) such that \\(y\\) is indistinguishable from \\(x\\),\ngiven \\(\\alpha\\)’s hard information at that state\n\\(x\\). That is, given what the waiter knows when he is in that\nstate. So, if \\(x\\) was the waiter’s information state before\nbeing informed that you ordered the soda, \\(y\\) would have included\nthe information that you ordered the espresso, as each option was as\ngood as the other until the waiter was informed otherwise. There is an\nimplicit assumption at work here—that some state \\(z\\) say,\nwhere you ordered both the soda and the espresso, is not in the\nwaiter’s information-range. That is, the waiter knows that\n\\(z\\) is not a possibility. Once informed however, the information\nstates supporting your ordering the espresso are eliminated from the\nrange of information corresponding to the waiter’s knowledge.\n \nBasic modal logic extends propositional formulas with modal operators\nsuch as \\(K_{\\alpha}\\). If \\(\\mathbf{K}\\) is the set\nof all Kripke models then we have the following:  \nIn hard information terms, (A1) states that hard information is closed\nunder (known) implications. Since the first conjunct states that all\nstates accessible by \\(\\alpha\\) are \\(\\phi\\) states, \\(\\alpha\\) possesses the\nhard information that \\(\\phi\\), hence \\(\\alpha\\) also possesses the hard\ninformation that \\(\\psi\\). (A2) states that if \\(\\phi\\) holds in the set of\nall models, then \\(\\alpha\\) possesses the hard information that \\(\\phi\\). In\nother words, (A2) states that all tautologies are known/hard stored by\nthe agent, and (A1) states that \\(\\alpha\\) knows the logical consequences\nof all propositions that \\(\\alpha\\) knows (be they tautologies or\notherwise). That is, the axioms state that the agent is logical\nomniscient, or an ideal reasoner, a property of agents that\nwe will return to in detail in the sections\n below.[1] \nThe framework explored so far concerns single-agent epistemic logic,\nbut reasoning and information flow are very often multi-agent\naffairs. Consider again the waiter example. Importantly, the\nwaiter is only able to execute the relevant reasoning procedure\ncorresponding to a restriction of the range of information states\non account of your announcement to him with regard to the\nespresso. That is, it is the verbal interaction between several agents\nthat facilitates the information flow that enabled the logical\nreasoning to be undertaken.  \nIt is at this point that multi-agent epistemic logic raises new\nquestions regarding the information in a group. “Everybody in\n\\(G\\) possesses the hard information that \\(\\phi\\)” (where\n\\(G\\) is any group of agents from a finite set of agents\n\\(G*)\\) written as \\(E_G\\phi .\nE_G\\) is defined for each \\(G \\subseteq G^*\\) in the following manner:  \nGroup knowledge is importantly different from common\nknowledge (Lewis 1969; Fagin et al. 1995). Common knowledge is\nthe condition of the group where everybody knows that everybody\nknows that everybody knows … that \\(\\phi\\). In other words,\ncommon knowledge concerns the hard information that each agent in the\ngroup possesses about the hard information possessed by the other\nmembers of the group. That everybody in \\(G\\) possesses the hard\ninformation that \\(\\phi\\) does not imply that \\(\\phi\\) is common knowledge.\nWith group knowledge each agent in the group may possess the same hard\ninformation (hence achieving group knowledge) without necessarily\npossessing hard information about the hard information possessed by\nthe other agents in the group. As noted by van Ditmarsh, van der Hoek,\nand Kooi (2008: 30), “the number of iterations of the\n\\(E\\)-operator makes a real difference in practice”.\n\\(C_G\\phi\\)—the common knowledge\nthat \\(\\phi\\) for members of \\(G\\), is defined as follows:\n \nTo appreciate the difference between \\(E\\) and \\(C\\), consider\nthe following “spy example” (originally Barwise 1988 with\nthe envelope details due to Johan van Benthem).  \nThere are a group of competing spies at a formal dinner. All of them\nare tasked with the mission of acquiring some secret information from\ninside the restaurant. Furthermore, it is common knowledge amongst\nthem that they want the information. Given this much, compare the\nfollowing:  \nVery obviously, the two scenarios will elicit very different types of\nbehaviour from the spies. The first would be relatively subtle, the\nlatter dramatically less so. See Vanderschraaf and Sillari (2009) for\nfurther details.  \nA still more fine-grained use of S5 based epistemic logics is that of\nZhou (2016). Zhou demonstrates that S5 based epistemic logic may be\nused to model the epistemic states of the agent from the perspective\nof the agent themselves. Hence Zhou refers to such an epistemic logic\nas internally epistemic. Zhou then uses a multi-valued logic\nto model the relationship between the agent’s internal knowledge\nbase and their external informational environment.  \nSee the full entry on\n Dynamic Epistemic Logic.\n As noted above, the waiter example from the beginning of this section\nis as much about information-gain via announcements, epistemic\nactions, as it is about information structures. In this section,\nwe will outline how it is that the expressive power of multi-agent\nepistemic logic can be extended so as to capture epistemic actions.\n \nHard information flow, that is, the flow of information between the\nknowledge states of two or more agents, can be facilitated by more\nthan one epistemic action. Two canonical examples are\nannouncements and observations. When\n“announcement” is restricted to true and public\nannouncement, its result on the receiving agent’s\nknowledge-base is similar to that of an observation (on the assumption\nthat the agent believes the content of the announcement). The public\nannouncement that \\(\\phi\\) will restrict the model of the agent’s\nknowledge-base to the information states where \\(\\phi\\) is true, hence\n“announce \\(\\phi\\)” is an epistemic state\ntransformer in the sense that it transforms the epistemic states\nof the agents in the group, (see van Ditmarsh, van der Hoek, and Kooi\n2008:\n 74).[2] \nDynamic epistemic logics extend the language of non-dynamic epistemic\nlogics with dynamic operators. In particular, public announcement\nlogic (PAL) extends the language of epistemic logics with the\ndynamic announcement operator [\\(\\phi\\)], where [\\(\\phi]\\psi\\) is read\n“after announcement \\(\\phi\\), it is the case that \\(\\psi\\)”. The\nkey reduction axioms of PAL are as follows:  \nRA1–RA5 capture the properties of the announcement operator by\nconnecting what is true before the announcement with what is true\nafter the announcement. The axioms are named ‘reduction’\naxioms because the left-to-right hand direction reduces either the\nnumber of announcement operators or the complexity of the formulas\nwithin their scope. For an in depth discussion see Pacuit (2011). RA1\nstates that announcements are truthful. RA5 specifies the\nepistemic-state-transforming properties of the announcement operator.\nIt states that \\(\\alpha\\) knows that \\(\\psi\\) after the announcement that\n\\(\\phi\\) iff \\(\\phi\\) implies that \\(\\alpha\\) knows that \\(\\psi\\) will be\ntrue after \\(\\phi\\) is announced in all \\(\\phi\\)-states. The “after\n\\(\\phi\\) is announced” condition is there to account for the fact\nthat \\(\\psi\\) might change its truth-value after the announcement. The\ninteraction between the dynamic announcement operator and the\nknowledge operator is described completely by RA5 (see van Benthem,\nvan Eijck, and Kooi 2006).  \nJust as adding the common knowledge operator \\(C\\) to\nmulti-agent epistemic logic extends the expressive capabilities of\nmulti-agent epistemic logic, adding \\(C\\) to PAL results in the\nmore expressive public announcement logic with common\nknowledge, (PAC). The exact relationship between public\nannouncements and common knowledge is captured by the announcement\nand common knowledge rule of the logic PAC as the following:  \nAgain, PAC is the dynamic logic of hard information. The epistemic\nlogics dealing with soft information fall within the scope of\nbelief revision theory (van Benthem 2004; Segerberg 1998).\nRecall that hard and soft information are not distinct types of\ninformation per se, rather they are distinct types of\ninformation storage. Hard-stored information is unrevisable,\nwhereas soft-stored information is revisable. Variants of PAL that\nmodel soft information augment their models with\nplausibility-orderings on information-states (Baltag and Smets 2008).\nThese orderings are known as preferential models in\nnon-monotonic logic and belief-revision theory. The logics can be made\ndynamic in virtue of the orderings changing in the face of new\ninformation (which is the mark of soft information as opposed to hard\ninformation). Such plausibility-orderings may be modelled\nqualitatively via partial orders etc., or modelled quantitatively via\nprobability-measures. Such quantitative measures provide a connection\nto a broader family of quantitative approaches to semantic information\nthat we will examine below. Recent work by Allo (2017) ties the soft\ninformation of dynamic epistemic logic to non-monotonic logics. This\nis an intuitive move. Soft information is information that has been\nstored in a revisable way, hence the revisable nature of conclusions\nin non-monotonic arguments makes non-monotonic logics a natural fit.\nOn this very topic, see also Chapter 13.7 of van Benthem (2011).  \nPrivate information. Private information is\nan equally important aspect of our social interaction. Consider\nscenarios where the announcing agent is aware of the private\ncommunication whilst other members of the group are not, such as\nemails in Bcc. Consider also scenarios where the sending agent is\nnot aware of the private communication, such as a\nsurveillance operation. The system of dynamic epistemic logic\n(DEL) models events that turn on private (and public) information by\nmodelling the agents’ information concerning the events\ntaking place in a given communicative scenario (see Baltag et\nal. 2008; van Ditmarsh et al. 2008; and Pacuit 2011). For an excellent\noverview and integration of all of the issues above, see the recent\nwork of van Benthem (2016), where the author discusses multiple\ninterrelated levels of logical dynamics, one level of update, and\nanother of representation. For an extensive collection of papers\nextending this and related approaches, see Baltag and Smets (2014) \nThe modal information theory approach to multi-agent information flow\nis the subject of a great amount of research. The semantics is not\nalways carried out in relational terms (i.e., with Kripke Frames) but\nis done often algebraically (see Blackburn et al. 2001 for details of\nthe algebraic approach to modal logic). For more details on algebraic\nas well as type-theoretic approaches, see the subsection on algebraic\nand other approaches to modal information theory in the supplementary\ndocument\n Abstract Approaches to Information Structure. \nQuantitative approaches to information as range also have\ntheir origins in the inverse relationship principle. To\nrestate—the motivation being that the less likely the truth of a\nproposition as expressed in a logical language with respect to a\nparticular domain, the greater the amount of information encoded by\nthe relevant formula. This is in contrast to the information measures\nin the mathematical theory of communication (Shannon 1953\n[1950]) where such measures are gotten via an inverse relationship on\nthe expectation of the receiver \\(R\\) of the receipt of a signal\nfrom some source \\(S\\).  \nAnother important aspect of the classical theory of information, is\nthat it is an entirely static theory—it is concerned\nwith the informational content and measure of particular formulas, and\nnot with information flow in any way at all.  \nThe formal details of classical information theory turn on the\nprobability calculus. These details may be left aside here, as the\nobvious conceptual point is that logical truths have a\ntruth-likelihood of 1, and therefore an information measure of 0.\nBar-Hillel and Carnap did not take this to mean that logical truths,\nor deductions, were without information yield, only that their theory\nof semantic information was not designed to capture such a property.\nThey coined the term psychological information for the\nproperty involved. See Floridi (2013) for further details.  \nA quantitative attempt at specifying the information yield of\ndeductions was undertaken by Jaakko Hintikka with his theory of\nsurface information and depth information (Hintikka\n1970, 1973). The theory of surface and depth information extends\nBar-Hillel and Carnap’s theory of semantic information from the\nmonadic predicate calculus all the way up to the full polyadic\npredicate calculus. This itself is a considerable achievement, but\nalthough technically astounding, a serious restriction of this\napproach is that it is only a fragment of the deductions carried out\nwithin full first-order logic that yield a non-zero information\nmeasure. The rest of the deductions in the full polyadic predicate\ncalculus, as well as all of those in the monadic predicate calculus\nand propositional calculus, measure 0, (see Sequoiah-Grayson 2008).\n \nThe obvious inverse situation with the theory of classical semantic\ninformation is that logical contradictions, having a truth-likelihood\nof 0, will deliver a maximal information measure of 1. Referred to in\nthe literature as the Bar-Hillel-Carnap Semantic Paradox, the\nmost developed quantitative approach to addressing it is the theory of\nstrongly semantic information (Floridi 2004). The conceptual\nmotivation behind strongly semantic information is that for a\nstatement to yield information, it must help us to narrow down the set\nof possible worlds. That is, it must assist us in the search for the\nactual world, so to speak (Sequoiah-Grayson 2007). Such a\ncontingency requirement on informativeness is violated by\nboth logical truths and logical contradictions, both of which measure\n0 on the theory of strongly semantic information. See Floridi (2013)\nfor further details. See also Brady (2016) for recent work on the\nrelationship between quantitative accounts of information and\nanalyticity. For a new approach to connecting quantitative and\nqualitative measures of information, see Harrison-Trainor et\nal. (2018) \nThe correlational take on information looks at how the existence of\nsystematic connections between the parts of a structured\ninformation environment permits that one part may carry\ninformation about another. For example: the pattern of pixels\nthat appear on the screen of a computer gives information (not\nnecessarily complete) about the sequence of keys that were\npressed by the person who is typing a document, and even a partial\nsnapshot of the clear starred sky your friend is looking at now will\ngive you information about his possible locations on Earth at\nthis moment. The focus on structured environments and the aboutness of\ninformation goes hand in hand with a third main topic of the\ninformation-as correlation approach, namely the situatedness of\ninformation, that is, its dependence on the particular setting on\nwhich an informational signal occurs. Take the starry sky as an\nexample again: the same pattern of stars, at different moments in time\nand locations in space will in general convey different information\nabout the location of your friend.  \nHistorically, the first paradigmatic setting of correlated information\nwas Shannon’s work on communication (1948), which we already\nmentioned in the last section. Shannon considered a communication\nsystem formed by two information sites, a source and a receiver,\nconnected via a noisy channel. He gave conclusive and extremely useful\nanswers to questions having to do with the construction of\ncommunication codes that help maximising the effectiveness of\ncommunication (in terms of bits of information that can be\ntransmitted) while minimizing the possibility of errors caused by\nchannel noise. As we previously said, Shannon’s concern was\npurely quantitative. The logical approach to information as\ncorrelation builds on Shannon’s ideas, but is concerned with\nqualitative aspects of information flow , like the ones we highlighted\nbefore: what information about a\n‘remote’ site (remote in terms of space, time,\nperspective, etc.) can be drawn out of information that is directly\navailable at a ‘proximal’ site? \nSituation theory (Barwise and Perry 1983; Devlin 1991) is the\nmajor logical framework so far that has made these ideas its starting\npoint for an analysis of information. Its origin and some of its\ncentral insights can be found in the project of naturalization of mind\nand the possibility of knowledge initiated by Fred Dretske (1981),\nwhich soon influenced the inception of situation semantics in the\ncontext of natural language (see Kratzer 2011).  \nTechnically, there are two kinds of developments in situation theory:\n \nThe next three subsections survey some of the basic notions from this\ntradition: the basic sites of information in situation theory (called\nsituations), the basic notion of information flow based on\ncorrelations between situations, and the mathematical theory of\nclassifications and channels mentioned in (b).  \nThe ontologies in (a) span a wide spectrum of entities. They are meant\nto reflect a particular way in which an agent may carve up a system.\nHere “a system” can be the world, or a part or aspect of\nit, while the agent (or kind of agent) can be an animal species, a\ndevice, a theorist, etc. The list of basic entities includes\nindividuals, relations (which come with roles attached to them),\ntemporal and spacial locations, and various other things. Distinctive\namong them are the situations and infons.  \nRoughly speaking, situations are highly structured parts of a system,\nsuch as a class session, a scene as seen from a certain perspective, a\nwar, etc. Situations are the basic supporters of information. Infons,\non the other hand, are the informational issues that situations may or\nmay not support. The simplest kind of informational issue is whether\nsome entities \\(a_1 , \\ldots ,a_n\\) stand (or do not stand) in a relation\n\\(R\\) when playing the roles \\(r_1 , \\ldots ,r_n\\), respectively. Such basic infon is usually\ndenoted as  \nwhere \\(i\\) is 1 or 0, according to whether the issue is positive\nor negative.  \nInfons are not intrinsic bearers of truth, and they are not claims\neither. They are simply informational issues that may or may not be\nsupported by particular situations. We’ll write \\(s \\models \\sigma\\) to mean that the situation\n\\(s\\) supports the infon \\(\\sigma\\). As an example, a successful\ntransaction whereby Mary bought a piece of cheese in the local market\nis a situation that supports the infon  \nThis situation does not support the infon  \nbecause Mary did buy cheese. Nor does the situation support the infon\n \nbecause Armstrong is not part of the situation in question at all.\n \nThe discrimination or individuation of a situation by an agent does\nnot entail that the agent has full information about it: when we\nwonder whether the local market is open, we have individuated a\nsituation about which we actually lack some information. See\nTextor (2012) for a detailed discussion on the nature of\nsituation-like entities and their relation with other ontological\ncategories such as the possible worlds used in modal logic.  \nBesides individuals, relations, locations, situations and basic\ninfons, there are usually various kinds of parametric and abstract\nentities. For example, there is a mechanism of type\nabstraction. According to it, if \\(y\\) is a parameter for\nsituations, then  \nis the type of situations where somebody buys cheese. There will be\nsome basic types in an ontology, and many other types obtained via\nabstraction, as just described.  \nThe collection of ontology entities also includes propositions and\nconstraints. They are key in the formulation of the basic principles\nof information content in situation theory, to be introduced next.\n \nThe following are typical statements about “information\nflow” as studied in situation theory:  \nThe general scheme has the form  \nwhere \\(s : T\\) is notation for “\\(s\\) is of type\n\\(T\\)”. The idea is that it is concrete parts of the world\nthat act as carriers of information (the concrete dot in the radar or\nthe footprints in Zhucheng), and that they do so by virtue of being of\na certain type (the dot moving upward or the footprints showing a\ncertain pattern). What each of these concrete instances indicates is a\nfact about another correlated part of the world. For the issues to be\ndiscussed below it will suffice to consider cases where the indicated\nfact— \\(p\\) in the formulation of\n[IC]—is of the form \\(s' : T '\\), as in the\nradar example.  \nThe conditions needed to verify informational signalling in the sense\nof [\\(\\mathbf{IC}\\)] rely on the existence of law-like\nconstraints such as natural laws, necessary laws such as\nthose of math, or conventions, thanks to which (in part) one situation\nmay serve as carrier of information about another one. Constraints\nspecify the correlations that exist between situations of\nvarious types, in the following sense: if two types \\(T\\) and \\(T '\\)\nare subject to the constraint \\(T \\Rightarrow T '\\), then for every\nsituation \\(s\\) of type \\(T\\) there is a relevantly connected\nsituation \\(s'\\) of type \\(T '\\). In the radar example, the relevant\ncorrelation would be captured by the\nconstraint GoingUpward\n\\(\\Rightarrow\\) GoingNorth, which says that\neach situation where a radar point moves upward is connected with\nanother situation where a plane is moving to the north. It is the\nexistence of this constraint that allows a particular situation where\nthe dot moves to indicate something about the connected plane\nsituation.  \nWith this background, the verification principle for information\nsignalling in situation theory can be formulated as follows:  \n[IS Verification] \\(s : T\\) indicates that \\(s' :\nT'\\) if \\(T \\Rightarrow T '\\) and \\(s\\) is relevantly\nconnected to \\(s'\\). \nThe relation \\(\\Rightarrow\\) is transitive. This ensures that Dretske’s\nXerox principle holds in this account of information transfer, that\nis, there can be no loss of semantic information through information\ntransfer chains.  \n[Xerox Principle]: If \\(s_1 : T_1\\) indicates that\n\\(s_2 : T_2\\) and \\(s_2 : T_2\\) indicates that \\(s_3 : T_3\\), then\n\\(s_1 : T_1\\) indicates that \\(s_3 : T_3\\). \nThe [IS Verification] principle deals with\ninformation that in principle could be acquired by an agent.\nThe access to some of this information will be blocked, for example,\nif the agent is oblivious to the correlation that exists between two\nkinds of situations. In addition, most correlations are not absolute,\nthey admit exceptions. Thus, for the signalling described in\n[E1] to be really informational, the extra\ncondition that the radar system is working properly must be\nmet. Conditional versions of the [IS Verification]\nprinciple may be used to insist that the carrier situation must meet\ncertain background conditions. The inability of an agent to keep track\nof changes on these background conditions may lead to errors. So, if\nthe radar is broken, the dot on the screen may end up moving upward\nwhile the plane is moving south. Unless the air controller is able to\nrecognise the problem, that is, unless she realises that the\nbackground conditions have changed, she may end up giving absurd\ninstructions to the pilot. Now, instructions are tied to actions. For\na treatment of actions from the situation-theoretical view, we refer\nthe reader to Israel and Perry (1991).  \nThe basic notion of information flow sketched in the previous section\ncan be lifted to a more abstract setting in which the supporters of\ninformation are not necessarily situations as concrete parts of the\nworld, but rather any entity which, as in the case of situations, can\nbe classified as being of or not of certain types. The mathematical\ntheory of distributed systems (Barwise and Seligman 1997) to be\ndescribed next takes this abstract approach by studying information\ntransfer within distributed systems in general.  \nA model of a distributed system in this framework will actually be a\nmodel of a kind of distributed system, hence the model of the\nradar-airplane system that we will use as a running example here will\nactually be a model of radar-airplane systems (in plural).\nSetting such a model requires describing the architecture of the\nsystem in terms of its parts and the way they are put together into a\nwhole. Once that is done, one can proceed to see how that architecture\nenables the flow of information among its parts.  \nA part of a system (again, really its kind) is modelled by saying how\nparticular instances of it are classified according to a given set of\ntypes. In other words, for each part of a system one has a\nclassification  \nwhere \\(\\models\\) is a binary relation such that \\(a \\models T\\) if\nthe instance \\(a\\) is of type \\(T\\).  In a simplistic analysis of the\nradar example, one could posit at least three classifications, one for\nthe monitor screen, one for the flying plane, and one for the whole\nmonitoring system:  \nA general version of a ‘part-of’ relation between\nclassifications is needed in order to model the way parts of a system\nare assembled together. Consider the case of the monitoring systems.\nThat each one of them has a screen as one of its parts means that\nthere is a function that assigns to each instance of the\nclassification MonitSit an instance of\nScreens. On the other hand, all the ways in which a\nscreen can be classified (the types of Screens)\nintuitively correspond to ways in which the whole screening system\ncould be classified: if a screen is part of a monitoring system and\nthe screen is blinking, say, then the whole monitoring situation is\nintuitively one of the type ‘its screen is blinking’.\nAccordingly, a generalised ‘part-of’ relation between any\ntwo arbitrary classifications \\(\\mathbf{A}, \\mathbf{C}\\)\ncan be modelled via two functions  \nthe first of which takes every type in \\(\\mathbf{A}\\) to its\ncounterpart in \\(\\mathbf{C}\\), and the second of which takes every\ninstance \\(c\\) of \\(\\mathbf{C}\\) to its\n \\(\\mathbf{A}\\)-component.[3] \nIf \\(f : \\mathbf{A} \\rightarrow \\mathbf{C}\\) is shortcut\nnotation for the existence of the two functions above (the pair\n\\(f\\) of functions is called an infomorphism), then an\narbitrary distributed system will consist of various classifications\nrelated by infomorphisms. For our purposes, it will suffice here to\nconsider three classifications \\(\\mathbf{A}, \\mathbf{B},\n\\mathbf{C}\\) together with two infomorphisms  \nThen a simple way to model the radar monitoring system would consist\nof the pair  \nThe common codomain in these cases \\((\\mathbf{C}\\) in the general\ncase and MonitSit in the example) works as a the core\nof a channel that connects two parts of the system. The core\ndetermines the correlations that obtain between the two parts, thus\nenabling information flow of the kind discussed in\n section 2.2.\n This is achieved via two kinds of links. On the one hand, two\ninstances \\(a\\) from \\(\\mathbf{A}\\) and \\(b\\) from\n\\(\\mathbf{B}\\) can be thought to be connected via the channel if\nthey are components of the same instance in \\(\\mathbf{C}\\), so the\ninstances of \\(\\mathbf{C}\\) act as connections between components.\nThus, in the radar example a particular screen will be connected to a\nparticular plane if they belong to the same monitoring situation.  \nOn the other hand, suppose that every instance in \\(\\mathbf{C}\\)\nverifies some relation between types that happen to be counterparts of\ntypes from \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\). Then such\nrelation captures a constraint on how the parts of the system\nare correlated. In the radar example, the theory of the core\nclassification MonitSit will include constraints such\nas PlainMovingNorth \\(\\Rightarrow\\) DotGoingUp.\nThis regularity of monitoring\nsituations, which act as connections between radar screen-shots and\nplanes, reveals a way in which radar screens and monitored planes\ncorrelate with each other. All this affords the following version of\ninformation transfer.  Channel-enabled signalling: Suppose that  \nThen instance \\(a\\) being of type \\(T\\) in \\(\\mathbf{A}\\) indicates\nthat instance \\(b\\) is of type \\(T'\\) in \\(\\mathbf{C}\\) if \\(a\\) and\n\\(b\\) are connected by a instance from \\(\\mathbf{C}\\) and the relation\n\\(f^{\\wedge}(T) \\Rightarrow g^{\\wedge}(T')\\) between the counterpart\ninterpreted types is satisfied by all instances of \\(\\mathbf{C}\\). \nNow, for each classification \\(\\mathbf{A}\\), the collection  \nformed by all the global constraints of the classification\ncan be thought of as a logic that is intrinsic to \\(\\mathbf{A}\\).\nThen a distributed system consisting of various classifications and\ninfomorphisms will have a logic of constraints attached to each part\nof\n it,[4]\n and more sophisticated questions about information flow within the\nsystem can be formulated.  \nFor example, suppose an infomorfism \\(f : \\mathbf{A} \\rightarrow\n\\mathbf{C}\\) is part of the distributed system under study. Then \\(f\\)\nnaturally transforms each global constraint \\(T\n\\Rightarrow T'\\) of \\(L_{\\mathbf{A}}\\) into \\(f^{\\wedge}(T)\n\\Rightarrow f^{\\wedge}(T')\\), which can always be shown to be an\nelement of \\(L_{\\mathbf{C}}\\). This means that one can reason within\n\\(\\mathbf{A}\\) and then reliably draw conclusions about\n\\(\\mathbf{C}\\). On the other hand, it can be shown that using\npreimages under \\(f^{\\wedge}\\) in order to translate global\nconstraints of \\(\\mathbf{C}\\) does not always guarantee the\nresult to be a global constraint of \\(\\mathbf{A}\\). It is then\ndesirable to identify extra conditions under which the reliability of\nthe inverse translation can be guaranteed, or at least improved. In a\nsense, these questions are qualitatively close to the concerns Shannon\noriginally had about noise and reliability.  \nAnother issue one may want to model is reasoning about a system from\nthe perspective of an agent that has only partial knowledge\nabout the parts of a system. For a running example, think of a plane\ncontroller who has only worked with ACME monitors and knows nothing\nabout electronics. The logic such an agent might use to reason about\npart \\(\\mathbf{A}\\) of a system (actually part\nScreens in the case of the controller) will in\ngeneral consist of some constraints that may not even be global, but\nsatisfied only by some subset of instances (the ACME monitors). The\nagent’s logic may be incomplete in the sense that it\nmight miss some of the global constraints of the classification (like\nthe ones involving inner components of the monitor). The agent’s\nlogic may also be unsound, in the sense that there might be\ninstances out of the awareness of the agent (say monitors of\nunfamiliar brands) that falsify some of the agent’s constraints\n(which do hold of all ACME monitors). A local logic \\(L\\) in\n\\(\\mathbf{A}\\) can be “moved” along an infomorphism \\(f :\n\\mathbf{A} \\rightarrow \\mathbf{C}\\) in the expected way, that\nis, its constraints are transformed via \\(f^{\\wedge}\\),\nwhile its instances are transformed via \\(f^{\\vee}\\).\nNatural questions studied in channel theory concerning these notions\ninclude the preservation (or not), under translation, of some\ndesirable properties of local logics, such as soundness.  \nA recent development in channel theory (Seligman 2014) uses a more\ngeneral definition of local logic, in which not all instances in the\nlogic need to satisfy all its constraints. This version of channel\ntheory is put to use in two important ways. Firstly, by using local\nlogics to stand for situations, and with a natural interpretation of\nwhat an infon should then be, a reconstruction is produced of the core\nmachinery of situation theory (barely presented in\n sections 2.1\n and\n section 2.2).\n Secondly, it is shown that this version of channel theory can deal\nwith probabilistic constraints. The rough idea is that any pair\nof a classification plus a probability measure over the set of\ninstances induces an extended classification with the same set of\ntypes, and where a constraint holds if and only if the set of\ncounterexample instances has measure 0. Notice that this set of\ncounterexamples might not be empty. Having probabilistic constraints\nis a crucial step towards the effort of formally relating channel\ntheory to Shannon’s theory of communication.  \nFor an extensive development of the theory of channels sketched here,\nplus several explorations towards applications, see Barwise and\nSeligman (1997). See van Benthem (2000) for a study of conditions\nunder which constraint satisfiability is preserved under\ninfomorphisms, and Allo (2009) for an application of this framework to\nan analysis of the distinction between cognitive states and\ncognitive commodities. Finally, it must be mentioned that the\nnotion of classification has been around for some years now in the\nliterature, having being independently studied and introduced under\nnames such as Chu spaces (Pratt 1995) or Formal Contexts (Ganter and Wille 1999).  \nFor information to be computed, it must be handled by the\ncomputational mechanism in question, and for such a handling to take\nplace, the information must be encoded. Information as\ncode is a stance that takes this encoding-condition very\nseriously. The result is the development of fine-grained models of\ninformation flow that turn on the syntactic properties of the encoding\nitself.  \nTo see how this is so, consider again cases involving information flow\nvia observations. Such observations are informative because we are not\nomniscient in the normal, God-like sense of the term. We have to go\nand observe that the cat is on the mat, for example, precisely because\nwe are not automatically aware of every fact in the universe.\nInferences work in an analogous manner. Deductions are informative for\nus precisely because we are not logically omniscient. We have\nto reason about matters, sometimes at great length, because we are not\nautomatically aware of the logical consequences of the body of\ninformation with which we are reasoning.  \nTo come full circle—reasoning explicitly with information\nrequires handling it, where in this case such handling is cognitive\nact. Hence the information in question is encoded in some manner,\nhence Information as code underpins the development of fine-grained\nmodels of information flow that turn on the syntactic properties of\nthe encoding itself, as well as the properties of the actions that\nunderpin the various information-processing contexts involved.  \nSuch information-processing contexts are not restricted to explicit\nacts of inferential reasoning by human agents, but include\nautomated reasoning and theorem proving, as well as\nmachine-based computational procedures in general. Approaches to\nmodelling the properties of these latter information-processing\nscenarios fall under algorithmic information theory.  \nIn\n section 3.1,\n we will explore a major approach to modelling the properties of\ninformation-processing within the information as code framework via\ncategorial information theory. In\n section 3.2,\n we will examine the more general approach to modelling information as\ncode of which categorial information theory is an instance, the\nmodelling of information as code via substructural logics. In\n section 3.3\n we will lay out the details of several other notable examples of\nlogics of information flow motivated by the information as code\napproach.  \nCategorial information theory is a theory of fine-grained\ninformation flow whose models are based upon those specified by the\ncategorial grammars underpinned by the Lambek Calculi, due originally\nto Lambek (1958, 1961). The motivation for categorial information\ntheory is to provide a logical framework for modelling the properties\nof the very cognitive procedures that underpin deductive reasoning.\n \nThe conceptual origin of categorial information theory is found in van\nBenthem (1995: 186). Understanding van Benthem’s use of\n“procedural” to be synonymous with “dynamic”:\n \n[I]t turns out that, in particular, the Lambek Calculus itself permits\nof procedural re-interpretation, and thus, categorial calculi may turn\nout to describe cognitive procedures just as much as the syntactic or\nsemantic structures which provided their original motivation. \nThe motivation for categorial information theory is to model the\ncognitive procedures constituting deductive reasoning. Consider as an\nanalogy the following example. You arrive home from IKEA with an\nunassembled table that is still flat-packed in its box. Now the\nquestion here is this, do you have your table? Well, there is a sense\nin which you do, and a sense in which you do not. You have your table\nin the sense that you have all of the pieces required to construct or\ngenerate the table, but this is not to say that you have the table in\nthe sense that you are able to use it. That is, you do not\nhave the table in any useful form, you have merely pieces of a table.\nIndeed, getting these table-pieces into their useful form, namely a\ntable, may be a long and arduous process…  \nThe analogy between the table-example above and deductive reasoning is\nthis. It is said often that the information encoded by (or\n“contained in” or “expressed by”) the\nconclusion of a deductive argument is encoded by the premises. So,\nwhen you possess the information encoded by the premises of some\ninstance of deductive reasoning, do you possess the information\nencoded by the conclusion? Just as with the table-pieces, you do not\npossess the information encoded by the conclusion in any useful form,\nnot until you have put the “information-pieces”\nconstituting the premises together in the correct manner. To be sure,\nwhen you possess the information-pieces encoded by the premises, you\npossess some of the information required for the construction or\ngeneration of the information encoded by the conclusion. As with the\ntable-pieces however, getting the information encoded by the\nconclusion from the information encoded by the premises may be a long\nand arduous process. You need also the instructional information that\ntells you how to combine the information encoded by the premises in\nthe right way. This information-generation via deductive inference may\nbe thought of also as the movement of information from implicit to\nexplicit storage in the mind of the reasoning agent, and it is the\ncognitive procedures facilitating this storage transfer that motivate\ncategorial information theory.  \nCategorial information theory is a theory of dynamic information\nprocessing based on the merge/fusion \\((\\otimes)\\) and typed\nfunction \\((\\rightarrow , \\leftarrow)\\) operations from categorial grammar. The\nconceptual motivation is to understand the information in the mind of\nan agent as the agent reasons deductively to be a database in much the\nsame way as a natural language lexicon is a database (see\nSequoiah-Grayson (2013), (2016)). In this case, a grammar\nwill be understood as a set of processing constraints so imposed as to\nguarantee information flow, or well-formed strings as outputs. Recent\nresearch on proofs as events from a very similar conceptual\nstarting point may by found in Stefaneas and Vandoulakis\n(forthcoming).  \nCategorial information theory is strongly algebraic in flavour. Fusion\n‘\\(\\otimes\\)’ corresponds to the binary composition operator\n‘.’, and ‘\\(\\vdash\\)’ to the partial order\n‘\\(\\le\\)’ (see Dunn 1993). The merge and function operations\nare related to each other via the familiar residuation\nconditions:  \nIn general, applications for directional function application will be\nrestricted to algebraic analyses of grammatical structures, where\ncommuted lexical items will result in non-well-formed strings.  \nDespite its algebraic nature, the operations can be given their\nevaluation conditions via “informationalised” Kripke\nframes (Kripke 1963, 1965). An information frame (Restall 1994)\n\\(\\mathbf{F}\\) is a triple \\(\\langle S, \\sqsubseteq, \\bullet\\rangle\\).\n\\(S\\) is a set of information states \\(x, y, z\\ldots\\) . \\(\\sqsubseteq\\)\nis a partial order of informational development/inclusion such that\n\\(x \\sqsubseteq y\\) is taken to mean that the information carried by\n\\(y\\) is a development of the information carried by \\(x\\), and \\(\\bullet\\)\nis an operation for combining information states. In other words, we\nhave a domain with a combination operation. The operation of\ninformation combination and the partial order of information inclusion\ninterrelate as follows:\n \nReading \\(x \\Vdash A\\) as state \\(x\\) carries\ninformation of type \\(A\\), we have it that:  \nAt the syntactic level, we read \\(X \\vdash A\\) as\nprocessing on \\(X\\) generates information of type A. In\nthis case we are understanding \\(\\vdash\\) as an information processing\nmechanism as suggested by Wansing (1993: 16), such that \\(\\vdash\\)\nencodes not just the output of an information processing procedure,\nbut the properties of the procedure itself. Just what this processing\nconsists of will depend on the processing constraints that we set up\non our database. These processing constraints will be imposed in order\nto guarantee an output from the processing itself, or to put this\nanother way, in order to preserve information flow. Such processing\nconstraints are fixed by the presence or absence of various\nstructural rules, and structural rules are the business of\nsubstructural logics.  \nCategorial information theory is precipitated by giving the Lambek\ncalculi an informational semantics. At a suitable level of\nabstraction, the Lambek calculi is seen to be a highly expressive\nsubstructural logic. Unsurprisingly, by giving an\ninformational semantics for substructural logics in general, we get a\nfamily of logics that exemplify the information as code approach. This\nlogical family is organised by expressive power, with the expressive\npower of the logics in question being captured by the presence of\nvarious structural rules.  \nA structural rule is of the following general form:  \nWe may read (11) as any information generated by processing on\n\\(X\\) is generated by processing on \\(Y\\) also. Hence the\nlong-form of (11) is as follows:  \nHence \\(X\\) is a structured body of information, or “data\nstructure” as Gabbay (1996: 423) puts it, where the actual\narrangement of the information plays a crucial role. The\nstructural rules will fix the structure of the information encoded by\n\\(X\\), and as such impact upon the granularity of the information\nbeing processed.  \nConsider Weakening, the most familiar of the structural rules\n(followed by its corresponding frame condition:  \nWith Weakening present, we loose track of which pieces of information\nwere actually used in an inference. This is precisely why it is that\nthe rejection of Weakening is the mark of relevant logics, where the\npreservation of bodies of information relevant to the derivation of\nthe conclusion is the motivation. By rejecting Weakening, we highlight\na certain type of informational taxonomy, in the sense that\nwe know which bodies of information were used. To preserve\nmore structural detail than simply which bodies of information were\nused, we need to consider rejecting further structural rules. \nSuppose that we want to record not only which pieces of information\nwere used in an inference, but also how often they were used. In this\ncase we would reject Contraction:  \nContraction allows the multiple use, without restriction, of a piece\nof information. So if keeping a record of the “informational\ncost” of the execution of some information processing is a\nconcern, Contraction will be rejected. The rejection of Contraction is\nthe mark of linear logics, which were designed for modelling just such\nprocessing costs (see Troelstra 1992).  \nIf we wish to preserve the order of use of pieces of\ninformation, then we will reject the structural rule of Commutation:\n \nInformation-order will be of particular concern in temporal settings\n(consider action-composition) and natural language semantics (Lambek\n1958), where non-commuting logics first appeared. Commutation comes\nalso in a more familiar strong form:  \nThe strong form of Commutation results from its combination with the\nstructural rule of\n Association:[5] \nRejecting Association will preserve the precise fine-grained\nproperties of the combination of pieces of information.\nNon-associative logics were introduced originally to capture the\ncombinatorial properties of language syntax (see Lambek 1961).  \nIn the presence of Commutation, a double implication pair \\((\\rightarrow ,\n\\leftarrow)\\) collapses into single implication \\(\\rightarrow\\). In the presence of\nall of the structural rules, fusion, \\(\\otimes\\), collapses into Boolean\nconjunction, \\(\\wedge\\). In this case, the residuation conditions outlined\nin (5) and (6) collapse into a mono-directional function.  \nThe choice of which structural rules to retain obviously depends on\njust what informational phenomena is being modelled, so there is a\nstrong pluralism at work. By rejecting Weakening say, we are\nspeaking of which data were relevant to the process, but are\nsaying nothing about its multiplicity (in which case we would reject\nContraction), its order (in which case we would reject Commutation),\nor the actual patterns of use (in which case we would reject\nAssociation). By allowing Association, Commutation, and Contraction,\nwe have the taxonomy locked down. We might not know the order or\nmultiplicity of the data that were used, but we do know what types,\nand exactly what types, were relevant to the successful processing.\nThe canonical contemporary exposition of such an information-based\ninterpretation of propositional relevant logic is Mares (2004). Such\nan interpretation allows for an elegant treatment of the\ncontradictions encoded by relevant logics. By distinguishing between\ntruth conditions and information conditions, we\nallow for an interpretation of \\(x \\Vdash A \\wedge \\neg A\\) as \\(x\\) carries the\ninformation that \\(A\\) and not \\(A\\). For an exploration of\nthe distinction between truth-conditions and information-conditions\nwithin quantified relevant logic, see Mares (2009).  \nAt such a stage, things are still fairly static. By shifting\nour attention from static bodies of information, to the manipulation\nof these bodies, we will reject structural rules beyond\nWeakening, arriving ultimately at categorial information theory, as it\nis encoded by the very weakest substructural logics. Hence the weaker\nwe go, the more “procedural” the flavour of the logics\ninvolved. From a dynamic/procedural perspective, linear logics might\nbe thought of as a “half way point” between static\nclassical logic, and fully procedural categorial information theory.\nFor a detailed exposition of the relationship between linear logic and\nother formal frameworks in the context of modelling information flow,\nsee Abramsky (2008).  \nRecent important work by Dunn (2015) ties\nsubstructural logics and structural rules together with\ninformational relevance in the following way. Dunn makes a\ndistinction between  programs and data, with the\nformer being dynamic and the latter static. We may think of programs\nas conditional statements of the form \\(A \\rightarrow B\\), and of\ndata as atomic propositions \\(A, B\\) etc. Given these two\ntypes of information artefacts, we have three possible combinations,\nprogram to data combination, program to program combination, and data\nto data combination. For program to data combination, commutation will\nhold whilst weakening and association will fail, and contraction not\napplying. For program to program combination association will hold,\nwhilst commutation, weakening fail. As demonstrated in\nSequoiah-Grayson (2016), the case of contraction for program to\nprogram combination is more complicated. The exact properties of data\nto data combination remain an interesting open issue. The connection\nwith informational relevance is made by interpreting the partial order\nrelation \\(\\sqsubseteq\\) as marking information relevance itself. In this\ncase, \\(x \\sqsubseteq y\\) is read as the information x\nis relevant to the information y. To what it is exactly that\ninformational relevance amounts will depend on the precise context of\ninformation processing in question. Sequoiah-Grayson (2016) extends\nthe framework about to contexts of information processing by an agent\nas the agent reasons explicitly. Given that the combination of\ninformation states \\(x \\bullet y\\) may sit on the left hand\nside of the partial order relation, the extension is an account of the\nepistemic relevance of epistemic actions. For a collection of recent\npapers exploring the information as code approach in depth, see Bimbo\n(2016). \nThe information as code approach is a very natural perspective on\ninformation flow, hence there are a number of related frameworks that\nexemplify it.  \nOne such approach to analysing information as code is to carry out\nsuch an analysis in terms of the computational complexity of various\npropositional logics. Such an approach may propose a hierarchy of\npropositional logics that are all decidable in polynomial time, with\nthis hierarchy being structured by the increasing computational\nresources required for the proofs in the various logics.\nD’Agostino and Floridi (2009) carry out just such an analysis,\nwith their central claim being that this hierarchy may be used to\nrepresent the increasing levels of informativeness of propositional\ndeductive reasoning.  \nGabbay’s (1993, 1996) framework of labelled deductive\nsystems exemplifies the information as code approach in manner\nvery similar to the informationalised substructural logics of\n section 3.1.\n An item of data (note that Gabbay refers to both atomic and\nconditional information as data, in contrast to Dunn and\nSequoiah-Grayson in the section above) is given as a pair of the form\n\\(x : A\\), where \\(A\\) is a piece of declarative\ninformation, and \\(x\\) is a label for \\(A. x\\) is a\nrepresentation of information that is needed operate on or alter the\ninformation encoded by \\(A\\). Suppose that we have also the\ndata-pair \\(y : A \\rightarrow B\\). We may apply \\(x\\)\nto \\(y\\), resulting in the data-pair \\(x + y : B\\)\nIn this case, a database is a configuration of labelled formulas, or\ndata-pairs (Gabbay 1993: 72). The labels and their corresponding\napplication operation are organised by an algebra, and the properties\nof this algebra will impose constraints on the applications operation.\nDifferent constraints, of “meta-conditions” as Gabbay\ncalls them (Gabbay 1993: 77), will correspond to different logics. For\nexample, if we were to ignore the labels, then we would have classical\nlogic, if we were to accept only the derivations which used all of the\nlabelled assumptions, then we would have relevance logic, and if we\naccepted only the derivations which used the labelled assumptions\nexactly once, then we would have linear logic. Labels are behaving\nvery much like possible worlds here, and the short step from possible\nworlds to information states makes it obvious how it is that the\nmeta-conditions on labels may be captured by structural rules.  \nArtemov’s (2008) framework of justification logic\nshares many surface similarities with Gabbay’s system of\nlabelled deduction. The logic is composed of justification\nassertions of the form \\(x : A\\), read as \\(x\\)\nis a justification for \\(A\\). Justifications themselves are\nevidential bases of varying sorts that will vary depending on the\ncontext. They might be mathematical proofs, sets of causes or\ncounterfactuals, or something else that fulfils the justificatory\nrole. What it means for \\(x\\) to justify \\(A\\) is not analysed\ndirectly in justification logic. Rather, attempts are made to\ncharacterise the justification relation \\(x : A\\) itself,\nvia various operations and their axioms. The application operation,\n‘.’ mimics the application operation ‘+’ from\nlabelled deduction, or the fusion ‘\\(\\otimes\\)’ operation\nfrom categorial information theory. In justification logic, the symbol\n‘+’ is reserved for the representation of joint evidence.\nHence ‘\\(x + y\\)’ is read as ‘the\njoint evidence of \\(x\\) and \\(y\\)’. Application and\njoin are characterised in justification logic by the following axioms\nrespectively:  \nThe latter axiom characterises the monotonicity of joint evidential\nbases. Apart from the commutativity of +, the structural properties of\nthe justification operations are currently unexplored, although the\npotential for such an exploration is exciting. Justification logic is\nused to analyse notoriously difficult epistemic problems such as the\nGettier cases and more. If we take our epistemology to be\ninformationalised, then the constitution of evidential bases as\ninformation states places justification logics within the information\nas code approach in a straightforward manner. For further details, see\nArtemov and Fitting (2012).  \nZalta’s work on object theory (Zalta 1983, 1993) provides a\ndifferent way to analyse informational content—understood as\npropositional content—and its structure. Motivated by\nmetaphysical considerations, object theory starts by proposing a\ntheory of objects and relations (usually formulated in a second order\nquantified modal language). This theory can then be used to define and\ncharacterise states of affairs, propositions, situations, possible\nworlds, and other related notions. The resulting picture is one where\nall these things have internal structure, their algebraic properties\nare axiomatized, and one can therefore reason about them in a\nclassical proof-theoretical way.  \nA philosophical point touched by this approach concerns the link\nbetween the propositional content (information) expressed by sentences\nand the idea of predication. Relevant to this entry is Zalta’s\n(1993) development of a version of situation theory that follows this\napproach, and where a key element is the usage of two forms of\npredication. Briefly, the formula ‘\\(Px\\)’ corresponds\nto the usual form of predication by exemplification (as in\n“Obama is American”), while ‘\\(xP\\)’\ncorresponds to predication via encoding. Abstract objects are\nthen defined to be (essentially) encodings of properties, in\ncombinations which might not even be made factual. These provisions\nenable the existence of information about abstract, possible, or\nfictional entities. For details on the tradition to which object\ntheory belongs see Textor (2012), McGrath (2012), and King (2012).\n \nWhile the three approaches discussed above (range, correlations, code)\ndiffer in that they emphasise different informational themes, the\nunderlying notion they aim to clarify is the same (information). It is\nthen natural to find that the similarities and synergies between the\napproaches invite the exploration of ways to combine them. Each one of\nthe next subsections illustrates how one could bring together two out\nof the three approaches.\n Section 4.1\n exemplifies the interface between the info-as-range and\ninfo-as-correlation views. Sections\n 4.2\n and\n 4.3\n do the same with the other two pairs of combinations, namely code and\ncorrelations, and code and ranges.  \nA central intuition in the information-as-range view is the\ncorrespondence that exists between information at hand (where this can\nbe qualified in various ways) and the range of possibilities which are\ncompatible with such information. On the other hand, a key feature of\nthe correlational approach to information is its reliance on a\nstructured information system formed by components that are\nsystematically connected. In general, many properties of a structured\nsystem will actually be local properties, in that they are\ndetermined by only some of the components (the fact that there is a\ndot moving upwards in a radar can be determined only by looking at the\nscreen, even if this behaviour is correlated with the motion of a\nremote plane, which is another component of the system). If one has\naccess to information pertaining to only a few of the many components\nof a system, a natural notion of range of possibilities arises,\nconsisting of all the possible global configurations of the system\nthat are compatible with such local information. This subsection\nexpands on this particular way to link the two approaches, but as it\nwill be noted at the end, this is not the only one and the search for\nother ways lies ahead as an open area of inquiry.  \nFormally, the link between ranges and correlations described above may\nbe approached by using a restricted product state space as a\nmodel of the architecture of the system (van Benthem 2006, van Benthem\nand Martinez 2008). The basic structures are constraint\nmodels, versions of which have been around in the literature for\nsome years (for example Fagin et al. 1995 in the study of epistemic\nlogic, and Ghidini and Giunchiglia 2001 in the study of context\ndependent reasoning). Constraint models have the form  \nHere, the basic component spaces are indexed by Comp, the\nstates of each component are taken from States (with\ndifferent components using maybe only a few of the elements of\nStates), and the global states of the system are global\nvaluations, that is, functions that assign a state to each basic\ncomponent Comp. Not all such functions are allowed, only\nthose in \\(C\\). Finally, Pred is a labelled family of\npredicates (sets of global states).  \nTo see how this fits with the information-as-correlation view,\nconsider again the example of planes being monitored by radars. As\nbefore, each monitoring situation will be modelled as having only two\nparts, now indexed by the members of \\(Comp = \\{ screen,\nplane\\}\\). The actual instances of screening situations would\ncorrespond to global states, which in this case — where we have\nonly two components — can be thought of as pairs \\((s, b)\\) where\n\\(s\\) is a particular screen and \\(b\\) a particular plane. Hence,\nglobal states connect instances of parts, so representing instances of\na whole system. But then a crucial restriction comes into play,\nbecause not all screens are connected with all planes, only with those\nbelonging to the same monitoring situation. The set \\(C\\) selects only\nsuch permissible pairs, thus playing a role similar to that of a\nchannel in\n section 1.\n Finally, Pred classifies global states into types, similar to\nthe classification relations of\n section 2.3.\n  \nAs we said before, some properties of systems are local properties,\nwith only some of the components of the systems being relevant in\ndetermining whether they hold or not. That a monitoring situation is\none where the plane is moving North depends only on the plane, not on\nthe screen. In general, if a property is completely determined by\nsubset of components \\(\\mathbf{x}\\) then, in what concerns that\nproperty, any two global states that agree on \\(\\mathbf{x}\\)\nshould be indistinguishable. In fact, each such \\(\\mathbf{x}\\)\ninduces an equivalence relation of local property determination so\nthat for every two global states \\(\\mathbf{s}, \\mathbf{t}\\):  \n\\(\\mathbf{s} \\sim_{\\mathbf{x}}\\mathbf{t}\\) if and only if\nthe values of \\(\\mathbf{s}\\) and \\(\\mathbf{t}\\) at each one of\nthe components in \\(\\mathbf{x}\\) are the same. \nIn this way one gets not only a conceptual but also formal link to the\ninformation-as-range approach, because constraint models can be used\nto interpret a basic modal language with atomic formulas of the form\n\\(P\\)—where \\(P\\) is one of the labels of predicates in\nPred—and with complex formulas of the form \\(\\neg \\phi,\n\\phi \\vee \\psi, U\\phi\\), and \\(\\Box_{\\mathbf{x}}\\phi\\), where\n\\(\\mathbf{x}\\) is a partial tuple of components and \\(U\\) is the\nuniversal modality.  More concretely, given a constraint model\n\\(\\mathscr{M}\\) and a global state \\(s\\), the crucial satisfaction\nconditions are given by:  \nThe resulting logic is axiomatised by the fusion of\n\\(S_5\\) modal logics for the universal modality \\(U\\)\nand each one of the \\(\\Box_{\\mathbf{x}}\\) modalities,\nplus the addition of axioms of the form \\(U \\phi \\rightarrow \\Box_{\\mathbf{x}}\\phi\\), and\n\\(\\Box_{\\mathbf{x}}\\phi \\rightarrow \\Box_{\\mathbf{y}}\\phi\\) whenever\n\\(\\sim_{\\mathbf{y}} \\subseteq \\sim_{\\mathbf{x}}\\).  \nThe information-as-range research agenda includes other topics, such\nas agency and the dynamics of information update, which can in\nprinciple be incorporated to the constraint models setting. For\nexample, in the case of agency, to the architectural structure of a\nstate system captured by a constraint model one could add epistemic\naccessibility relations for a group of agents \\(\\mathcal{A}\\),\nso to obtain epistemic constraint\nmodels of the form  \nwhere \\(\\approx_a\\) is the equivalence accessibility\nrelation of agent \\(a\\). Here one could refine the planes and radar\nexample above by adding some agents, say the controller and the pilot.\nBy relying only on the controls each agent can see, the controller\nwill not be able to distinguish states that agree on the direction of\nthe plane but differ, say, on the metereological conditions around the\nplane. Those states will be related by the controller’s relation\nin the model, but not by the pilot’s relation. In principle,\nthis merge of modal epistemic models and constraint models allows one\nto study, in a single setting, aspects of both the\ninformation-as-range and information-as-correlation points of view.\nThe corresponding logical language for epistemic constraint models is\nthe same as for basic constraint models, expanded with the\n\\(K_i\\) modal operators, one per agent. The logic\nis the fusion of the constraint logic from above and a\n\\(S_5\\) logic per each agent \\(a\\).  \nThere are some newer, different approaches to information modelling\nthat sit at the intersection of the information as range and\ninformation as correlation perspectives. One is van Benthem’s\nwork on information tracking (van Benthem 2016). Tracking is a new\nperspective that addresses both the connections between different\nrepresentations of information on the one hand, and the updates on\nthese connections on the other.  \nAnother recent development (Baltag 2016) comes from a line of work\nthat studies how to capture, in the style of epistemic logics such as\nthose described in\n section 1,\n the properties and dynamics of knowledge de re (Wang and Fan\n2014). Identifying this kind of knowledge with knowledge of the value\nof a variable, Baltag’s insight is to add, to the language of\nbasic epistemic logic, the usual first-order resources for\nconstructing terms and basic formulas (that is, symbols of constants,\nfunctions, relations, and variables), plus, crucially, a generalised\nconditional knowledge operator \\(K_{a}^{t_1 ,\\ldots ,t_n}\\).  The\nextended language has now formulas \\(K_{a}^{t_1 ,\\ldots ,t_n} t\\) and\n\\(K_{a}^{t_1 ,\\ldots ,t_n} \\phi\\), with the intended meaning that\nagent \\(a\\) knows the value of term \\(t\\) (or knows that \\(\\phi\\), for\nthe second formula), provided it knows the values of terms \\(t_1\n,\\ldots ,t_n\\). To be able to capture this idea on the semantic side,\nKripke models are enriched so that, in addition to the usual set of\ninformation states, interpretations for propositional letters, and\nagents relations, we will also have a domain of objects over which\nterms and basic relational formulas are locally interpreted at each\nstate (that is, the interpretations can vary from state to state, but\nthe underlying domain is the same across states). A sound and complete\naxiomatisation exists, and the resulting logical system is a sort of a\ngeneral, yet decidable, dependence logic where information about\ncorrelations can be captured via the conditional knowledge\noperators. Dynamic versions are also obtained where, in addition to\nthe public announcement operator \\([\\phi]\\), one has value\nannouncement operators \\([t_1,\\ldots ,t_n]\\), with formula\n\\([t_1,\\ldots ,t_n] \\phi\\) being read as “after the simultaneous\nannouncement of the values of terms \\(t_1,\\ldots ,t_n\\), it is the\ncase that \\(\\phi\\)”.  \nYet other links between the approaches have also be found, which are\nmotivated by other kind of questions and use formalisms that are\ncloser to the situation-theoretic ones. For example, consider a\nsetting in which agents have incomplete information about an\nintended subset of a set of epistemic states. How can a relation of\naccessibility arise from such a setting? (Notice that this is\ndifferent to the setting of epistemic constraint models described\nabove, where agents do have complete information about what holds true\nof all the epistemically accessible worlds). One way to address this\nquestion (Barwise 1997) is to consider a fixed classification\n\\(A\\), the instances of which are the epistemic states, plus a\nlocal logic per agent attached to each state. For some states these\nlocal logics may be incomplete (see\n section 2.3),\n so agents may not have information about everything that holds true\nof the intended range of states. Then, roughly, the states accessible\nfrom a given state \\(s\\) and agent \\(a\\) will be those whose\nproperties (types) do not contradict the local logic of \\(a\\) in\n\\(s\\). With these epistemic relations in place, classification\n\\(A\\) can be used to interpret a basic modal language.  \nLogical frameworks that crossover information as code and information\nas correlation get their most explicit representation in work that\ndoes just this—model the crossover between the two frameworks.\nRestall (1994) and Mares (1996) give independent proofs of the\nrepresentability of Barwise’s information as correlation\nchannel-theoretic framework within the information as code approach as\nexemplified by the substructural logics framework. In this section we\nwill trace the motivations and the main details of the proof, before\ndemonstrating the connection with category theory.  \nThe basic steps are these—if we understand information channels\nto be information states of a special sort, namely the sort of\ninformation state that carries information of conditional types, then\nthere is an obvious meeting point between information as correlation\nas exemplified by channel theory, and information as code as\nexemplified by informationalised substructural logics. The\nintermediate step is to reveal the connection between channel\nsemantics for conditional types, and the frame semantics for\nconditionals given by relevance logics.  \nStarting with the channel theoretic analysis of conditionals, as noted\nalready, the running motivation behind Barwise’s\nchannel-theoretic framework is that information flow is underpinned by\nan information channel. Barwise understood conditionals as\nconstraints in the sense that \\(A \\rightarrow B\\) is a\nconstraint from \\(A\\) to \\(B\\) in the sense of \\(A \\Rightarrow B\\) from\n section 2.2\n above. If the information that \\(A\\) is combined with the\ninformation encoded by the constraint, then the result or output is\nthe information that \\(B\\).  \nThe information that \\(A\\) and that \\(B\\) is carried by the\nsituations \\(s_1, s_2\\ldots\\). and the\ninformation encoded by the constraint is carried by an information\nchannel \\(c\\). Given this, Barwise’s evaluation condition for\na constraint is as follows (the condition is given here in\nBarwise’s notation from his later work on conditionals, although\nin earlier writings such conditions appeared in the notation given in\n section 2.2\n above):  \nwhere \\(s_1 \\stackrel{c}{\\mapsto} s_2\\) is read as \nthe information carried by the channel \\(c\\), when combined with the\ninformation carried by the situation \\(s_1\\), results in the\ninformation carried by the situation \\(s_2\\). \nObviously enough, this is very close in spirit to (9) in the section\non information as code above.  \nAs noted above, the intermediate step concerns the ternary relation\n\\(R\\) from the early semantics for relevance logic. The semantic\nclause for the conditional from relevance logic is:  \n\\(Rxyz\\) is, by itself, simply an abstract mathematical entity. One\nway or reading it, the way that became popular in relevance logic\ncircles, is \n\\(Rxyz\\) iff the result of combining \\(x\\) with \\(y\\) is true\nat \\(z\\). \nGiven that the points of evaluation in relevance logics were\nunderstood originally as impossible situations (since they may be both\ninconsistent and incomplete), the main conceptual move was to\nunderstand channels to be special types of situations. The full proofs\nmay be found in Restall (1994) and Mares (1996), and these demonstrate\nthat the expressive power of Barwise’s system may be captured by\nthe frame semantics of relevance logic. What it is that such\n“combining” of \\(x\\) and \\(y\\) amounts to depends\non, of course, which structural rules are operating on the frame in\nquestion. As explained in the previous section above, the choice of\nwhich rules to include will depend on the properties of the phenomena\nbeing modelled.  \nThe final step required for locating the meeting point between\ninformation as code and information as correlation is as follows.\nContemporary approaches to relevance and other substructural logics\nunderstand the points of evaluation (impossible situations) to be\ninformation states. There is certainly no constraint on information\nthat it be complete or consistent, so the expressibility of impossible\nsituations it not sacrificed. Such an informational reading (Paoli\n2002; Restall 2000; Mares 2004) lends itself to multiple applications\nof various substructural frameworks, and also does away with the\nontological baggage brought by questions like “what are\nimpossible situations?” in the “What are possible\nworlds?” spirit. An information-state reading of \\(Rxyz\\)\nwill be something like \nthe result of combining the information carried by \\(x\\) and\n\\(y\\) generates the informations carried by \\(z\\). \nMaking this explicit results in \\(Rxyz\\) being written down as \\(x\n\\bullet y \\sqsubseteq z\\), in which case (15) is, via (16), equivalent\nto (9).  \nAn important structural rule for the composition operation on\ninformation channels, that is, on information states that carry\ninformation of conditional types, is that it is associative. What this\nmeans is that:  \nWhere \\(z \\Vdash A\\) and \\(w \\Vdash D\\), this will be the case for all\n\\(x, y, v\\) s.t. \\(x \\Vdash A \\rightarrow\\), \\(y \\Vdash B \\rightarrow\nC\\), \\(v \\Vdash C \\rightarrow D\\). This is just the first step\nrequired to demonstrate that channel theory, and its underlying\nsubstructural logic, form a category.  \nCategory theory is an extremely powerful tool in its own right. For a\nthorough introduction see Awodey (2006). For more work on the\nrelationship between various substructural logics and channel theory,\nsee Restall (1994a, 1997, 2006). Further category-theoretic work on\ninformation flow may be found in Goguen (2004—see Other Internet\nResources). Recent important work on category-theoretic frameworks for\ninformation flow that extend to quantifiable/probabilistic\nframeworks is due to Seligman (2009). Perhaps the most in depth\ntreatment of information flow in category theoretic terms is to be\nfound in the work of Samson Abramsky, and an excellent overview may be\nfound in his “Information, Processes, and Games” (2008).\nRecent work on the intersection between information as code and\ninformation as correlation uses substructural logics (relevance and\nlinear logics in particular) to model logical proofs as information\nsources themselves. A proof is a source of information par\nexcellence, and the contributions in the area by Mares (2016) are\nvital. \nExcitingly, there has been a recent surge in the recent development of\ninformation logics that combine the flexibility of categorial\ninformation theory with the subject matter of dynamic epistemic logics\nin order to design  substructural epistemic logics. Sedlar\n(2015) combines the modal epistemic logics of implicit knowledge and\nbelief with substructural logics in order to capture the availability\nof evidence for the agent. Aucher (2015, 2014) redefines dynamic\nepistemic logic as a substructural logic corresponding to the Lambek\nCalculi of categorial information theory. Aucher shows also that the\nsemantics for DEL can be understood as providing a conceptual\nfoundation for the semantics of substructural logics in general. See\nHjortland and Roy (2016) for an extension of Aucher’s approach\nto soft information \nOther logical frameworks that model information as code and range\nalong with information about encoding have been developed by\nVelázquez-Quesada (2009), Liu (2009), Jago (2006), and others.\nThe key element to all of these approaches is the introduction of some\nsyntactic code to the conceptual architecture of the information as\nrange approach.  \nTaking Velázquez-Quesada (2009) as a working example, start\nwith a modal-access model \\(M =\\langle S, R, V, Y, Z\\rangle\\)\nwhere \\(\\langle S, R, V \\rangle\\) is a Kripke Model, \\(Y\\) is\nthe access set function, and \\(Z\\) is the rule set\nfunction s.t.  (where \\(I\\) is the set of classical propositional\nlanguage based on a set of atomic propositions):  \nA modal-access model is a member of the class of modal access models\n\\(\\mathbf{MA}\\) iff it satisfies truth for formulas and truth\npreservation for rules. \\(\\mathbf{MA}_k\\) models\nare those \\(\\mathbf{MA}\\) models such that \\(R\\) is an\nequivalence relation.  \nFrom here, inference is represented as a modal operation adding the\nrule’s conclusion to the access set of information states of the\nof the agent such that the agent can access both the rule and its\npremises. Where \\(Y(x)\\) is the access set at \\(x\\), and\n\\(Z(x)\\) is the rule set at \\(x\\):  \nInference on knowledge: Where \\(M = \\langle S, R,\nV, Y, Z\\rangle \\in \\mathbf{MA}_k\\), and \\(\\sigma\\) is a rule,\n\\(M_k\\sigma = \\langle S, R, V, Y', Z\\rangle\\) differs from \\(M\\) in\n\\(Y'\\), given by \\(Y'(x) := Y(x) \\cup \\{\\)conc\\((\\sigma)\\}\\) if\n\\(\\text{prem}(\\sigma) \\subseteq Y(x)\\) and \\(\\sigma \\in Z(x)\\), and by\n\\(Y'(x) := Y(x)\\) otherwise. \nThe dynamic logic for inference on knowledge then incorporates the\nability to represent “there is a knowledge inference\nwith \\(\\sigma\\) after which \\(\\phi\\) holds”\n(Velázquez-Quesada 2009). It is in just this sense that such\nmodal information theoretical approaches model the outputs of\ninferential processes, as opposed to the properties of the inferential\nprocesses that generate such outputs (see the section on\ncategorial information theory for models of such dynamic\nproperties).  \nJago (2009) proposes a rather different approach based upon the\nelimination of worlds considered possible by the agent as the\nagent reasons deductively. Such epistemic (doxastic) possibilities\nstructure an epistemic (doxastic) space under bounded rationality. The\nconnection with information as code is that the modal space is\nindividuated syntactically, with the worlds corresponding to possible\nresults of step-wise rule-governed inferences. The connection with\ninformation as range is that the rules that he agent does or does not\nhave access to will impact upon the range of discrimination for the\nagent. For example, if the agent’s epistemic-base contains two\nworlds, a \\(\\neg \\phi\\) world and a \\(\\phi \\vee \\psi\\) world say, then can\nrefine their epistemic base only if they have access to the\ndisjunctive syllogism rule.  \nA subtle but important contribution of Jago’s is the following:\nthe modal space in question will contain only those epistemic options\nwhich are not obviously impossible. However, what is or is\nnot obviously impossible will vary from both agent to agent, as well\nas for a single agent over time as that agent refines its logical\nacumen. This being the case, the modal space in question has\nfuzzy boundaries.  \nThere is a varied list of special topics pertaining to the logical\napproach to information. This section briefly illustrates just a\ncouple of them, which are important regardless of the particular\nstance one takes (information as range, as correlation, as code). The\nfirst topic is the issue of informational equivalence: when are two\nstructures in the logical approach one is using indistinguishable in\nterms of the information they are meant to encode, convey, or carry?\nThe second topic concerns the various ways in which the idea of\nnegative information can be understood conceptually, and properly\ndealt with formally.  \nEvery logical approach to information comes with its own kind of\ninformation structures. Depending on the particular stance and the\naspect of information to be stressed, these structures may stand for\ninformational states, structured syntactic representations, pieces of\ninformation understood as commodities, or global structures made up\nfrom local interrelated informational states or stages. Under which\nconditions can two informational structures be considered to be\ninformationally equivalent?  \nAddressing this question brings out the need to have it clear at which\nlevel of granularity one is testing for equivalence. Take for example\nthe classical extensional notion of logical equivalence. This is a\ncoarse equivalence, in that informationally different claims such as 2\nis even and 2 is prime cannot be distinguished, as\ntheir extensions will coincide. Take instead an equivalence given by\nidentity at the level of representations (say syntactic equality).\nThis is on the contrary too fine-grained in some cases: to a bilingual\nspeaker, the information that the shop is closed would be equally\nconveyed by a sign saying “Closed” as by a sign saying\n“Geschlossen”, even if the two words are\ndifferent.  \nAn intermediate notion of equivalence that has proved central to the\nrange, correlational, and code views on information is the relation of\nbisimulation between structures. A bisimulation relation between two\ngraphs \\(G\\) and \\(H\\) (where both the arrows and nodes of the\ngraphs are labelled) is a binary relation \\(R\\) between the nodes\nof the graphs with the property that whenever a node \\(g\\) of\n\\(G\\) is related to a node \\(h\\) of \\(H\\), then:  \nA simple example would be the relation between the following two\ngraphs (empty set of labels) that relates the point \\(x\\) with\n\\(a\\) and the point \\(y\\) with the points \\(b, c, d\\).  \nBisimulation is naturally a central notion for the\ninformation-as-range perspective because the Kripke models of\n section 1\n are precisely labelled graphs. It is a classical result of modal\nlogic that if two states of two models are related by a bisimulation,\nthen the states will satisfy exactly the same modal formulas, and in\naddition a first order property of states is definable in the basic\nmodal language if and only if the property is preserved under\nbisimulation.  \nAs for the correlational stance, in situation theory bisimulation\nturns out to be the right notion in determining whether two infons\nthat might look structurally different are actually the same as pieces\nof information. For example, one possible analysis of Liar-like claims\nleads to infons that are nested in themselves, such as  \nOne can naturally depict the structure of \\(\\sigma\\) as a labelled graph,\nwhich will be bisimilar to the graph associated with the apparently\ndifferent infon  \nThe notion of bisimulation appeared independently in computer science,\nso it so no surprise that it also features in matters related to the\ninformation-as-code approach, with its focus on representation and\ncomputation. In particular, several versions of bisimulation have been\napplied to classes of automata to determine when two of them are\nbehaviourally equivalent, and data encodings such as  \nboth of which represent the same object (an infinite list of zeroes),\ncan be identified as such by noticing that the graphs that depict the\nstructure of these two expressions are bisimilar. See Aczel (1988),\nBarwise and Moss (1996), and Moss (2009) for more information about\nbisimulation an circularity, connections with modal logic, data\nstructures, and coalgebras.  \nThis entry has focused mostly on positive information.\nFormally speaking, negative information is simply the\nextension-via-negation of the positive fragment of any logic built\naround information-states. Different negation-types will constrain the\nbehaviour of negative information in various ways. Informally,\nnegative information may be thought of variously as what is\ncanonically expressed with sentential negation, process exclusion\n(both propositional and sub-propositional) and more. Even when we\nrestrict ourselves to a single conceptual notion, there may be\nvigorous philosophical debate as to which formal construction best\ncaptures the notion in question. In this section, we run though\nseveral formal analyses of negative information, we examine some of\nthe philosophical debates surrounding the suitability of various\nformal constructions with respect to particular applications, and\nexamine the related topic of failure of information flow in the\nsituation-theoretic sense, which may give raise to misinformation or\nlack of information in particular settings.  \nNon-constructive intuitionistic negation, is aimed towards accounting\nfor negative information in the context of information flow via\nobservation. For more details on this point, see the subsection\nintuitionistic logics and Beth and Kripke models in the supplementary\ndocument:\n Abstract Approaches to Information Structure. \nWorking with the frames from\n section 3.1,\n non-constructive intuitionistic negation is defined in terms of the\nconstructive implication, (21), which is combined with bottom,\n\\(\\mathbf{0}\\), which holds nowhere, as specified by its frame\ncondition:  \nHence intuitionistic negation is defined as follows:  \nHence the frame condition for \\(-A\\) is as follows:  \n(20) states that if \\(x\\) carries the information that\n\\(-A\\), then there no state \\(y\\) such that \\(y\\) is an\ninformational development of \\(x\\) where \\(y\\) carries the\ninformation that \\(A\\).  \nThe definition of \\(-A\\) in terms of \\(A \\rightarrow \\mathbf{0}\\) throws up an\nasymmetry between positive and negative information. In an information\nmodel \\(-A\\) holds at \\(x \\in \\mathbf{F}\\) iff \\(A\\) does not hold at any \\(y \\in \\mathbf{F}\\) such that \\(x \\sqsubseteq y\\). Whilst the verification\nof \\(A\\) at \\(x \\in \\mathbf{F}\\) only involves\nchecking \\(x\\), verifying \\(-A\\) at \\(x \\in \\mathbf{F}\\) involves checking all \\(y \\in \\mathbf{F}\\) such that \\(x \\sqsubseteq y\\). According to Gurevich (1977) and Wansing\n(1993), this asymmetry means that intuitionistic logic does not\nprovide an adequate treatment of negative information, since, unlike\nthe verification of \\(A\\), there is no way of verifying\n\\(-A\\) “on the spot” so to speak. Gurevich and\nWansing’s objection to this asymmetry is a critical response to\nGrzegorczyk (1964). For arguments in support of Grzegorczyk’s\nasymmetry between positive and negative information, see\nSequoiah-Grayson (2009). A fully constructive negation that allows for\nfalsification “on the spot” is known also as Nelson\nNegation on account of it being embedded within Nelson’s\nconstructive systems (Nelson 1949, 1959). For a contemporary\ndevelopment of these constructive systems, see section 2.4.1 of\nWansing (1993).  \nIn a static logic setting, negation is, at the very least, used to\nrule out truth (if not to express explicit falsity). In a dynamic\nsetting, negation will be used to rule out particular\nprocesses. For a development negative information as process\nexclusion in the context of categorial information theory see\nSequoiah-Grayson (2013). This idea has its origins in the Dynamic\nPredicate Logic of Groenendijk and Stokhof (1991), in particular with\ntheir development of negative information via negation as\ntest-failure. For an exploration between the conceptions of\nnegative information as process exclusion and test-failure, see\nSequoiah-Grayson (2010).  \nIn any logic for negation as process-exclusion, the process-exclusion\nwill be non-directional if the logic in question is\ncommutative. Directional process-exclusion will result when we remove\nthe structural rule of commutation. For a discussion of the\nrelationship between the formalisation of directional process\nexclusion as commutation-failure along with symmetry-failure on\ncompatibility and incompatibility relations on information states, see\nSequoiah-Grayson (2011). For an extended discussion of negative\ninformation in the context of categorial grammars, see Buszkowski\n(1995).  \nThere is a bi-directional relation between logic and information. On\nthe one hand, information underlies the intuitive understanding of\nstandard logical notions such as inference (which may be thought of as\nthe process that turns implicit information into explicit informaiton)\nand computation. On the other hand, logic provides a formal framework\nfor the study of information itself.  \nThe logical study of information focuses on some of the most\nfundamental qualitative aspects of information. Different stances on\ninformation naturally highlight some of these aspects more than\nothers. Thus, the information-as-range stance most naturally\nhighlights agency and the dynamics of information in settings with\nmultiple agents that can interact with each other. The aboutness of\ninformation (information is always about something) is a central theme\nin the information-as-correlation stance. The topic of encoding\ninformation and its processing (as in the case of formal inference) is\nat the core of the information-as-code stance. None of these\nqualitative aspects of information is exclusive to just one of the\nstances, even if some stress certain topics more than others. Some\nthemes such as the structure of information and its relation with\ninformation content are equally pertinent regardless of the stance.\nThe ways in which information is studied in this entry differs from\nother important formal frameworks that study information\nquantitatively. For example, Shannon’s statistical theory of\ninformation is concerned with things such as optimizing the amount of\ndata that can be transmitted via a noisy channel, and the\nKolmogorov’s complexity theory quantifies the informational\ncomplexity of a string as the length of the shortest program that\noutputs it when executed by a fixed universal Turing machine.  \nThe logical analysis of information includes fruitful\nreinterpretations of known logical systems (such as epistemic logic or\nrelevance logic), and new systems that result from attempts to capture\nfurther aspects of informational. Still other logical approaches to\nthe analysis of information result from combining aspects of two\ndifferent stances, as with the constraint systems of\n section 4.\n New frameworks (situation theory in the 80s) have also resulted from\nexploring from scratch what sort of inferences—including those\nthat are novel and non-classical—one should allow in order to\nmodel certain aspects of information.  \nLooking for interfaces between the three stances is a nascent\ndirection of inquiry, discussed here in\n section 4.\n A complementary issue is whether the stances can be unified. There\nare several formal frameworks that, beyond serving as potential\nsettings for exploring the issue of unification, are abstract\nmathematical theories of information in their own right. Each of these\ngoes well beyond the scope of this entry:  \nThe logical study of information resembles in spirit other more\ntraditional endeavours, such as the logical study of the concept of\ntruth or computation: in all these cases the object of logical study\nplays a central role in the intuitive understanding of logic itself.\nThe three perspectives on qualitative information presented in this\nentry (ranges, correlations, and code) portrait the diverse state of\nthe art in this field, where many directions of research are open,\nboth as a way of searching for unifying or interfacing settings for\nthe different stances, and of deepening the understanding of the main\nqualitative features of information (dynamics, aboutness, encoding,\ninteraction, etc.) within each stance itself.  \nInterested readers may wish to pursue the topics in the supplementary\ndocument \nwhich covers the topics\n intuitionistic logic, Beth and Kripke models,\n and\n algebraic and other approaches to modal information theory and related areas.","contact.mail":"sequoiah@gmail.com","contact.domain":"gmail.com"}]
