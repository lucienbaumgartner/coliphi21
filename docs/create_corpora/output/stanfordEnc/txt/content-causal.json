[{"date.published":"2010-02-04","date.changed":"2017-04-18","url":"https://plato.stanford.edu/entries/content-causal/","author1":"Fred Adams","author1.info":"https://www.lingcogsci.udel.edu/people/faculty/Frederick%20Adams","author2.info":"https://ncas.rutgers.edu/about-us/faculty-staff/kenneth-aizawa","entry":"content-causal","body.text":"\n\n\nCausal theories of mental content attempt to explain how thoughts can\nbe about things. They attempt to explain how one can think about, for\nexample, dogs. These theories begin with the idea that there are\nmental representations and that thoughts are meaningful in virtue of a\ncausal connection between a mental representation and some part of the\nworld that is represented. In other words, the point of departure for\nthese theories is that thoughts of dogs are about dogs because dogs\ncause the mental representations of dogs.\n\nContent is what is said, asserted, thought, believed, desired, hoped\nfor, etc. Mental content is the content had by mental states and\nprocesses. Causal theories of mental content attempt to explain what\ngives thoughts, beliefs, desires, and so forth their contents. They\nattempt to explain how thoughts can be about\n things.[1] \nAlthough one might find precursors to causal theories of mental\ncontent scattered throughout the history of philosophy, the current\ninterest in the topic was spurred, in part, by perceived inadequacies\nin “similarity” or “picture” theories of\nmental representation. Where meaning and representation are asymmetric\nrelations—that is, a syntactic item “X” might mean\nor represent X, but X does not (typically) mean or represent\n“X”—similarity and resemblance are symmetric\nrelations. Dennis Stampe (1977), who played an important role in\ninitiating contemporary interest in causal theories, drew attention to\nrelated problems. Consider a photograph of one of two identical twins.\nWhat makes it a photo of Judy, rather than her identical twin Trudy?\nBy assumption, it cannot be the similarity of the photo to one twin\nrather than the other, since the twins are identical. Moreover, one\ncan have a photo of Judy even though the photo happens not to look\nvery much like her at all. What apparently makes a photo of Judy a\nphoto of Judy is that she was causally implicated, in the right way,\nin the production of the photo. Reinforcing the hunch that causation\ncould be relevant to meaning and representation is the observation\nthat there is a sense in which the number of rings in a tree stump\nrepresents the age of the tree when it died and that the presence of\nsmoke means fire. The history of contemporary developments of causal\ntheories of mental content consists largely of specifying what it is\nfor something to be causally implicated in the right way in the\nproduction of meaning and refining the sense in which smoke represents\nfire to the sense in which a person’s thoughts, sometimes at\nleast, represent the world. \nIf one wanted to trace a simple historical arc for recent causal\ntheories, one would have to begin with the seminal 1977 paper by\nDennis Stampe, “Toward a Causal Theory of Linguistic\nRepresentation.” Among the many important features of this paper\nis its having set much of the conceptual and theoretical stage to be\ndescribed in greater detail below. It drew a contrast between causal\ntheories and “picture theories” that try to explain\nrepresentational content by appeal to some form of similarity between\na representation and the thing represented. It also drew attention to\nthe problem of distinguishing the content determining causes of a\nrepresentation from adventitious non-content determining causes. So,\nfor example, one will want “X” to mean dogs because dogs\ncauses dogs, but one does not want “X” to mean\nblow-to-the-head, even though blows to the head might cause the\noccurrence of an “X”. (Much more of this will be described\nbelow.) Finally, it also provided some attempts to address this\nproblem, such as an appeal to the function a thing might have. \nFred Dretske’s 1981 Knowledge and the Flow of\nInformation offered a much expanded treatment of a type of causal\ntheory. Rather than basing semantic content on a causal connection\nper se, Dretske began with a type of informational connection\nderived from the mathematical theory of information. This has led some\nto refer to Dretske’s theory as “information\nsemantics”. Dretske also appealed to the notion of function in\nan attempt to distinguish content determining causes from adventitious\nnon-content determining causes. This has led some to refer to\nDretske’s theory as a “teleoinformational” theory or\na “teleosemantic” theory. Dretske’s 1988 book,\nExplaining Behavior, further refined his earlier\ntreatment. \nJerry Fodor’s 1984 “Semantics, Wisconsin Style” gave\nthe problem of distinguishing content-determining causes from\nnon-content determining causes its best-known guise as “the\ndisjunction problem”. How can a causal theory of content say\nthat “X” has the non-disjunctive content dog, rather than\nthe disjunctive content dog-or-blow-to-the-head, when both dogs and\nblows to the head cause instances of “X”? By 1987, in\nPsychosemantics, Fodor published his first attempt at an\nalternative method of solving the disjunction problem, the Asymmetric\n(Causal) Dependency Theory. This theory was further refined for the\ntitle essay in Fodor’s 1990 book A Theory of Content and\nOther Essays. \nAlthough these causal theories have subsequently spawned a significant\ncritical literature, other related causal theories have also been\nadvanced. Two of these are teleosemantic theories that are sometimes\ncontrasted with causal theories. (Cf., e.g., Papineau (1984), Millikan\n(1989), and Teleological Theories of Mental Content.) Other more\npurely causal theories are Dan Lloyd’s (1987, 1989) Dialectical\nTheory of Representation, Robert Rupert’s (1999) Best Test\nTheory (see section 3.5 below), Marius Usher’s (2001)\nStatistical Referential Theory, and Dan Ryder’s (2004) SINBAD\nneurosemantics. \nCausal theories of mental content are typically developed in the\ncontext of four principal assumptions. First, they typically\npresuppose that there is a difference between derived and underived\n meaning.[2]\n Normal humans can use one thing, such as “%”, to mean\npercent. They can use certain large red octagons to mean that one is\nto stop at an intersection. In such cases, there are collective\narrangements that confer relatively specific meanings on relatively\nspecific objects. In the case of human minds, however, it is proposed\nthat thoughts can have the meanings or contents they do without\nrecourse to collective arrangements. It is possible to think about\npercentage or ways of negotiating intersections prior to collective\nsocial arrangements. It, therefore, appears that the contents of our\nthoughts do not acquire the content they do in the way that\n“%” and certain large red octagons do. Causal theories of\nmental content presuppose that mental contents are underived, hence\nattempt to explain how underived meaning arises.  \nSecond, causal theories of mental content distinguish what has come to\nbe known as natural meaning and non-natural\n meaning.[3]\n Cases where an object or event X has natural meaning are those in\nwhich, given certain background conditions, the existence or\noccurrence of X “entails” the existence or occurrence of\nsome state of affairs. If smoke in the unspoiled forest naturally\nmeans fire then, given the presence of smoke, there was fire. Under\nthe relevant background conditions, the effect indicates or naturally\nmeans the cause. An important feature of natural meaning is that it\ndoes not generate falsity. If smoke naturally means fire, then there\nmust really be a fire. By contrast, many non-naturally meaningful\nthings can be false. Sentences, for example, can be meaningful and\nfalse. The utterance “Colleen currently has measles” means\nthat Colleen currently has measles but does not entail that Colleen\ncurrently has measles in the way that Colleen’s spots do entail\nthat she has measles. Like sentences, thoughts are also meaningful,\nbut often false. Thus, it is generally supposed that mental content\nmust be a form of non-natural unassigned\n meaning.[4] \nThird, these theories assume that it is possible to explain the origin\nof non-derived content without appeal to other semantic or contentful\nnotions. So, it is assumed that there is more to the project than\nsimply saying that one’s thoughts mean that Colleen currently\nhas the measles because one’s thoughts are about Colleen\ncurrently having the measles. Explicating meaning in terms of\naboutness, or aboutness in terms of meaning, or either in terms of\nsome still further semantic notion, does not go as far as is commonly\ndesired by those who develop causal theories of mental content. To\nnote some additional terminology, it is often said that causal\ntheories of mental content attempt to naturalize non-natural,\nnon-derived meaning. To put the matter less technically, one might say\nthat causal theories of mental content presuppose that it is possible\nfor a purely physical system to bear non-derived content. Thus, they\npresuppose that if one were to build a genuinely thinking robot or\ncomputer, one would have to design it in such a way that some of its\ninternal components would bear non-natural, non-derived content in\nvirtue of purely physical conditions. To get a feel for the difference\nbetween a naturalized theory and an unnaturalized theory of content,\none might note the theory developed by Grice (1948). Grice developed\nan unnaturalized theory. Speaking of linguistic items, Grice held that\n‘Speaker S non-naturally means something by\n“X”’ is roughly equivalent to ‘S intended the\nutterance of “X” to produce some effect in an audience by\nmeans of the recognition of this intention.’ Grice did not\nexplicate the origin of mental content of speaker’s intentions\nor audience recognition, hence he did not attempt to naturalize the\nmeaning of linguistic items. \nFourth, it is commonly presupposed that naturalistic analyses of\nnon-natural, non-derived meanings will apply, in the first instance,\nto the contents of thought. The physical items “X” that\nare supposed to be bearers of causally determined content will,\ntherefore, be something like the firings of a particular neuron or set\nof neurons. These contents of thoughts are said to be captured in what\nis sometimes called a “language of thought” or\n“mentalese.” The contents of items in natural languages,\nsuch as English, Japanese, and French, will then be given a separate\nanalysis, presumably in terms of a naturalistic account of non-natural\nderived meanings. It is, of course, possible to suppose that it is\nnatural language, or some other system of communication, that first\ndevelops content, which can then serve as a basis upon which to\nprovide an account of mental content. Among the reasons that threaten\nthis order of dependency is the fact that cognitive agents appear to\nhave evolved before systems of communication. Another reason is that\nhuman infants at least appear to have some sophisticated cognitive\ncapacities involving mental representation, before they speak or\nunderstand natural languages. Yet another reason is that, although\nsome social animals may have systems of communication complex enough\nto support the genesis of mental content, other non-social cognizing\nanimals may not. \nIt is worth noting that, in recent years, this last presupposition has\nsometimes been abandoned by philosophers attempting to understand\nanimal signaling or animal communication, as when toads emit mating\ncalls or vervet monkeys cry out when seeing a cheetah, eagle, or\nsnake. See, for example, Stegmann, 2005, 2009, Skyrms, 2008, 2010a, b,\n2012, and Birch, 2014. In other words, there have been efforts to use\nthe sorts of apparatus originally developed for theories of mental\ncontent, plus or minus a bit, as apparatus for handling animal\nsignaling. These approaches seem to allow that there are mental\nrepresentations in the brains of the signaling/communicating animals,\nbut do not reply on the content of the mental representations to\nprovide the representational contents of the signals. In this way, the\ncontents of the signals are not derived from the contents of the\nmental representations. \nThe unifying inspiration for causal theories of mental content is that\nsome syntactic item “X” means X because “X”s\nare caused by\n Xs.[5]\n Matters cannot be this simple, however, since in general one expects\nthat some causes of “X” are not among the\ncontent-specifying causes of “X”s. There are numerous\nexamples illustrating this point, each illustrating a kind of cause\nthat must not typically be among the content-determining causes of\n“X”: \nThe foregoing problem cases are generally developed under the rubric\nof “false beliefs” or “the disjunction\nproblem” in the following way and can be traced to Fodor (1984).\nNo one is perfect, so a theory of content should be able to explicate\nwhat is going on when a person makes a mistake, such as mistaking a\nfox for a dog. The first thought is that this happens when a fox (at a\ndistance or in poor lighting conditions) causes the occurrence of a\ntoken of “X” and, since “X” means dog, one has\nmistaken a fox for a dog. The problem with this first thought arises\nwith the invocation of the idea that “X” means dog. Why\nsay that “X” means dog, rather than dog or fox? On a\ncausal account, we need some principled reason to say that the content\nof “X” is dog, hence that the token of “X” is\nfalsely tokened by the fox, rather than the content of “X”\nis dog or fox, hence that the token of “X” is truly\ntokened by the fox. What basis is there for saying that\n“X” means dog, rather than dog or fox? Because there\nappears always to be this option of making the content of a term some\ndisjunction of items, the problem has been called “the\ndisjunction\n problem”.[6] \nAs was noted above, what unifies causal theories of mental content is\nsome version of the idea that “X”s being causally\nconnected to Xs makes “X”s mean Xs. What divides causal\ntheories of mental content, most notably, is the different approaches\nthey take to separating the content-determining causes from the\nnon-content-determining causes. Some of these different theories\nappeal to normal conditions, others to functions generated by natural\nselection, others to functions acquired ontogenetically, and still\nothers to dependencies among laws. At present there is no approach\nthat is commonly agreed to correctly separate the content-determining\ncauses from the non-content determining causes while at the same time\nrespecting the need not to invoke existing semantic concepts. Although\neach attempt may have technical problems of its own, the recurring\nproblem is that the attempts to separate content-determining from\nnon-content-determining causes threaten to smuggle in semantic\nelements. \nIn this section, we will review the internal problematic of causal\ntheories by examining how each theory fares on our battery of test\ncases (I)–(IV), along with other objections from time to time.\nThis provides a simple, readily understood organization of the project\nof developing a causal theory of mental content, but it does this at a\nprice. The primary literature is not arranged exactly in this way. The\npositive theories found in the primary literature are typically more\nnuanced than what we present here. Moreover, the criticisms are not\narranged into the kind of test battery we have with cases\n(I)–(IV). One paper might bring forward cases (I) and (III)\nagainst theory A, where another paper might bring forward cases (I)\nand (II) against theory B. Nor are the examples in our test battery\nexactly the ones developed in the primary literature. In other words,\nthe price one pays for this simplicity of organization is that we have\nsomething less like a literature review and more like a theoretical\nand conceptual toolbox for understanding causal theories. \nTrees usually grow a certain way. Each year, there is the passage of\nthe four seasons with a tree growing more quickly at some times and\nmore slowly at others. As a result, each year a tree adds a\n“ring” to its girth in such a way that one might say that\neach ring means a year of growth. If we find a tree stump that has\ntwelve rings, then that means that the tree was twelve years old when\nit died. But, it is not an entirely inviolable law that a tree grows a\nring each year. Such a law, if it is one, is at most a ceteris\nparibus law. It holds only given certain background conditions,\nsuch as that weather conditions are normal. If the weather conditions\nare especially bad one season, then perhaps the tree will not grow\nenough to produce a new ring. One might, therefore, propose that if\nconditions are normal, then n rings means that the tree was n years\nold when it died. This idea makes its first appearance when Stampe\n(1977) invokes it as part of his theory of “fidelity\nconditions.” \nAn appeal to normal conditions would seem to be an obvious way in\nwhich to bracket at least some non-content-determining causes of a\nwould-be mental representation “X”. It is only the causes\nthat operate under normal conditions that are content-determining. So,\nwhen it comes to human brains, under normal conditions one is not\nunder the influence of hallucinogens nor is one’s head being\ninvaded by an elaborate configuration of microelectrodes. So, even\nthough LSD and microelectrodes would, counterfactually speaking, cause\na token neural event “X”, these causes would not be among\nthe content-determining causes of “X”. Moreover, one can\ntake normal conditions of viewing to include good lighting, a\nparticular perspective, a particular viewing distance, a lack of\n(seriously) occluding objects, and so forth, so that foxes in dim\nlight, viewed from the bottom up, at a remove of a mile, or through a\ndense fog, would not be among the content-determining causes of\n“X”. Under normal viewing conditions, one does not confuse\na fox with a dog, so foxes are not to be counted as part of the\ncontent of “X”. Moreover, if one does confuse a fox with a\ndog under normal viewing conditions, then perhaps one does not really\nhave a mental representation of a dog, but maybe only a mental\nrepresentation of a member of the taxonomic family canidae. \nAlthough an appeal to normal conditions initially appears promising,\nit does not seem to be sufficient to rule out the causal\nintermediaries between objects in the environment and “X”.\nEven under normal conditions of viewing that include good lighting, a\nparticular perspective, a particular viewing distance, a lack of\n(seriously) occluding objects, and so forth, it is still the case that\nboth dogs and, say, retinal projections of dogs, lead to tokens of\n“X”. Why does the content of “X” not include\nretinal projections of dogs or any of the other causal intermediaries?\nNor do normal conditions suffice to keep questions from getting in\namong the content-determining causes. What abnormal conditions are\nthere when the question, “What kind of animal is named\n‘Fido’?,” leads to a tokening of an “X”\nwith the putative meaning of dog? Suppose there are instances of\nquantum mechanical fluctuations in the nervous system, wherein\nspontaneous changes in neurons lead to tokens of “X”. Do\nnormal conditions block these out? So, there are problem cases in\nwhich appeals to normal conditions do not seem to work. Fodor (1990b)\ndiscusses this problem with proximal stimulations in connection with\nhis asymmetric dependency theory, but it is one that clearly\nchallenges the causal theory plus normal conditions approach. \nNext, suppose that we tightly construe normal conditions to eliminate\nthe kinds of problem cases described above. So, when completely\nfleshed out, under normal conditions only dogs cause “X”s.\nWhat one intuitively wants is to be able to say that, under normal\nconditions of good lighting, proper viewing distance, etc.\n“X” means dog. But, another possibility is that in such a\nsituation “X” does not mean dog, but\ndog-under-normal-conditions-of-good-lighting, proper-viewing-distance,\netc. Why take one interpretation over another? One needs a principled\nbasis for distinguishing the cause of “X” from\nthe many causally contributing factors. In other words, we still have\nthe problem of bracketing non-content-determining causes, only in a\nslightly reformulated manner. This sort of objection may be found in\nFodor (1984). \nNow set the preceding problem aside. There is still another developed\nin Fodor (1984). Suppose that “X” does mean dog under\nconditions of good lighting, lack of serious occlusions, etc. Do not\nmerely suppose that “X” is caused by dogs under conditions\nof good light, lack of serious occlusions, etc.; grant that\n“X” really does mean dog under these conditions. Even\nthen, why does “X”, the firing of the neuronal circuit,\nstill mean dog, when those conditions do not hold? Why does\n“X” still mean dog under, say, degraded lighting\nconditions? After all, we could abide by another apparently true\nconditional regarding these other conditions, namely, if the lighting\nconditions were not so good, there were no serious occlusions, etc.,\nthen the neuronal circuit’s firing would mean dog or fox. Even\nif “X” means X under one set of conditions C1,\nwhy doesn’t “X” mean Y under a different set of\nconditions C2? It looks as though one could say that\nC1 provides normal conditions under which “X”\nmeans X and C2 provides normal conditions under which\n“X” means Y. We need some non-semantic notions to enable\nus to fix on one interpretation, rather than the other. At this point,\none might look to a notion of functions to solve these\n problems.[7] \nMany physical objects have functions. (Stampe (1977) was the first to\nnote this as a fact that might help causal theories of content.) A\nfamiliar mercury thermometer has the function of indicating\ntemperature. But, such a thermometer works against a set of background\nconditions which include the atmospheric pressure. The atmospheric\npressure influences the volume of the vacuum that forms above the\ncolumn of mercury in the glass tube. So, the height of the column of\nmercury is the product of two causally relevant features, the ambient\natmospheric temperature and the ambient atmospheric pressure. This\nsuggests that one and the same physical device with the same causal\ndependencies can be used in different ways. A column of mercury in a\nglass tube can be used to measure temperature, but it is possible to\nput it to use as a pressure gauge. Which thing a column of mercury\nmeasures is determined by its function. \nThis observation suggests a way to specify which causes of\n“X” determine its content. The content of “X”,\nsay, the firing of some neurons, is determined by dogs, and not foxes,\nbecause it is the function of those neurons to register the presence\nof dogs, but not foxes. Further, the content of “X” does\nnot include LSD, microelectrodes, or quantum mechanical fluctuations,\nbecause it is not the function of “X” to fire in response\nto LSD, microelectrodes, or quantum mechanical fluctuations in the\nbrain. Similarly, the content of “X” does not include\nproximal sensory projections of dogs, because the function of the\nneurons is to register the presence of the dogs, not the sensory\nstimulations. It is the objective features of the world that matter to\nan organism, not its sensory states. Finally, it is the function of\n“X” to register the presence of dogs, but not the presence\nof questions, such as ‘What kind of animal is named “Fido”?’,\nthat\nleads to “X” meaning dogs. Functions, thus, provide a\nprima facie attractive means of properly winnowing down the\ncauses of “X” to those that are genuinely content\ndetermining. \nIn addition, the theory of evolution by natural selection apparently\nprovides a non-semantic, non-intentional basis upon which to explicate\nfunctions and, in turn, semantic content. Individual organisms vary in\ntheir characteristics, such as how their neurons respond to features\nof the environment. Some of these differences in how neurons respond\nmake a difference to an organism’s survival and reproduction.\nFinally, some of these very differences may be heritable. Natural\nselection, commonly understood as this differential reproduction of\nheritable variation, is purely causal. Suppose that there is a\npopulation of rabbits. Further suppose that either by a genetic\nmutation or by the recombination of existing genes, some of these\nrabbits develop neurons that are wired into their visual systems in\nsuch a way that they fire (more or less reliably) in the presence of\ndogs. Further, the firing of these neurons is wired into a freezing\nbehavior in these rabbits. Because of this configuration, the rabbits\nwith the “dog neurons” are less likely to be detected by\ndogs, hence more likely to survive and reproduce. Finally, because the\ngenes for these neurons are heritable, the offspring of these\ndog-sensitive rabbits will themselves be dog-sensitive. Over time, the\nnumber of the dog-sensitive rabbits will increase, thereby displacing\nthe dog-insensitive rabbits. So, natural selection will, in such a\nscenario, give rise to mental representations of dogs. Insofar as such\na story is plausible, there is hope that natural selection and the\ngenesis of functions can provide a naturalistically acceptable means\nof delimiting content-determining causes. \nThere is no doubt that individual variation, differential\nreproduction, and inheritance can be understood in a purely causal\nmanner. Yet, there remains skepticism about how naturalistically one\ncan describe what natural selection can select for. There are doubts\nabout the extent to which the objects of selection really can be\nspecified without illicit importation of intentional notions. Fodor\n(1989, 1990a) give voice to some of this skepticism. Prima\nfacie, it makes sense to say that the neurons in our hypothetical\nrabbits fire in response to the presence of dogs, hence that there is\nselection for dog representations. But, it makes just as much sense,\none might worry, to say that it is sensitivity to dog-look-alikes that\nleads to the greater fitness of the rabbits with the new\n neurons.[8]\n There are genes for the dog-look-alike neurons and these genes are\nheritable. Moreover, those rabbits that freeze in response to\ndog-look-alikes are more likely to survive and reproduce than are\nthose that do not so freeze, hence one might say that the freezing is\nin response to dog-look-alikes. So, our ability to say that the\nmeaning of the rabbits’ mental representation “X” is\ndog, rather than dog-look-alike, depends on our ability to say that it\nis the dog-sensitivity of “X”, rather than the\ndog-look-alike-sensitivity of “X”, that keeps the rabbits\nalive longer. Of course, being dog-sensitive and being\ndog-look-alike-sensitive are connected, but the problem here is that\nboth being dog-look-alike-sensitive and being dog-sensitive can\nincrease fitness in ways that lead to the fixation of a genotype. And\nit can well be that it is avoidance of dogs that keeps a rabbit alive,\nbut one still needs some principled basis for saying that the rabbits\navoid dogs by being sensitive to dogs, rather than by being sensitive\nto dog-look-alikes. The latter appears to be good enough for the\ndifferential reproduction of heritable variation to do its work. Where\nwe risk importing semantic notions into the mix is in understanding\nselection intentionally, rather than purely causally. We need a notion\nof “selection for” that is both general enough to work for\nall the mental contents causal theorists aspire to address and that\ndoes not tacitly import semantic notions. \nIn response to this sort of objection, it has been proposed that the\ncorrect explanation of a rabbit’s evolutionary success with,\nsay, “X”, is not that this enables the rabbit to avoid\ndog-look-alikes, but that it enables them to avoid dogs. It is dogs,\nbut not mere dog-look-alikes, that prey on rabbits. (This sort of\nresponse is developed in Millikan (1991) and Neander (1995).) Yet, the\nrejoinder is that if we really want to get at the correct explanation\nof a rabbit-cum-“X” system, then we should not suppose\nthat “X” means dog. Instead, we should say that it is in\nvirtue of the fact that “X” picks up on something like,\nsay, predator of such and such characteristics that the\n“X” alarm system increases the chance of a rabbit’s\nsurvival. (This sort of rejoinder may be found in Agar (1993).) \nThis problem aside, there is also some concern about the extent to\nwhich it is plausible to suppose that natural selection could act on\nthe fine details of the operation of the brain, such as the firing of\nneurons in the presence of dogs. (This is an objection raised in Fodor\n(1990c)). Natural selection might operate to increase the size of the\nbrain so there is more cortical mass for cognitive processing. Natural\nselection might also operate to increase the folding of the brain so\nas to maximize the cortical surface area that can be contained within\nthe brain. Natural selection might also lead to compartmentalization\nof the brain, so that one particular region could be dedicated to\nvisual processing, another to auditory processing, and still another\nto face processing. Yet, many would take it to be implausible to\nsuppose that natural selection works at the level of individual mental\nrepresentations. The brain is too plastic and there is too much\nindividual variation in the brains of mammals to admit of selection\nacting in this way. Moreover, such far reaching effects of natural\nselection would lead to innate ideas not merely of colors and shapes,\nbut of dogs, cats, cars, skyscrapers, and movie stars. Rather than\nsupposing that functions are determined by natural selection across\nmultiple generations, many philosophers contend that it is more\nplausible that the functions that underlie mental representations are\nacquired through cognitive development. \nHypothesizing that certain activities or events within the brain mean\nwhat they do, in part, because of some function that develops over the\ncourse of an individual’s lifetime shares many of the attractive\nfeatures of the hypothesis that these same activities or events mean\nwhat they do, in part, because of some evolutionarily acquired\nfunction. One again can say that it is not the function of\n“X” to register the presence of LSD, microelectrodes,\nfoxes, stuffed dogs, or paper mâché dogs, or questions,\nbut it is their function to report on dogs. Moreover, it does not\ninvoke dubious suppositions about an intimate connection between\nnatural selection and the precise details of neuronal hardware and its\noperation. A functional account based on ontogenetic function\nacquisition or learning seems to be an improvement. This is the core\nof the approach taken in Dretske (1981; 1988). \nThe function acquisition story proposes that during development, an\norganism is trained to discriminate real flesh and blood dogs from\nquestions, foxes, stuffed dogs, paper mâché dogs under\nconditions of good lighting, without occlusions, or distractions. A\nteacher ensures that training proceeds according to plan. Once\n“X” has acquired the function to respond to dogs, the\ntraining is over. Thereafter, any instances in which “X”\nis triggered by foxes, stuffed dogs, paper mâché dogs,\nLSD, microelectrodes, etc., are false tokenings and figure into false\nbeliefs. \nAmong the most familiar objections to this proposal is that there is\nno principled distinction between when a creature is learning and when\nit is done learning. Instances in which a creature entertains the\nhypothesis that “X” means X, instances in which the\ncreature entertains the hypothesis that “X” means Y,\ninstances in which the creature straightforwardly uses “X”\nto mean X, and instances in which the creature straightforwardly uses\n“X” to mean Y are thoroughly intermingled. The problem is\nperhaps more clearly illustrated with tokens of natural language,\nwhere children will use words struggling through correct and incorrect\nuses of a word before (perhaps) finally settling on a correct usage.\nThere seems to be no principled way to specify if learning has stopped\nor whether there is instead “lifelong learning”. This is\namong the objections to be found in Fodor (1984). \nThis, however, is a relatively technical objection. Further reflection\nsuggests that there may be an underlying appeal to the intentions of\nthe teacher. Let us revisit the learning story. Suppose that during\nthe learning period the subject is trained to use “X” as a\nmental representation of dogs. Now, let the student graduate from\n“X” using school and immediately thereafter see a fox.\nSeeing this fox causes a token of “X” and one would like\nto say that this is an instance of mistaking a fox for a dog, hence a\nfalse tokening. But, consider the situation counterfactually. If the\nstudent had seen the fox during the training period just before\ngraduation, the fox would have triggered a token of “X”.\nThis suggests that we might just as well say that the student learned\nthat “X” means fox or dog as that the student learned that\n“X” means dog. Thus, we might just as well say that, after\ntraining, the graduate does not falsely think of a dog, but truly\nthinks of a fox or a dog. The threat of running afoul of naturalist\nscruples comes if one attempts to say, in one way or another, that it\nis because the teacher meant for the student to learn that\n“X” means dog, rather than “X” means fox or\ndog. The threatened violation of naturalism comes in invoking the\nteacher’s intentions. This, too, is an objection to be found in\nFodor (1984). \nThe preceding attempts to distinguish the content-determining causes\nfrom non-content-determining causes focused on the background or\nboundary conditions under which the distinct types of causes may be\nthought to act. Fodor’s Asymmetric Dependency Theory (ADT),\nhowever, represents a bold alternative to these approaches. Although\nFodor (1987, 1990a, b, 1994) contain numerous variations on the\ndetails of the theory, the core idea is that the content-determining\ncause is in an important sense fundamental, where the\nnon-content-determining causes are non-fundamental. The sense of being\nfundamental is that the non-content-determining causes depend on the\ncontent-determining cause; the non-content-determining causes would\nnot exist if not for the content-determining cause. Put a bit more\ntechnically, there are numerous laws such as ‘Y1\ncauses “X”,’ ‘Y2 causes\n“X”,’ etc., but none of these laws would exist were\nit not a law that X causes “X”. The fact that the ‘X\ncauses “X”’ law does not in the same way depend on\nany of the Y1, Y2, …, Yn laws\nmakes the dependence asymmetric. Hence, there is an asymmetric\ndependency between the laws. The intuition here is that the question,\n‘What kind of animal is called “Fido”?’ will\ncause an occurrence of the representation “X” only because\nof the fact that dogs cause “X”. Instances of foxes cause\ninstances of “X” only because foxes are mistaken for dogs\nand dogs cause instances of “X”. \nCausation is typically understood to have a temporal dimension. First\nthere is event C and this event C subsequently leads to event E. Thus,\nwhen the ADT is sometimes referred to as the “Asymmetric Causal\nDependency Theory,” the term “causal” might suggest\na diachronic picture in which there is, first, an X-“X”\nlaw which subsequently gives rise to the various Y-“X”\nlaws. Such a diachronic interpretation, however, would lead to\ncounterexamples for the ADT approach. Fodor (1987) discusses this\npossibility. Consider Pavlovian conditioning. Food causes salivation\nin a dog. Then a bell causes salivation in the dog. It is likely that\nthe bell causes salivation only because the food causes it. Yet,\nsalivation hardly means food. It may well naturally mean that food is\npresent, but salivation is not a thought or thought content and it is\nnot ripe for false semantic tokening. Or take a more exotic kind of\ncase. Suppose that one comes to apply “X” to dogs, but\nonly by means of observations of foxes. This would be a weird case of\n“learning”, but if things were to go this way, one would\nnot want “X” to mean fox. To block this kind of objection,\nthe theory maintains the dependency between the fundamental\nX-“X” law and the non-fundamental Y-“X” laws\nis synchronic. The dependency is such that if one were to break the\nX-“X” law at time t, then one would thereby\ninstantaneously break all the Y-“X” laws at that time. \nThe core of ADT, therefore, comes down to this. “X” means\nX if \nThis seems to get a number of cases right. The reason that questions\nlike “What kind of animal is named ‘Fido’?” or\n“What is a Sheltie?” trigger “X”, meaning dog,\nis that dogs are able to trigger “X”s. Foxes only trigger\n“X”s, meaning dog, because dogs are able to trigger them.\nMoreover, it appears to solve the disjunction problem. Suppose we have\na ‘dogs cause “X”s’ law and a ‘dogs or\nfoxes cause “X”s’ law. If one breaks the ‘dogs\ncause “X”s’ law, then one thereby breaks the\n‘dogs or foxes cause “X”s’ law, since the only\nreason either dogs or foxes cause “X”s is because dogs do.\nMoreover, if one breaks the ‘dogs or foxes cause\n“X”s’ law, one does not thereby break the\n‘dogs cause “X”s’ law, since dogs alone might\nsuffice to cause “X”s. So, the ‘dogs or foxes cause\n“X”s’ law depends on the ‘dogs cause\n“X”s’ law, but not vice versa. Asymmetric dependency\nof laws gives the right\n results.[9] \nAdams and Aizawa (1994) mention an important class of causes that the\nADT does not appear to handle, namely, the “non-psychological\ninterventions”. We have all along assumed the “X” is\nsome sort of brain event, such as the firing of some neurons. But, it\nis plausible that some interventions, such as a dose of hallucinogen\nor maybe some carefully placed microelectrodes, could trigger such\nbrain events, quite apart from the connection of those brain events to\nother events in the external world. If essentially all brain events\nare so artificially inducible, then it would appear that for all\nputative mental representations, there will be some laws, such as\n‘microelectrodes cause “X”s,’ that do not\ndepend on laws such as ‘dogs causes “X”s.’ If\nthis is the case, then the second condition of the ADT would rarely or\nnever be satisfied, so that the theory would have little relevance to\nactual cognitive scientific practice. \nFodor (1990a) discusses challenges that arise with the fact that the\nperception of objects involves causal intermediaries. Suppose that\nthere is a dog-“X” law that is mediated entirely by\nsensory mechanisms. In fact, suppose unrealistically that the\ndog-“X” law is mediated by a single visual sensory\nprojection. In other words let the dog-“X” law be mediated\nby the combination of a dog-dogsp law and a\ndogsp-“X” law. Under these conditions, it\nappears that “X” means dogsp, rather than dog.\nCondition (1) is satisfied, since there is a\ndogsp-“X” law. Condition (2) is satisfied,\nsince if one were to break the dogsp-“X” law\none would thereby break the dog-“X” law (i.e., there is a\ndependence of one law one the other) and breaking the\ndog-“X” law would not necessarily break the\ndogsp-“X” law (i.e., the dependence is not\nsymmetric). The dependence is asymmetric, because one can break the\ndog-“X” law by breaking the dog-dogsp law (by\nchanging the way dogs look) without thereby breaking the\ndogsp-“X” law. Finally, condition (3) is\nsatisfied, since the dependence of the dog-“X” law on the\ndogsp-“X” law is synchronic. \nThe foregoing version of the sensory projections problem relies on\nwhat was noted to be the unrealistic assumption that the\ndog-“X” law is mediated by a single visual sensory\nprojection. Relaxing the assumption does not so much solve the problem\nas transform it. So, adopt the more realistic assumption that the\ndog-“X” law is sustained by a combination of a large set\nof dog-sensory projection laws and a large set of\ndogsp-“X” laws. In the first set, we have laws\nconnecting dogs to particular patterns of retinal stimulation, laws\nconnecting dogs to particular patterns of acoustic stimulation, etc.\nIn the second set, we have certain psychological laws connecting\nparticular patterns of retinal stimulation to “X”, certain\npsychological laws connecting particular patterns of acoustic\nstimulation to “X”, etc. In this sort of situation, there\nthreatens to be no “fundamental” law, no law on which all\nother laws asymmetrically depend. If one breaks the\ndog-“X” law one does not thereby break any of the sensory\nprojection-“X” laws, since the former can be broken by\ndissolving all of the dog-sensory projection laws. If, however, one\nbreaks any one of the particular dogsp-“X”\nlaws, e.g. one connecting a particular doggish visual appearance to\n“X”, one does not thereby break the dog-“X”\nlaw. The other sensory projections might sustain the\ndog-“X” law. Moreover, breaking the law connecting a\nparticular doggish look to “X” will not thereby break a\nlaw connecting a particular doggish sound to “X”. Without\na “fundamental” law, there is no meaning in virtue of the\nconditions of the ADT. Further, the applicability of the ADT appears\nto be dramatically reduced insofar as connections between mental\nrepresentations and properties in the world are mediated by sensory\nprojections. \nAnother problem arises with items or kinds that are indistinguishable.\nAdams and Aizawa (1994), and, implicitly, McLaughlin, (1991), among\nothers, have discussed this problem. As one example, consider the time\nat which the two minerals, jadeite and nephrite, were chemically\nindistinguishable and were both thought to be jade. As another, one\nmight appeal to H2O and XYZ (the stuff of philosophical\nthought experiments, the water look-alike substance found on\ntwin-earth). Let X = jadeite and Y = nephrite and let there be laws\n‘jadeite causes “X”’ and ‘nephrite\ncauses “X”’. Can “X” mean jadeite? No.\nCondition (1) is satisfied, since it is a law that ‘jadeite\ncauses “X”’. Condition (3) is satisfied, since\nbreaking the jadeite-“X” law will immediately break the\nnephrite-“X” law. If jadeite cannot trigger an\n“X”, then neither can nephrite, since the two are\nindistinguishable. That is, there is a synchronic dependence of the\n‘nephrite causes “X”’ law on the\n‘jadeite causes “X” law. The problem arises with\ncondition (2). Breaking the jadeite-“X” law will thereby\nbreak the nephrite-“X” law, but breaking the\nnephrite-“X” law will also thereby break the\njadeite-“X” law. Condition (2) cannot be satisfied, since\nthere is a symmetric dependence between the jadeite-“X”\nlaw and the nephrite-“X” law. By parity of reasoning,\n“X” cannot mean nephrite. So, can “X” mean\njade? No. As before, conditions (1) and (3) could be satisfied, since\nthere could be a jade-“X” law and the\njadeite-“X” law and the nephrite-“X” law could\nsynchronically depend on it. The problem is, again, with condition\n(2). Presumably breaking the jade-“X” law would break the\njadeite-“X” and nephrite-“X” law, but breaking\neither of them would break the jade-“X” law. The problem\nis, again, with symmetric dependencies. \nHere is a problem that we earlier found in conjunction with other\ncausal theories. Despite the bold new idea underlying the ADT method\nof partitioning off non-content-determining causes, it too appears to\nsneak in naturalistically unacceptable assumptions. Like all causal\ntheories of mental content, the asymmetric causal dependencies are\nsupposed to be the basis upon which meaning is created; the\ndependencies are not themselves supposed to be a product, or\nbyproduct, of meaning. Yet, ADT appears to violate this naturalistic\npre-condition for causal theories. (This kind of objection may be\nfound in Seager (1993), Adams & Aizawa (1994), (1994b), Wallis\n(1995), and Gibson (1996)). Ys are supposed to cause “X”s\nonly because Xs do and this must not be because of any semantic facts\nabout “X”s. So, what sort of mechanism would bring about\nsuch asymmetric dependencies among things connected to the syntactic\nitem “X”? In fact, why wouldn’t lots of things be\nable to cause “X”s besides Xs, quite independently of the\nfact that Xs do? The instantiation of “X”s in the brain\nis, say, some set of neurochemical events. There should be natural\ncauses capable of producing such events in one’s brain under a\nvariety of circumstances. Why on earth would foxes be able to cause\nthe neurochemical “X” events in us only because dogs can?\nOne might be tempted to observe that “X” means dog,\n“Y” means fox, we associate foxes with dogs and that is\nwhy foxes cause “X”s only because dogs cause\n“X”s. We would not associate foxes with “X”s\nunless we associated “X”s with dogs and foxes with dogs.\nThis answer, however, involves deriving the asymmetric causal\ndependencies from meanings, which violates the background assumption\nof the naturalization project. Unless there is a better explanation of\nsuch asymmetrical dependencies, it may well be that the theory is\nmisguided in attempting to rest meaning upon them. \nA relatively more recent causal theory is Robert Rupert’s (1999)\nBest Test Theory (BTT) for the meanings of natural kind terms. Unlike\nmost causal theories, this one is restricted in scope to just natural\nkinds and terms for natural kinds. To mark this restriction, we will\nlet represented kinds be denoted by K’s, rather than our usual\nX’s. \nBest Test Theory: If a subject S bears no\nextension-fixing intentions toward “X” and “X”\nis an atomic natural kind term in S’s language of thought (i.e.,\nnot a compound of two or more other natural kind terms), then\n“X” has as its extension the members of natural kind K if\nand only if members of K are more efficient in their causing of\n“X” in S than are the members of any other natural\nkind. \nTo put the idea succinctly, “X” means, or refers to, those\nthings that are the most powerful stimulants of “X”. That\nsaid, we need an account of what it is for a member of a natural kind\nto be more efficient in causing “X”s than are other\nnatural kinds. We need an account of how to measure the power of a\nstimulus. This might be explained in terms of a kind of biography. Figure 1. A\nspreadsheet biography \nConsider an organism S that (a) causally interacts with three\ndifferent natural kinds, K1-K3, in its\nenvironment and (b) has a language of thought with five terms\n“X1”-“X5”. Further,\nsuppose that each time S interacts with an individual of kind\nKi this causes an occurrence of one or more of\n“X1”-“X5”. We can then\ncreate a kind of “spreadsheet biography” or “log of\nmental activity” for S in which there is a column for each of\n“X1”-“X5” and a row for\neach instance in which a member of K1-K3 causes\none or more instances of\n“X1”-“X5”. Each mental\nrepresentation “Xi” that Ki triggers\nreceives a “1” in its column. Thus, a single spreadsheet\nbiography might look like that shown in Figure 1. \nTo determine what a given term “Xi” means, we\nfind the kind Ki that is most effective at causing\n“Xi”. This can be computed from S’s\nbiography. For each Ki and “Xi”, we\ncompute the frequency with which Ki triggers\n“Xi”. “X1” is tokened\nfour out of six times that K1 is encountered, three out of\nthree times that K2 is encountered, and one out of four\ntimes that K3 is encountered. “Xi”\nmeans that Ki that has the highest sample frequency. Thus,\nin this case, “X1” means K2. Just to\nbe clear, when BTT claims that “Xi” means the\nKi that is the most powerful stimulant of “X”,\nthis is not to say that “X” means the most common\nstimulant of “X”. In our spreadsheet biography,\nK1 is the most common stimulant of\n“X1”, since it triggers\n“X1” four times, where K2 triggers\nit only three times, and K3 triggers it only one time. This\nis why, according to BTT, “X1” means\nK2, rather than K1 or K3. \nHow does the BTT handle our range of test cases? Consider, first, the\nstandard form of the disjunction problem, the case of “X”\nmeaning dog, rather than dog or fox-on-a-dark-night-at-a-distance.\nSince the latter is not apparently a natural kind, “X”\ncannot mean\n that.[10]\n Moreover, “X” means dog, rather than fox, because the\nonly times the many foxes that S encounters can trigger\n“X1”s is on dark nights at a distance, where\ndogs trigger “X”s more consistently under a wider range of\nconditions. \nHow does the BTT address the apparent problem of “brain\ninterventions,” such as LSD, microelectrodes, or brain tumors?\nThe answer is multi-faceted. The quickest method for taking much of\nthe sting out of these cases is to note that they generally do not\narise for most individuals. The Best Test Theory relies on personal\nbiographies in which only actual instances of kinds triggering mental\nrepresentations are used to specify causal efficiency. The\ncounterfactual truth that, were a stimulating microelectrode to be\napplied to, say, a particular neuron, it would perfectly reliably\nproduce an “X” token simply does not matter for the\ntheory. So, for all those individuals who do not take LSD, do not have\nmicroelectrodes inserted in their brains, do not have brain tumors,\netc., these sorts of counterfactual possibilities are irrelevant. A\nsecond line of defense against “brain interventions”\nappeals to the limitation to natural kinds. The BTT might set aside\nmicroelectrodes, since they do not constitute a natural kind. Maybe\nbrain tumors are; maybe not. Unfortunately, however, LSD is a very\nstrong candidate for a chemical natural kind. Still the BTT is not\nwithout a third line of defense for handling these cases. One might\nsuppose that LSD and brain tumors act on the brain in a rather diffuse\nmanner. Sometimes a dose of LSD triggers “Xi”,\nanother time it triggers “Xj”, and another time\nit triggers “Xk”. One might then propose that,\nif one counts all these episodes with LSD, none of these will act\noften enough on, say, “Xi” to get it to mean\nLSD, rather than, say, dog. This is the sort of strategy that Rupert\ninvokes to keep mental symbols from meaning omnipresent, but\nnon-specific causes such as the heart. The heart might causally\ncontribute to “X1”, but it also contributes to\nso many other “Xi”s, that the heart will turn\nout not to be the most efficient cause of\n“X1”. \nWhat about questions? Presumably questions as a category will count as\nan instance of a linguistic natural kind. Moreover, particular\nsentences will also count. So, the restriction of the BTT to natural\nkinds is of little use here. So, what of causal efficiency? Many\nsentences appear to provoke a wide range of possible responses. In\nresponse to, “I went to the zoo last week,” S could think\nof lions, tigers, bear, giraffes, monkeys, and any number of other\nnatural kinds. But, the question, “What animal goes ‘oink,\noink’?”—perhaps uttered in “Motherese”\nin a clear deliberate fashion so that it is readily comprehensible to\na child—will be rather efficient in generating thoughts of a\npig. Moreover, it could be more efficient than actual pigs, since a\nchild might have more experience with the question than with actual\npigs, often not figuring out that actual pigs are pigs. In such\nsituations, “pig” would turn out to mean “What\nanimal goes ‘oink, oink’?,” rather than pig. So,\nthere appear to be cases in which BTT could make prima facie\nincorrect content assignments. \nWhat, finally, of proximal projections of natural kinds? One plausible\nline might be to maintain that proximal projections of natural kinds\nare not themselves natural kinds, hence that they are automatically\nexcluded from the scope of the theory. This plausible line, however,\nmight be the only available line. Presumably, in the course of\nS’s life, the only way dogs can cause “X”s is by way\nof causal mediators between the dogs and the “X”s. Thus,\neach episode in which a dog causes an “X” is also an\nepisode in which a sensory projection of a dog causes an\n“X”. So, dog efficiency for “X” can be no\nhigher the efficiency of dog sensory projections. And, if it is\npossible for there to be a sensory projection of a dog without there\nbeing an actual dog, then the efficiency of the projections would be\ngreater than the efficiency of the dogs. So, “X” could not\nmean dog. But, this problem is not necessarily damaging to BTT. \nSince the BTT has not received a critical response in the literature,\nwe will not devote a section to objections to it. Instead, we will\nleave well enough alone with our somewhat speculative treatment of how\nBTT might handle our familiar test cases. The general upshot is that\nthe combination of actual causal efficiency over the course of an\nindividual’s lifetime along with the restriction to natural\nkinds provides a surprisingly rich means of addressing some\nlong-standing problems. \nIn the preceding section, we surveyed issues that face the philosopher\nattempting to work out the details of a causal theory of mental\ncontent. These issues are, therefore, one might say, internal to\ncausal theories. In this section, however, we shall review some of the\nobjections that have been brought forward to the very idea of a causal\ntheory of mental content. As such, these objections might be construed\nas external to the project of developing a causal theory of mental\ncontent. Some of these are coeval with causal theories and have been\naddressed in the literature, but some are relatively recent and have\nnot been discussed in the literature. The first objections, discussed\nin subsections 4.1–4.4, in one way or another push against the\nidea that all content could be explained by appeal to a causal theory,\nbut leave open the possibility that one or another causal theory might\nprovide sufficiency conditions for meaning. The last objections, those\ndiscussed in subsections 4.5–4.6 challenge the ability of causal\ntheories to provide even sufficiency conditions for mental\ncontent. \nOne might think that the meanings of terms that denote mathematical or\nlogical relations could not be handled by a causal theory. How could a\nmental version of the symbol “+” be causally connected to\nthe addition function? How could a mental version of the logical\nsymbol “¬” be causally connected to the negation truth\nfunction? The addition function and the negation function are abstract\nobjects. To avoid this problem, causal theories typically acquiesce\nand maintain that their conditions are merely sufficient conditions on\nmeaning. If an object meets the conditions, then that object bears\nmeaning. But, the conditions are not necessary for meaning, so that\nrepresentations of abstract objects get their meaning in some other\nway. Perhaps conceptual role semantics, wherein the meanings of terms\nare defined in terms of the meanings of other terms, could be made to\nwork for these other theories. \nAnother class of potential problem cases are vacuous terms. So, for\nexample, people can think about unicorns, fountains of youth, or the\nplanet Vulcan. Cases such as these are discussed in Stampe (1977) and\nFodor (1990a), among other places. These things would be physical\nobjects were they to exist, but they do not, so one cannot causally\ninteract with them. In principle, one could say that thoughts about\nsuch things are not counterexamples to causal theories, since causal\ntheories are meant only to offer sufficiency conditions for meaning.\nBut, this in principle reply appears to be ad hoc. It is not\nwarranted, for example, by the fact that these excluded meanings\ninvolve abstract objects. There are, however, a number of options that\nmight be explored here. \nOne strategy would be to turn to the basic ontology of one’s\ncausal theory of mental content. This is where a theory based on\nnomological relations might be superior to a version that is based on\ncausal relations between individuals. One might say that there can be\na unicorn-“unicorn” law, even if there are no actual\nunicorns. This story, however, would break down for mental\nrepresentations of individuals, such as the putative planet Vulcan.\nThere is no law that connects a mental representation to an\nindividual; laws are relations among properties. \nAnother strategy would be to propose that some thought symbols are\ncomplex and can decompose into meaningful primitive constituents. One\ncould then allow that “X” is a kind of abbreviation for,\nor logical construction of, or defined in terms of “Y1,”\n“Y2,” and “Y3,” and that a causal theory\napplies to “Y1,” “Y2,” and “Y3.”\nSo, for example, one might have a thought of a unicorn, but rather\nthan having a single unicorn mental representation there is another\nrepresentation made up of a representation of a horse, a\nrepresentation of a horn, and a representation of the relationship\nbetween the horse and the horn. “Horse”,\n“horn”, and “possession,” may then have\ninstantiated properties as their contents. \nHorgan and Tienson (2002) object to what they describe as\n“strong externalist theories” that maintain that causal\nconnections are necessary for content. They argue, first, that mental\nlife involves a lot of intentional content that is constituted by\nphenomenology alone. Perceptual states, such as seeing a red apple,\nare intentional. They are about apples. Believing that there are more\nthan 10 Mersenne primes and hoping to discover a new Mersenne prime\nare also intentional states, in this case about Mersenne primes. But,\nall these intentional states have a phenomenology—something it\nis like to be in these states. There is something it is like to see a\nred apple, something different that it is like to believe there are\nmore than 10 Mersenne primes, and something different still that it is\nlike to hope to discover a new Mersenne prime. Horgan and Tienson\npropose that there can be phenomenological duplicates—two\nindividuals with exactly the same phenomenology. Assume nothing about\nthese duplicates other than that they are phenomenological duplicates.\nIn such a situation, one can be neutral regarding how much of their\nphenomenological experience is veridical and how much illusory. So,\none can be neutral on whether or not a duplicate sees a red apple or\nwhether there really are more than 10 Mersenne primes. This suggests\nthat there is a kind of intentionality—that shared by the\nduplicates—that is purely phenomenological. Second, Horgan and\nTienson argue that phenomenology constitutively depends only on narrow\nfactors. They observe that one’s experiences are often caused or\ntriggered by events in the environment, but that these environmental\ncauses are only parts of causal chains that lead to the phenomenology\nitself. They do not constitute that phenomenology. The states that\nconstitute, or provide the supervenience base for, the phenomenology\nare not the elements of the causal chain leading back into the\nenvironment. If we combine the conclusions of these two arguments, we\nget Horgan and Tienson’s principal argument against any causal\ntheory that would maintain that causal connections are necessary for\ncontent. \nTherefore, \nThus, versions of causal theories that suppose that all content must\nbe based on causal connections are fundamentally mistaken. For those\nversions of causal theories that offer only sufficiency conditions on\nsemantic content, however, Horgan and Tienson’s argument may be\ntaken to provide a specific limitation on the scope of causal\ntheories, namely, that causal theories do not work for intentional\ncontent that is constituted by phenomenology alone. \nA relatively familiar challenge to this argument may be found in\ncertain representational theories of phenomenological properties.\n(See, for example, Dretske (1988) and Tye (1997).) According to these\nviews, the phenomenology of a mental state derives from that\nstate’s representational properties, but the representational\nproperties are determined by external factors, such as the environment\nin which an organism finds itself. Thus, such representationalist\ntheories challenge premise P2 of Horgan and Tienson’s\nargument. \nBuras (2009) presents another argument that is perhaps best thought of\nas providing a novel reason to think that causal theories of mental\nrepresentation only offer sufficiency conditions on meaning. This\nargument begins with the premise that some mental states are about\nthemselves. To motivate this claim, Buras notes that some sentences\nare about themselves. So, by analogy with, “This sentence is\nfalse,” which is about itself, one might think that there is a\nthought, “This thought is false,” that is also about\nitself. Or, how about “This thought is realized in brain\ntissue” or “This thought was caused by LSD”? These\nappear to be about themselves. Buras’ second premise is that\nnothing is a cause of itself. So, “This thought is false”\nis about itself, but could not be caused by itself. So, the sentence\n“This thought is false” could not mean that it itself is\nfalse in virtue of the fact that “This thought is false”\nwas caused by its being false. So, “This thought is false”\nmust get its meaning in some other way. It must get its meaning in\nvirtue of some other conditions of meaning acquisition. \nThis is not, however, exactly the way Buras develops his argument. In\nthe first place, he treats causal theories of mental content as\nmaintaining that, if “X” means X, then X causes\n“X”. (Cf. Buras, 2009, p. 118). He cites Stampe (1977),\nDretske (1988), and Fodor (1987) as maintaining this. Yet, Stampe,\nDretske, and Fodor explicitly formulate their theories in terms of\nsufficiency conditions, so that (roughly) “X” means X, if\nXs causes “X”s, etc. (See, for example, Stampe (1977), pp.\n82–3, Dretske (1988), p. 52, and Fodor (187), p. 100). In the\nsecond place, Buras seems to draw a conclusion that is orthogonal to\nthe truth or falsity of causal theories of mental content. He begins\nhis paper with an impressively succinct statement of his argument. \nSome mental states are about themselves. Nothing is a cause of itself.\nSo some mental states are not about their causes; they are about\nthings distinct from their causes (Buras, 2009, p. 117). \nThe causal theorist can admit that some mental states are not about\ntheir causes, since some states are thoughts and thoughts mean what\nthey do in virtue of, say, the meanings of mental sentences. These\nmental sentences might mean what they do in virtue of the meanings of\nprimitive mental representations (which may or may not mean what they\ndo in virtue of a causal theory of meaning) and the way in which those\nprimitive mental representations are put together. As was mentioned in\nsection 2, such a syntactically and semantically combinatorial\nlanguage of thought is a familiar background assumption for causal\ntheories. The conclusion that Buras may want, instead, is that there\nare some thoughts that do not mean what they do in virtue of what\ncauses them. So, through some slight amendments, one can understand\nBuras to be presenting a clarification of the scope of causal theories\nof mental content or as a challenge to a particularly strong version\nof causal theories, a version that takes them as offering a necessary\ncondition on meaning. \nAs noted above, one of the central challenges for causal theories of\nmental content has been to discriminate between a “core”\ncontent-determining causal connection, as between cows and “cow”s,\nand “peripheral” non-content-determining causal connections, as\nbetween horses and “cow”s. Cases of reliable misrepresentations\nare representations which always misrepresent in the same way. In such\ncases, there is supposed to be no “core” content-determining\ncausal connection; there are no X’s to which “X”s are causally\nconnected. Instead, there are only “peripheral” causal\nconnections. Mendelovici, (2013), following a discussion by Hohman,\n(2002), suggests that color representations may be like\n this.[11]\n Color anti-realism, according to which there are no colors in the\nworld, seems to be committed to the view that color representations\nare not caused by colors in the world. Color representations may be\nreliably tokened by something in the world, but not by colors that are\nin the world. \nIn some instances, reliable misrepresentations provide another take on\nsome of the familiar content-determination problems. So, take attempts\nto use normal conditions to distinguish between content-determining\ncauses and non-content-determining causes. Even in normal conditions,\ncolor representations are not caused by colors, but by, say, surface\nreflectances under certain conditions of illumination, just in the way\nthat, even in normal conditions cow representations are sometimes not\ncaused by cows, but by, say, a question such as, “What kind of\nanimal is sometimes named ‘Bessie’?” Take a version of the\nasymmetric dependency theory. On this theory applied to color terms,\nit might seem that there is no red-to-“red” law on which all the\nother laws depends in much the same way that it might seem there is no\nunicorn-to-“unicorn” law on which all other laws depends. (Cf.\nFodor (1987, pp. 163–4) and (1990, pp. 100–1)). \nUnlike the more familiar cases, Mendelovici, (2013), does not argue\nthat there actually are such problematic cases. The argument is not\nthat there are actual cases of reliable misrepresentations, but merely\nthat reliable misrepresentations are possible and that this is enough\nto create trouble for causal theories of mental representation. One\nsort of trouble stems from the need for a pattern of psychological\nexplanation. Let a mental representation “X” mean\nintrinsically-heavy. Such a representation is a misrepresentation,\nsince there is no such property of being intrinsically heavy. Such a\nmisrepresentation is, nonetheless, reliable (i.e. consistent), since\nit is consistently tokened by all the same sort of things on earth.\nBut, one can see how an agent using “X” could make a reasonable,\nyet mistaken, inference to the conclusion that an object that causes a\ntokening of “X” on earth would be hard to lift on the moon. To\nallow such a pattern of explanation, Mendelovici argues, a causal\ntheorist must allow for reliable misrepresentation. A theory of what\nmental representations are should not preclude such patterns of\nexplanation. Another sort of trouble stems from the idea that if a\ntheory of meaning does not allow for reliable misrepresentation, but\nrequires that there be a connection between “X”s and Xs, then this\nwould be constitute a commitment to a realist metaphysics for Xs.\nWhile there can be good reasons for realism, the needs of a theory of\ncontent would not seem to be a proper source for them.  \nArtiga, 2013, provides a defense of teleosemantic theories in the face\nof Mendelovici’s examples of reliable misrepresentation. Some of\nArtiga’s arguments might also be used by advocates of causal\ntheories of mental content. Mendelovici, (2016), replies to Artiga,\n2013, by providing refinements and a further defense of the view that\nreliable misrepresentations are a problem for causal theories of\nmental content. \nCummins (1997) argues that causal theories of mental content are\nincompatible with the fact that one’s perception of objects in\nthe physical environment is typically mediated by a theory. His\nargument proceeds in two stages. In one stage, he argues that, on a\ncausal theory, for each primitive “X” there must be some\nbit of machinery or mechanism that is responsible for detecting Xs.\nBut, since a finite device, such as the human brain, contains only a\nfinite amount of material, it can only generate a finite number of\nprimitive representations. Next, he observes that thought is\nproductive—that it can, in principle, generate an unbounded\nnumber of semantically distinct representations. This means that to\ngenerate the stock of mental representations corresponding to each of\nthese distinct thoughts, one must have a syntactically and\nsemantically combinatorial system of mental representation of the sort\nfound in a language of thought (LOT). More explicitly, this scheme of\nmental representation must have the following properties: \nThe conclusion of this first stage is, therefore, that a causal theory\nof mental representation requires a LOT. In the other stage of his\nargument, Cummins observes that, for a wide range of objects, their\nperception is mediated by a body of theory. Thus, to perceive\ndogs—for dogs to cause “dogs”—one has to know\nthings such as that dogs have tails, dogs have fur, and dogs four\nlegs. But, to know that dogs have tails, fur, and four legs, one needs\na set of mental representations, such as “tail”,\n“fur”, “four”, and “legs”. Now the\nproblem fully emerges. According to causal theories, having an\n“X” representation requires the ability to detect dogs.\nBut, the ability to detect dogs requires a theory of dogs. But, having\na theory of dogs requires already having a LOT—a system of\nmental representation. One cannot generate mental representations\nwithout already having\n them.[12] \nJason Bridges (2006) argues that the core hypothesis of informational\nsemantics conflicts with the idea that psychological laws are\nnon-basic. As we have just observed, causal theories are often taken\nto offer mere sufficiency conditions for meaning. Suppose, therefore,\nthat we suitably restrict the scope of a causal theory and understand\nits core hypothesis as asserting that all “X”s with the\ncontent X are reliably caused by X. (Nothing in the logic of\nBridges’ argument depends on any additional conditions on a\nputative causal theory of mental content, so for simplicity we can\nfollow Bridges in restricting attention to this simple version.)\nBridges proposes that this core claim of a causal theory of mental\ncontent is a constitution thesis. It specifies what constitutes the\nmeaning relation (at least in some restricted domain). Thus, if one\nwere to ask, “Why is it that all ‘X’s with content X\nare reliably caused by Xs?,” the answer is roughly,\n“That’s just what it is for ‘X’ to have the\ncontent X”. Being caused in that way is what constitutes having\nthat meaning. So, when a theory invokes this kind of constitutive\nrelation, there is this kind of constitutive explanation. So, the\nfirst premise of Bridges’ argument is that causal theories\nspecify a constitutive relation between meaning and reliable causal\nconnection. \nBridges next observes that causal theorists typically maintain that\nthe putative fact that all “X”s are reliably caused by Xs\nis mediated by underlying mechanisms of one sort or another. So,\n“X”s might be reliably caused by dogs in part through the\nmediation of a person’s visual system or auditory system.\nOne’s visual apparatus might causally connect particular\npatterns of color and luminance produced by dogs to “X”s.\nOne might put the point somewhat differently by saying that a causal\ntheorist’s hypothetical “Xs causes ‘X’s”\nlaw is not a basic or fundamental law of nature, but an implemented\nlaw. \nBridges’ third premise is a principle that he takes to be nearly\nself-evident, once understood. We can develop a better first-pass\nunderstanding of Bridges’ argument if, at the risk of distorting\nthe argument, we consider a slightly simplified version of this\nprinciple: \nTo illustrate the principle, suppose we say that gold is identical to\nthe element with atomic number 79, that all gold has atomic number 79.\nThen suppose one were to ask, “Why is it that all gold has the\natomic number 79?” The answer would be, “Gold just\nis the element with atomic number 79.” This would be a\nconstitutive explanation. According to (S), however, this constitutive\nexplanation precludes giving a further mechanistic explanation of why\ngold has atomic number 79. There is no mechanism by which gold gets\natomic number 79. Having atomic number 79 just is what makes gold\ngold. \nSo, here is the argument \nTherefore, by modus ponens on P1 and P2, \nBut, C1 contradicts the common assumption \nRupert (2008) challenges the first premise of Bridges’ argument\non two scores. First, he notes that claims about constitutive natures\nhave modal implications which at least some naturalistic philosophers\nhave found objectionable. Second, he claims that natural scientists do\nnot appeal to constitutive natures, so that one need not develop a\ntheory of mental content that invokes them. \nAlthough philosophers and cognitive scientists frequently propose to\ndispense with (one or another sort of) mental representation (cf.,\ne.g., Stich, 1983, Brooks, 1991, van Gelder, 1995, Haugeland, 1999,\nJohnson, 2007, Chemero, 2009), this is universally accepted to be a\nrevolutionary shift in thinking about minds. Short of taking on board\nsuch radical views, one will naturally want some explanation of how\nmental representations arise. In attempting such explanations, causal\ntheories have been widely perceived to have numerous attractive\nfeatures. If, for example, one use for mental representations is to\nhelp one keep track of events in the world, then some causal\nconnection between mind and world makes sense. This attractiveness has\nbeen enough to motivate new causal theories (e.g. Rupert, 1999, Usher,\n2001, and Ryder, 2004), despite the widespread recognition of serious\nchallenges to an earlier generation of theories developed by Stampe,\nDretske, Fodor, and others.","contact.mail":"fa@udel.edu","contact.domain":"udel.edu"},{"date.published":"2010-02-04","date.changed":"2017-04-18","url":"https://plato.stanford.edu/entries/content-causal/","author1":"Fred Adams","author1.info":"https://www.lingcogsci.udel.edu/people/faculty/Frederick%20Adams","author2.info":"https://ncas.rutgers.edu/about-us/faculty-staff/kenneth-aizawa","entry":"content-causal","body.text":"\n\n\nCausal theories of mental content attempt to explain how thoughts can\nbe about things. They attempt to explain how one can think about, for\nexample, dogs. These theories begin with the idea that there are\nmental representations and that thoughts are meaningful in virtue of a\ncausal connection between a mental representation and some part of the\nworld that is represented. In other words, the point of departure for\nthese theories is that thoughts of dogs are about dogs because dogs\ncause the mental representations of dogs.\n\nContent is what is said, asserted, thought, believed, desired, hoped\nfor, etc. Mental content is the content had by mental states and\nprocesses. Causal theories of mental content attempt to explain what\ngives thoughts, beliefs, desires, and so forth their contents. They\nattempt to explain how thoughts can be about\n things.[1] \nAlthough one might find precursors to causal theories of mental\ncontent scattered throughout the history of philosophy, the current\ninterest in the topic was spurred, in part, by perceived inadequacies\nin “similarity” or “picture” theories of\nmental representation. Where meaning and representation are asymmetric\nrelations—that is, a syntactic item “X” might mean\nor represent X, but X does not (typically) mean or represent\n“X”—similarity and resemblance are symmetric\nrelations. Dennis Stampe (1977), who played an important role in\ninitiating contemporary interest in causal theories, drew attention to\nrelated problems. Consider a photograph of one of two identical twins.\nWhat makes it a photo of Judy, rather than her identical twin Trudy?\nBy assumption, it cannot be the similarity of the photo to one twin\nrather than the other, since the twins are identical. Moreover, one\ncan have a photo of Judy even though the photo happens not to look\nvery much like her at all. What apparently makes a photo of Judy a\nphoto of Judy is that she was causally implicated, in the right way,\nin the production of the photo. Reinforcing the hunch that causation\ncould be relevant to meaning and representation is the observation\nthat there is a sense in which the number of rings in a tree stump\nrepresents the age of the tree when it died and that the presence of\nsmoke means fire. The history of contemporary developments of causal\ntheories of mental content consists largely of specifying what it is\nfor something to be causally implicated in the right way in the\nproduction of meaning and refining the sense in which smoke represents\nfire to the sense in which a person’s thoughts, sometimes at\nleast, represent the world. \nIf one wanted to trace a simple historical arc for recent causal\ntheories, one would have to begin with the seminal 1977 paper by\nDennis Stampe, “Toward a Causal Theory of Linguistic\nRepresentation.” Among the many important features of this paper\nis its having set much of the conceptual and theoretical stage to be\ndescribed in greater detail below. It drew a contrast between causal\ntheories and “picture theories” that try to explain\nrepresentational content by appeal to some form of similarity between\na representation and the thing represented. It also drew attention to\nthe problem of distinguishing the content determining causes of a\nrepresentation from adventitious non-content determining causes. So,\nfor example, one will want “X” to mean dogs because dogs\ncauses dogs, but one does not want “X” to mean\nblow-to-the-head, even though blows to the head might cause the\noccurrence of an “X”. (Much more of this will be described\nbelow.) Finally, it also provided some attempts to address this\nproblem, such as an appeal to the function a thing might have. \nFred Dretske’s 1981 Knowledge and the Flow of\nInformation offered a much expanded treatment of a type of causal\ntheory. Rather than basing semantic content on a causal connection\nper se, Dretske began with a type of informational connection\nderived from the mathematical theory of information. This has led some\nto refer to Dretske’s theory as “information\nsemantics”. Dretske also appealed to the notion of function in\nan attempt to distinguish content determining causes from adventitious\nnon-content determining causes. This has led some to refer to\nDretske’s theory as a “teleoinformational” theory or\na “teleosemantic” theory. Dretske’s 1988 book,\nExplaining Behavior, further refined his earlier\ntreatment. \nJerry Fodor’s 1984 “Semantics, Wisconsin Style” gave\nthe problem of distinguishing content-determining causes from\nnon-content determining causes its best-known guise as “the\ndisjunction problem”. How can a causal theory of content say\nthat “X” has the non-disjunctive content dog, rather than\nthe disjunctive content dog-or-blow-to-the-head, when both dogs and\nblows to the head cause instances of “X”? By 1987, in\nPsychosemantics, Fodor published his first attempt at an\nalternative method of solving the disjunction problem, the Asymmetric\n(Causal) Dependency Theory. This theory was further refined for the\ntitle essay in Fodor’s 1990 book A Theory of Content and\nOther Essays. \nAlthough these causal theories have subsequently spawned a significant\ncritical literature, other related causal theories have also been\nadvanced. Two of these are teleosemantic theories that are sometimes\ncontrasted with causal theories. (Cf., e.g., Papineau (1984), Millikan\n(1989), and Teleological Theories of Mental Content.) Other more\npurely causal theories are Dan Lloyd’s (1987, 1989) Dialectical\nTheory of Representation, Robert Rupert’s (1999) Best Test\nTheory (see section 3.5 below), Marius Usher’s (2001)\nStatistical Referential Theory, and Dan Ryder’s (2004) SINBAD\nneurosemantics. \nCausal theories of mental content are typically developed in the\ncontext of four principal assumptions. First, they typically\npresuppose that there is a difference between derived and underived\n meaning.[2]\n Normal humans can use one thing, such as “%”, to mean\npercent. They can use certain large red octagons to mean that one is\nto stop at an intersection. In such cases, there are collective\narrangements that confer relatively specific meanings on relatively\nspecific objects. In the case of human minds, however, it is proposed\nthat thoughts can have the meanings or contents they do without\nrecourse to collective arrangements. It is possible to think about\npercentage or ways of negotiating intersections prior to collective\nsocial arrangements. It, therefore, appears that the contents of our\nthoughts do not acquire the content they do in the way that\n“%” and certain large red octagons do. Causal theories of\nmental content presuppose that mental contents are underived, hence\nattempt to explain how underived meaning arises.  \nSecond, causal theories of mental content distinguish what has come to\nbe known as natural meaning and non-natural\n meaning.[3]\n Cases where an object or event X has natural meaning are those in\nwhich, given certain background conditions, the existence or\noccurrence of X “entails” the existence or occurrence of\nsome state of affairs. If smoke in the unspoiled forest naturally\nmeans fire then, given the presence of smoke, there was fire. Under\nthe relevant background conditions, the effect indicates or naturally\nmeans the cause. An important feature of natural meaning is that it\ndoes not generate falsity. If smoke naturally means fire, then there\nmust really be a fire. By contrast, many non-naturally meaningful\nthings can be false. Sentences, for example, can be meaningful and\nfalse. The utterance “Colleen currently has measles” means\nthat Colleen currently has measles but does not entail that Colleen\ncurrently has measles in the way that Colleen’s spots do entail\nthat she has measles. Like sentences, thoughts are also meaningful,\nbut often false. Thus, it is generally supposed that mental content\nmust be a form of non-natural unassigned\n meaning.[4] \nThird, these theories assume that it is possible to explain the origin\nof non-derived content without appeal to other semantic or contentful\nnotions. So, it is assumed that there is more to the project than\nsimply saying that one’s thoughts mean that Colleen currently\nhas the measles because one’s thoughts are about Colleen\ncurrently having the measles. Explicating meaning in terms of\naboutness, or aboutness in terms of meaning, or either in terms of\nsome still further semantic notion, does not go as far as is commonly\ndesired by those who develop causal theories of mental content. To\nnote some additional terminology, it is often said that causal\ntheories of mental content attempt to naturalize non-natural,\nnon-derived meaning. To put the matter less technically, one might say\nthat causal theories of mental content presuppose that it is possible\nfor a purely physical system to bear non-derived content. Thus, they\npresuppose that if one were to build a genuinely thinking robot or\ncomputer, one would have to design it in such a way that some of its\ninternal components would bear non-natural, non-derived content in\nvirtue of purely physical conditions. To get a feel for the difference\nbetween a naturalized theory and an unnaturalized theory of content,\none might note the theory developed by Grice (1948). Grice developed\nan unnaturalized theory. Speaking of linguistic items, Grice held that\n‘Speaker S non-naturally means something by\n“X”’ is roughly equivalent to ‘S intended the\nutterance of “X” to produce some effect in an audience by\nmeans of the recognition of this intention.’ Grice did not\nexplicate the origin of mental content of speaker’s intentions\nor audience recognition, hence he did not attempt to naturalize the\nmeaning of linguistic items. \nFourth, it is commonly presupposed that naturalistic analyses of\nnon-natural, non-derived meanings will apply, in the first instance,\nto the contents of thought. The physical items “X” that\nare supposed to be bearers of causally determined content will,\ntherefore, be something like the firings of a particular neuron or set\nof neurons. These contents of thoughts are said to be captured in what\nis sometimes called a “language of thought” or\n“mentalese.” The contents of items in natural languages,\nsuch as English, Japanese, and French, will then be given a separate\nanalysis, presumably in terms of a naturalistic account of non-natural\nderived meanings. It is, of course, possible to suppose that it is\nnatural language, or some other system of communication, that first\ndevelops content, which can then serve as a basis upon which to\nprovide an account of mental content. Among the reasons that threaten\nthis order of dependency is the fact that cognitive agents appear to\nhave evolved before systems of communication. Another reason is that\nhuman infants at least appear to have some sophisticated cognitive\ncapacities involving mental representation, before they speak or\nunderstand natural languages. Yet another reason is that, although\nsome social animals may have systems of communication complex enough\nto support the genesis of mental content, other non-social cognizing\nanimals may not. \nIt is worth noting that, in recent years, this last presupposition has\nsometimes been abandoned by philosophers attempting to understand\nanimal signaling or animal communication, as when toads emit mating\ncalls or vervet monkeys cry out when seeing a cheetah, eagle, or\nsnake. See, for example, Stegmann, 2005, 2009, Skyrms, 2008, 2010a, b,\n2012, and Birch, 2014. In other words, there have been efforts to use\nthe sorts of apparatus originally developed for theories of mental\ncontent, plus or minus a bit, as apparatus for handling animal\nsignaling. These approaches seem to allow that there are mental\nrepresentations in the brains of the signaling/communicating animals,\nbut do not reply on the content of the mental representations to\nprovide the representational contents of the signals. In this way, the\ncontents of the signals are not derived from the contents of the\nmental representations. \nThe unifying inspiration for causal theories of mental content is that\nsome syntactic item “X” means X because “X”s\nare caused by\n Xs.[5]\n Matters cannot be this simple, however, since in general one expects\nthat some causes of “X” are not among the\ncontent-specifying causes of “X”s. There are numerous\nexamples illustrating this point, each illustrating a kind of cause\nthat must not typically be among the content-determining causes of\n“X”: \nThe foregoing problem cases are generally developed under the rubric\nof “false beliefs” or “the disjunction\nproblem” in the following way and can be traced to Fodor (1984).\nNo one is perfect, so a theory of content should be able to explicate\nwhat is going on when a person makes a mistake, such as mistaking a\nfox for a dog. The first thought is that this happens when a fox (at a\ndistance or in poor lighting conditions) causes the occurrence of a\ntoken of “X” and, since “X” means dog, one has\nmistaken a fox for a dog. The problem with this first thought arises\nwith the invocation of the idea that “X” means dog. Why\nsay that “X” means dog, rather than dog or fox? On a\ncausal account, we need some principled reason to say that the content\nof “X” is dog, hence that the token of “X” is\nfalsely tokened by the fox, rather than the content of “X”\nis dog or fox, hence that the token of “X” is truly\ntokened by the fox. What basis is there for saying that\n“X” means dog, rather than dog or fox? Because there\nappears always to be this option of making the content of a term some\ndisjunction of items, the problem has been called “the\ndisjunction\n problem”.[6] \nAs was noted above, what unifies causal theories of mental content is\nsome version of the idea that “X”s being causally\nconnected to Xs makes “X”s mean Xs. What divides causal\ntheories of mental content, most notably, is the different approaches\nthey take to separating the content-determining causes from the\nnon-content-determining causes. Some of these different theories\nappeal to normal conditions, others to functions generated by natural\nselection, others to functions acquired ontogenetically, and still\nothers to dependencies among laws. At present there is no approach\nthat is commonly agreed to correctly separate the content-determining\ncauses from the non-content determining causes while at the same time\nrespecting the need not to invoke existing semantic concepts. Although\neach attempt may have technical problems of its own, the recurring\nproblem is that the attempts to separate content-determining from\nnon-content-determining causes threaten to smuggle in semantic\nelements. \nIn this section, we will review the internal problematic of causal\ntheories by examining how each theory fares on our battery of test\ncases (I)–(IV), along with other objections from time to time.\nThis provides a simple, readily understood organization of the project\nof developing a causal theory of mental content, but it does this at a\nprice. The primary literature is not arranged exactly in this way. The\npositive theories found in the primary literature are typically more\nnuanced than what we present here. Moreover, the criticisms are not\narranged into the kind of test battery we have with cases\n(I)–(IV). One paper might bring forward cases (I) and (III)\nagainst theory A, where another paper might bring forward cases (I)\nand (II) against theory B. Nor are the examples in our test battery\nexactly the ones developed in the primary literature. In other words,\nthe price one pays for this simplicity of organization is that we have\nsomething less like a literature review and more like a theoretical\nand conceptual toolbox for understanding causal theories. \nTrees usually grow a certain way. Each year, there is the passage of\nthe four seasons with a tree growing more quickly at some times and\nmore slowly at others. As a result, each year a tree adds a\n“ring” to its girth in such a way that one might say that\neach ring means a year of growth. If we find a tree stump that has\ntwelve rings, then that means that the tree was twelve years old when\nit died. But, it is not an entirely inviolable law that a tree grows a\nring each year. Such a law, if it is one, is at most a ceteris\nparibus law. It holds only given certain background conditions,\nsuch as that weather conditions are normal. If the weather conditions\nare especially bad one season, then perhaps the tree will not grow\nenough to produce a new ring. One might, therefore, propose that if\nconditions are normal, then n rings means that the tree was n years\nold when it died. This idea makes its first appearance when Stampe\n(1977) invokes it as part of his theory of “fidelity\nconditions.” \nAn appeal to normal conditions would seem to be an obvious way in\nwhich to bracket at least some non-content-determining causes of a\nwould-be mental representation “X”. It is only the causes\nthat operate under normal conditions that are content-determining. So,\nwhen it comes to human brains, under normal conditions one is not\nunder the influence of hallucinogens nor is one’s head being\ninvaded by an elaborate configuration of microelectrodes. So, even\nthough LSD and microelectrodes would, counterfactually speaking, cause\na token neural event “X”, these causes would not be among\nthe content-determining causes of “X”. Moreover, one can\ntake normal conditions of viewing to include good lighting, a\nparticular perspective, a particular viewing distance, a lack of\n(seriously) occluding objects, and so forth, so that foxes in dim\nlight, viewed from the bottom up, at a remove of a mile, or through a\ndense fog, would not be among the content-determining causes of\n“X”. Under normal viewing conditions, one does not confuse\na fox with a dog, so foxes are not to be counted as part of the\ncontent of “X”. Moreover, if one does confuse a fox with a\ndog under normal viewing conditions, then perhaps one does not really\nhave a mental representation of a dog, but maybe only a mental\nrepresentation of a member of the taxonomic family canidae. \nAlthough an appeal to normal conditions initially appears promising,\nit does not seem to be sufficient to rule out the causal\nintermediaries between objects in the environment and “X”.\nEven under normal conditions of viewing that include good lighting, a\nparticular perspective, a particular viewing distance, a lack of\n(seriously) occluding objects, and so forth, it is still the case that\nboth dogs and, say, retinal projections of dogs, lead to tokens of\n“X”. Why does the content of “X” not include\nretinal projections of dogs or any of the other causal intermediaries?\nNor do normal conditions suffice to keep questions from getting in\namong the content-determining causes. What abnormal conditions are\nthere when the question, “What kind of animal is named\n‘Fido’?,” leads to a tokening of an “X”\nwith the putative meaning of dog? Suppose there are instances of\nquantum mechanical fluctuations in the nervous system, wherein\nspontaneous changes in neurons lead to tokens of “X”. Do\nnormal conditions block these out? So, there are problem cases in\nwhich appeals to normal conditions do not seem to work. Fodor (1990b)\ndiscusses this problem with proximal stimulations in connection with\nhis asymmetric dependency theory, but it is one that clearly\nchallenges the causal theory plus normal conditions approach. \nNext, suppose that we tightly construe normal conditions to eliminate\nthe kinds of problem cases described above. So, when completely\nfleshed out, under normal conditions only dogs cause “X”s.\nWhat one intuitively wants is to be able to say that, under normal\nconditions of good lighting, proper viewing distance, etc.\n“X” means dog. But, another possibility is that in such a\nsituation “X” does not mean dog, but\ndog-under-normal-conditions-of-good-lighting, proper-viewing-distance,\netc. Why take one interpretation over another? One needs a principled\nbasis for distinguishing the cause of “X” from\nthe many causally contributing factors. In other words, we still have\nthe problem of bracketing non-content-determining causes, only in a\nslightly reformulated manner. This sort of objection may be found in\nFodor (1984). \nNow set the preceding problem aside. There is still another developed\nin Fodor (1984). Suppose that “X” does mean dog under\nconditions of good lighting, lack of serious occlusions, etc. Do not\nmerely suppose that “X” is caused by dogs under conditions\nof good light, lack of serious occlusions, etc.; grant that\n“X” really does mean dog under these conditions. Even\nthen, why does “X”, the firing of the neuronal circuit,\nstill mean dog, when those conditions do not hold? Why does\n“X” still mean dog under, say, degraded lighting\nconditions? After all, we could abide by another apparently true\nconditional regarding these other conditions, namely, if the lighting\nconditions were not so good, there were no serious occlusions, etc.,\nthen the neuronal circuit’s firing would mean dog or fox. Even\nif “X” means X under one set of conditions C1,\nwhy doesn’t “X” mean Y under a different set of\nconditions C2? It looks as though one could say that\nC1 provides normal conditions under which “X”\nmeans X and C2 provides normal conditions under which\n“X” means Y. We need some non-semantic notions to enable\nus to fix on one interpretation, rather than the other. At this point,\none might look to a notion of functions to solve these\n problems.[7] \nMany physical objects have functions. (Stampe (1977) was the first to\nnote this as a fact that might help causal theories of content.) A\nfamiliar mercury thermometer has the function of indicating\ntemperature. But, such a thermometer works against a set of background\nconditions which include the atmospheric pressure. The atmospheric\npressure influences the volume of the vacuum that forms above the\ncolumn of mercury in the glass tube. So, the height of the column of\nmercury is the product of two causally relevant features, the ambient\natmospheric temperature and the ambient atmospheric pressure. This\nsuggests that one and the same physical device with the same causal\ndependencies can be used in different ways. A column of mercury in a\nglass tube can be used to measure temperature, but it is possible to\nput it to use as a pressure gauge. Which thing a column of mercury\nmeasures is determined by its function. \nThis observation suggests a way to specify which causes of\n“X” determine its content. The content of “X”,\nsay, the firing of some neurons, is determined by dogs, and not foxes,\nbecause it is the function of those neurons to register the presence\nof dogs, but not foxes. Further, the content of “X” does\nnot include LSD, microelectrodes, or quantum mechanical fluctuations,\nbecause it is not the function of “X” to fire in response\nto LSD, microelectrodes, or quantum mechanical fluctuations in the\nbrain. Similarly, the content of “X” does not include\nproximal sensory projections of dogs, because the function of the\nneurons is to register the presence of the dogs, not the sensory\nstimulations. It is the objective features of the world that matter to\nan organism, not its sensory states. Finally, it is the function of\n“X” to register the presence of dogs, but not the presence\nof questions, such as ‘What kind of animal is named “Fido”?’,\nthat\nleads to “X” meaning dogs. Functions, thus, provide a\nprima facie attractive means of properly winnowing down the\ncauses of “X” to those that are genuinely content\ndetermining. \nIn addition, the theory of evolution by natural selection apparently\nprovides a non-semantic, non-intentional basis upon which to explicate\nfunctions and, in turn, semantic content. Individual organisms vary in\ntheir characteristics, such as how their neurons respond to features\nof the environment. Some of these differences in how neurons respond\nmake a difference to an organism’s survival and reproduction.\nFinally, some of these very differences may be heritable. Natural\nselection, commonly understood as this differential reproduction of\nheritable variation, is purely causal. Suppose that there is a\npopulation of rabbits. Further suppose that either by a genetic\nmutation or by the recombination of existing genes, some of these\nrabbits develop neurons that are wired into their visual systems in\nsuch a way that they fire (more or less reliably) in the presence of\ndogs. Further, the firing of these neurons is wired into a freezing\nbehavior in these rabbits. Because of this configuration, the rabbits\nwith the “dog neurons” are less likely to be detected by\ndogs, hence more likely to survive and reproduce. Finally, because the\ngenes for these neurons are heritable, the offspring of these\ndog-sensitive rabbits will themselves be dog-sensitive. Over time, the\nnumber of the dog-sensitive rabbits will increase, thereby displacing\nthe dog-insensitive rabbits. So, natural selection will, in such a\nscenario, give rise to mental representations of dogs. Insofar as such\na story is plausible, there is hope that natural selection and the\ngenesis of functions can provide a naturalistically acceptable means\nof delimiting content-determining causes. \nThere is no doubt that individual variation, differential\nreproduction, and inheritance can be understood in a purely causal\nmanner. Yet, there remains skepticism about how naturalistically one\ncan describe what natural selection can select for. There are doubts\nabout the extent to which the objects of selection really can be\nspecified without illicit importation of intentional notions. Fodor\n(1989, 1990a) give voice to some of this skepticism. Prima\nfacie, it makes sense to say that the neurons in our hypothetical\nrabbits fire in response to the presence of dogs, hence that there is\nselection for dog representations. But, it makes just as much sense,\none might worry, to say that it is sensitivity to dog-look-alikes that\nleads to the greater fitness of the rabbits with the new\n neurons.[8]\n There are genes for the dog-look-alike neurons and these genes are\nheritable. Moreover, those rabbits that freeze in response to\ndog-look-alikes are more likely to survive and reproduce than are\nthose that do not so freeze, hence one might say that the freezing is\nin response to dog-look-alikes. So, our ability to say that the\nmeaning of the rabbits’ mental representation “X” is\ndog, rather than dog-look-alike, depends on our ability to say that it\nis the dog-sensitivity of “X”, rather than the\ndog-look-alike-sensitivity of “X”, that keeps the rabbits\nalive longer. Of course, being dog-sensitive and being\ndog-look-alike-sensitive are connected, but the problem here is that\nboth being dog-look-alike-sensitive and being dog-sensitive can\nincrease fitness in ways that lead to the fixation of a genotype. And\nit can well be that it is avoidance of dogs that keeps a rabbit alive,\nbut one still needs some principled basis for saying that the rabbits\navoid dogs by being sensitive to dogs, rather than by being sensitive\nto dog-look-alikes. The latter appears to be good enough for the\ndifferential reproduction of heritable variation to do its work. Where\nwe risk importing semantic notions into the mix is in understanding\nselection intentionally, rather than purely causally. We need a notion\nof “selection for” that is both general enough to work for\nall the mental contents causal theorists aspire to address and that\ndoes not tacitly import semantic notions. \nIn response to this sort of objection, it has been proposed that the\ncorrect explanation of a rabbit’s evolutionary success with,\nsay, “X”, is not that this enables the rabbit to avoid\ndog-look-alikes, but that it enables them to avoid dogs. It is dogs,\nbut not mere dog-look-alikes, that prey on rabbits. (This sort of\nresponse is developed in Millikan (1991) and Neander (1995).) Yet, the\nrejoinder is that if we really want to get at the correct explanation\nof a rabbit-cum-“X” system, then we should not suppose\nthat “X” means dog. Instead, we should say that it is in\nvirtue of the fact that “X” picks up on something like,\nsay, predator of such and such characteristics that the\n“X” alarm system increases the chance of a rabbit’s\nsurvival. (This sort of rejoinder may be found in Agar (1993).) \nThis problem aside, there is also some concern about the extent to\nwhich it is plausible to suppose that natural selection could act on\nthe fine details of the operation of the brain, such as the firing of\nneurons in the presence of dogs. (This is an objection raised in Fodor\n(1990c)). Natural selection might operate to increase the size of the\nbrain so there is more cortical mass for cognitive processing. Natural\nselection might also operate to increase the folding of the brain so\nas to maximize the cortical surface area that can be contained within\nthe brain. Natural selection might also lead to compartmentalization\nof the brain, so that one particular region could be dedicated to\nvisual processing, another to auditory processing, and still another\nto face processing. Yet, many would take it to be implausible to\nsuppose that natural selection works at the level of individual mental\nrepresentations. The brain is too plastic and there is too much\nindividual variation in the brains of mammals to admit of selection\nacting in this way. Moreover, such far reaching effects of natural\nselection would lead to innate ideas not merely of colors and shapes,\nbut of dogs, cats, cars, skyscrapers, and movie stars. Rather than\nsupposing that functions are determined by natural selection across\nmultiple generations, many philosophers contend that it is more\nplausible that the functions that underlie mental representations are\nacquired through cognitive development. \nHypothesizing that certain activities or events within the brain mean\nwhat they do, in part, because of some function that develops over the\ncourse of an individual’s lifetime shares many of the attractive\nfeatures of the hypothesis that these same activities or events mean\nwhat they do, in part, because of some evolutionarily acquired\nfunction. One again can say that it is not the function of\n“X” to register the presence of LSD, microelectrodes,\nfoxes, stuffed dogs, or paper mâché dogs, or questions,\nbut it is their function to report on dogs. Moreover, it does not\ninvoke dubious suppositions about an intimate connection between\nnatural selection and the precise details of neuronal hardware and its\noperation. A functional account based on ontogenetic function\nacquisition or learning seems to be an improvement. This is the core\nof the approach taken in Dretske (1981; 1988). \nThe function acquisition story proposes that during development, an\norganism is trained to discriminate real flesh and blood dogs from\nquestions, foxes, stuffed dogs, paper mâché dogs under\nconditions of good lighting, without occlusions, or distractions. A\nteacher ensures that training proceeds according to plan. Once\n“X” has acquired the function to respond to dogs, the\ntraining is over. Thereafter, any instances in which “X”\nis triggered by foxes, stuffed dogs, paper mâché dogs,\nLSD, microelectrodes, etc., are false tokenings and figure into false\nbeliefs. \nAmong the most familiar objections to this proposal is that there is\nno principled distinction between when a creature is learning and when\nit is done learning. Instances in which a creature entertains the\nhypothesis that “X” means X, instances in which the\ncreature entertains the hypothesis that “X” means Y,\ninstances in which the creature straightforwardly uses “X”\nto mean X, and instances in which the creature straightforwardly uses\n“X” to mean Y are thoroughly intermingled. The problem is\nperhaps more clearly illustrated with tokens of natural language,\nwhere children will use words struggling through correct and incorrect\nuses of a word before (perhaps) finally settling on a correct usage.\nThere seems to be no principled way to specify if learning has stopped\nor whether there is instead “lifelong learning”. This is\namong the objections to be found in Fodor (1984). \nThis, however, is a relatively technical objection. Further reflection\nsuggests that there may be an underlying appeal to the intentions of\nthe teacher. Let us revisit the learning story. Suppose that during\nthe learning period the subject is trained to use “X” as a\nmental representation of dogs. Now, let the student graduate from\n“X” using school and immediately thereafter see a fox.\nSeeing this fox causes a token of “X” and one would like\nto say that this is an instance of mistaking a fox for a dog, hence a\nfalse tokening. But, consider the situation counterfactually. If the\nstudent had seen the fox during the training period just before\ngraduation, the fox would have triggered a token of “X”.\nThis suggests that we might just as well say that the student learned\nthat “X” means fox or dog as that the student learned that\n“X” means dog. Thus, we might just as well say that, after\ntraining, the graduate does not falsely think of a dog, but truly\nthinks of a fox or a dog. The threat of running afoul of naturalist\nscruples comes if one attempts to say, in one way or another, that it\nis because the teacher meant for the student to learn that\n“X” means dog, rather than “X” means fox or\ndog. The threatened violation of naturalism comes in invoking the\nteacher’s intentions. This, too, is an objection to be found in\nFodor (1984). \nThe preceding attempts to distinguish the content-determining causes\nfrom non-content-determining causes focused on the background or\nboundary conditions under which the distinct types of causes may be\nthought to act. Fodor’s Asymmetric Dependency Theory (ADT),\nhowever, represents a bold alternative to these approaches. Although\nFodor (1987, 1990a, b, 1994) contain numerous variations on the\ndetails of the theory, the core idea is that the content-determining\ncause is in an important sense fundamental, where the\nnon-content-determining causes are non-fundamental. The sense of being\nfundamental is that the non-content-determining causes depend on the\ncontent-determining cause; the non-content-determining causes would\nnot exist if not for the content-determining cause. Put a bit more\ntechnically, there are numerous laws such as ‘Y1\ncauses “X”,’ ‘Y2 causes\n“X”,’ etc., but none of these laws would exist were\nit not a law that X causes “X”. The fact that the ‘X\ncauses “X”’ law does not in the same way depend on\nany of the Y1, Y2, …, Yn laws\nmakes the dependence asymmetric. Hence, there is an asymmetric\ndependency between the laws. The intuition here is that the question,\n‘What kind of animal is called “Fido”?’ will\ncause an occurrence of the representation “X” only because\nof the fact that dogs cause “X”. Instances of foxes cause\ninstances of “X” only because foxes are mistaken for dogs\nand dogs cause instances of “X”. \nCausation is typically understood to have a temporal dimension. First\nthere is event C and this event C subsequently leads to event E. Thus,\nwhen the ADT is sometimes referred to as the “Asymmetric Causal\nDependency Theory,” the term “causal” might suggest\na diachronic picture in which there is, first, an X-“X”\nlaw which subsequently gives rise to the various Y-“X”\nlaws. Such a diachronic interpretation, however, would lead to\ncounterexamples for the ADT approach. Fodor (1987) discusses this\npossibility. Consider Pavlovian conditioning. Food causes salivation\nin a dog. Then a bell causes salivation in the dog. It is likely that\nthe bell causes salivation only because the food causes it. Yet,\nsalivation hardly means food. It may well naturally mean that food is\npresent, but salivation is not a thought or thought content and it is\nnot ripe for false semantic tokening. Or take a more exotic kind of\ncase. Suppose that one comes to apply “X” to dogs, but\nonly by means of observations of foxes. This would be a weird case of\n“learning”, but if things were to go this way, one would\nnot want “X” to mean fox. To block this kind of objection,\nthe theory maintains the dependency between the fundamental\nX-“X” law and the non-fundamental Y-“X” laws\nis synchronic. The dependency is such that if one were to break the\nX-“X” law at time t, then one would thereby\ninstantaneously break all the Y-“X” laws at that time. \nThe core of ADT, therefore, comes down to this. “X” means\nX if \nThis seems to get a number of cases right. The reason that questions\nlike “What kind of animal is named ‘Fido’?” or\n“What is a Sheltie?” trigger “X”, meaning dog,\nis that dogs are able to trigger “X”s. Foxes only trigger\n“X”s, meaning dog, because dogs are able to trigger them.\nMoreover, it appears to solve the disjunction problem. Suppose we have\na ‘dogs cause “X”s’ law and a ‘dogs or\nfoxes cause “X”s’ law. If one breaks the ‘dogs\ncause “X”s’ law, then one thereby breaks the\n‘dogs or foxes cause “X”s’ law, since the only\nreason either dogs or foxes cause “X”s is because dogs do.\nMoreover, if one breaks the ‘dogs or foxes cause\n“X”s’ law, one does not thereby break the\n‘dogs cause “X”s’ law, since dogs alone might\nsuffice to cause “X”s. So, the ‘dogs or foxes cause\n“X”s’ law depends on the ‘dogs cause\n“X”s’ law, but not vice versa. Asymmetric dependency\nof laws gives the right\n results.[9] \nAdams and Aizawa (1994) mention an important class of causes that the\nADT does not appear to handle, namely, the “non-psychological\ninterventions”. We have all along assumed the “X” is\nsome sort of brain event, such as the firing of some neurons. But, it\nis plausible that some interventions, such as a dose of hallucinogen\nor maybe some carefully placed microelectrodes, could trigger such\nbrain events, quite apart from the connection of those brain events to\nother events in the external world. If essentially all brain events\nare so artificially inducible, then it would appear that for all\nputative mental representations, there will be some laws, such as\n‘microelectrodes cause “X”s,’ that do not\ndepend on laws such as ‘dogs causes “X”s.’ If\nthis is the case, then the second condition of the ADT would rarely or\nnever be satisfied, so that the theory would have little relevance to\nactual cognitive scientific practice. \nFodor (1990a) discusses challenges that arise with the fact that the\nperception of objects involves causal intermediaries. Suppose that\nthere is a dog-“X” law that is mediated entirely by\nsensory mechanisms. In fact, suppose unrealistically that the\ndog-“X” law is mediated by a single visual sensory\nprojection. In other words let the dog-“X” law be mediated\nby the combination of a dog-dogsp law and a\ndogsp-“X” law. Under these conditions, it\nappears that “X” means dogsp, rather than dog.\nCondition (1) is satisfied, since there is a\ndogsp-“X” law. Condition (2) is satisfied,\nsince if one were to break the dogsp-“X” law\none would thereby break the dog-“X” law (i.e., there is a\ndependence of one law one the other) and breaking the\ndog-“X” law would not necessarily break the\ndogsp-“X” law (i.e., the dependence is not\nsymmetric). The dependence is asymmetric, because one can break the\ndog-“X” law by breaking the dog-dogsp law (by\nchanging the way dogs look) without thereby breaking the\ndogsp-“X” law. Finally, condition (3) is\nsatisfied, since the dependence of the dog-“X” law on the\ndogsp-“X” law is synchronic. \nThe foregoing version of the sensory projections problem relies on\nwhat was noted to be the unrealistic assumption that the\ndog-“X” law is mediated by a single visual sensory\nprojection. Relaxing the assumption does not so much solve the problem\nas transform it. So, adopt the more realistic assumption that the\ndog-“X” law is sustained by a combination of a large set\nof dog-sensory projection laws and a large set of\ndogsp-“X” laws. In the first set, we have laws\nconnecting dogs to particular patterns of retinal stimulation, laws\nconnecting dogs to particular patterns of acoustic stimulation, etc.\nIn the second set, we have certain psychological laws connecting\nparticular patterns of retinal stimulation to “X”, certain\npsychological laws connecting particular patterns of acoustic\nstimulation to “X”, etc. In this sort of situation, there\nthreatens to be no “fundamental” law, no law on which all\nother laws asymmetrically depend. If one breaks the\ndog-“X” law one does not thereby break any of the sensory\nprojection-“X” laws, since the former can be broken by\ndissolving all of the dog-sensory projection laws. If, however, one\nbreaks any one of the particular dogsp-“X”\nlaws, e.g. one connecting a particular doggish visual appearance to\n“X”, one does not thereby break the dog-“X”\nlaw. The other sensory projections might sustain the\ndog-“X” law. Moreover, breaking the law connecting a\nparticular doggish look to “X” will not thereby break a\nlaw connecting a particular doggish sound to “X”. Without\na “fundamental” law, there is no meaning in virtue of the\nconditions of the ADT. Further, the applicability of the ADT appears\nto be dramatically reduced insofar as connections between mental\nrepresentations and properties in the world are mediated by sensory\nprojections. \nAnother problem arises with items or kinds that are indistinguishable.\nAdams and Aizawa (1994), and, implicitly, McLaughlin, (1991), among\nothers, have discussed this problem. As one example, consider the time\nat which the two minerals, jadeite and nephrite, were chemically\nindistinguishable and were both thought to be jade. As another, one\nmight appeal to H2O and XYZ (the stuff of philosophical\nthought experiments, the water look-alike substance found on\ntwin-earth). Let X = jadeite and Y = nephrite and let there be laws\n‘jadeite causes “X”’ and ‘nephrite\ncauses “X”’. Can “X” mean jadeite? No.\nCondition (1) is satisfied, since it is a law that ‘jadeite\ncauses “X”’. Condition (3) is satisfied, since\nbreaking the jadeite-“X” law will immediately break the\nnephrite-“X” law. If jadeite cannot trigger an\n“X”, then neither can nephrite, since the two are\nindistinguishable. That is, there is a synchronic dependence of the\n‘nephrite causes “X”’ law on the\n‘jadeite causes “X” law. The problem arises with\ncondition (2). Breaking the jadeite-“X” law will thereby\nbreak the nephrite-“X” law, but breaking the\nnephrite-“X” law will also thereby break the\njadeite-“X” law. Condition (2) cannot be satisfied, since\nthere is a symmetric dependence between the jadeite-“X”\nlaw and the nephrite-“X” law. By parity of reasoning,\n“X” cannot mean nephrite. So, can “X” mean\njade? No. As before, conditions (1) and (3) could be satisfied, since\nthere could be a jade-“X” law and the\njadeite-“X” law and the nephrite-“X” law could\nsynchronically depend on it. The problem is, again, with condition\n(2). Presumably breaking the jade-“X” law would break the\njadeite-“X” and nephrite-“X” law, but breaking\neither of them would break the jade-“X” law. The problem\nis, again, with symmetric dependencies. \nHere is a problem that we earlier found in conjunction with other\ncausal theories. Despite the bold new idea underlying the ADT method\nof partitioning off non-content-determining causes, it too appears to\nsneak in naturalistically unacceptable assumptions. Like all causal\ntheories of mental content, the asymmetric causal dependencies are\nsupposed to be the basis upon which meaning is created; the\ndependencies are not themselves supposed to be a product, or\nbyproduct, of meaning. Yet, ADT appears to violate this naturalistic\npre-condition for causal theories. (This kind of objection may be\nfound in Seager (1993), Adams & Aizawa (1994), (1994b), Wallis\n(1995), and Gibson (1996)). Ys are supposed to cause “X”s\nonly because Xs do and this must not be because of any semantic facts\nabout “X”s. So, what sort of mechanism would bring about\nsuch asymmetric dependencies among things connected to the syntactic\nitem “X”? In fact, why wouldn’t lots of things be\nable to cause “X”s besides Xs, quite independently of the\nfact that Xs do? The instantiation of “X”s in the brain\nis, say, some set of neurochemical events. There should be natural\ncauses capable of producing such events in one’s brain under a\nvariety of circumstances. Why on earth would foxes be able to cause\nthe neurochemical “X” events in us only because dogs can?\nOne might be tempted to observe that “X” means dog,\n“Y” means fox, we associate foxes with dogs and that is\nwhy foxes cause “X”s only because dogs cause\n“X”s. We would not associate foxes with “X”s\nunless we associated “X”s with dogs and foxes with dogs.\nThis answer, however, involves deriving the asymmetric causal\ndependencies from meanings, which violates the background assumption\nof the naturalization project. Unless there is a better explanation of\nsuch asymmetrical dependencies, it may well be that the theory is\nmisguided in attempting to rest meaning upon them. \nA relatively more recent causal theory is Robert Rupert’s (1999)\nBest Test Theory (BTT) for the meanings of natural kind terms. Unlike\nmost causal theories, this one is restricted in scope to just natural\nkinds and terms for natural kinds. To mark this restriction, we will\nlet represented kinds be denoted by K’s, rather than our usual\nX’s. \nBest Test Theory: If a subject S bears no\nextension-fixing intentions toward “X” and “X”\nis an atomic natural kind term in S’s language of thought (i.e.,\nnot a compound of two or more other natural kind terms), then\n“X” has as its extension the members of natural kind K if\nand only if members of K are more efficient in their causing of\n“X” in S than are the members of any other natural\nkind. \nTo put the idea succinctly, “X” means, or refers to, those\nthings that are the most powerful stimulants of “X”. That\nsaid, we need an account of what it is for a member of a natural kind\nto be more efficient in causing “X”s than are other\nnatural kinds. We need an account of how to measure the power of a\nstimulus. This might be explained in terms of a kind of biography. Figure 1. A\nspreadsheet biography \nConsider an organism S that (a) causally interacts with three\ndifferent natural kinds, K1-K3, in its\nenvironment and (b) has a language of thought with five terms\n“X1”-“X5”. Further,\nsuppose that each time S interacts with an individual of kind\nKi this causes an occurrence of one or more of\n“X1”-“X5”. We can then\ncreate a kind of “spreadsheet biography” or “log of\nmental activity” for S in which there is a column for each of\n“X1”-“X5” and a row for\neach instance in which a member of K1-K3 causes\none or more instances of\n“X1”-“X5”. Each mental\nrepresentation “Xi” that Ki triggers\nreceives a “1” in its column. Thus, a single spreadsheet\nbiography might look like that shown in Figure 1. \nTo determine what a given term “Xi” means, we\nfind the kind Ki that is most effective at causing\n“Xi”. This can be computed from S’s\nbiography. For each Ki and “Xi”, we\ncompute the frequency with which Ki triggers\n“Xi”. “X1” is tokened\nfour out of six times that K1 is encountered, three out of\nthree times that K2 is encountered, and one out of four\ntimes that K3 is encountered. “Xi”\nmeans that Ki that has the highest sample frequency. Thus,\nin this case, “X1” means K2. Just to\nbe clear, when BTT claims that “Xi” means the\nKi that is the most powerful stimulant of “X”,\nthis is not to say that “X” means the most common\nstimulant of “X”. In our spreadsheet biography,\nK1 is the most common stimulant of\n“X1”, since it triggers\n“X1” four times, where K2 triggers\nit only three times, and K3 triggers it only one time. This\nis why, according to BTT, “X1” means\nK2, rather than K1 or K3. \nHow does the BTT handle our range of test cases? Consider, first, the\nstandard form of the disjunction problem, the case of “X”\nmeaning dog, rather than dog or fox-on-a-dark-night-at-a-distance.\nSince the latter is not apparently a natural kind, “X”\ncannot mean\n that.[10]\n Moreover, “X” means dog, rather than fox, because the\nonly times the many foxes that S encounters can trigger\n“X1”s is on dark nights at a distance, where\ndogs trigger “X”s more consistently under a wider range of\nconditions. \nHow does the BTT address the apparent problem of “brain\ninterventions,” such as LSD, microelectrodes, or brain tumors?\nThe answer is multi-faceted. The quickest method for taking much of\nthe sting out of these cases is to note that they generally do not\narise for most individuals. The Best Test Theory relies on personal\nbiographies in which only actual instances of kinds triggering mental\nrepresentations are used to specify causal efficiency. The\ncounterfactual truth that, were a stimulating microelectrode to be\napplied to, say, a particular neuron, it would perfectly reliably\nproduce an “X” token simply does not matter for the\ntheory. So, for all those individuals who do not take LSD, do not have\nmicroelectrodes inserted in their brains, do not have brain tumors,\netc., these sorts of counterfactual possibilities are irrelevant. A\nsecond line of defense against “brain interventions”\nappeals to the limitation to natural kinds. The BTT might set aside\nmicroelectrodes, since they do not constitute a natural kind. Maybe\nbrain tumors are; maybe not. Unfortunately, however, LSD is a very\nstrong candidate for a chemical natural kind. Still the BTT is not\nwithout a third line of defense for handling these cases. One might\nsuppose that LSD and brain tumors act on the brain in a rather diffuse\nmanner. Sometimes a dose of LSD triggers “Xi”,\nanother time it triggers “Xj”, and another time\nit triggers “Xk”. One might then propose that,\nif one counts all these episodes with LSD, none of these will act\noften enough on, say, “Xi” to get it to mean\nLSD, rather than, say, dog. This is the sort of strategy that Rupert\ninvokes to keep mental symbols from meaning omnipresent, but\nnon-specific causes such as the heart. The heart might causally\ncontribute to “X1”, but it also contributes to\nso many other “Xi”s, that the heart will turn\nout not to be the most efficient cause of\n“X1”. \nWhat about questions? Presumably questions as a category will count as\nan instance of a linguistic natural kind. Moreover, particular\nsentences will also count. So, the restriction of the BTT to natural\nkinds is of little use here. So, what of causal efficiency? Many\nsentences appear to provoke a wide range of possible responses. In\nresponse to, “I went to the zoo last week,” S could think\nof lions, tigers, bear, giraffes, monkeys, and any number of other\nnatural kinds. But, the question, “What animal goes ‘oink,\noink’?”—perhaps uttered in “Motherese”\nin a clear deliberate fashion so that it is readily comprehensible to\na child—will be rather efficient in generating thoughts of a\npig. Moreover, it could be more efficient than actual pigs, since a\nchild might have more experience with the question than with actual\npigs, often not figuring out that actual pigs are pigs. In such\nsituations, “pig” would turn out to mean “What\nanimal goes ‘oink, oink’?,” rather than pig. So,\nthere appear to be cases in which BTT could make prima facie\nincorrect content assignments. \nWhat, finally, of proximal projections of natural kinds? One plausible\nline might be to maintain that proximal projections of natural kinds\nare not themselves natural kinds, hence that they are automatically\nexcluded from the scope of the theory. This plausible line, however,\nmight be the only available line. Presumably, in the course of\nS’s life, the only way dogs can cause “X”s is by way\nof causal mediators between the dogs and the “X”s. Thus,\neach episode in which a dog causes an “X” is also an\nepisode in which a sensory projection of a dog causes an\n“X”. So, dog efficiency for “X” can be no\nhigher the efficiency of dog sensory projections. And, if it is\npossible for there to be a sensory projection of a dog without there\nbeing an actual dog, then the efficiency of the projections would be\ngreater than the efficiency of the dogs. So, “X” could not\nmean dog. But, this problem is not necessarily damaging to BTT. \nSince the BTT has not received a critical response in the literature,\nwe will not devote a section to objections to it. Instead, we will\nleave well enough alone with our somewhat speculative treatment of how\nBTT might handle our familiar test cases. The general upshot is that\nthe combination of actual causal efficiency over the course of an\nindividual’s lifetime along with the restriction to natural\nkinds provides a surprisingly rich means of addressing some\nlong-standing problems. \nIn the preceding section, we surveyed issues that face the philosopher\nattempting to work out the details of a causal theory of mental\ncontent. These issues are, therefore, one might say, internal to\ncausal theories. In this section, however, we shall review some of the\nobjections that have been brought forward to the very idea of a causal\ntheory of mental content. As such, these objections might be construed\nas external to the project of developing a causal theory of mental\ncontent. Some of these are coeval with causal theories and have been\naddressed in the literature, but some are relatively recent and have\nnot been discussed in the literature. The first objections, discussed\nin subsections 4.1–4.4, in one way or another push against the\nidea that all content could be explained by appeal to a causal theory,\nbut leave open the possibility that one or another causal theory might\nprovide sufficiency conditions for meaning. The last objections, those\ndiscussed in subsections 4.5–4.6 challenge the ability of causal\ntheories to provide even sufficiency conditions for mental\ncontent. \nOne might think that the meanings of terms that denote mathematical or\nlogical relations could not be handled by a causal theory. How could a\nmental version of the symbol “+” be causally connected to\nthe addition function? How could a mental version of the logical\nsymbol “¬” be causally connected to the negation truth\nfunction? The addition function and the negation function are abstract\nobjects. To avoid this problem, causal theories typically acquiesce\nand maintain that their conditions are merely sufficient conditions on\nmeaning. If an object meets the conditions, then that object bears\nmeaning. But, the conditions are not necessary for meaning, so that\nrepresentations of abstract objects get their meaning in some other\nway. Perhaps conceptual role semantics, wherein the meanings of terms\nare defined in terms of the meanings of other terms, could be made to\nwork for these other theories. \nAnother class of potential problem cases are vacuous terms. So, for\nexample, people can think about unicorns, fountains of youth, or the\nplanet Vulcan. Cases such as these are discussed in Stampe (1977) and\nFodor (1990a), among other places. These things would be physical\nobjects were they to exist, but they do not, so one cannot causally\ninteract with them. In principle, one could say that thoughts about\nsuch things are not counterexamples to causal theories, since causal\ntheories are meant only to offer sufficiency conditions for meaning.\nBut, this in principle reply appears to be ad hoc. It is not\nwarranted, for example, by the fact that these excluded meanings\ninvolve abstract objects. There are, however, a number of options that\nmight be explored here. \nOne strategy would be to turn to the basic ontology of one’s\ncausal theory of mental content. This is where a theory based on\nnomological relations might be superior to a version that is based on\ncausal relations between individuals. One might say that there can be\na unicorn-“unicorn” law, even if there are no actual\nunicorns. This story, however, would break down for mental\nrepresentations of individuals, such as the putative planet Vulcan.\nThere is no law that connects a mental representation to an\nindividual; laws are relations among properties. \nAnother strategy would be to propose that some thought symbols are\ncomplex and can decompose into meaningful primitive constituents. One\ncould then allow that “X” is a kind of abbreviation for,\nor logical construction of, or defined in terms of “Y1,”\n“Y2,” and “Y3,” and that a causal theory\napplies to “Y1,” “Y2,” and “Y3.”\nSo, for example, one might have a thought of a unicorn, but rather\nthan having a single unicorn mental representation there is another\nrepresentation made up of a representation of a horse, a\nrepresentation of a horn, and a representation of the relationship\nbetween the horse and the horn. “Horse”,\n“horn”, and “possession,” may then have\ninstantiated properties as their contents. \nHorgan and Tienson (2002) object to what they describe as\n“strong externalist theories” that maintain that causal\nconnections are necessary for content. They argue, first, that mental\nlife involves a lot of intentional content that is constituted by\nphenomenology alone. Perceptual states, such as seeing a red apple,\nare intentional. They are about apples. Believing that there are more\nthan 10 Mersenne primes and hoping to discover a new Mersenne prime\nare also intentional states, in this case about Mersenne primes. But,\nall these intentional states have a phenomenology—something it\nis like to be in these states. There is something it is like to see a\nred apple, something different that it is like to believe there are\nmore than 10 Mersenne primes, and something different still that it is\nlike to hope to discover a new Mersenne prime. Horgan and Tienson\npropose that there can be phenomenological duplicates—two\nindividuals with exactly the same phenomenology. Assume nothing about\nthese duplicates other than that they are phenomenological duplicates.\nIn such a situation, one can be neutral regarding how much of their\nphenomenological experience is veridical and how much illusory. So,\none can be neutral on whether or not a duplicate sees a red apple or\nwhether there really are more than 10 Mersenne primes. This suggests\nthat there is a kind of intentionality—that shared by the\nduplicates—that is purely phenomenological. Second, Horgan and\nTienson argue that phenomenology constitutively depends only on narrow\nfactors. They observe that one’s experiences are often caused or\ntriggered by events in the environment, but that these environmental\ncauses are only parts of causal chains that lead to the phenomenology\nitself. They do not constitute that phenomenology. The states that\nconstitute, or provide the supervenience base for, the phenomenology\nare not the elements of the causal chain leading back into the\nenvironment. If we combine the conclusions of these two arguments, we\nget Horgan and Tienson’s principal argument against any causal\ntheory that would maintain that causal connections are necessary for\ncontent. \nTherefore, \nThus, versions of causal theories that suppose that all content must\nbe based on causal connections are fundamentally mistaken. For those\nversions of causal theories that offer only sufficiency conditions on\nsemantic content, however, Horgan and Tienson’s argument may be\ntaken to provide a specific limitation on the scope of causal\ntheories, namely, that causal theories do not work for intentional\ncontent that is constituted by phenomenology alone. \nA relatively familiar challenge to this argument may be found in\ncertain representational theories of phenomenological properties.\n(See, for example, Dretske (1988) and Tye (1997).) According to these\nviews, the phenomenology of a mental state derives from that\nstate’s representational properties, but the representational\nproperties are determined by external factors, such as the environment\nin which an organism finds itself. Thus, such representationalist\ntheories challenge premise P2 of Horgan and Tienson’s\nargument. \nBuras (2009) presents another argument that is perhaps best thought of\nas providing a novel reason to think that causal theories of mental\nrepresentation only offer sufficiency conditions on meaning. This\nargument begins with the premise that some mental states are about\nthemselves. To motivate this claim, Buras notes that some sentences\nare about themselves. So, by analogy with, “This sentence is\nfalse,” which is about itself, one might think that there is a\nthought, “This thought is false,” that is also about\nitself. Or, how about “This thought is realized in brain\ntissue” or “This thought was caused by LSD”? These\nappear to be about themselves. Buras’ second premise is that\nnothing is a cause of itself. So, “This thought is false”\nis about itself, but could not be caused by itself. So, the sentence\n“This thought is false” could not mean that it itself is\nfalse in virtue of the fact that “This thought is false”\nwas caused by its being false. So, “This thought is false”\nmust get its meaning in some other way. It must get its meaning in\nvirtue of some other conditions of meaning acquisition. \nThis is not, however, exactly the way Buras develops his argument. In\nthe first place, he treats causal theories of mental content as\nmaintaining that, if “X” means X, then X causes\n“X”. (Cf. Buras, 2009, p. 118). He cites Stampe (1977),\nDretske (1988), and Fodor (1987) as maintaining this. Yet, Stampe,\nDretske, and Fodor explicitly formulate their theories in terms of\nsufficiency conditions, so that (roughly) “X” means X, if\nXs causes “X”s, etc. (See, for example, Stampe (1977), pp.\n82–3, Dretske (1988), p. 52, and Fodor (187), p. 100). In the\nsecond place, Buras seems to draw a conclusion that is orthogonal to\nthe truth or falsity of causal theories of mental content. He begins\nhis paper with an impressively succinct statement of his argument. \nSome mental states are about themselves. Nothing is a cause of itself.\nSo some mental states are not about their causes; they are about\nthings distinct from their causes (Buras, 2009, p. 117). \nThe causal theorist can admit that some mental states are not about\ntheir causes, since some states are thoughts and thoughts mean what\nthey do in virtue of, say, the meanings of mental sentences. These\nmental sentences might mean what they do in virtue of the meanings of\nprimitive mental representations (which may or may not mean what they\ndo in virtue of a causal theory of meaning) and the way in which those\nprimitive mental representations are put together. As was mentioned in\nsection 2, such a syntactically and semantically combinatorial\nlanguage of thought is a familiar background assumption for causal\ntheories. The conclusion that Buras may want, instead, is that there\nare some thoughts that do not mean what they do in virtue of what\ncauses them. So, through some slight amendments, one can understand\nBuras to be presenting a clarification of the scope of causal theories\nof mental content or as a challenge to a particularly strong version\nof causal theories, a version that takes them as offering a necessary\ncondition on meaning. \nAs noted above, one of the central challenges for causal theories of\nmental content has been to discriminate between a “core”\ncontent-determining causal connection, as between cows and “cow”s,\nand “peripheral” non-content-determining causal connections, as\nbetween horses and “cow”s. Cases of reliable misrepresentations\nare representations which always misrepresent in the same way. In such\ncases, there is supposed to be no “core” content-determining\ncausal connection; there are no X’s to which “X”s are causally\nconnected. Instead, there are only “peripheral” causal\nconnections. Mendelovici, (2013), following a discussion by Hohman,\n(2002), suggests that color representations may be like\n this.[11]\n Color anti-realism, according to which there are no colors in the\nworld, seems to be committed to the view that color representations\nare not caused by colors in the world. Color representations may be\nreliably tokened by something in the world, but not by colors that are\nin the world. \nIn some instances, reliable misrepresentations provide another take on\nsome of the familiar content-determination problems. So, take attempts\nto use normal conditions to distinguish between content-determining\ncauses and non-content-determining causes. Even in normal conditions,\ncolor representations are not caused by colors, but by, say, surface\nreflectances under certain conditions of illumination, just in the way\nthat, even in normal conditions cow representations are sometimes not\ncaused by cows, but by, say, a question such as, “What kind of\nanimal is sometimes named ‘Bessie’?” Take a version of the\nasymmetric dependency theory. On this theory applied to color terms,\nit might seem that there is no red-to-“red” law on which all the\nother laws depends in much the same way that it might seem there is no\nunicorn-to-“unicorn” law on which all other laws depends. (Cf.\nFodor (1987, pp. 163–4) and (1990, pp. 100–1)). \nUnlike the more familiar cases, Mendelovici, (2013), does not argue\nthat there actually are such problematic cases. The argument is not\nthat there are actual cases of reliable misrepresentations, but merely\nthat reliable misrepresentations are possible and that this is enough\nto create trouble for causal theories of mental representation. One\nsort of trouble stems from the need for a pattern of psychological\nexplanation. Let a mental representation “X” mean\nintrinsically-heavy. Such a representation is a misrepresentation,\nsince there is no such property of being intrinsically heavy. Such a\nmisrepresentation is, nonetheless, reliable (i.e. consistent), since\nit is consistently tokened by all the same sort of things on earth.\nBut, one can see how an agent using “X” could make a reasonable,\nyet mistaken, inference to the conclusion that an object that causes a\ntokening of “X” on earth would be hard to lift on the moon. To\nallow such a pattern of explanation, Mendelovici argues, a causal\ntheorist must allow for reliable misrepresentation. A theory of what\nmental representations are should not preclude such patterns of\nexplanation. Another sort of trouble stems from the idea that if a\ntheory of meaning does not allow for reliable misrepresentation, but\nrequires that there be a connection between “X”s and Xs, then this\nwould be constitute a commitment to a realist metaphysics for Xs.\nWhile there can be good reasons for realism, the needs of a theory of\ncontent would not seem to be a proper source for them.  \nArtiga, 2013, provides a defense of teleosemantic theories in the face\nof Mendelovici’s examples of reliable misrepresentation. Some of\nArtiga’s arguments might also be used by advocates of causal\ntheories of mental content. Mendelovici, (2016), replies to Artiga,\n2013, by providing refinements and a further defense of the view that\nreliable misrepresentations are a problem for causal theories of\nmental content. \nCummins (1997) argues that causal theories of mental content are\nincompatible with the fact that one’s perception of objects in\nthe physical environment is typically mediated by a theory. His\nargument proceeds in two stages. In one stage, he argues that, on a\ncausal theory, for each primitive “X” there must be some\nbit of machinery or mechanism that is responsible for detecting Xs.\nBut, since a finite device, such as the human brain, contains only a\nfinite amount of material, it can only generate a finite number of\nprimitive representations. Next, he observes that thought is\nproductive—that it can, in principle, generate an unbounded\nnumber of semantically distinct representations. This means that to\ngenerate the stock of mental representations corresponding to each of\nthese distinct thoughts, one must have a syntactically and\nsemantically combinatorial system of mental representation of the sort\nfound in a language of thought (LOT). More explicitly, this scheme of\nmental representation must have the following properties: \nThe conclusion of this first stage is, therefore, that a causal theory\nof mental representation requires a LOT. In the other stage of his\nargument, Cummins observes that, for a wide range of objects, their\nperception is mediated by a body of theory. Thus, to perceive\ndogs—for dogs to cause “dogs”—one has to know\nthings such as that dogs have tails, dogs have fur, and dogs four\nlegs. But, to know that dogs have tails, fur, and four legs, one needs\na set of mental representations, such as “tail”,\n“fur”, “four”, and “legs”. Now the\nproblem fully emerges. According to causal theories, having an\n“X” representation requires the ability to detect dogs.\nBut, the ability to detect dogs requires a theory of dogs. But, having\na theory of dogs requires already having a LOT—a system of\nmental representation. One cannot generate mental representations\nwithout already having\n them.[12] \nJason Bridges (2006) argues that the core hypothesis of informational\nsemantics conflicts with the idea that psychological laws are\nnon-basic. As we have just observed, causal theories are often taken\nto offer mere sufficiency conditions for meaning. Suppose, therefore,\nthat we suitably restrict the scope of a causal theory and understand\nits core hypothesis as asserting that all “X”s with the\ncontent X are reliably caused by X. (Nothing in the logic of\nBridges’ argument depends on any additional conditions on a\nputative causal theory of mental content, so for simplicity we can\nfollow Bridges in restricting attention to this simple version.)\nBridges proposes that this core claim of a causal theory of mental\ncontent is a constitution thesis. It specifies what constitutes the\nmeaning relation (at least in some restricted domain). Thus, if one\nwere to ask, “Why is it that all ‘X’s with content X\nare reliably caused by Xs?,” the answer is roughly,\n“That’s just what it is for ‘X’ to have the\ncontent X”. Being caused in that way is what constitutes having\nthat meaning. So, when a theory invokes this kind of constitutive\nrelation, there is this kind of constitutive explanation. So, the\nfirst premise of Bridges’ argument is that causal theories\nspecify a constitutive relation between meaning and reliable causal\nconnection. \nBridges next observes that causal theorists typically maintain that\nthe putative fact that all “X”s are reliably caused by Xs\nis mediated by underlying mechanisms of one sort or another. So,\n“X”s might be reliably caused by dogs in part through the\nmediation of a person’s visual system or auditory system.\nOne’s visual apparatus might causally connect particular\npatterns of color and luminance produced by dogs to “X”s.\nOne might put the point somewhat differently by saying that a causal\ntheorist’s hypothetical “Xs causes ‘X’s”\nlaw is not a basic or fundamental law of nature, but an implemented\nlaw. \nBridges’ third premise is a principle that he takes to be nearly\nself-evident, once understood. We can develop a better first-pass\nunderstanding of Bridges’ argument if, at the risk of distorting\nthe argument, we consider a slightly simplified version of this\nprinciple: \nTo illustrate the principle, suppose we say that gold is identical to\nthe element with atomic number 79, that all gold has atomic number 79.\nThen suppose one were to ask, “Why is it that all gold has the\natomic number 79?” The answer would be, “Gold just\nis the element with atomic number 79.” This would be a\nconstitutive explanation. According to (S), however, this constitutive\nexplanation precludes giving a further mechanistic explanation of why\ngold has atomic number 79. There is no mechanism by which gold gets\natomic number 79. Having atomic number 79 just is what makes gold\ngold. \nSo, here is the argument \nTherefore, by modus ponens on P1 and P2, \nBut, C1 contradicts the common assumption \nRupert (2008) challenges the first premise of Bridges’ argument\non two scores. First, he notes that claims about constitutive natures\nhave modal implications which at least some naturalistic philosophers\nhave found objectionable. Second, he claims that natural scientists do\nnot appeal to constitutive natures, so that one need not develop a\ntheory of mental content that invokes them. \nAlthough philosophers and cognitive scientists frequently propose to\ndispense with (one or another sort of) mental representation (cf.,\ne.g., Stich, 1983, Brooks, 1991, van Gelder, 1995, Haugeland, 1999,\nJohnson, 2007, Chemero, 2009), this is universally accepted to be a\nrevolutionary shift in thinking about minds. Short of taking on board\nsuch radical views, one will naturally want some explanation of how\nmental representations arise. In attempting such explanations, causal\ntheories have been widely perceived to have numerous attractive\nfeatures. If, for example, one use for mental representations is to\nhelp one keep track of events in the world, then some causal\nconnection between mind and world makes sense. This attractiveness has\nbeen enough to motivate new causal theories (e.g. Rupert, 1999, Usher,\n2001, and Ryder, 2004), despite the widespread recognition of serious\nchallenges to an earlier generation of theories developed by Stampe,\nDretske, Fodor, and others.","contact.mail":"ken.aizawa@gmail.com","contact.domain":"gmail.com"}]
