[{"date.published":"2008-11-26","url":"https://plato.stanford.edu/entries/human-genome/","author1":"Lisa Gannett","author1.info":"https://smu.ca/research/profiles/faculty/Gannett65.html","entry":"human-genome","body.text":"\n\n\n\nThe century that opened with rediscoveries of Gregor Mendel's\nstudies on patterns of inheritance in peas closed with a research\nproject in molecular biology heralded as the initial and necessary step\nfor attaining a complete understanding of the hereditary nature of\nhumankind. Both basic science and technological feat, the Human Genome\nProject (HGP) brought to biology a “big science” model\npreviously confined to physics. Although originating and centered in\nthe U.S., laboratories across the globe contributed to the mapping and\nsequencing of the haploid human genome's 22 autosomes and 2 sex\nchromosomes.\n\n\n\nThe official date of completion was timed to coincide with\ncelebrations of the 50th anniversary of James D. Watson and\nFrancis Crick's discovery of the double-helical structure of DNA.\nOn 12 April 2003, heads of government of the six countries which\ncontributed to the sequencing efforts (the U.S., the U.K., Japan,\nFrance, Germany, and China) issued a joint proclamation that the\n“essential sequence of three billion base pairs of DNA of the\nHuman Genome, the molecular instruction book of human life,” had\nbeen achieved (Dept. of Trade 2003). HGP researchers compared their\nfeat to the Apollo moon landing and splitting the atom, foreseeing the\ndawn of a new era, “the era of the genome” (NHGRI\n2003).\n\n\n\nWhat does the “era of the genome” promise? Bruce\nAlberts, president of the National Academy of Sciences, characterized\nthe completed human genome sequence as a “tremendous foundation\non which to build the science and medicine of the 21st\ncentury” (NHGRI 2003). The statement released by the six world\nleaders in April 2003 expressed the hope that this progress in science\nand medicine would establish “a healthier future for all the\npeoples of the globe” (Dept. of Trade 2003). Philosophical\ninterest in the HGP centers on claims and hopes of this sort and raises\na number of questions: How can DNA sequence information provide\nfoundations for scientific and medical knowledge? Who will have access\nto the potential benefits arising from this research, and will such\nbenefits be justly distributed? What possible harms lie ahead?\n\n\n\n\nThis article provides a brief history of the HGP and discusses a\nrange of associated issues that gained the attention of philosophers\nduring the project's planning stages and as it unfolded.\nProminent among philosophical concerns are the conceptual foundations\nof the project and its ethical implications.\n\nHGP at the start \n\nThe HGP began officially in October 1990, but its origins go back\nearlier. In the mid-1980s, three scientists independently came up with\nthe idea of sequencing the entire human genome: Robert Sinsheimer, then\nchancellor of University of California at Santa Cruz, as a way to spend\n$30 million donated to his institution to build a telescope when that\nproject fell through; Salk Institute researcher Rene Dulbecco as a way\nto understand the genetic origins of cancer and other diseases; and the\nDepartment of Energy's (DOE's) Charles DeLisi as a way to\ndetect radiation-induced mutations, an interest of that agency since\nthe atomic bombings of Hiroshima and Nagasaki. Such a project had\nbecome technically feasible due to advances made during the previous\ndecade or two: in the early 1970s, recombinant DNA technologies (use of\nrestriction enzymes to splice DNA, reverse transcriptase to make DNA\nfrom RNA, viral vectors to carry bits of DNA into cells, bacterial\ncloning to multiply quantities of DNA); in the late 1970s, DNA\nsequencing and use of RFLP (restriction fragment length polymorphism)\nmarkers for gene mapping; and in the early to mid-1980s, DNA synthesis,\npulsed-field gel electrophoresis, polymerase chain reaction (PCR), and\nautomated DNA sequencing. \n\nSinsheimer's, Dulbecco's, and DeLisi's idea found\nsupporters among a number of prominent molecular biologists and human\ngeneticists—for example, Walter Bodmer, Walter Gilbert, Leroy\nHood, Victor McKusick, and James D. Watson. However, many molecular\nbiologists expressed misgivings. Especially through 1986 and 1987,\nthere were concerns about the routine nature of sequencing and the\namount of “junk DNA” that would be sequenced, that the\nexpense and big science approach would drain resources from smaller and\nmore worthy projects, and that knowledge of gene sequence was\ninadequate to yield knowledge of gene\n function.[1]\nIn September 1986, committees\nwere established to study the feasibility of a publicly-funded project\nto sequence the human genome: one by the National Research Council\n(NRC) on scientific merit, and one by the Office for Technology\nAssessment (OTA) as a matter of public policy. Both committees released\nreports in 1988. The OTA report, Mapping Our Genes: Genome\nProjects: How Big, How Fast? downplayed the concerns of scientist\ncritics by emphasizing that there was not one but many genome projects,\nthat these were not on the scale of the Manhattan or Apollo projects,\nthat no agency was committed to massive sequencing, and that the study\nof other organisms was needed to understand human genes. The NRC\nreport, Mapping and Sequencing the Human Genome, sought to\naccommodate the scientists’ concerns by formulating\nrecommendations that genetic and physical mapping and the development\nof cheaper, more efficient sequencing technologies precede large-scale\nsequencing, and that funding be provided for the mapping and sequencing\nof nonhuman (“model”) organisms as well. \n\nIt was the DOE that made the first push toward a “Big\nScience” genome project: DeLisi advanced a five-year plan in\n1986, $4.5 million was allocated from the 1987 budget, and recognizing\nthe boost the endeavor would provide to national weapons laboratories,\nSenator Pete Domenici from New Mexico introduced a bill in Congress.\nThe DOE undertaking produced consternation among biomedical researchers\nwho were traditionally supported by the NIH's intramural and\nextramural programs—for example, Caltech's David Botstein\nreferred to the initiative as “DOE's program for unemployed\nbomb-makers” (in Cook-Deegan 1994, p. 98). James Wyngaarden, head\nof the NIH, was persuaded to lend his agency's support to the\nproject in 1987. Funding was in place in time for fiscal year (FY) 1988\nwith Congress awarding the DOE $10.7 million and the NIH\n$17.2\n million.[2]\nThe DOE\nand NIH coordinated their efforts with a Memorandum of Understanding in\n1988 that agreed on an official launch of the HGP on October 1, 1990\nand an expected date of completion of 2005. Total cost estimated by the\nNRC report was $3 billion.  \n\nThe project's specific goals at the outset were: (i) to\nidentify all genes of the human genome (initially estimated to be\n100,000); (ii) to sequence the approximately 3 billion nucleotides of\nthe human genome; (iii) to develop databases to store this information;\n(iv) to develop tools for data analysis; (v) to address ethical, legal,\nand social issues; and (vi) to sequence a number of “model\norganisms,” including the bacterium Escherichia coli,\nthe yeast Saccharomyces cerevisiae, the roundworm\nCaenorhabditis elegans, the fruitfly Drosophila\nmelanogaster, and the mouse Mus musculans. The DOE\nestablished three genome centers in 1988–89 at Lawrence Berkeley,\nLawrence Livermore, and Los Alamos National Laboratories; as Associate\nDirector of the DOE Office of Health and Environmental Research (OHER),\nDavid Galas oversaw the DOE's genome project from April 1990\nuntil he left for the private sector in 1993. The NIH instituted a\nuniversity grant-based program for human genome research and placed\nWatson, co-discoverer of the structure of DNA and director of Cold\nSpring Harbor Laboratory, in charge in 1988. In October 1989, the\nDepartment of Health and Human Services established the National Center\nfor Human Genome Research (NCHGR) at the NIH with Watson at the helm.\nDuring 1990 and 1991, Watson expanded the grants-based program to fund\nseven genome centers for five-year periods to work on large-scale\nmapping projects: Washington University, St. Louis; University of\nCalifornia, San Francisco; Massachusetts Institute of Technology;\nUniversity of Michigan; University of Utah; Baylor College of Medicine;\nand Children's Hospital of Philadelphia. \n\nAs the HGP got underway, a number of philosophers weighed in on its\nscientific merit—in terms of cost, potential impact on other\nareas of research, ability to lead to medical cures, and the usefulness\nof sequence data (Kitcher 1995; Rosenberg 1995; Tauber and Sarkar 1992;\nVicedo 1992). However, of particular interest to philosophers is goal\n(v) concerning ethical, legal, and social issues. At an October 1988\nnews conference called to announce his appointment, Watson, in an\napparently off-the-cuff response to a reporter who asked about the\nsocial implications of the project, promised that a portion of the\nfunding would be set aside to study such issues (Marshall 1996c). The\nresult was the NIH/DOE Joint Working Group on Ethical, Legal, and\nSocial Implications (ELSI) of Human Genome Research, chaired by Nancy\nWexler, which began to meet in September\n 1989.[3]\nThe Joint Working Group\nidentified four areas of high priority: “quality and access in\nthe use of genetic tests; fair use of genetic information by employers\nand insurers; privacy and confidentiality of genetic information; and\npublic and professional education” (Wexler in Cooper 1994, p.\n321). The NIH and DOE each established ELSI programs: philosopher Eric\nT. Juengst served as the first director of the NIH-NCHGR ELSI program\nfrom 1990 to 1994. ELSI was funded initially to the tune of three\npercent of the HGP budget for both agencies; this was increased to four\nand later five percent at the NIH. Map first, sequence later \n\nAs the NRC report had recommended, priority at the outset of the\nproject was given to mapping rather than sequencing the human genome.\nHGP scientists sought to construct two kinds of maps. Genetic maps\norder polymorphic markers linearly on chromosomes; the aim is to have\nthese markers densely enough situated that linkage relations can be\nused to locate chromosomal regions containing genes of interest to\nresearchers. Physical maps order collections (or\n“libraries”) of cloned DNA fragments that cover an\norganism's genome; these fragments can then be replicated in\nquantity for sequencing. The joint NIH-DOE five-year plan released in\n1990 set specific benchmarks: a resolution of 2 to 5 centimorgans (cM)\nfor genetic linkage maps and physical maps with sequence-tagged site\n(STS) markers (unique DNA sequences 100–200 base pairs long) spaced\napproximately 100 kilobases (kb) apart and 2-megabase (Mb) contiguous\noverlapping clones (“contigs”) assembled for large sections\nof the genome. Sequencing needed to be made more efficient and less\ncostly: aims were to reduce sequencing costs to $.50 per base and to\ncomplete 10 million bases of contiguous DNA (0.3 percent of the human\ngenome) but otherwise to focus efforts on the smaller genomes of less\ncomplex model organisms (Watson 1990). HGP goals were facilitated by a\nnumber of technological developments during this initial period. For\nphysical mapping, yeast artificial chromosomes (YACs) introduced in\n1987 (Burke et al. 1987) permitted much larger segments of DNA to be\nordered and stored for sequencing than was possible with plasmid or\ncosmid libraries. A new class of genetic markers, microsatellite\nrepeats, was identified in 1989 (Litt and Luty 1989; Tautz 1989; Weber\nand May 1989); because these sets of tandem repeats of short (either\ndinucleotide, trinucleotide, or tetranucleotide) DNA sequences are more\nhighly polymorphic and detectable by PCR, microsatellites quickly\nreplaced RFLPs as markers of choice for genetic linkage mapping and\nfurnished the STS markers which facilitated the integration of genetic\nand physical maps. Another technological achievement—the combined\nuse of reverse transcription, PCR, and automated sequencing to\nmap expressed genes—led to administrative changes at the NIH\nwhen, in April 1992, Watson resigned from his position as director of\nthe NCHGR following a conflict with NIH director Bernadine Healy over\ngene patenting. In 1991, while working at the NIH, J. Craig Venter\nsequenced small portions of cDNAs from existing libraries to provide\nidentifying expressed sequence tags (ESTs) of 200–300 bases which he\nthen compared to already identified genes from various species found in\nexisting databases (Adams et al.\n 1991).[4]\nWatson disagreed with Healy's decision to\napprove patent applications for the ESTs despite lack of knowledge of\ntheir\n function.[5]\nSoon after Watson's departure, Venter left NIH for the\nprivate\n sector.[6] \n\nFrancis Collins, an MD-PhD whose lab at University of Michigan\nco-discovered genes associated with cystic fibrosis and\nneurofibromatosis and contributed to efforts to isolate the gene for\nHuntington's disease, was appointed by Healy as Watson's\nreplacement, and he began at the NCHGR in April 1993. Collins\nestablished an intramural research program at the NCHGR to complement\nthe extramural program of grants for university-based research which\nalready existed; ELSI remained a grant-funded program. The original\nNIH-DOE five-year plan was updated in 1993. The new five-year plan, in\neffect through 1998, accommodated progress that had been made in\nmapping, sequencing, and technological development (Collins and Galas\n1993). The goal of a 2–5 cM genetic map was expected to be met by the\n1995 target date. The deadline for a physical map with STS markers at\nintervals of 100 kb was extended to 1998; a map with intervals\naveraging 300 kb was expected by 1995 or 1996. Although the goal of\n$.50 per base cost of sequencing was projected to be met by 1996, it\nwas recognized that this would be insufficient to meet the 2005 target\ndate. The updated goal was to build up to a collective sequencing\ncapacity of 50 Mb per year and to have 80 Mb of DNA (from both human\nand model organism genomes) sequenced by the end of 1998. This would be\nachieved by increasing the number of groups working on large-scale\nsequencing and heightening efforts to develop new sequencing\ntechnologies. Accordingly, in November 1995, the U.K.'s Wellcome\nTrust launched a $75 million, seven-year concentrated sequencing effort\nat the Sanger Centre in Cambridge, and in April 1996, the NCHGR awarded\ngrants totaling $20 million per year for six centers (Houston's\nBaylor College of Medicine, Stanford University, The Institute for\nGenomic Research [TIGR], University of Washington-Seattle, Washington\nUniversity School of Medicine in St. Louis, and Whitehead Institute for\nBiomedical Research—MIT Genome Center) to pilot high-volume\nsequencing approaches (Marshall 1996a). \n\nAlthough the HGP's inceptions were in the U.S., it had not\ntaken long for mapping and sequencing the human genome to become an\ninternational venture (see Cook-Deegan 1994). France began to fund\ngenome research in 1988 and had developed a more centralized, although\nnot very well-funded, program by 1990. More significant were the\ncontributions of Centre d’Etudes du Polymorphisme Humain (CEPH)\nand Généthon. CEPH, founded in 1983 by Jean Dausset,\nmaintained a collection of DNA donated by intergenerational families to\nhelp in the study of hereditary disease; Jean Weissenbach led an\ninternational effort to construct a complete genetic map of the human\ngenome using the CEPH collection; later, with funding from the French\nmuscular dystrophy association (AFM), director Daniel Cohen set out to\nconstruct a YAC clone library for physical mapping and oversaw the\nlaunching of Généthon in 1991 as an industrial-sized\nmapping and sequencing operation funded by the AFM. The U.K.'s\ngenome project received its official start in 1989 although Sydney\nBrenner had commenced genome research at the Medical Research Council\n(MRC) laboratory several years before this. MRC funding was\nsupplemented with private monies from the Imperial Cancer Research\nFund, and later, the Wellcome Trust. The Sanger Centre, led by John\nSulston and funded by Wellcome and the MRC, opened in October 1993. A\ncombined four-year, 15-million-euro genome program by the European\nCommunity (E.C.) commenced in 1990. Germany, its citizens all too aware\nof abuses in the name of genetics, lagged behind other European\ncountries: although individual researchers received government funds\nfor genome research in the late-1980s and participated in the E.C.\ninitiative, no actual national genome project was undertaken until 1995\n(Kahn 1996). Japan, ahead of the U.S. in having funded the development\nof automated sequencing technologies since the early 1980s, was the\nmajor genome player outside the U.S. and Europe with several government\nagencies beginning small-scale genome projects in the late-1980s and\nearly- 1990s, but a frequent target of U.S. criticism for the size of\nits investment relative to\n GNP.[7]\nChina was the latecomer on the international\nscene: with 250 million yuan ($30 million) over three years from\ngovernment and industry, the Chinese National Human Genome Center with\nbranches in Beijing and Shanghai opened in July 1998, and was followed\nin 1999 by the Beijing Genomics\n Institute.[8] \n\nAs 1998, the last year of the revised five-year plan and midpoint of\nthe project's projected 15-year span, approached, many mapping\ngoals had been met. In 1994, Généthon completed a genetic\nmap with more than 2000 microsatellite markers at an average spacing of\n2.9 cM and only one gap larger than 20 cM (Gyapay et al. 1994), though\nthe genetic mapping phase of the project did not finally come to a\nclose until March 1996 with publication of comprehensive genetic maps\nof the mouse and human genomes in Nature: the mouse map\nproduced by scientists at Whitehead-MIT Center for Genome Research\ncontained 7,377 genetic markers (both microsatellites and RFLPs) with\nan average spacing of 0.2 cM (Dietrich et al. 1996); the human map\nproduced by scientists at Généthon contained 5,264\nmicrosatellite markers located to 2335 positions with an average\nspacing of 1.6 cM (Dib et al. 1996). Physical mapping was on track: in\n1995, a physical map with 94 percent coverage of the genome and STS\nmarkers at average intervals of 199 kb was published (T. Hudson et al.\n1995), as was CEPH's updated physical map of 225 YAC contigs\ncovering 75 percent of the genome (Chumakov et al. 1995); however,\nbacterial artificial chromosomes (BACs), developed in DOE-funded\nresearch at Caltech in 1992 (Shizuya et al. 1992), soon replaced YACs\nbecause of their greater stability in propagating DNA for sequencing.\nSequencing itself presented more of a challenge. The genomes of the\nsmallest model organisms had been sequenced. In April 1996, an\ninternational consortium of mostly European laboratories published the\nsequence for S. cerevisiae which was the first eukaryote\ncompleted, with 12 million base pairs and 5,885 genes and at a cost of\n$40 million (Goffeau et al. 1996). In January 1997, University of\nWisconsin researchers completed the sequence of E. coli with\n4,638,858 base pairs and 4,286 genes (Blattner et al. 1997). However,\ndespite ramped-up sequencing efforts over the past several years at the\nSanger Centre and NHGRI-funded centers (the NCHGR had been elevated to\nthe status of a research institute in 1997 and renamed the National\nHuman Genome Research Institute), with only three percent of the human\ngenome sequenced, sequencing costs hovering at $.40/base, and the\ndesired high-output not yet achieved by the sequencing centers, and\nabout $1.8 billion spent, doubts existed about whether the target date\nof 2005 could be met. \n\nSuddenly, the HGP found itself challenged by sequencing plans from\nthe private sector. In May 1998, TIGR's Venter announced he would\npartner with Michael Hunkapiller's company Applied Biosystems\n(ABI), a division of Perkin-Elmer Corporation which manufactured\nsequencing machines, to form a new company which would sequence the\nentire genome in three short years and for a fraction of the cost. The\nforeseen profits rested in the construction of a\n“definitive” database that would outdo Genbank by\nintegrating medical and other information with the basic sequence and\npolymorphisms. The company, based in Rockville, MD and later named\nCelera Genomics, planned to use “whole-genome shotgun”\n(WGS) sequencing, an approach different from the HGP's. The HGP\nconfined the shotgun method to cloned fragments already mapped to\nspecific chromosomal regions: these are broken down into smaller bits\nthen amplified by bacterial clones, sequences are generated randomly by\nautomated machines, and computational resources are used to reassemble\nsequence using overlapping areas of bits. Shotgunning is followed by\npainstaking “finishing” to fill in gaps, correct mistakes,\nand resolve ambiguities. What Celera was proposing for the shotgun\nmethod was to break the organism's entire genome into millions of\npieces of DNA with high-frequency sound waves, sequence these pieces\nusing hundreds of ABI's new capillary model machines, and\nreassemble the sequences with one of the world's largest civilian\nsupercomputers without the assistance provided by the preliminary\nmapping of clones to chromosomes. When WGS sequencing was considered as\na possibility by the HGP, it was rejected because of the risk that\nrepeat sequences would yield mistakes in\n reassembly.[9]\nBut Venter by this time had\nsuccessfully used the technique to sequence the 1.83 million nucleotide\nbases of the bacterium Hemophilus influenzae—the first\nfree-living organism to be completely sequenced—in a year's\ntime (Fleischmann et al.\n 1995).[10] Race to the genome \n\nThe race to sequence the genome was on. The publicly-funded\nscientists downplayed the media image of a race often over the next\ncouple of years, but they were certainly propelled by worries that\nfunding would dry up before the sequence was complete given private\nsector willingness to take over and that the sequence data would become\nproprietary information—the Bermuda Accord, agreed to in February\n1996 by the world's major sequencing laboratories which at the\ntime included Venter's TIGR, required the public release of\nsequence data every 24 hours. Wellcome more than doubled its funds to\nthe Sanger Centre (to £205 million) and the center changed its\ngoal from sequencing one-sixth of the genome to sequencing one-third,\nand possibly one-half (Dickson 1998). The NHGRI and DOE published a new\nfive-year plan for 1998-2003 (Collins et al. 1998). The plan moved the\nfinal completion date forward from 2005 to 2003 and aimed for a\n“working draft” of the human genome sequence to be\ncompleted by December 2001. This would be achieved by delaying the\nfinishing process, no longer going clone-by-clone to shotgun,\nreassemble, and finish the sequence of one clone before proceeding to\nthe next. A physical map of 41,664 STS markers was soon published\n(Deloukas et al. 1998), and so the physical mapping goal was met, but\nwith only six percent of the human genome sequence completed, the plan\ncalled for new and improved sequencing technologies which could\nincrease the sequencing capacity from 90 Mb per year at about $.50 per\nbase to 500 Mb per year at no more than $.25 per base. Goals for\ncompleting the sequencing of the remaining model organisms were also\nset: December 1998 for C. elegans which was 80 percent\ncomplete, 2002 for D. melanogaster which was nine percent\ncomplete, and 2005 for M. musculus which was still at the\nphysical mapping stage. \n\nAn interim victory for the publicly-funded project followed when, on\nschedule, the first animal sequence, that of C. elegans with\n97 million bases and 19,099 genes, was published in Science in\nDecember 1998 (C. elegans Sequencing Consortium 1998). This\nwas the product of a 10-year collaboration between scientists at\nWashington University (headed by Bob Waterston) and the Sanger Centre\n(headed by John Sulston), carried out at a semi-industrial scale with\nmore than 200 people employed in each lab working around the clock. In\nMarch 1999, the main players—the NHGRI, Sanger Centre, and\nDOE—advanced the date of completion of the “working\ndraft”: five-fold coverage of at least 90 percent of the genome\nwas to be completed by the following spring (Pennisi 1999; Wadman\n1999). This change reflected improved output of the new model of\nautomated sequencing machines, diminished sequencing costs at $.20 to\n$.30 per base, and the desire to speed up the release of medically\nrelevant data. NHGRI would take responsibility for 60 percent of the\nsequence, concentrating these efforts at only three centers with\nBaylor, Washington University, and Whitehead-MIT sharing $81.6 million\nover the ensuing 10 months; 33 percent of the sequence would be the\nresponsibility of the Sanger Centre whose funds from Wellcome increased\nfrom $57 million to $77 million for the year; and the remaining\nsequence would be supplied by the DOE's Joint Genome Institute\n(JGI) in Walnut Creek, CA into which its three centers had merged in\nJanuary 1997. The smaller international centers involved in sequencing\nwere not consulted on this restructuring, but were later brought on\nboard on the condition that they could keep up with the pace. The first\nchromosomes to be completed (and this was to finished, not working\ndraft, standards) were the two smallest: the sequence for chromosome 22\nwas published by scientists at the Sanger Centre and partners at\nUniversity of Oklahoma, Washington University in St. Louis and Keio\nUniversity in Japan in December 1999 (Dunham et al. 1999); the sequence\nfor chromosome 21 was published by an international consortium of\nmostly Japanese and German labs—half at RIKEN—in May 2000\n(Hattori et al. 2000). The remaining chromosomes lagged behind, though\nthe DOE announced completion of working drafts of chromosomes 5, 16,\nand 19 with three-fold coverage in April 2000. The progress made by the\npublicly-funded project could be monitored because sequence data were\nreleased at 24-hour intervals, but Celera's progress was more\ndifficult to assess. HGP scientist Maynard Olson charged that Celera\nwas doing “science by press conference” (in Davies 2002, p.\n153). Certainly, Celera's press conferences gave the impression\nit was ahead in the race: on 10 January 2000 the company announced\ncompletion of 90 percent of the human genome sequence, and on 6 April\n2000 the company announced completion of three-fold coverage of the DNA\nof one male donor. But there was also evidence that Celera did remain a\nthreat: the validity of the WGS sequencing approach was demonstrated in\nMarch 2000 when Celera and the (publicly-funded) Berkeley Drosophila\nGenome Project published the sequence of D. melanogaster of\nabout 180 Mb (Adams et al. 2000). \n\nIn June 2000, the contest ended in what appeared to be a tie for the\nprize, but was more an arranged truce. On 26 June 2000, Collins,\nVenter, and the DOE's Ari Patrinos joined U.S. President Bill\nClinton (and British Prime Minister Tony Blair by satellite link) at a\nWhite House press conference to announce that the human genome had been\nsequenced. That Collins and Venter even shared the limelight on that\nday was itself a tremendous feat. The negotiated draw at the finish\nline permitted HGP scientists to save face and their upstart competitor\nto minimize the risk of alienating university-based researchers and\nlosing their business. The agreement between parties included eventual\nsimultaneous publication of their results. However, not only had\nresults not yet been readied for publication, neither of the two\nsequence maps was complete (Pennisi 2000). The HGP had not met its\nprevious year's goal of a working draft covering 90 percent of\nthe genome: Collins reported that ordered BACs existed for 97 percent\nof the genome and that BACs for 85 percent of the genome had been\nsequenced, with 24 percent of the genome sequence in finished form, 22\npercent of the genome sequence in near-finished form, and 38 percent of\nthe genome sequence in provisional form. Assisted by its\nresearchers’ access to HGP data stored on public databases,\nCelera's efforts were accepted as being further along: the\ncompany's press release that day announced completion of the\n“first assembly” of the human genome with 99 percent\ncoverage. An editorial in Nature described the fanfare of 26\nJune as an “extravagant” example—one reaching\n“an all-out zenith or nadir, according to taste”—of\nscientists making public announcements not linked to peer-reviewed\npublication, here to bolster share prices (Celera) and for political\neffect (the HGP) given the “months to go before even a draft\nsequence will be scientifically useful” (Anonymous 2000, p. 981).\nThe peer-reviewed publications came almost eight months later. Plans\nfor joint publication in Science broke down when terms of\nagreement over data release could not be negotiated:\nScience's editors were willing to publish Celera's\nfindings without Venter meeting the standard requirement that the\nsequence data be submitted to GenBank; Celera would instead make the\ndata available on its own website. Press conferences in London and\nWashington, D.C. on 12 February preceded publications that\nweek—by HGP scientists in Nature on 15 February 2001 and\nby Venter's team in Science on 16 February 2001. The HGP\ndraft genome sequence, prepared based on map and sequence data\navailable on 8 October 2000, covered about 94 percent of the genome,\nwith about 25 percent in the finished form already attained for\nchromosomes 21 and 22. Indeed, the authors themselves described it as\n“an incomplete, intermediate product” which “contains\nmany gaps and errors” (International Human Genome Sequencing\nConsortium 2001, p. 871). The results published by Celera, based on\nassemblies completed on 1 October 2001 using two different\ncomputational methods, had 84–90 percent of the genome covered by\nscaffolds at least 100 kb in length, with the composition of the\nscaffolds averaging 91–92 percent sequence and 8–9 percent gaps,\nleaving 93,857–105,264 gaps in total (Venter et al. 2001). In the end,\nCelera's published genome assembly made significant use of the\nHGP's publicly available map and sequence data, which left open\nthe question whether WGS sequencing alone would have\n worked.[11] \n\nSince the gaps in the sequence were unlikely to contain genes, and\nonly genes as functional segments of DNA have potential commercial\nvalue, Celera was happy to move on and leave these gaps for the HGP\nscientists to fill in. Celera was faced with deciding what sort of\ncompany it would be: sequences from three different mouse strains were\nadded to help attract subscribers to its database, and a brief foray\nwas made into proteomics, but Venter resigned as CEO in January 2002\nwith the company's decision to focus on drug discovery rather\nthan information (Davies 2002). Despite being timed to coincide with\ncelebrations of the 50th anniversary of the Watson-Crick\ndiscovery of the double-helical structure of DNA, there was less\nfanfare surrounding the official date of completion of the HGP in April\n2003, two years earlier than had been anticipated at the time of its\nofficial launch in October 1990, and several months earlier than called\nfor in the most recent five-year plan. Americans had terrorism and war\non their minds. In the end, sequencing—the third phase of the\npublicly-funded project—was carried out at 16 centers in six\ncountries by divvying up among them sections of chromosomes for\nsequencing. 85 percent of the sequencing, however, was done at the five\nmajor sequencing centers (Baylor, Washington University, Whitehead-MIT,\nSanger Center, and DOE's JGI), with the Sanger Centre responsible\nfor nearly one-third. The cost was lower than anticipated, with $2.7\nbillion spent by U.S. agencies and £150 million spent by Wellcome\nTrust. The “finished” reference DNA sequence for Homo\nsapiens—all 3.1 billion nucleotide bases—is publicly\naccessible on the Internet\n (NCBI Human Genome Resources).\nIf the As, Ts, Cs, and Gs of the genome sequence were printed in\nstandard type, they would fill 75,490 pages of the New York\nTimes (Wade\n 2003).[12] \n\nIn the project's early years, Norton Zinder, who chaired the\nNIH's Program Advisory Committee for the Human Genome,\ncharacterized it in this way: “This Project is creating an\ninfrastructure for doing science; it's not the doing of the\nscience per se. it will provide the biological community with the basic\nmaterials for doing research in human biology” (in Cooper 1994,\np. 74). The published human genome reference sequences are part of that\ninfrastructure, serving as tools for investigating human genetic\nvariation. So far gene identification has been successful for the\nsingle genes of large effect implicated in rare Mendelian disorders.\nDifficulties arise for identifying the multiple genes of variable\neffect that interact with nongenetic factors in more common, complex\nconditions and for understanding the physiological processes associated\nwith the development of these phenotypes. One approach to overcoming\nthese difficulties focuses on relatively genetically homogeneous\npopulations with members for whom extensive clinical data are\navailable, as in the case of the Icelandic genome project, and\nbasically extends the methods used for linkage mapping for diseases\nwithin families. Another approach is to conduct large-scale\ncase-control association studies between phenotypes of interest and\ngenetic markers. For both these approaches, single nucleotide\npolymorphisms (SNPs) are preferred as markers over the microsatellites\nused for genetic and physical mapping by the HGP. Worried about the\nprivate sector's efforts to patent SNPs, which would make them\ncostly to use for research, the NHGRI-DOE's five-year plan for\n1998–2003 included the goal of mapping 100,000 SNPs by 2003 (Collins et\nal. 1998). The development of a public database of SNPs received a $138\nmillion push from the International HapMap Project, a three-year\npublic-private partnership completed in 2005 that mapped variation in\nfour population groups. The NHGRI's involvement in the HapMap\nProject was part of the continuing leadership role in genome research\nit envisioned for itself upon completion of the HGP (Collins et al.\n2003). Other projects include ENCODE, which began as a pilot project to\nstudy gene function by analyzing one percent of the genome and is now\nlooking at the remaining 99 percent, and more recently, clinENCODE, in\nwhich disease risk is being calculated for 400 people based on the\ncorresponding one percent of the genome as a step toward personalized\nmedicine. However, the infrastructure of mapping and sequencing\ntechnologies developed as part of the HGP—especially the ability\nto sequence entire genomes of organisms—has changed the way\nbiology, not just human biology, is done. It is now recognized that\ngenome structure by itself tells us only so much. In functional\ngenomics, the interest is in how genomes—not just individual\ngenes anymore—function. By studying the coordinated expression of\nthe genome's various segments in different tissues at different\ntimes, researchers are coming to better understand organismal\ndevelopment. In comparative genomics, the study of genomic structure\nand function in different species is bringing about similar gains in\nunderstanding evolution. And genomics is now complemented by the field\nof proteomics which studies the structure and function of all of an\norganism's proteins, called the proteome. \n\nAt the June 2000 White House press conference, President Clinton\ncompared the feat of mapping and sequencing the human genome to the\nmapping of the Northwest Passage by early-nineteenth century explorers\nMeriwether Lewis and William Clark: \nNearly two centuries ago, in this room, on this floor, Thomas\nJefferson and a trusted aide spread out a magnificent map, a map\nJefferson had long prayed he would get to see in his lifetime. The aide\nwas Meriwether Lewis and the map was the product of his courageous\nexpedition across the American frontier, all the way to the\nPacific. It was a map that defined the contours and forever\nexpanded the frontiers of our continent and our imagination. \n\nToday the world is joining us here in the East Room to behold the\nmap of even greater significance. We are here to celebrate the\ncompletion of the first survey of the entire human genome.\nWithout a doubt, this is the most important, most wondrous map ever\nproduced by humankind. \n\nClinton continued on to say that he considered this\n“epoch-making triumph of science and reason” to be merely\na starting point. Three “majestic horizons” lay\nimmediately ahead: by 2003, production of a final version of the\nsequence map that would be complete and accurate; biotechnological\ndevelopment in the private sector based on the identification of all\nhuman genes and their functions; and ethical respect for “our\noldest and most cherished human values” to ensure that genome\nscience benefits “all citizens of the world,” protects\nprivacy, and prevents discrimination (White House 2000). \n\nClinton's comparison of the human genome sequence map to Lewis\nand Clark's map of the Northwest Passage is perhaps less\ngratuitous than it might appear. Some members of the 1804–1806\nexpedition, the “Corps of Discovery,” sought to obtain\nnatural scientific and anthropological knowledge over the course of\ntheir travels. The HGP shares the Enlightenment ideals of this period,\nespecially the faith in scientific progress, the goal of systematic\nknowledge, and the confidence that universal benefits for humanity\nwould ensue from the scientific pursuit of truth. Scientist Leroy Hood\nexpressed the belief that “we will learn more about human\ndevelopment and pathology in the next twenty-five years than we have in\nthe past two thousand” (1992, p. 163). He predicted that the HGP\nwould facilitate movement from a reactive to preventive mode of\nmedicine which would “enable most individuals to live a normal,\nhealthy, and intellectually alert life without disease” (p. 158).\nThe Lewis and Clark journey was an important symbol for encouraging\nAmericans to move westward (the frontier was declared gone by 1890);\nsimilarly, “getting” the genome was represented by HGP\nproponents as a “frontier” of knowledge that, like the moon\nlanding, needed to be conquered. But most important are the colonialist\nand economic aims associated with this early nineteenth-century\n“voyage of discovery.” Jefferson sought to establish a U.S.\npresence beyond its borders, in lands long inhabited by peoples\nindigenous to the Americas and to which Spain had already staked its\nclaim. He made clear to Lewis that the principal aim of the journey was\ncommercial: “The Object of your mission is to explore the\nMissouri river & such principal stream of it as by it's course and\ncommunication with the waters of the Pacific ocean, whether the\nColumbia, Oregon, Colorado or any other river may offer the most direct\n& practicable water communication across this continent for the\npurpose of commerce”\n (Discovering Lewis & Clark).\nThe infrastructure to be developed with the HGP was similarly presented\nas an opportunity to “secure the leadership of the United\nStates in biotechnology and present U.S. industry with a wealth of\nopportunities” (Hood 1992, p. 163). Legislative changes were\nenacted in the 1980s to encourage the commercial development of\nfederally funded research: universities and other nonprofit\ninstitutions were allowed to apply for patents on such research and tax\nincentives were provided to the private sector to encourage investment.\nAlthough Lewis and Clark depended extensively throughout their journey\non the assistance of Indians and French traders they encountered, they\nregarded the lands they covered as “virgin territory” that\nawaited the arrival of “civilized men” to be named and\nclaimed. Similar attitudes are implicated in controversies over the\ncommercialization of genomics research and intellectual property and\npatent rights: organizations representing indigenous peoples charge\nthat the patenting of human genes and cell lines is a continuation of\nthe “bioprospecting” and “biopiracy” carried\nout by multinational corporations in securing patents on medicinal and\nfood uses of plants which have been long a part of traditional\nknowledge (Shiva 1996). \n\nIn the early years of the HGP, the DOE's David Galas expressed\nskepticism that ELSI-sorts of concerns were anything new: “there\nare no new problems. Issues concerning privacy,\nconfidentiality, and discrimination will become much more pressing once\nthe Genome Project generates the tools to diagnose diseases\npresymptomatically. The basic problems, however, are not\nnew—they will simply be exacerbated” (in Cooper 1994, p.\n167). Although legal scholar George Annas agreed there were no new\nproblems, he argued that the combination and degree of problems\ninvolved did make the HGP unique: “there are probably no\nunique issues raised by the Human Genome Initiative. On the\nother hand, this project raises all of the issues in a much\nmore focused manner (certainly a difference in degree if not in kind),\nand the fact that all of these issues are implicated in the project may\nitself make the project societally unique” (1990, p. 640). Many\nof the issues are of interest to philosophers: these include conceptual\nquestions pertaining to scientific knowledge itself and the ethical\nramifications of such knowledge and related technological developments.\nPhilosophers of science, ethicists, political theorists and\nphilosophers working in other areas have benefited from ELSI-related\nfunding. There is now a vast literature on human genome-related topics,\nand this entry can do no more than provide a synopsis regarding what\nquestions have been asked, what range of responses has been offered,\nand what remains for philosophical attention and debate. \n\nBets placed during the HGP over how many genes would be discovered,\nas well as surprise expressed when far fewer than the original estimate\nwere found (about 25,000–30,000 rather than 100,000—the rice\ngenome apparently has more genes!) (Normile and Pennisi 2002; Pennisi\n2003), suggest that “gene”—a term introduced by\nWilhelm Johannsen in 1909—names a well-defined concept. The\nreport that because of alternate splicing each gene is responsible for\nthree or four proteins makes the same assumption. As does drawing\ndistinctions between normal and abnormal genes, or seeking to isolate\ndisease genes. The assumption is not very well substantiated, however.\nPhilosophers of biology recognize that the genes of classical genetics,\nmolecular genetics, evolutionary genetics, and more recently\ndevelopmental genetics do not necessarily map onto each\n other.[13]\nDifficulties\narriving at a definitive gene concept arise even when we confine\nourselves to contemporary molecular biology. Evelyn Fox Keller (2000)\npoints out an irony which has ensued from the HGP's successes:\neven though gene-talk is more pervasive than ever in the popular and\nscientific presses, the concept of the gene, whether defined\nstructurally or functionally, has been “radically\nundermined” (p. 5). Keller provides this description of current\nlaboratory practices: “As we listen to the ways in which the term\nis now used by working biologists, we find that the gene has become\nmany things—no longer a single entity but a word with great\nplasticity, defined only by the specific experimental context in which\nit is used” (p. 69). \n\nRecent philosophical efforts to define genes have sought to capture\nthese practices. C. Kenneth Waters (1994) recognizes that specific\nresearch contexts determine whether genes are considered to include\nintrons as well as exons, or regulatory or promoter regions as well as\nopen reading frames (ORFs), but argues that what remains\n“fundamental” across these contexts is the concept of a\ngene as a stretch of DNA the linear sequence of which provides a\ntemplate for a gene product, whether mRNA transcript or polypeptide.\nBecause of problems posed for Waters’ account by mRNA splicing\nand editing, Eva Neumann-Held (1999) recommends replacing the\n“classical molecular gene concept” of a stretch of DNA\ncoding for a single polypeptide with a “molecular process gene\nconcept” which includes not just the relevant stretches of DNA\nbut the entire cellular context in which polypeptides are produced.\nLenny Moss (2003) identifies two gene concepts: the preformationist\ngene, Gene-P, defined by its relationship to a phenotype, is of\ninstrumental utility for molecular geneticists—for example, the\nBRCA1 gene is used to predict breast cancer risk; the epigenesist gene,\nGene-D, defined by its molecular sequence, serves as a\n“developmental resource” in providing a template for RNA\nand protein synthesis but is indeterminate with respect to phenotype\nsince this depends on other developmental resources and the cellular\nand extracellular\n contexts.[14]\nPaul Griffiths and Karola Stotz (2006)\ndistinguish three gene concepts: “instrumental genes”\nremain important in molecular genetics when relationships between\ngenotype and phenotype are under investigation; “nominal\nmolecular genes” are specific DNA sequences annotated by\nresearchers as genes for structural reasons such as presence of ORFs;\n“postgenomic molecular genes” are not defined by structure\nbut “by the way DNA sequences are used in particular cellular and\nbroader contexts” (p. 515). \n\nGiven this context-dependence in what genes are considered to be and\ndo, it seems that pluralism has become the order of the day, for genes\nas for species. Along these lines, John Dupré (2004) advocates\n“an atheoretical pluralism” that abandons any pretence to a\n“theoretical core to the concept”: simply, “a gene is\nany bit of DNA that anyone has reason to name and keep track of”\n(pp.\n 332–333).[15]\nKeller (2000) agrees that the theoretical\nimportance of genes has faded; she writes: “it seems evident that\nthe primacy of the gene as the core explanatory concept of biological\nstructure and function is more a feature of the twentieth century than\nit will be of the twenty-first” (p. 9). She forecasts the\nemergence of new language; this is a situation for which Philip Kitcher\nbelieved molecular biology was ripe even 15 years ago when he wrote:\n“it is hard to see what would be lost by dropping talk of genes\nfrom molecular biology and simply discussing the properties of various\ninteresting regions of nucleic acid” (1992, p. 130). Keller\nbelieves that gene-talk has served a purpose though, providing a\nflexibility which permits communication across those specific\nexperimental practices within which “gene” attains\nprecision. Hans-Jörg Rheinberger (2000) takes this argument one\nstep further: gene concepts are not merely useful in spite of their\nambiguity, they are useful in virtue of their ambiguity because, as\n“tools of research, they must reach out into the realm of what we\ndo not yet know” (p. 223). He reminds us that this is nothing\nnew: “The spectacular rise of molecular biology has come about\nwithout a comprehensive, exact, and rigid definition of what a gene\nis” (p. 222). Keller's and Rheinberger's views\npresent an evident challenge to philosophical intuitions that\nscientific practice is furthered by arriving at precise definitions of\nbasic concepts. \n\nEarly in the debates surrounding plans for the HGP, questions arose\nconcerning what it means to map and sequence the human\ngenome—“get the genome,” as Watson (1992) put it.\nAbout these concerns, McKusick (1989) wrote: “The question often\nasked, especially by journalists, is ‘Whose genome will be\nsequenced?’ The answer is that it need not, and surely will not,\nbe the genome of any one person. Keeping track of the origin of the DNA\nthat is studied will be important, but the DNA can come from different\npersons chosen for study for particular parts of the genome” (p.\n913). The HGP and Celera reference sequences are indeed composites\nbased on chromosomal segments that originate from different\nindividuals: the sequence in any given region of the genome belongs to\na single individual, but sequences in different regions of the genome\nbelong to different individuals. However, in both cases, the majority\nof the sequence originates from just one person. As HGP sequencing\nefforts accelerated, concerns arose that only four genomes, a couple of\nwhich belonged to known laboratory personnel, were being used for\nphysical mapping and sequencing (Marshall 1996b). The decision was made\nto construct 10 new clone libraries for sequencing with each library\ncontributing about 10 percent of the total DNA. In the end, 74.3\npercent of the total number of bases sequenced was derived from a\nsingle clone library—that of a male, presumably from the Buffalo\narea; seven other clone libraries contributed to an additional 17.3\npercent of the sequence (International Human Genome Sequencing\nConsortium 2001, p. 866). A similar proportion—close to 71\npercent—of the Celera sequence belongs to just one male even\nthough five ethnically diverse donors were selected; incredibly enough,\nrumors have been confirmed that this individual is Venter himself\n(McKie 2002). \n\nThe deeper question, of course, is how we might understand a single\nhuman genome sequence, a composite that belongs to no actual individual\nin its entirety and only a handful of individuals in its parts, to be\nrepresentative of the entire species. This seems to ignore the\nextensive genetic variability which exists. The functional equivalence\nof many DNA polymorphisms led two early critics of the HGP to argue\nthat “there simply is no such entity as a ‘representative\nsequence’ or the human (or any) genome” making it\n“fallacious and even dangerous to call any one\n‘normal’” (Sarkar and Tauber 1991, p. 691). Another\ncritic pointed out that problems with the idea of a representative\nsequence persist even when consideration is limited to DNA differences\nthat are not functionally equivalent but related to health and disease:\nthe sequence will contain unknown defective genes (since no one,\nincluding donors, is free of these), there is a heterogeneity of\nmutations even in so-called single gene diseases, and it is impossible\nto identify the genetic basis of a disorder simply by comparing the\nsequences of sick and well people since there will be many differences\nbetween them (Lewontin 2000 [1992]). For Gilbert (1992), these\ncriticisms of representativeness arise from a failure to appreciate the\ndifference between the approaches of molecular biologists who attend to\nsimilarities and evolutionary biologists who attend to differences\nwithin the species: “The human genome project … is\ndirected toward a molecular biologist's view of a species rather\nthan a population biologist's view. The latter views a species as\nthe envelope of all possible variants that can breed together; the\nimportance of that envelope is that different aspects of a species\npopulation will be drawn forth if you change the environment. Molecular\nbiologists generally view the species as a single entity, sharply\ndefined by a set of genes and a set of functions that makes up that\nentity” (p. 84). Gilbert held that the two approaches are\nconsistent with each other, but many evolutionist critics of the\nHGP—both scientists and philosophers—did not, deriding the\naims of mapping and sequencing the human genome as a throwback to\nanti-evolutionary, preDarwinian, typological, and\nessentialist\n thinking.[16]\nThe\nfunctional approach of molecular biologists alluded to by Gilbert is\nsaid to represent genetic variation in improperly normative ways,\nwhereas “in evolutionary biology, variation is not the same as\ndeviation” (Hull 1994, p. 208). When molecular geneticists view\nmutations as abnormal, not in the sense that they are rare or a change\nin form, but as “errors” in the genetic code or\n“damage” to the genome's proper structure, they\nimpose an arbitrary a priori categorization: “it is genetic\n‘errors’ that made us as a biological species: we humans\nare integrated aggregates of such ‘errors.’ Genetic\nvariation is the source of evolution; it is the reason why there could\nbe primates and not just protists or their precursors” (Limoges\n1994, p. 124). \n\nThere are related worries that the human genome reference sequence\nwill arbitrate a standard of genetic normality; for example, the\napplication of concepts like “genetic error” and\n“damage” to the genome institutes a call for correction or\nrepair (Limoges 1994; also Murphy 1994). McKusick (1989) has defended\nthe HGP's approach as “consistent with that of most\nbiological research which depends on a few, and even on single\nindividuals, to represent the whole, and with the fact, recognized by\ngeneticists, that there is no single normal, ideal, or perfect\ngenome” (p. 913). However, the normal-abnormal distinction is\nfundamental to the structure-function studies of proximate fields of\nbiology like physiology and molecular genetics, and while McKusick is\nno doubt correct to say that geneticists accept that there is no single\nnormal, ideal, or perfect genome, this does not mean that individual\nDNA sequences are not constituted as normal or abnormal based on their\nfunctional significance or that entire genomes are not deemed to fall\ninside or outside of an acceptable range. Indeed, the 1988 OTA report\non the HGP recommends the “eugenic use of genetic information\n… to ensure … that each individual has at least a modicum\nof normal genes” (p. 85). It is little wonder that many worry\nthat as an increasing number of mutations are identified and tested\nfor, the range of what is considered normal may narrow, with diminished\ntolerance for those people who lie outside this range. And there can be\nno reassurance that judgments of health and disease, normality and\nabnormality, manage to escape normativity by being transported to the\nlevel of the genome; instead, they carry with them any social values\nand cultural biases that are implicated at the higher level. Says\ncritic Ruth Hubbard (in Holloway 1995, p. 50): “I have gone out\non a limb by saying that most people in our culture are very judgmental\nabout women who terminate a pregnancy because of sex. How different is\nthat from terminating a pregnancy because of Down syndrome?” \n\nWith the HGP reference sequence available as a basis for comparison,\nattention has shifted to the genetic variation within the species that\nevolutionist critics accused the project at the outset of ignoring.\nHumans have been found to be 99.9 percent alike, with common sequence\nvariants occurring every 1000 bases. There is interest in identifying\nthe sites of the genome where variation occurs, the frequency of these\ndifferences, and their significance. The social significance attaching\nto such research was foreseen during the early years of the project.\nScientist David Baltimore predicted that the HGP would reveal that the\nbelief that “we are all equal, all the same” is a myth:\n“We are going to have to come to terms with the fact that we are\nall born with different talents and tendencies” (in Cooper 1994,\np. 320). Similarly, philosopher Marc Lappé (1994) raised the\npossibility that the HGP could reveal group differences—with\nsequences localized to particular groups or varying in frequency among\ngroups—and that any such differences in the genetic lottery would\nraise significant ethical implications for health care and\nsocial\n policy.[17]\nBut\nthe conceptualization of this variation also presents\nchallenges—for example, in distinguishing between normal and\nabnormal genetic variation (Gannett 2003a), or drawing population\nboundaries in the constitution of individual versus group differences\n(Gannett 2003b). Pharmaceuticals are the most powerful engine driving\npost-HGP diversity research, and though “personalized\nmedicine” was touted as a benefit of the HGP, en route, a detour\nvia the study of group genetic differences has been taken. For example,\nthe International HapMap Project, in order to compile a map adequately\ndense with SNP markers to permit the identification of genes implicated\nin common diseases and drug responses, sampled the DNA of four\npopulations (European-Americans in Utah, Yoruba in Ibadan, Nigeria,\nJapanese in Tokyo, and Han Chinese in\n Beijing).[18]\nLikely due to lessons learned\nfrom the difficulties experienced by the Human Genome Diversity Project\n(see Reardon 2004), attempts were made to involve representatives of\nthese groups in the planning of research through “community\nengagement” and “community consultation.” These\nefforts raise conceptual questions not only about the relations between\nwhat are ostensibly distinct social and biological groups (Gannett\n2003b, Juengst 1998), but what makes a “community” (Davis\n2000). Now that “group” genetic differences have become of\ninterest to more than just evolutionary biologists and population\ngeneticists, impetus is provided to longstanding debates about whether\nrace is biologically real or socially constructed and more recent ones\nconcerning the appropriateness of the use of racial categories in\nbiomedical research (Gannett 2005; Root 2003). \n\nVarious HGP proponents told us that we would discover “our\nhuman essence” in the genome. According to Dulbecco (1986),\n“the sequence of the human DNA is the reality of our\nspecies” (p. 1056); Gilbert is quoted as saying “sequencing\nthe human genome is like pursuing the holy grail” (in Lee 1991,\np. 9); on the topic of his decision to dedicate three percent of HGP\nfunds to ELSI, Watson writes: “The Human Genome Project is much\nmore than a vast roll call of As, Ts, Gs, and Cs: it is as precious a\nbody of knowledge as humankind will ever acquire, with a potential to\nspeak to our most basic philosophical questions about human nature, for\npurposes of good and mischief alike” (with Berry 2003, p.\n172). \n\nThere are theological worries about a genetic reductionism that\nsuggests that we are no more than our smallest material parts—the\nbits of DNA that make up the genome. For example, Leon Kass, chairman\nof the President's Council on Bioethics from 2001 to 2005,\ndecries, with arrival of “the age of genetic technology,”\n“the erosion, perhaps the final erosion, of the idea of man as\nnoble, dignified, precious or godlike, and its replacement with a view\nof man, no less than of nature, as mere raw material for manipulation\nand homogenization” (2002, p. 138). Collins, an evangelical\nChristian, doesn’t share such worries; he is quoted in the\nLos Angeles Times as saying: “God is not threatened by\nall this. I think God thinks it's wonderful that we puny creatures\nare going about the business of trying to understand how our\ninstruction book works, because it's a very elegant instruction\nbook indeed” (Gosselin 2000). Of course, this particular\nreligious world view is countered by an evolutionist one held by other\nscientists. Gilbert's “holy grail” is not so holy\nafter all; he believes that the HGP reveals our place amidst the\ninterconnectedness of all life forms: “The data base of the human\ngenome, coupled with our knowledge of the genetic makeup of model\norganisms, promises to reveal patterns of genes and to show us how we\nourselves are embedded in the sweep of evolution that created our\nworld” (1992, p. 97). \n\nA more secular philosophical concern about essentialism is tied to\nlongstanding debates in philosophy of biology about species (see\nErefshefsky 1992). Gilbert (1992) foresaw from the HGP a DNA-based\ndefinition of Homo sapiens: “At the end of the genome\nproject, we will want to be able to identify all the genes that make up\na human being. For example, we will compare the sequences of the human\nand the mouse and be able to determine the genes that define a mammal\nby this comparison…. So by comparing a human to a primate, we\nwill be able to identify the genes that encode the features of primates\nand distinguish them from other mammals. Then, by tweaking our computer\nprograms, we will finally identify the regions of DNA that differ\nbetween the primate and the human—and understand those genes that\nmake us uniquely human” (p. 94). While it is true that any\nstretch of DNA that belongs to all and only humans would be among those\ndifferences found by comparing a single human genome sequence to a\nsingle nonhuman primate or mouse genome sequence, any “uniquely\nhuman” differences could not be distinguished from the others\nwithout extensive infra- and inter-specific population studies which\nare not part of the HGP. Even if such population studies were carried\nout, Gilbert's assumptions about species essentialism—that\nspecies can be defined or represented by properties (in this case,\ncertain stretches of DNA) universally shared among, and particular to,\ntheir members—have long been challenged by philosophers of\nbiology (Gannett 2003a; Robert and Baylis 2003). Because evolution is a\ngradual process where species are constantly undergoing change,\nAristotelian (essentialist) definitions of species need to be\nabandoned; from an evolutionary perspective, in David Hull's\n(1994) words: “The essence of a particular species is to have no\nessence” (p. 215). Species should instead be defined as cluster\nconcepts (Hull 1965) or recognized to be individuals (i.e.\nspatio-temporally restricted, historically contingent particulars) to\nwhich organisms belong as parts, and not classes, sets, or natural\nkinds at all (Ghiselin 1974; Hull 1978).  \n\nBesides these attempts to reduce species to beanbags of genes,\ngenetic reductionism enters in attempts to explain cellular or\norganismal properties solely in terms of genes, or entire organisms in\nterms of genomes. Gilbert (1992) endorses an essentialism of this sort\nas well: “The information carried on the DNA, that genetic\ninformation passed down from our parents,” he writes, “is\nthe most fundamental property of the body” (p. 83), so much so,\nin fact, that “one will be able to pull a CD out of one's\npocket and say, ‘Here is a human being; it's\nme!’” (p. 96). The social prevalence of this representation\nof the genome as the “most fundamental” aspect of the\nindividual means that genetic information has a particularly acute\nimpact on self-identity and self-understanding (Quaid 1994). Another\ngenome scientist Eric Lander (1996) characterizes the HGP as “the\n20th century's version of the discovery and\nconsolidation of the periodic table” with the genes\n“elements” and gene variants responsible for disease\nsusceptibilities “isotopes” (pp. 536–537). The probable\nsocial consequence of this beanbag conception of the organism, combined\nwith a concept of genetic disease that relocates the locus of disease\nfrom organism to genome, is the direction of technological fixes at the\ngenome (Keller 1994). When these technological fixes include prenatal\ngenetic screening and the possible modification of IVF embryos, it is\nsuggested that genetic reductionism contributes to the commodification\nof children by making them an instrument of parental desire (Darnovsky\n2001). The relevant notion of “reduction” at play here is\nthe explanation of wholes in terms of parts. As Sahotra Sarkar (1998)\nnotes, it is important to distinguish between genetic reductionism and\nphysical reductionism: “From the point of view of physical\nreductionism, DNA enters the molecular milieu on par with proteins or,\nfor that matter, lipids or any other molecules that are found in living\norganisms. Physical reductionism does not require any assumption about\nthe primacy of DNA or of genes in the explanation of biological\nbehavior” (p. 174). The reduction of organisms to their genomes\nby molecular geneticists takes yet further molecular\nbiology's—and, more generally, proximate\nbiology's—reduction of organisms to their constituent\nphysical parts in a way that effaces the contexts (provided by\npopulations and environments) in which organisms develop (Griesemer\n1994). Definitions of health and disease attach to organisms and their\nphysiological processes in particular environments and cannot simply be\nrelocated to the level of the genome (Limoges 1994; Lloyd 1994). It is\nwrong to presume that diseases become more objectively defined entities\nonce they receive a genetic basis since social and cultural values\nimplicated in designations of health and disease can merely become\nincorporated at the level of the genome, in what counts as a normal or\nmutant gene. \n\nThere is an additional sense in which genetic reductionism is\nimplicated in the HGP, and Gilbert makes reference to this as well.\nThis is the sense, familiar to philosophers of science, of\nintertheoretic reduction, whereby (usually) higher-level theories are\nsaid to be reduced by lower-level ones insofar as these lower-level\ntheories explain/predict the phenomena of the higher level. Gilbert\n(1992) foresaw that the HGP would furnish the basis for a\n“theoretical biology” in which from the genome's DNA\nsequence it would be possible to predict protein sequence, and from\nprotein sequence it would be possible to predict three-dimensional\nprotein structure—either from “first principles”\nbased on energy calculations or from observed structural similarities\nof the building blocks—and from there make predictions about\nfunction, a predictability that Gilbert suggests would extend to\nindividual organisms and their behavior, and might therefore be\ndifficult to accept: “To recognize that we are determined, in a\ncertain sense, by a finite collection of information that is knowable\nwill change our view of ourselves” (p. 96). Gilbert seems to\nconflate epistemology with ontology, moving from genetic reductionism\nwhere genes suffice to predict or explain behavior to genetic\ndeterminism where genes are sufficient causes of behavior (see next\nsection), but more importantly, even at the lowest level of\norganization, his vision faces formidable obstacles. Notwithstanding\nthe protein folding problem and the need to consider gene regulation in\norder to proceed beyond the level of protein structure, there are\nsignificant difficulties in attempting even to predict the linear\nstructure of proteins from sequence data alone: specifically, abilities\nare limited for recognizing transcription initiation sites and, in the\npresence of extensive RNA editing, the boundaries between introns and\nexons and coding and noncoding segments of DNA (Sarkar\n 1998).[19] \n\nMolecular biology's technological capacity to manipulate the\nhuman genome brings society into something of an existentialist\npredicament. Science has tended to conceive human essence as a fixed\nobject discoverable in nature. But a human essence embedded in\nmanipulable genome is not immutable—it is created, not\ndiscovered. There is a very real sense in which in making the difficult\nchoices we face—for example, those involved in prenatal genetic\ntesting and germ-line manipulation—we really are\nchoosing\n ourselves.[20] \n\nGilbert's reductionist vision of the sequenced human genome as\n“the grail” upon which a “theoretical biology”\ncan be founded brings to the fore philosophical questions about genetic\ndeterminism. One might ask with Richard Lewontin (2000 [1992], p. 139),\nhowever rhetorically: “How is it that a mere molecule [DNA] can\nhave the power of both self-reproduction and self-action, being the\ncause of itself and the cause of all other things?” Getting\nstraight on genetic determinism is important. There is a long, ignoble\nhistory of marshalling ideological justification for unjust and\noppressive social and political institutions and structures by\nappealing to the ostensibly scientific assertion that “human\nnature is fixed by our genes” (Rose et al. 1984; also\nLewontin\n 1993).[21]\nCritics of the HGP saw it as placing “the seal of approval from\nmainstream science” on hereditarianism, favoring nature over\nnurture like the eugenics of the early to mid-20th century,\nto promote a “technological fix” for social problems (Allen\n1994, p.\n 164).[22]\nHowever, with the HGP nearing completion and\nthe availability of entire genome sequences for numerous organisms\nsupporting the movement from structural to functional genomics,\nKeller—one such early critic—found that the deterministic\nand reductionistic assumptions underlying the HGP had actually been\nundermined by the research in molecular biology the HGP made possible:\n“What is most impressive to me is not so much the ways in which\nthe genome project has fulfilled our expectations but the ways in which\nit has transformed them…. Contrary to all expectations, instead\nof lending support to the familiar notions of genetic determinism that\nhave acquired so powerful grip on the popular imagination, these\nsuccesses pose critical challenges to such notions” (2000, p.\n5). \n\nYet, DNA is still portrayed as fundamental: in a public lecture held\nin celebration of the completion of the HGP, Collins characterized the\nHGP as “an amazing adventure into ourselves, to understand our\nown DNA instruction book, the shared inheritance of\nall\n humankind.”[23]\nWhile virtually all biologists disavow\ngenetic determinism today, it is not always so clear what exactly they\nare denying. Jonathan Kaplan (2000) identifies three different ways in\nwhich claims about genetic determinism might be understood: (i) as\n“complete information” where everything about us is viewed\nas predictable based on our genes; (ii) as “intervention is\nuseless” where traits are said to be impervious to environmental\nchanges; and (iii) as traits that are in some sense primarily, even if\nnot wholly, genetic. Kaplan argues that when biologists disavow genetic\ndeterminism it is (ii) they have in mind (with\nphenylketonuria—PKU—frequently used as an\n example).[24]\nAccording to\nKaplan, (i) is easily seen to be “trivially false,” and\ntherefore not worth disavowing—yet, this resembles Laplacian\ndeterminism's concern with predictability, and as we have seen,\nGilbert seems to be making such a claim. Despite their disavowals of\ngenetic determinism, Kaplan finds that biologists often adhere to\n(iii); however, the basis for the primacy of genes remains to be\nunderstood. Questions about genetic determinism and Collins’\nrepresentation of the sequenced human genome as “our own DNA\ninstruction book”—which suggests an asymmetry between\ngenetic and nongenetic causes—need to be approached at several\ndifferent levels: cellular, organismal, and\n societal.[25] \n\nAt the cellular level, the book is said to contain “the\ngenetic instructions for the entire repertoire of cellular\ncomponents” (Collins et al. 2003, p. 3). This genetic determinism\nat the cellular level is sustained by metaphors of Weismannism and DNA\nas “code” or “master molecule” (Griesemer 1994;\nKeller 1994). DNA is accorded causal priority over other cellular\ncomponents in a couple of ways. One way is to treat DNA as temporally\nprior. This may be in a physical sense: Weismannism assumes that\nintergenerational continuity exists only for germ cell nuclei whereas\nsomatic cells and germ cell cytoplasm arise anew in each generation. It\nmay also be in the sense of a point of origin for the transfer of\ninformation: the central dogma of molecular biology, which represents a\n1950s reformulation of Weismannism in terms of information theory,\nasserts that information travels unidirectionally from nucleic acids to\nprotein, and never vice versa. The chief difficulty for these claims of\ntemporal priority is of the chicken-and-egg variety: nucleic acids need\nproteins and other cellular components to make proteins (Smith 1992).\nAlthough it is fully accepted that the fertilized ovum contains the\ncytoplasmic contribution of at least the maternal germ cell, there\npersists a tendency in developmental genetics to focus on cytoplasmic\n(mitochondrial) DNA and to ignore the role of cytoplasmic proteins. It\nis also contentious whether amongst the cell's components only\nnucleic acids can be said to transmit information: for some\nphilosophers, genetic coding plays a theoretical role at least at this\ncellular level (Godfrey-Smith 2000); for others, genetic coding is\nmerely (and misleadingly) metaphorical, and all cellular components are\npotential bearers of information (Griffiths 2001; Griffiths and Gray\n1994; Sarkar 1996). An additional way in which DNA is accorded causal\npriority lies in its treatment as ontologically prior: this is\nexemplified in Watson's description of DNA as “the most\ngolden of molecules” (in Bodmer and McKie 1994, p. 10). Causal\nasymmetry provides a possible reason for privileging DNA on an\nontological basis: provided all cellular components necessary for\nprotein synthesis are present, modification of the DNA sequence may be\nfollowed by a predictable and specifiable change in protein sequence,\nbut the opposite will not occur. This difference could be conceived in\nterms of the Aristotelian distinction between formal and efficient\ncausation and the accompanying metaphysical preferences for form over\nmatter and mind over body that are deeply embedded in western\nphilosophy. Keller (2000) describes how, in the discourse of\n“gene action” which arose between the mid-1920s and 1960s\nand culminated in Francis Crick's “central dogma,”\n“the gene was bestowed with the properties of materiality,\nagency, life, and mind” and rendered “[p]art\nphysicist's atom and part Platonic soul” (p.\n 47).[26] \n\nAt the level of the organism, talk of genetic coding and the\nasymmetry between genetic and nongenetic causes such talk conveys, even\nwhen countenanced at the cellular level, are deemed less acceptable\n(Godfrey-Smith 2000). New research in functional genomics may well lead\nto less deterministic accounts even of so-called single gene disorders.\nFor these, the concepts of penetrance and expressivity operate in ways\nwhich accommodate the one-one genetic determinist model where the\nmutation is necessary and/or sufficient for both the presence of the\ncondition and confounding patterns of phenotypic variability. But the\nseverity of even a fully penetrant condition like Huntington's\ndisease seems to depend on not just genetic factors like the number of\nDNA repeats in the mutation but epigenetic factors like the sex of the\nparent who transmitted the mutation (Ridley et al. 1991). \n\nAt the level of individuals in society, when we consider complex\nconditions to which both genetic and environmental differences\ncontribute—for example, psychiatric disorders or behavioral\ndifferences—gene-centrism persists. The April 1998 cover of\nLife captures the reader's attention: “WERE YOU\nBORN THAT WAY? Personality, temperament, even life choices. New studies\nshow it's mostly in your genes.” Leading scientists have\nsaid similar things. At the outset of the HGP, Watson told us:\n“we used to think our fate is in our stars. Now we know, in large\nmeasure, our fate is in our genes” (in Jaroff 1989). Post-HGP,\nWatson seems unaffected by the changes that have so impressed Keller.\nWhile he introduces the recent book Behavioral Genetics in the\nPostgenomic Era by stating confidently that “with the\narrival of the human DNA genome sequence and its attendant list of\nhuman genes, the experimental procedures will soon be on hand to\nfinally settle the long contentious nature-nurture arguments” (p.\nxxii), the question seems already settled for him in his assertions\nthat “children come into the world with fixed\npersonalities” and “effective remedies for socially\ninappropriate behaviors” will best be carried out at the\nmolecular level (in Plomin et al. 2003, p. xxii). \n\nBut notice the waffle words used by Watson and on the cover of\nLife: “in large measure” and “mostly in your\ngenes.” Everyone is an interactionist these days, in some sense\nof “interaction.” Genes and environment, or nature and\nnurture, are recognized both to be necessary for development: by\nthemselves, genes can’t determine or do anything. Yet, theorists\nstill seem to give the nod to one or the other, suggesting that it is\nmostly genes or mostly the environment, mostly nature or mostly\nnurture, that make us what we are. This implies that it is possible to\napportion the relative contributions of each. Gilbert (1992) suggests\nthis in his dismissal of a more simplistic version of genetic\ndeterminism: “We must see beyond a first reaction that we are the\nconsequences of our genes; that we are guilty of a crime because our\ngenes made us do it; or that we are noble because our genes made us so.\nThis shallow genetic determinism is unwise and untrue. But society will\nhave to wrestle with the questions of how much of our makeup is\ndictated by the environment, how much is dictated by our genetics, and\nhow much is dictated by our own will and determination” (pp.\n96–97). However, the assertion that the relative contributions of genes\nand environment, nature and nurture, can be apportioned in this way is\nmisleading if not outright false. As Lewontin argued in his classic\npaper on heritability, it is impossible to infer causal relations from\nthe analysis of variance. The only legitimate exception is where there\nis “perfect or nearly perfect additivity between genotypic and\nenvironmental effects so that the differences among genotypes are the\nsame in all environments and the differences between environments are\nthe same for all genotypes” (1974, p.\n 408).[27]\nContrary to Watson's\nassertion, the replacement of quantitative with molecular genetic\ntechniques cannot resolve the nature-nurture controversy because of the\nsame problem that affects heritability measures: the context-dependence\nof genes as causes given the nonadditivity of gene-gene and\ngene-environment interactions. Recent work in developmental systems\ntheory (DST) which undermines any such attempts to apportion causal\nresponsibility in organismal development makes clear why: traits are\njointly determined by multiple causes, each context-sensitive and\ncontingent (Griffiths and Gray 1994; Griffiths and Knight 1998; Oyama\n1985; Oyama et al. 2001; Robert\n 2004).[28] \n\nWhen genetics enter philosophical debates about freedom and\ndeterminism, questions about moral and legal responsibility are\ncentral: if the genes a person happens to inherit can be said in some\nsense to determine her actions, is it legitimate to praise, blame,\nreward, or punish that person? Retributivism pulls in opposing\ndirections: genetic predisposition to violent or criminal acts may\nsuggest “a volitional disability that makes blame\ninappropriate” or “a permanence that invites blame”\n(Wasserman 2001, p. 304). The HGP is unlikely to enlighten or\ncomplicate these longstanding debates, however (Baron 2001). The thesis\nof universal causation (no uncaused events) and its implications for\nfreedom are unaffected by genetics: it makes no sense to claim that all\nevents have genetic causes, and were it to turn out that certain\nevents—i.e. human behaviors or actions—have genetic causes,\nthese pose no different a threat for freedom than their nongenetic\ncounterparts. In addition, genetic causes of behavior are likely to be\ntendencies or predispositions which do not necessitate their effects:\nfor incompatibilists, genetic and nongenetic causes are jointly\nresponsible for behaviors, and genetic determinism adds nothing to the\nchallenge determinism already poses for freedom; for compatibilists,\nsince a person's behavioral genetic tendencies or predispositions\ndo not compel her to act in a certain way, they are no different than\nnongenetic (biological or environmental) tendencies or predispositions\nnot of a person's own making. Of course, the general\npublic—in voting booths, on juries, etc.—may be swayed more\nby genetic explanations given beliefs in genetic determinism fuelled by\nmedia reports of apparent discoveries of genes for this or that\nbehavior. \n\nThe gene is a “cultural icon”: in popular culture, from\nmovies to cartoons to Dear Abby, quite apart from its biological and\nmedical contexts, the gene has become “a symbol, a metaphor, a\nconvenient way to define personhood, identity, and relationships in\nsocially meaningful ways” (Nelkin and Lindee 1995, p. 16). Hardly\na week goes by when we do not hear about a newly discovered gene for\none thing or another. “Geneticization” is a term used to\ndescribe this phenomenon marked by an increasing tendency to reduce\nhuman differences to genetic ones (Lippman\n 1991).[29]\nThis tendency is accompanied\nby worries of critics that embracing a reductionist approach to\nmedicine that conceives of human health and disease in wholly molecular\nor genetic terms individualizes these and detracts attention from our\nshared social and physical environments and the role of toxins, fast\nfood, poverty, lack of access to health care, etc. (Nelkin and Tancredi\n1989; Hubbard and Wald 1993). One of the justifications for spending\nseveral billion dollars on human genome research is the belief that\ngenes are key determinants of not only rare Mendelian diseases like\nHuntington's disease or cystic fibrosis but common\nmulti-factorial conditions like cancer, depression, and heart disease.\nIn Watson's words: “Some call New Jersey the Cancer State\nbecause of all the chemical companies there, but in fact, the major\nfactor is probably your genetic constitution” (in Cooper 1994, p.\n326).  \n\nWrites an early critic of the HGP: “Without question, it was\nthe technical prowess that molecular biology had achieved by the early\n1980s that made it possible even to imagine a task as formidable as\nthat of sequencing what has come to be called ‘the human\ngenome.’ But it was the concept of genetic disease that created\nthe climate in which such a project could appear both reasonable and\ndesirable” (Keller 1992, p. 293). Given that the development of\nany trait involves the interaction of both genetic and nongenetic\nfactors, on what bases can genes be privileged as causes in order to\nclaim that a particular disease or nondisease trait is\n“genetic” or caused by a “genetic\nsusceptibility” or “genetic predisposition”? Does it\nmake sense for HGP proponents like Bodmer to characterize even\nsmoking-induced forms of cancer as genetic? “Cancer, scientists\nhave discovered, is a genetic condition in which cells spread\nuncontrollably, and cigarette smoke contains chemicals which stimulate\nthose molecular changes” (Bodmer and McKie 1994, p.\n 89).[30]\nFrom the\noutset, we need to distinguish between genes conceived as causes of a\ntrait's appearance in a given individual (“x is a\ngene for trait y in organism z” or\n“My three-pack-a-day Aunt Viv must have the gene that causes\ncancer”) and genes as causes of differences in traits among\nindividuals (“x is a gene for trait y\nin population z” or “Lots of people in my family\nsmoke, but only Aunt Viv and Cousin Sal seem to have inherited the gene\nfor\n cancer”).[31] \n\nThe logical interrelatedness of cause and effect—that is,\nwhether a condition is necessary and/or sufficient for a given event to\noccur—is the approach taken to defining what makes a condition\n“genetic” in individuals. A strong sense of “genetic\ndisease” is recognized when the genetic factor is both necessary\nand sufficient for the disease to arise “regardless of\nenvironment” (Wulff 1984), or when the genetic factor is\nsufficient for the disease to present “in all known\nenvironments” (Kitcher 1996)—this latter definition\nrecognizes that, in some cases, a disease may have nongenetic as well\nas genetic origins (since the genetic factor is sufficient but not\nnecessary). “Genetic susceptibility” is defined as an\nincreased probability of disease in all known (strong sense) or some\n(weak sense) environments (Kitcher 1996). Note that ceteris\nparibus clauses referring to an assumed background of necessary,\nthough not sufficient, genetic and environment factors are required by\nthese definitions. Just as striking a match causes it to ignite only if\nit is dry and in the presence of oxygen, as we saw in the previous\nsection, genes don’t do anything alone. This is the first of\nthree ways in which genetic explanations are\ncontext-dependent. \n\nAdopting a population-based approach to genetic causation, where\ndifferences in genes are understood to explain differences in traits\nand not traits themselves, replaces the need for ceteris\nparibus clauses because they rely on the actual distribution of\nthe necessary genetic and nongenetic background factors in specific\npopulations. The case can be made that the first approach is indebted\nto the second, and that one never explains a property of an object\ntout court but only in relation to a reference class of an\nobject or objects that lack the property (but share the necessary\nbackground factors). Writes Germund Hesslow (1983), “all\nexplanations of individual facts of the form Fa—that is,\nwhere an object a has a certain property\nF—involve a comparison with other objects which lack the\nproperty in question” (p. 91). No trait can be labeled\n“genetic” in any absolute sense, but only relative to a\nspecific population. For example, lactose intolerance is considered to\nbe a genetic condition in northern European populations where ingestion\nof milk products is common and lactase deficiency rare, whereas in\nAfrican populations, where ingestion of milk products is rare and\nlactase deficiency common, it is considered to be an environmental\ncondition (Hesslow 1984). This is the second way in which genetic\nexplanations are\n context-dependent.[32] \n\nThe third, and final, way in which genetic explanations are\ncontext-dependent is that they are a function of the present state of\nknowledge. Huntington's disease is deemed a genetic condition on\nboth the individual and population accounts: a single mutant gene is\nnecessary, and arguably sufficient given necessary (and standard)\nbackground conditions, for symptoms to appear in a given person; the\npresence and absence of disease symptoms in members of the population\nis accounted for in terms of the presence and absence of the mutation.\nThis is nevertheless an epistemically relative claim. Once the relevant\ngene is mapped and sequenced, the mechanisms by which genetic and\nnongenetic factors interact to produce symptoms of the disease remain\nto be understood. Such causal knowledge is often obtained through the\nexperimental manipulation of conditions beyond “normal”\nlimits, and what conditions are exploited as possible causes in the\nlaboratory and what conditions are kept constant as necessary\nbackground, along with pragmatic decisions about how research efforts\nshould be expended more generally, are influenced by clinical and\nsocial, as well as scientific, contexts (Gannett\n 1999).[33] \n\nBehind philosophical attempts to seek objective, nonevaluative\nfoundations for designations of diseases as “genetic” or\n“environmental” lie positivist assumptions that theoretical\nunderstanding furnishes the basis for rational action. One concern with\ngeneticization and the trend to label an increasing number of diseases\nand conditions “genetic” is that this provides normative\nsupport for directing future research and therapeutic interventions in\nparticular ways, that is, at the level of the genome (Cranor 1994).\nWatson's (1992) colorful metaphor makes this normative support\nexplicit: “Ignoring genes is like trying to solve a murder\nwithout finding the murderer. All we have are victims” (p. 167).\nBut this is fallacious reasoning, as the context-dependence of genetic\nexplanations shows. We might instead understand geneticization to be\nthe consequence of an increased capacity to manipulate DNA in the\nlaboratory and (potentially) the clinic and not an advancement in\ntheoretical understanding. Genetic explanations, on such a view, are\npragmatic: there is a practical context in which genes are singled out\nas causes not only because they are amenable to technological control\nbut because they are increasingly perceived to be more tractable than\ntheir nongenetic counterparts and therefore the best means to a variety\nof ends (Gannett 1999). \n\nMany of the model organisms chosen for the HGP had already enjoyed\nillustrious careers in the history of genetics: the fruitfly D.\nmelanogaster was the organism that started it all in T. H.\nMorgan's lab at Columbia University in the 1910s, ushering in the\nera known today as classical genetics; with discoveries of spontaneous\nmutation and recombination in the 1940s, the bacterium E. coli\nhelped to take genetics molecular, serving also as host for the phage\nstudied by Max Delbrück's group; the nematode worm C.\nelegans was Sydney Brenner's choice to model the development\nof the nervous system in the mid-1960s at Cambridge\n University.[34]\nIt was, in\nfact, these histories that recommended them: “the experimental\norganisms that became ‘model organisms’ were not selected\nand constructed mainly on the basis of principles of universality or\neven typicality of their biological characteristics and processes,\nthough it was hoped that many features would prove to be shared or\ncommon to other organisms, particularly humans. Instead they were\nprimarily chosen for ease of experimental tractability and due to the\navailability of some background information on basic genetic\ncomposition and relation to phenotype” (Ankeny 2001, pp.\nS253-S254).  \n\nPhilosophical questions arise about the senses in which these\nvarious organisms serve as “models.” Potentially, models\nmay embody a range of characteristics: as typical or representative; as\nideal or perfect; as convenient, tractable, or manipulable; as\nhomologous (conserved evolutionarily); as analogous; as exemplars; as\nabstractions. Models may also be used in a variety of ways: to model\ndisease processes; to model normal processes; as structural models; as\ntype organisms representative of the species or higher phylogenetic\nlevel; as heuristic tools; as mathematical\n devices.[35]\nIn a recent article examining\nresearchers’ use of the flowering plant Arabidopsis\nthaliana as a model organism, Sabina Leonelli (2008) points out\nthat models can be abstract (vs. concrete) in different ways:\nabsolutely, in terms of their sense perceptibility; or relatively, in\nterms of their physical meaning with respect to the phenomena\nrepresented or the range of phenomena they are taken to represent. She\nthen shifts the philosophical focus from models themselves to modeling\npractices: abstraction, for example, becomes a component of the\nactivity of producing a model rather than solely an attribute of the\nmodel. This approach emphasizes the need to attend not just to the\nrelationship between model and phenomenon modeled but the material,\nsocial, and institutional settings and varied commitments of\nresearchers.  \n\nRachel Ankeny (2000, 2001) considers several ways in which organisms\nmight plausibly be considered models. She suggests that mapped and\nsequenced genomes for these various HGP model organisms serve as\n“descriptive models.” A genome reference sequence is a\nmodel because it is, first, “an idealized, abstract\nentity constructed from the natural organism” and, second,\n“‘model’ is used to indicate a promissory note about\nthis organism providing a framework for pursuing explanatory questions\nand ultimately serving as a prototype for understanding more complex\norganisms” (Ankeny 2000, p. S267). Genome reference sequences are\ndescriptive because they are “constructed largely\nwithout motivation by hypotheses to be tested or traditional\nexplanatory questions” but rather as preliminary to such work\n(Ankeny 2000, p. S267). In this, they are similar to earlier efforts to\nuse wiring diagrams for C. elegans to model neural structure:\nthese diagrams were based on data from several worms but were presented\nas canonical; the worms were “wild type” and presumed to\nexhibit species-typical structure; the diagrams served as a tool to\ninvestigate abnormalities (Ankeny 2000). Nonhuman genome reference\nsequences become tools as their corresponding organisms are used as\nexperimental models for understanding basic biological processes common\nto many species or diseases processes found in humans—for\nexample, by knocking out genes in mice. Here, analogical reasoning is\nat work; “[m]odels in this sense of the term seem to provide what\nmight be termed strong causal analog models” (Ankeny 2001, p.\nS255). According to Kenneth Schaffner (1998a), this is quite typical of\nbiological explanation: unlike physicists, biologists frame\nexplanations “around a few exemplar subsystems in specific\norganisms … used as (interlevel) prototypes to organize\ninformation about other similar (overlapping) models” (p.\n278). \n\nMonod famously once said that what is true of E. coli is\ntrue of the elephant, but just as famously, this proved not at all to\nbe the case in moving from prokaryotic to eukaryotic gene regulation.\nMore recently, biologist Bruce Alberts writes: “we can say with\nconfidence that the fastest and most efficient way of acquiring an\nunderstanding of ourselves is to devote an enormous effort trying to\nunderstand … relatively ‘simple’ organisms”\n(in Schaffner 1998a, p. 277). What inspires such confidence when a\nsimple organism like C. elegans with its 302 neurons and\nrepertoire of behaviors (movement in response to touch and chemical\nstimuli, egg laying, and mating) is used to model human behavior in all\nits complexity? Most importantly, the model organism must be\nrepresentative of the systems being modeled: genomic sequences must be\nsimilar in model and modeled organisms; there must be a known\ncause-effect relationship between the model organism's genotype\nand phenotype; there cannot be any “causally relevant\ndisanalogies” between model and modeled organisms, for example,\ndue to differences in complexity (Ankeny 2001, p. S257). Schaffner\n(1998b) examines molecular geneticists’ use of C.\nelegans as a behavioral model. Even in these simple organisms,\nrelations between genes, neurons, and behaviors are complex\n(many-many), with one gene-one behavior associations rare exceptions\nand their intervening causal chains yet to be understood. While such\ncomplexity is to be expected in humans with their more complicated\nnervous systems, Schaffner believes that there may be a small number of\nsingle gene effects on behavior where these genes are highly homologous\nand strongly conserved—hence, the usefulness of simple models\nlike C. elegans combined with others for investigating basic\nmechanisms and psychiatric disorders. \n\nThe model organism approach faces challenges, however. As Schaffner\nrecognizes, model organisms are also idealizations: organisms are\nselected for features not generalizable even to close relatives like\nrapid development, short generation time, small adult size, and\ninsensitivity to environmental variation (Wimsatt 1998); strains are\ninbred to remove genetic diversity. Context-sensitivity diminishes\nexpectations that similar mechanisms operate in simple and complex\nsystems; multiple realizeability creates doubts that similar\nexplanations will be found across taxa (Wimsatt 1998). Evolution is a\nbranching process; as Richard Burian (1993) emphasizes: “At\n(virtually?) all levels of the biological world—including the\nbiochemical—it is an open question how general the findings\nproduced by the use of a particular organism are” (p. 365).\nConsequently, support for theoretical hypotheses requires the\nexperimental findings to be placed within a comparative and\nevolutionary framework attentive to how widely the relevant nucleotide\nsequences and traits are distributed phylogenetically: “detailed\nknowledge of (historical) biological contingencies constrains—and\nought to constrain—the evaluation of experimental work in biology\nand the knowledge claims based on that work” (Burian 1993,\np.\n 366).[36] \n\nThe policy of the U.S. Patent and Trademark Office (PTO) used to be\nthat life forms, as products of nature, were unpatentable. Only\nproducts and processes invented by humans could be patented. But what\nabout genetically modified life forms: are they invented or discovered,\nthe product of nature or humans? In 1980, the U.S. Supreme Court issued\nits 5–4 decision in Diamond v Chakrabarty that a bacterial\nstrain that had been genetically modified to clean up oil spills could\nbe patented since it was “man-made” and not naturally\noccurring. Since this ruling, “the PTO has awarded thousands of\npatents on biological products, including patents on genes, SNPs, ESTs,\ncell lines, mice, plants, rhesus monkeys, and human stem cells”\n(Resnik 2004, p.\n 54).[37]\nFollowing Diamond v Chakrabarty,\ngenes and gene products are considered patentable in their\nnon-naturally-occurring forms, even if, unlike genetically modified\nlife forms, the sequence information they contain is unaltered from\nwhat occurs\n naturally.[38]\nPatents are granted on cloned genes because\nthey are isolated and purified versions of their naturally occurring\nstate; patents are granted on cDNAs because they are not found in\nnature (without intervening introns) but produced using RNA molecules\nand the enzyme reverse transcriptase; and patents are granted on\nproteins, even if they occur naturally, when genetically engineered on\nthe basis of DNA or RNA sequence data. For such cases, DNA sequences\nare treated by the PTO and U.S. Court of Appeals for the Federal\nCircuit as “compositions of matter” (the wording contained\nin the patent statute) analogous to patentable chemical compounds\n(Eisenberg\n 2002).[39] \n\nControversy about patenting genes was provoked over the course of\nthe HGP when researchers at the NIH and elsewhere applied for patents\non ESTs (expressed sequence tags) for cDNAs without having mapped and\nsequenced the relevant genes, and without any knowledge of gene\nfunction. Granting such patents means that subsequent researchers who\ndiscover functions conducive to the development of diagnostic tests or\ntreatments are faced with paying licensing fees or royalties. For\nexample, in 1996, researchers discovered that the protein CCR5 plays a\nrole in HIV infection, but a patent on the gene for CCR5 had already\nbeen applied for by the biotech company Human Genome Sciences having\npostulated a possible anti-inflammatory role for the gene using\ncomputational methods (Smaglik 2000). Many human genome scientists\nopposed the policy of granting gene patents without knowledge of\nfunction, whether for ESTs and cDNAs or complete gene sequences. Human Genome Organization (HUGO)\nissued this 1995 statement: “HUGO is worried that the patenting\nof partial and uncharacterized cDNA sequences will reward those who\nmake routine discoveries but penalize those who determine biological\nfunction or application. Such an outcome would impede that development\nof diagnostics and therapeutics, which is clearly not in the\npublic\n interest.”[40]\nIn 2000, the NIH and scientific bodies like\nthe National Academy of Sciences and Royal Society of London called for\ntighter rules to discourage patent applications on genes identified\nmerely by computational analysis of sequence data, the public release\nof which the 1996 Bermuda Accord required of sequencing centers every\n24 hours (Alberts and Klug 2000; Dickson 2000). That year, the PTO\nrevised its guidelines to require that researchers applying for patents\ndemonstrate utility (U.S. Department of Commerce\n 2001).[41] \n\nControversy has also arisen over whether the purposes of the\nintellectual property system are being fulfilled in the case of gene\npatents. Patents are supposed to work to stimulate scientific research\nand technological development by removing the need for secrecy, and\nthis in turn is supposed to benefit society as a whole. Many believe\nthat secrecy has increased, that there is less sharing of data and\ntechniques between laboratories, and that research and development\nefforts that build on initial discoveries are being stymied. When\nresearchers are issued a patent on a gene they have mapped and\nsequenced, they may license exclusive rights to a biotech or\npharmaceutical company to develop and market applications—perhaps\na drug or diagnostic test. Research costs are driven up when scientists\nwho must test individuals in their studies are beset by royalty\npayments. Without a competitive market, the costs of administering\ndiagnostic tests in the clinic also rise. For example, Myriad Genetics\npatented BRCA1 and BRCA2 breast and ovarian cancer genes and granted\nEli Lilly exclusive rights to market applications based on the BRCA1\nsequence. The cost for a woman at risk of developing cancer to be\ntested for BRCA mutations was $2400-$3500, and researchers working to\nunderstand how these mutations are implicated in the development of\ncancer also had to pay the fee (Reynolds 2000). Of particular ethical\nconcern is that much of the research which results in patented\ndiscoveries is funded by government or nonprofits. For example, the\nBRCA research was federally funded: it was conducted by researchers at\nthe University of Utah (which assigned its rights to Myriad) who were\nsupported by NIH and at the National Institute of Environmental Health\nSciences (Reynolds 2000). This is an outcome of a U.S. governmental\npolicy that encourages federally-funded researchers to seek\npartnerships with the private sector. So, as taxpayers and consumers,\nthe public ends up paying twice. These discoveries also could not have\nbeen made without the cooperation of patients and their families, and\nyet, while researchers and institutions profit, they are faced with\nmore expensive tests and treatments (see Merz et al. 2002 on\npatenting's effects on the cost and availability of genetic tests\nfor hereditary haemochromatosis). \n\nIntellectual property rights also raise ethical questions about\nconflict of interest and exploitation. Such potential exists in\nresearch that studies relatively small, isolated populations to see if\nthere are any rare genes present which may be of value for\npharmaceutical development (Dickenson 2004; Salopek 1997). In one\nwell-known episode, organizations representing indigenous peoples\nmounted opposition to a 1995 patent granted to the NIH on a cell line\nobtained from a Hagahai man from Papua New Guinea, a claim later\nwithdrawn by the NIH due to the controversy. The patenting of human\ngenes and cell lines is seen as a continuation of the\n“bioprospecting” and “biopiracy” that have\ntaken place over the past several decades with western corporations\nsecuring patents on medicinal and food uses of plants which have been\nlong a part of traditional knowledge (Shiva 1996). Problems also arise\nin clinical research. With multiple sources of conflict of interest\noperating, there was the potential for exploitation when teenager Jesse\nGelsinger died in a gene therapy trial in 1999. Researcher James Wilson\nheld patents on several of the procedures used; Genovo, the private\nfirm sponsoring the study, was founded by Wilson; Wilson and his\nemployer, the University of Pennsylvania, held equity in Genovo; and\nGenovo was providing $4 million per year to the university's\nHuman Gene Therapy Institute (Resnik 2004, p. 162). \n\nPhilosophical arguments in support of gene patents include: patents\nare just rewards for researchers’ efforts and costs; by mixing\ntheir labor with what occurs naturally, researchers acquire property\nrights in the Lockean sense; patents contribute to scientific and\ntechnological progress by diminishing secrecy and providing incentives;\nsociety ultimately benefits with better and more economical medical\ntreatments (Resnik 1997a). Philosophical arguments in opposition to\ngene patents include: patents exert negative effects on scientific\nresearch by promoting secrecy and impeding openness and sharing of data\nand technologies among scientists, leading to publication delays,\ninhibiting further research, and encouraging researchers to pursue\nprojects of short term, commercial benefit (Nelkin 2002); patents\noperate as monopolies which compromise medical care by driving up the\ncosts of diagnostic tests and pharmaceuticals and creating financial\nconflicts of interests which diminish trust between physicians and\npatients (Nelkin 2002); patenting is an unacceptable and dehumanizing\ncommodification of life (Dickenson 2004); genes even if isolated and\npurified remain products of nature and are therefore not patentable\n(Sagoff 2002); the human genome is the “common heritage of\nhumanity” not private property (HUGO\n 2000).[42] \n\nEarly on the in the HGP, it was recognized that the development of\ngenetic tests would precede, perhaps by decades, the capacity to treat\nthe corresponding conditions successfully: this emphasized the\nimportance of confronting the ethical implications of genetic testing.\nGenetic testing is carried out for a range of purposes: diagnostic,\npredictive, and reproductive. Diagnostic genetic testing is performed\non individuals already experiencing signs and symptoms of disease.\nPredictive genetic testing is performed on individuals who are at risk\nfor inheriting a familial condition but do not yet show any signs or\nsymptoms. Predictive testing is presently offered for a number of\nconditions: these include Huntington's disease (HD), cystic\nfibrosis (CF), sickle cell anemia, breast cancer, and colon cancer.\nAlthough the HGP was defended by its proponents on the basis that the\nknowledge of individual genome sequences would facilitate the movement\nfrom a reactive to predictive medicine, there are no cures yet for most\ndiseases identified as having a genetic basis. For some conditions,\nlike familial hypercholesterolemia or polycystic kidney disease, early\nintervention may lessen the severity of symptoms. Reproductive genetic\ntesting is carried out in several ways: through carrier screening,\nprenatal testing of the fetus in utero, and preimplantation genetic\ndiagnosis (PGD) of embryos created by in vitro fertilization (IVF). In\ncarrier screening, prospective parents find out whether they are at\nrisk for passing on disease-related genes to their offspring. Prenatal\ngenetic testing of fetuses in utero is conducted through the use of\nblood tests early in a woman's pregnancy, chorionic villus\nsampling (CVS) at 10–12 weeks, and amniocentesis at 15–18 weeks.\nTesting is generally offered to women for whom risk is elevated because\nof age or family history; based on the results, women can elect to\ncontinue the pregnancy or abort the\n fetus.[43]\nIn PGD, a single cell is\nremoved from the 8-cell embryo for testing; based on the results, a\ndecision is made about which embryo(s) to implant in the woman's\nuterus. In the U.S., PGD is restricted to private clinics because of\nthe government's ban on the use of federal funds for\nembryo\n research.[44] \n\nGenetic testing carried out at the population level—for any of\nthese purposes—is referred to as genetic screening. Newborn\nscreening programs to diagnose conditions like PKU, hemoglobinopathies,\netc. on the basis of blood components and circulating metabolites have\nbeen carried out for several decades in many jurisdictions; while such\nscreening could be expanded to include genetic tests for many more\ndiseases, without effective treatment measures, this is of limited use.\nEthical debate exists over whether newborn screening should be\nmandatory (see Andrews 1994). While successful population-based carrier\nscreening has been carried out in Sardinia for beta-thalassemia and for\nTay-Sachs among U.S. Ashkenazi Jews, the negative impacts of testing\nfor sickle-cell anemia among African Americans provide caution for\nlaunching further such programs—outcome is likely to depend on\nthe social status of the targeted group (Duster 1990). The pathway from\ngene discovery to population-level screening is not straightforward,\nhowever. Although the gene implicated in CF was identified in 1989,\nstatements issued by the American Society of Human Genetics in 1990 and\n1992 and by a NIH workshop in 1990 recommended against instituting\npopulation-based carrier screening because adequate detection rates\nwere compromised by the heterogeneity of mutations found; in addition,\nthe clinical course of the disease remained difficult to predict, and\nresources for education and counseling were lacking (ASHG 1990; ASHG\n1992; NIH Workshop\n 1990).[45] \n\nThe 1997 report of the ELSI Working Group's Task Force on\nGenetic Testing focuses on ensuring the safety and effectiveness of\ngenetic tests (Holtzman and Watson 1997)—an important concern\ngiven the social pressures likely to encourage the expansion of genetic\ntesting and screening as more tests become available: commercial\ninterests of the biotech industry; clinicians’ (and their\nmalpractice insurers’) fears of wrongful birth or wrongful life\nlawsuits; and expectations of governments and other institutions (e.g.\nprivate foundations, HMOs) that, by preventing the births of\nindividuals with costly conditions at sufficient rates, carrier and\nprenatal screening programs will prove cost-effective (Holtzman 1989;\nPaul 1994a). At the opposite end of the spectrum, the expanding\navailability of direct-to-consumer genetic tests, which the report\nrecommends against, has led to ethical controversy (Beckman 2004). Like\nthe 1994 report published by the Committee on Assessing Genetic Risks\n(Andrews et al. 1994), the Task Force report also stresses the need for\nvoluntariness, informed consent, confidentiality, legislative\nprotection against discrimination, improved education of clinicians and\nthe public, nondirectiveness in reproductive decisions, and\navailability of genetic counseling. Professional codes of ethics for\ngenetic counseling (e.g. the National Society of Genetic Counselors in\nthe U.S.) emphasize the importance of value-neutral, nondirective\ncounseling in all of these situations: the goal is for clients to make\ntheir own decisions based on their own values once they are fully\ninformed about their options. Ethical questions are raised about the\nideals of value-neutrality and nondirectiveness: choosing which\ninformation to present and how to do so inescapably makes evaluative\nchoices—for example, emphasizing medical over social information\nabout disability in prenatal testing (Vehmas 2001); preference for\ncertain outcomes is built into the system—for example, abortion\nof affected fetuses as society moves to routine prenatal screening\n(Bennett 2001); and nondirectiveness can be taken to an extreme where\ncounselors are hesitant to challenge clients’ decisions no matter\nhow inconsistent with their expressed values or irrational they appear\n(Singer 1996). Many difficult ethical questions confront genetic\ncounselors and other clinicians involved in genetic testing in\nparticular cases. Increased knowledge is not an unmitigated good:\ndenial may be a coping mechanism; individuals may feel guilty for\npassing on harmful mutations to their offspring or stigmatized as\nhaving the potential to do so; survivor guilt may arise in a person who\nfinds out she is not at risk for HD after all, or she may become at a\nloss about how to live her life differently; another person who finds\nout he is destined to develop HD or early-onset Alzheimer's may\nbecome depressed or even suicidal; paternity may not be what it is\nassumed to be. One person's decision to be tested may also have\nramifications for others: for tests that rely on linkage analysis,\nthere may be pressure on relatives to be tested; test results may have\nimplications for the risk status of yet-untested family members;\nprotecting confidentiality may deprive a spouse or child of important\ninformation. These issues will be tackled in the next two sections\n(“Genetic Discrimination” and “Genetic\nPrivacy”). \n\nPrenatal genetic testing raises serious ethical questions about\nreproductive rights and eugenics. Reproductive rights are no longer\njust about the right not to have a child (to use contraception, to have\nan abortion) or the right to bear a child (to refuse population control\nmeasures). Reproductive rights have come to encompass the right to\naccess technological assistance to procreate and to have a certain kind\nof child (Callahan\n 1998).[46]\nThe specter of eugenics—and its images\nof involuntary sterilizations, immigration quotas, “fitter\nfamily” contests, and Nazi death camps—reappears once\nchoices are being made about what sort of people are worth bringing\ninto the world. Many authors have appealed to the history of eugenics\nto provide warnings about the dangers of genetic testing or to urge\ncaution as we move\n forward.[47]\nHistorian Diane Paul (1994b) characterizes\neugenics as the “‘approved’ project anxiety”:\ngiven the lapse of time, we inevitably look good compared to earlier\ngenerations (p. 143). Paul notes that attempts to draw lessons from the\nhistory of eugenics are confounded by disagreements about how to define\n“eugenics”—whether to characterize eugenics according\nto a program's intentions or effects, its use of coercive rather\nthan voluntary means, or its appeals to social and political aims that\nextend beyond the immediate concerns of individual families. For some\ncommentators, even exhortations to choose the “best child”\nare not eugenic because they occur in the realm of “private\nenterprise” and are not directed at improving the population\n(Savulescu 2001). Others worry that the private reproductive choices of\nindividuals will have the “backdoor” effect of narrowing\nsociety's acceptance of diversity (Duster 1990). We are also\nreminded that valuing reproductive autonomy means supporting a\nwoman's decision not just to abort but to bear a child likely to\nhave a disease or disability: while she may be free of state\ninterference, many other factors make this choice\ndifficult—pressure from clinicians, refusal of health insurers to\ncover the child when born due to the “pre-existing\ncondition,” disapproval from other health insurance plan members,\nableist attitudes in society, lack of financial resources, lack of\nsocial support (Hubbard 1990; Hubbard and Wald 1993). Arguably, if\nintentions matter, prenatal diagnosis means that eugenics is already\nwith us: “prenatal diagnosis necessarily involves systematic and\nsystemic selection of fetuses, mostly frequently on genetic\ngrounds” (Lipmann 1991, p. 24). Indeed, Kitcher (1996) seeks to\nestablish a sound theoretical basis for eugenic practice, envisioning a\n“laissez-faire” eugenics where people are educated to make\n“responsible” procreative decisions based on objective\nevaluative judgments of quality of life, not just for the prospective\nindividual but all others involved—parents, siblings, and since\nfunds for social programs are limited, all members of\n society.[48] \n\nInsofar as “prenatal diagnosis presupposes that certain fetal\nconditions are intrinsically not bearable” (Lipmann 1991, p. 25),\nwe operate within the realm of “negative”\neugenics—the selection against unwelcome traits associated with\ndisease and disability. But the genetic testing of fetuses and embryos\nhas already moved beyond this. Selective abortion and implantation\ndecisions are made to obtain a child who is the correct gender or a\nprospective tissue donor (Kahn and Mastroianni 2004). A brief internet\nsearch reveals that selection of donor sperm or ova goes even further\nalong “positive” eugenic lines (eye and hair color, race\nand ethnicity, SAT scores, height, musical ability, etc.); this\ndemonstrates the central role consumer demand will play in any new\neugenics, calling to mind Robert Nozick's (1974) famous image of\na “genetic supermarket” (p. 315n). There is extensive\nethical debate about of what “responsible” procreative\ndecision-making consists in the context of genetic testing and\nselective abortion/implantation. It is argued that parents should\nprevent children being born with conditions that will cause severe pain\nand suffering (Brody 2002), that parental decisions should not restrict\nthe autonomy of children-to-be by compromising their right to an open\nfuture with as few limitations as possible (Davis 2001), that parents\nshould implant embryos without genes associated with disease and\ndisability to avoid doing harm (Harris 2001), and even that parents\nshould select the “best child”—ostensibly the\n“most intelligent”—of those they could possibly have\n(Savulescu 2001). It is also argued that responsible parenting involves\nan unconditional acceptance inconsistent with the willingness to commit\nto having only a certain kind of child (Vehmas 2001), and that it\nshould not be assumed that being born with a disability need be\ndetrimental “either to an individual's prospects of leading\na worthwhile life, or to the families in which they grow up, or to\nsociety at large” (Parens and Asch 1999). Often embedded in these\ndebates is the conundrum that complicates wrongful birth and wrongful\nlife lawsuits: under what conditions can we say an individual has been\nwronged in the context of genetic testing and selective\nabortion/implantation when that individual would not otherwise have\nexisted (Brock 1995)? A condition would have to be severe indeed for\nnonexistence to be preferable to existence with the condition. This\nprovides leeway to argue that parents do no wrong when they refuse\nprenatal testing/selective abortion or PGD/selective implantation, or\neven make use of these technologies to choose children who, like\nthemselves, will have congenital deafness or\n dwarfism.[49] \n\nPeople frequently express fears about genetic discrimination\nresulting from genetic testing made possible by the HGP. Most concerns\nhave focused on insurance companies and employers, but as the use of\ngenetic information proliferates, one can readily imagine other\ninstitutions in society developing an interest in discriminating among\nindividuals on the basis of such information: schools, departments of\nmotor vehicles, immigration authorities, creditors, adoption agencies\n(Nelkin 1992; Nelkin and Tancredi 1989). A number of general arguments\nhave been made against institutional forms of genetic discrimination:\nwe don’t choose our genes and ought not be punished for what is\noutside our control (Gostin 1991); the social costs of creating a\n“biologic” or “genetic underclass” of people\nwho lack health care and are unemployed or stuck in low-wage jobs are\ntoo great (Lee 1993; Nelkin and Tancredi 1989); people's fears of\ngenetic discrimination, whether realistic or not, may lead them to\nforego genetic testing that might benefit their lives and be less\ninclined to participate in genetic research (Kass 1997); people have\nthe right not to know their genetic risk status (Kass 1997). Genetic\ndiscrimination may also occur in less formal circumstances. Mate choice\ncould increasingly proceed on the basis of genetic information, with\ncertain people being labeled as undesirable. As more and more fetuses\nare aborted on genetic grounds, families of children born with similar\nconditions, and people with disabilities and their advocates more\nbroadly, worry that increased stigmatization will result. One\nrecommendation to address this concern is that society commit funds\ntoward maintaining social support services for those with disabilities\neven as the numbers of abortions of genetically abnormal fetuses climb\n(Kitcher 1996). In addition, group-based genetic research into diseases\nor behavioral differences risks stigmatizing people based on racial,\nethnic, and gender differences. It has been recommended that society\ninvest in public education to combat any racial prejudice behavioral\ngenetic research might unfurl (Kitcher 1996); arguments have also been\nadvanced that such research either should not be done or should be held\nto more demanding standards than is frequently the case (Kitcher\n2001). \n\nFocused ethical debate has taken place regarding both insurer- and\nemployer-based genetic discrimination. Insurance companies discriminate\nagainst applicants for health, disability, life, or even mortgage\npolicies when genetic testing reveals them or a family member to be at\nrisk of disease or disability. Discrimination takes the form of\nrefusing coverage on the basis that the genetic susceptibility counts\nas a “preexisting condition,” charging high premiums for\nthe policy, limiting benefits, or excluding certain conditions. The\ninsurance industry argues that there is no principled reason to treat\ngenetic information any differently from other medical information used\nin underwriting. They point to the problem of “adverse\nselection”: people who know themselves to be at high risk are\nmore likely to seek insurance than people who know themselves to be at\nlow risk, which threatens the market when insurers are deprived of the\nsame\n information.[50]\nIf insurers are prohibited from soliciting\ngenetic information altogether, they may decide to offer high-cost\npolicies for those diseases for which testing is available under the\nassumption that all applicants are high risk. After all, insurers are\nin the very business of discriminating among individuals: arguably,\nfair and equitable underwriting requires that those belonging to the\nsame risk class be treated the same (Meyer 2004; Pokorski 1994). In the\nU.S., where unlike other industrialized countries there is no\npublicly-funded system of universal health care, genetic discrimination\nis a particularly serious worry. The 1993 report of the ELSI Working\nGroup's Task Force on Genetic Information and Insurance\nrecommends against the use of genetic information as a basis for\ndenying health care coverage or care and determining the cost of basic\ncoverage, and calls for a voluntary moratorium on medical underwriting\nby insurance companies until a system of universal health care\nis\n established.[51]\nKitcher's major policy recommendation in his 1996 The Lives\nto Come is for a two-tiered health insurance system that provides\nuniversal basic coverage regardless of genetic profile or economic\nstatus, whether provided by a public health care system funded through\nprogressive taxation or a private system committed to affordable\npremiums based on ability to pay. Norman Daniels (1994, 2004) responds\nto the contention of insurers that it is their business to discriminate\nby drawing a distinction between “actuarial fairness” and\nmoral fairness and arguing that underwriting health insurance is unjust\ngiven health care's social function of protecting normal\nfunctioning to ensure equality of opportunity. The case for not\npermitting the use of genetic information in underwriting life\ninsurance has been found morally less compelling by Daniels (2004), but\naccess is actually more threatened given that the individual market\nmakes up 10–15 percent of private health insurance policies but 71\npercent of life insurance policies (Kass 1997). In fact, outside the\nU.S., where universal health care is available, it is life and other\nforms of insurance which have been at the forefront of debate, with\nphilosophical arguments often appealing to the value of social\nsolidarity in support of pooling risk (Launis 2003). One proposal is to\nprohibit underwriting for life insurance policies below a certain\namount (Kass 1997). \n\nSome genetic discrimination by employers is connected to health care\ncoverage, particularly in the U.S. given the lack of universal access:\nto lower insurance costs, existing or prospective employees found to be\nat genetic risk may be fired or not hired. Other reasons employers may\nbe interested in genetic information include reducing replacement and\nretraining costs, avoiding interruptions in production, and learning\nwhich workers are genetically susceptible or resistant to toxins in the\nwork environment (MacDonald and Williams-Jones 2002). Already, in the\n1960s-70s, companies like DuPont and Dow Chemical were engaged in\ngenetic testing, both screening for susceptibilities and monitoring for\ndamage (Draper 1991). Proponents of genetic testing by employers\ncontend that this is an effective form of preventive medicine and that\ncritics are technophobic or politically motivated. Opponents are\nconcerned about lack of empirical validation of tests and inferences,\ncapacity for informed consent, confidentiality, that focusing on\n“high-risk workers” instead of workplaces diverts attention\naway from environmental harms and may lead to the relaxation of\nprecautions, and that the institution of genetic testing programs\ndisproportionately affects groups (women, minorities) already\nmarginalized in the workplace (Draper 1991; Hubbard and Wald 1993;\nNelkin and Tancredi 1989). A familiar example is the use of sickle-cell\ncarrier screening to exclude African Americans from certain jobs\n(Duster 1990). \n\nAlthough there is controversy about how extensive genetic\ndiscrimination is (Wertz [2002], for example, questions whether fears\nare “overblown”), a number of documented cases involving\ninsurers and employers give cause for concern (Billings et al. 1992).\nThe HMO for a family who already had a child with CF was willing to pay\nfor the woman's amniocentesis and, if necessary, abortion but\nrefused health care coverage for the prospective child if born with the\ncondition (Thompson 1989). An air transportation company asked a\ngenetic screening program in Canada to test their employee for HD\nwithout his consent (Huggins et al. 1990). In 2002, the Burlington\nNorthern Santa Fe Railway Company (BNSF) was ordered to pay up to $2.2\nmillion to employees whom they had secretly tested for a genetic\nvariation supposed to be associated with carpal tunnel syndrome.\nIncredibly enough, one of the HGP's own labs—the\nDOE's Lawrence Berkeley Laboratory—was discovered to be\ntesting the sickle cell status of African American employees without\ntheir consent using blood samples submitted during their annual\nphysicals. The problem of genetic discrimination has received extensive\nattention from ELSI scholars, activists, politicians, and the public\nalike. Finally, in May 2008, the Genetic Information Nondiscrimination\nAct (GINA) was passed which prohibits U.S. insurance companies and\nemployers from discriminating on the basis of information derived from\ngenetic\n tests.[52]\nThere have been criticisms of this\nlegislative approach, however, and not only from insurers and\nemployers. The criticisms are directed at misconceptions underlying the\nsingling out of genetic information for protection: “These\nmisconceptions include the presumption that a clear distinction exists\nbetween genetic and nongenetic information, tests, and diseases and the\ngenetic essentialist belief that genetic information is more\ndefinitive, has greater predictive value, and is a greater threat to\nour privacy than is nongenetic medical information” (Beckwith and\nAlper 1998, p. 208; see also Rothstein 2005). This issue of\n“genetic exceptionalism” is taken up in the next section\n(“Genetic Privacy”). \n\nEthical debates over genetic privacy have been closely tied to\nworries about genetic discrimination by insurers and employers who have\nthird-party access to genetic information from tests and research.\nAlthough patient confidentiality and genetic privacy are protected\nwithin the physician-patient relationship by professional codes of\nethics, this protection is threatened by new information technologies\nand the cost-containment provisions of managed care (Orentlicher 1997).\nEmployers have access to extensive health information about their\nemployees, with limited legal protection against divulgence of this\ninformation (Rothstein 1997). Someone who undergoes genetic testing as\na research subject may be forced to divulge this when applying for\ninsurance (Clayton 1997a). Watson and other HGP proponents seemed aware\nfrom the outset that failure to address the problem of genetic\ndiscrimination would threaten the success of the project. Like many\npeople, Watson advanced the legal protection of genetic privacy as the\nsolution: “I think that somehow we have to get it into the laws\nthat anyone's DNA—the message it gives—is\nconfidential and that the only one who has a right to look at it is the\nperson herself or himself” (1992, p.\n 172).[53] \n\nThe strategy adopted in the U.S. to counter genetic discrimination\nthrough legislation has been to treat genetic information as inherently\nprivate and different from other medical\n information.[54]\nThe Genetic Privacy Act,\nproposed by George Annas, Leonard Glantz, and Patricia Roche and\ndesigned to prevent the unauthorized disclosure of genetic information,\nconsiders genetic information to be “uniquely powerful and\nuniquely personal,” and therefore deserving of “unique\nprivacy protection” under the law (Annas et al. 1995). Various\nreasons why genetic information should be regarded as unique or special\nare offered: genetic information is considered fundamentally important\nto personal identity (Andrews 1997); people may opt to have genetic\ntesting without considering the effects of such knowledge (Andrews\n1997); access to and control of genetic information make it possible for\nothers to have power over a person's life (Annas 1994); genetic\ninformation provides information about an individual's family\nmembers (Annas 1994; Andrews 1997); genetic information serves as a\n“future diary” (Annas 1993); the ease with which DNA\ntesting can be carried out makes possible its surreptitious\nuse—to ascertain paternity, for example (Richards 2001). The\napproach is challenged by bioethicist Thomas Murray, who dubs it\n“genetic exceptionalism” (1997, p. 61). Murray argues that\ngenetic information is not unique or even distinctive for a number of\nreasons: many diseases cannot be classified as genetic or nongenetic;\ngenetic information can be gleaned from sources other than DNA; it is\ndifficult to separate genetic and nongenetic information on medical\nrecords; genetic exceptionalism follows from, and in turn fosters,\nmyths of genetic determinism and genetic\n reductionism.[55]\nQuite aside from this\nconceptual debate, there are also practical problems. If genetic\nprivacy laws allow people the freedom to divulge their status\n“voluntarily” as they choose, insurers can offer low\npremiums as an incentive to applicants who agree to submit to testing\nand are found to be at low risk, and employers could hire only those\napplicants willing to undergo testing, just as pre-employment medical\nexams and drug screens are carried out now. \n\nLegal philosopher Anita Allen (1997) identifies four dimensions of\ngenetic privacy: informational privacy, decisional privacy, physical\nprivacy, and proprietary privacy. These provide a suitable framework\nfor discussing various ethical concerns about genetic privacy. The\nworries about genetic discrimination and third-party access to genetic\ninformation already discussed are included in informational\nprivacy (also associated with confidentiality, secrecy, and\nanonymity). So are other problems of third-party access which arise\ncloser to home. A person's test results often have implications\nfor family members who have not undergone testing, and despite the\nsacrifice of confidentiality involved, some geneticists call for family\nmembers to be informed when they are at risk of a serious disease which\nis preventable or treatable (see Rhodes 2000). Genetic counselors\nconfront dilemmas about keeping or revealing secrets, for example,\ninvolving the diagnosis of a child's condition or misattributed\npaternity that modifies genetic risk information within families\n(Biesecker 1997). Decisional privacy protects people's\nability to make autonomous choices. Decisional privacy is implicated\nwhen people who do not wish to know if they are at risk for a certain\ndisease face insurance- or employment-based testing or population-wide\nscreens, or perhaps feel pressured by relatives to be tested in order\nto locate a familial mutation. For example, in the case of HD, given\nthat no treatment at all exists, many at-risk individuals choose not to\navail themselves of testing (World Federation 1993). Bioethical debate\nover whether individuals have a right to remain ignorant about their\ngenetic make-up centers on the requirements of autonomy: it is\nalternatively argued that autonomy is advanced when a person's\nrefusal to be tested is respected (Takala 1999, Takala and Hayry 2000);\nthat there is a duty to know genetic information that could affect\ndecision-making, but given possible social consequences, the decision\nnot to know rests with individuals (Rhodes 1998, 2000); and that the\nfailure to consider all information relevant to making future choices\nundermines our ability to be rational, self-governing agents (Harris\nand Keywood 2001). Decisional privacy and the requirements of autonomy\nare also at issue in the debate over the legitimacy of parental consent\nfor the genetic testing of minors for late-onset conditions (Clayton\n1997b; Cohen 1998; Sevick et al. 2005). Physical (as well as\ndecisional) privacy becomes an issue when genetic tests or\nscreens are performed without a person's voluntary, informed\nconsent. This occurs, for example, when researchers use stored DNA\nsamples for purposes that go beyond those to which their donors\nprovided consent. For example, many countries have decades of stored\n“Guthrie cards” with newborn blood samples provided for\nmetabolic screening. In the U.S., concern has been raised about the\npaucity of regulations regarding third-party access to these as DNA\n“banks” for research (McEwen and Reilly 1994); in\nAustralia, interest in correlating health records with 30 years of\nblood test results provided by newborn screening has resulted in\nconsultation over the use and storage of genetic samples and\ninformation (Ankeny 2003). Proprietary privacy is involved\nshould a person's genetic material or information be appropriated\nfor economic purposes. Moore v. Regents of the University of\nCalifornia is a well-known litigation case involving proprietary\n(as well as decisional and physical) privacy. In the course of his\ntreatment for leukemia at the UCLA Medical Center, John Moore's\nspleen was removed, and then, without his knowledge or consent, used to\ndevelop a commercially lucrative cell line. The court ruled that\nMoore's right to informed consent was breached, but that he had\nno property rights over the cell line. Philosophers have approached\nthis question about property rights over DNA from Lockean and Rawlsian\nperspectives on justice (Farrelly 2002; Moore 2000). \n\nAs DNA banking and DNA data banking efforts proliferate with few\nlaws to restrict the future use of samples or prevent the transfer of\ndata, and given that any blood or tissue collection is a potential DNA\nbank, people may lose any hope of keeping their genetic profile\nprivate. The U.S. Department of Defense has the world's largest\nDNA bank: all military personnel are required to provide blood and\nsaliva samples so that the remains of missing soldiers can be\nidentified (McEwen 1997). The U.K.'s forensic DNA bank will\neventually include a substantial portion of its population: the 2003\nCriminal Justice Act requires anyone arrested for—not necessarily\ncharged with, much less convicted of—a recordable offense to\nsubmit a DNA\n sample[56];\nin the U.S. such requirements vary\nstate-by-state (McEwen 1997). As the technology becomes cheaper, the\nproposal to create a national DNA data bank by sampling every newborn\nchild could arise (Bereano 1992). This repository of data would serve a\nnumber of purposes in addition to forensic ones: provide an identity\ntag (like social security numbers); enable economic planning based on\ngenetic risk factors; support research into the genetic basis of\nmedical and behavioral traits; identify remains; identify children\nrecovered after being kidnapped; monitor individuals presumed to be\npredisposed to criminal violence; identify potential transplant donors;\netc. A number of countries have already created population-level DNA\nbanks and data banks to aid in research into complex diseases (Kaiser\n2002), with deCODE Genetics’ initiative in Iceland, which\ncombines genetic, medical, and genealogical data and operates on the\nbasis of presumed consent, the most well-known. On a smaller scale, DNA\nis also banked by researchers in academic institutions, commercial\nlabs, and hospitals. Additional privacy concerns arise in research\ncontexts where DNA profiles are linked to phenotypic features (clinical\ndata and lifestyle information) and not used just for forensic\nidentification. Bioethicists looking at “biobanks” have\nfocused on the problems prospective research poses for consent, and the\nadequacy of such provisions as opting in or opting out, waiving or\ngiving blanket consent, withdrawing or renewing consent, etc.\n(Helgesson and Johnsson 2005; Shickle 2006). Onora O’Neill (2001)\nquestions whether informed consent even remains a realistic goal given\nthe amount of genetic data being banked. She argues that consent to\nspecific propositions cannot be extended to closely related ones, and\nthat the lack of explicit dissent cannot be taken as tacit or implied\nconsent. O’Neill reminds us that consent to the collection,\nstorage, and use of genetic and other data is not sought within the\nconfines of the doctor-patient relationship where privacy is governed\nby professional standards; rather, the relationship is many-many and\nprivacy is dependent on methods of data protection for which public\nrather than individual consent becomes appropriate. However, as genetic\nsequencing becomes a routine part of health care, with all the\ninformation researchers need contained in patient records, similar\nconcerns about privacy will arise in the health care sector (Hansson\n2004). \n\nThe HGP will assist scientists in the identification of genes and\ntheir functions. As scientists gain knowledge about how various genes\ncontribute to phenotype, genetic modification will become possible.\nSuch interventions could involve knocking out a gene with a detrimental\nrole, or inserting a gene with a beneficial role, though because of\ngene-gene and gene-environment interactions (as discussed in section\n2.1.4.), it will be difficult to predict the effects of many such\ninterventions. The first approved GM procedure was performed in 1990\nwhen NIH scientists genetically modified white blood cells in Ashanti\nDeSilva, a four-year-old girl with adenosine deaminase deficiency\n(ADA). Over 1000 clinical trials have since been performed (Rasko et\nal. 2006, Gene Therapy Clinical Trials Worldwide Database),\nwith studies carried out on numerous diseases, including CF,\nFranconi's anemia, muscular dystrophy, and hemophilia. A serious\nsetback occurred when 18-year-old Jesse Gelsinger died in a University\nof Pennsylvania gene therapy trial for ornithine transcarbamylase (OTC)\ndeficiency in 1999. Shortly after, a French research team successfully\nrestored the immune systems of young children with severe combined\nimmunodeficiency (SCID) through gene therapy; this success was\ntempered, however, when several of the children later developed\nleukemia as a result of the procedure. Given these problems with safety\nand efficacy, gene therapy trials have shifted from\n“single-gene” diseases to conditions like cancer,\ncardiovascular disease, and rheumatoid arthritis, where vector\napplications are more local and transient (Branca 2005). \n\nProcedures carried out to date involve the GM of somatic cells, but\nGM could potentially be applied to gametes or IVF embryos, resulting in\nthe genetic changes being passed on to future generations: this is\nreferred to as germ-line GM, or more recently (Rasko et al. 2006),\n“inheritable genetic modification.” Some bioethicists argue\nthat the distinction between somatic cell and germ-line GM has moral\nsignificance, and that somatic cell interventions alone are\npermissible. A number of arguments are offered against germ-line GM:\nthe long-term effects are uncertain; germ-line interventions are\neugenic given that future generations are not patients; only existing\npersons can provide informed consent (Lappé 1991); humans have a\nbasic right to be born with an unmodified genome (Mauron and Thevoz\n1991); some modifications alter personal identity (Zohar 1991); and\nhuman trials face daunting ethical and practical challenges (Dresser\n2004). Bioethicists who support germ-line GM counter these arguments\nwith others: parents have reproductive autonomy (Zimmerman 1991); the\nfetus is being treated as a patient in prenatal surgeries, so why not\nthe embryo (Nolan 1991); we constantly modify the conditions future\ngenerations inherit, and usually not with altruistic purposes (e.g.,\ndegraded environments) (Peters 1995); members of future generations\nwould presumably consent to an augmentation of Rawlsian primary goods\n(Allhoff 2005); modifications could be reversed if harmful effects\narise (Moseley 1991); the germ line might be the only or most effective\nlevel for intervention in some diseases (Munson and Davis 1992;\nZimmerman 1991); germ-line interventions would avoid subsequent\ngenerations of a family having to undergo somatic cell GM or worry\nabout carrier status (Munson and Davis 1992). \n\nThe distinction between therapy and enhancement, as it applies to\nboth somatic cell and germ-line GM, is also considered by some\nbioethicists to have moral significance: interventions that restore\nnormal function from a state of disease or disability (negative GM) are\nconsidered permissible whereas interventions that improve on normal\nfunction (positive GM) are\n not.[57]\nArguments against genetic enhancement\ninclude: clinical risks outweigh possible benefits (Anderson 1990); it\nviolates parents’ fiduciary responsibilities (Schonfeld 2003);\ngenetic diversity will be diminished (Parens 1995); the socially\nacceptable range of traits will be narrowed (Agar 1995); the principle\nof equality of opportunity (Daniels 1994) and the Rawlsian imperative\nto help the worst off (Fleck 1994) support negative but not positive\nGM; appreciation of shared human “fragility” and the\naccomplishments of individuals may diminish (Parens 1995). A further\ndistinction can be drawn between enhancements that go beyond species\ntypicality to improve health (e.g. disease-resistant genes) and\nenhancements of social value (e.g. eye color), and it is argued that\nthere is no moral difference between using GM to augment function and\nusing GM to restore function if the aim is disease prevention (Harris\n1993; Juengst 1997). Some bioethicists who support GM for enhancement\nas well as therapeutic purposes counter these arguments by asserting\nthe reproductive autonomy of parents (Stock 2003). Others point to the\nlack of an objective and/or morally justified distinction between\ntherapy and enhancement, health and disease (Goering 2000;\nResnik\n 2000).[58]\nIndeed, with GM, we humans ourselves determine what counts as\n“normal” genetic variation, no longer needing to look to a\npre-existing species-typical functional norm (Resnik 1997b). The moral\npermissibility of particular genetic interventions, therapies and\nenhancements alike, might instead be assessed according to Rawlsian\nprinciples—with equality of opportunity and concern for the worst\noff supporting the prohibition of some enhancements and the guarantee\nof a minimal level of genetic health (Resnik 1997b), or a permissible\nintervention one which, when contemplated in the original position\nbehind a veil of ignorance, can be seen to prevent a condition that\nwould be detrimental in all societies or bring about a condition that\nwould never be detrimental in any society (Goering 2000). \n\nGM of the germ line for either therapeutic or enhancement purposes\nand GM of somatic cells for enhancement purposes have not yet been\ncarried out in\n humans.[59]\nOf the four classes of interventions\ndelineated by somatic-cell/germ-line and therapy/enhancement\ndistinctions, only germ-line GM for enhancement purposes remains\nparticularly contentious among bioethicists (Allhoff 2005). Of course,\nGM of the germ line of plants and nonhuman animals has\nbecome\n commonplace,[60]\nand some commentators argue that the use of GM for enhancement purposes\nin humans is inevitable (Baylis and Robert 2004; Stock 2003). After\nall, look at parents who seek every advantage for their child on route\nto an Ivy League education, athletes who use illegal\nperformance-enhancing drugs, increased use of plastic surgery,\nrecreational use of Viagra, etc. Genetic modification of the germ line\nmight be even more likely to be embraced for positive/enhancement than\nnegative/therapeutic purposes, since if IVF is being used, concerns\nabout a particular disease can be allayed with PGD and selective\nimplantation. However, as more and more conditions—whether\ndiseases or not—can be screened for in embryos, distinctions\nbetween therapy and enhancement and enhancement for health vs. social\npurposes—already problematic conceptually, a situation compounded\nby the newfound plasticity of the species adaptive norm—may break\ndown even further in practice. \n\nAlready in 1969, Sinsheimer foresaw the promise of molecular biology\nto remake human nature: “For the first time in all time, a living\ncreature understands its origin and can undertake to design its\nfuture” (in Kevles 1992, p. 18). Remaking human nature is likely\nto begin with genetic modifications that convey the possibility of\nresistance to a serious disease, like HIV/AIDS, or minimize the effects\nof aging to extend lifespan, but transhumanists who view human nature\nas “a work-in-progress, a half-baked beginning that we can learn\nto remold in desirable ways” welcome improvements in memory,\nintelligence, and emotional capacities as well (Bostrom 2003, p. 493).\nWidespread inheritable GM could lead to a “radical rupture”\nwith evolutionary processes as we understand them\ntoday—undermining the distinction between acquired and\ninheritable traits, challenging our abilities to reconstruct genetic\ngenealogies, and even creating a new hominid species (Baylis and Robert\n2006). Words chosen for titles of recent books—for example,\n“the future of human nature” (Habermas 2003), “our\nposthuman future” (Fukuyama 2002), and “redesigning\nhumans” (Stock 2003)—reveal that a serious debate is\nunderway about the prospect of a genetically engineered human nature\nand what this means for all of us. Some are practically giddy about\nthis “journey to destinations of new imagination” (Stock\n2003, p. 1); others warn that it will undo the “intimate\nconnection between human nature and human notions of rights, justice,\nand morality” (Fukuyama 2002, p. 101). \n\nThe HGP was initially criticized on the basis that the\ngovernment's support of such a large-scale project would take\nresources away from other programs arguably more important for health\nand wellbeing, such as, in the U.S., instituting a system of universal\nhealth care or funding prenatal care for poor women. Cures for disease\narising from the HGP, after all, are likely to benefit future not\nexisting people (Murphy 1994). These concerns were compounded by the\nworry that increased scientific attention paid to genetic factors\nimplicated in health and disease would promote interventions at the\nlevel of the genome—genetic tests, genetic modifications, drug\ndevelopment based on gene sequences, etc.—and that the need to\naddress the impacts of harmful physical and social environments due to\ntoxins, poverty, racism, etc. would be ignored (Hubbard and Wald 1993).\nThe problem of racial profiling in the legal system was raised: if\nmembers of particular racial and ethnic groups are more likely to be\narrested, charged, or convicted of a criminal offense, they are more\nlikely to be required to provide DNA samples to forensic databases, and\ntherefore more likely to come back into the system with future\noffenses. One proposed solution is for a national database which\nincludes everyone (Kitcher 1996), though it is also argued that this\ncompromises the autonomy and dignity of individual citizens (Boylan\n2002). And given the history of using biological explanations to\nprovide ideological justification for social inequalities associated\nwith oppressive power structures, the prospective use of molecular\ngenetics to explain race and sex differences has also met with caution\n(Hubbard 1994). \n\nWith completion of the HGP, concerns of justice increasingly focus\non the distribution of benefits and burdens of genetic technologies,\nespecially given that the HGP was funded by public monies. One of the\n“majestic horizons” identified by Clinton in June 2000\nrepresented a call for justice—specifically, ethical respect for\n“our oldest and most cherished human values” to ensure that\ngenome science benefits “all citizens of the world” and\nprevents discrimination. And yet, another of Clinton's\n“majestic horizons”—biotechnological development in\nthe private sector based on the identification of all human genes and\ntheir functions—raises significant challenges for justice. While\nthe HGP's vision of “individualized preventive\nmedicine” will no doubt promote biotechnological development in\nthe private sector, is this development likely to benefit “all\ncitizens of the world”? Globally, in the developing world,\ninfectious diseases are more of a priority than genetic diseases; in\nthe U.S., millions of citizens and residents are without or lack\nadequate health care coverage for the routine preventive care and\ntreatment of acute illnesses and injuries proponents of genomic\nmedicine take for granted. Profits direct pharmaceutical research and\ndevelopment: efforts are expended on conditions that are relatively\nfrequent and sometimes minor (e.g. male-pattern baldness [Ellis and\nSinclair 2008]); conditions prevalent in affluent countries receive\nmore attention than those prevalent in developing countries (Flory and\nKitcher 2004); and populations in the developing world are vulnerable\nto exploitation (Dickenson 2004). “All citizens of the\nworld” will not have access to the expensive new drugs: in the\nU.S., beneficiaries will be the wealthy and those with generous\nprescription drug plans; in the developing world, it remains a struggle\nto make drugs to treat serious diseases like HIV/AIDS affordable and\naccessible to all who need them. \n\nConcerns have arisen, not just that there will be unequal access to\nthe benefits of genetic technologies, but that the use of these\ntechnologies will exacerbate existing patterns of social and economic\ninequality. As mentioned in section 2.2.3, genetic discrimination by\ninsurers and employers could lead to creation of a “genetic\nunderclass”; similarly, should socioeconomically advantaged\nfamilies have greater access to prenatal genetic testing, an increased\nrelative frequency of conditions like CF, sickle cell anemia, or Down\nsyndrome would occur among less privileged families. To the extent that\nrace and ethnicity correlate with socioeconomic status, some\ngroups—already affected by disparities in health outcomes\nunrelated to genetic differences—will be disproportionately\nrepresented among this “genetic underclass.” If genetic\nenhancement becomes available and is used disproportionately by those\nbelonging to advantaged groups, there is the potential for the\nsocioeconomic gap to widen further—think of the diminished range\nof opportunities available to the genetically unenhanced Vincent in the\nfilm Gattaca. An entirely new class structure based on genetic\nmakeup as remolded by culture is foreseen: some welcome\nhumanity's fragmentation into “independent\nbreeds—future human Saint Bernard and dachshund analogs”\ncorresponding to the enhanced and the unenhanced, or family lineages\nwhere similar enhancement choices are made in successive generations\n(Stock 2003); others fear this imperils the continuation of liberal\ndemocratic society (Mehlman 2003; Mehlman and Botkin 1998). There are\nalso those who see possibilities for increased equality. One reason is\nthat environments could be tailored to the genetic predispositions of\nindividuals based on better knowledge of gene-environment interactions\n(Buchanan et al. 2000). Another reason is that the genetic technologies\non the horizon challenge the natural-social distinction by making the\nnatural lottery, like the social lottery, alterable: the state could\nredistribute or narrow the range of differences in natural abilities as\nis often done for social resources in order to promote equality of\nopportunity or benefit the worst off (Buchanan et al. 2000;\nResnik\n 1997b).[61] \n\nThese concerns about differential access to the benefits of genetic\ntechnologies and the likely social consequences of such differential\naccess have been addressed by bioethicists and moral philosophers from\nperspectives informed by competing theories of distributive justice.\nLibertarians support the “genetic supermarket” famously\nenvisioned by Nozick, where prospective parents and resulting market\nforces determine which genetic modifications are offered—not\ncentralized decisions made by biologists or government. It has been\nnoted, however, that what is harmless or beneficial in individual cases\n(e.g. sex selection, increased longevity, competitive goods like beauty\nor intelligence) may be neutral or even harmful for society taken\ncollectively (Kavka 1994). And yet, given that the state does not\nrestrict parental autonomy when it comes to environmental boosts to\ntheir children's abilities (e.g. private schools, music lessons),\nit is argued that there is no principled distinction between genes and\nenvironments which would override a similar right to genetic\nenhancements (Buchanan et al. 2000). Liberals have used Rawlsian\nprinciples to urge a variety of state-imposed limits on a free market\nin genetic technologies (Brown 2001; Buchanan et al. 2000; Daniels\n1994; Farrelly 2002; Resnik 1997b). In their 2000 book From Chance\nto Choice: Genetics and Justice, in order to protect equal\nopportunity, Allen Buchanan, Dan Brock, Norman Daniels, and Daniel\nWikler argue that the state should ensure access to a “genetic\ndecent minimum” which includes interventions directed to natural\nprimary goods—whether treatments or enhancements—insofar as\nthese are consistent with the neutrality of liberal democracies\nconcerning conceptions of the\n good.[62]\nParental choices would be limited only where\nthese are inconsistent with a child's right to an open future,\ncarry a risk to public good (e.g. sex selection resulting in societal\nimbalance), diminish equal opportunity if available only to the\ncomparatively well off, or involve enhancements with “positional\nadvantages” (e.g. height) that would be self-defeating if widely\navailable and unfair—and contrary to Rawls’ difference\nprinciple which requires permissible inequalities to work to the\nadvantage of all, especially the worst off—if not, and where\nenhancements would lead to the disabling of others by raising the bar\nfor what is required to participate in society's “dominant\ncooperative\n scheme.”[63] \n\nOf particular interest in current philosophical debates about\njustice and the genome is the consensus which has emerged among\nparticipants that our usual ways of thinking about justice are\nthemselves challenged by the new genetic technologies (Buchanan et\nal.\n 2000).[64]\nTheories of justice are typically based on conceptions of human nature,\nbut with the capacity to remake human nature, this foundation\ndisappears. Approaches to distributive justice seek to compensate for\nnatural inequalities existing at birth among individuals of fixed\nidentities, but considerations of justice are now implicated earlier,\nin decisions about the distribution of genetic characteristics,\ndecisions which affect the identities of those born. The potential for\ngenetic stratification places at risk the assumption central to all\ntheorizing about justice—that we share a moral community.","contact.mail":"lisa.gannett@smu.ca","contact.domain":"smu.ca"}]
