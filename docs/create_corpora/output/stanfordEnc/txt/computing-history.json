[{"date.published":"2000-12-18","date.changed":"2006-06-09","url":"https://plato.stanford.edu/entries/computing-history/","author1":"B. Jack Copeland","author1.info":"https://www.canterbury.ac.nz/arts/contact-us/people/jack-copeland.html","entry":"computing-history","body.text":"\n\n\n\nHistorically, computers were human clerks who calculated in accordance\nwith effective methods. These human computers did the sorts of\ncalculation nowadays carried out by electronic computers, and many\nthousands of them were employed in commerce, government, and research\nestablishments. The term computing machine, used increasingly\nfrom the 1920s, refers to any machine that does the work of a human\ncomputer, i.e., any machine that calculates in accordance with\neffective methods. During the late 1940s and early 1950s, with the\nadvent of electronic computing machines, the phrase ‘computing\nmachine’ gradually gave way simply to ‘computer’,\ninitially usually with the prefix ‘electronic’ or\n‘digital’. This entry surveys the history of these\nmachines. \n\n\n\n\nCharles Babbage was Lucasian Professor of Mathematics at Cambridge\nUniversity from 1828 to 1839 (a post formerly held by Isaac Newton).\nBabbage's proposed Difference Engine was a special-purpose digital\ncomputing machine for the automatic production of mathematical tables\n(such as logarithm tables, tide tables, and astronomical tables). The\nDifference Engine consisted entirely of mechanical components —\nbrass gear wheels, rods, ratchets, pinions, etc. Numbers were\nrepresented in the decimal system by the positions of 10-toothed metal\nwheels mounted in columns. Babbage exhibited a small working model in\n1822. He never completed the full-scale machine that he had designed\nbut did complete several fragments. The largest — one ninth of\nthe complete calculator — is on display in the London Science\nMuseum. Babbage used it to perform serious computational work,\ncalculating various mathematical tables. In 1990, Babbage's Difference\nEngine No. 2 was finally built from Babbage's designs and is also on\ndisplay at the London Science Museum.  \n\nThe Swedes Georg and Edvard Scheutz (father and son) constructed a\nmodified version of Babbage's Difference Engine. Three were made, a\nprototype and two commercial models, one of these being sold to an\nobservatory in Albany, New York, and the other to the\nRegistrar-General's office in London, where it calculated and printed\nactuarial tables. \n\nBabbage's proposed Analytical Engine, considerably more ambitious\nthan the Difference Engine, was to have been a general-purpose\nmechanical digital computer. The Analytical Engine was to have had a\nmemory store and a central processing unit (or ‘mill’) and\nwould have been able to select from among alternative actions\nconsequent upon the outcome of its previous actions (a facility\nnowadays known as conditional branching). The behaviour of the\nAnalytical Engine would have been controlled by a program of\ninstructions contained on punched cards connected together with ribbons\n(an idea that Babbage had adopted from the Jacquard weaving loom).\nBabbage emphasised the generality of the Analytical Engine, saying\n‘the conditions which enable a finite machine to make\ncalculations of unlimited extent are fulfilled in the Analytical\nEngine’ (Babbage [1994], p. 97). \n\nBabbage worked closely with Ada Lovelace, daughter of the poet\nByron, after whom the modern programming language ADA is named.\nLovelace foresaw the possibility of using the Analytical Engine for\nnon-numeric computation, suggesting that the Engine might even be\ncapable of composing elaborate pieces of music. \n\nA large model of the Analytical Engine was under construction at the\ntime of Babbage's death in 1871 but a full-scale version was never\nbuilt. Babbage's idea of a general-purpose calculating engine was never\nforgotten, especially at Cambridge, and was on occasion a lively topic\nof mealtime discussion at the war-time headquarters of the Government\nCode and Cypher School, Bletchley Park, Buckinghamshire, birthplace of\nthe electronic digital computer. \n\nThe earliest computing machines in wide use were not digital but\nanalog. In analog representation, properties of the representational\nmedium ape (or reflect or model) properties of the represented\nstate-of-affairs. (In obvious contrast, the strings of binary digits\nemployed in digital representation do not represent by means\nof possessing some physical property — such as length —\nwhose magnitude varies in proportion to the magnitude of the property\nthat is being represented.) Analog representations form a diverse\nclass. Some examples: the longer a line on a road map, the longer the\nroad that the line represents; the greater the number of clear plastic\nsquares in an architect's model, the greater the number of windows in\nthe building represented; the higher the pitch of an acoustic depth\nmeter, the shallower the water. In analog computers, numerical\nquantities are represented by, for example, the angle of rotation of a\nshaft or a difference in electrical potential. Thus the output voltage\nof the machine at a time might represent the momentary speed of the\nobject being modelled.  \n\nAs the case of the architect's model makes plain, analog\nrepresentation may be discrete in nature (there is no such\nthing as a fractional number of windows). Among computer scientists,\nthe term ‘analog’ is sometimes used narrowly, to indicate\nrepresentation of one continuously-valued quantity by another\n(e.g., speed by voltage). As Brian Cantwell Smith has remarked: \n\nJames Thomson, brother of Lord Kelvin, invented the mechanical\nwheel-and-disc integrator that became the foundation of analog\ncomputation (Thomson [1876]). The two brothers constructed a device for\ncomputing the integral of the product of two given functions, and\nKelvin described (although did not construct) general-purpose analog\nmachines for integrating linear differential equations of any order and\nfor solving simultaneous linear equations. Kelvin's most successful\nanalog computer was his tide predicting machine, which remained in use\nat the port of Liverpool until the 1960s. Mechanical analog devices\nbased on the wheel-and-disc integrator were in use during World War I\nfor gunnery calculations. Following the war, the design of the\nintegrator was considerably improved by Hannibal Ford (Ford [1919]).  \n\nStanley Fifer reports that the first semi-automatic mechanical\nanalog computer was built in England by the Manchester firm of\nMetropolitan Vickers prior to 1930 (Fifer [1961], p. 29); however, I\nhave so far been unable to verify this claim. In 1931, Vannevar Bush,\nworking at MIT, built the differential analyser, the first large-scale\nautomatic general-purpose mechanical analog computer. Bush's design was\nbased on the wheel and disc integrator. Soon copies of his machine were\nin use around the world (including, at Cambridge and Manchester\nUniversities in England, differential analysers built out of kit-set\nMeccano, the once popular engineering toy). \n\nIt required a skilled mechanic equipped with a lead hammer to set up\nBush's mechanical differential analyser for each new job. Subsequently,\nBush and his colleagues replaced the wheel-and-disc integrators and\nother mechanical components by electromechanical, and finally by\nelectronic, devices. \n\nA differential analyser may be conceptualised as a collection of\n‘black boxes’ connected together in such a way as to allow\nconsiderable feedback. Each box performs a fundamental process, for\nexample addition, multiplication of a variable by a constant, and\nintegration. In setting up the machine for a given task, boxes are\nconnected together so that the desired set of fundamental processes is\nexecuted. In the case of electrical machines, this was done typically\nby plugging wires into sockets on a patch panel (computing machines\nwhose function is determined in this way are referred to as\n‘program-controlled’). \n\nSince all the boxes work in parallel, an electronic differential\nanalyser solves sets of equations very quickly. Against this has to be\nset the cost of massaging the problem to be solved into the form\ndemanded by the analog machine, and of setting up the hardware to\nperform the desired computation. A major drawback of analog computation\nis the higher cost, relative to digital machines, of an increase in\nprecision. During the 1960s and 1970s, there was considerable interest\nin ‘hybrid’ machines, where an analog section is controlled\nby and programmed via a digital section. However, such machines are now\na rarity. \n\nIn 1936, at Cambridge University, Turing invented the principle of the\nmodern computer. He described an abstract digital computing machine\nconsisting of a limitless memory and a scanner that moves back and\nforth through the memory, symbol by symbol, reading what it finds and\nwriting further symbols (Turing [1936]). The actions of the scanner are\ndictated by a program of instructions that is stored in the memory in\nthe form of symbols. This is Turing's stored-program concept, and\nimplicit in it is the possibility of the machine operating on and\nmodifying its own program. (In London in 1947, in the course of what\nwas, so far as is known, the earliest public lecture to mention\ncomputer intelligence, Turing said, ‘What we want is a machine\nthat can learn from experience’, adding that the\n‘possibility of letting the machine alter its own instructions\nprovides the mechanism for this’ (Turing [1947] p. 393). Turing's\ncomputing machine of 1936 is now known simply as the universal Turing\nmachine. Cambridge mathematician Max Newman remarked that right from\nthe start Turing was interested in the possibility of actually building\na computing machine of the sort that he had described (Newman in\ninterview with Christopher Evans in Evans [197?].  \n\nFrom the start of the Second World War Turing was a leading\ncryptanalyst at the Government Code and Cypher School, Bletchley Park.\nHere he became familiar with Thomas Flowers' work involving large-scale\nhigh-speed electronic switching (described below). However, Turing\ncould not turn to the project of building an electronic stored-program\ncomputing machine until the cessation of hostilities in Europe in\n1945. \n\nDuring the wartime years Turing did give considerable thought to the\nquestion of machine intelligence. Colleagues at Bletchley Park recall\nnumerous off-duty discussions with him on the topic, and at one point\nTuring circulated a typewritten report (now lost) setting out some of\nhis ideas. One of these colleagues, Donald Michie (who later founded\nthe Department of Machine Intelligence and Perception at the University\nof Edinburgh), remembers Turing talking often about the possibility of\ncomputing machines (1) learning from experience and (2) solving\nproblems by means of searching through the space of possible solutions,\nguided by rule-of-thumb principles (Michie in interview with Copeland,\n1995). The modern term for the latter idea is ‘heuristic\nsearch’, a heuristic being any rule-of-thumb principle that cuts\ndown the amount of searching required in order to find a solution to a\nproblem. At Bletchley Park Turing illustrated his ideas on machine\nintelligence by reference to chess. Michie recalls Turing experimenting\nwith heuristics that later became common in chess programming (in\nparticular minimax and best-first). \n\nFurther information about Turing and the computer, including his\nwartime work on codebreaking and his thinking about artificial\nintelligence and artificial life, can be found in Copeland 2004. \n\nWith some exceptions — including Babbage's purely mechanical\nengines, and the finger-powered National Accounting Machine - early\ndigital computing machines were electromechanical. That is to say,\ntheir basic components were small, electrically-driven, mechanical\nswitches called ‘relays’. These operate relatively slowly,\nwhereas the basic components of an electronic computer —\noriginally vacuum tubes (valves) — have no moving parts save\nelectrons and so operate extremely fast. Electromechanical digital\ncomputing machines were built before and during the second world war by\n(among others) Howard Aiken at Harvard University, George Stibitz at\nBell Telephone Laboratories, Turing at Princeton University and\nBletchley Park, and Konrad Zuse in Berlin. To Zuse belongs the honour\nof having built the first working general-purpose program-controlled\ndigital computer. This machine, later called the Z3, was functioning in\n1941. (A program-controlled computer, as opposed to a stored-program\ncomputer, is set up for a new task by re-routing wires, by means of\nplugs etc.)  \n\nRelays were too slow and unreliable a medium for large-scale\ngeneral-purpose digital computation (although Aiken made a valiant\neffort). It was the development of high-speed digital techniques using\nvacuum tubes that made the modern computer possible. \n\nThe earliest extensive use of vacuum tubes for digital\ndata-processing appears to have been by the engineer Thomas Flowers,\nworking in London at the British Post Office Research Station at Dollis\nHill. Electronic equipment designed by Flowers in 1934, for controlling\nthe connections between telephone exchanges, went into operation in\n1939, and involved between three and four thousand vacuum tubes running\ncontinuously. In 1938–1939 Flowers worked on an experimental\nelectronic digital data-processing system, involving a high-speed data\nstore. Flowers' aim, achieved after the war, was that electronic\nequipment should replace existing, less reliable, systems built from\nrelays and used in telephone exchanges. Flowers did not investigate the\nidea of using electronic equipment for numerical calculation, but has\nremarked that at the outbreak of war with Germany in 1939 he was\npossibly the only person in Britain who realized that vacuum tubes\ncould be used on a large scale for high-speed digital computation. (See\nCopeland 2006 for m more information on Flowers' work.) \n\nThe earliest comparable use of vacuum tubes in the U.S. seems to have\nbeen by John Atanasoff at what was then Iowa State College (now\nUniversity). During the period 1937–1942 Atanasoff developed\ntechniques for using vacuum tubes to perform numerical calculations\ndigitally. In 1939, with the assistance of his student Clifford Berry,\nAtanasoff began building what is sometimes called the Atanasoff-Berry\nComputer, or ABC, a small-scale special-purpose electronic digital\nmachine for the solution of systems of linear algebraic equations. The\nmachine contained approximately 300 vacuum tubes. Although the\nelectronic part of the machine functioned successfully, the computer as\na whole never worked reliably, errors being introduced by the\nunsatisfactory binary card-reader. Work was discontinued in 1942 when\nAtanasoff left Iowa State.  \n\nThe first fully functioning electronic digital computer was Colossus,\nused by the Bletchley Park cryptanalysts from February 1944.  \n\nFrom very early in the war the Government Code and Cypher School\n(GC&CS) was successfully deciphering German radio communications\nencoded by means of the Enigma system, and by early 1942 about 39,000\nintercepted messages were being decoded each month, thanks to\nelectromechanical machines known as ‘bombes’. These were\ndesigned by Turing and Gordon Welchman (building on earlier work by\nPolish cryptanalysts). \n\nDuring the second half of 1941, messages encoded by means of a\ntotally different method began to be intercepted. This new cipher\nmachine, code-named ‘Tunny’ by Bletchley Park, was broken\nin April 1942 and current traffic was read for the first time in July\nof that year. Based on binary teleprinter code, Tunny was used in\npreference to Morse-based Enigma for the encryption of high-level\nsignals, for example messages from Hitler and members of the German\nHigh Command. \n\nThe need to decipher this vital intelligence as rapidly as possible\nled Max Newman to propose in November 1942 (shortly after his\nrecruitment to GC&CS from Cambridge University) that key parts of\nthe decryption process be automated, by means of high-speed electronic\ncounting devices. The first machine designed and built to Newman's\nspecification, known as the Heath Robinson, was relay-based with\nelectronic circuits for counting. (The electronic counters were\ndesigned by C.E. Wynn-Williams, who had been using thyratron tubes in\ncounting circuits at the Cavendish Laboratory, Cambridge, since 1932\n[Wynn-Williams 1932].) Installed in June 1943, Heath Robinson was\nunreliable and slow, and its high-speed paper tapes were continually\nbreaking, but it proved the worth of Newman's idea. Flowers recommended\nthat an all-electronic machine be built instead, but he received no\nofficial encouragement from GC&CS. Working independently at the\nPost Office Research Station at Dollis Hill, Flowers quietly got on\nwith constructing the world's first large-scale programmable electronic\ndigital computer. Colossus I was delivered to Bletchley Park in January\n1943. \n\nBy the end of the war there were ten Colossi working round the clock\nat Bletchley Park. From a cryptanalytic viewpoint, a major difference\nbetween the prototype Colossus I and the later machines was the\naddition of the so-called Special Attachment, following a key discovery\nby cryptanalysts Donald Michie and Jack Good. This broadened the\nfunction of Colossus from ‘wheel setting’ — i.e.,\ndetermining the settings of the encoding wheels of the Tunny machine\nfor a particular message, given the ‘patterns’ of the\nwheels — to ‘wheel breaking’, i.e., determining the\nwheel patterns themselves. The wheel patterns were eventually changed\ndaily by the Germans on each of the numerous links between the German\nArmy High Command and Army Group commanders in the field. By 1945 there\nwere as many 30 links in total. About ten of these were broken and read\nregularly. \n\nColossus I contained approximately 1600 vacuum tubes and each of the\nsubsequent machines approximately 2400 vacuum tubes. Like the smaller\nABC, Colossus lacked two important features of modern computers. First,\nit had no internally stored programs. To set it up for a new task, the\noperator had to alter the machine's physical wiring, using plugs and\nswitches. Second, Colossus was not a general-purpose machine, being\ndesigned for a specific cryptanalytic task involving counting and\nBoolean operations. \n\nF.H. Hinsley, official historian of GC&CS, has estimated that\nthe war in Europe was shortened by at least two years as a result of\nthe signals intelligence operation carried out at Bletchley Park, in\nwhich Colossus played a major role. Most of the Colossi were destroyed\nonce hostilities ceased. Some of the electronic panels ended up at\nNewman's Computing Machine Laboratory in Manchester (see below), all\ntrace of their original use having been removed. Two Colossi were\nretained by GC&CS (renamed GCHQ following the end of the war). The\nlast Colossus is believed to have stopped running in 1960. \n\nThose who knew of Colossus were prohibited by the Official Secrets\nAct from sharing their knowledge. Until the 1970s, few had any idea\nthat electronic computation had been used successfully during the\nsecond world war. In 1970 and 1975, respectively, Good and Michie\npublished notes giving the barest outlines of Colossus. By 1983,\nFlowers had received clearance from the British Government to publish a\npartial account of the hardware of Colossus I. Details of the later\nmachines and of the Special Attachment, the uses to which the Colossi\nwere put, and the cryptanalytic algorithms that they ran, have only\nrecently been declassified. (For the full account of Colossus and the\nattack on Tunny see Copeland 2006.) \n\nTo those acquainted with the universal Turing machine of 1936, and\nthe associated stored-program concept, Flowers' racks of digital\nelectronic equipment were proof of the feasibility of using large\nnumbers of vacuum tubes to implement a high-speed general-purpose\nstored-program computer. The war over, Newman lost no time in\nestablishing the Royal Society Computing Machine Laboratory at\nManchester University for precisely that purpose. A few months after\nhis arrival at Manchester, Newman wrote as follows to the Princeton\nmathematician John von Neumann (February 1946): \n\nTuring and Newman were thinking along similar lines. In 1945 Turing\njoined the National Physical Laboratory (NPL) in London, his brief to\ndesign and develop an electronic stored-program digital computer for\nscientific work. (Artificial Intelligence was not far from Turing's\nthoughts: he described himself as ‘building a brain’ and\nremarked in a letter that he was ‘more interested in the\npossibility of producing models of the action of the brain than in the\npractical applications to computing’.) John Womersley, Turing's\nimmediate superior at NPL, christened Turing's proposed machine the\nAutomatic Computing Engine, or ACE, in homage to Babbage's Difference\nEngine and Analytical Engine.  \n\nTuring's 1945 report ‘Proposed Electronic Calculator’\ngave the first relatively complete specification of an electronic\nstored-program general-purpose digital computer. The report is\nreprinted in full in Copeland 2005. \n\nThe first electronic stored-program digital computer to be proposed\nin the U.S. was the EDVAC (see below). The ‘First Draft of a\nReport on the EDVAC’ (May 1945), composed by von Neumann,\ncontained little engineering detail, in particular concerning\nelectronic hardware (owing to restrictions in the U.S.). Turing's\n‘Proposed Electronic Calculator’, on the other hand,\nsupplied detailed circuit designs and specifications of hardware units,\nspecimen programs in machine code, and even an estimate of the cost of\nbuilding the machine (£11,200). ACE and EDVAC differed\nfundamentally from one another; for example, ACE employed distributed\nprocessing, while EDVAC had a centralised structure. \n\nTuring saw that speed and memory were the keys to computing.\nTuring's colleague at NPL, Jim Wilkinson, observed that Turing\n‘was obsessed with the idea of speed on the machine’\n[Copeland 2005, p. 2]. Turing's design had much in common with today's\nRISC architectures and it called for a high-speed memory of roughly the\nsame capacity as an early Macintosh computer (enormous by the standards\nof his day). Had Turing's ACE been built as planned it would have been\nin a different league from the other early computers. However, progress\non Turing's Automatic Computing Engine ran slowly, due to\norganisational difficulties at NPL, and in 1948 a ‘very fed\nup’ Turing (Robin Gandy's description, in interview with\nCopeland, 1995) left NPL for Newman's Computing Machine Laboratory at\nManchester University. It was not until May 1950 that a small pilot\nmodel of the Automatic Computing Engine, built by Wilkinson, Edward\nNewman, Mike Woodger, and others, first executed a program. With an\noperating speed of 1 MHz, the Pilot Model ACE was for some time the\nfastest computer in the world. \n\nSales of DEUCE, the production version of the Pilot Model ACE, were\nbuoyant — confounding the suggestion, made in 1946 by the\nDirector of the NPL, Sir Charles Darwin, that ‘it is very\npossible that … one machine would suffice to solve all the\nproblems that are demanded of it from the whole country’\n[Copeland 2005, p. 4]. The fundamentals of Turing's ACE design were\nemployed by Harry Huskey (at Wayne State University, Detroit) in the\nBendix G15 computer (Huskey in interview with Copeland, 1998). The G15\nwas arguably the first personal computer; over 400 were sold worldwide.\nDEUCE and the G15 remained in use until about 1970. Another computer\nderiving from Turing's ACE design, the MOSAIC, played a role in\nBritain's air defences during the Cold War period; other derivatives\ninclude the Packard-Bell PB250 (1961). (More information about these\nearly computers is given in [Copeland 2005].) \n\nThe earliest general-purpose stored-program electronic digital computer\nto work was built in Newman's Computing Machine Laboratory at\nManchester University. The Manchester ‘Baby’, as it became\nknown, was constructed by the engineers F.C. Williams and Tom Kilburn,\nand performed its first calculation on 21 June 1948. The tiny program,\nstored on the face of a cathode ray tube, was just seventeen\ninstructions long. A much enlarged version of the machine, with a\nprogramming system designed by Turing, became the world's first\ncommercially available computer, the Ferranti Mark I. The first to be\ncompleted was installed at Manchester University in February 1951; in\nall about ten were sold, in Britain, Canada, Holland and Italy.  \n\nThe fundamental logico-mathematical contributions by Turing and\nNewman to the triumph at Manchester have been neglected, and the\nManchester machine is nowadays remembered as the work of Williams and\nKilburn. Indeed, Newman's role in the development of computers has\nnever been sufficiently emphasised (due perhaps to his thoroughly\nself-effacing way of relating the relevant events). \n\nIt was Newman who, in a lecture in Cambridge in 1935, introduced\nTuring to the concept that led directly to the Turing machine: Newman\ndefined a constructive process as one that a machine can carry\nout (Newman in interview with Evans, op. cit.). As a result of his\nknowledge of Turing's work, Newman became interested in the\npossibilities of computing machinery in, as he put it, ‘a rather\ntheoretical way’. It was not until Newman joined GC&CS in\n1942 that his interest in computing machinery suddenly became\npractical, with his realisation that the attack on Tunny could be\nmechanised. During the building of Colossus, Newman tried to interest\nFlowers in Turing's 1936 paper — birthplace of the stored-program\nconcept - but Flowers did not make much of Turing's arcane notation.\nThere is no doubt that by 1943, Newman had firmly in mind the idea of\nusing electronic technology in order to construct a stored-program\ngeneral-purpose digital computing machine. \n\nIn July of 1946 (the month in which the Royal Society approved\nNewman's application for funds to found the Computing Machine\nLaboratory), Freddie Williams, working at the Telecommunications\nResearch Establishment, Malvern, began the series of experiments on\ncathode ray tube storage that was to lead to the Williams tube memory.\nWilliams, until then a radar engineer, explains how it was that he came\nto be working on the problem of computer memory: \n\nNewman learned of Williams' work, and with the able help of Patrick\nBlackett, Langworthy Professor of Physics at Manchester and one of the\nmost powerful figures in the University, was instrumental in the\nappointment of the 35 year old Williams to the recently vacated Chair\nof Electro-Technics at Manchester. (Both were members of the appointing\ncommittee (Kilburn in interview with Copeland, 1997).) Williams\nimmediately had Kilburn, his assistant at Malvern, seconded to\nManchester. To take up the story in Williams' own words: \n\nElsewhere Williams is explicit concerning Turing's role and gives\nsomething of the flavour of the explanation that he and Kilburn\nreceived: \n\nIt seems that Newman must have used much the same words with\nWilliams and Kilburn as he did in an address to the Royal Society on\n4th March 1948: \n\nFollowing this explanation of Turing's three-address concept (source\n1, source 2, destination, function) Newman went on to describe program\nstorage (‘the orders shall be in a series of houses X1, X2,\n…’) and conditional branching. He then summed up: \n\nIn a letter written in 1972 Williams described in some detail what\nhe and Kilburn were told by Newman: \n\nTuring's early input to the developments at Manchester, hinted at by\nWilliams in his above-quoted reference to Turing, may have been via the\nlectures on computer design that Turing and Wilkinson gave in London\nduring the period December 1946 to February 1947 (Turing and Wilkinson\n[1946–7]). The lectures were attended by representatives of\nvarious organisations planning to use or build an electronic computer.\nKilburn was in the audience (Bowker and Giordano [1993]). (Kilburn\nusually said, when asked from where he obtained his basic knowledge of\nthe computer, that he could not remember (letter from Brian Napper to\nCopeland, 2002); for example, in a 1992 interview he said:\n‘Between early 1945 and early 1947, in that period, somehow or\nother I knew what a digital computer was … Where I got this\nknowledge from I've no idea’ (Bowker and Giordano [1993], p.\n19).) \n\nWhatever role Turing's lectures may have played in informing\nKilburn, there is little doubt that credit for the Manchester computer\n— called the ‘Newman-Williams machine’ in a\ncontemporary document (Huskey 1947) — belongs not only to\nWilliams and Kilburn but also to Newman, and that the influence on\nNewman of Turing's 1936 paper was crucial, as was the influence of\nFlowers' Colossus. \n\nThe first working AI program, a draughts (checkers) player written\nby Christopher Strachey, ran on the Ferranti Mark I in the Manchester\nComputing Machine Laboratory. Strachey (at the time a teacher at Harrow\nSchool and an amateur programmer) wrote the program with Turing's\nencouragement and utilising the latter's recently completed\nProgrammers' Handbook for the Ferranti. (Strachey later became Director\nof the Programming Research Group at Oxford University.) By the summer\nof 1952, the program could, Strachey reported, ‘play a complete\ngame of draughts at a reasonable speed’. (Strachey's program formed the\nbasis for Arthur Samuel's well-known checkers program.) The first\nchess-playing program, also, was written for the Manchester Ferranti,\nby Dietrich Prinz; the program first ran in November 1951. Designed for\nsolving simple problems of the mate-in-two variety, the program would\nexamine every possible move until a solution was found. Turing started\nto program his ‘Turochamp’ chess-player on the Ferranti\nMark I, but never completed the task. Unlike Prinz's program, the\nTurochamp could play a complete game (when hand-simulated) and operated\nnot by exhaustive search but under the guidance of heuristics. \n\nThe first fully functioning electronic digital computer to be built in\nthe U.S. was ENIAC, constructed at the Moore School of Electrical\nEngineering, University of Pennsylvania, for the Army Ordnance\nDepartment, by J. Presper Eckert and John Mauchly. Completed in 1945,\nENIAC was somewhat similar to the earlier Colossus, but considerably\nlarger and more flexible (although far from general-purpose). The\nprimary function for which ENIAC was designed was the calculation of\ntables used in aiming artillery. ENIAC was not a stored-program\ncomputer, and setting it up for a new job involved reconfiguring the\nmachine by means of plugs and switches. For many years, ENIAC was\nbelieved to have been the first functioning electronic digital\ncomputer, Colossus being unknown to all but a few.  \n\nIn 1944, John von Neumann joined the ENIAC group. He had become\n‘intrigued’ (Goldstine's word, [1972], p. 275) with\nTuring's universal machine while Turing was at Princeton University\nduring 1936–1938. At the Moore School, von Neumann emphasised the\nimportance of the stored-program concept for electronic computing,\nincluding the possibility of allowing the machine to modify its own\nprogram in useful ways while running (for example, in order to control\nloops and branching). Turing's paper of 1936 (‘On Computable\nNumbers, with an Application to the Entscheidungsproblem’) was\nrequired reading for members of von Neumann's post-war computer project\nat the Institute for Advanced Study, Princeton University (letter from\nJulian Bigelow to Copeland, 2002; see also Copeland [2004], p. 23).\nEckert appears to have realised independently, and prior to von\nNeumann's joining the ENIAC group, that the way to take full advantage\nof the speed at which data is processed by electronic circuits is to\nplace suitably encoded instructions for controlling the processing in\nthe same high-speed storage devices that hold the data itself\n(documented in Copeland [2004], pp. 26–7). In 1945, while ENIAC\nwas still under construction, von Neumann produced a draft report,\nmentioned previously, setting out the ENIAC group's ideas for an\nelectronic stored-program general-purpose digital computer, the EDVAC\n(von Neuman [1945]). The EDVAC was completed six years later, but not\nby its originators, who left the Moore School to build computers\nelsewhere. Lectures held at the Moore School in 1946 on the proposed\nEDVAC were widely attended and contributed greatly to the dissemination\nof the new ideas. \n\nVon Neumann was a prestigious figure and he made the concept of a\nhigh-speed stored-program digital computer widely known through his\nwritings and public addresses. As a result of his high profile in the\nfield, it became customary, although historically inappropriate, to\nrefer to electronic stored-program digital computers as ‘von\nNeumann machines’. \n\nThe Los Alamos physicist Stanley Frankel, responsible with von\nNeumann and others for mechanising the large-scale calculations\ninvolved in the design of the atomic bomb, has described von Neumann's\nview of the importance of Turing's 1936 paper, in a letter: \n\nOther notable early stored-program electronic digital computers were:  \n\nThe EDVAC and ACE proposals both advocated the use of mercury-filled\ntubes, called ‘delay lines’, for high-speed internal\nmemory. This form of memory is known as acoustic memory. Delay lines\nhad initially been developed for echo cancellation in radar; the idea\nof using them as memory devices originated with Eckert at the Moore\nSchool. Here is Turing's description:  \n\nMercury delay line memory was used in EDSAC, BINAC, SEAC, Pilot\nModel ACE, EDVAC, DEUCE, and full-scale ACE (1958). The chief advantage\nof the delay line as a memory medium was, as Turing put it, that delay\nlines were \"already a going concern\" (Turing [1947], p. 380). The\nfundamental disadvantages of the delay line were that random access is\nimpossible and, moreover, the time taken for an instruction, or number,\nto emerge from a delay line depends on where in the line it happens to\nbe. \n\nIn order to minimize waiting-time, Turing arranged for instructions\nto be stored not in consecutive positions in the delay line, but in\nrelative positions selected by the programmer in such a way that each\ninstruction would emerge at exactly the time it was required, in so far\nas this was possible. Each instruction contained a specification of the\nlocation of the next. This system subsequently became known as\n‘optimum coding’. It was an integral feature of every\nversion of the ACE design. Optimum coding made for difficult and untidy\nprogramming, but the advantage in terms of speed was considerable.\nThanks to optimum coding, the Pilot Model ACE was able to do a floating\npoint multiplication in 3 milliseconds (Wilkes's EDSAC required 4.5\nmilliseconds to perform a single fixed point multiplication). \n\nIn the Williams tube or electrostatic memory, previously mentioned,\na two-dimensional rectangular array of binary digits was stored on the\nface of a commercially-available cathode ray tube. Access to data was\nimmediate. Williams tube memories were employed in the Manchester\nseries of machines, SWAC, the IAS computer, and the IBM 701, and a\nmodified form of Williams tube in Whirlwind I (until replacement by\nmagnetic core in 1953). \n\nDrum memories, in which data was stored magnetically on the surface\nof a metal cylinder, were developed on both sides of the Atlantic. The\ninitial idea appears to have been Eckert's. The drum provided\nreasonably large quantities of medium-speed memory and was used to\nsupplement a high-speed acoustic or electrostatic memory. In 1949, the\nManchester computer was successfully equipped with a drum memory; this\nwas constructed by the Manchester engineers on the model of a drum\ndeveloped by Andrew Booth at Birkbeck College, London. \n\nThe final major event in the early history of electronic computation\nwas the development of magnetic core memory. Jay Forrester realised\nthat the hysteresis properties of magnetic core (normally used in\ntransformers) lent themselves to the implementation of a\nthree-dimensional solid array of randomly accessible storage points. In\n1949, at Massachusetts Institute of Technology, he began to investigate\nthis idea empirically. Forrester's early experiments with metallic core\nsoon led him to develop the superior ferrite core memory. Digital\nEquipment Corporation undertook to build a computer similar to the\nWhirlwind I as a test vehicle for a ferrite core memory. The Memory\nTest Computer was completed in 1953. (This computer was used in 1954\nfor the first simulations of neural networks, by Belmont Farley and\nWesley Clark of MIT's Lincoln Laboratory (see Copeland and Proudfoot\n[1996]). \n\nOnce the absolute reliability, relative cheapness, high capacity and\npermanent life of ferrite core memory became apparent, core soon\nreplaced other forms of high-speed memory. The IBM 704 and 705\ncomputers (announced in May and October 1954, respectively) brought\ncore memory into wide use.","contact.mail":"jack.copeland@canterbury.ac.nz","contact.domain":"canterbury.ac.nz"}]
