[{"date.published":"2020-04-30","url":"https://plato.stanford.edu/entries/ethics-ai/","author1":"Vincent C. Müller","author1.info":"http://www.sophia.de","entry":"ethics-ai","body.text":"\n\n\nArtificial intelligence (AI) and robotics are digital technologies\nthat will have significant impact on the development of humanity in the near\nfuture. They have raised fundamental questions about what we should do\nwith these systems, what the systems themselves should do, what risks\nthey involve, and how we can control these.\n\n\nAfter the Introduction to the field (§1), the main themes (§2) of this\narticle are: Ethical issues that arise with AI systems as\nobjects, i.e., tools made and used by humans. This includes\nissues of privacy (§2.1) and manipulation (§2.2), opacity (§2.3) and bias (§2.4), human-robot\ninteraction (§2.5), employment (§2.6), and the effects of autonomy (§2.7). Then AI systems\nas subjects, i.e., ethics for the AI systems themselves in\nmachine ethics (§2.8) and artificial moral agency (§2.9). Finally, the problem of a\npossible future AI superintelligence leading to a\n“singularity” (§2.10). We close with a remark on the vision of AI (§3).\n\n\nFor each section within these themes, we provide a general explanation\nof the ethical issues, outline existing positions\nand arguments, then analyse how these play out with current\ntechnologies and finally, what policy consequences\nmay be drawn.\n\n\nThe ethics of AI and robotics is often focused on\n“concerns” of various sorts, which is a typical response\nto new technologies. Many such concerns turn out to be rather quaint\n(trains are too fast for souls); some are predictably wrong when they\nsuggest that the technology will fundamentally change humans\n(telephones will destroy personal communication, writing will destroy\nmemory, video cassettes will make going out redundant); some are\nbroadly correct but moderately relevant (digital technology will\ndestroy industries that make photographic film, cassette tapes, or\nvinyl records); but some are broadly correct and deeply relevant (cars\nwill kill children and fundamentally change the landscape). The task\nof an article such as this is to analyse the issues and to deflate the\nnon-issues. \nSome technologies, like nuclear power, cars, or plastics, have caused\nethical and political discussion and significant policy efforts to\ncontrol the trajectory these technologies, usually only once some\ndamage is done. In addition to such “ethical concerns”,\nnew technologies challenge current norms and conceptual systems, which\nis of particular interest to philosophy. Finally, once we have\nunderstood a technology in its context, we need to shape our societal\nresponse, including regulation and law. All these features also exist\nin the case of new AI and Robotics technologies—plus the more\nfundamental fear that they may end the era of human control on\nEarth. \nThe ethics of AI and robotics has seen significant press coverage in\nrecent years, which supports related research, but also may end up\nundermining it: the press often talks as if the issues under\ndiscussion were just predictions of what future technology will bring,\nand as though we already know what would be most ethical and how to\nachieve that. Press coverage thus focuses on risk, security (Brundage\net al. 2018, in the Other Internet Resources section\nbelow, hereafter [OIR]),\n and prediction of impact (e.g., on the job market). The\nresult is a discussion of essentially technical problems that focus on\nhow to achieve a desired outcome. Current discussions in policy and\nindustry are also motivated by image and public relations, where the\nlabel “ethical” is really not much more than the new\n“green”, perhaps used for “ethics washing”.\nFor a problem to qualify as a problem for AI ethics would require that\nwe do not readily know what the right thing to do is. In this\nsense, job loss, theft, or killing with AI is not a problem in ethics,\nbut whether these are permissible under certain circumstances\nis a problem. This article focuses on the genuine problems of\nethics where we do not readily know what the answers are. \nA last caveat: The ethics of AI and robotics is a very young field\nwithin applied ethics, with significant dynamics, but few\nwell-established issues and no authoritative overviews—though\nthere is a promising outline (European Group on Ethics in Science and\nNew Technologies 2018) and there are beginnings on societal impact\n(Floridi et al. 2018; Taddeo and Floridi 2018; S. Taylor et al. 2018;\nWalsh 2018; Bryson 2019; Gibert 2019; \n \nWhittlestone et al.  2019), and policy recommendations (AI HLEG 2019\n[OIR]; IEEE 2019). So this article cannot merely reproduce what the\ncommunity has achieved thus far, but must propose an ordering where\nlittle order exists. \nThe notion of “artificial intelligence” (AI) is understood\nbroadly as any kind of artificial computational system that shows\nintelligent behaviour, i.e., complex behaviour that is conducive to\nreaching goals. In particular, we do not wish to restrict\n“intelligence” to what would require intelligence if done\nby humans, as Minsky had suggested (1985). This means we\nincorporate a range of machines, including those in “technical\nAI”, that show only limited abilities in learning or reasoning\nbut excel at the automation of particular tasks, as well as machines\nin “general AI” that aim to create a generally intelligent\nagent. \nAI somehow gets closer to our skin than other technologies—thus\nthe field of “philosophy of AI”. Perhaps this is because\nthe project of AI is to create machines that have a feature central to\nhow we humans see ourselves, namely as feeling, thinking, intelligent\nbeings. The main purposes of an artificially intelligent agent\nprobably involve sensing, modelling, planning and action, but current\nAI applications also include perception, text analysis, natural\nlanguage processing (NLP), logical reasoning, game-playing, decision\nsupport systems, data analytics, predictive analytics, as well as\nautonomous vehicles and other forms of robotics (P. Stone et al.\n2016). AI may involve any number of computational techniques to\nachieve these aims, be that classical symbol-manipulating AI, inspired\nby natural cognition, or machine learning via neural networks\n(Goodfellow, Bengio, and Courville 2016; Silver et al. 2018). \nHistorically, it is worth noting that the term “AI” was\nused as above ca. 1950–1975, then came into disrepute during the\n“AI winter”, ca. 1975–1995, and narrowed. As a\nresult, areas such as “machine learning”, “natural\nlanguage processing” and “data science” were often\nnot labelled as “AI”. Since ca. 2010, the use has\nbroadened again, and at times almost all of computer science and even\nhigh-tech is lumped under “AI”. Now it is a name to be\nproud of, a booming industry with massive capital investment (Shoham\net al. 2018), and on the edge of hype again. As Erik Brynjolfsson\nnoted, it may allow us to\n\n virtually eliminate global poverty, massively reduce disease\nand provide better education to almost everyone on the planet.\n(quoted in Anderson, Rainie, and Luchsinger 2018) \nWhile AI can be entirely software, robots are physical machines that\nmove. Robots are subject to physical impact, typically through\n“sensors”, and they exert physical force onto the world,\ntypically through “actuators”, like a gripper or a turning\nwheel. Accordingly, autonomous cars or planes are robots, and only a\nminuscule portion of robots is “humanoid” (human-shaped),\nlike in the movies. Some robots use AI, and some do not: Typical\nindustrial robots blindly follow completely defined scripts with\nminimal sensory input and no learning or reasoning (around 500,000\nsuch new industrial robots are installed each year (IFR 2019 [OIR])). It is\nprobably fair to say that while robotics systems cause more concerns\nin the general public, AI systems are more likely to have a greater\nimpact on humanity. Also, AI or robotics systems for a narrow set of\ntasks are less likely to cause new issues than systems that are more\nflexible and autonomous. \nRobotics and AI can thus be seen as covering two overlapping sets of\nsystems: systems that are only AI, systems that are only robotics, and\nsystems that are both. We are interested in all three; the scope of\nthis article is thus not only the intersection, but the union, of both\nsets. \nPolicy is only one of the concerns of this article. There is\nsignificant public discussion about AI ethics, and there are frequent\npronouncements from politicians that the matter requires new policy,\nwhich is easier said than done: Actual technology policy is difficult\nto plan and enforce. It can take many forms, from incentives and\nfunding, infrastructure, taxation, or good-will statements, to\nregulation by various actors, and the law. Policy for AI will possibly\ncome into conflict with other aims of technology policy or general\npolicy. Governments, parliaments, associations, and industry circles\nin industrialised countries have produced reports and white papers in\nrecent years, and some have generated good-will slogans\n(“trusted/responsible/humane/human-centred/good/beneficial\nAI”), but is that what is needed? For a survey, see Jobin,\nIenca, and Vayena (2019) and V. Müller’s list of\n PT-AI Policy Documents and Institutions. \nFor people who work in ethics and policy, there might be a tendency to\noverestimate the impact and threats from a new technology, and to\nunderestimate how far current regulation can reach (e.g., for product\nliability). On the other hand, there is a tendency for businesses, the\nmilitary, and some public administrations to “just talk”\nand do some “ethics washing” in order to preserve a good\npublic image and continue as before. Actually implementing legally\nbinding regulation would challenge existing business models and\npractices. Actual policy is not just an implementation of ethical\ntheory, but subject to societal power structures—and the agents\nthat do have the power will push against anything that restricts them.\nThere is thus a significant risk that regulation will remain toothless\nin the face of economical and political power. \nThough very little actual policy has been produced, there are some\nnotable beginnings: The latest EU policy document suggests\n“trustworthy AI” should be lawful, ethical, and\ntechnically robust, and then spells this out as seven requirements:\nhuman oversight, technical robustness, privacy and data governance,\ntransparency, fairness, well-being, and accountability (AI HLEG 2019 [OIR]).\nMuch European research now runs under the slogan of “responsible\nresearch and innovation” (RRI), and “technology\nassessment” has been a standard field since the advent of\nnuclear power. Professional ethics is also a standard field in\ninformation technology, and this includes issues that are relevant in\nthis article. Perhaps a “code of ethics” for AI engineers,\nanalogous to the codes of ethics for medical doctors, is an option\nhere (Véliz 2019). What data science itself should do is\naddressed in (L. Taylor and Purtova 2019). We also expect that much\npolicy will eventually cover specific uses or technologies of AI and\nrobotics, rather than the field as a whole. A useful summary of an\nethical framework for AI is given in (European Group on Ethics in\nScience and New Technologies 2018: 13ff). On general AI policy, see\nCalo (2018) as well as Crawford and Calo (2016); Stahl, Timmermans,\nand Mittelstadt (2016); Johnson and Verdicchio (2017); and Giubilini\nand Savulescu (2018). A more political angle of technology is often\ndiscussed in the field of “Science and Technology Studies” (STS). As\nbooks like The Ethics of Invention (Jasanoff 2016) show,\nconcerns in STS are often quite similar to those in ethics (Jacobs et\nal. 2019 [OIR]). In this article, we discuss the policy for each type of\nissue separately rather than for AI or robotics in general. \nIn this section we outline the ethical issues of human use of AI and\nrobotics systems that can be more or less autonomous—which means\nwe look at issues that arise with certain uses of the technologies\nwhich would not arise with others. It must be kept in mind, however,\nthat technologies will always cause some uses to be easier, and thus\nmore frequent, and hinder other uses. The design of technical\nartefacts thus has ethical relevance for their use (Houkes and Vermaas\n2010; Verbeek 2011), so beyond “responsible use”, we also\nneed “responsible design” in this field. The focus on use\ndoes not presuppose which ethical approaches are best suited for\ntackling these issues; they might well be virtue ethics (Vallor 2017)\nrather than consequentialist or value-based (Floridi et al. 2018).\nThis section is also neutral with respect to the question whether AI\nsystems truly have “intelligence” or other mental\nproperties: It would apply equally well if AI and robotics are merely\nseen as the current face of automation (cf. Müller\nforthcoming-b). \nThere is a general discussion about privacy and surveillance in\ninformation technology (e.g., Macnish 2017; Roessler 2017), which\nmainly concerns the access to private data and data that is personally\nidentifiable. Privacy has several well recognised aspects, e.g.,\n“the right to be let alone”, information privacy, privacy\nas an aspect of personhood, control over information about oneself,\nand the right to secrecy (Bennett and Raab 2006). Privacy studies have\nhistorically focused on state surveillance by secret services but now\ninclude surveillance by other state agents, businesses, and even\nindividuals. The technology has changed significantly in the last\ndecades while regulation has been slow to respond (though there is the\nRegulation (EU) 2016/679)—the result is a certain anarchy that\nis exploited by the most powerful players, sometimes in plain sight,\nsometimes in hiding. \nThe digital sphere has widened greatly: All data collection and\nstorage is now digital, our lives are increasingly digital, most\ndigital data is connected to a single Internet, and there is more and\nmore sensor technology in use that generates data about non-digital\naspects of our lives. AI increases both the possibilities of\nintelligent data collection and the possibilities for data analysis.\nThis applies to blanket surveillance of whole populations as well as\nto classic targeted surveillance. In addition, much of the data is\ntraded between agents, usually for a fee. \nAt the same time, controlling who collects which data, and who has\naccess, is much harder in the digital world than it was in the\nanalogue world of paper and telephone calls. Many new AI technologies\namplify the known issues. For example, face recognition in photos and\nvideos allows identification and thus profiling and searching for\nindividuals (Whittaker et al. 2018: 15ff). This continues using other\ntechniques for identification, e.g., “device\nfingerprinting”, which are commonplace on the Internet\n(sometimes revealed in the “privacy policy”). The result\nis that “In this vast ocean of data, there is a frighteningly\ncomplete picture of us” (Smolan 2016: 1:01). The result is\narguably a scandal that still has not received due public\nattention. \nThe data trail we leave behind is how our “free” services\nare paid for—but we are not told about that data collection and\nthe value of this new raw material, and we are manipulated into\nleaving ever more such data. For the “big 5” companies\n(Amazon, Google/Alphabet, Microsoft, Apple, Facebook), the main\ndata-collection part of their business appears to be based on\ndeception, exploiting human weaknesses, furthering procrastination,\ngenerating addiction, and manipulation (Harris 2016 [OIR]). The\nprimary focus of social media, gaming, and most of the Internet in\nthis “surveillance economy” is to gain, maintain, and\ndirect attention—and thus data supply. “Surveillance is\nthe business model of the Internet” (Schneier 2015). This\nsurveillance and attention economy is sometimes called\n“surveillance capitalism” (Zuboff 2019). It has caused\nmany attempts to escape from the grasp of these corporations, e.g., in\nexercises of “minimalism” (Newport 2019), sometimes\nthrough the open source movement, but it appears that present-day\ncitizens have lost the degree of autonomy needed to escape while fully\ncontinuing with their life and work. We have lost ownership of our\ndata, if “ownership” is the right relation here. Arguably,\nwe have lost control of our data. \nThese systems will often reveal facts about us that we ourselves wish\nto suppress or are not aware of: they know more about us than we know\nourselves. Even just observing online behaviour allows insights into\nour mental states (Burr and Christianini 2019) and manipulation (see\nbelow\n section 2.2).\n This has led to calls for the protection of “derived\ndata” (Wachter and Mittelstadt 2019). With the last sentence of\nhis bestselling book, Homo Deus, Harari asks about the\nlong-term consequences of AI:  \nWhat will happen to society, politics and daily life when\nnon-conscious but highly intelligent algorithms know us better than we\nknow ourselves? (2016: 462) \nRobotic devices have not yet played a major role in this area, except\nfor security patrolling, but this will change once they are more\ncommon outside of industry environments. Together with the\n“Internet of things”, the so-called “smart”\nsystems (phone, TV, oven, lamp, virtual assistant, home,…),\n“smart city” (Sennett 2018), and “smart\ngovernance”, they are set to become part of the data-gathering\nmachinery that offers more detailed data, of different types, in real\ntime, with ever more information. \nPrivacy-preserving techniques that can largely conceal the identity of\npersons or groups are now a standard staple in data science; they\ninclude (relative) anonymisation , access control (plus encryption),\nand other models where computation is carried out with fully or\npartially encrypted input data (Stahl and Wright 2018); in the case of\n“differential privacy”, this is done by adding calibrated\nnoise to encrypt the output of queries (Dwork et al. 2006; Abowd\n2017). While requiring more effort and cost, such techniques can avoid\nmany of the privacy issues. Some companies have also seen better\nprivacy as a competitive advantage that can be leveraged and sold at a\nprice. \nOne of the major practical difficulties is to actually enforce\nregulation, both on the level of the state and on the level of the\nindividual who has a claim. They must identify the responsible legal\nentity, prove the action, perhaps prove intent, find a court that\ndeclares itself competent … and eventually get the court to\nactually enforce its decision. Well-established legal protection of\nrights such as consumer rights, product liability, and other civil\nliability or protection of intellectual property rights is often\nmissing in digital products, or hard to enforce. This means that\ncompanies with a “digital” background are used to testing\ntheir products on the consumers without fear of liability while\nheavily defending their intellectual property rights. This\n“Internet Libertarianism” is sometimes taken to assume\nthat technical solutions will take care of societal problems by\nthemselves (Mozorov 2013). \nThe ethical issues of AI in surveillance go beyond the mere\naccumulation of data and direction of attention: They include\nthe use of information to manipulate behaviour, online and\noffline, in a way that undermines autonomous rational choice. Of\ncourse, efforts to manipulate behaviour are ancient, but they may gain\na new quality when they use AI systems. Given users’ intense\ninteraction with data systems and the deep knowledge about individuals\nthis provides, they are vulnerable to “nudges”,\nmanipulation, and deception. With sufficient prior data, algorithms\ncan be used to target individuals or small groups with just the kind\nof input that is likely to influence these particular individuals. A\n’nudge‘ changes the environment such that it influences\nbehaviour in a predictable way that is positive for the individual,\nbut easy and cheap to avoid (Thaler & Sunstein 2008). There is a\nslippery slope from here to paternalism and manipulation. \nMany advertisers, marketers, and online sellers will use any legal\nmeans at their disposal to maximise profit, including exploitation of\nbehavioural biases, deception, and addiction generation (Costa and\nHalpern 2019 [OIR]). Such manipulation is the\nbusiness model in much of the gambling and gaming industries, but it\nis spreading, e.g., to low-cost airlines.  In interface design on web\npages or in games, this manipulation uses what is called “dark\npatterns” (Mathur et al. 2019). At this moment, gambling and the\nsale of addictive substances are highly regulated, but online\nmanipulation and addiction are not—even though manipulation of\nonline behaviour is becoming a core business model of the\nInternet. \nFurthermore, social media is now the prime location for political\npropaganda. This influence can be used to steer voting behaviour, as\nin the Facebook-Cambridge Analytica “scandal” (Woolley and\nHoward 2017; Bradshaw, Neudert, and Howard 2019) and—if\nsuccessful—it may harm the autonomy of individuals (Susser,\nRoessler, and Nissenbaum 2019). \nImproved AI “faking” technologies make what once was\nreliable evidence into unreliable evidence—this has already\nhappened to digital photos, sound recordings, and video. It will soon\nbe quite easy to create (rather than alter) “deep fake”\ntext, photos, and video material with any desired content. Soon,\nsophisticated real-time interaction with persons over text, phone, or\nvideo will be faked, too. So we cannot trust digital interactions\nwhile we are at the same time increasingly dependent on such\ninteractions. \nOne more specific issue is that machine learning techniques in AI rely\non training with vast amounts of data. This means there will often be\na trade-off between privacy and rights to data vs. technical quality\nof the product. This influences the consequentialist evaluation of\nprivacy-violating practices. \nThe policy in this field has its ups and downs: Civil liberties and\nthe protection of individual rights are under intense pressure from\nbusinesses’ lobbying, secret services, and other state agencies\nthat depend on surveillance. Privacy protection has diminished\nmassively compared to the pre-digital age when communication was based\non letters, analogue telephone communications, and personal\nconversation and when surveillance operated under significant legal\nconstraints. \nWhile the EU General Data Protection Regulation (Regulation (EU)\n2016/679) has strengthened privacy protection, the US and China prefer\ngrowth with less regulation (Thompson and Bremmer 2018), likely in the\nhope that this provides a competitive advantage. It is clear that\nstate and business actors have increased their ability to invade\nprivacy and manipulate people with the help of AI technology and will\ncontinue to do so to further their particular interests—unless\nreined in by policy in the interest of general society. \nOpacity and bias are central issues in what is now sometimes called\n“data ethics” or “big data ethics” (Floridi\nand Taddeo 2016; Mittelstadt and Floridi 2016). AI systems for\nautomated decision support and “predictive analytics”\nraise “significant concerns about lack of due process,\naccountability, community engagement, and auditing” (Whittaker\net al. 2018: 18ff). They are part of a power structure in which\n“we are creating decision-making processes that constrain and\nlimit opportunities for human participation” (Danaher 2016b:\n245). At the same time, it will often be impossible for the affected\nperson to know how the system came to this output, i.e., the system is\n“opaque” to that person. If the system involves machine\nlearning, it will typically be opaque even to the expert, who will not\nknow how a particular pattern was identified, or even what the pattern\nis. Bias in decision systems and data sets is exacerbated by this\nopacity. So, at least in cases where there is a desire to remove bias,\nthe analysis of opacity and bias go hand in hand, and political\nresponse has to tackle both issues together. \nMany AI systems rely on machine learning techniques in (simulated)\nneural networks that will extract patterns from a given dataset, with\nor without “correct” solutions provided; i.e., supervised,\nsemi-supervised or unsupervised. With these techniques, the\n“learning” captures patterns in the data and these are\nlabelled in a way that appears useful to the decision the system\nmakes, while the programmer does not really know which\npatterns in the data the system has used. In fact, the programs are\nevolving, so when new data comes in, or new feedback is given\n(“this was correct”, “this was incorrect”),\nthe patterns used by the learning system change. What this means is\nthat the outcome is not transparent to the user or programmers: it is\nopaque. Furthermore, the quality of the program depends heavily on the\nquality of the data provided, following the old slogan “garbage\nin, garbage out”. So, if the data already involved a bias (e.g.,\npolice data about the skin colour of suspects), then the program will\nreproduce that bias. There are proposals for a standard description of\ndatasets in a “datasheet” that would make the\nidentification of such bias more feasible (Gebru et al. 2018 [OIR]). There\nis also significant recent literature about the limitations of machine\nlearning systems that are essentially sophisticated data filters\n(Marcus 2018 [OIR]). Some have argued that the ethical problems of today are\nthe result of technical “shortcuts” AI has taken\n(Cristianini forthcoming). \nThere are several technical activities that aim at “explainable\nAI”, starting with (Van Lent, Fisher, and Mancuso 1999; Lomas et\nal. 2012) and, more recently, a DARPA programme (Gunning 2017 [OIR]).\n\nMore broadly, the demand for  \na mechanism for elucidating and articulating the power structures,\nbiases, and influences that computational artefacts exercise in\nsociety (Diakopoulos 2015: 398) \nis sometimes called “algorithmic accountability\nreporting”. This does not mean that we expect an AI to\n“explain its reasoning”—doing so would require far\nmore serious moral autonomy than we currently attribute to AI systems\n(see below\n §2.10). \nThe politician Henry Kissinger pointed out that there is a fundamental\nproblem for democratic decision-making if we rely on a system that is\nsupposedly superior to humans, but cannot explain its decisions. He\nsays we may have “generated a potentially dominating technology\nin search of a guiding philosophy” (Kissinger 2018).  Danaher\n(2016b) calls this problem “the threat of algocracy”\n(adopting the previous use of ‘algocracy’ from Aneesh 2002\n[OIR], 2006).  In a similar vein, Cave (2019)\nstresses that we need a broader societal move towards more\n“democratic” decision-making to avoid AI being a force\nthat leads to a Kafka-style impenetrable suppression system in public\nadministration and elsewhere. The political angle of this discussion\nhas been stressed by O’Neil in her influential book Weapons\nof Math Destruction (2016), and by Yeung and Lodge (2019). \nIn the EU, some of these issues have been taken into account with the\n(Regulation (EU) 2016/679), which foresees that consumers, when faced\nwith a decision based on data processing, will have a legal\n“right to explanation”—how far this goes and to what\nextent it can be enforced is disputed (Goodman and Flaxman 2017;\nWachter, Mittelstadt, and Floridi 2016; Wachter, Mittelstadt, and\nRussell 2017). Zerilli et al. (2019) argue that there may be a double\nstandard here, where we demand a high level of explanation for\nmachine-based decisions despite humans sometimes not reaching that\nstandard themselves. \nAutomated AI decision support systems and “predictive\nanalytics” operate on data and produce a decision as\n“output”. This output may range from the relatively\ntrivial to the highly significant: “this restaurant matches your\npreferences”, “the patient in this X-ray has completed\nbone growth”, “application to credit card declined”,\n“donor organ will be given to another patient”,\n“bail is denied”, or “target identified and\nengaged”. Data analysis is often used in “predictive\nanalytics” in business, healthcare, and other fields, to foresee\nfuture developments—since prediction is easier, it will also\nbecome a cheaper commodity. One use of prediction is in\n“predictive policing” (NIJ 2014 [OIR]), which many fear might\nlead to an erosion of public liberties (Ferguson 2017) because it can\ntake away power from the people whose behaviour is predicted. It\nappears, however, that many of the worries about policing depend on\nfuturistic scenarios where law enforcement foresees and punishes\nplanned actions, rather than waiting until a crime has been committed\n(like in the 2002 film “Minority Report”). One concern is\nthat these systems might perpetuate bias that was already in the data\nused to set up the system, e.g., by increasing police patrols in an\narea and discovering more crime in that area. Actual “predictive\npolicing” or “intelligence led policing” techniques\nmainly concern the question of where and when police forces will be\nneeded most. Also, police officers can be provided with more data, offering\nthem more control and facilitating better decisions, in workflow\nsupport software (e.g., “ArcGIS”). Whether this is\nproblematic depends on the appropriate level of trust in the technical\nquality of these systems, and on the evaluation of aims of the police\nwork itself. Perhaps a recent paper title points in the right\ndirection here: “AI ethics in predictive policing: From models\nof threat to an ethics of care” (Asaro 2019). \nBias typically surfaces when unfair judgments are made because the\nindividual making the judgment is influenced by a characteristic that\nis actually irrelevant to the matter at hand, typically a\ndiscriminatory preconception about members of a group. So, one form of\nbias is a learned cognitive feature of a person, often not made\nexplicit. The person concerned may not be aware of having that\nbias—they may even be honestly and explicitly opposed to a bias\nthey are found to have (e.g., through priming, cf. Graham and Lowery\n2004). On fairness vs. bias in machine learning, see Binns (2018). \nApart from the social phenomenon of learned bias, the human cognitive\nsystem is generally prone to have various kinds of “cognitive\nbiases”, e.g., the “confirmation bias”: humans tend\nto interpret information as confirming what they already believe. This\nsecond form of bias is often said to impede performance in rational\njudgment (Kahnemann 2011)—though at least some cognitive biases\ngenerate an evolutionary advantage, e.g., economical use of resources\nfor intuitive judgment. There is a question whether AI systems could\nor should have such cognitive bias. \nA third form of bias is present in data when it exhibits systematic\nerror, e.g., “statistical bias”. Strictly, any given\ndataset will only be unbiased for a single kind of issue, so the mere\ncreation of a dataset involves the danger that it may be used for a\ndifferent kind of issue, and then turn out to be biased for that kind.\nMachine learning on the basis of such data would then not only fail to\nrecognise the bias, but codify and automate the “historical\nbias”. Such historical bias was discovered in an automated\nrecruitment screening system at Amazon (discontinued early 2017) that\ndiscriminated against women—presumably because the company had a\nhistory of discriminating against women in the hiring process. The\n“Correctional Offender Management Profiling for Alternative\nSanctions” (COMPAS), a system to predict whether a defendant\nwould re-offend, was found to be as successful (65.2% accuracy) as a\ngroup of random humans (Dressel and Farid 2018) and to produce more\nfalse positives and less false negatives for black defendants. The\nproblem with such systems is thus bias plus humans placing excessive\ntrust in the systems. The political dimensions of such automated\nsystems in the USA are investigated in Eubanks (2018). \nThere are significant technical efforts to detect and remove bias from\nAI systems, but it is fair to say that these are in early stages: see\nUK Institute for Ethical AI & Machine Learning (Brownsword,\nScotford, and Yeung 2017; Yeung and Lodge 2019). It appears that\ntechnological fixes have their limits in that they need a mathematical\nnotion of fairness, which is hard to come by (Whittaker et al. 2018:\n24ff; Selbst et al. 2019), as is a formal notion of “race”\n(see Benthall and Haynes 2019). An institutional proposal is in (Veale\nand Binns 2017). \nHuman-robot interaction (HRI) is an academic fields in its own right,\nwhich now pays significant attention to ethical matters, the dynamics\nof perception from both sides, and both the different interests\npresent in and the intricacy of the social context, including\nco-working (e.g., Arnold and Scheutz 2017). Useful surveys for the\nethics of robotics include Calo, Froomkin, and Kerr (2016); Royakkers\nand van Est (2016); Tzafestas (2016); a standard collection of papers\nis Lin, Abney, and Jenkins (2017). \nWhile AI can be used to manipulate humans into believing and doing\nthings (see section 2.2), it can also be used\nto drive robots that are problematic if their processes or appearance\ninvolve deception, threaten human dignity, or violate the Kantian\nrequirement of “respect for humanity”. Humans very easily\nattribute mental properties to objects, and empathise with them,\nespecially when the outer appearance of these objects is similar to\nthat of living beings. This can be used to deceive humans (or animals)\ninto attributing more intellectual or even emotional significance to\nrobots or AI systems than they deserve.  Some parts of humanoid\nrobotics are problematic in this regard (e.g., Hiroshi\nIshiguro’s remote-controlled Geminoids), and there are cases\nthat have been clearly deceptive for public-relations purposes (e.g. on the abilities of\nHanson Robotics’ “Sophia”). Of course, some fairly\nbasic constraints of business ethics and law apply to robots, too:\nproduct safety and liability, or non-deception in advertisement.  It\nappears that these existing constraints take care of many concerns\nthat are raised. There are cases, however, where human-human\ninteraction has aspects that appear specifically human in ways that\ncan perhaps not be replaced by robots: care, love, and sex. \nThe use of robots in health care for humans is currently at the level\nof concept studies in real environments, but it may become a usable\ntechnology in a few years, and has raised a number of concerns for a\ndystopian future of de-humanised care (A. Sharkey and N. Sharkey 2011;\nRobert Sparrow 2016). Current systems include robots that support\nhuman carers/caregivers (e.g., in lifting patients, or transporting\nmaterial), robots that enable patients to do certain things by\nthemselves (e.g., eat with a robotic arm), but also robots that are\ngiven to patients as company and comfort (e.g., the “Paro”\nrobot seal). For an overview, see van Wynsberghe (2016);\nNørskov (2017); Fosch-Villaronga and Albo-Canals (2019), for a\nsurvey of users Draper et al. (2014). \nOne reason why the issue of care has come to the fore is that people\nhave argued that we will need robots in ageing societies. This\nargument makes problematic assumptions, namely that with longer\nlifespan people will need more care, and that it will not be possible\nto attract more humans to caring professions. It may also show a bias\nabout age (Jecker forthcoming). Most importantly, it ignores the\nnature of automation, which is not simply about replacing humans, but\nabout allowing humans to work more efficiently. It is not very clear\nthat there really is an issue here since the discussion mostly focuses\non the fear of robots de-humanising care, but the actual and\nforeseeable robots in care are assistive robots for classic automation\nof technical tasks. They are thus “care robots” only in a\nbehavioural sense of performing tasks in care environments, not in the\nsense that a human “cares” for the patients. It appears\nthat the success of “being cared for” relies on this\nintentional sense of “care”, which foreseeable robots\ncannot provide. If anything, the risk of robots in care is the\nabsence of such intentional care—because less human\ncarers may be needed. Interestingly, caring for something, even a\nvirtual agent, can be good for the carer themselves (Lee et al. 2019).\nA system that pretends to care would be deceptive and thus\nproblematic—unless the deception is countered by sufficiently\nlarge utility gain (Coeckelbergh 2016). Some robots that pretend to\n“care” on a basic level are available (Paro seal) and\nothers are in the making. Perhaps feeling cared for by a machine, to\nsome extent, is progress for come patients. \nIt has been argued by several tech optimists that humans will likely\nbe interested in sex and companionship with robots and be comfortable\nwith the idea (Levy 2007). Given the variation of human sexual\npreferences, including sex toys and sex dolls, this seems very likely:\nThe question is whether such devices should be manufactured and\npromoted, and whether there should be limits in this touchy area.\nIt seems to have moved into the mainstream of “robot\nphilosophy” in recent times (Sullins 2012; Danaher and McArthur\n2017; N. Sharkey et al. 2017 [OIR]; Bendel 2018; Devlin 2018). \nHumans have long had deep emotional attachments to objects, so perhaps\ncompanionship or even love with a predictable android is attractive,\nespecially to people who struggle with actual humans, and already\nprefer dogs, cats, birds, a computer or a tamagotchi. Danaher\n(2019b) argues against (Nyholm and Frank 2017) that these can be true\nfriendships, and is thus a valuable goal. It certainly looks like such\nfriendship might increase overall utility, even if lacking in depth.\nIn these discussions there is an issue of deception, since a robot\ncannot (at present) mean what it says, or have feelings for a human.\nIt is well known that humans are prone to attribute feelings and\nthoughts to entities that behave as if they had sentience,even to\nclearly inanimate objects that show no behaviour at all. Also, paying\nfor deception seems to be an elementary part of the traditional sex\nindustry. \nFinally, there are concerns that have often accompanied matters of\nsex, namely consent (Frank and Nyholm 2017), aesthetic concerns, and\nthe worry that humans may be “corrupted” by certain\nexperiences. Old fashioned though this may seem, human behaviour is\ninfluenced by experience, and it is likely that pornography or sex\nrobots support the perception of other humans as mere objects of\ndesire, or even recipients of abuse, and thus ruin a deeper sexual and\nerotic experience. In this vein, the “Campaign Against Sex Robots”\nargues that these devices are a continuation of slavery and\nprostitution (Richardson 2016).  It seems clear that AI and robotics will lead to significant gains\nin productivity and thus overall wealth. The attempt to increase\nproductivity has often been a feature of the economy, though the\nemphasis on “growth” is a modern phenomenon (Harari 2016:\n240). However, productivity gains through automation typically mean\nthat fewer humans are required for the same output. This does not\nnecessarily imply a loss of overall employment, however, because\navailable wealth increases and that can increase demand sufficiently\nto counteract the productivity gain. In the long run, higher\nproductivity in industrial societies has led to more wealth overall.\nMajor labour market disruptions have occurred in the past, e.g.,\nfarming employed over 60% of the workforce in Europe and North-America\nin 1800, while by 2010 it employed ca. 5% in the EU, and even less in\nthe wealthiest countries (European Commission 2013). In the 20 years\nbetween 1950 and 1970 the number of hired agricultural workers in the\nUK was reduced by 50% (Zayed and Loft 2019). Some of these disruptions\nlead to more labour-intensive industries moving to places with lower\nlabour cost. This is an ongoing process. \nClassic automation replaced human muscle, whereas digital automation\nreplaces human thought or information-processing—and unlike\nphysical machines, digital automation is very cheap to duplicate\n(Bostrom and Yudkowsky 2014). It may thus mean a more radical change\non the labour market. So, the main question is: will the effects be\ndifferent this time? Will the creation of new jobs and wealth keep up\nwith the destruction of jobs? And even if it is not\ndifferent, what are the transition costs, and who bears them? Do we\nneed to make societal adjustments for a fair distribution of costs and\nbenefits of digital automation? \nResponses to the issue of unemployment from AI have ranged from the\nalarmed (Frey and Osborne 2013; Westlake 2014) to the neutral\n(Metcalf, Keller, and Boyd 2016 [OIR]; Calo 2018; Frey 2019) to the\noptimistic (Brynjolfsson and McAfee 2016; Harari 2016; Danaher 2019a).\nIn principle, the labour market effect of automation seems to be\nfairly well understood as involving two channels:  \n(i) the nature of interactions between differently skilled workers and\nnew technologies affecting labour demand and (ii) the equilibrium\neffects of technological progress through consequent changes in labour\nsupply and product markets. (Goos 2018: 362)  \nWhat currently seems to happen in the labour market as a result of AI\nand robotics automation is “job polarisation” or the\n“dumbbell” shape (Goos, Manning, and Salomons 2009): The\nhighly skilled technical jobs are in demand and highly paid, the low\nskilled service jobs are in demand and badly paid, but the\nmid-qualification jobs in factories and offices, i.e., the majority of\njobs, are under pressure and reduced because they are relatively\npredictable, and most likely to be automated (Baldwin 2019). \nPerhaps enormous productivity gains will allow the “age of\nleisure” to be realised, something (Keynes 1930) had predicted\nto occur around 2030, assuming a growth rate of 1% per annum.\nActually, we have already reached the level he anticipated for 2030,\nbut we are still working—consuming more and inventing ever more\nlevels of organisation. Harari explains how this economic development\nallowed humanity to overcome hunger, disease, and war—and now we\naim for immortality and eternal bliss through AI, thus his title\nHomo Deus (Harari 2016: 75). \nIn general terms, the issue of unemployment is an issue of how goods\nin a society should be justly distributed. A standard view is that\ndistributive justice should be rationally decided from behind a\n“veil of ignorance” (Rawls 1971), i.e., as if one does not\nknow what position in a society one would actually be taking (labourer\nor industrialist, etc.). Rawls thought the chosen principles would\nthen support basic liberties and a distribution that is of greatest\nbenefit to the least-advantaged members of society. It would appear\nthat the AI economy has three features that make such justice\nunlikely: First, it operates in a largely unregulated environment\nwhere responsibility is often hard to allocate. Second, it operates in\nmarkets that have a “winner takes all” feature where\nmonopolies develop quickly. Third, the “new economy” of\nthe digital service industries is based on intangible assets, also\ncalled “capitalism without capital” (Haskel and Westlake\n2017). This means that it is difficult to control multinational\ndigital corporations that do not rely on a physical plant in a\nparticular location. These three features seem to suggest that if we\nleave the distribution of wealth to free market forces, the result\nwould be a heavily unjust distribution: And this is indeed a\ndevelopment that we can already see. \nOne interesting question that has not received too much attention is\nwhether the development of AI is environmentally sustainable: Like all\ncomputing systems, AI systems produce waste that is very hard to\nrecycle and they consume vast amounts of energy, especially for the\ntraining of machine learning systems (and even for the\n“mining” of cryptocurrency). Again, it appears that some\nactors in this space offload such costs to the general society. \nThere are several notions of autonomy in the discussion of autonomous\nsystems. A stronger notion is involved in philosophical debates where\nautonomy is the basis for responsibility and personhood (Christman\n2003 [2018]). In this context, responsibility implies autonomy, but\nnot inversely, so there can be systems that have degrees of technical\nautonomy without raising issues of responsibility. The weaker, more\ntechnical, notion of autonomy in robotics is relative and gradual: A\nsystem is said to be autonomous with respect to human control to a\ncertain degree (Müller 2012). There is a parallel here to the\nissues of bias and opacity in AI since autonomy also concerns a\npower-relation: who is in control, and who is responsible? \nGenerally speaking, one question is the degree to which autonomous\nrobots raise issues our present conceptual schemes must adapt to, or\nwhether they just require technical adjustments. In most\njurisdictions, there is a sophisticated system of civil and criminal\nliability to resolve such issues. Technical standards, e.g., for the\nsafe use of machinery in medical environments, will likely need to be\nadjusted. There is already a field of “verifiable AI” for\nsuch safety-critical systems and for “security\napplications”. Bodies like the IEEE (The Institute of Electrical\nand Electronics Engineers) and the BSI (British Standards Institution)\nhave produced “standards”, particularly on more technical\nsub-problems, such as data security and transparency. Among the many\nautonomous systems on land, on water, under water, in air or space, we\ndiscuss two samples: autonomous vehicles and autonomous weapons. \nAutonomous vehicles hold the promise to reduce the very significant\ndamage that human driving currently causes—approximately 1\nmillion humans being killed per year, many more injured, the\nenvironment polluted, earth sealed with concrete and tarmac, cities\nfull of parked cars, etc. However, there seem to be questions on how\nautonomous vehicles should behave, and how responsibility and risk\nshould be distributed in the complicated system the vehicles operates\nin. (There is also significant disagreement over how long the\ndevelopment of fully autonomous, or “level 5” cars (SAE\nInternational 2018) will actually take.) \nThere is some discussion of “trolley problems” in this\ncontext. In the classic “trolley problems” (Thomson 1976;\nWoollard and Howard-Snyder 2016: section 2) various dilemmas are\npresented. The simplest version is that of a trolley train on a track\nthat is heading towards five people and will kill them, unless the\ntrain is diverted onto a side track, but on that track there is one\nperson, who will be killed if the train takes that side track. The\nexample goes back to a remark in (Foot 1967: 6), who discusses a\nnumber of dilemma cases where tolerated and intended consequences of\nan action differ. “Trolley problems” are not supposed to\ndescribe actual ethical problems or to be solved with a\n“right” choice. Rather, they are thought-experiments where\nchoice is artificially constrained to a small finite number of\ndistinct one-off options and where the agent has perfect knowledge.\nThese problems are used as a theoretical tool to investigate ethical\nintuitions and theories—especially the difference between\nactively doing vs. allowing something to happen, intended vs.\ntolerated consequences, and consequentialist vs. other normative\napproaches (Kamm 2016). This type of problem has reminded many of the\nproblems encountered in actual driving and in autonomous driving (Lin\n2016). It is doubtful, however, that an actual driver or autonomous\ncar will ever have to solve trolley problems (but see Keeling 2020).\nWhile autonomous car trolley problems have received a lot of media\nattention (Awad et al. 2018), they do not seem to offer anything new\nto either ethical theory or to the programming of autonomous\nvehicles. \nThe more common ethical problems in driving, such as speeding, risky\novertaking, not keeping a safe distance, etc. are classic problems of\npursuing personal interest vs. the common good. The vast majority of\nthese are covered by legal regulations on driving. Programming the car\nto drive “by the rules” rather than “by the interest\nof the passengers” or “to achieve maximum utility”\nis thus deflated to a standard problem of programming ethical machines\n(see\n section 2.9).\n There are probably additional discretionary rules of politeness and\ninteresting questions on when to break the rules (Lin 2016), but again\nthis seems to be more a case of applying standard considerations\n(rules vs. utility) to the case of autonomous vehicles. \nNotable policy efforts in this field include the report (German\nFederal Ministry of Transport and Digital Infrastructure 2017), which\nstresses that safety is the primary objective. Rule 10 states\n \nIn the case of automated and connected driving systems, the\naccountability that was previously the sole preserve of the individual\nshifts from the motorist to the manufacturers and operators of the\ntechnological systems and to the bodies responsible for taking\ninfrastructure, policy and legal decisions.  \n(See\n section 2.10.1\n below). The resulting German and EU laws on licensing automated\ndriving are much more restrictive than their US counterparts where\n“testing on consumers” is a strategy used by some\ncompanies—without informed consent of the consumers or their\npossible victims. \nThe notion of automated weapons is fairly old:  \nFor example, instead of fielding simple guided missiles or remotely\npiloted vehicles, we might launch completely autonomous land, sea, and\nair vehicles capable of complex, far-ranging reconnaissance and attack\nmissions. (DARPA 1983: 1)  \nThis proposal was ridiculed as “fantasy” at the time\n(Dreyfus, Dreyfus, and Athanasiou 1986: ix), but it is now a reality,\nat least for more easily identifiable targets (missiles, planes,\nships, tanks, etc.), but not for human combatants. The main arguments\nagainst (lethal) autonomous weapon systems (AWS or LAWS), are that\nthey support extrajudicial killings, take responsibility away from\nhumans, and make wars or killings more likely—for a detailed\nlist of issues see Lin, Bekey, and Abney (2008: 73–86). \nIt appears that lowering the hurdle to use such systems (autonomous\nvehicles, “fire-and-forget” missiles, or drones loaded\nwith explosives) and reducing the probability of being held\naccountable would increase the probability of their use. The crucial\nasymmetry where one side can kill with impunity, and thus has few\nreasons not to do so, already exists in conventional drone wars with\nremote controlled weapons (e.g., US in Pakistan). It is easy to\nimagine a small drone that searches, identifies, and kills an\nindividual human—or perhaps a type of human. These are the kinds\nof cases brought forward by the Campaign to Stop Killer\nRobots and other activist groups. Some seem to be equivalent to\nsaying that autonomous weapons are indeed weapons …, and\nweapons kill, but we still make them in gigantic numbers. On the\nmatter of accountability, autonomous weapons might make identification\nand prosecution of the responsible agents more difficult—but\nthis is not clear, given the digital records that one can keep, at\nleast in a conventional war. The difficulty of allocating punishment\nis sometimes called the “retribution gap” (Danaher\n2016a). \nAnother question is whether using autonomous weapons in war would make\nwars worse, or make wars less bad. If robots reduce war crimes and\ncrimes in war, the answer may well be positive and has been used as an\nargument in favour of these weapons (Arkin 2009; Müller 2016a)\nbut also as an argument against them (Amoroso and Tamburrini 2018).\nArguably the main threat is not the use of such weapons in\nconventional warfare, but in asymmetric conflicts or by non-state\nagents, including criminals. \nIt has also been said that autonomous weapons cannot conform to\nInternational Humanitarian Law, which requires observance of the\nprinciples of distinction (between combatants and civilians),\nproportionality (of force), and military necessity (of force) in\nmilitary conflict (A. Sharkey 2019). It is true that the distinction\nbetween combatants and non-combatants is hard, but the distinction\nbetween civilian and military ships is easy—so all this says is\nthat we should not construct and use such weapons if they do violate\nHumanitarian Law. Additional concerns have been raised that being\nkilled by an autonomous weapon threatens human dignity, but even the\ndefenders of a ban on these weapons seem to say that these are not\ngood arguments:  \nThere are other weapons, and other technologies, that also compromise\nhuman dignity. Given this, and the ambiguities inherent in the\nconcept, it is wiser to draw on several types of objections in\narguments against AWS, and not to rely exclusively on human dignity.\n(A. Sharkey 2019) \nA lot has been made of keeping humans “in the loop” or\n“on the loop” in the military guidance on\nweapons—these ways of spelling out “meaningful\ncontrol” are discussed in (Santoni de Sio and van den Hoven\n2018). There have been discussions about the difficulties of\nallocating responsibility for the killings of an autonomous weapon,\nand a “responsibility gap” has been suggested (esp. Rob\nSparrow 2007), meaning that neither the human nor the machine may be\nresponsible. On the other hand, we do not assume that for every event\nthere is someone responsible for that event, and the real issue may\nwell be the distribution of risk (Simpson and Müller 2016). Risk\nanalysis (Hansson 2013) indicates it is crucial to identify who is\nexposed to risk, who is a potential beneficiary, and\nwho makes the decisions (Hansson 2018: 1822–1824). \nMachine ethics is ethics for machines, for “ethical\nmachines”, for machines as subjects, rather than for\nthe human use of machines as objects. It is often not very\nclear whether this is supposed to cover all of AI ethics or to be a\npart of it (Floridi and Saunders 2004; Moor 2006; Anderson and\nAnderson 2011; Wallach and Asaro 2017). Sometimes it looks as though\nthere is the (dubious) inference at play here that if machines act in\nethically relevant ways, then we need a machine ethics. Accordingly,\nsome use a broader notion:  \nmachine ethics is concerned with ensuring that the behavior of\nmachines toward human users, and perhaps other machines as well, is\nethically acceptable. (Anderson and Anderson 2007: 15)  \nThis might include mere matters of product safety, for example. Other\nauthors sound rather ambitious but use a narrower notion:  \nAI reasoning should be able to take into account societal values,\nmoral and ethical considerations; weigh the respective priorities of\nvalues held by different stakeholders in various multicultural\ncontexts; explain its reasoning; and guarantee transparency. (Dignum\n2018: 1, 2)  \nSome of the discussion in machine ethics makes the very substantial\nassumption that machines can, in some sense, be ethical agents\nresponsible for their actions, or “autonomous moral\nagents” (see van Wynsberghe and Robbins 2019). The basic idea of\nmachine ethics is now finding its way into actual robotics where the\nassumption that these machines are artificial moral agents in any\nsubstantial sense is usually not made (Winfield et al. 2019). It is\nsometimes observed that a robot that is programmed to follow ethical\nrules can very easily be modified to follow unethical rules\n(Vanderelst and Winfield 2018). \nThe idea that machine ethics might take the form of “laws”\nhas famously been investigated by Isaac Asimov, who proposed\n“three laws of robotics” (Asimov 1942):  \nFirst Law—A robot may not injure a human being or, through\ninaction, allow a human being to come to harm. Second Law—A\nrobot must obey the orders given it by human beings except where such\norders would conflict with the First Law. Third Law—A robot must\nprotect its own existence as long as such protection does not conflict\nwith the First or Second Laws.  \nAsimov then showed in a number of stories how conflicts between these\nthree laws will make it problematic to use them despite their\nhierarchical organisation. \nIt is not clear that there is a consistent notion of “machine\nethics” since weaker versions are in danger of reducing\n“having an ethics” to notions that would not normally be\nconsidered sufficient (e.g., without “reflection” or even\nwithout “action”); stronger notions that move towards\nartificial moral agents may describe a—currently—empty\nset. \n\n If\n one takes machine ethics to concern moral agents, in some substantial\nsense, then these agents can be called “artificial moral\nagents”, having rights and responsibilities. However, the\ndiscussion about artificial entities challenges a number of common\nnotions in ethics and it can be very useful to understand these in\nabstraction from the human case (cf. Misselhorn 2020; Powers and\nGanascia forthcoming). \nSeveral authors use “artificial moral agent” in a less\ndemanding sense, borrowing from the use of “agent” in\nsoftware engineering in which case matters of responsibility and\nrights will not arise (Allen, Varner, and Zinser 2000). James Moor\n(2006) distinguishes four types of machine agents: ethical impact\nagents (e.g., robot jockeys), implicit ethical agents (e.g., safe\nautopilot), explicit ethical agents (e.g., using formal methods to\nestimate utility), and full ethical agents (who “can make\nexplicit ethical judgments and generally is competent to reasonably\njustify them. An average adult human is a full ethical agent”.)\nSeveral ways to achieve “explicit” or “full”\nethical agents have been proposed, via programming it in (operational\nmorality), via “developing” the ethics itself (functional\nmorality), and finally full-blown morality with full intelligence and\nsentience (Allen, Smit, and Wallach 2005; Moor 2006). Programmed\nagents are sometimes not considered “full” agents because\nthey are “competent without comprehension”, just like the\nneurons in a brain (Dennett 2017; Hakli and Mäkelä\n2019). \nIn some discussions, the notion of “moral patient” plays a\nrole: Ethical agents have responsibilities while ethical\npatients have rights because harm to them matters. It seems\nclear that some entities are patients without being agents, e.g.,\nsimple animals that can feel pain but cannot make justified choices.\nOn the other hand, it is normally understood that all agents will also\nbe patients (e.g., in a Kantian framework). Usually, being a person is\nsupposed to be what makes an entity a responsible agent, someone who\ncan have duties and be the object of ethical concerns. Such\npersonhood is typically a deep notion associated with phenomenal consciousness, intention and free will (Frankfurt 1971; Strawson 1998). Torrance (2011) suggests “artificial (or machine)\nethics could be defined as designing machines that do things that,\nwhen done by humans, are indicative of the possession of\n‘ethical status’ in those humans” (2011:\n116)—which he takes to be “ethical productivity\nand ethical receptivity” (2011: 117)—his\nexpressions for moral agents and patients. \nThere is broad consensus that accountability, liability, and the rule\nof law are basic requirements that must be upheld in the face of new\ntechnologies (European Group on Ethics in Science and New Technologies\n2018, 18), but the issue in the case of robots is how this can be done\nand how responsibility can be allocated. If the robots act, will they\nthemselves be responsible, liable, or accountable for their actions?\nOr should the distribution of risk perhaps take precedence over\ndiscussions of responsibility? \nTraditional distribution of responsibility already occurs: A car maker\nis responsible for the technical safety of the car, a driver is\nresponsible for driving, a mechanic is responsible for proper\nmaintenance, the public authorities are responsible for the technical\nconditions of the roads, etc. In general  \nThe effects of decisions or actions based on AI are often the result\nof countless interactions among many actors, including designers,\ndevelopers, users, software, and hardware.… With distributed\nagency comes distributed responsibility. (Taddeo and Floridi 2018:\n751).  \nHow this distribution might occur is not a problem that is specific to\nAI, but it gains particular urgency in this context (Nyholm 2018a,\n2018b). In classical control engineering, distributed control is often\nachieved through a control hierarchy plus control loops across these\nhierarchies. \nSome authors have indicated that it should be seriously considered\nwhether current robots must be allocated rights (Gunkel 2018a, 2018b;\nDanaher forthcoming; Turner 2019). This position seems to rely largely\non criticism of the opponents and on the empirical observation that\nrobots and other non-persons are sometimes treated as having rights.\nIn this vein, a “relational turn” has been proposed: If we\nrelate to robots as though they had rights, then we might be\nwell-advised not to search whether they “really” do have\nsuch rights (Coeckelbergh 2010, 2012, 2018). This raises the question\nhow far such anti-realism or quasi-realism can go, and what it means\nthen to say that “robots have rights” in a human-centred\napproach (Gerdes 2016). On the other side of the debate, Bryson has\ninsisted that robots should not enjoy rights (Bryson 2010), though she\nconsiders it a possibility (Gunkel and Bryson 2014). \nThere is a wholly separate issue whether robots (or other AI systems)\nshould be given the status of “legal entities” or\n“legal persons” in a sense natural persons, but also\nstates, businesses, or organisations are “entities”,\nnamely they can have legal rights and duties. The European Parliament\nhas considered allocating such status to robots in order to deal with\ncivil liability (EU Parliament 2016; Bertolini and Aiello 2018), but\nnot criminal liability—which is reserved for natural persons. It\nwould also be possible to assign only a certain subset of rights and\nduties to robots. It has been said that “such legislative action\nwould be morally unnecessary and legally troublesome” because it\nwould not serve the interest of humans (Bryson, Diamantis, and Grant\n2017: 273). In environmental ethics there is a long-standing\ndiscussion about the legal rights for natural objects like trees (C.\nD. Stone 1972). \nIt has also been said that the reasons for developing robots with\nrights, or artificial moral patients, in the future are ethically\ndoubtful (van Wynsberghe and Robbins 2019). In the community of\n“artificial consciousness” researchers there is a\nsignificant concern whether it would be ethical to create such\nconsciousness since creating it would presumably imply ethical\nobligations to a sentient being, e.g., not to harm it and not to end\nits existence by switching it off—some authors have called for a\n“moratorium on synthetic phenomenology” (Bentley et al.\n2018: 28f). \nIn some quarters, the aim of current AI is thought to be an\n“artificial general intelligence” (AGI), contrasted to a\ntechnical or “narrow” AI. AGI is usually distinguished\nfrom traditional notions of AI as a general purpose system, and from\nSearle’s notion of “strong AI”:  \ncomputers given the right programs can be literally said to\nunderstand and have other cognitive states. (Searle 1980:\n417) \nThe idea of singularity is that if the trajectory of\nartificial intelligence reaches up to systems that have a human level\nof intelligence, then these systems would themselves have the ability\nto develop AI systems that surpass the human level of intelligence,\ni.e., they are “superintelligent” (see below). Such\nsuperintelligent AI systems would quickly self-improve or develop even\nmore intelligent systems. This sharp turn of events after reaching\nsuperintelligent AI is the “singularity” from which the\ndevelopment of AI is out of human control and hard to predict\n(Kurzweil 2005: 487). \nThe fear that “the robots we created will take over the\nworld” had captured human imagination even before there were\ncomputers (e.g., Butler 1863) and is the central theme in\nČapek’s famous play that introduced the word\n“robot” (Čapek 1920). This fear was first formulated\nas a possible trajectory of existing AI into an “intelligence\nexplosion” by Irvin Good:  \nLet an ultraintelligent machine be defined as a machine that can far\nsurpass all the intellectual activities of any man however clever.\nSince the design of machines is one of these intellectual activities,\nan ultraintelligent machine could design even better machines; there\nwould then unquestionably be an “intelligence explosion”,\nand the intelligence of man would be left far behind. Thus the first\nultraintelligent machine is the last invention that man need ever\nmake, provided that the machine is docile enough to tell us how to\nkeep it under control. (Good 1965: 33) \nThe optimistic argument from acceleration to singularity is spelled\nout by Kurzweil (1999, 2005, 2012) who essentially points out that\ncomputing power has been increasing exponentially, i.e., doubling ca.\nevery 2 years since 1970 in accordance with “Moore’s\nLaw” on the number of transistors, and will continue to do so\nfor some time in the future. He predicted in (Kurzweil 1999) that by\n2010 supercomputers will reach human computation capacity, by 2030\n“mind uploading” will be possible, and by 2045 the\n“singularity” will occur. Kurzweil talks about an increase\nin computing power that can be purchased at a given cost—but of\ncourse in recent years the funds available to AI companies have also\nincreased enormously: Amodei and Hernandez (2018 [OIR]) thus estimate that\nin the years 2012–2018 the actual computing power available to\ntrain a particular AI system doubled every 3.4 months, resulting in an\n300,000x increase—not the 7x increase that doubling every two\nyears would have created. \nA common version of this argument (Chalmers 2010) talks about an\nincrease in “intelligence” of the AI system (rather than\nraw computing power), but the crucial point of\n“singularity” remains the one where further development of\nAI is taken over by AI systems and accelerates beyond human level.\nBostrom (2014) explains in some detail what would happen at that point\nand what the risks for humanity are. The discussion is summarised in\nEden et al. (2012); Armstrong (2014); Shanahan (2015). There are\npossible paths to superintelligence other than computing power\nincrease, e.g., the complete emulation of the human brain on a\ncomputer (Kurzweil 2012; Sandberg 2013), biological paths, or networks\nand organisations (Bostrom 2014: 22–51). \nDespite obvious weaknesses in the identification of\n“intelligence” with processing power, Kurzweil seems right\nthat humans tend to underestimate the power of exponential growth.\nMini-test: If you walked in steps in such a way that each step is\ndouble the previous, starting with a step of one metre, how far would\nyou get with 30 steps? (answer: almost 3 times further than the\nEarth’s only permanent natural satellite.) Indeed, most progress\nin AI is readily attributable to the availability of processors that\nare faster by degrees of magnitude, larger storage, and higher\ninvestment (Müller 2018). The actual acceleration and its speeds\nare discussed in (Müller and Bostrom 2016; Bostrom, Dafoe, and\nFlynn forthcoming); Sandberg (2019) argues that progress will continue\nfor some time. \nThe participants in this debate are united by being technophiles in\nthe sense that they expect technology to develop rapidly and bring\nbroadly welcome changes—but beyond that, they divide into those\nwho focus on benefits (e.g., Kurzweil) and those who focus on risks\n(e.g., Bostrom). Both camps sympathise with “transhuman”\nviews of survival for humankind in a different physical form, e.g.,\nuploaded on a computer (Moravec 1990, 1998; Bostrom 2003a, 2003c).\nThey also consider the prospects of “human enhancement” in\nvarious respects, including intelligence—often called\n“IA” (intelligence augmentation). It may be that future AI\nwill be used for human enhancement, or will contribute further to the\ndissolution of the neatly defined human single person. Robin Hanson\nprovides detailed speculation about what will happen economically in\ncase human “brain emulation” enables truly intelligent\nrobots or “ems” (Hanson 2016). \nThe argument from superintelligence to risk requires the assumption\nthat superintelligence does not imply benevolence—contrary to\nKantian traditions in ethics that have argued higher levels of\nrationality or intelligence would go along with a better understanding\nof what is moral and better ability to act morally (Gewirth 1978;\nChalmers 2010: 36f). Arguments for risk from superintelligence say\nthat rationality and morality are entirely independent\ndimensions—this is sometimes explicitly argued for as an\n“orthogonality thesis” (Bostrom 2012; Armstrong 2013;\nBostrom 2014: 105–109). \nCriticism of the singularity narrative has been raised from various\nangles. Kurzweil and Bostrom seem to assume that intelligence is a\none-dimensional property and that the set of intelligent agents is\ntotally-ordered in the mathematical sense—but neither discusses\nintelligence at any length in their books. Generally, it is fair to\nsay that despite some efforts, the assumptions made in the powerful\nnarrative of superintelligence and singularity have not been\ninvestigated in detail. One question is whether such a singularity\nwill ever occur—it may be conceptually impossible, practically\nimpossible or may just not happen because of contingent events,\nincluding people actively preventing it. Philosophically, the\ninteresting question is whether singularity is just a\n“myth” (Floridi 2016; Ganascia 2017), and not on the\ntrajectory of actual AI research. This is something that practitioners\noften assume (e.g., Brooks 2017 [OIR]). They may do so because they fear the\npublic relations backlash, because they overestimate the practical\nproblems, or because they have good reasons to think that\nsuperintelligence is an unlikely outcome of current AI research\n(Müller forthcoming-a). This discussion raises the question\nwhether the concern about “singularity” is just a\nnarrative about fictional AI based on human fears. But even if one\ndoes find negative reasons compelling and the singularity not\nlikely to occur, there is still a significant possibility that one may\nturn out to be wrong. Philosophy is not on the “secure path of a\nscience” (Kant 1791: B15), and maybe AI and robotics\naren’t either (Müller 2020). So, it appears that discussing\nthe very high-impact risk of singularity has justification even\nif one thinks the probability of such singularity ever occurring\nis very low. \nThinking about superintelligence in the long term raises the question\nwhether superintelligence may lead to the extinction of the human\nspecies, which is called an “existential risk” (or XRisk):\nThe superintelligent systems may well have preferences that conflict\nwith the existence of humans on Earth, and may thus decide to end that\nexistence—and given their superior intelligence, they will have\nthe power to do so (or they may happen to end it because they do not\nreally care). \nThinking in the long term is the crucial feature of this literature.\nWhether the singularity (or another catastrophic event) occurs in 30\nor 300 or 3000 years does not really matter (Baum et al. 2019).\nPerhaps there is even an astronomical pattern such that an intelligent\nspecies is bound to discover AI at some point, and thus bring about\nits own demise. Such a “great filter” would contribute to\nthe explanation of the “Fermi paradox” why there is no\nsign of life in the known universe despite the high probability of it\nemerging. It would be bad news if we found out that the “great\nfilter” is ahead of us, rather than an obstacle that Earth has\nalready passed. These issues are sometimes taken more narrowly to be\nabout human extinction (Bostrom 2013), or more broadly as concerning\nany large risk for the species (Rees 2018)—of which AI is only\none (Häggström 2016; Ord 2020). Bostrom also uses the\ncategory of “global catastrophic risk” for risks that are\nsufficiently high up the two dimensions of “scope” and\n“severity” (Bostrom and Ćirković 2011; Bostrom\n2013). \nThese discussions of risk are usually not connected to the general\nproblem of ethics under risk (e.g., Hansson 2013, 2018). The long-term\nview has its own methodological challenges but has produced a wide\ndiscussion: (Tegmark 2017) focuses on AI and human life\n“3.0” after singularity while Russell, Dewey, and Tegmark\n(2015) and Bostrom, Dafoe, and Flynn (forthcoming) survey longer-term\npolicy issues in ethical AI. Several collections of papers have\ninvestigated the risks of artificial general intelligence (AGI) and\nthe factors that might make this development more or less risk-laden\n(Müller 2016b; Callaghan et al. 2017; Yampolskiy 2018), including\nthe development of non-agent AI (Drexler 2019). \nIn a narrow sense, the “control problem” is how we humans\ncan remain in control of an AI system once it is superintelligent\n(Bostrom 2014: 127ff). In a wider sense, it is the problem of how we\ncan make sure an AI system will turn out to be positive according to\nhuman perception (Russell 2019); this is sometimes called “value\nalignment”. How easy or hard it is to control a\nsuperintelligence depends significantly on the speed of\n“take-off” to a superintelligent system. This has led to\nparticular attention to systems with self-improvement, such as\nAlphaZero (Silver et al. 2018). \nOne aspect of this problem is that we might decide a certain feature\nis desirable, but then find out that it has unforeseen consequences\nthat are so negative that we would not desire that feature after all.\nThis is the ancient problem of King Midas who wished that all he\ntouched would turn into gold. This problem has been discussed on the\noccasion of various examples, such as the “paperclip\nmaximiser” (Bostrom 2003b), or the program to optimise chess\nperformance (Omohundro 2014). \nDiscussions about superintelligence include speculation about\nomniscient beings, the radical changes on a “latter day”,\nand the promise of immortality through transcendence of our current\nbodily form—so sometimes they have clear religious undertones\n(Capurro 1993; Geraci 2008, 2010; O’Connell 2017: 160ff). These\nissues also pose a well-known problem of epistemology: Can we know the\nways of the omniscient (Danaher 2015)? The usual opponents have\nalready shown up: A characteristic response of an atheist is  \nPeople worry that computers will get too smart and take over the\nworld, but the real problem is that they’re too stupid and\nthey’ve already taken over the world (Domingos 2015)  \nThe new nihilists explain that a “techno-hypnosis” through\ninformation technologies has now become our main method of distraction\nfrom the loss of meaning (Gertz 2018). Both opponents would thus say\nwe need an ethics for the “small” problems that occur with\nactual AI and robotics \n (sections 2.1 through 2.9 above), \n and that there is less need for the “big ethics” of\nexistential risk from AI (section 2.10). \nThe singularity thus raises the problem of the concept of AI again. It\nis remarkable how imagination or “vision” has played a\ncentral role since the very beginning of the discipline at the\n“Dartmouth Summer Research Project” (McCarthy et al. 1955 [OIR];\nSimon and Newell 1958). And the evaluation of this vision is subject\nto dramatic change: In a few decades, we went from the slogans\n“AI is impossible” (Dreyfus 1972) and “AI is just\nautomation” (Lighthill 1973) to “AI will solve all\nproblems” (Kurzweil 1999) and “AI may kill us all”\n(Bostrom 2014). This created media attention and public relations\nefforts, but it also raises the problem of how much of this\n“philosophy and ethics of AI” is really about AI rather\nthan about an imagined technology. As we said at the outset, AI and\nrobotics have raised fundamental questions about what we should do\nwith these systems, what the systems themselves should do, and what\nrisks they have in the long term. They also challenge the human view\nof humanity as the intelligent and dominant species on Earth. We have\nseen issues that have been raised and will have to watch technological\nand social developments closely to catch the new issues early on,\ndevelop a philosophical analysis, and learn for traditional problems\nof philosophy.","contact.mail":"v.c.muller@leeds.ac.uk","contact.domain":"leeds.ac.uk"}]
