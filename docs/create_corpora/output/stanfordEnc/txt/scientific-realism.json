[{"date.published":"2011-04-27","date.changed":"2017-06-12","url":"https://plato.stanford.edu/entries/scientific-realism/","author1":"Anjan Chakravartty","author1.info":"https://anjanchakravartty.com/","entry":"scientific-realism","body.text":"\n\n\nDebates about scientific realism are closely connected to almost\neverything else in the philosophy of science, for they concern the\nvery nature of scientific knowledge. Scientific realism is a positive\nepistemic attitude toward the content of our best theories and models,\nrecommending belief in both observable and unobservable aspects of the\nworld described by the sciences. This epistemic attitude has important\nmetaphysical and semantic dimensions, and these various commitments\nare contested by a number of rival epistemologies of science, known\ncollectively as forms of scientific antirealism. This article explains\nwhat scientific realism is, outlines its main variants, considers the\nmost common arguments for and against the position, and contrasts it\nwith its most important antirealist counterparts.\n\n\nIt is perhaps only a slight exaggeration to say that scientific\nrealism is characterized differently by every author who discusses it,\nand this presents a challenge to anyone hoping to learn what it is.\nFortunately, underlying the many idiosyncratic qualifications and\nvariants of the position, there is a common core of ideas, typified by\nan epistemically positive attitude toward the outputs of scientific\ninvestigation, regarding both observable and unobservable aspects of\nthe world. The distinction here between the observable and the\nunobservable reflects human sensory capabilities: the observable is\nthat which can, under favorable conditions, be perceived using the\nunaided senses (for example, planets and platypuses); the unobservable\nis that which cannot be detected this way (for example, proteins and\nprotons). This is to privilege vision merely for terminological\nconvenience, and differs from scientific conceptions of observability,\nwhich generally extend to things that are detectable using instruments\n(Shapere 1982). The distinction itself has been problematized (Maxwell\n1962; Churchland 1985; Musgrave 1985; Dicken & Lipton 2006) and\ndefended (Muller 2004, 2005; cf. Turner 2007 regarding the distant\npast). If it is problematic, this is arguably a concern\nprimarily for certain forms of antirealism, which adopt an\nepistemically positive attitude only with respect to the\nobservable. It is not ultimately a concern for scientific realism,\nwhich does not discriminate epistemically between observables and\nunobservables per se. \nBefore considering the nuances of what scientific realism entails, it\nis useful to distinguish between two different kinds of definition in\nthis context. Most commonly, the position is described in terms of the\nepistemic achievements constituted by scientific theories\n(and models—this qualification will be taken as given\nhenceforth). On this approach, scientific realism is a position\nconcerning the actual epistemic status of theories (or some components\nthereof), and this is described in a number of ways. For example, most\npeople define scientific realism in terms of the truth or approximate\ntruth of scientific theories or certain aspects of theories. Some\ndefine it in terms of the successful reference of theoretical terms to\nthings in the world, both observable and unobservable. (A note about\nthe literature: “theoretical term”, prior to the 1980s,\nwas standardly used to denote terms for unobservables, but will be\nused here to refer to any scientific term, which is now the more\ncommon usage.) Others define scientific realism not in terms of truth\nor reference, but in terms of belief in the ontology of scientific\ntheories. What all of these approaches have in common is a commitment\nto the idea that our best theories have a certain epistemic status:\nthey yield knowledge of aspects of the world, including unobservable\naspects. (For definitions along these lines, see Smart 1963; Boyd\n1983; Devitt 1991; Kukla 1998; Niiniluoto 1999; Psillos 1999; and\nChakravartty 2007a.) \nAnother way to think about scientific realism is in terms of the\nepistemic aims of scientific inquiry (van Fraassen 1980: 8; Lyons\n2005). That is, some think of the position in terms of what science\naims to do: the scientific realist holds that science aims to produce\ntrue descriptions of things in the world (or approximately true\ndescriptions, or ones whose central terms successfully refer, and so\non). There is a weak implication here to the effect that if science\naims at truth, and scientific practice is at all successful, the\ncharacterization of scientific realism in terms of aim may then entail\nsome form of characterization in terms of achievement. But this is not\na strict implication, since defining scientific realism in terms of\naiming at truth does not, strictly speaking, suggest anything about\nthe success of scientific practice in this regard. For this reason,\nsome take the aspirational characterization of scientific realism to\nbe too weak (Kitcher 1993: 150; Devitt 2005: n. 10; Chakravartty\n2007b: 197; for skepticism about scientific aim-talk more generally,\nsee Rowbottom 2014)—it is compatible with the sciences never\nactually achieving, and even the impossibility of their achieving,\ntheir aim as conceived on this view of scientific realism. Most\nscientific realists commit to something more in terms of achievement,\nand this is assumed in what follows. \nThe description of scientific realism as a positive epistemic attitude\ntoward theories, including parts putatively concerning the\nunobservable, is a kind of shorthand for more precise commitments\n(Kukla 1998: ch. 1; Niiniluoto 1999: ch. 1; Psillos 1999:\nIntroduction; Chakravartty 2007a: ch. 1). Traditionally, realism more\ngenerally is associated with any position that endorses belief in the\nreality of something. Thus, one might be a realist about one’s\nperceptions of tables and chairs (sense datum realism), or about\ntables and chairs themselves (external world realism), or about\nmathematical entities such as numbers and sets (mathematical realism),\nand so on. Scientific realism is a realism about whatever is described\nby our best scientific theories—from this point on,\n“realism” here denotes scientific realism. But what, more\nprecisely, is that? In order to be clear about what realism in the\ncontext of the sciences amounts to, and to differentiate it from some\nimportant antirealist alternatives, it is useful to understand it in\nterms of three dimensions: a metaphysical (or ontological) dimension;\na semantic dimension; and an epistemological dimension. \nMetaphysically, realism is committed to the mind-independent existence\nof the world investigated by the sciences. This idea is best clarified\nin contrast with positions that deny it. For instance, it is denied by\nany position that falls under the traditional heading of\n“idealism”, including some forms of phenomenology,\naccording to which there is no world external to and thus independent\nof the mind. This sort of idealism, however, though historically\nimportant, is rarely encountered in contemporary philosophy of\nscience. More common rejections of mind-independence stem from\nneo-Kantian views of the nature of scientific knowledge, which deny\nthat the world of our experience is mind-independent, even if (in some\ncases) these positions accept that the world in itself does not depend\non the existence of minds. The contention here is that the world\ninvestigated by the sciences—as distinct from “the world\nin itself” (assuming this to be a coherent distinction)—is\nin some sense dependent on the ideas one brings to scientific\ninvestigation, which may include, for example, theoretical assumptions\nand perceptual training; this proposal is detailed further in\n section 4.\n It is important to note in this connection that human convention in\nscientific taxonomy is compatible with mind-independence. For example,\nthough Psillos (1999: xix) ties realism to a “mind-independent\nnatural-kind structure” of the world, Chakravartty (2007a: ch.\n6) argues that mind-independent properties are often conventionally\ngrouped into kinds (see also Boyd 1999; Humphreys 2004: 22–25, 35–36, and cf. the\n“promiscuous realism” of Dupré 1993). \nSemantically, realism is committed to a literal interpretation of\nscientific claims about the world. In common parlance, realists take\ntheoretical statements at “face value”. According to\nrealism, claims about scientific objects, events, processes,\nproperties, and relations (I will use the term “scientific\nentity” as a generic term for these sorts of things henceforth),\nwhether they be observable or unobservable, should be construed\nliterally as having truth values, whether true or false. This semantic\ncommitment contrasts primarily with those of certain\n“instrumentalist” epistemologies of science, which\ninterpret descriptions of unobservables simply as instruments for the\nprediction of observable phenomena, or for systematizing observation\nreports. Traditionally, instrumentalism holds that claims about\nunobservable things have no literal meaning at all (though the term is\noften used more liberally in connection with some antirealist\npositions today). Some antirealists contend that claims involving\nunobservables should not be interpreted literally, but as elliptical\nfor corresponding claims about observables. These positions are\ndescribed in more detail in\n section 4. \nEpistemologically, realism is committed to the idea that theoretical\nclaims (interpreted literally as describing a mind-independent\nreality) constitute knowledge of the world. This contrasts with\nskeptical positions which, even if they grant the metaphysical and\nsemantic dimensions of realism, doubt that scientific investigation is\nepistemologically powerful enough to yield such knowledge, or, as in\nthe case of some antirealist positions, insist that it is only\npowerful enough to yield knowledge regarding observables. The\nepistemological dimension of realism, though shared by realists\ngenerally, is sometimes described more specifically in contrary ways.\nFor example, while many realists subscribe to the truth (or\napproximate truth) of theories understood in terms of some version of\nthe correspondence theory of truth (as suggested by Fine 1986a and\ncontested by Ellis 1988), some prefer a truthmaker account (Asay 2013)\nor a deflationary account of truth (Giere 1988: 82; Devitt 2005; Leeds\n2007). Though most realists marry their position to the successful\nreference of theoretical terms, including those for unobservable\nentities (Boyd 1983, and as described by Laudan 1981), some deny that\nthis is a requirement (Cruse & Papineau 2002; Papineau 2010).\nAmidst these differences, however, a general recipe for realism is\nwidely shared: our best scientific theories give true or approximately\ntrue descriptions of observable and unobservable aspects of a\nmind-independent world. \nThe general recipe for realism just described is accurate so far as it\ngoes, but still falls short of the degree of precision offered by most\nrealists. The two main sources of imprecision thus far are found in\nthe general recipe itself, which makes reference to the idea of\n“our best scientific theories” and the notion of\n“approximate truth”. The motivation for these\nqualifications is perhaps clear. If one is to defend a positive\nepistemic attitude regarding scientific theories, it is presumably\nsensible to do so not merely in connection with any theory (especially\nwhen one considers that, over the long history of the sciences up to\nthe present, some theories were not or are not especially successful),\nbut rather with respect to theories (or aspects of theories,\nas we will see momentarily) that would appear, prima facie,\nto merit such a defense, viz. our best theories (or aspects\nthereof). And it is widely held, not least by realists, that even many\nof our best scientific theories are likely false, strictly speaking,\nhence the importance of the notion that theories may be “close\nto” the truth (that is, approximately true) even though they are\nfalse. The challenge of making these qualifications more precise,\nhowever, is significant, and has generated much discussion. \nConsider first the issue of how best to identify those theories that\nrealists should be realists about. A general disclaimer is in order\nhere: realists are generally fallibilists, holding that realism is\nappropriate in connection with our best theories even though they\nlikely cannot be proven with absolute certainty; some of our best\ntheories could conceivably turn out to be significantly mistaken, but\nrealists maintain that, granting this possibility, there are grounds\nfor realism nonetheless. These grounds are bolstered by restricting\nthe domain of theories suitable for realist commitment to those that\nare sufficiently mature and non-ad hoc (Worrall 1989:\n153–154; Psillos 1999: 105–108). Maturity may be thought\nof in terms of the well established nature of the field in which a\ntheory is developed, or the duration of time a theory has survived, or\nits survival in the face of significant testing; and the condition of\nbeing non-ad hoc is intended to guard against theories that\nare “cooked up” (that is, posited merely) in order to\naccount for some known observations in the absence of rigorous\ntesting. On these construals, however, both the notion of maturity and\nthe notion of being non-ad hoc are admittedly vague. One\nstrategy for adding precision here is to attribute these qualities to\ntheories that make successful, novel predictions. The ability of a\ntheory to do this, it is commonly argued, marks it as genuinely\nempirically successful, and the sort of theory to which realists\nshould be more inclined to commit (Musgrave 1988; Lipton 1990; Leplin\n1997; White 2003; Hitchcock & Sober 2004; Barnes 2008; for a\ndissenting view, see Harker 2008; cf. Alai 2014). \nThe idea that with the development of the sciences over time, theories\nare converging on (“moving in the direction of”,\n“getting closer to”) the truth, is a common theme in\nrealist discussions of theory change (for example, Hardin &\nRosenberg 1982 and Putnam 1982). Talk of approximate truth is often\ninvoked in this context and has produced a significant amount of often\nhighly technical work, conceptualizing the approximation of truth as\nsomething that can be quantified, such that judgments of relative\napproximate truth (of one proposition or theory in comparison to\nanother) can be formalized and given precise definitions. This work\nprovides one possible means by which to consider the convergentist\nclaim that theories can be viewed as increasingly approximately true\nover time, and this possibility is further considered in\n section 3.4. \nA final and especially important qualification to the general recipe\nfor realism described above comes in the form of a number of\nvariations. These species of generic realism can be viewed as falling\ninto three families or camps: explanationist realism; entity realism;\nand structural realism. There is a shared principle of speciation\nhere, in that all three approaches are attempts to identify more\nspecifically the component parts of scientific theories that are most\nworthy of epistemic commitment. Explanationism recommends\nrealist commitment with respect to those parts of our best\ntheories—regarding (unobservable) entities, laws,\netc.—that are in some sense indispensable or otherwise important\nto explaining their empirical success—for instance, components\nof theories that are crucial in order to derive successful, novel\npredictions. Entity realism is the view that under conditions\nin which one can demonstrate impressive causal knowledge of a putative\n(unobservable) entity, such as knowledge that facilitates the\nmanipulation of the entity and its use so as to intervene in other\nphenomena, one has good reason for realism regarding it.\nStructural realism is the view that one should be a realist,\nnot in connection with descriptions of the natures of things (like\nunobservable entities) found in our best theories, but rather with\nrespect to their structure. All three of these positions adopt a\nstrategy of selectivity, and this and the positions themselves are\nconsidered further in\n section 2.3. \nArguably, the fact that realists have endeavored to qualify their view\nand propose variations of it, as described above, suggests a\ncollective moral: though some (especially earlier) discussions of\nrealism give the impression that it is an attitude pertaining to\nscience across the board, this is likely too coarse a way to\nunderstand the position. Adopting a realist attitude toward the\ncontent of scientific theories does not entail that one believes all\nsuch content, but rather that one believes those aspects, including\nunobservable aspects, regarding which one takes such belief to be\nwarranted, thus indicating a realism about those things more\nspecifically. In a similar spirit, some argue for another sort of\nspecificity, suggesting that the best (or only good) arguments for\nrealism are formulated by concentrating on the details of specific\ncases—the so-called “first-order evidence” of\nscientific investigation itself. For example, leveraging a case study\nof Jean Perrin’s argument in 1908 for the reality of\nunobservable molecules, Achinstein (2002: 491–495) contends that\neven taking certain realist-friendly assumptions for granted, a\ncompelling argument for realism about any given entity can only be\ngiven in terms of the empirical evidence concerning that entity, not\nby means of more general philosophical arguments. (For similar views,\nsee Magnus & Callender 2004: 333–336 and Saatsi 2010; for\nskepticism about this, see Dicken 2013 and Park 2016.) \nThe most powerful intuition motivating realism is an old idea,\ncommonly referred to in recent discussions as the “miracle\nargument” or “no miracles argument”, after\nPutnam’s (1975a: 73) claim that realism “is the only\nphilosophy that doesn’t make the success of science a\nmiracle”. The argument begins with the widely accepted premise\nthat our best theories are extraordinarily successful: they facilitate\nempirical predictions, retrodictions, and explanations of the subject\nmatters of scientific investigation, often marked by astounding\naccuracy and intricate causal manipulations of the relevant phenomena.\nWhat explains this success? One explanation, favored by realists, is\nthat our best theories are true (or approximately true, or correctly\ndescribe a mind-independent world of entities, laws, etc.). Indeed, if\nthese theories were far from the truth, so the argument goes, the fact\nthat they are so successful would be miraculous. And given the choice\nbetween a straightforward explanation of success and a miraculous\nexplanation, clearly one should prefer the non-miraculous explanation,\nviz. that our best theories are approximately true (etc.).\n(For elaborations of the miracle argument, see J. Brown 1982; Boyd\n1989; Lipton 1994; Psillos 1999: ch. 4; Barnes 2002; Lyons 2003; Busch\n2008; Frost-Arnold 2010; and Dellsén 2016.) \nThough intuitively powerful, the miracle argument is contestable in a\nnumber of ways. One skeptical response is to question the very need\nfor an explanation of the success of science in the first place. For\nexample, van Fraassen (1980: 40; see also Wray 2007, 2010) suggests\nthat successful theories are analogous to well-adapted\norganisms—since only successful theories (organisms) survive, it\nis hardly surprising that our theories are successful, and therefore,\nthere is no demand here for an explanation of success. It is not\nentirely clear, however, whether the evolutionary analogy is\nsufficient to dissolve the intuition behind the miracle argument. One\nmight wonder, for instance, why a particular theory is\nsuccessful (as opposed to why theories in general are successful), and\nthe explanation sought may turn on specific features of the theory\nitself, including its descriptions of unobservables. Whether such\nexplanations need be true, though, is a matter of debate. While most\ntheories of explanation require that the explanans be true,\npragmatic theories of explanation do not (van Fraassen 1980: ch. 5).\nMore generally, any epistemology of science that does not accept one\nor more of the three dimensions of realism—commitment to a\nmind-independent world, literal semantics, and epistemic access to\nunobservables—will thereby present a putative reason for\nresisting the miracle argument. These positions are considered in\n section 4. \nSome authors contend that the miracle argument is, in fact, an\ninstance of fallacious reasoning called the base rate fallacy (Howson\n2000: ch. 3; Lipton [1991] 2004: 196–198; Magnus & Callender\n2004). Consider the following illustration. There is a test for a\ndisease for which the rate of false negatives (negative results in\ncases where the disease is present) is zero, and the rate of false\npositives (positive results in cases where the disease is absent) is\none in ten (that is, disease-free individuals test positive 10% of the\ntime). If one tests positive, what are the chances that one has the\ndisease? It would be a mistake to conclude that, based on the rate of\nfalse positives, the probability is 90%, for the actual probability\ndepends on some further, crucial information: the base rate of the\ndisease in the population (the proportion of people having it). The\nlower the incidence of the disease at large, the lower the probability\nthat a positive result signals the presence of the disease. \nBy analogy, using the success of a scientific theory as an indicator\nof its approximate truth (assuming a low rate of false\npositives—cases in which theories far from the truth are\nnonetheless successful) is arguably, likewise, an instance of the base\nrate fallacy. The success of a theory does not by itself suggest that\nit is likely approximately true, and since there is no independent way\nof knowing the base rate of approximately true theories, the chances\nof it being approximately true cannot be assessed. Worrall (unpublished,\nOther Internet Resources) maintains that these contentions are\nineffective against the miracle argument because they crucially depend\non a misleading formalization of it in terms of probabilities\n(cf. Menke 2014; for a criticism of the miracle argument based on a\ndifferent probabilistic framing in terms of likelihoods, see Sober\n2015: 912–915). \nOne motivation for realism in connection with at least some\nunobservables comes by way of “corroboration”. If an\nunobservable entity is putatively capable of being detected by means\nof a scientific instrument or experiment, this may well form the basis\nof a defeasible argument for realism concerning it. If, however, that\nsame entity is putatively capable of being detected by not just one,\nbut rather two or more different means of\ndetection—forms of detection that are distinct with respect to\nthe apparatuses they employ and the causal mechanisms and processes\nthey are described as exploiting in the course of detection—this\nmay serve as the basis of a significantly enhanced argument for\nrealism (cf. Eronen 2015). Hacking (1983: 201; see also Hacking 1985:\n146–147) gives the example of dense bodies in red blood\nplatelets that can be detected using different forms of microscopy.\nDifferent techniques of detection, such as those employed in light\nmicroscopy and transmission electron microscopy, make use of very\ndifferent sorts of physical processes, and these operations are\ndescribed theoretically in terms of correspondingly different causal\nmechanisms. (For similar examples, see Salmon 1984: 217–219 and\nFranklin 1986: 166–168, 1990: 103–115.) \nThe argument from corroboration thus runs as follows. The fact that\none and the same thing is apparently revealed by distinct modes of\ndetection suggests that it would be an extraordinary coincidence if\nthe supposed target of these revelations did not, in fact, exist. The\ngreater the extent to which detections can be corroborated by\ndifferent means, the stronger the argument for realism regarding their\nputative target. The argument here can be viewed as resting on an\nintuition similar to that underlying the miracle argument: realism\nbased on apparent detection may be only so compelling, but if\ndifferent, theoretically independent means of detection produce the\nsame result, suggesting the existence of one and the same\nunobservable, then realism provides a good explanation of the\nconsilient evidence, in contrast with the arguably miraculous state of\naffairs in which theoretically independent techniques produce the same\nresult in the absence of a shared target. The idea that techniques of\n(putative) detection are often constructed or calibrated precisely\nwith the intention of reproducing the outputs of others, however, may\nstand against the argument from corroboration. Additionally, van\nFraassen (1985: 297–298) argues that scientific explanations of\nevidential consilience may be accepted without the explanations\nthemselves being understood as true, which once again raises questions\nabout the nature of scientific explanation. \nIn\n section 1.3,\n the notion of selectivity was introduced as a general strategy for\nmaximizing the plausibility of realism, particularly with respect to\nscientific unobservables. This strategy is adopted in part to square\nrealism with the widely accepted view that most if not all of even our\nbest theories are false, strictly speaking. If, nevertheless, there\nare aspects of these theories that are true (or close to the truth)\nand one is able to identify these aspects, one might then plausibly\ncast one’s realism in terms of an epistemically positive\nattitude toward those aspects of theories that are most worthy of\nepistemic commitment. The most important variants of realism to\nimplement this strategy are explanationism, entity realism, and\nstructural realism. (For related work pertaining to the notion of\nselectivity more generally, see R. Miller 1987: chs. 8–10; Fine\n1991; Jones 1991; Musgrave 1992; Harker 2013; and Peters 2014.) \nExplanationists hold that a realist attitude can be justified in\nconnection with unobservables described by our best theories precisely\nwhen appealing to those unobservables is indispensable or otherwise\nimportant to explaining why these theories are successful. For\nexample, if one takes successful novel prediction to be a hallmark of\ntheories worthy of realist commitment generally, then explanationism\nsuggests that, more specifically, those aspects of the theory that are\nessential to the derivation of such novel predictions are the parts of\nthe theory most worthy of realist commitment. In this vein, Kitcher\n(1993: 140–149) draws a distinction between the\n“presuppositional posits” or “idle parts” of\ntheories, and the “working posits” to which realists\nshould commit. Psillos (1999: chs. 5–6) argues that realism can\nbe defended by demonstrating that the success of past theories did not\ndepend on their false components:  \nit is enough to show that the theoretical laws and mechanisms which\ngenerated the successes of past theories have been retained in our\ncurrent scientific image. (1999: 108)  \nThe immediate challenge to explanationism is to furnish a method with\nwhich to identify precisely those aspects of theories that are\nrequired for their success, in a way that is objective or principled\nenough to withstand the charge that realists are merely rationalizing\npost hoc, identifying the explanatorily crucial parts of past\ntheories with aspects that have been retained in our current best\ntheories. (For discussions, see Chang 2003; Stanford 2003a,b; Elsamahi\n2005; Saatsi 2005a; Lyons 2006; Harker 2010; Cordero 2011; Votsis\n2011; and Vickers 2013.) \nAnother version of realism that adopts the strategy of selectivity is\nentity realism. On this view, realist commitment is based on a\nputative ability to causally manipulate unobservable entities (like\nelectrons or gene sequences) to a high degree—for example, to\nsuch a degree that one is able to intervene in other phenomena so as\nto bring about certain effects. The greater the ability to exploit\none’s apparent causal knowledge of something so as to bring\nabout (often extraordinarily precise) outcomes, the greater the\nwarrant for belief (Hacking 1982, 1983; cf. B. Miller 2016; Cartwright\n1983: ch. 5; Giere 1988: ch. 5; on causal warrant more generally, see Egg 2012). Belief\nin scientific unobservables thus described is here partnered with a\ndegree of skepticism about scientific theories more generally, and\nthis raises questions about whether believing in entities while\nwithholding belief with respect to the theories that describe them is\na coherent or practicable combination (Morrison 1990; Elsamahi 1994;\nResnik 1994; Chakravartty 1998; Clarke 2001; Massimi 2004). Entity\nrealism is especially compatible with and nicely facilitated by the\ncausal theory of reference associated with Kripke (1980) and Putnam\n([1975b] 1985: ch. 12), according to which one can successfully refer\nto an entity despite significant or even radical changes in\ntheoretical descriptions of its properties; this allows for stability\nof epistemic commitment when theories change over time. Whether the\ncausal theory of reference can be applied successfully in this\ncontext, however, is a matter of dispute (see Hardin & Rosenberg\n1982; Laudan 1984; Psillos 1999: ch. 12; McLeish 2005, 2006; Chakravartty 2007a:\n52–56; and Landig 2014; see Weber 2014 for a case study on\ngenes). \nStructural realism is another view promoting selectivity, but in this\ncase it is the natures of unobservable entities that are viewed\nskeptically, with realism reserved for the structure of the\nunobservable realm, as represented by certain relations described by\nour best theories. All of the many versions of this position fall into\none of two camps: the first emphasizes an epistemic distinction\nbetween notions of structure and nature; the second emphasizes an\nontological thesis. The epistemic view holds that our best theories\nlikely do not correctly describe the natures of unobservable entities,\nbut do successfully describe certain relations between them. The ontic\nview suggests that the reason realists should aspire only to knowledge\nof structure is that the traditional concept of entities that stand in\nrelations is metaphysically problematic—there are, in fact, no\nsuch things, or if there are such things, they are in some sense\nemergent from or dependent on their relations. One challenge facing\nthe epistemic version is that of articulating a concept of structure\nthat makes knowledge of it effectively distinct from that of the\nnatures of entities. The ontological version faces the challenge of\nclarifying the relevant notions of emergence and/or dependence. (On\nepistemic structural realism, see Worrall 1989; Psillos 1995, 2006;\nVotsis 2003; and Morganti 2004; regarding ontic structural realism,\nsee French 1998, 2006, 2014; Ladyman 1998; Psillos 2001, 2006; Ladyman\n& Ross 2007; and Chakravartty 2007a: ch. 3. See Frigg & Votsis\n2011 for an extensive critical survey). \nLined up in opposition to the various motivations for realism\npresented in\n section 2\n are a number of important antirealist arguments, all of which have\npressed realists either to attempt their refutation, or to modify\ntheir realism accordingly. One of these challenges, the\nunderdetermination of theory by data, has a storied history in\ntwentieth century philosophy more generally, and is often traced to\nthe work of Duhem ([1906] 1954: ch. 6; this is not an argument for\nunderdetermination as such, but is regarded as sowing the seeds). In\nremarks concerning the confirmation of scientific hypotheses (in\nphysics, which he contrasted with chemistry and physiology), Duhem\nnoted that a hypothesis cannot be used to derive testable predictions\nin isolation. To derive predictions one also requires\n“auxiliary” assumptions, such as background theories,\nhypotheses about instruments and measurements, etc. If subsequent\nobservation and experiment produces data that conflict with those\npredicted, one might think that this reflects badly on the hypothesis\nunder test, but Duhem pointed out that given all of the assumptions\nrequired to derive predictions, it is no simple matter to identify\nwhere the error lies. Different amendments to one’s overall set\nof beliefs regarding hypotheses and theories will be consistent with\nthe data. A similar result is commonly associated with the later\n“confirmational holism” of Quine (1953), according to\nwhich experience (including, of course, that associated with\nscientific testing) does not confirm or disconfirm individual beliefs\nper se, but rather the set of one’s beliefs taken as a\nwhole. This sort of contention is now commonly referred to as the\n“Duhem-Quine thesis” (Quine 1975; see Ben-Menahem 2006 for\na historical introduction). \nHow then does this give rise to underdetermination, a presumptive\nconcern for realism? The argument from underdetermination proceeds as\nfollows: let us call the relevant, overall sets of scientific beliefs\n“theories”; different, conflicting theories are consistent\nwith the data; the data exhaust the evidence for belief; therefore,\nthere is no evidential reason to believe one of these theories as\nopposed to another. Given that the theories differ precisely in what\nthey say about the unobservable (their observable\nconsequences—the data—are all shared), a challenge to\nrealism emerges: the choice of which theory to believe is\nunderdetermined by the data. In contemporary discussions, the\nchallenge is usually presented using slightly different terminology.\nEvery theory, it is said, has empirically equivalent rivals—that\nis, rivals that agree with respect to the observable, but differ with\nrespect to the unobservable. This then serves as the basis of a\nskeptical argument regarding the truth of any particular theory the\nrealist may wish to endorse. Various forms of antirealism then suggest\nthat hypotheses and theories involving unobservables are endorsed, not\nmerely on the basis of evidence that may be relevant to their truth,\nbut also on the basis of other factors that are not indicative of\ntruth as such (see\n sections 3.2,\n and\n 4.2–4.4).\n (For recent explications, see van Fraassen 1980: ch. 3; Earman 1993;\nKukla 1998: chs. 5–6; and Stanford 2001.) \nThe argument from underdetermination is contested in a number of ways.\nOne might, for example, distinguish between underdetermination in\npractice (or at a time) and underdetermination in principle. In the\nformer case, there is underdetermination only because the data that\nwould support one theory or hypothesis at the expense of another is\nunavailable, pending foreseeable developments in experimental\ntechnique or instrumentation. Here, realism is arguably consistent\nwith a “wait and see” attitude, though if the prospect of\nfuture discriminating evidence is poor, a commitment to future realism\nmay be questioned thereby. In any case, most proponents of\nunderdetermination insist on the idea of underdetermination in\nprinciple: the idea that there are always (plausible) empirically\nequivalent rivals no matter what evidence may come to light. In\nresponse, some argue that the principled worry cannot be established,\nsince what counts as data is apt to change over time with the\ndevelopment of new techniques and instruments, and with changes in\nscientific background knowledge, which alter the auxiliary assumptions\nrequired to derive observable predictions (Laudan & Leplin 1991).\nSuch arguments may rest, however, on a different conception of\nobservation than that assumed by many antirealists (defined above, in\nterms of human sensory capacities). (For other responses, see Okasha\n2002; van Dyck 2007; Busch 2009; and Worrall 2011.) \nStanford (2006, 2015) proposes a historicized version of the argument\nfrom underdetermination, suggesting that the history of science\nreveals a recurring “problem of unconceived alternatives”:\ntypically, at any given time, there are theories that do not occur to\nscientists but which are just as well confirmed by the available\nevidence as those that are, in fact, accepted; furthermore, over time,\nsuch unconceived theories often supplant the theories adopted by\nhistorical actors as the relevant science develops. (For discussions\nand evaluations of this challenge, see Chakravartty 2008;\nGodfrey-Smith 2008; Magnus 2010; Lyons 2013; Mizrahi 2015:\n139–146; and Egg 2016; cf. Wray 2008 and Khalifa 2010 on the\nrelated notion of “underconsideration”, as described by\nLipton 1993, [1991] 2004: 151–163.) \nOne especially important reaction to concerns about the alleged\nunderdetermination of theory by data gives rise to another leading\nantirealist argument. This reaction is to reject one of the key\npremises of the argument from underdetermination, viz. that\nevidence for belief in a theory is exhausted by the empirical data.\nMany realists contend that other considerations—most\nprominently, explanatory considerations—play an\nevidential role in scientific inference. If this is so, then even if\none were to grant the idea that all theories have empirically\nequivalent rivals, this would not entail underdetermination, for the\nexplanatory superiority of one in particular may determine a choice\n(Laudan 1990; Day & Botterill 2008). This is a specific\nexemplification of a form of reasoning by which “we infer what\nwould, if true, provide the best explanation of [the] evidence”\n(Lipton [1991] 2004: 1). To put a realist-sounding spin on it:  \none infers, from the premise that a given hypothesis would provide a\n“better” explanation for the evidence than would any other\nhypothesis, to the conclusion that the given hypothesis is true.\n(Harman 1965: 89)  \nInference to the best explanation (as per Lipton’s formulation)\nseems ubiquitous in scientific practice. The question of whether it\ncan be expected to yield knowledge of the sort suggested by realism\n(as per Harman’s formulation) is, however, a matter of\ndispute. \nTwo difficulties are immediately apparent regarding the realist\naspiration to infer truth (approximate truth, existence of entities,\netc.) from hypotheses or theories that are judged best on explanatory\ngrounds. The first concerns the grounds themselves. In order to judge\nthat one theory furnishes a better explanation of some phenomenon than\nanother, one must employ some criterion or criteria on the basis of\nwhich the judgment is made. Many have been proposed: simplicity\n(whether of mathematical description or in terms of the number or\nnature of the entities involved); consistency and coherence (both\ninternally, and externally with respect to other theories and\nbackground knowledge); scope and unity (pertaining to the domain of\nphenomena explained); and so on. One challenge here concerns whether\nvirtues such as these can be defined precisely enough to permit\nrelative rankings of explanatory goodness. Another challenge concerns\nthe multiple meanings associated with some virtues (consider, for\nexample, mathematical versus ontological simplicity). Another concerns\nthe possibility that such virtues may not all favor any one theory in\nparticular. Finally, there is the question of whether these virtues\nshould be considered evidential or epistemic, as opposed to merely\npragmatic. What reason is there to think, for instance, that\nsimplicity is an indicator of truth? Thus, the ability to rank\ntheories with respect to their likelihood of being true may be\nquestioned. \nA second difficulty facing inference to the best explanation concerns\nthe pools of theories regarding which judgments of relative\nexplanatory efficacy are made. Even if scientists are likely reliable\nrankers of theories with respect to truth, this will not lead to\nbelief in a true theory (in some domain) unless that theory in\nparticular happens to be among those considered. Otherwise, as van\nFraassen (1989: 143) notes, one may simply end up with “the best\nof a bad lot”. Given the widespread view, even among realists,\nthat many and perhaps most of our best theories are false, strictly\nspeaking, this concern may seem especially pressing. However, in just\nthe way that the realist strategy of selectivity (see\n section 2.3)\n may offer responses to the question of what it could mean for a\ntheory to be close to the truth without being true\nsimpliciter, this same strategy may offer the beginnings of a\nresponse here. That is to say, the best theory of a bad lot may\nnonetheless describe unobservable aspects of the world in such a way\nas to meet the standards of variants of realism including\nexplanationism, entity realism, and structural realism. (For a\nbook-length treatment of inference to the best explanation, see Lipton\n[1991] 2004; for defenses, see Lipton 1993; Day & Kincaid 1994;\nand Psillos 1996, 2009: part III; for critiques, see van Fraassen\n1989: chs. 6–7; Ladyman, Douven, Horsten, & van Fraassen\n1997; Wray 2008; and Khalifa 2010.) \nWorries about underdetermination and inference to the best explanation\nare generally conceptual in nature, but the so-called pessimistic\ninduction (also called the “pessimistic meta-induction”,\nbecause it concerns the “ground level” inductive\ninferences that generate scientific theories and law statements) is\nintended as an argument from empirical premises. If one considers the\nhistory of scientific theories in any given discipline, what one\ntypically finds is a regular turnover of older theories in favor of\nnewer ones, as scientific knowledge develops. From the point of view\nof the present, most past theories must be considered false; indeed,\nthis will be true from the point of view of most times. Therefore, by\nenumerative induction (that is, generalizing from these cases), surely\ntheories at any given time will ultimately be replaced and regarded as\nfalse from some future perspective. Thus, current theories are also\nfalse. The general idea of the pessimistic induction has a rich\npedigree. Though neither endorse the argument, Poincaré ([1905]\n1952: 160), for instance, describes the seeming “bankruptcy of\nscience” given the apparently “ephemeral nature” of\nscientific theories, which one finds “abandoned one after\nanother”, and Putnam (1978: 22–25) describes the challenge\nin terms of the failure of reference of terms for unobservables, with\nthe consequence that theories incorporating them cannot be said to be\ntrue. (For a summary of different formulations, see Wray 2015.) \nContemporary discussion commonly focuses on Laudan’s (1981)\nargument to the effect that the history of science furnishes vast\nevidence of empirically successful theories that were later rejected;\nfrom subsequent perspectives, their unobservable terms were judged not\nto refer and thus, they cannot not be regarded as true or even\napproximately true. (If one prefers to define realism in terms of\nscientific ontology rather than reference and truth, one may rephrase\nthe worry in terms of the mistaken ontologies of past theories from\nlater perspectives.) Responses to this argument generally take one of\ntwo forms, the first stemming from the qualifications to realism\noutlined in\n section 1.3,\n and the second from the forms of realist selectivity outlined in\n section 2.3—both\n can be understood as attempts to restrict the inductive basis of the\nargument in such a way as to foil the pessimistic conclusion. For\nexample, one might contend that if only sufficiently mature and\nnon-ad hoc theories are considered, the number whose central\nterms did not refer and/or that cannot be regarded as approximately\ntrue is dramatically reduced (see references,\n section 1.3).\n Or, the realist might grant that the history of science presents a\nrecord of significant referential discontinuity, but contend that,\nnevertheless, it also presents a record of impressive continuity\nregarding what is properly endorsed by realism, as recommended by\nexplanationists, entity realists, or structural realists (see\nreferences,\n section 2.3).\n (For other responses, see Leplin 1981; McAllister 1993; Chakravartty\n2007a: ch. 2; Doppelt 2007; Nola 2008; Roush 2010, 2015; and Fahrbach\n2011. Hardin & Rosenberg 1982; Cruse & Papineau 2002; and\nPapineau 2010 explore the idea that reference is irrelevant to\napproximate truth). \nIn just the way that some authors suggest that the miracle argument is\nan instance of fallacious reasoning—the base rate fallacy (see\n section 2.1)—some\n suggest that the pessimistic induction is likewise flawed (Lewis\n2001; Lange 2002; Magnus & Callender 2004). The argument is\nanalogous: the putative failure of reference on the part of past\nsuccessful theories, or their putative lack of approximate truth,\ncannot be used to derive a conclusion regarding the chances that our\ncurrent best theories do not refer to unobservables, or that they are\nnot approximately true, unless one knows the base rate of\nnon-referring or non-approximately true theories in the relevant\npools. And since one cannot know this independently, the pessimistic\ninduction is fallacious. Again, analogously, one might argue that to\nformalize the argument in terms of probabilities, as is required in\norder to invoke the base rate fallacy, is to miss the more fundamental\npoint underlying the pessimistic induction (Saatsi 2005b). One might\nread the argument simply as cutting a supposed link between the\nempirical success of scientific theories and successful reference or\napproximate truth, as opposed to relying on an inductive inference\nper se. If even a few examples from the history of science\ndemonstrate that theories can be empirically successful and yet fail\nto refer to the central unobservables they invoke, or fail to be what\nrealists would regard as approximately true, this constitutes a\nprima facie challenge to the notion that only realism can\nexplain the success of science. \nThe regular appeal to the notion of approximate truth by realists has\nseveral motivations. The widespread use of abstraction (that is,\nincorporating some but not all of the relevant parameters into\nscientific descriptions) and idealization (distorting the natures of\ncertain parameters) suggests that even many of our best theories and\nmodels are not strictly correct. The common realist contention that\ntheories can be viewed as gradually converging on the truth as\nscientific inquiry advances suggests that such progress is amenable to\nassessment or measurement in some way, if only in principle. And even\nfor realists who are not convergentists as such, the importance of\ncashing out the metaphor of theories being close to the truth is\npressing in the face of antirealist assertions to the effect that the\nmetaphor is empty. The challenge to make good on the metaphor and\nexplicate, in precise terms, what approximate truth could be, is one\nsource of skepticism about realism. Two broad strategies have emerged\nin response to this challenge: attempts to quantify approximate truth\nby formally defining the concept and the related notion of relative\napproximate truth; and attempts to explicate the concept\ninformally. \nThe formal route was inaugurated by Popper (1972: 231–236), who\ndefined relative orderings of “verisimilitude” (literally,\n“likeness to truth”) between theories in a given domain\nover time by means of a comparison of their true and false\nconsequences. D. Miller (1974) and Tichý (1974) proved that\nthere is a technical problem with this account, however, yielding the\nconsequence that in order for theory A to have greater\nverisimilitude than theory B, A must be true\nsimpliciter, which leaves the realist desideratum of\nexplaining how strictly false theories can differ with respect to\napproximate truth unsatisfied (see also Oddie 1986a). Another formal\naccount is the possible worlds approach (also called the\n“similarity” approach), according to which the truth\nconditions of a theory are identified with the set of possible worlds\nin which it is true, and “truth-likeness” is calculated by\nmeans of a function that measures the average or some other\nmathematical “distance” between the actual world and the\nworlds in that set, thereby facilitating orderings of theories with\nrespect to truth-likeness (Tichý 1976, 1978; Oddie 1986b;\nNiiniluoto 1987, 1998; for critiques, see D. Miller 1976 and Aronson\n1990). One last attempt to formalize approximate truth is the type\nhierarchies approach, which analyzes truth-likeness in terms of\nsimilarity relationships between nodes in tree-structured graphs of\ntypes and subtypes representing scientific concepts on the one hand,\nand the entities in the world they putatively represent on the other\n(Aronson 1990; Aronson, Harré, & Way 1994: 15–49; for\na critique, see Psillos 1999: 270–273). \nLess formally and perhaps more typically, realists have attempted to\nexplicate approximate truth in qualitative terms. One common\nsuggestion is that a theory may be considered more approximately true\nthan one that preceded it if the earlier theory can be described as a\n“limiting case” of the later one. The idea of limiting\ncases and inter-theory relations more generally is elaborated by Post\n(1971; see also French & Kamminga 1993), who argues that certain\nheuristic principles in science yield theories that\n“conserve” the successful parts of their predecessors. His\n“General Correspondence Principle” states that later\ntheories commonly account for the successes of their predecessors by\n“degenerating” into earlier theories in domains in which\nthe earlier ones are well confirmed. Hence, for example, the often\ncited claim that certain equations in relativistic physics degenerate\ninto the corresponding equations in classical physics in the limit, as\nvelocity tends to zero. The realist may then contend that later\ntheories offer more approximately true descriptions of the relevant\nsubject matter, and that the ways in which they do this can be\nilluminated in part by studying the ways in which they build on the\nlimiting cases represented by their predecessors. (For further takes\non approximate truth, see Leplin 1981; Boyd 1990; Weston 1992; Smith\n1998; Chakravartty 2010, and Northcott 2013.) \nThe term “antirealism” (or “anti-realism”)\nencompasses any position that is opposed to realism along one or more\nof the dimensions canvassed in\n section 1.2:\n the metaphysical commitment to the existence of a mind-independent\nreality; the semantic commitment to interpret theories literally or at\nface value; and the epistemological commitment to regard theories as\nfurnishing knowledge of both observables and unobservables. As a\nresult, and as one might expect, there are many different ways to be\nan antirealist, and many different positions qualify as antirealism\n(cf. Kitcher 2001: 161–163). In the historical development of\nrealism, arguably the most important strains of antirealism have been\nvarieties of empiricism which, given their emphasis on experience as a\nsource and subject matter of knowledge, are naturally set against the\nidea of knowledge of unobservables. It is possible to be an empiricist\nmore broadly speaking in a way that is consistent with\nrealism—for example, one might endorse the idea that knowledge\nof the world stems from empirical investigation and contend that on\nthis basis, one can justifiably infer certain things about\nunobservables. In the first half of the twentieth century, however,\nempiricism came predominantly in the form of varieties of\n“instrumentalism”: the view that theories are merely\ninstruments for predicting observable phenomena or systematizing\nobservation reports. \nAccording to the best known, traditional form of instrumentalism,\nterms for unobservables have no meaning all by themselves; construed\nliterally, statements involving them are not even candidates for truth\nor falsity (cf. a more recent proposal in Rowbottom 2011). The most\ninfluential advocates of this view were the logical empiricists (or\nlogical positivists), including Carnap and Hempel, famously associated\nwith the Vienna Circle group of philosophers and scientists as well as\nimportant contributors elsewhere. In order to rationalize the\nubiquitous use of terms which might otherwise be taken to refer to\nunobservables in scientific discourse, they adopted a non-literal\nsemantics according to which these terms acquire meaning by being\nassociated with terms for observables (for example,\n“electron” might mean “white streak in a cloud\nchamber”), or with demonstrable laboratory procedures (a view\ncalled “operationalism”). Insuperable difficulties with\nthis semantics led ultimately (in large measure) to the demise of\nlogical empiricism and the growth of realism. The contrast here is not\nmerely in semantics and epistemology: a number of logical empiricists\nalso held the neo-Kantian view that ontological questions\n“external” to the frameworks for knowledge represented by\ntheories are also meaningless (the choice of a framework is made\nsolely on pragmatic grounds), thereby rejecting the metaphysical\ndimension of realism (as in Carnap 1950). (Duhem [1906] 1954 was\ninfluential with respect to instrumentalism; for a critique of logical\nempiricist semantics, see H. Brown 1977: ch. 3; on logical empiricism\nmore generally, see Giere & Richardson 1997 and Richardson &\nUebel 2007; on the neo-Kantian reading, see Richardson 1998 and\nFriedman 1999.) \nVan Fraassen (1980) reinvented empiricism in the scientific context,\nevading many of the challenges faced by logical empiricism by adopting\na realist semantics. His position, “constructive\nempiricism”, holds that the aim of science is empirical\nadequacy, where “a theory is empirically adequate exactly if\nwhat it says about the observable things and events in the world, is\ntrue” (1980: 12; p. 64 gives a more technical definition in\nterms of the embedding of observable structures in scientific models).\nCrucially, unlike logical empiricism, constructive empiricism\ninterprets theories in precisely the same manner as realism. The\nantirealism of the position is due entirely to its\nepistemology—it recommends belief in our best theories only\ninsofar as they describe observable phenomena, and is satisfied with\nan agnostic attitude regarding anything unobservable. The constructive\nempiricist thus recognizes claims about unobservables as true or\nfalse, but feels no need to believe or disbelieve them. In focusing on\nbelief in the domain of the observable, the position is similar to\ntraditional instrumentalism, and is for this reason sometimes\ndescribed as a form of instrumentalism. (For elaborations of the view,\nsee van Fraassen 1985, 2001 and Rosen 1994.) There are also affinities\nhere with the idea of fictionalism, according to which things in the\nworld are and behave as if our best scientific theories are\ntrue (Vaihinger [1911] 1923; Fine 1993). \nThe collapse of the logical empiricist program was in part facilitated\nby a historical turn in the philosophy of science in the 1960s,\nassociated with authors such as Kuhn, Feyerabend, and Hanson.\nKuhn’s highly influential work, The Structure of Scientific\nRevolutions, played a significant role in establishing a lasting\ninterest in a form of historicism about scientific knowledge,\nparticularly among those interested in the nature of scientific\npractice. An underlying principle of the historical turn was to take\nthe history of science and its practice seriously by furnishing\ndescriptions of scientific knowledge in situ. Kuhn argued\nthat the fruits of such history illuminate a recurring pattern:\nperiods of so-called normal science, often fairly long in duration\n(consider, for example, the periods dominated by classical physics, or\nrelativistic physics), punctuated by revolutions which lead scientific\ncommunities from one period of normal science into another. The\nimplications for realism on this picture derive from Kuhn’s\ncharacterization of knowledge on either side of a revolutionary\ndivide. Two different periods of normal science, he said, are\n“incommensurable” with one another, in such a way as to\nrender the world importantly different after a revolution (the\nphenomenon of “world change”). (Among the many detailed\nstudies of these topics, see Horwich 1993; Hoyningen-Huene 1993;\nSankey 1994; and Bird 2000.) \nThe notion of incommensurability applies to (inter alia) a\ncomparison of theories operative during different periods of normal\nscience. Kuhn held that if two theories are incommensurable, they are\nnot comparable in a way that would permit the judgment that one is\nepistemically superior to the other, because different periods of\nnormal science are characterized by different “paradigms”\n(commitments to symbolic representations of the phenomena,\nmetaphysical beliefs, values, and problem solving techniques). As a\nconsequence, scientists in different periods of normal science\ngenerally employ different methods and standards, experience the world\ndifferently via “theory laden” perceptions, and most\nimportantly for Kuhn (1983), differ with respect to the very meanings\nof their terms. This is a version of meaning holism or contextualism,\naccording to which the meaning of a term or concept is exhausted by\nits connections to others within a paradigm. A change in any part of\nthis network entails a change in meanings throughout—the term\n“mass”, for instance, has different meanings in the\ncontexts of classical physics and relativistic physics. Thus, any\njudgment to the effect that the latter’s characterization of\nmass is closer to the truth, or even that the relevant theories\ndescribe the same property, is importantly confused: it equivocates\nbetween two different concepts which can only be understood in an\nappropriately historicized manner, from the perspectives of the\nparadigms in which they occur. \nThe changes in perception, conceptualization, and language that Kuhn\nassociated with changes in paradigm also fuelled his notion of world\nchange, which further extends the contrast of the historicist approach\nwith realism. There is an important sense, Kuhn maintained, in which\nafter a scientific revolution, scientists live in a different world.\nThis is a famously cryptic remark in Structure ([1962] 1970:\n111, 121, 150), but he (2000: 264) later gives it a neo-Kantian spin:\nparadigms function so as to create the reality of scientific\nphenomena, thereby allowing scientists to engage with this reality. On\nsuch a view, it would seem that not only the meanings but also the\nreferents of terms are constrained by paradigmatic boundaries. And\nthus, reflecting an interesting parallel with neo-Kantian logical\nempiricism, the idea of a paradigm-transcendent world which is\ninvestigated by scientists, and about which one might have knowledge,\nhas no obvious cognitive content. On this picture, empirical reality\nis structured by scientific paradigms, and this conflicts with the\ncommitment of realism to knowledge of a mind-independent world. \nOne outcome of the historical turn in the philosophy of science and\nits emphasis on scientific practice was a focus on the complex social\ninteractions that inevitably surround and infuse the generation of\nscientific knowledge. Relations between experts, their students, and\nthe public, collaboration and competition between individuals and\ninstitutions, and social, economic, and political contexts became the\nsubjects of an approach to studying the sciences known as the\nsociology of scientific knowledge, or SSK. Though in theory, a\ncommitment to studying the sciences from a sociological perspective is\ninterpretable in such a way as to be neutral with respect to realism\n(Lewens 2005; cf. Kochan 2010), in practice, most accounts of science\ninspired by SSK are implicitly or explicitly antirealist. This\nantirealism in practice stems from the common suggestion that once one\nappreciates the role that social factors (using this as a generic term\nfor the sorts of interactions and contexts indicated above) play in\nthe production of scientific knowledge, a philosophical commitment to\nsome form of “social constructivism” is inescapable, and\nthis latter commitment is inconsistent with various aspects of\nrealism. \nThe term “social construction” refers to any\nknowledge-generating process in which what counts as a fact is\nsubstantively determined by social factors, and in which different\nsocial factors would likely generate facts that are inconsistent with\nwhat is actually produced. The important implication here is thus a\ncounterfactual claim about the dependence of facts on social factors.\nThere are numerous ways in which social determinants of facthood may\nbe consistent with realism. For example, social factors might\ndetermine the directions and methodologies of research that are\npermitted, encouraged, and funded, but this by itself need not\nundermine a realist attitude with respect to the outputs of scientific\nwork. Often, however, work in SSK takes the form of case studies that\naim to demonstrate how particular decisions affecting scientific work\nwere (or are) influenced by social factors which, had they been\ndifferent, would have facilitated results that are inconsistent with\nthose ultimately accepted as scientific fact. Some, including\nproponents of the so-called Strong Program in SSK, argue that for more\ngeneral, principled reasons, such factual contingency is inevitable.\n(For a sample of influential approaches to social constructivism, see\nLatour & Woolgar [1979] 1986; Knorr-Cetina 1981; Pickering 1984;\nShapin & Schaffer 1985; and Collins & Pinch 1993; on the\nStrong Program, see Barnes, Bloor, & Henry 1996; for a historical\nstudy of the transition from Kuhn to SSK and social constructivism,\nsee Zammito 2004: chs. 5–7.) \nBy making social factors an inextricable, substantive determinant of\nwhat counts as true or false in the realm of the sciences (and\nelsewhere), social constructivism stands opposed to the realist\ncontention that theories can be understood as furnishing knowledge of\na mind-independent world. And as in the historicist approach, notions\nsuch as truth, reference, and ontology are here relative to particular\ncontexts; they have no context-transcendent significance. The later\nwork of Kuhn and Wittgenstein in particular were influential in the\ndevelopment of the Strong Program doctrine of “meaning\nfinitism”, according to which the meanings of terms are\nconceived as social institutions: the various ways in which they are\nused successfully in communication within a linguistic community. This\ntheory of meaning forms the basis of an argument to the effect that\nthe meanings of scientific (and other) terms are products of social\nnegotiation and need not be fixed or determinate, which further\nconflicts with a number of realist notions, including the idea of\nconvergence toward true theories, improvements with respect to\nontology or approximate truth, and determinate reference to\nmind-independent entities. The subject of neo-Kantianism thus emerges\nhere again, though its strength in constructivist doctrines varies\nsignificantly. (For a robustly finitist view, see Kusch 2002; for a\nmore moderate constructivism, see Putnam’s (1981: ch. 3)\n“internal realism” and cf. Ellis 1988). \nFeminist engagements with science are linked thematically to SSK and\nforms of social constructivism by their recognition of the role of\nsocial factors as determinants of scientific fact. That said, they\nextend the analysis in a more specific way, reflecting particular\nconcerns about the marginalization of points of view based on gender,\nethnicity, socio-economic status, and political status. Not all\nfeminist approaches are antirealist, but nearly all are normative,\noffering prescriptions for revising both scientific practice and\nconcepts such as objectivity and knowledge that have direct\nimplications for realism. In this regard it is useful to distinguish\n(as originally proposed in Harding 1986) between three broad\napproaches. Feminist empiricism focuses on the possibility of\nwarranted belief within scientific communities as a function of the\ntransparency and consideration of biases associated with different\npoints of view which enter into scientific work. Standpoint theory\ninvestigates the idea that scientific knowledge is inextricably linked\nto perspectives arising from differences in such points of view.\nFeminist postmodernism rejects traditional conceptions of universal or\nabsolute objectivity and truth. (As one might expect, these views are\nnot always neatly distinguishable; for some early, influential\napproaches, see Keller 1985; Harding 1986; Haraway 1988; Longino 1990,\n2002; Alcoff & Potter 1993; and Nelson & Nelson 1996). \nThe notion of objectivity has a number of traditional\nconnotations—including disinterest (detachment, lack of bias)\nand universality (independence from any particular perspective or\nviewpoint)—which are commonly associated with knowledge of a\nmind-independent world. Feminist critiques are almost unanimous in\nrejecting scientific objectivity in the sense of disinterest, offering\ncase studies that aim to demonstrate how the presence of (for\nexample) androcentric bias in a scientific community can lead to the\nacceptance of one theory at the expense of alternatives (Kourany 2010:\nchs. 1–3; for detailed cases, see Longino 1990: ch. 6 and Lloyd\n2006). Arguably, the failure of objectivity in this sense is\nconsistent with realism under certain conditions. For example, if the\nrelevant bias is epistemically neutral (that is, if one’s\nassessment of scientific evidence is not influenced by it one way or\nanother), then realism may remain at least one viable interpretation\nof the outputs of scientific work. In the more interesting case where\nbias is epistemically consequential, the prospects for realism are\ndiminished, but may be enhanced by a scientific infrastructure that\nfunctions to bring it under scrutiny (by means of, for example,\neffective peer review, genuine consideration of minority views, etc.),\nthus facilitating corrective measures where appropriate. The\ncontention that the sciences do not generally exemplify such an\ninfrastructure is one motivation for the normativity of much feminist\nempiricism. \nThe challenge to objectivity in the sense of universality or\nperspective-independence can be, in some cases, more difficult to\nsquare with the possibility of realism. In a Marxist vein, some\nstandpoint theorists argue that certain perspectives are epistemically\nprivileged in the realm of science: viz., subjugated\nperspectives are epistemically privileged in comparison to dominant\nones in light of the deeper insight afforded the former (just as the\nproletariat has a deeper knowledge of human potential than the\nsuperficial knowledge typical of those in power). Others portray\nepistemic privilege in a more splintered or deflationary manner,\nsuggesting that no one point of view can be established as superior to\nanother by any overarching standard of epistemological assessment.\nThis view is most explicit in feminist postmodernism, which embraces a\nthoroughgoing relativism with respect to truth (and presumably\napproximate truth, scientific ontology, and other notions central to\nvarious descriptions of realism). As in the case of Strong Program\nSSK, truth and epistemic standards are here defined only within the\ncontext of a perspective, and thus cannot be interpreted in any\ncontext-transcendent or mind-independent manner. \nIt is not uncommon to hear philosophers remark that the dialogue\nbetween the forms of realism and antirealism surveyed in this article\nshows every symptom of a perennial philosophical dispute. The issues\ncontested range so broadly and elicit so many competing intuitions\n(about which, arguably, reasonable people may disagree) that some\nquestion whether a resolution is even possible. This prognosis of\npotentially irresolvable dialectical complexity is relevant to a\nnumber of further views in the philosophy of science, some of which\narise as direct responses to it. For example, Fine ([1986b] 1996: chs.\n7–8) argues that ultimately, neither realism nor antirealism is\ntenable, and recommends what he calls the “natural ontological\nattitude” (NOA) instead (see Rouse 1988, 1991 for detailed\nexplorations of the view). NOA is intended to comprise a neutral,\ncommon core of realist and antirealist attitudes of acceptance of our\nbest theories. The mistake that both parties make, Fine suggests, is\nto add further epistemological and metaphysical diagnoses to this\nshared position, such as pronouncements about which aspects of\nscientific ontology should be viewed as real, which are proper\nsubjects of belief, and so on. Others contend that this sort of\napproach to scientific knowledge is non- or anti-philosophical, and\ndefend philosophical engagement in debates about realism (Crasnow\n2000, Mcarthur 2006). Musgrave (1989) argues that the view is either\nempty or collapses into realism. \nThe idea of putting the conflict between realist and antirealist\napproaches to science aside is also a recurring theme in some accounts\nof pragmatism, and quietism. Regarding the first, Peirce ([1992] 1998,\nin “How to Make Our Ideas Clear”, for instance, originally\npublished in 1878) holds that the content of a proposition should be\nunderstood in terms of (among other things) its “practical\nconsequences” for human experience, such as implications for\nobservation or problem-solving. For James ([1907] 1979), positive\nutility measured in these terms is the very marker of truth (where\ntruth is whatever will be agreed in the ideal limit of scientific\ninquiry). Many of the points disputed by realists and\nantirealists—differences in epistemic commitment to scientific\nentities based on observability, for example—are effectively\nnon-issues on this view (Almeder 2007; Misak 2010). It is nevertheless\na form of antirealism on traditional readings of Peirce and James,\nsince both suggest that truth in the pragmatist sense exhausts our\nconception of reality, thus running foul of the metaphysical dimension\nof realism. The notion of quietism is often associated with\nWittgenstein’s response to philosophical problems about which,\nhe maintained, nothing sensible can be said. This is not to say that\nengaging with such a problem is not to one’s taste, but rather\nthat quite independently of one’s interest or lack thereof, the\ndispute itself concerns a pseudo-problem. Blackburn (2002) suggests\nthat disputes about realism may have this character. \nOne last take on the putative irresolvability of debates concerning\nrealism focuses on certain meta-philosophical commitments adopted by\nthe interlocutors. Wylie (1986: 287), for instance, claims that  \nthe most sophisticated positions on either side now incorporate\nself-justifying conceptions of the aim of philosophy and of the\nstandards of adequacy appropriate for judging philosophical theories\nof science.  \nDifferent assumptions ab initio regarding what sorts of\ninferences are legitimate, what sorts of evidence reasonably support\nbelief, whether there is a genuine demand for the explanation of\nobservable phenomena in terms of underlying realities, and so on, may\nrender some arguments between realists and antirealists\nquestion-begging. This diagnosis is arguably facilitated by van\nFraassen’s (1989: 170–176, 1994: 182) intimation that\nneither realism nor antirealism (in his case, empiricism) is ruled out\nby plausible canons of rationality; each is sustained by a different\nconception of how much epistemic risk one should take in forming\nbeliefs on the basis of one’s evidence. An intriguing question\nthen emerges as to whether disputes surrounding realism and\nantirealism are resolvable in principle, or whether, ultimately,\ninternally consistent and coherent formulations of these positions\nshould be regarded as irreconcilable but nonetheless permissible\ninterpretations of scientific knowledge (Chakravartty 2017; Forbes forthcoming).","contact.mail":"chakravartty@miami.edu","contact.domain":"miami.edu"}]
