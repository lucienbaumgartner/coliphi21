[{"date.published":"2019-05-28","url":"https://plato.stanford.edu/entries/language-thought/","author1":"Michael Rescorla","entry":"language-thought","body.text":"\n\n\nThe language of thought hypothesis (LOTH) proposes that\nthinking occurs in a mental language. Often called Mentalese,\nthe mental language resembles spoken language in several key respects:\nit contains words that can combine into sentences; the words and\nsentences are meaningful; and each sentence’s meaning depends in\na systematic way upon the meanings of its component words and the way\nthose words are combined. For example, there is a Mentalese\nword whale that denotes whales, and there is a\nMentalese word mammal that denotes\nmammals. These words can combine into a Mentalese\nsentence whales are mammals, which means that\nwhales are mammals. To believe that whales are mammals is to bear an\nappropriate psychological relation to this sentence. During a\nprototypical deductive inference, I might transform the Mentalese\nsentence whales are mammals and the Mentalese\nsentence Moby Dick is a whale into the\nMentalese sentence Moby Dick is a mammal. As I\nexecute the inference, I enter into a succession of mental states that\ninstantiate those sentences.\n\n\nLOTH emerged gradually through the writings of Augustine, Boethius,\nThomas Aquinas, John Duns Scotus, and many others. William of Ockham\noffered the first systematic treatment in his Summa Logicae\n(c. 1323), which\nmeticulously analyzed the meaning and structure of Mentalese\nexpressions. LOTH was quite popular during the late medieval era, but\nit slipped from view in the sixteenth and seventeenth centuries. From\nthat point through the mid-twentieth century, it played little serious\nrole within theorizing about the mind.\n\n\nIn the 1970s, LOTH underwent a dramatic revival. The watershed was\npublication of Jerry Fodor’s The Language of Thought\n(1975). Fodor argued abductively: our current best scientific theories\nof psychological activity postulate Mentalese; we therefore have good\nreason to accept that Mentalese exists. Fodor’s analysis exerted\ntremendous impact. LOTH once again became a focus of discussion, some\nsupportive and some critical. Debates over the existence and nature of\nMentalese continue to figure prominently within philosophy and\ncognitive science. These debates have pivotal importance for our\nunderstanding of how the mind works.\n\n\n\n\nWhat does it mean to posit a mental language? Or to say that thinking\noccurs in this language? Just how “language-like” is\nMentalese supposed to be? To address these questions, we will isolate\nsome core commitments that are widely shared among LOT theorists. \nFolk psychology routinely explains and predicts behavior by citing\nmental states, including beliefs, desires, intentions, fears, hopes,\nand so on. To explain why Mary walked to the refrigerator, we might\nnote that she believed there was orange juice in the refrigerator and\nwanted to drink orange juice. Mental states such as belief and desire\nare called propositional attitudes. They can be specified\nusing locutions of the form \nX believes that p. \nX desires that p. \nX intends that p. \nX fears that p. \netc. \nBy replacing “p” with a sentence, we specify the\ncontent of X’s mental state. Propositional attitudes have\n intentionality\n or aboutness: they are about a subject matter.\nFor that reason, they are often called intentional\nstates. \nThe term “propositional attitude” originates with Russell\n(1918–1919 [1985]) and reflects his own preferred analysis: that\npropositional attitudes are relations to\n  propositions. \n A proposition is an abstract entity that determines\na truth-condition. To illustrate, suppose John believes that\nParis is north of London. Then John’s belief is a relation to\nthe proposition that Paris is north of London, and this\nproposition is true iff Paris is north of London. Beyond the thesis\nthat propositions determine truth-conditions, there is little\nagreement about what propositions are like. The literature offers many\noptions, mainly derived from theories of Frege (1892 [1997]), Russell\n(1918–1919 [1985]), and Wittgenstein (1921 [1922]). \nFodor (1981: 177–203; 1987: 16–26) proposes a theory of\npropositional attitudes that assigns a central role to mental\nrepresentations. A \n  mental representation\n is a mental item with\nsemantic properties (such as a denotation, or a meaning, or a\ntruth-condition, etc.). To believe that p, or hope that\np, or intend that p, is to bear an appropriate relation\nto a mental representation whose meaning is that p. For\nexample, there is a relation belief* between thinkers and mental\nrepresentations, where the following biconditional is true no matter\nwhat English sentence one substitutes for “p”: \nX believes that p iff there is a mental representation\nS such that X believes* S and S means that\np. \nMore generally: \nOn this analysis, mental representations are the most direct objects\nof propositional attitudes. A propositional attitude inherits its\nsemantic properties, including its truth-condition, from the mental\nrepresentation that is its object. \nProponents of (1) typically invoke \n functionalism to analyze A*.\nEach psychological relation A* is associated with a distinctive\nfunctional role: a role that S plays within your\nmental activity just in case you bear A* to S. When\nspecifying what it is to believe* S, for example, we might\nmention how S serves as a basis for inferential reasoning, how\nit interacts with desires to produce actions, and so on. Precise\nfunctional roles are to be discovered by scientific psychology.\nFollowing Schiffer (1981), it is common to use the term\n“belief-box” as a placeholder for the functional role\ncorresponding to belief*: to believe* S is to place S in\nyour belief box. Similarly for “desire-box”, etc. \n(1) is compatible with the view that propositional attitudes are\nrelations to propositions. One might analyze the locution\n“S means that p” as involving a relation\nbetween S and a proposition expressed by S. It would\nthen follow that someone who believes* S stands in a\npsychologically important relation to the proposition expressed by\nS. Fodor (1987: 17) adopts this approach. He combines a\ncommitment to mental representations with a commitment to\npropositions. In contrast, Field (2001: 30–82) declines to\npostulate propositions when analyzing “S means that\np”. He posits mental representations with semantic\nproperties, but he does not posit propositions expressed by the mental\nrepresentations. \nThe distinction between \n  types and tokens is crucial for understanding\n(1). A mental representation is a repeatable type that can be\ninstantiated on different occasions. In the current literature, it is\ngenerally assumed that a mental representation’s tokens are\nneurological. For present purposes, the key point is that mental\nrepresentations are instantiated by mental events. Here we\nconstrue the category of \n events broadly so as to include both\noccurrences (e.g., I form an intention to drink orange juice)\nand enduring states (e.g., my longstanding belief that\nAbraham Lincoln was president of the United States). When mental event\ne instantiates representation S, we say that S is\ntokened and that e is a tokening of S.\nFor example, if I believe that whales are mammals, then my belief (a\nmental event) is a tokening of a mental representation whose meaning\nis that whales are mammals. \nAccording to Fodor (1987: 17), thinking consists in chains of mental\nevents that instantiate mental representations: \nA paradigm example is deductive inference: I transition from\nbelieving* the premises to believing* the conclusion. The first mental\nevent (my belief* in the premises) causes the second (my belief* in\nthe conclusion). \n(1) and (2) fit together naturally as a package that one might call\nthe representational theory of thought (RTT). RTT postulates\nmental representations that serve as the objects of propositional\nattitudes and that constitute the domain of thought\n processes.[1] \nRTT as stated requires qualification. There is a clear sense in which\nyou believe that there are no elephants on Jupiter. However, you\nprobably never considered the question until now. It is not plausible\nthat your belief box previously contained a mental representation with\nthe meaning that there are no elephants on Jupiter. Fodor (1987:\n20–26) responds to this sort of example by restricting (1) to\ncore cases. Core cases are those where the propositional\nattitude figures as a causally efficacious episode in a mental\nprocess. Your tacit belief that there are no elephants on Jupiter does\nnot figure in your reasoning or decision-making, although it can come\nto do so if the question becomes salient and you consciously judge\nthat there are no elephants on Jupiter. So long as the belief remains\ntacit, (1) need not apply. In general, Fodor says, an intentional\nmental state that is causally efficacious must involve explicit\ntokening of an appropriate mental representation. In a slogan:\n“No Intentional Causation without Explicit Representation”\n(Fodor 1987: 25). Thus, we should not construe (1) as an attempt at\nfaithfully analyzing informal discourse about propositional attitudes.\nFodor does not seek to replicate folk psychological categories. He\naims to identify mental states that resemble the propositional\nattitudes adduced within folk psychology, that play roughly similar\nroles in mental activity, and that can support systematic\ntheorizing. \nDennett’s (1977 [1981]) review of The Language of\nThought raises a widely cited objection to RTT: \nIn a recent conversation with the designer of a chess-playing program\nI heard the following criticism of a rival program: “it thinks\nit should get its queen out early”. This ascribes a\npropositional attitude to the program in a very useful and predictive\nway, for as the designer went on to say, one can usefully count on\nchasing that queen around the board. But for all the many levels of\nexplicit representation to be found in that program, nowhere is\nanything roughly synonymous with “I should get my queen out\nearly” explicitly tokened. The level of analysis to which the\ndesigner’s remark belongs describes features of the program that\nare, in an entirely innocent way, emergent properties of the\ncomputational processes that have “engineering reality”. I\nsee no reason to believe that the relation between belief-talk and\npsychological talk will be any more direct. \nIn Dennett’s example, the chess-playing machine does not\nexplicitly represent that it should get the queen out early, yet in\nsome sense it acts upon a belief that it should do so. Analogous\nexamples arise for human cognition. For example, we often follow rules\nof deductive inference without explicitly representing the rules. \nTo assess Dennett’s objection, we must distinguish sharply\nbetween mental representations and rules governing the manipulation of\nmental representations (Fodor 1987: 25). RTT does not require that\nevery such rule be explicitly represented. Some rules may be\nexplicitly represented—we can imagine a reasoning system that\nexplicitly represents deductive inference rules to which it conforms.\nBut the rules need not be explicitly represented. They may\nmerely be implicit in the system’s operations. Only when\nconsultation of a rule figures as a causally efficacious episode in\nmental activity does RTT require that the rule be explicitly\nrepresented. Dennett’s chess machine explicitly represents chess\nboard configurations and perhaps some rules for manipulating chess\npieces. It never consults any rule akin to Get the Queen out\nearly. For that reason, we should not expect that the machine\nexplicitly represents this rule even if the rule is in some sense\nbuilt into the machine’s programming. Similarly, typical\nthinkers do not consult inference rules when engaging in deductive\ninference. So RTT does not demand that a typical thinker explicitly\nrepresent inference rules, even if she conforms to them and in some\nsense tacitly believes that she should conform to them. \nNatural language is \n compositional: complex \nlinguistic expressions are\nbuilt from simpler linguistic expressions, and the meaning of a\ncomplex expression is a function of the meanings of its constituents\ntogether with the way those constituents are combined.\nCompositional semantics describes in a systematic way how\nsemantic properties of a complex expression depend upon semantic\nproperties of its constituents and the way those constituents are\ncombined. For example, the truth-condition of a conjunction is\ndetermined as follows: the conjunction is true iff both conjuncts are\ntrue. \nHistorical and contemporary LOT theorists universally agree that\nMentalese is compositional: \nCompositionality of mental representations\n(COMP): Mental representations have a compositional\nsemantics: complex representations are composed of simple\nconstituents, and the meaning of a complex representation depends upon\nthe meanings of its constituents together with the constituency\nstructure into which those constituents are arranged. \nClearly, mental language and natural language must differ in many\nimportant respects. For example, Mentalese surely does not have a\nphonology. It may not have a morphology either. Nevertheless, COMP\narticulates a fundamental point of similarity. Just like natural\nlanguage, Mentalese contains complex symbols amenable to semantic\nanalysis. \nWhat is it for one representation to be a “constituent” of\nanother? According to Fodor (2008: 108), “constituent structure\nis a species of the part/whole relation”. Not all parts\nof a linguistic expression are constituents: “John ran” is\na constituent of “John ran and Mary jumped”, but\n“ran and Mary” is not a constituent because it is not\nsemantically interpretable. The important point for our purposes is\nthat all constituents are parts. When a complex representation is\ntokened, so are its parts. For example,  \nintending that \\(P \\amp Q\\) requires having a sentence in your\nintention box… one of whose parts is a token of the very same\ntype that’s in the intention box when you intend that \\(P\\), and\nanother of whose parts is a token of the very same type that’s\nin the intention box when you intend that \\(Q\\). (Fodor 1987: 139)\n \nMore generally: mental event \\(e\\) instantiates a complex mental\nrepresentation only if \\(e\\) instantiates all of the\nrepresentation’s constituent parts. In that sense, \\(e\\) itself\nhas internal complexity. \nThe complexity of mental events figures crucially here, as highlighted\nby Fodor in the following passage (1987: 136): \nPractically everybody thinks that the objects of intentional states\nare in some way complex… [For example], what you believe when\nyou believe that \\(P \\amp Q\\) is… something composite, whose\nelements are—as it might be—the proposition that P\nand the proposition that Q. But the (putative) complexity of\nthe intentional object of a mental state does not, of course,\nentail the complexity of the mental state itself… LOT claims\nthat mental states—and not just their propositional\nobjects—typically have constituent structure. \nMany philosophers, including Frege and Russell, regard propositions as\nstructured entities. These philosophers apply a part/whole model to\npropositions but not necessarily to mental events during which\nthinkers entertain propositions. LOTH as developed by Fodor applies\nthe part/whole model to the mental events themselves:  \nwhat’s at issue here is the complexity of mental events and not\nmerely the complexity of the propositions that are their intentional\nobjects. (Fodor 1987: 142)  \nOn this approach, a key element of LOTH is the thesis that mental\nevents have semantically relevant complexity. \nContemporary proponents of LOTH endorse RTT+COMP. Historical\nproponents also believed something in the vicinity (Normore 1990,\n2009; Panaccio 1999 [2017]), although of course they did not use\nmodern terminology to formulate their views. We may regard RTT+COMP as\na minimalist formulation of LOTH, bearing in mind that many\nphilosophers have used the phrase “language of thought\nhypothesis” to denote one of the stronger theses discussed\nbelow. As befits a minimalist formulation, RTT+COMP leaves unresolved\nnumerous questions about the nature, structure, and psychological role\nof Mentalese expressions. \nIn practice, LOT theorists usually adopt a more specific view of the\ncompositional semantics for Mentalese. They claim that Mentalese\nexpressions have \n  logical form (Fodor 2008: 21). More\nspecifically, they claim that Mentalese contains analogues to the\nfamiliar logical connectives (and, or, not,\nif-then, some, all, the).\nIterative application of logical connectives generates complex\nexpressions from simpler expressions. The meaning of a logically\ncomplex expression depends upon the meanings of its parts and upon its\nlogical structure. Thus, LOT theorists usually endorse a doctrine\nalong the following lines: \nLogically structured mental representations\n(LOGIC): Some mental representations have logical\nstructure. The compositional semantics for these mental\nrepresentations resembles the compositional semantics for logically\nstructured natural language expressions. \nMedieval LOT theorists used syllogistic and propositional logic to\nanalyze the semantics of Mentalese (King 2005; Normore 1990).\nContemporary proponents instead use the predicate calculus,\nwhich was discovered by Frege (1879 [1967]) and whose semantics was\nfirst systematically articulated by Tarski (1933 [1983]). The view is\nthat Mentalese contains primitive words—including predicates,\nsingular terms, and logical connectives—and that these words\ncombine to form complex sentences governed by something like the\nsemantics of the predicate calculus. \nThe notion of a Mentalese word corresponds roughly to the\nintuitive notion of a concept. In fact, Fodor (1998: 70)\nconstrues a concept as a Mentalese word together with its denotation.\nFor example, a thinker has the concept of a cat only if she has in her\nrepertoire a Mentalese word that denotes cats. \nLogical structure is just one possible paradigm for the structure of\nmental representations. Human society employs a wide range of\nnon-sentential representations, including pictures, maps, diagrams,\nand graphs. Non-sentential representations typically contain parts\narranged into a compositionally significant structure. In many cases,\nit is not obvious that the resulting complex representations have\nlogical structure. For example, maps do not seem to contain logical\nconnectives (Fodor 1991: 295; Millikan 1993: 302; Pylyshyn 2003: 424–5). Nor is\nit evident that they contain predicates (Camp 2018; Rescorla 2009c),\nalthough some philosophers contend that they do (Blumson 2012; Casati\n& Varzi 1999; Kulvicki 2015). \nTheorists often posit mental representations that conform to COMP but\nthat lack logical structure. The British empiricists postulated\nideas, which they characterized in broadly imagistic terms.\nThey emphasized that simple ideas can combine to form complex ideas.\nThey held that the representational import of a complex idea depends\nupon the representational import of its parts and the way those parts\nare combined. So they accepted COMP or something close to it\n(depending on what exactly “constituency” amounts\n to).[2]\n They did not say in much detail how compounding of ideas was supposed\nto work, but imagistic structure seems to be the paradigm in at least\nsome passages. LOGIC plays no significant role in their\n writings.[3]\n Partly inspired by the British empiricists, Prinz (2002) and Barsalou\n(1999) analyze cognition in terms of image-like representations\nderived from perception. Armstrong (1973) and Braddon-Mitchell and\nJackson (2007) propose that propositional attitudes are relations not\nto mental sentences but to mental maps analogous in important\nrespects to ordinary concrete maps. \nOne problem facing imagistic and cartographic theories of thought is\nthat propositional attitudes are often logically complex (e.g., John\nbelieves that if Plácido Domingo does not sing then either\nGustavo Dudamel will conduct or the concert will be cancelled).\nImages and maps do not seem to support logical operations: the\nnegation of a map is not a map; the disjunction of two maps is not a\nmap; similarly for other logical operations; and similarly for images.\nGiven that images and maps do not support logical operations, theories\nthat analyze thought in exclusively imagistic or cartographic terms\nwill struggle to explain logically complex propositional\n attitudes.[4] \nThere is room here for a pluralist position that allows mental\nrepresentations of different kinds: some with logical structure, some\nmore analogous to pictures, or maps, or diagrams, and so on. The\npluralist position is widespread within cognitive science, which\nposits a range of formats for mental representation (Block 1983; Camp 2009; Johnson-Laird\n2004: 187; Kosslyn 1980; McDermott 2001: 69; Pinker 2005: 7; Sloman 1978:\n144–76). Fodor himself (1975: 184–195) suggests a view on\nwhich imagistic mental representations co-exist alongside, and\ninteract with, logically structured Mentalese expressions. \nGiven the prominent role played by logical structure within historical\nand contemporary discussion of Mentalese, one might take LOGIC to be\ndefinitive of LOTH. One might insist that mental representations\ncomprise a mental language only if they have logical\nstructure. We need not evaluate the merits of this terminological\nchoice. \nRTT concerns propositional attitudes and the mental processes in which\nthey figure, such as deductive inference, reasoning, decision-making,\nand planning. It does not address perception, motor control,\nimagination, dreaming, pattern recognition, linguistic processing, or\nany other mental activity distinct from high-level cognition. Hence\nthe emphasis upon a language of thought: a system of mental\nrepresentations that underlie thinking, as opposed to perceiving,\nimagining, etc. Nevertheless, talk about a mental language generalizes\nnaturally from high-level cognition to other mental phenomena. \nPerception is a good example. The perceptual system\ntransforms proximal sensory stimulations (e.g., retinal stimulations)\ninto perceptual estimates of environmental conditions (e.g., estimates\nof shapes, sizes, colors, locations, etc.). Helmholtz (1867 [1925])\nproposed that the transition from proximal sensory input to perceptual\nestimates features an unconscious inference, similar in key\nrespects to high-level conscious inference yet inaccessible to\nconsciousness. Helmholtz’s proposal is foundational to\ncontemporary perceptual psychology, which constructs detailed\nmathematical models of unconscious perceptual inference (Knill &\nRichards 1996; Rescorla 2015). Fodor (1975: 44–55) argues that\nthis scientific research program presupposes mental representations.\nThe representations participate in unconscious inferences or\ninference-like transitions executed by the perceptual\n system.[5] \nNavigation is another good example. Tolman (1948)\nhypothesized that rats navigate using cognitive maps: mental\nrepresentations that represent the layout of the spatial environment.\nThe cognitive map hypothesis, advanced during the heyday of\nbehaviorism, initially encountered great scorn. It remained a fringe\nposition well into the 1970s, long after the demise of behaviorism.\nEventually, mounting behavioral and neurophysiological evidence won it\nmany converts (Gallistel 1990; Gallistel & Matzel 2013; Jacobs\n& Menzel 2014; O’Keefe & Nadel 1978; Weiner et al.\n2011). Although a few researchers remain skeptical (Mackintosh 20002),\nthere is now a broad consensus that mammals (and possibly even some\ninsects) navigate using mental representations of spatial layout.\nRescorla (2017b) summarizes the case for cognitive maps and reviews\nsome of their core properties. \nTo what extent should we expect perceptual representations and\ncognitive maps to resemble the mental representations that figure in\nhigh-level human thought? It is generally agreed that all these mental\nrepresentations have compositional structure. For example, the\nperceptual system can bind together a representation of shape and a\nrepresentation of size to form a complex representation that an object\nhas a certain shape and size; the representational import of the\ncomplex representation depends in a systematic way upon the\nrepresentational import of the component representations. On the other\nhand, it is not clear that perceptual representations have anything\nresembling logical structure, including even predicative\nstructure (Burge 2010: 540–544; Fodor 2008: 169–195). Nor\nis it evident that cognitive maps contain logical connectives or\npredicates (Rescorla 2009a, 2009b). Perceptual processing and\nnon-human navigation certainly do not seem to instantiate mental\nprocesses that would exploit putative logical structure. In\nparticular, they do not seem to instantiate deductive inference. \nThese observations provide ammunition for pluralism about\nrepresentational format. Pluralists can posit one system of\ncompositionally structured mental representations for perception,\nanother for navigation, another for high-level cognition, and so on.\nDifferent representational systems potentially feature different\ncompositional mechanisms. As indicated in\n section 1.3,\n pluralism figures prominently in contemporary cognitive science.\nPluralists face some pressing questions. Which compositional\nmechanisms figure in which psychological domains? Which\nrepresentational formats support which mental operations? How do\ndifferent representational formats interface with each other? Further\nresearch bridging philosophy and cognitive science is needed to\naddress such questions. \nModern proponents of LOTH typically endorse the \n computational theory of mind\n (CTM), which claims that the mind is a computational system.\nSome authors use the phrase “language of thought\nhypothesis” so that it definitionally includes CTM as one\ncomponent. \nIn a seminal contribution, Turing (1936) introduced what is now called\nthe \n  Turing machine: an abstract model\n of an idealized computing\ndevice. A Turing machine contains a central processor, governed by\nprecise mechanical rules, that manipulates symbols inscribed along a\nlinear array of memory locations. Impressed by the enormous power of\nthe Turing machine formalism, many researchers seek to construct\ncomputational models of core mental processes, including reasoning,\ndecision-making, and problem solving. This enterprise bifurcates into\ntwo main branches. The first branch is artificial\nintelligence (AI), which aims to build “thinking\nmachines”. Here the goal is primarily an engineering\none—to build a system that instantiates or at least simulates\nthought—without any pretense at capturing how the human mind\nworks. The second branch, computational psychology, aims to\nconstruct computational models of human mental activity. AI and\ncomputational psychology both emerged in the 1960s as crucial elements\nin the new interdisciplinary initiative \n cognitive science, which\nstudies the mind by drawing upon psychology, computer science\n(especially AI), linguistics, philosophy, economics (especially game\ntheory and behavioral economics), anthropology, and neuroscience. \nFrom the 1960s to the early 1980s, computational models offered within\npsychology were mainly Turing-style models. These models embody a\nviewpoint known as the classical computational theory of mind\n(CCTM). According to CCTM, the mind is a computational system similar\nin important respects to a Turing machine, and certain core mental\nprocesses are computations similar in important respects to\ncomputations executed by a Turing machine. \nCCTM fits together nicely with RTT+COMP. Turing-style computation\noperates over symbols, so any Turing-style mental computations must\noperate over mental symbols. The essence of RTT+COMP is postulation of\nmental symbols. Fodor (1975, 1981) advocates RTT+COMP+CCTM. He holds\nthat certain core mental processes are Turing-style computations over\nMentalese expressions. \nOne can endorse RTT+COMP without endorsing CCTM. By positing a system\nof compositionally structured mental representations, one does not\ncommit oneself to saying that operations over the representations are\ncomputational. Historical LOT theorists could not even\nformulate CCTM, for the simple reason that the Turing formalism had\nnot been discovered. In the modern era, Harman (1973) and Sellars\n(1975) endorse something like RTT+COMP but not CCTM. Horgan and\nTienson (1996) endorse RTT+COMP+CTM but not CCTM, i.e.,\nclassical CTM. They favor a version of CTM grounded in\n  connectionism, an alternative\n computational framework that differs\nquite significantly from Turing’s approach. Thus, proponents of\nRTT+COMP need not accept that mental activity instantiates\nTuring-style computation. \nFodor (1981) combines RTT+COMP+CCTM with a view that one might call\nthe formal-syntactic conception of computation (FSC).\nAccording to FSC, computation manipulates symbols in virtue of their\nformal syntactic properties but not their semantic properties. \nFSC draws inspiration from modern logic, which emphasizes the\nformalization of deductive reasoning. To formalize, we\nspecify a formal language whose component linguistic\nexpressions are individuated non-semantically (e.g., by their\ngeometric shapes). We describe the expressions as pieces of formal\nsyntax, without considering what if anything the expressions mean. We\nthen specify inference rules in syntactic, non-semantic\nterms. Well-chosen inference rules will carry true premises to true\nconclusions. By combining formalization with Turing-style computation,\nwe can build a physical machine that manipulates symbols based solely\non the formal syntax of the symbols. If we program the machine to\nimplement appropriate inference rules, then its syntactic\nmanipulations will transform true premises into true conclusions. \nCCTM+FSC says that the mind is a formal syntactic computing system:\nmental activity consists in computation over symbols with formal\nsyntactic properties; computational transitions are sensitive to the\nsymbols’ formal syntactic properties but not their semantic\nproperties. The key term “sensitive” is rather imprecise,\nallowing some latitude as to the precise import of CCTM+FSC.\nIntuitively, the picture is that a mental symbol’s formal syntax\nrather than its semantics determines how mental computation\nmanipulates it. The mind is a “syntactic engine”. \nFodor (1987: 18–20) argues that CCTM+FSC helps illuminate a\ncrucial feature of cognition: semantic coherence. For the\nmost part, our thinking does not move randomly from thought to\nthought. Rather, thoughts are causally connected in a way that\nrespects their semantics. For example, deductive inference carries\ntrue beliefs to true beliefs. More generally, thinking tends to\nrespect epistemic properties such as warrant and degree of\nconfirmation. In some sense, then, our thinking tends to cohere with\nsemantic relations among thoughts. How is semantic coherence achieved?\nHow does our thinking manage to track semantic properties? CCTM+FSC\ngives one possible answer. It shows how a physical system operating in\naccord with physical laws can execute computations that coherently\ntrack semantic properties. By treating the mind as a syntax-driven\nmachine, we explain how mental activity achieves semantic coherence.\nWe thereby answer the question: How is rationality mechanically\npossible? \nFodor’s argument convinced many researchers that CCTM+FSC\ndecisively advances our understanding of the mind’s relation to\nthe physical world. But not everyone agrees that CCTM+FSC adequately\nintegrates semantics into the causal order. A common worry is that the\nformal syntactic picture veers dangerously close to\nepiphenomenalism (Block 1990; Kazez 1994). Pre-theoretically,\nsemantic properties of mental states seem highly relevant to mental\nand behavioral outcomes. For example, if I form an intention to walk\nto the grocery store, then the fact that my intention concerns the\ngrocery store rather than the post office helps explain why I walk to\nthe grocery store rather than the post office. Burge (2010) and\nPeacocke (1994) argue that cognitive science theorizing likewise\nassigns causal and explanatory importance to semantic properties. The\nworry is that CCTM+FSC cannot accommodate the causal and explanatory\nimportance of semantic properties because it depicts them as causally\nirrelevant: formal syntax, not semantics, drives mental computation\nforward. Semantics looks epiphenomenal, with syntax doing all the work\n(Stich 1983). \nFodor (1990, 1994) expends considerable energy trying to allay\nepiphenomenalist worries. Advancing a detailed theory of the relation\nbetween Mentalese syntax and Mentalese semantics, he insists that FSC\ncan honor the causal and explanatory relevance of semantic properties.\nFodor’s treatment is widely regarded as problematic (Arjo 1996;\nAydede 1997b, 1998; Aydede & Robbins 2001; Perry 1998; Prinz 2011;\nWakefield 2002), although Rupert (2008) and Schneider (2005) espouse\nsomewhat similar positions. \nPartly in response to epiphenomenalist worries, some authors recommend\nthat we replace FSC with an alternative semantic conception\nof computation (Block 1990; Burge 2010: 95–101; Figdor 2009;\nO’Brien & Opie 2006; Peacocke 1994, 1999; Rescorla 2012a).\nSemantic computationalists claim that computational transitions are\nsometimes sensitive to semantic properties, perhaps in addition to\nsyntactic properties. More specifically, semantic computationalists\ninsist that mental computation is sometimes sensitive to\nsemantics. Thus, they reject any suggestion that the mind is a\n“syntactic engine” or that mental computation is sensitive\nonly to formal\n syntax.[6]\n To illustrate, consider Mentalese conjunction. This mental symbol\nexpresses the truth-table for conjunction. According to semantic\ncomputationalists, the symbol’s meaning is relevant (both\ncausally and explanatorily) to mechanical operations over it. That the\nsymbol expresses the truth-table for conjunction rather than, say,\ndisjunction influences the course of computation. We should therefore\nreject any suggestion that mental computation is sensitive to the\nsymbol’s syntactic properties rather than its semantic\nproperties. The claim is not that mental computation explicitly\nrepresents semantic properties of mental symbols. All parties\nagree that, in general, it does not. There is no homunculus inside\nyour head interpreting your mental language. The claim is rather that\nsemantic properties influence how mental computation proceeds.\n(Compare: the momentum of a baseball thrown at a window causally\ninfluences whether the window breaks, even though the window does not\nexplicitly represent the baseball’s momentum.) \nProponents of the semantic conception differ as to how exactly they\ngloss the core claim that some computations are\n“sensitive” to semantic properties. They also differ in\ntheir stance towards CCTM. Block (1990) and Rescorla (2014a) focus upon CCTM. They argue\nthat a symbol’s semantic properties can impact mechanical\noperations executed by a Turing-style computational system. In\ncontrast, O’Brien and Opie (2006) favor connectionism over\nCCTM. \nTheorists who reject FSC must reject Fodor’s explanation of\nsemantic coherence. What alternative explanation might they offer? So\nfar, the question has received relatively little attention. Rescorla\n(2017a) argues that semantic computationalists can explain semantic\ncoherence and simultaneously avoid epiphenomenalist worries by\ninvoking neural implementation of semantically-sensitive mental\ncomputations. \nFodor’s exposition sometimes suggests that CTM, CCTM, or\nCCTM+FSC is definitive of LOTH (1981: 26). Yet not everyone who\nendorses RTT+COMP endorses CTM, CCTM, or FSC. One can postulate a\nmental language without agreeing that mental activity is\ncomputational, and one can postulate mental computations over a mental\nlanguage without agreeing that the computations are sensitive only to\nsyntactic properties. For most purposes, it is not important whether\nwe regard CTM, CCTM, or CCTM+FSC as definitive of LOTH. More important\nis that we track the distinctions among the doctrines. \nThe literature offers many arguments for LOTH. This section introduces\nfour influential arguments, each of which supports LOTH abductively by\nciting its explanatory benefits.\n Section 5\n discusses some prominent objections to the four arguments. \nFodor (1975) defends RTT+COMP+CCTM by appealing to scientific\npractice: our best cognitive science postulates Turing-style mental\ncomputations over Mentalese expressions; therefore, we should accept\nthat mental computation operates over Mentalese expressions. Fodor\ndevelops his argument by examining detailed case studies, including\nperception, decision-making, and linguistic comprehension. He argues\nthat, in each case, computation over mental representations plays a\ncentral explanatory role. Fodor’s argument was widely heralded\nas a compelling analysis of then-current cognitive science. \nWhen evaluating cognitive science support for LOTH, it is crucial to\nspecify what version of LOTH one has in mind. Specifically,\nestablishing that certain mental processes operate over mental\nrepresentations is not enough to establish RTT. For example, one might\naccept that mental representations figure in perception and animal\nnavigation but not in high-level human cognition. Gallistel and King\n(2009) defend COMP+CCTM+FSC through a number of (mainly non-human)\nempirical case studies, but they do not endorse RTT. They focus on\nrelatively low-level phenomena, such as animal navigation, without\ndiscussing human decision-making, deductive inference, problem\nsolving, or other high-level cognitive phenomena. \nDuring your lifetime, you will only entertain a finite number of\nthoughts. In principle, though, there are infinitely many thoughts you\nmight entertain. Consider: \nMary gave the test tube to John’s daughter. \nMary gave the test tube to John’s daughter’s daughter. \nMary gave the test tube to John’s daughter’s\ndaughter’s daughter. \n⋮ \nThe moral usually drawn is that you have the competence to\nentertain a potential infinity of thoughts, even though your\nperformance is bounded by biological limits upon memory,\nattention, processing capacity, and so on. In a slogan: thought is\nproductive. \nRTT+COMP straightforwardly explains productivity. We postulate a\nfinite base of primitive Mentalese symbols, along with operations for\ncombining simple expressions into complex expressions. Iterative\napplication of the compounding operations generates an infinite array\nof mental sentences, each in principle within your cognitive\nrepertoire. By tokening a mental sentence, you entertain the thought\nexpressed by it. This explanation leverages the recursive nature of\ncompositional mechanisms to generate infinitely many expressions from\na finite base. It thereby illuminates how finite creatures such as\nourselves are able to entertain a potential infinity of thoughts. \nFodor and Pylyshyn (1988) argue that, since RTT+COMP provides a\nsatisfying explanation for productivity, we have good reason to accept\nRTT+COMP. A potential worry about this argument is that it rests upon\nan infinitary competence never manifested within actual performance.\nOne might dismiss the supposed infinitary competence as an\nidealization that, while perhaps convenient for certain purposes, does\nnot stand in need of explanation. \nThere are systematic interrelations among the thoughts a thinker can\nentertain. For example, if you can entertain the thought that John\nloves Mary, then you can also entertain the thought that Mary loves\nJohn. Systematicity looks like a crucial property of human thought and\nso demands a principled explanation. \nRTT+COMP gives a compelling explanation. According to RTT+COMP, your\nability to entertain the thought that p hinges upon your\nability to bear appropriate psychological relations to a Mentalese\nsentence S whose meaning is that p. If you are able to\nthink that John loves Mary, then your internal system of mental\nrepresentations includes a mental sentence John loves\nMary, composed of mental words John,\nloves, and Mary\ncombined in the right way. If you have the capacity to stand in\npsychological relation A* to John loves\nMary, then you also have the capacity to stand in relation\nA* to a distinct mental sentence Mary loves\nJohn. The constituent words John, loves,\nand Mary make the\nsame semantic contribution to both mental sentences (John\ndenotes John, loves\ndenotes the loving relation, and Mary denotes\nMary), but the words are arranged in different constituency structures\nso that the sentences have different meanings. Whereas John\nloves Mary means that John loves Mary, Mary\nloves John means that Mary loves John. By\nstanding in relation A* to the sentence Mary\nloves John, you entertain the thought that Mary loves John.\nThus, an ability to think that John loves Mary entails an ability to\nthink that John loves Mary. By comparison, an ability to think that\nJohn loves Mary does not entail an ability to think that whales are\nmammals or an ability to think that \\(56 + 138 = 194\\). \nFodor (1987: 148–153) supports RTT+COMP by citing its ability to\nexplain systematicity. In contrast with the productivity argument, the\nsystematicity argument does not depend upon infinitary idealizations\nthat outstrip finite performance. Note that neither argument provides\nany direct support for CTM. Neither argument even mentions\ncomputation. \nThere are systematic interrelations among which inferences a thinker\ncan draw. For example, if you can infer p from p\nand q, then you can also infer m from m and\nn. The systematicity of thinking requires explanation. Why is it\nthat thinkers who can infer p from p and\nq can also infer m from m and\nn? \nRTT+COMP+CCTM gives a compelling explanation. During an inference from\np and q to p, you transit from believing* mental\nsentence \\(S_1 \\amp S_2\\) (which means that p and q) to\nbelieving* mental sentence \\(S_{1}\\) (which means that p).\nAccording to CCTM, the transition involves symbol manipulation. A\nmechanical operation detaches the conjunct \\(S_{1}\\) from the\nconjunction \\(S_1 \\amp S_2\\). The same mechanical operation is\napplicable to a conjunction \\(S_{3} \\amp S_{4}\\) (which means that\nm and n), corresponding to the inference from m and\nn to n. An ability to execute the first inference entails\nan ability to execute the second, because drawing the inference in\neither case corresponds to executing a single uniform mechanical\noperation. More generally, logical inference deploys mechanical\noperations over structured symbols, and the mechanical operation\ncorresponding to a given inference pattern (e.g., conjunction\nintroduction, disjunction elimination, etc.) is applicable to any\npremises with the right logical structure. The uniform applicability\nof a single mechanical operation across diverse symbols explains\ninferential systematicity. Fodor and Pylyshyn (1988) conclude that\ninferential systematicity provides reason to accept RTT+COMP+CCTM. \nFodor and Pylyshyn (1988) endorse an additional thesis about the\nmechanical operations corresponding to logical transitions. In keeping\nwith FSC, they claim that the operations are sensitive to formal\nsyntactic properties but not semantic properties. For example,\nconjunction elimination responds to Mentalese conjunction as a piece\nof pure formal syntax, much as a computer manipulates items in a\nformal language without considering what those items mean. \nSemantic computationalists reject FSC. They claim that mental\ncomputation is sometimes sensitive to semantic properties. Semantic\ncomputationalists can agree that drawing an inference involves\nexecuting a mechanical operation over structured symbols, and they can\nagree that the same mechanical operation uniformly applies to any\npremises with appropriate logical structure. So they can still explain\ninferential systematicity. However, they can also say that the\npostulated mechanical operation is sensitive to semantic properties.\nFor example, they can say that conjunction elimination is sensitive to\nthe meaning of Mentalese conjunction. \nIn assessing the debate between FSC and semantic computationalism, one\nmust distinguish between logical versus non-logical\nsymbols. For present purposes, it is common ground that the meanings\nof non-logical symbols do not inform logical inference. The\ninference from \\(S_1 \\amp S_2\\) to \\(S_{1}\\) features the same\nmechanical operation as the inference from \\(S_{3} \\amp S_{4}\\) to\n\\(S_{4}\\), and this mechanical operation is not sensitive to the\nmeanings of the conjuncts \\(S_{1}\\), \\(S_{2}\\), \\(S_{3}\\), or\n\\(S_{4}\\). It does not follow that the mechanical operation is\ninsensitive to the meaning of Mentalese conjunction. The meaning of\nconjunction might influence how the logical inference proceeds, even\nthough the meanings of the conjuncts do not. \nIn the 1960s and 1970s, cognitive scientists almost universally\nmodeled mental activity as rule-governed symbol manipulation. In the\n1980s, connectionism gained currency as an alternative computational\nframework. Connectionists employ computational models, called\nneural networks, that differ quite significantly from\nTuring-style models. There is no central processor. There are no\nmemory locations for symbols to be inscribed. Instead, there is a\nnetwork of nodes bearing weighted connections to one another.\nDuring computation, waves of activation spread through the network. A\nnode’s activation level depends upon the weighted activations of\nthe nodes to which it is connected. Nodes function somewhat\nanalogously to neurons, and connections between nodes function\nsomewhat analogously to synapses. One should receive the\nneurophysiological analogy cautiously, as there are numerous important\ndifferences between neural networks and actual neural configurations\nin the brain (Bechtel & Abramson 2002: 341–343;\nBermúdez 2010: 237–239; Clark 2014: 87–89; Harnish\n2002: 359–362). \nConnectionists raise many objections to the classical computational\nparadigm (Rumelhart, McClelland, & the PDP Research Group 1986;\nHorgan & Tienson 1996; McLaughlin & Warfield 1994; Bechtel\n& Abrahamsen 2002), such as that classical systems are not\nbiologically realistic or that they are unable to model certain\npsychological tasks. Classicists in turn launch various arguments\nagainst connectionism. The most famous arguments showcase\nproductivity, systematicity of thought, and systematicity of thinking.\nFodor and Pylyshyn (1988) argue that these phenomena support classical\nCTM over connectionist CTM. \nFodor and Pylyshyn’s argument hinges on the distinction between\neliminative connectionism and implementationist\nconnectionism (cf. Pinker & Prince 1988). Eliminative\nconnectionists advance neural networks as a replacement for\nthe Turing-style formalism. They deny that mental computation consists\nin rule-governed symbol manipulation. Implementationist connectionists\nallow that, in some cases, mental computation may instantiate\nrule-governed symbol manipulation. They advance neural networks not to\nreplace classical computations but rather to model how classical\ncomputations are implemented in the brain. The hope is that, because\nneural network computation more closely resembles actual brain\nactivity, it can illuminate the physical realization of rule-governed\nsymbol manipulation. \nBuilding on Aydede’s (2015) discussion, we may reconstruct Fodor\nand Pylyshyn’s argument like so: \nThe argument does not say that neural networks are unable to\nmodel systematicity. One can certainly build a neural network that is\nsystematic. For example, one might build a neural network that can\nrepresent that John loves Mary only if it can represent that Mary\nloves John. The problem is that one might just as well build a neural\nnetwork that can represent that John loves Mary but cannot represent\nthat Mary loves John. Hence, nothing about the connectionist framework\nper se guarantees systematicity. For that reason, the\nframework does not explain the nomic necessity of systematicity. It\ndoes not explain why all the minds we find are systematic. In\ncontrast, the classical framework mandates systematicity, and so it\nexplains the nomic necessity of systematicity. The only apparent\nrecourse for connectionists is to adopt the classical explanation,\nthereby becoming implementationist rather than eliminative\nconnectionists. \nFodor and Pylyshyn’s argument has spawned a massive literature,\nincluding too many rebuttals to survey here. The most popular\nresponses fall into five categories: \nWe focus here on (vi). \nAs discussed in\n section 1.2,\n Fodor elucidates constituency structure in terms of part/whole\nrelations. A complex representation’s constituents are literal\nparts of it. One consequence is that, whenever the first\nrepresentation is tokened, so are its constituents. Fodor takes this\nconsequence to be definitive of classical computation. As Fodor and\nMcLaughlin (1990: 186) put it:  \nfor a pair of expression types E1, E2, the first is a\nClassical constituent of the second only if the\nfirst is tokened whenever the second is tokened.  \nThus, structured representations have a concatenative\nstructure: each token of a structured representation involves a\nconcatenation of tokens of the constituent representations.\nConnectionists who deny (vi) espouse a non-concatenative\nconception of constituency structure, according to which structure is\nencoded by a suitable distributed representation.\nDevelopments of the non-concatenative conception are usually quite\ntechnical (Elman 1989; Hinton 1990; Pollack 1990; Smolensky 1990, 1991, 1995; Touretzky 1990).\nMost models use vector or tensor algebra to define\noperations over connectionist representations, which are codified by\nactivity vectors across nodes in a neural network. The representations\nare said to have implicit constituency structure: the\nconstituents are not literal parts of the complex representation, but\nthey can be extracted from the complex representation through suitable\ncomputational operations over it. \nFodor and McLaughlin (1990) grant that distributed representations may\nhave constituency structure “in an extended sense”. But\nthey insist that distributed representations are ill-suited to explain\nsystematicity. They focus especially on the systematicity of thinking,\nthe classical explanation for which postulates mechanical operations\nthat respond to constituency structure. Fodor and McLaughlin argue\nthat the non-concatenative conception cannot replicate the classical\nexplanation and offers no satisfactory substitute for it. Chalmers\n(1993) and Niklasson and van Gelder (1994) disagree. They contend that\na neural network can execute structure-sensitive computations over\nrepresentations that have non-concatenative constituency structure.\nThey conclude that connectionists can explain productivity and\nsystematicity without retreating to implementationist\nconnectionism. \nAydede (1995, 1997a) agrees that there is a legitimate notion of\nnon-concatenative constituency structure, but he questions whether the\nresulting models are non-classical. He denies that we should regard\nconcatenative structure as integral to LOTH. According to Aydede,\nconcatenative structure is just one possible physical realization of\nconstituency structure. Non-concatenative structure is another\npossible realization. We can accept RTT+COMP without glossing\nconstituency structure in concatenative terms. On this view, a neural\nnetwork whose operations are sensitive to non-concatenative\nconstituency structure may still count as broadly classical and in\nparticular as manipulating Mentalese expressions. \nThe debate between classical and connectionist CTM is still active,\nalthough not as active as during the 1990s. Recent anti-connectionist\narguments tend to have a more empirical flavor. For example, Gallistel\nand King (2009) defend CCTM by canvassing a range of non-human\nempirical case studies. According to Gallistel and King, the case\nstudies manifest a kind of productivity that CCTM can easily explain\nbut eliminative connectionism cannot. \nLOTH has elicited too many objections to cover in a single\nencyclopedia entry. We will discuss two objections, both alleging that\nLOTH generates a vicious regress. The first objection emphasizes\nlanguage learning. The second emphasizes\nlanguage understanding. \nLike many cognitive scientists, Fodor holds that children learn a\nnatural language via hypothesis formation and testing.\nChildren formulate, test, and confirm hypotheses about the denotations\nof words. For example, a child learning English will confirm the\nhypothesis that “cat” denotes cats. According to Fodor,\ndenotations are represented in Mentalese. To formulate the hypothesis\nthat “cat” denotes cats, the child uses a Mentalese word\ncat that denotes cats. It may seem that a regress is now in the\noffing, sparked by the question: How does the child learn Mentalese?\nSuppose we extend the hypothesis formation and testing model\n(henceforth HF) to Mentalese. Then we must posit a meta-language to\nexpress hypotheses about denotations of Mentalese words, a\nmeta-meta-language to express hypotheses about denotations of\nmeta-language words, and so on ad infinitum (Atherton and\nSchwartz 1974: 163). \nFodor responds to the threatened regress by denying we should apply HF\nto Mentalese (1975: 65). Children do not test hypotheses about the\ndenotations of Mentalese words. They do not learn Mentalese at all.\nThe mental language is innate. \nThe doctrine that some concepts are innate was a focal point\nin the clash between rationalism versus empiricism. Rationalists\ndefended the innateness of certain fundamental ideas, such as god and\ncause, while empiricists held that all ideas derive from sensory\nexperience. A major theme in the 1960s cognitive science revolution\nwas revival of a nativist picture, inspired by the\nrationalists, on which many key elements of cognition are innate. Most\nfamously, Chomsky (1965) explained language acquisition by positing\ninnate knowledge about possible human languages. Fodor’s\ninnateness thesis was widely perceived as going way beyond all\nprecedent, verging on the preposterous (P.S. Churchland 1986; Putnam\n1988). How could we have an innate ability to represent all the\ndenotations we mentally represent? For example, how could we innately\npossess a Mentalese word carburetor that represents carburetors? \nIn evaluating these issues, it is vital to distinguish between\nlearning a concept versus acquiring a concept. When\nFodor says that a concept is innate, he does not mean to deny that we\nacquire the concept or even that certain kinds of experience are\nneeded to acquire it. Fodor fully grants that we cannot mentally\nrepresent carburetors at birth and that we come to represent them only\nby undergoing appropriate experiences. He agrees that most concepts\nare acquired. He denies that they are learned. In\neffect, he uses “innate” as a synonym for\n“unlearned” (1975: 96). One might reasonably challenge\nFodor’s usage. One might resist classifying a concept as innate\nsimply because it is unlearned. However, that is how Fodor\nuses the word “innate”. Properly understood, then,\nFodor’s position is not as far-fetched as it may\n sound.[7] \nFodor gives a simple but striking argument that concepts are\nunlearned. The argument begins from the premise that HF is the only\npotentially viable model of concept learning. Fodor then\nargues that HF is not a viable model of concept learning,\nfrom which he concludes that concepts are unlearned. He offers various\nformulations and refinements of the argument over his career. Here is\na relatively recent rendition (2008: 139): \nNow, according to HF, the process by which one learns C must\ninclude the inductive evaluation of some such hypothesis as “The\nC things are the ones that are green or triangular”. But\nthe inductive evaluation of that hypothesis itself requires (inter\nalia) bringing the property green or triangular before\nthe mind as such… Quite generally, you can’t represent\nanything as such and such unless you already have the concept\nsuch and such. All that being so, it follows, on pain of\ncircularity, that “concept learning” as HF understands it\ncan’t be a way of acquiring concept C…\nConclusion: If concept learning is as HF understands it, there can\nbe no such thing. This conclusion is entirely general; it\ndoesn’t matter whether the target concept is primitive (like\ngreen) or complex (like green\nor triangular). \nFodor’s argument does not presuppose RTT, COMP, or CTM. To the\nextent that the argument works, it applies to any view on which people\nhave concepts. \nIf concepts are not learned, then how are they acquired? Fodor offers\nsome preliminary remarks (2008: 144–168), but by his own\nadmission the remarks are sketchy and leave numerous questions\nunanswered (2008: 144–145). Prinz (2011) critiques Fodor’s\npositive treatment of concept acquisition. \nThe most common rejoinder to Fodor’s innateness argument is to\ndeny that HF is the only viable model of concept learning. The\nrejoinder acknowledges that concepts are not learned through\nhypothesis testing but insists they are learned through other\nmeans. Three examples: \nA lot depends here upon what counts as “learning” and what\ndoes not, a question that seems difficult to adjudicate. A closely\nconnected question is whether concept acquisition is a\nrational process or a mere causal process. To the\nextent that acquiring some concept is a rational achievement, we will\nwant to say that one learned the concept. To the extent that acquiring\nthe concept is a mere causal process (more like catching a cold than\nconfirming a hypothesis), we will feel less inclined to say that\ngenuine learning took place (Fodor 1981: 275). \nThese issues lie at the frontier of psychological and philosophical\nresearch. The key point for present purposes is that there are two\noptions for halting the regress of language learning: we can say that\nthinkers acquire concepts but do not learn them; or we can say that\nthinkers learn concepts through some means other than hypothesis\ntesting. Of course, it is not enough just to note that the two options\nexist. Ultimately, one must develop one’s favored option into a\ncompelling theory. But there is no reason to think that doing so would\nreinitiate the regress. In any event, explaining concept acquisition\nis an important task facing any theorist who accepts that we have\nconcepts, whether or not the theorist accepts LOTH. Thus, the learning\nregress objection is best regarded not as posing a challenge specific\nto LOTH but rather as highlighting a more widely shared theoretical\nobligation: the obligation to explain how we acquire concepts. \nFor further discussion, see the entry on innateness. See also the\nexchange between Cowie (1999) and Fodor (2001). \nWhat is it to understand a natural language word? On a popular\npicture, understanding a word requires that you mentally represent the\nword’s denotation. For example, understanding the word\n“cat” requires representing that it denotes cats. LOT\ntheorists will say that you use Mentalese words to represent\ndenotations. The question now arises what it is to understand a\nMentalese word. If understanding the Mentalese word requires\nrepresenting that it has a certain denotation, then we face an\ninfinite regress of meta-languages (Blackburn 1984: 43–44). \nThe standard response is to deny that ordinary thinkers represent\nMentalese words as having denotations (Bach 1987; Fodor 1975:\n66–79). Mentalese is not an instrument of communication.\nThinking is not “talking to oneself” in Mentalese. A\ntypical thinker does not represent, perceive, interpret, or reflect\nupon Mentalese expressions. Mentalese serves as a medium within which\nher thought occurs, not an object of interpretation. We should not say\nthat she “understands” Mentalese in the same way that she\nunderstands a natural language. \nThere is perhaps another sense in which the thinker\n“understands” Mentalese: her mental activity coheres with\nthe meanings of Mentalese words. For example, her deductive reasoning\ncoheres with the truth-tables expressed by Mentalese logical\nconnectives. More generally, her mental activity is semantically\ncoherent. To say that the thinker “understands” Mentalese\nin this sense is not to say that she represents Mentalese\ndenotations. Nor is there any evident reason to suspect that\nexplaining semantic coherence will ultimately require us to posit\nmental representation of Mentalese denotations. So there is no regress\nof understanding. \nFor further criticism of this regress argument, see the discussions of\nKnowles (1998) and\nLaurence and Margolis\n (1997).[8] \n Naturalism\n is a movement that seeks to ground philosophical theorizing\nin the scientific enterprise. As so often in philosophy, different\nauthors use the term “naturalism” in different ways. Usage\nwithin philosophy of mind typically connotes an effort to depict\nmental states and processes as denizens of the physical world, with no\nirreducibly mental entities or properties allowed. In the modern era,\nphilosophers have often recruited LOTH to advance naturalism. Indeed,\nLOTH’s supposed contribution to naturalism is frequently cited\nas a significant consideration in its favor. One example is\nFodor’s use of CCTM+FSC to explain semantic coherence. The other\nmain example turns upon the problem of intentionality. \nHow does intentionality arise? How do mental states come to be\nabout anything, or to have semantic properties? Brentano\n(1874 [1973: 97]) maintained that intentionality is a hallmark of the\nmental as opposed to the physical: “The reference to something\nas an object is a distinguishing characteristic of all mental\nphenomena. No physical phenomenon exhibits anything similar”. In\nresponse, contemporary naturalists seek to naturalize\nintentionality. They want to explain in naturalistically\nacceptable terms what makes it the case that mental states have\nsemantic properties. In effect, the goal is to reduce the intentional\nto the non-intentional. Beginning in the 1980s, philosophers have\noffered various proposals about how to naturalize intentionality. Most\nproposals emphasize causal or nomic links between mind and world\n(Aydede & Güzeldere 2005; Dretske 1981; Fodor 1987, 1990;\nStalnaker 1984), sometimes also invoking teleological factors\n(Millikan 1984, 1993; Neander 2017l; Papineau 1987; Dretske 1988) or\nhistorical lineages of mental states (Devitt 1995; Field 2001).\nAnother approach, functional role semantics, emphasizes the\nfunctional role of a mental state: the cluster of causal or\ninferential relations that the state bears to other mental states. The\nidea is that meaning emerges at least partly through these causal and\ninferential relations. Some functional role theories cite causal\nrelations to the external world (Block 1987; Loar 1982), and others do\nnot (Cummins 1989). \nEven the best developed attempts at naturalizing intentionality, such\nas Fodor’s (1990) version of the nomic strategy, face serious\nproblems that no one knows how to solve (M. Greenberg 2014; Loewer\n1997). Partly for that reason, the flurry of naturalizing attempts\nabated in the 2000s. Burge (2010: 298) reckons that the naturalizing\nproject is not promising and that current proposals are\n“hopeless”. He agrees that we should try to illuminate\nrepresentationality by limning its connections to the physical, the\ncausal, the biological, and the teleological. But he insists that\nillumination need not yield a reduction of the intentional to the\nnon-intentional. \nLOTH is neutral as to the naturalization of intentionality. An LOT\ntheorist might attempt to reduce the intentional to the\nnon-intentional. Alternatively, she might dismiss the reductive\nproject as impossible or pointless. Assuming she chooses the reductive\nroute, LOTH provides guidance regarding how she might proceed.\nAccording to RTT, \nX A’s that p iff there is a mental\nrepresentation S such that X bears A* to S\nand S means that p. \nThe task of elucidating “X A’s that\np” in naturalistically acceptable terms factors into two\nsub-tasks (Field 2001: 33): \nAs we have seen, functionalism helps with (a). Moreover, COMP provides\na blueprint for tackling (b). We can first delineate a compositional\nsemantics describing how S’s meaning depends upon\nsemantic properties of its component words and upon the compositional\nimport of the constituency structure into which those words are\narranged. We can then explain in naturalistically acceptable terms why\nthe component words have the semantic properties that they have and\nwhy the constituency structure has the compositional import that it\nhas. \nHow much does LOTH advance the naturalization of intentionality? Our\ncompositional semantics for Mentalese may illuminate how the semantic\nproperties of a complex expression depend upon the semantic properties\nof primitive expressions, but it says nothing about how primitive\nexpressions get their semantic properties in the first place.\nBrentano’s challenge (How could intentionality arise from\npurely physical entities and processes?) remains unanswered. To\nmeet the challenge, we must invoke naturalizing strategies that go\nwell beyond LOTH itself, such as the causal or nomic strategies\nmentioned above. Those naturalizing strategies are not specifically\nlinked to LOTH and can usually be tailored to semantic properties of\nneural states rather than semantic properties of Mentalese\nexpressions. Thus, it is debatable how much LOTH ultimately helps us\nnaturalize intentionality. Naturalizing strategies orthogonal to LOTH\nseem to do the heavy lifting. \nHow are Mentalese expressions individuated? Since Mentalese\nexpressions are types, answering this question requires us to consider\nthe type/token relation for Mentalese. We want to fill in the\nschema \ne and e* are tokens of the same Mentalese type iff\nR(e, e*). \nWhat should we substitute for R(e, e*)? The\nliterature typically focuses on primitive symbol types, and\nwe will follow suit here. \nIt is almost universally agreed among contemporary LOT theorists that\nMentalese tokens are neurophysiological entities of some sort. One\nmight therefore hope to individuate Mentalese types by citing neural\nproperties of the tokens. Drawing R(e, e*) from\nthe language of neuroscience induces a theory along the following\nlines: \nNeural individuation: e and e*\nare tokens of the same primitive Mentalese type iff e and\ne* are tokens of the same neural type. \nThis schema leaves open how neural types are individuated. We may\nbypass that question here, because neural individuation of Mentalese\ntypes finds no proponents in the contemporary literature. The main\nreason is that it conflicts with \n  multiple realizability: \n the doctrine that a single mental state type can be realized by\nphysical systems that are wildly heterogeneous when described in\nphysical, biological, or neuroscientific terms. Putnam (1967)\nintroduced multiple realizability as evidence against the \n mind/brain identity theory,\n which asserts that mental state types are brain state\ntypes. Fodor (1975: 13–25) further developed the multiple\nrealizability argument, presenting it as foundational to\nLOTH. Although the multiple realizability argument has subsequently\nbeen challenged (Polger 2004), LOT theorists widely agree that we\nshould not individuate Mentalese types in neural terms. \nThe most popular strategy is to individuate Mentalese types\nfunctionally: \nFunctional individuation: e and\ne* are tokens of the same primitive Mentalese type iff e\nand e* have the same functional role. \nField (2001: 56–67), Fodor (1994: 105–109), and Stich (1983:\n149–151) pursue functional individuation. They specify\nfunctional roles using a Turing-style computationalism formalism, so\nthat “functional role” becomes something like\n“computational role”, i.e., role within mental\ncomputation. \nFunctional roles theories divide into two categories:\nmolecular and holist. Molecular theories isolate\nprivileged canonical relations that a symbol bears to other symbols.\nCanonical relations individuate the symbol, but non-canonical\nrelations do not. For example, one might individuate Mentalese\nconjunction solely through the introduction and elimination rules\ngoverning conjunction while ignoring any other computational rules. If\nwe say that a symbol’s “canonical functional role”\nis constituted by its canonical relations to other symbols, then we\ncan offer the following theory: \nMolecular functional individuation: e\nand e* are tokens of the same primitive Mentalese type iff\ne and e* have the same canonical functional role. \nOne problem facing molecular individuation is that, aside from logical\nconnectives and a few other special cases, it is difficult to draw any\nprincipled demarcation between canonical and non-canonical relations\n(Schneider 2011: 106). Which relations are canonical for\n SOFA?[9]\n Citing the demarcation problem, Schneider espouses a holist approach\nthat individuates mental symbols through total functional\nrole, i.e., every single aspect of the role that a symbol plays\nwithin mental activity: \nHolist functional individuation: e\nand e* are tokens of the same primitive Mentalese type iff\ne and e* have the same total functional role. \nHolist individuation is very fine-grained: the slightest difference in\ntotal functional role entails that different types are tokened. Since\ndifferent thinkers will always differ somewhat in their mental\ncomputations, it now looks like two thinkers will never share the same\nmental language. This consequence is worrisome, for two reasons\nemphasized by Aydede (1998). First, it violates the plausible\npublicity constraint that propositional attitudes are in\nprinciple shareable. Second, it apparently precludes interpersonal\npsychological explanations that cite Mentalese expressions. Schneider\n(2011: 111–158) addresses both concerns, arguing that they are\nmisdirected. \nA crucial consideration when individuating mental symbols is what role\nto assign to semantic properties. Here we may usefully compare\nMentalese with natural language. It is widely agreed that natural\nlanguage words do not have their denotations essentially. The English\nword “cat” denotes cats, but it could just as well have\ndenoted dogs, or the number 27, or anything else, or nothing at all,\nif our linguistic conventions had been different. Virtually all\ncontemporary LOT theorists hold that a Mentalese word likewise does\nnot have its denotation essentially. The Mentalese word cat denotes\ncats, but it could have had a different denotation had it born\ndifferent causal relations to the external world or had it occupied a\ndifferent role in mental activity. In that sense, cat is a piece of\nformal syntax. Fodor’s early view (1981: 225–253) was that\na Mentalese word could have had a different denotation but\nnot an arbitrarily different denotation: cat could not have\ndenoted just anything—it could not have denoted the number\n27—but it could have denoted some other animal species had the\nthinker suitably interacted with that species rather than with cats.\nFodor eventually (1994, 2008) embraces the stronger thesis that a\nMentalese word bears an arbitrary relation to its denotation:\ncat could have had any arbitrarily different denotation. Most\ncontemporary theorists agree (Egan 1992: 446; Field 2001: 58; Harnad\n1994: 386; Haugeland 1985: 91: 117–123; Pylyshyn 1984: 50). \nThe historical literature on LOTH suggests an alternative\nsemantically permeated view: Mentalese words are individuated\npartly through their denotations. The Mentalese word cat is not a\npiece of formal syntax subject to reinterpretation. It could not have\ndenoted another species, or the number 27, or anything else. It\ndenotes cats by its inherent nature. From a semantically\npermeated viewpoint, a Mentalese word has its denotation essentially.\nThus, there is a profound difference between natural language and\nmental language. Mental words, unlike natural language words, bring\nwith them one fixed semantic interpretation. The semantically\npermeated approach is present in Ockham, among other medieval LOT\ntheorists (Normore 2003, 2009). In light of the problems facing neural\nand functional individuation, Aydede (2005) recommends that we\nconsider taking semantics into account when individuating Mentalese\nexpressions. Rescorla (2012b) concurs, defending a semantically permeated approach as\napplied to at least some mental representations. He proposes that\ncertain mental computations operate over mental symbols with essential\nsemantic properties, and he argues that the proposal fits well with\nmany sectors of cognitive\n science.[10] \nA recurring complaint about the semantically permeated approach is\nthat inherently meaningful mental representations seem like highly\nsuspect entities (Putnam 1988: 21). How could a mental word have one\nfixed denotation by its inherent nature? What magic ensures\nthe necessary connection between the word and the denotation? These\nworries diminish in force if one keeps firmly in mind that Mentalese\nwords are types. Types are abstract entities corresponding to a scheme\nfor classifying, or type-identifying, tokens. To ascribe a\ntype to a token is to type-identify the token as belonging to some\ncategory. Semantically permeated types correspond to a classificatory\nscheme that takes semantics into account when categorizing tokens. As\nBurge emphasizes (2007: 302), there is nothing magical about\nsemantically-based classification. On the contrary, both folk\npsychology and cognitive science routinely classify mental events\nbased at least partly upon their semantic properties. \nA simplistic implementation of the semantically permeated approach\nindividuates symbol tokens solely through their\ndenotations: \nDenotational individuation: e and\ne* are tokens of the same primitive Mentalese type iff e\nand e* have the same denotation. \nAs Aydede (2000) and Schneider (2011) emphasize, denotational\nindividuation is unsatisfying. Co-referring words may play\nsignificantly different roles in mental activity. Frege’s (1892\n[1997]) famous Hesperus-Phosphorus example illustrates: one can\nbelieve that Hesperus is Hesperus without believing that Hesperus is\nPhosphorus. As Frege put it, one can think about the same denotation\n“in different ways”, or “under different modes of\npresentation”. Different modes of presentation have different\nroles within mental activity, implicating different psychological\nexplanations. Thus, a semantically permeated individuative scheme\nadequate for psychological explanation must be finer-grained than\ndenotational individuation allows. It must take mode of presentation\ninto account. But what it is to think about a denotation “under\nthe same mode of presentation”? How are “modes of\npresentation” individuated? Ultimately, semantically permeated\ntheorists must grapple with these questions. Rescorla (forthcoming)\noffers some suggestions about how to\n proceed.[11] \nChalmers (2012) complains that semantically permeated individuation\nsacrifices significant virtues that made LOTH attractive in the first\nplace. LOTH promised to advance naturalism by grounding cognitive\nscience in non-representational computational models.\nRepresentationally-specified computational models seem like a\nsignificant retrenchment from these naturalistic ambitions. For\nexample, semantically permeated theorists cannot accept the FSC\nexplanation of semantic coherence, because they do not postulate\nformal syntactic types manipulated during mental computation. \nHow compelling one finds naturalistic worries about semantically\npermeated individuation will depend on how impressive one finds the\nnaturalistic contributions made by formal mental syntax. We saw\nearlier that FSC arguably engenders a worrisome epiphenomenalism.\nMoreover, the semantically permeated approach in no way precludes a\nnaturalistic reduction of intentionality. It merely precludes invoking\nformal syntactic Mentalese types while executing such a reduction. For\nexample, proponents of the semantically permeated approach can still\npursue the causal or nomic naturalizing strategies discussed in\n section 7.\n Nothing about either strategy presupposes formal syntactic Mentalese\ntypes. Thus, it is not clear that replacing a formal syntactic\nindividuative scheme with a semantically permeated scheme\nsignificantly impedes the naturalistic endeavor. \nNo one has yet provided an individuative scheme for Mentalese that\ncommands widespread assent. The topic demands continued investigation,\nbecause LOTH remains highly schematic until its proponents clarify\nsameness and difference of Mentalese types.","contact.mail":"rescorla@ucla.edu","contact.domain":"ucla.edu"}]
