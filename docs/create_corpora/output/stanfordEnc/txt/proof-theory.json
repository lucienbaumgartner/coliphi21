[{"date.published":"2018-08-13","url":"https://plato.stanford.edu/entries/proof-theory/","author1":"Michael Rathjen","author2":"Wilfried Sieg","author1.info":"http://www1.maths.leeds.ac.uk/~rathjen/rathjen.html","entry":"proof-theory","body.text":"\n\n\nProof theory is not an esoteric technical subject that was invented to\nsupport a formalist doctrine in the philosophy of mathematics; rather,\nit has been developed as an attempt to analyze aspects of mathematical\nexperience and to isolate, possibly overcome, methodological problems\nin the foundations of mathematics. The origins of those problems,\nforcefully and sometimes contentiously formulated in the 1920s, are\ntraceable to the transformation of mathematics in the nineteenth\ncentury: the emergence of abstract mathematics, its reliance on set\ntheoretic notions, and its focus on logic in a broad, foundational\nsense. Substantive issues came to the fore already in the mathematical\nwork and the foundational essays of Dedekind and Kronecker; they\nconcerned the legitimacy of undecidable concepts, the existence of\ninfinite mathematical objects, and the sense of non-constructive\nproofs of existential statements.\n\n\nIn an attempt to mediate between conflicting foundational positions,\nHilbert shifted issues, already around 1900, from a mathematical to a\nvaguely conceived metamathematical level. That approach was rigorously\nrealized in the 1920s, when he took advantage of the possibility of\nformalizing mathematics in deductive systems and investigated the\nunderlying formal frames from a strictly constructive,\n“finitist” standpoint. Hilbert’s approach raised\nfascinating metamathematical questions—from semantic\ncompleteness through mechanical decidability to syntactic\nincompleteness; however, the hoped-for mathematical resolution of the\nfoundational issues was not achieved. The failure of his finitist\nconsistency program raised and deepened equally fascinating\nmethodological questions. A broadened array of problems with only\npartial solutions has created a vibrant subject that spans\ncomputational, mathematical, and philosophical issues—with a\nrich history.\n\n\nThe main part of our article covers these exciting investigations for\nan expanded Hilbert Program through 1999—with special, detailed\nattention to results and techniques that by now can be called\n“classical” and are of continued interest. Newer, but\nstill closely connected developments are sketched in Appendices: the\nproof theory of set theories in Appendix D combinatorial\nindependence results in Appendix E, and provably total\nfunctions in Appendix F. Here (infinitary) sequent calculi and\nsuitable systems of ordinal notations are crucial proof theoretic\ntools. However, we discuss in section 4.2 also Gödel’s\nDialectica Interpretation and some of its extensions as an alternative\nfor obtaining relative consistency proofs and describe in section\n5.2.1 the systematic attempt of completing the incomplete through\nrecursive progressions. Both topics are analyzed further in Appendix\nC.2 and Appendix B, respectively. To complete this bird’s eye\nview of our article, we mention that the Epilogue, section 6,\nnot only indicates further proof theoretic topics, but also some\ndirections of current research that are connected to proof theory and\nof deep intrinsic interest. We have tried to convey the vibrancy of a\nsubject that thrives on concrete computational and (meta-)\nmathematical work, but also invites and is grounded in general\nphilosophical reflection.\n\nHilbert viewed the axiomatic method as the crucial tool for\nmathematics (and rational discourse in general). In a talk to the\nSwiss Mathematical Society in 1917, published the following year as\nAxiomatisches Denken (1918), he articulates his broad\nperspective on that method and presents it “at work” by\nconsidering, in detail, examples from various parts of mathematics and\nalso from physics. Proceeding axiomatically is not just developing a\nsubject in a rigorous way from first principles, but rather requires,\nfor advanced subjects, their deeper conceptual organization and\nserves, for newer investigations, as a tool for discovery. In his talk\nHilbert reflects on his investigations of the arithmetic of real\nnumbers and of Euclidean geometry from before 1900. We emphasize the\nparticular form of his axiomatic formulations; they are not logical\nformulations, but rather mathematical ones: he defines Euclidean space\nin a similar way as other abstract notions like group or field;\nthat’s why we call it structural\n axiomatics.[1]\n However, Hilbert turns from the past and looks to the future,\npointing out a programmatic direction for research in the\nfoundations of mathematics; he writes: \nTo conquer this field [concerning the foundations of mathematics] we\nmust turn the concept of a specifically mathematical proof itself into\nan object of investigation, just as the astronomer considers the\nmovement of his position, the physicist studies the theory of his\napparatus, and the philosopher criticizes reason itself. \nHe then asserts, “The execution of this program is at present\nstill an unsolved task”. During the following winter term\n1917–18, Hilbert—with the assistance of Paul\nBernays—gave a lecture course entitled Prinzipien der\nMathematik. Here modern mathematical logic is invented in one\nfell swoop and completes the shift from structural to formal\naxiomatics. This dramatic shift allows the constructive, elementary\ndefinition of the syntax of theories and, in particular, of the\nconcept of proof in a formal theory. This fundamental insight\nunderpins the articulation of the consistency problem and seems to\nopen a way of proving, meta-mathematically, that no proof in a formal\ntheory establishes a contradiction. \nThat perspective is formulated first in a talk Bernays presented in\nthe fall of 1921, published as “Über Hilberts Gedanken zur\nGrundlegung der Mathematik” (1922). Starting with a discussion\nof structural axiomatics and pointing out its assumption of a\nsystem of objects that satisfies the axioms, he asserts this\nassumption contains “something so-to-speak transcendent for\nmathematics”. He raises the question, “which principled\nposition should be taken with respect to it?” Bernays believes\nthat it might be perfectly coherent to appeal to an intuitive\ngrasp of the natural number sequence or even of the manifold of real\nnumbers. However, that could not be an intuition in any primitive\nsense and would conflict with the tendency of the exact sciences to\nuse only restricted means to acquire knowledge. \nUnder this perspective we are going to try, whether it is not possible\nto give a foundation to these transcendent assumptions in such a way\nthat only primitive intuitive knowledge is used. (Bernays 1922:\n11) \nMeaningful mathematics is to be based, Bernays demands, on primitive\nintuitive knowledge that includes, however, induction concerning\nnatural numbers—both as a proof and definition principle. In the\noutline for the lectures Grundlagen der Mathematik to be\ngiven in the winter term 1921–22, Bernays discusses a few weeks\nafter his talk “constructive arithmetic” and then the\n“broader formulation of constructive thought”: \nConstruction of the proofs, by means of which the formalization of the\nhigher inferences is made possible and the consistency problem is\nbecoming accessible in a general way.  \nBernays concludes the outline by suggesting, “This would be\nfollowed by the development of proof theory”. The outline was\nsent to Hilbert on 17 October 1921 and it was followed strictly in the\nlectures of the following term—with only one terminological\nchange: “constructive” in the above formulations is turned\ninto\n “finitist”.[2] \nBernays’s notes of the 1921/22 lectures reflect the consequence\nof that change in attitude. They contain a substantial development of\n“finitist arithmetic” and “finitist logic” in\na quantifier-free formalism. Finitist arithmetic involves induction\nand primitive\n recursion[3]\n from the outset, and the central metamathematical arguments all\nproceed straightforwardly by induction on proof figures. The third\npart of these lectures is entitled The grounding of the\nconsistency of arithmetic by Hilbert’s new proof theory.\nHere we find the first significant consistency proof—from a\nfinitist\n perspective.[4]\n The proof is sketched in Hilbert’s Leipzig talk (Hilbert 1923:\n184) and was presented in a modified form during the winter term of\n1922/23; in that form the proof is given in Ackermann 1925: 4–7.\nAckermann’s article was submitted for publication in early 1924,\nand by then the proof had taken on a certain canonical form that is\nstill found in the presentation of Hilbert and Bernays 1934:\n220–230. Let us see what was achieved by following\nAckermann’s concise discussion. \nThe proof is given in section II of Ackermann’s paper entitled,\ntellingly, The consistency proof before the addition of the\ntransfinite axioms. Ackermann uses a logical calculus in\naxiomatic form that is taken over from Hilbert’s lectures and is\ndiscussed below in\n section 2.\n Here we just note that it involves two logical rules, namely, modus\nponens and substitution (for individual, function and statement\nvariables) in axioms. The non-logical axioms concern identity, zero\nand successor, and recursion equations that define primitive recursive\nfunctions. The first step in the argument turns the linear proof into\na tree, so that any formula occurrence is used at most once as a\npremise of an inference (Auflösung in Beweisfäden);\nthis is done in preparation for the second step, namely, the\nelimination of all necessarily free variables (Ausschaltung der\nVariablen); in the third step, the numerical value of the closed\nterms is computed (Reduktion der Funktionale). The resulting\nsyntactic configurations, a Beweisfigur, contains now only\nnumerical formulae that are built up from equations or\ninequations between numerals and Boolean connectives; these formulae\ncan be effectively determined to be true or false. By induction on the\n“Beweisfigur” one shows that all its component\nformulae are true; thus, a formula like \\(0\\ne 0\\) is not provable.\nThe induction principle can be directly incorporated into these\nconsiderations when it is formulated as a rule for quantifier-free\nstatements. That was not done in Ackermann’s discussion of the\nproof, but had been achieved already by Hilbert and Bernays in their\n1922/23 lectures. \nThese proof theoretic considerations are striking and important as\nthey involve for the first time genuine transformations of formal\nderivations. Nevertheless, they are preliminary as they concern a\nquantifier-free theory that is a part of finitist mathematics\nand need not be secured by a consistency proof. What has to\nbe secured is “transfinite logic” with its “ideal\nelements”, as Hilbert put it later. The strategy was direct and\nstarted to emerge already in 1921. First, introduce functional terms\nby the transfinite\n axiom[5] \nand define quantifiers by  \nand \nUsing the epsilon terms, quantifiers can now be eliminated from proofs\nin quantificational logic, thus transforming them into quantifier-free\nones. Finally, the given derivation allows one, so it was conjectured,\nto determine numerical values for the epsilon terms. In his Leipzig\ntalk of September 1922, published in 1923, Hilbert discussed this\nAnsatz for eliminating quantifiers and reducing the\ntransfinite case to that of the quantifier-free theory. He presented\nthe actual execution of this strategic plan only “for the\nsimplest case” (in Hilbert 1923: 1143–1144). However, the\ntalk was crucial in the development of proof theory and the finitist\nprogram: “With the structure of proof theory, presented to us in\nthe Leipzig talk, the principled form of its conception had been\nreached”. That is how Bernays characterizes its achievement in\nhis essay on Hilbert’s investigations of the foundations of\narithmetic (1935: 204) \nAckermann continued in section III of his 1925 at the very spot where\nHilbert and Bernays had left off. His paper, submitted to\nMathematische Annalen in March of 1924, and the corrective\nwork he did in 1925 led to the conviction that the consistency of\nelementary arithmetic had been established. The corrective work had\nbeen done to address difficulties von Neumann had pointed out, but was\nnot published by Ackermann; it was only presented in the second volume\nof Hilbert and Bernays 1939 (pp.\n 93–130).[6]\n Von Neumann’s own proof theoretic investigations, submitted to\nMathematische Zeitschrift in July 1925, were published under\nthe title Zur Hilbertschen Beweistheorie in 1927.\nHilbert’s 1928 Bologna Lecture prominently took\nAckermann’s and von Neumann’s work as having established\nthe consistency of elementary arithmetic, the proof making use only of\nfinitist principles. Let F be a theory containing exclusively\nsuch principles, like primitive recursive arithmetic\nPRA; the principles of PRA consist\nof the Peano axioms for zero and successor, the defining equations for\nall primitive recursive functions (defined in\n note 3),\n and quantifier-free induction. Now the significance of a consistency\nproof in F can be articulated as follows: \nTheorem 1.1 Let T be a theory that\ncontains a modicum of arithmetic and let A be a\n\\(\\Pi^0_1\\)-statement, i.e., one of the form \\(\\forall\nx_1\\ldots\\forall x_n\\,P(x_1,\\ldots,x_n)\\) with quantifiers ranging\nover naturals and P a primitive recursive predicate, i.e., a\npredicate with a primitive recursive characteristic function. If\nF proves the consistency of T and T proves\nA, then F proves A. \nThis theorem can be expressed and proved in PRA and\nensures that a T-proof of a “real”, finitistically\nmeaningful statement A leads to a finitistically valid statement.\nThis point is made clear in Hilbert’s 1927-Hamburg lecture\n(Hilbert 1927). There he takes A to be the Fermat proposition\nand argues that if we had a proof of A in a theory containing\n“ideal” elements, a finistist consistency proof for that\ntheory would allow us to transform that proof into a finitist one. \nThe belief that Ackermann and von Neumann had established the\nconsistency of elementary arithmetic was expressed as late as December\n1930 by Hilbert in his third Hamburg talk (Hilbert 1931a) and by Bernays in April 1931\nin a letter to Gödel (see Gödel 2003: 98–103). Bernays\nasserts there that he has “repeatedly considered\n[Ackermann’s modified proof] and viewed it as correct”. He\ncontinues, referring to Gödel’s incompleteness results, \nOn the basis of your results one must now conclude, however, that that\nproof cannot be formalized within the system Z [of elementary\nnumber theory]; this must in fact hold true even if the system is\nrestricted so that, of the recursive definitions, only those for\naddition and multiplication are retained. On the other hand, I\ndon’t see at which place in Ackermann’s proof the\nformalization within Z should become impossible, … \nAt the end of his letter, Bernays mentions that Herbrand misunderstood\nhim in a recent conversation on which Herbrand had reported in a\nletter to Gödel with a copy to Bernays. Not only had Herbrand\nmisunderstood Bernays, but Bernays had also misunderstood Herbrand as\nto the extent of the latter’s consistency result that was to be\npublished a few months later as Herbrand 1931. Bernays understood\nHerbrand as having claimed that he had established the consistency of\nfull first-order arithmetic: Herbrand’s system is indeed a\nfirst-order theory with a rich collection of finitist functions, but\nit uses the induction principle only for quantifier-free\n formulae.[7]\n Gödel asserted in December 1933 that this theorem of\nHerbrand’s was even then the strongest result that had been\nobtained in the pursuit of Hilbert’s finitist program, and he\nformulated the result in a beautiful informal way as follows: \nIf we take a theory, which is constructive in the sense that each\nexistence assertion made in the axioms is covered by a construction,\nand if we add to this theory the non-constructive notion of existence\nand all the logical rules concerning it, e.g., the law of the excluded\nmiddle, we shall never get into any contradiction. (Gödel 1933:\n52) \nGödel himself had been much more ambitious in early 1930; his\ngoal was then to prove the consistency of analysis! According to Wang\n(1981: 654), his idea was “to prove the consistency of analysis\nby number theory, where one can assume the truth of number theory, not\nonly the consistency”. The plan for establishing the consistency\nof analysis relative to number theory did not work out, instead\nGödel found that sufficiently strong formal theories like\nPrincipia Mathematica and Zermelo-Fraenkel set theory are\n(syntactically) incomplete. \nIn 1931 Gödel published a paper (1931a) that showed that there\nare true arithmetic statements that cannot be proved in the formal\nsystem of Principia Mathematica, assuming PM to be\nconsistent. His methods not only applied to PM but to any\nformal system that contains a modicum of arithmetic. A couple of\nmonths after Gödel had announced this result at a conference in\nKönigsberg in September 1930, von Neumann and Gödel\nindependently realized that a striking corollary could be drawn from\nthe incompleteness theorem. Every consistent and effectively\naxiomatized theory that allows for the development of basic parts of\narithmetic cannot prove its own consistency. This came to be known as\nthe second incompleteness theorem. (For details on these\ntheorems and their history see\n appendix A.4)\n The second incompleteness theorem refutes the general ambitions of\nHilbert’s program under the sole and very plausible assumption\nthat finitist mathematics is contained in one of the formal theories\nof arithmetic, analysis or set theory. As a matter of fact,\ncontemporary characterizations of finitist mathematics have elementary\narithmetic as an upper\n bound.[8]\n In response to Gödel’s result, Hilbert attempted in his\nlast published paper (1931b) to formulate a strategy for consistency\nproofs that is reminiscent of his considerations in the early 1920s\n(when thinking about the object theories as constructive) and clearly\nextends the finitist standpoint. He introduced a broad constructive\nframework that includes full intuitionist arithmetic and suggested\nextendibility of the new “method” to “the case of\nfunction variables and even higher sorts of variables”. He also\nformulated a new kind of rule that allowed the introduction of a new\naxiom \\(\\forall x A(x)\\) as soon as all the numerical instances\n\\(A(n)\\) had been established by finitist proofs; in 1931 that is done\nfor quantifier-free \\(A(x)\\), whereas in 1931b that is extended to\nformulae of arbitrary complexity. The semi-formal calculi, which\narticulate the broader framework, are based on rules that reflect\nmathematical practice, but also define the meaning of logical\nconnectives. Indeed, Hilbert’s reasons for taking them to be\nevidently consistent are expressed in a single sentence: “All\ntransfinite rules and inference schemata are consistent; for they\namount to definitions”. Adding the tertium non datur in\nthe form  \nyields now the classical version of the theory and it is that addition\nthat has to be\n justified.[9]\n Hilbert’s problematic considerations for this new\nmetamathematical step inspired Gentzen’s\n“Urdissertation” when he began working in late 1931 on a\nconsistency proof for elementary\n arithmetic.[10] \nAs part of his “Urdissertation”, Gentzen had established\nby the end of 1932 the reduction of classical to intuitionist\narithmetic, a result that had also been obtained by Gödel.\nGentzen’s investigations led, finally in 1935, to his first\nconsistency proof for arithmetic. In the background was a normal form\ntheorem for intuitionist logic that will be discussed in the next\nsection together with Gentzen’s actual dissertation and the\nspecial calculi he introduced there. Now we just formulate the\nGentzen-Gödel result “connecting” classical\nfirst-order number theory PA with its intuitionist\nversion HA. The non-logical principles of these\ntheories aim at describing \\(\\fN\\), the arguably most important\nstructure in mathematics, namely, Dedekind’s simply infinite\nsystem \\(\\bbN\\) together with zero, successor, multiplication,\nexponentiation and the less-than relation:  \nThey are formulated in the first-order language that has the relation\nsymbols =, <, the function symbols S, +, \\(\\times\\),\nE and the constant symbol 0. The axioms comprise the usual\nequations for zero, successor, addition, multiplication,\nexponentiation, and the less-than relation. In addition, the\ninduction principle is given by the schema \n\n\\[\\tag{IND}\n{F(0)} \\land {\\forall x[F(x)\\to F(Sx)]\\to \\forall x F(x)}\n\\]\n\n for\nall formulae \\(F(x)\\) of the language. These principles together with\nclassical logic constitute the theory of first order\narithmetic or first order number theory, also known as\nDedekind-Peano arithmetic, PA; together with\nintuitionist logic they constitute intuitionistic first order\narithmetic commonly known as Heyting-arithmetic,\nHA. \nNow we are considering the syntactic translation \\(\\tau\\) from the\ncommon language of PA and HA into\nits “negative” fragment that replaces disjunctions\n\\(A\\lor B\\) by their de Morgan equivalent \\(\\neg (\\neg A\\land \\neg\nB)\\) and existential statements \\(\\exists x A(x)\\) by \\(\\neg \\forall\nx \\neg A(x)\\). The reductive result obtained by Gentzen and Gödel\nin 1933 is now stated quite easily: \nTheorem 1.2 \nPA proves the equivalence of A and \\(\\tau(A)\\)\nfor any formula A. \nIf PA proves A, then HA\nproves \\(\\tau(A)\\). \nFor atomic sentences like \\(1\\ne 1\\) the translation \\(\\tau\\) is\nclearly the identity, and the provability of \\(1\\ne 1\\) in\nPA would imply its provability in\nHA. Thus, PA is consistent relative\nto HA. This result is technically of great interest\nand had a profound effect on the perspective concerning the\nrelationship between finitism and intuitionism: finitist and\nintuitionist mathematics were considered as co-extensional; this\ntheorem showed that intuitionist mathematics is actually stronger than\nfinitist mathematics. Thus, if the intuitionist standpoint is taken to\nguarantee the soundness of HA, then it guarantees the\nconsistency of PA. The corresponding connection\nbetween classical and intuitionist logic had been established already\nby Kolmogorov (1925) who not only formalized intuitionist logic but\nalso observed the translatability of classical into intuitionist\nlogic. His work, though, seems not to have been noticed at the time or\neven in 1932, when Gentzen and Gödel established their result for\nclassical and intuitionist arithmetic. \nThe foundational discussion concerning extended\n“constructive” viewpoints is taken up in\n section 4.\n There, and throughout our paper the concepts of\n“proof-theoretic reducibility” and “proof-theoretic\nequivalence” will play a central role. The connection between\nPA and HA is paradigmatic and leads\nto the notion of proof-theoretic reduction. Before we can\nfurnish a precise definition, we should perhaps stress that many\nconcepts can be expressed in the language of PRA (as\nwell as PA) via coding, also known as Gödel\nnumbering. Any finite object such as a string of symbols or an\narray of symbols can be coded via a single natural number in such a\nway that the string or array can be retrieved from the number when we\nknow how the coding is done. Typical finite objects include formulae\nin a given language and also proofs in a theory. Talk about formulae\nor proofs can then be replaced by talking about predicates of numbers\nthat single out the codes of formulae and proofs, respectively. We\nthen say that the concepts of formula and proof have been arithmetized\nand thereby rendered expressible in the language of\nPRA. \nDefinition 1.3 Let \\(\\bT_1\\), \\(\\bT_2\\) be a\npair of theories with languages \\(\\cL_1\\) and \\(\\cL_2\\), respectively,\nand let \\(\\Phi\\) be a (primitive recursive) collection of formulae\ncommon to both languages. Furthermore, \\(\\Phi\\) should contain the\nclosed equations of the language of PRA. \nWe then say that \\(\\bT_1\\) is proof-theoretically\n\\(\\Phi\\)-reducible to \\(\\bT_2\\), written\n\\(\\bT_1\\leq_{\\Phi}\\bT_2\\), if there exists a primitive recursive\nfunction f such that  \nHere \\(\\rform_{\\Phi}\\) and \\(\\proof_{\\bT_i}\\) are arithmetized\nformalizations of \\(\\Phi\\) and the proof relation in \\(\\bT_i\\),\nrespectively, i.e., \\(\\rform_{\\Phi}(x)\\) expresses that x is\nthe Gödel number of a formula in \\(\\Phi\\) while\n\\(\\proof_{\\bT_i}(y,x)\\) expresses that y codes a proof in\n\\(\\bT_i\\) of a formula with Gödel number x. \n\\(\\bT_1\\) and \\(\\bT_2\\) are said to be proof-theoretically\n\\(\\Phi\\)-equivalent, written \\(\\bT_1\\equiv_{\\Phi}\\bT_2\\), if\n\\(\\bT_1\\leq_{\\Phi}\\bT_2\\) and \\(\\bT_2\\leq_{\\Phi}\\bT_1\\). \nThe appropriate class \\(\\Phi\\) is revealed in the process of reduction\nitself, so that in the statement of theorems we simply say that\n\\(\\bT_1\\) is proof-theoretically reducible to \\(\\bT_2\\)\n(written \\(\\bT_1\\leq \\bT_2\\)) and \\(\\bT_1\\) and \\(\\bT_2\\) are\nproof-theoretically equivalent (written \\(\\bT_1 \\equiv\n\\bT_2\\)), respectively. Alternatively, we shall say that \\(\\bT_1\\) and\n\\(\\bT_2\\) have the same proof-theoretic strength when\n\\(\\bT_1\\equiv \\bT_2\\). In practice, if \\(\\bT_1\\equiv \\bT_2\\) is shown\nvia proof-theoretic\n means[11]\n this always entails that the two theories prove at least the same\n\\(\\Pi^0_2\\) sentences (those of the complexity of the twin prime\nconjecture). The complexity of formulae of PRA is\nstratified as follows. The \\(\\Sigma^0_0\\) and \\(\\Pi^0_0\\) formulae are\nof the form \\(R(t_1,\\ldots,t_n)\\) where R is a predicate symbol\nfor an n-ary primitive recursive predicate. A formula is\n\\(\\Sigma^0_{k+1}\\) (\\(\\Pi^0_{k+1}\\)) if it is of the form  \nwith \\(F(y_1,\\ldots,y_m)\\) being of complexity \\(\\Pi^0_k\\)\n(\\(\\Sigma^0_k\\)). Thus the complexity of a formula is measured in\nterms of quantifier alternations. For instance \\(\\Pi^0_2\\)-formulae\nhave two alternations starting with a block of universal quantifiers\nor more explicitly they are of the shape  \nwith R primitive recursive. \nFor the reduction of classical elementary number theory to its\nintuitionist version, Gödel and Gentzen used different logical\ncalculi. Gödel used the system Herbrand had investigated in his\n1931, whereas Gentzen employed the formalization of intuitionist\narithmetic from Heyting 1930. For his further finitist investigations\nGentzen introduced new calculi that were to become of utmost\nimportance for proof theory: natural deduction and sequent\ncalculi. \nAs we noted above, Gentzen had already begun in 1931 to be concerned\nwith the consistency of full elementary number theory. As the logical\nframework he used, what we now call, natural deduction calculi. They\nevolved from an axiomatic calculus that had been used by Hilbert and\nBernays since 1922 and introduced an important modification of the\ncalculus for sentential logic. The connectives \\(\\land \\) and \\(\\lor\\)\nare incorporated, and the axioms for these connectives are as follows:\n \nHilbert and Bernays introduced this new logical formalism for two\nreasons, (i) to be able to better and more easily formalize\nmathematics, and (ii) to bring out the understanding of logical\nconnectives in methodological parallel to the treatment of geometric\nconcepts in Foundations of geometry. The methodological\nadvantages of this calculus are discussed in Bernays 1927: 10: \nThe starting formulae can be chosen in quite different ways. A great\ndeal of effort has been spent, in particular, to get by with a minimal\nnumber of axioms, and the limit of what is possible has indeed been\nreached. However, for the purposes of logical investigations it is\nbetter to separate out, in line with the axiomatic procedure for\ngeometry, different axiom groups in such a way that each of them\nexpresses the role of a single logical operation. \nThen Bernays lists four groups, namely, axioms for the conditional\n\\(\\to\\), for \\(\\land \\) and \\(\\lor\\) as above, and for negation\n\\(\\neg\\). The axioms for the conditional are not only reflecting\nlogical properties, but also structural features as in the later\nsequent calculus (and in Frege’s Begriffsschrift,\n1879).  \nAs axioms for negation one can choose:  \nHilbert formulates this logical system in Über das\nUnendliche and in his second Hamburg talk of 1927, but gives a\nslightly different formulation of the axioms for negation, calling\nthem the principle of contradiction and the principle of\ndouble negation:  \nClearly, the axioms correspond directly to the natural deduction rules\nfor these connectives, and one finds here the origin of\nGentzen’s natural deduction calculi. Bernays had investigated in\nhis Habilitationsschrift (1918) rule based calculi. However,\nin the given context, the simplicity of the metamathematical\ndescription of calculi seemed paramount, and in Bernays 1927 (p. 17)\none finds the programmatic remark: “We want to have as few rules\nas possible, rather put much into axioms”. \nGentzen was led to a rule-based calculus with introduction\nand elimination rules for every logical connective. The truly\ndistinctive feature of this new type of calculus was for Gentzen,\nhowever, making and discharging assumptions. This feature, he\nremarked, most directly reflects a crucial aspect of mathematical\n argumentation.[12]\n Here we formulate the distinctive rules that involve contradictions\nand go beyond minimal logic that has I- and E-rules for all logical\nconnectives. Intuitionist logic is obtained from minimal logic by\nadding the rule: from \\(\\perp\\) infer any formula A, i.e.,\nex falso quodlibet. In the case of classical logic, if a\nproof of \\(\\perp\\) is obtained from the assumption \\(\\neg A\\), then\ninfer A (and cancel the assumption \\(\\neg A\\)). Gentzen\ndiscovered a remarkable fact for the intuitionist calculus, having\nobserved that proofs can have peculiar detours of the\nfollowing form: a formula is obtained by an I-rule and is then the\nmajor premise of the corresponding E-rule. For conjunction such a\ndetour is depicted as follows:  \nClearly, a proof of B is already contained in the given\nderivation. Proofs without detours are called normal, and\nGentzen showed that any proof can be effectively transformed via\n“reduction steps” into a normal one. \nTheorem 2.1 (Normalization for intuitionist\nlogic) A proof of A from a set of assumptions \\(\\Gamma\\)\ncan be transformed into a normal proof of A from the same set\nof assumptions. \nFocusing on normal proofs, Gentzen proved then that the complexity of\nformulae in such proofs can be bounded by that of assumptions and\nconclusion. \nCorollary 2.2 (Subformula property) If\n\\(\\cD\\) is a normal proof of A from \\(\\Gamma\\), then every\nformula in \\(\\cD\\) is either a subformula of an element \\(\\Gamma\\) or\nof A. \nAs Gentzen recounts matters at the very beginning of his dissertation\n(1934/35), he was led by the\ninvestigation of the natural calculus to his\n Hauptsatz[13]\n when he could not extend the considerations to classical logic. \nTo be able to formulate it [the Hauptsatz] in a direct way, I\nhad to base it on a particularly suitable logical calculus. The\ncalculus of natural deduction turned out not to be appropriate for\nthat purpose. \nSo, Gentzen focused his attention on sequent calculi that had been\nintroduced by Paul Hertz and which had been the subject of\nGentzen’s first scientific paper (1932). \nIn his thesis Gentzen introduced a form of the sequent calculus and\nhis technique of cut elimination. As this is a tool of utmost\nimportance in proof theory, an outline of the underlying ideas will be\ndiscussed next. The sequent calculus can be generalized to so-called\ninfinitary logics and is central for ordinal analysis. The\nHauptsatz is also called the cut elimination\ntheorem. \nWe use upper case Greek letters\n\\(\\Gamma,\\Delta,\\Lambda,\\Theta,\\Xi\\ldots\\) to range over finite lists\nof formulae. \\(\\Gamma\\subseteq \\Delta\\) means that every formula of\n\\(\\Gamma\\) is also a formula of \\(\\Delta\\). A sequent is an\nexpression \\(\\Gamma\\Rightarrow \\Delta\\) where \\(\\Gamma\\) and\n\\(\\Delta\\) are finite sequences of formulae \\(A_1,\\ldots,A_n\\) and\n\\(B_1,\\ldots, B_m\\), respectively. We also allow for the possibility\nthat \\(\\Gamma\\) or \\(\\Delta\\) (or both) are empty. The empty sequence\nwill be denoted by \\(\\emptyset\\). \\(\\Sigma \\Rightarrow \\Delta\\) is\nread, informally, as \\(\\Gamma\\) yields \\(\\Delta\\) or, rather, the\nconjunction of the \\(A_i\\) yields the disjunction of\nthe \\(B_j\\). \nThe logical axioms of the calculus are of the form  \nwhere A is any formula. In point of fact, one could limit this\naxiom to the case of atomic formulae A. We have structural\nrules of the form  \nA special case of the structural rule, known as contraction,\noccurs when the lower sequent has fewer occurrences of a formula than\nthe upper sequent. For instance, \\(A, \\Gamma\\Rightarrow\\Delta, B\\)\nfollows structurally from \\(A,A,\\Gamma \\Rightarrow \\Delta,B,B\\). \nNow we list the rules for the logical connectives.  \nIn \\(\\forall L\\) and \\(\\exists R\\), t is an arbitrary term. The\nvariable a in \\(\\forall R\\) and \\(\\exists L\\) is an\neigenvariable of the respective inference, i.e., a is\nnot to occur in the lower sequent. \nFinally, we have the special Cut Rule \nThe formula A is called the cut formula of the\ninference. \nIn the rules for logical operations, the formulae highlighted in the\npremises are called the minor formulae of that inference,\nwhile the formula highlighted in the conclusion is the principal\nformula of that inference. The other formulae of an inference are\ncalled side formulae. A proof (also known as a\ndeduction or derivation) \\(\\cD\\) is a tree of\nsequents satisfying the conditions that (i) the topmost sequents of\n\\(\\cD\\) are logical axioms and (ii) every sequent in \\(\\cD\\) except\nthe lowest one is an upper sequent of an inference whose lower sequent\nis also in \\(\\cD\\). A sequent \\(\\Gamma \\Rightarrow \\Delta\\) is\ndeducible if there is a proof having \\(\\Gamma \\Rightarrow\n\\Delta\\) as its bottom sequent. \nThe Cut rule differs from the other rules in an important respect.\nWith the rules for introducing connectives, one sees that every\nformula that occurs above the line occurs below the line either\ndirectly, or as a subformula of a formula below the line. That is also\ntrue for the structural rules. (Here \\(A(t)\\) is counted as a\nsubformula, in a slightly extended sense, of both \\(\\exists xA(x)\\)\nand \\(\\forall xA(x)\\).) But in the case of the Cut rule, the cut\nformula A vanishes. Gentzen showed that such “vanishing\nrules” can be eliminated. \nTheorem 2.3 (Gentzen’s\nHauptsatz) If a sequent \\(\\Gamma \\Rightarrow \\Delta\\)\nis deducible, then it is also deducible without the Cut rule; the\nresulting proof is called cut-free or normal. \nThe secret to Gentzen’s Hauptsatz is the symmetry of\nleft and right rules for all the logical connectives including\nnegation. The proof of the cut elimination theorem is rather intricate\nas the process of removing cuts interferes with the structural rules.\nIt is contraction that accounts for the high cost of eliminating cuts.\nLet \\(\\lvert \\cD\\rvert\\) be the height of the deduction \\(\\cD\\) and let\n\\(rank(\\cD)\\) be the supremum of the lengths of cut\nformulae occurring in \\(\\cD\\). Turning \\(\\cD\\) into a cut-free\ndeduction of the same end sequent results, in the worst case, in a\ndeduction of height \\(\\cH(rank(\\cD),\\lvert \\cD\\rvert)\\) where \\(\\cH(0,n)=n\\) and\n\\(\\cH(k+1,n)=4^{\\cH(k,n)}\\), yielding hyper-exponential growth. \nThe sequent calculus we have been discussing allows the proof of\nclassically, but not intuitionistically correct formulae, for example,\nthe law of excluded middle. An intuitionist version of the sequent\ncalculus can be obtained by a very simple structural restriction:\nthere can be at most one formula on the right hand side of the sequent\nsymbol \\(\\Rightarrow\\). The cut elimination theorem is also provable\nfor this intuitionist variant. In either case, the Hauptsatz\nhas an important corollary that parallels that of the Normalization\ntheorem (for intuitionist logic) and expresses the subformula\nproperty. \nCorollary 2.4 (Subformula property) If\n\\(\\cD\\) is a cut-free proof of the sequent\n\\(\\Gamma\\Rightarrow\\Delta\\), then all formulae in \\(\\cD\\) are\nsubformulae of elements in either \\(\\Gamma\\) or \\(\\Delta\\). \nThis Corollary has another direct consequence that explains the\ncrucial role of the Hauptsatz for obtaining consistency\nproofs. \nCorollary 2.5 (Consistency) A contradiction,\ni.e., the empty sequent \\(\\emptyset \\Rightarrow \\emptyset\\), is not\nprovable. \nProof: Assume that the empty sequent is provable;\nthen, according to the Hauptsatz it has a cut-free derivation\n\\(\\cD\\). The previous corollary assures us that only empty sequents\ncan occur in \\(\\cD\\); but such a \\(\\cD\\) does not exist since every\nproof must contain axioms. \\(\\qed\\) \nThe foregoing results are solely concerned with pure logic. Formal\ntheories that axiomatize mathematical structures or serve as formal\nframeworks for developing substantial chunks of mathematics are based\non logic but have additional axioms germane to their purpose. If they\nare of the latter kind, such as first-order arithmetic or\nZermelo-Fraenkel set theory, they will assert the existence\nof mathematical objects and their properties. What happens\nwhen we try to apply the procedure of cut elimination to theories?\nAxioms are usually detrimental to this procedure. It breaks down\nbecause the symmetry of the sequent calculus is lost. In general, one\ncannot remove cuts from deductions in a theory T when the cut\nformula is an axiom of T. However, sometimes the axioms of a\ntheory are of bounded syntactic complexity. Then the\nprocedure applies partially in that one can remove all cuts that\nexceed the complexity of the axioms of T. This gives rise to\npartial cut elimination. It is a very important tool in proof\ntheory. For example, it can be used to analyze theories with\nrestricted induction (such as fragments of PA; cf.\nSieg 1985). It also works very well if the axioms of a theory can be\npresented as atomic intuitionist sequents (also called\nHorn clauses), yielding the completeness of\nRobinson’s resolution method (see Harrison\n2009). \nUsing the Hauptsatz and its Corollary, Gentzen was able to\ncapture all of the consistency results that had been obtained prior to\n1934 including Herbrand’s, that had been called by Gödel in\nhis 1933 “the most far-reaching” result. They had been\nobtained at least in principle for fragments of elementary number\ntheory; in practice, Gentzen did not include the quantifier-free\ninduction principle. Having completed his dissertation, Gentzen went\nback to investigate natural deduction calculi and obtained in 1935 his\nfirst consistency proof for full first-order arithmetic. He\nformulated, however, the natural calculus now in “sequent\nform”: instead of indicating the assumptions on which a\nparticular claim depended by the undischarged ones in its proof tree,\nthey are attached now to every node of the tree. The\n“sequents” that are being proved are of the form \\(\\Gamma\n\\Rightarrow A\\), where all the logical inferences are carried out on\nthe right hand side. This proof was published only in 1974; it was\nsubsequently analyzed most carefully in Tait 2015 and Buchholz 2015.\nIt was due to criticism of Bernays and Gödel that Gentzen\nmodified his consistency proof quite dramatically; he made use of\ntransfinite induction, as will be discussed in detail in the next\nsection. Here we just mention that Bernays extensively discussed\ntransfinite induction in Grundlagen der Mathematik II. The\nmain issue for Bernays was the question, is it still a finitist\nprinciple?—Bernays did not discuss, however, two other\naspects of Gentzen’s work, namely, the use of structural\nfeatures of formal proofs for consistency proofs and the attempt of\ngaining a constructive, semantic understanding of intuitionist\narithmetic. The former became crucial for proof theoretic\ninvestigations; the latter influenced Gödel and his functional\ninterpretation via computable functionals of finite\n type.[14]\n The two aspects together opened a new era for proof theory and\nmathematical logic with the goal of proving the consistency of\nanalysis. We will see, how far current techniques lead us and what\nfoundational significance one can attribute to them. \nCut elimination fails for first-order arithmetic (i.e.,\nPA), not even partial cut elimination is possible\nsince the induction axioms have unbounded complexity. Gentzen,\nhowever, found an ingenious way of dealing with purported\ncontradictions in arithmetic. In Gentzen 1938b he showed how to effectively\ntransform an alleged PA-proof of an inconsistency\n(the empty sequent) in his sequent calculus into another proof of the\nempty sequent such that the latter gets assigned a smaller ordinal\nthan the former. Ordinals are a central concept in set theory as well\nas in proof theory. To present Gentzen’s work we shall first\ndiscuss the notion of ordinal from a proof-theoretic point of\nview. \nThis is the first time we talk about the transfinite and ordinals in\nproof theory. Ordinals have become very important in advanced proof\ntheory. The concept of an ordinal is a generalization of that of a\nnatural number. The latter are used for counting finitely many things\nwhereas ordinals can also “count” infinitely many things.\nIt is derived from the concept of an ordering \\(\\prec\\) of a set\nX which arranges the objects of X in order, one after\nanother, in such a way that for every predicate P on X\nthere is always a first element of X with respect to \\(\\prec\\)\nthat satisfies P if there is at least one object in X\nsatisfying P. Such an ordering is called a\nwell-ordering of X. Certainly the usual less-than\nrelation on \\(\\bbN\\) is a well-ordering. Here every number \\(\\ne 0\\)\nis the successor of another number. If one orders the natural numbers\n\\(>0\\) in the usual way but declares that 0 is bigger than every\nnumber \\(\\ne 0\\) one arrives at another ordering of \\(\\bbN\\).\nLet’s call it \\(\\prec\\). \\(\\prec\\) is also a well-ordering of\n\\(\\bbN\\). This time 1 is the least number with respect to \\(\\prec\\).\nHowever, 0 plays a unique role. There are infinitely many numbers\n\\(\\prec 0\\) and there is no number \\(m\\prec 0\\) such that 0 is the\nnext number after m. Such numbers are are called limit numbers\n(with respect to \\(\\prec\\)). \nIn order to be able to formulate Gentzen’s results from the end\nof section 3.3, we have to “arithmetize” the treatment of\nordinals. Let us first state some precise definitions and a Cantorian\ntheorem. \nDefinition 3.1 A non-empty set A\nequipped with a total ordering \\(\\prec\\) (i.e., \\(\\prec\\) is\ntransitive, irreflexive, and  \nis a well-ordering if every non-empty subset X of\nA contains a \\(\\prec\\)-least element, i.e., \nThe elements of a well-ordering \\((A,\\prec)\\) can be divided into\nthree types. Since A is non-empty there is least element with\nrespect to \\(\\prec\\) which is customarily denoted by 0 or \\(0_A\\).\nThen there are elements \\(a\\in A\\) such that there exists \\(b\\prec a\\)\nbut there is no c between b and a. These are the\nsuccessor elements of A, with a being the successor of\nb. An element \\(c\\in A\\) such that \\(0\\prec c\\) and for all\n\\(b\\prec c\\) there exists \\(d\\in A\\) with \\(b\\prec d\\prec c\\) is said\nto be a limit element of A. \nIn set theory a set is called transitive just in case all its\nelements are also subsets. An ordinal in the set-theoretic\nsense is a transitive set that is well-ordered by the elementhood\nrelation \\(\\in\\). It follows that each ordinal is the set of\npredecessors. According to the trichotomy above, there is a least\nordinal (which is just the empty set) and all other ordinals are\neither successor or limit ordinals. The first limit ordinal is denoted\nby \\(\\omega\\). \nFact 3.2 Every well-ordering \\((A,\\prec)\\)\nis order isomorphic to a unique ordinal \\((\\alpha,\\in)\\). \nOrdinals are traditionally denoted by lower case Greek letters\n\\(\\alpha,\\beta,\\gamma,\\delta,\\ldots\\) and the relation \\(\\in\\) on\nordinals is notated simply by \\(<\\). If \\(\\beta \\) is a successor\nordinal, i.e., \\(\\beta\\) is the successor of some (necessarily unique)\nordinal \\(\\alpha\\) we also denotes \\(\\beta\\) by \\(\\alpha'\\).\nAnother important fact is that for any family of ordinals \\(\\alpha_i\\)\nfor \\(i\\in I\\) (I some set) there is a smallest ordinal,\ndenoted by \\(\\sup_{i\\in I}\\alpha_i\\) that is bigger than every ordinal\n\\(\\alpha_i\\). \nThe operations of addition, multiplication, and exponentiation can be\ndefined on all ordinals by using case distinctions and transfinite\nrecursion (on \\(\\alpha\\)). The following states the definitions just\nto convey the flavor:  \nHowever, addition and multiplication are in general not\ncommutative. \nWe are interested in representing specific ordinals \\(\\alpha\\) as\nrelations on \\(\\bbN\\). In essence Cantor defined the first ordinal\nrepresentation system in 1897. Natural ordinal representation\nsystems are frequently derived from structures of the form  \nwhere \\(\\alpha\\) is an ordinal, \\(<_{\\alpha}\\) is the ordering of\nordinals restricted to elements of \\(\\alpha\\) and the \\(f_i\\) are\nfunctions  \nfor some natural number \\(k_i\\).  \nis a computable (or recursive)\nrepresentation of \\(\\frakA\\) if the following conditions\nhold: \n\\(A\\subseteq\\bbN\\) and A is a computable set. \n\\(\\prec\\) is a computable total ordering on A and the functions\n\\(g_i\\) are computable. \n\\(\\frakA \\cong\\bbA\\), i.e., the two structures are\nisomorphic. \nTheorem 3.3 (Cantor 1897) For every ordinal\n\\(\\beta>0\\) there exist unique ordinals\n\\(\\beta_0\\geq\\beta_1\\geq\\dots\\geq\\beta_n\\) such that  \nThe representation of \\(\\beta\\) in (4) is called the Cantor normal\nform. We shall write \\(\\beta \\mathbin{=_{\\sCNF}}\n\\omega^{\\beta_1}+\\cdots +\\omega^{\\beta_n}\\) to convey that\n\\(\\beta_0\\geq\\beta_1\\geq\\dots\\geq\\beta_k\\). \nThe rather famous ordinal that emerged in Gentzen’s consistency\nproof of PA is denoted by \\(\\varepsilon_0\\). It\nrefers to first ordinal \\(\\alpha>0\\) such that \\((\\forall\n\\beta<\\alpha)\\,\\omega^{\\beta}<\\alpha\\). \\(\\varepsilon_0\\) can\nalso be described as the least ordinal \\(\\alpha\\) such that\n\\(\\omega^{\\alpha}=\\alpha\\). \nOrdinals \\(\\beta<\\varepsilon_0\\) have a Cantor normal form with\nexponents \\(\\beta_i<\\beta\\) and these exponents have Cantor normal\nforms with yet again smaller exponents. As this process must\nterminate, ordinals \\(<\\varepsilon_0\\) can be coded by natural\nnumbers. For instance a coding function  \ncould be defined as follows:  \nwhere \\(\\langle k_1,\\cdots,k_n\\rangle\\coloneqq\n2^{k_1+1}\\cdot\\ldots\\cdot p_n^{k_n+1}\\) with \\(p_i\\) being the\nith prime number (or any other coding of tuples). Further\ndefine:  \nThen  \n\\(A_0,\\hat{+},\\hat{\\cdot},x\\mapsto\\hat{\\omega}^{x},\\prec\\) are\nprimitive recursive. Finally, we can spell out the scheme\nPR-TI\\((\\varepsilon_0)\\) in the language of PA:  \nfor all primitive recursive predicates P. \nGiven a natural ordinal representation system \\(\\langle\nA,\\prec,\\ldots\\rangle\\) of order type \\(\\tau\\) let\n\\(\\PRA+\\rTI_{\\qf}(<\\tau)\\) be PRA augmented by\nquantifier-free induction over all initial (externally indexed)\nsegments of \\(\\prec\\). This is perhaps best explained via the\nrepresentation system for \\(\\varepsilon_0\\) given above. There one can\ntake the initial segments of \\(\\prec\\) to be determined by the\nGödel numbers of the ordinals \\(\\omega_0\\coloneqq 1\\) and\n\\(\\omega_{n+1}\\coloneqq \\omega^{\\omega_n}\\) whose limit is\n\\(\\varepsilon_0\\). \nDefinition 3.4 We say that a theory T\nhas proof-theoretic ordinal \\(\\tau\\), written \\(\\lvert T\\rvert =\\tau\\),\nif T can be proof-theoretically reduced to\n\\(\\PRA+\\rTI_{\\qf}(<\\tau)\\), i.e.,  \nUnsurprisingly, the above notion has certain intensional aspects and\nhinges on the naturality of the representation system (for a\ndiscussion see Rathjen 1999a: section 2.).  \nGentzen’s consistency proof for PA employs a\nreduction procedure \\(\\cR\\) on proofs P of the empty sequent\ntogether with an assignment ord of representations for ordinals\nto proofs such that \\(\\ord(\\cR(P))< \\ord(P)\\). Here \\(<\\)\ndenotes the ordering on ordinal representations induced by the\nordering of the pertinent ordinals. For this purpose he needed\nrepresentations for ordinals \\(<\\varepsilon_0\\) where\n\\(\\varepsilon_0\\) is the smallest ordinal \\(\\tau\\) such that whenever\n\\(\\alpha<\\tau\\) then also \\(\\omega^{\\alpha}<\\tau\\) with\n\\(\\alpha\\mapsto \\omega^{\\alpha}\\) being the function of ordinal\nexponentiation with base \\(\\omega\\). Moreover, the functions \\(\\cR\\)\nand ord and the relation \\(<\\) are computable (when viewed\nas acting on codes for the syntactic objects), they can be chosen to\nbe primitive recursive. With \\(g(n)= \\ord(\\cR^n(P))\\), the\nn-fold iteration of \\(\\cR\\) applied to P, one has\n\\(g(0)>g(1)> g(2)> \\ldots > g(n)\\) for all n, which\nis absurd as the ordinals \\(<\\varepsilon_0\\) are well-founded.\nHence PA is consistent. \nGentzen’s proof, though elementary, was very intricate and thus\nmore transparent proofs were sought. As it turned out, the obstacles\nto cut elimination, inherent to PA, could be overcome\nby moving to a richer proof system, albeit in a drastic way by going\ninfinite. This richer system allows for proof rules with\ninfinitely many\n premises.[15]\n The inference commonly known as the \\(\\omega\\)-rule consists\nof the two types of infinitary inferences: \nThe price to pay will be that deductions become infinite objects,\ni.e., infinite well-founded trees. \nThe sequent-style version of Peano arithmetic with the\n\\(\\omega\\)-rule will be denoted by \\(\\PA_{\\omega}\\).\n\\(\\PA_{\\omega}\\) has no use for free variables. Thus free variables\nare discarded and all terms will be closed. All formulae of\nthis system are therefore closed, too. The numerals are the\nterms \\(\\bar{n}\\), where \\(\\bar{0}=0\\) and\n\\(\\overline{n+1}=S\\bar{n}\\). We shall identify \\(\\bar{n}\\) with the\nnatural number n. All terms t of \\(\\PA_{\\omega}\\)\nevaluate to a numeral \\(\\bar{n}\\). \n\\(\\PA_{\\omega}\\) has all the inference rules of the sequent calculus\nexcept for \\(\\forall R\\) and \\(\\exists L\\). In their stead,\n\\(\\PA_{\\omega}\\) has the \\(\\omega R\\) and \\(\\omega L\\) inferences. The\nAxioms of \\(\\PA_{\\omega}\\) are the following: (i)\n\\(\\emptyset\\Rightarrow A\\) if A is a true atomic\nsentence; (ii) \\(B\\Rightarrow \\emptyset\\) if B is a\nfalse atomic sentence; (iii) \\(F(s_1,\\ldots,s_n)\\Rightarrow\nF(t_1,\\ldots,t_n)\\) if \\(F(s_1,\\ldots,s_n)\\) is an atomic sentence and\nthe \\(s_i\\) and \\(t_i\\) evaluate to the same numerals,\nrespectively. \nWith the aid of the \\(\\omega\\)-rule, each instance of the induction\nscheme becomes logically deducible with an infinite proof tree. To\ndescribe the cost of cut elimination for \\(\\PA_{\\omega}\\), we\nintroduce the measures of height and cut rank of a\n\\(\\PA_{\\omega}\\) deduction \\(\\cD\\). We will notate this by  \nThe above relation is defined inductively following the buildup of the\ndeduction \\(\\cD\\). For the cut rank we need the definition of\nthe length, \\(\\lvert A\\rvert\\) of a formula:  \nwhere \\(\\Box=\\land,\\lor,\\to\\); \\(\\lvert \\exists x\\,F(x)\\rvert =\\lvert \\forall\nx\\,F(x)\\rvert =\\lvert F(0)\\rvert +1\\). \nNow suppose the last inference I of \\(\\cD\\) is of the form  \nwhere \\(\\tau=1,2,\\omega\\) and the \\(\\cD_n\\) are the immediate\nsubdeductions of \\(\\cD\\). If  \nand \\(\\alpha_n<\\alpha\\) for all \\(n<\\tau\\) then  \nproviding that in the case of I being a cut with cut\nformula A we also have \\(\\lvert A\\rvert <k\\). We will write\n\\({\\PA_{\\omega} \\stile{\\alpha}{k} \\Gamma} \\Rightarrow \\Delta\\) to\nconvey that there exists a \\(\\PA_{\\omega}\\)-deduction\n\\({\\cD\\stile{\\alpha}{k} \\Gamma}\\Rightarrow \\Delta\\). The ordinal\nanalysis of PA proceeds by first unfolding any\nPA-deduction into a \\(\\PA_{\\omega}\\)-deduction:  \nfor some \\(m,k<\\omega\\). The next step is to get rid of the cuts.\nIt turns out that the cost of lowering the cut rank from \\(k+1\\) to\nk is an exponential with base \\(\\omega\\). \nTheorem 3.5 (Cut Elimination for\n\\(\\PA_{\\omega}\\)) If \\({\\PA_{\\omega} \\stile{\\alpha}{k+1}\n\\Gamma} \\Rightarrow \\Delta\\), then \nAs a result, if \\({\\PA_{\\omega} \\stile{\\alpha}{n} \\Gamma}\\Rightarrow\n\\Delta\\), we may apply the previous theorem n times to arrive\nat a cut-free deduction \\({\\PA_{\\omega} \\stile{\\rho}{0} \\Gamma}\n\\Rightarrow \\Delta\\) with\n\\(\\rho=\\omega^{\\omega^{\\iddots^{\\omega^{\\alpha}}}}\\), where the stack\nhas height n. Combining this with the result from\n\\((\\ref{einbett})\\), it follows that every sequent \\(\\Gamma\\Rightarrow\n\\Delta\\) deducible in PA has a cut-free deduction in\n\\(\\PA_{\\omega}\\) of length \\(<\\varepsilon_0\\). Ruminating on the\ndetails of how this result was achieved yields a consistency proof for\nPA from transfinite induction up to \\(\\varepsilon_0\\)\nfor elementary decidable predicates on the basis of finitist reasoning\n(as described below). \nGentzen did not deal explicitly with infinite proof trees in his\nsecond published proof of the consistency of PA\n(Gentzen 1938b). However, in\nthe unpublished first consistency proof of Gentzen 1974 he aims at\nshowing that a proof of a sequent in first-order arithmetic gives rise\nto a a well-founded reduction tree; that tree can be identified with a\ncut-free proof in the sequent calculus with the \\(\\omega\\)-rule. The\ninfinitary version of PA with the \\(\\omega\\)-rule was\ninvestigated by Schütte (1950). There remained the puzzle, how\nGentzen’s work that used an ingenious method of assigning\nordinals to purported proofs of the empty sequent relates to the\ninfinitary approach. Much later work by Buchholz (1997) and others\nrevealed an intrinsic connection between Gentzen’s assignment of\nordinals to deductions in PA and the standard one to\ninfinite deductions in \\(\\PA_{\\omega}\\). In the 1950s infinitary proof\ntheory flourished in the hands of Schütte. He extended his\napproach to PA to systems of ramified analysis to be\ndiscussed below in\n section 5.2. \nOne last remark about the use of ordinals: Gentzen showed that\ntransfinite induction up to the ordinal \\(\\varepsilon_0\\) suffices to\nprove the consistency of PA. Using the arithmetized\nformalization of the proof predicate (see above, after\n Definition 1.3)\n and taking k as the numeral denoting the Gödel number of\nthe formula \\(0=1\\), we can express the consistency of\nPA, \\(\\Con (\\PA)\\), by the formula \\(\\forall x\n\\,\\neg\\proof_{\\PA}(x,k)\\). To appreciate Gentzen’s result it is\npivotal to note that he applied transfinite induction up to\n\\(\\varepsilon_0\\) only for primitive recursive predicates (the latter\nprinciple was denoted above by PR-TI\\((\\varepsilon_0)\\)). Otherwise,\nGentzen’s proof used only finistist means. Hence, a more\naccurate formulation of Gentzen’s result is  \nwhere F, as above, contains only finitistically acceptable\nmeans. In his 1943 paper Gentzen also showed that this result is best\npossible, as PA proves transfinite induction up to\nany \\(\\alpha<\\varepsilon_0\\). So one might argue that the\nnon-finitist part of PA is encapsulated in\nPR-TI\\((\\varepsilon_0)\\) and therefore “measured” by\n\\(\\varepsilon_0\\). \\(\\varepsilon_0\\) is also the proof-theoretic\nordinal of PA as specified in\n Definition 3.4.\n Gentzen hoped that results of this character could also be obtained\nfor stronger mathematical theories, in particular for analysis.\nHilbert’s famous second problem asked for a direct consistency\nproof of that mathematical theory. Gentzen wrote in 1938 that \nthe most important [consistency] proof of all in practice, that for\nanalysis, is still outstanding. (1938a [Gentzen 1969: 236]).\n \nHe actually worked on a consistency proof for analysis as letters \n(e.g. one to Bernays on 23.6.1935 translated in von Plato 2017: 240) and stenographic\nnotes from 1945 (e.g., Gentzen 1945) show. Formally, “analysis” was identified\nalready in Hilbert 1917/18 as a form of second order number theory.\nHilbert and Bernays developed mathematical analysis in a supplement of\nthe second volume of their “Grundlagen der Mathematik”. We\ntake \\(\\bZ_2\\) as given in the following way. Its language extends\nthat of PA by an additional sort of variables\n\\(X,Y,Z,\\ldots\\) that range over sets of numbers and the binary\nmembership relation \\(t\\in X\\). Its axioms are those of\nPA, but the principle of proof by induction is\nformulated as the second order induction axiom  \nFinally, the axiom schema of comprehension,\nCA, asserts that for every formula \\({F}(u)\\) of the\nlanguage of \\(\\bZ_2\\), there is a set \\(X=\\{u\\mid {F}(u)\\}\\) having\nexactly those numbers u as members that satisfy \\({F}(u)\\).\nMore formally,  \nfor all formulae \\({F}(u)\\) in which X does not occur. That\n\\(\\bZ_2\\) is often called “analysis” is due to the\nrealization (e.g., in Hilbert & Bernays 1939) that, via the coding\nof real numbers and continuous functions as sets of natural numbers, a\ngood theory of the continuum can be developed from these axioms. \nModern analyses of “finitist mathematics” consider it as\nsituated between PRA and PA. When\narguing that Gödel’s second incompleteness theorem refutes\nHilbert’s finitist program, von Neumann argued that finitist\nmathematics is included in PA and, if not there,\nundoubtedly in \\(\\bZ_2\\). So it is quite clear that a consistency\nproof of \\(\\bZ_2\\) would use non-finitist principles or that the\npursuit of the consistency program would require an extension of the\nfinitist standpoint. In the next section we discuss briefly a variety\nof extensions and elaborate two in greater detail. \nAccording to Bernays, the reductive result due to Gödel and\nGentzen,\n Theorem 1.2,\n has a dramatic impact on the work concerned with Hilbert’s\nprogram. It opened in a very concrete and precise way the finitist\nperspective to a broader “constructive” one. Hilbert\nhimself had taken such a step in a much vaguer way in his last paper\n(Hilbert 1931b).\n Theorem 1.2\n showed, after all, that PA is contained in\nHA via the negative translation. Since\nHA comprises just a fragment of Brouwer’s\nintuitionism, the consistency of PA is secured on the\nbasis of the intuitionist standpoint. In a letter to Heyting of 25\nFebruary 1933, Gentzen suggested investigating the consistency of\nHA since a consistency proof for classical arithmetic\nhad not been given so far by finitist means. He then continued \nIf on the other hand, one admits the intuitionistic position as a\nsecure basis in itself, i.e., as a consistent one, the consistency of\nclassical arithmetic is secured by my result. If one wished to satisfy\nHilbert’s requirements, the task would still remain of showing\nintuitionistic arithmetic consistent. This, however, is not possible\nby even the formal apparatus of classical arithmetic, on the basis of\nGödel’s result in combination with my proof. Even so, I am\ninclined to believe that a consistency proof for intuitionistic\narithmetic, from an even more evident position, is possible and\ndesirable. (quoted and translated in von Plato 2009: 672) \nGödel took a very similar position in December of 1933\n(Gödel 1995: 53). There he broadened the idea of a revised\nversion of Hilbert’s program allowing constructive means that go\nbeyond the finitist ones without accepting fully fledged intuitionism;\nthe latter he considered to be problematic, in particular on account\nof the impredicative nature of intuitionist implication. As to an\nextension of Hilbert’s position he wrote: \nBut there remains the hope that in future one may find other and more\nsatisfactory methods of construction beyond the limits of the system A\n[capturing finitist methods], which may enable us to found classical\narithmetic and analysis upon them. This question promises to be a\nfruitful field for further investigations. \nIn\n section 3.2\n we described Gentzen’s considerations; in\n section 4.2\n we discuss Gödel’s as developed in the late 1930s. In\n section 4.1\n we sketch some other constructive positions. \nA particularly appealing idea is to pursue Hilbert’s program\nrelative to a constructive point of view and determine which parts of\nclassical mathematics are demonstrably consistent relative to that\nstandpoint (see Rathjen 2009 for pursuing this with regard to\nMartin-Löf type theory). As one would suspect, there are\ndiffering “schools” of constructivism and different layers\nof constructivism. Several frameworks for developing mathematics from\nsuch a point of view have been proposed. Some we will refer to in this\narticle (arguably the most important) are: \nAt this point we will just give a very rough description of these\nfoundational views. A few more details, especially about their scope\non a standard scale of theories and proof-theoretic ordinals, will be\nprovided at the very end of\n section 5.3. \n(a) Arithmetical Predicativism originated in the writings of\nPoincaré and Russell in response to the set-theoretic\nparadoxes. It is characterized by a ban of impredicative definitions.\nWhilst it accepts the completed infinite set of naturals numbers, all\nother sets are required to be constructed out of them via an\nautonomous process of arithmetical definitions. A first systematic\nattempt at developing mathematics predicatively was made in\nWeyl’s 1918 monograph Das Kontinuum (Weyl 1918). \n(b) Theories of higher type functionals comprise Gödel’s\nT and Spector’s extension of T via functionals\ndefined by bar recursion. The basic idea goes back to\nGödel’s 1938 lecture at Zilsel’s (Gödel 1995:\n94). It was inspired by Hilbert’s 1926 Über das\nUnendliche, which considered a hierarchy of functionals over the\nnatural numbers, not only of finite but also of transfinite type. \n(c) To understand Takeuti’s finitist standpoint it is important\nto pinpoint the place where in a consistency proof à la Gentzen\nthe means of PRA are exceeded. Gentzen’s proof\nemploys a concrete ordering \\(\\prec\\) of type \\(\\varepsilon_0\\), it\nuses an assignment of ordinals to proofs and provides a reduction\nprocedure on proofs such that any alleged proof of an inconsistency is\nreduced to another proof of an inconsistency which gets assigned a\nsmaller element of the ordering. The ordering, the ordinal assignment\nand the reduction procedure are actually primitive recursive and the\nsteps described so far can be carried out in a small fragment of\nPRA. The additional principle needed to infer the\nconsistency of PA is the following: \nTakeuti refers to (*) as the accessibility of \\(\\prec\\). Note that\nthis is a weaker property than the well-foundedness of \\(\\prec\\) which\nrefers to arbitrary sequences. There is nothing special about the case\nof PA since any ordinal analysis of a theory T\nin the literature can be made to fit this format. Thus\nepistemologically (*) is the fulcrum in any such consistency proof.\nTakeuti’s central idea (1987, 1975) was that we can carry out\nGedankenexperimente (thought experiments) on concretely given\n(elementary) sequences to arrive at the insight that (*)\n obtains.[16] \n(d) Errett Bishop’s novel (informal) approach to constructive\nanalysis (1967) made a great on impression on mathematicians with\nconstructive leanings. In it, he dealt with different kinds of\nmathematical objects (numbers, functions, sets) as if they were given\nby explicit presentations, each kind being equipped with its own\ngermane “equality” relation conceived in such a way that\noperations on them would lead from representations to representations\nrespecting the equality relation. An important ingredient that made\nBishop’s constructivism workable is the systematic use of\nwitnessing data as an integral part of what constitutes a mathematical\nobject. For instance, a real number comes with a modulus of\nconvergence while a function of real numbers comes equipped with a\nmodulus of (uniform) convergence. In his explicit\nmathematics, Feferman (1975, 1979) aims (among other things) at\nformalizing the core of Bishop’s ontology. Explicit mathematics\nis a theory that describes a realm of concretely and explicitly given\nobjects (a universe U of symbols) equipped with an operation\n\\(\\bullet\\) of application in such a way that given two objects\n\\(a,b\\in U\\), a may be viewed as a program which can be run on\ninput b and may produce an output \\(a\\bullet b\\in U\\) or never\nhalt (such structures are known as partial combinatory algebras or\nSchönfinkel algebras). Moreover, some of the objects of U\nrepresent sets of elements of U. The construction of new sets\nout of given sets is either done explicitly by elementary\ncomprehension or by a process of inductive generation. If one also\nadds principles to the effect that every internal operation (given as\n\\(\\lambda x.a\\bullet x\\) for some \\(a\\in U\\)) which is monotone on\nsets possesses a least fixed point one arrives at a remarkably strong\ntheory (cf. Rathjen 1998, 1999b, 2002). \n(e) Martin-Löf type theory is an intuitionist theory of dependent\ntypes intended to be a full scale system for formalizing constructive\nmathematics. Its origins of can be traced to Principia\nMathematica, Hilbert’s Über das Unendliche,\nthe natural deduction systems of Gentzen, taken in conjunction with\nPrawitz’s reduction procedures, and to Gödel’s\nDialectica system. It incorporates inductively defined data types\nwhich together with the vehicle of internal reflection via universes\nendow it with considerable consistency strength. \n(f) Constructive set theory (as do the theories under (d) and (e))\nsets out to develop a framework for the style of constructive\nmathematics of Bishop’s 1967 Foundations of constructive\nanalysis in which he carried out a development of constructive\nanalysis, based on informal notions of constructive function and set,\nwhich went substantially further mathematically than anything done\nbefore by constructivists. Where Brouwer reveled in differences,\nBishop stressed the commonalities with classical mathematics. What was\nnovel about his work was that it could be read as a piece of classical\nmathematics as well. \nThe ‘manifesto’ of constructive set theory was most vividly\nexpressed by Myhill:  \n… the argumentation of [Bishop 1967] looks very smooth and\nseems to follow directly from a certain conception of what sets,\nfunctions, etc. are, and we wish to discover a formalism which\nisolates the principles underlying this conception in the same way\nthat Zermelo-Fraenkel set-theory isolates the principles underlying\nclassical (nonconstructive) mathematics. We want these principles to\nbe such as to make the process of formalization completely trivial, as\nit is in the classical case. (Myhill 1975: 347) \nDespite first appearances, there are close connections between the\napproaches of (d)–(f). Constructive set theory can be\ninterpreted in Martin-Löf type theory (due to Aczel 1978) and\nexplicit mathematics can be interpreted in constructive set theory\n(see Rathjen 1993b, in Other Internet Resources). Perhaps the closest\nfit between (e) and (f), giving back and forth interpretations, is\nprovided by Rathjen & Tupailo 2006. Some concrete mathematical\nresults are found at the end of\n section 5.3. \nAmong the proposals for extending finitist methods put forward in his\n1938 lecture at Zilsel’s, Gödel appears to have favored the\nroute via higher type functions. Details of what came to be known as\nthe Dialectica interpretation were not published until 1958\n(Gödel 1958) but the D-interpretation itself was arrived at by\n1941. Gödel’s system \\({T}\\) axiomatizes a class of\nfunctions that he called the primitive recursive functionals of\nfinite type. \\({T}\\) is a largely equational theory whose axioms\nare equations involving terms for higher type functionals with just a\nlayer of propositional logic on top of that. In this way the\nquantifiers, problematic for finists and irksome to intuitionists, are\navoided. To explain the benefits of the D-interpretation we need to\nhave a closer look at the syntax of \\({T}\\). \nDefinition 4.1 \\({T}\\) has a many-sorted\nlanguage in that each terms is assigned a type. Type (symbols) are\ngenerated from 0 by the rule: If \\(\\sigma\\) and \\(\\tau\\) are types\nthen so is \\(\\sigma\\to\\tau\\). Intuitively the ground type 0 is the\ntype of natural numbers. If \\(\\sigma\\) and \\(\\tau\\) are types that are\nalready understood then \\(\\sigma\\to\\tau\\) is a type whose objects are\nconsidered to be functions from objects of type \\(\\sigma\\) to objects\nof type \\(\\tau\\). In addition to variables\n\\(x^{\\tau},y^{\\tau},z^{\\tau},\\ldots\\) for each type \\(\\tau\\), the\nlanguage of \\({T}\\) has special constants 0, \\(\\rsuc\\),\n\\(\\rK_{\\sigma,\\tau}\\), \\(\\rS_{\\rho,\\sigma,\\tau}\\), and\n\\(\\rR_{\\sigma}\\) for all types \\(\\rho,\\sigma,\\tau\\). The meaning of\nthese constants is explained by their defining equations.\n\\(\\rK_{\\sigma,\\tau}\\) and \\(\\rS_{\\rho,\\sigma,\\tau}\\) are familiar from\ncombinatory logic which was introduced by Schönfinkel in 1924 and\nbecame more widely known through Curry’s work (1930). 0 plays the role of the\nfirst natural number while \\(\\rsuc\\) embodies the successor function\non objects of type 0. The constants \\(\\rR_{\\sigma}\\), called\nrecursors, provide the main vehicle for defining functionals\nby recursion on \\(\\bbN\\). Term formation starts with constants and\nvariables, and if s and t are terms of type\n\\(\\sigma\\to\\tau\\) and \\(\\sigma\\), respectively, then \\(s(t)\\) is a\nterm of type \\(\\tau\\). To increase readability we shall write\n\\(t(r,s)\\) instead of \\((t(r))(s)\\) and \\(t(r,s,q)\\) instead of\n\\((t(r,s))(q)\\) etc. Also \\(\\rsuc(t)\\) will be shortened to\n\\(t'\\). The defining axioms for the constants are the\n following:[17] \nThe axioms of \\({T}\\) consist of the above defining axioms, equality\naxioms and axioms for propositional logic. Inference rules are modus\nponens and the induction rule  \nfor t of type 0 and x not in \\(A(0)\\). \nThe first step towards the D-interpretation of Heyting arithmetic in\n\\({T}\\) consists of associating to each formula A of arithmetic\na syntactic translation \\(A^D\\) which is of the form  \nwith \\(A_D(\\vec{x},\\vec{y})\\) being quantifier free. Thus \\(A^D\\) is\nnot a formula of \\({T}\\) but of its augmentation via quantifiers\n\\(\\forall x^{\\tau}\\) and \\(\\exists y^{\\tau}\\) for all types \\(\\tau\\).\nThe translation proceeds by induction on the buildup of A. The\ncases where the outermost logical symbol of A is among\n\\(\\land\\), \\(\\lor\\), \\(\\exists x\\), \\(\\forall x\\) are rather\nstraightforward. The crucial case occurs when A is an\nimplication \\(B\\to C\\). To increase readability we shall suppress the\ntyping of variables. Let \\(B^D\\equiv \\exists \\vec{x}\\, \\forall\n\\vec{y}\\, B_D(\\vec{x},\\vec{y})\\) and \\(C^D\\equiv {\\exists \\vec{u}\\,\n\\forall \\vec{v}\\, C_D(\\vec{u},\\vec{v})}\\). Then one uses a series of\njudicious equivalences to bring the quantifiers in \\(B^D\\to C^D\\) to\nthe front and finally employs skolemization of existential variables\nas follows:  \n\\(A^D\\) is then defined to be the formula in (vii). Note, however,\nthat these equivalences are not necessarily justified constructively.\nOnly (i) \\(\\Leftrightarrow\\) (ii) and (iii) \\(\\Leftrightarrow\\) (iv)\nhold constructively whereas (v) \\(\\Leftrightarrow\\) (vi) and (vi)\n\\(\\Leftrightarrow\\) (vii) are justified constructively only if one\nalso accepts the axiom of choice for all finite types\n(ACft). Equivalences (ii) \\(\\Leftrightarrow\\) (iii)\nand (iv) \\(\\Leftrightarrow\\) (v) use a certain amount of classical\nlogic known as the principle of independence of premise\n(IPft) and Markov’s principle\n(MPft) for all finite types, respectively. At this\npoint \\(A\\mapsto A^D\\) is just a syntactic translation. But amazingly\nit gives rise to a meaningful interpretation of HA in\nT. \nTheorem 4.2 (Gödel 1958) Suppose\n\\(\\cD\\) is a proof of A in HA and \\(A^D\\) as\nin \\((\\ref{D-Form})\\). Then one can effectively construct a sequence\nof terms \\(\\vec t\\) (from \\(\\cD\\)) such that \\({T}\\) proves \\(A_D(\\vec\nt,\\vec y\\,)\\). \nIf one combines the D-interpretation with the\nKolmogorov-Gentzen-Gödel negative translation of\nPA into HA one also arrives at an\ninterpretation of PA in \\({T}\\). Some interesting\nconsequences of the latter are that the consistency of\nPA follows finitistically from the consistency of\n\\({T}\\) and that every total recursive function of PA\nis denoted by a term of \\({T}\\). \nThe three principles ACft, IPft\nand MPft which figured in the D-translation\nactually characterize the D-translation in the sense that over\nthe quantifier extension of \\({T}\\) with intuitionistic logic, called\n\\(\\bHA^{\\omega}\\), they are equivalent to the schema  \nfor all formulae C of that theory. Principles similar to the\nthree above are also often validated in another type of computational\ninterpretation of intuitionistic theories known as\nrealizability. Thus it appears that they are intrinsically\nrelated to computational interpretations of such theories. \nA further pleasing aspect of Gödel’s interpretation is that\nit can be extended to stronger systems such as higher order systems\nand even to set theory (Burr 2000, Diller 2008). Moreover, it sometimes allows one to\nextract computational information even from proofs of specific\nclassical theorems (see, e.g., Kohlenbach 2007). It behaves nicely\nwith respect to modus ponens and thus works well for ordinary proofs\nthat are usually structured via a series of lemmata. This is in\ncontrast to cut elimination which often requires a computationally\ncostly transformation of proofs. \nSpector (1962) extended Gödel’s functional interpretation,\nengineering an interpretation of \\(\\bZ_2\\) into T augmented via\na scheme of transfinite recursion on higher type orderings. This type\nof recursion, called bar recursion, is conceptually related\nto Brouwer’s bar induction principle. (For a definition of bar\ninduction and a presentation of Spector’s result see\n appendix C.) \nWe described the system \\(\\bZ_2\\) of second order arithmetic already\nat the end of section 3.2. It was viewed as the\n“next”system to be proved consistent—after\nfirst-order arithmetic PA had been shown to be. As we\nmentioned \\(\\bZ_2\\) is also called “analysis”, because it\nallows the development of classical mathematical analysis: coding real\nnumbers and continuous functions as sets of natural numbers, a good\ntheory of the continuum can be developed from \\(\\bZ_2\\)’s\naxioms. Indeed, Hermann Weyl showed in 1918 that a considerable\nportion of analysis can be developed in small fragments of \\(\\bZ_2\\)\nthat are actually conservative over PA. The idea of\nsingling out the minimal fragment of \\(\\bZ_2\\) required to expose a\nparticular part of ordinary mathematics led in the 1980s to the\nresearch program of reverse mathematics. However, before\ndiscussing that program, we are going to proof-theoretic\ninvestigations of \\(\\bZ_2\\) and its subsystems that have been a focal\npoint until the very early 1980s. \nAfter Gentzen, it was Gaisi Takeuti who worked on a consistency proof\nfor \\(\\bZ_2\\) in the late 1940s. He conjectured that Gentzen’s\nHauptsatz not only holds for first order logic but also for\nhigher order logic, also known as simple type theory,\nSTT. This came to be known as Takeuti’s\nfundamental\n conjecture.[18]\n The particular sequent calculus he introduced was called a\ngeneralized logic calculus, GLC (Takeuti 1953). \\(\\bZ_2\\) can be\nviewed as a subtheory of GLC. In the setting of GLC the comprehension\nprinciple CA is encapsulated in the right\nintroduction rule for the existential second-order quantifier and the\nleft introduction rule for the universal second-order quantifier. In\norder to display these rules the following notation is convenient. If\n\\(F(U)\\) and \\(A(a)\\) are formulae then \\(F(\\{v\\mid A(v)\\})\\) arises\nfrom \\(F(U)\\) by replacing all subformulae \\(t\\in U\\) of \\(F(U)\\)\n(with U indicated) by \\(A(t)\\). The rules for second order\nquantifiers can then be stated as\n follows:[19] \nTo deduce an instance \\(\\exists X\\,\\forall x\\,[x\\in X \\leftrightarrow\nA(x)]\\) of CA just let \\(F(U)\\) be the formula\n\\(\\forall x\\,[x\\in U \\leftrightarrow A(x)]\\) and observe that  \nand hence  \nAs the deducibility of the empty sequent is ruled out if cut\nelimination holds for GLC (or just the fragment GLC2\ncorresponding to \\(\\bZ_2\\)), Takeuti’s Fundamental Conjecture\nentails the consistency of \\(\\bZ_2\\). However note that it does not\nyield the subformula property as in the first-order case since the\nminor formula \\(F(\\{x\\mid A(x)\\})\\) in \\((\\exists_2\\,\\rR)\\) and\n\\((\\forall_2\\,\\bL)\\) may have a much higher (quantifier) complexity\nthan the principal formula \\(\\exists XF(X)\\) and \\(\\forall XF(X)\\),\nrespectively. Indeed, \\(\\exists XF(X)\\) may be a proper subformula of\n\\(A(x)\\) which clearly exhibits the impredicative nature of these\ninferences and shows that they are strikingly different from those in\npredicative analysis where a proper subformula property obtains. \nIn 1960a Schütte\ndeveloped a semantic equivalent to the (syntactic) fundamental\nconjecture using partial or semi-valuations. He employed the method of\nsearch trees (or deduction chains) to show that a formula F\nthat cannot be deduced in the cut-free system has a deduction chain\nwithout axioms which then gives rise to a partial valuation V\nassigning the value “false” to F. From the latter\nhe inferred that the completeness of the cut-free\n system[20]\n is equivalent to the semantic property that every partial valuation\ncan be extended to a total valuation (basically a Henkin model of\nSTT). In 1966 Tait succeeded in proving\ncut-elimination for second order logic using Schütte’s\nsemantic equivalent for that fragment. Soon afterwards, Takahashi\n(1967) and Prawitz (1968) independently proved for full classical\nsimple type that every partial valuation extends to a total one,\nthereby establishing Takeuti’s fundamental conjecture. These\nresults, though, were somewhat disappointing as they were obtained by\nhighly non-constructive methods that provided no concrete method for\neliminating cuts in a derivation. However, Girard showed in 1971 that\nsimple type theory not only allows cut-elimination but that there is\nalso a terminating normalization\n procedure.[21]\n These are clearly very interesting results, but as far as instilling\ntrust in the consistency of \\(\\bZ_2\\) or SST is\nconcerned, the cut elimination or termination proofs are just circular\nsince they blatantly use the very comprehension principles formalized\nin these theories (and a bit more). To quote Takeuti:  \nMy fundamental conjecture itself has been resolved in a sense by Motoo\nTakahashi and Dag Prawitz independently. However, their proofs rely on\nset theory, and so it cannot be regarded as an execution of\nHilbert’s Program. (Takeuti 2003: 133)  \nTakeuti’s work on his conjecture instead focused on partial\nresults. A major breakthrough that galvanized research in proof\ntheory, especially ordinal-theoretic investigations, was made by him\nin 1967. In Takeuti 1967 he gave a consistency proof for\n\\(\\Pi^1_1\\)-comprehension and thereby for the first time obtained an\nordinal analysis of an impredicative theory. For this Takeuti vastly\nextended Gentzen’s method of assigning ordinals (ordinal\ndiagrams, to be precise) to purported derivations of the empty\nsequent. It is worth quoting Takeuti’s own assessment of his\nachievements. \n… the subsystems for which I have been able to prove the\nfundamental conjecture are the system with \\(\\Pi^1_1\\) comprehension\naxiom and a slightly stronger system, that is, the one with\n\\(\\Pi^1_1\\) comprehension axiom together with inductive\ndefinitions.[…] I tried to resolve the fundamental conjecture\nfor the system with the \\(\\Delta^1_2\\) comprehension axiom within our\nextended version of the finite standpoint. Ultimately, our success was\nlimited to the system with provably \\(\\Delta^1_2\\) comprehension\naxiom. This was my last successful result in this area. (Takeuti 2003:\n133) \nThe subsystems of \\(\\bZ_2\\) that are alluded to in the above\ndiscussion are now to be described. We consider the axiom schema of\n\\(\\cC\\)-comprehension for formula classes \\(\\cC\\) which is\ngiven by  \nfor all formulae \\({F}\\in\\cC\\) in which X does not occur.\nNatural formula classes are the arithmetical formulae,\nconsisting of all formulae without second order quantifiers \\(\\forall\nX\\) and \\(\\exists X\\), and the \\(\\Pi^1_n\\)-formulae, where a\n\\(\\Pi^1_n\\)-formula is a formula of the form \\(\\forall X_1\\ldots Q\nX_n\\,A(X_1,\\ldots,X_n)\\) with \\(\\forall X_1\\ldots Q X_n\\) being a\nstring of n alternating set quantifiers, commencing with a\nuniversal one, followed by an arithmetical formula\n\\(A(X_1,\\ldots,X_n)\\). Note that in this notation the class of\narithmetical formulae is denoted by \\(\\Pi^1_0\\). \nAlso “mixed” forms of comprehension are of interest, e.g.,\n \nwhere \\(F(u)\\) is in \\(\\Pi^1_n\\) and \\(G(u)\\) in \\(\\Sigma^1_n\\). \nOne also considers \\(\\Delta^1_n\\) comprehension rules:  \nFor each axiom scheme \\(\\mathbf{Ax}\\) we denote by \\((\\mathbf{Ax})_0\\)\nthe theory consisting of the basic arithmetical axioms plus the scheme\n\\(\\mathbf{Ax}\\). By contrast, \\((\\mathbf{Ax})\\) stands for the theory\n\\((\\mathbf{Ax})_0\\) augmented by the scheme of induction for all\n\\(\\cL_2\\)-formulae. An example for these notations is the theory\n\\((\\bPi^1_1-\\bCA)_0\\) which has the comprehension schema for\n\\(\\bPi^1_1\\)-formulae. \nIn PA one can define an elementary injective pairing\nfunction on numbers, e.g., \\((n,m)\\coloneqq 2^n\\times3^m\\). With the\nhelp of this function an infinite sequence of sets of natural numbers\ncan be coded as a single set of natural numbers. The \\(n^{th}\\)\nsection of set of natural numbers U is defined by\n\\(U_n\\,\\coloneqq \\,\\{m:\\,(n,m)\\in U\\}\\). Using this coding, we can\nformulate a form of the axiom of choice for formulae \\({F}\\) in\n\\(\\cC\\) by  \nThe basic relations between the above theories are discussed in\nFeferman and Sieg 1981a. \nA major stumbling block for proving Takeuti’s fundamental\nconjecture is that in \\((\\forall_2\\bL)\\) and \\((\\exists_2\\rR)\\)\ninferences the minor formula \\(F(\\{v\\mid A(v)\\})\\) can have a much\nhigher complexity than the principal (inferred) formula \\(QX\\,F(X)\\).\nIf, instead, one allowed these inferences only in cases where the\n‘abstraction’ term \\(\\{v\\mid A(v)\\}\\) had (in some sense) a\nlower complexity than \\(QX\\,F(X)\\), cut elimination could be restored.\nTo implement this idea, one introduces a hierarchy of sets (formally\nrepresented by abstraction terms) whose complexity is stratified by\nordinal levels \\(\\alpha\\), and a pertaining hierarchy of quantifiers\n\\(\\forall X^{\\beta}\\) and \\(\\exists X^{\\beta}\\) conceived to range\nover sets of levels \\(<\\beta\\). This is the basic idea underlying\nthe ramified analytic hierarchy. The problem of which ordinals could\nbe used for the transfinite iteration led to the concept of\nautonomous progressions of theories. The general idea of\nprogressions of theories is very natural and we shall consider it\nfirst before discussing the autonomous versions. \nAs observed earlier, Hilbert attempted to overcome the incompleteness\nof first-order arithmetic by introducing as axioms\n\\(\\Pi^0_1\\)-statements all of whose instances had been finitistically\nproved (Hilbert 1931a). In a way he modified the concept of a\n“formal” theory by invoking finitist provability. Bernays,\nin his letter to Gödel of January 18, 1931 (Gödel 2003:\n86–88), proposed a rule of a more general form. He\nindicated also that it would allow the elimination of the induction\nprinciple—in exchange for dealing with infinite proofs. \nThese considerations among others raised the issue of what constitutes\na properly formal theory. Gödel paid very special\nattention to it when giving his Princeton Lectures in 1934. At the\nvery end he introduced the general recursive functions. This class of\nnumber theoretic functions was shown to be co-extensional with\nChurch’s λ-definable ones by Church and Kleene. In\nChurch 1936 an “identification”\nof effective\ncalculability and general recursiveness was proposed, what is usually\ncalled Church’s thesis. Turing, of course, proposed his\nmachine computability for a very similar purpose and proved its\nequivalence to λ-definability in an appendix to his\n1936. Church\nand Turing used their respective notion to establish the\nundecidability of first-order logic. For Gödel, this was the\nbackground for formulating the incompleteness theorems in “full\ngenerality” for all formal theories (containing a modicum of\nnumber or set theory); see the Postscriptum\nto the Princeton Lectures Gödel wrote in 1964: \nIn consequence of later advances, in particular of the fact that, due\nto A.M. Turing’s work, a precise and unquestionably adequate\ndefinition of the general concept of formal system can now be given,\nthe existence of undecidable arithmetical propositions and the\nnon-demonstrability of the consistency of a system in the same system\ncan now be proved rigorously for every consistent formal\nsystem containing a certain amount of finitary number theory.\n(Gödel 1986: 369). \nThe first incompleteness is proved for any such theory T, by\nexplicitly producing an unprovable yet true statement \\(G_\\bT\\). That\nformula can then be added to T making \\(\\bT+G_\\bT\\) a\n“less incomplete” theory. Von Neumann had already\nestablished the equivalence of \\(G_\\bT\\) with the consistency\nstatement for T, \\(\\Con (\\bT)\\); the latter expresses that\nthere is no proof in T of a blatantly false statement such as\n\\(0=1\\). This gives then rise to an extension procedure leading from\nT to \\(\\bT'\\), namely (R1) \\(\\bT'=\\bT+G_\\bT\\). \nThus one might try to address the incompleteness of T by\nforming a sequence of theories \\(\\bT=\\bT_0\\subset\n\\bT_1\\subset\\bT_2\\subset\\ldots\\) where \\(\\bT_{i+1}=\\bT_i'\\) and to\ncontinue this into the transfinite. The latter can be achieved by\nletting \\(\\bT_{\\lambda}=\\bigcup_{\\alpha<\\lambda}\\bT_{\\alpha}\\) for\nlimit ordinals λ and \\(\\bT_{\\alpha+1}=\\bT_{\\alpha}'\\)\nfor successor ordinals \\(\\alpha+1\\). However, the consistency\nstatement for \\(\\bT_{\\lambda}\\), thus the provability predicate for\nthe theory, has to be expressed in the language of \\(\\bT_{\\lambda}\\),\nand one cannot simply use set theoretic ordinals. Furthermore, the\nextensions of T are all supposed to be formal theories, i.e.,\nthe axioms have to be enumerable by recursive functions. To deal with\nboth issues at once, one has to deal with ordinals in an effective\nway. \nThat is what Turing did in his Princeton dissertation (1939)\nconcerning, what he called, ordinal logics. There he\nconsiders two ways of achieving the effective representation of\nordinals. The first way is via the set \\(\\rW\\) of numbers e for\nrecursive well-orderings \\(\\leq_e\\), the second is provided by the\nclass of Church-Kleene notations for ordinals (Church and Kleene\n1936) that used expressions in the λ-calculus to describe\nordinals. The latter approach was then modified in Kleene 1938 to an\nequivalent recursion-theoretic definition that uses numerical codes to\ndenote countable ordinals and is known as Kleene’s\n\\({\\cO}\\). \nDefinition 5.1 A computable or\nrecursive function on the naturals is one that can be\ncomputed by a Turing machine. The program of a Turing machine M\ncan be assigned a Gödel number \\({\\Corner{M}}\\). For natural\nnumbers \\(e,n\\), to convey that the Turing machine with Gödel\nnumber e computes a number m on input n, we use\nthe notation \\(\\{e\\}(n)\\) for m. \nKleene uses \\({\\rsuc}(a)\\coloneqq 2^a\\) as notations for successor\nordinals and and \\({\\rlim}(e)\\coloneqq 3\\cdot 5^e\\) for limit\nordinals. \nThe class \\({\\cO}\\) of ordinal notations, the partial ordering\nrelation \\(\\relLTcO\\) between such notations, and the ordinal\n\\({\\mid}{a}{\\mid}\\) denoted by \\(a\\in {\\cO}\\) are defined\nsimultaneously as follows: \nThe first ordinal \\(\\tau\\) such that there is no recursive\nwell-ordering of order type \\(\\tau\\) is usually denoted by\n\\(\\omega^{CK}\\) in honor of Church and Kleene. It can be shown for the\nabove definition of \\({\\cO}\\) that the recursive ordinals are exactly\nthose that have a notation in \\({\\cO}\\). \nWhen it comes to theories T, quite unlike to other areas of\nlogic (e.g., model theory), results as those presented in this section\ndepend not only on the set of axioms of T, but also on the way\nthey are presented. When talking about a theory T we assume\nthat T is given by a \\(\\Sigma^0_1\\)-formula \\(\\psi(v_0)\\) such\nthat F is an axiom of T just in case\n\\(\\psi({\\Corner{F}})\\) holds; a \\(\\Sigma^0_1\\)-formula is of the form\n\\(\\exists y_1\\ldots\\exists y_n\\,R(y_1,\\ldots y_n)\\) with R\nprimitive recursive. This consideration together with Kleene’s\n\\({\\cO}\\) allows us to build a transfinite hierarchy of theories based\non any suitable theory T. A consistency progression\nbased on T is a primitive recursive function \\(n\\mapsto\n\\psi_n\\) that associates with every natural number n a\n\\(\\Sigma^0_1\\)-formula \\(\\psi_n(v_0)\\) that defines \\(\\bT_n\\) such\nthat PA proves: (i) \\(\\bT_0=\\bT\\); (ii)\n\\(\\bT_{{\\rsuc}(n)}=\\bT_n'\\), and (iii)\n\\(\\bT_{{\\rlim}(n)}=\\bigcup_x\\bT_{\\{n\\}(x)}\\). So, finally we can\nformulate Turing’s completeness result. \nTheorem 5.2 For any true \\(\\Pi^0_1\\)\nsentence F a number \\(a_{F}\\in{\\cO}\\) can be constructed such\nthat \\({{\\mid}\\,{a_{F}}\\,{\\mid}}=\\omega+1\\) and \\(\\bT_{a_{F}}\\vdash\nF\\). Moreover, the function \\(F\\mapsto a_{F}\\) is given by a primitive\nrecursive function. \nAt first glance Turing’s theorem seems to provide some insight\ninto the nature of true \\(\\Pi^0_1\\)-statements. That this is an\n“illusion” is revealed by the analysis of its simple proof\nwhich is just based on the trick of coding the truth of F as a\nmember of \\({\\cO}\\). The proof also shows that the infinitely many\niterated consistency axioms \\(\\Con (\\bT_0),\\Con (\\bT_1),\\ldots\\) of\n\\(\\bT_{{{\\rsuc}}({\\rlim}(e))}\\) are irrelevant for proving F.\nAs it turns out, the reason why one has to go to stage \\(\\omega+1\\) is\nsimply that only at stage \\(\\omega\\) a non-standard definition of the\naxioms of \\(\\bigcup_{n<\\omega}\\bT_n\\) can be introduced. More\ndetails and other results on recursive progressions are discussed in\n Appendix B.\n Here let us just mention that one has considered other progressions\nbased on various extension procedures \\(\\bT \\mapsto \\bT'\\) that\nstrengthen a given theory,\n notably:[22] \n(R2) is called an extension by the local reflection\nprinciple, whereas (R3) uses the uniform reflection\nprinciple. Feferman obtained in 1962 an amazing result that\nstrengthens Turing’s result in a dramatic way. \nLet \\((\\bT_a)_{a\\in{\\cO}}\\) be a progression using the uniform\nreflection principle with PA as the base theory\nT. Then we have: for any true arithmetical sentence F\nthere is an \\(a\\in{\\cO}\\), such that \\(\\bT_a\\vdash F\\). Moreover,\n\\(a\\in{\\cO}\\) can be chosen such that \\({{\\mid}\\,{a}\\,{\\mid}}\\leq\n\\omega^{\\omega^{\\omega+1}}\\). \nFor further discussion see\n Appendix B.\n Here we just note that the union of the \\(\\bT_a\\) is no longer a\nformal theory. \nIn the foregoing progressions the ordinals remained external to the\ntheory. Autonomous progressions of theories are the proper\ninternalization of the general concept of progressions. In the\nautonomous case one is allowed to ascend to a theory \\(\\bT_a\\) only if\none already has shown in a previously accepted theory \\(\\bT_b\\) that\n\\(a\\in{\\cO}\\). This idea of generating a hierarchy of theories via a\nboot-strapping process appeared for the first time in Kreisel 1960,\nwhere it was proposed as a\nway of characterizing finitism and predicativism in mathematically\nprecise way. In more formal terms, the starting point is a theory\n\\(\\bT_0\\) which is accepted as correct and an extension procedure\n\\(\\bT\\mapsto\\bT'\\) which is viewed as leading from a correct\ntheory T to a more encompassing correct theory \\(\\bT'\\).\nMoreover, the language of these theories is supposed to contain a\nformula \\(\\rAcc(x)\\) such that provability of \\(\\rAcc(\\bar{a})\\) in a\ncorrect theory entails that\n \\(a\\in{\\cO}\\).[23]\n Kreisel singled out two autonomous progressions of theories\n\\(\\{{{\\bF}}_a\\}\\) and \\(\\{{{\\bR}}_a\\}\\) for finitism and\npredicativity, respectively, and determined the least upper bound of\nthe \\(\\lvert a\\rvert\\) appearing in the first hierarchy to be the ordinal\n\\(\\varepsilon_0\\) which is also the proof-theoretic ordinal of\nPA. The determination of the least upper bound for\nthe predicative hierarchy \\(\\{{{\\bR}}_a\\}\\) was achieved independently\nby Feferman (1964) and Schütte (1964, 1965). It turned out that\nthis ordinal can be expressed in a notation system developed by Veblen\nthat will be presented next. \nAs we saw above, ordinals below \\(\\varepsilon_0\\) suffice for the\nproof-theoretic treatment of PA. For stronger\ntheories segments larger than \\(\\varepsilon_0\\) have to be employed,\nrequiring new normal forms akin to Cantor’s normal form. Ordinal\nrepresentation systems utilized by proof theorists in the 1960s first\nemerged in a purely set-theoretic context more than 50 years earlier.\nIn 1904 Hardy wanted to “construct” a subset of \\(\\bbR\\)\nof size \\(\\aleph_{1}\\), the first uncountable cardinal. His method was\nto represent countable ordinals via increasing sequences of natural\nnumbers and then to correlate a decimal expansion with each such\nsequence. Hardy used two processes on sequences: (i) Removing the\nfirst element to represent the successor; (ii) Diagonalizing at\nlimits. For example, if the sequence \\(1,2,3,\\ldots\\) represents the\nordinal 1, then \\(2,3,4,\\ldots\\) represents the ordinal 2 and\n\\(3,4,5,\\ldots\\) represents the ordinal 3 etc., while the\n‘diagonal’ \\(1,3,5,\\ldots\\) provides a representation of\n\\(\\omega\\). In general, if \\(\\lambda=\\lim_{n\\in\\bbN}\\lambda_n\\) is a\nlimit ordinal with \\(b_{n1},b_{n2},b_{n3},\\ldots\\) representing\n\\(\\lambda_n<\\lambda\\), then \\(b_{11},b_{22},b_{33},\\ldots\\)\nrepresents λ. This representation, however, depends on the\nsequence chosen with limit λ. A sequence\n\\((\\lambda_n)_{n\\in\\bbN}\\) with \\(\\lambda_n<\\lambda\\) and\n\\(\\lim_{n\\in \\bN}\\lambda_n=\\lambda\\) is called a fundamental\nsequence for λ. Hardy’s two operations give\nexplicit representations for all ordinals \\(<\\omega^2\\). \nVeblen in 1908 extended the initial segment of the countable for which\nfundamental sequences can be given effectively. The new tools he\ndevised were the operations of derivation and transfinite\niteration applied to continuous increasing functions on\nordinals. \nDefinition 5.4 Let \\({\\ON}\\) be the class of\nordinals. A (class) function \\(f:{\\ON}\\to{\\ON}\\) is said to be\nincreasing if \\(\\alpha<\\beta\\) implies\n\\(f(\\alpha)<f(\\beta)\\) and continuous (in the order\ntopology on \\({\\ON}\\)) if \nholds for every limit ordinal λ and increasing sequence\n\\((\\alpha_{\\xi})_{\\xi<\\lambda}\\). f is called\nnormal if it is increasing and continuous. \nThe function \\(\\beta\\mapsto \\omega+\\beta\\) is normal while\n\\(\\beta\\mapsto \\beta+\\omega\\) is not continuous at \\(\\omega\\) since\n\\(\\lim_{\\xi<\\omega}(\\xi+\\omega)=\\omega\\) but\n\\((\\lim_{\\xi<\\omega}\\xi)+\\omega=\\omega+\\omega\\). \nDefinition 5.5 The derivative\n\\(f'\\) of a function \\(f:{\\ON}\\rightarrow {\\ON}\\) is the function\nwhich enumerates in increasing order the solutions of the equation\n\\(f( \\alpha )= \\alpha\\), also called the fixed points of\nf.  \nIf f is a normal function, \\(\\{\\alpha:\\,f(\\alpha)=\\alpha\\}\\) is\na proper class and \\(f'\\) will be a normal function, too. \nDefinition 5.6 Now, given a normal function\n\\(f:\\ON \\rightarrow \\ON\\), define a hierarchy of normal functions as\nfollows:  \nIn this way, from the normal function f we get a two-place\nfunction, \\(\\varphi_{f} ( \\alpha , \\beta )\\coloneqq f_{\\alpha} ( \\beta\n)\\). One usually discusses the hierarchy when \\(f=\\ell\\), where\n\\(\\ell( \\alpha )=\\omega^{\\alpha}\\). The least ordinal \\(\\gamma>0\\)\nclosed under \\(\\varphi_{\\ell}\\), i.e., the least ordinal \\(>0\\)\nsatisfying\n\\((\\forall\\alpha,\\beta<\\gamma)\\;\\varphi_{\\ell}(\\alpha,\\beta)<\\gamma\\)\nis called \\(\\Gamma_0\\). It has a somewhat iconic status, in particular\nsince Feferman and Schütte determined it to be the least ordinal\n‘unreachable’ by certain predicative means expressed\nin terms of autonomous progressions of theories (defined in\n section 5.2). \nVeblen extended this idea first to arbitrary finite numbers of\narguments, but then also to transfinite numbers of arguments, with the\nproviso that in, for example \\(\\Phi_{f} ( \\alpha_{0} , \\alpha_{1} ,\n\\ldots , \\alpha_{\\eta} )\\), only a finite number of the arguments\n\\(\\alpha_{\\nu}\\) may be non-zero. Finally, Veblen singled out the\nordinal \\(E(0)\\), where \\(E(0)\\) is the least ordinal \\(\\delta >\n0\\) which cannot be named in terms of functions \\(\\Phi_{\\ell} (\n\\alpha_{0} , \\alpha_{1} , \\ldots , \\alpha_{\\eta} )\\) with \\(\\eta <\n\\delta\\), and each \\(\\alpha_{ \\gamma } < \\delta\\). \nThough the “great Veblen number” (as \\(E(0)\\) is sometimes\ncalled) is quite an impressive ordinal it does not furnish an ordinal\nrepresentation sufficient for the task of analyzing a theory as strong\nas \\(\\Pi^1_1\\)-comprehension. Of course, it is possible to go beyond\n\\(E(0)\\) and initiate a new hierarchy based on the function\n\\(\\xi\\mapsto E(\\xi)\\) or even consider hierarchies utilizing finite\ntype functionals over the ordinals. Still all these further steps\namount to rather modest progress over Veblen’s methods. In 1950\nBachmann presented a new kind of operation on ordinals which dwarfs\nall hierarchies obtained by iterating Veblen’s methods. Bachmann\nbuilds on Veblen’s work but his novel idea was the systematic\nuse of uncountable ordinals to keep track of the functions\ndefined by diagonalization. Let \\({\\Omega}\\) be the first uncountable\nordinal. Bachmann defines a set of ordinals \\(\\fB\\) closed under\nsuccessor such that with each limit \\(\\lambda\\in \\fB\\) is associated\nan increasing sequence \\(\\langle\n\\lambda[\\xi]:\\,\\xi<\\tau_{\\lambda}\\rangle\\) of ordinals \\(\n\\lambda[\\xi] \\in \\fB\\) of length \\(\\tau_{\\lambda}\\leq\\Omega\\) and\n\\(\\lim_{\\xi<\\tau_{\\lambda}}\\lambda[\\xi]=\\lambda\\). A hierarchy of\nfunctions \\((\\varphi^{\\ssfB}_{\\alpha})_{\\alpha \\in \\fB}\\) is then\nobtained as follows: \nAfter the work of Bachmann, the story of ordinal representations\nbecomes very complicated. Significant papers (by Isles, Bridge,\nPfeiffer, Schütte, Gerber to mention a few) involve quite\nhorrendous computations to keep track of the fundamental sequences.\nAlso Bachmann’s approach was combined with uses of higher type\nfunctionals by Aczel and Weyhrauch. Feferman proposed an entirely\ndifferent method for generating a Bachmann-type hierarchy of normal\nfunctions which does not involve fundamental sequences. Buchholz\nfurther simplified the systems and proved their computability. For\ndetails we recommend the preface to Buchholz et al. 1981. \nThe ordinal that Feferman and Schütte determined as the least\nupper bound for \\(\\{{{\\bR}}_a\\}\\) is \\(\\Gamma_0\\), the least non-zero\nordinal closed under the Veblen function\n\\(\\alpha,\\beta\\mapsto\\varphi_{\\alpha}(\\beta)\\). This was a genuine\nproof-theoretic result with the tools coming ready-made from\nSchütte’s (1960b) monograph. There he had calculated the\nproof-theoretic ordinals of the \\({{\\bR}}_a\\) as a function of\n\\(\\lvert a\\rvert\\), using cut elimination techniques for logics with infinitary\nrules (dubbed “semi-formal systems”). If\n\\(\\lvert a\\rvert =\\omega^{\\alpha}\\) then \\(\\lvert {{\\bR}}_a\\rvert =\\varphi_{\\alpha}(0)\\).\n“Semi-formal” is a terminology employed by Schütte\nand refers to the fact that this proof system has rules with\ninfinitely many premises, similar to the \\(\\omega\\)-rule. \nDefinition 5.7 In the following we assume\nthat the ordinals come from some representation system. The language\nof \\(\\bRA^*\\) is an extension of that of first order arithmetic. For\neach ordinal \\(\\alpha\\) and \\(\\beta>0\\) it has free set variables\n\\(U_0^\\alpha,U^\\alpha_1,U^\\alpha_2\\ldots\\) of level \\(\\alpha\\) and\nbound set variables of level \\(\\beta\\). The level,\n\\({\\textrm{lev}(A)}\\), of a formula A of \\(\\bRA^*\\) is defined\nto be the maximum of the levels of set variables that occur in\nA. Expressions of the form \\(\\{x\\mid A(x)\\}\\) with \\(A(u)\\) a\nformula will be called abstraction terms, their level being\nthe same as that of the formula \\(A(u)\\). \nThe inference rules of \\(\\bRA^*\\) comprise those of the sequent\ncalculus with the exception of \\((\\forall R)\\) and \\((\\exists L)\\)\nwhich are replaced by those for the \\(\\omega\\)-rule: \\(\\omega\\)R and\n\\(\\omega\\)L. Below \\(\\fP_{\\beta}\\) stands for the set of all\nabstraction terms with levels \\(<\\beta\\). The rules for the set\nquantifiers are as follows:  \nwhere in \\(\\forall_\\beta\\bL\\) and \\(\\exists_\\beta\\rR\\), P is an\nabstraction term of level \\(<\\beta\\). \nAs per usual, the price one has to pay for rules with infinitely many\npremises is that derivations become infinite (well-founded) trees. The\nlength of a derivation can then be measured by the ordinal rank\nassociated with the tree. One also wants to keep track of the\ncomplexity of cuts in the derivation. The length we assign to a\nformula A, \\(\\lvert A\\rvert\\), measures its complexity. It is an ordinal\nof the form \\(\\omega\\cdot \\alpha+n\\) where \\(\\alpha\\) is the level of\nA and \\(n<\\omega\\). One then defines a notion of\nderivability in \\({{\\bRA}}\\),  \nwhere \\(\\alpha\\) majorizes the transfinite length of the derivation\nand \\(\\rho\\) conveys that all cut formulae in the derivation have\nlength \\(<\\rho\\). \nCut elimination works smoothly for \\(\\bRA^*\\), however, the prize one\nhas to pay can only be measured in terms of Veblen’s \\(\\varphi\\)\nfunction. The optimal result is the so-called second cut elimination\ntheorem. \nTheorem 5.8 (Second Cut Elimination Theorem)\n \nIt entails of course the special case that \\(\\bRA^*\n\\stile{\\alpha}{\\omega^\\nu} \\Gamma\\Rightarrow \\Delta\\) yields \\(\\bRA^*\n\\stile{\\varphi_{\\nu}(\\alpha)}{0} \\Gamma\\Rightarrow \\Delta\\), and thus,\nas the latter deduction is cut-free, all cuts can be removed. Several\nsubtheories of \\(\\bZ_2\\) can be interpreted in \\(\\bRA^*\\), yielding\nupper bounds for their proof-theoretic ordinals via\n Theorem 5.8.\n Here is a selection of such\n results:[24] \nTheorem 5.9 \nFor the definitions of the theories in this theorem, see end of\n section 5.1.\n To obtain the results about theories in (iii) and (iv) it is somewhat\neasier to first reduce them to systems of the form\n\\((\\Pi^0_1{\\Hy}\\bCA)_{<\\rho}\\) as the latter have a straightforward\ninterpretation in \\({{\\bRA}}^*\\). Reductions of\n\\((\\Delta^1_1{\\Hy}\\bCR)\\) to\n\\((\\Pi^0_1{\\Hy}\\bCA)_{<\\omega^{\\omega}}\\) and of\n\\((\\Sigma^1_1{\\Hy}\\bAC)\\) to\n\\((\\Pi^0_1{\\Hy}\\bCA)_{<\\varepsilon_0}\\) are due to Feferman (1964)\nand Friedman (1970),\n respectively.[25] \nThe investigation of such subsystems of analysis and the determined\neffort to establish their mathematical power led to a research program\nthat was initiated by Friedman and Simpson some thirty years ago and\ndubbed Reverse Mathematics. The objective of the program is\nto investigate the role of set existence principles in ordinary\n mathematics.[26]\n The main question can be stated as follows: \nGiven a specific theorem \\(\\tau\\) of ordinary mathematics, which set\nexistence axioms are needed in order to prove \\(\\tau\\)? \nCentral to the above is the reference to what is called ‘ordinary\nmathematics’. This concept, of course, doesn’t have a\nprecise definition. Roughly speaking, by ordinary mathematics we mean\nmain-stream, non-set-theoretic mathematics, i.e., the core areas of\nmathematics which make no essential use of the concepts and methods of\nset theory and do not essentially depend on the theory of uncountable\ncardinal numbers. \nFor many mathematical theorems \\(\\tau\\), there is a weakest natural\nsubsystem \\({\\bS}(\\tau)\\) of \\(\\bZ_2\\) such that \\({\\bS}(\\tau)\\)\nproves \\(\\tau\\). Very often, if a theorem of ordinary mathematics is\nproved from the weakest possible set existence axioms, the statement\nof that theorem will turn out to be provably equivalent to those\naxioms over a still weaker base theory. Moreover, it has turned out\nthat \\({\\bS}(\\tau)\\) often belongs to a small list of specific\nsubsystems of \\(\\bZ_2\\) dubbed \\(\\RCA_0\\), \\(\\WKL_0\\), \\(\\ACA_0\\),\n\\(\\ATR_0\\) and \\(({\\bPi}^1_1{\\Hy}{\\bCA})_0\\),\n respectively.[27]\n The systems are enumerated in increasing strength. \\(\\ACA_0\\) is\nactually the same theory as \\((\\Pi^1_0{\\Hy}\\bCA)_0\\). The main set\nexistence axioms of \\(\\RCA_0\\), \\(\\WKL_0\\), \\(\\ACA_0\\), \\(\\ATR_0\\),\nand \\(({{\\bPi}}^1_1{\\Hy}{\\bCA})_0\\) are recursive comprehension, weak\nKönig’s lemma, arithmetical comprehension, arithmetical\ntransfinite recursion, and \\(\\bPi_1^1\\)-comprehension, respectively.\n\\(\\ACA_0\\) is actually the same theory as \\((\\Pi^1_0{\\Hy}\\bCA)_0\\).\nFor exact definitions of all these systems and their role in reverse\nmathematics see Simpson 1999. Examples of mathematical statements\nprovable in \\(\\RCA_0\\) are the intermediate value theorem and the\nBaire category theorem. Reversals for \\(\\WKL_0\\) are the Heine/Borel\ncovering lemma and the local existence theorem for solutions of\nordinary differential equations. Among the many reversals for\n\\(\\ACA_0\\), \\(\\ATR_0\\), and \\((\\bPi^1_1{\\Hy}{\\bCA})_0\\) one finds the\nexistence of maximal ideals in countable rings, Ulm’s theorem,\nand the Cantor-Bendixson theorem, respectively. \nThe proof-theoretic strength of \\(\\RCA_0\\) is weaker than that of\nPA while \\(\\ACA_0\\) has the same strength as\nPA. To get a sense of scale, the strengths of the\nfirst four theories are best expressed via their proof-theoretic\nordinals: \nTheorem 5.10 \n\\(\\lvert (\\bPi^1_1\\Hy\\bCA)_0\\rvert\\), however, eludes expression in the ordinal\nrepresentations introduced so far. This will require the much stronger\nrepresentation to be introduced in\n Definition 5.11. \nThere are important precursors of reverse mathematics. Weyl (1918)\nstarted to develop analysis using a minimalist foundation (that\nequates to \\(\\ACA_0\\)) whilst Hilbert and Bernays (1939) developed\nanalysis in second order arithmetic, whereby on closer inspection one\nsees that all they used is \\((\\bPi^1_1\\Hy\\bCA)_0\\). The first theorem\nof genuinely reverse mathematics was established by Dedekind in his\nessay Stetigkeit und irrationale Zahlen (1872). It states\nthat his continuity (or completeness) principle is equivalent to a\nwell-known theorem of analysis, namely, every bounded, monotonically\nincreasing sequence has a limit. He emphasizes,  \nThis theorem is equivalent to the principle of continuity, i.e., it\nloses its validity as soon as we assume a single real number not to be\ncontained in the domain \\(\\cR\\) [of all real numbers, i.e., of all\ncuts of rational numbers]. (1872 [1996: 778])  \nIt is to bring out “the connection between the principle of\ncontinuity and infinitesimal analysis” (1872 [1996: 779]). \nSpector’s (1962) functional interpretation of \\(\\bZ_2\\) via bar\nrecursive functionals was of great interest to proof theory. However,\nit was not clear whether there was a constructive foundation of these\nfunctionals along the lines of hereditarily continuous functionals\nthat can be represented by computable functions (akin to Kleene 1959;\nKreisel 1959) which would make them acceptable on intuitionistic\ngrounds. In 1963 Kreisel conducted a seminar the expressed aim of\nwhich was to assay the constructivity of Spector’s\ninterpretation (see Kreisel 1963). Specifically he asked whether an\nintuitionistic theory of monotonic inductive definitions,\n\\(\\bID^i_1(\\text{mon})\\), could model bar recursion, or even more\nspecifically, formally capture a class of indices of representing\nfunctions of these functionals. In a subsequent report the\nseminar’s conclusion was later summarized by Kreisel: \n… the answer is negative by a wide margin, since not even bar\nrecursion of type 2 can be proved consistent [from intuitionistically\naccepted principles]. (Kreisel in “Reports of the Seminar on the Foundations of Analysis,\nStanford, Summer 1963”, as quoted in Feferman 1998: 223)\n \nHe not only introduced theories of one inductive definition but also\nof \\(\\nu\\)-times transfinitely iterated inductive definitions,\n\\(\\bID_{\\nu}\\). Albeit it soon became clear that even the theories\n\\(\\bID_{\\nu}\\) couldn’t reach the strength of \\(\\bZ_2\\) (in\npoint of fact, such theories are much weaker than the fragment of\n\\(\\bZ_2\\) based on \\(\\Pi^1_2\\)-comprehension); they became the subject\nof proof-theoretic investigation in their own right and occupied the\nattention of proof theorists for at least another 15 years. One reason\nfor this interest was surely that the intuitionistic versions\ncorresponding to the accessible (i.e., well-founded) part of a\nprimitive recursive ordering are immediately constructively appealing\nand a further reason was that they were thought to be more amenable to\ndirect proof-theoretic treatments than fragments of \\(\\bZ_2\\) or set\ntheories. \nWe shall not give a detailed account of the formalization of these\ntheories, but focus on the non-iterated case \\(\\bID_1\\) and its\nintuitionistic version \\(\\bID_1^i\\) to convey the idea. A monotone\noperator on \\(\\bbN\\) is a map \\(\\Gamma\\) that sends a set \\(X\\subseteq\n\\bbN\\) to a subset \\(\\Gamma(X)\\) of \\(\\bbN\\) and is monotone, i.e.,\n\\(X\\subseteq Y\\subseteq \\bbN\\) implies \\(\\Gamma(X) \\subseteq\n\\Gamma(Y)\\). Owing to monotonicity, the operator \\(\\Gamma\\)\nwill have a least fixed point \\(I_{\\Gamma}\\subseteq\\bbN\\), i.e.,\n\\(\\Gamma( I_{\\Gamma})=I_{\\Gamma}\\) and for every other fixed point\nX it holds \\(I_{\\Gamma} \\subseteq X\\).\nSet-theoretically \\(I_{\\Gamma}\\) is obtained by iterating \\(\\Gamma\\)\nthrough the ordinals,  \nMonotonicity ensures (in set theory) that one finds an ordinal\n\\(\\tau\\) such that \\(\\Gamma(\\Gamma^{\\tau})=\\Gamma^{\\tau}\\), and the\nset \\(\\Gamma^{\\tau}\\) will be the least fixed point. If one adds a new\n1-place predicate symbol P to the language of arithmetic, one\ncan describe the so-called positive arithmetical operators. They are\nof the form  \nwhere \\(A(x,P)\\) is a formula of the language of PA\naugmented by P in which the predicate P occurs only\n positively.[28]\n The syntactic condition of positivity then ensures that the operator\n\\(\\Gamma_A\\) is monotone. The language of \\(\\bID_1\\) is an extension\nof that of PA. It contains a unary predicate symbol\n\\(I_A\\) for each positive arithmetical operator \\(\\Gamma_A\\) and the\naxioms  \nwhere in (Id2) \\(F(x)\\) is an arbitrary formula of \\(\\bID_1\\) and\n\\(A(x,F)\\) arises from \\(A(x,P)\\) by replacing every occurrence of\n\\(P(t)\\) in the formula by \\(F(t)\\). Collectively these axioms assert\nthat \\(I_A\\) is the least fixed point of \\(\\Gamma_A\\), or more\naccurately the least among all sets of natural numbers definable in\nthe language of \\(\\bID_1\\). \\(\\bID_1^i\\) will be used to denote the\nintuitionistic version. Its subtheory \\(\\bID_{1}^{i}(\\cO)\\) is\nobtained by just adding the predicate symbol \\(I_A\\) and the\npertaining axioms (Id1) and (Id2), where \\(\\Gamma_A\\) is the operator\nthat defines Kleene’s \\(\\cO\\) (cf.\n Definition 5.1). \nBy a complicated passage through formal theories for choice sequences\nit was known that the theory \\(\\bID_{1}\\) can be reduced to\n\\(\\bID_{1}^{i}(\\cO)\\). The first ordinal analysis for the theory\n\\(\\bID_{1}^{i}(\\cO)\\) was obtained by Howard (1972). Via the known\nproof-theoretical reductions this entailed also an ordinal analysis\nfor \\(\\bID_{1}\\). The proof-theoretic ordinal of \\(\\bID_{1}\\) is the\nBachmann-Howard ordinal, which is denoted by\n\\({\\psi_{\\sOmega_{1}}(\\varepsilon_{\\Omega_1+1})}\\) in the system of\n Definition 5.11. \nAs inductively defined sets can be the starting point of another\ninductive definition, the procedure of inductively defining predicates\ncan be iterated along any well-ordering \\(\\nu\\) in a uniform way. This\nleads to the theories \\(\\bID_{\\nu}\\) which allow one to formalize\n\\(\\nu\\)-times iterated inductive definitions, where \\(\\nu\\) stands for\na primitive recursive well-ordering. If \\(\\nu\\) is a well-ordering on\nconstructive grounds then also the \\(\\nu\\)-times iterated version of\nKleene’s \\(\\cO\\) has a clear constructive meaning. As a result\nthe formal theories \\({{\\bID}}^i_{\\nu}(\\cO)\\) that embody this process\nare constructively justified. The topic of theories of iterated\ninductive definitions was flourishing at the 1968 conference on\nIntuitionism and Proof Theory in Buffalo (see Kino et al. 1970). One\nof the main proof-theoretic goals was to find a reduction of the\nclassical theories \\(\\bID_{\\nu}\\) to their intuitionistic counterparts\n\\({{\\bID}}^i_{\\nu}(\\cO)\\). This was all the more desirable because of\nknown reductions of important fragments of second order arithmetic to\ntheories of the former kind. Friedman (1970) had shown that the second\norder system with the \\(\\Sigma _2^1\\)-axiom of choice can be\ninterpreted in the system\n\\(\\hbox{($\\Pi_1^1$-CA)}_{<{\\varepsilon_0}}\\) of less than\n\\({\\varepsilon_0}\\)-fold iterated \\(\\Pi_1^1\\)-comprehensions and\nFeferman (1970a) had shown that less than \\(\\nu\\)-fold iterated\n\\(\\Pi_1^1\\)-comprehensions could be interpreted in the system  \nfor \\(\\nu=\\omega^{\\gamma}\\) with \\(\\gamma\\) limit. However, Zucker\n(1973) showed that there are definite obstacles to a straightforward\nreduction of the theories \\(\\text{ID}_{\\nu}\\) for \\(\\nu > 1\\) to\ntheir intuitionistic cousins. Sieg (1977) attacked the problem by a\nmethod adapted from Tait (1970) who had used cut elimination for an\ninfinitary propositional logic with formulae indexed over constructive\nnumber classes to obtain a consistency proof for second order\narithmetic theory with the schema of \\(\\Sigma _2^1\\) dependent\nchoices. He achieved a reduction of \\(\\bID_{<\\nu}\\) to\n\\(\\bID_{<\\nu}^i(\\cO)\\) for limit \\(\\nu\\) by carrying out the proof\ntheory for a system of \\(\\PL_{\\alpha}\\) of propositional logic with\ninfinitely long conjunctions and disjunctions indexed over the\nconstructive number classes \\(\\cO_{\\alpha}\\) for \\(\\alpha<\\nu\\)\ninside \\(\\bID^i_{\\alpha+1}(\\cO)\\). As \\(\\bID_{\\alpha}\\) can be reduced\nto \\(\\PL_{\\alpha}\\) this brought about the reduction. Ordinal analyses\nfor theories of iterated inductive definitions, first for finite and\nthen also for transfinite iterations, were obtained by Pohlers using\nTakeuti’s reduction procedure for \\(\\Pi_1^1\\)-comprehension (see\nPohlers 1975, 1977). Working independently, Buchholz (1977a) used a new type of rules, dubbed\n\\(\\Omega_{\\mu+1}\\)-rules to recapture these results without use of\nTakeuti’s methods. These rules are an extension of the\nΩ-rule described in\n Definition C.3. \nThe ordinal representation systems encountered so far are not\nsufficient for expressing the strength of theories of iterated\ninductive definitions nor that of the strongest of the standard system\nof reverse mathematics, \\((\\Pi^1_1{\\Hy}\\bCA)_0\\). Therefore we\nintersperse a brief account of how to proceed onwards, adumbrating the\nmain ideas. \nBachmann’s bold move of using large ordinals to generate names\nfor small ordinals was a very important idea. To obtain ordinal\nanalyses of ever stronger theories one has to find new ways of\ndefining ordinal representation systems that can encapsulate their\nstrength. The latter goes hand in hand with the development of new cut\nelimination techniques that are capable of removing cuts in\n(infinitary) proof systems with strong reflection rules. Ordinal\nrepresentations, however, appear to pose a considerable barrier to\nunderstanding books and articles in this research area. Nonetheless we\nthink that they are the best way to express the proof-theoretic\nstrength of a theory as they provide a scale by means of which one can\nget a grasp of how much stronger a theory \\(S_1\\) is than another\ntheory \\(S_2\\) (rather than the bland statement that \\(S_1\\) is\nstronger than \\(S_2\\)). \nAs an example we will introduce an ordinal representation system which\ncharacterizes the theory \\((\\Pi^1_1{\\Hy}\\bCA)+{{\\BI}}\\), following\nBuchholz 1977b. It is\nbased on certain ordinal functions \\(\\psi_{\\sOmega_n}\\) which are\noften called collapsing functions. The definition of these\nfunctions, that is of the value \\({\\psi_{\\sOmega_{n}}(\\alpha)}\\) at\n\\(\\alpha\\), proceeds by recursion on \\(\\alpha\\) and gets intertwined\nwith the definition of sets of ordinals\n\\(C^{{\\Omega_{\\omega}}}(\\alpha,\\beta)\\), dubbed “Skolem\nhulls” since they are defined as the smallest structures closed\nunder certain functions specified below. \nLet \\(\\bbN^+\\) be the natural numbers without 0. Below we shall assume\nthat \\(\\Omega_n\\) \\((n\\in\\bbN^+)\\) is a “large” ordinal\nand that \\(\\omega<\\Omega_n <\\Omega_{n+1}\\). Their limit,\n\\(\\sup_{n\\in\\bbN^+}\\Omega_n\\), will be denoted by\n\\(\\Omega_{\\omega}\\). \nDefinition 5.11 By recursion on \\(\\alpha\\)\nwe define:  \nAt this point it is not clear whether \\({\\psi_{\\sOmega_{n}}(\\alpha)}\\)\nwill actually be defined for all \\(\\alpha\\) since there might not\nexist a \\(\\rho<\\Omega_n\\) such that  \nThis is where the “largeness” of \\(\\Omega_n\\) comes into\nplay. One (easy) way of guaranteeing this consists in letting\n\\(\\Omega_n\\) be the \\(n^{th}\\) uncountable regular cardinal, that is\n\\(\\Omega_n\\coloneqq \\aleph_n\\). However, such strong set-theoretic\nassumptions can be avoided. For instance, it suffices to let\n\\(\\Omega_n\\) be the \\(n^{th}\\) recursively regular ordinal (which is a\ncountable ordinal) (see Rathjen 1993a). \nTo get a better feel for what \\(\\psi_{\\sOmega_{n}}\\) is doing, note\nthat if \\(\\rho={\\psi_{\\sOmega_{n}}(\\alpha)}\\), then\n\\(\\rho<\\Omega_n\\) and with \\([\\rho,\\Omega_n)\\) being the interval\nconsisting of ordinals \\(\\rho\\leq\\alpha<\\Omega_n\\) one has  \nthus the order-type of the ordinals below \\(\\Omega_n\\) which belong to\nthe “Skolem hull” \\(C^{{\\Omega_{\\omega}}}(\\alpha,\\rho)\\)\nis \\(\\rho\\). In more pictorial terms, \\(\\rho\\) is said to be the\n\\(\\alpha^{th}\\) collapse of \\(\\Omega_n\\) since the order-type\nof \\(\\Omega_n\\) viewed from within the structure\n\\(C^{{\\Omega_{\\omega}}}(\\alpha,\\rho)\\) is actually \\(\\rho\\). \nThe ordinal representation system we are after is provided by the set\n \nwhere \\(\\varepsilon_{\\Omega_{\\omega}+1} \\) is the least epsilon number\nafter \\(\\Omega_{\\omega}\\), i.e., the least ordinal\n\\(\\eta>\\Omega_{\\omega}\\) such that \\(\\omega^{\\eta}=\\eta\\). The\nproof-theoretic ordinal of \\((\\Pi^1_1{\\Hy}\\bCA)+{{\\BI}}\\) is\n\\({\\psi_{\\sOmega_{1}}(\\varepsilon_{\\Omega_{\\omega}+1})}\\). Although\nthe definition of the set\n\\(C^{{\\Omega_{\\omega}}}(\\varepsilon_{\\Omega_{\\omega}+1},0)\\) and its\nordering is set-theoretic, it turns that it also has a purely\nprimitive recursive definition which can be given in a fragment of\nPRA. Thus the set-theoretic presentation mainly\nserves the purpose of a “visualization” of an elementary\nwell-ordering. \nThe pattern of definition exhibited in\n Definition 5.11\n continues for stronger systems, albeit only as a basic template since\nfor theories beyond the level of \\((\\Delta^1_2{\\Hy}\\bCA)+{{\\BI}}\\)\nsubstantially new ideas are required. Analogies between large\nset-theoretic ordinals (cardinals) and recursively large ordinals on\nthe one hand and ordinal representation systems on the other hand can\nbe a fruitful source of inspiration for devising new representation\nsystems. More often than not, hierarchies and structural properties\nthat have been investigated in set theory and recursion theory on\nordinals turn out to have proof-theoretic counterparts. \nUsing an extended version of the representation system from\n Definition 5.11\n if\n \\(\\nu>\\omega\\),[29]\n the outcome of all the work on the theories of inductive definitions\ncan be summarized by the following\n theorem.[30] \nTheorem 5.12 For recursive \\(\\nu\\),  \nA generalized treatment of theories of iterated inductive definitions\nfor arbitrary well-orderings and of autonomous iteration was carried\nout in Rathjen 1988, 2010. These theories are stronger than\n\\((\\Delta^1_2{\\Hy}\\bCA)\\) if \\(\\nu\\geq\\varepsilon_0\\). \n\n Theorem 5.12\n played an important role in determining the exact strength of some\nfragments of \\(\\bZ_2\\). The major ordinal-theoretic results pertaining\nto subsystems of \\(\\bZ_2\\) of the pre-1980 area given in the next\n theorem.[31] \nTheorem 5.13 \nThe next challenge after \\((\\Delta^1_2{\\Hy}\\bCA)\\) was posed by the\ntheory \\((\\Delta^1_2{\\Hy}\\bCA)+{{\\BI}}\\). Its treatment not only\nrequired a considerably stronger ordinal representation system but\nalso coincided with a shift away from \\(\\cL_2\\) theories and theories\nof iterated inductive definitions to a direct proof-theoretic\ntreatment of set theories. Pioneering work on the proof theory of set\ntheories is mainly due to Jäger (1980, 1982). The analysis of\n\\((\\Delta^1_2{\\Hy}\\bCA)+{{\\BI}}\\) in Jäger & Pohlers 1982\nprovides a particularly nice application of this new approach which\nhas been called admissible proof theory, owing to its concern\nwith theories extending Kripke-Platek set theory by axioms asserting\nthe existence of admissible sets (for some details see\n Appendix D.1). \nTheorem 5.14 \\(\\lvert (\\Delta^1_2{\\Hy}\\bCA)+\\BI\\rvert =\n\\psi_{\\sOmega_1}(\\varepsilon_{I+1})\\) \nThe “I” in the foregoing notation is supposed to be\nindicative of “inaccessible cardinal”. Indeed,\nthe easiest way to build an extended ordinal representation system\nsufficient unto this task (modeled on\n Definition 5.11)\n is to add an inaccessible I, close the Skolem hulls under\n\\(\\xi\\mapsto \\Omega_\\xi\\) for \\(\\xi<I\\) and introduce collapsing\nfunctions \\(\\psi_{\\pi}\\) for all \\(\\pi\\) of either form I or\n\\(\\Omega_{\\xi+1}\\). \nThe goal of giving an ordinal analysis of full second order arithmetic\nhas not been attained yet. For many years \\(\\Pi^1_2\\)-comprehension\nposed a formidable challenge and the quest for its ordinal analysis\nattained something of a holy grail status (cf. Feferman 1989). At\nfirst blush it might be difficult to see why the latter comprehension\nis so much more powerful than \\(\\Delta^1_2\\)-comprehension (plus\n\\({{\\BI}}\\)). To get a sense for the enormous difference, it seems\nadvisable to work in (admissible) set theory and consider a hierarchy\nof recursively large ordinal notions wherein these comprehension\nschemes correspond to the bottom and the top end of the scale,\nrespectively. That is discussed in\n Appendix D.\n Here we just mention a few reductions to “constructive”\nframeworks. The reductions we have in mind, underlie a broadened view\nof “constructivity”. Constructive theories of functions\nand sets that relate to Bishop’s constructive mathematics as\ntheories like ZFC relate to Cantorian set theory have\nbeen proposed by Myhill, Martin–Löf, Feferman and Aczel.\nAmong those are Feferman’s constructive theory of operations and\nclasses, \\(\\bT_0\\) in 1975 and 1979, Martin-Löf’s\nintuitionistic type theory (1984) and constructive set theory,\nespecially Constructive Zermelo-Fraenkel Set Theory,\nCZF, the latter also combined with the regular\nextension axiom, REA. By employing an ordinal\nanalysis for set theory KPi which is an extension of\nKripke-Platek set theory via an axiom asserting that every set is\ncontained in an admissible set (see\n Appendix D)\n it has been shown that KPi and consequently\n\\((\\Delta^1_2{\\Hy}\\bCA)+{{\\BI}}\\) can be reduced to both of these\ntheories: \nTheorem 5.15 (Feferman 1975; Jäger\n1983; Jäger & Pohlers 1982; Rathjen 1993b)\n\\((\\Delta^1_2{\\Hy}\\bCA)+\\BI\\), KPi, \\(\\bT_0\\) and\nCZF+REA are proof-theoretically\nequivalent. In particular, these theories prove the same theorems in\nthe negative arithmetic fragment. \nTheorem 5.16 (Rathjen 1993b; Setzer 1998)\nThe soundness of the negative arithmetic fragment of\n\\(\\Delta^1_2-\\bCA+\\BI\\) and KPi is provable in\nMartin-Löf’s 1984 type theory. \nA detailed account of these results has been given in section 3 of\nRathjen 1999a. \nProof theory has become a large subject with many specialized branches\nthat can be mathematically quite complex. So we have tried to present\ndevelopments close to the main artery of its body, starting with its\ninception at the beginning of the twentieth century and ending with\nresults from the close of the century. The theorems mentioned at the\nend of section 5 foreshadow investigations in the twenty-first century\nthat are presented in Rathjen (2018) and concern relationships\nbetween Feferman’s systems of explicit mathematics and\nMartin-Löf type theories; the paper touches also on the\ncompletely new developments of homotopy type theory (see Awodey 2012).\nSome additional contemporary proof theoretic developments are\ndescribed in appendices\n D,\n E and\n F.\n The theme of\n Appendix E,\n the extraction of computational information from proofs in particular\nformal theories, is the central topic of Schwichtenberg and\nWainer’s 2012. They deal with theories of arithmetic and\nanalysis up to \\((\\Pi^1_1{\\Hy}\\bCA)_0\\). Standard texts on proof\ntheory covering ordinal analysis are Takeuti 1985 and Schütte\n1977. First steps into ordinal analysis are taken in Pohlers 2009.\nFinally, some new directions of proof theoretic work are taken in\ncontributions to both Kahle and Rathjen 2015 and Jäger and\nSieg 2018. \nLet us also report on progress on the methodological issues the\nfinitist consistency program was to address. First of all, due to\nquite a bit of important historical work, we have a much better grasp\nof the evolution of the program in the 1920s and its roots in the\ndevelopment toward modern structuralist mathematics in the nineteenth\ncentury. The work of editing Hilbert’s unpublished lecture notes\nhas opened a treasure of\n information.[32]\n The connections to the development in nineteenth century mathematics\nare hinted at in\n Appendix A,\n but they are explored in greater depth, for example, in\nFerreirós 2008; Reck 2003, 2013; and the papers Sieg wrote on\nDedekind with Morris (forthcoming) and Schlimm (2005, 2017),\nrespectively. Secondly, as to the properly methodological issues, we\npresented some broad considerations in\n section 4.1,\n namely, that consistency proofs should be given relative to\n“constructive” theories. We did not make any remarks about\nwhat is characteristic of a constructive perspective and why such a\nperspective has not only a mathematical, but also a philosophical\npoint. There is, of course, a very rich literature. Let us point to\nsome informative sources: van Atten (2007) as defending\nBrouwer’s views, Martin-Löf (1984) as exposing the\nphilosophical motivation for his type theory, Feferman (1988, 2000)\ndiscussing the foundational point of proof theory, Bernays (1976) as\npresenting crucial aspects of an informed philosophy of mathematics,\nand (Sieg 2013) as\nexplicating (the context for) his reductive\nstructuralism. \nBack to proof theory: We have to admit that we neglected some\nclassical topics. One is the study of different proof systems and\ntheir relationships going back to Gentzen’s dissertation (1935).\nIn their Basic Proof Theory, Troelstra and Schwichtenberg\n(2000) give an excellent selection, but some important calculi such as\nthe Schütte proof systems are not covered (see, for example,\nSchütte 1960b, 1977). They also do not cover proof systems for\ntemporal and modal logic, neither are substructural logics\n presented.[33] \nA second omission concerns Bounded Arithmetic, where feasibility\nissues are a central focus: one studies formal theories with provably\nrecursive functions that form very small subclasses of the primitive\nrecursive ones. Indeed, the class of the provably total functions of\nBuss’ base theory is that of functions that can be computed in\npolynomial time, and there was the hope that proof theoretic\ninvestigations might contribute novel results in complexity theory. A\nthird omission concerns proof mining; that line of deep mathematical\ninvestigations using proof theoretic tools was initiated by Kreisel\n(1958, 1982) and Luckhardt (1989), but really perfected only by Kohlenbach\n(2007). We hinted at the work of his school at the very end of\n section 4.1. \nProof theory, as we described it, deals primarily with formal proofs\nor derivations. Hilbert aimed, however, as we pointed out in\n section 1,\n for a more general analysis of ordinary, informal mathematical\nproofs. For Gentzen in his 1936, “the objects of proof theory\nshall be the proofs carried out in mathematics proper” (p. 499). The aim of Hilbert and his\ncollaborators was undoubtedly to achieve a deeper mathematical and\nconceptual understanding, but also to find general methods of proof\nconstruction in formal calculi. This is now being pursued in the very\nactive area of using powerful computers for the interactive\nverification of proofs and programs as well as the fully automated\nsearch for proofs of mathematical\n theorems.[34]\n That can be pursued with a cognitive scientific purpose of modeling\nmathematical reasoning (see Sieg 2010 and Ganesalingam & Gowers 2017). It is clearly\nin the spirit of Hilbert who articulated matters in his second Hamburg\ntalk of 1927 as follows: \nThe formula game … has, besides its mathematical value, an\nimportant general philosophical significance. For this formula game is\ncarried out according to certain definite rules, in which the\ntechnique of our thinking is expressed. These rules form a closed\nsystem that can be discovered and definitively stated. \nThen he continues with a provocative statement about the cognitive\ngoal of proof theoretic investigations. \nThe fundamental idea of my proof theory is none other than to describe\nthe activity of our understanding, to make a protocol of the rules\naccording to which our thinking actually proceeds. (Hilbert 1927\n[1967: 475])  \nIt is clear to us, and it was clear to Hilbert, that mathematical\nthinking does not proceed in the strictly regimented ways imposed by\nan austere formal theory. Though formal rigor is crucial, it is not\nsufficient to shape proofs intelligibly or to discover them\nefficiently, even in pure logic. Recalling the principle that\nmathematics should solve problems “by a minimum of blind\ncalculation and a maximum of guiding thought”, we have to\ninvestigate the subtle interaction between understanding and\nreasoning, i.e., between introducing concepts and proving\ntheorems.","contact.mail":"rathjen@maths.leeds.ac.uk","contact.domain":"maths.leeds.ac.uk"},{"date.published":"2018-08-13","url":"https://plato.stanford.edu/entries/proof-theory/","author1":"Michael Rathjen","author2":"Wilfried Sieg","author1.info":"http://www1.maths.leeds.ac.uk/~rathjen/rathjen.html","entry":"proof-theory","body.text":"\n\n\nProof theory is not an esoteric technical subject that was invented to\nsupport a formalist doctrine in the philosophy of mathematics; rather,\nit has been developed as an attempt to analyze aspects of mathematical\nexperience and to isolate, possibly overcome, methodological problems\nin the foundations of mathematics. The origins of those problems,\nforcefully and sometimes contentiously formulated in the 1920s, are\ntraceable to the transformation of mathematics in the nineteenth\ncentury: the emergence of abstract mathematics, its reliance on set\ntheoretic notions, and its focus on logic in a broad, foundational\nsense. Substantive issues came to the fore already in the mathematical\nwork and the foundational essays of Dedekind and Kronecker; they\nconcerned the legitimacy of undecidable concepts, the existence of\ninfinite mathematical objects, and the sense of non-constructive\nproofs of existential statements.\n\n\nIn an attempt to mediate between conflicting foundational positions,\nHilbert shifted issues, already around 1900, from a mathematical to a\nvaguely conceived metamathematical level. That approach was rigorously\nrealized in the 1920s, when he took advantage of the possibility of\nformalizing mathematics in deductive systems and investigated the\nunderlying formal frames from a strictly constructive,\n“finitist” standpoint. Hilbert’s approach raised\nfascinating metamathematical questions—from semantic\ncompleteness through mechanical decidability to syntactic\nincompleteness; however, the hoped-for mathematical resolution of the\nfoundational issues was not achieved. The failure of his finitist\nconsistency program raised and deepened equally fascinating\nmethodological questions. A broadened array of problems with only\npartial solutions has created a vibrant subject that spans\ncomputational, mathematical, and philosophical issues—with a\nrich history.\n\n\nThe main part of our article covers these exciting investigations for\nan expanded Hilbert Program through 1999—with special, detailed\nattention to results and techniques that by now can be called\n“classical” and are of continued interest. Newer, but\nstill closely connected developments are sketched in Appendices: the\nproof theory of set theories in Appendix D combinatorial\nindependence results in Appendix E, and provably total\nfunctions in Appendix F. Here (infinitary) sequent calculi and\nsuitable systems of ordinal notations are crucial proof theoretic\ntools. However, we discuss in section 4.2 also Gödel’s\nDialectica Interpretation and some of its extensions as an alternative\nfor obtaining relative consistency proofs and describe in section\n5.2.1 the systematic attempt of completing the incomplete through\nrecursive progressions. Both topics are analyzed further in Appendix\nC.2 and Appendix B, respectively. To complete this bird’s eye\nview of our article, we mention that the Epilogue, section 6,\nnot only indicates further proof theoretic topics, but also some\ndirections of current research that are connected to proof theory and\nof deep intrinsic interest. We have tried to convey the vibrancy of a\nsubject that thrives on concrete computational and (meta-)\nmathematical work, but also invites and is grounded in general\nphilosophical reflection.\n\nHilbert viewed the axiomatic method as the crucial tool for\nmathematics (and rational discourse in general). In a talk to the\nSwiss Mathematical Society in 1917, published the following year as\nAxiomatisches Denken (1918), he articulates his broad\nperspective on that method and presents it “at work” by\nconsidering, in detail, examples from various parts of mathematics and\nalso from physics. Proceeding axiomatically is not just developing a\nsubject in a rigorous way from first principles, but rather requires,\nfor advanced subjects, their deeper conceptual organization and\nserves, for newer investigations, as a tool for discovery. In his talk\nHilbert reflects on his investigations of the arithmetic of real\nnumbers and of Euclidean geometry from before 1900. We emphasize the\nparticular form of his axiomatic formulations; they are not logical\nformulations, but rather mathematical ones: he defines Euclidean space\nin a similar way as other abstract notions like group or field;\nthat’s why we call it structural\n axiomatics.[1]\n However, Hilbert turns from the past and looks to the future,\npointing out a programmatic direction for research in the\nfoundations of mathematics; he writes: \nTo conquer this field [concerning the foundations of mathematics] we\nmust turn the concept of a specifically mathematical proof itself into\nan object of investigation, just as the astronomer considers the\nmovement of his position, the physicist studies the theory of his\napparatus, and the philosopher criticizes reason itself. \nHe then asserts, “The execution of this program is at present\nstill an unsolved task”. During the following winter term\n1917–18, Hilbert—with the assistance of Paul\nBernays—gave a lecture course entitled Prinzipien der\nMathematik. Here modern mathematical logic is invented in one\nfell swoop and completes the shift from structural to formal\naxiomatics. This dramatic shift allows the constructive, elementary\ndefinition of the syntax of theories and, in particular, of the\nconcept of proof in a formal theory. This fundamental insight\nunderpins the articulation of the consistency problem and seems to\nopen a way of proving, meta-mathematically, that no proof in a formal\ntheory establishes a contradiction. \nThat perspective is formulated first in a talk Bernays presented in\nthe fall of 1921, published as “Über Hilberts Gedanken zur\nGrundlegung der Mathematik” (1922). Starting with a discussion\nof structural axiomatics and pointing out its assumption of a\nsystem of objects that satisfies the axioms, he asserts this\nassumption contains “something so-to-speak transcendent for\nmathematics”. He raises the question, “which principled\nposition should be taken with respect to it?” Bernays believes\nthat it might be perfectly coherent to appeal to an intuitive\ngrasp of the natural number sequence or even of the manifold of real\nnumbers. However, that could not be an intuition in any primitive\nsense and would conflict with the tendency of the exact sciences to\nuse only restricted means to acquire knowledge. \nUnder this perspective we are going to try, whether it is not possible\nto give a foundation to these transcendent assumptions in such a way\nthat only primitive intuitive knowledge is used. (Bernays 1922:\n11) \nMeaningful mathematics is to be based, Bernays demands, on primitive\nintuitive knowledge that includes, however, induction concerning\nnatural numbers—both as a proof and definition principle. In the\noutline for the lectures Grundlagen der Mathematik to be\ngiven in the winter term 1921–22, Bernays discusses a few weeks\nafter his talk “constructive arithmetic” and then the\n“broader formulation of constructive thought”: \nConstruction of the proofs, by means of which the formalization of the\nhigher inferences is made possible and the consistency problem is\nbecoming accessible in a general way.  \nBernays concludes the outline by suggesting, “This would be\nfollowed by the development of proof theory”. The outline was\nsent to Hilbert on 17 October 1921 and it was followed strictly in the\nlectures of the following term—with only one terminological\nchange: “constructive” in the above formulations is turned\ninto\n “finitist”.[2] \nBernays’s notes of the 1921/22 lectures reflect the consequence\nof that change in attitude. They contain a substantial development of\n“finitist arithmetic” and “finitist logic” in\na quantifier-free formalism. Finitist arithmetic involves induction\nand primitive\n recursion[3]\n from the outset, and the central metamathematical arguments all\nproceed straightforwardly by induction on proof figures. The third\npart of these lectures is entitled The grounding of the\nconsistency of arithmetic by Hilbert’s new proof theory.\nHere we find the first significant consistency proof—from a\nfinitist\n perspective.[4]\n The proof is sketched in Hilbert’s Leipzig talk (Hilbert 1923:\n184) and was presented in a modified form during the winter term of\n1922/23; in that form the proof is given in Ackermann 1925: 4–7.\nAckermann’s article was submitted for publication in early 1924,\nand by then the proof had taken on a certain canonical form that is\nstill found in the presentation of Hilbert and Bernays 1934:\n220–230. Let us see what was achieved by following\nAckermann’s concise discussion. \nThe proof is given in section II of Ackermann’s paper entitled,\ntellingly, The consistency proof before the addition of the\ntransfinite axioms. Ackermann uses a logical calculus in\naxiomatic form that is taken over from Hilbert’s lectures and is\ndiscussed below in\n section 2.\n Here we just note that it involves two logical rules, namely, modus\nponens and substitution (for individual, function and statement\nvariables) in axioms. The non-logical axioms concern identity, zero\nand successor, and recursion equations that define primitive recursive\nfunctions. The first step in the argument turns the linear proof into\na tree, so that any formula occurrence is used at most once as a\npremise of an inference (Auflösung in Beweisfäden);\nthis is done in preparation for the second step, namely, the\nelimination of all necessarily free variables (Ausschaltung der\nVariablen); in the third step, the numerical value of the closed\nterms is computed (Reduktion der Funktionale). The resulting\nsyntactic configurations, a Beweisfigur, contains now only\nnumerical formulae that are built up from equations or\ninequations between numerals and Boolean connectives; these formulae\ncan be effectively determined to be true or false. By induction on the\n“Beweisfigur” one shows that all its component\nformulae are true; thus, a formula like \\(0\\ne 0\\) is not provable.\nThe induction principle can be directly incorporated into these\nconsiderations when it is formulated as a rule for quantifier-free\nstatements. That was not done in Ackermann’s discussion of the\nproof, but had been achieved already by Hilbert and Bernays in their\n1922/23 lectures. \nThese proof theoretic considerations are striking and important as\nthey involve for the first time genuine transformations of formal\nderivations. Nevertheless, they are preliminary as they concern a\nquantifier-free theory that is a part of finitist mathematics\nand need not be secured by a consistency proof. What has to\nbe secured is “transfinite logic” with its “ideal\nelements”, as Hilbert put it later. The strategy was direct and\nstarted to emerge already in 1921. First, introduce functional terms\nby the transfinite\n axiom[5] \nand define quantifiers by  \nand \nUsing the epsilon terms, quantifiers can now be eliminated from proofs\nin quantificational logic, thus transforming them into quantifier-free\nones. Finally, the given derivation allows one, so it was conjectured,\nto determine numerical values for the epsilon terms. In his Leipzig\ntalk of September 1922, published in 1923, Hilbert discussed this\nAnsatz for eliminating quantifiers and reducing the\ntransfinite case to that of the quantifier-free theory. He presented\nthe actual execution of this strategic plan only “for the\nsimplest case” (in Hilbert 1923: 1143–1144). However, the\ntalk was crucial in the development of proof theory and the finitist\nprogram: “With the structure of proof theory, presented to us in\nthe Leipzig talk, the principled form of its conception had been\nreached”. That is how Bernays characterizes its achievement in\nhis essay on Hilbert’s investigations of the foundations of\narithmetic (1935: 204) \nAckermann continued in section III of his 1925 at the very spot where\nHilbert and Bernays had left off. His paper, submitted to\nMathematische Annalen in March of 1924, and the corrective\nwork he did in 1925 led to the conviction that the consistency of\nelementary arithmetic had been established. The corrective work had\nbeen done to address difficulties von Neumann had pointed out, but was\nnot published by Ackermann; it was only presented in the second volume\nof Hilbert and Bernays 1939 (pp.\n 93–130).[6]\n Von Neumann’s own proof theoretic investigations, submitted to\nMathematische Zeitschrift in July 1925, were published under\nthe title Zur Hilbertschen Beweistheorie in 1927.\nHilbert’s 1928 Bologna Lecture prominently took\nAckermann’s and von Neumann’s work as having established\nthe consistency of elementary arithmetic, the proof making use only of\nfinitist principles. Let F be a theory containing exclusively\nsuch principles, like primitive recursive arithmetic\nPRA; the principles of PRA consist\nof the Peano axioms for zero and successor, the defining equations for\nall primitive recursive functions (defined in\n note 3),\n and quantifier-free induction. Now the significance of a consistency\nproof in F can be articulated as follows: \nTheorem 1.1 Let T be a theory that\ncontains a modicum of arithmetic and let A be a\n\\(\\Pi^0_1\\)-statement, i.e., one of the form \\(\\forall\nx_1\\ldots\\forall x_n\\,P(x_1,\\ldots,x_n)\\) with quantifiers ranging\nover naturals and P a primitive recursive predicate, i.e., a\npredicate with a primitive recursive characteristic function. If\nF proves the consistency of T and T proves\nA, then F proves A. \nThis theorem can be expressed and proved in PRA and\nensures that a T-proof of a “real”, finitistically\nmeaningful statement A leads to a finitistically valid statement.\nThis point is made clear in Hilbert’s 1927-Hamburg lecture\n(Hilbert 1927). There he takes A to be the Fermat proposition\nand argues that if we had a proof of A in a theory containing\n“ideal” elements, a finistist consistency proof for that\ntheory would allow us to transform that proof into a finitist one. \nThe belief that Ackermann and von Neumann had established the\nconsistency of elementary arithmetic was expressed as late as December\n1930 by Hilbert in his third Hamburg talk (Hilbert 1931a) and by Bernays in April 1931\nin a letter to Gödel (see Gödel 2003: 98–103). Bernays\nasserts there that he has “repeatedly considered\n[Ackermann’s modified proof] and viewed it as correct”. He\ncontinues, referring to Gödel’s incompleteness results, \nOn the basis of your results one must now conclude, however, that that\nproof cannot be formalized within the system Z [of elementary\nnumber theory]; this must in fact hold true even if the system is\nrestricted so that, of the recursive definitions, only those for\naddition and multiplication are retained. On the other hand, I\ndon’t see at which place in Ackermann’s proof the\nformalization within Z should become impossible, … \nAt the end of his letter, Bernays mentions that Herbrand misunderstood\nhim in a recent conversation on which Herbrand had reported in a\nletter to Gödel with a copy to Bernays. Not only had Herbrand\nmisunderstood Bernays, but Bernays had also misunderstood Herbrand as\nto the extent of the latter’s consistency result that was to be\npublished a few months later as Herbrand 1931. Bernays understood\nHerbrand as having claimed that he had established the consistency of\nfull first-order arithmetic: Herbrand’s system is indeed a\nfirst-order theory with a rich collection of finitist functions, but\nit uses the induction principle only for quantifier-free\n formulae.[7]\n Gödel asserted in December 1933 that this theorem of\nHerbrand’s was even then the strongest result that had been\nobtained in the pursuit of Hilbert’s finitist program, and he\nformulated the result in a beautiful informal way as follows: \nIf we take a theory, which is constructive in the sense that each\nexistence assertion made in the axioms is covered by a construction,\nand if we add to this theory the non-constructive notion of existence\nand all the logical rules concerning it, e.g., the law of the excluded\nmiddle, we shall never get into any contradiction. (Gödel 1933:\n52) \nGödel himself had been much more ambitious in early 1930; his\ngoal was then to prove the consistency of analysis! According to Wang\n(1981: 654), his idea was “to prove the consistency of analysis\nby number theory, where one can assume the truth of number theory, not\nonly the consistency”. The plan for establishing the consistency\nof analysis relative to number theory did not work out, instead\nGödel found that sufficiently strong formal theories like\nPrincipia Mathematica and Zermelo-Fraenkel set theory are\n(syntactically) incomplete. \nIn 1931 Gödel published a paper (1931a) that showed that there\nare true arithmetic statements that cannot be proved in the formal\nsystem of Principia Mathematica, assuming PM to be\nconsistent. His methods not only applied to PM but to any\nformal system that contains a modicum of arithmetic. A couple of\nmonths after Gödel had announced this result at a conference in\nKönigsberg in September 1930, von Neumann and Gödel\nindependently realized that a striking corollary could be drawn from\nthe incompleteness theorem. Every consistent and effectively\naxiomatized theory that allows for the development of basic parts of\narithmetic cannot prove its own consistency. This came to be known as\nthe second incompleteness theorem. (For details on these\ntheorems and their history see\n appendix A.4)\n The second incompleteness theorem refutes the general ambitions of\nHilbert’s program under the sole and very plausible assumption\nthat finitist mathematics is contained in one of the formal theories\nof arithmetic, analysis or set theory. As a matter of fact,\ncontemporary characterizations of finitist mathematics have elementary\narithmetic as an upper\n bound.[8]\n In response to Gödel’s result, Hilbert attempted in his\nlast published paper (1931b) to formulate a strategy for consistency\nproofs that is reminiscent of his considerations in the early 1920s\n(when thinking about the object theories as constructive) and clearly\nextends the finitist standpoint. He introduced a broad constructive\nframework that includes full intuitionist arithmetic and suggested\nextendibility of the new “method” to “the case of\nfunction variables and even higher sorts of variables”. He also\nformulated a new kind of rule that allowed the introduction of a new\naxiom \\(\\forall x A(x)\\) as soon as all the numerical instances\n\\(A(n)\\) had been established by finitist proofs; in 1931 that is done\nfor quantifier-free \\(A(x)\\), whereas in 1931b that is extended to\nformulae of arbitrary complexity. The semi-formal calculi, which\narticulate the broader framework, are based on rules that reflect\nmathematical practice, but also define the meaning of logical\nconnectives. Indeed, Hilbert’s reasons for taking them to be\nevidently consistent are expressed in a single sentence: “All\ntransfinite rules and inference schemata are consistent; for they\namount to definitions”. Adding the tertium non datur in\nthe form  \nyields now the classical version of the theory and it is that addition\nthat has to be\n justified.[9]\n Hilbert’s problematic considerations for this new\nmetamathematical step inspired Gentzen’s\n“Urdissertation” when he began working in late 1931 on a\nconsistency proof for elementary\n arithmetic.[10] \nAs part of his “Urdissertation”, Gentzen had established\nby the end of 1932 the reduction of classical to intuitionist\narithmetic, a result that had also been obtained by Gödel.\nGentzen’s investigations led, finally in 1935, to his first\nconsistency proof for arithmetic. In the background was a normal form\ntheorem for intuitionist logic that will be discussed in the next\nsection together with Gentzen’s actual dissertation and the\nspecial calculi he introduced there. Now we just formulate the\nGentzen-Gödel result “connecting” classical\nfirst-order number theory PA with its intuitionist\nversion HA. The non-logical principles of these\ntheories aim at describing \\(\\fN\\), the arguably most important\nstructure in mathematics, namely, Dedekind’s simply infinite\nsystem \\(\\bbN\\) together with zero, successor, multiplication,\nexponentiation and the less-than relation:  \nThey are formulated in the first-order language that has the relation\nsymbols =, <, the function symbols S, +, \\(\\times\\),\nE and the constant symbol 0. The axioms comprise the usual\nequations for zero, successor, addition, multiplication,\nexponentiation, and the less-than relation. In addition, the\ninduction principle is given by the schema \n\n\\[\\tag{IND}\n{F(0)} \\land {\\forall x[F(x)\\to F(Sx)]\\to \\forall x F(x)}\n\\]\n\n for\nall formulae \\(F(x)\\) of the language. These principles together with\nclassical logic constitute the theory of first order\narithmetic or first order number theory, also known as\nDedekind-Peano arithmetic, PA; together with\nintuitionist logic they constitute intuitionistic first order\narithmetic commonly known as Heyting-arithmetic,\nHA. \nNow we are considering the syntactic translation \\(\\tau\\) from the\ncommon language of PA and HA into\nits “negative” fragment that replaces disjunctions\n\\(A\\lor B\\) by their de Morgan equivalent \\(\\neg (\\neg A\\land \\neg\nB)\\) and existential statements \\(\\exists x A(x)\\) by \\(\\neg \\forall\nx \\neg A(x)\\). The reductive result obtained by Gentzen and Gödel\nin 1933 is now stated quite easily: \nTheorem 1.2 \nPA proves the equivalence of A and \\(\\tau(A)\\)\nfor any formula A. \nIf PA proves A, then HA\nproves \\(\\tau(A)\\). \nFor atomic sentences like \\(1\\ne 1\\) the translation \\(\\tau\\) is\nclearly the identity, and the provability of \\(1\\ne 1\\) in\nPA would imply its provability in\nHA. Thus, PA is consistent relative\nto HA. This result is technically of great interest\nand had a profound effect on the perspective concerning the\nrelationship between finitism and intuitionism: finitist and\nintuitionist mathematics were considered as co-extensional; this\ntheorem showed that intuitionist mathematics is actually stronger than\nfinitist mathematics. Thus, if the intuitionist standpoint is taken to\nguarantee the soundness of HA, then it guarantees the\nconsistency of PA. The corresponding connection\nbetween classical and intuitionist logic had been established already\nby Kolmogorov (1925) who not only formalized intuitionist logic but\nalso observed the translatability of classical into intuitionist\nlogic. His work, though, seems not to have been noticed at the time or\neven in 1932, when Gentzen and Gödel established their result for\nclassical and intuitionist arithmetic. \nThe foundational discussion concerning extended\n“constructive” viewpoints is taken up in\n section 4.\n There, and throughout our paper the concepts of\n“proof-theoretic reducibility” and “proof-theoretic\nequivalence” will play a central role. The connection between\nPA and HA is paradigmatic and leads\nto the notion of proof-theoretic reduction. Before we can\nfurnish a precise definition, we should perhaps stress that many\nconcepts can be expressed in the language of PRA (as\nwell as PA) via coding, also known as Gödel\nnumbering. Any finite object such as a string of symbols or an\narray of symbols can be coded via a single natural number in such a\nway that the string or array can be retrieved from the number when we\nknow how the coding is done. Typical finite objects include formulae\nin a given language and also proofs in a theory. Talk about formulae\nor proofs can then be replaced by talking about predicates of numbers\nthat single out the codes of formulae and proofs, respectively. We\nthen say that the concepts of formula and proof have been arithmetized\nand thereby rendered expressible in the language of\nPRA. \nDefinition 1.3 Let \\(\\bT_1\\), \\(\\bT_2\\) be a\npair of theories with languages \\(\\cL_1\\) and \\(\\cL_2\\), respectively,\nand let \\(\\Phi\\) be a (primitive recursive) collection of formulae\ncommon to both languages. Furthermore, \\(\\Phi\\) should contain the\nclosed equations of the language of PRA. \nWe then say that \\(\\bT_1\\) is proof-theoretically\n\\(\\Phi\\)-reducible to \\(\\bT_2\\), written\n\\(\\bT_1\\leq_{\\Phi}\\bT_2\\), if there exists a primitive recursive\nfunction f such that  \nHere \\(\\rform_{\\Phi}\\) and \\(\\proof_{\\bT_i}\\) are arithmetized\nformalizations of \\(\\Phi\\) and the proof relation in \\(\\bT_i\\),\nrespectively, i.e., \\(\\rform_{\\Phi}(x)\\) expresses that x is\nthe Gödel number of a formula in \\(\\Phi\\) while\n\\(\\proof_{\\bT_i}(y,x)\\) expresses that y codes a proof in\n\\(\\bT_i\\) of a formula with Gödel number x. \n\\(\\bT_1\\) and \\(\\bT_2\\) are said to be proof-theoretically\n\\(\\Phi\\)-equivalent, written \\(\\bT_1\\equiv_{\\Phi}\\bT_2\\), if\n\\(\\bT_1\\leq_{\\Phi}\\bT_2\\) and \\(\\bT_2\\leq_{\\Phi}\\bT_1\\). \nThe appropriate class \\(\\Phi\\) is revealed in the process of reduction\nitself, so that in the statement of theorems we simply say that\n\\(\\bT_1\\) is proof-theoretically reducible to \\(\\bT_2\\)\n(written \\(\\bT_1\\leq \\bT_2\\)) and \\(\\bT_1\\) and \\(\\bT_2\\) are\nproof-theoretically equivalent (written \\(\\bT_1 \\equiv\n\\bT_2\\)), respectively. Alternatively, we shall say that \\(\\bT_1\\) and\n\\(\\bT_2\\) have the same proof-theoretic strength when\n\\(\\bT_1\\equiv \\bT_2\\). In practice, if \\(\\bT_1\\equiv \\bT_2\\) is shown\nvia proof-theoretic\n means[11]\n this always entails that the two theories prove at least the same\n\\(\\Pi^0_2\\) sentences (those of the complexity of the twin prime\nconjecture). The complexity of formulae of PRA is\nstratified as follows. The \\(\\Sigma^0_0\\) and \\(\\Pi^0_0\\) formulae are\nof the form \\(R(t_1,\\ldots,t_n)\\) where R is a predicate symbol\nfor an n-ary primitive recursive predicate. A formula is\n\\(\\Sigma^0_{k+1}\\) (\\(\\Pi^0_{k+1}\\)) if it is of the form  \nwith \\(F(y_1,\\ldots,y_m)\\) being of complexity \\(\\Pi^0_k\\)\n(\\(\\Sigma^0_k\\)). Thus the complexity of a formula is measured in\nterms of quantifier alternations. For instance \\(\\Pi^0_2\\)-formulae\nhave two alternations starting with a block of universal quantifiers\nor more explicitly they are of the shape  \nwith R primitive recursive. \nFor the reduction of classical elementary number theory to its\nintuitionist version, Gödel and Gentzen used different logical\ncalculi. Gödel used the system Herbrand had investigated in his\n1931, whereas Gentzen employed the formalization of intuitionist\narithmetic from Heyting 1930. For his further finitist investigations\nGentzen introduced new calculi that were to become of utmost\nimportance for proof theory: natural deduction and sequent\ncalculi. \nAs we noted above, Gentzen had already begun in 1931 to be concerned\nwith the consistency of full elementary number theory. As the logical\nframework he used, what we now call, natural deduction calculi. They\nevolved from an axiomatic calculus that had been used by Hilbert and\nBernays since 1922 and introduced an important modification of the\ncalculus for sentential logic. The connectives \\(\\land \\) and \\(\\lor\\)\nare incorporated, and the axioms for these connectives are as follows:\n \nHilbert and Bernays introduced this new logical formalism for two\nreasons, (i) to be able to better and more easily formalize\nmathematics, and (ii) to bring out the understanding of logical\nconnectives in methodological parallel to the treatment of geometric\nconcepts in Foundations of geometry. The methodological\nadvantages of this calculus are discussed in Bernays 1927: 10: \nThe starting formulae can be chosen in quite different ways. A great\ndeal of effort has been spent, in particular, to get by with a minimal\nnumber of axioms, and the limit of what is possible has indeed been\nreached. However, for the purposes of logical investigations it is\nbetter to separate out, in line with the axiomatic procedure for\ngeometry, different axiom groups in such a way that each of them\nexpresses the role of a single logical operation. \nThen Bernays lists four groups, namely, axioms for the conditional\n\\(\\to\\), for \\(\\land \\) and \\(\\lor\\) as above, and for negation\n\\(\\neg\\). The axioms for the conditional are not only reflecting\nlogical properties, but also structural features as in the later\nsequent calculus (and in Frege’s Begriffsschrift,\n1879).  \nAs axioms for negation one can choose:  \nHilbert formulates this logical system in Über das\nUnendliche and in his second Hamburg talk of 1927, but gives a\nslightly different formulation of the axioms for negation, calling\nthem the principle of contradiction and the principle of\ndouble negation:  \nClearly, the axioms correspond directly to the natural deduction rules\nfor these connectives, and one finds here the origin of\nGentzen’s natural deduction calculi. Bernays had investigated in\nhis Habilitationsschrift (1918) rule based calculi. However,\nin the given context, the simplicity of the metamathematical\ndescription of calculi seemed paramount, and in Bernays 1927 (p. 17)\none finds the programmatic remark: “We want to have as few rules\nas possible, rather put much into axioms”. \nGentzen was led to a rule-based calculus with introduction\nand elimination rules for every logical connective. The truly\ndistinctive feature of this new type of calculus was for Gentzen,\nhowever, making and discharging assumptions. This feature, he\nremarked, most directly reflects a crucial aspect of mathematical\n argumentation.[12]\n Here we formulate the distinctive rules that involve contradictions\nand go beyond minimal logic that has I- and E-rules for all logical\nconnectives. Intuitionist logic is obtained from minimal logic by\nadding the rule: from \\(\\perp\\) infer any formula A, i.e.,\nex falso quodlibet. In the case of classical logic, if a\nproof of \\(\\perp\\) is obtained from the assumption \\(\\neg A\\), then\ninfer A (and cancel the assumption \\(\\neg A\\)). Gentzen\ndiscovered a remarkable fact for the intuitionist calculus, having\nobserved that proofs can have peculiar detours of the\nfollowing form: a formula is obtained by an I-rule and is then the\nmajor premise of the corresponding E-rule. For conjunction such a\ndetour is depicted as follows:  \nClearly, a proof of B is already contained in the given\nderivation. Proofs without detours are called normal, and\nGentzen showed that any proof can be effectively transformed via\n“reduction steps” into a normal one. \nTheorem 2.1 (Normalization for intuitionist\nlogic) A proof of A from a set of assumptions \\(\\Gamma\\)\ncan be transformed into a normal proof of A from the same set\nof assumptions. \nFocusing on normal proofs, Gentzen proved then that the complexity of\nformulae in such proofs can be bounded by that of assumptions and\nconclusion. \nCorollary 2.2 (Subformula property) If\n\\(\\cD\\) is a normal proof of A from \\(\\Gamma\\), then every\nformula in \\(\\cD\\) is either a subformula of an element \\(\\Gamma\\) or\nof A. \nAs Gentzen recounts matters at the very beginning of his dissertation\n(1934/35), he was led by the\ninvestigation of the natural calculus to his\n Hauptsatz[13]\n when he could not extend the considerations to classical logic. \nTo be able to formulate it [the Hauptsatz] in a direct way, I\nhad to base it on a particularly suitable logical calculus. The\ncalculus of natural deduction turned out not to be appropriate for\nthat purpose. \nSo, Gentzen focused his attention on sequent calculi that had been\nintroduced by Paul Hertz and which had been the subject of\nGentzen’s first scientific paper (1932). \nIn his thesis Gentzen introduced a form of the sequent calculus and\nhis technique of cut elimination. As this is a tool of utmost\nimportance in proof theory, an outline of the underlying ideas will be\ndiscussed next. The sequent calculus can be generalized to so-called\ninfinitary logics and is central for ordinal analysis. The\nHauptsatz is also called the cut elimination\ntheorem. \nWe use upper case Greek letters\n\\(\\Gamma,\\Delta,\\Lambda,\\Theta,\\Xi\\ldots\\) to range over finite lists\nof formulae. \\(\\Gamma\\subseteq \\Delta\\) means that every formula of\n\\(\\Gamma\\) is also a formula of \\(\\Delta\\). A sequent is an\nexpression \\(\\Gamma\\Rightarrow \\Delta\\) where \\(\\Gamma\\) and\n\\(\\Delta\\) are finite sequences of formulae \\(A_1,\\ldots,A_n\\) and\n\\(B_1,\\ldots, B_m\\), respectively. We also allow for the possibility\nthat \\(\\Gamma\\) or \\(\\Delta\\) (or both) are empty. The empty sequence\nwill be denoted by \\(\\emptyset\\). \\(\\Sigma \\Rightarrow \\Delta\\) is\nread, informally, as \\(\\Gamma\\) yields \\(\\Delta\\) or, rather, the\nconjunction of the \\(A_i\\) yields the disjunction of\nthe \\(B_j\\). \nThe logical axioms of the calculus are of the form  \nwhere A is any formula. In point of fact, one could limit this\naxiom to the case of atomic formulae A. We have structural\nrules of the form  \nA special case of the structural rule, known as contraction,\noccurs when the lower sequent has fewer occurrences of a formula than\nthe upper sequent. For instance, \\(A, \\Gamma\\Rightarrow\\Delta, B\\)\nfollows structurally from \\(A,A,\\Gamma \\Rightarrow \\Delta,B,B\\). \nNow we list the rules for the logical connectives.  \nIn \\(\\forall L\\) and \\(\\exists R\\), t is an arbitrary term. The\nvariable a in \\(\\forall R\\) and \\(\\exists L\\) is an\neigenvariable of the respective inference, i.e., a is\nnot to occur in the lower sequent. \nFinally, we have the special Cut Rule \nThe formula A is called the cut formula of the\ninference. \nIn the rules for logical operations, the formulae highlighted in the\npremises are called the minor formulae of that inference,\nwhile the formula highlighted in the conclusion is the principal\nformula of that inference. The other formulae of an inference are\ncalled side formulae. A proof (also known as a\ndeduction or derivation) \\(\\cD\\) is a tree of\nsequents satisfying the conditions that (i) the topmost sequents of\n\\(\\cD\\) are logical axioms and (ii) every sequent in \\(\\cD\\) except\nthe lowest one is an upper sequent of an inference whose lower sequent\nis also in \\(\\cD\\). A sequent \\(\\Gamma \\Rightarrow \\Delta\\) is\ndeducible if there is a proof having \\(\\Gamma \\Rightarrow\n\\Delta\\) as its bottom sequent. \nThe Cut rule differs from the other rules in an important respect.\nWith the rules for introducing connectives, one sees that every\nformula that occurs above the line occurs below the line either\ndirectly, or as a subformula of a formula below the line. That is also\ntrue for the structural rules. (Here \\(A(t)\\) is counted as a\nsubformula, in a slightly extended sense, of both \\(\\exists xA(x)\\)\nand \\(\\forall xA(x)\\).) But in the case of the Cut rule, the cut\nformula A vanishes. Gentzen showed that such “vanishing\nrules” can be eliminated. \nTheorem 2.3 (Gentzen’s\nHauptsatz) If a sequent \\(\\Gamma \\Rightarrow \\Delta\\)\nis deducible, then it is also deducible without the Cut rule; the\nresulting proof is called cut-free or normal. \nThe secret to Gentzen’s Hauptsatz is the symmetry of\nleft and right rules for all the logical connectives including\nnegation. The proof of the cut elimination theorem is rather intricate\nas the process of removing cuts interferes with the structural rules.\nIt is contraction that accounts for the high cost of eliminating cuts.\nLet \\(\\lvert \\cD\\rvert\\) be the height of the deduction \\(\\cD\\) and let\n\\(rank(\\cD)\\) be the supremum of the lengths of cut\nformulae occurring in \\(\\cD\\). Turning \\(\\cD\\) into a cut-free\ndeduction of the same end sequent results, in the worst case, in a\ndeduction of height \\(\\cH(rank(\\cD),\\lvert \\cD\\rvert)\\) where \\(\\cH(0,n)=n\\) and\n\\(\\cH(k+1,n)=4^{\\cH(k,n)}\\), yielding hyper-exponential growth. \nThe sequent calculus we have been discussing allows the proof of\nclassically, but not intuitionistically correct formulae, for example,\nthe law of excluded middle. An intuitionist version of the sequent\ncalculus can be obtained by a very simple structural restriction:\nthere can be at most one formula on the right hand side of the sequent\nsymbol \\(\\Rightarrow\\). The cut elimination theorem is also provable\nfor this intuitionist variant. In either case, the Hauptsatz\nhas an important corollary that parallels that of the Normalization\ntheorem (for intuitionist logic) and expresses the subformula\nproperty. \nCorollary 2.4 (Subformula property) If\n\\(\\cD\\) is a cut-free proof of the sequent\n\\(\\Gamma\\Rightarrow\\Delta\\), then all formulae in \\(\\cD\\) are\nsubformulae of elements in either \\(\\Gamma\\) or \\(\\Delta\\). \nThis Corollary has another direct consequence that explains the\ncrucial role of the Hauptsatz for obtaining consistency\nproofs. \nCorollary 2.5 (Consistency) A contradiction,\ni.e., the empty sequent \\(\\emptyset \\Rightarrow \\emptyset\\), is not\nprovable. \nProof: Assume that the empty sequent is provable;\nthen, according to the Hauptsatz it has a cut-free derivation\n\\(\\cD\\). The previous corollary assures us that only empty sequents\ncan occur in \\(\\cD\\); but such a \\(\\cD\\) does not exist since every\nproof must contain axioms. \\(\\qed\\) \nThe foregoing results are solely concerned with pure logic. Formal\ntheories that axiomatize mathematical structures or serve as formal\nframeworks for developing substantial chunks of mathematics are based\non logic but have additional axioms germane to their purpose. If they\nare of the latter kind, such as first-order arithmetic or\nZermelo-Fraenkel set theory, they will assert the existence\nof mathematical objects and their properties. What happens\nwhen we try to apply the procedure of cut elimination to theories?\nAxioms are usually detrimental to this procedure. It breaks down\nbecause the symmetry of the sequent calculus is lost. In general, one\ncannot remove cuts from deductions in a theory T when the cut\nformula is an axiom of T. However, sometimes the axioms of a\ntheory are of bounded syntactic complexity. Then the\nprocedure applies partially in that one can remove all cuts that\nexceed the complexity of the axioms of T. This gives rise to\npartial cut elimination. It is a very important tool in proof\ntheory. For example, it can be used to analyze theories with\nrestricted induction (such as fragments of PA; cf.\nSieg 1985). It also works very well if the axioms of a theory can be\npresented as atomic intuitionist sequents (also called\nHorn clauses), yielding the completeness of\nRobinson’s resolution method (see Harrison\n2009). \nUsing the Hauptsatz and its Corollary, Gentzen was able to\ncapture all of the consistency results that had been obtained prior to\n1934 including Herbrand’s, that had been called by Gödel in\nhis 1933 “the most far-reaching” result. They had been\nobtained at least in principle for fragments of elementary number\ntheory; in practice, Gentzen did not include the quantifier-free\ninduction principle. Having completed his dissertation, Gentzen went\nback to investigate natural deduction calculi and obtained in 1935 his\nfirst consistency proof for full first-order arithmetic. He\nformulated, however, the natural calculus now in “sequent\nform”: instead of indicating the assumptions on which a\nparticular claim depended by the undischarged ones in its proof tree,\nthey are attached now to every node of the tree. The\n“sequents” that are being proved are of the form \\(\\Gamma\n\\Rightarrow A\\), where all the logical inferences are carried out on\nthe right hand side. This proof was published only in 1974; it was\nsubsequently analyzed most carefully in Tait 2015 and Buchholz 2015.\nIt was due to criticism of Bernays and Gödel that Gentzen\nmodified his consistency proof quite dramatically; he made use of\ntransfinite induction, as will be discussed in detail in the next\nsection. Here we just mention that Bernays extensively discussed\ntransfinite induction in Grundlagen der Mathematik II. The\nmain issue for Bernays was the question, is it still a finitist\nprinciple?—Bernays did not discuss, however, two other\naspects of Gentzen’s work, namely, the use of structural\nfeatures of formal proofs for consistency proofs and the attempt of\ngaining a constructive, semantic understanding of intuitionist\narithmetic. The former became crucial for proof theoretic\ninvestigations; the latter influenced Gödel and his functional\ninterpretation via computable functionals of finite\n type.[14]\n The two aspects together opened a new era for proof theory and\nmathematical logic with the goal of proving the consistency of\nanalysis. We will see, how far current techniques lead us and what\nfoundational significance one can attribute to them. \nCut elimination fails for first-order arithmetic (i.e.,\nPA), not even partial cut elimination is possible\nsince the induction axioms have unbounded complexity. Gentzen,\nhowever, found an ingenious way of dealing with purported\ncontradictions in arithmetic. In Gentzen 1938b he showed how to effectively\ntransform an alleged PA-proof of an inconsistency\n(the empty sequent) in his sequent calculus into another proof of the\nempty sequent such that the latter gets assigned a smaller ordinal\nthan the former. Ordinals are a central concept in set theory as well\nas in proof theory. To present Gentzen’s work we shall first\ndiscuss the notion of ordinal from a proof-theoretic point of\nview. \nThis is the first time we talk about the transfinite and ordinals in\nproof theory. Ordinals have become very important in advanced proof\ntheory. The concept of an ordinal is a generalization of that of a\nnatural number. The latter are used for counting finitely many things\nwhereas ordinals can also “count” infinitely many things.\nIt is derived from the concept of an ordering \\(\\prec\\) of a set\nX which arranges the objects of X in order, one after\nanother, in such a way that for every predicate P on X\nthere is always a first element of X with respect to \\(\\prec\\)\nthat satisfies P if there is at least one object in X\nsatisfying P. Such an ordering is called a\nwell-ordering of X. Certainly the usual less-than\nrelation on \\(\\bbN\\) is a well-ordering. Here every number \\(\\ne 0\\)\nis the successor of another number. If one orders the natural numbers\n\\(>0\\) in the usual way but declares that 0 is bigger than every\nnumber \\(\\ne 0\\) one arrives at another ordering of \\(\\bbN\\).\nLet’s call it \\(\\prec\\). \\(\\prec\\) is also a well-ordering of\n\\(\\bbN\\). This time 1 is the least number with respect to \\(\\prec\\).\nHowever, 0 plays a unique role. There are infinitely many numbers\n\\(\\prec 0\\) and there is no number \\(m\\prec 0\\) such that 0 is the\nnext number after m. Such numbers are are called limit numbers\n(with respect to \\(\\prec\\)). \nIn order to be able to formulate Gentzen’s results from the end\nof section 3.3, we have to “arithmetize” the treatment of\nordinals. Let us first state some precise definitions and a Cantorian\ntheorem. \nDefinition 3.1 A non-empty set A\nequipped with a total ordering \\(\\prec\\) (i.e., \\(\\prec\\) is\ntransitive, irreflexive, and  \nis a well-ordering if every non-empty subset X of\nA contains a \\(\\prec\\)-least element, i.e., \nThe elements of a well-ordering \\((A,\\prec)\\) can be divided into\nthree types. Since A is non-empty there is least element with\nrespect to \\(\\prec\\) which is customarily denoted by 0 or \\(0_A\\).\nThen there are elements \\(a\\in A\\) such that there exists \\(b\\prec a\\)\nbut there is no c between b and a. These are the\nsuccessor elements of A, with a being the successor of\nb. An element \\(c\\in A\\) such that \\(0\\prec c\\) and for all\n\\(b\\prec c\\) there exists \\(d\\in A\\) with \\(b\\prec d\\prec c\\) is said\nto be a limit element of A. \nIn set theory a set is called transitive just in case all its\nelements are also subsets. An ordinal in the set-theoretic\nsense is a transitive set that is well-ordered by the elementhood\nrelation \\(\\in\\). It follows that each ordinal is the set of\npredecessors. According to the trichotomy above, there is a least\nordinal (which is just the empty set) and all other ordinals are\neither successor or limit ordinals. The first limit ordinal is denoted\nby \\(\\omega\\). \nFact 3.2 Every well-ordering \\((A,\\prec)\\)\nis order isomorphic to a unique ordinal \\((\\alpha,\\in)\\). \nOrdinals are traditionally denoted by lower case Greek letters\n\\(\\alpha,\\beta,\\gamma,\\delta,\\ldots\\) and the relation \\(\\in\\) on\nordinals is notated simply by \\(<\\). If \\(\\beta \\) is a successor\nordinal, i.e., \\(\\beta\\) is the successor of some (necessarily unique)\nordinal \\(\\alpha\\) we also denotes \\(\\beta\\) by \\(\\alpha'\\).\nAnother important fact is that for any family of ordinals \\(\\alpha_i\\)\nfor \\(i\\in I\\) (I some set) there is a smallest ordinal,\ndenoted by \\(\\sup_{i\\in I}\\alpha_i\\) that is bigger than every ordinal\n\\(\\alpha_i\\). \nThe operations of addition, multiplication, and exponentiation can be\ndefined on all ordinals by using case distinctions and transfinite\nrecursion (on \\(\\alpha\\)). The following states the definitions just\nto convey the flavor:  \nHowever, addition and multiplication are in general not\ncommutative. \nWe are interested in representing specific ordinals \\(\\alpha\\) as\nrelations on \\(\\bbN\\). In essence Cantor defined the first ordinal\nrepresentation system in 1897. Natural ordinal representation\nsystems are frequently derived from structures of the form  \nwhere \\(\\alpha\\) is an ordinal, \\(<_{\\alpha}\\) is the ordering of\nordinals restricted to elements of \\(\\alpha\\) and the \\(f_i\\) are\nfunctions  \nfor some natural number \\(k_i\\).  \nis a computable (or recursive)\nrepresentation of \\(\\frakA\\) if the following conditions\nhold: \n\\(A\\subseteq\\bbN\\) and A is a computable set. \n\\(\\prec\\) is a computable total ordering on A and the functions\n\\(g_i\\) are computable. \n\\(\\frakA \\cong\\bbA\\), i.e., the two structures are\nisomorphic. \nTheorem 3.3 (Cantor 1897) For every ordinal\n\\(\\beta>0\\) there exist unique ordinals\n\\(\\beta_0\\geq\\beta_1\\geq\\dots\\geq\\beta_n\\) such that  \nThe representation of \\(\\beta\\) in (4) is called the Cantor normal\nform. We shall write \\(\\beta \\mathbin{=_{\\sCNF}}\n\\omega^{\\beta_1}+\\cdots +\\omega^{\\beta_n}\\) to convey that\n\\(\\beta_0\\geq\\beta_1\\geq\\dots\\geq\\beta_k\\). \nThe rather famous ordinal that emerged in Gentzen’s consistency\nproof of PA is denoted by \\(\\varepsilon_0\\). It\nrefers to first ordinal \\(\\alpha>0\\) such that \\((\\forall\n\\beta<\\alpha)\\,\\omega^{\\beta}<\\alpha\\). \\(\\varepsilon_0\\) can\nalso be described as the least ordinal \\(\\alpha\\) such that\n\\(\\omega^{\\alpha}=\\alpha\\). \nOrdinals \\(\\beta<\\varepsilon_0\\) have a Cantor normal form with\nexponents \\(\\beta_i<\\beta\\) and these exponents have Cantor normal\nforms with yet again smaller exponents. As this process must\nterminate, ordinals \\(<\\varepsilon_0\\) can be coded by natural\nnumbers. For instance a coding function  \ncould be defined as follows:  \nwhere \\(\\langle k_1,\\cdots,k_n\\rangle\\coloneqq\n2^{k_1+1}\\cdot\\ldots\\cdot p_n^{k_n+1}\\) with \\(p_i\\) being the\nith prime number (or any other coding of tuples). Further\ndefine:  \nThen  \n\\(A_0,\\hat{+},\\hat{\\cdot},x\\mapsto\\hat{\\omega}^{x},\\prec\\) are\nprimitive recursive. Finally, we can spell out the scheme\nPR-TI\\((\\varepsilon_0)\\) in the language of PA:  \nfor all primitive recursive predicates P. \nGiven a natural ordinal representation system \\(\\langle\nA,\\prec,\\ldots\\rangle\\) of order type \\(\\tau\\) let\n\\(\\PRA+\\rTI_{\\qf}(<\\tau)\\) be PRA augmented by\nquantifier-free induction over all initial (externally indexed)\nsegments of \\(\\prec\\). This is perhaps best explained via the\nrepresentation system for \\(\\varepsilon_0\\) given above. There one can\ntake the initial segments of \\(\\prec\\) to be determined by the\nGödel numbers of the ordinals \\(\\omega_0\\coloneqq 1\\) and\n\\(\\omega_{n+1}\\coloneqq \\omega^{\\omega_n}\\) whose limit is\n\\(\\varepsilon_0\\). \nDefinition 3.4 We say that a theory T\nhas proof-theoretic ordinal \\(\\tau\\), written \\(\\lvert T\\rvert =\\tau\\),\nif T can be proof-theoretically reduced to\n\\(\\PRA+\\rTI_{\\qf}(<\\tau)\\), i.e.,  \nUnsurprisingly, the above notion has certain intensional aspects and\nhinges on the naturality of the representation system (for a\ndiscussion see Rathjen 1999a: section 2.).  \nGentzen’s consistency proof for PA employs a\nreduction procedure \\(\\cR\\) on proofs P of the empty sequent\ntogether with an assignment ord of representations for ordinals\nto proofs such that \\(\\ord(\\cR(P))< \\ord(P)\\). Here \\(<\\)\ndenotes the ordering on ordinal representations induced by the\nordering of the pertinent ordinals. For this purpose he needed\nrepresentations for ordinals \\(<\\varepsilon_0\\) where\n\\(\\varepsilon_0\\) is the smallest ordinal \\(\\tau\\) such that whenever\n\\(\\alpha<\\tau\\) then also \\(\\omega^{\\alpha}<\\tau\\) with\n\\(\\alpha\\mapsto \\omega^{\\alpha}\\) being the function of ordinal\nexponentiation with base \\(\\omega\\). Moreover, the functions \\(\\cR\\)\nand ord and the relation \\(<\\) are computable (when viewed\nas acting on codes for the syntactic objects), they can be chosen to\nbe primitive recursive. With \\(g(n)= \\ord(\\cR^n(P))\\), the\nn-fold iteration of \\(\\cR\\) applied to P, one has\n\\(g(0)>g(1)> g(2)> \\ldots > g(n)\\) for all n, which\nis absurd as the ordinals \\(<\\varepsilon_0\\) are well-founded.\nHence PA is consistent. \nGentzen’s proof, though elementary, was very intricate and thus\nmore transparent proofs were sought. As it turned out, the obstacles\nto cut elimination, inherent to PA, could be overcome\nby moving to a richer proof system, albeit in a drastic way by going\ninfinite. This richer system allows for proof rules with\ninfinitely many\n premises.[15]\n The inference commonly known as the \\(\\omega\\)-rule consists\nof the two types of infinitary inferences: \nThe price to pay will be that deductions become infinite objects,\ni.e., infinite well-founded trees. \nThe sequent-style version of Peano arithmetic with the\n\\(\\omega\\)-rule will be denoted by \\(\\PA_{\\omega}\\).\n\\(\\PA_{\\omega}\\) has no use for free variables. Thus free variables\nare discarded and all terms will be closed. All formulae of\nthis system are therefore closed, too. The numerals are the\nterms \\(\\bar{n}\\), where \\(\\bar{0}=0\\) and\n\\(\\overline{n+1}=S\\bar{n}\\). We shall identify \\(\\bar{n}\\) with the\nnatural number n. All terms t of \\(\\PA_{\\omega}\\)\nevaluate to a numeral \\(\\bar{n}\\). \n\\(\\PA_{\\omega}\\) has all the inference rules of the sequent calculus\nexcept for \\(\\forall R\\) and \\(\\exists L\\). In their stead,\n\\(\\PA_{\\omega}\\) has the \\(\\omega R\\) and \\(\\omega L\\) inferences. The\nAxioms of \\(\\PA_{\\omega}\\) are the following: (i)\n\\(\\emptyset\\Rightarrow A\\) if A is a true atomic\nsentence; (ii) \\(B\\Rightarrow \\emptyset\\) if B is a\nfalse atomic sentence; (iii) \\(F(s_1,\\ldots,s_n)\\Rightarrow\nF(t_1,\\ldots,t_n)\\) if \\(F(s_1,\\ldots,s_n)\\) is an atomic sentence and\nthe \\(s_i\\) and \\(t_i\\) evaluate to the same numerals,\nrespectively. \nWith the aid of the \\(\\omega\\)-rule, each instance of the induction\nscheme becomes logically deducible with an infinite proof tree. To\ndescribe the cost of cut elimination for \\(\\PA_{\\omega}\\), we\nintroduce the measures of height and cut rank of a\n\\(\\PA_{\\omega}\\) deduction \\(\\cD\\). We will notate this by  \nThe above relation is defined inductively following the buildup of the\ndeduction \\(\\cD\\). For the cut rank we need the definition of\nthe length, \\(\\lvert A\\rvert\\) of a formula:  \nwhere \\(\\Box=\\land,\\lor,\\to\\); \\(\\lvert \\exists x\\,F(x)\\rvert =\\lvert \\forall\nx\\,F(x)\\rvert =\\lvert F(0)\\rvert +1\\). \nNow suppose the last inference I of \\(\\cD\\) is of the form  \nwhere \\(\\tau=1,2,\\omega\\) and the \\(\\cD_n\\) are the immediate\nsubdeductions of \\(\\cD\\). If  \nand \\(\\alpha_n<\\alpha\\) for all \\(n<\\tau\\) then  \nproviding that in the case of I being a cut with cut\nformula A we also have \\(\\lvert A\\rvert <k\\). We will write\n\\({\\PA_{\\omega} \\stile{\\alpha}{k} \\Gamma} \\Rightarrow \\Delta\\) to\nconvey that there exists a \\(\\PA_{\\omega}\\)-deduction\n\\({\\cD\\stile{\\alpha}{k} \\Gamma}\\Rightarrow \\Delta\\). The ordinal\nanalysis of PA proceeds by first unfolding any\nPA-deduction into a \\(\\PA_{\\omega}\\)-deduction:  \nfor some \\(m,k<\\omega\\). The next step is to get rid of the cuts.\nIt turns out that the cost of lowering the cut rank from \\(k+1\\) to\nk is an exponential with base \\(\\omega\\). \nTheorem 3.5 (Cut Elimination for\n\\(\\PA_{\\omega}\\)) If \\({\\PA_{\\omega} \\stile{\\alpha}{k+1}\n\\Gamma} \\Rightarrow \\Delta\\), then \nAs a result, if \\({\\PA_{\\omega} \\stile{\\alpha}{n} \\Gamma}\\Rightarrow\n\\Delta\\), we may apply the previous theorem n times to arrive\nat a cut-free deduction \\({\\PA_{\\omega} \\stile{\\rho}{0} \\Gamma}\n\\Rightarrow \\Delta\\) with\n\\(\\rho=\\omega^{\\omega^{\\iddots^{\\omega^{\\alpha}}}}\\), where the stack\nhas height n. Combining this with the result from\n\\((\\ref{einbett})\\), it follows that every sequent \\(\\Gamma\\Rightarrow\n\\Delta\\) deducible in PA has a cut-free deduction in\n\\(\\PA_{\\omega}\\) of length \\(<\\varepsilon_0\\). Ruminating on the\ndetails of how this result was achieved yields a consistency proof for\nPA from transfinite induction up to \\(\\varepsilon_0\\)\nfor elementary decidable predicates on the basis of finitist reasoning\n(as described below). \nGentzen did not deal explicitly with infinite proof trees in his\nsecond published proof of the consistency of PA\n(Gentzen 1938b). However, in\nthe unpublished first consistency proof of Gentzen 1974 he aims at\nshowing that a proof of a sequent in first-order arithmetic gives rise\nto a a well-founded reduction tree; that tree can be identified with a\ncut-free proof in the sequent calculus with the \\(\\omega\\)-rule. The\ninfinitary version of PA with the \\(\\omega\\)-rule was\ninvestigated by Schütte (1950). There remained the puzzle, how\nGentzen’s work that used an ingenious method of assigning\nordinals to purported proofs of the empty sequent relates to the\ninfinitary approach. Much later work by Buchholz (1997) and others\nrevealed an intrinsic connection between Gentzen’s assignment of\nordinals to deductions in PA and the standard one to\ninfinite deductions in \\(\\PA_{\\omega}\\). In the 1950s infinitary proof\ntheory flourished in the hands of Schütte. He extended his\napproach to PA to systems of ramified analysis to be\ndiscussed below in\n section 5.2. \nOne last remark about the use of ordinals: Gentzen showed that\ntransfinite induction up to the ordinal \\(\\varepsilon_0\\) suffices to\nprove the consistency of PA. Using the arithmetized\nformalization of the proof predicate (see above, after\n Definition 1.3)\n and taking k as the numeral denoting the Gödel number of\nthe formula \\(0=1\\), we can express the consistency of\nPA, \\(\\Con (\\PA)\\), by the formula \\(\\forall x\n\\,\\neg\\proof_{\\PA}(x,k)\\). To appreciate Gentzen’s result it is\npivotal to note that he applied transfinite induction up to\n\\(\\varepsilon_0\\) only for primitive recursive predicates (the latter\nprinciple was denoted above by PR-TI\\((\\varepsilon_0)\\)). Otherwise,\nGentzen’s proof used only finistist means. Hence, a more\naccurate formulation of Gentzen’s result is  \nwhere F, as above, contains only finitistically acceptable\nmeans. In his 1943 paper Gentzen also showed that this result is best\npossible, as PA proves transfinite induction up to\nany \\(\\alpha<\\varepsilon_0\\). So one might argue that the\nnon-finitist part of PA is encapsulated in\nPR-TI\\((\\varepsilon_0)\\) and therefore “measured” by\n\\(\\varepsilon_0\\). \\(\\varepsilon_0\\) is also the proof-theoretic\nordinal of PA as specified in\n Definition 3.4.\n Gentzen hoped that results of this character could also be obtained\nfor stronger mathematical theories, in particular for analysis.\nHilbert’s famous second problem asked for a direct consistency\nproof of that mathematical theory. Gentzen wrote in 1938 that \nthe most important [consistency] proof of all in practice, that for\nanalysis, is still outstanding. (1938a [Gentzen 1969: 236]).\n \nHe actually worked on a consistency proof for analysis as letters \n(e.g. one to Bernays on 23.6.1935 translated in von Plato 2017: 240) and stenographic\nnotes from 1945 (e.g., Gentzen 1945) show. Formally, “analysis” was identified\nalready in Hilbert 1917/18 as a form of second order number theory.\nHilbert and Bernays developed mathematical analysis in a supplement of\nthe second volume of their “Grundlagen der Mathematik”. We\ntake \\(\\bZ_2\\) as given in the following way. Its language extends\nthat of PA by an additional sort of variables\n\\(X,Y,Z,\\ldots\\) that range over sets of numbers and the binary\nmembership relation \\(t\\in X\\). Its axioms are those of\nPA, but the principle of proof by induction is\nformulated as the second order induction axiom  \nFinally, the axiom schema of comprehension,\nCA, asserts that for every formula \\({F}(u)\\) of the\nlanguage of \\(\\bZ_2\\), there is a set \\(X=\\{u\\mid {F}(u)\\}\\) having\nexactly those numbers u as members that satisfy \\({F}(u)\\).\nMore formally,  \nfor all formulae \\({F}(u)\\) in which X does not occur. That\n\\(\\bZ_2\\) is often called “analysis” is due to the\nrealization (e.g., in Hilbert & Bernays 1939) that, via the coding\nof real numbers and continuous functions as sets of natural numbers, a\ngood theory of the continuum can be developed from these axioms. \nModern analyses of “finitist mathematics” consider it as\nsituated between PRA and PA. When\narguing that Gödel’s second incompleteness theorem refutes\nHilbert’s finitist program, von Neumann argued that finitist\nmathematics is included in PA and, if not there,\nundoubtedly in \\(\\bZ_2\\). So it is quite clear that a consistency\nproof of \\(\\bZ_2\\) would use non-finitist principles or that the\npursuit of the consistency program would require an extension of the\nfinitist standpoint. In the next section we discuss briefly a variety\nof extensions and elaborate two in greater detail. \nAccording to Bernays, the reductive result due to Gödel and\nGentzen,\n Theorem 1.2,\n has a dramatic impact on the work concerned with Hilbert’s\nprogram. It opened in a very concrete and precise way the finitist\nperspective to a broader “constructive” one. Hilbert\nhimself had taken such a step in a much vaguer way in his last paper\n(Hilbert 1931b).\n Theorem 1.2\n showed, after all, that PA is contained in\nHA via the negative translation. Since\nHA comprises just a fragment of Brouwer’s\nintuitionism, the consistency of PA is secured on the\nbasis of the intuitionist standpoint. In a letter to Heyting of 25\nFebruary 1933, Gentzen suggested investigating the consistency of\nHA since a consistency proof for classical arithmetic\nhad not been given so far by finitist means. He then continued \nIf on the other hand, one admits the intuitionistic position as a\nsecure basis in itself, i.e., as a consistent one, the consistency of\nclassical arithmetic is secured by my result. If one wished to satisfy\nHilbert’s requirements, the task would still remain of showing\nintuitionistic arithmetic consistent. This, however, is not possible\nby even the formal apparatus of classical arithmetic, on the basis of\nGödel’s result in combination with my proof. Even so, I am\ninclined to believe that a consistency proof for intuitionistic\narithmetic, from an even more evident position, is possible and\ndesirable. (quoted and translated in von Plato 2009: 672) \nGödel took a very similar position in December of 1933\n(Gödel 1995: 53). There he broadened the idea of a revised\nversion of Hilbert’s program allowing constructive means that go\nbeyond the finitist ones without accepting fully fledged intuitionism;\nthe latter he considered to be problematic, in particular on account\nof the impredicative nature of intuitionist implication. As to an\nextension of Hilbert’s position he wrote: \nBut there remains the hope that in future one may find other and more\nsatisfactory methods of construction beyond the limits of the system A\n[capturing finitist methods], which may enable us to found classical\narithmetic and analysis upon them. This question promises to be a\nfruitful field for further investigations. \nIn\n section 3.2\n we described Gentzen’s considerations; in\n section 4.2\n we discuss Gödel’s as developed in the late 1930s. In\n section 4.1\n we sketch some other constructive positions. \nA particularly appealing idea is to pursue Hilbert’s program\nrelative to a constructive point of view and determine which parts of\nclassical mathematics are demonstrably consistent relative to that\nstandpoint (see Rathjen 2009 for pursuing this with regard to\nMartin-Löf type theory). As one would suspect, there are\ndiffering “schools” of constructivism and different layers\nof constructivism. Several frameworks for developing mathematics from\nsuch a point of view have been proposed. Some we will refer to in this\narticle (arguably the most important) are: \nAt this point we will just give a very rough description of these\nfoundational views. A few more details, especially about their scope\non a standard scale of theories and proof-theoretic ordinals, will be\nprovided at the very end of\n section 5.3. \n(a) Arithmetical Predicativism originated in the writings of\nPoincaré and Russell in response to the set-theoretic\nparadoxes. It is characterized by a ban of impredicative definitions.\nWhilst it accepts the completed infinite set of naturals numbers, all\nother sets are required to be constructed out of them via an\nautonomous process of arithmetical definitions. A first systematic\nattempt at developing mathematics predicatively was made in\nWeyl’s 1918 monograph Das Kontinuum (Weyl 1918). \n(b) Theories of higher type functionals comprise Gödel’s\nT and Spector’s extension of T via functionals\ndefined by bar recursion. The basic idea goes back to\nGödel’s 1938 lecture at Zilsel’s (Gödel 1995:\n94). It was inspired by Hilbert’s 1926 Über das\nUnendliche, which considered a hierarchy of functionals over the\nnatural numbers, not only of finite but also of transfinite type. \n(c) To understand Takeuti’s finitist standpoint it is important\nto pinpoint the place where in a consistency proof à la Gentzen\nthe means of PRA are exceeded. Gentzen’s proof\nemploys a concrete ordering \\(\\prec\\) of type \\(\\varepsilon_0\\), it\nuses an assignment of ordinals to proofs and provides a reduction\nprocedure on proofs such that any alleged proof of an inconsistency is\nreduced to another proof of an inconsistency which gets assigned a\nsmaller element of the ordering. The ordering, the ordinal assignment\nand the reduction procedure are actually primitive recursive and the\nsteps described so far can be carried out in a small fragment of\nPRA. The additional principle needed to infer the\nconsistency of PA is the following: \nTakeuti refers to (*) as the accessibility of \\(\\prec\\). Note that\nthis is a weaker property than the well-foundedness of \\(\\prec\\) which\nrefers to arbitrary sequences. There is nothing special about the case\nof PA since any ordinal analysis of a theory T\nin the literature can be made to fit this format. Thus\nepistemologically (*) is the fulcrum in any such consistency proof.\nTakeuti’s central idea (1987, 1975) was that we can carry out\nGedankenexperimente (thought experiments) on concretely given\n(elementary) sequences to arrive at the insight that (*)\n obtains.[16] \n(d) Errett Bishop’s novel (informal) approach to constructive\nanalysis (1967) made a great on impression on mathematicians with\nconstructive leanings. In it, he dealt with different kinds of\nmathematical objects (numbers, functions, sets) as if they were given\nby explicit presentations, each kind being equipped with its own\ngermane “equality” relation conceived in such a way that\noperations on them would lead from representations to representations\nrespecting the equality relation. An important ingredient that made\nBishop’s constructivism workable is the systematic use of\nwitnessing data as an integral part of what constitutes a mathematical\nobject. For instance, a real number comes with a modulus of\nconvergence while a function of real numbers comes equipped with a\nmodulus of (uniform) convergence. In his explicit\nmathematics, Feferman (1975, 1979) aims (among other things) at\nformalizing the core of Bishop’s ontology. Explicit mathematics\nis a theory that describes a realm of concretely and explicitly given\nobjects (a universe U of symbols) equipped with an operation\n\\(\\bullet\\) of application in such a way that given two objects\n\\(a,b\\in U\\), a may be viewed as a program which can be run on\ninput b and may produce an output \\(a\\bullet b\\in U\\) or never\nhalt (such structures are known as partial combinatory algebras or\nSchönfinkel algebras). Moreover, some of the objects of U\nrepresent sets of elements of U. The construction of new sets\nout of given sets is either done explicitly by elementary\ncomprehension or by a process of inductive generation. If one also\nadds principles to the effect that every internal operation (given as\n\\(\\lambda x.a\\bullet x\\) for some \\(a\\in U\\)) which is monotone on\nsets possesses a least fixed point one arrives at a remarkably strong\ntheory (cf. Rathjen 1998, 1999b, 2002). \n(e) Martin-Löf type theory is an intuitionist theory of dependent\ntypes intended to be a full scale system for formalizing constructive\nmathematics. Its origins of can be traced to Principia\nMathematica, Hilbert’s Über das Unendliche,\nthe natural deduction systems of Gentzen, taken in conjunction with\nPrawitz’s reduction procedures, and to Gödel’s\nDialectica system. It incorporates inductively defined data types\nwhich together with the vehicle of internal reflection via universes\nendow it with considerable consistency strength. \n(f) Constructive set theory (as do the theories under (d) and (e))\nsets out to develop a framework for the style of constructive\nmathematics of Bishop’s 1967 Foundations of constructive\nanalysis in which he carried out a development of constructive\nanalysis, based on informal notions of constructive function and set,\nwhich went substantially further mathematically than anything done\nbefore by constructivists. Where Brouwer reveled in differences,\nBishop stressed the commonalities with classical mathematics. What was\nnovel about his work was that it could be read as a piece of classical\nmathematics as well. \nThe ‘manifesto’ of constructive set theory was most vividly\nexpressed by Myhill:  \n… the argumentation of [Bishop 1967] looks very smooth and\nseems to follow directly from a certain conception of what sets,\nfunctions, etc. are, and we wish to discover a formalism which\nisolates the principles underlying this conception in the same way\nthat Zermelo-Fraenkel set-theory isolates the principles underlying\nclassical (nonconstructive) mathematics. We want these principles to\nbe such as to make the process of formalization completely trivial, as\nit is in the classical case. (Myhill 1975: 347) \nDespite first appearances, there are close connections between the\napproaches of (d)–(f). Constructive set theory can be\ninterpreted in Martin-Löf type theory (due to Aczel 1978) and\nexplicit mathematics can be interpreted in constructive set theory\n(see Rathjen 1993b, in Other Internet Resources). Perhaps the closest\nfit between (e) and (f), giving back and forth interpretations, is\nprovided by Rathjen & Tupailo 2006. Some concrete mathematical\nresults are found at the end of\n section 5.3. \nAmong the proposals for extending finitist methods put forward in his\n1938 lecture at Zilsel’s, Gödel appears to have favored the\nroute via higher type functions. Details of what came to be known as\nthe Dialectica interpretation were not published until 1958\n(Gödel 1958) but the D-interpretation itself was arrived at by\n1941. Gödel’s system \\({T}\\) axiomatizes a class of\nfunctions that he called the primitive recursive functionals of\nfinite type. \\({T}\\) is a largely equational theory whose axioms\nare equations involving terms for higher type functionals with just a\nlayer of propositional logic on top of that. In this way the\nquantifiers, problematic for finists and irksome to intuitionists, are\navoided. To explain the benefits of the D-interpretation we need to\nhave a closer look at the syntax of \\({T}\\). \nDefinition 4.1 \\({T}\\) has a many-sorted\nlanguage in that each terms is assigned a type. Type (symbols) are\ngenerated from 0 by the rule: If \\(\\sigma\\) and \\(\\tau\\) are types\nthen so is \\(\\sigma\\to\\tau\\). Intuitively the ground type 0 is the\ntype of natural numbers. If \\(\\sigma\\) and \\(\\tau\\) are types that are\nalready understood then \\(\\sigma\\to\\tau\\) is a type whose objects are\nconsidered to be functions from objects of type \\(\\sigma\\) to objects\nof type \\(\\tau\\). In addition to variables\n\\(x^{\\tau},y^{\\tau},z^{\\tau},\\ldots\\) for each type \\(\\tau\\), the\nlanguage of \\({T}\\) has special constants 0, \\(\\rsuc\\),\n\\(\\rK_{\\sigma,\\tau}\\), \\(\\rS_{\\rho,\\sigma,\\tau}\\), and\n\\(\\rR_{\\sigma}\\) for all types \\(\\rho,\\sigma,\\tau\\). The meaning of\nthese constants is explained by their defining equations.\n\\(\\rK_{\\sigma,\\tau}\\) and \\(\\rS_{\\rho,\\sigma,\\tau}\\) are familiar from\ncombinatory logic which was introduced by Schönfinkel in 1924 and\nbecame more widely known through Curry’s work (1930). 0 plays the role of the\nfirst natural number while \\(\\rsuc\\) embodies the successor function\non objects of type 0. The constants \\(\\rR_{\\sigma}\\), called\nrecursors, provide the main vehicle for defining functionals\nby recursion on \\(\\bbN\\). Term formation starts with constants and\nvariables, and if s and t are terms of type\n\\(\\sigma\\to\\tau\\) and \\(\\sigma\\), respectively, then \\(s(t)\\) is a\nterm of type \\(\\tau\\). To increase readability we shall write\n\\(t(r,s)\\) instead of \\((t(r))(s)\\) and \\(t(r,s,q)\\) instead of\n\\((t(r,s))(q)\\) etc. Also \\(\\rsuc(t)\\) will be shortened to\n\\(t'\\). The defining axioms for the constants are the\n following:[17] \nThe axioms of \\({T}\\) consist of the above defining axioms, equality\naxioms and axioms for propositional logic. Inference rules are modus\nponens and the induction rule  \nfor t of type 0 and x not in \\(A(0)\\). \nThe first step towards the D-interpretation of Heyting arithmetic in\n\\({T}\\) consists of associating to each formula A of arithmetic\na syntactic translation \\(A^D\\) which is of the form  \nwith \\(A_D(\\vec{x},\\vec{y})\\) being quantifier free. Thus \\(A^D\\) is\nnot a formula of \\({T}\\) but of its augmentation via quantifiers\n\\(\\forall x^{\\tau}\\) and \\(\\exists y^{\\tau}\\) for all types \\(\\tau\\).\nThe translation proceeds by induction on the buildup of A. The\ncases where the outermost logical symbol of A is among\n\\(\\land\\), \\(\\lor\\), \\(\\exists x\\), \\(\\forall x\\) are rather\nstraightforward. The crucial case occurs when A is an\nimplication \\(B\\to C\\). To increase readability we shall suppress the\ntyping of variables. Let \\(B^D\\equiv \\exists \\vec{x}\\, \\forall\n\\vec{y}\\, B_D(\\vec{x},\\vec{y})\\) and \\(C^D\\equiv {\\exists \\vec{u}\\,\n\\forall \\vec{v}\\, C_D(\\vec{u},\\vec{v})}\\). Then one uses a series of\njudicious equivalences to bring the quantifiers in \\(B^D\\to C^D\\) to\nthe front and finally employs skolemization of existential variables\nas follows:  \n\\(A^D\\) is then defined to be the formula in (vii). Note, however,\nthat these equivalences are not necessarily justified constructively.\nOnly (i) \\(\\Leftrightarrow\\) (ii) and (iii) \\(\\Leftrightarrow\\) (iv)\nhold constructively whereas (v) \\(\\Leftrightarrow\\) (vi) and (vi)\n\\(\\Leftrightarrow\\) (vii) are justified constructively only if one\nalso accepts the axiom of choice for all finite types\n(ACft). Equivalences (ii) \\(\\Leftrightarrow\\) (iii)\nand (iv) \\(\\Leftrightarrow\\) (v) use a certain amount of classical\nlogic known as the principle of independence of premise\n(IPft) and Markov’s principle\n(MPft) for all finite types, respectively. At this\npoint \\(A\\mapsto A^D\\) is just a syntactic translation. But amazingly\nit gives rise to a meaningful interpretation of HA in\nT. \nTheorem 4.2 (Gödel 1958) Suppose\n\\(\\cD\\) is a proof of A in HA and \\(A^D\\) as\nin \\((\\ref{D-Form})\\). Then one can effectively construct a sequence\nof terms \\(\\vec t\\) (from \\(\\cD\\)) such that \\({T}\\) proves \\(A_D(\\vec\nt,\\vec y\\,)\\). \nIf one combines the D-interpretation with the\nKolmogorov-Gentzen-Gödel negative translation of\nPA into HA one also arrives at an\ninterpretation of PA in \\({T}\\). Some interesting\nconsequences of the latter are that the consistency of\nPA follows finitistically from the consistency of\n\\({T}\\) and that every total recursive function of PA\nis denoted by a term of \\({T}\\). \nThe three principles ACft, IPft\nand MPft which figured in the D-translation\nactually characterize the D-translation in the sense that over\nthe quantifier extension of \\({T}\\) with intuitionistic logic, called\n\\(\\bHA^{\\omega}\\), they are equivalent to the schema  \nfor all formulae C of that theory. Principles similar to the\nthree above are also often validated in another type of computational\ninterpretation of intuitionistic theories known as\nrealizability. Thus it appears that they are intrinsically\nrelated to computational interpretations of such theories. \nA further pleasing aspect of Gödel’s interpretation is that\nit can be extended to stronger systems such as higher order systems\nand even to set theory (Burr 2000, Diller 2008). Moreover, it sometimes allows one to\nextract computational information even from proofs of specific\nclassical theorems (see, e.g., Kohlenbach 2007). It behaves nicely\nwith respect to modus ponens and thus works well for ordinary proofs\nthat are usually structured via a series of lemmata. This is in\ncontrast to cut elimination which often requires a computationally\ncostly transformation of proofs. \nSpector (1962) extended Gödel’s functional interpretation,\nengineering an interpretation of \\(\\bZ_2\\) into T augmented via\na scheme of transfinite recursion on higher type orderings. This type\nof recursion, called bar recursion, is conceptually related\nto Brouwer’s bar induction principle. (For a definition of bar\ninduction and a presentation of Spector’s result see\n appendix C.) \nWe described the system \\(\\bZ_2\\) of second order arithmetic already\nat the end of section 3.2. It was viewed as the\n“next”system to be proved consistent—after\nfirst-order arithmetic PA had been shown to be. As we\nmentioned \\(\\bZ_2\\) is also called “analysis”, because it\nallows the development of classical mathematical analysis: coding real\nnumbers and continuous functions as sets of natural numbers, a good\ntheory of the continuum can be developed from \\(\\bZ_2\\)’s\naxioms. Indeed, Hermann Weyl showed in 1918 that a considerable\nportion of analysis can be developed in small fragments of \\(\\bZ_2\\)\nthat are actually conservative over PA. The idea of\nsingling out the minimal fragment of \\(\\bZ_2\\) required to expose a\nparticular part of ordinary mathematics led in the 1980s to the\nresearch program of reverse mathematics. However, before\ndiscussing that program, we are going to proof-theoretic\ninvestigations of \\(\\bZ_2\\) and its subsystems that have been a focal\npoint until the very early 1980s. \nAfter Gentzen, it was Gaisi Takeuti who worked on a consistency proof\nfor \\(\\bZ_2\\) in the late 1940s. He conjectured that Gentzen’s\nHauptsatz not only holds for first order logic but also for\nhigher order logic, also known as simple type theory,\nSTT. This came to be known as Takeuti’s\nfundamental\n conjecture.[18]\n The particular sequent calculus he introduced was called a\ngeneralized logic calculus, GLC (Takeuti 1953). \\(\\bZ_2\\) can be\nviewed as a subtheory of GLC. In the setting of GLC the comprehension\nprinciple CA is encapsulated in the right\nintroduction rule for the existential second-order quantifier and the\nleft introduction rule for the universal second-order quantifier. In\norder to display these rules the following notation is convenient. If\n\\(F(U)\\) and \\(A(a)\\) are formulae then \\(F(\\{v\\mid A(v)\\})\\) arises\nfrom \\(F(U)\\) by replacing all subformulae \\(t\\in U\\) of \\(F(U)\\)\n(with U indicated) by \\(A(t)\\). The rules for second order\nquantifiers can then be stated as\n follows:[19] \nTo deduce an instance \\(\\exists X\\,\\forall x\\,[x\\in X \\leftrightarrow\nA(x)]\\) of CA just let \\(F(U)\\) be the formula\n\\(\\forall x\\,[x\\in U \\leftrightarrow A(x)]\\) and observe that  \nand hence  \nAs the deducibility of the empty sequent is ruled out if cut\nelimination holds for GLC (or just the fragment GLC2\ncorresponding to \\(\\bZ_2\\)), Takeuti’s Fundamental Conjecture\nentails the consistency of \\(\\bZ_2\\). However note that it does not\nyield the subformula property as in the first-order case since the\nminor formula \\(F(\\{x\\mid A(x)\\})\\) in \\((\\exists_2\\,\\rR)\\) and\n\\((\\forall_2\\,\\bL)\\) may have a much higher (quantifier) complexity\nthan the principal formula \\(\\exists XF(X)\\) and \\(\\forall XF(X)\\),\nrespectively. Indeed, \\(\\exists XF(X)\\) may be a proper subformula of\n\\(A(x)\\) which clearly exhibits the impredicative nature of these\ninferences and shows that they are strikingly different from those in\npredicative analysis where a proper subformula property obtains. \nIn 1960a Schütte\ndeveloped a semantic equivalent to the (syntactic) fundamental\nconjecture using partial or semi-valuations. He employed the method of\nsearch trees (or deduction chains) to show that a formula F\nthat cannot be deduced in the cut-free system has a deduction chain\nwithout axioms which then gives rise to a partial valuation V\nassigning the value “false” to F. From the latter\nhe inferred that the completeness of the cut-free\n system[20]\n is equivalent to the semantic property that every partial valuation\ncan be extended to a total valuation (basically a Henkin model of\nSTT). In 1966 Tait succeeded in proving\ncut-elimination for second order logic using Schütte’s\nsemantic equivalent for that fragment. Soon afterwards, Takahashi\n(1967) and Prawitz (1968) independently proved for full classical\nsimple type that every partial valuation extends to a total one,\nthereby establishing Takeuti’s fundamental conjecture. These\nresults, though, were somewhat disappointing as they were obtained by\nhighly non-constructive methods that provided no concrete method for\neliminating cuts in a derivation. However, Girard showed in 1971 that\nsimple type theory not only allows cut-elimination but that there is\nalso a terminating normalization\n procedure.[21]\n These are clearly very interesting results, but as far as instilling\ntrust in the consistency of \\(\\bZ_2\\) or SST is\nconcerned, the cut elimination or termination proofs are just circular\nsince they blatantly use the very comprehension principles formalized\nin these theories (and a bit more). To quote Takeuti:  \nMy fundamental conjecture itself has been resolved in a sense by Motoo\nTakahashi and Dag Prawitz independently. However, their proofs rely on\nset theory, and so it cannot be regarded as an execution of\nHilbert’s Program. (Takeuti 2003: 133)  \nTakeuti’s work on his conjecture instead focused on partial\nresults. A major breakthrough that galvanized research in proof\ntheory, especially ordinal-theoretic investigations, was made by him\nin 1967. In Takeuti 1967 he gave a consistency proof for\n\\(\\Pi^1_1\\)-comprehension and thereby for the first time obtained an\nordinal analysis of an impredicative theory. For this Takeuti vastly\nextended Gentzen’s method of assigning ordinals (ordinal\ndiagrams, to be precise) to purported derivations of the empty\nsequent. It is worth quoting Takeuti’s own assessment of his\nachievements. \n… the subsystems for which I have been able to prove the\nfundamental conjecture are the system with \\(\\Pi^1_1\\) comprehension\naxiom and a slightly stronger system, that is, the one with\n\\(\\Pi^1_1\\) comprehension axiom together with inductive\ndefinitions.[…] I tried to resolve the fundamental conjecture\nfor the system with the \\(\\Delta^1_2\\) comprehension axiom within our\nextended version of the finite standpoint. Ultimately, our success was\nlimited to the system with provably \\(\\Delta^1_2\\) comprehension\naxiom. This was my last successful result in this area. (Takeuti 2003:\n133) \nThe subsystems of \\(\\bZ_2\\) that are alluded to in the above\ndiscussion are now to be described. We consider the axiom schema of\n\\(\\cC\\)-comprehension for formula classes \\(\\cC\\) which is\ngiven by  \nfor all formulae \\({F}\\in\\cC\\) in which X does not occur.\nNatural formula classes are the arithmetical formulae,\nconsisting of all formulae without second order quantifiers \\(\\forall\nX\\) and \\(\\exists X\\), and the \\(\\Pi^1_n\\)-formulae, where a\n\\(\\Pi^1_n\\)-formula is a formula of the form \\(\\forall X_1\\ldots Q\nX_n\\,A(X_1,\\ldots,X_n)\\) with \\(\\forall X_1\\ldots Q X_n\\) being a\nstring of n alternating set quantifiers, commencing with a\nuniversal one, followed by an arithmetical formula\n\\(A(X_1,\\ldots,X_n)\\). Note that in this notation the class of\narithmetical formulae is denoted by \\(\\Pi^1_0\\). \nAlso “mixed” forms of comprehension are of interest, e.g.,\n \nwhere \\(F(u)\\) is in \\(\\Pi^1_n\\) and \\(G(u)\\) in \\(\\Sigma^1_n\\). \nOne also considers \\(\\Delta^1_n\\) comprehension rules:  \nFor each axiom scheme \\(\\mathbf{Ax}\\) we denote by \\((\\mathbf{Ax})_0\\)\nthe theory consisting of the basic arithmetical axioms plus the scheme\n\\(\\mathbf{Ax}\\). By contrast, \\((\\mathbf{Ax})\\) stands for the theory\n\\((\\mathbf{Ax})_0\\) augmented by the scheme of induction for all\n\\(\\cL_2\\)-formulae. An example for these notations is the theory\n\\((\\bPi^1_1-\\bCA)_0\\) which has the comprehension schema for\n\\(\\bPi^1_1\\)-formulae. \nIn PA one can define an elementary injective pairing\nfunction on numbers, e.g., \\((n,m)\\coloneqq 2^n\\times3^m\\). With the\nhelp of this function an infinite sequence of sets of natural numbers\ncan be coded as a single set of natural numbers. The \\(n^{th}\\)\nsection of set of natural numbers U is defined by\n\\(U_n\\,\\coloneqq \\,\\{m:\\,(n,m)\\in U\\}\\). Using this coding, we can\nformulate a form of the axiom of choice for formulae \\({F}\\) in\n\\(\\cC\\) by  \nThe basic relations between the above theories are discussed in\nFeferman and Sieg 1981a. \nA major stumbling block for proving Takeuti’s fundamental\nconjecture is that in \\((\\forall_2\\bL)\\) and \\((\\exists_2\\rR)\\)\ninferences the minor formula \\(F(\\{v\\mid A(v)\\})\\) can have a much\nhigher complexity than the principal (inferred) formula \\(QX\\,F(X)\\).\nIf, instead, one allowed these inferences only in cases where the\n‘abstraction’ term \\(\\{v\\mid A(v)\\}\\) had (in some sense) a\nlower complexity than \\(QX\\,F(X)\\), cut elimination could be restored.\nTo implement this idea, one introduces a hierarchy of sets (formally\nrepresented by abstraction terms) whose complexity is stratified by\nordinal levels \\(\\alpha\\), and a pertaining hierarchy of quantifiers\n\\(\\forall X^{\\beta}\\) and \\(\\exists X^{\\beta}\\) conceived to range\nover sets of levels \\(<\\beta\\). This is the basic idea underlying\nthe ramified analytic hierarchy. The problem of which ordinals could\nbe used for the transfinite iteration led to the concept of\nautonomous progressions of theories. The general idea of\nprogressions of theories is very natural and we shall consider it\nfirst before discussing the autonomous versions. \nAs observed earlier, Hilbert attempted to overcome the incompleteness\nof first-order arithmetic by introducing as axioms\n\\(\\Pi^0_1\\)-statements all of whose instances had been finitistically\nproved (Hilbert 1931a). In a way he modified the concept of a\n“formal” theory by invoking finitist provability. Bernays,\nin his letter to Gödel of January 18, 1931 (Gödel 2003:\n86–88), proposed a rule of a more general form. He\nindicated also that it would allow the elimination of the induction\nprinciple—in exchange for dealing with infinite proofs. \nThese considerations among others raised the issue of what constitutes\na properly formal theory. Gödel paid very special\nattention to it when giving his Princeton Lectures in 1934. At the\nvery end he introduced the general recursive functions. This class of\nnumber theoretic functions was shown to be co-extensional with\nChurch’s λ-definable ones by Church and Kleene. In\nChurch 1936 an “identification”\nof effective\ncalculability and general recursiveness was proposed, what is usually\ncalled Church’s thesis. Turing, of course, proposed his\nmachine computability for a very similar purpose and proved its\nequivalence to λ-definability in an appendix to his\n1936. Church\nand Turing used their respective notion to establish the\nundecidability of first-order logic. For Gödel, this was the\nbackground for formulating the incompleteness theorems in “full\ngenerality” for all formal theories (containing a modicum of\nnumber or set theory); see the Postscriptum\nto the Princeton Lectures Gödel wrote in 1964: \nIn consequence of later advances, in particular of the fact that, due\nto A.M. Turing’s work, a precise and unquestionably adequate\ndefinition of the general concept of formal system can now be given,\nthe existence of undecidable arithmetical propositions and the\nnon-demonstrability of the consistency of a system in the same system\ncan now be proved rigorously for every consistent formal\nsystem containing a certain amount of finitary number theory.\n(Gödel 1986: 369). \nThe first incompleteness is proved for any such theory T, by\nexplicitly producing an unprovable yet true statement \\(G_\\bT\\). That\nformula can then be added to T making \\(\\bT+G_\\bT\\) a\n“less incomplete” theory. Von Neumann had already\nestablished the equivalence of \\(G_\\bT\\) with the consistency\nstatement for T, \\(\\Con (\\bT)\\); the latter expresses that\nthere is no proof in T of a blatantly false statement such as\n\\(0=1\\). This gives then rise to an extension procedure leading from\nT to \\(\\bT'\\), namely (R1) \\(\\bT'=\\bT+G_\\bT\\). \nThus one might try to address the incompleteness of T by\nforming a sequence of theories \\(\\bT=\\bT_0\\subset\n\\bT_1\\subset\\bT_2\\subset\\ldots\\) where \\(\\bT_{i+1}=\\bT_i'\\) and to\ncontinue this into the transfinite. The latter can be achieved by\nletting \\(\\bT_{\\lambda}=\\bigcup_{\\alpha<\\lambda}\\bT_{\\alpha}\\) for\nlimit ordinals λ and \\(\\bT_{\\alpha+1}=\\bT_{\\alpha}'\\)\nfor successor ordinals \\(\\alpha+1\\). However, the consistency\nstatement for \\(\\bT_{\\lambda}\\), thus the provability predicate for\nthe theory, has to be expressed in the language of \\(\\bT_{\\lambda}\\),\nand one cannot simply use set theoretic ordinals. Furthermore, the\nextensions of T are all supposed to be formal theories, i.e.,\nthe axioms have to be enumerable by recursive functions. To deal with\nboth issues at once, one has to deal with ordinals in an effective\nway. \nThat is what Turing did in his Princeton dissertation (1939)\nconcerning, what he called, ordinal logics. There he\nconsiders two ways of achieving the effective representation of\nordinals. The first way is via the set \\(\\rW\\) of numbers e for\nrecursive well-orderings \\(\\leq_e\\), the second is provided by the\nclass of Church-Kleene notations for ordinals (Church and Kleene\n1936) that used expressions in the λ-calculus to describe\nordinals. The latter approach was then modified in Kleene 1938 to an\nequivalent recursion-theoretic definition that uses numerical codes to\ndenote countable ordinals and is known as Kleene’s\n\\({\\cO}\\). \nDefinition 5.1 A computable or\nrecursive function on the naturals is one that can be\ncomputed by a Turing machine. The program of a Turing machine M\ncan be assigned a Gödel number \\({\\Corner{M}}\\). For natural\nnumbers \\(e,n\\), to convey that the Turing machine with Gödel\nnumber e computes a number m on input n, we use\nthe notation \\(\\{e\\}(n)\\) for m. \nKleene uses \\({\\rsuc}(a)\\coloneqq 2^a\\) as notations for successor\nordinals and and \\({\\rlim}(e)\\coloneqq 3\\cdot 5^e\\) for limit\nordinals. \nThe class \\({\\cO}\\) of ordinal notations, the partial ordering\nrelation \\(\\relLTcO\\) between such notations, and the ordinal\n\\({\\mid}{a}{\\mid}\\) denoted by \\(a\\in {\\cO}\\) are defined\nsimultaneously as follows: \nThe first ordinal \\(\\tau\\) such that there is no recursive\nwell-ordering of order type \\(\\tau\\) is usually denoted by\n\\(\\omega^{CK}\\) in honor of Church and Kleene. It can be shown for the\nabove definition of \\({\\cO}\\) that the recursive ordinals are exactly\nthose that have a notation in \\({\\cO}\\). \nWhen it comes to theories T, quite unlike to other areas of\nlogic (e.g., model theory), results as those presented in this section\ndepend not only on the set of axioms of T, but also on the way\nthey are presented. When talking about a theory T we assume\nthat T is given by a \\(\\Sigma^0_1\\)-formula \\(\\psi(v_0)\\) such\nthat F is an axiom of T just in case\n\\(\\psi({\\Corner{F}})\\) holds; a \\(\\Sigma^0_1\\)-formula is of the form\n\\(\\exists y_1\\ldots\\exists y_n\\,R(y_1,\\ldots y_n)\\) with R\nprimitive recursive. This consideration together with Kleene’s\n\\({\\cO}\\) allows us to build a transfinite hierarchy of theories based\non any suitable theory T. A consistency progression\nbased on T is a primitive recursive function \\(n\\mapsto\n\\psi_n\\) that associates with every natural number n a\n\\(\\Sigma^0_1\\)-formula \\(\\psi_n(v_0)\\) that defines \\(\\bT_n\\) such\nthat PA proves: (i) \\(\\bT_0=\\bT\\); (ii)\n\\(\\bT_{{\\rsuc}(n)}=\\bT_n'\\), and (iii)\n\\(\\bT_{{\\rlim}(n)}=\\bigcup_x\\bT_{\\{n\\}(x)}\\). So, finally we can\nformulate Turing’s completeness result. \nTheorem 5.2 For any true \\(\\Pi^0_1\\)\nsentence F a number \\(a_{F}\\in{\\cO}\\) can be constructed such\nthat \\({{\\mid}\\,{a_{F}}\\,{\\mid}}=\\omega+1\\) and \\(\\bT_{a_{F}}\\vdash\nF\\). Moreover, the function \\(F\\mapsto a_{F}\\) is given by a primitive\nrecursive function. \nAt first glance Turing’s theorem seems to provide some insight\ninto the nature of true \\(\\Pi^0_1\\)-statements. That this is an\n“illusion” is revealed by the analysis of its simple proof\nwhich is just based on the trick of coding the truth of F as a\nmember of \\({\\cO}\\). The proof also shows that the infinitely many\niterated consistency axioms \\(\\Con (\\bT_0),\\Con (\\bT_1),\\ldots\\) of\n\\(\\bT_{{{\\rsuc}}({\\rlim}(e))}\\) are irrelevant for proving F.\nAs it turns out, the reason why one has to go to stage \\(\\omega+1\\) is\nsimply that only at stage \\(\\omega\\) a non-standard definition of the\naxioms of \\(\\bigcup_{n<\\omega}\\bT_n\\) can be introduced. More\ndetails and other results on recursive progressions are discussed in\n Appendix B.\n Here let us just mention that one has considered other progressions\nbased on various extension procedures \\(\\bT \\mapsto \\bT'\\) that\nstrengthen a given theory,\n notably:[22] \n(R2) is called an extension by the local reflection\nprinciple, whereas (R3) uses the uniform reflection\nprinciple. Feferman obtained in 1962 an amazing result that\nstrengthens Turing’s result in a dramatic way. \nLet \\((\\bT_a)_{a\\in{\\cO}}\\) be a progression using the uniform\nreflection principle with PA as the base theory\nT. Then we have: for any true arithmetical sentence F\nthere is an \\(a\\in{\\cO}\\), such that \\(\\bT_a\\vdash F\\). Moreover,\n\\(a\\in{\\cO}\\) can be chosen such that \\({{\\mid}\\,{a}\\,{\\mid}}\\leq\n\\omega^{\\omega^{\\omega+1}}\\). \nFor further discussion see\n Appendix B.\n Here we just note that the union of the \\(\\bT_a\\) is no longer a\nformal theory. \nIn the foregoing progressions the ordinals remained external to the\ntheory. Autonomous progressions of theories are the proper\ninternalization of the general concept of progressions. In the\nautonomous case one is allowed to ascend to a theory \\(\\bT_a\\) only if\none already has shown in a previously accepted theory \\(\\bT_b\\) that\n\\(a\\in{\\cO}\\). This idea of generating a hierarchy of theories via a\nboot-strapping process appeared for the first time in Kreisel 1960,\nwhere it was proposed as a\nway of characterizing finitism and predicativism in mathematically\nprecise way. In more formal terms, the starting point is a theory\n\\(\\bT_0\\) which is accepted as correct and an extension procedure\n\\(\\bT\\mapsto\\bT'\\) which is viewed as leading from a correct\ntheory T to a more encompassing correct theory \\(\\bT'\\).\nMoreover, the language of these theories is supposed to contain a\nformula \\(\\rAcc(x)\\) such that provability of \\(\\rAcc(\\bar{a})\\) in a\ncorrect theory entails that\n \\(a\\in{\\cO}\\).[23]\n Kreisel singled out two autonomous progressions of theories\n\\(\\{{{\\bF}}_a\\}\\) and \\(\\{{{\\bR}}_a\\}\\) for finitism and\npredicativity, respectively, and determined the least upper bound of\nthe \\(\\lvert a\\rvert\\) appearing in the first hierarchy to be the ordinal\n\\(\\varepsilon_0\\) which is also the proof-theoretic ordinal of\nPA. The determination of the least upper bound for\nthe predicative hierarchy \\(\\{{{\\bR}}_a\\}\\) was achieved independently\nby Feferman (1964) and Schütte (1964, 1965). It turned out that\nthis ordinal can be expressed in a notation system developed by Veblen\nthat will be presented next. \nAs we saw above, ordinals below \\(\\varepsilon_0\\) suffice for the\nproof-theoretic treatment of PA. For stronger\ntheories segments larger than \\(\\varepsilon_0\\) have to be employed,\nrequiring new normal forms akin to Cantor’s normal form. Ordinal\nrepresentation systems utilized by proof theorists in the 1960s first\nemerged in a purely set-theoretic context more than 50 years earlier.\nIn 1904 Hardy wanted to “construct” a subset of \\(\\bbR\\)\nof size \\(\\aleph_{1}\\), the first uncountable cardinal. His method was\nto represent countable ordinals via increasing sequences of natural\nnumbers and then to correlate a decimal expansion with each such\nsequence. Hardy used two processes on sequences: (i) Removing the\nfirst element to represent the successor; (ii) Diagonalizing at\nlimits. For example, if the sequence \\(1,2,3,\\ldots\\) represents the\nordinal 1, then \\(2,3,4,\\ldots\\) represents the ordinal 2 and\n\\(3,4,5,\\ldots\\) represents the ordinal 3 etc., while the\n‘diagonal’ \\(1,3,5,\\ldots\\) provides a representation of\n\\(\\omega\\). In general, if \\(\\lambda=\\lim_{n\\in\\bbN}\\lambda_n\\) is a\nlimit ordinal with \\(b_{n1},b_{n2},b_{n3},\\ldots\\) representing\n\\(\\lambda_n<\\lambda\\), then \\(b_{11},b_{22},b_{33},\\ldots\\)\nrepresents λ. This representation, however, depends on the\nsequence chosen with limit λ. A sequence\n\\((\\lambda_n)_{n\\in\\bbN}\\) with \\(\\lambda_n<\\lambda\\) and\n\\(\\lim_{n\\in \\bN}\\lambda_n=\\lambda\\) is called a fundamental\nsequence for λ. Hardy’s two operations give\nexplicit representations for all ordinals \\(<\\omega^2\\). \nVeblen in 1908 extended the initial segment of the countable for which\nfundamental sequences can be given effectively. The new tools he\ndevised were the operations of derivation and transfinite\niteration applied to continuous increasing functions on\nordinals. \nDefinition 5.4 Let \\({\\ON}\\) be the class of\nordinals. A (class) function \\(f:{\\ON}\\to{\\ON}\\) is said to be\nincreasing if \\(\\alpha<\\beta\\) implies\n\\(f(\\alpha)<f(\\beta)\\) and continuous (in the order\ntopology on \\({\\ON}\\)) if \nholds for every limit ordinal λ and increasing sequence\n\\((\\alpha_{\\xi})_{\\xi<\\lambda}\\). f is called\nnormal if it is increasing and continuous. \nThe function \\(\\beta\\mapsto \\omega+\\beta\\) is normal while\n\\(\\beta\\mapsto \\beta+\\omega\\) is not continuous at \\(\\omega\\) since\n\\(\\lim_{\\xi<\\omega}(\\xi+\\omega)=\\omega\\) but\n\\((\\lim_{\\xi<\\omega}\\xi)+\\omega=\\omega+\\omega\\). \nDefinition 5.5 The derivative\n\\(f'\\) of a function \\(f:{\\ON}\\rightarrow {\\ON}\\) is the function\nwhich enumerates in increasing order the solutions of the equation\n\\(f( \\alpha )= \\alpha\\), also called the fixed points of\nf.  \nIf f is a normal function, \\(\\{\\alpha:\\,f(\\alpha)=\\alpha\\}\\) is\na proper class and \\(f'\\) will be a normal function, too. \nDefinition 5.6 Now, given a normal function\n\\(f:\\ON \\rightarrow \\ON\\), define a hierarchy of normal functions as\nfollows:  \nIn this way, from the normal function f we get a two-place\nfunction, \\(\\varphi_{f} ( \\alpha , \\beta )\\coloneqq f_{\\alpha} ( \\beta\n)\\). One usually discusses the hierarchy when \\(f=\\ell\\), where\n\\(\\ell( \\alpha )=\\omega^{\\alpha}\\). The least ordinal \\(\\gamma>0\\)\nclosed under \\(\\varphi_{\\ell}\\), i.e., the least ordinal \\(>0\\)\nsatisfying\n\\((\\forall\\alpha,\\beta<\\gamma)\\;\\varphi_{\\ell}(\\alpha,\\beta)<\\gamma\\)\nis called \\(\\Gamma_0\\). It has a somewhat iconic status, in particular\nsince Feferman and Schütte determined it to be the least ordinal\n‘unreachable’ by certain predicative means expressed\nin terms of autonomous progressions of theories (defined in\n section 5.2). \nVeblen extended this idea first to arbitrary finite numbers of\narguments, but then also to transfinite numbers of arguments, with the\nproviso that in, for example \\(\\Phi_{f} ( \\alpha_{0} , \\alpha_{1} ,\n\\ldots , \\alpha_{\\eta} )\\), only a finite number of the arguments\n\\(\\alpha_{\\nu}\\) may be non-zero. Finally, Veblen singled out the\nordinal \\(E(0)\\), where \\(E(0)\\) is the least ordinal \\(\\delta >\n0\\) which cannot be named in terms of functions \\(\\Phi_{\\ell} (\n\\alpha_{0} , \\alpha_{1} , \\ldots , \\alpha_{\\eta} )\\) with \\(\\eta <\n\\delta\\), and each \\(\\alpha_{ \\gamma } < \\delta\\). \nThough the “great Veblen number” (as \\(E(0)\\) is sometimes\ncalled) is quite an impressive ordinal it does not furnish an ordinal\nrepresentation sufficient for the task of analyzing a theory as strong\nas \\(\\Pi^1_1\\)-comprehension. Of course, it is possible to go beyond\n\\(E(0)\\) and initiate a new hierarchy based on the function\n\\(\\xi\\mapsto E(\\xi)\\) or even consider hierarchies utilizing finite\ntype functionals over the ordinals. Still all these further steps\namount to rather modest progress over Veblen’s methods. In 1950\nBachmann presented a new kind of operation on ordinals which dwarfs\nall hierarchies obtained by iterating Veblen’s methods. Bachmann\nbuilds on Veblen’s work but his novel idea was the systematic\nuse of uncountable ordinals to keep track of the functions\ndefined by diagonalization. Let \\({\\Omega}\\) be the first uncountable\nordinal. Bachmann defines a set of ordinals \\(\\fB\\) closed under\nsuccessor such that with each limit \\(\\lambda\\in \\fB\\) is associated\nan increasing sequence \\(\\langle\n\\lambda[\\xi]:\\,\\xi<\\tau_{\\lambda}\\rangle\\) of ordinals \\(\n\\lambda[\\xi] \\in \\fB\\) of length \\(\\tau_{\\lambda}\\leq\\Omega\\) and\n\\(\\lim_{\\xi<\\tau_{\\lambda}}\\lambda[\\xi]=\\lambda\\). A hierarchy of\nfunctions \\((\\varphi^{\\ssfB}_{\\alpha})_{\\alpha \\in \\fB}\\) is then\nobtained as follows: \nAfter the work of Bachmann, the story of ordinal representations\nbecomes very complicated. Significant papers (by Isles, Bridge,\nPfeiffer, Schütte, Gerber to mention a few) involve quite\nhorrendous computations to keep track of the fundamental sequences.\nAlso Bachmann’s approach was combined with uses of higher type\nfunctionals by Aczel and Weyhrauch. Feferman proposed an entirely\ndifferent method for generating a Bachmann-type hierarchy of normal\nfunctions which does not involve fundamental sequences. Buchholz\nfurther simplified the systems and proved their computability. For\ndetails we recommend the preface to Buchholz et al. 1981. \nThe ordinal that Feferman and Schütte determined as the least\nupper bound for \\(\\{{{\\bR}}_a\\}\\) is \\(\\Gamma_0\\), the least non-zero\nordinal closed under the Veblen function\n\\(\\alpha,\\beta\\mapsto\\varphi_{\\alpha}(\\beta)\\). This was a genuine\nproof-theoretic result with the tools coming ready-made from\nSchütte’s (1960b) monograph. There he had calculated the\nproof-theoretic ordinals of the \\({{\\bR}}_a\\) as a function of\n\\(\\lvert a\\rvert\\), using cut elimination techniques for logics with infinitary\nrules (dubbed “semi-formal systems”). If\n\\(\\lvert a\\rvert =\\omega^{\\alpha}\\) then \\(\\lvert {{\\bR}}_a\\rvert =\\varphi_{\\alpha}(0)\\).\n“Semi-formal” is a terminology employed by Schütte\nand refers to the fact that this proof system has rules with\ninfinitely many premises, similar to the \\(\\omega\\)-rule. \nDefinition 5.7 In the following we assume\nthat the ordinals come from some representation system. The language\nof \\(\\bRA^*\\) is an extension of that of first order arithmetic. For\neach ordinal \\(\\alpha\\) and \\(\\beta>0\\) it has free set variables\n\\(U_0^\\alpha,U^\\alpha_1,U^\\alpha_2\\ldots\\) of level \\(\\alpha\\) and\nbound set variables of level \\(\\beta\\). The level,\n\\({\\textrm{lev}(A)}\\), of a formula A of \\(\\bRA^*\\) is defined\nto be the maximum of the levels of set variables that occur in\nA. Expressions of the form \\(\\{x\\mid A(x)\\}\\) with \\(A(u)\\) a\nformula will be called abstraction terms, their level being\nthe same as that of the formula \\(A(u)\\). \nThe inference rules of \\(\\bRA^*\\) comprise those of the sequent\ncalculus with the exception of \\((\\forall R)\\) and \\((\\exists L)\\)\nwhich are replaced by those for the \\(\\omega\\)-rule: \\(\\omega\\)R and\n\\(\\omega\\)L. Below \\(\\fP_{\\beta}\\) stands for the set of all\nabstraction terms with levels \\(<\\beta\\). The rules for the set\nquantifiers are as follows:  \nwhere in \\(\\forall_\\beta\\bL\\) and \\(\\exists_\\beta\\rR\\), P is an\nabstraction term of level \\(<\\beta\\). \nAs per usual, the price one has to pay for rules with infinitely many\npremises is that derivations become infinite (well-founded) trees. The\nlength of a derivation can then be measured by the ordinal rank\nassociated with the tree. One also wants to keep track of the\ncomplexity of cuts in the derivation. The length we assign to a\nformula A, \\(\\lvert A\\rvert\\), measures its complexity. It is an ordinal\nof the form \\(\\omega\\cdot \\alpha+n\\) where \\(\\alpha\\) is the level of\nA and \\(n<\\omega\\). One then defines a notion of\nderivability in \\({{\\bRA}}\\),  \nwhere \\(\\alpha\\) majorizes the transfinite length of the derivation\nand \\(\\rho\\) conveys that all cut formulae in the derivation have\nlength \\(<\\rho\\). \nCut elimination works smoothly for \\(\\bRA^*\\), however, the prize one\nhas to pay can only be measured in terms of Veblen’s \\(\\varphi\\)\nfunction. The optimal result is the so-called second cut elimination\ntheorem. \nTheorem 5.8 (Second Cut Elimination Theorem)\n \nIt entails of course the special case that \\(\\bRA^*\n\\stile{\\alpha}{\\omega^\\nu} \\Gamma\\Rightarrow \\Delta\\) yields \\(\\bRA^*\n\\stile{\\varphi_{\\nu}(\\alpha)}{0} \\Gamma\\Rightarrow \\Delta\\), and thus,\nas the latter deduction is cut-free, all cuts can be removed. Several\nsubtheories of \\(\\bZ_2\\) can be interpreted in \\(\\bRA^*\\), yielding\nupper bounds for their proof-theoretic ordinals via\n Theorem 5.8.\n Here is a selection of such\n results:[24] \nTheorem 5.9 \nFor the definitions of the theories in this theorem, see end of\n section 5.1.\n To obtain the results about theories in (iii) and (iv) it is somewhat\neasier to first reduce them to systems of the form\n\\((\\Pi^0_1{\\Hy}\\bCA)_{<\\rho}\\) as the latter have a straightforward\ninterpretation in \\({{\\bRA}}^*\\). Reductions of\n\\((\\Delta^1_1{\\Hy}\\bCR)\\) to\n\\((\\Pi^0_1{\\Hy}\\bCA)_{<\\omega^{\\omega}}\\) and of\n\\((\\Sigma^1_1{\\Hy}\\bAC)\\) to\n\\((\\Pi^0_1{\\Hy}\\bCA)_{<\\varepsilon_0}\\) are due to Feferman (1964)\nand Friedman (1970),\n respectively.[25] \nThe investigation of such subsystems of analysis and the determined\neffort to establish their mathematical power led to a research program\nthat was initiated by Friedman and Simpson some thirty years ago and\ndubbed Reverse Mathematics. The objective of the program is\nto investigate the role of set existence principles in ordinary\n mathematics.[26]\n The main question can be stated as follows: \nGiven a specific theorem \\(\\tau\\) of ordinary mathematics, which set\nexistence axioms are needed in order to prove \\(\\tau\\)? \nCentral to the above is the reference to what is called ‘ordinary\nmathematics’. This concept, of course, doesn’t have a\nprecise definition. Roughly speaking, by ordinary mathematics we mean\nmain-stream, non-set-theoretic mathematics, i.e., the core areas of\nmathematics which make no essential use of the concepts and methods of\nset theory and do not essentially depend on the theory of uncountable\ncardinal numbers. \nFor many mathematical theorems \\(\\tau\\), there is a weakest natural\nsubsystem \\({\\bS}(\\tau)\\) of \\(\\bZ_2\\) such that \\({\\bS}(\\tau)\\)\nproves \\(\\tau\\). Very often, if a theorem of ordinary mathematics is\nproved from the weakest possible set existence axioms, the statement\nof that theorem will turn out to be provably equivalent to those\naxioms over a still weaker base theory. Moreover, it has turned out\nthat \\({\\bS}(\\tau)\\) often belongs to a small list of specific\nsubsystems of \\(\\bZ_2\\) dubbed \\(\\RCA_0\\), \\(\\WKL_0\\), \\(\\ACA_0\\),\n\\(\\ATR_0\\) and \\(({\\bPi}^1_1{\\Hy}{\\bCA})_0\\),\n respectively.[27]\n The systems are enumerated in increasing strength. \\(\\ACA_0\\) is\nactually the same theory as \\((\\Pi^1_0{\\Hy}\\bCA)_0\\). The main set\nexistence axioms of \\(\\RCA_0\\), \\(\\WKL_0\\), \\(\\ACA_0\\), \\(\\ATR_0\\),\nand \\(({{\\bPi}}^1_1{\\Hy}{\\bCA})_0\\) are recursive comprehension, weak\nKönig’s lemma, arithmetical comprehension, arithmetical\ntransfinite recursion, and \\(\\bPi_1^1\\)-comprehension, respectively.\n\\(\\ACA_0\\) is actually the same theory as \\((\\Pi^1_0{\\Hy}\\bCA)_0\\).\nFor exact definitions of all these systems and their role in reverse\nmathematics see Simpson 1999. Examples of mathematical statements\nprovable in \\(\\RCA_0\\) are the intermediate value theorem and the\nBaire category theorem. Reversals for \\(\\WKL_0\\) are the Heine/Borel\ncovering lemma and the local existence theorem for solutions of\nordinary differential equations. Among the many reversals for\n\\(\\ACA_0\\), \\(\\ATR_0\\), and \\((\\bPi^1_1{\\Hy}{\\bCA})_0\\) one finds the\nexistence of maximal ideals in countable rings, Ulm’s theorem,\nand the Cantor-Bendixson theorem, respectively. \nThe proof-theoretic strength of \\(\\RCA_0\\) is weaker than that of\nPA while \\(\\ACA_0\\) has the same strength as\nPA. To get a sense of scale, the strengths of the\nfirst four theories are best expressed via their proof-theoretic\nordinals: \nTheorem 5.10 \n\\(\\lvert (\\bPi^1_1\\Hy\\bCA)_0\\rvert\\), however, eludes expression in the ordinal\nrepresentations introduced so far. This will require the much stronger\nrepresentation to be introduced in\n Definition 5.11. \nThere are important precursors of reverse mathematics. Weyl (1918)\nstarted to develop analysis using a minimalist foundation (that\nequates to \\(\\ACA_0\\)) whilst Hilbert and Bernays (1939) developed\nanalysis in second order arithmetic, whereby on closer inspection one\nsees that all they used is \\((\\bPi^1_1\\Hy\\bCA)_0\\). The first theorem\nof genuinely reverse mathematics was established by Dedekind in his\nessay Stetigkeit und irrationale Zahlen (1872). It states\nthat his continuity (or completeness) principle is equivalent to a\nwell-known theorem of analysis, namely, every bounded, monotonically\nincreasing sequence has a limit. He emphasizes,  \nThis theorem is equivalent to the principle of continuity, i.e., it\nloses its validity as soon as we assume a single real number not to be\ncontained in the domain \\(\\cR\\) [of all real numbers, i.e., of all\ncuts of rational numbers]. (1872 [1996: 778])  \nIt is to bring out “the connection between the principle of\ncontinuity and infinitesimal analysis” (1872 [1996: 779]). \nSpector’s (1962) functional interpretation of \\(\\bZ_2\\) via bar\nrecursive functionals was of great interest to proof theory. However,\nit was not clear whether there was a constructive foundation of these\nfunctionals along the lines of hereditarily continuous functionals\nthat can be represented by computable functions (akin to Kleene 1959;\nKreisel 1959) which would make them acceptable on intuitionistic\ngrounds. In 1963 Kreisel conducted a seminar the expressed aim of\nwhich was to assay the constructivity of Spector’s\ninterpretation (see Kreisel 1963). Specifically he asked whether an\nintuitionistic theory of monotonic inductive definitions,\n\\(\\bID^i_1(\\text{mon})\\), could model bar recursion, or even more\nspecifically, formally capture a class of indices of representing\nfunctions of these functionals. In a subsequent report the\nseminar’s conclusion was later summarized by Kreisel: \n… the answer is negative by a wide margin, since not even bar\nrecursion of type 2 can be proved consistent [from intuitionistically\naccepted principles]. (Kreisel in “Reports of the Seminar on the Foundations of Analysis,\nStanford, Summer 1963”, as quoted in Feferman 1998: 223)\n \nHe not only introduced theories of one inductive definition but also\nof \\(\\nu\\)-times transfinitely iterated inductive definitions,\n\\(\\bID_{\\nu}\\). Albeit it soon became clear that even the theories\n\\(\\bID_{\\nu}\\) couldn’t reach the strength of \\(\\bZ_2\\) (in\npoint of fact, such theories are much weaker than the fragment of\n\\(\\bZ_2\\) based on \\(\\Pi^1_2\\)-comprehension); they became the subject\nof proof-theoretic investigation in their own right and occupied the\nattention of proof theorists for at least another 15 years. One reason\nfor this interest was surely that the intuitionistic versions\ncorresponding to the accessible (i.e., well-founded) part of a\nprimitive recursive ordering are immediately constructively appealing\nand a further reason was that they were thought to be more amenable to\ndirect proof-theoretic treatments than fragments of \\(\\bZ_2\\) or set\ntheories. \nWe shall not give a detailed account of the formalization of these\ntheories, but focus on the non-iterated case \\(\\bID_1\\) and its\nintuitionistic version \\(\\bID_1^i\\) to convey the idea. A monotone\noperator on \\(\\bbN\\) is a map \\(\\Gamma\\) that sends a set \\(X\\subseteq\n\\bbN\\) to a subset \\(\\Gamma(X)\\) of \\(\\bbN\\) and is monotone, i.e.,\n\\(X\\subseteq Y\\subseteq \\bbN\\) implies \\(\\Gamma(X) \\subseteq\n\\Gamma(Y)\\). Owing to monotonicity, the operator \\(\\Gamma\\)\nwill have a least fixed point \\(I_{\\Gamma}\\subseteq\\bbN\\), i.e.,\n\\(\\Gamma( I_{\\Gamma})=I_{\\Gamma}\\) and for every other fixed point\nX it holds \\(I_{\\Gamma} \\subseteq X\\).\nSet-theoretically \\(I_{\\Gamma}\\) is obtained by iterating \\(\\Gamma\\)\nthrough the ordinals,  \nMonotonicity ensures (in set theory) that one finds an ordinal\n\\(\\tau\\) such that \\(\\Gamma(\\Gamma^{\\tau})=\\Gamma^{\\tau}\\), and the\nset \\(\\Gamma^{\\tau}\\) will be the least fixed point. If one adds a new\n1-place predicate symbol P to the language of arithmetic, one\ncan describe the so-called positive arithmetical operators. They are\nof the form  \nwhere \\(A(x,P)\\) is a formula of the language of PA\naugmented by P in which the predicate P occurs only\n positively.[28]\n The syntactic condition of positivity then ensures that the operator\n\\(\\Gamma_A\\) is monotone. The language of \\(\\bID_1\\) is an extension\nof that of PA. It contains a unary predicate symbol\n\\(I_A\\) for each positive arithmetical operator \\(\\Gamma_A\\) and the\naxioms  \nwhere in (Id2) \\(F(x)\\) is an arbitrary formula of \\(\\bID_1\\) and\n\\(A(x,F)\\) arises from \\(A(x,P)\\) by replacing every occurrence of\n\\(P(t)\\) in the formula by \\(F(t)\\). Collectively these axioms assert\nthat \\(I_A\\) is the least fixed point of \\(\\Gamma_A\\), or more\naccurately the least among all sets of natural numbers definable in\nthe language of \\(\\bID_1\\). \\(\\bID_1^i\\) will be used to denote the\nintuitionistic version. Its subtheory \\(\\bID_{1}^{i}(\\cO)\\) is\nobtained by just adding the predicate symbol \\(I_A\\) and the\npertaining axioms (Id1) and (Id2), where \\(\\Gamma_A\\) is the operator\nthat defines Kleene’s \\(\\cO\\) (cf.\n Definition 5.1). \nBy a complicated passage through formal theories for choice sequences\nit was known that the theory \\(\\bID_{1}\\) can be reduced to\n\\(\\bID_{1}^{i}(\\cO)\\). The first ordinal analysis for the theory\n\\(\\bID_{1}^{i}(\\cO)\\) was obtained by Howard (1972). Via the known\nproof-theoretical reductions this entailed also an ordinal analysis\nfor \\(\\bID_{1}\\). The proof-theoretic ordinal of \\(\\bID_{1}\\) is the\nBachmann-Howard ordinal, which is denoted by\n\\({\\psi_{\\sOmega_{1}}(\\varepsilon_{\\Omega_1+1})}\\) in the system of\n Definition 5.11. \nAs inductively defined sets can be the starting point of another\ninductive definition, the procedure of inductively defining predicates\ncan be iterated along any well-ordering \\(\\nu\\) in a uniform way. This\nleads to the theories \\(\\bID_{\\nu}\\) which allow one to formalize\n\\(\\nu\\)-times iterated inductive definitions, where \\(\\nu\\) stands for\na primitive recursive well-ordering. If \\(\\nu\\) is a well-ordering on\nconstructive grounds then also the \\(\\nu\\)-times iterated version of\nKleene’s \\(\\cO\\) has a clear constructive meaning. As a result\nthe formal theories \\({{\\bID}}^i_{\\nu}(\\cO)\\) that embody this process\nare constructively justified. The topic of theories of iterated\ninductive definitions was flourishing at the 1968 conference on\nIntuitionism and Proof Theory in Buffalo (see Kino et al. 1970). One\nof the main proof-theoretic goals was to find a reduction of the\nclassical theories \\(\\bID_{\\nu}\\) to their intuitionistic counterparts\n\\({{\\bID}}^i_{\\nu}(\\cO)\\). This was all the more desirable because of\nknown reductions of important fragments of second order arithmetic to\ntheories of the former kind. Friedman (1970) had shown that the second\norder system with the \\(\\Sigma _2^1\\)-axiom of choice can be\ninterpreted in the system\n\\(\\hbox{($\\Pi_1^1$-CA)}_{<{\\varepsilon_0}}\\) of less than\n\\({\\varepsilon_0}\\)-fold iterated \\(\\Pi_1^1\\)-comprehensions and\nFeferman (1970a) had shown that less than \\(\\nu\\)-fold iterated\n\\(\\Pi_1^1\\)-comprehensions could be interpreted in the system  \nfor \\(\\nu=\\omega^{\\gamma}\\) with \\(\\gamma\\) limit. However, Zucker\n(1973) showed that there are definite obstacles to a straightforward\nreduction of the theories \\(\\text{ID}_{\\nu}\\) for \\(\\nu > 1\\) to\ntheir intuitionistic cousins. Sieg (1977) attacked the problem by a\nmethod adapted from Tait (1970) who had used cut elimination for an\ninfinitary propositional logic with formulae indexed over constructive\nnumber classes to obtain a consistency proof for second order\narithmetic theory with the schema of \\(\\Sigma _2^1\\) dependent\nchoices. He achieved a reduction of \\(\\bID_{<\\nu}\\) to\n\\(\\bID_{<\\nu}^i(\\cO)\\) for limit \\(\\nu\\) by carrying out the proof\ntheory for a system of \\(\\PL_{\\alpha}\\) of propositional logic with\ninfinitely long conjunctions and disjunctions indexed over the\nconstructive number classes \\(\\cO_{\\alpha}\\) for \\(\\alpha<\\nu\\)\ninside \\(\\bID^i_{\\alpha+1}(\\cO)\\). As \\(\\bID_{\\alpha}\\) can be reduced\nto \\(\\PL_{\\alpha}\\) this brought about the reduction. Ordinal analyses\nfor theories of iterated inductive definitions, first for finite and\nthen also for transfinite iterations, were obtained by Pohlers using\nTakeuti’s reduction procedure for \\(\\Pi_1^1\\)-comprehension (see\nPohlers 1975, 1977). Working independently, Buchholz (1977a) used a new type of rules, dubbed\n\\(\\Omega_{\\mu+1}\\)-rules to recapture these results without use of\nTakeuti’s methods. These rules are an extension of the\nΩ-rule described in\n Definition C.3. \nThe ordinal representation systems encountered so far are not\nsufficient for expressing the strength of theories of iterated\ninductive definitions nor that of the strongest of the standard system\nof reverse mathematics, \\((\\Pi^1_1{\\Hy}\\bCA)_0\\). Therefore we\nintersperse a brief account of how to proceed onwards, adumbrating the\nmain ideas. \nBachmann’s bold move of using large ordinals to generate names\nfor small ordinals was a very important idea. To obtain ordinal\nanalyses of ever stronger theories one has to find new ways of\ndefining ordinal representation systems that can encapsulate their\nstrength. The latter goes hand in hand with the development of new cut\nelimination techniques that are capable of removing cuts in\n(infinitary) proof systems with strong reflection rules. Ordinal\nrepresentations, however, appear to pose a considerable barrier to\nunderstanding books and articles in this research area. Nonetheless we\nthink that they are the best way to express the proof-theoretic\nstrength of a theory as they provide a scale by means of which one can\nget a grasp of how much stronger a theory \\(S_1\\) is than another\ntheory \\(S_2\\) (rather than the bland statement that \\(S_1\\) is\nstronger than \\(S_2\\)). \nAs an example we will introduce an ordinal representation system which\ncharacterizes the theory \\((\\Pi^1_1{\\Hy}\\bCA)+{{\\BI}}\\), following\nBuchholz 1977b. It is\nbased on certain ordinal functions \\(\\psi_{\\sOmega_n}\\) which are\noften called collapsing functions. The definition of these\nfunctions, that is of the value \\({\\psi_{\\sOmega_{n}}(\\alpha)}\\) at\n\\(\\alpha\\), proceeds by recursion on \\(\\alpha\\) and gets intertwined\nwith the definition of sets of ordinals\n\\(C^{{\\Omega_{\\omega}}}(\\alpha,\\beta)\\), dubbed “Skolem\nhulls” since they are defined as the smallest structures closed\nunder certain functions specified below. \nLet \\(\\bbN^+\\) be the natural numbers without 0. Below we shall assume\nthat \\(\\Omega_n\\) \\((n\\in\\bbN^+)\\) is a “large” ordinal\nand that \\(\\omega<\\Omega_n <\\Omega_{n+1}\\). Their limit,\n\\(\\sup_{n\\in\\bbN^+}\\Omega_n\\), will be denoted by\n\\(\\Omega_{\\omega}\\). \nDefinition 5.11 By recursion on \\(\\alpha\\)\nwe define:  \nAt this point it is not clear whether \\({\\psi_{\\sOmega_{n}}(\\alpha)}\\)\nwill actually be defined for all \\(\\alpha\\) since there might not\nexist a \\(\\rho<\\Omega_n\\) such that  \nThis is where the “largeness” of \\(\\Omega_n\\) comes into\nplay. One (easy) way of guaranteeing this consists in letting\n\\(\\Omega_n\\) be the \\(n^{th}\\) uncountable regular cardinal, that is\n\\(\\Omega_n\\coloneqq \\aleph_n\\). However, such strong set-theoretic\nassumptions can be avoided. For instance, it suffices to let\n\\(\\Omega_n\\) be the \\(n^{th}\\) recursively regular ordinal (which is a\ncountable ordinal) (see Rathjen 1993a). \nTo get a better feel for what \\(\\psi_{\\sOmega_{n}}\\) is doing, note\nthat if \\(\\rho={\\psi_{\\sOmega_{n}}(\\alpha)}\\), then\n\\(\\rho<\\Omega_n\\) and with \\([\\rho,\\Omega_n)\\) being the interval\nconsisting of ordinals \\(\\rho\\leq\\alpha<\\Omega_n\\) one has  \nthus the order-type of the ordinals below \\(\\Omega_n\\) which belong to\nthe “Skolem hull” \\(C^{{\\Omega_{\\omega}}}(\\alpha,\\rho)\\)\nis \\(\\rho\\). In more pictorial terms, \\(\\rho\\) is said to be the\n\\(\\alpha^{th}\\) collapse of \\(\\Omega_n\\) since the order-type\nof \\(\\Omega_n\\) viewed from within the structure\n\\(C^{{\\Omega_{\\omega}}}(\\alpha,\\rho)\\) is actually \\(\\rho\\). \nThe ordinal representation system we are after is provided by the set\n \nwhere \\(\\varepsilon_{\\Omega_{\\omega}+1} \\) is the least epsilon number\nafter \\(\\Omega_{\\omega}\\), i.e., the least ordinal\n\\(\\eta>\\Omega_{\\omega}\\) such that \\(\\omega^{\\eta}=\\eta\\). The\nproof-theoretic ordinal of \\((\\Pi^1_1{\\Hy}\\bCA)+{{\\BI}}\\) is\n\\({\\psi_{\\sOmega_{1}}(\\varepsilon_{\\Omega_{\\omega}+1})}\\). Although\nthe definition of the set\n\\(C^{{\\Omega_{\\omega}}}(\\varepsilon_{\\Omega_{\\omega}+1},0)\\) and its\nordering is set-theoretic, it turns that it also has a purely\nprimitive recursive definition which can be given in a fragment of\nPRA. Thus the set-theoretic presentation mainly\nserves the purpose of a “visualization” of an elementary\nwell-ordering. \nThe pattern of definition exhibited in\n Definition 5.11\n continues for stronger systems, albeit only as a basic template since\nfor theories beyond the level of \\((\\Delta^1_2{\\Hy}\\bCA)+{{\\BI}}\\)\nsubstantially new ideas are required. Analogies between large\nset-theoretic ordinals (cardinals) and recursively large ordinals on\nthe one hand and ordinal representation systems on the other hand can\nbe a fruitful source of inspiration for devising new representation\nsystems. More often than not, hierarchies and structural properties\nthat have been investigated in set theory and recursion theory on\nordinals turn out to have proof-theoretic counterparts. \nUsing an extended version of the representation system from\n Definition 5.11\n if\n \\(\\nu>\\omega\\),[29]\n the outcome of all the work on the theories of inductive definitions\ncan be summarized by the following\n theorem.[30] \nTheorem 5.12 For recursive \\(\\nu\\),  \nA generalized treatment of theories of iterated inductive definitions\nfor arbitrary well-orderings and of autonomous iteration was carried\nout in Rathjen 1988, 2010. These theories are stronger than\n\\((\\Delta^1_2{\\Hy}\\bCA)\\) if \\(\\nu\\geq\\varepsilon_0\\). \n\n Theorem 5.12\n played an important role in determining the exact strength of some\nfragments of \\(\\bZ_2\\). The major ordinal-theoretic results pertaining\nto subsystems of \\(\\bZ_2\\) of the pre-1980 area given in the next\n theorem.[31] \nTheorem 5.13 \nThe next challenge after \\((\\Delta^1_2{\\Hy}\\bCA)\\) was posed by the\ntheory \\((\\Delta^1_2{\\Hy}\\bCA)+{{\\BI}}\\). Its treatment not only\nrequired a considerably stronger ordinal representation system but\nalso coincided with a shift away from \\(\\cL_2\\) theories and theories\nof iterated inductive definitions to a direct proof-theoretic\ntreatment of set theories. Pioneering work on the proof theory of set\ntheories is mainly due to Jäger (1980, 1982). The analysis of\n\\((\\Delta^1_2{\\Hy}\\bCA)+{{\\BI}}\\) in Jäger & Pohlers 1982\nprovides a particularly nice application of this new approach which\nhas been called admissible proof theory, owing to its concern\nwith theories extending Kripke-Platek set theory by axioms asserting\nthe existence of admissible sets (for some details see\n Appendix D.1). \nTheorem 5.14 \\(\\lvert (\\Delta^1_2{\\Hy}\\bCA)+\\BI\\rvert =\n\\psi_{\\sOmega_1}(\\varepsilon_{I+1})\\) \nThe “I” in the foregoing notation is supposed to be\nindicative of “inaccessible cardinal”. Indeed,\nthe easiest way to build an extended ordinal representation system\nsufficient unto this task (modeled on\n Definition 5.11)\n is to add an inaccessible I, close the Skolem hulls under\n\\(\\xi\\mapsto \\Omega_\\xi\\) for \\(\\xi<I\\) and introduce collapsing\nfunctions \\(\\psi_{\\pi}\\) for all \\(\\pi\\) of either form I or\n\\(\\Omega_{\\xi+1}\\). \nThe goal of giving an ordinal analysis of full second order arithmetic\nhas not been attained yet. For many years \\(\\Pi^1_2\\)-comprehension\nposed a formidable challenge and the quest for its ordinal analysis\nattained something of a holy grail status (cf. Feferman 1989). At\nfirst blush it might be difficult to see why the latter comprehension\nis so much more powerful than \\(\\Delta^1_2\\)-comprehension (plus\n\\({{\\BI}}\\)). To get a sense for the enormous difference, it seems\nadvisable to work in (admissible) set theory and consider a hierarchy\nof recursively large ordinal notions wherein these comprehension\nschemes correspond to the bottom and the top end of the scale,\nrespectively. That is discussed in\n Appendix D.\n Here we just mention a few reductions to “constructive”\nframeworks. The reductions we have in mind, underlie a broadened view\nof “constructivity”. Constructive theories of functions\nand sets that relate to Bishop’s constructive mathematics as\ntheories like ZFC relate to Cantorian set theory have\nbeen proposed by Myhill, Martin–Löf, Feferman and Aczel.\nAmong those are Feferman’s constructive theory of operations and\nclasses, \\(\\bT_0\\) in 1975 and 1979, Martin-Löf’s\nintuitionistic type theory (1984) and constructive set theory,\nespecially Constructive Zermelo-Fraenkel Set Theory,\nCZF, the latter also combined with the regular\nextension axiom, REA. By employing an ordinal\nanalysis for set theory KPi which is an extension of\nKripke-Platek set theory via an axiom asserting that every set is\ncontained in an admissible set (see\n Appendix D)\n it has been shown that KPi and consequently\n\\((\\Delta^1_2{\\Hy}\\bCA)+{{\\BI}}\\) can be reduced to both of these\ntheories: \nTheorem 5.15 (Feferman 1975; Jäger\n1983; Jäger & Pohlers 1982; Rathjen 1993b)\n\\((\\Delta^1_2{\\Hy}\\bCA)+\\BI\\), KPi, \\(\\bT_0\\) and\nCZF+REA are proof-theoretically\nequivalent. In particular, these theories prove the same theorems in\nthe negative arithmetic fragment. \nTheorem 5.16 (Rathjen 1993b; Setzer 1998)\nThe soundness of the negative arithmetic fragment of\n\\(\\Delta^1_2-\\bCA+\\BI\\) and KPi is provable in\nMartin-Löf’s 1984 type theory. \nA detailed account of these results has been given in section 3 of\nRathjen 1999a. \nProof theory has become a large subject with many specialized branches\nthat can be mathematically quite complex. So we have tried to present\ndevelopments close to the main artery of its body, starting with its\ninception at the beginning of the twentieth century and ending with\nresults from the close of the century. The theorems mentioned at the\nend of section 5 foreshadow investigations in the twenty-first century\nthat are presented in Rathjen (2018) and concern relationships\nbetween Feferman’s systems of explicit mathematics and\nMartin-Löf type theories; the paper touches also on the\ncompletely new developments of homotopy type theory (see Awodey 2012).\nSome additional contemporary proof theoretic developments are\ndescribed in appendices\n D,\n E and\n F.\n The theme of\n Appendix E,\n the extraction of computational information from proofs in particular\nformal theories, is the central topic of Schwichtenberg and\nWainer’s 2012. They deal with theories of arithmetic and\nanalysis up to \\((\\Pi^1_1{\\Hy}\\bCA)_0\\). Standard texts on proof\ntheory covering ordinal analysis are Takeuti 1985 and Schütte\n1977. First steps into ordinal analysis are taken in Pohlers 2009.\nFinally, some new directions of proof theoretic work are taken in\ncontributions to both Kahle and Rathjen 2015 and Jäger and\nSieg 2018. \nLet us also report on progress on the methodological issues the\nfinitist consistency program was to address. First of all, due to\nquite a bit of important historical work, we have a much better grasp\nof the evolution of the program in the 1920s and its roots in the\ndevelopment toward modern structuralist mathematics in the nineteenth\ncentury. The work of editing Hilbert’s unpublished lecture notes\nhas opened a treasure of\n information.[32]\n The connections to the development in nineteenth century mathematics\nare hinted at in\n Appendix A,\n but they are explored in greater depth, for example, in\nFerreirós 2008; Reck 2003, 2013; and the papers Sieg wrote on\nDedekind with Morris (forthcoming) and Schlimm (2005, 2017),\nrespectively. Secondly, as to the properly methodological issues, we\npresented some broad considerations in\n section 4.1,\n namely, that consistency proofs should be given relative to\n“constructive” theories. We did not make any remarks about\nwhat is characteristic of a constructive perspective and why such a\nperspective has not only a mathematical, but also a philosophical\npoint. There is, of course, a very rich literature. Let us point to\nsome informative sources: van Atten (2007) as defending\nBrouwer’s views, Martin-Löf (1984) as exposing the\nphilosophical motivation for his type theory, Feferman (1988, 2000)\ndiscussing the foundational point of proof theory, Bernays (1976) as\npresenting crucial aspects of an informed philosophy of mathematics,\nand (Sieg 2013) as\nexplicating (the context for) his reductive\nstructuralism. \nBack to proof theory: We have to admit that we neglected some\nclassical topics. One is the study of different proof systems and\ntheir relationships going back to Gentzen’s dissertation (1935).\nIn their Basic Proof Theory, Troelstra and Schwichtenberg\n(2000) give an excellent selection, but some important calculi such as\nthe Schütte proof systems are not covered (see, for example,\nSchütte 1960b, 1977). They also do not cover proof systems for\ntemporal and modal logic, neither are substructural logics\n presented.[33] \nA second omission concerns Bounded Arithmetic, where feasibility\nissues are a central focus: one studies formal theories with provably\nrecursive functions that form very small subclasses of the primitive\nrecursive ones. Indeed, the class of the provably total functions of\nBuss’ base theory is that of functions that can be computed in\npolynomial time, and there was the hope that proof theoretic\ninvestigations might contribute novel results in complexity theory. A\nthird omission concerns proof mining; that line of deep mathematical\ninvestigations using proof theoretic tools was initiated by Kreisel\n(1958, 1982) and Luckhardt (1989), but really perfected only by Kohlenbach\n(2007). We hinted at the work of his school at the very end of\n section 4.1. \nProof theory, as we described it, deals primarily with formal proofs\nor derivations. Hilbert aimed, however, as we pointed out in\n section 1,\n for a more general analysis of ordinary, informal mathematical\nproofs. For Gentzen in his 1936, “the objects of proof theory\nshall be the proofs carried out in mathematics proper” (p. 499). The aim of Hilbert and his\ncollaborators was undoubtedly to achieve a deeper mathematical and\nconceptual understanding, but also to find general methods of proof\nconstruction in formal calculi. This is now being pursued in the very\nactive area of using powerful computers for the interactive\nverification of proofs and programs as well as the fully automated\nsearch for proofs of mathematical\n theorems.[34]\n That can be pursued with a cognitive scientific purpose of modeling\nmathematical reasoning (see Sieg 2010 and Ganesalingam & Gowers 2017). It is clearly\nin the spirit of Hilbert who articulated matters in his second Hamburg\ntalk of 1927 as follows: \nThe formula game … has, besides its mathematical value, an\nimportant general philosophical significance. For this formula game is\ncarried out according to certain definite rules, in which the\ntechnique of our thinking is expressed. These rules form a closed\nsystem that can be discovered and definitively stated. \nThen he continues with a provocative statement about the cognitive\ngoal of proof theoretic investigations. \nThe fundamental idea of my proof theory is none other than to describe\nthe activity of our understanding, to make a protocol of the rules\naccording to which our thinking actually proceeds. (Hilbert 1927\n[1967: 475])  \nIt is clear to us, and it was clear to Hilbert, that mathematical\nthinking does not proceed in the strictly regimented ways imposed by\nan austere formal theory. Though formal rigor is crucial, it is not\nsufficient to shape proofs intelligibly or to discover them\nefficiently, even in pure logic. Recalling the principle that\nmathematics should solve problems “by a minimum of blind\ncalculation and a maximum of guiding thought”, we have to\ninvestigate the subtle interaction between understanding and\nreasoning, i.e., between introducing concepts and proving\ntheorems.","contact.mail":"sieg@cmu.edu","contact.domain":"cmu.edu"}]
